   #[1]alternate [2]edit this page [3]wikipedia (en)

long short-term memory

   from wikipedia, the free encyclopedia
   [4]jump to navigation [5]jump to search
   [6]machine learning and
   [7]data mining
   [8]kernel machine.svg
   problems
     * [9]classification
     * [10]id91
     * [11]regression
     * [12]anomaly detection
     * [13]automl
     * [14]association rules
     * [15]id23
     * [16]id170
     * [17]feature engineering
     * [18]id171
     * [19]online learning
     * [20]semi-supervised learning
     * [21]unsupervised learning
     * [22]learning to rank
     * [23]grammar induction

   [24]supervised learning
   ([25]classification     [26]regression)
     * [27]id90
     * [28]ensembles
          + [29]id112
          + [30]boosting
          + [31]id79
     * [32]id92
     * [33]id75
     * [34]naive bayes
     * [35]id158s
     * [36]id28
     * [37]id88
     * [38]relevance vector machine (rvm)
     * [39]support vector machine (id166)

   [40]id91
     * [41]birch
     * [42]cure
     * [43]hierarchical
     * [44]id116
     * [45]expectation   maximization (em)
     *
       [46]dbscan
     * [47]optics
     * [48]mean-shift

   [49]id84
     * [50]factor analysis
     * [51]cca
     * [52]ica
     * [53]lda
     * [54]nmf
     * [55]pca
     * [56]id167

   [57]id170
     * [58]id114
          + [59]bayes net
          + [60]conditional random field
          + [61]hidden markov

   [62]anomaly detection
     * [63]id92
     * [64]local outlier factor

   [65]id158s
     * [66]autoencoder
     * [67]deep learning
     * [68]deepdream
     * [69]multilayer id88
     * [70]id56
          + lstm
          + [71]gru)
     * [72]restricted id82
     * [73]som
     * [74]convolutional neural network
          + [75]u-net

   [76]id23
     * [77]id24
     * [78]sarsa
     * [79]temporal difference (td)

   theory
     * [80]bias-variance dilemma
     * [81]computational learning theory
     * [82]empirical risk minimization
     * [83]occam learning
     * [84]pac learning
     * [85]statistical learning
     * [86]vc theory

   machine-learning venues
     * [87]nips
     * [88]icml
     * [89]ml
     * [90]jmlr
     * [91]arxiv:cs.lg

   [92]glossary of artificial intelligence
     * [93]glossary of artificial intelligence

   related articles
     * [94]list of datasets for machine-learning research
     * [95]outline of machine learning

     * [96]portal-puzzle.svg [97]machine learning portal

     * [98]v
     * [99]t
     * [100]e

   the long short-term memory (lstm) cell can process data sequentially
   and keep its hidden state through time.

   long short-term memory (lstm) is an artificial [101]recurrent neural
   network (id56) architecture^[102][1] used in the field of [103]deep
   learning. unlike standard [104]feedforward neural networks, lstm has
   feedback connections that make it a "general purpose computer" (that
   is, it can compute anything that a [105]turing machine can).^[106][2]
   it cannot only process single data points (such as images), but also
   entire sequences of data (such as speech or video). for example, lstm
   is applicable to tasks such as unsegmented, connected [107]handwriting
   recognition^[108][3] or [109]id103.^[110][4]^[111][5]
   [112]bloomberg business week wrote: "these powers make lstm arguably
   the most commercial ai achievement, used for everything from predicting
   diseases to composing music."^[113][6]

   a common lstm unit is composed of a cell, an input gate, an output gate
   and a forget gate. the cell remembers values over arbitrary time
   intervals and the three gates regulate the flow of information into and
   out of the cell.

   id137 are well-suited to [114]classifying, [115]processing and
   [116]making predictions based on [117]time series data, since there can
   be lags of unknown duration between important events in a time series.
   lstms were developed to deal with the exploding and [118]vanishing
   gradient problems that can be encountered when training traditional
   id56s. relative insensitivity to gap length is an advantage of lstm over
   id56s, [119]id48 and other sequence learning methods in
   numerous applications^[[120]citation needed].
   [ ]

contents

     * [121]1 history
     * [122]2 idea
     * [123]3 architecture
     * [124]4 variants
          + [125]4.1 lstm with a forget gate
               o [126]4.1.1 variables
               o [127]4.1.2 id180
          + [128]4.2 peephole lstm
          + [129]4.3 peephole convolutional lstm
     * [130]5 training
          + [131]5.1 ctc score function
          + [132]5.2 alternatives
               o [133]5.2.1 success
     * [134]6 applications
     * [135]7 see also
     * [136]8 references
     * [137]9 external links

history[[138]edit]

   lstm was proposed in 1997 by [139]sepp hochreiter and [140]j  rgen
   schmidhuber.^[141][1] by introducing constant error carousel (cec)
   units, lstm deals with the exploding and vanishing gradient problems.
   the initial version of lstm block included cells, input and output
   gates.^[142][7]

   in 1999, [143]felix gers and his advisor [144]j  rgen schmidhuber and
   fred cummins introduced the forget gate (also called    keep gate   ) into
   lstm architecture,^[145][8] enabling the lstm to reset its own
   state.^[146][7]

   in 2000, gers & schmidhuber & cummins added peephole connections
   (connections from the cell to the gates) into the
   architecture.^[147][9] additionally, the output activation function was
   omitted.^[148][7]

   in 2014, kyunghyun cho et al. put forward a simplified variant called
   [149]gated recurrent unit (gru).^[150][10]

   among other successes, lstm achieved record results in natural language
   text compression,^[151][11] unsegmented connected [152]handwriting
   recognition^[153][12] and won the [154]icdar handwriting competition
   (2009). id137 were a major component of a network that achieved
   a record 17.7% [155]phoneme error rate on the classic [156]timit
   natural speech dataset (2013).^[157][13]

   as of 2016, major technology companies including [158]google,
   [159]apple, and [160]microsoft were using lstm as fundamental
   components in new products.^[161][14] for example, google used lstm for
   id103 on the [162]smartphone,^[163][15]^[164][16] for the
   smart assistant allo^[165][17] and for [166]google
   translate.^[167][18]^[168][19] [169]apple uses lstm for the "quicktype"
   function on the [170]iphone^[171][20]^[172][21] and for
   [173]siri.^[174][22] [175]amazon uses lstm for [176]amazon
   alexa.^[177][23]

   in 2017, facebook performed some 4.5 billion automatic translations
   every day using id137.^[178][24]

   in 2017, researchers from [179]michigan state university, [180]ibm
   research, and [181]cornell university published a study in the
   knowledge discovery and data mining (kdd)
   conference.^[182][25]^[183][26]^[184][27] their study describes a novel
   neural network that performs better on certain data sets than the
   widely used long short-term memory neural network.

   further in 2017 microsoft reported reaching 95.1% recognition accuracy
   on the switchboard corpus, incorporating a vocabulary of 165,000 words.
   the approach used "dialog session-based long-short-term
   memory".^[185][28]

idea[[186]edit]

   in theory, classic (or "vanilla") [187]id56s can keep track of arbitrary
   long-term dependencies in the input sequences. the problem of vanilla
   id56s is computational (or practical) in nature: when training a vanilla
   id56 using [188]back-propagation, the gradients which are
   back-propagated can [189]"vanish" (that is, they can tend to zero) or
   "explode" (that is, they can tend to infinity), because of the
   computations involved in the process, which use [190]finite-precision
   numbers. id56s using lstm units partially solve the [191]vanishing
   gradient problem, because lstm units allow gradients to also flow
   unchanged. however, id137 can still suffer from the exploding
   gradient problem.^[192][29]

architecture[[193]edit]

   there are several architectures of lstm units. a common architecture is
   composed of a cell (the memory part of the lstm unit) and three
   "regulators", usually called gates, of the flow of information inside
   the lstm unit: an input gate, an output gate and a forget gate. some
   variations of the lstm unit do not have one or more of these gates or
   maybe have other gates. for example, [194]id149 (grus)
   do not have an output gate.

   intuitively, the cell is responsible for keeping track of the
   dependencies between the elements in the input sequence. the input gate
   controls the extent to which a new value flows into the cell, the
   forget gate controls the extent to which a value remains in the cell
   and the output gate controls the extent to which the value in the cell
   is used to compute the output activation of the lstm unit. the
   activation function of the lstm gates is often the [195]logistic
   function.

   there are connections into and out of the lstm gates, a few of which
   are recurrent. the weights of these connections, which need to be
   learned during [196]training, determine how the gates operate.

variants[[197]edit]

   in the equations below, the lowercase variables represent vectors.
   matrices
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>w</mi> <mrow
   class="mjx-texatom-ord"> <mi>q</mi> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle
   w_{q}}</annotation> </semantics> :math]
   {\displaystyle w_{q}} and
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>u</mi> <mrow
   class="mjx-texatom-ord"> <mi>q</mi> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle
   u_{q}}</annotation> </semantics> :math]
   {\displaystyle u_{q}} contain, respectively, the weights of the input
   and recurrent connections, where the subscript
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi></mi> <mrow
   class="mjx-texatom-ord"> <mi>q</mi> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle
   _{q}}</annotation> </semantics> :math]
   {\displaystyle _{q}} can either be the input gate
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>i</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle i}</annotation>
   </semantics> :math]
   i , output gate
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>o</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle o}</annotation>
   </semantics> :math]
   o , the forget gate
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>f</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
   </semantics> :math]
   f or the memory cell
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>c</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle c}</annotation>
   </semantics> :math]
   c , depending on the activation being calculated. in this section, we
   are thus using a "vector notation". so, for example,
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>c</mi> <mrow
   class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> <mo>   </mo> <msup>
   <mrow class="mjx-texatom-ord"> <mi mathvariant="double-struck">r</mi>
   </mrow> <mrow class="mjx-texatom-ord"> <mi>h</mi> </mrow> </msup>
   </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle c_{t}\in \mathbb {r}
   ^{h}}</annotation> </semantics> :math]
   {\displaystyle c_{t}\in \mathbb {r} ^{h}} is not just one cell of one
   lstm unit, but contains
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>h</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle h}</annotation>
   </semantics> :math]
   h lstm unit's cells.

lstm with a forget gate[[198]edit]

   the compact forms of the equations for the forward pass of an lstm unit
   with a forget gate are:^[199][1]^[200][9]

          [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
          displaystyle="true" scriptlevel="0"> <mrow
          class="mjx-texatom-ord"> <mtable columnalign="right left right
          left right left right left right left right left"
          rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em
          0em 2em 0em" displaystyle="true"> <mtr> <mtd> <msub> <mi>f</mi>
          <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> </mtd>
          <mtd> <mi></mi> <mo>=</mo> <msub> <mi>  </mi> <mrow
          class="mjx-texatom-ord"> <mi>g</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>w</mi> <mrow
          class="mjx-texatom-ord"> <mi>f</mi> </mrow> </msub> <msub>
          <mi>x</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> <mo>+</mo> <msub> <mi>u</mi> <mrow
          class="mjx-texatom-ord"> <mi>f</mi> </mrow> </msub> <msub>
          <mi>h</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo>
          <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub> <mi>b</mi> <mrow
          class="mjx-texatom-ord"> <mi>f</mi> </mrow> </msub> <mo
          stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <msub>
          <mi>i</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> </mtd> <mtd> <mi></mi> <mo>=</mo> <msub> <mi>  </mi>
          <mrow class="mjx-texatom-ord"> <mi>g</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>w</mi> <mrow
          class="mjx-texatom-ord"> <mi>i</mi> </mrow> </msub> <msub>
          <mi>x</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> <mo>+</mo> <msub> <mi>u</mi> <mrow
          class="mjx-texatom-ord"> <mi>i</mi> </mrow> </msub> <msub>
          <mi>h</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo>
          <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub> <mi>b</mi> <mrow
          class="mjx-texatom-ord"> <mi>i</mi> </mrow> </msub> <mo
          stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <msub>
          <mi>o</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> </mtd> <mtd> <mi></mi> <mo>=</mo> <msub> <mi>  </mi>
          <mrow class="mjx-texatom-ord"> <mi>g</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>w</mi> <mrow
          class="mjx-texatom-ord"> <mi>o</mi> </mrow> </msub> <msub>
          <mi>x</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> <mo>+</mo> <msub> <mi>u</mi> <mrow
          class="mjx-texatom-ord"> <mi>o</mi> </mrow> </msub> <msub>
          <mi>h</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo>
          <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub> <mi>b</mi> <mrow
          class="mjx-texatom-ord"> <mi>o</mi> </mrow> </msub> <mo
          stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <msub>
          <mi>c</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> </mtd> <mtd> <mi></mi> <mo>=</mo> <msub> <mi>f</mi>
          <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub>
          <mo>   </mo> <msub> <mi>c</mi> <mrow class="mjx-texatom-ord">
          <mi>t</mi> <mo>   </mo> <mn>1</mn> </mrow> </msub> <mo>+</mo>
          <msub> <mi>i</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi>
          </mrow> </msub> <mo>   </mo> <msub> <mi>  </mi> <mrow
          class="mjx-texatom-ord"> <mi>c</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>w</mi> <mrow
          class="mjx-texatom-ord"> <mi>c</mi> </mrow> </msub> <msub>
          <mi>x</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> <mo>+</mo> <msub> <mi>u</mi> <mrow
          class="mjx-texatom-ord"> <mi>c</mi> </mrow> </msub> <msub>
          <mi>h</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo>
          <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub> <mi>b</mi> <mrow
          class="mjx-texatom-ord"> <mi>c</mi> </mrow> </msub> <mo
          stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <msub>
          <mi>h</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> </mtd> <mtd> <mi></mi> <mo>=</mo> <msub> <mi>o</mi>
          <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub>
          <mo>   </mo> <msub> <mi>  </mi> <mrow class="mjx-texatom-ord">
          <mi>h</mi> </mrow> </msub> <mo stretchy="false">(</mo> <msub>
          <mi>c</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> <mo stretchy="false">)</mo> </mtd> </mtr> </mtable>
          </mrow> </mstyle> </mrow> <annotation
          encoding="application/x-tex">{\displaystyle
          {\begin{aligned}f_{t}&=\sigma
          _{g}(w_{f}x_{t}+u_{f}h_{t-1}+b_{f})\\i_{t}&=\sigma
          _{g}(w_{i}x_{t}+u_{i}h_{t-1}+b_{i})\\o_{t}&=\sigma
          _{g}(w_{o}x_{t}+u_{o}h_{t-1}+b_{o})\\c_{t}&=f_{t}\circ
          c_{t-1}+i_{t}\circ \sigma
          _{c}(w_{c}x_{t}+u_{c}h_{t-1}+b_{c})\\h_{t}&=o_{t}\circ \sigma
          _{h}(c_{t})\end{aligned}}}</annotation> </semantics> :math]
          {\displaystyle {\begin{aligned}f_{t}&=\sigma
          _{g}(w_{f}x_{t}+u_{f}h_{t-1}+b_{f})\\i_{t}&=\sigma
          _{g}(w_{i}x_{t}+u_{i}h_{t-1}+b_{i})\\o_{t}&=\sigma
          _{g}(w_{o}x_{t}+u_{o}h_{t-1}+b_{o})\\c_{t}&=f_{t}\circ
          c_{t-1}+i_{t}\circ \sigma
          _{c}(w_{c}x_{t}+u_{c}h_{t-1}+b_{c})\\h_{t}&=o_{t}\circ \sigma
          _{h}(c_{t})\end{aligned}}}

   where the initial values are
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>c</mi> <mrow
   class="mjx-texatom-ord"> <mn>0</mn> </mrow> </msub> <mo>=</mo>
   <mn>0</mn> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle c_{0}=0}</annotation>
   </semantics> :math]
   c_{0}=0 and
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>h</mi> <mrow
   class="mjx-texatom-ord"> <mn>0</mn> </mrow> </msub> <mo>=</mo>
   <mn>0</mn> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle h_{0}=0}</annotation>
   </semantics> :math]
   {\displaystyle h_{0}=0} and the operator
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mo>   </mo> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle \circ
   }</annotation> </semantics> :math]
   \circ denotes the [201]hadamard product (element-wise product). the
   subscript
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>t</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle t}</annotation>
   </semantics> :math]
   t indexes the time step.

variables[[202]edit]

     *
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <msub> <mi>x</mi> <mrow
       class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> <mo>   </mo>
       <msup> <mrow class="mjx-texatom-ord"> <mi
       mathvariant="double-struck">r</mi> </mrow> <mrow
       class="mjx-texatom-ord"> <mi>d</mi> </mrow> </msup> </mstyle>
       </mrow> <annotation encoding="application/x-tex">{\displaystyle
       x_{t}\in \mathbb {r} ^{d}}</annotation> </semantics> :math]
       {\displaystyle x_{t}\in \mathbb {r} ^{d}} : input vector to the
       lstm unit
     *
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <msub> <mi>f</mi> <mrow
       class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> <mo>   </mo>
       <msup> <mrow class="mjx-texatom-ord"> <mi
       mathvariant="double-struck">r</mi> </mrow> <mrow
       class="mjx-texatom-ord"> <mi>h</mi> </mrow> </msup> </mstyle>
       </mrow> <annotation encoding="application/x-tex">{\displaystyle
       f_{t}\in \mathbb {r} ^{h}}</annotation> </semantics> :math]
       {\displaystyle f_{t}\in \mathbb {r} ^{h}} : forget gate's
       activation vector
     *
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <msub> <mi>i</mi> <mrow
       class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> <mo>   </mo>
       <msup> <mrow class="mjx-texatom-ord"> <mi
       mathvariant="double-struck">r</mi> </mrow> <mrow
       class="mjx-texatom-ord"> <mi>h</mi> </mrow> </msup> </mstyle>
       </mrow> <annotation encoding="application/x-tex">{\displaystyle
       i_{t}\in \mathbb {r} ^{h}}</annotation> </semantics> :math]
       {\displaystyle i_{t}\in \mathbb {r} ^{h}} : input gate's activation
       vector
     *
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <msub> <mi>o</mi> <mrow
       class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> <mo>   </mo>
       <msup> <mrow class="mjx-texatom-ord"> <mi
       mathvariant="double-struck">r</mi> </mrow> <mrow
       class="mjx-texatom-ord"> <mi>h</mi> </mrow> </msup> </mstyle>
       </mrow> <annotation encoding="application/x-tex">{\displaystyle
       o_{t}\in \mathbb {r} ^{h}}</annotation> </semantics> :math]
       {\displaystyle o_{t}\in \mathbb {r} ^{h}} : output gate's
       activation vector
     *
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <msub> <mi>h</mi> <mrow
       class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> <mo>   </mo>
       <msup> <mrow class="mjx-texatom-ord"> <mi
       mathvariant="double-struck">r</mi> </mrow> <mrow
       class="mjx-texatom-ord"> <mi>h</mi> </mrow> </msup> </mstyle>
       </mrow> <annotation encoding="application/x-tex">{\displaystyle
       h_{t}\in \mathbb {r} ^{h}}</annotation> </semantics> :math]
       {\displaystyle h_{t}\in \mathbb {r} ^{h}} : hidden state vector
       also known as output vector of the lstm unit
     *
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <msub> <mi>c</mi> <mrow
       class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> <mo>   </mo>
       <msup> <mrow class="mjx-texatom-ord"> <mi
       mathvariant="double-struck">r</mi> </mrow> <mrow
       class="mjx-texatom-ord"> <mi>h</mi> </mrow> </msup> </mstyle>
       </mrow> <annotation encoding="application/x-tex">{\displaystyle
       c_{t}\in \mathbb {r} ^{h}}</annotation> </semantics> :math]
       {\displaystyle c_{t}\in \mathbb {r} ^{h}} : cell state vector
     *
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <mi>w</mi> <mo>   </mo> <msup>
       <mrow class="mjx-texatom-ord"> <mi
       mathvariant="double-struck">r</mi> </mrow> <mrow
       class="mjx-texatom-ord"> <mi>h</mi> <mo>  </mo> <mi>d</mi> </mrow>
       </msup> </mstyle> </mrow> <annotation
       encoding="application/x-tex">{\displaystyle w\in \mathbb {r}
       ^{h\times d}}</annotation> </semantics> :math]
       {\displaystyle w\in \mathbb {r} ^{h\times d}} ,
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <mi>u</mi> <mo>   </mo> <msup>
       <mrow class="mjx-texatom-ord"> <mi
       mathvariant="double-struck">r</mi> </mrow> <mrow
       class="mjx-texatom-ord"> <mi>h</mi> <mo>  </mo> <mi>h</mi> </mrow>
       </msup> </mstyle> </mrow> <annotation
       encoding="application/x-tex">{\displaystyle u\in \mathbb {r}
       ^{h\times h}}</annotation> </semantics> :math]
       {\displaystyle u\in \mathbb {r} ^{h\times h}} and
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <mi>b</mi> <mo>   </mo> <msup>
       <mrow class="mjx-texatom-ord"> <mi
       mathvariant="double-struck">r</mi> </mrow> <mrow
       class="mjx-texatom-ord"> <mi>h</mi> </mrow> </msup> </mstyle>
       </mrow> <annotation encoding="application/x-tex">{\displaystyle
       b\in \mathbb {r} ^{h}}</annotation> </semantics> :math]
       {\displaystyle b\in \mathbb {r} ^{h}} : weight matrices and bias
       vector parameters which need to be learned during training

   where the superscripts
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>d</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle d}</annotation>
   </semantics> :math]
   d and
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>h</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle h}</annotation>
   </semantics> :math]
   h refer to the number of input features and number of hidden units,
   respectively.

[203]id180[[204]edit]

     *
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <msub> <mi>  </mi> <mrow
       class="mjx-texatom-ord"> <mi>g</mi> </mrow> </msub> </mstyle>
       </mrow> <annotation encoding="application/x-tex">{\displaystyle
       \sigma _{g}}</annotation> </semantics> :math]
       \sigma _{g} : [205]sigmoid function.
     *
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <msub> <mi>  </mi> <mrow
       class="mjx-texatom-ord"> <mi>c</mi> </mrow> </msub> </mstyle>
       </mrow> <annotation encoding="application/x-tex">{\displaystyle
       \sigma _{c}}</annotation> </semantics> :math]
       \sigma_c : [206]hyperbolic tangent function.
     *
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <msub> <mi>  </mi> <mrow
       class="mjx-texatom-ord"> <mi>h</mi> </mrow> </msub> </mstyle>
       </mrow> <annotation encoding="application/x-tex">{\displaystyle
       \sigma _{h}}</annotation> </semantics> :math]
       \sigma _{h} : hyperbolic tangent function or, as the peephole lstm
       paper^[207][30]^[208][31] suggests,
       [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
       displaystyle="true" scriptlevel="0"> <msub> <mi>  </mi> <mrow
       class="mjx-texatom-ord"> <mi>h</mi> </mrow> </msub> <mo
       stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo>
       <mo>=</mo> <mi>x</mi> </mstyle> </mrow> <annotation
       encoding="application/x-tex">{\displaystyle \sigma
       _{h}(x)=x}</annotation> </semantics> :math]
       {\displaystyle \sigma _{h}(x)=x} .

peephole lstm[[209]edit]

   a [210]peephole lstm unit with input (i.e.
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>i</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle i}</annotation>
   </semantics> :math]
   i ), output (i.e.
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>o</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle o}</annotation>
   </semantics> :math]
   o ), and forget (i.e.
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>f</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
   </semantics> :math]
   f ) gates. each of these gates can be thought as a "standard" neuron in
   a feed-forward (or multi-layer) neural network: that is, they compute
   an activation (using an activation function) of a weighted sum.
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>i</mi> <mrow
   class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> <mo>,</mo> <msub>
   <mi>o</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub>
   </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle i_{t},o_{t}}</annotation>
   </semantics> :math]
   {\displaystyle i_{t},o_{t}} and
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>f</mi> <mrow
   class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle
   f_{t}}</annotation> </semantics> :math]
   f_{t} represent the activations of respectively the input, output and
   forget gates, at time step
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>t</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle t}</annotation>
   </semantics> :math]
   t . the 3 exit arrows from the memory cell
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>c</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle c}</annotation>
   </semantics> :math]
   c to the 3 gates
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>i</mi> <mo>,</mo> <mi>o</mi>
   </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle i,o}</annotation>
   </semantics> :math]
   {\displaystyle i,o} and
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>f</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
   </semantics> :math]
   f represent the peephole connections. these peephole connections
   actually denote the contributions of the activation of the memory cell
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>c</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle c}</annotation>
   </semantics> :math]
   c at time step
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>t</mi> <mo>   </mo> <mn>1</mn>
   </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle t-1}</annotation>
   </semantics> :math]
   t-1 , i.e. the contribution of
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>c</mi> <mrow
   class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo> <mn>1</mn> </mrow>
   </msub> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle c_{t-1}}</annotation>
   </semantics> :math]
   {\displaystyle c_{t-1}} (and not
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>c</mi> <mrow
   class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle
   c_{t}}</annotation> </semantics> :math]
   {\displaystyle c_{t}} , as the picture may suggest). in other words,
   the gates
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>i</mi> <mo>,</mo> <mi>o</mi>
   </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle i,o}</annotation>
   </semantics> :math]
   {\displaystyle i,o} and
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>f</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle f}</annotation>
   </semantics> :math]
   f calculate their activations at time step
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>t</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle t}</annotation>
   </semantics> :math]
   t (i.e., respectively,
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>i</mi> <mrow
   class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> <mo>,</mo> <msub>
   <mi>o</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub>
   </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle i_{t},o_{t}}</annotation>
   </semantics> :math]
   {\displaystyle i_{t},o_{t}} and
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>f</mi> <mrow
   class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle
   f_{t}}</annotation> </semantics> :math]
   f_{t} ) also considering the activation of the memory cell
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>c</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle c}</annotation>
   </semantics> :math]
   c at time step
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>t</mi> <mo>   </mo> <mn>1</mn>
   </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle t-1}</annotation>
   </semantics> :math]
   t-1 , i.e.
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>c</mi> <mrow
   class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo> <mn>1</mn> </mrow>
   </msub> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle c_{t-1}}</annotation>
   </semantics> :math]
   {\displaystyle c_{t-1}} . the single left-to-right arrow exiting the
   memory cell is not a peephole connection and denotes
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>c</mi> <mrow
   class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle
   c_{t}}</annotation> </semantics> :math]
   {\displaystyle c_{t}} . the little circles containing a
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mo>  </mo> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle \times
   }</annotation> </semantics> :math]
   \times symbol represent an element-wise multiplication between its
   inputs. the big circles containing an s-like curve represent the
   application of a differentiable function (like the sigmoid function) to
   a weighted sum. there are many other kinds of lstms as well.^[211][7]

   the figure on the right is a graphical representation of an lstm unit
   with peephole connections (i.e. a peephole lstm).^[212][30]^[213][31]
   peephole connections allow the gates to access the constant error
   carousel (cec), whose activation is the cell state.^[214][32]
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>h</mi> <mrow
   class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo> <mn>1</mn> </mrow>
   </msub> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle h_{t-1}}</annotation>
   </semantics> :math]
   {\displaystyle h_{t-1}} is not used,
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <msub> <mi>c</mi> <mrow
   class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo> <mn>1</mn> </mrow>
   </msub> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle c_{t-1}}</annotation>
   </semantics> :math]
   {\displaystyle c_{t-1}} is used instead in most places.

          [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
          displaystyle="true" scriptlevel="0"> <mrow
          class="mjx-texatom-ord"> <mtable columnalign="right left right
          left right left right left right left right left"
          rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em
          0em 2em 0em" displaystyle="true"> <mtr> <mtd> <msub> <mi>f</mi>
          <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> </mtd>
          <mtd> <mi></mi> <mo>=</mo> <msub> <mi>  </mi> <mrow
          class="mjx-texatom-ord"> <mi>g</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>w</mi> <mrow
          class="mjx-texatom-ord"> <mi>f</mi> </mrow> </msub> <msub>
          <mi>x</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> <mo>+</mo> <msub> <mi>u</mi> <mrow
          class="mjx-texatom-ord"> <mi>f</mi> </mrow> </msub> <msub>
          <mi>c</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo>
          <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub> <mi>b</mi> <mrow
          class="mjx-texatom-ord"> <mi>f</mi> </mrow> </msub> <mo
          stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <msub>
          <mi>i</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> </mtd> <mtd> <mi></mi> <mo>=</mo> <msub> <mi>  </mi>
          <mrow class="mjx-texatom-ord"> <mi>g</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>w</mi> <mrow
          class="mjx-texatom-ord"> <mi>i</mi> </mrow> </msub> <msub>
          <mi>x</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> <mo>+</mo> <msub> <mi>u</mi> <mrow
          class="mjx-texatom-ord"> <mi>i</mi> </mrow> </msub> <msub>
          <mi>c</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo>
          <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub> <mi>b</mi> <mrow
          class="mjx-texatom-ord"> <mi>i</mi> </mrow> </msub> <mo
          stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <msub>
          <mi>o</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> </mtd> <mtd> <mi></mi> <mo>=</mo> <msub> <mi>  </mi>
          <mrow class="mjx-texatom-ord"> <mi>g</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>w</mi> <mrow
          class="mjx-texatom-ord"> <mi>o</mi> </mrow> </msub> <msub>
          <mi>x</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> <mo>+</mo> <msub> <mi>u</mi> <mrow
          class="mjx-texatom-ord"> <mi>o</mi> </mrow> </msub> <msub>
          <mi>c</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo>
          <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub> <mi>b</mi> <mrow
          class="mjx-texatom-ord"> <mi>o</mi> </mrow> </msub> <mo
          stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <msub>
          <mi>c</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> </mtd> <mtd> <mi></mi> <mo>=</mo> <msub> <mi>f</mi>
          <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub>
          <mo>   </mo> <msub> <mi>c</mi> <mrow class="mjx-texatom-ord">
          <mi>t</mi> <mo>   </mo> <mn>1</mn> </mrow> </msub> <mo>+</mo>
          <msub> <mi>i</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi>
          </mrow> </msub> <mo>   </mo> <msub> <mi>  </mi> <mrow
          class="mjx-texatom-ord"> <mi>c</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>w</mi> <mrow
          class="mjx-texatom-ord"> <mi>c</mi> </mrow> </msub> <msub>
          <mi>x</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> <mo>+</mo> <msub> <mi>u</mi> <mrow
          class="mjx-texatom-ord"> <mi>c</mi> </mrow> </msub> <msub>
          <mi>c</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo>
          <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub> <mi>b</mi> <mrow
          class="mjx-texatom-ord"> <mi>c</mi> </mrow> </msub> <mo
          stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <msub>
          <mi>h</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> </mtd> <mtd> <mi></mi> <mo>=</mo> <msub> <mi>o</mi>
          <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub>
          <mo>   </mo> <msub> <mi>  </mi> <mrow class="mjx-texatom-ord">
          <mi>h</mi> </mrow> </msub> <mo stretchy="false">(</mo> <msub>
          <mi>c</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> <mo stretchy="false">)</mo> </mtd> </mtr> </mtable>
          </mrow> </mstyle> </mrow> <annotation
          encoding="application/x-tex">{\displaystyle
          {\begin{aligned}f_{t}&=\sigma
          _{g}(w_{f}x_{t}+u_{f}c_{t-1}+b_{f})\\i_{t}&=\sigma
          _{g}(w_{i}x_{t}+u_{i}c_{t-1}+b_{i})\\o_{t}&=\sigma
          _{g}(w_{o}x_{t}+u_{o}c_{t-1}+b_{o})\\c_{t}&=f_{t}\circ
          c_{t-1}+i_{t}\circ \sigma
          _{c}(w_{c}x_{t}+u_{c}c_{t-1}+b_{c})\\h_{t}&=o_{t}\circ \sigma
          _{h}(c_{t})\end{aligned}}}</annotation> </semantics> :math]
          {\displaystyle {\begin{aligned}f_{t}&=\sigma
          _{g}(w_{f}x_{t}+u_{f}c_{t-1}+b_{f})\\i_{t}&=\sigma
          _{g}(w_{i}x_{t}+u_{i}c_{t-1}+b_{i})\\o_{t}&=\sigma
          _{g}(w_{o}x_{t}+u_{o}c_{t-1}+b_{o})\\c_{t}&=f_{t}\circ
          c_{t-1}+i_{t}\circ \sigma
          _{c}(w_{c}x_{t}+u_{c}c_{t-1}+b_{c})\\h_{t}&=o_{t}\circ \sigma
          _{h}(c_{t})\end{aligned}}}

peephole convolutional lstm[[215]edit]

   peephole [216]convolutional lstm.^[217][33] the
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mo>   </mo> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle *}</annotation>
   </semantics> :math]
   * denotes the [218]convolution operator.

          [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
          displaystyle="true" scriptlevel="0"> <mrow
          class="mjx-texatom-ord"> <mtable columnalign="right left right
          left right left right left right left right left"
          rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em
          0em 2em 0em" displaystyle="true"> <mtr> <mtd> <msub> <mi>f</mi>
          <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> </mtd>
          <mtd> <mi></mi> <mo>=</mo> <msub> <mi>  </mi> <mrow
          class="mjx-texatom-ord"> <mi>g</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>w</mi> <mrow
          class="mjx-texatom-ord"> <mi>f</mi> </mrow> </msub> <mo>   </mo>
          <msub> <mi>x</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi>
          </mrow> </msub> <mo>+</mo> <msub> <mi>u</mi> <mrow
          class="mjx-texatom-ord"> <mi>f</mi> </mrow> </msub> <mo>   </mo>
          <msub> <mi>h</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi>
          <mo>   </mo> <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub>
          <mi>v</mi> <mrow class="mjx-texatom-ord"> <mi>f</mi> </mrow>
          </msub> <mo>   </mo> <msub> <mi>c</mi> <mrow
          class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo> <mn>1</mn>
          </mrow> </msub> <mo>+</mo> <msub> <mi>b</mi> <mrow
          class="mjx-texatom-ord"> <mi>f</mi> </mrow> </msub> <mo
          stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <msub>
          <mi>i</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> </mtd> <mtd> <mi></mi> <mo>=</mo> <msub> <mi>  </mi>
          <mrow class="mjx-texatom-ord"> <mi>g</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>w</mi> <mrow
          class="mjx-texatom-ord"> <mi>i</mi> </mrow> </msub> <mo>   </mo>
          <msub> <mi>x</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi>
          </mrow> </msub> <mo>+</mo> <msub> <mi>u</mi> <mrow
          class="mjx-texatom-ord"> <mi>i</mi> </mrow> </msub> <mo>   </mo>
          <msub> <mi>h</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi>
          <mo>   </mo> <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub>
          <mi>v</mi> <mrow class="mjx-texatom-ord"> <mi>i</mi> </mrow>
          </msub> <mo>   </mo> <msub> <mi>c</mi> <mrow
          class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo> <mn>1</mn>
          </mrow> </msub> <mo>+</mo> <msub> <mi>b</mi> <mrow
          class="mjx-texatom-ord"> <mi>i</mi> </mrow> </msub> <mo
          stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <msub>
          <mi>o</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> </mtd> <mtd> <mi></mi> <mo>=</mo> <msub> <mi>  </mi>
          <mrow class="mjx-texatom-ord"> <mi>g</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>w</mi> <mrow
          class="mjx-texatom-ord"> <mi>o</mi> </mrow> </msub> <mo>   </mo>
          <msub> <mi>x</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi>
          </mrow> </msub> <mo>+</mo> <msub> <mi>u</mi> <mrow
          class="mjx-texatom-ord"> <mi>o</mi> </mrow> </msub> <mo>   </mo>
          <msub> <mi>h</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi>
          <mo>   </mo> <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub>
          <mi>v</mi> <mrow class="mjx-texatom-ord"> <mi>o</mi> </mrow>
          </msub> <mo>   </mo> <msub> <mi>c</mi> <mrow
          class="mjx-texatom-ord"> <mi>t</mi> <mo>   </mo> <mn>1</mn>
          </mrow> </msub> <mo>+</mo> <msub> <mi>b</mi> <mrow
          class="mjx-texatom-ord"> <mi>o</mi> </mrow> </msub> <mo
          stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <msub>
          <mi>c</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> </mtd> <mtd> <mi></mi> <mo>=</mo> <msub> <mi>f</mi>
          <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub>
          <mo>   </mo> <msub> <mi>c</mi> <mrow class="mjx-texatom-ord">
          <mi>t</mi> <mo>   </mo> <mn>1</mn> </mrow> </msub> <mo>+</mo>
          <msub> <mi>i</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi>
          </mrow> </msub> <mo>   </mo> <msub> <mi>  </mi> <mrow
          class="mjx-texatom-ord"> <mi>c</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>w</mi> <mrow
          class="mjx-texatom-ord"> <mi>c</mi> </mrow> </msub> <mo>   </mo>
          <msub> <mi>x</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi>
          </mrow> </msub> <mo>+</mo> <msub> <mi>u</mi> <mrow
          class="mjx-texatom-ord"> <mi>c</mi> </mrow> </msub> <mo>   </mo>
          <msub> <mi>h</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi>
          <mo>   </mo> <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub>
          <mi>b</mi> <mrow class="mjx-texatom-ord"> <mi>c</mi> </mrow>
          </msub> <mo stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd>
          <msub> <mi>h</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi>
          </mrow> </msub> </mtd> <mtd> <mi></mi> <mo>=</mo> <msub>
          <mi>o</mi> <mrow class="mjx-texatom-ord"> <mi>t</mi> </mrow>
          </msub> <mo>   </mo> <msub> <mi>  </mi> <mrow
          class="mjx-texatom-ord"> <mi>h</mi> </mrow> </msub> <mo
          stretchy="false">(</mo> <msub> <mi>c</mi> <mrow
          class="mjx-texatom-ord"> <mi>t</mi> </mrow> </msub> <mo
          stretchy="false">)</mo> </mtd> </mtr> </mtable> </mrow>
          </mstyle> </mrow> <annotation
          encoding="application/x-tex">{\displaystyle
          {\begin{aligned}f_{t}&=\sigma
          _{g}(w_{f}*x_{t}+u_{f}*h_{t-1}+v_{f}\circ
          c_{t-1}+b_{f})\\i_{t}&=\sigma
          _{g}(w_{i}*x_{t}+u_{i}*h_{t-1}+v_{i}\circ
          c_{t-1}+b_{i})\\o_{t}&=\sigma
          _{g}(w_{o}*x_{t}+u_{o}*h_{t-1}+v_{o}\circ
          c_{t-1}+b_{o})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma
          _{c}(w_{c}*x_{t}+u_{c}*h_{t-1}+b_{c})\\h_{t}&=o_{t}\circ \sigma
          _{h}(c_{t})\end{aligned}}}</annotation> </semantics> :math]
          {\displaystyle {\begin{aligned}f_{t}&=\sigma
          _{g}(w_{f}*x_{t}+u_{f}*h_{t-1}+v_{f}\circ
          c_{t-1}+b_{f})\\i_{t}&=\sigma
          _{g}(w_{i}*x_{t}+u_{i}*h_{t-1}+v_{i}\circ
          c_{t-1}+b_{i})\\o_{t}&=\sigma
          _{g}(w_{o}*x_{t}+u_{o}*h_{t-1}+v_{o}\circ
          c_{t-1}+b_{o})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma
          _{c}(w_{c}*x_{t}+u_{c}*h_{t-1}+b_{c})\\h_{t}&=o_{t}\circ \sigma
          _{h}(c_{t})\end{aligned}}}

training[[219]edit]

   a id56 using lstm units can be trained in a supervised fashion, on a set
   of training sequences, using an optimization algorithm, like
   [220]id119, combined with [221]id26 through time
   to compute the gradients needed during the optimization process, in
   order to change each weight of the lstm network in proportion to the
   derivative of the error (at the output layer of the lstm network) with
   respect to corresponding weight.

   a problem with using [222]id119 for standard id56s is that
   error gradients [223]vanish exponentially quickly with the size of the
   time lag between important events. this is due to
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <munder> <mo movablelimits="true"
   form="prefix">lim</mo> <mrow class="mjx-texatom-ord"> <mi>n</mi> <mo
   stretchy="false">   </mo> <mi mathvariant="normal">   </mi> </mrow>
   </munder> <msup> <mi>w</mi> <mrow class="mjx-texatom-ord"> <mi>n</mi>
   </mrow> </msup> <mo>=</mo> <mn>0</mn> </mstyle> </mrow> <annotation
   encoding="application/x-tex">{\displaystyle \lim _{n\to \infty
   }w^{n}=0}</annotation> </semantics> :math]
   {\displaystyle \lim _{n\to \infty }w^{n}=0} if the [224]spectral radius
   of
   [math: <semantics> <mrow class="mjx-texatom-ord"> <mstyle
   displaystyle="true" scriptlevel="0"> <mi>w</mi> </mstyle> </mrow>
   <annotation encoding="application/x-tex">{\displaystyle w}</annotation>
   </semantics> :math]
   w is smaller than 1.^[225][34]^[226][35]

   however, with lstm units, when error values are back-propagated from
   the output layer, the error remains in the lstm unit's cell. this
   "error carousel" continuously feeds error back to each of the lstm
   unit's gates, until they learn to cut off the value.

ctc score function[[227]edit]

   many applications use stacks of lstm id56s^[228][36] and train them by
   [229]connectionist temporal classification (ctc)^[230][37] to find an
   id56 weight matrix that maximizes the id203 of the label sequences
   in a training set, given the corresponding input sequences. ctc
   achieves both alignment and recognition.

alternatives[[231]edit]

   sometimes, it can be advantageous to train (parts of) an lstm by
   neuroevolution^[232][38] or by id189, especially when
   there is no "teacher" (that is, training labels).

success[[233]edit]

   there have been several successful stories of training, in a
   non-supervised fashion, id56s with lstm units.

   in 2018, [234]bill gates called it a    huge milestone in advancing
   artificial intelligence    when bots developed by [235]openai were able
   to beat humans in the game of dota 2.^[236][39] openai five consists of
   five independent but coordinated neural networks. each network is
   trained by a policy gradient method without supervising teacher and
   contains a single-layer, 1024-unit long-short-term-memory that sees the
   current game state and emits actions through several possible action
   heads.^[237][39]

   in 2018, [238]openai also trained a similar lstm by policy gradients to
   control a human-like robot hand that manipulates physical objects with
   unprecedented dexterity.^[239][40]

   in 2019, [240]deepmind's program alphastar used a deep lstm core to
   excel at the complex video game [241]starcraft.^[242][41] this was
   viewed as significant progress towards artificial general
   intelligence.^[243][41]

applications[[244]edit]

   applications of lstm include:
     * [245]robot control^[246][42]
     * [247]time series prediction^[248][38]
     * [249]id103^[250][43]^[251][44]^[252][45]
     * rhythm learning^[253][31]
     * music composition^[254][46]
     * grammar learning^[255][47]^[256][30]^[257][48]
     * [258]handwriting recognition^[259][49]^[260][50]
     * human action recognition^[261][51]
     * [262]sign language translation^[263][52]
     * protein homology detection^[264][53]
     * predicting subcellular localization of proteins^[265][54]
     * time series anomaly detection^[266][55]
     * several prediction tasks in the area of business process
       management^[267][56]
     * prediction in medical care pathways^[268][57]
     * [269]id29^[270][58]
     * [271]object co-segmentation^[272][59]^[273][60]

see also[[274]edit]

     * [275]1 the road
     * [276]recurrent neural network
     * [277]deep learning
     * [278]gated recurrent unit
     * [279]differentiable neural computer
     * [280]long-term potentiation
     * [281]prefrontal cortex basal ganglia working memory
     * [282]time series

references[[283]edit]

    1. ^ [284]^a [285]^b [286]^c [287]sepp hochreiter; [288]j  rgen
       schmidhuber (1997). [289]"long short-term memory". [290]neural
       computation. 9 (8): 1735   1780.
       [291]doi:[292]10.1162/neco.1997.9.8.1735. [293]pmid [294]9377276.
    2. [295]^ siegelmann, hava t.; sontag, eduardo d. (1992). on the
       computational power of neural nets. acm. colt '92. pp. 440   449.
       [296]doi:[297]10.1145/130385.130432. [298]isbn [299]978-0897914970.
    3. [300]^ graves, a.; liwicki, m.; fernandez, s.; bertolami, r.;
       bunke, h.; [301]schmidhuber, j. (2009). [302]"a novel connectionist
       system for improved unconstrained handwriting recognition" (pdf).
       ieee transactions on pattern analysis and machine intelligence. 31
       (5): 855   868. [303]citeseerx [304]10.1.1.139.4502.
       [305]doi:[306]10.1109/tpami.2008.137. [307]pmid [308]19299860.
    4. [309]^ sak, hasim; senior, andrew; beaufays, francoise (2014).
       [310]"long short-term memory recurrent neural network architectures
       for large scale acoustic modeling" (pdf).
    5. [311]^ li, xiangang; wu, xihong (2014-10-15). "constructing long
       short-term memory based deep recurrent neural networks for large
       vocabulary id103". [312]arxiv:[313]1410.4281
       [[314]cs.cl].
    6. [315]^ vance, ashlee (may 15, 2018). [316]"quote: these powers make
       lstm arguably the most commercial ai achievement, used for
       everything from predicting diseases to composing music". bloomberg
       business week. retrieved 2019-01-16.
    7. ^ [317]^a [318]^b [319]^c [320]^d klaus greff; rupesh kumar
       srivastava; jan koutn  k; bas r. steunebrink; j  rgen schmidhuber
       (2015). "lstm: a search space odyssey". ieee transactions on neural
       networks and learning systems. 28 (10): 2222   2232.
       [321]arxiv:[322]1503.04069.
       [323]doi:[324]10.1109/tnnls.2016.2582924. [325]pmid [326]27411231.
    8. [327]^ [328]felix gers; [329]j  rgen schmidhuber; fred cummins
       (1999). [330]"learning to forget: continual prediction with lstm".
       proc. icann'99, iee, london: 850   855.
    9. ^ [331]^a [332]^b felix a. gers; j  rgen schmidhuber; fred cummins
       (2000). "learning to forget: continual prediction with lstm".
       [333]neural computation. 12 (10): 2451   2471.
       [334]citeseerx [335]10.1.1.55.5709.
       [336]doi:[337]10.1162/089976600300015015.
   10. [338]^ cho, kyunghyun; van merrienboer, bart; gulcehre, caglar;
       bahdanau, dzmitry; bougares, fethi; schwenk, holger; bengio, yoshua
       (2014). "learning phrase representations using id56 encoder-decoder
       for id151". [339]arxiv:[340]1406.1078
       [[341]cs.cl].
   11. [342]^ [343]"the large text compression benchmark". retrieved
       2017-01-13.
   12. [344]^ graves, a.; liwicki, m.; fern  ndez, s.; bertolami, r.;
       bunke, h.; schmidhuber, j. (may 2009). "a novel connectionist
       system for unconstrained handwriting recognition". ieee
       transactions on pattern analysis and machine intelligence. 31 (5):
       855   868. [345]citeseerx [346]10.1.1.139.4502.
       [347]doi:[348]10.1109/tpami.2008.137. [349]issn [350]0162-8828.
       [351]pmid [352]19299860.
   13. [353]^ graves, alex; mohamed, abdel-rahman; hinton, geoffrey
       (2013-03-22). "id103 with deep recurrent neural
       networks". [354]arxiv:[355]1303.5778 [[356]cs.ne].
   14. [357]^ metz, cade (2016-06-14). [358]"apple is bringing the ai
       revolution to your iphone". wired. retrieved 2016-06-16.
   15. [359]^ beaufays, fran  oise (august 11, 2015). [360]"the neural
       networks behind google voice transcription". research blog.
       retrieved 2017-06-27.
   16. [361]^ sak, ha  im; senior, andrew; rao, kanishka; beaufays,
       fran  oise; schalkwyk, johan (september 24, 2015). [362]"google
       voice search: faster and more accurate". research blog. retrieved
       2017-06-27.
   17. [363]^ khaitan, pranav (may 18, 2016). [364]"chat smarter with
       allo". research blog. retrieved 2017-06-27.
   18. [365]^ wu, yonghui; schuster, mike; chen, zhifeng; le, quoc v.;
       norouzi, mohammad; macherey, wolfgang; krikun, maxim; cao, yuan;
       gao, qin (2016-09-26). "google's id4 system:
       bridging the gap between human and machine translation".
       [366]arxiv:[367]1609.08144 [[368]cs.cl].
   19. [369]^ metz, cade (september 27, 2016). [370]"an infusion of ai
       makes google translate more powerful than ever | wired". wired.
       retrieved 2017-06-27.
   20. [371]^ efrati, amir (june 13, 2016). [372]"apple's machines can
       learn too". the information. retrieved 2017-06-27.
   21. [373]^ ranger, steve (june 14, 2016). [374]"iphone, ai and big
       data: here's how apple plans to protect your privacy | zdnet".
       zdnet. retrieved 2017-06-27.
   22. [375]^ smith, chris (2016-06-13). [376]"ios 10: siri now works in
       third-party apps, comes with extra ai features". bgr. retrieved
       2017-06-27.
   23. [377]^ vogels, werner (30 november 2016). [378]"bringing the magic
       of amazon ai and alexa to apps on aws. - all things distributed".
       www.allthingsdistributed.com. retrieved 2017-06-27.
   24. [379]^ ong, thuy (4 august 2017). [380]"facebook's translations are
       now powered completely by ai". www.allthingsdistributed.com.
       retrieved 2019-02-15.
   25. [381]^ [382]"patient subtyping via time-aware id137" (pdf).
       msu.edu. retrieved 21 nov 2018.
   26. [383]^ [384]"patient subtyping via time-aware id137".
       kdd.org. retrieved 24 may 2018.
   27. [385]^ [386]"sigkdd". kdd.org. retrieved 24 may 2018.
   28. [387]^ haridy, rich (august 21, 2017). [388]"microsoft's speech
       recognition system is now as good as a human". newatlas.com.
       retrieved 2017-08-27.
   29. [389]^ bro, n. [390]"why can id56s with lstm units also suffer from
       "exploding gradients"?". cross validated. retrieved 25 december
       2018.
   30. ^ [391]^a [392]^b [393]^c gers, f. a.; schmidhuber, j. (2001).
       [394]"lstm recurrent networks learn simple context free and context
       sensitive languages" (pdf). ieee transactions on neural networks.
       12 (6): 1333   1340. [395]doi:[396]10.1109/72.963769.
       [397]pmid [398]18249962.
   31. ^ [399]^a [400]^b [401]^c gers, f.; schraudolph, n.; schmidhuber,
       j. (2002). [402]"learning precise timing with lstm recurrent
       networks" (pdf). journal of machine learning research. 3: 115   143.
   32. [403]^ gers, f. a.; schmidhuber, e. (november 2001). [404]"lstm
       recurrent networks learn simple context-free and context-sensitive
       languages" (pdf). ieee transactions on neural networks. 12 (6):
       1333   1340. [405]doi:[406]10.1109/72.963769.
       [407]issn [408]1045-9227. [409]pmid [410]18249962.
   33. [411]^ xingjian shi; zhourong chen; hao wang; dit-yan yeung;
       wai-kin wong; wang-chun woo (2015). "convolutional lstm network: a
       machine learning approach for precipitation nowcasting".
       proceedings of the 28th international conference on neural
       information processing systems: 802   810.
       [412]arxiv:[413]1506.04214. [414]bibcode:[415]2015arxiv150604214s.
   34. [416]^ s. hochreiter. untersuchungen zu dynamischen neuronalen
       netzen. diploma thesis, institut f. informatik, technische univ.
       munich, 1991.
   35. [417]^ hochreiter, s.; bengio, y.; frasconi, p.; schmidhuber, j.
       (2001). [418]"gradient flow in recurrent nets: the difficulty of
       learning long-term dependencies (pdf download available)". in
       kremer and, s. c.; kolen, j. f. a field guide to dynamical
       recurrent neural networks. ieee press.
   36. [419]^ fern  ndez, santiago; graves, alex; schmidhuber, j  rgen
       (2007). "sequence labelling in structured domains with hierarchical
       recurrent neural networks". proc. 20th int. joint conf. on
       artificial in   ligence, ijcai 2007: 774   779.
       [420]citeseerx [421]10.1.1.79.1887.
   37. [422]^ graves, alex; fern  ndez, santiago; gomez, faustino (2006).
       "connectionist temporal classification: labelling unsegmented
       sequence data with recurrent neural networks". in proceedings of
       the international conference on machine learning, icml 2006:
       369   376. [423]citeseerx [424]10.1.1.75.6306.
   38. ^ [425]^a [426]^b wierstra, daan; schmidhuber, j.; gomez, f. j.
       (2005). [427]"evolino: hybrid neuroevolution/optimal linear search
       for sequence learning". proceedings of the 19th international joint
       conference on artificial intelligence (ijcai), edinburgh: 853   858.
   39. ^ [428]^a [429]^b rodriguez, jesus (july 2, 2018). [430]"the
       science behind openai five that just produced one of the greatest
       breakthrough in the history of ai". towards data science. retrieved
       2019-01-15.
   40. [431]^ [432]"learning dexterity". openai blog. july 30, 2018.
       retrieved 2019-01-15.
   41. ^ [433]^a [434]^b stanford, stacy (january 25, 2019).
       [435]"deepmind's ai, alphastar showcases significant progress
       towards agi". medium ml memoirs. retrieved 2019-01-15.
   42. [436]^ mayer, h.; gomez, f.; wierstra, d.; nagy, i.; knoll, a.;
       schmidhuber, j. (october 2006). a system for robotic heart surgery
       that learns to tie knots using recurrent neural networks. 2006
       ieee/rsj international conference on intelligent robots and
       systems. pp. 543   548. [437]citeseerx [438]10.1.1.218.3399.
       [439]doi:[440]10.1109/iros.2006.282190.
       [441]isbn [442]978-1-4244-0258-8.
   43. [443]^ graves, a.; schmidhuber, j. (2005). "framewise phoneme
       classification with bidirectional lstm and other neural network
       architectures". neural networks. 18 (5   6): 602   610.
       [444]citeseerx [445]10.1.1.331.5800.
       [446]doi:[447]10.1016/j.neunet.2005.06.042.
       [448]pmid [449]16112549.
   44. [450]^ fern  ndez, santiago; graves, alex; schmidhuber, j  rgen
       (2007). [451]an application of recurrent neural networks to
       discriminative keyword spotting. proceedings of the 17th
       international conference on id158s. icann'07.
       berlin, heidelberg: springer-verlag. pp. 220   229.
       [452]isbn [453]978-3540746935.
   45. [454]^ graves, alex; mohamed, abdel-rahman; hinton, geoffrey
       (2013). "id103 with deep recurrent neural networks".
       acoustics, speech and signal processing (icassp), 2013 ieee
       international conference on: 6645   6649.
   46. [455]^ eck, douglas; schmidhuber, j  rgen (2002-08-28). learning the
       long-term structure of the blues. id158s    
       icann 2002. lecture notes in computer science. 2415. springer,
       berlin, heidelberg. pp. 284   289.
       [456]citeseerx [457]10.1.1.116.3620.
       [458]doi:[459]10.1007/3-540-46084-5_47.
       [460]isbn [461]978-3540460848.
   47. [462]^ schmidhuber, j.; gers, f.; eck, d.; schmidhuber, j.; gers,
       f. (2002). "learning nonregular languages: a comparison of simple
       recurrent networks and lstm". neural computation. 14 (9):
       2039   2041. [463]citeseerx [464]10.1.1.11.7369.
       [465]doi:[466]10.1162/089976602320263980. [467]pmid [468]12184841.
   48. [469]^ perez-ortiz, j. a.; gers, f. a.; eck, d.; schmidhuber, j.
       (2003). "kalman filters improve lstm network performance in
       problems unsolvable by traditional recurrent nets". neural
       networks. 16 (2): 241   250. [470]citeseerx [471]10.1.1.381.1992.
       [472]doi:[473]10.1016/s0893-6080(02)00219-8.
       [474]pmid [475]12628609.
   49. [476]^ a. graves, j. schmidhuber. offline handwriting recognition
       with multidimensional recurrent neural networks. advances in neural
       information processing systems 22, nips'22, pp 545   552, vancouver,
       mit press, 2009.
   50. [477]^ graves, alex; fern  ndez, santiago; liwicki, marcus; bunke,
       horst; schmidhuber, j  rgen (2007). [478]unconstrained online
       handwriting recognition with recurrent neural networks. proceedings
       of the 20th international conference on neural information
       processing systems. nips'07. usa: curran associates inc.
       pp. 577   584. [479]isbn [480]9781605603520.
   51. [481]^ m. baccouche, f. mamalet, c wolf, c. garcia, a. baskurt.
       sequential deep learning for human action recognition. 2nd
       international workshop on human behavior understanding (hbu), a.a.
       salah, b. lepri ed. amsterdam, netherlands. pp. 29   39. lecture
       notes in computer science 7065. springer. 2011
   52. [482]^ huang, jie; zhou, wengang; zhang, qilin; li, houqiang; li,
       weiping (2018-01-30). "video-based sign language recognition
       without temporal segmentation". [483]arxiv:[484]1801.10111
       [[485]cs.cv].
   53. [486]^ hochreiter, s.; heusel, m.; obermayer, k. (2007). "fast
       model-based protein homology detection without alignment".
       bioinformatics. 23 (14): 1728   1736.
       [487]doi:[488]10.1093/bioinformatics/btm247.
       [489]pmid [490]17488755.
   54. [491]^ thireou, t.; reczko, m. (2007). "bidirectional long
       short-term memory networks for predicting the subcellular
       localization of eukaryotic proteins". ieee/acm transactions on
       computational biology and bioinformatics (tcbb). 4 (3): 441   446.
       [492]doi:[493]10.1109/tcbb.2007.1015. [494]pmid [495]17666763.
   55. [496]^ malhotra, pankaj; vig, lovekesh; shroff, gautam; agarwal,
       puneet (april 2015). [497]"long short term memory networks for
       anomaly detection in time series" (pdf). european symposium on
       id158s, computational intelligence and machine
       learning     esann 2015.
   56. [498]^ tax, n.; verenich, i.; la rosa, m.; dumas, m. (2017).
       predictive business process monitoring with lstm neural networks.
       proceedings of the international conference on advanced information
       systems engineering (caise). lecture notes in computer science.
       10253. pp. 477   492. [499]arxiv:[500]1612.02130.
       [501]doi:[502]10.1007/978-3-319-59536-8_30.
       [503]isbn [504]978-3-319-59535-1.
   57. [505]^ choi, e.; bahadori, m.t.; schuetz, e.; stewart, w.; sun, j.
       (2016). [506]"doctor ai: predicting clinical events via recurrent
       neural networks". proceedings of the 1st machine learning for
       healthcare conference: 301   318. [507]arxiv:[508]1511.05942.
       [509]bibcode:[510]2015arxiv151105942c.
   58. [511]^ jia, robin; liang, percy (2016-06-11). [512]"data
       recombination for neural id29". arxiv:1606.03622 [cs].
   59. [513]^ wang, le; duan, xuhuan; zhang, qilin; niu, zhenxing; hua,
       gang; zheng, nanning (2018-05-22). [514]"segment-tube:
       spatio-temporal action localization in untrimmed videos with
       per-frame segmentation" (pdf). sensors. 18 (5): 1657.
       [515]doi:[516]10.3390/s18051657. [517]issn [518]1424-8220.
       [519]pmc [520]5982167. [521]pmid [522]29789447.
   60. [523]^ duan, xuhuan; wang, le; zhai, changbo; zheng, nanning;
       zhang, qilin; niu, zhenxing; hua, gang (2018). joint
       spatio-temporal action localization in untrimmed videos with
       per-frame segmentation. 25th ieee international conference on image
       processing (icip). [524]doi:[525]10.1109/icip.2018.8451692.
       [526]isbn [527]978-1-4799-7061-2.

external links[[528]edit]

     * [529]recurrent neural networks with over 30 lstm papers by
       [530]j  rgen schmidhuber's group at [531]idsia
     * gers, felix (2001). [532]"long short-term memory in recurrent
       neural networks" (pdf). phd thesis.

     gers, felix a.; schraudolph, nicol n.; schmidhuber, j  rgen (aug
   2002). [533]"learning precise timing with lstm recurrent networks"
   (pdf). journal of machine learning research. 3: 115   143.

     abidogun, olusola adeniyi (2005). [534]"data mining, fraud detection
   and mobile telecommunications: call pattern analysis with unsupervised
   neural networks". master's thesis. [535]hdl:[536]11394/249.
   [537]archived (pdf) from the original on may 22, 2012.
     * [538]original with two chapters devoted to explaining recurrent
       neural networks, especially lstm.

     monner, derek d.; reggia, james a. (2010). [539]"a generalized
   lstm-like training algorithm for second-order recurrent neural
   networks" (pdf). "high-performing extension of lstm that has been
   simplified to a single node type and can train arbitrary architectures"

     herta, christian. [540]"how to implement lstm in python with theano".
   tutorial.

     [541]chevalier, guillaume. tutorial: how to use lstms with tensorflow
   in python on cellphone sensor data on [542]github

   retrieved from
   "[543]https://en.wikipedia.org/w/index.php?title=long_short-term_memory
   &oldid=890927981"

   [544]categories:
     * [545]id158s

   hidden categories:
     * [546]cs1: long volume value
     * [547]all articles with unsourced statements
     * [548]articles with unsourced statements from october 2017

navigation menu

personal tools

     * not logged in
     * [549]talk
     * [550]contributions
     * [551]create account
     * [552]log in

namespaces

     * [553]article
     * [554]talk

   [ ]

variants

views

     * [555]read
     * [556]edit
     * [557]view history

   [ ]

more

search

   ____________________ search go

navigation

     * [558]main page
     * [559]contents
     * [560]featured content
     * [561]current events
     * [562]random article
     * [563]donate to wikipedia
     * [564]wikipedia store

interaction

     * [565]help
     * [566]about wikipedia
     * [567]community portal
     * [568]recent changes
     * [569]contact page

tools

     * [570]what links here
     * [571]related changes
     * [572]upload file
     * [573]special pages
     * [574]permanent link
     * [575]page information
     * [576]wikidata item
     * [577]cite this page

print/export

     * [578]create a book
     * [579]download as pdf
     * [580]printable version

languages

     * [581]catal  
     * [582]deutsch
     * [583]          
     * [584]         
     * [585]              
     * [586]                    
     * [587]      

   [588]edit links

     * this page was last edited on 4 april 2019, at 13:54 (utc).
     * text is available under the [589]creative commons
       attribution-sharealike license; additional terms may apply. by
       using this site, you agree to the [590]terms of use and
       [591]privacy policy. wikipedia   is a registered trademark of the
       [592]wikimedia foundation, inc., a non-profit organization.

     * [593]privacy policy
     * [594]about wikipedia
     * [595]disclaimers
     * [596]contact wikipedia
     * [597]developers
     * [598]cookie statement
     * [599]mobile view

     * [600]wikimedia foundation
     * [601]powered by mediawiki

references

   visible links
   1. android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/long_short-term_memory
   2. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit
   3. https://en.wikipedia.org/w/opensearch_desc.php
   4. https://en.wikipedia.org/wiki/long_short-term_memory#mw-head
   5. https://en.wikipedia.org/wiki/long_short-term_memory#p-search
   6. https://en.wikipedia.org/wiki/machine_learning
   7. https://en.wikipedia.org/wiki/data_mining
   8. https://en.wikipedia.org/wiki/file:kernel_machine.svg
   9. https://en.wikipedia.org/wiki/statistical_classification
  10. https://en.wikipedia.org/wiki/cluster_analysis
  11. https://en.wikipedia.org/wiki/regression_analysis
  12. https://en.wikipedia.org/wiki/anomaly_detection
  13. https://en.wikipedia.org/wiki/automated_machine_learning
  14. https://en.wikipedia.org/wiki/association_rule_learning
  15. https://en.wikipedia.org/wiki/reinforcement_learning
  16. https://en.wikipedia.org/wiki/structured_prediction
  17. https://en.wikipedia.org/wiki/feature_engineering
  18. https://en.wikipedia.org/wiki/feature_learning
  19. https://en.wikipedia.org/wiki/online_machine_learning
  20. https://en.wikipedia.org/wiki/semi-supervised_learning
  21. https://en.wikipedia.org/wiki/unsupervised_learning
  22. https://en.wikipedia.org/wiki/learning_to_rank
  23. https://en.wikipedia.org/wiki/grammar_induction
  24. https://en.wikipedia.org/wiki/supervised_learning
  25. https://en.wikipedia.org/wiki/statistical_classification
  26. https://en.wikipedia.org/wiki/regression_analysis
  27. https://en.wikipedia.org/wiki/decision_tree_learning
  28. https://en.wikipedia.org/wiki/ensemble_learning
  29. https://en.wikipedia.org/wiki/bootstrap_aggregating
  30. https://en.wikipedia.org/wiki/boosting_(machine_learning)
  31. https://en.wikipedia.org/wiki/random_forest
  32. https://en.wikipedia.org/wiki/k-nearest_neighbors_algorithm
  33. https://en.wikipedia.org/wiki/linear_regression
  34. https://en.wikipedia.org/wiki/naive_bayes_classifier
  35. https://en.wikipedia.org/wiki/artificial_neural_network
  36. https://en.wikipedia.org/wiki/logistic_regression
  37. https://en.wikipedia.org/wiki/id88
  38. https://en.wikipedia.org/wiki/relevance_vector_machine
  39. https://en.wikipedia.org/wiki/support_vector_machine
  40. https://en.wikipedia.org/wiki/cluster_analysis
  41. https://en.wikipedia.org/wiki/birch
  42. https://en.wikipedia.org/wiki/cure_data_id91_algorithm
  43. https://en.wikipedia.org/wiki/hierarchical_id91
  44. https://en.wikipedia.org/wiki/id116_id91
  45. https://en.wikipedia.org/wiki/expectation   maximization_algorithm
  46. https://en.wikipedia.org/wiki/dbscan
  47. https://en.wikipedia.org/wiki/optics_algorithm
  48. https://en.wikipedia.org/wiki/mean-shift
  49. https://en.wikipedia.org/wiki/dimensionality_reduction
  50. https://en.wikipedia.org/wiki/factor_analysis
  51. https://en.wikipedia.org/wiki/canonical_correlation_analysis
  52. https://en.wikipedia.org/wiki/independent_component_analysis
  53. https://en.wikipedia.org/wiki/linear_discriminant_analysis
  54. https://en.wikipedia.org/wiki/non-negative_matrix_factorization
  55. https://en.wikipedia.org/wiki/principal_component_analysis
  56. https://en.wikipedia.org/wiki/t-distributed_stochastic_neighbor_embedding
  57. https://en.wikipedia.org/wiki/structured_prediction
  58. https://en.wikipedia.org/wiki/graphical_model
  59. https://en.wikipedia.org/wiki/bayesian_network
  60. https://en.wikipedia.org/wiki/conditional_random_field
  61. https://en.wikipedia.org/wiki/hidden_markov_model
  62. https://en.wikipedia.org/wiki/anomaly_detection
  63. https://en.wikipedia.org/wiki/k-nearest_neighbors_classification
  64. https://en.wikipedia.org/wiki/local_outlier_factor
  65. https://en.wikipedia.org/wiki/artificial_neural_networks
  66. https://en.wikipedia.org/wiki/autoencoder
  67. https://en.wikipedia.org/wiki/deep_learning
  68. https://en.wikipedia.org/wiki/deepdream
  69. https://en.wikipedia.org/wiki/multilayer_id88
  70. https://en.wikipedia.org/wiki/recurrent_neural_network
  71. https://en.wikipedia.org/wiki/gated_recurrent_unit
  72. https://en.wikipedia.org/wiki/restricted_boltzmann_machine
  73. https://en.wikipedia.org/wiki/self-organizing_map
  74. https://en.wikipedia.org/wiki/convolutional_neural_network
  75. https://en.wikipedia.org/wiki/u-net
  76. https://en.wikipedia.org/wiki/reinforcement_learning
  77. https://en.wikipedia.org/wiki/id24
  78. https://en.wikipedia.org/wiki/state   action   reward   state   action
  79. https://en.wikipedia.org/wiki/temporal_difference_learning
  80. https://en.wikipedia.org/wiki/bias-variance_dilemma
  81. https://en.wikipedia.org/wiki/computational_learning_theory
  82. https://en.wikipedia.org/wiki/empirical_risk_minimization
  83. https://en.wikipedia.org/wiki/occam_learning
  84. https://en.wikipedia.org/wiki/probably_approximately_correct_learning
  85. https://en.wikipedia.org/wiki/statistical_learning_theory
  86. https://en.wikipedia.org/wiki/vapnik   chervonenkis_theory
  87. https://en.wikipedia.org/wiki/conference_on_neural_information_processing_systems
  88. https://en.wikipedia.org/wiki/international_conference_on_machine_learning
  89. https://en.wikipedia.org/wiki/machine_learning_(journal)
  90. https://en.wikipedia.org/wiki/journal_of_machine_learning_research
  91. https://arxiv.org/list/cs.lg/recent
  92. https://en.wikipedia.org/wiki/glossary_of_artificial_intelligence
  93. https://en.wikipedia.org/wiki/glossary_of_artificial_intelligence
  94. https://en.wikipedia.org/wiki/list_of_datasets_for_machine-learning_research
  95. https://en.wikipedia.org/wiki/outline_of_machine_learning
  96. https://en.wikipedia.org/wiki/file:portal-puzzle.svg
  97. https://en.wikipedia.org/wiki/portal:machine_learning
  98. https://en.wikipedia.org/wiki/template:machine_learning_bar
  99. https://en.wikipedia.org/wiki/template_talk:machine_learning_bar
 100. https://en.wikipedia.org/w/index.php?title=template:machine_learning_bar&action=edit
 101. https://en.wikipedia.org/wiki/recurrent_neural_network
 102. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-lstm1997-1
 103. https://en.wikipedia.org/wiki/deep_learning
 104. https://en.wikipedia.org/wiki/feedforward_neural_network
 105. https://en.wikipedia.org/wiki/turing_machine
 106. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-siegelmann92-2
 107. https://en.wikipedia.org/wiki/handwriting_recognition
 108. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-3
 109. https://en.wikipedia.org/wiki/speech_recognition
 110. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-sak2014-4
 111. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-liwu2015-5
 112. https://en.wikipedia.org/wiki/bloomberg_business_week
 113. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-bloomberg2018-6
 114. https://en.wikipedia.org/wiki/classification_in_machine_learning
 115. https://en.wikipedia.org/wiki/computer_data_processing
 116. https://en.wikipedia.org/wiki/predict
 117. https://en.wikipedia.org/wiki/time_series
 118. https://en.wikipedia.org/wiki/vanishing_gradient_problem
 119. https://en.wikipedia.org/wiki/hidden_markov_models
 120. https://en.wikipedia.org/wiki/wikipedia:citation_needed
 121. https://en.wikipedia.org/wiki/long_short-term_memory#history
 122. https://en.wikipedia.org/wiki/long_short-term_memory#idea
 123. https://en.wikipedia.org/wiki/long_short-term_memory#architecture
 124. https://en.wikipedia.org/wiki/long_short-term_memory#variants
 125. https://en.wikipedia.org/wiki/long_short-term_memory#lstm_with_a_forget_gate
 126. https://en.wikipedia.org/wiki/long_short-term_memory#variables
 127. https://en.wikipedia.org/wiki/long_short-term_memory#activation_functions
 128. https://en.wikipedia.org/wiki/long_short-term_memory#peephole_lstm
 129. https://en.wikipedia.org/wiki/long_short-term_memory#peephole_convolutional_lstm
 130. https://en.wikipedia.org/wiki/long_short-term_memory#training
 131. https://en.wikipedia.org/wiki/long_short-term_memory#ctc_score_function
 132. https://en.wikipedia.org/wiki/long_short-term_memory#alternatives
 133. https://en.wikipedia.org/wiki/long_short-term_memory#success
 134. https://en.wikipedia.org/wiki/long_short-term_memory#applications
 135. https://en.wikipedia.org/wiki/long_short-term_memory#see_also
 136. https://en.wikipedia.org/wiki/long_short-term_memory#references
 137. https://en.wikipedia.org/wiki/long_short-term_memory#external_links
 138. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=1
 139. https://en.wikipedia.org/wiki/sepp_hochreiter
 140. https://en.wikipedia.org/wiki/j  rgen_schmidhuber
 141. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-lstm1997-1
 142. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-asearchspaceodyssey-7
 143. https://en.wikipedia.org/wiki/felix_gers
 144. https://en.wikipedia.org/wiki/j  rgen_schmidhuber
 145. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-lstm1999-8
 146. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-asearchspaceodyssey-7
 147. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-lstm2000-9
 148. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-asearchspaceodyssey-7
 149. https://en.wikipedia.org/wiki/gated_recurrent_unit
 150. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-10
 151. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-11
 152. https://en.wikipedia.org/wiki/handwriting_recognition
 153. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-12
 154. https://en.wikipedia.org/wiki/icdar
 155. https://en.wikipedia.org/wiki/phoneme
 156. https://en.wikipedia.org/wiki/timit
 157. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-13
 158. https://en.wikipedia.org/wiki/google
 159. https://en.wikipedia.org/wiki/apple_inc.
 160. https://en.wikipedia.org/wiki/microsoft
 161. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-14
 162. https://en.wikipedia.org/wiki/smartphone
 163. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-beau15-15
 164. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-googlevoicesearch-16
 165. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-googleallo-17
 166. https://en.wikipedia.org/wiki/google_translate
 167. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-googletranslate-18
 168. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-wiredgoogletranslate-19
 169. https://en.wikipedia.org/wiki/apple_inc.
 170. https://en.wikipedia.org/wiki/iphone
 171. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-applequicktype-20
 172. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-applequicktype2-21
 173. https://en.wikipedia.org/wiki/siri
 174. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-applesiri-22
 175. https://en.wikipedia.org/wiki/amazon_inc.
 176. https://en.wikipedia.org/wiki/amazon_alexa
 177. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-amazonalexa-23
 178. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-facebooktranslate-24
 179. https://en.wikipedia.org/wiki/michigan_state_university
 180. https://en.wikipedia.org/wiki/ibm_research
 181. https://en.wikipedia.org/wiki/cornell_university
 182. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-25
 183. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-26
 184. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-27
 185. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-28
 186. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=2
 187. https://en.wikipedia.org/wiki/recurrent_neural_network
 188. https://en.wikipedia.org/wiki/back-propagation
 189. https://en.wikipedia.org/wiki/vanishing_gradient_problem
 190. https://en.wikipedia.org/wiki/round-off_error
 191. https://en.wikipedia.org/wiki/vanishing_gradient_problem
 192. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-29
 193. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=3
 194. https://en.wikipedia.org/wiki/gated_recurrent_unit
 195. https://en.wikipedia.org/wiki/logistic_function
 196. https://en.wikipedia.org/wiki/supervised_learning
 197. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=4
 198. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=5
 199. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-lstm1997-1
 200. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-lstm2000-9
 201. https://en.wikipedia.org/wiki/hadamard_product_(matrices)
 202. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=6
 203. https://en.wikipedia.org/wiki/activation_function
 204. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=7
 205. https://en.wikipedia.org/wiki/sigmoid_function
 206. https://en.wikipedia.org/wiki/hyperbolic_tangent
 207. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-peepholelstm-30
 208. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-peephole2002-31
 209. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=8
 210. https://en.wikipedia.org/wiki/long_short-term_memory#peephole_lstm
 211. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-asearchspaceodyssey-7
 212. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-peepholelstm-30
 213. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-peephole2002-31
 214. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-32
 215. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=9
 216. https://en.wikipedia.org/wiki/convolutional_neural_network
 217. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-33
 218. https://en.wikipedia.org/wiki/convolution
 219. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=10
 220. https://en.wikipedia.org/wiki/gradient_descent
 221. https://en.wikipedia.org/wiki/id26_through_time
 222. https://en.wikipedia.org/wiki/gradient_descent
 223. https://en.wikipedia.org/wiki/vanishing_gradient_problem
 224. https://en.wikipedia.org/wiki/spectral_radius
 225. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-34
 226. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-gradf-35
 227. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=11
 228. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-fernandez2007-36
 229. https://en.wikipedia.org/wiki/connectionist_temporal_classification_(ctc)
 230. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-graves2006-37
 231. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=12
 232. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-wierstra2005-38
 233. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=13
 234. https://en.wikipedia.org/wiki/bill_gates
 235. https://en.wikipedia.org/wiki/openai
 236. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-openaifive-39
 237. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-openaifive-39
 238. https://en.wikipedia.org/wiki/openai
 239. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-openaihand-40
 240. https://en.wikipedia.org/wiki/deepmind
 241. https://en.wikipedia.org/wiki/starcraft
 242. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-alphastar-41
 243. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-alphastar-41
 244. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=14
 245. https://en.wikipedia.org/wiki/robot_control
 246. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-42
 247. https://en.wikipedia.org/wiki/time_series_prediction
 248. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-wierstra2005-38
 249. https://en.wikipedia.org/wiki/speech_recognition
 250. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-43
 251. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-44
 252. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-referencea-45
 253. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-peephole2002-31
 254. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-46
 255. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-47
 256. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-peepholelstm-30
 257. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-48
 258. https://en.wikipedia.org/wiki/handwriting_recognition
 259. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-49
 260. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-50
 261. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-51
 262. https://en.wikipedia.org/wiki/sign_language
 263. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-52
 264. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-53
 265. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-54
 266. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-55
 267. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-56
 268. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-57
 269. https://en.wikipedia.org/wiki/semantic_parsing
 270. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-58
 271. https://en.wikipedia.org/wiki/object_co-segmentation
 272. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-wang_duan_zhang_niu_p=1657-59
 273. https://en.wikipedia.org/wiki/long_short-term_memory#cite_note-duan_wang_zhai_zheng_2018_p.-60
 274. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=15
 275. https://en.wikipedia.org/wiki/1_the_road
 276. https://en.wikipedia.org/wiki/recurrent_neural_network
 277. https://en.wikipedia.org/wiki/deep_learning
 278. https://en.wikipedia.org/wiki/gated_recurrent_unit
 279. https://en.wikipedia.org/wiki/differentiable_neural_computer
 280. https://en.wikipedia.org/wiki/long-term_potentiation
 281. https://en.wikipedia.org/wiki/prefrontal_cortex_basal_ganglia_working_memory
 282. https://en.wikipedia.org/wiki/time_series
 283. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=16
 284. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-lstm1997_1-0
 285. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-lstm1997_1-1
 286. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-lstm1997_1-2
 287. https://en.wikipedia.org/wiki/sepp_hochreiter
 288. https://en.wikipedia.org/wiki/j  rgen_schmidhuber
 289. https://www.researchgate.net/publication/13853244
 290. https://en.wikipedia.org/wiki/neural_computation_(journal)
 291. https://en.wikipedia.org/wiki/digital_object_identifier
 292. https://doi.org/10.1162/neco.1997.9.8.1735
 293. https://en.wikipedia.org/wiki/pubmed_identifier
 294. https://www.ncbi.nlm.nih.gov/pubmed/9377276
 295. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-siegelmann92_2-0
 296. https://en.wikipedia.org/wiki/digital_object_identifier
 297. https://doi.org/10.1145/130385.130432
 298. https://en.wikipedia.org/wiki/international_standard_book_number
 299. https://en.wikipedia.org/wiki/special:booksources/978-0897914970
 300. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-3
 301. https://en.wikipedia.org/wiki/j  rgen_schmidhuber
 302. http://www.idsia.ch/~juergen/tpami_2008.pdf
 303. https://en.wikipedia.org/wiki/citeseerx
 304. https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.4502
 305. https://en.wikipedia.org/wiki/digital_object_identifier
 306. https://doi.org/10.1109/tpami.2008.137
 307. https://en.wikipedia.org/wiki/pubmed_identifier
 308. https://www.ncbi.nlm.nih.gov/pubmed/19299860
 309. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-sak2014_4-0
 310. https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf
 311. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-liwu2015_5-0
 312. https://en.wikipedia.org/wiki/arxiv
 313. https://arxiv.org/abs/1410.4281
 314. https://arxiv.org/archive/cs.cl
 315. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-bloomberg2018_6-0
 316. https://www.bloomberg.com/news/features/2018-05-15/google-amazon-and-facebook-owe-j-rgen-schmidhuber-a-fortune
 317. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-asearchspaceodyssey_7-0
 318. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-asearchspaceodyssey_7-1
 319. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-asearchspaceodyssey_7-2
 320. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-asearchspaceodyssey_7-3
 321. https://en.wikipedia.org/wiki/arxiv
 322. https://arxiv.org/abs/1503.04069
 323. https://en.wikipedia.org/wiki/digital_object_identifier
 324. https://doi.org/10.1109/tnnls.2016.2582924
 325. https://en.wikipedia.org/wiki/pubmed_identifier
 326. https://www.ncbi.nlm.nih.gov/pubmed/27411231
 327. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-lstm1999_8-0
 328. https://en.wikipedia.org/wiki/felix_gers
 329. https://en.wikipedia.org/wiki/j  rgen_schmidhuber
 330. https://ieeexplore.ieee.org/document/818041
 331. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-lstm2000_9-0
 332. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-lstm2000_9-1
 333. https://en.wikipedia.org/wiki/neural_computation_(journal)
 334. https://en.wikipedia.org/wiki/citeseerx
 335. https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.5709
 336. https://en.wikipedia.org/wiki/digital_object_identifier
 337. https://doi.org/10.1162/089976600300015015
 338. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-10
 339. https://en.wikipedia.org/wiki/arxiv
 340. https://arxiv.org/abs/1406.1078
 341. https://arxiv.org/archive/cs.cl
 342. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-11
 343. http://www.mattmahoney.net/dc/text.html#1218
 344. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-12
 345. https://en.wikipedia.org/wiki/citeseerx
 346. https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.4502
 347. https://en.wikipedia.org/wiki/digital_object_identifier
 348. https://doi.org/10.1109/tpami.2008.137
 349. https://en.wikipedia.org/wiki/international_standard_serial_number
 350. https://www.worldcat.org/issn/0162-8828
 351. https://en.wikipedia.org/wiki/pubmed_identifier
 352. https://www.ncbi.nlm.nih.gov/pubmed/19299860
 353. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-13
 354. https://en.wikipedia.org/wiki/arxiv
 355. https://arxiv.org/abs/1303.5778
 356. https://arxiv.org/archive/cs.ne
 357. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-14
 358. https://www.wired.com/2016/06/apple-bringing-ai-revolution-iphone/
 359. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-beau15_15-0
 360. http://googleresearch.blogspot.co.at/2015/08/the-neural-networks-behind-google-voice.html
 361. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-googlevoicesearch_16-0
 362. http://googleresearch.blogspot.co.uk/2015/09/google-voice-search-faster-and-more.html
 363. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-googleallo_17-0
 364. http://googleresearch.blogspot.co.at/2016/05/chat-smarter-with-allo.html
 365. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-googletranslate_18-0
 366. https://en.wikipedia.org/wiki/arxiv
 367. https://arxiv.org/abs/1609.08144
 368. https://arxiv.org/archive/cs.cl
 369. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-wiredgoogletranslate_19-0
 370. https://www.wired.com/2016/09/google-claims-ai-breakthrough-machine-translation/
 371. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-applequicktype_20-0
 372. https://www.theinformation.com/apples-machines-can-learn-too
 373. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-applequicktype2_21-0
 374. http://www.zdnet.com/article/ai-big-data-and-the-iphone-heres-how-apple-plans-to-protect-your-privacy
 375. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-applesiri_22-0
 376. http://bgr.com/2016/06/13/ios-10-siri-third-party-apps/
 377. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-amazonalexa_23-0
 378. http://www.allthingsdistributed.com/2016/11/amazon-ai-and-alexa-for-all-aws-apps.html
 379. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-facebooktranslate_24-0
 380. https://www.theverge.com/2017/8/4/16093872/facebook-ai-translations-artificial-intelligence
 381. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-25
 382. http://biometrics.cse.msu.edu/publications/machinelearning/baytasetal_patientsubtypingviatimeawarelstmnetworks.pdf
 383. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-26
 384. http://www.kdd.org/kdd2017/papers/view/patient-subtyping-via-time-aware-lstm-networks
 385. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-27
 386. http://www.kdd.org/
 387. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-28
 388. http://newatlas.com/microsoft-speech-recognition-equals-humans/50999
 389. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-29
 390. https://stats.stackexchange.com/q/320919/82135
 391. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-peepholelstm_30-0
 392. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-peepholelstm_30-1
 393. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-peepholelstm_30-2
 394. ftp://ftp.idsia.ch/pub/juergen/l-ieee.pdf
 395. https://en.wikipedia.org/wiki/digital_object_identifier
 396. https://doi.org/10.1109/72.963769
 397. https://en.wikipedia.org/wiki/pubmed_identifier
 398. https://www.ncbi.nlm.nih.gov/pubmed/18249962
 399. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-peephole2002_31-0
 400. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-peephole2002_31-1
 401. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-peephole2002_31-2
 402. http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf
 403. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-32
 404. ftp://ftp.idsia.ch/pub/juergen/l-ieee.pdf
 405. https://en.wikipedia.org/wiki/digital_object_identifier
 406. https://doi.org/10.1109/72.963769
 407. https://en.wikipedia.org/wiki/international_standard_serial_number
 408. https://www.worldcat.org/issn/1045-9227
 409. https://en.wikipedia.org/wiki/pubmed_identifier
 410. https://www.ncbi.nlm.nih.gov/pubmed/18249962
 411. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-33
 412. https://en.wikipedia.org/wiki/arxiv
 413. https://arxiv.org/abs/1506.04214
 414. https://en.wikipedia.org/wiki/bibcode
 415. http://adsabs.harvard.edu/abs/2015arxiv150604214s
 416. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-34
 417. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-gradf_35-0
 418. https://www.researchgate.net/publication/2839938
 419. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-fernandez2007_36-0
 420. https://en.wikipedia.org/wiki/citeseerx
 421. https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.79.1887
 422. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-graves2006_37-0
 423. https://en.wikipedia.org/wiki/citeseerx
 424. https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.75.6306
 425. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-wierstra2005_38-0
 426. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-wierstra2005_38-1
 427. https://www.academia.edu/5830256
 428. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-openaifive_39-0
 429. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-openaifive_39-1
 430. https://towardsdatascience.com/the-science-behind-openai-five-that-just-produced-one-of-the-greatest-breakthrough-in-the-history-b045bcdc2b69
 431. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-openaihand_40-0
 432. https://blog.openai.com/learning-dexterity/
 433. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-alphastar_41-0
 434. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-alphastar_41-1
 435. https://medium.com/mlmemoirs/deepminds-ai-alphastar-showcases-significant-progress-towards-agi-93810c94fbe9
 436. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-42
 437. https://en.wikipedia.org/wiki/citeseerx
 438. https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.218.3399
 439. https://en.wikipedia.org/wiki/digital_object_identifier
 440. https://doi.org/10.1109/iros.2006.282190
 441. https://en.wikipedia.org/wiki/international_standard_book_number
 442. https://en.wikipedia.org/wiki/special:booksources/978-1-4244-0258-8
 443. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-43
 444. https://en.wikipedia.org/wiki/citeseerx
 445. https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.331.5800
 446. https://en.wikipedia.org/wiki/digital_object_identifier
 447. https://doi.org/10.1016/j.neunet.2005.06.042
 448. https://en.wikipedia.org/wiki/pubmed_identifier
 449. https://www.ncbi.nlm.nih.gov/pubmed/16112549
 450. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-44
 451. http://dl.acm.org/citation.cfm?id=1778066.1778092
 452. https://en.wikipedia.org/wiki/international_standard_book_number
 453. https://en.wikipedia.org/wiki/special:booksources/978-3540746935
 454. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-referencea_45-0
 455. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-46
 456. https://en.wikipedia.org/wiki/citeseerx
 457. https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.116.3620
 458. https://en.wikipedia.org/wiki/digital_object_identifier
 459. https://doi.org/10.1007/3-540-46084-5_47
 460. https://en.wikipedia.org/wiki/international_standard_book_number
 461. https://en.wikipedia.org/wiki/special:booksources/978-3540460848
 462. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-47
 463. https://en.wikipedia.org/wiki/citeseerx
 464. https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.7369
 465. https://en.wikipedia.org/wiki/digital_object_identifier
 466. https://doi.org/10.1162/089976602320263980
 467. https://en.wikipedia.org/wiki/pubmed_identifier
 468. https://www.ncbi.nlm.nih.gov/pubmed/12184841
 469. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-48
 470. https://en.wikipedia.org/wiki/citeseerx
 471. https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.381.1992
 472. https://en.wikipedia.org/wiki/digital_object_identifier
 473. https://doi.org/10.1016/s0893-6080(02)00219-8
 474. https://en.wikipedia.org/wiki/pubmed_identifier
 475. https://www.ncbi.nlm.nih.gov/pubmed/12628609
 476. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-49
 477. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-50
 478. http://dl.acm.org/citation.cfm?id=2981562.2981635
 479. https://en.wikipedia.org/wiki/international_standard_book_number
 480. https://en.wikipedia.org/wiki/special:booksources/9781605603520
 481. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-51
 482. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-52
 483. https://en.wikipedia.org/wiki/arxiv
 484. https://arxiv.org/abs/1801.10111
 485. https://arxiv.org/archive/cs.cv
 486. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-53
 487. https://en.wikipedia.org/wiki/digital_object_identifier
 488. https://doi.org/10.1093/bioinformatics/btm247
 489. https://en.wikipedia.org/wiki/pubmed_identifier
 490. https://www.ncbi.nlm.nih.gov/pubmed/17488755
 491. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-54
 492. https://en.wikipedia.org/wiki/digital_object_identifier
 493. https://doi.org/10.1109/tcbb.2007.1015
 494. https://en.wikipedia.org/wiki/pubmed_identifier
 495. https://www.ncbi.nlm.nih.gov/pubmed/17666763
 496. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-55
 497. https://www.elen.ucl.ac.be/proceedings/esann/esannpdf/es2015-56.pdf
 498. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-56
 499. https://en.wikipedia.org/wiki/arxiv
 500. https://arxiv.org/abs/1612.02130
 501. https://en.wikipedia.org/wiki/digital_object_identifier
 502. https://doi.org/10.1007/978-3-319-59536-8_30
 503. https://en.wikipedia.org/wiki/international_standard_book_number
 504. https://en.wikipedia.org/wiki/special:booksources/978-3-319-59535-1
 505. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-57
 506. http://proceedings.mlr.press/v56/choi16.html
 507. https://en.wikipedia.org/wiki/arxiv
 508. https://arxiv.org/abs/1511.05942
 509. https://en.wikipedia.org/wiki/bibcode
 510. http://adsabs.harvard.edu/abs/2015arxiv151105942c
 511. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-58
 512. https://arxiv.org/abs/1606.03622
 513. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-wang_duan_zhang_niu_p=1657_59-0
 514. https://qilin-zhang.github.io/_pages/pdfs/segment-tube_spatio-temporal_action_localization_in_untrimmed_videos_with_per-frame_segmentation.pdf
 515. https://en.wikipedia.org/wiki/digital_object_identifier
 516. https://doi.org/10.3390/s18051657
 517. https://en.wikipedia.org/wiki/international_standard_serial_number
 518. https://www.worldcat.org/issn/1424-8220
 519. https://en.wikipedia.org/wiki/pubmed_central
 520. https://www.ncbi.nlm.nih.gov/pmc/articles/pmc5982167
 521. https://en.wikipedia.org/wiki/pubmed_identifier
 522. https://www.ncbi.nlm.nih.gov/pubmed/29789447
 523. https://en.wikipedia.org/wiki/long_short-term_memory#cite_ref-duan_wang_zhai_zheng_2018_p._60-0
 524. https://en.wikipedia.org/wiki/digital_object_identifier
 525. https://doi.org/10.1109/icip.2018.8451692
 526. https://en.wikipedia.org/wiki/international_standard_book_number
 527. https://en.wikipedia.org/wiki/special:booksources/978-1-4799-7061-2
 528. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit&section=17
 529. http://www.idsia.ch/~juergen/id56.html
 530. https://en.wikipedia.org/wiki/j  rgen_schmidhuber
 531. https://en.wikipedia.org/wiki/idsia
 532. http://www.felixgers.de/papers/phd.pdf
 533. http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf
 534. http://etd.uwc.ac.za/xmlui/handle/11394/249
 535. https://en.wikipedia.org/wiki/handle_system
 536. https://hdl.handle.net/11394/249
 537. https://web.archive.org/web/20120522234026/http://etd.uwc.ac.za/usrfiles/modules/etd/docs/etd_init_3937_1174040706.pdf
 538. http://etd.uwc.ac.za/bitstream/handle/11394/249/abidogun_msc_2005.pdf
 539. http://www.cs.umd.edu/~dmonner/papers/nn2012.pdf
 540. http://christianherta.de/lehre/datascience/machinelearning/neuralnetworks/lstm.html
 541. https://github.com/guillaume-chevalier/lstm-human-activity-recognition
 542. https://en.wikipedia.org/wiki/github
 543. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&oldid=890927981
 544. https://en.wikipedia.org/wiki/help:category
 545. https://en.wikipedia.org/wiki/category:artificial_neural_networks
 546. https://en.wikipedia.org/wiki/category:cs1:_long_volume_value
 547. https://en.wikipedia.org/wiki/category:all_articles_with_unsourced_statements
 548. https://en.wikipedia.org/wiki/category:articles_with_unsourced_statements_from_october_2017
 549. https://en.wikipedia.org/wiki/special:mytalk
 550. https://en.wikipedia.org/wiki/special:mycontributions
 551. https://en.wikipedia.org/w/index.php?title=special:createaccount&returnto=long+short-term+memory
 552. https://en.wikipedia.org/w/index.php?title=special:userlogin&returnto=long+short-term+memory
 553. https://en.wikipedia.org/wiki/long_short-term_memory
 554. https://en.wikipedia.org/wiki/talk:long_short-term_memory
 555. https://en.wikipedia.org/wiki/long_short-term_memory
 556. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=edit
 557. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=history
 558. https://en.wikipedia.org/wiki/main_page
 559. https://en.wikipedia.org/wiki/portal:contents
 560. https://en.wikipedia.org/wiki/portal:featured_content
 561. https://en.wikipedia.org/wiki/portal:current_events
 562. https://en.wikipedia.org/wiki/special:random
 563. https://donate.wikimedia.org/wiki/special:fundraiserredirector?utm_source=donate&utm_medium=sidebar&utm_campaign=c13_en.wikipedia.org&uselang=en
 564. https://shop.wikimedia.org/
 565. https://en.wikipedia.org/wiki/help:contents
 566. https://en.wikipedia.org/wiki/wikipedia:about
 567. https://en.wikipedia.org/wiki/wikipedia:community_portal
 568. https://en.wikipedia.org/wiki/special:recentchanges
 569. https://en.wikipedia.org/wiki/wikipedia:contact_us
 570. https://en.wikipedia.org/wiki/special:whatlinkshere/long_short-term_memory
 571. https://en.wikipedia.org/wiki/special:recentchangeslinked/long_short-term_memory
 572. https://en.wikipedia.org/wiki/wikipedia:file_upload_wizard
 573. https://en.wikipedia.org/wiki/special:specialpages
 574. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&oldid=890927981
 575. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&action=info
 576. https://www.wikidata.org/wiki/special:entitypage/q6673524
 577. https://en.wikipedia.org/w/index.php?title=special:citethispage&page=long_short-term_memory&id=890927981
 578. https://en.wikipedia.org/w/index.php?title=special:book&bookcmd=book_creator&referer=long+short-term+memory
 579. https://en.wikipedia.org/w/index.php?title=special:electronpdf&page=long+short-term+memory&action=show-download-screen
 580. https://en.wikipedia.org/w/index.php?title=long_short-term_memory&printable=yes
 581. https://ca.wikipedia.org/wiki/long_short-term_memory
 582. https://de.wikipedia.org/wiki/long_short-term_memory
 583. https://fa.wikipedia.org/wiki/          _            _          -      
 584. https://ja.wikipedia.org/wiki/                  
 585. https://ru.wikipedia.org/wiki/            _                          _            
 586. https://uk.wikipedia.org/wiki/          _                        _      '      
 587. https://zh.wikipedia.org/wiki/               
 588. https://www.wikidata.org/wiki/special:entitypage/q6673524#sitelinks-wikipedia
 589. https://en.wikipedia.org/wiki/wikipedia:text_of_creative_commons_attribution-sharealike_3.0_unported_license
 590. https://foundation.wikimedia.org/wiki/terms_of_use
 591. https://foundation.wikimedia.org/wiki/privacy_policy
 592. https://www.wikimediafoundation.org/
 593. https://foundation.wikimedia.org/wiki/privacy_policy
 594. https://en.wikipedia.org/wiki/wikipedia:about
 595. https://en.wikipedia.org/wiki/wikipedia:general_disclaimer
 596. https://en.wikipedia.org/wiki/wikipedia:contact_us
 597. https://www.mediawiki.org/wiki/special:mylanguage/how_to_contribute
 598. https://foundation.wikimedia.org/wiki/cookie_statement
 599. https://en.m.wikipedia.org/w/index.php?title=long_short-term_memory&mobileaction=toggle_view_mobile
 600. https://wikimediafoundation.org/
 601. https://www.mediawiki.org/

   hidden links:
 603. https://en.wikipedia.org/wiki/file:the_lstm_cell.png
 604. https://en.wikipedia.org/wiki/file:the_lstm_cell.png
 605. https://en.wikipedia.org/wiki/file:peephole_long_short-term_memory.svg
 606. https://en.wikipedia.org/wiki/file:peephole_long_short-term_memory.svg
 607. https://en.wikipedia.org/wiki/main_page
 608. https://creativecommons.org/licenses/by-sa/3.0/
