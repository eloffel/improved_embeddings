   #[1]github [2]recent commits to td-treelstm:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]11
     * [35]star [36]75
     * [37]fork [38]22

[39]xingxingzhang/[40]td-treelstm

   [41]code [42]issues 0 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   top-down tree lstm (naacl 2016)
   [47]http://aclweb.org/anthology/n/n16/n16-1035.pdf
     * [48]8 commits
     * [49]1 branch
     * [50]0 releases
     * [51]fetching contributors
     * [52]view license

    1. [53]lua 70.6%
    2. [54]perl 22.1%
    3. [55]shell 4.0%
    4. [56]python 3.3%

   (button) lua perl shell python
   branch: master (button) new pull request
   [57]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/x
   [58]download zip

downloading...

   want to be notified of new releases in xingxingzhang/td-treelstm?
   [59]sign in [60]sign up

launching github desktop...

   if nothing happens, [61]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [62]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [63]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [64]download the github extension for visual studio
   and try again.

   (button) go back
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [65]permalink
   type        name        latest commit message  commit time
        failed to load latest commit information.
        [66]conllx_scripts
        [67]dataset
        [68]experiments
        [69]layers         [70]intial commit     mar 13, 2016
        [71]msr_scripts
        [72]nnets
        [73]scripts
        [74]utils
        [75]license
        [76]old_version.md
        [77]readme.md
        [78]dep_rerank.lua
        [79]gpu_lock.py
        [80]init.lua
        [81]main.lua
        [82]main_bid.lua
        [83]main_nce.lua
        [84]rerank.lua
        [85]sampler.lua
        [86]train_mlp.lua

readme.md

top-down tree id137

   a [87]torch implementation of the top-down treelstm described in the
   following paper.

[88]top-down tree id137

   xingxing zhang, liang lu and mirella lapata. in proceedings of the 2016
   conference of the north american chapter of the association for
   computational linguistics: human language technologies (naacl 2016).
@inproceedings{zhang-lu-lapata:2016:n16-1,
  author    = {zhang, xingxing  and  lu, liang  and  lapata, mirella},
  title     = {top-down tree id137},
  booktitle = {proceedings of the 2016 conference of the north american chapter
of the association for computational linguistics: human language technologies},
  month     = {june},
  year      = {2016},
  address   = {san diego, california},
  publisher = {association for computational linguistics},
  pages     = {310--320},
  url       = {http://www.aclweb.org/anthology/n16-1035}
}

implemented models

     * treelstm
     * treelstm-nce
     * ldtreelstm
     * ldtreelstm-nce

   treelstm and ldtreelstm (check the details in the paper above) are
   trained with negative log-likelihood (nll); while treelstm-nce and
   ldtreelstm-nce are trained with [89]noise contrastive estimation (nce)
   (see [90]this paper and also [91]this paper for details).

   note that in experiments, the id172 term z of nce is learned
   automatically. the implemented nce module also support keeping z fixed.

requirements

     * a nvidia gpu
     * [92]cuda 6.5.19 (higher version should be fine)
     * [93]torch
     * [94]torch-hdf5

   torch can be installed with the instructions [95]here. you also need to
   install some torch components.
luarocks install nn
luarocks install nngraph
luarocks install cutorch
luarocks install cunn

   you may find [96]this document useful when installing torch-hdf5 (don't
   use luarocks).

   please also note that to run the code, you need to use an old version
   of torch with the instructions [97]here.

id38 (msr sentence completion)

   pre-trained models (treelstm-400 and ldtreelstm-400) are available
   [98]https://drive.google.com/file/d/0b6-ykfw-mnboc3pya29uzkzpufu/view?u
   sp=sharing

preprocessing

   first, parse the dataset into dependency trees using [99]stanford
   corenlp toolkit. it should looks like this
silap10.txt#0   det(etext-4, the-1) nn(etext-4, project-2) nn(etext-4, gutenberg
-3) root(root-0, etext-4) prep(etext-4, of-5) det(rise-7, the-6) pobj(of-5, rise
-7) prep(rise-7, of-8) nn(lapham-10, silas-9) pobj(of-8, lapham-10) prep(etext-4
, by-11) nn(howells-14, william-12) nn(howells-14, dean-13) pobj(by-11, howells-
14) det(rise-16, the-15) dep(etext-4, rise-16) prep(rise-16, of-17) nn(lapham-19
, silas-18) pobj(of-17, lapham-19) prep(rise-16, by-20) nn(howells-23, william-2
1) nn(howells-23, dean-22) pobj(by-20, howells-23) npadvmod(howells-23, i-24) pu
nct(etext-4, .-25)
silap10.txt#1   advmod(went-4, when-1) nn(hubbard-3, bartley-2) nsubj(went-4, hu
bbard-3) advcl(received-40, went-4) aux(interview-6, to-5) xcomp(went-4, intervi
ew-6) nn(lapham-8, silas-7) dobj(interview-6, lapham-8) prep(interview-6, for-9)
 det(men-13, the-10) punct(men-13, ``-11) amod(men-13, solid-12) pobj(for-9, men
-13) prep(men-13, of-14) pobj(of-14, boston-15) punct(men-13, ''-16) dep(men-13,
 series-17) punct(series-17, ,-18) dobj(undertook-21, which-19) nsubj(undertook-
21, he-20) rcmod(series-17, undertook-21) aux(finish-23, to-22) xcomp(undertook-
21, finish-23) prt(finish-23, up-24) prep(finish-23, in-25) det(events-27, the-2
6) pobj(in-25, events-27) punct(received-40, ,-28) mark(replaced-31, after-29) n
subj(replaced-31, he-30) advcl(received-40, replaced-31) poss(projector-34, thei
r-32) amod(projector-34, original-33) dobj(replaced-31, projector-34) prep(repla
ced-31, on-35) det(newspaper-37, that-36) pobj(on-35, newspaper-37) punct(receiv
ed-40, ,-38) nsubj(received-40, lapham-39) root(root-0, received-40) dobj(receiv
ed-40, him-41) prep(received-40, in-42) poss(office-45, his-43) amod(office-45,
private-44) pobj(in-42, office-45) prep(received-40, by-46) amod(appointment-48,
 previous-47) pobj(by-46, appointment-48) punct(received-40, .-49)
...
...
...

   each line is a sentence (format: label \t dependency tuples), where
   silap10.txt#0 is the label for the sentence (it can be any string and
   it doesn't matter).

   dataset after the preprocessing above can be downloaded [100]here.

   then, convert the dependency tree dataset into hdf5 format and sort the
   dataset to make sure sentences in each batch have similar length.
   sorting the dataset is for faster training, which is a commonly used
   strategy for training id56 or sequence based models.

create dataset for treelstm

cd scripts
./run_msr.sh
./run_msr_sort.sh
./run_msr_test.sh

   note the program will crash when running ./run_msr.sh. you can ignore
   the crash or you should use --sort 0 switch instead of --sort 20.

create dataset for ldtreelstm

cd scripts
./run_msr_bid.sh
./run_msr_sort_bid.sh
./run_msr_test_bid.sh

   alternately, you can contact the first author to request the dataset
   after preprocessing.

training and evaluation

   basically it is just one command.
cd experiments/msr
# to run treelstm with hidden size 400
./treelstm_h400.sh
# to run ldtreelstm with hidden size 400
./ldtreelstm_h400.sh

   but don't forget to specify where is your code, your dataset and
   whatever by modifying treelstm_h400.sh or ldtreelstm_h400.sh.
# where is your code? (you should use absolute path)
codedir=/afs/inf.ed.ac.uk/group/project/xtreelstm/xtreelstm/td-treelstm-release
# where is your dataset (you should use absolute path)
dataset=/disk/scratch/xingxingzhang/treelstm/dataset/msr/msr.dep.100.bid.sort20.
h5
# label for this model
label=.ldtree.400

# where is your testset (you should use absolute path); this will only be used i
n evaluation
testfile=/disk/scratch/xingxingzhang/treelstm/dataset/msr/msr.dep.100.bid.questi
on.h5

id33 reranking

preprocessing

for treelstm

cd scripts
./run_conllx_sort.sh

for ldtreelstm

cd scripts
./run_conllx_sort_bid.sh

train and evaluate dependency reranking models

   training treelstms and ldtreelstms are quit similar. the following is
   about training a treelstm.
cd experiments/depparse
./treelstm_we_train.sh


   then, you will get a trained treelstm. we can use this treelstm to
   rerank the k dependencies produced by the second order [101]mstparser.

   the following script will use the trained dependency model to rerank
   the top 20 dependencies from msrparser on the validation set. the
   script will try different k and choose the one gives best uas.
./treelstm_we_rerank_valid.sh

   given the k we've got from the validation set, we can get the reranking
   performance on test set by using the following script.
./treelstm_we_rerank_test.sh

dependency tree generation

how will we generate dependency trees? (details see section 3.4 of the paper)

     * run the id38 experiment or the id33
       experiment to get a trained treelstm or ldtreelstm
     * generate training data for the four classifiers (add-left,
       add-right, add-nx-left, add-nx-right)
     * train add-left, add-right, add-nx-left and add-nx-right
     * generate dependency trees with a trained treelstm (or ldtreelstm)
       and the four classifiers

generate training data

   go to sampler.lua and run the following code
-- model_1.0.w200.t7 is the trained treelstm
-- penn_wsj.conllx.sort.h5 is the dataset for the trained treelstm
-- eot.penn_wsj.conllx.sort.h5 is the output dataset for the four classifiers
generatedataset('/disk/scratch/xingxingzhang/treelstm/experiments/ptb_depparse/2
layer_w_wo_we/model_1.0.w200.t7',
    '/disk/scratch/xingxingzhang/treelstm/dataset/depparse/dataset/penn_wsj.conl
lx.sort.h5',
    '/disk/scratch/xingxingzhang/treelstm/dataset/depparse/eot.penn_wsj.conllx.s
ort.h5')


train the four classifiers

   use train_mlp.lua
$ th train_mlp.lua -h
usage: /afs/inf.ed.ac.uk/user/s12/s1270921/torch/install/lib/luarocks/rocks/trep
l/scm-1/bin/th [options]
====== mlp v 1.0 ======

  --seed        random seed [123]
  --usegpu      use gpu [false]
  --snhids      string hidden sizes for each layer [400,300,300,2]
  --activ       options: tanh, relu [tanh]
  --dropout     dropout rate (dropping) [0]
  --maxepoch    max number of epochs [10]
  --dataset     dataset [/disk/scratch/xingxingzhang/treelstm/dataset/depparse/e
ot.penn_wsj.conllx.sort.h5]
  --ftype        [|x|oe|]
  --ytype        [1]
  --batchsize    [256]
  --lr           [0.01]
  --optimmethod options: sgd, adagrad [adagrad]
  --save        save path [model.t7]


   note --ytype 1, 2, 3, 4 corresponds to the four classifiers. here is a
   sample script:
id=`./gpu_lock.py --id-to-hog 2`
echo $id
if [ $id -eq -1 ]; then
    echo "no gpu is free"
    exit
fi
./gpu_lock.py

curdir=`pwd`
codedir=/afs/inf.ed.ac.uk/group/project/xtreelstm/xtreelstm/mlp_test
lr=0.01
label=yt1.x.oe
model=model.$label.t7
log=log.$label.txt
echo $curdir
echo $codedir

cd $codedir
cuda_visible_devices=$id th train_mlp.lua --usegpu \
    --activ relu --dropout 0.5 --lr $lr --maxepoch 10 \
    --snhids "400,300,300,2" --ftype "|x|oe|" --ytype 1 \
    --save $curdir/$model | tee $curdir/$log

cd $curdir

./gpu_lock.py --free $id
./gpu_lock.py


generation by sampling

   go to sampler.lua and run the following code. the code will output
   dependency trees in latex format.
-- model_1.0.w200.t7: trained treelstm
-- model.yt%d.x.oe.t7: trained classifiers, note that model.yt1.x.oe.t7, model.y
t2.x.oe.t7, model.yt3.x.oe.t7 and model.yt4.x.oe.t7 must all exist
sampletrees('/disk/scratch/xingxingzhang/treelstm/experiments/ptb_depparse/2laye
r_w_wo_we/model_1.0.w200.t7',
    '/disk/scratch/xingxingzhang/treelstm/experiments/sampling/eot_classify/mode
l.yt%d.x.oe.t7',
    's100.txt', -- output dependency trees
    1,     -- rand seed
    100)   -- number of tree samples

     *    2019 github, inc.
     * [102]terms
     * [103]privacy
     * [104]security
     * [105]status
     * [106]help

     * [107]contact github
     * [108]pricing
     * [109]api
     * [110]training
     * [111]blog
     * [112]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [113]reload to refresh your
   session. you signed out in another tab or window. [114]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/xingxingzhang/td-treelstm/commits/master.atom
   3. https://github.com/xingxingzhang/td-treelstm#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/xingxingzhang/td-treelstm
  32. https://github.com/join
  33. https://github.com/login?return_to=/xingxingzhang/td-treelstm
  34. https://github.com/xingxingzhang/td-treelstm/watchers
  35. https://github.com/login?return_to=/xingxingzhang/td-treelstm
  36. https://github.com/xingxingzhang/td-treelstm/stargazers
  37. https://github.com/login?return_to=/xingxingzhang/td-treelstm
  38. https://github.com/xingxingzhang/td-treelstm/network/members
  39. https://github.com/xingxingzhang
  40. https://github.com/xingxingzhang/td-treelstm
  41. https://github.com/xingxingzhang/td-treelstm
  42. https://github.com/xingxingzhang/td-treelstm/issues
  43. https://github.com/xingxingzhang/td-treelstm/pulls
  44. https://github.com/xingxingzhang/td-treelstm/projects
  45. https://github.com/xingxingzhang/td-treelstm/pulse
  46. https://github.com/join?source=prompt-code
  47. http://aclweb.org/anthology/n/n16/n16-1035.pdf
  48. https://github.com/xingxingzhang/td-treelstm/commits/master
  49. https://github.com/xingxingzhang/td-treelstm/branches
  50. https://github.com/xingxingzhang/td-treelstm/releases
  51. https://github.com/xingxingzhang/td-treelstm/graphs/contributors
  52. https://github.com/xingxingzhang/td-treelstm/blob/master/license
  53. https://github.com/xingxingzhang/td-treelstm/search?l=lua
  54. https://github.com/xingxingzhang/td-treelstm/search?l=perl
  55. https://github.com/xingxingzhang/td-treelstm/search?l=shell
  56. https://github.com/xingxingzhang/td-treelstm/search?l=python
  57. https://github.com/xingxingzhang/td-treelstm/find/master
  58. https://github.com/xingxingzhang/td-treelstm/archive/master.zip
  59. https://github.com/login?return_to=https://github.com/xingxingzhang/td-treelstm
  60. https://github.com/join?return_to=/xingxingzhang/td-treelstm
  61. https://desktop.github.com/
  62. https://desktop.github.com/
  63. https://developer.apple.com/xcode/
  64. https://visualstudio.github.com/
  65. https://github.com/xingxingzhang/td-treelstm/tree/5f11c4f4e22362a476b9900f87e65f238b891dcb
  66. https://github.com/xingxingzhang/td-treelstm/tree/master/conllx_scripts
  67. https://github.com/xingxingzhang/td-treelstm/tree/master/dataset
  68. https://github.com/xingxingzhang/td-treelstm/tree/master/experiments
  69. https://github.com/xingxingzhang/td-treelstm/tree/master/layers
  70. https://github.com/xingxingzhang/td-treelstm/commit/b2958f29399d252ac00e4d944e46a1edd4f7f479
  71. https://github.com/xingxingzhang/td-treelstm/tree/master/msr_scripts
  72. https://github.com/xingxingzhang/td-treelstm/tree/master/nnets
  73. https://github.com/xingxingzhang/td-treelstm/tree/master/scripts
  74. https://github.com/xingxingzhang/td-treelstm/tree/master/utils
  75. https://github.com/xingxingzhang/td-treelstm/blob/master/license
  76. https://github.com/xingxingzhang/td-treelstm/blob/master/old_version.md
  77. https://github.com/xingxingzhang/td-treelstm/blob/master/readme.md
  78. https://github.com/xingxingzhang/td-treelstm/blob/master/dep_rerank.lua
  79. https://github.com/xingxingzhang/td-treelstm/blob/master/gpu_lock.py
  80. https://github.com/xingxingzhang/td-treelstm/blob/master/init.lua
  81. https://github.com/xingxingzhang/td-treelstm/blob/master/main.lua
  82. https://github.com/xingxingzhang/td-treelstm/blob/master/main_bid.lua
  83. https://github.com/xingxingzhang/td-treelstm/blob/master/main_nce.lua
  84. https://github.com/xingxingzhang/td-treelstm/blob/master/rerank.lua
  85. https://github.com/xingxingzhang/td-treelstm/blob/master/sampler.lua
  86. https://github.com/xingxingzhang/td-treelstm/blob/master/train_mlp.lua
  87. https://github.com/torch
  88. http://aclweb.org/anthology/n/n16/n16-1035.pdf
  89. https://www.cs.helsinki.fi/u/ahyvarin/papers/gutmann12jmlr.pdf
  90. https://www.cs.helsinki.fi/u/ahyvarin/papers/gutmann12jmlr.pdf
  91. https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf
  92. http://www.nvidia.com/object/cuda_home_new.html
  93. https://github.com/torch
  94. https://github.com/deepmind/torch-hdf5
  95. http://torch.ch/docs/getting-started.html
  96. https://github.com/deepmind/torch-hdf5/blob/master/doc/usage.md
  97. https://github.com/xingxingzhang/td-treelstm/blob/master/old_version.md
  98. https://drive.google.com/file/d/0b6-ykfw-mnboc3pya29uzkzpufu/view?usp=sharing
  99. http://stanfordnlp.github.io/corenlp/
 100. https://drive.google.com/file/d/0b6-ykfw-mnbocme5tmryvlzytjg/view?usp=sharing
 101. http://www.seas.upenn.edu/~strctlrn/mstparser/mstparser.html
 102. https://github.com/site/terms
 103. https://github.com/site/privacy
 104. https://github.com/security
 105. https://githubstatus.com/
 106. https://help.github.com/
 107. https://github.com/contact
 108. https://github.com/pricing
 109. https://developer.github.com/
 110. https://training.github.com/
 111. https://github.blog/
 112. https://github.com/about
 113. https://github.com/xingxingzhang/td-treelstm
 114. https://github.com/xingxingzhang/td-treelstm

   hidden links:
 116. https://github.com/
 117. https://github.com/xingxingzhang/td-treelstm
 118. https://github.com/xingxingzhang/td-treelstm
 119. https://github.com/xingxingzhang/td-treelstm
 120. https://help.github.com/articles/which-remote-url-should-i-use
 121. https://github.com/xingxingzhang/td-treelstm#top-down-tree-long-short-term-memory-networks
 122. https://github.com/xingxingzhang/td-treelstm#top-down-tree-long-short-term-memory-networks-1
 123. https://github.com/xingxingzhang/td-treelstm#implemented-models
 124. https://github.com/xingxingzhang/td-treelstm#requirements
 125. https://github.com/xingxingzhang/td-treelstm#language-modeling-msr-sentence-completion
 126. https://github.com/xingxingzhang/td-treelstm#preprocessing
 127. https://github.com/xingxingzhang/td-treelstm#create-dataset-for-treelstm
 128. https://github.com/xingxingzhang/td-treelstm#create-dataset-for-ldtreelstm
 129. https://github.com/xingxingzhang/td-treelstm#training-and-evaluation
 130. https://github.com/xingxingzhang/td-treelstm#dependency-parsing-reranking
 131. https://github.com/xingxingzhang/td-treelstm#preprocessing-1
 132. https://github.com/xingxingzhang/td-treelstm#for-treelstm
 133. https://github.com/xingxingzhang/td-treelstm#for-ldtreelstm
 134. https://github.com/xingxingzhang/td-treelstm#train-and-evaluate-dependency-reranking-models
 135. https://github.com/xingxingzhang/td-treelstm#dependency-tree-generation
 136. https://github.com/xingxingzhang/td-treelstm#how-will-we-generate-dependency-trees-details-see-section-34-of-the-paper
 137. https://github.com/xingxingzhang/td-treelstm#generate-training-data
 138. https://github.com/xingxingzhang/td-treelstm#train-the-four-classifiers
 139. https://github.com/xingxingzhang/td-treelstm#generation-by-sampling
 140. https://github.com/
