   #[1]index [2]search [3]modeling and generating sequences of polyphonic
   music with the id56-rbm [4]recurrent neural networks with word
   embeddings

navigation

     * [5]index
     * [6]next |
     * [7]previous |
     * [8]deeplearning 0.1 documentation   

[9]table of contents

     * [10]id137 for id31
          + [11]summary
          + [12]data
          + [13]model
               o [14]lstm
               o [15]our model
          + [16]code - citations - contact
               o [17]code
               o [18]papers
               o [19]contact
          + [20]references

previous topic

   [21]recurrent neural networks with id27s

next topic

   [22]modeling and generating sequences of polyphonic music with the
   id56-rbm

this page

     * [23]show source

quick search

   ____________________
   go

id137 for id31[24]  

summary[25]  

   this tutorial aims to provide an example of how a recurrent neural
   network (id56) using the long short term memory (lstm) architecture can
   be implemented using theano. in this tutorial, this model is used to
   perform id31 on movie reviews from the [26]large movie
   review dataset, sometimes known as the imdb dataset.

   in this task, given a movie review, the model attempts to predict
   whether it is positive or negative. this is a binary classification
   task.

data[27]  

   as previously mentioned, the provided scripts are used to train a lstm
   recurrent neural network on the large movie review dataset dataset.

   while the dataset is public, in this tutorial we provide a copy of the
   dataset that has previously been preprocessed according to the needs of
   this lstm implementation. running the code provided in this tutorial
   will automatically download the data to the local directory. in order
   to use your own data, please use a ([28]preprocessing script) provided
   as a part of this tutorial.

   once the model is trained, you can test it with your own corpus using
   the word-index dictionary ([29]imdb.dict.pkl.gz) provided as a part of
   this tutorial.

model[30]  

lstm[31]  

   in a traditional recurrent neural network, during the gradient
   back-propagation phase, the gradient signal can end up being multiplied
   a large number of times (as many as the number of timesteps) by the
   weight matrix associated with the connections between the neurons of
   the recurrent hidden layer. this means that, the magnitude of weights
   in the transition matrix can have a strong impact on the learning
   process.

   if the weights in this matrix are small (or, more formally, if the
   leading eigenvalue of the weight matrix is smaller than 1.0), it can
   lead to a situation called vanishing gradients where the gradient
   signal gets so small that learning either becomes very slow or stops
   working altogether. it can also make more difficult the task of
   learning long-term dependencies in the data. conversely, if the weights
   in this matrix are large (or, again, more formally, if the leading
   eigenvalue of the weight matrix is larger than 1.0), it can lead to a
   situation where the gradient signal is so large that it can cause
   learning to diverge. this is often referred to as exploding gradients.

   these issues are the main motivation behind the lstm model which
   introduces a new structure called a memory cell (see figure 1 below). a
   memory cell is composed of four main elements: an input gate, a neuron
   with a self-recurrent connection (a connection to itself), a forget
   gate and an output gate. the self-recurrent connection has a weight of
   1.0 and ensures that, barring any outside interference, the state of a
   memory cell can remain constant from one timestep to another. the gates
   serve to modulate the interactions between the memory cell itself and
   its environment. the input gate can allow incoming signal to alter the
   state of the memory cell or block it. on the other hand, the output
   gate can allow the state of the memory cell to have an effect on other
   neurons or prevent it. finally, the forget gate can modulate the memory
   cell   s self-recurrent connection, allowing the cell to remember or
   forget its previous state, as needed.
   _images/lstm_memorycell.png

   figure 1: illustration of an lstm memory cell.

   the equations below describe how a layer of memory cells is updated at
   every timestep t . in these equations:
     * x_t is the input to the memory cell layer at time t
     * w_i , w_f , w_c , w_o , u_i , u_f , u_c , u_o and v_o are weight
       matrices
     * b_i , b_f , b_c and b_o are bias vectors

   first, we compute the values for i_t , the input gate, and
   \widetilde{c_t} the candidate value for the states of the memory cells
   at time t :

   (1)[32]   i_t = \sigma(w_i x_t + u_i h_{t-1} + b_i)

   (2)[33]   \widetilde{c_t} = tanh(w_c x_t + u_c h_{t-1} + b_c)

   second, we compute the value for f_t , the activation of the memory
   cells    forget gates at time t :

   (3)[34]   f_t = \sigma(w_f x_t + u_f h_{t-1} + b_f)

   given the value of the input gate activation i_t , the forget gate
   activation f_t and the candidate state value \widetilde{c_t} , we can
   compute c_t the memory cells    new state at time t :

   (4)[35]   c_t = i_t * \widetilde{c_t} + f_t * c_{t-1}

   with the new state of the memory cells, we can compute the value of
   their output gates and, subsequently, their outputs:

   (5)[36]   o_t = \sigma(w_o x_t + u_o h_{t-1} + v_o c_t + b_o)

   (6)[37]   h_t = o_t * tanh(c_t)

our model[38]  

   the model we used in this tutorial is a variation of the standard lstm
   model. in this variant, the activation of a cell   s output gate does not
   depend on the memory cell   s state c_t . this allows us to perform part
   of the computation more efficiently (see the implementation note,
   below, for details). this means that, in the variant we have
   implemented, there is no matrix v_o and equation [39](5) is replaced by
   equation [40](7):

   (7)[41]   o_t = \sigma(w_o x_t + u_o h_{t-1} + b_o)

   our model is composed of a single lstm layer followed by an average
   pooling and a id28 layer as illustrated in figure 2
   below. thus, from an input sequence x_0, x_1, x_2, ..., x_n , the
   memory cells in the lstm layer will produce a representation sequence
   h_0, h_1, h_2, ..., h_n . this representation sequence is then averaged
   over all timesteps resulting in representation h. finally, this
   representation is fed to a id28 layer whose target is
   the class label associated with the input sequence.
   _images/lstm.png

   figure 2 : illustration of the model used in this tutorial. it is
   composed of a single lstm layer followed by mean pooling over time and
   id28.

   implementation note : in the code included this tutorial, the equations
   [42](1), [43](2), [44](3) and [45](7) are performed in parallel to make
   the computation more efficient. this is possible because none of these
   equations rely on a result produced by the other ones. it is achieved
   by concatenating the four matrices w_* into a single weight matrix w
   and performing the same concatenation on the weight matrices u_* to
   produce the matrix u and the bias vectors b_* to produce the vector b .
   then, the pre-nonlinearity activations can be computed with:

   z = w x_t + u h_{t-1} + b

   the result is then sliced to obtain the pre-nonlinearity activations
   for i , f , \widetilde{c_t} , and o and the non-linearities are then
   applied independently for each.

code - citations - contact[46]  

code[47]  

   the lstm implementation can be found in the two following files:
     * [48]lstm.py: main script. defines and train the model.
     * [49]imdb.py: secondary script. handles the loading and
       preprocessing of the imdb dataset.

   after downloading both scripts and putting both in the same folder, the
   user can run the code by calling:
theano_flags="floatx=float32" python lstm.py

   the script will automatically download the data and decompress it.

   note: the provided code supports the stochastic id119 (sgd),
   adadelta and rmsprop optimization methods. you are advised to use
   adadelta or rmsprop because sgd appears to performs poorly on this task
   with this particular model.

papers[50]  

   if you use this tutorial, please cite the following papers.

   introduction of the lstm model:
     * [51][pdf] hochreiter, s., & schmidhuber, j. (1997). long short-term
       memory. neural computation, 9(8), 1735-1780.

   addition of the forget gate to the lstm model:
     * [52][pdf] gers, f. a., schmidhuber, j., & cummins, f. (2000).
       learning to forget: continual prediction with lstm. neural
       computation, 12(10), 2451-2471.

   more recent lstm paper:
     * [53][pdf] graves, alex. supervised sequence labelling with
       recurrent neural networks. vol. 385. springer, 2012.

   papers related to theano:
     * [54][pdf] bastien, fr  d  ric, lamblin, pascal, pascanu, razvan,
       bergstra, james, goodfellow, ian, bergeron, arnaud, bouchard,
       nicolas, and bengio, yoshua. theano: new features and speed
       improvements. nips workshop on deep learning and unsupervised
       id171, 2012.
     * [55][pdf] bergstra, james, breuleux, olivier, bastien, fr  d  ric,
       lamblin, pascal, pascanu, razvan, desjardins, guillaume, turian,
       joseph, warde-farley, david, and bengio, yoshua. theano: a cpu and
       gpu math expression compiler. in proceedings of the python for
       scientific computing conference (scipy), june 2010.

   thank you!

contact[56]  

   please email [57]pierre luc carrier or [58]kyunghyun cho for any
   problem report or feedback. we will be glad to hear from you.

references[59]  

     * hochreiter, s., & schmidhuber, j. (1997). long short-term memory.
       neural computation, 9(8), 1735-1780.
     * gers, f. a., schmidhuber, j., & cummins, f. (2000). learning to
       forget: continual prediction with lstm. neural computation, 12(10),
       2451-2471.
     * graves, a. (2012). supervised sequence labelling with recurrent
       neural networks (vol. 385). springer.
     * hochreiter, s., bengio, y., frasconi, p., & schmidhuber, j. (2001).
       gradient flow in recurrent nets: the difficulty of learning
       long-term dependencies.
     * bengio, y., simard, p., & frasconi, p. (1994). learning long-term
       dependencies with id119 is difficult. neural networks,
       ieee transactions on, 5(2), 157-166.
     * maas, a. l., daly, r. e., pham, p. t., huang, d., ng, a. y., &
       potts, c. (2011, june). learning word vectors for sentiment
       analysis. in proceedings of the 49th annual meeting of the
       association for computational linguistics: human language
       technologies-volume 1 (pp. 142-150). association for computational
       linguistics.

navigation

     * [60]index
     * [61]next |
     * [62]previous |
     * [63]deeplearning 0.1 documentation   

      copyright 2008--2010, lisa lab. last updated on jun 15, 2018. created
   using [64]sphinx 1.5.

references

   1. http://deeplearning.net/tutorial/genindex.html
   2. http://deeplearning.net/tutorial/search.html
   3. http://deeplearning.net/tutorial/id56rbm.html
   4. http://deeplearning.net/tutorial/id56slu.html
   5. http://deeplearning.net/tutorial/genindex.html
   6. http://deeplearning.net/tutorial/id56rbm.html
   7. http://deeplearning.net/tutorial/id56slu.html
   8. http://deeplearning.net/tutorial/contents.html
   9. http://deeplearning.net/tutorial/contents.html
  10. http://deeplearning.net/tutorial/lstm.html
  11. http://deeplearning.net/tutorial/lstm.html#summary
  12. http://deeplearning.net/tutorial/lstm.html#data
  13. http://deeplearning.net/tutorial/lstm.html#model
  14. http://deeplearning.net/tutorial/lstm.html#id1
  15. http://deeplearning.net/tutorial/lstm.html#our-model
  16. http://deeplearning.net/tutorial/lstm.html#code-citations-contact
  17. http://deeplearning.net/tutorial/lstm.html#code
  18. http://deeplearning.net/tutorial/lstm.html#papers
  19. http://deeplearning.net/tutorial/lstm.html#contact
  20. http://deeplearning.net/tutorial/lstm.html#references
  21. http://deeplearning.net/tutorial/id56slu.html
  22. http://deeplearning.net/tutorial/id56rbm.html
  23. http://deeplearning.net/tutorial/_sources/lstm.txt
  24. http://deeplearning.net/tutorial/lstm.html#lstm-networks-for-sentiment-analysis
  25. http://deeplearning.net/tutorial/lstm.html#summary
  26. http://ai.stanford.edu/~amaas/data/sentiment/
  27. http://deeplearning.net/tutorial/lstm.html#data
  28. https://raw.githubusercontent.com/kyunghyuncho/deeplearningtutorials/master/code/imdb_preprocess.py
  29. http://www.iro.umontreal.ca/~lisa/deep/data/imdb.dict.pkl.gz
  30. http://deeplearning.net/tutorial/lstm.html#model
  31. http://deeplearning.net/tutorial/lstm.html#id1
  32. http://deeplearning.net/tutorial/lstm.html#equation-1
  33. http://deeplearning.net/tutorial/lstm.html#equation-2
  34. http://deeplearning.net/tutorial/lstm.html#equation-3
  35. http://deeplearning.net/tutorial/lstm.html#equation-4
  36. http://deeplearning.net/tutorial/lstm.html#equation-5
  37. http://deeplearning.net/tutorial/lstm.html#equation-6
  38. http://deeplearning.net/tutorial/lstm.html#our-model
  39. http://deeplearning.net/tutorial/lstm.html#equation-5
  40. http://deeplearning.net/tutorial/lstm.html#equation-5-alt
  41. http://deeplearning.net/tutorial/lstm.html#equation-5-alt
  42. http://deeplearning.net/tutorial/lstm.html#equation-1
  43. http://deeplearning.net/tutorial/lstm.html#equation-2
  44. http://deeplearning.net/tutorial/lstm.html#equation-3
  45. http://deeplearning.net/tutorial/lstm.html#equation-5-alt
  46. http://deeplearning.net/tutorial/lstm.html#code-citations-contact
  47. http://deeplearning.net/tutorial/lstm.html#code
  48. http://deeplearning.net/tutorial/code/lstm.py
  49. http://deeplearning.net/tutorial/code/imdb.py
  50. http://deeplearning.net/tutorial/lstm.html#papers
  51. http://deeplearning.cs.cmu.edu/pdfs/hochreiter97_lstm.pdf
  52. http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015
  53. http://www.cs.toronto.edu/~graves/preprint.pdf
  54. http://www.iro.umontreal.ca/~lisa/pointeurs/nips2012_deep_workshop_theano_final.pdf
  55. http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf
  56. http://deeplearning.net/tutorial/lstm.html#contact
  57. mailto:carriepl..at..iro.umontreal.ca
  58. http://www.kyunghyuncho.me/
  59. http://deeplearning.net/tutorial/lstm.html#references
  60. http://deeplearning.net/tutorial/genindex.html
  61. http://deeplearning.net/tutorial/id56rbm.html
  62. http://deeplearning.net/tutorial/id56slu.html
  63. http://deeplearning.net/tutorial/contents.html
  64. http://sphinx-doc.org/
