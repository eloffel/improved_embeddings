   iframe: [1]//www.googletagmanager.com/ns.html?id=gtm-p824sk

   [2]contact us [3][logo_skymind_white.svg]
   ai platform
   [4]skil [5]interactive demo [6]subscription [7]documentation
   [8]community support
   why skymind?
   [9]data scientists [10]solution architects [11]devops and sre
   [12]innovation leaders
   [13]solutions
   case studies
   [14]rpa [15]telecom fraud [16]insurance [17]supply chain & logistics
   [18]cybersecurity and data centers [19]financial services [20]image
   recognition [21]commerce and crm
   [22]about
   resources
   [23]blog [24]ai wiki [25]open-source

a.i. wiki

   do you like this content? we'll send you more.

   iframe: [26]https://go.pardot.com/l/456082/2018-09-28/jsv68v

   [ ] directory
   [27]artificial intelligence wiki
   ____________________
     *
     *
     * [28]ai vs. ml vs. dl
     *
     * [29]apache spark & deep learning
     *
     * [30]attention mechanisms & memory networks
     * [31]automated machine learning & ai
     * [32]ai & autonomous vehicles
     * [33]id26
     * [34]bag of words & tf-idf
     * [35]clojure ai
     * [36]comparison of ai frameworks
     * [37]convolutional neural network (id98)
     * [38]data for deep learning
     * [39]datasets and machine learning
     *
     * [40]decision tree
     * [41]deep autoencoders
     * [42]deep-belief networks
     * [43]deep id23
     * [44]deep learning resources
     * [45]deeplearning4j
     *
     * [46]denoising autoencoders
     * [47]machine learning devops
     * [48]differentiable programming
     * [49]eigenvectors, eigenvalues, pca, covariance and id178
     * [50]evolutionary & id107
     * [51]fraud and anomaly detection
     * [52]generative adversarial network (gan)
     * [53]glossary
     * [54]gluon
     * [55]graph analytics
     * [56]hopfield networks
     * [57]hyperparameter
     * [58]wiki home
     * [59]java ai
     * [60]jumpy
     * [61]id28
     * [62]lstms & id56s
     * [63]machine learning algorithms
     * [64]machine learning demos
     * [65]machine learning software
     * [66]machine learning operations (mlops)
     * [67]machine learning research groups & labs
     * [68]machine learning workflows
     * [69]machine learning
     * [70]id115
     * [71]multilayer id88
     *
     * [72]natural language processing (nlp)
     * [73]nd4j
     * [74]neural network tuning
     * [75]neural networks
     * [76]open datasets
     * [77]python ai
     * [78]questions when applying deep learning
     * [79]id80s
     * [80]id79
     * [81]recurrent network (id56)
     * [82]recursive neural tensor network
     * [83]restricted id82 (rbm)
     * [84]robotic process automation (rpa) & ai
     * [85]scala ai
     * [86]single-layer network
     * [87]skynet, or how to regulate ai
     * [88]spiking neural networks
     * [89]stacked denoising autoencoder (sda)
     * [90]strong ai vs. weak ai
     * [91]supervised learning
     * [92]symbolic reasoning
     * [93]text analysis
     * [94]thought vectors
     * [95]unsupervised learning
     * [96]deep learning use cases
     * [97]variational autoencoder (vae)
     * [98]id97, doc2vec and neural id27s

   [hr_white.png]

a beginner's guide to lstms and recurrent neural networks

     data can only be understood backwards; but it must be lived
     forwards.         s  ren kierkegaard, journals

   contents
     * [99]feedforward networks
     * [100]recurrent networks
     * [101]id26 through time
     * [102]vanishing and exploding gradients
     * [103]long short-term memory units (lstms)
     * [104]capturing diverse time scales
     * [105]code sample & comments
     * [106]resources

   actually, s  ren kierkegaard didn   t say that: instead of    data   , he used
   the word    life   . but for an algorithm, the two words are
   interchangeable, and it   s the algorithm   s understanding that we care
   about.

   the purpose of this post is to give students of neural networks an
   intuition about the functioning of recurrent neural networks and
   purpose and structure of a prominent id56 variation, lstms.

   recurrent nets are a type of id158 designed to
   recognize patterns in sequences of data, such as text, genomes,
   handwriting, the spoken word, or numerical times series data emanating
   from sensors, stock markets and government agencies. these algorithms
   take time and sequence into account, they have a temporal dimension.

   research shows them to be one of the most powerful and useful type of
   neural network, alongside the [107]attention mechanism and memory
   networks. id56s are applicable even to images, which can be decomposed
   into a series of patches and treated as a sequence.

   since recurrent networks possess a certain type of memory, and memory
   is also part of the human condition, we   ll make repeated analogies to
   memory in the brain.^[108]1

   [109]learn to build ai apps now   

review of feedforward networks

   to understand recurrent nets, first you have to understand the basics
   of [110]feedforward nets. both of these networks are named after the
   way they channel information through a series of mathematical
   operations performed at the nodes of the network. one feeds information
   straight through (never touching a given node twice), while the other
   cycles it through a loop, and the latter are called recurrent.

   in the case of feedforward networks, input examples are fed to the
   network and transformed into an output; with supervised learning, the
   output would be a label, a name applied to the input. that is, they map
   raw data to categories, recognizing patterns that may signal, for
   example, that an input image should be labeled    cat    or    elephant.   

   alt text

   a feedforward network is trained on labeled images until it minimizes
   the error it makes when guessing their categories. with the trained set
   of parameters (or weights, collectively known as a model), the network
   sallies forth to categorize data it has never seen. a trained
   feedforward network can be exposed to any random collection of
   photographs, and the first photograph it is exposed to will not
   necessarily alter how it classifies the second. seeing photograph of a
   cat will not lead the net to perceive an elephant next.

   that is, a feedforward network has no notion of order in time, and the
   only input it considers is the current example it has been exposed to.
   feedforward networks are amnesiacs regarding their recent past; they
   remember nostalgically only the formative moments of training.

recurrent networks

   recurrent networks, on the other hand, take as their input not just the
   current input example they see, but also what they have perceived
   previously in time. here   s a diagram of an early, [111]simple recurrent
   net proposed by elman, where the btsxpe at the bottom of the drawing
   represents the input example in the current moment, and context unit
   represents the output of the previous moment.

   alt text

   the decision a recurrent net reached at time step t-1 affects the
   decision it will reach one moment later at time step t. so recurrent
   networks have two sources of input, the present and the recent past,
   which combine to determine how they respond to new data, much as we do
   in life.

   recurrent networks are distinguished from feedforward networks by that
   feedback loop connected to their past decisions, ingesting their own
   outputs moment after moment as input. it is often said that recurrent
   networks have memory.^[112]2 adding memory to neural networks has a
   purpose: there is information in the sequence itself, and recurrent
   nets use it to perform tasks that feedforward networks can   t.

   that sequential information is preserved in the recurrent network   s
   hidden state, which manages to span many time steps as it cascades
   forward to affect the processing of each new example. it is finding
   correlations between events separated by many moments, and these
   correlations are called    long-term dependencies   , because an event
   downstream in time depends upon, and is a function of, one or more
   events that came before. one way to think about id56s is this: they are
   a way to share weights over time.

   just as human memory circulates invisibly within a body, affecting our
   behavior without revealing its full shape, information circulates in
   the hidden states of recurrent nets. the english language is full of
   words that describe the feedback loops of memory. when we say a person
   is haunted by their deeds, for example, we are simply talking about the
   consequences that past outputs wreak on present time. the french call
   this    le pass   qui ne passe pas,    or    the past that does not pass
   away.   

   we   ll describe the process of carrying memory forward mathematically:

   alt text

   the hidden state at time step t is h_t. it is a function of the input
   at the same time step x_t, modified by a weight matrix w (like the one
   we used for feedforward nets) added to the hidden state of the previous
   time step h_t-1 multiplied by its own hidden-state-to-hidden-state
   matrix u, otherwise known as a transition matrix and similar to a
   markov chain. the weight matrices are filters that determine how much
   importance to accord to both the present input and the past hidden
   state. the error they generate will return via id26 and be
   used to adjust their weights until error can   t go any lower.

   the sum of the weight input and hidden state is squashed by the
   function        either a logistic sigmoid function or tanh, depending    
   which is a standard tool for condensing very large or very small values
   into a logistic space, as well as making [113]gradients workable for
   id26.

   because this feedback loop occurs at every time step in the series,
   each hidden state contains traces not only of the previous hidden
   state, but also of all those that preceded h_t-1 for as long as memory
   can persist.

   given a series of letters, a recurrent network will use the first
   character to help determine its perception of the second character,
   such that an initial q might lead it to infer that the next letter will
   be u, while an initial t might lead it to infer that the next letter
   will be h.

   since recurrent nets span time, they are probably best illustrated with
   animation (the first vertical line of nodes to appear can be thought of
   as a feedforward network, which becomes recurrent as it unfurls over
   time).

     [114]how recurrent neural networks work #deeplearning4j #dl4j

   in the [115]diagram above, each x is an input example, w is the weights
   that filter inputs, a is the activation of the hidden layer (a
   combination of weighted input and the previous hidden state), and b is
   the output of the hidden layer after it has been transformed, or
   squashed, using a rectified linear or sigmoid unit.

id26 through time (bptt)

   remember, the purpose of recurrent nets is to accurately classify
   sequential input. we rely on the id26 of error and gradient
   descent to do so.

   id26 in feedforward networks moves backward from the final
   error through the outputs, weights and inputs of each hidden layer,
   assigning those weights responsibility for a portion of the error by
   calculating their partial derivatives        e/   w, or the relationship
   between their rates of change. those derivatives are then used by our
   learning rule, id119, to adjust the weights up or down,
   whichever direction decreases error.

   recurrent networks rely on an extension of id26 called
   [116]id26 through time, or bptt. time, in this case, is
   simply expressed by a well-defined, ordered series of calculations
   linking one time step to the next, which is all id26 needs
   to work.

   neural networks, whether they are recurrent or not, are simply nested
   composite functions like f(g(h(x))). adding a time element only extends
   the series of functions for which we calculate derivatives with the
   chain rule.

truncated bptt

   [117]truncated bptt is an approximation of full bptt that is preferred
   for long sequences, since full bptt   s forward/backward cost per
   parameter update becomes very high over many time steps. the downside
   is that the gradient can only flow back so far due to that truncation,
   so the network can   t learn dependencies that are as long as in full
   bptt.

vanishing (and exploding) gradients

   like most neural networks, recurrent nets are old. by the early 1990s,
   the vanishing gradient problem emerged as a major obstacle to recurrent
   net performance.

   just as a straight line expresses a change in x alongside a change in
   y, the gradient expresses the change in all weights with regard to the
   change in error. if we can   t know the gradient, we can   t adjust the
   weights in a direction that will decrease error, and our network ceases
   to learn.

   recurrent nets seeking to establish connections between a final output
   and events many time steps before were hobbled, because it is very
   difficult to know how much importance to accord to remote inputs. (like
   great-great-*-grandparents, they multiply quickly in number and their
   legacy is often obscure.)

   this is partially because the information flowing through neural nets
   passes through many stages of multiplication.

   everyone who has studied compound interest knows that any quantity
   multiplied frequently by an amount slightly greater than one can become
   immeasurably large (indeed, that simple mathematical truth underpins
   network effects and inevitable social inequalities). but its inverse,
   multiplying by a quantity less than one, is also true. gamblers go
   bankrupt fast when they win just 97 cents on every dollar they put in
   the slots.

   because the layers and time steps of deep neural networks relate to
   each other through multiplication, derivatives are susceptible to
   vanishing or exploding.

   exploding gradients treat every weight as though it were the proverbial
   butterfly whose flapping wings cause a distant hurricane. those
   weights    gradients become saturated on the high end; i.e. they are
   presumed to be too powerful. but exploding gradients can be solved
   relatively easily, because they can be truncated or squashed. vanishing
   gradients can become too small for computers to work with or for
   networks to learn     a harder problem to solve.

   below you see the effects of applying a sigmoid function over and over
   again. the data is flattened until, for large stretches, it has no
   detectable slope. this is analogous to a gradient vanishing as it
   passes through many layers.

   alt text

long short-term memory units (lstms)

   in the mid-90s, a variation of recurrent net with so-called long
   short-term memory units, or lstms, was proposed by the german
   researchers sepp hochreiter and juergen schmidhuber as a solution to
   the vanishing gradient problem.

   lstms help preserve the error that can be backpropagated through time
   and layers. by maintaining a more constant error, they allow recurrent
   nets to continue to learn over many time steps (over 1000), thereby
   opening a channel to link causes and effects remotely. this is one of
   the central challenges to machine learning and ai, since algorithms are
   frequently confronted by environments where reward signals are sparse
   and delayed, such as life itself. (religious thinkers have tackled this
   same problem with ideas of karma or divine reward, theorizing invisible
   and distant consequences to our actions.)

   lstms contain information outside the normal flow of the recurrent
   network in a gated cell. information can be stored in, written to, or
   read from a cell, much like data in a computer   s memory. the cell makes
   decisions about what to store, and when to allow reads, writes and
   erasures, via gates that open and close. unlike the digital storage on
   computers, however, these gates are analog, implemented with
   element-wise multiplication by sigmoids, which are all in the range of
   0-1. analog has the advantage over digital of being differentiable, and
   therefore suitable for id26.

   those gates act on the signals they receive, and similar to the neural
   network   s nodes, they block or pass on information based on its
   strength and import, which they filter with their own sets of weights.
   those weights, like the weights that modulate input and hidden states,
   are adjusted via the recurrent networks learning process. that is, the
   cells learn when to allow data to enter, leave or be deleted through
   the iterative process of making guesses, backpropagating error, and
   adjusting weights via id119.

   the diagram below illustrates how data flows through a memory cell and
   is controlled by its gates.

   alt text

   there are a lot of moving parts here, so if you are new to lstms, don   t
   rush this diagram     contemplate it. after a few minutes, it will begin
   to reveal its secrets.

   starting from the bottom, the triple arrows show where information
   flows into the cell at multiple points. that combination of present
   input and past cell state is fed not only to the cell itself, but also
   to each of its three gates, which will decide how the input will be
   handled.

   the black dots are the gates themselves, which determine respectively
   whether to let new input in, erase the present cell state, and/or let
   that state impact the network   s output at the present time step. s_c is
   the current state of the memory cell, and g_y_in is the current input
   to it. remember that each gate can be open or shut, and they will
   recombine their open and shut states at each step. the cell can forget
   its state, or not; be written to, or not; and be read from, or not, at
   each time step, and those flows are represented here.

   the large bold letters give us the result of each operation.

   here   s another diagram for good measure, comparing a simple recurrent
   network (left) to an lstm cell (right). the blue lines can be ignored;
   the legend is helpful.

   alt text

   it   s important to note that lstms    memory cells give different roles to
   addition and multiplication in the transformation of input. the central
   plus sign in both diagrams is essentially the secret of lstms. stupidly
   simple as it may seem, this basic change helps them preserve a constant
   error when it must be backpropagated at depth. instead of determining
   the subsequent cell state by multiplying its current state with new
   input, they add the two, and that quite literally makes the difference.
   (the forget gate still relies on multiplication, of course.)

   different sets of weights filter the input for input, output and
   forgetting. the forget gate is represented as a linear identity
   function, because if the gate is open, the current state of the memory
   cell is simply multiplied by one, to propagate forward one more time
   step.

   furthermore, while we   re on the topic of simple hacks, [118]including a
   bias of 1 to the forget gate of every lstm cell is also shown to
   [119]improve performance. (sutskever, on the other hand, recommends a
   bias of 5.)

   you may wonder why lstms have a forget gate when their purpose is to
   link distant occurrences to a final output. well, sometimes it   s good
   to forget. if you   re analyzing a text corpus and come to the end of a
   document, for example, you may have no reason to believe that the next
   document has any relationship to it whatsoever, and therefore the
   memory cell should be set to zero before the net ingests the first
   element of the next document.

   in the diagram below, you can see the gates at work, with straight
   lines representing closed gates, and blank circles representing open
   ones. the lines and circles running horizontal down the hidden layer
   are the forget gates.

   alt text

   it should be noted that while feedforward networks map one input to one
   output, recurrent nets can map one to many, as above (one image to many
   words in a caption), many to many (translation), or many to one
   (classifying a voice).

capturing diverse time scales and remote dependencies

   you may also wonder what the precise value is of input gates that
   protect a memory cell from new data coming in, and output gates that
   prevent it from affecting certain outputs of the id56. you can think of
   lstms as allowing a neural network to operate on different scales of
   time at once.

   let   s take a human life, and imagine that we are receiving various
   streams of data about that life in a time series. geolocation at each
   time step is pretty important for the next time step, so that scale of
   time is always open to the latest information.

   perhaps this human is a diligent citizen who votes every couple years.
   on democratic time, we would want to pay special attention to what they
   do around elections, before they return to making a living, and away
   from larger issues. we would not want to let the constant noise of
   geolocation affect our political analysis.

   if this human is also a diligent daughter, then maybe we can construct
   a familial time that learns patterns in phone calls which take place
   regularly every sunday and spike annually around the holidays. little
   to do with political cycles or geolocation.

   other data is like that. music is polyrhythmic. text contains recurrent
   themes at varying intervals. stock markets and economies experience
   jitters within longer waves. they operate simultaneously on different
   time scales that lstms can capture.

id149 (grus)

   a gated recurrent unit (gru) is basically an lstm without an output
   gate, which therefore fully writes the contents from its memory cell to
   the larger net at each time step.

   alt text

code sample

   a [120]commented example of a lstm learning how to replicate
   shakespearian drama, and implemented with deeplearning4j, can be found
   here. the api is commented where it   s not self-explanatory. if you have
   questions, please join us on [121]gitter.

   here   s what the lstm configuration looks like:

lstm hyperparameter tuning

   here are a few ideas to keep in mind when manually optimizing
   hyperparameters for id56s:
     * watch out for overfitting, which happens when a neural network
       essentially    memorizes    the training data. overfitting means you
       get great performance on training data, but the network   s model is
       useless for out-of-sample prediction.
     * id173 helps: id173 methods include l1, l2, and
       dropout among others.
     * so have a separate test set on which the network doesn   t train.
     * the larger the network, the more powerful, but it   s also easier to
       overfit. don   t want to try to learn a million parameters from
       10,000 examples     parameters > examples = trouble.
     * more data is almost always better, because it helps fight
       overfitting.
     * train over multiple epochs (complete passes through the dataset).
     * evaluate test set performance at each epoch to know when to stop
       (early stopping).
     * the learning rate is the single most important hyperparameter. tune
       this using [122]deeplearning4j-ui; see [123]this graph
     * in general, stacking layers can help.
     * for lstms, use the softsign (not softmax) activation function over
       tanh (it   s faster and less prone to saturation (~0 gradients)).
     * updaters: rmsprop, adagrad or momentum (nesterovs) are usually good
       choices. adagrad also decays the learning rate, which can help
       sometimes.
     * finally, remember data id172, mse id168 + identity
       activation function for regression, [124]xavier weight
       initialization

resources

     * [125]draw: a recurrent neural network for image generation;
       (id12)
     * [126]gated feedback recurrent neural networks
     * [127]recurrent neural networks; juergen schmidhuber
     * [128]modeling sequences with id56s and lstms; geoff hinton
     * [129]the unreasonable effectiveness of recurrent neural networks;
       andrej karpathy
     * [130]understanding lstms; christopher olah
     * [131]id26 through time: what it does and how to do it;
       paul werbos
     * [132]empirical evaluation of gated recurrent neural networks on
       sequence modeling; cho et al
     * [133]training recurrent neural networks; ilya sutskever   s
       dissertation
     * [134]supervised sequence labelling with recurrent neural networks;
       alex graves
     * [135]long short-term memory in recurrent neural networks; felix
       gers
     * [136]lstm: a search space oddyssey; klaus greff et al

footnotes

   1) while recurrent networks may seem like a far cry from general
   artificial intelligence, it   s our belief that intelligence, in fact, is
   probably dumber than we thought. that is, with a simple feedback loop
   to serve as memory, we have one of the basic ingredients of
   consciousness     a necessary but insufficient component. others, not
   discussed above, might include additional variables that represent the
   network and its state, and a framework for decisionmaking logic based
   on interpretations of data. the latter, ideally, would be part of a
   larger problem-solving loop that rewards success and punishes failure,
   much like id23. come to think of it, [137]deepmind
   already built that   

   2) all neural networks whose parameters have been optimized have memory
   in a sense, because those parameters are the traces of past data. but
   in feedforward networks, that memory may be frozen in time. that is,
   after a network is trained, the model it learns may be applied to more
   data without further adapting itself. in addition, it is monolithic in
   the sense that the same memory (or set of weights) is applied to all
   incoming data. recurrent networks, which also go by the name of dynamic
   (translation:    changing   ) neural networks, are distinguished from
   feedforward nets not so much by having memory as by giving particular
   weight to events that occur in a series. while those events do not need
   to follow each other immediately, they are presumed to be linked,
   however remotely, by the same temporal thread. feedforward nets do not
   make such a presumption. they treat the world as a bucket of objects
   without order or time. it may be helpful to map two types of neural
   network to two types of human knowledge. when we are children, we learn
   to recognize colors, and we go through the rest of our lives
   recognizing colors wherever we see them, in highly varied contexts and
   independent of time. we only had to learn the colors once. that
   knowledge is like memory in feedforward nets; they rely on a past
   without scope, undefined. ask them what colors they were fed five
   minutes ago and they don   t know or care. they are short-term amnesiacs.
   on the other hand, we also learn as children to decipher the flow of
   sound called language, and the meanings we extract from sounds such as
      toe    or    roe    or    z    are always highly dependent on the sounds
   preceding (and following) them. each step of the sequence builds on
   what went before, and meaning emerges from their order. indeed, whole
   sentences conspire to convey the meaning of each syllable within them,
   their redundant signals acting as a protection against ambient noise.
   that is similar to the memory of recurrent nets, which look to a
   particular slice of the past for help. both types of nets bring the
   past, or different pasts, to bear in different ways.

   [hr_gradient.png]

interactive demo

   learn to build ai applications using our interactive learning portal.
   [138]try it now

   [logo_footer.svg]

company

     * [139]about
     * [140]press kit
     * [141]contact us
     * [142]press
     * [143]privacy

platform

     * [144]skil
     * [145]subscriptions
     * [146]documentation
     * [147]community support

international

     * [148]english
     * [149]japanese

follow us

     * [150]facebook
     * [151]twitter
     * [152]linkedin
     * [153]gitter

subscribe to our mailing list

   iframe: [154]https://go.pardot.com/l/456082/2017-11-01/d9xsrj

references

   visible links
   1. https://www.googletagmanager.com/ns.html?id=gtm-p824sk
   2. https://skymind.ai/contact
   3. https://skymind.ai/
   4. https://skymind.ai/platform
   5. https://skymind.ai/learn
   6. https://skymind.ai/subscription
   7. https://docs.skymind.ai/docs
   8. https://github.com/skymindio/skil-ce/issues
   9. https://skymind.ai/audience/datascientist
  10. https://skymind.ai/audience/architect
  11. https://skymind.ai/audience/devops
  12. https://skymind.ai/audience/executives
  13. https://skymind.ai/solutions
  14. https://solutions.skymind.ai/l/456082/2019-03-08/lxfw7m
  15. https://skymind.ai/case-studies/orange
  16. https://skymind.ai/case-studies/insurance
  17. https://skymind.ai/case-studies/logistics
  18. https://skymind.ai/case-studies/canonical
  19. https://skymind.ai/case-studies/finance
  20. https://skymind.ai/case-studies/image
  21. https://skymind.ai/case-studies/commerce
  22. https://skymind.ai/about
  23. https://blog.skymind.ai/
  24. https://skymind.ai/wiki
  25. https://skymind.ai/open-source
  26. https://go.pardot.com/l/456082/2018-09-28/jsv68v
  27. https://skymind.ai/wiki/
  28. https://skymind.ai/wiki/ai-vs-machine-learning-vs-deep-learning
  29. https://skymind.ai/wiki/apache-spark-deep-learning
  30. https://skymind.ai/wiki/attention-mechanism-memory-network
  31. https://skymind.ai/wiki/automl-automated-machine-learning-ai
  32. https://skymind.ai/wiki/autonomous-vehicle
  33. https://skymind.ai/wiki/id26
  34. https://skymind.ai/wiki/bagofwords-tf-idf
  35. https://skymind.ai/wiki/clojure-ai
  36. https://skymind.ai/wiki/comparison-frameworks-dl4j-tensorflow-pytorch
  37. https://skymind.ai/wiki/convolutional-network
  38. https://skymind.ai/wiki/data-for-deep-learning
  39. https://skymind.ai/wiki/datasets-ml
  40. https://skymind.ai/wiki/decision-tree
  41. https://skymind.ai/wiki/deep-autoencoder
  42. https://skymind.ai/wiki/deep-belief-network
  43. https://skymind.ai/wiki/deep-reinforcement-learning
  44. https://skymind.ai/wiki/deeplearning-research-papers
  45. https://skymind.ai/wiki/deeplearning4j
  46. https://skymind.ai/wiki/denoising-autoencoder
  47. https://skymind.ai/wiki/devops-machine-learning
  48. https://skymind.ai/wiki/differentiableprogramming
  49. https://skymind.ai/wiki/eigenvector
  50. https://skymind.ai/wiki/evolutionary-genetic-algorithm
  51. https://skymind.ai/wiki/fraud-detection
  52. https://skymind.ai/wiki/generative-adversarial-network-gan
  53. https://skymind.ai/wiki/glossary
  54. https://skymind.ai/wiki/gluon
  55. https://skymind.ai/wiki/graph-analysis
  56. https://skymind.ai/wiki/hopfieldnetworks
  57. https://skymind.ai/wiki/hyperparameter
  58. https://skymind.ai/wiki/index
  59. https://skymind.ai/wiki/java-ai
  60. https://skymind.ai/wiki/jumpy
  61. https://skymind.ai/wiki/logistic-regression
  62. https://skymind.ai/wiki/lstm
  63. https://skymind.ai/wiki/machine-learning-algorithms
  64. https://skymind.ai/wiki/machine-learning-demos
  65. https://skymind.ai/wiki/machine-learning-library-software
  66. https://skymind.ai/wiki/machine-learning-operations-mlops
  67. https://skymind.ai/wiki/machine-learning-research-groups-labs
  68. https://skymind.ai/wiki/machine-learning-workflow
  69. https://skymind.ai/wiki/machine-learning
  70. https://skymind.ai/wiki/markov-chain-monte-carlo
  71. https://skymind.ai/wiki/multilayer-id88
  72. https://skymind.ai/wiki/natural-language-processing-nlp
  73. https://skymind.ai/wiki/nd4j
  74. https://skymind.ai/wiki/neural-network-tuning
  75. https://skymind.ai/wiki/neural-network
  76. https://skymind.ai/wiki/open-datasets
  77. https://skymind.ai/wiki/python-ai
  78. https://skymind.ai/wiki/questions-when-applying-deep-learning
  79. https://skymind.ai/wiki/radial-basis-function-network-rbf
  80. https://skymind.ai/wiki/random-forest
  81. https://skymind.ai/wiki/recurrent-network-id56
  82. https://skymind.ai/wiki/recursive-neural-tensor-network
  83. https://skymind.ai/wiki/restricted-boltzmann-machine
  84. https://skymind.ai/wiki/robotic-process-automation-rpa
  85. https://skymind.ai/wiki/scala-ai
  86. https://skymind.ai/wiki/single-layer-network
  87. https://skymind.ai/wiki/skynet
  88. https://skymind.ai/wiki/spiking-neural-network-snn
  89. https://skymind.ai/wiki/stacked-denoising-autoencoder
  90. https://skymind.ai/wiki/strong-ai-general-ai
  91. https://skymind.ai/wiki/supervised-learning
  92. https://skymind.ai/wiki/symbolic-reasoning
  93. https://skymind.ai/wiki/text-analysis
  94. https://skymind.ai/wiki/thought-vectors
  95. https://skymind.ai/wiki/unsupervised-learning
  96. https://skymind.ai/wiki/use-cases
  97. https://skymind.ai/wiki/variational-autoencoder
  98. https://skymind.ai/wiki/id97
  99. https://skymind.ai/wiki/lstm#feedforward
 100. https://skymind.ai/wiki/lstm#recurrent
 101. https://skymind.ai/wiki/lstm#id26
 102. https://skymind.ai/wiki/lstm#vanishing
 103. https://skymind.ai/wiki/lstm#long
 104. https://skymind.ai/wiki/lstm#capturing
 105. https://skymind.ai/wiki/lstm#code
 106. https://skymind.ai/wiki/lstm#resources
 107. https://skymind.ai/wiki/attention-mechanism-memory-network
 108. https://skymind.ai/wiki/lstm#one
 109. https://skymind.ai/learn
 110. https://skymind.ai/wiki/restricted-boltzmann-machine
 111. https://web.stanford.edu/group/pdplab/pdphandbook/handbookch8.html
 112. https://skymind.ai/wiki/lstm#two
 113. https://skymind.ai/wiki/glossary#gradient
 114. https://imgur.com/kpzbdfv
 115. http://imgur.com/kpzbdfv
 116. https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/fall.2015/pdfs/werbos.backprop.pdf
 117. http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf
 118. http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf
 119. http://www.felixgers.de/papers/phd.pdf
 120. https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/character/lstmcharmodellingexample.java
 121. https://gitter.im/deeplearning4j/deeplearning4j
 122. http://deeplearning4j.org/visualization
 123. http://cs231n.github.io/neural-networks-3/#baby
 124. https://skymind.ai/wiki/glossary#xavier
 125. http://arxiv.org/pdf/1502.04623v2.pdf
 126. http://arxiv.org/pdf/1502.02367v4.pdf
 127. http://people.idsia.ch/~juergen/id56.html
 128. https://class.coursera.org/neuralnets-2012-001/lecture/77
 129. https://karpathy.github.io/2015/05/21/id56-effectiveness/
 130. https://colah.github.io/posts/2015-08-understanding-lstms/
 131. https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/fall.2015/pdfs/werbos.backprop.pdf
 132. http://arxiv.org/pdf/1412.3555v1.pdf
 133. https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf
 134. http://www.cs.toronto.edu/~graves/phd.pdf
 135. http://www.felixgers.de/papers/phd.pdf
 136. http://arxiv.org/pdf/1503.04069.pdf
 137. https://www.cs.toronto.edu/~vmnih/docs/id25.pdf
 138. https://skymind.ai/learn
 139. https://skymind.ai/about
 140. https://drive.google.com/drive/folders/1s9n-mdi17euhjmdkp-x0t05e1lgy1ly3
 141. https://skymind.ai/contact
 142. https://skymind.ai/press
 143. https://skymind.ai/privacy
 144. https://skymind.ai/platform
 145. https://skymind.ai/subscription
 146. https://docs.skymind.ai/docs
 147. https://github.com/skymindio/skil-ce/issues
 148. https://skymind.ai/
 149. https://skymind.ai/jp
 150. https://www.facebook.com/deeplearning4j/
 151. https://twitter.com/deeplearning4j
 152. https://www.linkedin.com/company/skymind-io
 153. https://gitter.im/deeplearning4j/deeplearning4j
 154. https://go.pardot.com/l/456082/2017-11-01/d9xsrj

   hidden links:
 156. https://skymind.ai/wiki/accuracy-precision-recall-f1
 157. https://skymind.ai/wiki/ai-infrastructure-machine-learning-operations-mlops
 158. https://skymind.ai/wiki/ai-winter
 159. https://skymind.ai/wiki/arbiter
 160. https://skymind.ai/wiki/datavec
 161. https://skymind.ai/wiki/define-artificial-intelligence-ai
 162. https://skymind.ai/wiki/naive-bayes-theorem
