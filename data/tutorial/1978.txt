   #[1]wildml    feed [2]wildml    comments feed [3]wildml    recurrent
   neural network tutorial, part 4     implementing a gru/lstm id56 with
   python and theano comments feed [4]recurrent neural networks tutorial,
   part 3     id26 through time and vanishing gradients
   [5]understanding convolutional neural networks for nlp [6]alternate
   [7]alternate

   [8]skip to content

   [9]wildml

   artificial intelligence, deep learning, and nlp

   (button) menu
     * [10]home
     * [11]ai newsletter
     * [12]deep learning glossary
     * [13]contact
     * [14]about

   posted on [15]october 27, 2015january 10, 2016 by [16]denny britz

recurrent neural network tutorial, part 4     implementing a gru/lstm id56 with
python and theano

   [17]the code for this post is on github. this is part 4, the last part
   of the recurrent neural network tutorial. the previous parts are:
     * [18]recurrent neural networks tutorial, part 1     introduction to
       id56s
     * [19]recurrent neural networks tutorial, part 2     implementing a id56
       with python, numpy and theano
     * [20]recurrent neural networks tutorial, part 3     id26
       through time and vanishing gradients

   in this post we   ll learn about lstm (long short term memory) networks
   and grus (id149).  lstms were [21]first proposed in
   1997 by sepp hochreiter and j  rgen schmidhuber, and are among the most
   widely used models in deep learning for nlp today. grus, [22]first used
   in  2014, are a simpler variant of lstms that share many of the
   same properties.  let   s start by looking at lstms, and then we   ll see
   how grus are different.

id137

   in [23]part 3 we looked at how the vanishing gradient problem prevents
   standard id56s from learning long-term dependencies. lstms were designed
   to combat vanishing gradients through a gating mechanism.  to
   understand what this means, let   s look at how a lstm calculates
   a hidden state s_t (i   m using \circ to mean elementwise
   multiplication):

   \begin{aligned} i &=\sigma(x_tu^i + s_{t-1} w^i) \\ f &=\sigma(x_t u^f
   +s_{t-1} w^f) \\ o &=\sigma(x_t u^o + s_{t-1} w^o) \\ g &=\ tanh(x_t
   u^g + s_{t-1}w^g) \\ c_t &= c_{t-1} \circ f + g \circ i \\ s_t
   &=\tanh(c_t) \circ o \end{aligned}

   these equations look quite complicated, but actually it   s not that
   hard. first, notice that a lstm layer is  just another way to compute
   a hidden state. previously, we computed the hidden state as  s_t =
   \tanh(ux_t + ws_{t-1}) . the inputs to this unit were  x_t , the
   current input at step t , and s_{t-1} , the previous hidden state.  the
   output was a new hidden state s_t . a lstm unit does the exact same
   thing, just in a different way! this is key to understanding the big
   picture. you can essentially treat lstm (and gru) units as a black
   boxes. given the current input and previous hidden state, they compute
   the next hidden state in some way.

   [24]a gru/lstm unit is just another way to calculate the next hidden
   state.

   with that in mind let   s try to get an intuition for how a lstm unit
   computes the hidden state. chris olah has an [25]excellent post that
   goes into details on this and to avoid duplicating his effort i will
   only give a brief explanation here. i urge you to read his post to
   for deeper insight and nice visualizations. but, to summarize:
     * i, f, o are called the input, forget and output gates,
       respectively. note that they have the exact same equations, just
       with different parameter matrices. they care called gates because
       the sigmoid function squashes the values of these vectors between 0
       and 1, and by multiplying them elementwise with another vector you
       define how much of that other vector you want to    let through   . the
       input gate defines how much of the newly computed state for the
       current input you want to let through. the forget gate defines how
       much of the previous state you want to let through. finally, the
       output gate defines how much of the internal state you want to
       expose to the external network (higher layers and the next time
       step). all the gates have the same dimensions d_s , the size of
       your hidden state.
     * g is a    candidate    hidden state that is computed based on the
       current input and the previous hidden state. it is exactly the same
       equation we had in our vanilla id56, we just renamed the parameters
       u and w to u^g and w^g . however, instead of taking g as the new
       hidden state as we did in the id56, we will use the input gate from
       above to pick some of it.
     * c_t is the internal memory of the unit. it is a combination of the
       previous memory c_{t-1} multiplied by the forget gate, and the
       newly computed hidden state g , multiplied by the input gate. thus,
       intuitively it is a combination of how we want to combine previous
       memory and the new input. we could choose to ignore the old memory
       completely (forget gate all 0   s) or ignore the newly computed state
       completely (input gate all 0   s), but most likely we want something
       in between these two extremes.
     * given the memory  c_t , we finally compute the output hidden state
       s_t by multiplying the memory with the output gate. not all of the
       internal memory may be relevant to the hidden state used by other
       units in the network.

   [26]lstm gating diagram lstm gating. chung, junyoung, et al.    empirical
   evaluation of gated recurrent neural networks on sequence modeling.   
   (2014)

   intuitively, plain id56s could be considered a special case of lstms. if
   you fix the input gate all 1   s, the forget gate to all 0   s (you always
   forget the previous memory) and the output gate to all one   s (you
   expose the whole memory) you almost get standard id56. there   s just an
   additional  \tanh that squashes the output a bit. the gating mechanism
   is what allows lstms to explicitly model long-term dependencies. by
   learning the parameters for its gates, the network learns how
   its memory should behave.

   notably, there exist several variations on the basic lstm architecture.
   a common one is creating peephole connections that allow the gates to
   not only depend on the previous hidden state s_{t-1} , but also on the
   previous internal state c_{t-1} , adding an additional term in the gate
   equations. there are many more variations. [27]lstm: a search space
   odyssey empirically evaluates different lstm architectures.

grus

   the idea behind a gru layer is quite similar to that of a lstm layer,
   as are the equations.

   \begin{aligned} z &=\sigma(x_tu^z + s_{t-1} w^z) \\ r &=\sigma(x_t u^r
   +s_{t-1} w^r) \\ h &= tanh(x_t u^h + (s_{t-1} \circ r) w^h) \\ s_t &=
   (1 - z) \circ h + z \circ s_{t-1} \end{aligned}

   a gru has two gates, a reset gate r , and an update gate z .
   intuitively, the reset gate determines how to combine the new input
   with the previous memory, and the update gate defines how much of the
   previous memory to keep around. if we set the reset to all 1   s and
   update gate to all 0   s we again arrive at our plain id56 model. the
   basic idea of using a gating mechanism to learn long-term dependencies
   is the same as in a lstm, but there are a few key differences:
     * a gru has two gates, an lstm has three gates.
     * grus don   t possess and internal memory ( c_t ) that is different
       from the exposed hidden state. they don   t have the output gate that
       is present in lstms.
     * the input and forget gates are coupled by an update gate  z and the
       reset gate r is applied directly to the previous hidden state.
       thus, the responsibility of the reset gate in a lstm is really
       split up into both r and z .
     * we don   t apply a second nonlinearity when computing the output.

   [28]gru gating diagram gru gating. chung, junyoung, et al.    empirical
   evaluation of gated recurrent neural networks on sequence modeling.   
   (2014)

gru vs lstm

   now that you   ve seen two models  to combat the vanishing gradient
   problem you may be wondering: which one to use? grus are quite new
   (2014), and their tradeoffs haven   t been fully explored yet.  according
   to empirical evaluations in [29]empirical evaluation of gated recurrent
   neural networks on sequence modeling  and [30]an empirical exploration
   of recurrent network architectures, there isn   t a clear winner. in many
   tasks both architectures yield comparable performance and tuning
   hyperparameters like layer size is probably more important than picking
   the ideal architecture. grus have fewer parameters (u and w are
   smaller) and thus may train a bit faster or need less data to
   generalize. on the other hand, if you have enough data, the greater
   expressive power of lstms may lead to better results.

implementation

   let   s return  to the implementation of the language model from[31] part
   2 and let   s use gru units in our id56. there is no principled reason why
   i   ve chosen grus instead lstms in this part (other that i also wanted
   to become more familiar with grus). their implementations are almost
   identical so you should be able to  modify the code to go from gru to
   lstm quite easily by changing the equations.

   we base the code on our previous theano implementation. remember that a
   gru (lstm) layer is just another way of computing the hidden state. so
   all we really need to do is change the hidden state computation in our
   forward propagation function.
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   def forward_prop_step(x_t, s_t1_prev):
         # this is how we calculated the hidden state in a simple id56. no
   longer!
         # s_t = t.tanh(u[:,x_t] + w.dot(s_t1_prev))

         # get the word vector
         x_e = e[:,x_t]

         # gru layer
         z_t1 = t.nnet.hard_sigmoid(u[0].dot(x_e) + w[0].dot(s_t1_prev) +
   b[0])
         r_t1 = t.nnet.hard_sigmoid(u[1].dot(x_e) + w[1].dot(s_t1_prev) +
   b[1])
         c_t1 = t.tanh(u[2].dot(x_e) + w[2].dot(s_t1_prev * r_t1) + b[2])
         s_t1 = (t.ones_like(z_t1) - z_t1) * c_t1 + z_t1 * s_t1_prev

         # final output calculation
         # theano's softmax returns a matrix with one row, we only need
   the row
         o_t = t.nnet.softmax(v.dot(s_t1) + c)[0]

         return [o_t, s_t1]

   in our implementation we also added bias units b, c . it   s quite
   typical that these are not shown in the equations. of course we also
   need to change the initialization of our parameters u and   w because
   they now have a different sizes. i don   t show the initialization code
   here, but [32]it is on gitub. i also added a id27 layer e ,
   but more on that below.

   that was pretty simple. but what about the gradients? we could derive
   the gradients for e, w, u, b and c by hand using the chain rule, just
   like we did before. but in practice most people use libraries like
   theano that support auto-differenation of expressions. if you are for
   somehow forced to calculate the gradients yourself, you probably want
   to modularize different units and have your own version of
   auto-differentiation using the chain rule. we let theano calculate the
   gradients for us:
   1
   2
   3
   4
   5
   6
   7
   # gradients using theano
   de = t.grad(cost, e)
   du = t.grad(cost, u)
   dw = t.grad(cost, w)
   db = t.grad(cost, b)
   dv = t.grad(cost, v)
   dc = t.grad(cost, c)

   that   s pretty much it. to get better results we also use a few
   additional tricks in our implementation.

using rmsprop for parameter updates

   in [33]part 2 we used the most basic version of stochastic gradient
   descent (sgd) to update our parameters. it turns out this isn   t such a
   great idea. if you set your learning rate low enough, sgd is guaranteed
   to make progress towards a good solution, but in practice that would
   take a very long time. there exist a number of commonly used variations
   on sgd, including the [34](nesterov) momentum method,
   [35]adagrad, [36]adadelta and [37]rmsprop.  [38]this post contains a
   good overview of many of these methods. i   m also planning to explore
   the implementation of each of these methods in detail in a future post.
   for this part of the tutorial i chose to go with rmsprop. the basic
   idea behind rmsprop is to adjust the learning rate per-parameter
   according to the a (smoothed) sum of the previous gradients.
   intuitively this means that frequently occurring features get a smaller
   learning rate (because the sum of their gradients is larger), and rare
   features get a larger learning rate.

   the implementation of rmsprop is quite simple. for each parameter we
   keep a cache variable and during id119 we update the
   parameter and the cache as follows (example for w ):
   1
   2
   cachew = decay * cachew + (1 - decay) * dw ** 2
   w = w - learning_rate * dw / np.sqrt(cachew + 1e-6)

   the decay is typically set to 0.9 or 0.95 and the 1e-6 term is added to
   avoid division by 0.

adding an embedding layer

   using id27s such as [39]id97 and [40]glove is a popular
   method to improve the accuracy of your model. instead of using one-hot
   vectors to represent our words, the low-dimensional vectors learned
   using id97 or glove carry semantic meaning     similar words
   have similar vectors. using these vectors is a form of
   pre-training. intuitively, you are telling the network which words are
   similar so that it needs to learn less about the language. using
   pre-trained vectors is particularly useful if you don   t have a lot of
   data because it allows the network to generalize to unseen words. i
   didn   t use pre-trained word vectors in my experiments, but adding an
   embedding layer (the matrix e in our code) makes it easy to plug them
   in. the embedding matrix is really just a lookup table     the ith column
   vector corresponds to the ith word in our vocabulary. by updating the
   matrix e we are learning word vectors ourselves, but they are very
   specific to our task (and data set) and not as general as those that
   you can download, which are trained on millions or billions of
   documents.

adding a second gru layer

   adding a second layer to our network allows our model to capture
   higher-level interactions. you could add additional layers, but i
   didn   t try that for this experiment. you   ll likely see diminishing
   returns after 2-3 layers and unless you have a huge amount of data
   (which we don   t) more layers are unlikely to make a big difference and
   may lead to overfitting.

   [41]2 layer gru/lstm unit

   adding a second layer to our network is straightforward, we (again)
   only need to modify the forward propagation calculation and
   initialization function.
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   # gru layer 1
   z_t1 = t.nnet.hard_sigmoid(u[0].dot(x_e) + w[0].dot(s_t1_prev) + b[0])
   r_t1 = t.nnet.hard_sigmoid(u[1].dot(x_e) + w[1].dot(s_t1_prev) + b[1])
   c_t1 = t.tanh(u[2].dot(x_e) + w[2].dot(s_t1_prev * r_t1) + b[2])
   s_t1 = (t.ones_like(z_t1) - z_t1) * c_t1 + z_t1 * s_t1_prev

   # gru layer 2
   z_t2 = t.nnet.hard_sigmoid(u[3].dot(s_t1) + w[3].dot(s_t2_prev) + b[3])
   r_t2 = t.nnet.hard_sigmoid(u[4].dot(s_t1) + w[4].dot(s_t2_prev) + b[4])
   c_t2 = t.tanh(u[5].dot(s_t1) + w[5].dot(s_t2_prev * r_t2) + b[5])
   s_t2 = (t.ones_like(z_t2) - z_t2) * c_t2 + z_t2 * s_t2_prev

   [42]the full code for the gru network is available here.

a note on performance

   i   ve gotten questions about this in the past, so i want to clarify that
   the code i showed here isn   t very efficient. it   s optimized for clarity
   and was primarily written for educational purposes. it   s probably good
   enough to play around with the model, but you should not use it in
   production or expect to train on a large dataset with it. there are
   many tricks to [43]optimize id56 performance, but the perhaps most
   important one would be to batch together your updates. instead of
   learning from one sentence at a time, you want to group sentences of
   the same length (or even pad all sentences to have the same length) and
   then perform large id127s and sum up gradients for the
   whole batch. that   s because such large id127s
   are efficiently handled by a gpu. by not doing this we can get little
   speed-up from using a gpu and training can be extremely slow.

   so, if you want to train a large model i highly recommended using one
   of the [44]existing deep learning libraries that are optimized for
   performance. a model that would take days/weeks to train with the above
   code will only take a few hours with these libraries. i personally like
   [45]keras, which is quite simple to use and comes with good examples
   for id56s.

results

   to spare you the pain of training a model over many days i trained a
   model very similar to that in [46]part 2. i used a vocabulary size of
   8000, mapped words into 48-dimensional vectors, and used two
   128-dimensional gru layers. the [47]ipython notebook contains code to
   load the model so you can play with it, modify it, and use it to
   generate text.

   here are a few good examples of the network output (capitalization
   added by me).
     * i am a bot , and this action was performed automatically .
     * i enforce myself ridiculously well enough to just youtube.
     * i   ve got a good rhythm going !
     * there is no problem here, but at least still wave !
     * it depends on how plausible my judgement is .
     * ( with the constitution which makes it impossible )

   it is interesting to look at the semantic dependencies of these
   sentences over multiple time steps. for example, bot and automatically
   are clearly related, as are the opening and closing brackets. our
   network was able to learn that, pretty cool!

   that   s it for now. i hope you had fun and please leave
   questions/feedback in the comments!


   categories[48]deep learning, [49]id38, [50]neural
   networks, [51]recurrent neural networks, [52]id56s

post navigation

   [53]previous postprevious recurrent neural networks tutorial, part 3    
   id26 through time and vanishing gradients
   [54]next postnext understanding convolutional neural networks for nlp

subscribe to blog via email

   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

recent posts

     * [55]introduction to learning to trade with id23
     * [56]ai and deep learning in 2017     a year in review
     * [57]hype or not? some perspective on openai   s dota 2 bot
     * [58]learning id23 (with code, exercises and
       solutions)
     * [59]id56s in tensorflow, a practical guide and undocumented features
     * [60]deep learning for chatbots, part 2     implementing a
       retrieval-based model in tensorflow
     * [61]deep learning for chatbots, part 1     introduction
     * [62]attention and memory in deep learning and nlp

archives

     * [63]february 2018
     * [64]december 2017
     * [65]august 2017
     * [66]october 2016
     * [67]august 2016
     * [68]july 2016
     * [69]april 2016
     * [70]january 2016
     * [71]december 2015
     * [72]november 2015
     * [73]october 2015
     * [74]september 2015

categories

     * [75]conversational agents
     * [76]convolutional neural networks
     * [77]deep learning
     * [78]gpu
     * [79]id38
     * [80]memory
     * [81]neural networks
     * [82]news
     * [83]nlp
     * [84]recurrent neural networks
     * [85]id23
     * [86]id56s
     * [87]tensorflow
     * [88]trading
     * [89]uncategorized

meta

     * [90]log in
     * [91]entries rss
     * [92]comments rss
     * [93]wordpress.org

   [94]proudly powered by wordpress

references

   1. http://www.wildml.com/feed/
   2. http://www.wildml.com/comments/feed/
   3. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/feed/
   4. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
   5. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
   6. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/
   7. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/&format=xml
   8. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/#content
   9. http://www.wildml.com/
  10. http://www.wildml.com/
  11. https://www.getrevue.co/profile/wildml
  12. http://www.wildml.com/deep-learning-glossary/
  13. mailto:dennybritz@gmail.com
  14. http://www.wildml.com/about/
  15. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/
  16. http://www.wildml.com/author/dennybritz/
  17. https://github.com/dennybritz/id56-tutorial-gru-lstm
  18. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  19. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
  20. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
  21. http://deeplearning.cs.cmu.edu/pdfs/hochreiter97_lstm.pdf
  22. http://arxiv.org/pdf/1406.1078v3.pdf
  23. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
  24. http://www.wildml.com/wp-content/uploads/2015/10/gru-lstm.png
  25. http://colah.github.io/posts/2015-08-understanding-lstms/
  26. http://www.wildml.com/wp-content/uploads/2015/10/screen-shot-2015-10-23-at-10.00.55-am.png
  27. http://arxiv.org/pdf/1503.04069.pdf
  28. http://www.wildml.com/wp-content/uploads/2015/10/screen-shot-2015-10-23-at-10.36.51-am.png
  29. http://arxiv.org/abs/1412.3555
  30. http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf
  31. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
  32. https://github.com/dennybritz/id56-tutorial-gru-lstm
  33. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
  34. http://www.cs.toronto.edu/~fritz/absps/momentum.pdf
  35. http://www.magicbroom.info/papers/duchihasi10.pdf
  36. http://arxiv.org/abs/1212.5701
  37. http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
  38. http://cs231n.github.io/neural-networks-3/#update
  39. https://code.google.com/p/id97/
  40. http://nlp.stanford.edu/projects/glove/
  41. http://www.wildml.com/wp-content/uploads/2015/10/gru-lstm-2-layer.png
  42. https://github.com/dennybritz/id56-tutorial-gru-lstm/blob/master/gru_theano.py
  43. http://svail.github.io/
  44. http://www.teglor.com/b/deep-learning-libraries-language-cm569/
  45. http://keras.io/
  46. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
  47. https://github.com/dennybritz/id56-tutorial-gru-lstm
  48. http://www.wildml.com/category/deep-learning/
  49. http://www.wildml.com/category/language-modeling/
  50. http://www.wildml.com/category/neural-networks/
  51. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  52. http://www.wildml.com/category/id56s/
  53. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
  54. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
  55. http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/
  56. http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/
  57. http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/
  58. http://www.wildml.com/2016/10/learning-reinforcement-learning/
  59. http://www.wildml.com/2016/08/id56s-in-tensorflow-a-practical-guide-and-undocumented-features/
  60. http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/
  61. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  62. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  63. http://www.wildml.com/2018/02/
  64. http://www.wildml.com/2017/12/
  65. http://www.wildml.com/2017/08/
  66. http://www.wildml.com/2016/10/
  67. http://www.wildml.com/2016/08/
  68. http://www.wildml.com/2016/07/
  69. http://www.wildml.com/2016/04/
  70. http://www.wildml.com/2016/01/
  71. http://www.wildml.com/2015/12/
  72. http://www.wildml.com/2015/11/
  73. http://www.wildml.com/2015/10/
  74. http://www.wildml.com/2015/09/
  75. http://www.wildml.com/category/conversational-agents/
  76. http://www.wildml.com/category/neural-networks/convolutional-neural-networks/
  77. http://www.wildml.com/category/deep-learning/
  78. http://www.wildml.com/category/gpu/
  79. http://www.wildml.com/category/language-modeling/
  80. http://www.wildml.com/category/memory/
  81. http://www.wildml.com/category/neural-networks/
  82. http://www.wildml.com/category/news/
  83. http://www.wildml.com/category/nlp/
  84. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  85. http://www.wildml.com/category/reinforcement-learning/
  86. http://www.wildml.com/category/id56s/
  87. http://www.wildml.com/category/tensorflow/
  88. http://www.wildml.com/category/trading/
  89. http://www.wildml.com/category/uncategorized/
  90. http://www.wildml.com/wp-login.php
  91. http://www.wildml.com/feed/
  92. http://www.wildml.com/comments/feed/
  93. https://wordpress.org/
  94. https://wordpress.org/
