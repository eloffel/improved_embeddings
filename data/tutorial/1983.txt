   #[1]triangleinequality    feed [2]triangleinequality    comments feed
   [3]triangleinequality    id75: the math(s) comments feed
   [4]id79 [5]id75: the code [6]alternate
   [7]alternate [8]triangleinequality [9]wordpress.com

     * [10]home
     * [11]about
     * [12]contents

[13]triangleinequality

   for matters mathematical
   search ____________________ (button) search
     __________________________________________________________________

   [14]home    [15]maths    [16]id202    id75:
   the math(s)

id75: the math(s)

   let   s suppose we have vectors x^1, \dots , x^k \in \mathbb{r}^n where
   each coordinate represents a variable, perhaps x_1 is height, x_2 is
   weight and x_3 is age, and so on. associated to each x^i we have y^i
   \in \mathbb{r} , this we think of as the response, or what we are
   trying to predict given x^i . the task is to find a model or algorithm
   that will spit out a good approximation to y^i given x^i as an input,
   in particular one that will work for new unseen pieces of data for
   which we do not know the response.

   the id75 approach to this is as follows. let x \in
   \mathbb{r}^{k \times n} be the matrix with rows x^i , and y \in
   \mathbb{r}^k be the matrix with rows y^i . what we are seeking is a
   \beta \in \mathbb{r}^k so that x\beta = y + \epsilon where \epsilon is
   an error term, and \beta is chosen to minimize this error. so we
   minimize the error on the training data, and we hope that this
   generalizes to new cases.

   the natural measure of error to use is the euclidean metric given by
   the inner product, that is \langle x,y \rangle = \sum \limits
   _{i=1}^nx_iy_i for x,y \in \mathbb{r}^n . recall that an inner product
   is a function taking two vectors to their underlying scalar field that
   represents (at least for unit vectors), the magnitude of the projection
   of one onto the other, see [17]wikipedia for further details.. this
   naturally gives rise to a norm defined \vert x\vert_2^2=\langle x,x
   \rangle.

   now we think geometrically: \beta is telling us how to combine the
   columns of x , and so \beta tells us where in the column space of x ,
   which you can imagine as being like a plane, is the best approximation
   to y . with our geometric cap on, the natural point in the column space
   to pick is the orthogonal projection of y onto the column space r(x) .

   what does this mean? well a vector p is an orthogonal projection of y
   onto r(x) when the vector between p and y , y-p , is orthogonal to r(x)
   , meaning that \langle x,y-p \rangle=0 for all x \in r(x) .
   geometrically this means that the line you draw from y to p in r(x)
   hits it at right-angles     the line is normal to the space.

   here is a picture of projection of a point onto a plane, courtesy of
   wikibooks:

   [18]linalg_projection_onto_plane_2

   there n is the normal to the plane, see how the line from the point to
   its projection is parallel to this line, meaning that any vector in the
   plane is perpendicular to the path of of the point to its projection.

   how do we know that this is the right point to pick? pythagoras   
   theorem. given orthogonal x,y pythagoras    theorem states that

   \vert x+y \vert_2^2 = \vert x \vert_2^2 + \vert y \vert_2^2 .

   so let q \in r(x) be another candidate for \beta . then \langle y-p,
   p-q\rangle=0 since p is the orthogonal projection, and so plugging this
   into the above gives

   \vert (y-p)+(p-q)\vert_2^2=\vert y-q\vert_2^2=\vert y-p\vert_2^2+\vert
   p-q \vert_2^2

   and so

   \vert y-q\vert_2 > \vert y-p\vert_2 ,

   meaning that q gives a larger error for any q \ne p. note that this is
   nothing more than the observation that the hypotenuse of a right-angled
   triangle is the longest side.

   great, so now we need to find \beta . fortunately all we need to do is
   arrange everything we know in an equation:

   0= x^t (y-x\beta) = x^ty-x^tx \beta

   giving that

   x^ty = x^tx \beta \ \ \ \ (*)

   and so

   (x^tx)^{-1}x^ty = \beta \ \ \ \ (**).

   noting that x^tx will be invertible when the columns of x are linearly
   independent, if this fails, then we can discard some redundant columns
   and start again. the matrix (x^tx)^{-1}x^t is called the projection
   matrix for the column space.  simples.

   i should point out that it is actually better not to solve for \beta on
   one side by inverting x^tx as in (**) , you should leave it as (*) and
   solve the linear system, finding the inverse is computationally
   expensive!

   next time we will implement this in python and test it on some toy
   data.
   advertisements

share this:

     * [19]twitter
     * [20]facebook
     *

like this:

   like loading...

related

   tags: [21]id202, [22]id75, [23]machine learning,
   [24]orthogonal projection, [25]projection, [26]statistics
   by [27]triangleinequality in [28]id202, [29]id75,
   [30]id75, [31]machine learning, [32]maths, [33]statistics
   on [34]november 17, 2013.
   [35]    id79 [36]id75: the code    
     __________________________________________________________________

2 comments

    1. [37]id75: the code    triangleinequality says:
       [38]november 28, 2013 at 5:06 pm
       [   ] so last time we saw what id75 was all about: you
       have data in the form of a matrix where rows [   ]
       [39]reply
    2. [40]neural networks: part 1    triangleinequality says:
       [41]march 27, 2014 at 3:53 pm
       [   ] the identity function as seen in id75, [   ]
       [42]reply

leave a reply [43]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [44]googleplus-sign-in

     *
     *

   [45]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [46]log out /
   [47]change )
   google photo

   you are commenting using your google account. ( [48]log out /
   [49]change )
   twitter picture

   you are commenting using your twitter account. ( [50]log out /
   [51]change )
   facebook photo

   you are commenting using your facebook account. ( [52]log out /
   [53]change )
   [54]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

   [55]create a free website or blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [56]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [57]cookie policy

   iframe: [58]likes-master

   %d bloggers like this:

references

   visible links
   1. https://triangleinequality.wordpress.com/feed/
   2. https://triangleinequality.wordpress.com/comments/feed/
   3. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/feed/
   4. https://triangleinequality.wordpress.com/2013/11/03/random-forest/
   5. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/&for=wpcom-auto-discovery
   8. https://triangleinequality.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://triangleinequality.wordpress.com/
  11. https://triangleinequality.wordpress.com/about/
  12. https://triangleinequality.wordpress.com/contents/
  13. https://triangleinequality.wordpress.com/
  14. https://triangleinequality.wordpress.com/
  15. https://triangleinequality.wordpress.com/category/maths/
  16. https://triangleinequality.wordpress.com/category/maths/linear-algebra/
  17. http://en.wikipedia.org/wiki/inner_product_space
  18. http://en.wikibooks.org/wiki/linear_algebra/projection_onto_a_subspace
  19. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/?share=twitter
  20. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/?share=facebook
  21. https://triangleinequality.wordpress.com/tag/linear-algebra/
  22. https://triangleinequality.wordpress.com/tag/linear-regression-2/
  23. https://triangleinequality.wordpress.com/tag/machine-learning/
  24. https://triangleinequality.wordpress.com/tag/orthogonal-projection/
  25. https://triangleinequality.wordpress.com/tag/projection/
  26. https://triangleinequality.wordpress.com/tag/statistics/
  27. https://triangleinequality.wordpress.com/author/triangleinequality/
  28. https://triangleinequality.wordpress.com/tag/linear-algebra/
  29. https://triangleinequality.wordpress.com/category/maths/statistics/linear-regression/
  30. https://triangleinequality.wordpress.com/category/machine-learning/linear-regression-machine-learning/
  31. https://triangleinequality.wordpress.com/tag/machine-learning/
  32. https://triangleinequality.wordpress.com/category/maths/
  33. https://triangleinequality.wordpress.com/tag/statistics/
  34. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/
  35. https://triangleinequality.wordpress.com/2013/11/03/random-forest/
  36. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/
  37. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/
  38. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/#comment-55
  39. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/?replytocom=55#respond
  40. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  41. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/#comment-84
  42. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/?replytocom=84#respond
  43. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/#respond
  44. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://triangleinequality.wordpress.com&color_scheme=light
  45. https://gravatar.com/site/signup/
  46. javascript:highlandercomments.doexternallogout( 'wordpress' );
  47. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/
  48. javascript:highlandercomments.doexternallogout( 'googleplus' );
  49. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/
  50. javascript:highlandercomments.doexternallogout( 'twitter' );
  51. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/
  52. javascript:highlandercomments.doexternallogout( 'facebook' );
  53. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/
  54. javascript:highlandercomments.cancelexternalwindow();
  55. https://wordpress.com/?ref=footer_website
  56. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/
  57. https://automattic.com/cookies
  58. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  60. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/#comment-form-guest
  61. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/#comment-form-load-service:wordpress.com
  62. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/#comment-form-load-service:twitter
  63. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/#comment-form-load-service:facebook
