   #[1]triangleinequality    feed [2]triangleinequality    comments feed
   [3]triangleinequality    id75: the code comments feed
   [4]id75: the math(s) [5]id28 [6]alternate
   [7]alternate [8]triangleinequality [9]wordpress.com

     * [10]home
     * [11]about
     * [12]contents

[13]triangleinequality

   for matters mathematical
   search ____________________ (button) search
     __________________________________________________________________

   [14]home    [15]maths    [16]statistics    [17]id75    linear
   regression: the code

id75: the code

   ok so [18]last time we saw what id75 was all about: you
   have data in the form of a matrix x where rows correspond to
   datapoints, and columns correspond to features, and a column vector y
   of corresponding response values. we seek a \beta so that x \beta is as
   close to y as possible.

   we saw that we can find the solution by solving the linear system
   x^t\beta = x^txy for \beta .

   to do this in python will be very simple using  scipy.linalg! i have
   created an [19]ipython notebook taking you through everything, which
   you can run interactively.

   let   s begin by creating some data to model to get a feel for what we
   are doing. we   ll start with a linear function f, and add some gaussian
   noise to make it a bit more realistic, then use id75 to
   fit that data, and see how well we do compared with the true function
   f.
#first get all the inclusions out of the way.
import numpy as np
from scipy import linalg
import random as rd
import matplotlib.pyplot as plt
def test_set(a,b, no_samples, intercept, slope, variance):
    #first we grab some random x-values in the range [a,b].
    x = [rd.uniform(a,b) for i in range(no_samples)]
    #sorting them seems to make the plots work better later on.
    x.sort()
    #now we define our linear function that will underly our synthetic dataset:
the true model.
    def f(x):
        return intercept + slope*x
    #we create some y values to go with each x, given by f(x) plus a small pertu
rbation by gaussian noise.
    y = [f(x)+rd.gauss(0,variance) for x in x]
    #now we return these as numpy column vectors, as well as the points given by
 f without noise.
    return [np.array([x]).t,np.array([y]).t, map(f, x)]

   great. now let   s see what this looks like by plotting the data along
   with the graph of f.
x,y,f = test_set(a=0, b=5, no_samples=70, intercept=1, slope=3, variance=6)
fig, ax = plt.subplots()
ax.plot(x, y, '.')
ax.plot(x, f)
plt.show()

   this will look something like:
   [20]graph1

   but of course yours will not look exactly the same because the data is
   generated randomly.
   the code for doing the id75 is simplicity itself.
def linear_regression(x, y):
    #solve linear system x^txy = xb
    beta=np.linalg.solve(x.t.dot(x),x.t.dot(y))
    return beta

   now we want to test it on our data, note that we want to add an extra
   column to x filled with ones, since if we just take a linear
   combination of x we will always go through the origin     we want an
   intercept term.
x = np.hstack((x, np.ones((x.shape[0], 1), dtype=x.dtype)))
beta=linear_regression(x, y)
fig, ax = plt.subplots()
ax.plot(x[:,0], y, '.', label="true model + noise")
ax.plot(x[:,0], f, '-', color='red', label="true model")
ax.plot(x[:,0], x.dot(beta), color='green',label="fitted model")
legend = ax.legend(loc='lower right', shadow=true)
plt.show()
print "fitted slope = ", beta[0][0], "true slope = 3"
print "intercept =", beta[1][0], "true intercept = 1"


   producing something like:

   [21]graph2
fitted slope =  3.30648992096 true slope = 3
intercept = -0.627196179429 true intercept = 1

   well, the slope is not far off, but the intercept is wrong! this mostly
   comes from the noise we added,larger variance will mean poorer
   approximations to the true model, and more data will mean better
   approximations.if you run the code a couple of times, you will see
   quite a lot of variation. so maybe what we need to is try and quantify
   our uncertainty about the fit. we can do this by using id64 as
   we discussed previously.

   to get 95 \% condidence bounds for our fit, we will take a 1000
   resamples of our data and fit each, then extract the quantiles from
   their predictions.

   first we get the beta for each resample.
betas=[]
n= x.shape[0]
for resample in range(1000):
    index = [rd.choice(range(n)) for i in range(n)]
    re_x=x[index,:]
    betas.append(linear_regression(re_x, y[index,:]))

   then for each data point, we collect all of the predictions and take
   quantiles.
def confidence_bounds(x):
    predictions=[x.dot(b) for b in betas]
    predictions.sort()
    return [predictions[24], predictions[-25]]
upper=[]
lower=[]
for x in x:
    conf_bounds = confidence_bounds(x)
    upper.append(conf_bounds[1])
    lower.append(conf_bounds[0])

   now we put everything together in a plot.
fig, ax = plt.subplots()
ax.plot(x[:,0], y, '.', label="true function + noise")
ax.plot(x[:,0], x.dot(beta), 'red', label="id75 fit")
ax.plot(x[:,0], upper, color='blue', label="bootstrapped 95% confidence interval
s for fit")
ax.plot(x[:,0], lower, 'blue')
ax.plot(x[:,0], f, 'green', label="true function")
legend = ax.legend(loc='upper left', shadow=true,prop={'size':8})
plt.show()

   which looks like this:

   [22]graph3 next time we will cover id28, used when we
   want to predict a binary outcome, which may prove useful on the titanic
   data. the code for everything is [23]here, but i recommend using this
   [24]ipython notebook if you have it.
   advertisements

share this:

     * [25]twitter
     * [26]facebook
     *

like this:

   like loading...

related

   tags: [27]id64, [28]gaussian noise, [29]ipython, [30]linear
   regression, [31]python, [32]statistics
   by [33]triangleinequality in [34]id75, [35]linear
   regression, [36]machine learning, [37]python, [38]statistics on
   [39]november 28, 2013.
   [40]    id75: the math(s) [41]id28    
     __________________________________________________________________

2 comments

    1. [42]id28    triangleinequality says:
       [43]december 4, 2013 at 9:30 am
       [   ] back. last time we showed how to use id75 to
       predict a numeric response. today we are going to talk [   ]
       [44]reply
    2. [45]   python                - it       says:
       [46]march 15, 2016 at 7:48 am
       [   ] id75: the code data analysis with python [   ]
       [47]reply

leave a reply [48]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [49]googleplus-sign-in

     *
     *

   [50]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [51]log out /
   [52]change )
   google photo

   you are commenting using your google account. ( [53]log out /
   [54]change )
   twitter picture

   you are commenting using your twitter account. ( [55]log out /
   [56]change )
   facebook photo

   you are commenting using your facebook account. ( [57]log out /
   [58]change )
   [59]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

   [60]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [61]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [62]cookie policy

   iframe: [63]likes-master

   %d bloggers like this:

references

   visible links
   1. https://triangleinequality.wordpress.com/feed/
   2. https://triangleinequality.wordpress.com/comments/feed/
   3. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/feed/
   4. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/
   5. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/&for=wpcom-auto-discovery
   8. https://triangleinequality.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://triangleinequality.wordpress.com/
  11. https://triangleinequality.wordpress.com/about/
  12. https://triangleinequality.wordpress.com/contents/
  13. https://triangleinequality.wordpress.com/
  14. https://triangleinequality.wordpress.com/
  15. https://triangleinequality.wordpress.com/category/maths/
  16. https://triangleinequality.wordpress.com/category/maths/statistics/
  17. https://triangleinequality.wordpress.com/category/maths/statistics/linear-regression/
  18. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/
  19. http://sourceforge.net/projects/triangleinequal/files/machine learning/id75(1).ipynb/download
  20. https://triangleinequality.files.wordpress.com/2013/11/graph1.png
  21. https://triangleinequality.files.wordpress.com/2013/11/graph2.png
  22. https://triangleinequality.files.wordpress.com/2013/11/graph3.png
  23. http://sourceforge.net/projects/triangleinequal/files/machine learning/id75.py/download
  24. http://sourceforge.net/projects/triangleinequal/files/machine learning/id75(1).ipynb/download
  25. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/?share=twitter
  26. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/?share=facebook
  27. https://triangleinequality.wordpress.com/tag/id64/
  28. https://triangleinequality.wordpress.com/tag/gaussian-noise/
  29. https://triangleinequality.wordpress.com/tag/ipython/
  30. https://triangleinequality.wordpress.com/tag/linear-regression-2/
  31. https://triangleinequality.wordpress.com/tag/python/
  32. https://triangleinequality.wordpress.com/tag/statistics/
  33. https://triangleinequality.wordpress.com/author/triangleinequality/
  34. https://triangleinequality.wordpress.com/category/maths/statistics/linear-regression/
  35. https://triangleinequality.wordpress.com/category/machine-learning/linear-regression-machine-learning/
  36. https://triangleinequality.wordpress.com/category/machine-learning/
  37. https://triangleinequality.wordpress.com/tag/python/
  38. https://triangleinequality.wordpress.com/tag/statistics/
  39. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/
  40. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/
  41. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/
  42. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/
  43. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/#comment-58
  44. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/?replytocom=58#respond
  45. http://www.itdadao.com/article/593325/
  46. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/#comment-307
  47. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/?replytocom=307#respond
  48. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/#respond
  49. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://triangleinequality.wordpress.com&color_scheme=light
  50. https://gravatar.com/site/signup/
  51. javascript:highlandercomments.doexternallogout( 'wordpress' );
  52. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/
  53. javascript:highlandercomments.doexternallogout( 'googleplus' );
  54. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/
  55. javascript:highlandercomments.doexternallogout( 'twitter' );
  56. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/
  57. javascript:highlandercomments.doexternallogout( 'facebook' );
  58. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/
  59. javascript:highlandercomments.cancelexternalwindow();
  60. https://wordpress.com/?ref=footer_blog
  61. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/
  62. https://automattic.com/cookies
  63. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  65. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/#comment-form-guest
  66. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/#comment-form-load-service:wordpress.com
  67. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/#comment-form-load-service:twitter
  68. https://triangleinequality.wordpress.com/2013/11/28/linear-regression-the-code/#comment-form-load-service:facebook
