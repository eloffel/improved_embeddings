   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    linear
   regression, least squares & id127: a concise technical
   overview comments feed [5]top kdnuggets tweets, nov 16-22: top 20
   #python #machinelearning #opensource projects; shortcomings of
   #deeplearning [6]neighbors know best: (re) classifying an
   underappreciated beer

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2016    [28]nov    [29]tutorials,
   overviews    id75, least squares & id127: a
   concise technical overview ( [30]16:n42 )

id75, least squares & id127: a concise technical
overview

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 147
   tags: [33]algorithms, [34]id75

   id75 is a simple algebraic tool which attempts to find the
      best    line fitting 2 or more attributes. read here to discover the
   relationship between id75, the least squares method, and
   id127.
     __________________________________________________________________

   by [35]matthew mayo, kdnuggets.

   regression is a time-tested manner for approximating relationships
   among a given collection of data, and the recipient of unhelpful naming
   via [36]unfortunate circumstances.

   id75 is a simple algebraic tool which attempts to find the
      best    (generally straight) line fitting 2 or more attributes, with one
   attribute (simple id75), or a combination of several
   (multiple id75), being used to predict another, the class
   attribute. a set of training instances is used to compute the linear
   model, with one attribute, or a set of attributes, being plotted
   against another. the model then attempts to identify where new
   instances would lie on the regression line, given a particular class
   attribute.

                           id75 setup
   fig.1: plotting data, preparing for id75 ([37]han, kamber
                                   & pei).

   the relationship between response and predictor variables (y and x,
   respectively) can be expressed via the following equation, with which
   everyone reading this is undoubtedly familiar:

                                  equation

   m and b are the regression coefficients, and denote line slope and
   y-intercept, respectively. i suggest you check your elementary school
   algebra notes if you are having trouble recalling :)

   the equation for multiple id75 is generalized for n
   attributes as follows:

                                  equation

   it is often confusing for people without a sufficient math background
   to understand how id127 fits into id75.

   least squares method & id127

   one method of approaching linear analysis is the [38]least squares
   method, which minimizes the sum of the squared residuals. residuals are
   the differences between the model fitted value and an observed value,
   or the predicted and actual values. the exercise of minimizing these
   residuals would be the trial and error fitting of a line "through" the
   cartesian coordinates representing these values.

   one way to proceed with the least squares method is to solve via matrix
   multiplication. how so? the mathematics which underlie the least
   squares method can be seen [39]here, while a short video with example
   is shown below.

            iframe: [40]https://www.youtube.com/embed/qa_fi92_qo8

   if you are interested in a video with some additional insight, a proof,
   and some further examples, have a look [41]here. a number of linear
   regression for machine learning implementations are available, examples
   of which include those in the popular [42]scikit-learn library for
   python and the formerly-popular [43]weka machine learning toolkit.

   the least squares method can more formally be described as follows:

   given a dataset of points (x[1], y[1] ), (x[2], y[2] ), ..., (x[n],
   y[n]), we derive the matrices:

                                  matrices

   we then set up the matrix equation:

                                  equation

   where matrix y contains the y values, matrix x contains a row of 1s and
   along with the x values, matrix a consists of the y-intercept and
   slope, and matrix e is the errors. note the row of 1s in matrix x are
   needed to allow multiplication with matrix a (2 rows and 2 columns,
   respectively).

   we then solve for a, which is:

                                  equation

   this is the matrix equation ultimately used for the least squares
   method of solving a linear system.

   some example (python) code

   the following is [44]a sample implementation of simple linear
   regression using least squares id127, relying on numpy
   for heavy lifting and matplotlib for visualization
   .

   a particular run of this code generates the following input matrix:
[[ 0.64840322  0.97285346]
 [ 0.77867147  0.87310339]
 [ 0.85072744  0.59023482]
 [ 0.3692784   0.59567815]
 [ 0.14654649  0.79422356]
 [ 0.46897942  0.06988269]
 [ 0.79239438  0.33157126]
 [ 0.88935174  0.04946074]
 [ 0.0615097   0.68082408]
 [ 0.63675227  0.93102028]]

   and solving results in this projection matrix, the values of which are
   the y-intercept and slope of the regression line, respectively:
[[ 0.73461589]
 [-0.25826794]]

   this solution is visualized below.

                           id75 plot
            fig.2: id75 example visualization plot.

   of course, in the context of machine learning, data instances of
   unknown response variable values would then be placed on the regression
   line based on their predictor variables, which constitutes predictive
   power.

   id75 is an incredibly powerful prediction tool, and is one
   of the [45]most widely used algorithms available to data scientists.

   related:
     * [46]decision tree classifiers: a concise technical overview
     * [47]frequent pattern mining and the apriori algorithm: a concise
       technical overview
     * [48]support vector machines: a concise technical overview
     __________________________________________________________________

   [49][prv.gif] previous post
   [50]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [51]another 10 free must-read books for machine learning and data
       science
    2. [52]9 must-have skills you need to become a data scientist, updated
    3. [53]who is a typical data scientist in 2019?
    4. [54]the pareto principle for data scientists
    5. [55]what no one will tell you about data science job applications
    6. [56]19 inspiring women in ai, big data, data science, machine
       learning
    7. [57]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [58]id158s optimization using genetic algorithm
       with python
    2. [59]who is a typical data scientist in 2019?
    3. [60]8 reasons why you should get a microsoft azure certification
    4. [61]the pareto principle for data scientists
    5. [62]r vs python for data visualization
    6. [63]how to work in data science, ai, big data
    7. [64]the deep learning toolset     an overview

[65]latest news

     * [66]download your datax guide to ai in marketing
     * [67]kdnuggets offer: save 20% on strata in london
     * [68]training a champion: building deep neural nets for big ...
     * [69]building a recommender system
     * [70]predict age and gender using convolutional neural netwo...
     * [71]top tweets, mar 27     apr 02: here is a great ex...

   [72]kdnuggets home    [73]news    [74]2016    [75]nov    [76]tutorials,
   overviews    id75, least squares & id127: a
   concise technical overview ( [77]16:n42 )
      2019 kdnuggets. [78]about kdnuggets.  [79]privacy policy. [80]terms
   of service

   [81]subscribe to kdnuggets news
   [82][tw_c48.png] [83]facebook [84]linkedin
   x

   [envelope.png] [85]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2016/11/linear-regression-least-squares-matrix-multiplication-concise-technical-overview.html/feed
   5. https://www.kdnuggets.com/2016/11/top-tweets-nov16-22.html
   6. https://www.kdnuggets.com/2016/11/neighbors-know-best-classifying-underappreciated-beer.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2016/index.html
  28. https://www.kdnuggets.com/2016/11/index.html
  29. https://www.kdnuggets.com/2016/11/tutorials.html
  30. https://www.kdnuggets.com/2016/n42.html
  31. https://www.kdnuggets.com/2016/11/top-tweets-nov16-22.html
  32. https://www.kdnuggets.com/2016/11/neighbors-know-best-classifying-underappreciated-beer.html
  33. https://www.kdnuggets.com/tag/algorithms
  34. https://www.kdnuggets.com/tag/linear-regression
  35. https://www.kdnuggets.com/author/matt-mayo
  36. https://en.wikipedia.org/wiki/regression_analysis#history
  37. http://hanj.cs.illinois.edu/bk3/
  38. https://en.wikipedia.org/wiki/least_squares
  39. https://en.wikipedia.org/wiki/linear_least_squares_(mathematics)
  40. https://www.youtube.com/embed/qa_fi92_qo8
  41. https://www.youtube.com/watch?v=z0weliinnvq
  42. http://scikit-learn.org/stable/modules/linear_model.html
  43. http://www.cs.waikato.ac.nz/ml/weka/
  44. https://en.wikipedia.org/wiki/linear_least_squares_(mathematics)#python
  45. https://www.kdnuggets.com/2016/09/poll-algorithms-used-data-scientists.html
  46. https://www.kdnuggets.com/2016/10/decision-trees-concise-technical-overview.html
  47. https://www.kdnuggets.com/2016/10/association-rule-learning-concise-technical-overview.html
  48. https://www.kdnuggets.com/2016/09/support-vector-machines-concise-technical-overview.html
  49. https://www.kdnuggets.com/2016/11/top-tweets-nov16-22.html
  50. https://www.kdnuggets.com/2016/11/neighbors-know-best-classifying-underappreciated-beer.html
  51. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  52. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  53. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  54. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  55. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  56. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  57. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  58. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  59. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  60. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  61. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  62. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  63. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  64. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  65. https://www.kdnuggets.com/news/index.html
  66. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  67. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  68. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  69. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  70. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  71. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  72. https://www.kdnuggets.com/
  73. https://www.kdnuggets.com/news/index.html
  74. https://www.kdnuggets.com/2016/index.html
  75. https://www.kdnuggets.com/2016/11/index.html
  76. https://www.kdnuggets.com/2016/11/tutorials.html
  77. https://www.kdnuggets.com/2016/n42.html
  78. https://www.kdnuggets.com/about/index.html
  79. https://www.kdnuggets.com/news/privacy-policy.html
  80. https://www.kdnuggets.com/terms-of-service.html
  81. https://www.kdnuggets.com/news/subscribe.html
  82. https://twitter.com/kdnuggets
  83. https://facebook.com/kdnuggets
  84. https://www.linkedin.com/groups/54257
  85. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
  87. https://www.kdnuggets.com/
  88. https://www.kdnuggets.com/
