nlp

introduction to nlp

probabilities 2/2

the chain rule

    p(w1,w2,w3   wn) = ?
    using the chain rule:

    p(w1,w2,w3   wn) =p(w1) p(w2|w1) p(w3|w1,w2)    

p(wn|w1,w2   wn-1)

    this rule is used in many ways in statistical 

nlp, more specifically in markov models

independence

    two events are independent when 
    unless p(b)=0 this is equivalent to saying 
    if two events are not independent, they are 

p(a  b) = p(a)p(b)
that p(a) = p(a|b)
considered dependent

adding vs. removing constraints
    adding constraints

    p(walk=yes|weather=nice)
    p(walk=yes|weather=nice,freetime=yes,crowded=yes)
    more accurate
    but more difficult to estimate
    removing constraints (backoff)

    p(walk=yes|weather=nice,freetime=yes,crowded=yes)
    p(walk=yes|weather=nice,freetime=yes)
    p(walk=yes|weather=nice)
    note that it is notpossible to do backoff on the left hand side of 

the conditional

[example modified from jason eisner]

random variables

x: w    rn

    simply a function:
    the numbers are generated by a stochastic processwith a 
    example

certain id203 distribution

    the discrete random variable x that is the sum of the faces of two 

randomly thrown fair dice

    id203 mass function (pmf) which gives the id203 

that the random variable has different numeric values:

p(x) = p(x = x)  = p(ax) where ax = {w    w : x(w) = x}

random variables

if a random variable x is distributed according to 
the pmf p(x), then we write x ~ p(x)

   
    for a discrete random variable, we have

sp(xi) = p(w) = 1

six-sided fair die

    p(1) = 1/6
    p(2) = 1/6
    etc.
    p(d)=?
    p(d) = {1/6, 1/6, 1/6, 1/6, 1/6, 1/6}
    p(d|odd) = {1/3, 0, 1/3, 0, 1/3, 0}

[slide from brendan o   connor]

[slide from brendan o   connor]

nlp

