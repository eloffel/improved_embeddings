nlp

introduction to nlp

language models (2/3)

   

smoothing
if the vocabulary size is |v|=1m
    too many parameters to estimate even a unigram model
    id113 assigns values of 0 to unseen (yet not impossible) 
    let alone bigram or trigram models

data

    smoothing (id173)

    reassigning some id203 mass to unseen data

smoothing

    how to model novel words?

    or novel bigrams?
    distributing some of the id203 mass to allow for novel 

    add-one (laplace) smoothing: 

    bigrams: p(wi|wi-1) = (c(wi-1,wi)+1)/(c(wi-1)+v)
    this method reassigns too much id203 mass to unseen 

events

events

    possible to do add-kinstead of add-one

    both of these don   t work well in practice

advanced smoothing

    try to predict the probabilities of unseen events based on the 

    good-turing

probabilities of seen events

    kneser-ney
    class-based id165s

example

    corpus:

    cat dog cat rabbit mouse fish fish mouse hamster hamster fish turtle tiger 

cat rabbit cat dog dog fox lion

    what is the id203 the next item is    mouse   ?
    what is the id203 the next item is    elephant    or some other 

    pid113 (mouse) = 2/20
previously unseen animal?
    trickier
    is it 0/20?
    note that p (that the next animal is unseen) > 0
    therefore we need to discount the probabilities of the animals that have 
    pid113 (mouse) < 2/20

already been seen

good turing

once

   

idea
    estimate the frequencies of unseen events based on the events seen 

corpus

    actual counts c
    nr = number of id165s that occur exactly c times in the 
    n0 = total number of id165s in the corpus
    revised counts c*
    c* = (c+1) nc+1/nc

example

    cat dog cat rabbit mouse fish fish mouse hamster hamster fish turtle tiger cat rabbit 

    corpus:

    counts

cat dog dog fox lion

    c(cat) = 4
    c(dog) = 3
    c(fish) = 3
    c(mouse) = 2
    c(rabbit) = 2
    c(hamster) = 2
    c(fox) = 1
    c(turtle) = 1
    c(tiger) = 1
    c(lion) = 1

    n1=4, n2=3, n3=2, n4=1

example (cont   d)

    n1=4, n2=3, n3=2, n4=1
    revised counts c* = (c+1) nc+1/nc (or best-fit power law estimate if 

counts are unreliable)
    c*(cat) = 4 
    c*(dog) = (3+1) x 1/2 = 2
    c*(mouse) = (2+1) x 2/3 = 2
    c*(rabbit) = (2+1) x 2/3 = 2
    c*(hamster) = (2+1) x 2/3 = 2
    c*(fox) = (1+1) x 3/4 = 6/4
    c*(turtle) = (1+1) x 3/4 = 6/4
    c*(tiger) = (1+1) x 3/4 = 6/4
    c*(lion) = (1+1) x 3/4 = 6/4
    c*(elephant) = n1/n = 4/20
to be normalized.
    p*(lion) = 6/4 / 20 = 6/80
    more information
    http://beyondexpectations.quora.com/an-intuitive-explanation-of-good-turing-

    note that these counts don   t necessarily add to 1, so they still need 

smoothing

dealing with sparse data

    two main techniques used

    backoff
    interpolation

backoff

    going back to the lower-order id165 model if 
the higher-order model is sparse (e.g., frequency 
<= 1)

    learning the parameters
    from a development data set

interpolation

   

if p   (wi|wi-1,wi-2) is sparse:
    use   1p   (wi|wi-1,wi-2) +  2p   (wi|wi-1)+  3p   (wi)
    ensure that   1+  2+  3=1,   1,  2,  3   1,   1,  2,  3   0
    better than backoff
    estimate the hyper-parameters   1,  2,  3 from held-out data 

(or using em), e.g., using 5-fold cross-validation

    see [chen and goodman 1998] for more details
    software:

    http://www.speech.cs.cmu.edu/slm/toolkit_documentation.h

tml

[slide from michael collins]

nlp

