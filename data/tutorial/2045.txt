nlp

introduction to nlp

language models (3/3)

evaluation of lm

    extrinsic
    use in an application
intrinsic
    cheaper

   
    correlate the two for validation purposes

perplexity

    does the model fit the data?

    a good model will give a high id203 to a real 

sentence
    perplexity

    average branching factor in predicting the next word
    lower is better (lower perplexity -> higher id203)
    n = number of words
=

per

1

n

nwwwp

...

(

2

1

)

perplexity

    example:

    a sentence consisting of n equiprobable words: p(wi) = 1/k

1

n

per

=
nwwwp
    per = ((k-1)n)(-1/n) = k
    perplexity is like a branching factor
    logarithmic version

...

(

2

1

)

    the exponent is = #bits to encode each word)
iwp
)

log

/1(

n

(

)

2

  

per

= -
2

the shannon game

    consider the shannon game:
    what is the perplexity of guessing a digit if all 

    new york governor andrew cuomo said ...
digits are equally likely? do the math.
    10

    how about a letter?
    how about guessing a (   operator   ) with a 

    26
id203 of 1/4, b (   sales   ) with a id203 of 
1/4 and 10,000 other cases with a id203 of 
1/2 total
    example modified from joshua goodman.

perplexity across distributions
    what if the actual distribution is very different 
    example:

from the expected one?

    all of the 10,000 other cases are equally likely but p(a) 
    cross-id178 = log (perplexity), measured in 

= p(b) = 0.

bits

sample values for perplexity

    wall street journal (wsj) corpus

    38 m words (tokens)
    20 k types
    perplexity

documents

    evaluated on a separate 1.5m sample of wsj 
    unigram 962
    bigram 170
    trigram 109

word error rate

    another evaluation metric

    number of insertions, deletions, and substitutions
    normalized by sentence length
    same as levenshtein id153

    example:

    governor dan malloy met with the mayor
    the governor met the senator
    3 deletions + 1 insertion + 1 substitution = wer of 5

issues

    out of vocabulary words (oov)
    split the training set into two parts
    label all words in part 2 that were not in part 1 
as <unk>
    id91

    e.g., dates, monetary amounts, organizations, 
years

long distance dependencies

    this is where id165 language models fail by 
    missing syntactic information

definition

    the students who participated in the game are tired
    the student who participated in the game is tired

    missing semantic information

    the pizza that i had last night was tasty
    the class that i had last night was interesting

other ideas in lm

    syntactic models

    caching models

    condition words on other words that appear in a 
specific syntactic relation with them

    take advantage of the fact that words appear in 
bursts

external resources

    http://www.speech.sri.com/projects/srilm/

    http://www.speech.cs.cmu.edu/slm/toolkit.html

    sri-lm
    cmu-lm
    google id165 corpus
gram-are-belong-to-you.html

    google book id165s

    http://ngrams.googlelabs.com/

    http://googleresearch.blogspot.com/2006/08/all-our-n-

example google id165s

302435

125206

house a
118894
house after
105970
house all
3880495
house and
136475
house are
house arrest 254629
house as
339590
house at
694739
house before 102663
189451
house built
house but
137151
house by
249118
house can
133187
house cleaning
house design 120500
house down
109663
house fire
112325
house for
1635280
house former 112559
house from 249091
house had
154848
house has
440396
115434
house he

139282
house hotel
house in
3553052
1962473
house is
house music 199346
house near
131889
127043
house now
house of
3164591
house on
1077835
house or
1172783
162668
house party
house plan
172765
house plans
434398
house price
158422
house prices 643669
house rental 209614
house rules
108025
house share
101238
133405
house so
687925
house that
house the
478204
house to
1452996
house training163056
135820
house value

nlp

