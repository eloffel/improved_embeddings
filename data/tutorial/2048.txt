nlp

introduction to nlp

id48 (1/2)

markov models

    sequence of random variables that aren   t 
    examples 

independent

    weather reports
    text
    stock market numbers

properties

p(xt+1 = sk|x1,   ,xt) = p(xt+1 = sk|xt)

    limited horizon:
    time invariant (stationary)
    definition: in terms of a transition matrix a and 

= p(x2=sk|x1)
initial state probabilities p.

example

1.0

f

e

0.2

1.0

1.0

b

c

0.3

0.8

0.1

0.7

0.2

a

d

0.7

start

visible mm

p(x1,   xt) = p(x1) p(x2|x1) p(x3|x1,x2)     p(xt|x1,   ,xt-1)

= p(x1) p(x2|x1) p(x3|x2)     p(xt|xt-1)
=   x1

axtxt+1

t    1
   
t=1

p(d, a, b) = p(x1=d) p(x2=a|x1=d) p(x3=b|x2=a)

= 1.0 x 0.7 x 0.8
= 0.56

hidden mm

    motivation

hidden
    definition

    observing a sequence of symbols
    the sequence of states that led to the generation of the symbols is 
    the states correspond to hidden (latent) variables
    q = states
    o = observations, drawn from a vocabulary
    q0,qf = special (start, final) states
    a = state transition probabilities
    b = symbol emission probabilities
    p = initial state probabilities
       = (a,b,p) = complete probabilistic model

hidden mm

    uses

    id52
    id103
    gene sequencing

hidden markov model (id48)

    can be used to model state sequences and 
    example:

observation sequences

    p(s,w) = p i p(si|si-1)p(wi|si)

s0

s1

s2

s3

   

w1

w2

w3

sn

wn

generative algorithm

    pick start state from p
    for t = 1..t

    move to another state based on a
    emit an observation based on b

state transition probabilities

start

0.8

g

0.2

0.6

h

0.4

emission probabilities

    p(ot=k|xt=si,xt+1=sj) = bijk

y

x

z
g 0.7 0.2 0.1
h 0.3 0.5 0.2

all parameters of the model

initial
    p(g|start) = 1.0, p(h|start) = 0.0

   
    transition
    emission

    p(x|g) = 0.7, p(y|g) = 0.2, p(z|g) = 0.1
    p(x|h) = 0.3, p(y|h) = 0.5, p(z|h) = 0.2

    p(g|g) = 0.8, p(g|h) = 0.6, p(h|g) = 0.2, p(h|h) = 0.4

observation sequence    yz   

    starting in state g (or h), p(yz) = ?
    possible sequences of states:

    gg
    gh
    hg
    hh
= .8 x .2 x .8 x .1 
+ .8 x .2 x .2 x .2
+ .2 x .5 x .4 x .2
+ .2 x .5 x .6 x .1
= .0128+.0064+.0080+.0060 =.0332

    p(yz) = p(yz|gg) + p(yz|gh) + p(yz|hg) + p(yz|hh) =

states and transitions

    an id48 is essentially a weighted finite-state 

transducer
    the states encode the most recent history
    the transitions encode likely sequences of states

    e.g., adj-noun or noun-verb 
    or perhaps art-adj-noun

    use id113 to estimate the probabilities
    another way to think of an id48

    it   s a natural extension of na  ve bayes to sequences

emissions

    estimating the emission probabilities

    harder than transition probabilities
    there may be novel uses of word/pos combinations

    suggestions

    it is possible to use standard smoothing
    as well as heuristics (e.g., based on the spelling of the 

words)

sequence of observations

    the observer can only see the emitted symbols
    observation likelihood

    given the observation sequence s and the model    = 

(a,b,p), what is the id203 p(s|  ) that the sequence 
was generated by that model.

    being able to compute the id203 of the 
observations sequence turns the id48 into a 
language model

tasks with id48

    given    = (a,b,p), find p(o|  )
    uses the forward algorithm
    given o,   , find (x1,   xt+1)
    uses the viterbi algorithm
    given o and a space of all possible   1..m, find model   i
that best describes the observations
    uses expectation-maximization

id136

    find the most likely sequence of tags, given the sequence of 

words
    t* = argmaxt p(t|w)
values of t
    in practice, there are way too many combinations

    given the model   , it is possible to compute p (t|w) for all 

    greedy search
    id125 

    one possible solution
    uses partial hypotheses
    at each state, only keep the k best hypotheses so far
    may not work

viterbi algorithm

    find the best path up to observation i and state s
    characteristics
    uses id145
    memoization
    backpointers

id48 trellis

end

end

end

end

g

h

start

p(y|g)

p(h|g)

g

h

g

h

g

h

p(h|h)

start

start

start

id48 trellis

p(g,t=1) = 
p(start) x p(g|start) x p(y|g)

end

end

end

end

g

h

start

p(g,t=1)

g

h

start

y

g

h

start

z

g

h

start

.

id48 trellis

end

end

end

end

p(h,t=1) = 
p(start) x p(h|start) x p(y|h)

g

h

start

p(h,t=1)

g

h

start

y

g

h

start

z

g

h

start

.

id48 trellis

p(h,t=2) = 
max (p(g,t=1) x p(h|g) x p(z|h),
p(h,t=1) x p(h|h) x p(z|h))

end

end

end

end

g

h

start

g

h

start

y

p(h,t=2)

g

h

start

z

g

h

start

.

id48 trellis

end

end

end

end

g

h

start

g

h

start

y

p(h,t=2)

g

h

start

z

g

h

start

.

id48 trellis

end

end

end

end

g

h

start

g

h

start

y

g

h

start

z

g

h

start

.

id48 trellis

end

end

end

end

p(end,t=3)

g

h

start

g

h

start

y

g

h

start

z

g

h

start

.

id48 trellis

p(end,t=3)

p(end,t=3) = 
max (p(g,t=2) x p(end|g),
p(h,t=2) x p(end|h))

end

end

end

end

g

h

start

g

h

start

y

g

h

start

z

g

h

start

.

id48 trellis

p(end,t=3)

p(end,t=3) = 
max (p(g,t=2) x p(end|g),
p(h,t=2) x p(end|h))

p(end,t=3) = best score for the sequence

use the backpointers to find the 
sequence of states.

end

end

end

end

g

h

start

g

h

start

y

g

h

start

z

g

h

start

.

some observations

    advantages of id48s
    relatively high accuracy
    easy to train

    higher-order id48

    the previous example was about bigram id48s
    how can you modify it to work with trigrams?

how to compute p(o)

    viterbi was used to find the most likely sequence of 
states that matches the observation
    what if we want to find all sequences that match the 
observation
    we can add their probabilities (because they are 
mutually exclusive) to form the id203 of the 
observation

    this is done using the forward algorithm

the forward algorithm

    very similar to viterbi
   

instead of maxwe use sum

source: wikipedia

nlp

