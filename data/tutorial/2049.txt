nlp

introduction to nlp

id48 (2/2)

id48 learning

    supervised
    unsupervised

    training sequences are labeled 

    training sequences are unlabeled
    known number of states

    semi-supervised

    some training sequences are labeled

supervised id48 learning

    estimate the static transition probabilities using 

id113

a
ij

=

count

(

q
t

=
count
(

s

j

)

s
i
q
t

q ,
=

=
)

1t
+
s
i

    estimate the observation probabilities using id113

count

v

)

kb
)(

j

=

(

q
os
,
=
i
i
j
q
count
s
(
=
i

=
)

j

k

    use smoothing

unsupervised id48 training

    observation sequences

    given: 
    goal: 
    use em (expectation maximization) methods 
    forward-backward (baum-welch) algorithm
    baum-welch finds an approximate solution for p(o|  )

    build the id48

outline of baum-welch

    algorithm

    randomly set the parameters of the id48
    until the parameters converge repeat:

    e step     determine the id203 of the various state sequences 
    m step     reestimate the parameters based on these probabilities

for generating the observations

of the data p(o|  ) increases

    the algorithm guarantees that at each iteration the likelihood 
    it can be stopped at any point and give a partial solution
    it converges to a local maximum

    notes

nlp

