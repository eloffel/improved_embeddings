nlp

introduction to nlp

probabilistic parsing (2/2)

main tasks with pid18s

all parse trees that correspond to s

    given a grammar g and a sentence s, let t(s) be 
    task 1
    task 2

    find which tree t among t(s) maximizes the id203 p(t)

    find the id203 of the sentence p(s) as the sum of all 

possible tree probabilities p(t)

probabilistic parsing methods

    probabilistic earley algorithm
    top-down parser with a id145 table
    probabilistic cocke-kasami-younger (cky) 
algorithm
    bottom-up parser with a id145 table

probabilistic grammars

    probabilities can be learned from a training 

corpus
    treebank

    parse #1 is twice as probable as parse #2

    intuitive meaning
    possible to do reranking
    possible to combine with other stages

    e.g., id103, translation

maximum likelihood estimates

    use the parsed training set for getting the counts

    pml(      ) = count(      )/count(  )

    example: 

    pml(s  np vp) = count(s  np vp)/count(s)

the image part with relationship id r  was not found in the    le.

example from jurafsky and martin

example

s -> np vp    [p0=1]
np -> dt n    [p1=.8]
np -> np pp   [p2=.2]
pp -> prp np  [p3=1]
vp -> v np    [p4=.7]
vp -> vp pp   [p5=.3]
dt -> 'a'     [p6=.25]
dt -> 'the'   [p7=.75]
n -> 'child'  [p8=.5]
n -> 'cake'   [p9=.3]
n -> 'fork'   [p10=.2]
prp -> 'with' [p11=.1]
prp -> 'to'   [p12=.9]
v -> 'saw'    [p13=.4]
v -> 'ate'    [p14=.6]

the

child

ate

the

cake

with

the

fork

the

dt
.75

child

ate

the

cake

with

the

fork

the

dt
.75

child

n
.5

ate

the

cake

with

the

fork

the

dt
.75

child

np
.8

n
.5

ate

the

cake

with

the

fork

the

dt
.75

np
.8*.5*.75

child

n
.5

ate

the

cake

with

the

fork

the

dt
.75

np
.8*.5*.75

child

n
.5

ate

the

cake

with

the

keep only the highest score in each cell

fork

question

    now, on your own, compute the id203 of the 
    don   t forget that there may be multiple parses, 

entire sentence using probabilistic cky.
so you will need to add the corresponding 
probabilities.

notes

    stanford demo
    ptb statistics
    ptb peculiarities

    http://nlp.stanford.edu:8080/parser/

    50,000 sentences (40,000 training; 2,400 testing)

    includes traces and other null elements
    flat np structure (e.g., np -> dt jj jj nnp nns)

    parent transformation

    subject nps are more likely to be modified than object nps
    e.g., replace np with np^s

nlp

