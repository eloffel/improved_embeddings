nlp 

parsing

prepositional phrase attachment 

(3/3) 

algorithm 2a 

if the preposition is    of   , label the tuple as    low   . 
else 
     if the preposition is    to   , label the tuple as 
   high   . 
     else 
          label the tuple as    high   	
   

some observations (1/2) 

       first, even though the expected performance of rule 3 was 52%, 
its actual performance on the training set dropped to 39% after 
rules 1 and 2 were applied. 
in other words, these rules used up some of the information hidden in the data 
      
ahead of rule 3 and left it less useful information to rely upon. 
       even more, one can see that a better decision would have been to replace rule 3 
with its exact opposite, label everything left at this stage as    high   , which would 
have boosted the combined performance. 
an overall accuracy of 74% on the training set.

       algorithm 2a would achieve 5,527 + 2,172 + 7,714 = 15,413 correct decisions for 

       second, one cannot help but notice that algorithms 2 and 2a 

each have only three rules. 
       we can imagine a classifier with 20,801 rules, one per training example, each rule 
of the form    if the preposition is    of    and the nouns are such and such and the 
verbs are such and such, then classify the data point as the actual class observed 
in the training set   . 

some observations (2/2) 

       third, we so far reported performance on the training set. 
       can we project the performance on the training set to the test set? 
       let   s start with algorithms 1 and 3. 
       algorithm 1 labels everything as low attachment. it achieved 52% on the 
training set. we expect its performance on the test set to be similar. in 
fact it is 59% (1,826/3,097).

       this clearly demonstrates the variability of text across subsets 

of the data. 
      

in this case, this variability favors algorithm 1 since its performance 
actually goes up when moving to the test set. in other cases (e.g., if we 
had swapped the training and test sets), its performance would have 
gone down. 
around the performance on the training data. 

       on average though, its performance on the test data is expected to vary 

algorithm 3 

if the preposition is    on    and the verb is    casting    and the first noun is 
      cloud    and the second noun is    economy   , label the phrase as    high   . 
else 
     if the preposition is    of    and the verb is    opened    and the first noun 
         is    can    and the second noun is    worms   , label the phrase as    low   . 
     else 
           20799 more rules    	
   

some observations 1/5 

       now, let   s consider algorithm 3. 
it achieved a very high performance on the training data (way above the 
      
   upper bound    achieved by humans). 
       however, we will now see the meaning of the word overfitting in action. 
       algorithm 3 was so specific to the training data that most of the rules it 
       only 117 combinations (out of 3032) of words in the test set match a 

learned don   t apply at all in the test set. 
combination previously seen in the training set. 
in other words, algorithm 3 learned a lot of good rules, but it failed to learn 
many more. in fact, its accuracy on the test data is only around 4%. 

      

some observations 2/5 

       an alternative to algorithm 3 would be to combine it with a 

default rule (just like rule 3 in algorithm 2) that labels everything 
that algorithm 3 missed as noun attachment. 
       unfortunately, even this algorithm (let   s call it algorithm 3a) would only 

       the lesson to learn here is that, on unseen data, a simple algorithm 

achieve a performance slightly above the baseline (algorithm 1) of 59% on 
the test data. 
(algorithm 1) is much better than a really complicated one that overfits 
(algorithm 3). also, the combination of the two (overfitting + baseline) just 
barely outperforms the baseline itself and is nowhere close to competitive. 

some observations 3/5 

       clearly this algorithm (algorithm 3) would achieve close to 

100% accuracy on the training set. 
       why    close to 100%    and not    100%   ? 
it turns out that the training set there are mutually inconsistent labels for 
      
the same data point. 
       for example,    won verdict in case    appears once as high and once as low 
attachment. 
       there are a total of 56 such    discrepancies    in the training set. 
       some of them are caused by inconsistent annotators whereas others 

would require more context (e.g., the entire paragraph or document) to 
be correctly disambiguated.

some observations 4/5 

       next, let   s see how algorithms 2 and 2a will fare on the test 

set. 
       first, let   s look at algorithm 2. 
       there are 3097 items to classify in the test set. 
       rule 1 correctly classifies 918 out of 926 instances of    of    (99% accuracy) 

while rule 2 gets 70% accuracy (234/332 correctly classified). 

       rule 3 achieves 810/1,839 = 44%. 

       overall the accuracy of algorithm 2 on the test set is 63% (1,962/3,097). 
       again, on the test data, algorithm 2a outperforms algorithm 2. its rule 3 
gets 1,029/1,839 = 56% accuracy and the overall accuracy of algorithm 
2a on the test set is 70% (2,181/3,097). 

some observations 5/5 

       let   s now summarize the performance of 
the five algorithms that we have looked at 
so far.

number of rules	
    training set accuracy	
    test set accuracy	
   

algorithm	
   
algorithm 1: default	
   
algorithm 2: of/to + default	
   
algorithm 2a: of/to + better default	
   
algorithm 3: memorize everything	
   
algorithm 3a: memorize + default	
   

1	
   
3	
   
3	
   
20801	
   
20802	
   

52%	
   
60%	
   
74%	
   
nearly 100%	
   
nearly 100%	
   

59%	
   
63%	
   
70%	
   
4%	
   
62%	
   

what   s next? 

accuracy to 70% with two simple rules. 
the algorithm? 

       so far, so good. we have been able to go from 59% test set 
       what additional sources of information can we use to improve 
       here are some ideas:
some verb and nouns)

       use a few more good word features (e.g., more prepositions, perhaps 
       use clever ways to deal with missing information
       use lexical semantic information (e.g., synonyms)
       use additional context beyond the four feature types used so far.

noun1	
   
61	
   
6	
   
17	
   
23	
   
72	
   
noun2	
   
5	
   
33	
   
22	
   
23	
   
68	
   
57	
   
427	
   

company	
   
director	
   
increase	
   
loan	
   
rate	
   

asset	
   
bank	
   
board	
   
client	
   
company	
   
day	
   
year	
   

33	
   
51	
   
57	
   
9	
   
68	
   

49	
   
31	
   
23	
   
9	
   
204	
   
14	
   
106	
   

about	
   
as	
   
at	
   
for	
   
in	
   
of	
   
on	
   
to	
   
with	
   

bring	
   
buy	
   
cut	
   
drop	
   
follow	
   
include	
   
put	
   

prep	
   
67	
   
380	
   
552	
   
1136	
   
2251	
   
73	
   
666	
   
2182	
   
698	
   
verb	
   
58	
   
217	
   
57	
   
84	
   
15	
   
22	
   
178	
   

132	
   
94	
   
136	
   
1044	
   
1577	
   
5534	
   
550	
   
517	
   
340	
   

18	
   
126	
   
34	
   
15	
   
91	
   
221	
   
37	
   

what   s next? 

       let   s first consider a combination of the first two ideas above: 

looking for ways to use all possible information that can be 
extracted from the training data. 
       this is the approach that was used by collins and brooks (1995). 
       their method was based on a principle called backoff which is somewhat 
of a combination of all the algorithms used so far (e.g., algorithms 1, 2, 
and 3). 
       backoff allows us to use the most specific evidence from the training 
data, when available but then make reasonable approximations for the 
missing evidence.

backoff method 

      

      
      

       collins and brooks algorithm:
if a 4-tuple is available, use it. 
if not, combine the evidence from 
the triples that form the 4-tuple 
(looking only at the triples that 
include the preposition). 
if that is not available, look at the 
pairs, then the singletons, and finally 
use a default class. 
in a particular order, e.g., (verb, 
noun1, preposition, noun2). 
       the matching term for 3 features is a 
triple; for 2 features it is a pair; and 
for 1 feature, the word singleton is 
used.

       a 4-tuple is just a set of 4 features 

what   s next? 

       the idea behind algorithm 3 was quite reasonable     assume that 

if the same object appear again (as defined by the same set of 
four features), it will likely have the same tag. 
       the problem with this approach is that there is not enough data in the 
       let   s do the math. to cover all the data points in the test set, we   d need 

training set to learn the likely classes of all possible combinations of features. 
information in the training set for a total of 102,998,280,840 combinations 
(more than 100 billion combinations)! 

       how did we arrive at this number? 
      

it is simply the product of the numbers 1123, 1295, 52, and 1362, which are, 
respectively, the numbers of distinct verbs, noun1s, prepositions, and noun2s 
in the test set. 
it is impossible to label so much data and even if it could be done, there 
would be billions more combinations needed to cover a new test set. 

      

other methods 

       zhao and lin 2004
       nearest neighbors
       find most similar examples     86.5% best accuracy
       similar to zavrel, daelemans, and veenstra 1997     memory-based 

learning

       boosting

       abney et al. 1999
       stetina and nagao 1997
       toutanova et al. 2004
       graph-based method

       semantics

comparative results 

algorithm	
   
algorithm 1: default	
   
algorithm 2a: of/to + better default	
   
best class per preposition	
   
collins and brooks	
   
k-nearest neighbors	
   
tumbl	
   
human average (4-tuples only)	
   
human average (whole sentence)	
   

test set accuracy	
   
59%	
   
70%	
   
72%	
   
84%	
   
80%	
   
82%	
   
88%	
   
93%	
   

nlp 

