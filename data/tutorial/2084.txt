nlp

text similarity

vector semantics

what does    acerola    mean?

    acerolais a significant source of vitamin c.
    the pulp of the acerolais very soft
    acerolaare now found growing in most sub-
    acerolacan be eaten fresh or used to make jams 

tropical regions of the world.
or jellies.

distributional similarity

    two words that appear in similar contexts are likely 
to be semantically related, e.g.,
    schedule a test driveand investigate honda's financing options
    volkswagen debuted a new version of its front-wheel-drivegolf
    the jeep reminded me of a recent drive
    our test drivetook place at the wheel of loaded ford el model
keeps.    (j.r. firth 1957)

       you will know a word by the company that it 

basic ideas

    represent words as vectors
    similar words (synonyms) should have similar 
    different senses of the same word should have 
    relations should be preserved

    for example, based on nearby words
representations
different representations

    for example,    cat   -   kitten    should be similar to    dog   -

   puppy   

context features

    the context features can be any of the following:

    the word before the target word
    the word after the target word
    any word within nwords of the target word
    any word within a specific syntactic relationship with the 

target word (e.g., the head of the dependency or the 
subject of the sentence)

    any word within the same sentence
    any word within the same document

example

options
drivegolf

    s1: schedule a test driveand investigate honda's financing 
    s2: volkswagen debuted a new version of its front-wheel-
    s3: the jeep reminded me of a recent drive
    s4: our test drivetook place at the wheel of loaded ford el 
test drive version front
1

schedule
1

recent model

model

honda
vokswagen
jeep
ford

1
1
1
1

1

1

1

1

1

association strength

    frequency matters

    we want to ignore spurious word pairings.
    however, frequency alone is not sufficient, e.g., 
being near the words    and    or    the is not very 
informative
    pointwise mutual information (pmi). 
    here wis a word and cis a feature from the 
context

pmi    ,     =log     (    ,    )
            (    )

association strength

    positive pmi (ppmi):

    if pmi(w,c)<0, that indicates that the word and the 
    in ppmi, negative values are replaced by zeros

feature are negatively associated
    smoothing may be needed
    if the vectors are probabilities
    use id181
    or jensen-shannon divergence

nlp

