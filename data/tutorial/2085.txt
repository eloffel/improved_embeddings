nlp

text similarity

id84

issues with vector similarity

    polysemy (sim < cos)
    bar, bank, jaguar, hot
    synonymy (sim > cos)
    relatedness (people are really good at figuring 

    building/edifice, large/big, spicy/hot
this)
    doctor/patient/nurse/treatment

semantic matching

query =    natural language 
processing   
document 1 =    linguistics semantics viterbi learning   
document 2 =    welcome to new haven   
    which one should we rank higher?
    query vocabulary & doc vocabulary mismatch!
    if only we can represent documents/queries as concepts!
    that   s where id84 helps

semantic concepts

election
4
3
1
5
0
0
0

news1
news2
news3
news4
recipe1
recipe2
recipe3

vote
4
3
1
5
0
0
0

president
4
3
1
5
0
0
0

tomato
0
0
0
0
1
4
1

salad
0
0
0
0
1
4
1

semantic concepts

election
4
3
1
5
0
0
0

news1
news2
news3
news4
recipe1
recipe2
recipe3

vote
4
3
1
5
0
0
0

president
4
3
1
5
0
0
0

tomato
0
0
0
0
1
4
1

salad
0
0
0
0
1
4
1

concept space = dimension reduction

    number of concepts (k) is smaller than the 

number of words (n) or number of documents 
(m).
if we represent a document as a n-dimensional 
vector; and the corpus as an m*n matrix    
    the goal is to reduce the dimensionality from n to k.
    but how can we do that?  

   

toefl synonyms and sat analogies 

    word similarity vs. analogies

example from peter turney

29 degrees

29 degrees

vectors and matrices

numbers)

    a matrix is an mx ntable of objects (in our case, 
    each row (or column) is a vector.
    matrices of compatible dimensions can be multiplied 
    what is the result of the multiplication below?

together. 

1 2
4
2 5
7
4 9 14

21   1 =

answer to the quiz

1 2
4
2 5
7
4 9 14

21   1 = 1  2+2  1+4  (   1)
4  2+9  1+14  (   1) = 023
2  2+5  1+7  (   1)

eigenvectors and eigenvalues
    an eigenvector is an implicit    direction    for a matrix 

a

! l=
!
va
v

    v (the eigenvector) is non-zero
      (the eigenvalue) can be any complex number, in principle.

    computing eigenvalues:

det(

- i
a l

0)
=

eigenvectors and eigenvalues

example:

a

=

31
  -
    
0
2
  

  
    
  

a

-

i
l

=

l

1
--
2

  
    
  

3
  
    
-
l
  

det (a-li) = (-1-l)*(-l)-3*2=0
then: l+l2-6=0;   l1=2;   l2=-3
for l1=2:

3
-
2

3
2
-
solutions: v1=v2

  
    
  

  
  
    
    
  
  

v
  
1 =    
v
  

2

0

matrix decomposition

   

if s is a square matrix, it can be decomposed into ulu-1, 
where
u = matrix of eigenvectors
l = diagonal matrix of eigenvalues

su = ul
u-1su = l
s = ulu-1

example

ll
2

=

,1

1

=

3

s

=

u

=

1u

=-

  
    
  
  
    
  

12
  
,
    
21
  
1
1
  
    
11
-
  
2/1
  
    
2/1
  

1uus
-

l=

=

2/1
-
  
    
2/1
  
1
1
  
  
    
    
11
-
  
  

  
    
  

01
30

2/1
2/1

  
  
    
    
  
  

2/1
-
2/1

  
    
  

svd: singular value decomposition
    a=usvt

    u is the matrix of orthogonal eigenvectors of aat
    v is the matrix of orthogonal eigenvectors of ata (co-variance matrix)
    the components of s are the eigenvalues of ata

    properties

    this decomposition exists for all matrices and is unique
    u, v are column orthonormal
    ut u = i; vt v = i
    s is diagonal and sorted by absolute value of the singular values (large to 
    each column (row) of s corresponds to a principal component 
    if a has 5 columns and 3 rows,  then u will be 5x5 and v will be 3x3

small)

example (berry and browne)

t1: baby
t2: child
t3: guide
t4: health 
t5: home
t6: infant
t7: 
proofing
t8: safety
t9: toddler

d1: infant & toddler first aid
d2: babies & children   s room (for your home)
d3: child safety at home
d4: your baby   s health and safety: from infant to toddler
d5: baby proofing basics
d6: your guide to easy rust proofing
d7: beanie babies collector   s guide

example

d1: t6, t9
d2: t1, t2
d3: t2, t5, t8
d4: t1, t4, t6, t8, t9
d5: t1, t7
d6: t3, t7
d7: t1, t3

example

d1: t6, t9
d2: t1, t2
d3: t2, t5, t8
d4: t1, t4, t6, t8, t9
d5: t1, t7
d6: t3, t7
d7: t1, t3

d1

d2

d3

d4

d5

d6

d7

t1

t2

t3

t4

t5

t6

t7

t8

t9

document-term matrix

raw

a

=

1011010
  
  
0000110
  
1100000
  
  
0001000
  
  
0000110
  
0001001
  
  
0110000
  
0001100
  
  
0001001
  

  
  
  
  
  
  
  
  
  
  
  
  
  
  

normalized

)(na

=

0
0
0
0
0
71.0
0
0
71.0

  
  
  
  
  
  
  
  
  
  
  
  
  
  

58.0
58.0
0
0
58.0
0
0
0
0

0
58.0
0
0
58.0
0
0
58.0
0

45.0
0
0
45.0
0
45.0
0
45.0
45.0

71.0
0
0
0
0
0
71.0
0
0

0
0
71.0
0
0
0
71.0
0
0

71.0
  
  
0
  
71.0
  
  
0
  
  
0
  
0
  
  
0
  
0
  
  
0
  

svd decomposition

u =
[[-0.70 -0.09  0.02 -0.70  0.00  0.02  0.14 -0.00  0.00]
[-0.26  0.30  0.47  0.20  0.00 -0.25 -0.16 -0.64  0.31]
[-0.35 -0.45 -0.10  0.40  0.71 -0.01 -0.05  0.00  0.00]
[-0.11  0.14 -0.15 -0.07 -0.00  0.48 -0.84  0.00 -0.00]
[-0.26  0.30  0.47  0.20  0.00 -0.25 -0.16  0.64 -0.31]
[-0.19  0.37 -0.50  0.13  0.00 -0.23  0.03 -0.31 -0.64]
[-0.35 -0.45 -0.10  0.40 -0.71 -0.01 -0.05 -0.00  0.00]
[-0.21  0.33  0.10  0.28  0.00  0.73  0.47 -0.00  0.00]
[-0.19  0.37 -0.50  0.13  0.00 -0.23  0.03  0.31  0.64]]
v    =
[[-0.17 -0.45 -0.27 -0.40 -0.47 -0.32 -0.47]
[ 0.42  0.23  0.42  0.40 -0.30 -0.50 -0.30]
[-0.60  0.46  0.50 -0.39 -0.05 -0.12 -0.05]
[ 0.23 -0.22  0.49 -0.13 -0.26  0.71 -0.26]
[ 0.00  0.00  0.00  0.00 -0.71 -0.00  0.71]
[-0.57 -0.49  0.25  0.61  0.01 -0.02  0.01]
[ 0.24 -0.50  0.45 -0.37  0.34 -0.35  0.34]]

svd decomposition

s =
[[ 1.58
0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  1.27  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  1.19  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.80  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.71  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.57  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.20]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]]

svd decomposition

u*s*v    =
[[ 0.00  0.58  0.00  0.45  0.71  0.00  0.71]
[ 0.00  0.58  0.58  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.71  0.71]
[ 0.00  0.00  0.00  0.45  0.00  0.00  0.00]
[ 0.00  0.58  0.58  0.00  0.00  0.00  0.00]
[ 0.71  0.00  0.00  0.45  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.71  0.71  0.00]
[ 0.00  0.00  0.58  0.45  0.00  0.00  0.00]]

id84

    low rank matrix approximation
    s is a diagonal matrix of eigenvalues
   

a[m*n] = u[m*m]s[m*n]vt
if we only keep the largest reigenvalues 
a     u[m*r]s[r*r]vt

[n*n]

[n*r]

rank-4 approximation of s

s =
[[ 1.58
0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  1.27  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  1.19  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.80  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]]

rank-4 approximation of a

u*s*v    =
[[ 0.00  0.58  0.00  0.45  0.71  0.00  0.71]
[ 0.00  0.58  0.58  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.71  0.71]
[ 0.00  0.00  0.00  0.45  0.00  0.00  0.00]
[ 0.00  0.58  0.58  0.00  0.00  0.00  0.00]
[ 0.71  0.00  0.00  0.45  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.71  0.71  0.00]
[ 0.00  0.00  0.58  0.45  0.00  0.00  0.00]
u*s4*v    =
[[-0.00  0.60 -0.01  0.45  0.70  0.01  0.70]
[-0.07  0.49  0.63  0.07  0.01 -0.01  0.01]
[ 0.00 -0.01  0.01 -0.00  0.36  0.70  0.36]
[ 0.20  0.05  0.01  0.22  0.05 -0.05  0.05]
[-0.07  0.49  0.63  0.07  0.01 -0.01  0.01]
[ 0.63 -0.06  0.03  0.53 -0.00  0.00 -0.00]
[ 0.00 -0.01  0.01 -0.00  0.36  0.70  0.36]
[ 0.22  0.25  0.43  0.23 -0.04  0.04 -0.04]
[ 0.63 -0.06  0.03  0.53 -0.00  0.00 -0.00]]

rank-2 approximation of s

s =
[[ 1.58  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  1.27  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]]

rank-2 approximation of a

u*s*v    =
[[ 0.00  0.58  0.00  0.45  0.71  0.00  0.71]
[ 0.00  0.58  0.58  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.71  0.71]
[ 0.00  0.00  0.00  0.45  0.00  0.00  0.00]
[ 0.00  0.58  0.58  0.00  0.00  0.00  0.00]
[ 0.71  0.00  0.00  0.45  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.71  0.71  0.00]
[ 0.00  0.00  0.58  0.45  0.00  0.00  0.00]
u*s2*v    =
[[ 0.14  0.47  0.25  0.39  0.55  0.41  0.55]
[ 0.23  0.27  0.27  0.31  0.08 -0.06  0.08]
[-0.14  0.12 -0.09 -0.01  0.43  0.46  0.43]
[ 0.10  0.12  0.12  0.14  0.03 -0.03  0.03]
[ 0.23  0.27  0.27  0.31  0.08 -0.06  0.08]
[ 0.25  0.24  0.28  0.31 -0.00 -0.14 -0.00]
[-0.14  0.12 -0.09 -0.01  0.43  0.46  0.43]
[ 0.23  0.24  0.27  0.30  0.03 -0.11  0.03]
[ 0.25  0.24  0.28  0.31 -0.00 -0.14 -0.00]]

rank-2 representation

u*s2 =
[[-1.10 -0.12  0.00  0.00  0.00  0.00  0.00]
[-0.41  0.38  0.00  0.00  0.00  0.00  0.00]
[-0.56 -0.57  0.00  0.00  0.00  0.00  0.00]
[-0.18  0.18  0.00  0.00  0.00  0.00  0.00]
[-0.41  0.38  0.00  0.00  0.00  0.00  0.00]
[-0.30  0.47  0.00  0.00  0.00  0.00  0.00]
[-0.56 -0.57  0.00  0.00  0.00  0.00  0.00]
[-0.33  0.42  0.00  0.00  0.00  0.00  0.00]
[-0.30  0.47  0.00  0.00  0.00  0.00  0.00]]
s2*v    = 
[[-0.26 -0.71 -0.42 -0.62 -0.74 -0.50 -0.74]
[ 0.53  0.29  0.54  0.51 -0.38 -0.64 -0.38]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]
[ 0.00  0.00  0.00  0.00  0.00  0.00  0.00]]

d3

d1 d2
-0.26 -0.71 -0.42 -0.62 -0.74 -0.50 -0.74
0.53 0.29
0.54 0.51 -0.38 -0.64 -0.38

d4 d5 d6 d7

t4

t8 t9
t1 t2 t3
-1.10 -0.41 -0.56 -0.18 -0.41 -0.30 -0.56 -0.33-0.30
-0.12 0.38 -0.57 0.18 0.38 0.47 -0.57 0.42 0.47

t5 t6 t7

0.6

0.4

0.2

0

0

-1.2 

y

-1 

-0.8 

-0.6 

-0.4 

-0.2 

d7

t3,t7
d6

-0.2 

-0.4 

-0.6 

-0.8 
x

0.2

0.4

0.6

0.8

1

1.2

d3

d1 d2
-0.26 -0.71 -0.42 -0.62 -0.74 -0.50 -0.74
0.53 0.29
0.54 0.51 -0.38 -0.64 -0.38

d4 d5 d6 d7

t4

t8 t9
t1 t2 t3
-1.10 -0.41 -0.56 -0.18 -0.41 -0.30 -0.56 -0.33-0.30
-0.12 0.38 -0.57 0.18 0.38 0.47 -0.57 0.42 0.47

t5 t6 t7

-1.2 

y

-1 
t1

0.6

0.4

0.2

0

0

-0.8 

-0.6 

-0.4 

-0.2 

d7

t3,t7
d6

-0.2 

-0.4 

-0.6 

-0.8 
x

0.2

0.4

0.6

0.8

1

1.2

example

d1: t6, t9
d2: t1, t2
d3: t2, t5, t8
d4: t1, t4, t6, t8, t9
d5: t1, t7
d6: t3, t7
d7: t1, t3

d1

d2

d3

d4

d5

d6

d7

t1

t2

t3

t4

t5

t6

t7

t8

t9

semantic concepts

election
4
3
1
5
0
0
0

news1
news2
news3
news4
recipe1
recipe2
recipe3

vote
4
3
1
5
0
0
0

president
4
3
1
5
0
0
0

tomato
0
0
0
0
1
4
1

salad
0
0
0
0
1
4
1

semantic concepts

election
4
3
1
5
0
0
0

news1
news2
news3
news4
recipe1
recipe2
recipe3

vote
4
3
1
5
0
0
0

president
4
3
1
5
0
0
0

tomato
0
0
0
0
1
4
1

salad
0
0
0
0
1
4
1

quiz

    can you explain why 
this graphic looks this 
way?

quiz

    compare with this:

adding noise

election
4
3
1
5
0
0
0

news1
news2
news3
news4
recipe1
recipe2
recipe3

vote
4
0
1
5
0
1
0

president
4
3
1
3
0
0
0

tomato
0
0
1
0
1
4
0

salad
0
2
0
0
1
4
1

quiz

    let a be a document x term matrix.
    what is a*a   ?
    what about a   *a?

interpretation of svd

    best direction to project on

variance

    the principal eigenvector is the dimension that explains most of the 

    finding hidden concepts
    mapping documents, terms to a lower-dimensional space
    turning the matrix into block-diagonal form
    (same as finding bi-partite cores)
in the nlp/ir literature, svd is called lsa (lsi)
   
    latent semantic analysis (indexing)
    keep as many dimensions as necessary to explain 80-90% of the 
data (energy)
    in practice, use 300 dimensions or so

fmri example

   

fmri
    functional mri (magnetic resonance imaging)
    used to measure activity in different parts of the brain when 

exposed to various stimuli

    factor analysis 
    paper

    just, m. a., cherkassky, v. l., aryal, s., & mitchell, t. m. 
(2010). a neurosemantic theory of concrete noun 
representation based on the underlying brain codes. plos
one, 5, e8622

[just et al. 2010]

[just et al. 2010]

[just et al. 2010]

external pointers

    http://lsa.colorado.edu
    http://www.cs.utk.edu/~lsi

example of lsi

a         =      u
retrieval

l
cs-concept md-concept

vt

datainf
1 1
2 2
1 1
5 5
0 0
0 0
0 0

brainlung
0
0
0
0
2
3
1

0
0
0
0
2
3
1

=

1
2
1
5
0
0
0

cs

md

0.18 0
0.36 0
0.18 0
0.90 0
0
0
0

0.53
0.80
0.27

strength of cs-concept

x

9.64 0
0

5.29

dim. reduction
x

0.58 0.58 0.58 0
0

0

0

0.71 0.71

0

[example modified from christos faloutsos]

term rep of concept

how to map query/doc to 
the same concept space?

qtconcept = qt v                
dtconcept = dt v
datainf.retrieval

brainlung

qt=

  

1  0  0  0  0 

cs-concept

0.58  0 
0.58  0 
0.58  0 
0 
0 

0.71 
0.71 

  

similarity with 

cs-concept

= 0.58  0 

  

dt=

0  1   1  0   0

1.16   0

[example modified from christos faloutsos]

nlp

