nlp

introduction to nlp

text classification

classification

    assigning	documents	or	sentences	to	

predefined	categories
    topics,	languages,	users	   

    input:

    a	document	(or	sentence)	d
    a	fixed	set	of	classes		c	= {c1,	c2,   ,	cj}

    output:	a	predicted	class	c    c

variants of problem formulation
    binary categorization: only two categories

    k-category categorization: more than two 

    retrieval: {relevant-doc, irrelevant-doc}
    spam filtering: {spam, non-spam}
    opinion: {positive, negative}
categories
    topic categorization: {sports, science, travel, business,   }
    id51:{bar1, bar2, bar3,    }

    hierarchical vs. flat
    overlapping (soft) vs non-overlapping (hard)

hierarchical classification

image from sch  tze & krisnawati

borges   s classification

the list divides all animals into 14 categories:
   
those that belong to the emperor
   
embalmed ones
   
those that are trained
   
sucking pigs
   
    mermaids (or sirens)
fabulous ones
   
stray dogs
   
those that are included in this classification
   
those that tremble as if they were mad
   
innumerable ones
   
those drawn with a very fine camel hair brush
   
    et cetera
those that have just broken the flower vase
   
those that, at a distance, resemble flies
   

celestial emporium of benevolent knowledge

hand-coded rules

    rules based on combinations of words or other 
features
    spam: black-list-address or (   dollars    and    have been 

selected   )

    accuracy can be high
    but building and maintaining these rules is 

    if rules carefully refined by expert
expensive

supervised machine learning

    a given set of classes c
    given text data x, determine its class y in c

spam recognition

return-path: <*****@rediffmail.com>
x-sieve: cmu sieve 2.2
from: "ibrahim galadima" <*****@rediffmail.com>
reply-to: galadima_esq@netpiper.com
to: *****
subject: gooday

dear sir

funds for investments

this letter may come to you as a surprise since i had no previous 
correspondence with you

i am the chairman tender board of independent national electoral 
commission inec i got your contact in the course of my search for a 
reliable person with whom to handle a very confidential
transaction involving the ! transfer of fund valued at twenty one 
million six hundred thousand united states dollars us$20m to a 
safe foreign account

spamassassin

    http://spamassassin.apache.org/
    http://wiki.apache.org/spamassassin/howscoresareassig
    http://spamassassin.apache.org/tests_3_3_x.html
    example features:

ned

    body 
    body 
    header  date: is 3 to 6 hours before received: date 
    body 
    header 
    header 

incorporates a tracking id number 
html and text parts are different 
html font size is huge 
attempt to obfuscate words in subject: 
subject =~ /^urgent(?:[\s\w]*(dollar) | .{1,40}(?:alert| 

response| assistance| proposal| reply| warning| noti(?:ce| fication)| 
greeting| matter))/i

features for classification

    vector-based

    words:    cat   ,    dog   ,    great   ,    horrible   , etc.
    meta features: document length, author name, 
etc.
    each document (or sentence) is represented as a 
vector in an n-dimensional space
    similar documents appear nearby in the vector 
space

    (more later) 

classification in nlp

    id52
    id31
    id51
    parsing
    id42
    id147
    named entity classification

introduction to nlp

vector space classification

vector space classification

x2

topic2
topic1

x1

14

decision surfaces

x2

topic2
topic1

x1

id90

x2

topic2
topic1

x1

classification using centroids

    centroid
    the point most representative of a class
    compute centroid by finding vector average of 
known class members
    decision boundary is a line that is equidistant 
from two centroids.
    new document on one side of the goes in one 
class; new document on the other side goes in 
the other. 

linear boundary

x2

topic2
topic1

x1

classification using centroids

x2

topic2
topic1
centro 
centroid1

x1

19

introduction to nlp

linear classifiers

linear separators

    two-dimensional line:

w1x1+w2x2=b is the linear separator
w1x1+w2x2>b for the positive class
in n-dimensional spaces:

    "       =                  
(   )*

=    1    1+    2    2+   +            n

   
    one can also add w1=1, x0=b (constant)
    wis the weight vector
    xis the feature vector

decision boundary

x

w

example

bxwt =!!

    bias b=0 (in this example)
    sentence is    a d e h   
    its score will be

0.6*1+0.4*1+0.4*1+(-0.5)*1 = 0.9>0

wi
0.6
0.5
0.5
0.4
0.4
0.3

xi
a
b
c
d
e
f

wi
xi
-0.7 g
-0.5
h
i
-0.3
-0.2
j
k
-0.2
-0.2
l

how to find the linear boundary?
    find the linear boundary = 
find 
    many methods
    id88
    linear least squares

w!

x2

    problem:

    there are infinite number of 
linear boundaries if the two 
classes are linearly separable!
machines (id166)

    maximum margin: support vector 

+
+
+
+
+
+
+ +

+

+
++
+

-

-
-

-
-

-
-
--

-

-

-
-

-
-

x1

general training idea

    go through the training data
    predict the class y (1 or -1)
   
    w(t+1)=wt+yx
    used in the id88

if the prediction is wrong, update w

na  ve bayes

[1, 

    multinomial na  ve bayes is a linear model
x= 
   ]
w=     [log p(y), 

log p(w1|y), 

log p(w2|y)    ]

x1, 

x2

nlp

