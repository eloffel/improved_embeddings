nlp

introduction to nlp

generative vs. discriminative 

models

generative vs. discriminative models
    generative
   

    discriminative
    model posterior id203 
    class is a function of document 

learn a model of the joint 
id203 p(d, c)

    use bayes    rule to calculate p(c|d) 
build a model of each class; given 
   
example, return the model most 
likely to have generated that 
example
examples: na  ve bayes, gaussian 
discriminant analysis, id48, 
probabilistic id18

   

p(c|d) directly
vector
find the exact function that 
minimizes classification errors 
on the training data 
examples: id28, 
neural networks (nns), 
support vector machines 
(id166s), id90

   

   

assumptions of discriminative 

classifiers

    data examples (documents) are represented as 
vectors of features (words, phrases, ngrams, etc)
    looking for a function that maps each vector into a 
class. 
    this function can be found by minimizing the 
errors on the training data (plus other various 
criteria)
like, and how they find the function

    different classifiers vary on what the function looks 

discriminative vs. generative 

classifiers

    discriminative classifiers are generally more 

these features are extracted heuristically

effective, since they directly optimize the 
classification accuracy. but
    they are all sensitive to the choice of features, and so far 
    also, overfitting can happen if data is sparse
    generative classifiers are the    opposite   
    they directly model text, an unnecessarily harder problem 
    they can easily exploit unlabeled data

than classification

introduction to nlp

generative classifier:

na  ve bayes

na  ve bayes intuition

    simple (   na  ve   ) classification method based 
    relies on very simple representation of 

on bayes rule
document
    bag of words

bag of words

i love this movie! it's sweet, 
i love this movie! it's sweet, 
but with satirical humor. the 
but with satirical humor. the 
dialogue is great and the 
dialogue is great and the 
adventure scenes are fun... 
adventure scenes are fun... 
it manages to be whimsical 
it manages to be whimsical 
and romantic while laughing 
and romantic while laughing 
at the conventions of the 
at the conventions of the 
fairy tale genre. i would 
fairy tale genre. i would 
recommend it to just about 
anyone. i've seen it several 
recommend it to just about 
times, and i'm always happy 
anyone. i've seen it several 
to see it again whenever i 
times, and i'm always happy 
have a friend who hasn't 
to see it again whenever i 
seen it yet!
have a friend who hasn't 
seen it yet!

i love this movie! it's sweet, 
but with satirical humor. the 
dialogue is great and the 
adventure scenes are fun... 
it manages to be whimsical 
and romantic while laughing 
at the conventions of the 
fairy tale genre. i would 
recommend it to just about 
anyone. i've seen it several 
times, and i'm always happy 
to see it again whenever i 
have a friend who hasn't 
seen it yet!

it
it
fairy
fairy
it
fairy
love
always
always
love
always
love
to
to
to
it
it
it
it
whimsical
whimsical
it
whimsical
it
i
i
i
areanyone
areanyone
and
and
seen
seen
and
areanyone
seen
friend
friend
dialogue
dialogue
friend
happy
happy
dialogue
happy
recommend
recommend
adventure
adventure
recommend
satirical
satirical
adventure
of
sweet
sweet
of
it
it
who
who
satirical
movie
movie
sweet
of
to
i
to
i
who
it
but
it
it
but
romantic
romantic
movie
i
i
i
to
yet
yet
several
several
it
but
romantic
i
humor
humor
the
again
again
the
yet
it
it
several
the
the
humor
would
would
seen
seen
again
the
it
to
to
scenes
scenes
i
i
manages
the
the
manages
the
would
seen
the
the
fun
times
times
fun
scenes
to
i
i
and
i
and
and
and
the
manages
about
about
the
while
while
whenever
whenever
fun
times
have
have
i
and
and
conventions
conventions
about
while
with
with
whenever
have
conventions
with

it 
i
the
to
and
seen
yet
would
whimsical
times
sweet
satirical
adventure
genre
fairy
humor
have
great
   

it 
6 
6 
it 
i
5
5
i
the
4
4
the
3
3
to
to
3
3
and
and
seen
2
2
seen
yet
1
1
yet
would
1
1
would
whimsical
1
1
whimsical
times
1
1
times
sweet
1
1
1
satirical
1
sweet
1
1
adventure
satirical
genre
1
1
adventure
fairy
1
1
genre
1
humor
1
fairy
have
1
1
humor
great
1
1
have
   
   
   
great
   

remember bayes    rule?
bayes    rule: p(c | d) =

p(d |c)p(c)

p(d)

    d is the document (represented as a list of features 
    cis a class (e.g.,    not spam   )

of a document, x1,    , xn)

na  ve bayes classifier (i)

c   c

cmap = argmax
= argmax
c   c
= argmax

c   c

p(c | d)
p(d |c)p(c)

p(d)

p(d |c)p(c)

map is    maximum a 
posteriori     = most likely 
class

bayes rule

dropping the 
denominator

na  ve bayes classifier (ii)

cmap = argmax

c   c

p(d |c)p(c)

=

argmax

cc
  

xxp
(
,

1

2

,

!

,

x

n

cpc
)|
)(

document d 
represented as 
features x1..xn

but where will we get these probabilitites?

na  ve bayesian classifiers

    na  ve bayesian classifier
(

1

1

|

,

,

(

(

)

)

=

|
,

  

  

f
k

,...

)
)

ffp
2

ffcdp
2

cdpcdf
,...
  
k
fffp
(
    assuming statistical independence
,...2
k
)
  
fp
(

)
    features = words (or phrases) typically

ffcdp
2

(
|
k
  
j

cdpcdfp

f
,...
k

  

  

  

=

k
j

1
=

1
=

(

(

)

,

|

1

1

j

j

)

multinomial na  ve bayes 
independence assumptions

,
    bag	of	words	assumption

xxp
(
,

2

1

x
n!

,

c
)|

    assume	position	doesn   t	matter

    conditional	independence

    assume	the	feature	probabilities	p(xi|c)	are	independent	given	
the	class	c.
xp
c
c
c
)|
)|
(
)|
,
!
1
[jurafsky and martin]

xpcxp
(
(

xpc
)|
(
3

xp
(

)|

!

=

x

   

   

   

   

,

n

n

2

1

multinomial na  ve bayes 

classifier
x
xxp
(
,
,
!

,

2

1

cpc
)|
)(

n

c

map

=

argmax

cc
  

cnb = argmax

c   c

p(cj)

   
x   x

p(x |c)

this is why it   s na  ve!

[jurafsky and martin]

learning the multinomial 

na  ve bayes model

    first	attempt:	maximum	likelihood	estimates

    simply	use	the	frequencies	in	the	data

  p(cj) =
  p(wi |cj) =

doccount(c = cj)

ndoc

count(wi,cj)
count(w,cj)
   
w   v

[jurafsky and martin]

parameter estimation

  p(wi |cj) =

count(wi,cj)
count(w,cj)
   
w   v

fraction	of	times	word	wi appears	

among	all	words	in	documents	of	topic	cj

    create mega-document for topic jby 

concatenating all docs in this topic
    use frequency of win mega-document

[jurafsky and martin]

problem with maximum likelihood

    what if we have seen no training documents with the 

word fantastic and classified in the topic positive
(thumbs-up)?
fantastic"
positive
  

positive
 ,

"(  
p

0  
=

)

count
fantastic"
"(
  )
=   
count

w
,

(

  vw

positive

)

    zero probabilities cannot be conditioned away, no matter 

the other evidence!

c

map

=

argmax

c

)(  
cp

  

i

(  
cxp
)|

i

[jurafsky and martin]

laplace smoothing

  p(wi |c) =

  p(wi |c) =

=

#
%%
$

   
w   v

count(wi,c)
count(w,c)
   
)
(
w   v
count(wi,c) +1
count(w,c) +1
   
(
)
w   v
count(wi,c) +1
&
count(w,c
((  +   v
'

)

[jurafsky and martin]

multinomial na  ve bayes: 

learning

    from training corpus, extract 
vocabulary
    calculate p(cj) terms
    for each cjin cdo

docsj   all docs with  class =cj

p(cj)    

| docsj |

| total # documents|

    calculate p(wk| cj) terms

    textj   single doc containing all 
docsj
    for each word wkin vocabulary

textj

nk   # of occurrences of wkin 
p(wk |cj)    
n +  |vocabulary |

nk +  

[jurafsky and martin]

example
    features = {i hate love this book}
    training

         = 1/2 1/2
    = 1 1 0 1 1
0 0 1 1 1
0
             = 1/4 1/4
1/4 1/4
0
1/3 1/3 1/3
0
        |    	  	1/2  1/4  1/4 1/2  0  1/3 = 1	0
    = 2 2 1 2 2
1 1 2 2 2
             = 2/9 2/9 1/9 2/9 2/9
1/8 1/8 2/8 2/8 2/8
        |    	  	1/2  2/9  2/9 1/2  1/8  2/8 = 0.613	0.387

    i hate this book
    love this book
    what is p(y|x)?
    prior p(y) 
    testing
    hate book
    different conditions
    a = 0 (no smoothing)
    a = 1 (smoothing)

example

         = 1/2 1/2
             = 2/9 2/9 1/9 2/9 2/9
1/8 1/8 2/8 2/8 2/8

        |    	  	1/2  2/9  2/9 1/2  1/8  2/8 = 0.613	0.387

ways naive bayes is not so naive
    very fast, low storage requirements
    robust to irrelevant features
    irrelevant features cancel each other without affecting results
    very good in domains with many equally important features
    id90 suffer from fragmentationin such cases     especially if little data
    optimal if the independence assumptions hold

    if assumed independence is correct, then it is the bayes optimal classifier for 

problem

    a good, dependable baseline for text classification

    but other classifiers give better accuracy

[jurafsky and martin]

nlp

