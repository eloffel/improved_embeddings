nlp

introduction to nlp

id88: discriminative 

classifier

the id88

    a simple but very 
important classifier
    model a neuron
    input excitations
    if excitation > inhibition, 

send an electrical signal out 
the axon

    earliest neural network

    invented in 1957!

id88 idea

input

x0
x1
x2

w0

w1

w2

  

> threshold?

no

yes

-1

1

question: where can we get these weights?

quick reminder: dot products

this equation

3 x
3 x

2 y
2 y

5 z
5 z

can be written as a dot product of two 
vectors: 

7

7

3

2 5

x
y
z

so we can rewrite

as a dot product of two vectors

  

x0
x1
x2

w0

w2

w1

    "    

derivatives and gradients

https://en.wikipedia.org/wiki/gradient_descent

updating parameters

    basic approach

    we want to increase the id203 of the entire data set

;    )

    -./=argmax67                (    =|    =
       	    +	    	                (    )

    gradient ascent
    take the derivative of the log likelihood with respect to the 
    make a little change to the parameters in the direction the 

parameters 
derivative tells you is uphill:
       here is the learning rate

    how much do you want to change each time? 

 
 
id88 algorithm
,

),...,

    

!
yx
1
1

!
x
1

)),

!
x

y

(

,

n

n

input:

y

i

-  

}1,1{

n

,

0

s
((
=
    
h
!
!
w
k
,0
=
=
0
n
i
 to 1
for
 
=
!
!
xwy
     0)
(
 if    
   
i
i
!
!
!
w
w
xy
 
          
=
k
k
i
i
1
+
k
k
  
1
          
+=
end
    
end
kw!

  
+

h

k

algorithm:

output:

[example: chris bishop]

gradient ascent example

    starting value
w= (1,-1,-2,3)
accuracy 66%
    next value
w= (2,-2,-1,2)
accuracy 73%
    next value
w= (3,-2,-1,4)
accuracy 80%

    etc.

id173

500,000)

    penalize large weights (we don   t want weights of 
    l2 regularizer

    .bcd/e=argmax6                 (    =|    =;    )       7(    h)b
    .icd/e=argmax6                 (    =|    =;    )       7|    h|

    l1 regularizer

	

	

 
 
 
 
updating parameters

    (    |    ,    )= exp	(       =    =
)
1+exp	(       =    =
)
            =    (    )	=7                      =

 
 
 
 
 
 
stochastic gradient ascent

    batch mode
    consider each data point for each update of w
    this is slow

    stochastic mode

    update wafter each data point
    id28 stochastic update (p is between 0 and 1)

    id88 stochastic update (y is 0 or 1; approximation for 

    =+                          =
    =+               q    =

lr)

notes

    datasets:

    http://blog.webkid.io/datasets-for-machine-learning/

nlp

