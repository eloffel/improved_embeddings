nlp

introduction to nlp

id28

example with interpolation

    linear interpolation for id38

    estimating the trigram id203 p(c|ab) using pid113(c|a), pid113(c), 
    weights l1, l2, etc.

etc.

    we may want to consider other features
    general idea

    e.g., pos tags of previous words, heads, word endings, etc.

    compute the id155 p(y|x)
    p(y|x) = sum of weights*features
    label y for a given history x in x

id28

input: 

vector and yis the label

    an example of a discriminative classifier
   

    training example pairs of (       ,    ) where         is the feature 
    set of weights      that maximizes likelihood of correct 

    build a model that predicts the id203 of the label

    goal:
    output: 

labels on training examples

id28

    similar to na  ve bayes (but discriminative!)

    log-linear model
    features don   t have to be independent

    examples of features

    anything of use
    linguistic and non-linguistic
    count of    good   
    count of    not good   
    sentence length

classification using lr

    compute the feature vector x
    multiply with weight vector w

    compute the logistic function

    =0    1    1
11+    ./
         =

 
 
examples

    example 1
x= (2,1,1,1)
w= (1,-1,-2,3)
z= 2-1-2+3=2
f(z)= 1/(1+e-2)

    example 2
x= (2,1,0,1)
w= (0,0,-3,0)
z= 0
f(z)= 1/(1+e0) = 1/2

id28

for a pair (    6,    (6)), the optimal weights are 
    7=	argmax=
log    (    6,    (6))
    7=	argmax= 0log    (    6,    (6))
6
function          =0log    (    6,    (6))

if we look at all the examples in our training set, 
the optimal weights are  

so we   d like to maximize a log likelihood 

6

w

x

  

        =    		    )

 
 
nlp

