nlp

introduction to nlp

practical issues in text 

classification

pitfall: overfitting

what happens when your model learns your training data a little too well?

development test sets and cross

-validation

training set

development test set

test set

    metric: p/r/f1  or accuracy
    unseen test set

    avoid overfitting (   tuning to the test set   )
    more conservative estimate of performance

    cross-validation over multiple splits

    handle sampling errors from different datasets

    pool results over each split
    compute pooled dev set performance

training set

dev test

training set

dev test

                         training set
dev test

test set

underflow prevention: log space

    multiplying lots of probabilities can result in floating-point underflow.
   

since log(xy) = log(x) + log(y)
    better to sum logs of probabilities instead of multiplying probabilities.
    class with highest un-normalized log id203 score is still most 

probable.

    model is now just max of sum of weights

no labeled data: manual rules

if (wheat or grain) and not (whole or bread) then

categorize as grain

    need careful crafting 

    human tuning on development data
    time-consuming: 2 days per class

very little data?

    use na  ve bayes

    na  ve bayes is a    high-bias    algorithm (ng and jordan 2002 nips)

    get more labeled data 

    find clever ways to get humans to label data for you

    try semi-supervised training methods:

    id64, em over unlabeled documents,    

7

a reasonable amount of data?

    perfect for all the clever classifiers

    id166
    regularized id28

    you can even use user-interpretable id90

    users like to hack
    management likes quick fixes

8

a huge amount of data?

    can achieve high accuracy!
    at a cost:

    id166s (train time) or knn (test time) can be too slow
    regularized id28 can be somewhat better
    so na  ve bayes can come back into its own again!
    or you can play with deep learning   

9

nlp

