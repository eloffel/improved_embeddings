nlp

introduction to nlp

introduction to deep learning

neural networks

    remember id28?

    input: vector of features
    output: one or more nodes

    trained using sgd
    one-layer lr (maxent model) is a simple neural 

network
    neuron (id88): output is a non-linear combination of the 
inputs

    sample code:

    http://ai.stanford.edu/~ajoulin/code/nn.zip

id88

y

w3

w4

x3

x4

w1

w2

x2

x1

    " = 1  iff s xiwi     b

non-linearities (e.g., sigmoid, tanh)

multi-layer id88

(multi-layer lr)

z

[output layer]

v1

v2

y1

y2

[hidden layer]

w1,1

w2,1

w1,2

w3,1

w4,2

w2,2 w3,2
w4,1

x1

x2

x3

x4

[input layer]

network with a hidden layer

it is a universal function approximator
   
    it can model any continuous function (hornik 1991)
    this is not true for a simpler network

    it cannot learn xor, for example (no one-layer linear classifier 

can)

    the deeper network can represent xor 
    an even deeper network can also represent an 

    as a disjunction of two lower-level features
arbitrary function
    but with fewer nodes

multi-layer id88

x2

x1

neural networks

    remember id28?

    input: vector of features
    output: one or more nodes

    trained using sgd
    one-layer lr (maxent model) is a simple neural 

network
    neuron (id88): output is a non-linearcombination of 

the inputs

non-linearities (e.g., sigmoid, tanh)

deeper networks

    multiple layers

    input layer
    low-level feature layer
    higher-level feature layer
    output layer
intuition
    learning features automatically

   

+

=

example (from lee et al. 2009)

example (from lee et al. 2009)

what is deep learning

    architecture 

    neural networks with multiple layers
    non-linearities (e.g., sigmoid, tanh)

    use

    single-layer nn used as simple classifier
    multi-layer nn can express more complex functions and learn 

complex features

    hot area of research

    very popular these days, especially in speech and vision, but also in 
    works best with high-performance computers using gpus and very 

nlp
large data sets

why deep learning?

    amazing results

    speech: dahl et al. 2010     30% improvement in wer (word error 
    vision: id163 (krizhevsky et al. 2012)     scarily complex 

rate) 
architecture 

    the big players (google, facebook, baidu, microsoft, 
    the hot new thing?

ibm   ) are investing billions in dl

    actually, many of the architectures that we   ll talk about were 
    what   s new is hardware (gpu) that can use these architectures at 

invented in the 1980s and 1990s
scale  

id163 architecture

[krizhevsky et al. 2012]

[krizhevsky et al. 2012]

[clarifai.com demo]

autoencoder

z1

z2

z3

z4

[output layer]

z = x

y1

y2

[hidden layer]

x1

x2

x3

x4

[input layer]

autoencoder

    the size of the hidden layer should be smaller than 

the size of the input layer.
    otherwise the hidden layer will learn the identity mapping

    minimize loss
    denoising autoencoder

    difference between zi and xi
    add some domain-independent noise to the input
    example: rotate or stretch the image
    still try to have the output match the input

    the autoencoder is closely related to svd

quick math review

this equation

3 x
3 x

2 y
2 y

5 z
5 z

can be written as a dot product of two 
vectors: 

7

7

3

2 5

x
y
z

id202

likewise, these equations

can be written as a dot product of a matrixand a 
vector: 

3 x
-1 x
4 x

2 y
3 y

12 y

5 z
9 z
0 z

3

2 5
3 9

-1 12 0

4

x
y
z

7
12
8

7
12
8

supervisedmachine learning

parameters
(things we're 
learning)

w

x

input feature vector

sigmoid or 
other 
nonlinearity

predicted 
value

  

  

supervisedmachine learning

w

update 
parameters

x

  

  

y actual
value

how 
wrong 
were we?

a simplified diagram

w

x

  

y

nlp

