nlp

deep learning

id27s (1/2)

what is the feature vector x?

or word

    typically a vector representation of a single character 
    often reflects the contextin which that word is found
    could just do counts, but that leads to sparse vectors
    commonly used techniques: id97or gloveword 
    https://code.google.com/p/id97/

embeddings
    includes the models and pre-trained embeddings
    pre-trained is good, because training takes a lot of data
    gensim: python library that works with id97
    https://radimrehurek.com/gensim/

embeddings are magic, part 1

vector(   king   ) - vector(   man   ) + vector(   woman   )    
	vector(   queen   )

4

image courtesy of jurafsky & martin

embeddings are magic, part 2

glove vectors for 
comparative and 
superlative 
adjectives

http://nlp.stanford.edu/proj
ects/glove/images/comparat
ive_superlative.jpg

more examples

examples from richard socher

skip-gram and cbow (continuous bag of words) (mikolov et al. 2013, mikolov
cbow et al. 2013a), draw inspiration from the neural methods for id38 intro-
duced in chapter 5. like the neural language models, these models train a network
to predict neighboring words, and while doing so learn dense embeddings for the
words in the training corpus. the advantage of these methods is that they are fast,
ef   cient to train, and easily available online in the id97 package; code and
pretrained embeddings are both available.

skip-grams
    predict each neighboring word 
    in a context window of 2c words 
    from the current word.
    e.g., for c=2, we are given word wt and 
predicting these 4 words:

we   ll begin with the skip-gram model. the skip-gram model predicts each
neighboring word in a context window of 2c words from the current word. so
for a context window c = 2 the context is [wt 2,wt 1,wt+1,wt+2] and we are pre-
dicting each of these from word wt. fig. 17.12 sketches the architecture for a sample

skip-grams learn 2 

embeddings for each w
input embedding v, in the input matrix 
w
    column iof the input matrix w is the 
1  d embedding vifor word iin the 
vocabulary. 

output embedding v   , in output matrix 
w   
    row iof the output matrix w    is a d   
1 vector embedding v   ifor word iin 
the vocabulary.

[jurafsky & martin]

1
2
.
.
d

1
2
.
.
.
.

i

.
.
.
.
|v|

w
i
.
.

1 2

   

|v|

d x  |v|

w   
d   

1 2

 |v| x d

setup

    walking through corpus pointing at word w(t), 
whose index in the vocabulary is j, so we   ll call it 
wj(1 < j < |v|). 
    let   s predict w(t+1), whose index in the 
vocabulary is k (1 < k < |v |). hence our task is to 
compute p(wk|wj). 

slide courtesy of jurafsky & martin

one-hot vectors

    a vector of length |v| 
    example:

    [0,0,0,0,1,0,0,0,0      .0]

cbow and skipgram (mikolov 2013) 

wi-2

wi-1

wi+1

wi+2

s

wi

wi

wi-2

wi-1

wi+1

wi+2

skip-gram
projection layer
embedding for wt

output layer

probabilities of
context words

y1
y2

input layer
1-hot input vector
x1
x2

wt

xj

w

|v|   d

x|v|
1   |v|

1   d

slide courtesy of jurafsky & martin

w    d     |v|

w    d     |v|

yk

wt-1

y|v|
y1
y2

yk

y|v|

wt+1

skip-gram
projection layer
embedding for wt

h = vj

input layer
1-hot input vector
x1
x2

wt

xj

w

|v|   d

x|v|
1   |v|

1   d

output layer

probabilities of
context words

y1
y2

w    d     |v|

w    d     |v|

wt-1
o = w   h

wt+1
o = w   h

yk

y|v|
y1
y2

yk

y|v|

slide courtesy of jurafsky & martin

notes

    sparse vs. dense vectors

    100,000 dimensions vs. 300 dimensions
    <10 non-zero dimensions vs. 300 non-zero dimensions

    dense vectors

    semantic similarity (cf. lsa)

similarity computation

    computed using the dot product of the two 
    to convert a similarity to a id203, use 

vectors
softmax

        %    & = exp	(    %    &)
   exp	(    0    &)
 0

   

in practice, use negative sampling
    too many words in the denominator
    the denominator is only computed for a few words

softmax

evaluating embeddings

    nearest neighbors
    analogies
    (a:b)::(c:?)
information retrieval

   
    semantic hashing

similarity data sets

[table from faruqui et al. 2016]

[mikolov et al. 2013]

semantic hashing

[salakhutdinov and hinton 2007]

wevi (xin rong)

https://ronxin.github.io/wevi/

eat|apple, eat|orange, eat|rice, drink|juice, 
drink|milk, drink|water, orange|juice, apple|juice, 
rice|milk, milk|drink, water|drink, juice|drink

embeddings for word senses

[rothe and schuetze 2015]

non-compositionality

    black cat = black + cat
    black market    black + market

notes

    id27s perform id105 of 
    id97 is a simple feed-forward neural 
    training is done using id26 using 
    negative sampling for training

the co-occurrence matrix
network
sgd

nlp

