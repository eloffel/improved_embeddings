nlp

deep learning

libraries for deep learning

id127 in python

http://stackoverflow.com/questions/10508021/matrix-multiplication-in-python

id127 in numpy

libraries for deep learning

    theano (python): 
    lasagne: 

http://deeplearning.net/software/theano/
    built on top of theano, has pre-designed networks: 

https://github.com/lasagne/lasagne

    torch (lua): 
    tensorflow (python and c++):
    keras

    http://torch.ch/
    https://www.tensorflow.org/

id127 in theano

t

import theano
import theano.tensor as 
import numpy as np
#    symbolic    variables
x = t.matrix('x')
y = t.matrix(   y   )
dot = t.dot(x, y)

id127 in theano

t

import theano
import theano.tensor as 
import numpy as np
#    symbolic    variables
x = t.matrix('x')
y = t.matrix(   y   )
dot = t.dot(x, y)

#this is the slow part
f = theano.function([x,y], 

[dot])

#now we can use this 
function
a = 
np.random.random((2,3)
)
b = 
np.random.random((3,4)
)

sigmoid in theano

in = t.vector(   in   )
sigmoid = 1 / (1 + t.exp(-in))
#same as t.nnet.sigmoid
sigmoid = t.nnet.sigmoid(x)

shared variables vs symbolic variables

# this is symbolic
x = t.matrix('x')
#shared means that it is not symbolic
w = theano.shared(np.random.randn(n))
b = theano.shared(0.)

computational graph

# this is symbolic
x = t.matrix('x')
#shared means that it is not symbolic
w = theano.shared(np.random.randn(n))
b = theano.shared(0.)

# computational graph
p_1 = sigmoid(t.dot(x, w) + b)
xent = -y * t.log(p_1) - (1-y) * t.log(1-p_1) # cross-id178
cost = xent.mean() # the cost to minimize

automatic gradient computation
p_1 = sigmoid(t.dot(x, w) + b)
xent = -y * t.log(p_1) - (1-y) * t.log(1-p_1) # cross-

id178

cost = xent.mean() # the cost to minimize
gw, gb = t.grad(cost, [w, b]) 

compile a function

train = theano.function(

inputs=[x,y],
outputs=[prediction, xent],
updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)))

computation graphs in theano

computation graphs in tensorflow

lstm id31 demo

   

if you   re new to deep learning and want to work 
with theano, do yourself a favor and work 
through http://deeplearning.net/tutorial/
http://deeplearning.net/tutorial/lstm.html
reviews

    a lstm demo is described here: 
    id31 model trained on imdb movie 

lstms: one time step

c0

~
c1

h0

  

x1

f1

i1

+

c1

tanh

o1

h1

[slides from catherine finegan-dollak]

lstms: building a sequence

the

cat

sat

on

   

theano implementation of an lstm 

step

(lstm.py, l. 174) 
def _step(m_, x_, h_, c_):

preact = tensor.dot(h_, tparams[_p(prefix, 'u')])
preact += x_
i = tensor.nnet.sigmoid(_slice(preact, 0, options['dim_proj']))
f = tensor.nnet.sigmoid(_slice(preact, 1, options['dim_proj']))
o = tensor.nnet.sigmoid(_slice(preact, 2, options['dim_proj']))
c = tensor.tanh(_slice(preact, 3, options['dim_proj']))
c = f * c_ + i * c
c = m_[:, none] * c + (1. - m_)[:, none] * c_
h = o * tensor.tanh(c)
h = m_[:, none] * h + (1. - m_)[:, none] * h_
return h, c

   preact    is the sum of wx with the dot 
product of the previous step   s h with 
the weight matrix u; u concatenates 
ui, uf, uo, and uc, for computational 
efficiency; w does the same with all 
the w matrices. then the _slice 
function splits the dot product back out 
again to generate the three gates, i, f, 

and o, and the candidate     ".

m_ is a mask, used for dealing with 
variable-length input.

theano.scan iterates through a 

series of steps

rval, updates = theano.scan(_step,
sequences=[mask, state_below],
outputs_info=[tensor.alloc(numpy_floatx(0.),
n_samples, dim_proj),
tensor.alloc(numpy_floatx(0.),
n_samples, dim_proj)],

name=_p(prefix, '_layers'),
n_steps=nsteps)
(lstm.py, l. 195) 

links about deep learning

    long lists of resources and papers:
    http://www.cs.yale.edu/homes/radev/dlnlp2017.pdf
    http://clair.si.umich.edu/~radev/dl/dl.pdf

    richard socher   s stanford class
    learn theano + deep learning in one tutorial

    http://cs224d.stanford.edu/

    http://deeplearning.net/tutorial/

nlp

