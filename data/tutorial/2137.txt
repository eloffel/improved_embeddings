nlp

deep learning

long short-term memory 

networks (lstm)

lstm motivation

remember how we update an 
id56?

h0

wh

  
wx
x1
the

h1

wh

  
wx
x2
cat

h2

wh

  
wx
x3
sat

cost

y3
softmax
wy

h3

[slides from catherine finegan-dollak]

the vanishing gradient problem

    deep neural networks use 
id26.
    back propagation uses the chain rule.
    the chain rule multiplies derivatives.
    often these derivatives between 0 and 1.
    as the chain gets longer, products get smaller

   

until they disappear.

derivative of sigmoid function

wolfram|alpha

or do they explode?

    with gradients larger than 1,
    you encounter the opposite problem
    with products becoming larger and larger 
    as the chain becomes longer and longer,
    causing overlarge updates to parameters.
    this is the exploding gradient 
problem.

vanishing/exploding gradients 

are bad.

   

if we cannot backpropagate very far through the network, 
the network cannot learn long-term dependencies. 

    my dog [chase/chases] squirrels.

vs.

    my dog, whom i adopted in 2009, [chase/chases] squirrels. 

6

lstm solution

   use memory cell to store information at each time step.
   use    gates    to control the flow of information through 

the network.
   input gate: protect the current step from irrelevant 
   output gate: prevent the current step from passing 
   forget gate: limit information passed from one cell to 

inputs
irrelevant outputs to later steps
the next

transforming id56 to lstm

    "=    (          ")*+    ,    ")

h0

wh

  
wx
x1

u1

transforming id56 to lstm

c0

h0

wh

  
wx
x1

u1

transforming id56 to lstm    "=    "       ")*+    "       "

f1

+

c1

c0

h0

wh

  
wx
x1

u1

i1

transforming id56 to lstm    "=    "       ")*+    "       "

f1

+

c1

c0

h0

wh

  
wx
x1

u1

i1

transforming id56 to lstm    "=    "       ")*+    "       "

f1

+

c1

c0

h0

wh

  
wx
x1

u1

i1

transforming id56 to lstm

    "=    (       3   ")*+    ,3    ")

c0

f1
f1

+

c1

whf

  

h0
h0

wh

  
wx
x1
x1

u1
wxf

i1

transforming id56 to lstm    "=    (       4   ")*+    ,4    ")

f1

+

c1

c0

whi

  

h0
h0

wh

  
wx
x1
x1

u1
wxi

i1
i1

transforming id56 to lstm

   "=    "   tanh    "

c0

h0

wh

  
wx
x1

u1

f1

i1

+

c1

tanh

o1

h1

lstm for sequences

f1

i1

+

c1

tanh

o1

h1

c0

h0

wh

  

wx

u1

x1

the

u2

wh

  

wx

x2

cat

f2

i2

+

c2

tanh

o2

h2

u2

wh

  

wx

x2

sat

f2

i2

+

c2

tanh

o2

h2

lstm applications

http://www.cs.toronto.edu/~graves/handwriting.html

    id46 (gonzalez-dominguez et al., 2014)
    paraphrase detection (cheng & kartsaklis, 2015)
    id103 (graves, abdel-rahman, & hinton, 2013)
    handwriting recognition (graves & schmidhuber, 2009)
    music composition (eck & schmidhuber, 2002) and lyric generation 
    robot control (mayer et al., 2008)
    id86 (wen et al. 2015) (best paper at 
    id39 (hammerton, 2003)

(potash, romanov, & rumshisky, 2015)

emnlp)

related architectures: gru

wh

h0

r1

  1

  
wx
x1

z1

1-z1

+

h1

chung et al. (2014) reports comparable performance to lstm

related architectures: tree lstms

c0

h0

x2

u2

c1

h1

f

i

f

c2

o

h2

tai, socher, manning 2015

external links

    http://colah.github.io/posts/2015-08-

understanding-lstms/

nlp

