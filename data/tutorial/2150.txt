   glove: global vectors for word representation [1]jeffrey pennington,
   [2]richard socher,   [3]christopher d. manning

   introduction

   glove is an unsupervised learning algorithm for obtaining vector
   representations for words. training is performed on aggregated global
   word-word co-occurrence statistics from a corpus, and the resulting
   representations showcase interesting linear substructures of the word
   vector space.

   getting started (code download)
     * download the [4]code (licensed under the [5]apache license, version
       2.0)
     * unpack the files:  unzip glove-1.2.zip
     * compile the source:  cd glove-1.2 && make
     * run the demo script: ./demo.sh
     * consult the included readme for further usage details, or ask a
       [6]question
     * the code is also available [7]on github

   download pre-trained word vectors
     * pre-trained word vectors. this data is made available under the
       [8]public domain dedication and license v1.0 whose full text can be
       found at: [9]http://www.opendatacommons.org/licenses/pddl/1.0/.
          + [10]wikipedia 2014 + [11]gigaword 5 (6b tokens, 400k vocab,
            uncased, 50d, 100d, 200d, & 300d vectors, 822 mb download):
            [12]glove.6b.zip
          + common crawl (42b tokens, 1.9m vocab, uncased, 300d vectors,
            1.75 gb download): [13]glove.42b.300d.zip
          + common crawl (840b tokens, 2.2m vocab, cased, 300d vectors,
            2.03 gb download): [14]glove.840b.300d.zip
          + twitter (2b tweets, 27b tokens, 1.2m vocab, uncased, 25d, 50d,
            100d, & 200d vectors, 1.42 gb download):
            [15]glove.twitter.27b.zip
     * ruby [16]script for preprocessing twitter data

   citing glove

   jeffrey pennington, richard socher, and christopher d. manning. 2014.
   [17]glove: global vectors for word representation. [[18]pdf] [[19]bib]

   highlights

   1.   nearest neighbors

   the euclidean distance (or cosine similarity) between two word vectors
   provides an effective method for measuring the linguistic or semantic
   similarity of the corresponding words. sometimes, the nearest neighbors
   according to this metric reveal rare but relevant words that lie
   outside an average human's vocabulary. for example, here are the
   closest words to the target word frog:
    0. frog
    1. frogs
    2. toad
    3. litoria
    4. leptodactylidae
    5. rana
    6. lizard
    7. eleutherodactylus

   [litoria.jpg]

   3. litoria
   [leptodactylidae.jpg]

   4. leptodactylidae
   [rana.jpg]

   5. rana
   [eleutherodactylus.jpg]

   7. eleutherodactylus

   2.   linear substructures

   the similarity metrics used for nearest neighbor evaluations produce a
   single scalar that quantifies the relatedness of two words. this
   simplicity can be problematic since two given words almost always
   exhibit more intricate relationships than can be captured by a single
   number. for example, man may be regarded as similar to woman in that
   both words describe human beings; on the other hand, the two words are
   often considered opposites since they highlight a primary axis along
   which humans differ from one another.

   in order to capture in a quantitative way the nuance necessary to
   distinguish man from woman, it is necessary for a model to associate
   more than a single number to the word pair. a natural and simple
   candidate for an enlarged set of discriminative numbers is the vector
   difference between the two word vectors. glove is designed in order
   that such vector differences capture as much as possible the meaning
   specified by the juxtaposition of two words.
   [20][man_woman_small.jpg]

   [21]man - woman
   [22][company_ceo_small.jpg]

   [23]company - ceo
   [24][city_zip_small.jpg]

   [25]city - zip code
   [26][comparative_superlative_small.jpg]

   [27]comparative - superlative

   the underlying concept that distinguishes man from woman, i.e. sex or
   gender, may be equivalently specified by various other word pairs, such
   as king and queen or brother and sister. to state this observation
   mathematically, we might expect that the vector differences man -
   woman, king - queen, and brother - sister might all be roughly equal.
   this property and other interesting patterns can be observed in the
   above set of visualizations.

   training

   the glove model is trained on the non-zero entries of a global
   word-word co-occurrence matrix, which tabulates how frequently words
   co-occur with one another in a given corpus. populating this matrix
   requires a single pass through the entire corpus to collect the
   statistics. for large corpora, this pass can be computationally
   expensive, but it is a one-time up-front cost. subsequent training
   iterations are much faster because the number of non-zero matrix
   entries is typically much smaller than the total number of words in the
   corpus.

   the tools provided in this package automate the collection and
   preparation of co-occurrence statistics for input into the model. the
   core training code is separated from these preprocessing steps and can
   be executed independently.

   model overview

   glove is essentially a log-bilinear model with a weighted least-squares
   objective. the main intuition underlying the model is the simple
   observation that ratios of word-word co-occurrence probabilities have
   the potential for encoding some form of meaning. for example, consider
   the co-occurrence probabilities for target words ice and steam with
   various probe words from the vocabulary. here are some actual
   probabilities from a 6 billion word corpus:
   [table.png]

   as one might expect, ice co-occurs more frequently with solid than it
   does with gas, whereas steam co-occurs more frequently with gas than it
   does with solid. both words co-occur with their shared property water
   frequently, and both co-occur with the unrelated word fashion
   infrequently. only in the ratio of probabilities does noise from
   non-discriminative words like water and fashion cancel out, so that
   large values (much greater than 1) correlate well with properties
   specific to ice, and small values (much less than 1) correlate well
   with properties specific of steam. in this way, the ratio of
   probabilities encodes some crude form of meaning associated with the
   abstract concept of thermodynamic phase.
   the training objective of glove is to learn word vectors such that
   their dot product equals the logarithm of the words' id203 of
   co-occurrence. owing to the fact that the logarithm of a ratio equals
   the difference of logarithms, this objective associates (the logarithm
   of) ratios of co-occurrence probabilities with vector differences in
   the word vector space. because these ratios can encode some form of
   meaning, this information gets encoded as vector differences as well.
   for this reason, the resulting word vectors perform very well on word
   analogy tasks, such as those examined in the [28]id97 package.

   visualization

   glove produces word vectors with a marked banded structure that is
   evident upon visualization:
   [29][word_vectors_small.jpg]

   the horizontal bands result from the fact that the multiplicative
   interactions in the model occur component-wise. while there are
   additive interactions resulting from a dot product, in general there is
   little room for the individual dimensions to cross-pollinate.

   the horizontal bands become more pronounced as the word frequency
   increases. indeed, there are noticeable long-range trends as a function
   of word frequency, and they are unlikely to have a linguistic origin.
   this feature is not unique to glove -- in fact, i'm unaware of any
   model for word vector learning that avoids this issue.

   the vertical bands, such as the one around word 230k-233k, are due to
   local densities of related words (usually numbers) that happen to have
   similar frequencies.

   release history
     * [30]glove v.1.2: minor bug fixes in code (memory, off-by-one,
       errors). eval code now also available in python and octave. utf-8
       encoding of largest data file fixed. prepared by russell stewart
       and christopher manning. oct 2015.
     * [31]glove v.1.0: original release. prepared by jeffrey pennington.
       aug 2014.

   bugs/issues/discussion

   github: glove is [32]on github. for bug reports and patches, you're
   best off using the github issues and pull requests features.

   google group: the google group [33]globalvectors can be used for
   questions and general discussion on glove.

   jeffrey pennington | august 2014

   site design courtesy of [34]jason chuang

references

   visible links
   1. http://stanford.edu/~jpennin
   2. http://www.socher.org/
   3. http://nlp.stanford.edu/~manning
   4. https://nlp.stanford.edu/software/glove-1.2.zip
   5. http://www.apache.org/licenses/license-2.0
   6. https://nlp.stanford.edu/projects/glove/#discuss
   7. https://github.com/stanfordnlp/glove
   8. http://opendatacommons.org/licenses/pddl/
   9. http://www.opendatacommons.org/licenses/pddl/1.0/
  10. http://dumps.wikimedia.org/enwiki/20140102/
  11. https://catalog.ldc.upenn.edu/ldc2011t07
  12. http://nlp.stanford.edu/data/glove.6b.zip
  13. http://nlp.stanford.edu/data/glove.42b.300d.zip
  14. http://nlp.stanford.edu/data/glove.840b.300d.zip
  15. http://nlp.stanford.edu/data/glove.twitter.27b.zip
  16. https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb
  17. https://nlp.stanford.edu/pubs/glove.pdf
  18. https://nlp.stanford.edu/pubs/glove.pdf
  19. https://nlp.stanford.edu/pubs/glove.bib
  20. https://nlp.stanford.edu/projects/glove/images/man_woman.jpg
  21. https://nlp.stanford.edu/projects/glove/images/man_woman.jpg
  22. https://nlp.stanford.edu/projects/glove/images/company_ceo.jpg
  23. https://nlp.stanford.edu/projects/glove/images/man_woman.jpg
  24. https://nlp.stanford.edu/projects/glove/images/city_zip.jpg
  25. https://nlp.stanford.edu/projects/glove/images/man_woman.jpg
  26. https://nlp.stanford.edu/projects/glove/images/comparative_superlative.jpg
  27. https://nlp.stanford.edu/projects/glove/images/comparative_superlative.jpg
  28. http://code.google.com/p/id97/
  29. https://nlp.stanford.edu/projects/glove/images/word_vectors.jpg
  30. https://nlp.stanford.edu/software/glove-1.2.zip
  31. https://nlp.stanford.edu/software/glove-1.0.tar.gz
  32. https://github.com/stanfordnlp/glove
  33. https://groups.google.com/forum/#!forum/globalvectors
  34. http://jason.chuang.info/

   hidden links:
  36. http://www.linkedin.com/in/jpennin
  37. https://www.facebook.com/jeffrey.s.pennington
