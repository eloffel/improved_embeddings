   #[1]the morning paper    feed [2]the morning paper    comments feed
   [3]the morning paper    the amazing power of word vectors comments feed
   [4]id163 classification with deep convolutional neural networks
   [5]glove: global vectors for word representation [6]alternate
   [7]alternate [8]the morning paper [9]wordpress.com

   [10]skip to content

   [11]the morning paper

an interesting/influential/important paper from the world of cs every weekday
morning, as selected by adrian colyer

     * [12]home
     * [13]about
     * [14]infoq qr editions
     * [15]subscribe
     * [16]privacy

the amazing power of word vectors

   april 21, 2016
   tags: [17]google, [18]machine learning

   for today   s post, i   ve drawn material not just from one paper, but from
   five! the subject matter is    id97        the work of mikolov et al. at
   google on efficient vector representations of words (and what you can
   do with them). the papers are:
     * [19]efficient estimation of word representations in vector space    
       mikolov et al. 2013
     * [20]distributed representations of words and phrases and their
       compositionality     mikolov et al. 2013
     * [21]linguistic regularities in continuous space word
       representations     mikolov et al. 2013
     * [22]id97 parameter learning explained     rong 2014
     * [23]id97 explained: deriving mikolov et al   s negative sampling
       word-embedding method     goldberg and levy 2014

   from the first of these papers (   efficient estimation      ) we get a
   description of the continuous bag-of-words and continuous skip-gram
   models for learning word vectors (we   ll talk about what a word vector
   is in a moment   ). from the second paper we get more illustrations of
   the power of word vectors, some additional information on optimisations
   for the skip-gram model (hierarchical softmax and negative sampling),
   and a discussion of applying word vectors to phrases. the third paper
   (   linguistic regularities      ) describes vector-oriented reasoning based
   on word vectors and introduces the famous    king     man + woman = queen   
   example. the last two papers give a more detailed explanation of some
   of the very concisely expressed ideas in the milokov papers.

   check out the [24]id97 implementation on google code.

what is a word vector?

   at one level, it   s simply a vector of weights. in a simple 1-of-n (or
      one-hot   ) encoding every element in the vector is associated with a
   word in the vocabulary. the encoding of a given word is simply the
   vector in which the corresponding element is set to one, and all other
   elements are zero.

   suppose our vocabulary has only five words: king, queen, man, woman,
   and child. we could encode the word    queen    as:

   using such an encoding, there   s no meaningful comparison we can make
   between word vectors other than equality testing.

   in id97, a distributed representation of a word is used. take a
   vector with several hundred dimensions (say 1000). each word is
   representated by a distribution of weights across those elements. so
   instead of a one-to-one mapping between an element in the vector and a
   word, the representation of a word is spread across all of the elements
   in the vector, and each element in the vector contributes to the
   definition of many words.

   if i label the dimensions in a hypothetical word vector (there are no
   such pre-assigned labels in the algorithm of course), it might look a
   bit like this:

   such a vector comes to represent in some abstract way the    meaning    of
   a word. and as we   ll see next, simply by examining a large corpus it   s
   possible to learn word vectors that are able to capture the
   relationships between words in a surprisingly expressive way. we can
   also use the vectors as inputs to a neural network.

reasoning with word vectors

     we find that the learned word representations in fact capture
     meaningful syntactic and semantic regularities in a very simple way.
     specifically, the regularities are observed as constant vector
     offsets between pairs of words sharing a particular relationship.
     for example, if we denote the vector for word i as x[i], and focus
     on the singular/plural relation, we observe that x[apple]    
     x[apples]    x[car]     x[cars], x[family]     x[families]     x[car]    
     x[cars], and so on. perhaps more surprisingly, we find that this is
     also the case for a variety of semantic relations, as measured by
     the semeval 2012 task of measuring relation similarity.

   the vectors are very good at answering analogy questions of the form a
   is to b as c is to ?. for example, man is to woman as uncle is to ?
   (aunt) using a simple vector offset method based on cosine distance.

   for example, here are vector offsets for three word pairs illustrating
   the gender relation:

   and here we see the singular plural relation:

   this kind of vector composition also lets us answer    king     man + woman
   = ?    question and arrive at the result    queen    ! all of which is truly
   remarkable when you think that all of this knowledge simply comes from
   looking at lots of word in context (as we   ll see soon) with no other
   information provided about their semantics.

     somewhat surprisingly, it was found that similarity of word
     representations goes beyond simple syntatic regularities. using a
     word offset technique where simple algebraic operations are
     performed on the word vectors, it was shown for example that
     vector(   king   )     vector(   man   ) + vector(   woman   ) results in a vector
     that is closest to the vector representation of the word queen.

   vectors for king, man, queen, & woman:

   the result of the vector composition king     man + woman = ?

   here are some more results achieved using the same technique:

   here   s what the country-capital city relationship looks like in a
   2-dimensional pca projection:

   here are some more examples of the    a is to b as c is to ?    style
   questions answered by word vectors:

   we can also use element-wise addition of vector elements to ask
   questions such as    german + airlines    and by looking at the closest
   tokens to the composite vector come up with impressive answers:

     word vectors with such semantic relationships could be used to
     improve many existing nlp applications, such as machine translation,
     information retrieval and id53 systems, and may enable
     other future applications yet to be invented.

   the semantic-syntatic word relationship tests for understanding of a
   wide variety of relationships as shown below. using 640-dimensional
   word vectors, a skip-gram trained model achieved 55% semantic accuracy
   and 59% syntatic accuracy.

learning word vectors

   mikolov et al. weren   t the first to use continuous vector
   representations of words, but they did show how to reduce the
   computational complexity of learning such representations     making it
   practical to learn high dimensional word vectors on a large amount of
   data. for example,    we have used a google news corpus for training the
   word vectors. this corpus contains about 6b tokens. we have restricted
   the vocabulary size to the 1 million most frequent words      

   the complexity in neural network language models (feedforward or
   recurrent) comes from the non-linear hidden layer(s).

     while this is what makes neural networks so attractive, we decided
     to explore simpler models that might not be able to represent the
     data as precisely as neural networks, but can posssible be trained
     on much more data efficiently.

   two new architectures are proposed: a continuous bag-of-words model,
   and a continuous skip-gram model. let   s look at the continuous
   bag-of-words (cbow) model first.

   consider a piece of prose such as    the recently introduced continuous
   skip-gram model is an efficient method for learning high-quality
   distributed vector representations that capture a large number of
   precises syntatic and semantic word relationships.    imagine a sliding
   window over the text, that includes the central word currently in
   focus, together with the four words and precede it, and the four words
   that follow it:

   the context words form the input layer. each word is encoded in one-hot
   form, so if the vocabulary size is v these will be v-dimensional
   vectors with just one of the elements set to one, and the rest all
   zeros. there is a single hidden layer and an output layer.

   the training objective is to maximize the id155 of
   observing the actual output word (the focus word) given the input
   context words, with regard to the weights. in our example, given the
   input (   an   ,    efficient   ,    method   ,    for   ,    high   ,    quality   ,
      distributed   ,    vector   ) we want to maximize the id203 of getting
      learning    as the output.

   since our input vectors are one-hot, multiplying an input vector by the
   weight matrix w[1] amounts to simply selecting a row from w[1].

   given c input word vectors, the activation function for the hidden
   layer h amounts to simply summing the corresponding    hot    rows in w[1],
   and dividing by c to take their average.

     this implies that the link (activation) function of the hidden layer
     units is simply linear (i.e., directly passing its weighted sum of
     inputs to the next layer).

   from the hidden layer to the output layer, the second weight matrix
   w[2]can be used to compute a score for each word in the vocabulary, and
   softmax can be used to obtain the posterior distribution of words.

   the skip-gram model is the opposite of the cbow model. it is
   constructed with the focus word as the single input vector, and the
   target context words are now at the output layer:

   the activation function for the hidden layer simply amounts to copying
   the corresponding row from the weights matrix w[1] (linear) as we saw
   before. at the output layer, we now output c multinomial distributions
   instead of just one. the training objective is to mimimize the summed
   prediction error across all context words in the output layer. in our
   example, the input would be    learning   , and we hope to see (   an   ,
      efficient   ,    method   ,    for   ,    high   ,    quality   ,    distributed   ,
      vector   ) at the output layer.

optimisations

   having to update every output word vector for every word in a training
   instance is very expensive   .

     to solve this problem, an intuition is to limit the number of output
     vectors that must be updated per training instance. one elegant
     approach to achieving this is hierarchical softmax; another approach
     is through sampling.

   hierarchical softmax uses a binary tree to represent all words in the
   vocabulary. the words themselves are leaves in the tree. for each leaf,
   there exists a unique path from the root to the leaf, and this path is
   used to estimate the id203 of the word represented by the leaf.
      we define this id203 as the id203 of a random walk
   starting from the root ending at the leaf in question.   

     the main advantage is that instead of evaluating v output nodes in
     the neural network to obtain the id203 distribution, it is
     needed to evaluate only about log[2](v) words    in our work we use a
     binary huffman tree, as it assigns short codes to the frequent words
     which results in fast training.

   negative sampling is simply the idea that we only update a sample of
   output words per iteration. the target output word should be kept in
   the sample and gets updated, and we add to this a few (non-target)
   words as negative samples.    a probabilistic distribution is needed for
   the sampling process, and it can be arbitrarily chosen    one can
   determine a good distribution empirically.   

   mikolov et al. also use a simple subsampling approach to counter the
   imbalance between rare and frequent words in the training set (for
   example,    in   ,    the   , and    a    provide less information value than rare
   words). each word in the training set is discarded with id203
   p(w[i]) where

   f(w[i]) is the frequency of word w[i] and t is a chosen threshold,
   typically around 10^-5.

share this:

     * [25]twitter
     * [26]linkedin
     * [27]email
     * [28]print
     *

like this:

   like loading...

related

   from     [29]machine learning, [30]uncategorized
   [31]    id163 classification with deep convolutional neural networks
   [32]glove: global vectors for word representation    
   167 comments [33]leave one    
    1. sp [34]permalink
       april 21, 2016 5:10 pm
       nice concise overview. thanks!
       [35]reply
    2. johan zing [36]permalink
       april 22, 2016 8:46 am
       nice article, and a great explanation of id97! i   d just like to
       point out that in    linguistic regularities in continuous space word
       representations   , the word vectors are learned using a recursive nn
       (as opposed to the feed forward architecture of cbow and
       skip-gram). one again, the vector representations are taken from
       the weight matrix between input and hidden layer.
       [37]reply
    3. [38]@feedly [39]permalink
       may 31, 2016 10:28 pm
       very nice overview. thanks! -edwin
       [40]reply
    4. zteve [41]permalink
       november 4, 2016 10:53 am
       hi adrian,
       just noticed a few formatting problems in the last paragraph. i   m
       sure you can fix    em easily.
       [42]reply
          + [43]adriancolyer [44]permalink*
            november 5, 2016 12:57 pm
            fixed, thank you!
            [45]reply
    5. saurav [46]permalink
       december 30, 2016 8:01 pm
       probably the best explanation i found on the internet.
       cleared all my doubts!
       [47]reply
    6. rong [48]permalink
       january 5, 2017 4:14 pm
       hi, when using negative sampling,the nn   s loss isn   t
          2-p(d=1|w,t)+p(d=0|wi,t)   ? i mean the input-label as a input,not
       for computer loss as usual,is that correct ?
       [49]reply
    7. [50]kirill kovalewsky [51]permalink
       january 9, 2017 9:20 pm
       thank fort this overview
       [52]reply
    8. godson [53]permalink
       january 12, 2017 11:26 am
       awesome    great blog
       [54]reply
    9. irma_ravkic [55]permalink
       january 19, 2017 1:08 am
       very nice, illustrative and clear.
       [56]reply
   10. varun mishra [57]permalink
       february 6, 2017 2:53 pm
       very well explained     
       [58]reply
   11. [59]jenner little [60]permalink
       march 3, 2017 8:58 am
       what mikolov and google based their work on:
       [61]https://www.kaggle.com/c/id97-nlp-tutorial/discussion/12349
       [62]reply
   12. [63]allan wind [64]permalink
       march 3, 2017 5:19 pm
       the link for    linguistic regularities in continuous space word
       representations    is not found (404).
       [65]reply
          + [66]adriancolyer [67]permalink*
            march 4, 2017 10:43 am
            updated the link, thank you!
            [68]reply
   13. [69]david parks [70]permalink
       april 20, 2017 3:36 am
       fabulous article! what did you use to produce the hand sketch
       drawings? they   re fabulous.
       [71]reply
          + [72]adriancolyer [73]permalink*
            april 20, 2017 7:11 am
            thank you! the sketch drawings are done    by hand    using an app
            called notability on an ipad pro with the apple pencil.
            [74]reply
   14. [75]makar [76]permalink
       april 24, 2017 3:31 am
       hi, well written. could you explain (in cbow), from where the
       weight matrix (v x n) is coming from? how are we calculating this
       matrix?
       [77]reply
   15. xiaotong xu [78]permalink
       june 30, 2017 6:30 am
       is there any info about lookup table and weight matrix?i have read
       a lot of papers and block about id97, but few about lookup
       table or weight matrix.thx
       [79]reply
          + xiaotong xu [80]permalink
            june 30, 2017 7:13 am
            sorry for forgetting to describe the [lookup table] and
            [weight matrix]
            lookup table: w1v*n in your paper.
            weight matrix:w2n*v in your paper.
            [81]reply
   16. mpq [82]permalink
       july 1, 2017 4:51 am
       i really want to know how we can obtain c different output matrixes
       of skip-gram? the hidden layer is the same(size(n * 1)), w2 is the
       not the same?
       [83]reply
   17. repzz [84]permalink
       july 7, 2017 2:30 pm
       thank you for the interesting paper! just wanted to notice that
       there is a mistake in the mikolov   s name in the last paragraph.
       best regrds!
       [85]reply
          + [86]adriancolyer [87]permalink*
            july 8, 2017 8:14 am
            fixed, thank you!
            [88]reply
               o nikhil goel [89]permalink
                 august 4, 2017 4:10 pm
                 exceptional explanation to a complex topic.. best article
                 read ever on w2vec
   18. abhi [90]permalink
       september 18, 2017 3:45 pm
       thanks
       [91]reply
   19. miguel [92]permalink
       november 8, 2017 12:14 pm
       thanks for the intuitive description of id97, and the cbow and
       skip-gram models!      it would have been nice if you had tied the two
       concepts together at the end. it is not clear from your post how
       the output of the cbow/skip-gram neural network is converted into a
       single, lower dimensional vector representation of a given word.
       [93]reply
   20. [94]radek skowron [95]permalink
       february 24, 2018 6:38 pm
       i have found this article thanks to seo by the sea. frankly, i
       didn   t know about    word vectors    and that   s why i was quite
       interested how it   s actually works. your article sheds some light
       about the thing, thanks!
       [96]reply
   21. mt [97]permalink
       february 27, 2018 6:03 pm
       syntactic not syntatic      otherwise great post!
       [98]reply
   22. jon m [99]permalink
       april 30, 2018 10:47 am
       excellent overview. filled in some gaps. thanks
       [100]reply
   23. nisar khan [101]permalink
       october 16, 2018 5:46 pm
       hi adrian,
       instead of the    gender    relation analogy example { man:woman}
       implying { uncle:aunt, king:queen} , it would be good if we can
       instead have {uncle:aunt } implying { king:queen, grandpa:grandma }
       and the relation as    married   . because {man: woman}    gender   
       analogy can also imply {uncle:grandma, king:princess} etc.,
       best regards.
       [102]reply
   24. [103]tomcircle [104]permalink
       december 10, 2018 5:43 pm
       reblogged this on [105]math online tom circle.
       [106]reply
   25. ignatius ezeani [107]permalink
       january 29, 2019 12:54 pm
       thanks for this extremely interesting post on word vectors. i
       assume this isn   t a problem but please can i    officially    request
       for the permission to use the images here in my thesis with due
       author recognition.
       [108]reply
          + [109]adriancolyer [110]permalink*
            january 30, 2019 12:59 pm
            you are very welcome to! thanks, a.
            [111]reply
   26. [112]alex souza [113]permalink
       february 28, 2019 7:31 pm
       reblogged this on [114]alex souza.
       [115]reply
   27. [116]gordonhutchison [117]permalink
       march 13, 2019 12:24 pm
       thanks for the great article adrian, the best introduction to
       id97 i have read so far.
       a tiny typo in    looking at lots of word in context         does not
       prevent it from being an outstandingly good piece of technical
       writing.
       [118]reply

trackbacks

    1. [119]big analytics roundup (april 25, 2016) | the big analytics
       blog
    2. [120]2016/04/25 ml reddit     cuponthetop
    3. [121]distributed representations of sentences and documents | the
       morning paper
    4. [122]sequence to sequence learning with neural networks | the
       morning paper
    5. [123]building end-to-end dialogue systems using generative
       hierarchical neural network models | the morning paper
    6. [124]natural language understanding (almost) from scratch | the
       morning paper
    7. [125]end of term, and the power of compound interest | the morning
       paper
    8. [126]id27     badripatro
    9. [127]glove: global vectors for word representation     quyv
   10. [128]distributed representations of sentences and documents     quyv
   11. [129]id97     learning the meaning behind words | one-tech-a-day
   12. [130]deep neural networks for youtube recommendations | the morning
       paper
   13. [131]cognitionx data science, ai and machine learning briefing
       issue 21 - cognitionx
   14. [132]word vectors | what i learned
   15. [133]achieving human parity in conversational id103 |
       the morning paper
   16. [134]so that was 2016 | the morning paper
   17. [135]word vector representation: id97 & glove
   18. [136]the brain   s registers | pointers gone wild
   19. [137]teaching machines to read emails: feature selection | stacks
       and q's
   20. [138]&quot;king     man + woman = ?&quot; | cs130 journals
   21. [139]   king     man + woman = ?    | cs130 journals
   22. [140]understanding, generalisation, and id21 in deep
       neural networks | the morning paper
   23. [141]unsupervised learning and gans | the morning paper
   24. [142]the amazing power of word vectors     thoughts   
   25. [143]word vectors (id97) | ace infoway
   26. [144]glove: global vectors for word representation | the morning
       paper
   27. [145]cooking receipts (i) | the data explorer
   28. [146]word (thought) vectors | delightful & distinctive colrs
   29. [147]pupper2vec: analysing internet dog slang with machine learning
       | gutterstats
   30. [148]which id27s to use?     ift6266     h2017 deep learning
   31. [149]id27s     piano finish standard
   32. [150]using id27 to enable semantic queries on relational
       databases | the morning paper
   33. [151]word vectors for non-nlp data and research people     data
       flume.
   34. [152]topic classification     bridging topic modelling and text
       classification     understandling blog
   35. [153]the (non-)sense of word vectors (1/2)     understandling blog
   36. [154]150        ml   nlp     python                 | hello word !
   37. [155]accelerating innovation through analogy mining | the morning
       paper
   38. [156]struc2vec: learning node representations from structural
       identity | the morning paper
   39. [157]chamber of secrets: teaching a machine what congress cares
       about | the reader magazine
   40. [158]chamber of secrets: teaching a machine what congress cares
       about | radio free
   41. [159]fascinating application: teaching a machine what members of
       congress care about | later on
   42. [160]machine learning overview     machine learning for mathies
   43. [161]teaching machines to read emails: feature selection     stacks &
       q's
   44. [162]                                                             |                
   45. [163]convolutional neural networks for text classification    
       artificial intelligence and parrots
   46. [164]150        ml   nlp     python                 -          blog      
   47. [165]how intent classification works - mr. bot
   48. [166]understanding intents classification - mr. bot
   49. [167]episode 3: id27s     new paradigm
   50. [168]building a basic fatwa chat bot - fine tuned
   51. [169]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe | techcrunch
   52. [170]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe     the huffington global
   53. [171]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe | aquaiver it solutions (pvt) ltd
   54. [172]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe     cheyyur
   55. [173]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - technology news
   56. [174]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe                                                                
   57. [175]how facebook's new way of classifying what you write may speed
       feature rollouts across the globe | bakhabar
   58. [176]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe | kwotable
   59. [177]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - dailyscene.com |
   60. [178]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - data science tidings
   61. [179]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe     snoopy tech
   62. [180]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe | techdom.tk
   63. [181]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe
   64. [182]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe     my cell phone blog
   65. [183]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe | techlear
   66. [184]how facebook's new way of classifying what you write may speed
       feature rollouts across the globe | admk agency
   67. [185]how facebook's new way of classifying what you write may speed
       feature rollouts across the globe | news and stuff
   68. [186]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe     magickafe.com
   69. [187]how facebook's new way of classifying what you write may speed
       feature rollouts across the globe | greenground it
   70. [188]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - versatile blogger
   71. [189]how facebook's new way of classifying what you write may speed
       feature rollouts across the globe | indiblog
   72. [190]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe | bullviral.com
   73. [191]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe     internetwork .online
   74. [192]how facebook's new way of classifying what you write may speed
       feature rollouts across the globe | hash virals
   75. [193]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - tech news hub
   76. [194]how facebook's new way of classifying what you write may speed
       feature rollouts across the globe | viral buff
   77. [195]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - domains4change
   78. [196]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - cosmo peek
   79. [197]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe     thetechworld
   80. [198]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - tech scenes
   81. [199]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - tech news finder
   82. [200]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe > futuretechnologynews 2018
   83. [201]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe     inix zone
   84. [202]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe | my tech news today
   85. [203]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe | future technology news 2018
   86. [204]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - digit cool
   87. [205]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - tech news project
   88. [206]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - technewssites
   89. [207]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe    mytechnewstoday
   90. [208]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe | cafenews
   91. [209]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe     the tech world
   92. [210]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - tech world
   93. [211]   bookr        how facebook   s new way of classifying what you write
       may speed feature rollouts across the globe | bookr ..     bookr
   94. [212]how facebook's new way of classifying what you write may speed
       feature rollouts across the globe | digitpol
   95. [213]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe     the engineering of conscious
       experience
   96. [214]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - webdnet
   97. [215]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - gas news
   98. [216]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe     kopitiam bot
   99. [217]how facebook   s new way of classifying what you write may speed
       feature rollouts across the globe - tech life
   100. [218]how facebooks new way of classifying what you write may speed
       feature rollouts across the globe - packaging design services -
       powered by interdigitel
   101. [219]how facebooks new way of classifying what you write may speed
       feature rollouts across the globe | technewzz
   102. [220]how facebook   s new way of classifying what you write may
       speed feature rollouts across the globe | dawnhost
   103. [221]learning meaningful location embeddings from unlabeled visits
       | sentiance
   104. [222]       |                2018               150                           nlp   python      -            
   105. [223]get busy with id27s     an introduction | shane lynn
   106. [224]dynamic id27s for evolving semantic discovery | the
       morning paper
   107. [225]word vectors: the foundation of natural language processing
       (nlp)
   108. [226]vecteurs de mots et traitement automatique du langage naturel
       (taln) | master caweb
   109. [227]game of missuggestions: semantic analysis of search
       autocomplete manipulation | the morning paper
   110. [228]id97                  gensim              represent
   111. [229]150        ml   nlp     python                 - caaslglobal
   112. [230]understanding id27s     towards machine learning
   113. [231]deep code search | the morning paper
   114. [232]hexbyte hacker news computers deep code search | hexbyte inc.
   115. [233]                   (nlp)                           
   116. [234]                                                                                           - pansci          
   117. [235]                                                                                               my blog
   118. [236]                                                                                           - dropblog
   119. [237]getting started with word vectors     site title
   120. [238]over 200 of the best machine learning, nlp, and python
       tutorials     2018 edition - coding videos
   121. [239]data science resources     themenyouwanttobe
   122. [240]data science resources     mohit sharma (themenyouwanttobe)    
       medium - coding videos
   123. [241]artificial intelligence: links and resources (81) | angel
          java    lopez on blog
   124. [242]cdl2018
   125. [243]semantics, meet data science: graphdb adds support for data
       wrangling and similarity search     cdl2018
   126. [244]the id97 algorithm     site title
   127. [245]the id97 algorithm - data science central
   128. [246]in math we trust: quantum computing, ai and blockchain     the
       future of it | math online tom circle
   129. [247]data science in international development. part i: working
       with text - ai+ news
   130. [248]data science in international development. part i: working
       with text     data science austria
   131. [249][                       ] bert     state-of-the-art                                     
       104             .                                      bert                          google colab    
       it                         
   132. [250][                       ] bert     state-of-the-art                                     
       104             .                                      bert                          google colab    
       chepa website
   133. [251]2019                   semantic seo,                          |                         

leave a reply [252]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [253]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [254]log out /
   [255]change )
   google photo

   you are commenting using your google account. ( [256]log out /
   [257]change )
   twitter picture

   you are commenting using your twitter account. ( [258]log out /
   [259]change )
   facebook photo

   you are commenting using your facebook account. ( [260]log out /
   [261]change )
   [262]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

   this site uses akismet to reduce spam. [263]learn how your comment data
   is processed.
     * subscribe

                       [264][mail-new-icon.png?w=600]
      never miss an issue! the morning paper delivered straight to your
                                   inbox.
     * search
       ____________________
     * archives archives [select month________]
     * most read in the last few days
          + [265]amazon aurora: design considerations for high throughput
            cloud-native id208
          + [266]ginseng: keeping secrets in registers when you distrust
            the operating system
          + [267]establishing software root of trust unconditionally
          + [268]amazon aurora: on avoiding distributed consensus for
            i/os, commits, and membership changes
          + [269]calvin: fast distributed transactions for partitioned
            database systems
          + [270]the amazing power of word vectors
          + [271]on the criteria to be used in decomposing systems into
            modules
          + [272]neural ordinary differential equations
          + [273]applying the universal scalability law to organisations
          + [274]programming paradigms for dummies: what every programmer
            should know
     * rss
          + [275]rss - posts
          + [276]rss - comments
     * live on twitter[277]my tweets

   [278]blog at wordpress.com.

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [279]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [280]likes-master

   %d bloggers like this:

references

   visible links
   1. https://blog.acolyer.org/feed/
   2. https://blog.acolyer.org/comments/feed/
   3. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/feed/
   4. https://blog.acolyer.org/2016/04/20/id163-classification-with-deep-convolutional-neural-networks/
   5. https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/&for=wpcom-auto-discovery
   8. https://blog.acolyer.org/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#content
  11. https://blog.acolyer.org/
  12. https://blog.acolyer.org/
  13. https://blog.acolyer.org/about/
  14. https://blog.acolyer.org/infoq-quarterly-review-editions/
  15. https://blog.acolyer.org/email-subs/
  16. https://blog.acolyer.org/privacy/
  17. https://blog.acolyer.org/tag/google/
  18. https://blog.acolyer.org/tag/machine-learning/
  19. http://arxiv.org/pdf/1301.3781.pdf
  20. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  21. http://www.aclweb.org/anthology/n13-1090
  22. http://arxiv.org/pdf/1411.2738v3.pdf
  23. http://arxiv.org/pdf/1402.3722v1.pdf
  24. https://code.google.com/archive/p/id97
  25. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?share=twitter
  26. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?share=linkedin
  27. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?share=email
  28. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#print
  29. https://blog.acolyer.org/category/machine-learning/
  30. https://blog.acolyer.org/category/uncategorized/
  31. https://blog.acolyer.org/2016/04/20/id163-classification-with-deep-convolutional-neural-networks/
  32. https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/
  33. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#respond
  34. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-6948
  35. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=6948#respond
  36. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-6951
  37. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=6951#respond
  38. http://blog.feedly.com/
  39. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-7267
  40. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=7267#respond
  41. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-8647
  42. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=8647#respond
  43. https://adriancolyer.wordpress.com/
  44. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-8659
  45. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=8659#respond
  46. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-9269
  47. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=9269#respond
  48. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-9341
  49. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=9341#respond
  50. https://www.facebook.com/app_scoped_user_id/10206106535728267/
  51. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-9412
  52. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=9412#respond
  53. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-9455
  54. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=9455#respond
  55. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-9534
  56. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=9534#respond
  57. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-9851
  58. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=9851#respond
  59. https://www.facebook.com/app_scoped_user_id/931771996860123/
  60. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-10290
  61. https://www.kaggle.com/c/id97-nlp-tutorial/discussion/12349
  62. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=10290#respond
  63. https://yaxto.com/
  64. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-10303
  65. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=10303#respond
  66. https://adriancolyer.wordpress.com/
  67. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-10313
  68. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=10313#respond
  69. https://www.facebook.com/app_scoped_user_id/10155181175096064/
  70. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-11068
  71. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=11068#respond
  72. https://adriancolyer.wordpress.com/
  73. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-11072
  74. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=11072#respond
  75. http://gravatar.com/kartikeyagupta4
  76. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-11114
  77. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=11114#respond
  78. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-12782
  79. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=12782#respond
  80. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-12783
  81. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=12783#respond
  82. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-12800
  83. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=12800#respond
  84. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-12939
  85. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=12939#respond
  86. https://adriancolyer.wordpress.com/
  87. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-12959
  88. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=12959#respond
  89. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-13724
  90. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-15580
  91. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=15580#respond
  92. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-17836
  93. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=17836#respond
  94. http://radekskowron.pl/
  95. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-21580
  96. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=21580#respond
  97. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-21655
  98. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=21655#respond
  99. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-24107
 100. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=24107#respond
 101. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-32861
 102. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=32861#respond
 103. http://tomcircle.wordpress.com/
 104. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-37565
 105. https://tomcircle.wordpress.com/2018/12/11/the-amazing-power-of-word-vectors/
 106. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=37565#respond
 107. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-46526
 108. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=46526#respond
 109. https://adriancolyer.wordpress.com/
 110. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-46572
 111. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=46572#respond
 112. http://pessoalex.wordpress.com/
 113. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-49808
 114. https://pessoalex.wordpress.com/2019/02/28/the-amazing-power-of-word-vectors/
 115. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=49808#respond
 116. http://erasureheads.wordpress.com/
 117. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-51345
 118. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/?replytocom=51345#respond
 119. http://thomaswdinsmore.com/2016/04/25/big-analytics-roundup-april-25-2016/
 120. https://cuponthetop.wordpress.com/2016/04/26/20160425-ml-reddit/
 121. https://blog.acolyer.org/2016/06/01/distributed-representations-of-sentences-and-documents/
 122. https://blog.acolyer.org/2016/06/02/sequence-to-sequence-learning-with-neural-networks/
 123. https://blog.acolyer.org/2016/07/01/building-end-to-end-dialogue-systems-using-generative-hierarchical-neural-network-models/
 124. https://blog.acolyer.org/2016/07/04/natural-language-understanding-almost-from-scratch/
 125. https://blog.acolyer.org/2016/07/22/end-of-term-and-the-power-of-compound-interest/
 126. https://badripatro.wordpress.com/2016/08/20/word-embedding/
 127. https://quyv.wordpress.com/2016/08/24/glove-global-vectors-for-word-representation/
 128. https://quyv.wordpress.com/2016/08/30/distributed-representations-of-sentences-and-documents/
 129. https://onetechaday.wordpress.com/2016/09/17/id97-learning-the-meaning-behind-words/
 130. https://blog.acolyer.org/2016/09/19/deep-neural-networks-for-youtube-recommendations/
 131. http://cxcommdev.savvydigitalmedia.com/2016/08/08/cognitionx-data-science-ai-machine-learning-briefing-issue-21/
 132. https://allilearned.wordpress.com/2016/09/23/word-vectors/
 133. https://blog.acolyer.org/2016/11/22/achieving-human-parity-in-conversational-speech-recognition/
 134. https://blog.acolyer.org/2016/12/19/so-that-was-2016/
 135. http://socs.binus.ac.id/2016/12/22/word-vector-representation-id97-glove/
 136. http://pointersgonewild.com/2016/12/22/the-brains-registers/
 137. https://www.qualtrics.com/eng/teaching-machines-read-emails/
 138. https://cs130x20162.wordpress.com/2017/01/29/king-man-woman/
 139. https://cs130x20162.wordpress.com/2017/01/29/king-man-woman-2/
 140. https://blog.acolyer.org/2017/02/27/understanding-generalisation-and-transfer-learning-in-deep-neural-networks/
 141. https://blog.acolyer.org/2017/03/02/unsupervised-learning-and-gans/
 142. http://irrlab.com/2017/03/02/the-amazing-power-of-word-vectors/
 143. http://zerozone.com/mmp/aceinfoway/word-vectors-id97-/
 144. https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/
 145. https://thedataexplorer.wordpress.com/2017/03/20/cooking-receipts-i/
 146. https://ddcolrs.wordpress.com/2017/04/09/word-thought-vectors/
 147. http://gutterstats.com/2017/04/16/pupper2vec-analysing-internet-dog-slang-with-machine-learning/
 148. https://pulkit6266dl.wordpress.com/2017/04/25/which-word-vectors-to-use/
 149. https://pianofinish.wordpress.com/2017/05/17/first-blog-post/
 150. https://blog.acolyer.org/2017/07/06/using-word-embedding-to-enable-semantic-queries-on-relational-databases/
 151. https://dataflume.wordpress.com/2017/07/11/word-vectors-for-non-nlp-data-and-research-people/
 152. https://blog.understandling.com/?p=34
 153. https://blog.understandling.com/?p=41
 154. http://www.bibiqiqi.com/archives/171
 155. https://blog.acolyer.org/2017/09/14/accelerating-innovation-through-analogy-mining/
 156. https://blog.acolyer.org/2017/09/15/struc2vec-learning-node-representations-from-structural-identity/
 157. http://www.reader.us/chamber-of-secrets-teaching-a-machine-what-congress-cares-about/
 158. https://www.radiofree.org/us/chamber-of-secrets-teaching-a-machine-what-congress-cares-about/
 159. https://leisureguy.wordpress.com/2017/10/07/fascinating-application-teaching-a-machine-what-members-of-congress-care-about/
 160. http://www.nullplug.org/ml-blog/2017/09/26/machine-learning-overview/
 161. https://wordpressstaging.qualtrics.com/eng/teaching-machines-read-emails/
 162. http://www.shellsec.com/news/49580.html
 163. https://smartparrot.wordpress.com/2017/11/18/convolutional-neural-networks-for-text-classification/
 164. http://www.eyiyie.com/res/blog/?p=327
 165. http://mrbot.ai/blog/nlu-understand-intent-classification/
 166. http://mrbot.ai/blog/natural-language-processing/understanding-intent-classification/
 167. https://mungingdata.wordpress.com/2018/01/15/episode-3-word-embeddings/
 168. https://omarito.me/building-a-basic-fatwa-chat-bot/
 169. http://techcrunch.com/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 170. http://huffington-global.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 171. https://aquaiver.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 172. http://www.cheyyur.com/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 173. https://whiteeyes.design/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 174. https://tech.alawalsite.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 175. https://bakhabar.tv/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 176. http://kwotable.com/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 177. https://www.dailyscene.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 178. http://datasciencetidings.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 179. http://snoopytech.com/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 180. http://techdom.tk/tc/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 181. http://www.toptechnicalsolutions.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 182. http://mycellphoneblog.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 183. http://www.techlear.com/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 184. http://www.admkagency.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 185. https://www.what-is-new.info/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 186. http://magickafe.com/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 187. https://greenground.it/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 188. http://versatileblogger.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 189. https://indiblog.blog/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 190. http://www.bullviral.com/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 191. http://internetwork.online/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 192. http://hashvirals.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 193. http://www.technewshub.com/social/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 194. http://viralbuff.co/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 195. http://domains4change.com/social/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 196. http://cosmopeek.com/social/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 197. http://www.thetechworld.info/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 198. http://www.techscenes.com/social/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 199. http://technewsfinder.com/social/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 200. http://www.futuretechnologynews.info/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 201. http://www.inixzone.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 202. http://mytechnewstoday.org/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 203. http://futuretechnologynews.xyz/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 204. http://technologyrat.com/social/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 205. http://technewsproject.com/social/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 206. http://technewssites.info/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 207. http://www.mytechnewstoday.xyz/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 208. http://cafenews.in/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 209. http://thetechworld.xyz/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 210. https://tech-world.ga/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 211. https://bookr.vip/bookr-how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe-bookr/
 212. https://digitpol.info/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 213. http://www.theengineeringofconsciousexperience.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 214. http://webdnet.com/2018/01/24/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 215. http://www.gasnews.ga/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 216. http://kopitiambot.com/2018/01/25/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 217. http://techlife.gigfa.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 218. http://packagingdesignservices.com/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 219. http://www.technewzz.com/2018/01/26/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 220. http://www.dawnhost.net/facebook/how-facebooks-new-way-of-classifying-what-you-write-may-speed-feature-rollouts-across-the-globe/
 221. http://www.sentiance.com/2018/01/29/learning-meaningful-location-embeddings-from-unlabeled-visits/
 222. http://www.shixunkuaibao.com/?p=1121680
 223. https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/
 224. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/
 225. https://mastercaweb.u-strasbg.fr/word-vectors-nlp/?lang=en
 226. https://mastercaweb.u-strasbg.fr/vecteurs-mots-taln/
 227. https://blog.acolyer.org/2018/03/14/game-of-missuggestions-semantic-analysis-of-search-autocomplete-manipulation/
 228. http://www.sly-ali.cn/wordpress/index.php/2018/04/25/id97-gensim/
 229. http://caaslglobal.com/               /150-      -ml   nlp-   -python-               /
 230. http://towardsml.com/2018/06/12/understanding-word-embeddings/
 231. https://blog.acolyer.org/2018/06/26/deep-code-search/
 232. http://hexbyteinc.com/hexbyte-hacker-news-computers-deep-code-search/
 233. http://research.sinica.edu.tw/nlp-natural-language-processing-chinese-knowledge-information/
 234. https://pansci.asia/archives/144346
 235. https://www.pes2018.club/blog/2018/07/13/                                                     
 236. http://dropblog.2013.duckla.com/2018/07/13/                                                    %
 237. https://funwithdatascience.wordpress.com/2018/07/16/getting-started-with-word-embedding/
 238. https://codingvideos.net/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition/
 239. https://themenyouwanttobe.wordpress.com/2018/08/20/data-science-resources/
 240. https://codingvideos.net/data-science-resources-mohit-sharma-themenyouwanttobe-medium/
 241. https://ajlopez.wordpress.com/2018/10/21/artificial-intelligence-links-and-resources-81/
 242. http://connected-data.london/2018/10/22/8175/
 243. http://connected-data.london/2018/10/22/semantics-meet-data-science-graphdb-adds-support-for-data-wrangling-and-similarity-search/
 244. https://ryanmitchum.wordpress.com/2018/11/29/the-id97-algorithm/
 245. https://www.datasciencecentral.com/xn/detail/6448529:blogpost:781508
 246. https://tomcircle.wordpress.com/2018/12/10/in-math-we-trust-quantum-computing-ai-and-blockchain-the-future-of-it/
 247. https://ainews.spxbot.com/2019/01/09/data-science-in-international-development-part-i-working-with-text/
 248. https://data-science-austria.at/2019/01/09/data-science-in-international-development-part-i-working-with-text/
 249. http://rustat-it.ru/2019/01/21/    -                  -bert-state-of-the-art-                -          
 250. http://chepa.net/all/2019/01/21/    -                  -bert-state-of-the-art-                -         %b
 251. http://www.imnini.com/650.html
 252. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#respond
 253. https://gravatar.com/site/signup/
 254. javascript:highlandercomments.doexternallogout( 'wordpress' );
 255. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
 256. javascript:highlandercomments.doexternallogout( 'googleplus' );
 257. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
 258. javascript:highlandercomments.doexternallogout( 'twitter' );
 259. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
 260. javascript:highlandercomments.doexternallogout( 'facebook' );
 261. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
 262. javascript:highlandercomments.cancelexternalwindow();
 263. https://akismet.com/privacy/
 264. http://eepurl.com/bbzpm9
 265. https://blog.acolyer.org/2019/03/25/amazon-aurora-design-considerations-for-high-throughput-cloud-native-relational-databases/
 266. https://blog.acolyer.org/2019/04/05/ginseng-keeping-secrets-in-registers-when-you-distrust-the-operating-system/
 267. https://blog.acolyer.org/2019/04/03/establishing-software-root-of-trust-unconditionally/
 268. https://blog.acolyer.org/2019/03/27/amazon-aurora-on-avoiding-distributed-consensus-for-i-os-commits-and-membership-changes/
 269. https://blog.acolyer.org/2019/03/29/calvin-fast-distributed-transactions-for-partitioned-database-systems/
 270. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
 271. https://blog.acolyer.org/2016/09/05/on-the-criteria-to-be-used-in-decomposing-systems-into-modules/
 272. https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/
 273. https://blog.acolyer.org/2015/04/29/applying-the-universal-scalability-law-to-organisations/
 274. https://blog.acolyer.org/2019/01/25/programming-paradigms-for-dummies-what-every-programmer-should-know/
 275. https://blog.acolyer.org/feed/
 276. https://blog.acolyer.org/comments/feed/
 277. https://twitter.com/519408925733425153
 278. https://wordpress.com/?ref=footer_blog
 279. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#cancel
 280. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 282. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-form-guest
 283. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-form-load-service:wordpress.com
 284. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-form-load-service:twitter
 285. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/#comment-form-load-service:facebook
