   #[1]tensorflow

   (button)
   [2]tensorflow
     *

   [3]install [4]learn
     * [5]introduction
       new to tensorflow?
     * [6]tensorflow
       the core open source ml library
     * [7]for javascript
       tensorflow.js for ml using javascript
     * [8]for mobile & iot
       tensorflow lite for mobile and embedded devices
     * [9]for production
       tensorflow extended for end-to-end ml components
     * [10]swift for tensorflow (in beta)

   [11]api
     * api r1
     * [12]r1.13 (stable)
     * [13]r1.12
     * [14]r1.11
     * [15]r1.10
     * [16]r1.9
     * [17]more   

     * api r2
     * [18]r2.0 (preview)

   [19]resources
     * [20]models & datasets
       pre-trained models and datasets built by google and the community
     * [21]tools
       ecosystem of tools to help you use tensorflow
     * [22]libraries & extensions
       libraries and extensions built on tensorflow

   [23]community [24]why tensorflow
     * [25]about
     * [26]case studies

   ____________________
   (button)
   (button)
   [27]github
     * [28]tensorflow core

   [29]overview [30]tutorials [31]guide [32]tf 2.0 alpha

   (button)
     * [33]install
     * [34]learn
          + more
          + [35]overview
          + [36]tutorials
          + [37]guide
          + [38]tf 2.0 alpha
     * [39]api
          + more
     * [40]resources
          + more
     * [41]community
     * [42]why tensorflow
          + more
     * [43]github

     * [44]get started with tensorflow
     * learn and use ml
          + [45]overview
          + [46]basic classification
          + [47]text classification
          + [48]regression
          + [49]overfitting and underfitting
          + [50]save and restore models
     * research and experimentation
          + [51]overview
          + [52]eager execution
          + [53]automatic differentiation
          + [54]custom training: basics
          + [55]custom layers
          + [56]custom training: walkthrough
     * ml at production scale
          + [57]linear model with estimators
          + [58]wide and deep learning
          + [59]boosted trees
          + [60]boosted trees model understanding
          + [61]build a id98 using estimators
     * generative models
          + [62]translation with attention
          + [63]image captioning
          + [64]dcgan
          + [65]vae
     * images
          + [66]image recognition
          + [67]pix2pix
          + [68]neural style transfer
          + [69]image segmentation
          + [70]advanced id98
     * sequences
          + [71]text generation with an id56
          + [72]recurrent neural network
          + [73]drawing classification
          + [74]simple audio recognition
          + [75]id4
     * load data
          + [76]load images
          + [77]tfrecords and tf.example
     * data representation
          + [78]vector representations of words
          + [79]kernel methods
          + [80]large-scale linear models
          + [81]unicode
     * non-ml
          + [82]mandelbrot set
          + [83]partial differential equations

     * [84]introduction
     * [85]tensorflow
     * [86]for javascript
     * [87]for mobile & iot
     * [88]for production
     * [89]swift for tensorflow (in beta)

     * api r1
     * [90]r1.13 (stable)
     * [91]r1.12
     * [92]r1.11
     * [93]r1.10
     * [94]r1.9
     * [95]more   
     * api r2
     * [96]r2.0 (preview)

     * [97]models & datasets
     * [98]tools
     * [99]libraries & extensions

     * [100]about
     * [101]case studies

   watch talks from the 2019 tensorflow dev summit [102]watch now
     * [103]tensorflow
     * [104]learn
     * [105]tensorflow core
     * [106]tutorials

vector representations of words

   in this tutorial we look at the id97 model by [107]mikolov et al.
   this model is used for learning vector representations of words, called
   "id27s".

highlights

   this tutorial is meant to highlight the interesting, substantive parts
   of building a id97 model in tensorflow.
     * we start by giving the motivation for why we would want to
       represent words as vectors.
     * we look at the intuition behind the model and how it is trained
       (with a splash of math for good measure).
     * we also show a simple implementation of the model in tensorflow.
     * finally, we look at ways to make the naive version scale better.

   we walk through the code later during the tutorial, but if you'd prefer
   to dive straight in, feel free to look at the minimalistic
   implementation in
   [108]tensorflow/examples/tutorials/id97/id97_basic.py this
   basic example contains the code needed to download some data, train on
   it a bit and visualize the result. once you get comfortable with
   reading and running the basic version, you can graduate to
   [109]models/tutorials/embedding/id97.py which is a more serious
   implementation that showcases some more advanced tensorflow principles
   about how to efficiently use threads to move data into a text model,
   how to checkpoint during training, etc.

   but first, let's look at why we would want to learn id27s in
   the first place. feel free to skip this section if you're an embedding
   pro and you'd just like to get your hands dirty with the details.

motivation: why learn id27s?

   image and audio processing systems work with rich, high-dimensional
   datasets encoded as vectors of the individual raw pixel-intensities for
   image data, or e.g. power spectral density coefficients for audio data.
   for tasks like object or id103 we know that all the
   information required to successfully perform the task is encoded in the
   data (because humans can perform these tasks from the raw data).
   however, natural language processing systems traditionally treat words
   as discrete atomic symbols, and therefore 'cat' may be represented as
    37 and 'dog' as  3. these encodings are arbitrary, and provide no
   useful information to the system regarding the relationships that may
   exist between the individual symbols. this means that the model can
   leverage very little of what it has learned about 'cats' when it is
   processing data about 'dogs' (such that they are both animals,
   four-legged, pets, etc.). representing words as unique, discrete ids
   furthermore leads to data sparsity, and usually means that we may need
   more data in order to successfully train statistical models. using
   vector representations can overcome some of these obstacles.

   [110]vector space models (vsms) represent (embed) words in a continuous
   vector space where semantically similar words are mapped to nearby
   points ('are embedded nearby each other'). vsms have a long, rich
   history in nlp, but all methods depend in some way or another on the
   [111]distributional hypothesis, which states that words that appear in
   the same contexts share semantic meaning. the different approaches that
   leverage this principle can be divided into two categories: count-based
   methods (e.g. [112]latent semantic analysis), and predictive methods
   (e.g. [113]neural probabilistic language models).

   this distinction is elaborated in much more detail by [114]baroni et
   al., but in a nutshell: count-based methods compute the statistics of
   how often some word co-occurs with its neighbor words in a large text
   corpus, and then map these count-statistics down to a small, dense
   vector for each word. predictive models directly try to predict a word
   from its neighbors in terms of learned small, dense embedding vectors
   (considered parameters of the model).

   id97 is a particularly computationally-efficient predictive model
   for learning id27s from raw text. it comes in two flavors,
   the continuous bag-of-words model (cbow) and the skip-gram model
   (section 3.1 and 3.2 in [115]mikolov et al.). algorithmically, these
   models are similar, except that cbow predicts target words (e.g. 'mat')
   from source context words ('the cat sits on the'), while the skip-gram
   does the inverse and predicts source context-words from the target
   words. this inversion might seem like an arbitrary choice, but
   statistically it has the effect that cbow smoothes over a lot of the
   distributional information (by treating an entire context as one
   observation). for the most part, this turns out to be a useful thing
   for smaller datasets. however, skip-gram treats each context-target
   pair as a new observation, and this tends to do better when we have
   larger datasets. we will focus on the skip-gram model in the rest of
   this tutorial.

scaling up with noise-contrastive training

   neural probabilistic language models are traditionally trained using
   the [116]maximum likelihood (ml) principle to maximize the id203
   of the next word \(w_t\) (for "target") given the previous words \(h\)
   (for "history") in terms of a [117]softmax function,
   $$ \begin{align} p(w_t | h) &= \text{softmax} (\text{score} (w_t, h))
   \\ &= \frac{\exp \{ \text{score} (w_t, h) \} } {\sum_\text{word w' in
   vocab} \exp \{ \text{score} (w', h) \} } \end{align} $$

   where \(\text{score} (w_t, h)\) computes the compatibility of word
   \(w_t\) with the context \(h\) (a dot product is commonly used). we
   train this model by maximizing its [118]log-likelihood on the training
   set, i.e. by maximizing
   $$ \begin{align} j_\text{ml} &= \log p(w_t | h) \\ &= \text{score}
   (w_t, h) - \log \left( \sum_\text{word w' in vocab} \exp \{
   \text{score} (w', h) \} \right). \end{align} $$

   this yields a properly normalized probabilistic model for language
   modeling. however this is very expensive, because we need to compute
   and normalize each id203 using the score for all other \(v\)
   words \(w'\) in the current context \(h\), at every training step.

   on the other hand, for id171 in id97 we do not need a
   full probabilistic model. the cbow and skip-gram models are instead
   trained using a binary classification objective ([119]logistic
   regression) to discriminate the real target words \(w_t\) from \(k\)
   imaginary (noise) words \(\tilde w\), in the same context. we
   illustrate this below for a cbow model. for skip-gram the direction is
   simply inverted.

   mathematically, the objective (for each example) is to maximize
   $$j_\text{neg} = \log q_\theta(d=1 |w_t, h) + k
   \mathop{\mathbb{e}}_{\tilde w \sim p_\text{noise}} \left[ \log
   q_\theta(d = 0 |\tilde w, h) \right]$$

   where \(q_\theta(d=1 | w, h)\) is the binary id28
   id203 under the model of seeing the word \(w\) in the context
   \(h\) in the dataset \(d\), calculated in terms of the learned
   embedding vectors \(\theta\). in practice we approximate the
   expectation by drawing \(k\) contrastive words from the noise
   distribution (i.e. we compute a [120]monte carlo average).

   this objective is maximized when the model assigns high probabilities
   to the real words, and low probabilities to noise words. technically,
   this is called [121]negative sampling, and there is good mathematical
   motivation for using this id168: the updates it proposes
   approximate the updates of the softmax function in the limit. but
   computationally it is especially appealing because computing the loss
   function now scales only with the number of noise words that we select
   (\(k\)), and not all words in the vocabulary (\(v\)). this makes it
   much faster to train. we will actually make use of the very similar
   [122]noise-contrastive estimation (nce) loss, for which tensorflow has
   a handy helper function tf.nn.nce_loss().

   let's get an intuitive feel for how this would work in practice!

the skip-gram model

   as an example, let's consider the dataset

   the quick brown fox jumped over the lazy dog

   we first form a dataset of words and the contexts in which they appear.
   we could define 'context' in any way that makes sense, and in fact
   people have looked at syntactic contexts (i.e. the syntactic dependents
   of the current target word, see e.g. [123]levy et al.),
   words-to-the-left of the target, words-to-the-right of the target, etc.
   for now, let's stick to the vanilla definition and define 'context' as
   the window of words to the left and to the right of a target word.
   using a window size of 1, we then have the dataset

   ([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox),
   ...

   of (context, target) pairs. recall that skip-gram inverts contexts and
   targets, and tries to predict each context word from its target word,
   so the task becomes to predict 'the' and 'brown' from 'quick', 'quick'
   and 'fox' from 'brown', etc. therefore our dataset becomes

   (quick, the), (quick, brown), (brown, quick), (brown, fox), ...

   of (input, output) pairs. the objective function is defined over the
   entire dataset, but we typically optimize this with [124]stochastic
   id119 (sgd) using one example at a time (or a 'minibatch' of
   batch_size examples, where typically 16 <= batch_size <= 512). so let's
   look at one step of this process.

   let's imagine at training step \(t\) we observe the first training case
   above, where the goal is to predict the from quick. we select num_noise
   number of noisy (contrastive) examples by drawing from some noise
   distribution, typically the unigram distribution, \(p(w)\). for
   simplicity let's say num_noise=1 and we select sheep as a noisy
   example. next we compute the loss for this pair of observed and noisy
   examples, i.e. the objective at time step \(t\) becomes
   $$j^{(t)}_\text{neg} = \log q_\theta(d=1 | \text{the, quick}) +
   \log(q_\theta(d=0 | \text{sheep, quick}))$$

   the goal is to make an update to the embedding parameters \(\theta\) to
   improve (in this case, maximize) this objective function. we do this by
   deriving the gradient of the loss with respect to the embedding
   parameters \(\theta\), i.e. \(\frac{\partial}{\partial \theta}
   j_\text{neg}\) (luckily tensorflow provides easy helper functions for
   doing this!). we then perform an update to the embeddings by taking a
   small step in the direction of the gradient. when this process is
   repeated over the entire training set, this has the effect of 'moving'
   the embedding vectors around for each word until the model is
   successful at discriminating real words from noise words.

   we can visualize the learned vectors by projecting them down to 2
   dimensions using for instance something like the [125]id167
   id84 technique. when we inspect these
   visualizations it becomes apparent that the vectors capture some
   general, and in fact quite useful, semantic information about words and
   their relationships to one another. it was very interesting when we
   first discovered that certain directions in the induced vector space
   specialize towards certain semantic relationships, e.g. male-female,
   verb tense and even country-capital relationships between words, as
   illustrated in the figure below (see also for example [126]mikolov et
   al., 2013).

   this explains why these vectors are also useful as features for many
   canonical nlp prediction tasks, such as part-of-speech tagging or named
   entity recognition (see for example the original work by [127]collobert
   et al., 2011 ([128]pdf), or follow-up work by [129]turian et al.,
   2010).

   but for now, let's just use them to draw pretty pictures!

building the graph

   this is all about embeddings, so let's define our embedding matrix.
   this is just a big random matrix to start. we'll initialize the values
   to be uniform in the unit cube.
embeddings = tf.variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))

   the noise-contrastive estimation loss is defined in terms of a logistic
   regression model. for this, we need to define the weights and biases
   for each word in the vocabulary (also called the output weights as
   opposed to the input embeddings). so let's define that.
nce_weights = tf.variable(
  tf.truncated_normal([vocabulary_size, embedding_size],
                      stddev=1.0 / math.sqrt(embedding_size)))
nce_biases = tf.variable(tf.zeros([vocabulary_size]))

   now that we have the parameters in place, we can define our skip-gram
   model graph. for simplicity, let's suppose we've already integerized
   our text corpus with a vocabulary so that each word is represented as
   an integer (see
   [130]tensorflow/examples/tutorials/id97/id97_basic.py for the
   details). the skip-gram model takes two inputs. one is a batch full of
   integers representing the source context words, the other is for the
   target words. let's create placeholder nodes for these inputs, so that
   we can feed in data later.
# placeholders for inputs
train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])

   now what we need to do is look up the vector for each of the source
   words in the batch. tensorflow has handy helpers that make this easy.
embed = tf.nn.embedding_lookup(embeddings, train_inputs)

   ok, now that we have the embeddings for each word, we'd like to try to
   predict the target word using the noise-contrastive training objective.
# compute the nce loss, using a sample of the negative labels each time.
loss = tf.reduce_mean(
  tf.nn.nce_loss(weights=nce_weights,
                 biases=nce_biases,
                 labels=train_labels,
                 inputs=embed,
                 num_sampled=num_sampled,
                 num_classes=vocabulary_size))

   now that we have a loss node, we need to add the nodes required to
   compute gradients and update the parameters, etc. for this we will use
   stochastic id119, and tensorflow has handy helpers to make
   this easy as well.
# we use the sgd optimizer.
optimizer = tf.train.gradientdescentoptimizer(learning_rate=1.0).minimize(loss)

training the model

   training the model is then as simple as using a feed_dict to push data
   into the placeholders and calling [131]tf.session.run with this new
   data in a loop.
for inputs, labels in generate_batch(...):
  feed_dict = {train_inputs: inputs, train_labels: labels}
  _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)

   see the full example code in
   [132]tensorflow/examples/tutorials/id97/id97_basic.py.

visualizing the learned embeddings

   after training has finished we can visualize the learned embeddings
   using id167.

   et voila! as expected, words that are similar end up id91 nearby
   each other. for a more heavyweight implementation of id97 that
   showcases more of the advanced features of tensorflow, see the
   implementation in [133]models/tutorials/embedding/id97.py.

evaluating embeddings: analogical reasoning

   embeddings are useful for a wide variety of prediction tasks in nlp.
   short of training a full-blown part-of-speech model or named-entity
   model, one simple way to evaluate embeddings is to directly use them to
   predict syntactic and semantic relationships like king is to queen as
   father is to ?. this is called analogical reasoning and the task was
   introduced by [134]mikolov and colleagues . download the dataset for
   this task from [135]download.tensorflow.org.

   to see how we do this evaluation, have a look at the build_eval_graph()
   and eval() functions in [136]models/tutorials/embedding/id97.py.

   the choice of hyperparameters can strongly influence the accuracy on
   this task. to achieve state-of-the-art performance on this task
   requires training over a very large dataset, carefully tuning the
   hyperparameters and making use of tricks like subsampling the data,
   which is out of the scope of this tutorial.

optimizing the implementation

   our vanilla implementation showcases the flexibility of tensorflow. for
   example, changing the training objective is as simple as swapping out
   the call to tf.nn.nce_loss() for an off-the-shelf alternative such as
   tf.nn.sampled_softmax_loss(). if you have a new idea for a loss
   function, you can manually write an expression for the new objective in
   tensorflow and let the optimizer compute its derivatives. this
   flexibility is invaluable in the exploratory phase of machine learning
   model development, where we are trying out several different ideas and
   iterating quickly.

   once you have a model structure you're satisfied with, it may be worth
   optimizing your implementation to run more efficiently (and cover more
   data in less time). for example, the naive code we used in this
   tutorial would suffer compromised speed because we use python for
   reading and feeding data items -- each of which require very little
   work on the tensorflow back-end. if you find your model is seriously
   bottlenecked on input data, you may want to implement a custom data
   reader for your problem, as described in [137]new data formats. for the
   case of skip-gram modeling, we've actually already done this for you as
   an example in [138]models/tutorials/embedding/id97.py.

   if your model is no longer i/o bound but you want still more
   performance, you can take things further by writing your own tensorflow
   ops, as described in [139]adding a new op. again we've provided an
   example of this for the skip-gram case
   [140]models/tutorials/embedding/id97_optimized.py. feel free to
   benchmark these against each other to measure performance improvements
   at each stage.

conclusion

   in this tutorial we covered the id97 model, a computationally
   efficient model for learning id27s. we motivated why
   embeddings are useful, discussed efficient training techniques and
   showed how to implement all of this in tensorflow. overall, we hope
   that this has show-cased how tensorflow affords you the flexibility you
   need for early experimentation, and the control you later need for
   bespoke optimized implementation.

   except as otherwise noted, the content of this page is licensed under
   the [141]creative commons attribution 3.0 license, and code samples are
   licensed under the [142]apache 2.0 license. for details, see the
   [143]google developers site policies. java is a registered trademark of
   oracle and/or its affiliates.

     * stay connected
          + [144]blog
          + [145]github
          + [146]twitter
          + [147]youtube
     * support
          + [148]issue tracker
          + [149]release notes
          + [150]stack overflow
          + [151]brand guidelines

     * [152]terms
     * [153]privacy

   [english_____]

references

   visible links
   1. https://www.tensorflow.org/s/opensearch.xml
   2. https://www.tensorflow.org/
   3. https://www.tensorflow.org/install
   4. https://www.tensorflow.org/learn
   5. https://www.tensorflow.org/learn
   6. https://www.tensorflow.org/overview
   7. https://www.tensorflow.org/js
   8. https://www.tensorflow.org/lite
   9. https://www.tensorflow.org/tfx
  10. https://www.tensorflow.org/swift
  11. https://www.tensorflow.org/api_docs/python/tf
  12. https://www.tensorflow.org/api_docs/python/tf
  13. https://www.tensorflow.org/versions/r1.12/api_docs/python/tf
  14. https://www.tensorflow.org/versions/r1.11/api_docs/python/tf
  15. https://www.tensorflow.org/versions/r1.10/api_docs/python/tf
  16. https://www.tensorflow.org/versions/r1.9/api_docs/python/tf
  17. https://www.tensorflow.org/versions
  18. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf
  19. https://www.tensorflow.org/resources/models-datasets
  20. https://www.tensorflow.org/resources/models-datasets
  21. https://www.tensorflow.org/resources/tools
  22. https://www.tensorflow.org/resources/libraries-extensions
  23. https://www.tensorflow.org/community
  24. https://www.tensorflow.org/about
  25. https://www.tensorflow.org/about
  26. https://www.tensorflow.org/about/case-studies
  27. https://github.com/tensorflow
  28. https://www.tensorflow.org/overview
  29. https://www.tensorflow.org/overview
  30. https://www.tensorflow.org/tutorials
  31. https://www.tensorflow.org/guide
  32. https://www.tensorflow.org/alpha
  33. https://www.tensorflow.org/install
  34. https://www.tensorflow.org/learn
  35. https://www.tensorflow.org/overview
  36. https://www.tensorflow.org/tutorials
  37. https://www.tensorflow.org/guide
  38. https://www.tensorflow.org/alpha
  39. https://www.tensorflow.org/api_docs/python/tf
  40. https://www.tensorflow.org/resources/models-datasets
  41. https://www.tensorflow.org/community
  42. https://www.tensorflow.org/about
  43. https://github.com/tensorflow
  44. https://www.tensorflow.org/tutorials
  45. https://www.tensorflow.org/tutorials/keras
  46. https://www.tensorflow.org/tutorials/keras/basic_classification
  47. https://www.tensorflow.org/tutorials/keras/basic_text_classification
  48. https://www.tensorflow.org/tutorials/keras/basic_regression
  49. https://www.tensorflow.org/tutorials/keras/overfit_and_underfit
  50. https://www.tensorflow.org/tutorials/keras/save_and_restore_models
  51. https://www.tensorflow.org/tutorials/eager
  52. https://www.tensorflow.org/tutorials/eager/eager_basics
  53. https://www.tensorflow.org/tutorials/eager/automatic_differentiation
  54. https://www.tensorflow.org/tutorials/eager/custom_training
  55. https://www.tensorflow.org/tutorials/eager/custom_layers
  56. https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough
  57. https://www.tensorflow.org/tutorials/estimators/linear
  58. https://github.com/tensorflow/models/tree/master/official/wide_deep
  59. https://www.tensorflow.org/tutorials/estimators/boosted_trees
  60. https://www.tensorflow.org/tutorials/estimators/boosted_trees_model_understanding
  61. https://www.tensorflow.org/tutorials/estimators/id98
  62. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/id4_with_attention/id4_with_attention.ipynb
  63. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb
  64. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb
  65. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/cvae.ipynb
  66. https://www.tensorflow.org/tutorials/images/hub_with_keras
  67. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb
  68. https://github.com/tensorflow/models/blob/master/research/nst_blogpost/4_neural_style_transfer_with_eager_execution.ipynb
  69. https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb
  70. https://www.tensorflow.org/tutorials/images/deep_id98
  71. https://www.tensorflow.org/tutorials/sequences/text_generation
  72. https://www.tensorflow.org/tutorials/sequences/recurrent
  73. https://www.tensorflow.org/tutorials/sequences/recurrent_quickdraw
  74. https://www.tensorflow.org/tutorials/sequences/audio_recognition
  75. https://github.com/tensorflow/id4
  76. https://www.tensorflow.org/tutorials/load_data/images
  77. https://www.tensorflow.org/tutorials/load_data/tf_records
  78. https://www.tensorflow.org/tutorials/representation/id97
  79. https://www.tensorflow.org/tutorials/representation/kernel_methods
  80. https://www.tensorflow.org/tutorials/representation/linear
  81. https://www.tensorflow.org/tutorials/representation/unicode
  82. https://www.tensorflow.org/tutorials/non-ml/mandelbrot
  83. https://www.tensorflow.org/tutorials/non-ml/pdes
  84. https://www.tensorflow.org/learn
  85. https://www.tensorflow.org/overview
  86. https://www.tensorflow.org/js
  87. https://www.tensorflow.org/lite
  88. https://www.tensorflow.org/tfx
  89. https://www.tensorflow.org/swift
  90. https://www.tensorflow.org/api_docs/python/tf
  91. https://www.tensorflow.org/versions/r1.12/api_docs/python/tf
  92. https://www.tensorflow.org/versions/r1.11/api_docs/python/tf
  93. https://www.tensorflow.org/versions/r1.10/api_docs/python/tf
  94. https://www.tensorflow.org/versions/r1.9/api_docs/python/tf
  95. https://www.tensorflow.org/versions
  96. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf
  97. https://www.tensorflow.org/resources/models-datasets
  98. https://www.tensorflow.org/resources/tools
  99. https://www.tensorflow.org/resources/libraries-extensions
 100. https://www.tensorflow.org/about
 101. https://www.tensorflow.org/about/case-studies
 102. https://www.youtube.com/playlist?list=plqy2h8rroyvzouyi26khmksjbedn3squb
 103. https://www.tensorflow.org/
 104. https://www.tensorflow.org/learn
 105. https://www.tensorflow.org/overview
 106. https://www.tensorflow.org/tutorials
 107. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
 108. https://www.tensorflow.org/code/tensorflow/examples/tutorials/id97/id97_basic.py
 109. https://github.com/tensorflow/models/tree/master/tutorials/embedding/id97.py
 110. https://en.wikipedia.org/wiki/vector_space_model
 111. https://en.wikipedia.org/wiki/distributional_semantics#distributional_hypothesis
 112. https://en.wikipedia.org/wiki/latent_semantic_analysis
 113. http://www.scholarpedia.org/article/neural_net_language_models
 114. http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf
 115. https://arxiv.org/pdf/1301.3781.pdf
 116. https://en.wikipedia.org/wiki/maximum_likelihood
 117. https://en.wikipedia.org/wiki/softmax_function
 118. https://en.wikipedia.org/wiki/likelihood_function
 119. https://en.wikipedia.org/wiki/logistic_regression
 120. https://en.wikipedia.org/wiki/monte_carlo_integration
 121. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
 122. https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf
 123. https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf
 124. https://en.wikipedia.org/wiki/stochastic_gradient_descent
 125. https://lvdmaaten.github.io/tsne/
 126. https://www.aclweb.org/anthology/n13-1090
 127. https://arxiv.org/abs/1103.0398
 128. https://arxiv.org/pdf/1103.0398.pdf
 129. https://www.aclweb.org/anthology/p10-1040
 130. https://www.tensorflow.org/code/tensorflow/examples/tutorials/id97/id97_basic.py
 131. https://www.tensorflow.org/api_docs/python/tf/session#run
 132. https://www.tensorflow.org/code/tensorflow/examples/tutorials/id97/id97_basic.py
 133. https://github.com/tensorflow/models/tree/master/tutorials/embedding/id97.py
 134. https://www.aclweb.org/anthology/n13-1090
 135. http://download.tensorflow.org/data/questions-words.txt
 136. https://github.com/tensorflow/models/tree/master/tutorials/embedding/id97.py
 137. https://www.tensorflow.org/extend/new_data_formats
 138. https://github.com/tensorflow/models/tree/master/tutorials/embedding/id97.py
 139. https://www.tensorflow.org/guide/extend/op
 140. https://github.com/tensorflow/models/tree/master/tutorials/embedding/id97_optimized.py
 141. https://creativecommons.org/licenses/by/3.0/
 142. https://www.apache.org/licenses/license-2.0
 143. https://developers.google.com/site-policies
 144. https://medium.com/tensorflow
 145. https://github.com/tensorflow/
 146. https://twitter.com/tensorflow
 147. https://youtube.com/tensorflow
 148. https://github.com/tensorflow/tensorflow/issues
 149. https://github.com/tensorflow/tensorflow/blob/master/release.md
 150. https://stackoverflow.com/questions/tagged/tensorflow
 151. https://www.tensorflow.org/extras/tensorflow_brand_guidelines.pdf
 152. https://policies.google.com/terms
 153. https://policies.google.com/privacy

   hidden links:
 155. https://www.tensorflow.org/tutorials/representation/id97
 156. https://www.tensorflow.org/tutorials/representation/id97
 157. https://www.tensorflow.org/tutorials/representation/id97
 158. https://www.tensorflow.org/tutorials/representation/id97
