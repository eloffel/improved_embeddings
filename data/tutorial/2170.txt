   #[1]the data science lab    feed [2]the data science lab    comments feed
   [3]the data science lab    machine learning classics: the id88
   comments feed [4]list comprehension in python [5]improved seeding for
   id91 with id116++ [6]alternate [7]alternate [8]the data science
   lab [9]wordpress.com

[10]the data science lab

   experiments with data

     * [11]about
     * [12]contact
     * [13]archives

   january 10, 2014

machine learning classics: the id88

   an important problem in statistics and machine learning is that of
   classification, which is the task of identifying the category that an
   observation belongs to, on the basis of a training set of data
   containing other instances.

     in the terminology of machine learning, classification is considered
     an instance of supervised learning, i.e. learning where a training
     set of correctly identified observations is available. the
     corresponding unsupervised procedure is known as id91 or
     cluster analysis, and involves grouping data into categories based
     on some measure of inherent similarity (e.g. the distance between
     instances, considered as vectors in a multi-dimensional vector
     space). ([14]wikipedia)

   at the data science lab, we have already reviewed some basics of
   unsupervised classification with the [15]lloyd algorithm for id116
   id91 and have investigated [16]how to find the appropriate number
   of clusters. today   s post will be devoted to a classical machine
   learning algorithm for supervised classification: the id88
   learning algorithm.

theory behind the id88

   the id88 learning algorithm was invented in the late 1950s by
   [17]frank rosenblatt. it belongs to the class of linear classifiers,
   this is, for a data set classified according to binary categories
   (which we will assume to be labeled +1 and -1), the classifier seeks to
   divide the two classes by a linear separator. the separator is a
   (n-1)-dimensional hyperplane in a n-dimensional space, in particular it
   is a line in the plane and a plane in the 3-dimensional space.

   our data set will be assumed to consist of n observations characterized
   by d features or attributes, \mathbf{x}_n = (x_1, \ldots, x_d) for n =
   (1, \ldots, n) . the problem of binary classifying these data points
   can be translated to that of finding a series of weights w_i such that
   all vectors verifying

   \displaystyle \sum_{i=1}^d w_i x_i < b

   are assigned to one of the classes whereas those verifying

   \displaystyle \sum_{i=1}^d w_i x_i > b

   are assigned to the other, for a given threshold value b . if we rename
   b = w_0 and introduce an artificial coordinate x_0 = 1 in our vectors
   \mathbf{x}_n , we can write the id88 separator formula as

   \displaystyle h(\mathbf{x}) = \mathrm{sign}\left(\sum_{i=0}^d w_i
   x_i\right) = \mathrm{sign}\left(
   \mathbf{w}^{\mathbf{t}}\mathbf{x}\right)

   note that \mathbf{w}^{\mathbf{t}}\mathbf{x} is notation for the
   [18]scalar product between vectors \mathbf{w} and \mathbf{x} . thus the
   problem of classifying is that of finding the vector of weights
   \mathbf{w} given a training data set of n vectors \mathbf{x} with their
   corresponding labeled classification vector (y_1, \ldots, y_n) .

the id88 learning algorithm (pla)

   the learning algorithm for the id88 is online, meaning that
   instead of considering the entire data set at the same time, it only
   looks at one example at a time, processes it and goes on to the next
   one. the algorithm starts with a guess for the vector \mathbf{w}
   (without loss of generalization one can begin with a vector of zeros).
   [19]id88_update it then assesses how good of a guess that is by
   comparing the predicted labels with the actual, correct labels
   (remember that those are available for the training test, since we are
   doing supervised learning). as long as there are misclassified points,
   the algorithm corrects its guess for the weight vector by updating the
   weights in the correct direction, until all points are correctly
   classified.

   that direction is as follows: given a labeled training data set, if
   \mathbf{w} is the guessed weight vector and \mathbf{x}_n is an
   incorrectly classified point with \mathbf{w}^{\mathbf{t}}\mathbf{x}_n
   \neq y_n , then the weight \mathbf{w} is updated to \mathbf{w} + y_n
   \mathbf{x}_n . this is illustrated in the plot on the right, taken from
   [20]this clear article on the id88.

   a nice feature of the id88 learning rule is that if there exist a
   set of weights that solve the problem (i.e. if the data is linearly
   separable), then the id88 will find these weights.

a python implementation of the id88

   for our python implementation we will use a trivial example on two
   dimensions: within the [-1,1]\times[-1,1] space, we define two random
   points and draw the line that joins them. the general equation of a
   line given two points in it, (x_1, y_1) and (x_2, y_2) , is a + bx + cy
   = 0 where a, b, c can be written in terms of the two points. defining a
   vector \mathrm{v} = (a, b, c) , any point (x,y) belongs to the line if
   \mathrm{v}^\mathrm{t}\mathrm{x} = 0 , where \mathrm{x} = (1,x,y) .
   points for which the dot product is positive fall on one side of the
   line, negatives fall on the other.

   this procedure automatically divides the plane linearly in two regions.
   we randomly choose n points in this space and classify them as +1 or -1
   according to the dividing line defined before. the id88 class
   defined below is initialized exactly in this way. the id88
   learning algorithm is implemented in the pla function, and the
   classification error, defined as the fraction of misclassified points,
   is in classification_error. the code is as follows:
import numpy as np
import random
import os, subprocess

class id88:
    def __init__(self, n):
        # random linearly separated data
        xa,ya,xb,yb = [random.uniform(-1, 1) for i in range(4)]
        self.v = np.array([xb*ya-xa*yb, yb-ya, xa-xb])
        self.x = self.generate_points(n)

    def generate_points(self, n):
        x = []
        for i in range(n):
            x1,x2 = [random.uniform(-1, 1) for i in range(2)]
            x = np.array([1,x1,x2])
            s = int(np.sign(self.v.t.dot(x)))
            x.append((x, s))
        return x

    def plot(self, mispts=none, vec=none, save=false):
        fig = plt.figure(figsize=(5,5))
        plt.xlim(-1,1)
        plt.ylim(-1,1)
        v = self.v
        a, b = -v[1]/v[2], -v[0]/v[2]
        l = np.linspace(-1,1)
        plt.plot(l, a*l+b, 'k-')
        cols = {1: 'r', -1: 'b'}
        for x,s in self.x:
            plt.plot(x[1], x[2], cols[s]+'o')
        if mispts:
            for x,s in mispts:
                plt.plot(x[1], x[2], cols[s]+'.')
        if vec != none:
            aa, bb = -vec[1]/vec[2], -vec[0]/vec[2]
            plt.plot(l, aa*l+bb, 'g-', lw=2)
        if save:
            if not mispts:
                plt.title('n = %s' % (str(len(self.x))))
            else:
                plt.title('n = %s with %s test points' \
                          % (str(len(self.x)),str(len(mispts))))
            plt.savefig('p_n%s' % (str(len(self.x))), \
                        dpi=200, bbox_inches='tight')

    def classification_error(self, vec, pts=none):
        # error defined as fraction of misclassified points
        if not pts:
            pts = self.x
        m = len(pts)
        n_mispts = 0
        for x,s in pts:
            if int(np.sign(vec.t.dot(x))) != s:
                n_mispts += 1
        error = n_mispts / float(m)
        return error

    def choose_miscl_point(self, vec):
        # choose a random point among the misclassified
        pts = self.x
        mispts = []
        for x,s in pts:
            if int(np.sign(vec.t.dot(x))) != s:
                mispts.append((x, s))
        return mispts[random.randrange(0,len(mispts))]

    def pla(self, save=false):
        # initialize the weigths to zeros
        w = np.zeros(3)
        x, n = self.x, len(self.x)
        it = 0
        # iterate until all points are correctly classified
        while self.classification_error(w) != 0:
            it += 1
            # pick random misclassified point
            x, s = self.choose_miscl_point(w)
            # update weights
            w += s*x
            if save:
                self.plot(vec=w)
                plt.title('n = %s, iteration %s\n' \
                          % (str(n),str(it)))
                plt.savefig('p_n%s_it%s' % (str(n),str(it)), \
                            dpi=200, bbox_inches='tight')
        self.w = w

    def check_error(self, m, vec):
        check_pts = self.generate_points(m)
        return self.classification_error(vec, pts=check_pts)

   to start a run of the id88 with 20 data points and visualize the
   initial configuration we simply initialize the id88 class and
   call the plot function:
p = id88(20)
p.plot()

   [21]p_n20_o
   on the right is the plane that we obtain, divided in two by the black
   line. red points are labeled as +1 while blue ones are -1. the purpose
   of the id88 learning algorithm is to    learn    a linear classifier
   that correctly separates red from blue points given the labeled set of
   20 points shown in the figure. this is, we want to learn the black line
   as faithfully as possible.

   the call to p.pla() runs the algorithm and stores the final weights
   learned in p.w. to save a plot of each of the iterations, we can use
   the option p.pla(save=true). the following snippet will concatenate all
   images together to produce an animated gif of the running algorithm:
basedir = '/my/output/directory/'
os.chdir(basedir)
pngs = [pl for pl in os.listdir(basedir) if pl.endswith('png')]
sortpngs = sorted(pngs, key=lambda a:int(a.split('it')[1][:-4]))
basepng = pngs[0][:-8]
[sortpngs.append(sortpngs[-1]) for i in range(4)]
comm = ("convert -delay 50 %s %s.gif" % (' '.join(sortpngs), basepng)).split()
proc = subprocess.popen(comm, stdin = subprocess.pipe, stdout = subprocess.pipe)
(out, err) = proc.communicate()

   below we can see how the algorithm tries successive values for the
   weight vector, leading to a succession of guesses for the linear
   separator, which are plotted in green. for as long as the green line
   misclassifies points (meaning that it does not separate the blue from
   the right points correctly), the id88 keeps updating the weights
   in the manner described above. eventually all points are on the correct
   side of the guessed line, the classification error in the training data
   set ( e_{in} = 0 for the in-sample points) is thus zero and the
   algorithm converges and stops.

   [22]p_n20

   clearly the final guessed green line after the 7 iterations does
   separate the training data points correctly but it does not completely
   agree with the target black line. an error in classifying not-seen data
   points is bound to exist ( e_{out} \neq 0 for out-of-sample points),
   and we can quantify it easily by evaluating the performance of the
   linear classifier on fresh, unseen data points:
p.plot(p.generate_points(500), p.w, save=true)

   [23]p_n20 in this image we can observe how, even if e_{in} = 0 for the
   training points represented by the large dots, e_{out} \neq 0 , as
   shown by the small red and blue dots that fall on one side of the black
   (target) line but are incorrectly classified by the green (guessed)
   one. the exact out-of-sample error is given by the area between both
   lines. this can be thus computed analytically and exactly. but it can
   also be estimated with a repeated monte carlo sampling:
err = []
for i in range(100):
    err.append(p.check_error(500, p.w))
np.mean(err)

   0.0598200

   the id88 algorithm has thus learned a linear binary classifier
   that correctly classifies data in 94% of the cases, having an
   out-of-sample error rate of 6%.

table-top data experiment take-away message

   the id88 learning algorithm is a classical example of binary
   linear supervised classifier. its implementation involves finding a
   linear boundary that completely separates points belonging to the two
   classes. if the data is linearly separable, then the procedure will
   converge to a weight vector that separates the data. (and if the data
   is inseparable, the pla will never converge.) the id88 is thus
   fundamentally limited in that its decision boundaries can only be
   linear. eventually we will explore other methods that overcome this
   limitation, either combining multiple id88s in a single framework
   (neural networks) or by mapping features in an efficient way (kernels).
   advertisements

share!

     * [24]twitter
     * [25]facebook
     *

like this:

   like loading...

related

   written by [26]datasciencelab posted in [27]experiments tagged with
   [28]linear classifier, [29]machine learning, [30]id88,
   [31]supervised learning

24 comments

    1.
   [32]january 11, 2014 - 4:27 am [33]joe lotz
       good job, keep them coming!!
       [34]reply
    2.
   [35]october 3, 2014 - 10:27 am jamone
       first, you forgot to add the    import matplotlib    line at the
       beginning.
       second, p.plot() seems not to do anything.
       if it does generate a plot, where does the plot display?
       [36]reply
          +
        [37]november 4, 2014 - 5:31 pm [38]shashankgaur
            jamone: you need to save the plot. try p.plot(save=true) or if
            you want to display then put plt.show() in the plot() function
            of the class id88.
            [39]reply
    3.
   [40]october 12, 2014 - 5:12 am dominic
       how does the vector inner product works with $v^{t}x$? i   m a bit
       lost, especially with how you   ve defined the vector in your code   
       is there an elegant proof i can follow?
       [41]reply
          +
        [42]december 10, 2014 - 2:31 pm [43]datasciencelab
            hi dominic, the dot product of a vector, say v = (v1, v2) with
            another vector x = (x1, x2) is simply v1*x1 + v2*x2. i define
            the vector as a numpy array, and then the dot vector can be
            simply computed as v.t.dot(x), where the .t operation is the
            transverse of the vector (simply turns a 1-d row vector into a
            column and viceversa). you can read explanatory articles in
            the wikipedia: [44]http://en.wikipedia.org/wiki/dot_product
            and [45]http://en.wikipedia.org/wiki/transpose.
            [46]reply
    4.
   [47]october 16, 2014 - 7:00 am [48]roshan singh
       can you fix the indentation in the code?
       [49]reply
          +
        [50]october 16, 2014 - 7:03 am [51]roshan singh
            it seems reloading fixed it.
            [52]reply
    5.
   [53]february 8, 2015 - 12:12 am [54]sergulaydore
       thanks for the post. i am having problem with comm = (   convert
       -delay 50 %s %s.gif    % (       .join(sortpngs), basepng)).split()
       why are you inserting .gif to png files? in my case, comm is like
       this:
       [   convert   ,
          -delay   ,
          50   ,
          it0.png   ,
          it50.png   ,
          it100.png   ,
          it150.png   ,
          it200.png   ,
          it250.png   ,
          it300.png   ,
          it305.png   ,
          it350.png   ,
          it366.png   ,
          it387.png   ,
          it387.png   ,
          it387.png   ,
          it387.png   ,
          it387.png   ]
       but it does not work when i feed it to popen. can you please guide
       me with this?
       error says:
       oserror: [errno 2] no such file or directory
       thanks
       [55]reply
          +
        [56]february 8, 2015 - 12:35 am [57]sergulaydore
            i solved the problem. i did not know i had to install
            imagemagick for convert function.
            [58]reply
    6.
   [59]august 29, 2015 - 3:04 pm henry
       what if vec[2] is zero in line 36?
       [60]reply
    7. september 30, 2015 - 6:30 pm pingback: [61]introducing the
       id88 | limited rationality
    8.
   [62]may 1, 2016 - 1:17 pm anonymous
       how the code is work
       [63]reply
    9.
   [64]may 16, 2016 - 2:38 pm aos
       hi, thanks for the great job. i could run this code in my computer,
       but i have a question. this code is generating the input values
       with random values. i have some data from different sensors and i
       want to classifying them with this method. what should i do now?
       how can i use these data as input to this code.
       thanks in advance.
       [65]reply
          +
        [66]may 18, 2016 - 6:36 pm [67]datasciencelab
            hi aos,
            to feed the code above any other data than the random values,
            you need to modify function generate_points of the id88
            class. you need to fill the array self.x with the data from
            your sensors, which may be in a text file that you open and
            read inside the generate_points function. assuming your sensor
            data is 2-dimentional, array self.x is of the form [(x0,y0),
            (x1,y1),     , (xn, yn)] . if your sensors data is of higher
            dimension, you need to fill self.x as [(x00,x10,   ,xk0),
            (x01,x11,   ,xk1),     , (x0n, x1n,   ,xkn)], where k is the number
            of dimensions of your data points and n the number of data
            points you have.
            [68]reply
               o
             [69]may 29, 2016 - 11:31 am lenny
                 hi, thanks for your code example. i would also like to
                 modify the function generate_points in order to deal with
                 my own sensor data. i filled the vector x with the data
                 just like you described in your post. my question ist
                 now, how to fill the vector v.
                 i don   t understand what this vector actually does.
                 thank you in advance!
   10.
   [70]june 8, 2016 - 3:03 pm henry
       hi,
       quick question: line #9 is self.v = np.array([xb*ya-xa*yb, yb-ya,
       xa-xb]) and it seems like the way you constructed it isn   t
       something important that self.v can also be something like
       np.array([xa*ya/xb*yb, xb-ya, ya-xb]) because it is just the
       hyperplane we randomly come up at the beginning and then the actual
       x-y pairs (data points) will be generated based on that hyperplane
       (i.e. self.v). am i correct? thanks in advance.
       [71]reply
          +
        [72]june 8, 2016 - 5:19 pm [73]datasciencelab
            you are correct!
            [74]reply
               o
             [75]june 9, 2016 - 9:24 am henry
                 appreciate your confirmation! thanks!
          +
        [76]june 28, 2016 - 12:06 pm nanos mangart
            hi, maybe i am missing something, but with the definition on
            line #9 the geometric meaning is clear. if a=(1,xa,ya) and
            b=(1,xb,yb) are two points defining a line, then v=axb is the
            cross product between a and b. and the    points for which the
            dot product is positive fall on one side of the line,
            negatives fall on the other.    only makes sense when v is
            defined as above.
            [77]reply
   11. august 24, 2016 - 1:54 am pingback: [78]d191: machine learning
       classics: the id88     ai
   12.
   [79]september 29, 2016 - 6:45 am [80]chowdhury rahim
       i am trying to determine how fast does the pla converge. so i
       changed the codes as follows:
       while self.classification_error(w) != 0:
       it += 1
       iter = it
       # pick random misclassified point
       x, s = self.choose_miscl_point(w)
       # update weights
       w += s*x
       if save:
       self.plot(vec=w)
       plt.title(   n = %s, iteration %s\n    \
       % (str(n),str(it)))
       plt.savefig(   p_n%s_it%s    % (str(n),str(it)), \
       dpi=200, bbox_inches=   tight   )
       print(iter)
       self.w = w
       i.e. added a variable iter, and tried to print it. nothing shows
       up. can you please comment how to fix it?
       [81]reply
   13.
   [82]october 3, 2016 - 9:11 am anonymous
       hi there, thank you. plt seems to be an unresolved reference, code
       won   t run.
       [83]reply
   14. may 10, 2018 - 8:11 am pingback: [84]   it   s either a panda or a
       gibbon   : ai winters and the limits of deep learning
   15. september 23, 2018 - 4:03 pm pingback: [85]tsm_delearn | andreas'
       blog

post a comment [86]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [87]googleplus-sign-in

     *
     *

   [88]gravatar
   email (address never made public)
   ____________________
   name
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [89]log out /
   [90]change )
   google photo

   you are commenting using your google account. ( [91]log out /
   [92]change )
   twitter picture

   you are commenting using your twitter account. ( [93]log out /
   [94]change )
   facebook photo

   you are commenting using your facebook account. ( [95]log out /
   [96]change )
   [97]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

   [98]list comprehension in python
   [99]improved seeding for id91 with id116++

the data science lab

   this blog is a collection of articles on data science topics, machine
   learning, visualization and fun experiments with data.

recent posts

     * [100]2014 in review
     * [101]selection of k in id116 id91, reloaded
     * [102]improved seeding for id91 with id116++
     * [103]machine learning classics: the id88
     * [104]list comprehension in python

data science lab tweets

   [105]my tweets

data science lab tags

   [106]animations [107]authorship attribution [108]automata
   [109]id91 [110]coursera [111]courses [112]data exploration
   [113]edx [114]game of life [115]gap statistic [116]general
   [117]initialization [118]id116 [119]linear classifier [120]list
   comprehension [121]lloyd's algorithm [122]machine learning
   [123]matplotlib [124]mooc [125]pandas [126]id88 [127]plotting
   [128]python [129]supervised learning [130]udacity [131]unsupervised
   learning [132]video [133]visualization

   [134]follow the data science lab on wordpress.com

rss

     * [135]rss - posts

   [136]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [137]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [138]cookie policy

   iframe: [139]likes-master

   %d bloggers like this:

references

   visible links
   1. https://datasciencelab.wordpress.com/feed/
   2. https://datasciencelab.wordpress.com/comments/feed/
   3. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/feed/
   4. https://datasciencelab.wordpress.com/2014/01/08/list-comprehension-in-python/
   5. https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-id91-with-id116/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/&for=wpcom-auto-discovery
   8. https://datasciencelab.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://datasciencelab.wordpress.com/
  11. https://datasciencelab.wordpress.com/about/
  12. https://datasciencelab.wordpress.com/contact/
  13. https://datasciencelab.wordpress.com/archives/
  14. http://en.wikipedia.org/wiki/classification_(machine_learning)
  15. https://datasciencelab.wordpress.com/2013/12/12/id91-with-id116-in-python/
  16. https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-id116-id91/
  17. http://en.wikipedia.org/wiki/frank_rosenblatt
  18. http://en.wikipedia.org/wiki/scalar_product
  19. https://datasciencelab.files.wordpress.com/2014/01/id88_update.png
  20. http://www.mblondel.org/journal/2010/10/31/kernel-id88-in-python/
  21. https://datasciencelab.files.wordpress.com/2014/01/p_n20_o1.png
  22. https://datasciencelab.files.wordpress.com/2014/01/p_n201.gif
  23. https://datasciencelab.files.wordpress.com/2014/01/p_n201.png
  24. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?share=twitter
  25. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?share=facebook
  26. https://datasciencelab.wordpress.com/author/datasciencelab/
  27. https://datasciencelab.wordpress.com/category/experiments/
  28. https://datasciencelab.wordpress.com/tag/linear-classifier/
  29. https://datasciencelab.wordpress.com/tag/machine-learning/
  30. https://datasciencelab.wordpress.com/tag/id88/
  31. https://datasciencelab.wordpress.com/tag/supervised-learning/
  32. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-24
  33. https://plus.google.com/+joelotz
  34. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=24#respond
  35. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-255
  36. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=255#respond
  37. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-280
  38. http://shashankgaur.wordpress.com/
  39. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=280#respond
  40. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-264
  41. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=264#respond
  42. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-313
  43. https://datasciencelab.wordpress.com/
  44. http://en.wikipedia.org/wiki/dot_product
  45. http://en.wikipedia.org/wiki/transpose
  46. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=313#respond
  47. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-267
  48. http://roshansingh.wordpress.com/about/
  49. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=267#respond
  50. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-268
  51. http://roshansingh.wordpress.com/about/
  52. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=268#respond
  53. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-362
  54. http://www.sergulaydore.com/
  55. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=362#respond
  56. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-363
  57. http://www.sergulaydore.com/
  58. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=363#respond
  59. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-419
  60. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=419#respond
  61. http://limitedrationality.com/introducing-the-id88/
  62. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-540
  63. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=540#respond
  64. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-550
  65. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=550#respond
  66. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-551
  67. https://datasciencelab.wordpress.com/
  68. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=551#respond
  69. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-555
  70. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-566
  71. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=566#respond
  72. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-567
  73. https://datasciencelab.wordpress.com/
  74. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=567#respond
  75. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-568
  76. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-573
  77. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=573#respond
  78. https://ai25blog.wordpress.com/2016/08/24/d191-machine-learning-classics-the-id88/
  79. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-601
  80. http://rc/
  81. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=601#respond
  82. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-602
  83. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/?replytocom=602#respond
  84. https://warontherocks.com/2018/05/its-either-a-panda-or-a-gibbon-ai-winters-and-the-limits-of-deep-learning/
  85. http://blog.bachi.net/?p=8418
  86. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#respond
  87. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://datasciencelab.wordpress.com&color_scheme=light
  88. https://gravatar.com/site/signup/
  89. javascript:highlandercomments.doexternallogout( 'wordpress' );
  90. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/
  91. javascript:highlandercomments.doexternallogout( 'googleplus' );
  92. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/
  93. javascript:highlandercomments.doexternallogout( 'twitter' );
  94. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/
  95. javascript:highlandercomments.doexternallogout( 'facebook' );
  96. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/
  97. javascript:highlandercomments.cancelexternalwindow();
  98. https://datasciencelab.wordpress.com/2014/01/08/list-comprehension-in-python/
  99. https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-id91-with-id116/
 100. https://datasciencelab.wordpress.com/2014/12/31/2014-in-review/
 101. https://datasciencelab.wordpress.com/2014/01/21/selection-of-k-in-id116-id91-reloaded/
 102. https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-id91-with-id116/
 103. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/
 104. https://datasciencelab.wordpress.com/2014/01/08/list-comprehension-in-python/
 105. https://twitter.com/408172415013761025
 106. https://datasciencelab.wordpress.com/tag/animations/
 107. https://datasciencelab.wordpress.com/tag/authorship-attribution/
 108. https://datasciencelab.wordpress.com/tag/automata/
 109. https://datasciencelab.wordpress.com/tag/id91/
 110. https://datasciencelab.wordpress.com/tag/coursera/
 111. https://datasciencelab.wordpress.com/tag/courses/
 112. https://datasciencelab.wordpress.com/tag/data-exploration/
 113. https://datasciencelab.wordpress.com/tag/edx/
 114. https://datasciencelab.wordpress.com/tag/game-of-life/
 115. https://datasciencelab.wordpress.com/tag/gap-statistic/
 116. https://datasciencelab.wordpress.com/tag/general/
 117. https://datasciencelab.wordpress.com/tag/initialization/
 118. https://datasciencelab.wordpress.com/tag/id116/
 119. https://datasciencelab.wordpress.com/tag/linear-classifier/
 120. https://datasciencelab.wordpress.com/tag/list-comprehension/
 121. https://datasciencelab.wordpress.com/tag/lloyds-algorithm/
 122. https://datasciencelab.wordpress.com/tag/machine-learning/
 123. https://datasciencelab.wordpress.com/tag/matplotlib/
 124. https://datasciencelab.wordpress.com/tag/mooc/
 125. https://datasciencelab.wordpress.com/tag/pandas/
 126. https://datasciencelab.wordpress.com/tag/id88/
 127. https://datasciencelab.wordpress.com/tag/plotting/
 128. https://datasciencelab.wordpress.com/tag/python/
 129. https://datasciencelab.wordpress.com/tag/supervised-learning/
 130. https://datasciencelab.wordpress.com/tag/udacity/
 131. https://datasciencelab.wordpress.com/tag/unsupervised-learning/
 132. https://datasciencelab.wordpress.com/tag/video/
 133. https://datasciencelab.wordpress.com/tag/visualization/
 134. https://datasciencelab.wordpress.com/
 135. https://datasciencelab.wordpress.com/feed/
 136. https://wordpress.com/?ref=footer_blog
 137. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/
 138. https://automattic.com/cookies
 139. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 141. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-form-guest
 142. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-form-load-service:wordpress.com
 143. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-form-load-service:twitter
 144. https://datasciencelab.wordpress.com/2014/01/10/machine-learning-classics-the-id88/#comment-form-load-service:facebook
