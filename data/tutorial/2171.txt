   #[1]triangleinequality    feed [2]triangleinequality    comments feed
   [3]triangleinequality    enter the id88 comments feed [4]snakes on
   a manifold: part 2 [5]neural networks part 1: feedingforward and
   id26 [6]alternate [7]alternate [8]triangleinequality
   [9]wordpress.com

     * [10]home
     * [11]about
     * [12]contents

[13]triangleinequality

   for matters mathematical
   search ____________________ (button) search
     __________________________________________________________________

   [14]home    [15]machine learning    enter the id88

enter the id88

   introduction

   some time ago in [16]my post on id28, i talked about the
   problem where you have a n \times p matrix x representing n data points
   with p features, and a vector y \in \{0,1 \}^n of classifications.

   in id75 you would produce a vector \beta \in \mathbb{r}^p
   so that x\beta \approx y , but the problem here is that our output
   could be any real number, not \{0,1\} . the solution we investigated
   was using the logistic link function p(x) = \frac{1}{1+\exp(-x)} which
   turns any real number into something we interpret as a id203.

   that was one solution, another even simpler one is the function defined
   s(x) = \left \{\begin{array}{cc} 1 & x \ge 0 \\ -1 &x<0\end{array}
   \right. .  it is convenient in this case to think of the response as
   taking values in \{-1,1 \} not \{0,1\} .

   previously we added a column of ones to x in order to create an
   intercept term, here we will write it separately so so that x \mapsto
   x\beta + b where b \in \mathbb{r}^n is called the bias.

   this post will take the book [17]   the elements of statistical learning   
   as its guide (the section starting on p. 130), which is freely
   available online, but there is also a great blog post about it
   [18]here.

   the algorithm

   so define f(x):=\beta^tx+b , we classify a point as being in the
   positive class if f(x)>0 and negative if f(x) <0 . geometrically we
   have an affine hyperplane l:=\{x:f(x)=0 \} , and points are classified
   depending on whether they are    above    or    below    the hyperplane.

   the unit normal u to l is given by \beta/ \vert \beta \vert , points
   above l then mean points with positive projection onto this vector.

   the signed distance of a point x to l is found by projecting the vector
   x-x_0 onto u , where x_0 is the    origin    of the hyperplane (since an
   affine hyperplane is a translation of a linear subspace, there is a
   point corresponding to the translation of the origin). so we get

   u^t(x-x_0)= \beta^t/ \vert \beta \vert (x-x_0) = (\beta^tx+b)/ \vert
   \beta \vert = f(x)/ \vert \beta \vert = f(x)/ \vert f'(x) \vert ,

   notice this means that f(x) is proportional to the signed distance of x
   from l .

   if m is the set of misclassified points, then we can measure the error
   as the sum of the distances (up to a constant of proportionality), of
   the misclassified points to l , that is

   d(\beta, b):=-\sum \limits _{x \in m}y_x f(x) ,

   where y_x is the response of x , notice that this makes each summand
   negative, which is reversed by the minus sign outside.

   if we fix m we can differentiate with respect to \beta and b , giving

   \frac{\partial}{\partial \beta}d(\beta, b)= -\sum \limits _{x \in
   m}y_xx ,

   and

   \frac{\partial}{\partial b}d(\beta, b)= -\sum \limits _{x \in m}y_x .

   just like in id28, we can use these partial derivatives
   to search for a minimum of d . the algorithm purposed by frank
   rosenblatt, the inventor of the id88, is to modify the hyperplane
   after looking at each individual misclassified item in m as it comes,
   that is (\beta, b) \mapsto (\beta,b) + \lambda(y_xx,y_x) , where
   \lambda >0 is the learning rate. this has the advantage of being much
   faster than computing the global gradient informed by all of the data,
   and is an example of [19]stochastic id119.

   of course by changing the hyperplane as we go we may change m , and so
   what we actually do is iterate over each datapoint in a fixed order,
   and check if it is misclassified under the latest hyperplane, then
   update and move onto the next point. this is then very easy to code.

   the code

   first let   s recycle some code we used for id28 to get
   some test data.
import numpy as np
from scipy import linalg
import random as rd
import matplotlib.pyplot as plt
def test_set(a,b, no_samples, intercept, slope, variance, offset):
    #first we grab some random x-values in the range [a,b].
    x1 = [rd.uniform(a,b) for i in range(no_samples)]
    #sorting them seems to make the plots work better later on.
    x1.sort()
    #now we define our linear function that will underly our synthetic dataset:
the true decision   boundary.
    def g(x):
        return intercept + slope*x
    #we create two classes of points r,b. r points are given by g(x) plus a gaus
sian noise term with positive mean,
    #b points are given by g(x) minus a gaussian norm term with positive mean.
    r_turn=true
    x2=[] #this will hold the y-coordinates of our data.
    y=[]  #this will hold the classification of our data.
    for x in x1:
        x2=g(x)
        if r_turn:
            x2 += rd.gauss(offset, variance)
            y.append(1)
        else:
            x2 -= rd.gauss(offset, variance)
            y.append(-1)
        r_turn=not r_turn
        x2.append(x2)
    #now we combine the input data into a single matrix.
    x1 = np.array([x1]).t
    x2 = np.array([x2]).t
    x = np.hstack((x1, x2))
    #now we return the input, output, and true decision boundary.
    return [ x,np.array(y).t, map(g, x1)]

   fairly straightforward stuff i hope! if not then it should make sense
   when we plot it.
x,y,g = test_set(a=0, b=5, no_samples=200, intercept=10, slope=0, variance=0.5,
offset=1)
fig, ax = plt.subplots()
r =  x[::2] #even terms
b =  x[1::2] #odd terms

ax.scatter(r[:,0],r[:,1],marker='o', facecolor='red', label="outcome 1")
ax.scatter(b[:,0],b[:,1],marker='o', facecolor='blue', label="outcome 0")
ax.plot(x[:,0], g, color='green', label="true decision boundary")
ax.legend(loc="upper left", prop={'size':8})
plt.show()

   this will produce something like:

   [20]id88_test_data

   now to implement the algorithm.
def id88(x,y, learning_rate=0.1, max_iter=1000):
    #expecting x = n x m inputs, y = n x 1 outputs taking values in -1,1
    m=x.shape[1]
    n=x.shape[0]
    weight = np.zeros((m))
    bias = 0
    n_iter=0
    index = list(range(n))
    while n_iter <= max_iter:
        rd.shuffle(index)
        for row in index:
            if (x[row,:].dot(weight)+bias)*y[row] <=0: #misclassified point.
                weight += learning_rate*x[row,:]*y[row]
                bias += learning_rate*y[row]
        n_iter+=1
    return {'weight':weight, 'bias':bias}

   we shuffle the order we take the data points in at each step to get rid
   of any peculiarities in the order we update. now let   s try it on our
   test data.
result= id88(x,y,0.1)
def decision_boundary(x, weight, bias):
    return (-bias - x*weight[0])/weight[1]

fig, ax = plt.subplots()
r =  x[::2] #even terms
b =  x[1::2] #odd terms

ax.scatter(r[:,0],r[:,1],marker='o', facecolor='red', label="outcome 1")
ax.scatter(b[:,0],b[:,1],marker='o', facecolor='blue', label="outcome -1")
ax.plot(x[:,0], g, color='green', label="true decision boundary")
ax.plot(x[:,0], [decision_boundary(x, result['weight'], result['bias']) for x in
 x[:,0]],
        label='predicted decision boundary')
ax.legend(loc="upper left", prop={'size':8})
plt.show()

   producing:

   [21]id88_test_results not bad! get the code [22]here.
   advertisements

share this:

     * [23]twitter
     * [24]facebook
     *

like this:

   like loading...

related

   tags: [25]machine learning, [26]id88, [27]python
   by [28]triangleinequality in [29]machine learning on [30]february 24,
   2014.
   [31]    snakes on a manifold: part 2 [32]neural networks part 1:
   feedingforward and id26    
     __________________________________________________________________

3 comments

    1. [33]neural networks: part 1    triangleinequality says:
       [34]march 27, 2014 at 3:53 pm
       [   ] time we looked at the id88 model for classification,
       which is similar to id28, in that it uses a
       nonlinear transformation of a linear [   ]
       [35]reply
    2. [36]support vector machines and the kernel trick   
       triangleinequality says:
       [37]august 14, 2014 at 4:54 pm
       [   ] premise is exactly the same as the id88 algorithm, we
       have input data with class labels [   ]
       [38]reply
    3. [39]support vector machines and the kernel trick   
       triangleinequality says:
       [40]august 14, 2014 at 4:54 pm
       [   ] premise is exactly the same as the id88 algorithm, we
       have input data with class labels [   ]
       [41]reply

leave a reply [42]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [43]googleplus-sign-in

     *
     *

   [44]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [45]log out /
   [46]change )
   google photo

   you are commenting using your google account. ( [47]log out /
   [48]change )
   twitter picture

   you are commenting using your twitter account. ( [49]log out /
   [50]change )
   facebook photo

   you are commenting using your facebook account. ( [51]log out /
   [52]change )
   [53]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

   [54]create a free website or blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [55]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [56]cookie policy

   iframe: [57]likes-master

   %d bloggers like this:

references

   visible links
   1. https://triangleinequality.wordpress.com/feed/
   2. https://triangleinequality.wordpress.com/comments/feed/
   3. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/feed/
   4. https://triangleinequality.wordpress.com/2014/02/04/snakes-on-a-manifold-part-2/
   5. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/&for=wpcom-auto-discovery
   8. https://triangleinequality.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://triangleinequality.wordpress.com/
  11. https://triangleinequality.wordpress.com/about/
  12. https://triangleinequality.wordpress.com/contents/
  13. https://triangleinequality.wordpress.com/
  14. https://triangleinequality.wordpress.com/
  15. https://triangleinequality.wordpress.com/category/machine-learning/
  16. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/
  17. http://statweb.stanford.edu/~tibs/elemstatlearn/
  18. http://jeremykun.com/2011/08/11/the-id88-and-all-the-things-it-cant-perceive/
  19. http://en.wikipedia.org/wiki/stochastic_gradient_descent
  20. https://triangleinequality.files.wordpress.com/2014/02/id88_test_data.png
  21. https://triangleinequality.files.wordpress.com/2014/02/id88_test_results.png
  22. http://sourceforge.net/projects/triangleinequal/files/machine learning/id88.py/download
  23. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/?share=twitter
  24. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/?share=facebook
  25. https://triangleinequality.wordpress.com/tag/machine-learning/
  26. https://triangleinequality.wordpress.com/tag/id88/
  27. https://triangleinequality.wordpress.com/tag/python/
  28. https://triangleinequality.wordpress.com/author/triangleinequality/
  29. https://triangleinequality.wordpress.com/tag/machine-learning/
  30. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/
  31. https://triangleinequality.wordpress.com/2014/02/04/snakes-on-a-manifold-part-2/
  32. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  33. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  34. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/#comment-82
  35. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/?replytocom=82#respond
  36. https://triangleinequality.wordpress.com/2014/08/14/support-vector-machines-and-the-kernel-trick/
  37. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/#comment-149
  38. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/?replytocom=149#respond
  39. https://triangleinequality.wordpress.com/2014/08/14/support-vector-machines-and-the-kernel-trick/
  40. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/#comment-150
  41. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/?replytocom=150#respond
  42. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/#respond
  43. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://triangleinequality.wordpress.com&color_scheme=light
  44. https://gravatar.com/site/signup/
  45. javascript:highlandercomments.doexternallogout( 'wordpress' );
  46. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/
  47. javascript:highlandercomments.doexternallogout( 'googleplus' );
  48. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/
  49. javascript:highlandercomments.doexternallogout( 'twitter' );
  50. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/
  51. javascript:highlandercomments.doexternallogout( 'facebook' );
  52. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/
  53. javascript:highlandercomments.cancelexternalwindow();
  54. https://wordpress.com/?ref=footer_website
  55. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/
  56. https://automattic.com/cookies
  57. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  59. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/#comment-form-guest
  60. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/#comment-form-load-service:wordpress.com
  61. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/#comment-form-load-service:twitter
  62. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/#comment-form-load-service:facebook
