   (button) [1]ufldl tutorial

autoencoders
     __________________________________________________________________

   so far, we have described the application of neural networks to
   supervised learning, in which we have labeled training examples. now
   suppose we have only a set of unlabeled training examples \textstyle
   \{x^{(1)}, x^{(2)}, x^{(3)}, \ldots\}, where \textstyle x^{(i)} \in
   \re^{n}. an autoencoder neural network is an unsupervised learning
   algorithm that applies id26, setting the target values to be
   equal to the inputs. i.e., it uses \textstyle y^{(i)} = x^{(i)}.

   here is an autoencoder:

   [autoencoder636.png]

   the autoencoder tries to learn a function \textstyle h_{w,b}(x) \approx
   x. in other words, it is trying to learn an approximation to the
   identity function, so as to output \textstyle \hat{x} that is similar
   to \textstyle x. the identity function seems a particularly trivial
   function to be trying to learn; but by placing constraints on the
   network, such as by limiting the number of hidden units, we can
   discover interesting structure about the data. as a concrete example,
   suppose the inputs \textstyle x are the pixel intensity values from a
   \textstyle 10 \times 10 image (100 pixels) so \textstyle n=100, and
   there are \textstyle s_2=50 hidden units in layer \textstyle l_2. note
   that we also have \textstyle y \in \re^{100}. since there are only 50
   hidden units, the network is forced to learn a    compressed   
   representation of the input. i.e., given only the vector of hidden unit
   activations \textstyle a^{(2)} \in \re^{50}, it must try to
         reconstruct       the 100-pixel input \textstyle x. if the input were
   completely random   say, each \textstyle x_i comes from an iid gaussian
   independent of the other features   then this compression task would be
   very difficult. but if there is structure in the data, for example, if
   some of the input features are correlated, then this algorithm will be
   able to discover some of those correlations. in fact, this simple
   autoencoder often ends up learning a low-dimensional representation
   very similar to pcas.

   our argument above relied on the number of hidden units \textstyle s_2
   being small. but even when the number of hidden units is large (perhaps
   even greater than the number of input pixels), we can still discover
   interesting structure, by imposing other constraints on the network. in
   particular, if we impose a       sparsity       constraint on the hidden units,
   then the autoencoder will still discover interesting structure in the
   data, even if the number of hidden units is large.

   informally, we will think of a neuron as being    active    (or as
      firing   ) if its output value is close to 1, or as being    inactive    if
   its output value is close to 0. we would like to constrain the neurons
   to be inactive most of the time. this discussion assumes a sigmoid
   activation function. if you are using a tanh activation function, then
   we think of a neuron as being inactive when it outputs values close to
   -1.

   recall that \textstyle a^{(2)}_j denotes the activation of hidden unit
   \textstyle j in the autoencoder. however, this notation doesn   t make
   explicit what was the input \textstyle x that led to that activation.
   thus, we will write \textstyle a^{(2)}_j(x) to denote the activation of
   this hidden unit when the network is given a specific input \textstyle
   x.

   further, let
   \begin{align} \hat\rho_j = \frac{1}{m} \sum_{i=1}^m \left[
   a^{(2)}_j(x^{(i)}) \right] \end{align}

   be the average activation of hidden unit \textstyle j (averaged over
   the training set). we would like to (approximately) enforce the
   constraint
   \begin{align} \hat\rho_j = \rho, \end{align}

   where \textstyle \rho is a       sparsity parameter      , typically a small
   value close to zero (say \textstyle \rho = 0.05). in other words, we
   would like the average activation of each hidden neuron \textstyle j to
   be close to 0.05 (say). to satisfy this constraint, the hidden unit   s
   activations must mostly be near 0.

   to achieve this, we will add an extra penalty term to our optimization
   objective that penalizes \textstyle \hat\rho_j deviating significantly
   from \textstyle \rho. many choices of the penalty term will give
   reasonable results. we will choose the following:
   \begin{align} \sum_{j=1}^{s_2} \rho \log \frac{\rho}{\hat\rho_j} +
   (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j}. \end{align}

   here, \textstyle s_2 is the number of neurons in the hidden layer, and
   the index \textstyle j is summing over the hidden units in our network.
   if you are familiar with the concept of kl divergence, this penalty
   term is based on it, and can also be written
   \begin{align} \sum_{j=1}^{s_2} {\rm kl}(\rho || \hat\rho_j),
   \end{align}

   where \textstyle {\rm kl}(\rho || \hat\rho_j) = \rho \log
   \frac{\rho}{\hat\rho_j} + (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j} is
   the kullback-leibler (kl) divergence between a bernoulli random
   variable with mean \textstyle \rho and a bernoulli random variable with
   mean \textstyle \hat\rho_j. kl-divergence is a standard function for
   measuring how different two different distributions are. (if you   ve not
   seen kl-divergence before, don   t worry about it; everything you need to
   know about it is contained in these notes.)

   this penalty function has the property that \textstyle {\rm kl}(\rho ||
   \hat\rho_j) = 0 if \textstyle \hat\rho_j = \rho, and otherwise it
   increases monotonically as \textstyle \hat\rho_j diverges from
   \textstyle \rho. for example, in the figure below, we have set
   \textstyle \rho = 0.2, and plotted \textstyle {\rm kl}(\rho ||
   \hat\rho_j) for a range of values of \textstyle \hat\rho_j:

   [klpenaltyexample.png]

   we see that the kl-divergence reaches its minimum of 0 at
   \textstyle \hat\rho_j = \rho

   , and blows up (it actually

   approaches \textstyle \infty) as \textstyle \hat\rho_j approaches 0 or
   1. thus, minimizing this penalty term has the effect of causing
   \textstyle \hat\rho_j to be close to \textstyle \rho.

   our overall cost function is now
   \begin{align} j_{\rm sparse}(w,b) = j(w,b) + \beta \sum_{j=1}^{s_2}
   {\rm kl}(\rho || \hat\rho_j), \end{align}

   where \textstyle j(w,b) is as defined previously, and \textstyle \beta
   controls the weight of the sparsity penalty term. the term \textstyle
   \hat\rho_j (implicitly) depends on \textstyle w,b also, because it is
   the average activation of hidden unit \textstyle j, and the activation
   of a hidden unit depends on the parameters \textstyle w,b.

   to incorporate the kl-divergence term into your derivative calculation,
   there is a simple-to-implement trick involving only a small change to
   your code. specifically, where previously for the second layer
   (\textstyle l=2), during id26 you would have computed
   \begin{align} \delta^{(2)}_i = \left( \sum_{j=1}^{s_{2}} w^{(2)}_{ji}
   \delta^{(3)}_j \right) f'(z^{(2)}_i), \end{align}

   now instead compute
   \begin{align} \delta^{(2)}_i = \left( \left( \sum_{j=1}^{s_{2}}
   w^{(2)}_{ji} \delta^{(3)}_j \right) + \beta \left( -
   \frac{\rho}{\hat\rho_i} + \frac{1-\rho}{1-\hat\rho_i} \right) \right)
   f'(z^{(2)}_i) . \end{align}

   one subtlety is that you   ll need to know \textstyle \hat\rho_i to
   compute this term. thus, you   ll need to compute a forward pass on all
   the training examples first to compute the average activations on the
   training set, before computing id26 on any example. if your
   training set is small enough to fit comfortably in computer memory
   (this will be the case for the programming assignment), you can compute
   forward passes on all your examples and keep the resulting activations
   in memory and compute the \textstyle \hat\rho_is. then you can use your
   precomputed activations to perform id26 on all your
   examples. if your data is too large to fit in memory, you may have to
   scan through your examples computing a forward pass on each to
   accumulate (sum up) the activations and compute \textstyle \hat\rho_i
   (discarding the result of each forward pass after you have taken its
   activations \textstyle a^{(2)}_i into account for computing \textstyle
   \hat\rho_i). then after having computed \textstyle \hat\rho_i, you   d
   have to redo the forward pass for each example so that you can do
   id26 on that example. in this latter case, you would end up
   computing a forward pass twice on each example in your training set,
   making it computationally less efficient.

   the full derivation showing that the algorithm above results in
   id119 is beyond the scope of these notes. but if you
   implement the autoencoder using id26 modified this way, you
   will be performing id119 exactly on the objective \textstyle
   j_{\rm sparse}(w,b). using the derivative checking method, you will be
   able to verify this for yourself as well.

visualizing a trained autoencoder

   having trained a (sparse) autoencoder, we would now like to visualize
   the function learned by the algorithm, to try to understand what it has
   learned. consider the case of training an autoencoder on \textstyle 10
   \times 10 images, so that \textstyle n = 100. each hidden unit
   \textstyle i computes a function of the input:
   \begin{align} a^{(2)}_i = f\left(\sum_{j=1}^{100} w^{(1)}_{ij} x_j +
   b^{(1)}_i \right). \end{align}

   we will visualize the function computed by hidden unit \textstyle
   i   which depends on the parameters \textstyle w^{(1)}_{ij} (ignoring the
   bias term for now)   using a 2d image. in particular, we think of
   \textstyle a^{(2)}_i as some non-linear feature of the input \textstyle
   x. we ask: what input image \textstyle x would cause \textstyle
   a^{(2)}_i to be maximally activated? (less formally, what is the
   feature that hidden unit \textstyle i is looking for?) for this
   question to have a non-trivial answer, we must impose some constraints
   on \textstyle x. if we suppose that the input is norm constrained by
   \textstyle ||x||^2 = \sum_{i=1}^{100} x_i^2 \leq 1, then one can show
   (try doing this yourself) that the input which maximally activates
   hidden unit \textstyle i is given by setting pixel \textstyle x_j (for
   all 100 pixels, \textstyle j=1,\ldots, 100) to
   \begin{align} x_j = \frac{w^{(1)}_{ij}}{\sqrt{\sum_{j=1}^{100}
   (w^{(1)}_{ij})^2}}. \end{align}

   by displaying the image formed by these pixel intensity values, we can
   begin to understand what feature hidden unit \textstyle i is looking
   for.

   if we have an autoencoder with 100 hidden units (say), then we our
   visualization will have 100 such images   one per hidden unit. by
   examining these 100 images, we can try to understand what the ensemble
   of hidden units is learning.

   when we do this for a sparse autoencoder (trained with 100 hidden units
   on 10x10 pixel inputs^1 we get the following result:

   [examplesparseautoencoderweights.png]

   each square in the figure above shows the (norm bounded) input image
   \textstyle x that maximally actives one of 100 hidden units. we see
   that the different hidden units have learned to detect edges at
   different positions and orientations in the image.

   these features are, not surprisingly, useful for such tasks as object
   recognition and other vision tasks. when applied to other input domains
   (such as audio), this algorithm also learns useful
   representations/features for those domains too.
     __________________________________________________________________

   ^1   the learned features were obtained by training on       whitened      
   natural images. whitening is a preprocessing step which removes
   redundancy in the input, by causing adjacent pixels to become less
   correlated.

references

   1. http://ufldl.stanford.edu/tutorial/
