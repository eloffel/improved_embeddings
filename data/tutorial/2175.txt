   #[1]triangleinequality    feed [2]triangleinequality    comments feed
   [3]triangleinequality    theano, autoencoders and mnist comments feed
   [4]how to balance your binary search trees     avl trees [5]support
   vector machines and the kernel trick [6]alternate [7]alternate
   [8]triangleinequality [9]wordpress.com

     * [10]home
     * [11]about
     * [12]contents

[13]triangleinequality

   for matters mathematical
   search ____________________ (button) search
     __________________________________________________________________

   [14]home    [15]machine learning    theano, autoencoders and mnist

theano, autoencoders and mnist

   recently i have finally upgraded my ancient laptop. other than the
   ability for me to play the occasional video game, this means that i now
   have a dedicated nvidia graphics card, in particular one that supports
   something called [16]cuda.

   now that i have the hardware, i thought that it would be fun to check
   out this python library [17]theano. as i understand it, it has two very
   interesting aspects.

   one is that it does [18]symbolic computation, that is it represents
   variables as in mathematics, and their manipulations.

   a [19]variable in computer science is something like a integer or a
   string or  a list etc   , a piece of information stored somewhere in
   memory with a name made up by the programmer, and the compiler takes
   care of associating the human understandable name to the address of the
   information in memory.

   a[20] variable in mathematics is like x in x^2 + x + 1 , an
   indeterminate quantity that can be manipulated using arithmetic
   operations, or using calculus and so on. so what in mathematics we call
   a variable corresponds roughly to a function in computer science.

   the difference between symbolic computation and its variables, and
   regular functions, is that the programming language is aware of the
   mathematical nature of a variable, such that when you apply operations
   to variables to create new variables, it does this in an efficient way.
   the best way to understand this is a simple example. let   s suppose we
   have two functions f(x) =x, g(x) = -x . now we can combine these
   functions by adding them to make a new function h(x) = f(x) + g(x) .

   now in python say, when you call h with an argument, it will pass this
   argument to both f and g and then add the result and return it.

   if we are doing symbolic computation, and f,g were variables with f=-g
   , then the variable h=f+g=0 constantly. and so the computer does no
   work at all when using h , because it knows that it must always be
   zero.

   so one important aspect is that symbolic computation can simplify and
   hence often optimize functions automatically.

   another is that theano can automagically differentiate variables with
   respect to variables! this might not sound like a big deal, but it is.
   in machine learning, we often create a complicated function, like in
   id28 or neural networks, and then derive a helpful form
   for the derivative to do id119 to optimize it. in theano,
   all you have to do is specify the objective function, and it will let
   you do id119 automatically.

   the second amazing thing about theano is that it compiles its theano
   functions into c, and so it often runs much much faster than native
   python or numpy. it can also make it easier to run computations on a
   gpu which can mean a 100x speedup for the right cases!

   hopefully i   ve made you curious, so  [21]click here for an installation
   guide. on this linux machine it was very simple. i am dual booting
   windows 8.1 and ubuntu at the moment, it is very tricky to get the gpu
   aspect to work on windows, but here is[22] a guide for the brave. to
   check that the gpu aspect is working, run the test code [23]here.

   introduction to theano

   i recommend that you read the[24] tutorials here, and also this
   [25]excellent ipython worksheet here.

   let   s little play with the basic functionality.
import theano as th
from theano import tensor as t

#to make a variable, specify the type of variable from the tensor
#library.

#we can have scalars.
x=t.dscalar(name='scalar_x')

#note that the name is just a string that theano will use
#to communicate to us about the variable.
print x
print x.type
#we can also have vectors.
v = t.dvector(name='vector_v')
print v
print v.type
#and matrices.
a = t.dmatrix(name='matrix_a')
print a
print a.type

#we can also make new variables using standard mathematical operations.
x_2 = x*x
print x_2
print x_2.type

#we can also make python functions which return new variables.
def power(variable, n):
    return variable**n
x_10 = power(x,10)
print x_10
print x_10.type

#we can of course do standard id202 operations also.
av = t.dot(a,v)
print av
print av.type

#of course when a variable is actually evaluated, you must
#ensure that the dimensions are correct, or you will
#get an error.

#to see the value of a variable for a particular value of the variables
#comprising it, we make a theano function.
f = th.function([a,v], [av])
#the syntax is a list of input variables, and then a list of output variables.

#the python code takes a little longer to run initially, because thenao
#compiles the function into c, but thereafter it will run extremely fast.

#let's try using the function.
import numpy as np
m=np.ones((2,2))
print m
vec = np.asarray([1,2])
print vec
print f(m,vec)

#now we can try computing gradients.
#first let's make a scalar variable by taking an inner product.
w=t.dvector(name='w_vector')
vtw = t.dot(v,w)

#now we take the gradient with respect to w.
vtw_grad = t.grad(vtw,w)
print vtw_grad
print vtw_grad.type

#now let's test it.
vec1 = np.asarray([1,2])
vec2 = np.asarray([0,0])

#to evaulate a variable given inputs, there is another syntax
#in additon to creating a thenano function.
print vtw_grad.eval({w:vec1,v:vec2})


#next we discuss theano's shared variables,
#these differ from regular theano variables in that
#theano variables only have a value within a theano function
#whereas shared theano variables have a value independent of
#being called in a function.
w=t.shared(name='shared_matrix', value=np.ones((2,3)))
print w
print w.type
print w.get_value()

#you can also set a shared variable's value.
w.set_value(np.zeros((2,3)))
print w.get_value()

#you can also have theano functions update shared variables,
#we illustrate this with a silly example that updates a matrix
#like in id119.
x=t.dvector('x')
wx=t.dot(w,x)
cost = wx.mean()
cost_grad = t.grad(cost, w)

f=th.function(inputs=[x], outputs=[wx, cost], updates=[(w, w-0.1*cost_grad)])
#notice the syntax of updates argument, should be a list
#of two tuples of the form: (variable_to_be_updated, updated_variable).
print f([1,1,1])
print w.get_value()

   if you want to tinker with this, get the sourcecode [26]here.

   autoencoders

   i don   t want to go into too much detail here, but an [27]autoencoder is
   a[28] feedforward neural network with a single hidden layer with the
   twist being that its target output is exactly the same as its input. if
   the number of hidden neurons is less than the number of input/output
   neurons, then the network will be forced to learn some sort of
   compression.

   even when the number of hidden neurons is large, there are various
   tricks in cost functions you can use to ensure that the network does
   something more interesting than learning the identity function.

   the hope of an autoencoder is that it learns some structure in the
   data, so that the hidden layer can be thought of as a feature space
   that one can use for a supervised learning algorithm.

   basically then we have x \mapsto f_1(w_1x+b_1) =h the hidden layer,
   then h \mapsto f_2(w_2h+b_2) , where b_1, b_2 are bias vectors, w_1,
   w_2 are matrices, and f_1, f_2 are element-wise non-linearities like
   the logistic function.

   a simplification that is often made is to enforce w_1 = w_2^t . this
   gives the model less free parameters to learn, and so is a form of
   id173. it also uses less memory. this is referred to as an
   autoencoder with tied weights.

   autoencoders with theano

   thanks to automatic differentiation, backprogation is effortless with
   theano, and thanks to gpu computation, much faster also! i recommend
   having a look at my [29]python implementation of id26 just
   so you can see the effort saved. this implementation benefited greatly
   from [30]this tutorial.
import numpy as np
import theano as th
from theano import tensor as t
from numpy import random as rng

class autoencoder(object):
    def __init__(self, x, hidden_size, activation_function,
                 output_function):
        #x is the data, an m x n numpy matrix
        #where rows correspond to datapoints
        #and columns correspond to features.
        assert type(x) is np.ndarray
        assert len(x.shape)==2
        self.x=x
        self.x=th.shared(name='x', value=np.asarray(self.x,
                         dtype=th.config.floatx),borrow=true)
        #the config.floatx and borrow=true stuff is to get this to run
        #fast on the gpu. i recommend just doing this without thinking about
        #it until you understand the code as a whole, then learning more
        #about gpus and theano.
        self.n = x.shape[1]
        self.m = x.shape[0]
        #hidden_size is the number of neurons in the hidden layer, an int.
        assert type(hidden_size) is int
        assert hidden_size > 0
        self.hidden_size=hidden_size
        initial_w = np.asarray(rng.uniform(
                 low=-4 * np.sqrt(6. / (self.hidden_size + self.n)),
                 high=4 * np.sqrt(6. / (self.hidden_size + self.n)),
                 size=(self.n, self.hidden_size)), dtype=th.config.floatx)
        self.w = th.shared(value=initial_w, name='w', borrow=true)
        self.b1 = th.shared(name='b1', value=np.zeros(shape=(self.hidden_size,),
                            dtype=th.config.floatx),borrow=true)
        self.b2 = th.shared(name='b2', value=np.zeros(shape=(self.n,),
                            dtype=th.config.floatx),borrow=true)
        self.activation_function=activation_function
        self.output_function=output_function

    def train(self, n_epochs=100, mini_batch_size=1, learning_rate=0.1):
        index = t.lscalar()
        x=t.matrix('x')
        params = [self.w, self.b1, self.b2]
        hidden = self.activation_function(t.dot(x, self.w)+self.b1)
        output = t.dot(hidden,t.transpose(self.w))+self.b2
        output = self.output_function(output)

        #use cross-id178 loss.
        l = -t.sum(x*t.log(output) + (1-x)*t.log(1-output), axis=1)
        cost=l.mean()
        updates=[]

        #return gradient with respect to w, b1, b2.
        gparams = t.grad(cost,params)

        #create a list of 2 tuples for updates.
        for param, gparam in zip(params, gparams):
            updates.append((param, param-learning_rate*gparam))

        #train given a mini-batch of the data.
        train = th.function(inputs=[index], outputs=[cost], updates=updates,
                            givens={x:self.x[index:index+mini_batch_size,:]})


        import time
        start_time = time.clock()
        for epoch in xrange(n_epochs):
            print "epoch:",epoch
            for row in xrange(0,self.m, mini_batch_size):
                train(row)
        end_time = time.clock()
        print "average time per epoch=", (end_time-start_time)/n_epochs

    def get_hidden(self,data):
        x=t.dmatrix('x')
        hidden = self.activation_function(t.dot(x,self.w)+self.b1)
        transformed_data = th.function(inputs=[x], outputs=[hidden])
        return transformed_data(data)

    def get_weights(self):
        return [self.w.get_value(), self.b1.get_value(), self.b2.get_value()]

   and that is it! now we should test this puppy out, so let   s find some
   data.

   mnist

   the mnist data is famous in machine learning circles, it consists of
   single handwritten digits. nearly every paper written on neural
   networks and so on tests their contribution on the data. the task is to
   classify the digits, but we will just test our autoencoder. first we
   will steal a very nice helper function to load the data set, this will
   download it for you if you don   t have it already. you can find it
   [31]here.
import cpickle
import gzip
import os


def load_data(dataset):
    ''' loads the dataset

    :type dataset: string
    :param dataset: the path to the dataset (here mnist)
    '''

    #############
    # load data #
    #############

    # download the mnist dataset if it is not present
    data_dir, data_file = os.path.split(dataset)
    if data_dir == "" and not os.path.isfile(dataset):
        # check if dataset is in the data directory.
        new_path = os.path.join(os.path.split(__file__)[0], "..", "data", datase
t)
        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':
            dataset = new_path

    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':
        import urllib
        origin = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz
'
        print 'downloading data from %s' % origin
        urllib.urlretrieve(origin, dataset)

    print '... loading data'

    # load the dataset
    f = gzip.open(dataset, 'rb')
    train_set, valid_set, test_set = cpickle.load(f)
    f.close()
    #train_set, valid_set, test_set format: tuple(input, target)
    #input is an numpy.ndarray of 2 dimensions (a matrix)
    #which row's correspond to an example. target is a
    #numpy.ndarray of 1 dimensions (vector)) that have the same length as
    #the number of rows in the input. it should give the target
    #target to the example with the same index in the input.

    return (train_set, valid_set, test_set)


path="/home/harri/dropbox/work/blogs/triangleinequality/theano/data/mnist.pkl.gz
"

data=load_data(path)

   we also want a plotting function to visualize the features learned, i
   won   t explain this here, but if you google    visualizing features of
   neural network    you can learn plenty about it.
def plot_first_k_numbers(x,k):
    from matplotlib import mpl,pyplot
    m=x.shape[0]
    k=min(m,k)
    j = int(round(k / 10.0))

    fig, ax = pyplot.subplots(j,10)

    for i in range(k):

        w=x[i,:]


        w=w.reshape(28,28)
        ax[i/10, i%10].imshow(w,cmap=pyplot.cm.gist_yarg,
                      interpolation='nearest', aspect='equal')
        ax[i/10, i%10].axis('off')


    pyplot.tick_params(\
        axis='x',          # changes apply to the x-axis
        which='both',      # both major and minor ticks are affected
        bottom='off',      # ticks along the bottom edge are off
        top='off',         # ticks along the top edge are off
        labelbottom='off')
    pyplot.tick_params(\
        axis='y',          # changes apply to the x-axis
        which='both',      # both major and minor ticks are affected
        left='off',
        right='off',    # ticks along the top edge are off
        labelleft='off')

    fig.show()

   now we put it together.
def m_test(data):
    x=data[0][0]
    activation_function = t.nnet.sigmoid
    output_function=activation_function
    a = autoencoder(x, 500, activation_function, output_function)
    a.train(20,20)
    w=np.transpose(a.get_weights()[0])
    plot_first_k_numbers(w, 100, false)
    plot_first_k_numbers(w,100, true)

m_test(data)

   running this, with 500 hidden units for 20 epochs with minibatch size
   of 20, the following features were learned:

   [32]features_learned_500h20_20 nothing too interesting yet, in order to
   get more useful features we could change the cost function to encourage
   sparsity, or introduce noise into the inputs, or use a different
   activation function. hopefully this has been helpful for those just
   starting out with theano. [33]here is the source code.
   advertisements

share this:

     * [34]twitter
     * [35]facebook
     *

like this:

   like loading...

related

   tags: [36]autoencoder, [37]neural network, [38]python, [39]theano
   by [40]triangleinequality in [41]machine learning, [42]python on
   [43]august 12, 2014.
   [44]    how to balance your binary search trees     avl trees [45]support
   vector machines and the kernel trick    
     __________________________________________________________________

2 comments

    1. [46]quoc le   s lectures on deep learning says:
       [47]august 22, 2014 at 11:19 am
       [   ] theano, autoencoders and mnist [   ]
       [48]reply
    2. [49]understanding scan() in theano    triangleinequality says:
       [50]august 22, 2014 at 2:12 pm
       [   ] iteration over tensors and recurrences. if you are a complete
       beginner to theano, check out my previous post. it seemed a bit
       obscure when i first started trying to understand it, so i thought
       i would share [   ]
       [51]reply

leave a reply [52]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [53]googleplus-sign-in

     *
     *

   [54]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [55]log out /
   [56]change )
   google photo

   you are commenting using your google account. ( [57]log out /
   [58]change )
   twitter picture

   you are commenting using your twitter account. ( [59]log out /
   [60]change )
   facebook photo

   you are commenting using your facebook account. ( [61]log out /
   [62]change )
   [63]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

   [64]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [65]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [66]cookie policy

   iframe: [67]likes-master

   %d bloggers like this:

references

   visible links
   1. https://triangleinequality.wordpress.com/feed/
   2. https://triangleinequality.wordpress.com/comments/feed/
   3. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/feed/
   4. https://triangleinequality.wordpress.com/2014/07/15/how-to-balance-your-binary-search-trees-avl-trees/
   5. https://triangleinequality.wordpress.com/2014/08/14/support-vector-machines-and-the-kernel-trick/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/&for=wpcom-auto-discovery
   8. https://triangleinequality.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://triangleinequality.wordpress.com/
  11. https://triangleinequality.wordpress.com/about/
  12. https://triangleinequality.wordpress.com/contents/
  13. https://triangleinequality.wordpress.com/
  14. https://triangleinequality.wordpress.com/
  15. https://triangleinequality.wordpress.com/category/machine-learning/
  16. http://en.wikipedia.org/wiki/cuda
  17. http://deeplearning.net/software/theano/
  18. http://en.wikipedia.org/wiki/symbolic_computation
  19. http://en.wikipedia.org/wiki/variable_(computer_science)
  20. http://en.wikipedia.org/wiki/variable_(mathematics)
  21. http://deeplearning.net/software/theano/install.html#install
  22. http://pavel.surmenok.com/2014/05/31/installing-theano-with-gpu-on-windows-64-bit/
  23. http://deeplearning.net/software/theano/tutorial/using_gpu.html
  24. http://deeplearning.net/software/theano/tutorial/index.html#tutorial
  25. http://nbviewer.ipython.org/github/craffel/theano-tutorial/blob/master/theano tutorial.ipynb
  26. https://sourceforge.net/projects/triangleinequal/files/theano/introduction_to_theano.py/download
  27. http://en.wikipedia.org/wiki/autoencoder
  28. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  29. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/
  30. http://deeplearning.net/tutorial/da.html
  31. http://deeplearning.net/tutorial/logreg.html
  32. https://triangleinequality.files.wordpress.com/2014/08/features_learned_500h20_20.png
  33. https://sourceforge.net/projects/triangleinequal/files/theano/theano_autoencoder.py/download
  34. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/?share=twitter
  35. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/?share=facebook
  36. https://triangleinequality.wordpress.com/tag/autoencoder/
  37. https://triangleinequality.wordpress.com/tag/neural-network/
  38. https://triangleinequality.wordpress.com/tag/python/
  39. https://triangleinequality.wordpress.com/tag/theano/
  40. https://triangleinequality.wordpress.com/author/triangleinequality/
  41. https://triangleinequality.wordpress.com/category/machine-learning/
  42. https://triangleinequality.wordpress.com/category/python/
  43. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/
  44. https://triangleinequality.wordpress.com/2014/07/15/how-to-balance-your-binary-search-trees-avl-trees/
  45. https://triangleinequality.wordpress.com/2014/08/14/support-vector-machines-and-the-kernel-trick/
  46. http://bigsonata.com/quoc-les-lectures-on-deep-learning/
  47. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/#comment-155
  48. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/?replytocom=155#respond
  49. https://triangleinequality.wordpress.com/2014/08/22/understanding-scan-in-theano/
  50. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/#comment-156
  51. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/?replytocom=156#respond
  52. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/#respond
  53. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://triangleinequality.wordpress.com&color_scheme=light
  54. https://gravatar.com/site/signup/
  55. javascript:highlandercomments.doexternallogout( 'wordpress' );
  56. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/
  57. javascript:highlandercomments.doexternallogout( 'googleplus' );
  58. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/
  59. javascript:highlandercomments.doexternallogout( 'twitter' );
  60. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/
  61. javascript:highlandercomments.doexternallogout( 'facebook' );
  62. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/
  63. javascript:highlandercomments.cancelexternalwindow();
  64. https://wordpress.com/?ref=footer_blog
  65. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/
  66. https://automattic.com/cookies
  67. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  69. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/#comment-form-guest
  70. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/#comment-form-load-service:wordpress.com
  71. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/#comment-form-load-service:twitter
  72. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/#comment-form-load-service:facebook
