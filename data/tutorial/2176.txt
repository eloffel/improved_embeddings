   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets   
   attention and memory in deep learning and nlp comments feed [5]predict.
   share. deploy. with open data science     jan 20 webinar [6]wikipedia
   mining reveals hidden revolution of human priorities

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2016    [28]jan    [29]tutorials,
   overviews    attention and memory in deep learning and nlp
   ( [30]16:n01 )

attention and memory in deep learning and nlp

   [31][prv.gif] previous post
   [32]next post [nxt.gif]


   tags: [33]deep learning, [34]machine translation, [35]nlp,
   [36]recurrent neural networks

   an overview of attention mechanisms and memory in deep neural networks
   and why they work, including some specific applications in natural
   language processing and beyond.
     __________________________________________________________________

   by [37]denny britz, wildml.
   a recent trend in deep learning are attention mechanisms. in an
   [38]interview, ilya sutskever, now the research director of openai,
   mentioned that attention mechanisms are one of the most exciting
   advancements, and that they are here to stay. that sounds exciting. but
   what are attention mechanisms?
   attention mechanisms in neural networks are (very) loosely based on the
   visual attention mechanism found in humans. human visual attention is
   well-studied and while there exist different models, all of them
   essentially come down to being able to focus on a certain region of an
   image with    high resolution    while perceiving the surrounding image in
      low resolution   , and then adjusting the focal point over time.
   attention in neural networks has a long history, particularly in image
   recognition. examples include [39]learning to combine foveal glimpses
   with a third-order id82 or [40]learning where to attend
   with deep architectures for image tracking. but only recently have
   attention mechanisms made their way into recurrent neural networks
   architectures that are typically used in nlp (and increasingly also in
   vision). that   s what we   ll focus on in this post.
   what problem does attention solve?
   to understand what attention can do for us, let   s use neural machine
   translation (id4) as an example. traditional machine translation
   systems typically rely on sophisticated feature engineering based on
   the statistical properties of text. in short, these systems are
   complex, and a lot of engineering effort goes into building them.
   id4 systems work a bit differently. in id4, we
   map the meaning of a sentence into a fixed-length vector representation
   and then generate a translation based on that vector. by not relying on
   things like id165 counts and instead trying to capture the
   higher-level meaning of a text, id4 systems generalize to new sentences
   better than many other approaches. perhaps more importantly, id4
   systems are much easier to build and train, and they don   t require any
   manual feature engineering. in fact, [41]a simple implementation in
   tensorflow is no more than a few hundred lines of code.
   most id4 systems work by encoding the source sentence (e.g. a german
   sentence) into a vector using a [42]recurrent neural network, and then
   decoding an english sentence based on that vector, also using a id56.
   id56 nlp example
   in the picture above,    echt   ,    dicke    and    kiste    words are fed into an
   encoder, and after a special signal (not shown) the decoder starts
   producing a translated sentence. the decoder keeps generating words
   until a special end of sentence token is produced. here, the h vectors
   represent the internal state of the encoder.
   if you look closely, you can see that the decoder is supposed to
   generate a translation solely based on the last hidden state (h[3]
   above) from the encoder. this h[3] vector must encode everything we
   need to know about the source sentence. it must fully capture its
   meaning. in more technical terms, that vector is a sentence embedding.
   in fact, if you plot the embeddings of different sentences in a low
   dimensional space using pca or id167 for id84,
   [43]you can see that semantically similar phrases end up close to each
   other. that   s pretty amazing.
   still, it seems somewhat unreasonable to assume that we can encode all
   information about a potentially very long sentence into a single vector
   and then have the decoder produce a good translation based on only
   that. let   s say your source sentence is 50 words long. the first word
   of the english translation is probably highly correlated with the first
   word of the source sentence. but that means decoder has to consider
   information from 50 steps ago, and that information needs to be somehow
   encoded in the vector. recurrent neural networks are known to have
   problems dealing with such long-range dependencies. in theory,
   architectures like [44]lstms should be able to deal with this, but in
   practice long-range dependencies are still problematic. for example,
   researchers have found that reversing the source sequence (feeding it
   backwards into the encoder) produces significantly better results
   because it shortens the path from the decoder to the relevant parts of
   the encoder. similarly, [45]feeding an input sequence twice also seems
   to help a network to better memorize things.
   i consider the approach of reversing a sentence a    hack   . it makes
   things work better in practice, but it   s not a principled solution.
   most translation benchmarks are done on languages like french and
   german, which are quite similar to english (even chinese word order is
   quite similar to english). but there are languages (like japanese)
   where the last word of a sentence could be highly predictive of the
   first word in an english translation. in that case, reversing the input
   would make things worse. so, what   s an alternative? attention
   mechanisms.
   with an attention mechanism we no longer try encode the full source
   sentence into a fixed-length vector. rather, we allow the decoder to
      attend    to different parts of the source sentence at each step of the
   output generation. importantly, we let the model learn what to attend
   to based on the input sentence and what it has produced so far. so, in
   languages that are pretty well aligned (like english and german) the
   decoder would probably choose to attend to things sequentially.
   attending to the first word when producing the first english word, and
   so on. that   s what was done in [46]id4 by
   jointly learning to align and translate and look as follows:
   sequential id4
   here, the y   s are our translated words produced by the decoder, and the
   x   s are our source sentence words. the above illustration uses a
   bidirectional recurrent network, but that   s not important and you can
   just ignore the inverse direction. the important part is that each
   decoder output word y[t] now depends on a weighted combination of all
   the input states, not just the last state. the a   s are weights that
   define in how much of each input state should be considered for each
   output. so, if a[3,2] is a large number, this would mean that the
   decoder pays a lot of attention to the second state in the source
   sentence while producing the third word of the target sentence. the a   s
   are typically normalized to sum to 1 (so they are a distribution over
   the input states).
   a big advantage of attention is that it gives us the ability to
   interpret and visualize what the model is doing. for example, by
   visualizing the attention weight matrix a when a sentence is
   translated, we can understand how the model is translating:
   attention weight matrix visualized
   here we see that while translating from french to english, the network
   attends sequentially to each input state, but sometimes it attends to
   two words at time while producing an output, as in translation    la
   syrie    to    syria    for example.

   pages: 1 [47]2
     __________________________________________________________________

   [48][prv.gif] previous post
   [49]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [50]another 10 free must-read books for machine learning and data
       science
    2. [51]9 must-have skills you need to become a data scientist, updated
    3. [52]who is a typical data scientist in 2019?
    4. [53]the pareto principle for data scientists
    5. [54]what no one will tell you about data science job applications
    6. [55]19 inspiring women in ai, big data, data science, machine
       learning
    7. [56]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [57]id158s optimization using genetic algorithm
       with python
    2. [58]who is a typical data scientist in 2019?
    3. [59]8 reasons why you should get a microsoft azure certification
    4. [60]the pareto principle for data scientists
    5. [61]r vs python for data visualization
    6. [62]how to work in data science, ai, big data
    7. [63]the deep learning toolset     an overview

[64]latest news

     * [65]statistical thinking for industrial problem solving (st...
     * [66]from business intelligence to machine intelligence
     * [67]what is missing when ai makes a decision?
     * [68]spatio-temporal statistics: a primer
     * [69]another 10 free must-see courses for machine learning a...
     * [70]download your datax guide to ai in marketing

more recent stories

     * [71]download your datax guide to ai in marketing
     * [72]kdnuggets offer: save 20% on strata in london
     * [73]training a champion: building deep neural nets for big data
       an...
     * [74]building a recommender system
     * [75]predict age and gender using convolutional neural network and
       ...
     * [76]top tweets, mar 27     apr 02: here is a great explanat...
     * [77]odsc east is selling out; odsc india announced
     * [78]accelerate ai and data science career expo, may 4, 2019
     * [79]grow your data career at datasciencego, san diego, sep 27-29
     * [80]getting started with nlp using the pytorch framework
     * [81]how to diy your data science education
     * [82]top 8 data science use cases in gaming
     * [83]kdnuggets 19:n13, apr 3: top 10 data scientist coding mista...
     * [84]make better data-driven business decisions
     * [85]top stories, mar 25-31: r vs python for data visualization;
       th...
     * [86]two predictive analytics world events in europe this fall
     * [87]7 qualities your big data visualization tools absolutely must
       ...
     * [88]yeshiva university: tenure-track faculty in ai and machine
       lea...
     * [89]which face is real?
     * [90]yeshiva university: program director / tenure track faculty
       me...

   [91]kdnuggets home    [92]news    [93]2016    [94]jan    [95]tutorials,
   overviews    attention and memory in deep learning and nlp
   ( [96]16:n01 )
      2019 kdnuggets. [97]about kdnuggets.  [98]privacy policy. [99]terms
   of service

   [100]subscribe to kdnuggets news
   [101][tw_c48.png] [102]facebook [103]linkedin
   x
   [envelope.png] [104]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2016/01/attention-memory-deep-learning-nlp.html/feed
   5. https://www.kdnuggets.com/2016/01/continuum-predict-share-deploy-anaconda-webinar.html
   6. https://www.kdnuggets.com/2016/01/wikipedia-mining-hidden-revolution-human-priorities.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2016/index.html
  28. https://www.kdnuggets.com/2016/01/index.html
  29. https://www.kdnuggets.com/2016/01/tutorials.html
  30. https://www.kdnuggets.com/2016/n01.html
  31. https://www.kdnuggets.com/2016/01/continuum-predict-share-deploy-anaconda-webinar.html
  32. https://www.kdnuggets.com/2016/01/wikipedia-mining-hidden-revolution-human-priorities.html
  33. https://www.kdnuggets.com/tag/deep-learning
  34. https://www.kdnuggets.com/tag/machine-translation
  35. https://www.kdnuggets.com/tag/nlp
  36. https://www.kdnuggets.com/tag/recurrent-neural-networks
  37. https://www.kdnuggets.com/author/denny-britz
  38. https://re-work.co/blog/deep-learning-ilya-sutskever-google-openai
  39. http://papers.nips.cc/paper/4089-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine
  40. http://arxiv.org/abs/1109.3737
  41. https://www.tensorflow.org/versions/master/tutorials/id195/index.html#sequence-to-sequence-models
  42. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  43. http://arxiv.org/abs/1409.3215
  44. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/
  45. http://arxiv.org/abs/1410.4615
  46. http://arxiv.org/abs/1409.0473
  47. https://www.kdnuggets.com/2016/01/attention-memory-deep-learning-nlp.html/2
  48. https://www.kdnuggets.com/2016/01/continuum-predict-share-deploy-anaconda-webinar.html
  49. https://www.kdnuggets.com/2016/01/wikipedia-mining-hidden-revolution-human-priorities.html
  50. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  51. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  52. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  53. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  54. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  55. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  56. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  57. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  58. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  59. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  60. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  61. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  62. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  63. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  64. https://www.kdnuggets.com/news/index.html
  65. https://www.kdnuggets.com/2019/04/jmp-statistical-thinking-free-online-course.html
  66. https://www.kdnuggets.com/2019/04/datarobot-from-business-intelligence-machine-intelligence.html
  67. https://www.kdnuggets.com/2019/04/ai-makes-decision.html
  68. https://www.kdnuggets.com/2019/04/spatio-temporal-statistics-primer.html
  69. https://www.kdnuggets.com/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html
  70. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  71. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  72. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  73. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  74. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  75. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  76. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  77. https://www.kdnuggets.com/2019/04/odsc-east-selling-out-india-announced.html
  78. https://www.kdnuggets.com/jobs/19/04-03-accelerate-ai-data-science-career-expo-2019.html
  79. https://www.kdnuggets.com/2019/04/formulated-data-career-datasciencego-san-diego.html
  80. https://www.kdnuggets.com/2019/04/nlp-pytorch.html
  81. https://www.kdnuggets.com/2019/04/diy-your-data-science-education.html
  82. https://www.kdnuggets.com/2019/04/top-8-data-science-use-cases-gaming.html
  83. https://www.kdnuggets.com/2019/n13.html
  84. https://www.kdnuggets.com/2019/04/psu-make-better-data-driven-business-decisions.html
  85. https://www.kdnuggets.com/2019/04/top-news-week-0325-0331.html
  86. https://www.kdnuggets.com/2019/04/paw-two-predictive-analytics-world-events-europe-fall.html
  87. https://www.kdnuggets.com/2019/04/7-qualities-big-data-visualization-tools.html
  88. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-faculty-ai-machine-learning.html
  89. https://www.kdnuggets.com/2019/04/which-face-real-stylegan.html
  90. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-program-director-faculty-artificial-intelligence-machine-learning.html
  91. https://www.kdnuggets.com/
  92. https://www.kdnuggets.com/news/index.html
  93. https://www.kdnuggets.com/2016/index.html
  94. https://www.kdnuggets.com/2016/01/index.html
  95. https://www.kdnuggets.com/2016/01/tutorials.html
  96. https://www.kdnuggets.com/2016/n01.html
  97. https://www.kdnuggets.com/about/index.html
  98. https://www.kdnuggets.com/news/privacy-policy.html
  99. https://www.kdnuggets.com/terms-of-service.html
 100. https://www.kdnuggets.com/news/subscribe.html
 101. https://twitter.com/kdnuggets
 102. https://facebook.com/kdnuggets
 103. https://www.linkedin.com/groups/54257
 104. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
 106. https://www.kdnuggets.com/
 107. https://www.kdnuggets.com/
