   #[1]wildml    feed [2]wildml    comments feed [3]wildml    attention and
   memory in deep learning and nlp comments feed [4]implementing a id98 for
   text classification in tensorflow [5]deep learning for chatbots, part 1
       introduction [6]alternate [7]alternate

   [8]skip to content

   [9]wildml

   artificial intelligence, deep learning, and nlp

   (button) menu
     * [10]home
     * [11]ai newsletter
     * [12]deep learning glossary
     * [13]contact
     * [14]about

   posted on [15]january 3, 2016april 27, 2016 by [16]denny britz

attention and memory in deep learning and nlp

   a recent trend in deep learning are attention mechanisms. in an
   [17]interview, ilya sutskever, now the research director of openai,
   mentioned that attention mechanisms are one of the most exciting
   advancements, and that they are here to stay. that sounds exciting. but
   what are attention mechanisms?

   attention mechanisms in neural networks are (very) loosely based on the
   visual attention mechanism found in humans. human visual attention is
   well-studied and while there exist different models, all of them
   essentially come down to being able to focus on a certain region of an
   image with    high resolution    while perceiving the surrounding image in
      low resolution   , and then adjusting the focal point over time.

   attention in neural networks has a long history, particularly in image
   recognition. examples include [18]learning to combine foveal glimpses
   with a third-order id82 or [19]learning where to attend
   with deep architectures for image tracking. but only recently have
   attention mechanisms made their way into recurrent neural networks
   architectures that are typically used in nlp (and increasingly also in
   vision). that   s what we   ll focus on in this post.

what problem does attention solve?

   to understand what attention can do for us, let   s use neural machine
   translation (id4) as an example. traditional machine translation
   systems typically rely on sophisticated feature engineering based on
   the statistical properties of text. in short, these systems are
   complex, and a lot of engineering effort goes into building them.
   id4 systems work a bit differently. in id4, we
   map the meaning of a sentence into a fixed-length vector representation
   and then generate a translation based on that vector. by not relying on
   things like id165 counts and instead trying to capture the
   higher-level meaning of a text, id4 systems generalize to new sentences
   better than many other approaches. perhaps more importantly, ntm
   systems are much easier to build and train, and they don   t require any
   manual feature engineering. in fact, [20]a simple implementation in
   tensorflow is no more than a few hundred lines of code.

   most id4 systems work by encoding the source sentence (e.g. a german
   sentence) into a vector using a [21]recurrent neural network, and then
   decoding an english sentence based on that vector, also using a id56.

   [22]id56 for machine translation

   in the picture above,    echt   ,    dicke    and    kiste    words are fed into an
   encoder, and after a special signal (not shown) the decoder starts
   producing a translated sentence. the decoder keeps generating words
   until a special end of sentence token is produced. here, the h vectors
   represent the internal state of the encoder.

   if you look closely, you can see that the decoder is supposed to
   generate a translation solely based on the last hidden state ( h_3
   above) from the encoder. this h3 vector must encode everything we need
   to know about the source sentence. it must fully capture its meaning.
   in more technical terms, that vector is a sentence embedding. in fact,
   if you plot the embeddings of different sentences in a low dimensional
   space using pca or id167 for id84, [23]you can see
   that semantically similar phrases end up close to each other. that   s
   pretty amazing.

   still, it seems somewhat unreasonable to assume that we can encode all
   information about a potentially very long sentence into a single vector
   and then have the decoder produce a good translation based on only
   that. let   s say your source sentence is 50 words long. the first word
   of the english translation is probably highly correlated with the first
   word of the source sentence. but that means decoder has to consider
   information from 50 steps ago, and that information needs to be somehow
   encoded in the vector. recurrent neural networks are known to have
   problems dealing with such long-range dependencies. in theory,
   architectures like [24]lstms should be able to deal with this, but in
   practice long-range dependencies are still problematic. for example,
   researchers have found that reversing the source sequence (feeding it
   backwards into the encoder) produces significantly better results
   because it shortens the path from the decoder to the relevant parts of
   the encoder. similarly, [25]feeding an input sequence twice also seems
   to help a network to better memorize things.

   i consider the approach of reversing a sentence a    hack   . it makes
   things work better in practice, but it   s not a principled solution.
   most translation benchmarks are done on languages like french and
   german, which are quite similar to english (even chinese word order is
   quite similar to english). but there are languages (like japanese)
   where the last word of a sentence could be highly predictive of the
   first word in an english translation. in that case, reversing the input
   would make things worse. so, what   s an alternative? attention
   mechanisms.

   with an attention mechanism we no longer try encode the full source
   sentence into a fixed-length vector. rather, we allow the decoder to
      attend    to different parts of the source sentence at each step of the
   output generation. importantly, we let the model learn what to attend
   to based on the input sentence and what it has produced so far. so, in
   languages that are pretty well aligned (like english and german) the
   decoder would probably choose to attend to things sequentially.
   attending to the first word when producing the first english word, and
   so on. that   s what was done in [26]id4 by
   jointly learning to align and translate and look as follows:

   [27]id4 attention

   here, the y    s are our translated words produced by the decoder, and
   the x    s are our source sentence words. the above illustration uses a
   bidirectional recurrent network, but that   s not important and you can
   just ignore the inverse direction. the important part is that each
   decoder output word y_t now depends on a weighted combination of all
   the input states, not just the last state. the a    s are weights that
   define in how much of each input state should be considered for each
   output. so, if a_{3,2} is a large number, this would mean that the
   decoder pays a lot of attention to the second state in the source
   sentence while producing the third word of the target sentence. the a's
   are typically normalized to sum to 1 (so they are a distribution over
   the input states).

   a big advantage of attention is that it gives us the ability to
   interpret and visualize what the model is doing. for example, by
   visualizing the attention weight matrix a when a sentence is
   translated, we can understand how the model is translating:

   [28]id4 attention matrix

   here we see that while translating from french to english, the network
   attends sequentially to each input state, but sometimes it attends to
   two words at time while producing an output, as in translation    la
   syrie    to    syria    for example.

the cost of attention

   if we look a bit more look closely at the equation for attention we can
   see that attention comes at a cost. we need to calculate an attention
   value for each combination of input and output word. if you have a
   50-word input sequence and generate a 50-word output sequence that
   would be 2500 attention values. that   s not too bad, but if you do
   character-level computations and deal with sequences consisting of
   hundreds of tokens the above attention mechanisms can become
   prohibitively expensive.

   actually, that   s quite counterintuitive. human attention is something
   that   s supposed to save computational resources. by focusing on one
   thing, we can neglect many other things. but that   s not really what
   we   re doing in the above model. we   re essentially looking at everything
   in detail before deciding what to focus on. intuitively that   s
   equivalent outputting a translated word, and then going back through
   all of your internal memory of the text in order to decide which word
   to produce next. that seems like a waste, and not at all what humans
   are doing. in fact, it   s more akin to memory access, not attention,
   which in my opinion is somewhat of a misnomer (more on that below).
   still, that hasn   t stopped attention mechanisms from becoming quite
   popular and performing well on many tasks.

   an alternative approach to attention is to use id23
   to predict an approximate location to focus to. that sounds a lot more
   like human attention, and that   s what   s done in [29]recurrent models of
   visual attention.

attention beyond machine translation

   so far we   ve looked at attention applied to machine translation. but
   the same attention mechanism from above can be applied to any recurrent
   model. so let   s look at a few more examples.

   in [30]show, attend and tell the authors apply attention mechanisms to
   the problem of generating image descriptions. they use a convolutional
   neural network to    encode    the image, and a recurrent neural network
   with attention mechanisms to generate a description. by visualizing the
   attention weights (just like in the translation example), we interpret
   what the model is looking at while generating a word:

   [31]show, attend and tell attention visualization

   in [32]grammar as a foreign language, the authors use a recurrent
   neural network with attention mechanisk to generate sentence parse
   trees. the visualized attention matrix gives insight into how the
   network generates those trees:

   [33]screen shot 2015-12-30 at 1.49.19 pm

   in [34]teaching machines to read and comprehend, the authors use a id56
   to read a text, read a (synthetically generated) question, and then
   produce an answer. by visualizing the attention matrix we can see where
   the networks    looks    while it tries to find the answer to the question:

   [35]teaching machines to read and comprehend attention

attention = (fuzzy) memory?

   the basic problem that the attention mechanism solves is that it allows
   the network to refer back to the input sequence, instead of forcing it
   to encode all information into one fixed-length vector. as i mentioned
   above, i think that attention is somewhat of a misnomer. interpreted
   another way, the attention mechanism is simply giving the network
   access to its internal memory, which is the hidden state of the
   encoder. in this interpretation, instead of choosing what to    attend   
   to, the network chooses what to retrieve from memory. unlike typical
   memory, the memory access mechanism here is soft, which means that the
   network retrieves a weighted combination of all memory locations, not a
   value from a single discrete location. making the memory access soft
   has the benefit that we can easily train the network end-to-end using
   id26 (though there have been non-fuzzy approaches where the
   gradients are calculated using sampling methods instead of
   id26).

   memory mechanisms themselves have a much longer history. the hidden
   state of a standard recurrent neural network is itself a type of
   internal memory. id56s suffer from the [36]vanishing gradient problem
   that prevents them from learning long-range dependencies. [37]lstms
   improved upon this by using a gating mechanism that allows for explicit
   memory deletes and updates.

   the trend towards more complex memory structures is now continuing.
   [38]end-to-end memory networks allow the network to read same input
   sequence multiple times before making an output, updating the memory
   contents at each step. for example, answering a question by making
   multiple reasoning steps over an input story. however, when the
   networks parameter weights are tied in a certain way, the memory
   mechanism inend-to-end memory networks identical to the attention
   mechanism presented here, only that it makes multiple hops over the
   memory (because it tries to integrate information from multiple
   sentences).

   [39]id63s use a similar form of memory mechanism, but
   with a more sophisticated type of addressing that using both
   content-based (like here) and location-based addressing, allowing the
   network to learn addressing pattern to execute simple computer
   programs, like sorting algorithms.

   it   s likely that in the future we will see a clearer distinction
   between memory and attention mechanisms, perhaps along the lines of
   [40]id23 id63s, which try to learn
   access patterns to deal with external interfaces.


   categories[41]deep learning, [42]id38, [43]memory,
   [44]neural networks, [45]nlp

post navigation

   [46]previous postprevious implementing a id98 for text classification in
   tensorflow
   [47]next postnext deep learning for chatbots, part 1     introduction

subscribe to blog via email

   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

recent posts

     * [48]introduction to learning to trade with id23
     * [49]ai and deep learning in 2017     a year in review
     * [50]hype or not? some perspective on openai   s dota 2 bot
     * [51]learning id23 (with code, exercises and
       solutions)
     * [52]id56s in tensorflow, a practical guide and undocumented features
     * [53]deep learning for chatbots, part 2     implementing a
       retrieval-based model in tensorflow
     * [54]deep learning for chatbots, part 1     introduction
     * [55]attention and memory in deep learning and nlp

archives

     * [56]february 2018
     * [57]december 2017
     * [58]august 2017
     * [59]october 2016
     * [60]august 2016
     * [61]july 2016
     * [62]april 2016
     * [63]january 2016
     * [64]december 2015
     * [65]november 2015
     * [66]october 2015
     * [67]september 2015

categories

     * [68]conversational agents
     * [69]convolutional neural networks
     * [70]deep learning
     * [71]gpu
     * [72]id38
     * [73]memory
     * [74]neural networks
     * [75]news
     * [76]nlp
     * [77]recurrent neural networks
     * [78]id23
     * [79]id56s
     * [80]tensorflow
     * [81]trading
     * [82]uncategorized

meta

     * [83]log in
     * [84]entries rss
     * [85]comments rss
     * [86]wordpress.org

   [87]proudly powered by wordpress

references

   1. http://www.wildml.com/feed/
   2. http://www.wildml.com/comments/feed/
   3. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/feed/
   4. http://www.wildml.com/2015/12/implementing-a-id98-for-text-classification-in-tensorflow/
   5. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
   6. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
   7. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/&format=xml
   8. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/#content
   9. http://www.wildml.com/
  10. http://www.wildml.com/
  11. https://www.getrevue.co/profile/wildml
  12. http://www.wildml.com/deep-learning-glossary/
  13. mailto:dennybritz@gmail.com
  14. http://www.wildml.com/about/
  15. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  16. http://www.wildml.com/author/dennybritz/
  17. https://re-work.co/blog/deep-learning-ilya-sutskever-google-openai
  18. http://papers.nips.cc/paper/4089-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine
  19. http://arxiv.org/abs/1109.3737
  20. https://www.tensorflow.org/versions/master/tutorials/id195/index.html#sequence-to-sequence-models
  21. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  22. http://www.wildml.com/wp-content/uploads/2015/09/screen-shot-2015-09-17-at-10.39.06-am.png
  23. http://arxiv.org/abs/1409.3215
  24. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/
  25. http://arxiv.org/abs/1410.4615
  26. http://arxiv.org/abs/1409.0473
  27. http://www.wildml.com/wp-content/uploads/2015/12/screen-shot-2015-12-30-at-1.16.08-pm.png
  28. http://www.wildml.com/wp-content/uploads/2015/12/screen-shot-2015-12-30-at-1.23.48-pm.png
  29. http://arxiv.org/abs/1406.6247
  30. http://arxiv.org/abs/1502.03044
  31. http://www.wildml.com/wp-content/uploads/2015/12/screen-shot-2015-12-30-at-1.42.58-pm.png
  32. http://arxiv.org/abs/1412.7449
  33. http://www.wildml.com/wp-content/uploads/2015/12/screen-shot-2015-12-30-at-1.49.19-pm.png
  34. http://arxiv.org/abs/1506.03340
  35. http://www.wildml.com/wp-content/uploads/2015/12/screen-shot-2015-12-30-at-1.51.19-pm.png
  36. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
  37. http://deeplearning.cs.cmu.edu/pdfs/hochreiter97_lstm.pdf
  38. http://arxiv.org/abs/1503.08895
  39. https://github.com/dennybritz/deeplearning-papernotes/blob/master/neural-turing-machines.md
  40. http://arxiv.org/abs/1505.00521
  41. http://www.wildml.com/category/deep-learning/
  42. http://www.wildml.com/category/language-modeling/
  43. http://www.wildml.com/category/memory/
  44. http://www.wildml.com/category/neural-networks/
  45. http://www.wildml.com/category/nlp/
  46. http://www.wildml.com/2015/12/implementing-a-id98-for-text-classification-in-tensorflow/
  47. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  48. http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/
  49. http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/
  50. http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/
  51. http://www.wildml.com/2016/10/learning-reinforcement-learning/
  52. http://www.wildml.com/2016/08/id56s-in-tensorflow-a-practical-guide-and-undocumented-features/
  53. http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/
  54. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  55. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  56. http://www.wildml.com/2018/02/
  57. http://www.wildml.com/2017/12/
  58. http://www.wildml.com/2017/08/
  59. http://www.wildml.com/2016/10/
  60. http://www.wildml.com/2016/08/
  61. http://www.wildml.com/2016/07/
  62. http://www.wildml.com/2016/04/
  63. http://www.wildml.com/2016/01/
  64. http://www.wildml.com/2015/12/
  65. http://www.wildml.com/2015/11/
  66. http://www.wildml.com/2015/10/
  67. http://www.wildml.com/2015/09/
  68. http://www.wildml.com/category/conversational-agents/
  69. http://www.wildml.com/category/neural-networks/convolutional-neural-networks/
  70. http://www.wildml.com/category/deep-learning/
  71. http://www.wildml.com/category/gpu/
  72. http://www.wildml.com/category/language-modeling/
  73. http://www.wildml.com/category/memory/
  74. http://www.wildml.com/category/neural-networks/
  75. http://www.wildml.com/category/news/
  76. http://www.wildml.com/category/nlp/
  77. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  78. http://www.wildml.com/category/reinforcement-learning/
  79. http://www.wildml.com/category/id56s/
  80. http://www.wildml.com/category/tensorflow/
  81. http://www.wildml.com/category/trading/
  82. http://www.wildml.com/category/uncategorized/
  83. http://www.wildml.com/wp-login.php
  84. http://www.wildml.com/feed/
  85. http://www.wildml.com/comments/feed/
  86. https://wordpress.org/
  87. https://wordpress.org/
