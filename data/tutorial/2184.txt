   #[1]wildml    feed [2]wildml    comments feed [3]wildml    deep learning
   for chatbots, part 1     introduction comments feed [4]attention and
   memory in deep learning and nlp [5]deep learning for chatbots, part 2    
   implementing a retrieval-based model in tensorflow [6]alternate
   [7]alternate

   [8]skip to content

   [9]wildml

   artificial intelligence, deep learning, and nlp

   (button) menu
     * [10]home
     * [11]ai newsletter
     * [12]deep learning glossary
     * [13]contact
     * [14]about

   posted on [15]april 6, 2016may 30, 2016 by [16]denny britz

deep learning for chatbots, part 1     introduction

   chatbots, also called conversational agents or id71, are a
   hot topic. microsoft is making [17]big bets on chatbots, and so are
   companies like facebook (m), apple (siri), google, wechat, and slack.
   there is a new wave of startups trying to change how consumers interact
   with services by building consumer apps like [18]operator or [19]x.ai,
   bot platforms like [20]chatfuel, and bot libraries like [21]howdy   s
   botkit. microsoft recently released their own [22]bot developer
   framework.

   many companies are hoping to develop bots to have natural conversations
   indistinguishable from human ones, and many are claiming to be using
   nlp and deep learning techniques to make this possible. but with all
   the hype around ai it   s sometimes difficult to tell fact from fiction.

   in this series i want to go over some of the deep learning techniques
   that are used to build conversational agents, starting off by
   explaining where we are right now, what   s possible, and what will stay
   nearly impossible for at least a little while. this post will serve as
   an introduction, and we   ll get into the implementation details in
   upcoming posts.

a taxonomy of models

retrieval-based vs. generative models

   retrieval-based models (easier) use a repository of predefined
   responses and some kind of heuristic to pick an appropriate response
   based on the input and context. the heuristic could be as simple as a
   rule-based expression match, or as complex as an ensemble of machine
   learning classifiers. these systems don   t generate any new text, they
   just pick a response from a fixed set.

   generative models (harder) don   t rely on pre-defined responses. they
   generate new responses from scratch. generative models are typically
   based on machine translation techniques, but instead of translating
   from one language to another, we    translate    from an input to an output
   (response).

   [23]neural conversational model

   both approaches have some obvious pros and cons. due to the repository
   of handcrafted responses, retrieval-based methods don   t make
   grammatical mistakes. however, they may be unable to handle unseen
   cases for which no appropriate predefined response exists. for the same
   reasons, these models can   t refer back to contextual entity information
   like names mentioned earlier in the conversation. generative models are
      smarter   . they can refer back to entities in the input and give the
   impression that you   re talking to a human. however, these models are
   hard to train, are quite likely to make grammatical mistakes
   (especially on longer sentences), and typically require huge amounts of
   training data.

   deep learning techniques can be used for both retrieval-based or
   generative models, but research seems to be moving into the generative
   direction. deep learning architectures like [24]sequence to sequence
   are uniquely suited for generating text and researchers are hoping to
   make rapid progress in this area. however, we   re still at the early
   stages of building generative models that work reasonably well.
   production systems are more likely to be retrieval-based for now.

long vs. short conversations

   the longer the conversation the more difficult to automate it. on one
   side of the spectrum are short-text conversations (easier) where the
   goal is to create a single response to a single input. for example, you
   may receive a specific question from a user and reply with an
   appropriate answer. then there are long conversations (harder) where
   you go through multiple turns and need to keep track of what has been
   said. customer support conversations are typically long conversational
   threads with multiple questions.

open domain vs. closed domain

   in an open domain (harder) setting the user can take the conversation
   anywhere. there isn   t necessarily have a well-defined goal or
   intention. conversations on social media sites like twitter and reddit
   are typically open domain     they can go into all kinds of directions.
   the infinite number of topics and the fact that a certain amount of
   world knowledge is required to create reasonable responses makes this a
   hard problem.

   in a closed domain (easier) setting the space of possible inputs and
   outputs is somewhat limited because the system is trying to achieve a
   very specific goal. technical customer support or shopping assistants
   are examples of closed domain problems. these systems don   t need to be
   able to talk about politics, they just need to fulfill their specific
   task as efficiently as possible. sure, users can still take the
   conversation anywhere they want, but the system isn   t required to
   handle all these cases     and the users don   t expect it to.

common challenges

   there are some obvious and not-so-obvious challenges when building
   conversational agents most of which are active research areas.

incorporating context

   to produce sensible responses systems may need to incorporate both
   linguistic context and physical context. in long dialogs people keep
   track of what has been said and what information has been exchanged.
   that   s an example of linguistic context. the most common approach is to
   [25]embed the conversation into a vector, but doing that with long
   conversations is challenging. experiments in [26]building end-to-end
   dialogue systems using generative hierarchical neural network models
   and [27]attention with intention for a neural network conversation
   model both go into that direction. one may also need to incorporate
   other kinds of contextual data such as date/time, location, or
   information about a user.

coherent personality

   when generating responses the agent should ideally produce consistent
   answers to semantically identical inputs. for example, you want to get
   the same reply to    how old are you?    and    what is your age?   . this may
   sound simple, but incorporating such fixed knowledge or    personality   
   into models is very much a research problem. many systems learn to
   generate linguistic plausible responses, but they are not trained to
   generate semantically consistent ones. usually that   s because they are
   trained on a lot of data from multiple different users. models like
   that in [28]a persona-based neural conversation model are making first
   steps into the direction of explicitly modeling a personality.

   [29]example of incoherent responses of neural conversational model

evaluation of models

   the ideal way to evaluate a conversational agent is to measure whether
   or not it is fulfilling its task, e.g. solve a customer support
   problem, in a given conversation. but such labels are expensive to
   obtain because they require human judgment and evaluation. sometimes
   there is no well-defined goal, as is the case with open-domain models.
   common metrics such as [30]id7 that are used for machine translation
   and are based on text matching aren   t well suited because sensible
   responses can contain completely different words or phrases. in fact,
   in [31]how not to evaluate your dialogue system: an empirical study of
   unsupervised id74 for dialogue response generation
   researchers find that none of the commonly used metrics really
   correlate with human judgment.

intention and diversity

   a common problem with generative systems is that they tend to produce
   generic responses like    that   s great!    or    i don   t know    that work for
   a lot of input cases. early versions of google   s smart reply [32]tended
   to respond with    i love you    to almost anything. that   s partly a result
   of how these systems are trained, both in terms of data and in terms of
   actual training objective/algorithm. [33]some researchers have tried to
   artificially promote diversity through various objective functions.
   however, humans typically produce responses that are specific to the
   input and carry an intention. because generative systems (and
   particularly open-domain systems) aren   t trained to have specific
   intentions they lack this kind of diversity.

how well does it actually work?

   given all the cutting edge research right now, where are we and how
   well do these systems actually work? let   s consider our taxonomy again.
   a retrieval-based open domain system is obviously impossible because
   you can never handcraft enough responses to cover all cases. a
   generative open-domain system is almost artificial general intelligence
   (agi) because it needs to handle all possible scenarios. we   re very far
   away from that as well (but a lot of research is going on in that
   area).

   this leaves us with problems in restricted domains where both
   generative and retrieval based methods are appropriate. the longer the
   conversations and the more important the context, the more difficult
   the problem becomes.

   in a [34]recent interview, andrew ng, now chief scientist of baidu,
   puts it well:

     most of the value of deep learning today is in narrow domains where
     you can get a lot of data. here   s one example of something it cannot
     do: have a meaningful conversation. there are demos, and if you
     cherry-pick the conversation, it looks like it   s having a meaningful
     conversation, but if you actually try it yourself, it quickly goes
     off the rails.

   many companies start off by outsourcing their conversations to human
   workers and promise that they can    automate    it once they   ve collected
   enough data. that   s likely to happen only if they are operating in a
   pretty narrow domain     like a chat interface to call an uber for
   example. anything that   s a bit more open domain (like sales emails) is
   beyond what we can currently do. however, we can also use these systems
   to assist human workers by proposing and correcting responses. that   s
   much more feasible.

   grammatical mistakes in production systems are very costly and may
   drive away users. that   s why most systems are probably best off using
   retrieval-based methods that are free of grammatical errors and
   offensive responses. if companies can somehow get their hands on huge
   amounts of data then generative models become feasible     but they must
   be assisted by other techniques to prevent them from going off the
   rails [35]like microsoft   s tay did.

upcoming & reading list

   we   ll get into the technical details of how to implement
   retrieval-based and generative conversational models using deep
   learning in the next post, but if you   re interested in looking at some
   of the research then the following papers are a good starting point:
     * [36]neural responding machine for short-text conversation (2015-03)
     * [37]a neural conversational model (2015-06)
     * [38]a neural network approach to context-sensitive generation of
       conversational responses (2015-06)
     * [39]the ubuntu dialogue corpus: a large dataset for research in
       unstructured multi-turn dialogue systems (2015-06)
     * [40]building end-to-end dialogue systems using generative
       hierarchical neural network models (2015-07)
     * [41]a diversity-promoting objective function for neural
       conversation models (2015-10)
     * [42]attention with intention for a neural network conversation
       model (2015-10)
     * [43]improved deep learning baselines for ubuntu corpus dialogs
       (2015-10)
     * [44]a survey of available corpora for building data-driven dialogue
       systems (2015-12)
     * [45]incorporating copying mechanism in sequence-to-sequence
       learning (2016-03)
     * [46]a persona-based neural conversation model (2016-03)
     * [47]how not to evaluate your dialogue system: an empirical study of
       unsupervised id74 for dialogue response generation
       (2016-03)


   categories[48]conversational agents, [49]deep learning, [50]neural
   networks, [51]nlp, [52]id56s

post navigation

   [53]previous postprevious attention and memory in deep learning and nlp
   [54]next postnext deep learning for chatbots, part 2     implementing a
   retrieval-based model in tensorflow

subscribe to blog via email

   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

recent posts

     * [55]introduction to learning to trade with id23
     * [56]ai and deep learning in 2017     a year in review
     * [57]hype or not? some perspective on openai   s dota 2 bot
     * [58]learning id23 (with code, exercises and
       solutions)
     * [59]id56s in tensorflow, a practical guide and undocumented features
     * [60]deep learning for chatbots, part 2     implementing a
       retrieval-based model in tensorflow
     * [61]deep learning for chatbots, part 1     introduction
     * [62]attention and memory in deep learning and nlp

archives

     * [63]february 2018
     * [64]december 2017
     * [65]august 2017
     * [66]october 2016
     * [67]august 2016
     * [68]july 2016
     * [69]april 2016
     * [70]january 2016
     * [71]december 2015
     * [72]november 2015
     * [73]october 2015
     * [74]september 2015

categories

     * [75]conversational agents
     * [76]convolutional neural networks
     * [77]deep learning
     * [78]gpu
     * [79]id38
     * [80]memory
     * [81]neural networks
     * [82]news
     * [83]nlp
     * [84]recurrent neural networks
     * [85]id23
     * [86]id56s
     * [87]tensorflow
     * [88]trading
     * [89]uncategorized

meta

     * [90]log in
     * [91]entries rss
     * [92]comments rss
     * [93]wordpress.org

   [94]proudly powered by wordpress

references

   1. http://www.wildml.com/feed/
   2. http://www.wildml.com/comments/feed/
   3. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/feed/
   4. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
   5. http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/
   6. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
   7. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/&format=xml
   8. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/#content
   9. http://www.wildml.com/
  10. http://www.wildml.com/
  11. https://www.getrevue.co/profile/wildml
  12. http://www.wildml.com/deep-learning-glossary/
  13. mailto:dennybritz@gmail.com
  14. http://www.wildml.com/about/
  15. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  16. http://www.wildml.com/author/dennybritz/
  17. http://www.bloomberg.com/features/2016-microsoft-future-ai-chatbots/
  18. https://operator.com/
  19. https://x.ai/
  20. http://chatfuel.com/
  21. http://howdy.ai/botkit/
  22. https://dev.botframework.com/
  23. http://www.wildml.com/wp-content/uploads/2016/04/nct-id195.png
  24. http://arxiv.org/abs/1409.3215
  25. https://en.wikipedia.org/wiki/word_embedding
  26. http://arxiv.org/abs/1507.04808
  27. http://arxiv.org/abs/1510.08565
  28. http://arxiv.org/abs/1603.06155
  29. http://www.wildml.com/wp-content/uploads/2016/04/screen-shot-2016-04-04-at-6.36.59-pm.png
  30. https://en.wikipedia.org/wiki/id7
  31. http://arxiv.org/abs/1603.08023
  32. http://googleresearch.blogspot.com/2015/11/computer-respond-to-this-email.html
  33. http://arxiv.org/abs/1510.03055
  34. http://www.seattletimes.com/business/baidu-research-chief-andrew-ng-fixed-on-self-taught-computers-self-driving-cars/
  35. http://www.businessinsider.com/microsoft-deletes-racist-genocidal-tweets-from-ai-chatbot-tay-2016-3
  36. http://arxiv.org/abs/1503.02364
  37. http://arxiv.org/abs/1506.05869
  38. http://arxiv.org/abs/1506.06714
  39. http://arxiv.org/abs/1506.08909
  40. http://arxiv.org/abs/1507.04808
  41. http://arxiv.org/abs/1510.03055
  42. http://arxiv.org/abs/1510.08565
  43. http://arxiv.org/abs/1510.03753
  44. http://arxiv.org/abs/1512.05742
  45. http://arxiv.org/abs/1603.06393
  46. http://arxiv.org/abs/1603.06155
  47. http://arxiv.org/abs/1603.08023
  48. http://www.wildml.com/category/conversational-agents/
  49. http://www.wildml.com/category/deep-learning/
  50. http://www.wildml.com/category/neural-networks/
  51. http://www.wildml.com/category/nlp/
  52. http://www.wildml.com/category/id56s/
  53. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  54. http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/
  55. http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/
  56. http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/
  57. http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/
  58. http://www.wildml.com/2016/10/learning-reinforcement-learning/
  59. http://www.wildml.com/2016/08/id56s-in-tensorflow-a-practical-guide-and-undocumented-features/
  60. http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/
  61. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  62. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  63. http://www.wildml.com/2018/02/
  64. http://www.wildml.com/2017/12/
  65. http://www.wildml.com/2017/08/
  66. http://www.wildml.com/2016/10/
  67. http://www.wildml.com/2016/08/
  68. http://www.wildml.com/2016/07/
  69. http://www.wildml.com/2016/04/
  70. http://www.wildml.com/2016/01/
  71. http://www.wildml.com/2015/12/
  72. http://www.wildml.com/2015/11/
  73. http://www.wildml.com/2015/10/
  74. http://www.wildml.com/2015/09/
  75. http://www.wildml.com/category/conversational-agents/
  76. http://www.wildml.com/category/neural-networks/convolutional-neural-networks/
  77. http://www.wildml.com/category/deep-learning/
  78. http://www.wildml.com/category/gpu/
  79. http://www.wildml.com/category/language-modeling/
  80. http://www.wildml.com/category/memory/
  81. http://www.wildml.com/category/neural-networks/
  82. http://www.wildml.com/category/news/
  83. http://www.wildml.com/category/nlp/
  84. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  85. http://www.wildml.com/category/reinforcement-learning/
  86. http://www.wildml.com/category/id56s/
  87. http://www.wildml.com/category/tensorflow/
  88. http://www.wildml.com/category/trading/
  89. http://www.wildml.com/category/uncategorized/
  90. http://www.wildml.com/wp-login.php
  91. http://www.wildml.com/feed/
  92. http://www.wildml.com/comments/feed/
  93. https://wordpress.org/
  94. https://wordpress.org/
