   #[1]rare technologies    feed [2]rare technologies    comments feed
   [3]rare technologies    text summarization in python: extractive vs.
   abstractive techniques revisited comments feed [4]alternate
   [5]alternate

   [tr?id=1761346240851963&ev=pageview&noscript=1]

   iframe: [6]https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld

   [7]pragmatic machine learning rare technologies [8]navigation

     * [9]services
     * [10]products
          + [11]pii tools
          + [12]scaletext
     * [13]corporate training
          + [14]overview
          + [15]python best practices
          + [16]practical machine learning
          + [17]topic modelling
          + [18]deep learning in practice
     * [19]for students
          + [20]open source
          + [21]incubator
          + [22]competitions
     * [23]company
          + [24]careers
          + [25]our team
     * [26]blog
     * [27]contact
     * [28]search

     * [29]services
     * [30]products
          + [31]pii tools
          + [32]scaletext
     * [33]corporate training
          + [34]overview
          + [35]python best practices
          + [36]practical machine learning
          + [37]topic modelling
          + [38]deep learning in practice
     * [39]for students
          + [40]open source
          + [41]incubator
          + [42]competitions
     * [43]company
          + [44]careers
          + [45]our team
     * [46]blog
     * [47]contact
     * [48]search

text summarization in python: extractive vs. abstractive techniques revisited

   [49]pranay, aman and aayush 2017-04-05[50] gensim, [51]student
   incubator, [52]summarization

   this blog is a gentle introduction to text summarization and can serve
   as a practical summary of the current landscape. it describes how we, a
   team of three students in the rare [53]incubator programme, have
   experimented with existing algorithms and python tools in this domain.

   we compare modern extractive methods like lexrank, lsa, luhn and
   [54]gensim   s existing textrank summarization module on the [55]opinosis
   dataset of 51 article-summary pairs. we also had a try with an
   abstractive technique using [56]tensorflow   s text summarization
   algorithm, but didn   t obtain good results due to its extremely high
   hardware demands [57](7000 gpu hours, ~$30k cloud credits).

why text summarization?

   with push notifications and article digests gaining more and more
   traction, the task of generating intelligent and accurate summaries for
   long pieces of text has become a popular research as well as industry
   problem.

   there are two fundamental approaches to text summarization: extractive
   and abstractive. the former extracts words and word phrases from the
   original text to create a summary. the latter learns an internal
   language representation to generate more human-like summaries,
   id141 the intent of the original text.
   [header-1-e1486937763385.jpg]

   there are two fundamental approaches to text summarization: extractive
   and abstractive.

extractive text summarization

   first, a quick description of some popular algorithms & implementations
   for text summarization that exist today:
     * text summarization in gensim
       gensim.summarization module implements textrank, an unsupervised
       algorithm based on weighted-graphs from a [58]paper by mihalcea et
       al. it was added by another incubator student olavur mortensen    
       see his previous [59]post on this blog. it is built on top of the
       popular [60]id95 algorithm that google used for ranking
       webpages. textrank works as follows:

    1. pre-process the text: remove stop words and stem the remaining
       words.
    2. create a graph where vertices are sentences.
    3. connect every sentence to every other sentence by an edge. the
       weight of the edge is how similar the two sentences are.
    4. run the id95 algorithm on the graph.
    5. pick the vertices(sentences) with the highest id95 score

   in original textrank the weights of an edge between two sentences is
   the percentage of words appearing in both of them. gensim   s textrank
   uses [61]okapi bm25 function to see how similar the sentences are. it
   is an improvement from a [62]paper by barrios et al..

     pyteaser

   [63]pyteaser is a python implementation of the scala project
   [64]textteaser, which is a heuristic approach for extractive text
   summarization.

   textteaser associates a score with every sentence. this score is a
   linear combination of features extracted from that sentence. features
   that textteaser looks at are:
     * titlefeature: the count of words which are common to title of the
       document and sentence.
     * sentencelength: authors of textteaser defined a constant    ideal   
       (with value 20), which represents the ideal length of the summary,
       in terms of number of words. sentencelength is calculated as a
       normalized distance from this value.
     * sentenceposition: normalized sentence number (position in the list
       of sentences).
     * keywordfrequency: term frequency in the bag-of-words model (after
       removing stop words).

   more on the sentence features for summarization see [65]sentence
   extraction based single document summarization by jagadeesh et al.

     pytextrank

   [66]pytextrank is a python implementation of the original textrank
   algorithm with a few enhancements like using lemmatization instead of
   id30, incorporating part-of-speech tagging and named entity
   resolution, extracting key phrases from the article and extracting
   summary sentences based on them.  along with a summary of the article,
   pytextrank also extracts meaningful key phrases from the article.
   pytextrank works in four stages, each feeding its output to the next:
    1. in the first stage, part-of-speech tagging and lemmatization is
       performed for every sentence in the document.
    2. in the second stage, key phrases are extracted along with their
       counts, and are normalized.
    3. calculates a score for each sentence by approximating jaccard
       distance between the sentence and key phrases.
    4. summarizes the document based on most significant sentences and key
       phrases.

     luhn   s algorithm

   published in 1958, this algorithm [[67]pdf] ranks sentences for
   summarization extracts by considering    significant    words, which are
   frequently occurring words in a document, and the linear distance
   between these words due to non-significant words.

     lexrank

   [68]lexrank is an unsupervised graph based approach similar to
   textrank. lexrank uses idf-modified cosine as the similarity measure
   between two sentences. this similarity is used as weight of the graph
   edge between two sentences. lexrank also incorporates an intelligent
   post-processing step which makes sure that top sentences chosen for the
   summary are not too similar to each other.

   more on lexrank vs. textrank can be found [69]here.

     latent semantic analysis (lsa) in text summarization

   lsa works by projecting the data into a lower dimensional space without
   any significant loss of information. one way to interpret this spatial
   decomposition operation is that singular vectors can capture and
   represent word combination patterns which are recurring in the corpus.
   the magnitude of the singular value indicates the importance of the
   pattern in a document.

   if terms like singular vectors and singular values seem unfamiliar, we
   recommend [70]this tutorial, which covers the theory of lsa, including
   an instructive, if naive, python implementation (for a robust and fast
   implementation, use [71]lsa in gensim, of course).

  how to evaluate text summarization quality?

     * the id8-n metric
       for lexrank, luhn and lsa methods we made use of the
       [72]sumy summarization library which implements these algorithms.
       we used the id8-1 metric to compare the discussed techniques.
       [73]id8-n is a word id165 measure between the model and the gold
       summary.
       specifically, it is the ratio of the count of id165 phrases which
       occur in both the model and gold summary, to the count of all
       id165 phrases that are present in the gold summary.
       another way to interpret it is as the recall value which measures
       how many id165s from the gold summaries appeared in the model
       summaries.
       generally for summarization evaluation, only id8-1 and id8-2
       (sometimes id8-3, if we have really long gold and model
       summaries) metrics are used, rationale being that as we increase n,
       we increase the length of the id165 word phrase that needs to be
       matched completely in both the gold and model summary.
       as an example, consider two semantically similar phrases    apples
       bananas    and    bananas apples   . if we use id8-1 we only consider
       uni-grams, which are the same for both phrases. but if we use
       id8-2, we use 2-word phrases, so    apples bananas    become a single
       entity which is different from    bananas apples   , leading to a
          miss    and lower evaluation score.
       example:
       gold summary: a good diet must have apples and bananas.
       model apples and bananas are must for a good diet.
       if we use the id8-1, the score is 7/8 = 0.875.
       for id8-2, it is 4/7 = ~0.57.
       the above ratios can be interpreted as the amount of relevant
       information that our algorithm managed to extract from the set of
       all the relevant information, which is exactly the definition of
       recall, and hence id8 is recall based.
       more examples of how to calculate the scores are in this [74]gist.
     * the id7 metric
       [75]id7 metric is a modified form of precision, extensively used
       in machine translation evaluation.
       precision is the ratio of the number of words that co-occur in both
       gold and model translation/summary to the number of words in the
       model summary. unlike id8, id7 directly accounts for variable
       length phrases     unigrams, bigrams, trigrams etc., by taking a
       weighted average.
       the actual metric is just precision which is modified to avoid the
       problem when a model   s translation/summary contains repeated
       relevant information.
       example:
       gold summary: a good diet must have apples and bananas.
       model summary: apples and bananas are must for a good diet.
       if we use the id7 score considering only unigrams, i.e., weight of
       unigram is 1 and 0 for all other id165s, our ratio for id7 is
       calculated as 7/9 = 0.778.
       for weights [0.6, 0.4] for unigram and bigram respectively, the
       ratio becomes 0.6 * (7/9) + 0.4 * (4/8) = 0.667.
     * id7 with modified id165 precision
       the key intuition behind modified id165 precision is that a
       reference phrase/word should be considered exhausted once it has
       been identified in the model summary. this idea addresses the
       problem of repeated/over-generated words in the model summary.
       modified id165 precision is computed by first finding the maximum
       number of times a word/phrase occurs in any single reference. this
       count becomes the maximum reference count for that word/phrase. we
       then clip the total count of each model word/phrase by its maximum
       reference count, add the clipped counts for each word in the model
       translation/summary and divide the sum by the total number of
       words/phrases in the model translation/summary.
       the link to the paper on id7 (see above) has good examples on its
       modified id165 precision.

   tl;dr: the greater the id8 and id7 score, the better the summary.

  dataset

   comparison was performed using the [76]opinosis dataset of 51 articles.
   each article is about a product   s feature, like ipod   s battery life,
   etc. and is a collection of reviews by customers who purchased that
   product. each article in the dataset has 5 manually written    gold   
   summaries. usually the 5 gold summaries are different but they can also
   be the same text repeated 5 times.

  model parameters

   for gensim textrank, the count of words in the output summary,
   word_count was set to 75.
   for sumy-lsa and sumy-lex_rank the count of sentences in the output
   summary(sentence_count) was set to 2.

  results

   the mean and standard deviation of id8-1 and id7 scores obtained are
   shown in the table below

   model maximum id8-1 score std deviation of id8-1 score id7 score
   std deviation of id7 score
   sumy-lex_rank 0.26 0.06 0.391 0.095
   sumy-lsa 0.211 0.059 0.374 0.086
   sumy-luhn 0.126 0.038 0.157 0.044
   pyteaser 0.221 0.053 0.343 0.072
   gensim textrank 0.230 0.058 0.388 0.095
   pytextrank 0.197 0.045 0.269 0.059

   id8 scores for every summary is the maximum id8 score amongst the
   five (individual gold summary) scores.

   for id7 score we used nltk   s  [77]id7_score module with weights for
   unigrams, bigrams and trigrams as 0.4, 0.3, 0.2 respectively.

   garmin

   for a concrete example, have a look at [78]this review of the garmin
   255w navigation device. see both the [79]human and model-generated
   summaries.

  qualitative assessment

   lexrank is the winner here as it yields a better id8 and id7 score.
   unfortunately we found the summaries generated by it to be less
   informative than summaries by gensim   s textrank and luhn   s model.
   furthermore, lexrank doesn   t always beat textrank in the id8 score    
   for example, textrank performs marginally better than lexrank on the
   [80]duc 2002 dataset. so the choice between lexrank and textrank
   depends on your dataset, it   s worth trying both.

   another conclusion from the data is that gensim   s textrank outperforms
   the plain pytextrank because it uses the bm25 function instead of
   cosine idf in plain textrank.

   another point from the table is that luhn   s algorithm has a lower id7
   score. this is because it extracts a longer summary and hence covers
   more reviews of the product. unfortunately, we couldn   t make it shorter
   because the wrapper for luhn   s algorithm in sumy doesn   t provide the
   parameters to change the word limit.

  abstractive text summarization

   a neural network approach

   google   s [81]textsum is a state of the art open-source abstractive text
   summarization architecture. it can create headlines for news articles
   based on their first two sentences.

   it has shown good results after training on 4 million pairs from the
   gigaword dataset of the form (first two sentences, headline). during
   training it optimises the likelihood of the summary given the article   s
   first two sentences. both the encoding layer and language model are
   trained at the same time. in order to generate a summary it searches
   the space of all possible summaries to find the most likely sequence of
   words for the given article.

   here is an example of the data used for training the textsum model
   together with the model-generated summary.

   article novell inc. chief executive officer eric schmidt has been named
   chairman of the internet search-engine company google .
   human summary novell ceo named google chairman
   textsum novell chief executive named to head internet company

   notice that the word    head    doesn   t appear in the original text. the
   model has generated it. this would never happen in an extractive
   algorithm above.
     __________________________________________________________________

   we ran the tensorflow network provided by google and tweaked some of
   its hyperparameters. unfortunately we could only train the model for
   10% of the time of what was needed and got summaries of very low
   quality. we couldn   t even use the id8 and id7 scores above due to
   the summaries not making any sense.

   to compare different tweaks to the neural network architecture we had
   to resort to using a mathematical measure of the model fit on the
   training set [82]   running average loss   . the average running loss
   graphs for the models can be found in [83]this gist.

   how much is    sufficiently trained   ?
   it is advised by the authors of tensorflow   s implementation to
   [84]train for over million time-steps to successfully reproduce their
   results. this would mean weeks of training time on gpu enabled
   clusters. google themselves used [85]10 machines with 4 gpus each,
   training for a
   week. that is equivalent to 7000 gpu hours or $30k aws cloud credits.
   we didn   t have such hardware resources at our disposal.

   also google textsum authors use the annotated english gigaword dataset
   which requires a $3000 license. so instead, we use a relatively small
   but free news article data set: [86]id98 and dailymail. these 320k
   articles are converted into a textsum compatible format and vocabulary.
   you can generate your own textsum compatible pre-processed id98 and
   dailymail data by using our code from [87]github.

   initially, the training with default parameters was done on an nvidia
   gtx 950m laptop but the algorithm did not seem to converge even after
   training for more than 48 hours. to speed up the process and generate
   meaningful summaries we switched to a g2.2xlarge amazon ec2 instance,
   equipped with nvidia   s k520 gpu.

  observations

   some examples of the very bad summaries generated by our insufficently
   trained textsum model. this is similar to the attempt to train textsum
   in [88]pavel surmenok   s blog.

   human written summary textsum summary (trained for less than 50k steps)
   textsum generated summary (trained for 100k+ steps)
   criticised over     incompetent     handling of bank    s tax evasion scandal
   <unk> <unk> the to to to in a manchester united face manchester city in
   the premier league on sunday night
   alleged hoax on manti te   o is compared with the documentary     catfish    
   <unk> <unk> miss , to the supreme anniversary of the number of people
   are less than 200 days from the u . s . airstrikes
   queensland opposition leader annastacia palaszczukr has opened up about
   the heartbreak of losing her baby at 11 weeks said is to to to to to in
   to to ex     marine scott olsen now awake after being hit on tuesday

   the generated summaries for a major portion of the test set are
   redundant and do not resemble the actual summaries in the test set.

   certain words recur in many summaries irrespective of whether or not
   these words are present in the the actual articles and their summaries
   in the test set, e.g. the phrase    manchester united    and    manchester
   city    repeat a lot of time in the generated summaries.

   another observation is that initially (global_steps< 50000) the model
   was not generating grammatically correct sentences, as we trained the
   model for a greater duration the generated summaries began to make some
   sense and the grammar was somewhat more correct. however, the generated
   summaries were still completely irrelevant to the original articles as
   well as the corresponding human-written summaries.

   there was a visible improvement in loss (and in the semantic quality of
   summaries) only after 50,000 time-steps. after having trained for
   100,000 time-steps for close to one day, we observed the quality     here
   we use our subjective understanding to judge said quality     of
   summaries improve ever so slightly. even so the summaries are clearly
   not up to the mark. this is to be expected, given the training time.
   the authors of the model claim that it is possible to get [89]much
   better results provided the user is willing to trade-off in terms of
   required time and compute.

  to    summarize   

   for extractive techniques, our metrics tell us that lexrank outperforms
   gensim   s textrank by a narrow margin but we   ve also observed cases
   where textrank gives higher quality of summaries. we believe that the
   dataset which is used affects the quality of obtained summaries. a good
   practise would be to run both the algorithms and use the one which
   gives more satisfactory summaries. a future direction is to compare
   gensim   s textrank implementation with paco nathan   s [90]pytextrank.

   due to lack of gpu resources and a lot of parameters to tune we end our
   research on abstractive summarization at a point where we cannot
   conclude with absolute certainty that the model can be used as an
   alternative to current extractive implementations. but of course, one
   can always try training the model for a few million (more) timesteps
   and tweak some parameters to see whether the results get better on
   id98-dailymail dataset or on another dataset.

   [91]text summarization

author of post

   pranay, aman and aayush

pranay, aman and aayush's bio:

   pranay mathur, aman gill and aayush yadav are computer engineering
   students from pune, india. their fields of interest are deep learning
   and ai.

need expert consulting in ml and nlp?

   ________________________________________

   ________________________________________


   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   please leave this field empty. ________________________________________

   send

categories

   categories[select category___________]

archives

   archives [select month__]

recent posts

     * [92]export pii drill-down reports
     * [93]personal data analytics
     * [94]scanning office 365 for sensitive pii information
     * [95]pivoted document length normalisation
     * [96]sent2vec: an unsupervised approach towards learning sentence
       embeddings

stay ahead of the curve

get our latest tutorials, updates and insights delivered straight to your
inbox.

   ____________________

   ____________________

   subscribe
   ____________________
   1-2 times a month, if lucky. your information will not be shared.

   [97][footer-logo.png]
     * [98]services
     * [99]careers
     * [100]our team
     * [101]corporate training
     * [102]blog
     * [103]incubator
     * [104]contact
     * [105]competitions
     * [106]site map

   rare technologies [107][email protected] sv  tova 5, prague, czech
   republic [108](eu) +420 776 288 853

   type and press    enter    to search ____________________

references

   visible links
   1. https://rare-technologies.com/feed/
   2. https://rare-technologies.com/comments/feed/
   3. https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/feed/
   4. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/
   5. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/&format=xml
   6. https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld
   7. https://rare-technologies.com/
   8. https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/
   9. https://rare-technologies.com/services/
  10. https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/
  11. https://pii-tools.com/
  12. https://scaletext.com/
  13. https://rare-technologies.com/corporate-training/
  14. https://rare-technologies.com/corporate-training/
  15. https://rare-technologies.com/python-best-practices/
  16. https://rare-technologies.com/practical-machine-learning/
  17. https://rare-technologies.com/topic-modelling-training/
  18. https://rare-technologies.com/deep_learning_training/
  19. https://rare-technologies.com/incubator
  20. https://github.com/rare-technologies/
  21. https://rare-technologies.com/incubator/
  22. https://rare-technologies.com/competitions/
  23. https://rare-technologies.com/#braintrust
  24. https://rare-technologies.com/careers/
  25. https://rare-technologies.com/our-team/
  26. https://rare-technologies.com/blog/
  27. https://rare-technologies.com/contact/
  28. https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/
  29. https://rare-technologies.com/services/
  30. https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/
  31. https://pii-tools.com/
  32. https://scaletext.com/
  33. https://rare-technologies.com/corporate-training/
  34. https://rare-technologies.com/corporate-training/
  35. https://rare-technologies.com/python-best-practices/
  36. https://rare-technologies.com/practical-machine-learning/
  37. https://rare-technologies.com/topic-modelling-training/
  38. https://rare-technologies.com/deep_learning_training/
  39. https://rare-technologies.com/incubator
  40. https://github.com/rare-technologies/
  41. https://rare-technologies.com/incubator/
  42. https://rare-technologies.com/competitions/
  43. https://rare-technologies.com/#braintrust
  44. https://rare-technologies.com/careers/
  45. https://rare-technologies.com/our-team/
  46. https://rare-technologies.com/blog/
  47. https://rare-technologies.com/contact/
  48. https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/
  49. https://rare-technologies.com/author/aayush/
  50. https://rare-technologies.com/category/gensim/
  51. https://rare-technologies.com/category/student-incubator/
  52. https://rare-technologies.com/category/summarization/
  53. https://rare-technologies.com/incubator/
  54. https://rare-technologies.com/text-summarization-with-gensim/
  55. http://kavita-ganesan.com/opinosis-opinion-dataset
  56. https://github.com/tensorflow/models/tree/master/textsum
  57. https://github.com/tensorflow/models/issues/373#issuecomment-256408591
  58. https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf
  59. https://rare-technologies.com/text-summarization-with-gensim/
  60. https://en.wikipedia.org/wiki/id95
  61. https://en.wikipedia.org/wiki/okapi_bm25
  62. https://raw.githubusercontent.com/summanlp/docs/master/articulo/articulo-en.pdf
  63. https://github.com/xiaoxu193/pyteaser
  64. https://github.com/mojojolo/textteaser
  65. http://oldwww.iiit.ac.in/cgi-bin/techreports/display_detail.cgi?id=iiit/tr/2008/97
  66. https://github.com/ceteri/pytextrank
  67. http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf
  68. https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html
  69. https://en.wikipedia.org/wiki/automatic_summarization#textrank_and_lexrank
  70. https://technowiki.wordpress.com/2011/08/27/latent-semantic-analysis-lsa-tutorial/
  71. https://radimrehurek.com/gensim/models/lsimodel.html
  72. https://github.com/miso-belica/sumy
  73. http://www.aclweb.org/anthology/w04-1013
  74. https://gist.github.com/dust0x/47ffa11d7c78e5f02196f3891f29c401
  75. http://www.aclweb.org/anthology/p02-1040.pdf
  76. http://kavita-ganesan.com/opinosis-opinion-dataset
  77. http://www.nltk.org/api/nltk.translate.html
  78. https://gist.github.com/dust0x/d7cc4aaf9cfa677b54c29bb33c285468#file-accuracy_garmin_nuvi_255w_gps-txt
  79. https://gist.github.com/dust0x/d7cc4aaf9cfa677b54c29bb33c285468#file-extractive-results-md
  80. http://ltrc.iiit.ac.in/icon/2013/proceedings/file49-paper108.pdf
  81. https://github.com/tensorflow/models/tree/master/textsum
  82. https://github.com/tensorflow/models/blob/master/textsum/id195_attention.py#l68
  83. https://gist.github.com/dust0x/9930aeea63307aca5fdcac3409e3d09e
  84. https://github.com/tensorflow/models/issues/373#issuecomment-256700203
  85. https://github.com/tensorflow/models/issues/373#issuecomment-256408591
  86. http://cs.nyu.edu/~kcho/dmqa/
  87. https://github.com/pranay360/textsum_data_generation
  88. http://pavel.surmenok.com/2016/10/15/how-to-run-text-summarization-with-tensorflow/
  89. https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html
  90. https://github.com/ceteri/pytextrank
  91. https://rare-technologies.com/tag/text-summarization/
  92. https://rare-technologies.com/personal-data-reports/
  93. https://rare-technologies.com/pii_analytics/
  94. https://rare-technologies.com/pii-scan-o365-connector/
  95. https://rare-technologies.com/pivoted-document-length-normalisation/
  96. https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/
  97. https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/
  98. https://rare-technologies.com/services/
  99. https://rare-technologies.com/careers/
 100. https://rare-technologies.com/our-team/
 101. https://rare-technologies.com/corporate-training/
 102. https://rare-technologies.com/blog/
 103. https://rare-technologies.com/incubator/
 104. https://rare-technologies.com/contact/
 105. https://rare-technologies.com/competitions/
 106. https://rare-technologies.com/sitemap
 107. https://rare-technologies.com/cdn-cgi/l/email-protection#7e171018113e0c1f0c1b530a1b1d161011121119171b0d501d1113
 108. tel:+420 776 288 853

   hidden links:
 110. https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/#top
 111. https://www.facebook.com/raretechnologies
 112. https://twitter.com/raretechteam
 113. https://www.linkedin.com/company/6457766
 114. https://github.com/piskvorky/
 115. https://rare-technologies.com/feed/
