   #[1]github [2]recent commits to namas:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]7
     * [35]star [36]67
     * [37]fork [38]242

[39]harvardnlp/[40]namas forked from [41]facebookarchive/namas

   [42]code [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   neural attention model for abstractive summarization
     * [47]3 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors
     * [51]view license

    1. [52]lua 67.8%
    2. [53]python 23.4%
    3. [54]shell 8.8%

   (button) lua python shell
   branch: master (button) new pull request
   [55]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/h
   [56]download zip

downloading...

   want to be notified of new releases in harvardnlp/namas?
   [57]sign in [58]sign up

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [60]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [61]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [62]download the github extension for visual studio
   and try again.

   (button) go back
   [63]pull request [64]compare this branch is 2 commits behind
   facebookarchive:master.
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [65]permalink
   type          name             latest commit message    commit time
        failed to load latest commit information.
        [66]duc
        [67]dataset
        [68]summary
        [69]tuning
        [70]contributing.md
        [71]license
        [72]patents
        [73]readme.md
        [74]construct_data.sh  [75]fixing the licence file sep 1, 2015
        [76]prep_torch_data.sh
        [77]test_model.sh      [78]fixing the licence file sep 1, 2015
        [79]train_model.sh

readme.md

attention-based summarization

   this project contains the abs. neural abstractive summarization system
   from the paper
 a neural attention model for abstractive summarization.
 alexander m. rush, sumit chopra, jason weston.

   the release includes code for:
     * extracting the summarization data set
     * training the neural summarization model
     * constructing evaluation sets with id8
     * tuning extractive features

setup

   to run the system, you will need to have [80]torch7) and [81]fbcunn
   (facebook's deep learning library) installed. you will also need python
   2.7, nltk, and gnu parallel to run the data processing scripts.
   additionally the code currently requires a cuda gpu for training and
   decoding.

   finally the scripts require that you set the $abs environment variable.
> export abs=$pwd
> export lua_path="$lua_path;$abs/?.lua"

constructing the data set

   the model is trained to perform title generation from the first line of
   newspaper articles. since the system is completely data-driven it
   requires a large set of aligned input-title pairs for training.

   to provide these pairs we use the [82]annotated gigaword corpus as our
   main data set. the corpus is available on ldc, but it requires
   membership. once the annotated gigaword is obtained, you can simply run
   the provided script to extract the data set in text format.

generating the data

   to construct the data set run the following script to produce
   working_dir/, where `working_dir/' is the path to the directory where
   you want to store the processed data. the script 'construct_data.sh'
   makes use of the 'parallel' utility, so please make sure that it is in
   your path. warning: this may take a couple hours to run.
 > ./construct_data.sh agiga/ working_dir/

format of the data files

   the above command builds aligned files of the form split.type.txt where
   split is train/valid/test and type is title/article.

   the output of the script is several aligned plain-text files. each has
   one title or article per line.
 > head train.title.txt
 australian current account deficit narrows sharply
 at least two dead in southern philippines blast
 australian stocks close down #.# percent
 envoy urges north korea to restart nuclear disablement
 skorea announces tax cuts to stimulate economy

   these files can be used to train the abs system or be used by other
   baseline models.

training the model

   once the data set has been constructed, we provide a simple script to
   train the model.

     ./train_model.sh working_dir/ model.th

   the training process consists of two stages. first we convert the text
   files into generic input-title matrices and then we train a conditional
   nnlm on this representation.

   once the model has been fully trained (this may require 3-4 days), you
   can use the test script to produce summaries of any plain text file.w

     ./test_model.sh working_dir/valid.article.filter.txt model.th
     length_of_summary

training options

   these scripts utilize the torch code available in $abs/summary/

   there are two main torch entry points. one for training the model from
   data matrices and the other for evaluating the model on plain-text.
 > th summary/train.lua -help

 train a summarization model.

   -articledir      directory containing article training matrices. []
   -titledir        directory containing title training matrices. []
   -validarticledir directory containing article matricess for validation. []
   -validtitledir   directory containing title matrices for validation. []
   -auxmodel        the encoder model to use. [bow]
   -bowdim          article embedding size. [50]
   -attenpool       attention model pooling size. [5]
   -hiddenunits     conv net encoder hidden units. [1000]
   -kernelwidth     conv net encoder kernel width. [5]
   -epochs          number of epochs to train. [5]
   -minibatchsize   size of training minibatch. [64]
   -printevery      how often to print during training. [1000]
   -modelfilename   file for saving loading/model. []
   -window          size of nnlm window. [5]
   -embeddingdim    size of nnlm embeddings. [50]
   -hiddensize      size of nnlm hidden layer. [100]
   -learningrate    sgd learning rate. [0.1]

testing options

   the run script is used for beam-search decoding with a trained model.
   see the paper for a description of the extractive features used at
   decoding time.
> th summary/run.lua -help

-blockrepeatwords disallow generating a repeated word. [false]
-allowunk         allow generating <unk>. [false]
-fixedlength      produce exactly -length words. [true]
-lmweight         weight for main model. [1]
-beamsize         size of the beam. [100]
-extractive       force fully extractive summary. [false]
-lmweight         feature weight for the neural model. [1]
-unigrambonus     feature weight for unigram extraction. [0]
-bigrambonus      feature weight for bigram extraction. [0]
-trigrambonus     feature weight for trigram extraction. [0]
-lengthbonus      feature weight for length. [0]
-unorderbonus     feature weight for out-of-order extraction. [0]
-modelfilename    model to test. []
-inputf           input article files.  []
-nbest            write out the nbest list in zmert format. [false]
-length           maximum length of summary.. [5]

evaluation data sets

   we evaluate the abs model using the shared task from the document
   understanding conference (duc).

   this release also includes code for interactive with the duc shared
   task on headline generation. the scripts for processing and evaluating
   on this data set are in the duc/ directory.

   the [83]duc data set is available online, unfortunately you must
   manually fill out a form to request the data from nist. send the
   request to [84]angela ellis.

processing duc

   after receiving credentials you should obtain a series of tar files
   containing the data used as part of this shared task.
    1. make a directory duc_data/ which should contain the given files
>duc2003\_summarization\_documents.tgz
>duc2004\_summarization\_documents.tgz
>duc2004\_results.tgz
>detagged.duc2003.abstracts.tar.gz

    2. run the setup script (this requires python and nltk for
       id121)

     ./duc/setup.sh duc_data/

   after running the scripts there should be directories
   duc_data/clean_2003/
   duc_data/clean_2004/

   each contains a file input.txt where each line is a tokenized first
   line of an article.
 > head duc_data/clean_2003/input.txt
 schizophrenia patients whose medication could n't stop the imaginary voices in
their heads gained some relief after researchers repeatedly sent a magnetic fiel
d into a small area of their brains .
 scientists trying to fathom the mystery of schizophrenia say they have found th
e strongest evidence to date that the disabling psychiatric disorder is caused b
y gene abnormalities , according to a researcher at two state universities .
 a yale school of medicine study is expanding upon what scientists know  about t
he link between schizophrenia and nicotine addiction .
 exploring chaos in a search for order , scientists who study the reality-shatte
ring mental disease schizophrenia are becoming fascinated by the chemical enviro
nment of areas of the brain where perception is regulated .

   as well as a set of references:
> head duc_data/clean_2003/references/task1_ref0.txt
magnetic treatment may ease or lessen occurrence of schizophrenic voices.
evidence shows schizophrenia caused by gene abnormalities of chromosome 1.
researchers examining evidence of link between schizophrenia and nicotine addict
ion.
scientists focusing on chemical environment of brain to understand schizophrenia
.
schizophrenia study shows disparity between what's known and what's provided to
patients.

   system output should be added to the directory system/task1_{name}.txt.
   for instance the script includes a baseline prefix system.
duc_data/clean_2003/references/task1_prefix.txt

id8 for eval

   to evaluate the summaries you will need the [85]id8 eval system.

   the id8 script requires output in a very complex html form. to
   simplify this process we include a script to convert the simple output
   to one that id8 can handle.

   export the id8 directory export id8={path_to_id8} and then run
   the eval scripts
> ./duc/eval.sh duc_data/clean_2003/
full length
   ---------------------------------------------
   prefix id8-1 average_r: 0.17831 (95%-conf.int. 0.16916 - 0.18736)
   prefix id8-1 average_p: 0.15445 (95%-conf.int. 0.14683 - 0.16220)
   prefix id8-1 average_f: 0.16482 (95%-conf.int. 0.15662 - 0.17318)
   ---------------------------------------------
   prefix id8-2 average_r: 0.04936 (95%-conf.int. 0.04420 - 0.05452)
   prefix id8-2 average_p: 0.04257 (95%-conf.int. 0.03794 - 0.04710)
   prefix id8-2 average_f: 0.04550 (95%-conf.int. 0.04060 - 0.05026)

tuning feature weights

   for our system abs+ we additionally tune extractive features on the duc
   summarization data. the final features we obtained our distributed with
   the system as tuning/params.best.txt.

   the mert tuning code itself is located in the tuning/ directory. our
   setup uses [86]zmert for this process.

   it should be straightforward to tune the system on any developments
   summarization data. take the following steps to run tuning on the
   duc-2003 data set described above.

   first copy over reference files to the tuning directoy. for instance to
   tune on duc-2003:
ln -s duc_data/clean_2003/references/task1_ref0.txt tuning/ref.0
ln -s duc_data/clean_2003/references/task1_ref1.txt tuning/ref.1
ln -s duc_data/clean_2003/references/task1_ref2.txt tuning/ref.2
ln -s duc_data/clean_2003/references/task1_ref3.txt tuning/ref.3

   next copy the sdecoder template, cp sdecoder_cmd.tpl sdecoder_cmd.py
   and modify the sdecoder_cmd.py to point to the model and input text.
{"model" : "model.th",
 "src" : "/data/users/sashar/duc_data/clean_2003/input.txt",
 "title_len" : 14}

   now you should be able to run z-mert and let it do its thing.
> cd tuning/; java -cp zmert/lib/zmert.jar zmert zmert_id18.txt

   when z-mert has finished you can run on new data using command:
> python sdecoder_test.py input.txt model.th

     *    2019 github, inc.
     * [87]terms
     * [88]privacy
     * [89]security
     * [90]status
     * [91]help

     * [92]contact github
     * [93]pricing
     * [94]api
     * [95]training
     * [96]blog
     * [97]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [98]reload to refresh your
   session. you signed out in another tab or window. [99]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/harvardnlp/namas/commits/master.atom
   3. https://github.com/harvardnlp/namas#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/harvardnlp/namas
  32. https://github.com/join
  33. https://github.com/login?return_to=/harvardnlp/namas
  34. https://github.com/harvardnlp/namas/watchers
  35. https://github.com/login?return_to=/harvardnlp/namas
  36. https://github.com/harvardnlp/namas/stargazers
  37. https://github.com/login?return_to=/harvardnlp/namas
  38. https://github.com/harvardnlp/namas/network/members
  39. https://github.com/harvardnlp
  40. https://github.com/harvardnlp/namas
  41. https://github.com/facebookarchive/namas
  42. https://github.com/harvardnlp/namas
  43. https://github.com/harvardnlp/namas/pulls
  44. https://github.com/harvardnlp/namas/projects
  45. https://github.com/harvardnlp/namas/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/harvardnlp/namas/commits/master
  48. https://github.com/harvardnlp/namas/branches
  49. https://github.com/harvardnlp/namas/releases
  50. https://github.com/harvardnlp/namas/graphs/contributors
  51. https://github.com/harvardnlp/namas/blob/master/license
  52. https://github.com/harvardnlp/namas/search?l=lua
  53. https://github.com/harvardnlp/namas/search?l=python
  54. https://github.com/harvardnlp/namas/search?l=shell
  55. https://github.com/harvardnlp/namas/find/master
  56. https://github.com/harvardnlp/namas/archive/master.zip
  57. https://github.com/login?return_to=https://github.com/harvardnlp/namas
  58. https://github.com/join?return_to=/harvardnlp/namas
  59. https://desktop.github.com/
  60. https://desktop.github.com/
  61. https://developer.apple.com/xcode/
  62. https://visualstudio.github.com/
  63. https://github.com/harvardnlp/namas/pull/new/master
  64. https://github.com/harvardnlp/namas/compare
  65. https://github.com/harvardnlp/namas/tree/cc227460368bad37b2370bd7a10ef4087f8767f5
  66. https://github.com/harvardnlp/namas/tree/master/duc
  67. https://github.com/harvardnlp/namas/tree/master/dataset
  68. https://github.com/harvardnlp/namas/tree/master/summary
  69. https://github.com/harvardnlp/namas/tree/master/tuning
  70. https://github.com/harvardnlp/namas/blob/master/contributing.md
  71. https://github.com/harvardnlp/namas/blob/master/license
  72. https://github.com/harvardnlp/namas/blob/master/patents
  73. https://github.com/harvardnlp/namas/blob/master/readme.md
  74. https://github.com/harvardnlp/namas/blob/master/construct_data.sh
  75. https://github.com/harvardnlp/namas/commit/9bfcd0daea97a7b7552d8367a1948a6f0958771b
  76. https://github.com/harvardnlp/namas/blob/master/prep_torch_data.sh
  77. https://github.com/harvardnlp/namas/blob/master/test_model.sh
  78. https://github.com/harvardnlp/namas/commit/9bfcd0daea97a7b7552d8367a1948a6f0958771b
  79. https://github.com/harvardnlp/namas/blob/master/train_model.sh
  80. http://torch.ch/
  81. https://github.com/facebook/fbcunn
  82. https://catalog.ldc.upenn.edu/ldc2012t21
  83. http://duc.nist.gov/duc2004/tasks.html
  84. mailto:angela.ellis@nist.gov
  85. http://research.microsoft.com/~cyl/download/id8-1.5.5.tgz
  86. http://cs.jhu.edu/~ozaidan/zmert/
  87. https://github.com/site/terms
  88. https://github.com/site/privacy
  89. https://github.com/security
  90. https://githubstatus.com/
  91. https://help.github.com/
  92. https://github.com/contact
  93. https://github.com/pricing
  94. https://developer.github.com/
  95. https://training.github.com/
  96. https://github.blog/
  97. https://github.com/about
  98. https://github.com/harvardnlp/namas
  99. https://github.com/harvardnlp/namas

   hidden links:
 101. https://github.com/
 102. https://github.com/harvardnlp/namas
 103. https://github.com/harvardnlp/namas
 104. https://github.com/harvardnlp/namas
 105. https://help.github.com/articles/which-remote-url-should-i-use
 106. https://github.com/harvardnlp/namas#attention-based-summarization
 107. https://github.com/harvardnlp/namas#setup
 108. https://github.com/harvardnlp/namas#constructing-the-data-set
 109. https://github.com/harvardnlp/namas#generating-the-data
 110. https://github.com/harvardnlp/namas#format-of-the-data-files
 111. https://github.com/harvardnlp/namas#training-the-model
 112. https://github.com/harvardnlp/namas#training-options
 113. https://github.com/harvardnlp/namas#testing-options
 114. https://github.com/harvardnlp/namas#evaluation-data-sets
 115. https://github.com/harvardnlp/namas#processing-duc
 116. https://github.com/harvardnlp/namas#id8-for-eval
 117. https://github.com/harvardnlp/namas#tuning-feature-weights
 118. https://github.com/
