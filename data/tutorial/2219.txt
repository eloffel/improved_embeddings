   #[1]the keras blog atom feed

                               [2]the keras blog

   [3]keras is a deep learning library for python, that is simple,
   modular, and extensible.

     * [4]archives
     * [5]github
     * [6]documentation
     * [7]google group

                       [8]building autoencoders in keras

   sat 14 may 2016


    by [9]francois chollet

   in [10]tutorials.

   in this tutorial, we will answer some common questions about
   autoencoders, and we will cover code examples of the following models:
     * a simple autoencoder based on a fully-connected layer
     * a sparse autoencoder
     * a deep fully-connected autoencoder
     * a deep convolutional autoencoder
     * an image denoising model
     * a sequence-to-sequence autoencoder
     * a variational autoencoder

   note: all code examples have been updated to the keras 2.0 api on march
   14, 2017. you will need keras version 2.0.0 or higher to run them.
     __________________________________________________________________

what are autoencoders?

   autoencoder: schema

   "autoencoding" is a data compression algorithm where the compression
   and decompression functions are 1) data-specific, 2) lossy, and 3)
   learned automatically from examples rather than engineered by a human.
   additionally, in almost all contexts where the term "autoencoder" is
   used, the compression and decompression functions are implemented with
   neural networks.

   1) autoencoders are data-specific, which means that they will only be
   able to compress data similar to what they have been trained on. this
   is different from, say, the mpeg-2 audio layer iii (mp3) compression
   algorithm, which only holds assumptions about "sound" in general, but
   not about specific types of sounds. an autoencoder trained on pictures
   of faces would do a rather poor job of compressing pictures of trees,
   because the features it would learn would be face-specific.

   2) autoencoders are lossy, which means that the decompressed outputs
   will be degraded compared to the original inputs (similar to mp3 or
   jpeg compression). this differs from lossless arithmetic compression.

   3) autoencoders are learned automatically from data examples, which is
   a useful property: it means that it is easy to train specialized
   instances of the algorithm that will perform well on a specific type of
   input. it doesn't require any new engineering, just appropriate
   training data.

   to build an autoencoder, you need three things: an encoding function, a
   decoding function, and a distance function between the amount of
   information loss between the compressed representation of your data and
   the decompressed representation (i.e. a "loss" function). the encoder
   and decoder will be chosen to be parametric functions (typically neural
   networks), and to be differentiable with respect to the distance
   function, so the parameters of the encoding/decoding functions can be
   optimize to minimize the reconstruction loss, using stochastic gradient
   descent. it's simple! and you don't even need to understand any of
   these words to start using autoencoders in practice.

are they good at data compression?

   usually, not really. in picture compression for instance, it is pretty
   difficult to train an autoencoder that does a better job than a basic
   algorithm like jpeg, and typically the only way it can be achieved is
   by restricting yourself to a very specific type of picture (e.g. one
   for which jpeg does not do a good job). the fact that autoencoders are
   data-specific makes them generally impractical for real-world data
   compression problems: you can only use them on data that is similar to
   what they were trained on, and making them more general thus requires
   lots of training data. but future advances might change this, who
   knows.

what are autoencoders good for?

   they are rarely used in practical applications. in 2012 they briefly
   found an application in greedy layer-wise pretraining for deep
   convolutional neural networks [1], but this quickly fell out of fashion
   as we started realizing that better random weight initialization
   schemes were sufficient for training deep networks from scratch. in
   2014, batch id172 [2] started allowing for even deeper
   networks, and from late 2015 we could train arbitrarily deep networks
   from scratch using residual learning [3].

   today two interesting practical applications of autoencoders are data
   denoising (which we feature later in this post), and dimensionality
   reduction for data visualization. with appropriate dimensionality and
   sparsity constraints, autoencoders can learn data projections that are
   more interesting than pca or other basic techniques.

   for 2d visualization specifically, [11]id167 (pronounced "tee-snee") is
   probably the best algorithm around, but it typically requires
   relatively low-dimensional data. so a good strategy for visualizing
   similarity relationships in high-dimensional data is to start by using
   an autoencoder to compress your data into a low-dimensional space (e.g.
   32 dimensional), then use id167 for mapping the compressed data to a 2d
   plane. note that a nice parametric implementation of id167 in keras was
   developed by kyle mcdonald and [12]is available on github. otherwise
   [13]scikit-learn also has a simple and practical implementation.

so what's the big deal with autoencoders?

   their main claim to fame comes from being featured in many introductory
   machine learning classes available online. as a result, a lot of
   newcomers to the field absolutely love autoencoders and can't get
   enough of them. this is the reason why this tutorial exists!

   otherwise, one reason why they have attracted so much research and
   attention is because they have long been thought to be a potential
   avenue for solving the problem of unsupervised learning, i.e. the
   learning of useful representations without the need for labels. then
   again, autoencoders are not a true unsupervised learning technique
   (which would imply a different learning process altogether), they are a
   self-supervised technique, a specific instance of supervised learning
   where the targets are generated from the input data. in order to get
   self-supervised models to learn interesting features, you have to come
   up with an interesting synthetic target and id168, and that's
   where problems arise: merely learning to reconstruct your input in
   minute detail might not be the right choice here. at this point there
   is significant evidence that focusing on the reconstruction of a
   picture at the pixel level, for instance, is not conductive to learning
   interesting, abstract features of the kind that label-supervized
   learning induces (where targets are fairly abstract concepts "invented"
   by humans such as "dog", "car"...). in fact, one may argue that the
   best features in this regard are those that are the worst at exact
   input reconstruction while achieving high performance on the main task
   that you are interested in (classification, localization, etc).

   in self-supervized learning applied to vision, a potentially fruitful
   alternative to autoencoder-style input reconstruction is the use of toy
   tasks such as jigsaw puzzle solving, or detail-context matching (being
   able to match high-resolution but small patches of pictures with
   low-resolution versions of the pictures they are extracted from). the
   following paper investigates jigsaw puzzle solving and makes for a very
   interesting read: noroozi and favaro (2016) [14]unsupervised learning
   of visual representations by solving jigsaw puzzles. such tasks are
   providing the model with built-in assumptions about the input data
   which are missing in traditional autoencoders, such as "visual
   macro-structure matters more than pixel-level details".

   jigsaw puzzle task
     __________________________________________________________________

let's build the simplest possible autoencoder

   we'll start simple, with a single fully-connected neural layer as
   encoder and as decoder:
from keras.layers import input, dense
from keras.models import model

# this is the size of our encoded representations
encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input
 is 784 floats

# this is our input placeholder
input_img = input(shape=(784,))
# "encoded" is the encoded representation of the input
encoded = dense(encoding_dim, activation='relu')(input_img)
# "decoded" is the lossy reconstruction of the input
decoded = dense(784, activation='sigmoid')(encoded)

# this model maps an input to its reconstruction
autoencoder = model(input_img, decoded)

   let's also create a separate encoder model:
# this model maps an input to its encoded representation
encoder = model(input_img, encoded)

   as well as the decoder model:
# create a placeholder for an encoded (32-dimensional) input
encoded_input = input(shape=(encoding_dim,))
# retrieve the last layer of the autoencoder model
decoder_layer = autoencoder.layers[-1]
# create the decoder model
decoder = model(encoded_input, decoder_layer(encoded_input))

   now let's train our autoencoder to reconstruct mnist digits.

   first, we'll configure our model to use a per-pixel binary crossid178
   loss, and the adadelta optimizer:
autoencoder.compile(optimizer='adadelta', loss='binary_crossid178')

   let's prepare our input data. we're using mnist digits, and we're
   discarding the labels (since we're only interested in encoding/decoding
   the input images).
from keras.datasets import mnist
import numpy as np
(x_train, _), (x_test, _) = mnist.load_data()

   we will normalize all values between 0 and 1 and we will flatten the
   28x28 images into vectors of size 784.
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
print x_train.shape
print x_test.shape

   now let's train our autoencoder for 50 epochs:
autoencoder.fit(x_train, x_train,
                epochs=50,
                batch_size=256,
                shuffle=true,
                validation_data=(x_test, x_test))

   after 50 epochs, the autoencoder seems to reach a stable train/test
   loss value of about 0.11. we can try to visualize the reconstructed
   inputs and the encoded representations. we will use matplotlib.
# encode and decode some digits
# note that we take them from the *test* set
encoded_imgs = encoder.predict(x_test)
decoded_imgs = decoder.predict(encoded_imgs)

# use matplotlib (don't ask)
import matplotlib.pyplot as plt

n = 10  # how many digits we will display
plt.figure(figsize=(20, 4))
for i in range(n):
    # display original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(false)
    ax.get_yaxis().set_visible(false)

    # display reconstruction
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(false)
    ax.get_yaxis().set_visible(false)
plt.show()

   here's what we get. the top row is the original digits, and the bottom
   row is the reconstructed digits. we are losing quite a bit of detail
   with this basic approach.

   basic autoencoder
     __________________________________________________________________

adding a sparsity constraint on the encoded representations

   in the previous example, the representations were only constrained by
   the size of the hidden layer (32). in such a situation, what typically
   happens is that the hidden layer is learning an approximation of
   [15]pca (principal component analysis). but another way to constrain
   the representations to be compact is to add a sparsity contraint on the
   activity of the hidden representations, so fewer units would "fire" at
   a given time. in keras, this can be done by adding an
   activity_regularizer to our dense layer:
from keras import regularizers

encoding_dim = 32

input_img = input(shape=(784,))
# add a dense layer with a l1 activity regularizer
encoded = dense(encoding_dim, activation='relu',
                activity_regularizer=regularizers.l1(10e-5))(input_img)
decoded = dense(784, activation='sigmoid')(encoded)

autoencoder = model(input_img, decoded)

   let's train this model for 100 epochs (with the added id173
   the model is less likely to overfit and can be trained longer). the
   models ends with a train loss of 0.11 and test loss of 0.10. the
   difference between the two is mostly due to the id173 term
   being added to the loss during training (worth about 0.01).

   here's a visualization of our new results:

   sparse autoencoder

   they look pretty similar to the previous model, the only significant
   difference being the sparsity of the encoded representations.
   encoded_imgs.mean() yields a value 3.33 (over our 10,000 test images),
   whereas with the previous model the same quantity was 7.30. so our new
   model yields encoded representations that are twice sparser.
     __________________________________________________________________

deep autoencoder

   we do not have to limit ourselves to a single layer as encoder or
   decoder, we could instead use a stack of layers, such as:
input_img = input(shape=(784,))
encoded = dense(128, activation='relu')(input_img)
encoded = dense(64, activation='relu')(encoded)
encoded = dense(32, activation='relu')(encoded)

decoded = dense(64, activation='relu')(encoded)
decoded = dense(128, activation='relu')(decoded)
decoded = dense(784, activation='sigmoid')(decoded)

   let's try this:
autoencoder = model(input_img, decoded)
autoencoder.compile(optimizer='adadelta', loss='binary_crossid178')

autoencoder.fit(x_train, x_train,
                epochs=100,
                batch_size=256,
                shuffle=true,
                validation_data=(x_test, x_test))

   after 100 epochs, it reaches a train and test loss of ~0.097, a bit
   better than our previous models. our reconstructed digits look a bit
   better too:

   deep autoencoder
     __________________________________________________________________

convolutional autoencoder

   since our inputs are images, it makes sense to use convolutional neural
   networks (convnets) as encoders and decoders. in practical settings,
   autoencoders applied to images are always convolutional autoencoders
   --they simply perform much better.

   let's implement one. the encoder will consist in a stack of conv2d and
   maxpooling2d layers (max pooling being used for spatial down-sampling),
   while the decoder will consist in a stack of conv2d and upsampling2d
   layers.
from keras.layers import input, dense, conv2d, maxpooling2d, upsampling2d
from keras.models import model
from keras import backend as k

input_img = input(shape=(28, 28, 1))  # adapt this if using `channels_first` ima
ge data format

x = conv2d(16, (3, 3), activation='relu', padding='same')(input_img)
x = maxpooling2d((2, 2), padding='same')(x)
x = conv2d(8, (3, 3), activation='relu', padding='same')(x)
x = maxpooling2d((2, 2), padding='same')(x)
x = conv2d(8, (3, 3), activation='relu', padding='same')(x)
encoded = maxpooling2d((2, 2), padding='same')(x)

# at this point the representation is (4, 4, 8) i.e. 128-dimensional

x = conv2d(8, (3, 3), activation='relu', padding='same')(encoded)
x = upsampling2d((2, 2))(x)
x = conv2d(8, (3, 3), activation='relu', padding='same')(x)
x = upsampling2d((2, 2))(x)
x = conv2d(16, (3, 3), activation='relu')(x)
x = upsampling2d((2, 2))(x)
decoded = conv2d(1, (3, 3), activation='sigmoid', padding='same')(x)

autoencoder = model(input_img, decoded)
autoencoder.compile(optimizer='adadelta', loss='binary_crossid178')

   to train it, we will use the original mnist digits with shape (samples,
   3, 28, 28), and we will just normalize pixel values between 0 and 1.
from keras.datasets import mnist
import numpy as np

(x_train, _), (x_test, _) = mnist.load_data()

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using
`channels_first` image data format
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `ch
annels_first` image data format

   let's train this model for 50 epochs. for the sake of demonstrating how
   to visualize the results of a model during training, we will be using
   [16]the tensorflow backend and the tensorboard callback.

   first, let's open up a terminal and start a tensorboard server that
   will read logs stored at /tmp/autoencoder.
tensorboard --logdir=/tmp/autoencoder

   then let's train our model. in the callbacks list we pass an instance
   of the tensorboard callback. after every epoch, this callback will
   write logs to /tmp/autoencoder, which can be read by our tensorboard
   server.
from keras.callbacks import tensorboard

autoencoder.fit(x_train, x_train,
                epochs=50,
                batch_size=128,
                shuffle=true,
                validation_data=(x_test, x_test),
                callbacks=[tensorboard(log_dir='/tmp/autoencoder')])

   this allows us to monitor training in the tensorboard web interface (by
   navighating to http://0.0.0.0:6006):

   tensorboard curves

   the model converges to a loss of 0.094, significantly better than our
   previous models (this is in large part due to the higher entropic
   capacity of the encoded representation, 128 dimensions vs. 32
   previously). let's take a look at the reconstructed digits:
decoded_imgs = autoencoder.predict(x_test)

n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    # display original
    ax = plt.subplot(2, n, i)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(false)
    ax.get_yaxis().set_visible(false)

    # display reconstruction
    ax = plt.subplot(2, n, i + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(false)
    ax.get_yaxis().set_visible(false)
plt.show()

   convolutional autoencoder

   we can also have a look at the 128-dimensional encoded representations.
   these representations are 8x4x4, so we reshape them to 4x32 in order to
   be able to display them as grayscale images.
n = 10
plt.figure(figsize=(20, 8))
for i in range(n):
    ax = plt.subplot(1, n, i)
    plt.imshow(encoded_imgs[i].reshape(4, 4 * 8).t)
    plt.gray()
    ax.get_xaxis().set_visible(false)
    ax.get_yaxis().set_visible(false)
plt.show()

   latent representations
     __________________________________________________________________

application to image denoising

   let's put our convolutional autoencoder to work on an image denoising
   problem. it's simple: we will train the autoencoder to map noisy digits
   images to clean digits images.

   here's how we will generate synthetic noisy digits: we just apply a
   gaussian noise matrix and clip the images between 0 and 1.
from keras.datasets import mnist
import numpy as np

(x_train, _), (x_test, _) = mnist.load_data()

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using
`channels_first` image data format
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `ch
annels_first` image data format

noise_factor = 0.5
x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, si
ze=x_train.shape)
x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size
=x_test.shape)

x_train_noisy = np.clip(x_train_noisy, 0., 1.)
x_test_noisy = np.clip(x_test_noisy, 0., 1.)

   here's what the noisy digits look like:
n = 10
plt.figure(figsize=(20, 2))
for i in range(n):
    ax = plt.subplot(1, n, i)
    plt.imshow(x_test_noisy[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(false)
    ax.get_yaxis().set_visible(false)
plt.show()

   noisy digits

   if you squint you can still recognize them, but barely. can our
   autoencoder learn to recover the original digits? let's find out.

   compared to the previous convolutional autoencoder, in order to improve
   the quality of the reconstructed, we'll use a slightly different model
   with more filters per layer:
input_img = input(shape=(28, 28, 1))  # adapt this if using `channels_first` ima
ge data format

x = conv2d(32, (3, 3), activation='relu', padding='same')(input_img)
x = maxpooling2d((2, 2), padding='same')(x)
x = conv2d(32, (3, 3), activation='relu', padding='same')(x)
encoded = maxpooling2d((2, 2), padding='same')(x)

# at this point the representation is (7, 7, 32)

x = conv2d(32, (3, 3), activation='relu', padding='same')(encoded)
x = upsampling2d((2, 2))(x)
x = conv2d(32, (3, 3), activation='relu', padding='same')(x)
x = upsampling2d((2, 2))(x)
decoded = conv2d(1, (3, 3), activation='sigmoid', padding='same')(x)

autoencoder = model(input_img, decoded)
autoencoder.compile(optimizer='adadelta', loss='binary_crossid178')

   let's train it for 100 epochs:
autoencoder.fit(x_train_noisy, x_train,
                epochs=100,
                batch_size=128,
                shuffle=true,
                validation_data=(x_test_noisy, x_test),
                callbacks=[tensorboard(log_dir='/tmp/tb', histogram_freq=0, writ
e_graph=false)])

   now let's take a look at the results. top, the noisy digits fed to the
   network, and bottom, the digits are reconstructed by the network.

   denoised digits

   it seems to work pretty well. if you scale this process to a bigger
   convnet, you can start building document denoising or audio denoising
   models. [17]kaggle has an interesting dataset to get you started.
     __________________________________________________________________

sequence-to-sequence autoencoder

   if you inputs are sequences, rather than vectors or 2d images, then you
   may want to use as encoder and decoder a type of model that can capture
   temporal structure, such as a lstm. to build a lstm-based autoencoder,
   first use a lstm encoder to turn your input sequences into a single
   vector that contains information about the entire sequence, then repeat
   this vector n times (where n is the number of timesteps in the output
   sequence), and run a lstm decoder to turn this constant sequence into
   the target sequence.

   we won't be demonstrating that one on any specific dataset. we will
   just put a code example here for future reference for the reader!
from keras.layers import input, lstm, repeatvector
from keras.models import model

inputs = input(shape=(timesteps, input_dim))
encoded = lstm(latent_dim)(inputs)

decoded = repeatvector(timesteps)(encoded)
decoded = lstm(input_dim, return_sequences=true)(decoded)

sequence_autoencoder = model(inputs, decoded)
encoder = model(inputs, encoded)
     __________________________________________________________________

variational autoencoder (vae)

   id5 are a slightly more modern and interesting
   take on autoencoding.

   what is a variational autoencoder, you ask? it's a type of autoencoder
   with added constraints on the encoded representations being learned.
   more precisely, it is an autoencoder that learns a [18]latent variable
   model for its input data. so instead of letting your neural network
   learn an arbitrary function, you are learning the parameters of a
   id203 distribution modeling your data. if you sample points from
   this distribution, you can generate new input data samples: a vae is a
   "generative model".

   how does a variational autoencoder work?

   first, an encoder network turns the input samples x into two parameters
   in a latent space, which we will note z_mean and z_log_sigma. then, we
   randomly sample similar points z from the latent normal distribution
   that is assumed to generate the data, via z = z_mean + exp(z_log_sigma)
   * epsilon, where epsilon is a random normal tensor. finally, a decoder
   network maps these latent space points back to the original input data.

   the parameters of the model are trained via two id168s: a
   reconstruction loss forcing the decoded samples to match the initial
   inputs (just like in our previous autoencoders), and the kl divergence
   between the learned latent distribution and the prior distribution,
   acting as a id173 term. you could actually get rid of this
   latter term entirely, although it does help in learning well-formed
   latent spaces and reducing overfitting to the training data.

   because a vae is a more complex example, we have made the code
   available on github as [19]a standalone script. here we will review
   step by step how the model is created.

   first, here's our encoder network, mapping inputs to our latent
   distribution parameters:
x = input(batch_shape=(batch_size, original_dim))
h = dense(intermediate_dim, activation='relu')(x)
z_mean = dense(latent_dim)(h)
z_log_sigma = dense(latent_dim)(h)

   we can use these parameters to sample new similar points from the
   latent space:
def sampling(args):
    z_mean, z_log_sigma = args
    epsilon = k.random_normal(shape=(batch_size, latent_dim),
                              mean=0., std=epsilon_std)
    return z_mean + k.exp(z_log_sigma) * epsilon

# note that "output_shape" isn't necessary with the tensorflow backend
# so you could write `lambda(sampling)([z_mean, z_log_sigma])`
z = lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])

   finally, we can map these sampled latent points back to reconstructed
   inputs:
decoder_h = dense(intermediate_dim, activation='relu')
decoder_mean = dense(original_dim, activation='sigmoid')
h_decoded = decoder_h(z)
x_decoded_mean = decoder_mean(h_decoded)

   what we've done so far allows us to instantiate 3 models:
     * an end-to-end autoencoder mapping inputs to reconstructions
     * an encoder mapping inputs to the latent space
     * a generator that can take points on the latent space and will
       output the corresponding reconstructed samples.

# end-to-end autoencoder
vae = model(x, x_decoded_mean)

# encoder, from inputs to latent space
encoder = model(x, z_mean)

# generator, from latent space to reconstructed inputs
decoder_input = input(shape=(latent_dim,))
_h_decoded = decoder_h(decoder_input)
_x_decoded_mean = decoder_mean(_h_decoded)
generator = model(decoder_input, _x_decoded_mean)

   we train the model using the end-to-end model, with a custom loss
   function: the sum of a reconstruction term, and the kl divergence
   id173 term.
def vae_loss(x, x_decoded_mean):
    xent_loss = objectives.binary_crossid178(x, x_decoded_mean)
    kl_loss = - 0.5 * k.mean(1 + z_log_sigma - k.square(z_mean) - k.exp(z_log_si
gma), axis=-1)
    return xent_loss + kl_loss

vae.compile(optimizer='rmsprop', loss=vae_loss)

   we train our vae on mnist digits:
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

vae.fit(x_train, x_train,
        shuffle=true,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(x_test, x_test))

   because our latent space is two-dimensional, there are a few cool
   visualizations that can be done at this point. one is to look at the
   neighborhoods of different classes on the latent 2d plane:
x_test_encoded = encoder.predict(x_test, batch_size=batch_size)
plt.figure(figsize=(6, 6))
plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)
plt.colorbar()
plt.show()

   vae classes plane

   each of these colored clusters is a type of digit. close clusters are
   digits that are structurally similar (i.e. digits that share
   information in the latent space).

   because the vae is a generative model, we can also use it to generate
   new digits! here we will scan the latent plane, sampling latent points
   at regular intervals, and generating the corresponding digit for each
   of these points. this gives us a visualization of the latent manifold
   that "generates" the mnist digits.
# display a 2d manifold of the digits
n = 15  # figure with 15x15 digits
digit_size = 28
figure = np.zeros((digit_size * n, digit_size * n))
# we will sample n points within [-15, 15] standard deviations
grid_x = np.linspace(-15, 15, n)
grid_y = np.linspace(-15, 15, n)

for i, yi in enumerate(grid_x):
    for j, xi in enumerate(grid_y):
        z_sample = np.array([[xi, yi]]) * epsilon_std
        x_decoded = generator.predict(z_sample)
        digit = x_decoded[0].reshape(digit_size, digit_size)
        figure[i * digit_size: (i + 1) * digit_size,
               j * digit_size: (j + 1) * digit_size] = digit

plt.figure(figsize=(10, 10))
plt.imshow(figure)
plt.show()

   vae classes plane

   that's it! if you have suggestions for more topics to be covered in
   this post (or in future posts), you can contact me on twitter at
   [20]@fchollet.
     __________________________________________________________________

references

   [1] [21]why does unsupervised pre-training help deep learning?

   [2] [22]batch id172: accelerating deep network training by
   reducing internal covariate shift.

   [3] [23]deep residual learning for image recognition

   [4] [24]auto-encoding id58


    powered by [25]pelican, which takes great advantages of [26]python.

references

   1. https://blog.keras.io/
   2. https://blog.keras.io/index.html
   3. https://github.com/fchollet/keras
   4. https://blog.keras.io/
   5. https://github.com/fchollet/keras
   6. http://keras.io/
   7. https://groups.google.com/forum/#!forum/keras-users
   8. https://blog.keras.io/building-autoencoders-in-keras.html
   9. https://twitter.com/fchollet
  10. https://blog.keras.io/category/tutorials.html
  11. https://en.wikipedia.org/wiki/t-distributed_stochastic_neighbor_embedding
  12. https://github.com/kylemcdonald/parametric-id167/blob/master/parametric id167 (keras).ipynb
  13. http://scikit-learn.org/stable/modules/generated/sklearn.manifold.tsne.html
  14. http://arxiv.org/abs/1603.09246
  15. https://en.wikipedia.org/wiki/principal_component_analysis
  16. https://blog.keras.io/building-autoencoders-in-keras.html
  17. https://www.kaggle.com/c/denoising-dirty-documents
  18. https://en.wikipedia.org/wiki/latent_variable_model
  19. https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py
  20. https://twitter.com/fchollet
  21. http://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf
  22. http://arxiv.org/abs/1502.03167
  23. http://arxiv.org/abs/1512.03385
  24. http://arxiv.org/abs/1312.6114
  25. http://alexis.notmyidea.org/pelican/
  26. http://python.org/
