community

   news
   beta
   tutorials
   cheat sheets
   open courses
   podcast - dataframed
   chat
   new

datacamp

   official blog
   tech thoughts
   (button)
   search
   [1](button)
   log in
   (button)
   create account
   (button)
   share an article
   (button)
   back to official blog
   official blog
   [2]0
   11
   11
   karlijn willems
   april 25th, 2017
   python
   +4

keras cheat sheet: neural networks in python

   make your own neural networks with this keras cheat sheet to deep
   learning in python for beginners, with code samples.

   [3]keras is an easy-to-use and powerful library for theano and
   tensorflow that provides a high-level neural networks api to develop
   and evaluate deep learning models.

   we recently launched one of the first online interactive deep learning
   course using keras 2.0, called "[4]deep learning in python".

   now, datacamp has created a keras cheat sheet for those who have
   already taken the course and that still want a handy one-page reference
   or for those who need an extra push to get started.

   in no time, this keras cheat sheet will make you familiar with how you
   can load datasets from the library itself, preprocess the data, build
   up a model architecture, and compile, train, and evaluate it. as there
   is a considerable amount of freedom in how you build up your models,
   you'll see that the cheat sheet uses some of the simple key code
   examples of the keras library that you need to know to get started with
   building your own neural networks in python.

   furthermore, you'll also see some examples of how to inspect your
   model, and how you can save and reload it. lastly, you   ll also find
   examples of how you can predict values for test data and how you can
   fine tune your models by adjusting the optimization parameters and
   early stopping.

   in short, you'll see that this cheat sheet not only presents you with
   the six steps that you can go through to make neural networks in python
   with the keras library.

   [5]keras cheat sheet

   in short, this cheat sheat will boost your journey with deep learning
   in python: you'll have preprocessed, created, validated and tuned your
   deep learning models in no time thanks to the code examples!

   (click above to download a printable version or read the online version
   below.)

python for data science cheat sheet: keras

   keras is a powerful and easy-to-use deep learning library for theano
   and tensorflow that provides a high-level neural networks api to
   develop and evaluate deep learning models.

a basic example

>>> import numpy as np
>>> from keras.models import sequential
>>> from keras.layers import dense
>>> data = np.random.random((1000,100))
>>> labels = np.random.randint(2,size=(1000,1))
>>> model = sequential()
>>> model.add(dense(32, activation='relu', input_dim=100))
>>> model.add(dense(1, activation='sigmoid'))
>>> model.compile(optimizer='rmsprop', loss='binary_crossid178', metrics=['acc
uracy'])
>>>  model.fit(data,labels,epochs=10,batch_size=32)
>>>  predictions = model.predict(data)

data

   your data needs to be stored as numpy arrays or as a list of numpy
   arrays. ideally, you split the data in training and test sets, for
   which you can also resort to the train_test_split module of
   sklearn.cross_validation.

keras data sets

>>> from keras.datasets import boston_housing, mnist, cifar10, imdb
>>> (x_train,y_train),(x_test,y_test) = mnist.load_data()
>>> (x_train2,y_train2),(x_test2,y_test2) = boston_housing.load_data()
>>> (x_train3,y_train3),(x_test3,y_test3) = cifar10.load_data()
>>> (x_train4,y_train4),(x_test4,y_test4) = imdb.load_data(num_words=20000)
>>> num_classes = 10

other

>>> from urllib.request import urlopen
>>> data = np.loadtxt(urlopen("http://archive.ics.uci.edu/ml/machine-learning-da
tabases/pima-indians-diabetes/pima-indians-diabetes.data"),delimiter=",")
>>> x = data[:,0:8]
>>> y = data [:,8]

preprocessing

sequence padding

>>> from keras.preprocessing import sequence
>>> x_train4 = sequence.pad_sequences(x_train4,maxlen=80)
>>> x_test4 = sequence.pad_sequences(x_test4,maxlen=80)

one-hot encoding

>>> from keras.utils import to_categorical
>>> y_train = to_categorical(y_train, num_classes)
>>> y_test = to_categorical(y_test, num_classes)
>>> y_train3 = to_categorical(y_train3, num_classes)
>>> y_test3 = to_categorical(y_test3, num_classes)

train and test sets

>>> from sklearn.model_selection import train_test_split
>>> x_train5, x_test5, y_train5, y_test5 = train_test_split(x, y, test_size=0.33
, random_state=42)

standardization/id172

>>> from sklearn.preprocessing import standardscaler
>>> scaler = standardscaler().fit(x_train2)
>>> standardized_x = scaler.transform(x_train2)
>>> standardized_x_test = scaler.transform(x_test2)

model architecture

sequential model

>>> from keras.models import sequential
>>> model = sequential()
>>> model2 = sequential()
>>> model3 = sequential()

multi-layer id88 (mlp)

   binary classification
>>> from keras.layers import dense
>>> model.add(dense(12, input_dim=8, kernel_initializer='uniform', activation='r
elu'))
>>> model.add(dense(8, kernel_initializer='uniform', activation='relu'))
>>> model.add(dense(1, kernel_initializer='uniform', activation='sigmoid'))

   multi-class classification
>>> from keras.layers import dropout
>>> model.add(dense(512,activation='relu',input_shape=(784,)))
>>> model.add(dropout(0.2))
>>> model.add(dense(512,activation='relu'))
>>> model.add(dropout(0.2))
>>> model.add(dense(10,activation='softmax'))

   regression
>>> model.add(dense(64, activation='relu', input_dim=train_data.shape[1]))
>>> model.add(dense(1))

convolutional neural network (id98)

>>> from keras.layers import activation, conv2d, maxpooling2d, flatten
>>> model2.add(conv2d(32, (3,3), padding='same', input_shape=x_train.shape[1:]))
>>> model2.add(activation('relu'))
>>> model2.add(conv2d(32, (3,3)))
>>> model2.add(activation('relu'))
>>> model2.add(maxpooling2d(pool_size=(2,2)))
>>> model2.add(dropout(0.25))
>>> model2.add(conv2d(64, (3,3), padding='same'))
>>> model2.add(activation('relu'))
>>> model2.add(conv2d(64, (3, 3)))
>>> model2.add(activation('relu'))
>>> model2.add(maxpooling2d(pool_size=(2,2)))
>>> model2.add(dropout(0.25))
>>> model2.add(flatten())
>>> model2.add(dense(512))
>>> model2.add(activation('relu'))
>>> model2.add(dropout(0.5))
>>> model2.add(dense(num_classes))
>>> model2.add(activation('softmax'))

recurrent neural network (id56)

>>> from keras.klayers import embedding,lstm
>>> model3.add(embedding(20000,128))
>>> model3.add(lstm(128,dropout=0.2,recurrent_dropout=0.2))
>>> model3.add(dense(1,activation='sigmoid'))

inspect model

   model output shape
>>> model.output_shape

   model summary representation
>>> model.summary()

   model configuration
>>> model.get_config()

   list all weight tensors in the model
>>> model.get_weights()

compile model

multi-layer id88 (mlp)

   mlp: binary classification
>>> model.compile(optimizer='adam', loss='binary_crossid178', metrics=['accura
cy'])

   mlp: multi-class classification
>>> model.compile(optimizer='rmsprop', loss='categorical_crossid178', metrics=
['accuracy'])

   mlp: regression
>>> model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])

recurrent neural network (id56)

>>> model3.compile(loss='binary_crossid178', optimizer='adam', metrics=['accur
acy'])

model training

>>> model3.fit(x_train4, y_train4, batch_size=32, epochs=15, verbose=1, validati
on_data=(x_test4, y_test4))

evaluate your model's performance

>>> score = model3.evaluate(x_test, y_test, batch_size=32)

prediction

>>> model3.predict(x_test4, batch_size=32)
>>> model3.predict_classes(x_test4,batch_size=32)

save/reload models

>>> from keras.models import load_model
>>> model3.save('model_file.h5')
>>> my_model = load_model('my_model.h5')

model fine-tuning

optimization parameters

>>> from keras.optimizers import rmsprop
>>> opt = rmsprop(lr=0.0001, decay=1e-6)
>>> model2.compile(loss='categorical_crossid178', optimizer=opt, metrics=['acc
uracy'])

early stopping

>>> from keras.callbacks import earlystopping
>>> early_stopping_monitor = earlystopping(patience=2)
>>> model3.fit(x_train4, y_train4, batch_size=32, epochs=15, validation_data=(x_
test4, y_test4), callbacks=[early_stopping_monitor])

going further

   begin with [6]our keras tutorial for beginners, in which you'll learn
   in an easy, step-by-step way how to explore and preprocess the wine
   quality data set, build up a multi-layer id88 for classification
   and regression tasks, compile, fit and evaluate the model and fine-tune
   the model that you have built.

   also, don't miss out on our[7] scikit-learn cheat sheet, [8]numpy cheat
   sheet and [9]pandas cheat sheet!
   11
   11
   [10]0
   (button)
   post a comment

   [11]subscribe to rss
   [12]about[13]terms[14]privacy

   want to leave a comment?

references

   visible links
   1. https://www.datacamp.com/users/sign_in
   2. https://www.datacamp.com/community/blog/keras-cheat-sheet#comments
   3. http://keras.io/
   4. https://www.datacamp.com/courses/deep-learning-in-python/
   5. https://s3.amazonaws.com/assets.datacamp.com/blog_assets/keras_cheat_sheet_python.pdf
   6. https://www.datacamp.com/community/tutorials/deep-learning-python
   7. https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet/
   8. https://www.datacamp.com/community/blog/python-numpy-cheat-sheet/
   9. https://www.datacamp.com/community/blog/python-pandas-cheat-sheet/
  10. https://www.datacamp.com/community/blog/keras-cheat-sheet#comments
  11. https://www.datacamp.com/community/rss.xml
  12. https://www.datacamp.com/about
  13. https://www.datacamp.com/terms-of-use
  14. https://www.datacamp.com/privacy-policy

   hidden links:
  16. https://www.datacamp.com/
  17. https://www.datacamp.com/community
  18. https://www.datacamp.com/community/tutorials
  19. https://www.datacamp.com/community/data-science-cheatsheets
  20. https://www.datacamp.com/community/open-courses
  21. https://www.datacamp.com/community/podcast
  22. https://www.datacamp.com/community/chat
  23. https://www.datacamp.com/community/blog
  24. https://www.datacamp.com/community/tech
  25. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/blog/keras-cheat-sheet
  26. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/blog/keras-cheat-sheet
  27. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/blog/keras-cheat-sheet
  28. https://www.datacamp.com/profile/karlijn
  29. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/blog/keras-cheat-sheet
  30. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/blog/keras-cheat-sheet
  31. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/blog/keras-cheat-sheet
  32. https://www.facebook.com/pages/datacamp/726282547396228
  33. https://twitter.com/datacamp
  34. https://www.linkedin.com/company/datamind-org
  35. https://www.youtube.com/channel/uc79gv3myp6zkiswyemeik9a
