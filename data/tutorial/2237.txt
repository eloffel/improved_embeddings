   [1][mainlogowhitethick.jpg]
     * search
     * ____________________
     * [2]home
     * [3]+=1
     * [4]support the content
     * [5]community
     * [6]log in
     * [7]sign up

     * ____________________
     * [8]home
     * [9]+=1
     * [10]support the content
     * [11]community
     * [12]log in
     * [13]sign up

   [unsupervised-machine-learning-id91-mean-shift-and-kmeans.png]

unsupervised machine learning: flat id91

id116 clusternig example with python and scikit-learn

   iframe:
   [14]https://www.youtube.com/embed/zs-im9c3efg?list=plqvvvaa0qudd0flggph
   kcej-9jp-qdzz3

   this series is concerning "unsupervised machine learning." the
   difference between supervised and unsupervised machine learning is
   whether or not we, the scientist, are providing the machine with
   labeled data.

   unsupervised machine learning is where the scientist does not provide
   the machine with labeled data, and the machine is expected to derive
   structure from the data all on its own.

   there are many forms of this, though the main form of unsupervised
   machine learning is id91. within id91, you have "flat"
   id91 or "hierarchical" id91.

   flat id91

   flat id91 is where the scientist tells the machine how many
   categories to cluster the data into.

   hierarchical

   hierarchical id91 is where the machine is allowed to decide how
   many clusters to create based on its own algorithms.

   this page will cover a flat id91 example, and the next tutorial
   will cover a hierarchical id91 example.

   now, what can we use unsupervised machine learning for? in general,
   unsupervised machine learning can actually solve the exact same
   problems as supervised machine learning, though it may not be as
   efficient or accurate.

   unsupervised machine learning is most often applied to questions of
   underlying structure. genomics, for example, is an area where we do not
   truly understand the underlying structure. thus, we use unsupervised
   machine learning to help us figure out the structure.

   unsupervised learning can also aid in "feature reduction." a term we
   will cover eventually here is "principal component analysis," or pca,
   which is another form of feature reduction, used frequently with
   unsupervised machine learning. pca attempts to locate linearly
   uncorrelated variables, calling these the principal components, since
   these are the more "unique" elements that differentiate or describe
   whatever the object of analysis is.

   there is also a meshing of supervised and unsupervised machine
   learning, often called semi-supervised machine learning. you will often
   find things get more complicated with real world examples. you may
   find, for example, that first you want to use unsupervised machine
   learning for feature reduction, then you will shift to supervised
   machine learning once you have used, for example, flat id91 to
   group your data into two clusters, which are now going to be your two
   labels for supervised learning.

   what might be an actual example of this? how about we're trying to
   differentiate between male and female faces. we aren't already sure
   what the defining differences between a male and female face are, so we
   take to unsupervised machine learning first. our hopeful goal will be
   to create an algorithm that will naturally just group the faces into
   two groups. we're likely to go ahead and use flat id91 for this,
   and then we'll likely test the algorithm to see if it was indeed
   accurate, using labeled data only for testing, not for training. if we
   find the machine is successful, we now actually have our labels, and
   features that make up a male face and a female face. we can then use
   pca, or maybe we already did. either way, we can try to get feature
   count down. once we've done this, we use these labels with their
   principle components as features, which we can then feed into a
   supervised machine learning algorithm for actual future identification.

   now that you know some of the uses and some key terms, let's see an
   actual example with [15]python and the [16]scikit-learn (sklearn)
   module.
   don't have python or sklearn?

   [17]python is a programming language, and the language this entire
   website covers tutorials on. if you need python, click on the link to
   python.org and download the latest version of python.

   [18]scikit-learn (sklearn) is a popular machine learning module for the
   python programming language.

   the scikit-learn module depends on matplotlib, scipy, and numpy as
   well. you can use pip to install all of these, once you have python.
   don't know what pip is or how to install modules?

   pip is probably the easiest way to install packages once you install
   python, you should be able to open your command prompt, like cmd.exe on
   windows, or bash on linux, and type:
   pip install scikit-learn

   having trouble still? no problem, there's a tutorial for that: [19]pip
   install python modules tutorial.

   if you're still having trouble, feel free to contact us, using the
   contact in the footer of this website.
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
style.use("ggplot")
from sklearn.cluster import kmeans

   here, we're just doing our basic imports. we're importing numpy which
   is a useful numbers crunching module, then matplotlib for graphing, and
   then kmeans from sklearn.
   confused about imports and modules? (other than "kmeans")

   if you're confused about imports, you may need to first run through the
   [20]python 3 basics series, or specifically the [21]module import
   syntax tutorial.

   the kmeans import from sklearn.cluster is in reference to the id116
   id91 algorithm. the general idea of id91 is to cluster data
   points together using various methods. you can probably guess that
   id116 uses something to do with means. what ends up happening is a
   centroid, or prototype point, is identified, and data points are
   "clustered" into their groups by the centroid they are the closest to.
   clusters being called cells, or voronoi cells, and references to
   lloyd's algorithm

   one of the things that makes any new topic confusing is a lot of
   complex sounding terms. i do my best to keep things simple, but not
   everyone is as kind as me.

   some people will refer to the style of clusters that you wind up seeing
   as "voronoi" cells. usually the "clusters" have defining "edges" to
   them that, when shaded or colored in, look like geometrical polygons,
   or cells, like this:
   [plot_kmeans_digits_001.png]

   the id116 algorithm gets its origins from "lloyd's algorithm," which
   basically does the exact same thing.

x = [1, 5, 1.5, 8, 1, 9]
y = [2, 8, 1.8, 8, 0.6, 11]

plt.scatter(x,y)
plt.show()

   this block of code is not required for machine learning. what we're
   doing here is plotting and visualizing our data before feeding it into
   the machine learning algorithm.

   running the code up to this point will provide you with the following
   graph:
   [flat-id91-python-tutorial-unmarked.png]

   this is the same set of data and graph that we used for our [22]support
   vector machine / linear svc example with supervised machine learning.

   you can probably look at this graph, and group this data all on your
   own. imagine if this graph was 3d. it would be a little harder. now
   imagine this graph is 50-dimensional. suddenly you're immobilized!

   in the supervised machine learning example with this data, we were
   allowed to feed this data to the machine along with labels. thus, the
   lower left group had labeling, and the upper right grouping did too.
   our task there was to then accept future points and properly group them
   according the groups. easy enough.

   our task here, however, is a bit different. we do not have labeled
   data, and we want the machine to figure out all on its own that it
   needs to group the data.

   for now, since we're doing flat-id91, our task is a bit easier
   since we can tell the machine that we want it categorized into two
   groups. still, however, how might you do this?

   id116 approaches the problem by finding similar means, repeatedly
   trying to find centroids that match with the least variance in groups

   this repeatedly trying ends up leaving this algorithm with fairly poor
   performance, though performance is an issue with all machine learning
   algorithms. this is why it is usually suggested that you use a highly
   stream-lined and efficient algorithm that is already tested heavily
   rather than creating your own.

   you also have to decide, as the scientist, how highly you value
   precision as compared to speed. there is always a balance between
   precision and speed/performance. more on this later, however. moving on
   with the code
x = np.array([[1, 2],
              [5, 8],
              [1.5, 1.8],
              [8, 8],
              [1, 0.6],
              [9, 11]])

   here, we're simply converting our data to a numpy array. see the video
   if you're confused. you should see each of the brackets here are the
   same x,y coordinates as before. we're doing this because a numpy array
   of features is what scikit-learn / sklearn expects.
kmeans = kmeans(n_clusters=2)
kmeans.fit(x)

centroids = kmeans.cluster_centers_
labels = kmeans.labels_

print(centroids)
print(labels)

   here, we initialize kmeans to be the kmeans algorithm (flat
   id91), with the required parameter of how many clusters
   (n_clusters).

   next, we use .fit() to fit the data (learning)

   next, we're grabbing the values found for the centroids, based on the
   fitment, as well as the labels for each centroid.

   note here that the "labels" here are labels that the machine has
   assigned on its own, same with the centroids.

   now we're going to actually plot and visualize the machine's findings
   based on our data, and the fitment according to the number of clusters
   we said to find.
colors = ["g.","r.","c.","y."]

for i in range(len(x)):
    print("coordinate:",x[i], "label:", labels[i])
    plt.plot(x[i][0], x[i][1], colors[labels[i]], markersize = 10)


plt.scatter(centroids[:, 0],centroids[:, 1], marker = "x", s=150, linewidths = 5
, zorder = 10)

plt.show()

   the above code is all "visualization" code, having nothing more to do
   with machine learning than just showing us some results.

   first, we have a "colors" list. this list will be used to be iterated
   through to get some custom colors for the resulting graph. just a nice
   box of colors to use.

   we only need two colors at first, but soon we're going to ask the
   machine to classify into other numbers of groups just for learning
   purposes, so i decided to put four choices here. the period after the
   letters is just the type of "plot marker" to use.

   now, we're using a for loop to iterate through our plots.

   if you're confused about the for loop, you may need to first run
   through the [23]python 3 basics series, or specifically the [24]for
   loop basics tutorial.

   if you're confused about the actual code being used, especially with
   iterating through this loop, or the scatter plotting code slices that
   look like this: [:, 0], then check out the video. i explain them there.

   the resulting graph, after the one that just shows the points, should
   look like:
   [unsupervised-machine-learning-flat-id91-kmeans-example.png]

   do you see the voronoi cells? i hope not, we didn't draw them.
   remember, those are the polygons that mark the divisions between
   clusters. here, we have each plot marked, by color, what group it
   belongs to. we have also marked the centroids as big blue "x" shapes.

   as we can see, the machine was very successful! now, i encourage you to
   play with the n_clusters variable. first, decide how many clusters you
   will do, then try to predict where the centroids will be.

   we do this in the video, choosing 3 and 4 clusters. it's relatively
   easy to predict with these points if you understand how the algorithm
   works, and makes for a good learning exercise.

   that's it for our flat id91 example for unsupervised learning,
   how about hierarchical id91 next?
   [25]there exists 1 challenge(s) for this tutorial. (button) sign up to
   +=1 for access to these, video downloads, and no ads.
   [26]there exists 2 quiz/question(s) for this tutorial. (button) sign up
   to +=1 for access to these, video downloads, and no ads.

   the next tutorial: [27](button) unsupervised machine learning:
   hierarchical id91
   [plus-equals-1-v4.png]
     * unsupervised machine learning: flat id91
     * unsupervised machine learning: hierarchical id91
       [28](button) go

you've reached the end!

   contact: harrison@pythonprogramming.net.
     * [29]support this website!
     * [30]hire me
     * [31]facebook
     * [32]twitter
     * [33]google+

legal stuff:

     * [34]terms and conditions
     * [35]privacy policy

   [36]

   programming is a superpower.

      over 9000! pythonprogramming.net

references

   visible links
   1. https://pythonprogramming.net/
   2. https://pythonprogramming.net/
   3. https://pythonprogramming.net/+=1/
   4. https://pythonprogramming.net/support/
   5. https://goo.gl/7zgavq
   6. https://pythonprogramming.net/login/
   7. https://pythonprogramming.net/register/
   8. https://pythonprogramming.net/
   9. https://pythonprogramming.net/+=1/
  10. https://pythonprogramming.net/support/
  11. https://goo.gl/7zgavq
  12. https://pythonprogramming.net/login/
  13. https://pythonprogramming.net/register/
  14. https://www.youtube.com/embed/zs-im9c3efg?list=plqvvvaa0qudd0flggphkcej-9jp-qdzz3
  15. https://www.python.org/
  16. http://scikit-learn.org/
  17. https://www.python.org/
  18. http://scikit-learn.org/
  19. https://pythonprogramming.net/using-pip-install-for-python-modules/
  20. https://pythonprogramming.net/introduction-to-python-programming/
  21. https://pythonprogramming.net/module-import-syntax-python-3-tutorial/
  22. https://pythonprogramming.net/linear-svc-example-scikit-learn-id166-python/
  23. https://pythonprogramming.net/introduction-to-python-programming/
  24. https://pythonprogramming.net/loop-python-3-basics-tutorial/
  25. https://pythonprogramming.net/+=1/
  26. https://pythonprogramming.net/+=1/
  27. https://pythonprogramming.net/hierarchical-id91-machine-learning-python-scikit-learn/?completed=/flat-id91-machine-learning-python-scikit-learn/
  28. https://pythonprogramming.net/hierarchical-id91-machine-learning-python-scikit-learn/
  29. https://pythonprogramming.net/support-donate/
  30. https://pythonprogramming.net/consulting/
  31. https://www.facebook.com/pythonprogramming.net/
  32. https://twitter.com/sentdex
  33. https://plus.google.com/+sentdex
  34. https://pythonprogramming.net/about/tos/
  35. https://pythonprogramming.net/about/privacy-policy/
  36. https://xkcd.com/353/

   hidden links:
  38. https://pythonprogramming.net/flat-id91-machine-learning-python-scikit-learn/
  39. https://pythonprogramming.net/+=1/?a=6&t=/flat-id91-machine-learning-python-scikit-learn/
  40. https://pythonprogramming.net/+=1/?a=6&t=/flat-id91-machine-learning-python-scikit-learn/
