   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    practical guide on id174 in python
   using scikit learn comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]business analytics [94]practical guide on data
   preprocessing in python using scikit learn

   [95]business analytics[96]machine learning[97]python

practical guide on id174 in python using scikit learn

   [98]syed danish, july 18, 2016

introduction

   this article primarily focuses on data pre-processing techniques in
   python. learning algorithms have affinity towards certain data types
   on which they perform incredibly well. they are also known to give
   reckless predictions with unscaled or unstandardized
   features. algorithm like xgboost, specifically requires dummy encoded
   data while algorithm like decision tree doesn   t seem to care at all
   (sometimes)!

   in simple words, pre-processing refers to the transformations applied
   to your data before feeding it to the algorithm. in python,
   scikit-learn library has a pre-built functionality under
   [99]sklearn.preprocessing. there are many more options for
   pre-processing which we   ll explore.

   after finishing this article, you will be equipped with the basic
   techniques of data pre-processing and their in-depth understanding. for
   your convenience, i   ve attached some resources for in-depth learning of
   machine learning algorithms and designed few exercises to get a good
   grip of the concepts.

   practical guide on id174 in python using scikit learn


available data set

   for this article, i have used a subset of the [100]loan
   prediction (missing value observations are dropped) data set from you
   can download the final training and testing data set from here:
   [101]download data

   note : testing data that you are provided is the subset of the training
   data from loan prediction problem.


   now, lets get started by importing important packages and the data set.
# importing pandas
>> import pandas as pd
# importing training data set
>> x_train=pd.read_csv('x_train.csv')
>> y_train=pd.read_csv('y_train.csv')
# importing testing data set
>> x_test=pd.read_csv('x_test.csv')
>> y_test=pd.read_csv('y_test.csv')

   lets take a closer look at our data set.
>> print (x_train.head())

      loan_id gender married dependents education self_employed
15   lp001032   male      no          0  graduate            no
248  lp001824   male     yes          1  graduate            no
590  lp002928   male     yes          0  graduate            no
246  lp001814   male     yes          2  graduate            no
388  lp002244   male     yes          0  graduate            no

     applicantincome  coapplicantincome  loanamount  loan_amount_term
15              4950                0.0       125.0             360.0
248             2882             1843.0       123.0             480.0
590             3000             3416.0        56.0             180.0
246             9703                0.0       112.0             360.0
388             2333             2417.0       136.0             360.0

     credit_history property_area
15              1.0         urban
248             1.0     semiurban
590             1.0     semiurban
246             1.0         urban
388             1.0         urban


feature scaling

   feature scaling is the method to limit the range of variables so that
   they can be compared on common grounds. it is performed on continuous
   variables. lets plot the distribution of all the continuous variables
   in  the data set.
>> import matplotlib.pyplot as plt
>> x_train[x_train.dtypes[(x_train.dtypes=="float64")|(x_train.dtypes=="int64")]
                        .index.values].hist(figsize=[11,11])


   [102][index-4.png]

   after understanding these plots, we infer that applicantincome and
   coapplicantincome are in similar range (0-50000$) where as
   loanamount is in thousands and it ranges from 0 to 600$. the story for
   loan_amount_term is completely different from other variables because
   its unit is months as opposed to other variables where the unit is
   dollars.

   if we try to apply distance based methods such as knn on these
   features, feature with the largest range will dominate the outcome
   results and we   ll obtain less accurate predictions. we can overcome
   this trouble using feature scaling. let   s do it practically.

   resources : check out this article on [103]knn for better
   understanding.

   lets try out knn on our data set to see how well it will perform.
# initializing and fitting a id92 model
>> from sklearn.neighbors import kneighborsclassifier
>> knn=kneighborsclassifier(n_neighbors=5)
>> knn.fit(x_train[['applicantincome', 'coapplicantincome','loanamount',
                   'loan_amount_term', 'credit_history']],y_train)
# checking the performance of our model on the testing data set
>> from sklearn.metrics import accuracy_score
>> accuracy_score(y_test,knn.predict(x_test[['applicantincome', 'coapplicantinco
me',
                             'loanamount', 'loan_amount_term', 'credit_history']
]))
out : 0.61458333333333337

   we got around 61% of correct prediction which is not bad but in real
   world practices will this be enough ? can we deploy this model in real
   world problem? to answer this question lets take a look at distribution
   of loan_status in train data set.
>> y_train.target.value_counts()/y_train.target.count()
out : y 0.705729
      n 0.294271
name: loan_status, dtype: float64

   there are 70% of approved loans, since there are more number of
   approved loans we will generate a prediction where all the loans are
   approved and lets go ahead and check the accuracy of our prediction
>> y_test.target.value_counts()/y_test.target.count()
out :  y 0.635417
       n 0.364583
name: loan_status, dtype: float64

   wow !! we got an accuracy of 63% just by guessing, what is the meaning
   of this, getting better accuracy than our prediction model ?

   this might be happening because of some insignificant variable with
   larger range will be dominating the objective function. we can remove
   this problem by scaling down all the features to a same range. sklearn
   provides a tool minmaxscaler that will scale down all the features
   between 0 and 1. mathematical formula for minmaxscaler is.

   [104]screenshot from 2016-06-29 14-16-29

   lets try this tool on our problem.
# importing minmaxscaler and initializing it
>> from sklearn.preprocessing import minmaxscaler
>> min_max=minmaxscaler()
# scaling down both train and test data set
>> x_train_minmax=min_max.fit_transform(x_train[['applicantincome', 'coapplicant
income',
                'loanamount', 'loan_amount_term', 'credit_history']])
>> x_test_minmax=min_max.fit_transform(x_test[['applicantincome', 'coapplicantin
come',
                'loanamount', 'loan_amount_term', 'credit_history']])

   now, that we are done with scaling, lets apply knn on our scaled data
   and check its accuracy.
# fitting id92 on our scaled data set
>> knn=kneighborsclassifier(n_neighbors=5)
>> knn.fit(x_train_minmax,y_train)
# checking the model's accuracy
>> accuracy_score(y_test,knn.predict(x_test_minmax))
out : 0.75

   great !! our accuracy has increased from 61% to 75%. this means that
   some of the features with larger range were dominating the prediction
   outcome in the domain of distance based methods(knn).

   it should be kept in mind while performing distance based methods we
   must attempt to scale the data, so that the feature with lesser
   significance might not end up dominating the objective function due to
   its larger range. in addition, features having different unit should
   also be scaled thus providing each feature equal initial weightage and
   at the end we will have a better prediction model.

exercise 1

   try to do the same exercise with a id28 model(parameters
   : penalty=   l2   ,c=0.01) and provide your accuracy before and after
   scaling in the comment section.


feature standardization

   before jumping to this section i suggest you to complete exercise 1.

   in the previous section, we worked on the loan_prediction data set and
   fitted a knn learner on the data set. after scaling down the data, we
   have got an accuracy of 75% which is very considerably good. i tried
   the same exercise on id28 and i got the following result
   :

   before scaling : 61%

   after scaling : 63%

   the accuracy we got after scaling is close to the prediction which we
   made by guessing, which is not a very impressive achievement. so, what
   is happening here? why hasn   t the accuracy increased by a satisfactory
   amount as it increased in knn?

   resources : go through this article on [105]id28 for
   better understanding.

   here is the answer:

   in id28, each feature is assigned a weight or
   coefficient (wi). if there is a feature with relatively large range and
   it is insignificant in the objective function then id28
   will itself assign a very low value to its co-efficient, thus
   neutralizing the dominant effect of that particular feature, whereas
   distance based method such as knn does not have this inbuilt strategy,
   thus it requires scaling.

   aren   t we forgetting something ? our logistic model is still predicting
   with an accuracy almost closer to a guess.

   now, i   ll be introducing a new concept here called standardization.
   many machine learning algorithms in sklearn requires standardized data
   which means having zero mean and unit variance.

   standardization (or z-score id172) is the process where the
   features are rescaled so that they   ll have the properties of a standard
   normal distribution with   =0 and   =1, where    is the mean (average) and
      is the standard deviation from the mean. standard scores (also called
   z scores) of the samples are calculated as follows :

   [106]screenshot from 2016-06-29 14-06-42

   elements such as l1 ,l2 regularizer in linear models (logistic comes
   under this category) and rbf kernel in id166 in objective function of
   learners assumes that all the features are centered around zero and
   have variance in the same order.

   features having larger order of variance would dominate on the
   objective function as it happened in the previous section with the
   feature having large range. as we saw in the exercise 1 that without
   any preprocessing on the data the accuracy was 61%, lets standardize
   our data apply id28 on that. sklearn provides scale to
   standardize the data.
# standardizing the train and test data
>> from sklearn.preprocessing import scale
>> x_train_scale=scale(x_train[['applicantincome', 'coapplicantincome',
                'loanamount', 'loan_amount_term', 'credit_history']])
>> x_test_scale=scale(x_test[['applicantincome', 'coapplicantincome',
               'loanamount', 'loan_amount_term', 'credit_history']])
# fitting id28 on our standardized data set
>> from sklearn.linear_model import logisticregression
>> log=logisticregression(penalty='l2',c=.01)
>> log.fit(x_train_scale,y_train)
# checking the model's accuracy
>> accuracy_score(y_test,log.predict(x_test_scale))
out : 0.75

   we again reached to our maximum score that was attained using knn after
   scaling. this means standardizing the data when using a estimator
   having l1 or l2 id173 helps us to increase the accuracy of the
   prediction model. other learners like knn with euclidean distance
   measure, id116, id166, id88, neural networks, linear discriminant
   analysis, principal component analysis may perform better with
   standardized data.

   though, i suggest you to understand your data and what kind of
   algorithm you are going to apply on it; over the time you will be able
   to judge weather to standardize your data or not.

   note : choosing between scaling and standardizing is a confusing
   choice, you have to dive deeper in your data and learner that you are
   going to use to reach the decision. for starters, you can try both the
   methods and check cross validation score for making a choice.

   resources : go through this article on [107]cross validation for better
   understanding.

exercise 2

   try to do the same exercise with id166 model and provide your accuracy
   before and after standardization in the comment section.

   resources : go through this article on [108]support vector machines for
   better understanding.


label encoding

   in previous sections, we did the pre-processing for continuous numeric
   features. but, our data set has other features too such as gender,
   married, dependents, self_employed and education. all these categorical
   features have string values. for example, gender has two levels either
   male or female. lets feed the features in our id28
   model.
# fitting a id28 model on whole data
>> log=logisticregression(penalty='l2',c=.01)
>> log.fit(x_train,y_train)
# checking the model's accuracy
>> accuracy_score(y_test,log.predict(x_test))
out : valueerror: could not convert string to float: semiurban

   we got an error saying that it cannot convert string to float. so,
   what   s actually happening here is learners like id28,
   distance based methods such as knn, support vector machines, tree based
   methods etc. in sklearn needs numeric arrays. features having string
   values cannot be handled by these learners.

   sklearn provides a very efficient tool for encoding the levels of a
   categorical features into numeric values. labelencoder encode labels
   with value between 0 and n_classes-1.

   lets encode all the categorical features.
# importing labelencoder and initializing it
>> from sklearn.preprocessing import labelencoder
>> le=labelencoder()
# iterating over all the common columns in train and test
>> for col in x_test.columns.values:
       # encoding only categorical variables
       if x_test[col].dtypes=='object':
       # using whole data to form an exhaustive list of levels
       data=x_train[col].append(x_test[col])
       le.fit(data.values)
       x_train[col]=le.transform(x_train[col])
       x_test[col]=le.transform(x_test[col])

   all our categorical features are encoded. you can look at your updated
   data set using x_train.head(). we are going to take a look at
   gender frequency distribution before and after the encoding.
before : male 318
         female 66
name: gender, dtype: int64
after : 1 318
        0 66
name: gender, dtype: int64

   now that we are done with label encoding, lets now run a logistic
   regression model on the data set with both categorical and continuous
   features.
# standardizing the features
>> x_train_scale=scale(x_train)
>> x_test_scale=scale(x_test)
# fitting the id28 model
>> log=logisticregression(penalty='l2',c=.01)
>> log.fit(x_train_scale,y_train)
# checking the models accuracy
>> accuracy_score(y_test,log.predict(x_test_scale))
out : 0.75

   its working now. but, the accuracy is still the same as we got with
   id28 after standardization from numeric features.
   this means categorical features we added are not very significant in
   our objective function.

exercise 3

   try out decision tree classifier with all the features as independent
   variables and comment your accuracy.

   resources : go through this article on [109]id90 for better
   understanding.


one-hot encoding

   one-hot encoding transforms each categorical feature with n possible
   values into n binary features, with only one active.

   most of the ml algorithms either learn a single weight for each feature
   or it computes distance between the samples. algorithms like linear
   models (such as id28) belongs to the first category.

   lets take a look at an example from loan_prediction data set. feature
   dependents have 4 possible values 0,1,2 and 3+ which are then encoded
   without loss of generality to 0,1,2 and 3.

   we, then have a weight    w    assigned for this feature in a linear
   classifier,which will make a decision based on the constraints
   w*dependents + k > 0 or eqivalently  w*dependents < k.

   let f(w)= w*dependents

   possible values that can be attained by the equation are 0, w, 2w and
   3w. a problem with this equation is that the weight    w    cannot make
   decision based on four choices. it can reach to a decision in following
   ways:
     * all leads to the same decision (all of them <k or vice versa)
     * 3:1 division of the levels (decision boundary at f(w)>2w)
     * 2:2 division of the levels (decision boundary at f(w)>w)

   here we can see that we are loosing many different possible decisions
   such as the case where    0    and    2w    should be given same label and    3w   
   and    w    are odd one out.

   this problem can be solved by one-hot-encoding as it effectively
   changes the dimensionality of the feature    dependents    from one to
   four, thus every value in the feature    dependents    will have their own
   weights. updated equation for the decison would be f'(w) < k.

   where,  f'(w) = w1*d_0 + w2*d_1 + w3*d_2 + w4*d_3
   all four new variable has boolean values (0 or 1).

   the same thing happens with distance based methods such as knn. without
   encoding, distance between    0    and    1    values of dependents is 1
   whereas distance between    0    and    3+    will be 3, which is not desirable
   as both the distances should be similar. after encoding, the values
   will be new features (sequence of columns is 0,1,2,3+) : [1,0,0,0] and
   [0,0,0,1] (initially we were finding distance between    0    and    3+   ),
   now the distance would be    2.

   for tree based methods, same situation (more than two values in a
   feature) might effect the outcome to extent but if methods like random
   forests are deep enough, it can handle the categorical variables
   without one-hot encoding.

   now, lets take look at the implementation of one-hot encoding with
   various algorithms.

   lets create a id28 model for classification without
   one-hot encoding.
# we are using scaled variable as we saw in previous section that
# scaling will effect the algo with l1 or l2 reguralizer
>> x_train_scale=scale(x_train)
>> x_test_scale=scale(x_test)
# fitting a id28 model
>> log=logisticregression(penalty='l2',c=1)
>> log.fit(x_train_scale,y_train)
# checking the model's accuracy
>> accuracy_score(y_test,log.predict(x_test_scale))
out : 0.73958333333333337

   now we are going to encode the data.
>> from sklearn.preprocessing import onehotencoder
>> enc=onehotencoder(sparse=false)
>> x_train_1=x_train
>> x_test_1=x_test
>> columns=['gender', 'married', 'dependents', 'education','self_employed',
          'credit_history', 'property_area']
>> for col in columns:
       # creating an exhaustive list of all possible categorical values
       data=x_train[[col]].append(x_test[[col]])
       enc.fit(data)
       # fitting one hot encoding on train data
       temp = enc.transform(x_train[[col]])
       # changing the encoded features into a data frame with new column names
       temp=pd.dataframe(temp,columns=[(col+"_"+str(i)) for i in data[col]
            .value_counts().index])
       # in side by side concatenation index values should be same
       # setting the index values similar to the x_train data frame
       temp=temp.set_index(x_train.index.values)
       # adding the new one hot encoded varibales to the train data frame
       x_train_1=pd.concat([x_train_1,temp],axis=1)
       # fitting one hot encoding on test data
       temp = enc.transform(x_test[[col]])
       # changing it into data frame and adding column names
       temp=pd.dataframe(temp,columns=[(col+"_"+str(i)) for i in data[col]
            .value_counts().index])
       # setting the index for proper concatenation
       temp=temp.set_index(x_test.index.values)
       # adding the new one hot encoded varibales to test data frame
       x_test_1=pd.concat([x_test_1,temp],axis=1)

   now, lets apply id28 model on one-hot encoded data.
# standardizing the data set
>> x_train_scale=scale(x_train_1)
>> x_test_scale=scale(x_test_1)
# fitting a id28 model
>> log=logisticregression(penalty='l2',c=1)
>> log.fit(x_train_scale,y_train)
# checking the model's accuracy
>> accuracy_score(y_test,log.predict(x_test_scale))
out : 0.75

   here, again we got the maximum accuracy as 0.75 that we have gotten so
   far. in this case, id28 id173(c) parameter 1
   where as earlier we used c=0.01.


end notes

   the aim of this article is to familiarize you with the basic data
   pre-processing techniques and have a deeper understanding of the
   situations of where to apply those techniques.

   these methods work because of the underlying assumptions of the
   algorithms. this is by no means an exhaustive list of the methods.
   i   d encourage you to experiment with these methods since they can be
   heavily modified according to the problem at hand.

   i plan to provide more advance techniques of data pre-processing such
   as [110]pipeline and noise reduction in my next post, so stay tuned to
   dive deeper into the data pre-processing.

   did you like reading this article ? do you follow a different approach
   / package / library to perform these talks. i   d love to interact with
   you in comments.

you can test your skills and knowledge. check out [111]live competitions and
compete with best data scientists from all over the world.

   you can also read this article on analytics vidhya's android app
   [112]get it on google play

share this:

     * [113]click to share on linkedin (opens in new window)
     * [114]click to share on facebook (opens in new window)
     * [115]click to share on twitter (opens in new window)
     * [116]click to share on pocket (opens in new window)
     * [117]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [118]categorical variable, [119]continuous variable, [120]data
   exploration, [121]data mining, [122]id174, [123]decision
   trees, [124]dummy encoding, [125]label encoding, [126]logistic
   regression, [127]machine learning, [128]missing value,
   [129]id172, [130]python, [131]scaling, [132]scikit-learn,
   [133]standardization
   next article

20 challenging job interview puzzles which every analyst should solve atleast
once

   previous article

going deeper into regression analysis with assumptions, plots & solutions

[134]syed danish

   i am syed danish, currently pursuing my bachelors in electronics &
   communication engineering from ism dhanbad. i am a data science and
   machine learning enthusiast.
     *
     *
     *
     *

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [135]discussion portal to get your queries resolved

30 comments

     * dr venugopala rao manneni says:
       [136]july 18, 2016 at 5:57 am
       useful   
       [137]reply
     * oluwadara says:
       [138]july 18, 2016 at 9:24 am
       thanks for the post. it was useful to me!
       [139]reply
     * mukul says:
       [140]july 18, 2016 at 10:34 am
       feature scaling code
       x_train[x_train.dtypes[(x_train.dtypes==   float64   )|(x_train.dtypes=
       =   int64   )].index.values].hist(figsize=[11,11])
       is not working. there is something wrong there.
       [141]reply
          + [142]syed danish says:
            [143]july 18, 2016 at 2:13 pm
            there was some problem with the formatting of inverted commas,
            thanks for bringing my attention towards it.
            [144]reply
          + mukul says:
            [145]july 18, 2016 at 5:02 pm
            thanks a lot. .
            [146]reply
     * nabar says:
       [147]july 18, 2016 at 3:11 pm
       this is also not working    
       accuracy_score(y_test,knn.predict(x_test[[   applicantincome   ,
          coapplicantincome   ,   loanamount   ,    loan_amount_term   ,
          credit_history   ]]))
       do you have working code?
       [148]reply
          + [149]syed danish says:
            [150]july 18, 2016 at 3:33 pm
            every line of code is tested. there is a problem with the
            formatting of the apostrophe(   ) when i copied your code from
            comment section, although it is working fine for me when i
            copied the code from the article.
            p.s. formatting of apostrophe and inverted commas are messing
            up the code, please check the formatting before running the
            code.
            [151]reply
     * anonymous says:
       [152]july 18, 2016 at 4:19 pm
       when typing:
       knn.fit(x_train[[   applicantincome   ,
          coapplicantincome   ,   loanamount   ,
          loan_amount_term   ,    credit_history   ]],y_train)
       i received the following error:
       valueerror: found arrays with inconsistent numbers of samples: [383
       384]
       do you know what   s wrong?
       [153]reply
          + [154]syed danish says:
            [155]july 19, 2016 at 10:51 am
            please download the data set again,there was a problem in it
            which has already been corrected.
            [156]reply
     * shan says:
       [157]july 19, 2016 at 5:53 pm
       ####exercise 1
       before scaling accuracy 0.614583333333
       after scaling accuracy 0.635416666667
       #### exercise 2
       before standardization 0.645833333333
       after standardization 0.75
       ####exercise 3
       before standardization accuracy 0.677083333333
       after standardization accuracy 0.71875
       [158]reply
          + [159]syed danish says:
            [160]july 20, 2016 at 3:04 am
            it sounds right. were you able to understand the jump(amount
            of jump also) in the accuracy of the given exercises for
            corresponding learning algorithm?
            [161]reply
     * anit says:
       [162]july 22, 2016 at 9:09 am
       knn.fit(x_train[[   applicantincome   ,
          coapplicantincome   ,   loanamount   ,
          loan_amount_term   ,    credit_history   ]],y_train)
       there seems to be some problem with the code. can you please help
       me in rectifying the error. i am assuming it is python version
       error.
       knn.fit(x_train[[   applicantincome   ,
          coapplicantincome   ,   loanamount   ,
          loan_amount_term   ,    credit_history   ]],y_train)
       traceback (most recent call last):
       file       , line 2, in
          loan_amount_term   ,    credit_history   ]],y_train)
       file
          /home/fractaluser/anaconda3/lib/python3.5/site-packages/sklearn/ne
       ighbors/base.py   , line 792, in fit
       check_classification_targets(y)
       file
          /home/fractaluser/anaconda3/lib/python3.5/site-packages/sklearn/ut
       ils/multiclass.py   , line 170, in check_classification_targets
       y_type = type_of_target(y)
       file
          /home/fractaluser/anaconda3/lib/python3.5/site-packages/sklearn/ut
       ils/multiclass.py   , line 238, in type_of_target
       if is_multilabel(y):
       file
          /home/fractaluser/anaconda3/lib/python3.5/site-packages/sklearn/ut
       ils/multiclass.py   , line 154, in is_multilabel
       labels = np.unique(y)
       file
          /home/fractaluser/anaconda3/lib/python3.5/site-packages/numpy/lib/
       arraysetops.py   , line 198, in unique
       ar.sort()
       typeerror: unorderable types: int() > str()
       [163]reply
          + [164]syed danish says:
            [165]july 22, 2016 at 9:51 pm
            hi anit, i also think that there is a problem with the version
            of python as it is working fine on my machine(python 2.7),
            although after looking at the errors i suggest you to convert
            the outcome feature i.e, y_train to numerics (   y   =1 and
               n   =0). you can use label enoder for the same or use apply
            function.
            let me know if this works out for you.
            [166]reply
               o anit says:
                 [167]july 26, 2016 at 1:31 pm
                 thanks for the prompt reply.
                 i have changed the y and n using label encoder. but when
                 i am checking accuracy it gave me following error. can
                 you please help?
                 accuracy_score(y_test,knn.predict(x_test[[   applicantincom
                 e   ,    coapplicantincome   ,   loanamount   ,    loan_amount_term   ,
                    credit_history   ]]))
                 traceback (most recent call last):
                 file       , line 1, in
                 accuracy_score(y_test,knn.predict(x_test[[   applicantincom
                 e   ,    coapplicantincome   ,   loanamount   ,    loan_amount_term   ,
                    credit_history   ]]))
                 file
                    /home/fractaluser/anaconda3/lib/python3.5/site-packages/
                 sklearn/metrics/classification.py   , line 172, in
                 accuracy_score
                 y_type, y_true, y_pred = _check_targets(y_true, y_pred)
                 file
                    /home/fractaluser/anaconda3/lib/python3.5/site-packages/
                 sklearn/metrics/classification.py   , line 89, in
                 _check_targets
                 raise valueerror(   {0} is not supported   .format(y_type))
                 valueerror: multiclass-multioutput is not supported
                 [168]reply
                    # [169]syed danish says:
                      [170]july 26, 2016 at 7:16 pm
                      did you label encoded the    y_test   ?
                      [171]reply
                         @ anit says:
                           [172]july 27, 2016 at 6:30 am
                           yes i have encoded y_test.
                           below is the code.
                           y_test.status =
                           le_status.fit_transform(y_test.status)
                           y_test is the object and status is the variable
                           name which has y/n as values
                           after encoding it becomes as 1/0
                           [173]reply
                         @ anit says:
                           [174]august 10, 2016 at 1:14 pm
                           hi syed,
                           any comments on this piece of code?
                           how can i resolve this?
                           [175]reply
     * nishant gupta says:
       [176]july 24, 2016 at 12:15 pm
       le.fit(data.values)
       this line is throwing an error . please help
       ar.sort()
       typeerror: unorderable types: str() > float()
       [177]reply
          + [178]syed danish says:
            [179]july 26, 2016 at 7:25 pm
            hi nishant,
            check if you are using only the categorical variables for
            label encoding. if you still face this problem, send me the
            codes that you are using.
            [180]reply
               o nishant gupta says:
                 [181]july 28, 2016 at 5:31 pm
                 thank you very much for help, but i found the reason it
                 was because of n.a values
                 [182]reply
     * [183]nba 2k vc says:
       [184]august 11, 2016 at 8:29 am
       many thanks very practical. will certainly share site with my
       buddies
       [185]reply
     * dai zhongxiang says:
       [186]august 23, 2016 at 4:00 pm
       thanks for the wonderful post. i have a question regarding the
       standardization: i read from the documentation of sklearn that the
       feature preprocessing should be learned from the training set and
       applied on the testing set, but in the post, the standardization
       was done as:
       >>x_train_scale=scale(x_train[[   applicantincome   ,
          coapplicantincome   ,
          loanamount   ,    loan_amount_term   ,    credit_history   ]])
       >> x_test_scale=scale(x_test[[   applicantincome   ,
          coapplicantincome   ,
          loanamount   ,    loan_amount_term   ,    credit_history   ]])
       according to my understanding, shouldn   t it be something like:
       >>scaler = standardscaler.fit(x_train[[   applicantincome   ,
          coapplicantincome   ,
          loanamount   ,    loan_amount_term   ,    credit_history   ]])
       >>x_train_scale = scaler.transform(x_train[[   applicantincome   ,
          coapplicantincome   ,
          loanamount   ,    loan_amount_term   ,    credit_history   ]])
       >> x_test_scale = scaler.transform(x_test[[   applicantincome   ,
          coapplicantincome   ,
          loanamount   ,    loan_amount_term   ,    credit_history   ]])
       please point out if my understanding is incorrect. thanks a lot.
       [187]reply
          + [188]syed danish says:
            [189]august 24, 2016 at 9:50 am
            hi dai zhongxiang,
            standardscaler uses the transformer api to compute the mean
            and standard deviation on a the data so as to be able to later
            reapply the transformation where as scale method provides
            quick way to perform the same operation.
            p.s. you can use both methods to perform the scaling
            operation.
            [190]reply
     * jack says:
       [191]august 24, 2016 at 4:09 pm
       thank you. very detailed and impressive article for beginners.
       how do we know which model use which standardization methods? for
       example, you use minmaxscaler for knn and z-score for logistic
       regression, how about the other models? is there any cheat sheet
       has the summary about this? thank you again.
       [192]reply
          + [193]syed danish says:
            [194]september 26, 2016 at 6:21 am
            hi jack, thank you for your support. using particular methods
            for a model mostly depends on observation but after learning
            the basics of algorithms and preprocessing methods you can get
            some idea about the type of method to use for a particular
            algorithm, as i mentioned in the article    
               in id28, each feature is assigned a weight or
            coefficient (wi). if there is a feature with relatively large
            range and it is insignificant in the objective function then
            id28 will itself assign a very low value to its
            co-efficient, thus neutralizing the dominant effect of that
            particular feature, whereas distance based method such as knn
            does not have this inbuilt strategy, thus it requires
            scaling.   
            and also this section    
                this means standardizing the data when using a estimator
            having l1 or l2 id173 helps us to increase the
            accuracy of the prediction model. other learners like knn with
            euclidean distance measure, id116, id166, id88, neural
            networks, id156, principal component
            analysis may perform better with standardized data.   
            lastly it all come down to experience and deeper
            understanding. hope this will help you understand the concepts
            more clearly.
            [195]reply
     * clare says:
       [196]september 26, 2016 at 6:20 pm
       thank you for the post! i found it very helpful (i am new to
       python). i have one question about the id28 in
       exercise one. we are using the sklearn.linear_model for logistic
       regression. in this case, the decision boundary is a straight line.
       is there a way to add some non-linearity the decision boundary?
       [197]reply
     * rishu says:
       [198]february 3, 2017 at 5:28 am
       sir, how to assign weights to certain attributes in dataset as
       these attributes is more prominent in predicting
       [199]reply
     * vivek says:
       [200]february 10, 2017 at 6:49 am
       very userful article
       [201]reply
     * john constantine says:
       [202]july 6, 2017 at 6:18 pm
       for k-neares neightbour implementation, why is there
       >> knn=kneighborsclassifier(n_neighbors=5)
       shouldn   t n_neighbors=2 as there are two possible outcomes ?
       [203]reply
     * deepika says:
       [204]july 19, 2017 at 11:28 am
       hi   
       how the test and train files are extracted?
       [205]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [206]srk       3924
   2    [2.jpg?date=2019-04-05] [207]mark12    3510
   3    [3.jpg?date=2019-04-05] [208]nilabha   3261
   4    [4.jpg?date=2019-04-05] [209]nitish007 3237
   5    [5.jpg?date=2019-04-05] [210]tezdhar   3082
   [211]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [212]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [213]understanding support vector machine algorithm from examples
       (along with code)
     * [214]essentials of machine learning algorithms (with python and r
       codes)
     * [215]a complete tutorial to learn data science with python from
       scratch
     * [216]7 types of regression techniques you should know!
     * [217]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [218]a simple introduction to anova (with applications in excel)
     * [219]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [220]top 5 machine learning github repositories and reddit discussions
   from march 2019

[221]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [222]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[223]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [224]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[225]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [226]16 opencv functions to start your id161 journey (with
   python code)

[227]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [228][ds-finhack.jpg]

   [229][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [230]about us
     * [231]our team
     * [232]career
     * [233]contact us
     * [234]write for us

   [235]about us
   [236]   
   [237]our team
   [238]   
   [239]careers
   [240]   
   [241]contact us

data scientists

     * [242]blog
     * [243]hackathon
     * [244]discussions
     * [245]apply jobs
     * [246]leaderboard

companies

     * [247]post jobs
     * [248]trainings
     * [249]hiring hackathons
     * [250]advertising
     * [251]reach us

   don't have an account? [252]sign up here.

join our community :

   [253]46336 [254]followers
   [255]20224 [256]followers
   [257]followers
   [258]7513 [259]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [260]privacy policy
     * [261]terms of use
     * [262]refund policy

   don't have an account? [263]sign up here

   iframe: [264]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [265](button) join now

   subscribe!

   iframe: [266]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [267](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/business-analytics/
  94. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/
  95. https://www.analyticsvidhya.com/blog/category/business-analytics/
  96. https://www.analyticsvidhya.com/blog/category/machine-learning/
  97. https://www.analyticsvidhya.com/blog/category/python-2/
  98. https://www.analyticsvidhya.com/blog/author/syed4194/
  99. http://scikit-learn.org/stable/modules/preprocessing.html
 100. http://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii
 101. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/loan_prediction-1.zip
 102. https://www.analyticsvidhya.com/wp-content/uploads/2016/06/index-4.png
 103. https://www.analyticsvidhya.com/blog/2014/10/introduction-k-neighbours-algorithm-id91/
 104. https://www.analyticsvidhya.com/wp-content/uploads/2016/06/screenshot-from-2016-06-29-14-16-29.png
 105. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 106. https://www.analyticsvidhya.com/wp-content/uploads/2016/06/screenshot-from-2016-06-29-14-06-42.png
 107. https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-python-r/
 108. https://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/
 109. https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/
 110. http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.pipeline.html
 111. http://datahack.analyticsvidhya.com/contest/all
 112. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 113. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/?share=linkedin
 114. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/?share=facebook
 115. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/?share=twitter
 116. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/?share=pocket
 117. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/?share=reddit
 118. https://www.analyticsvidhya.com/blog/tag/categorical-variable/
 119. https://www.analyticsvidhya.com/blog/tag/continuous-variable/
 120. https://www.analyticsvidhya.com/blog/tag/data-exploration/
 121. https://www.analyticsvidhya.com/blog/tag/data-mining/
 122. https://www.analyticsvidhya.com/blog/tag/data-preprocessing/
 123. https://www.analyticsvidhya.com/blog/tag/decision-trees/
 124. https://www.analyticsvidhya.com/blog/tag/dummy-encoding/
 125. https://www.analyticsvidhya.com/blog/tag/label-encoding/
 126. https://www.analyticsvidhya.com/blog/tag/logistic-regression/
 127. https://www.analyticsvidhya.com/blog/tag/machine-learning/
 128. https://www.analyticsvidhya.com/blog/tag/missing-value/
 129. https://www.analyticsvidhya.com/blog/tag/id172/
 130. https://www.analyticsvidhya.com/blog/tag/python/
 131. https://www.analyticsvidhya.com/blog/tag/scaling/
 132. https://www.analyticsvidhya.com/blog/tag/scikit-learn/
 133. https://www.analyticsvidhya.com/blog/tag/standardization/
 134. https://www.analyticsvidhya.com/blog/author/syed4194/
 135. https://discuss.analyticsvidhya.com/
 136. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113593
 137. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113593
 138. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113599
 139. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113599
 140. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113602
 141. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113602
 142. http://www.syeddanish.com/
 143. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113615
 144. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113615
 145. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113626
 146. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113626
 147. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113619
 148. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113619
 149. http://www.syeddanish.com/
 150. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113620
 151. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113620
 152. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113623
 153. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113623
 154. http://www.syeddanish.com/
 155. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113670
 156. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113670
 157. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113687
 158. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113687
 159. http://www.syeddanish.com/
 160. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113705
 161. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113705
 162. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113818
 163. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113818
 164. http://www.syeddanish.com/
 165. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113851
 166. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113851
 167. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114056
 168. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114056
 169. http://www.syeddanish.com/
 170. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114074
 171. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114074
 172. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114110
 173. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114110
 174. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114641
 175. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114641
 176. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113919
 177. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-113919
 178. http://www.syeddanish.com/
 179. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114076
 180. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114076
 181. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114208
 182. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114208
 183. http://foxyfeetdating.com/blogs/post/10902
 184. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114672
 185. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-114672
 186. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-115058
 187. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-115058
 188. http://www.syeddanish.com/
 189. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-115085
 190. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-115085
 191. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-115099
 192. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-115099
 193. http://www.syeddanish.com/
 194. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-116482
 195. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-116482
 196. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-116506
 197. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-116506
 198. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-122095
 199. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-122095
 200. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-122458
 201. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-122458
 202. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-131631
 203. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-131631
 204. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-132369
 205. https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#comment-132369
 206. https://datahack.analyticsvidhya.com/user/profile/srk
 207. https://datahack.analyticsvidhya.com/user/profile/mark12
 208. https://datahack.analyticsvidhya.com/user/profile/nilabha
 209. https://datahack.analyticsvidhya.com/user/profile/nitish007
 210. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 211. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 212. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 213. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 214. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 215. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 216. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 217. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 218. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 219. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 220. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 221. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 222. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 223. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 224. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 225. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 226. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 227. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 228. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 229. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 230. http://www.analyticsvidhya.com/about-me/
 231. https://www.analyticsvidhya.com/about-me/team/
 232. https://www.analyticsvidhya.com/career-analytics-vidhya/
 233. https://www.analyticsvidhya.com/contact/
 234. https://www.analyticsvidhya.com/about-me/write/
 235. http://www.analyticsvidhya.com/about-me/
 236. https://www.analyticsvidhya.com/about-me/team/
 237. https://www.analyticsvidhya.com/about-me/team/
 238. https://www.analyticsvidhya.com/about-me/team/
 239. https://www.analyticsvidhya.com/career-analytics-vidhya/
 240. https://www.analyticsvidhya.com/about-me/team/
 241. https://www.analyticsvidhya.com/contact/
 242. https://www.analyticsvidhya.com/blog
 243. https://datahack.analyticsvidhya.com/
 244. https://discuss.analyticsvidhya.com/
 245. https://www.analyticsvidhya.com/jobs/
 246. https://datahack.analyticsvidhya.com/users/
 247. https://www.analyticsvidhya.com/corporate/
 248. https://trainings.analyticsvidhya.com/
 249. https://datahack.analyticsvidhya.com/
 250. https://www.analyticsvidhya.com/contact/
 251. https://www.analyticsvidhya.com/contact/
 252. https://datahack.analyticsvidhya.com/signup/
 253. https://www.facebook.com/analyticsvidhya/
 254. https://www.facebook.com/analyticsvidhya/
 255. https://twitter.com/analyticsvidhya
 256. https://twitter.com/analyticsvidhya
 257. https://plus.google.com/+analyticsvidhya
 258. https://in.linkedin.com/company/analytics-vidhya
 259. https://in.linkedin.com/company/analytics-vidhya
 260. https://www.analyticsvidhya.com/privacy-policy/
 261. https://www.analyticsvidhya.com/terms/
 262. https://www.analyticsvidhya.com/refund-policy/
 263. https://id.analyticsvidhya.com/accounts/signup/
 264. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 265. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 266. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 267. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 269. https://www.facebook.com/analyticsvidhya
 270. https://twitter.com/analyticsvidhya
 271. https://plus.google.com/+analyticsvidhya/posts
 272. https://in.linkedin.com/company/analytics-vidhya
 273. https://www.analyticsvidhya.com/blog/2016/07/20-challenging-job-interview-puzzles-which-every-analyst-solve-atleast/
 274. https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/
 275. https://www.analyticsvidhya.com/blog/author/syed4194/
 276. http://www.syeddanish.com/
 277. https://twitter.com/syeddanish_4
 278. https://in.linkedin.com/in/syed-danish-332952b2
 279. https://github.com/syeddanish41
 280. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 281. https://www.facebook.com/analyticsvidhya/
 282. https://twitter.com/analyticsvidhya
 283. https://plus.google.com/+analyticsvidhya
 284. https://plus.google.com/+analyticsvidhya
 285. https://in.linkedin.com/company/analytics-vidhya
 286. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 287. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 288. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 289. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 290. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 291. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 292. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 293. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 294. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 295. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 296. javascript:void(0);
 297. javascript:void(0);
 298. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 299. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 300. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 301. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 302. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 303. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 304. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 305. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 306. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 307. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f07%2fpractical-guide-data-preprocessing-python-scikit-learn%2f&linkname=practical%20guide%20on%20data%20preprocessing%20in%20python%20using%20scikit%20learn
 308. javascript:void(0);
 309. javascript:void(0);
