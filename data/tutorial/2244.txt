   iframe: [1]https://www.googletagmanager.com/ns.html?id=gtm-mp366cc

gensim logo

   [2]gensim
   gensim tagline

get expert help from the gensim authors

       [3]consulting in machine learning & nlp

       commercial document similarity engine: [4]scaletext.ai

       [5]corporate trainings in python data science and deep learning
     * [6]home
     * [7]tutorials
     * [8]install
     * [9]support
     * [10]api
     * [11]about

   models.id97     id97 embeddings

models.id97     id97 embeddings[12]  

   this module implements the id97 family of algorithms, using highly
   optimized c routines, data streaming and pythonic interfaces.

   the id97 algorithms include skip-gram and cbow models, using either
   hierarchical softmax or negative sampling: [13]tomas mikolov et al:
   efficient estimation of word representations in vector space, [14]tomas
   mikolov et al: distributed representations of words and phrases and
   their compositionality.

other embeddings[15]  

   there are more ways to train word vectors in gensim than just id97.
   see also [16]doc2vec, [17]fasttext and wrappers for varembed and
   wordrank.

   the training algorithms were originally ported from the c package
   [18]https://code.google.com/p/id97/ and extended with additional
   functionality and optimizations over the years.

   for a tutorial on gensim id97, with an interactive web app trained
   on googlenews, visit
   [19]https://rare-technologies.com/id97-tutorial/.

   make sure you have a c compiler before installing gensim, to use the
   optimized id97 routines (70x speedup compared to plain numpy
   implementation,
   [20]https://rare-technologies.com/parallelizing-id97-in-python/).

usage examples[21]  

   initialize a model with e.g.:
>>> from gensim.test.utils import common_texts, get_tmpfile
>>> from gensim.models import id97
>>>
>>> path = get_tmpfile("id97.model")
>>>
>>> model = id97(common_texts, size=100, window=5, min_count=1, workers=4)
>>> model.save("id97.model")

   the training is streamed, meaning sentences can be a generator, reading
   input data from disk on-the-fly, without loading the entire corpus into
   ram.

   it also means you can continue training the model later:
>>> model = id97.load("id97.model")
>>> model.train([["hello", "world"]], total_examples=1, epochs=1)
(0, 2)

   the trained word vectors are stored in a [22]keyedvectors instance in
   model.wv:
>>> vector = model.wv['computer']  # numpy vector of a word

   the reason for separating the trained vectors into keyedvectors is that
   if you don   t need the full model state any more (don   t need to continue
   training), the state can discarded, resulting in a much smaller and
   faster object that can be mmapped for lightning fast loading and
   sharing the vectors in ram between processes:
>>> from gensim.models import keyedvectors
>>>
>>> path = get_tmpfile("wordvectors.kv")
>>>
>>> model.wv.save(path)
>>> wv = keyedvectors.load("model.wv", mmap='r')
>>> vector = wv['computer']  # numpy vector of a word

   gensim can also load word vectors in the    id97 c format   , as a
   [23]keyedvectors instance:
>>> from gensim.test.utils import datapath
>>>
>>> wv_from_text = keyedvectors.load_id97_format(datapath('id97_pre_kv_c
'), binary=false)  # c text format
>>> wv_from_bin = keyedvectors.load_id97_format(datapath("euclidean_vectors.
bin"), binary=true)  # c bin format

   it is impossible to continue training the vectors loaded from the c
   format because the hidden weights, vocabulary frequencies and the
   binary tree are missing. to continue training, you   ll need the full
   [24]id97 object state, as stored by [25]save(), not just the
   [26]keyedvectors.

   you can perform various nlp word tasks with a trained model. some of
   them are already built-in - you can see it in
   [27]gensim.models.keyedvectors.

   if you   re finished training a model (i.e. no more updates, only
   querying), you can switch to the [28]keyedvectors instance:
>>> word_vectors = model.wv
>>> del model

   to trim unneeded model state = use much less ram and allow fast loading
   and memory sharing (mmap).

   note that there is a [29]gensim.models.phrases module which lets you
   automatically detect phrases longer than one word. using phrases, you
   can learn a id97 model where    words    are actually multiword
   expressions, such as new_york_times or financial_crisis:
>>> from gensim.test.utils import common_texts
>>> from gensim.models import phrases
>>>
>>> bigram_transformer = phrases(common_texts)
>>> model = id97(bigram_transformer[common_texts], min_count=1)

   class gensim.models.id97.browncorpus(dirname)[30]  
          bases: object

          iterate over sentences from the [31]brown corpus (part of
          [32]nltk data).

   class gensim.models.id97.linesentence(source,
          max_sentence_length=10000, limit=none)[33]  
          bases: object

          iterate over a file that contains sentences: one line = one
          sentence. words must be already preprocessed and separated by
          whitespace.

   parameters:
          + source (string or a file-like object)     path to the file on
            disk, or an already-open file object (must support seek(0)).
          + limit (int or none)     clip the file to the first limit lines.
            do no clipping if limit is none (the default).

          examples

>>> from gensim.test.utils import datapath
>>> sentences = linesentence(datapath('lee_background.cor'))
>>> for sentence in sentences:
...     pass

   class gensim.models.id97.pathlinesentences(source,
          max_sentence_length=10000, limit=none)[34]  
          bases: object

          like [35]linesentence, but process all files in a directory in
          alphabetical order by filename.

          the directory must only contain files that can be read by
          [36]gensim.models.id97.linesentence: .bz2, .gz, and text
          files. any file not ending with .bz2 or .gz is assumed to be a
          text file.

          the format of files (either text, or compressed text files) in
          the path is one sentence = one line, with words already
          preprocessed and separated by whitespace.

          warning

          does not recurse into subdirectories.

   parameters:
          + source (str)     path to the directory.
          + limit (int or none)     read only the first limit lines from
            each file. read all if limit is none (the default).

   class gensim.models.id97.text8corpus(fname,
          max_sentence_length=10000)[37]  
          bases: object

          iterate over sentences from the    text8    corpus, unzipped from
          [38]http://mattmahoney.net/dc/text8.zip.

   class gensim.models.id97.id97(sentences=none, corpus_file=none,
          size=100, alpha=0.025, window=5, min_count=5,
          max_vocab_size=none, sample=0.001, seed=1, workers=3,
          min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75,
          cbow_mean=1, hashfxn=<built-in function hash>, iter=5,
          null_word=0, trim_rule=none, sorted_vocab=1, batch_words=10000,
          compute_loss=false, callbacks=(), max_final_vocab=none)[39]  
          bases: [40]gensim.models.base_any2vec.basewordembeddingsmodel

          train, use and evaluate neural networks described in
          [41]https://code.google.com/p/id97/.

          once you   re finished training a model (=no more updates, only
          querying) store and use only the [42]keyedvectors instance in
          self.wv to reduce memory.

          the model can be stored/loaded via its [43]save() and [44]load()
          methods.

          the trained word vectors can also be stored/loaded from a format
          compatible with the original id97 implementation via
          self.wv.save_id97_format and
          gensim.models.keyedvectors.keyedvectors.load_id97_format().

          some important attributes are the following:

        wv[45]  
                this object essentially contains the mapping between words
                and embeddings. after training, it can be used directly to
                query those embeddings in various ways. see the module
                level docstring for examples.

                type: [46]id97keyedvectors

        vocabulary[47]  
                this object represents the vocabulary (sometimes called
                dictionary in gensim) of the model. besides keeping track
                of all unique words, this object provides extra
                functionality, such as constructing a huffman tree
                (frequent words are closer to the root), or discarding
                extremely rare words.

                type: [48]id97vocab

        trainables[49]  
                this object represents the inner shallow neural network
                used to train the embeddings. the semantics of the network
                differ slightly in the two available training modes (cbow
                or sg) but you can think of it as a nn with a single
                projection and hidden layer which we train on the corpus.
                the weights are then used as our embeddings (which means
                that the size of the hidden layer is equal to the number
                of features self.size).

                type: [50]id97trainables

   parameters:
          + sentences (iterable of iterables, optional)     the sentences
            iterable can be simply a list of lists of tokens, but for
            larger corpora, consider an iterable that streams the
            sentences directly from disk/network. see [51]browncorpus,
            [52]text8corpus or [53]linesentence in [54]id97 module for
            such examples. see also the [55]tutorial on data streaming in
            python. if you don   t supply sentences, the model is left
            uninitialized     use if you plan to initialize it in some other
            way.
          + corpus_file (str, optional)     path to a corpus file in
            [56]linesentence format. you may use this argument instead of
            sentences to get performance boost. only one of sentences or
            corpus_file arguments need to be passed (or none of them, in
            that case, the model is left uninitialized).
          + size (int, optional)     dimensionality of the word vectors.
          + window (int, optional)     maximum distance between the current
            and predicted word within a sentence.
          + min_count (int, optional)     ignores all words with total
            frequency lower than this.
          + workers (int, optional)     use these many worker threads to
            train the model (=faster training with multicore machines).
          + sg ({0, 1}, optional)     training algorithm: 1 for skip-gram;
            otherwise cbow.
          + hs ({0, 1}, optional)     if 1, hierarchical softmax will be
            used for model training. if 0, and negative is non-zero,
            negative sampling will be used.
          + negative (int, optional)     if > 0, negative sampling will be
            used, the int for negative specifies how many    noise words   
            should be drawn (usually between 5-20). if set to 0, no
            negative sampling is used.
          + ns_exponent (float, optional)     the exponent used to shape the
            negative sampling distribution. a value of 1.0 samples exactly
            in proportion to the frequencies, 0.0 samples all words
            equally, while a negative value samples low-frequency words
            more than high-frequency words. the popular default value of
            0.75 was chosen by the original id97 paper. more recently,
            in [57]https://arxiv.org/abs/1804.04212, caselles-dupr  ,
            lesaint, & royo-letelier suggest that other values may perform
            better for recommendation applications.
          + cbow_mean ({0, 1}, optional)     if 0, use the sum of the
            context word vectors. if 1, use the mean, only applies when
            cbow is used.
          + alpha (float, optional)     the initial learning rate.
          + min_alpha (float, optional)     learning rate will linearly drop
            to min_alpha as training progresses.
          + seed (int, optional)     seed for the random number generator.
            initial vectors for each word are seeded with a hash of the
            concatenation of word + str(seed). note that for a fully
            deterministically-reproducible run, you must also limit the
            model to a single worker thread (workers=1), to eliminate
            ordering jitter from os thread scheduling. (in python 3,
            reproducibility between interpreter launches also requires use
            of the pythonhashseed environment variable to control hash
            randomization).
          + max_vocab_size (int, optional)     limits the ram during
            vocabulary building; if there are more unique words than this,
            then prune the infrequent ones. every 10 million word types
            need about 1gb of ram. set to none for no limit.
          + max_final_vocab (int, optional)     limits the vocab to a target
            vocab size by automatically picking a matching min_count. if
            the specified min_count is more than the calculated min_count,
            the specified min_count will be used. set to none if not
            required.
          + sample (float, optional)     the threshold for configuring which
            higher-frequency words are randomly downsampled, useful range
            is (0, 1e-5).
          + hashfxn (function, optional)     hash function to use to
            randomly initialize weights, for increased training
            reproducibility.
          + iter (int, optional)     number of iterations (epochs) over the
            corpus.
          + trim_rule (function, optional)    
            vocabulary trimming rule, specifies whether certain words
            should remain in the vocabulary, be trimmed away, or handled
            using the default (discard if word count < min_count). can be
            none (min_count will be used, look to [58]keep_vocab_item()),
            or a callable that accepts parameters (word, count, min_count)
            and returns either gensim.utils.rule_discard,
            gensim.utils.rule_keep or gensim.utils.rule_default. the rule,
            if given, is only used to prune vocabulary during
            build_vocab() and is not stored as part of the model.

              the input parameters are of the following types:

                    # word (str) - the word we are examining
                    # count (int) - the word   s frequency count in the
                      corpus
                    # min_count (int) - the minimum count threshold.

          + sorted_vocab ({0, 1}, optional)     if 1, sort the vocabulary by
            descending frequency before assigning word indexes. see
            [59]sort_vocab().
          + batch_words (int, optional)     target size (in words) for
            batches of examples passed to worker threads (and thus cython
            routines).(larger batches will be passed if individual texts
            are longer than 10000 words, but the standard cython code
            truncates to that maximum.)
          + compute_loss (bool, optional)     if true, computes and stores
            loss value which can be retrieved using
            [60]get_latest_training_loss().
          + callbacks (iterable of [61]callbackany2vec, optional)    
            sequence of callbacks to be executed at specific stages during
            training.

          examples

          initialize and train a [62]id97 model

>>> from gensim.models import id97
>>> sentences = [["cat", "say", "meow"], ["dog", "say", "woof"]]
>>> model = id97(sentences, min_count=1)

        accuracy(**kwargs)[63]  
                deprecated. use self.wv.accuracy instead. see accuracy().

        build_vocab(sentences=none, corpus_file=none, update=false,
                progress_per=10000, keep_raw_vocab=false, trim_rule=none,
                **kwargs)[64]  
                build vocabulary from a sequence of sentences (can be a
                once-only generator stream).

   parameters:
               o sentences (iterable of list of str)     can be simply a
                 list of lists of tokens, but for larger corpora, consider
                 an iterable that streams the sentences directly from
                 disk/network. see [65]browncorpus, [66]text8corpus or
                 [67]linesentence module for such examples.
               o corpus_file (str, optional)     path to a corpus file in
                 [68]linesentence format. you may use this argument
                 instead of sentences to get performance boost. only one
                 of sentences or corpus_file arguments need to be passed
                 (not both of them).
               o update (bool)     if true, the new words in sentences will
                 be added to model   s vocab.
               o progress_per (int, optional)     indicates how many words
                 to process before showing/updating the progress.
               o keep_raw_vocab (bool, optional)     if false, the raw
                 vocabulary will be deleted after the scaling is done to
                 free up ram.
               o trim_rule (function, optional)    
                 vocabulary trimming rule, specifies whether certain words
                 should remain in the vocabulary, be trimmed away, or
                 handled using the default (discard if word count <
                 min_count). can be none (min_count will be used, look to
                 [69]keep_vocab_item()), or a callable that accepts
                 parameters (word, count, min_count) and returns either
                 gensim.utils.rule_discard, gensim.utils.rule_keep or
                 gensim.utils.rule_default. the rule, if given, is only
                 used to prune vocabulary during current method call and
                 is not stored as part of the model.

                    the input parameters are of the following types:

                         @ word (str) - the word we are examining
                         @ count (int) - the word   s frequency count in the
                           corpus
                         @ min_count (int) - the minimum count threshold.

               o **kwargs (object)     key word arguments propagated to
                 self.vocabulary.prepare_vocab

        build_vocab_from_freq(word_freq, keep_raw_vocab=false,
                corpus_count=none, trim_rule=none, update=false)[70]  
                build vocabulary from a dictionary of word frequencies.

   parameters:
               o word_freq (dict of (str, int))     a mapping from a word in
                 the vocabulary to its frequency count.
               o keep_raw_vocab (bool, optional)     if false, delete the
                 raw vocabulary after the scaling is done to free up ram.
               o corpus_count (int, optional)     even if no corpus is
                 provided, this argument can set corpus_count explicitly.
               o trim_rule (function, optional)    
                 vocabulary trimming rule, specifies whether certain words
                 should remain in the vocabulary, be trimmed away, or
                 handled using the default (discard if word count <
                 min_count). can be none (min_count will be used, look to
                 [71]keep_vocab_item()), or a callable that accepts
                 parameters (word, count, min_count) and returns either
                 gensim.utils.rule_discard, gensim.utils.rule_keep or
                 gensim.utils.rule_default. the rule, if given, is only
                 used to prune vocabulary during current method call and
                 is not stored as part of the model.

                    the input parameters are of the following types:

                         @ word (str) - the word we are examining
                         @ count (int) - the word   s frequency count in the
                           corpus
                         @ min_count (int) - the minimum count threshold.

               o update (bool, optional)     if true, the new provided words
                 in word_freq dict will be added to model   s vocab.

        clear_sims()[72]  
                remove all l2-normalized word vectors from the model, to
                free up memory.

                you can recompute them later again using the
                [73]init_sims() method.

        cum_table[74]  

        delete_temporary_training_data(replace_word_vectors_with_normalize
                d=false)[75]  
                discard parameters that are used in training and scoring,
                to save memory.

                warning

                use only if you   re sure you   re done training a model.

   parameters: replace_word_vectors_with_normalized (bool, optional)     if
   true, forget the original (not normalized) word vectors and only keep
   the l2-normalized word vectors, to save even more memory.

        doesnt_match(**kwargs)[76]  
                deprecated, use self.wv.doesnt_match() instead.

                refer to the documentation for [77]doesnt_match().

        estimate_memory(vocab_size=none, report=none)[78]  
                estimate required memory for a model using current
                settings and provided vocabulary size.

   parameters:
               o vocab_size (int, optional)     number of unique tokens in
                 the vocabulary
               o report (dict of (str, int), optional)     a dictionary from
                 string representations of the model   s memory consuming
                 members to their size in bytes.

                  returns:

   a dictionary from string representations of the model   s memory
   consuming members to their size in bytes.
                return type:

   dict of (str, int)

        evaluate_word_pairs(**kwargs)[79]  
                deprecated, use self.wv.evaluate_word_pairs() instead.

                refer to the documentation for [80]evaluate_word_pairs().

        get_latest_training_loss()[81]  
                get current value of the training loss.

                  returns:   current training loss.
                return type: float

        hashfxn[82]  

        init_sims(replace=false)[83]  
                deprecated. use self.wv.init_sims instead. see
                [84]init_sims().

        intersect_id97_format(fname, lockf=0.0, binary=false,
                encoding='utf8', unicode_errors='strict')[85]  
                merge in an input-hidden weight matrix loaded from the
                original c id97-tool format, where it intersects with
                the current vocabulary.

                no words are added to the existing vocabulary, but
                intersecting words adopt the file   s weights, and
                non-intersecting words are left alone.

   parameters:
               o fname (str)     the file path to load the vectors from.
               o lockf (float, optional)     lock-factor value to be set for
                 any imported word-vectors; the default value of 0.0
                 prevents further updating of the vector during subsequent
                 training. use 1.0 to allow further training updates of
                 merged vectors.
               o binary (bool, optional)     if true, fname is in the binary
                 id97 c format.
               o encoding (str, optional)     encoding of text for unicode
                 function (python2 only).
               o unicode_errors (str, optional)     error handling
                 behaviour, used as parameter for unicode function
                 (python2 only).

        iter[86]  

        layer1_size[87]  

        classmethod load(*args, **kwargs)[88]  
                load a previously saved [89]id97 model.

                see also

              [90]save()
                      save model.

                parameters:  fname (str)     path to the saved file.
                  returns:   loaded model.
                return type: [91]id97

        classmethod load_id97_format(fname, fvocab=none, binary=false,
                encoding='utf8', unicode_errors='strict', limit=none,
                datatype=<type 'numpy.float32'>)[92]  
                deprecated. use
                gensim.models.keyedvectors.load_id97_format() instead.

        static log_accuracy(section)[93]  
                deprecated. use self.wv.log_accuracy instead. see
                log_accuracy().

        min_count[94]  

        most_similar(**kwargs)[95]  
                deprecated, use self.wv.most_similar() instead.

                refer to the documentation for [96]most_similar().

        most_similar_cosmul(**kwargs)[97]  
                deprecated, use self.wv.most_similar_cosmul() instead.

                refer to the documentation for [98]most_similar_cosmul().

        n_similarity(**kwargs)[99]  
                deprecated, use self.wv.n_similarity() instead.

                refer to the documentation for [100]n_similarity().

        predict_output_word(context_words_list, topn=10)[101]  
                get the id203 distribution of the center word given
                context words.

   parameters:
               o context_words_list (list of str)     list of context words.
               o topn (int, optional)     return topn words and their
                 probabilities.

                  returns:

   topn length list of tuples of (word, id203).
                return type:

   list of (str, float)

        reset_from(other_model)[102]  
                borrow shareable pre-built structures from other_model and
                reset hidden layer weights.

              structures copied are:

                    # vocabulary
                    # index to word mapping
                    # cumulative frequency table (used for negative
                      sampling)
                    # cached corpus length

                useful when testing multiple models on the same corpus in
                parallel.

   parameters: other_model ([103]id97)     another model to copy the
   internal structures from.

        sample[104]  

        save(*args, **kwargs)[105]  
                save the model. this saved model can be loaded again using
                [106]load(), which supports online training and getting
                vectors for vocabulary words.

                parameters: fname (str)     path to the file.

        save_id97_format(fname, fvocab=none, binary=false)[107]  
                deprecated. use model.wv.save_id97_format instead. see
                gensim.models.keyedvectors.save_id97_format().

        score(sentences, total_sentences=1000000, chunksize=100,
                queue_factor=2, report_delay=1)[108]  
                score the log id203 for a sequence of sentences.
                this does not change the fitted model in any way (see
                [109]train() for that).

                gensim has currently only implemented score for the
                hierarchical softmax scheme, so you need to have run
                id97 with hs=1 and negative=0 for this to work.

                note that you should specify total_sentences; you   ll run
                into problems if you ask to score more than this number of
                sentences but it is inefficient to set the value too high.

                see the [110]article by matt taddy:    document
                classification by inversion of distributed language
                representations    and the [111]gensim demo for examples of
                how to use such scores in document classification.

   parameters:
               o sentences (iterable of list of str)     the sentences
                 iterable can be simply a list of lists of tokens, but for
                 larger corpora, consider an iterable that streams the
                 sentences directly from disk/network. see
                 [112]browncorpus, [113]text8corpus or [114]linesentence
                 in [115]id97 module for such examples.
               o total_sentences (int, optional)     count of sentences.
               o chunksize (int, optional)     chunksize of jobs
               o queue_factor (int, optional)     multiplier for size of
                 queue (number of workers * queue_factor).
               o report_delay (float, optional)     seconds to wait before
                 reporting progress.

        similar_by_vector(**kwargs)[116]  
                deprecated, use self.wv.similar_by_vector() instead.

                refer to the documentation for [117]similar_by_vector().

        similar_by_word(**kwargs)[118]  
                deprecated, use self.wv.similar_by_word() instead.

                refer to the documentation for [119]similar_by_word().

        similarity(**kwargs)[120]  
                deprecated, use self.wv.similarity() instead.

                refer to the documentation for [121]similarity().

        syn0_lockf[122]  

        syn1[123]  

        syn1neg[124]  

        train(sentences=none, corpus_file=none, total_examples=none,
                total_words=none, epochs=none, start_alpha=none,
                end_alpha=none, word_count=0, queue_factor=2,
                report_delay=1.0, compute_loss=false, callbacks=())[125]  
                update the model   s neural weights from a sequence of
                sentences.

                notes

                to support linear learning-rate decay from (initial) alpha
                to min_alpha, and accurate progress-percentage logging,
                either total_examples (count of sentences) or total_words
                (count of raw words in sentences) must be provided. if
                sentences is the same corpus that was provided to
                [126]build_vocab() earlier, you can simply use
                total_examples=self.corpus_count.

                warning

                to avoid common mistakes around the model   s ability to do
                multiple training passes itself, an explicit epochs
                argument must be provided. in the common and recommended
                case where [127]train() is only called once, you can set
                epochs=self.iter.

   parameters:
               o sentences (iterable of list of str)    
                 the sentences iterable can be simply a list of lists of
                 tokens, but for larger corpora, consider an iterable that
                 streams the sentences directly from disk/network. see
                 [128]browncorpus, [129]text8corpus or [130]linesentence
                 in [131]id97 module for such examples. see also the
                 [132]tutorial on data streaming in python.
               o corpus_file (str, optional)     path to a corpus file in
                 [133]linesentence format. you may use this argument
                 instead of sentences to get performance boost. only one
                 of sentences or corpus_file arguments need to be passed
                 (not both of them).
               o total_examples (int)     count of sentences.
               o total_words (int)     count of raw words in sentences.
               o epochs (int)     number of iterations (epochs) over the
                 corpus.
               o start_alpha (float, optional)     initial learning rate. if
                 supplied, replaces the starting alpha from the
                 constructor, for this one call to`train()`. use only if
                 making multiple calls to train(), when you want to manage
                 the alpha learning-rate yourself (not recommended).
               o end_alpha (float, optional)     final learning rate. drops
                 linearly from start_alpha. if supplied, this replaces the
                 final min_alpha from the constructor, for this one call
                 to train(). use only if making multiple calls to train(),
                 when you want to manage the alpha learning-rate yourself
                 (not recommended).
               o word_count (int, optional)     count of words already
                 trained. set this to 0 for the usual case of training on
                 all words in sentences.
               o queue_factor (int, optional)     multiplier for size of
                 queue (number of workers * queue_factor).
               o report_delay (float, optional)     seconds to wait before
                 reporting progress.
               o compute_loss (bool, optional)     if true, computes and
                 stores loss value which can be retrieved using
                 [134]get_latest_training_loss().
               o callbacks (iterable of [135]callbackany2vec, optional)    
                 sequence of callbacks to be executed at specific stages
                 during training.

                examples

>>> from gensim.models import id97
>>> sentences = [["cat", "say", "meow"], ["dog", "say", "woof"]]
>>>
>>> model = id97(min_count=1)
>>> model.build_vocab(sentences)  # prepare the model vocabulary
>>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)
  # train word vectors
(1, 30)

        wmdistance(**kwargs)[136]  
                deprecated, use self.wv.wmdistance() instead.

                refer to the documentation for [137]wmdistance().

   class gensim.models.id97.id97trainables(vector_size=100,
          seed=1, hashfxn=<built-in function hash>)[138]  
          bases: [139]gensim.utils.saveload

          represents the inner shallow neural network used to train
          [140]id97.

        classmethod load(fname, mmap=none)[141]  
                load an object previously saved using [142]save() from a
                file.

   parameters:
               o fname (str)     path to file that contains needed object.
               o mmap (str, optional)     memory-map option. if the object
                 was saved with large arrays stored separately, you can
                 load these arrays via mmap (shared memory) using
                 mmap=   r   . if the file being loaded is compressed (either
                    .gz    or    .bz2   ), then `mmap=none must be set.

                see also

              [143]save()
                      save object to file.

   returns: object loaded from fname.
   return type: object
   raises: attributeerror     when called on an object instance instead of
   class (this is a class method).

        prepare_weights(hs, negative, wv, update=false,
                vocabulary=none)[144]  
                build tables and model weights based on final vocabulary
                settings.

        reset_weights(hs, negative, wv)[145]  
                reset all projection weights to an initial (untrained)
                state, but keep the existing vocabulary.

        save(fname_or_handle, separately=none, sep_limit=10485760,
                ignore=frozenset([]), pickle_protocol=2)[146]  
                save the object to a file.

   parameters:
               o fname_or_handle (str or file-like)     path to output file
                 or already opened file-like object. if the object is a
                 file handle, no special array handling will be performed,
                 all attributes will be saved to the same file.
               o separately (list of str or none, optional)    
                 if none, automatically detect large numpy/scipy.sparse
                 arrays in the object being stored, and store them into
                 separate files. this prevent memory errors for large
                 objects, and also allows [147]memory-mapping the large
                 arrays for efficient loading and sharing the large arrays
                 in ram between multiple processes.
                 if list of str: store these attributes into separate
                 files. the automated size check is not performed in this
                 case.
               o sep_limit (int, optional)     don   t store arrays smaller
                 than this separately. in bytes.
               o ignore (frozenset of str, optional)     attributes that
                 shouldn   t be stored at all.
               o pickle_protocol (int, optional)     protocol number for
                 pickle.

                see also

              [148]load()
                      load object from file.

        seeded_vector(seed_string, vector_size)[149]  
                get a random vector (but deterministic by seed_string).

        update_weights(hs, negative, wv)[150]  
                copy all the existing weights, and reset the weights for
                the newly added vocabulary.

   class gensim.models.id97.id97vocab(max_vocab_size=none,
          min_count=5, sample=0.001, sorted_vocab=true, null_word=0,
          max_final_vocab=none, ns_exponent=0.75)[151]  
          bases: [152]gensim.utils.saveload

          vocabulary used by [153]id97.

        add_null_word(wv)[154]  

        create_binary_tree(wv)[155]  
                create a [156]binary huffman tree using stored vocabulary
                word counts. frequent words will have shorter binary
                codes. called internally from build_vocab().

        classmethod load(fname, mmap=none)[157]  
                load an object previously saved using [158]save() from a
                file.

   parameters:
               o fname (str)     path to file that contains needed object.
               o mmap (str, optional)     memory-map option. if the object
                 was saved with large arrays stored separately, you can
                 load these arrays via mmap (shared memory) using
                 mmap=   r   . if the file being loaded is compressed (either
                    .gz    or    .bz2   ), then `mmap=none must be set.

                see also

              [159]save()
                      save object to file.

   returns: object loaded from fname.
   return type: object
   raises: attributeerror     when called on an object instance instead of
   class (this is a class method).

        make_cum_table(wv, domain=2147483647)[160]  
                create a cumulative-distribution table using stored
                vocabulary word counts for drawing random words in the
                negative-sampling training routines.

                to draw a word index, choose a random integer up to the
                maximum value in the table (cum_table[-1]), then finding
                that integer   s sorted insertion point (as if by
                bisect_left or ndarray.searchsorted()). that insertion
                point is the drawn index, coming up in proportion equal to
                the increment at that slot.

                called internally from build_vocab().

        prepare_vocab(hs, negative, wv, update=false,
                keep_raw_vocab=false, trim_rule=none, min_count=none,
                sample=none, dry_run=false)[161]  
                apply vocabulary settings for min_count (discarding
                less-frequent words) and sample (controlling the
                downsampling of more-frequent words).

                calling with dry_run=true will only simulate the provided
                settings and report the size of the retained vocabulary,
                effective corpus length, and estimated memory
                requirements. results are both printed via logging and
                returned as a dict.

                delete the raw vocabulary after the scaling is done to
                free up ram, unless keep_raw_vocab is set.

        save(fname_or_handle, separately=none, sep_limit=10485760,
                ignore=frozenset([]), pickle_protocol=2)[162]  
                save the object to a file.

   parameters:
               o fname_or_handle (str or file-like)     path to output file
                 or already opened file-like object. if the object is a
                 file handle, no special array handling will be performed,
                 all attributes will be saved to the same file.
               o separately (list of str or none, optional)    
                 if none, automatically detect large numpy/scipy.sparse
                 arrays in the object being stored, and store them into
                 separate files. this prevent memory errors for large
                 objects, and also allows [163]memory-mapping the large
                 arrays for efficient loading and sharing the large arrays
                 in ram between multiple processes.
                 if list of str: store these attributes into separate
                 files. the automated size check is not performed in this
                 case.
               o sep_limit (int, optional)     don   t store arrays smaller
                 than this separately. in bytes.
               o ignore (frozenset of str, optional)     attributes that
                 shouldn   t be stored at all.
               o pickle_protocol (int, optional)     protocol number for
                 pickle.

                see also

              [164]load()
                      load object from file.

        scan_vocab(sentences=none, corpus_file=none, progress_per=10000,
                workers=none, trim_rule=none)[165]  

        sort_vocab(wv)[166]  
                sort the vocabulary so the most frequent words have the
                lowest indexes.

   gensim.models.id97.score_cbow_pair(model, word, l1)[167]  
          score the trained cbow model on a pair of words.

   parameters:
          + model ([168]id97)     the trained model.
          + word ([169]vocab)     vocabulary representation of the first
            word.
          + l1 (list of float)     vector representation of the second word.

            returns:

   logarithm of the sum of exponentiations of input words.
          return type:

   float

   gensim.models.id97.score_sg_pair(model, word, word2)[170]  
          score the trained skip-gram model on a pair of words.

   parameters:
          + model ([171]id97)     the trained model.
          + word ([172]vocab)     vocabulary representation of the first
            word.
          + word2 ([173]vocab)     vocabulary representation of the second
            word.

            returns:

   logarithm of the sum of exponentiations of input words.
          return type:

   float

   gensim.models.id97.train_cbow_pair(model, word, input_word_indices,
          l1, alpha, learn_vectors=true, learn_hidden=true,
          compute_loss=false, context_vectors=none, context_locks=none,
          is_ft=false)[174]  
          train the passed model instance on a word and its context, using
          the cbow algorithm.

   parameters:
          + model ([175]id97)     the model to be trained.
          + word (str)     the label (predicted) word.
          + input_word_indices (list of int)     the vocabulary indices of
            the words in the context.
          + l1 (list of float)     vector representation of the label word.
          + alpha (float)     learning rate.
          + learn_vectors (bool, optional)     whether the vectors should be
            updated.
          + learn_hidden (bool, optional)     whether the weights of the
            hidden layer should be updated.
          + compute_loss (bool, optional)     whether or not the training
            loss should be computed.
          + context_vectors (list of list of float, optional)     vector
            representations of the words in the context. if none, these
            will be retrieved from the model.
          + context_locks (list of float, optional)     the lock factors for
            each word in the context.
          + is_ft (bool, optional)     if true, weights will be computed
            using model.wv.syn0_vocab and model.wv.syn0_ngrams instead of
            model.wv.syn0.

            returns:

   error vector to be back-propagated.
          return type:

   numpy.ndarray

   gensim.models.id97.train_sg_pair(model, word, context_index, alpha,
          learn_vectors=true, learn_hidden=true, context_vectors=none,
          context_locks=none, compute_loss=false, is_ft=false)[176]  
          train the passed model instance on a word and its context, using
          the skip-gram algorithm.

   parameters:
          + model ([177]id97)     the model to be trained.
          + word (str)     the label (predicted) word.
          + context_index (list of int)     the vocabulary indices of the
            words in the context.
          + alpha (float)     learning rate.
          + learn_vectors (bool, optional)     whether the vectors should be
            updated.
          + learn_hidden (bool, optional)     whether the weights of the
            hidden layer should be updated.
          + context_vectors (list of list of float, optional)     vector
            representations of the words in the context. if none, these
            will be retrieved from the model.
          + context_locks (list of float, optional)     the lock factors for
            each word in the context.
          + compute_loss (bool, optional)     whether or not the training
            loss should be computed.
          + is_ft (bool, optional)     if true, weights will be computed
            using model.wv.syn0_vocab and model.wv.syn0_ngrams instead of
            model.wv.syn0.

            returns:

   error vector to be back-propagated.
          return type:

   numpy.ndarray

   smaller gensim logo [178]gensim footer image
      copyright 2009-now, [179]radim   eh    ek
   last updated on jan 31, 2019.
     * [180]home
     * |
     * [181]tutorials
     * |
     * [182]install
     * |
     * [183]support
     * |
     * [184]api
     * |
     * [185]about

   [186]tweet @gensim_py
   support:
   [187]stay informed via gensim mailing list:
   ____________________________ subscribe

references

   1. https://www.googletagmanager.com/ns.html?id=gtm-mp366cc
   2. https://radimrehurek.com/gensim/index.html
   3. https://rare-technologies.com/
   4. https://scaletext.com/
   5. https://rare-technologies.com/corporate-training/
   6. https://radimrehurek.com/gensim/index.html
   7. https://radimrehurek.com/gensim/tutorial.html
   8. https://radimrehurek.com/gensim/install.html
   9. https://radimrehurek.com/gensim/support.html
  10. https://radimrehurek.com/gensim/apiref.html
  11. https://radimrehurek.com/gensim/about.html
  12. https://radimrehurek.com/gensim/models/id97.html#module-gensim.models.id97
  13. https://arxiv.org/pdf/1301.3781.pdf
  14. https://arxiv.org/abs/1310.4546
  15. https://radimrehurek.com/gensim/models/id97.html#other-embeddings
  16. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec
  17. https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.fasttext
  18. https://code.google.com/p/id97/
  19. https://rare-technologies.com/id97-tutorial/
  20. https://rare-technologies.com/parallelizing-id97-in-python/
  21. https://radimrehurek.com/gensim/models/id97.html#usage-examples
  22. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.keyedvectors
  23. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.keyedvectors
  24. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
  25. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.save
  26. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.keyedvectors
  27. https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors
  28. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.keyedvectors
  29. https://radimrehurek.com/gensim/models/phrases.html#module-gensim.models.phrases
  30. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.browncorpus
  31. https://en.wikipedia.org/wiki/brown_corpus
  32. https://www.nltk.org/data.html
  33. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
  34. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.pathlinesentences
  35. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
  36. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
  37. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.text8corpus
  38. http://mattmahoney.net/dc/text8.zip
  39. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
  40. https://radimrehurek.com/gensim/models/base_any2vec.html#gensim.models.base_any2vec.basewordembeddingsmodel
  41. https://code.google.com/p/id97/
  42. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.keyedvectors
  43. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.save
  44. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.load
  45. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.wv
  46. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.id97keyedvectors
  47. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.vocabulary
  48. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97vocab
  49. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.trainables
  50. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97trainables
  51. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.browncorpus
  52. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.text8corpus
  53. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
  54. https://radimrehurek.com/gensim/models/id97.html#module-gensim.models.id97
  55. https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/
  56. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
  57. https://arxiv.org/abs/1804.04212
  58. https://radimrehurek.com/gensim/utils.html#gensim.utils.keep_vocab_item
  59. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97vocab.sort_vocab
  60. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.get_latest_training_loss
  61. https://radimrehurek.com/gensim/models/callbacks.html#gensim.models.callbacks.callbackany2vec
  62. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
  63. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.accuracy
  64. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.build_vocab
  65. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.browncorpus
  66. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.text8corpus
  67. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
  68. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
  69. https://radimrehurek.com/gensim/utils.html#gensim.utils.keep_vocab_item
  70. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.build_vocab_from_freq
  71. https://radimrehurek.com/gensim/utils.html#gensim.utils.keep_vocab_item
  72. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.clear_sims
  73. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.init_sims
  74. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.cum_table
  75. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.delete_temporary_training_data
  76. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.doesnt_match
  77. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.doesnt_match
  78. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.estimate_memory
  79. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.evaluate_word_pairs
  80. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.evaluate_word_pairs
  81. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.get_latest_training_loss
  82. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.hashfxn
  83. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.init_sims
  84. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.id97keyedvectors.init_sims
  85. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.intersect_id97_format
  86. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.iter
  87. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.layer1_size
  88. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.load
  89. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
  90. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.save
  91. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
  92. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.load_id97_format
  93. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.log_accuracy
  94. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.min_count
  95. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.most_similar
  96. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.most_similar
  97. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.most_similar_cosmul
  98. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.most_similar_cosmul
  99. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.n_similarity
 100. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.n_similarity
 101. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.predict_output_word
 102. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.reset_from
 103. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
 104. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.sample
 105. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.save
 106. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.load
 107. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.save_id97_format
 108. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.score
 109. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.train
 110. https://arxiv.org/pdf/1504.07295.pdf
 111. https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb
 112. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.browncorpus
 113. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.text8corpus
 114. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
 115. https://radimrehurek.com/gensim/models/id97.html#module-gensim.models.id97
 116. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.similar_by_vector
 117. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.similar_by_vector
 118. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.similar_by_word
 119. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.similar_by_word
 120. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.similarity
 121. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.similarity
 122. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.syn0_lockf
 123. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.syn1
 124. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.syn1neg
 125. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.train
 126. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.build_vocab
 127. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.train
 128. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.browncorpus
 129. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.text8corpus
 130. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
 131. https://radimrehurek.com/gensim/models/id97.html#module-gensim.models.id97
 132. https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/
 133. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
 134. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.get_latest_training_loss
 135. https://radimrehurek.com/gensim/models/callbacks.html#gensim.models.callbacks.callbackany2vec
 136. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.wmdistance
 137. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.wmdistance
 138. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97trainables
 139. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload
 140. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
 141. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97trainables.load
 142. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.save
 143. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.save
 144. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97trainables.prepare_weights
 145. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97trainables.reset_weights
 146. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97trainables.save
 147. https://en.wikipedia.org/wiki/mmap
 148. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
 149. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97trainables.seeded_vector
 150. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97trainables.update_weights
 151. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97vocab
 152. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload
 153. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
 154. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97vocab.add_null_word
 155. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97vocab.create_binary_tree
 156. https://en.wikipedia.org/wiki/huffman_coding
 157. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97vocab.load
 158. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.save
 159. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.save
 160. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97vocab.make_cum_table
 161. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97vocab.prepare_vocab
 162. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97vocab.save
 163. https://en.wikipedia.org/wiki/mmap
 164. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
 165. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97vocab.scan_vocab
 166. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97vocab.sort_vocab
 167. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.score_cbow_pair
 168. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
 169. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.vocab
 170. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.score_sg_pair
 171. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
 172. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.vocab
 173. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.vocab
 174. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.train_cbow_pair
 175. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
 176. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.train_sg_pair
 177. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
 178. https://radimrehurek.com/gensim/index.html
 179. https://radimrehurek.com/cdn-cgi/l/email-protection#0d7f6c6964607f6865787f68664d7e6877636c60236e77
 180. https://radimrehurek.com/gensim/index.html
 181. https://radimrehurek.com/gensim/tutorial.html
 182. https://radimrehurek.com/gensim/install.html
 183. https://radimrehurek.com/gensim/support.html
 184. https://radimrehurek.com/gensim/apiref.html
 185. https://radimrehurek.com/gensim/about.html
 186. https://twitter.com/gensim_py
 187. https://groups.google.com/group/gensim
