   iframe: [1]https://www.googletagmanager.com/ns.html?id=gtm-mp366cc

gensim logo

   [2]gensim
   gensim tagline

get expert help from the gensim authors

       [3]consulting in machine learning & nlp

       commercial document similarity engine: [4]scaletext.ai

       [5]corporate trainings in python data science and deep learning
     * [6]home
     * [7]tutorials
     * [8]install
     * [9]support
     * [10]api
     * [11]about

   corpora and vector spaces

corpora and vector spaces[12]  

   this tutorial is available as a jupyter notebook [13]here.

   don   t forget to set
>>> import logging
>>> logging.basicconfig(format='%(asctime)s : %(levelname)s : %(message)s', leve
l=logging.info)

   if you want to see logging events.

from strings to vectors[14]  

   this time, let   s start from documents represented as strings:
>>> documents = ["human machine interface for lab abc computer applications",
>>>              "a survey of user opinion of computer system response time",
>>>              "the eps user interface management system",
>>>              "system and human system engineering testing of eps",
>>>              "relation of user perceived response time to error measurement"
,
>>>              "the generation of random binary unordered trees",
>>>              "the intersection graph of paths in trees",
>>>              "graph minors iv widths of trees and well quasi ordering",
>>>              "graph minors a survey"]

   this is a tiny corpus of nine documents, each consisting of only a
   single sentence.

   first, let   s tokenize the documents, remove common words (using a toy
   stoplist) as well as words that only appear once in the corpus:
>>> from pprint import pprint  # pretty-printer
>>> from collections import defaultdict
>>>
>>> # remove common words and tokenize
>>> stoplist = set('for a of the and to in'.split())
>>> texts = [[word for word in document.lower().split() if word not in stoplist]
>>>          for document in documents]
>>>
>>> # remove words that appear only once
>>> frequency = defaultdict(int)
>>> for text in texts:
>>>     for token in text:
>>>         frequency[token] += 1
>>>
>>> texts = [[token for token in text if frequency[token] > 1]
>>>          for text in texts]
>>>
>>> pprint(texts)
[['human', 'interface', 'computer'],
 ['survey', 'user', 'computer', 'system', 'response', 'time'],
 ['eps', 'user', 'interface', 'system'],
 ['system', 'human', 'system', 'eps'],
 ['user', 'response', 'time'],
 ['trees'],
 ['graph', 'trees'],
 ['graph', 'minors', 'trees'],
 ['graph', 'minors', 'survey']]

   your way of processing the documents will likely vary; here, i only
   split on whitespace to tokenize, followed by lowercasing each word. in
   fact, i use this particular (simplistic and inefficient) setup to mimic
   the experiment done in deerwester et al.   s original lsa article
   [15][1].

   the ways to process documents are so varied and application- and
   language-dependent that i decided to not constrain them by any
   interface. instead, a document is represented by the features extracted
   from it, not by its    surface    string form: how you get to the features
   is up to you. below i describe one common, general-purpose approach
   (called bag-of-words), but keep in mind that different application
   domains call for different features, and, as always, it   s [16]garbage
   in, garbage out   

   to convert documents to vectors, we   ll use a id194
   called [17]bag-of-words. in this representation, each document is
   represented by one vector where each vector element represents a
   question-answer pair, in the style of:

      how many times does the word system appear in the document? once.   

   it is advantageous to represent the questions only by their (integer)
   ids. the mapping between the questions and ids is called a dictionary:
>>> dictionary = corpora.dictionary(texts)
>>> dictionary.save('/tmp/deerwester.dict')  # store the dictionary, for future
reference
>>> print(dictionary)
dictionary(12 unique tokens)

   here we assigned a unique integer id to all words appearing in the
   corpus with the [18]gensim.corpora.dictionary.dictionary class. this
   sweeps across the texts, collecting word counts and relevant
   statistics. in the end, we see there are twelve distinct words in the
   processed corpus, which means each document will be represented by
   twelve numbers (ie., by a 12-d vector). to see the mapping between
   words and their ids:
>>> print(dictionary.token2id)
{'minors': 11, 'graph': 10, 'system': 5, 'trees': 9, 'eps': 8, 'computer': 0,
'survey': 4, 'user': 7, 'human': 1, 'time': 6, 'interface': 2, 'response': 3}

   to actually convert tokenized documents to vectors:
>>> new_doc = "human computer interaction"
>>> new_vec = dictionary.doc2bow(new_doc.lower().split())
>>> print(new_vec)  # the word "interaction" does not appear in the dictionary a
nd is ignored
[(0, 1), (1, 1)]

   the function doc2bow() simply counts the number of occurrences of each
   distinct word, converts the word to its integer word id and returns the
   result as a sparse vector. the sparse vector [(0, 1), (1, 1)] therefore
   reads: in the document    human computer interaction   , the words computer
   (id 0) and human (id 1) appear once; the other ten dictionary words
   appear (implicitly) zero times.
>>> corpus = [dictionary.doc2bow(text) for text in texts]
>>> corpora.mmcorpus.serialize('/tmp/deerwester.mm', corpus)  # store to disk, f
or later use
>>> print(corpus)
[(0, 1), (1, 1), (2, 1)]
[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]
[(2, 1), (5, 1), (7, 1), (8, 1)]
[(1, 1), (5, 2), (8, 1)]
[(3, 1), (6, 1), (7, 1)]
[(9, 1)]
[(9, 1), (10, 1)]
[(9, 1), (10, 1), (11, 1)]
[(4, 1), (10, 1), (11, 1)]

   by now it should be clear that the vector feature with id=10 stands for
   the question    how many times does the word graph appear in the
   document?    and that the answer is    zero    for the first six documents
   and    one    for the remaining three. as a matter of fact, we have arrived
   at exactly the same corpus of vectors as in the [19]quick example.

corpus streaming     one document at a time[20]  

   note that corpus above resides fully in memory, as a plain python list.
   in this simple example, it doesn   t matter much, but just to make things
   clear, let   s assume there are millions of documents in the corpus.
   storing all of them in ram won   t do. instead, let   s assume the
   documents are stored in a file on disk, one document per line. gensim
   only requires that a corpus must be able to return one document vector
   at a time:
>>> class mycorpus(object):
>>>     def __iter__(self):
>>>         for line in open('mycorpus.txt'):
>>>             # assume there's one document per line, tokens separated by whit
espace
>>>             yield dictionary.doc2bow(line.lower().split())

   download the sample [21]mycorpus.txt file here. the assumption that
   each document occupies one line in a single file is not important; you
   can mold the __iter__ function to fit your input format, whatever it
   is. walking directories, parsing xml, accessing network    just parse
   your input to retrieve a clean list of tokens in each document, then
   convert the tokens via a dictionary to their ids and yield the
   resulting sparse vector inside __iter__.
>>> corpus_memory_friendly = mycorpus()  # doesn't load the corpus into memory!
>>> print(corpus_memory_friendly)
<__main__.mycorpus object at 0x10d5690>

   corpus is now an object. we didn   t define any way to print it, so print
   just outputs address of the object in memory. not very useful. to see
   the constituent vectors, let   s iterate over the corpus and print each
   document vector (one at a time):
>>> for vector in corpus_memory_friendly:  # load one vector into memory at a ti
me
...     print(vector)
[(0, 1), (1, 1), (2, 1)]
[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]
[(2, 1), (5, 1), (7, 1), (8, 1)]
[(1, 1), (5, 2), (8, 1)]
[(3, 1), (6, 1), (7, 1)]
[(9, 1)]
[(9, 1), (10, 1)]
[(9, 1), (10, 1), (11, 1)]
[(4, 1), (10, 1), (11, 1)]

   although the output is the same as for the plain python list, the
   corpus is now much more memory friendly, because at most one vector
   resides in ram at a time. your corpus can now be as large as you want.

   similarly, to construct the dictionary without loading all texts into
   memory:
>>> from six import iteritems
>>> # collect statistics about all tokens
>>> dictionary = corpora.dictionary(line.lower().split() for line in open('mycor
pus.txt'))
>>> # remove stop words and words that appear only once
>>> stop_ids = [dictionary.token2id[stopword] for stopword in stoplist
>>>             if stopword in dictionary.token2id]
>>> once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if doc
freq == 1]
>>> dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words
 that appear only once
>>> dictionary.compactify()  # remove gaps in id sequence after words that were
removed
>>> print(dictionary)
dictionary(12 unique tokens)

   and that is all there is to it! at least as far as bag-of-words
   representation is concerned. of course, what we do with such corpus is
   another question; it is not at all clear how counting the frequency of
   distinct words could be useful. as it turns out, it isn   t, and we will
   need to apply a transformation on this simple representation first,
   before we can use it to compute any meaningful document vs. document
   similarities. transformations are covered in the [22]next tutorial, but
   before that, let   s briefly turn our attention to corpus persistency.

corpus formats[23]  

   there exist several file formats for serializing a vector space corpus
   (~sequence of vectors) to disk. gensim implements them via the
   streaming corpus interface mentioned earlier: documents are read from
   (resp. stored to) disk in a lazy fashion, one document at a time,
   without the whole corpus being read into main memory at once.

   one of the more notable file formats is the [24]market matrix format.
   to save a corpus in the matrix market format:
>>> # create a toy corpus of 2 documents, as a plain python list
>>> corpus = [[(1, 0.5)], []]  # make one document empty, for the heck of it
>>>
>>> corpora.mmcorpus.serialize('/tmp/corpus.mm', corpus)

   other formats include [25]joachim   s id166light format, [26]blei   s lda-c
   format and [27]gibbslda++ format.
>>> corpora.id166lightcorpus.serialize('/tmp/corpus.id166light', corpus)
>>> corpora.bleicorpus.serialize('/tmp/corpus.lda-c', corpus)
>>> corpora.lowcorpus.serialize('/tmp/corpus.low', corpus)

   conversely, to load a corpus iterator from a matrix market file:
>>> corpus = corpora.mmcorpus('/tmp/corpus.mm')

   corpus objects are streams, so typically you won   t be able to print
   them directly:
>>> print(corpus)
mmcorpus(2 documents, 2 features, 1 non-zero entries)

   instead, to view the contents of a corpus:
>>> # one way of printing a corpus: load it entirely into memory
>>> print(list(corpus))  # calling list() will convert any sequence to a plain p
ython list
[[(1, 0.5)], []]

   or
>>> # another way of doing it: print one document at a time, making use of the s
treaming interface
>>> for doc in corpus:
...     print(doc)
[(1, 0.5)]
[]

   the second way is obviously more memory-friendly, but for testing and
   development purposes, nothing beats the simplicity of calling
   list(corpus).

   to save the same matrix market document stream in blei   s lda-c format,
>>> corpora.bleicorpus.serialize('/tmp/corpus.lda-c', corpus)

   in this way, gensim can also be used as a memory-efficient i/o format
   conversion tool: just load a document stream using one format and
   immediately save it in another format. adding new formats is dead easy,
   check out the [28]code for the id166light corpus for an example.

compatibility with numpy and scipy[29]  

   gensim also contains [30]efficient utility functions to help converting
   from/to numpy matrices
>>> import gensim
>>> import numpy as np
>>> numpy_matrix = np.random.randint(10, size=[5, 2])  # random matrix as an exa
mple
>>> corpus = gensim.matutils.dense2corpus(numpy_matrix)
>>> numpy_matrix = gensim.matutils.corpus2dense(corpus, num_terms=number_of_corp
us_features)

   and from/to scipy.sparse matrices
>>> import scipy.sparse
>>> scipy_sparse_matrix = scipy.sparse.random(5, 2)  # random sparse matrix as e
xample
>>> corpus = gensim.matutils.sparse2corpus(scipy_sparse_matrix)
>>> scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)
     __________________________________________________________________

   for a complete reference (want to prune the dictionary to a smaller
   size? optimize converting between corpora and numpy/scipy arrays?), see
   the [31]api documentation. or continue to the next tutorial on
   [32]topics and transformations.
   [33][1] this is the same corpus as used in [34]deerwester et al.
   (1990): indexing by latent semantic analysis, table 2.

   smaller gensim logo [35]gensim footer image
      copyright 2009-now, [36]radim   eh    ek
   last updated on jan 31, 2019.
     * [37]home
     * |
     * [38]tutorials
     * |
     * [39]install
     * |
     * [40]support
     * |
     * [41]api
     * |
     * [42]about

   [43]tweet @gensim_py
   support:
   [44]stay informed via gensim mailing list: ____________________________
   subscribe

references

   1. https://www.googletagmanager.com/ns.html?id=gtm-mp366cc
   2. https://radimrehurek.com/gensim/index.html
   3. https://rare-technologies.com/
   4. https://scaletext.com/
   5. https://rare-technologies.com/corporate-training/
   6. https://radimrehurek.com/gensim/index.html
   7. https://radimrehurek.com/gensim/tutorial.html
   8. https://radimrehurek.com/gensim/install.html
   9. https://radimrehurek.com/gensim/support.html
  10. https://radimrehurek.com/gensim/apiref.html
  11. https://radimrehurek.com/gensim/about.html
  12. https://radimrehurek.com/gensim/tut1.html#corpora-and-vector-spaces
  13. https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/corpora_and_vector_spaces.ipynb
  14. https://radimrehurek.com/gensim/tut1.html#from-strings-to-vectors
  15. https://radimrehurek.com/gensim/tut1.html# 
  16. https://en.wikipedia.org/wiki/garbage_in,_garbage_out
  17. https://en.wikipedia.org/wiki/bag_of_words
  18. https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.dictionary
  19. https://radimrehurek.com/gensim/tutorial.html#first-example
  20. https://radimrehurek.com/gensim/tut1.html#corpus-streaming-one-document-at-a-time
  21. https://radimrehurek.com/gensim/mycorpus.txt
  22. https://radimrehurek.com/gensim/tut2.html
  23. https://radimrehurek.com/gensim/tut1.html#corpus-formats
  24. http://math.nist.gov/matrixmarket/formats.html
  25. http://id166light.joachims.org/
  26. https://www.cs.princeton.edu/~blei/lda-c/
  27. http://gibbslda.sourceforge.net/
  28. https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/id166lightcorpus.py
  29. https://radimrehurek.com/gensim/tut1.html#compatibility-with-numpy-and-scipy
  30. https://radimrehurek.com/gensim/matutils.html
  31. https://radimrehurek.com/gensim/apiref.html
  32. https://radimrehurek.com/gensim/tut2.html
  33. https://radimrehurek.com/gensim/tut1.html#id1
  34. http://www.cs.bham.ac.uk/~pxt/ida/lsa_ind.pdf
  35. https://radimrehurek.com/gensim/index.html
  36. https://radimrehurek.com/cdn-cgi/l/email-protection#e69487828f8b94838e9394838da695839c88878bc8859c
  37. https://radimrehurek.com/gensim/index.html
  38. https://radimrehurek.com/gensim/tutorial.html
  39. https://radimrehurek.com/gensim/install.html
  40. https://radimrehurek.com/gensim/support.html
  41. https://radimrehurek.com/gensim/apiref.html
  42. https://radimrehurek.com/gensim/about.html
  43. https://twitter.com/gensim_py
  44. https://groups.google.com/group/gensim
