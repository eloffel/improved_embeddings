   #[1]rare technologies    feed [2]rare technologies    comments feed
   [3]rare technologies    fasttext and gensim id27s comments
   feed [4]alternate [5]alternate

   [tr?id=1761346240851963&ev=pageview&noscript=1]

   iframe: [6]https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld

   [7]pragmatic machine learning rare technologies [8]navigation

     * [9]services
     * [10]products
          + [11]pii tools
          + [12]scaletext
     * [13]corporate training
          + [14]overview
          + [15]python best practices
          + [16]practical machine learning
          + [17]topic modelling
          + [18]deep learning in practice
     * [19]for students
          + [20]open source
          + [21]incubator
          + [22]competitions
     * [23]company
          + [24]careers
          + [25]our team
     * [26]blog
     * [27]contact
     * [28]search

     * [29]services
     * [30]products
          + [31]pii tools
          + [32]scaletext
     * [33]corporate training
          + [34]overview
          + [35]python best practices
          + [36]practical machine learning
          + [37]topic modelling
          + [38]deep learning in practice
     * [39]for students
          + [40]open source
          + [41]incubator
          + [42]competitions
     * [43]company
          + [44]careers
          + [45]our team
     * [46]blog
     * [47]contact
     * [48]search

fasttext and gensim id27s

   [49]jayant jain 2016-08-31[50] gensim

   facebook research open sourced a great project recently     [51]fasttext,
   a fast (no surprise) and effective method to learn word representations
   and perform text classification. i was curious about comparing these
   embeddings to other commonly used embeddings, so id97 seemed like
   the obvious choice, especially considering fasttext embeddings are an
   extension of id97.

   the main goal of the fast text embeddings is to take into account the
   internal structure of words while learning word representations     this
   is especially useful for morphologically rich languages, where
   otherwise the representations for different morphological forms of
   words would be learnt independently. the limitation becomes even more
   important when these words occur rarely.

   i   ve used the analogical reasoning task described in the
   paper [52]efficient estimation of word representations in vector space,
   which evaluates word vectors on semantic and syntactic word analogies.
   id27s compared have been trained using the skipgram
   architecture.

comparisons

   the first comparison is on gensim and fasttext models trained on the
   brown corpus. for detailed code and information about the
   hyperparameters, you can have a look at this [53]ipython notebook.

   brown_gensim_fasttext

   id97 embeddings seem to be slightly better than fasttext embeddings
   at the semantic tasks, while the fasttext embeddings do significantly
   better on the syntactic analogies. makes sense, since fasttext
   embeddings are trained for understanding morphological nuances, and
   most of the syntactic analogies are morphology based.

   let me explain that better. according to the paper ([54]enriching word
   vectors with subword information), embeddings for words are represented
   by the sum of their id165 embeddings. this is meant to be useful for
   morphologically rich languages     so theoretically, the embedding for
   apparently would include information from both character id165s
   apparent and ly (as well as other id165s), and the id165s would
   combine in a simple, linear manner. this is very similar to what most
   of our syntactic tasks look like.

   example analogy: amazing amazingly calm calmly

   this analogy is marked correct if:

   embedding(amazing)     embedding(amazingly)

   = embedding(calm)     embedding(calmly)

   both these subtractions would result in a very similar set of remaining
   ngrams. no surprise the fasttext embeddings do extremely well on this.

   let   s do a small test to validate this hypothesis     fasttext differs
   from id97 only in that it uses char id165 embeddings as well as
   the actual id27 in the scoring function to calculate scores
   and then likelihoods for each word, given a context word. in case char
   id165 embeddings are not present, this reduces (atleast theoretically)
   to the original id97 model. this can be implemented by setting 0
   for the max length of char id165s for fasttext.

   brown_fasttext_no_ngrams

   a-ha! the results for fasttext with no id165s and id97 look a lot
   more similar (as they should)     the differences could easily result
   from differences in implementation between fasttext and gensim, and
   randomization. especially telling is that the semantic accuracy for
   fasttext has improved slightly after removing id165s, while the
   syntactic accuracy has taken a giant dive. our hypothesis that the char
   id165s result in better performance on syntactic analogies seems fair.
   it also seems possible that char id165s hurt semantic accuracy a
   little. however, the brown corpus is too small to be able to draw any
   definite conclusions     the accuracies seem to vary significantly over
   different runs.

   let   s try with a larger corpus now     text8 (collection of wiki
   articles). i   m also curious about the impact on semantic accuracy     for
   models trained on the brown corpus, the difference in the semantic
   accuracy and the accuracy values themselves are too small to be
   conclusive. hopefully a larger corpus helps, and the text8 corpus
   likely has a lot more information about capitals, currencies, cities
   etc, which should be relevant to the semantic tasks.

   text8_all text8_ft_nong text8_fasttext with the text8 corpus, we
   observe a similar pattern. semantic accuracy falls by a small but
   significant amount when id165s are included in fasttext, while
   fasttext with id165s performs far better on the syntactic analogies.
   fasttext without id165s are largely similar to id97.

   my hypothesis for semantic accuracy being lower for the
   fasttext-with-ngrams model is that most of the words in the semantic
   analogies are standalone words and are unrelated to their morphemes
   (eg: father, mother, france, paris), hence inclusion of the char
   id165s into the scoring function actually makes the embeddings worse.

   this trend is observed in the original paper too where the performance
   of embeddings with id165s is worse on semantic tasks than both
   id97 cbow and skipgram models.

   doing a similar evaluation on an even larger corpus     text9     and
   plotting a graph for training times and accuracies, we obtain    

   final_graph

   the results from text9 seem to confirm our hypotheses so far. briefly
   summarising the main points    
    1. fasttext models with id165s do significantly better on syntactic
       tasks, because of the syntactic questions being related to
       morphology of the words
    2. both gensim id97 and the fasttext model with no id165s do
       slightly better on the semantic tasks, presumably because words
       from the semantic questions are standalone words and unrelated to
       their char id165s
    3. in general, the performance of the models seems to get closer with
       the increasing corpus size. however, this might possibly be due to
       the size of the model staying constant at 100, and a larger model
       size for large corpora might result in higher performance gains.
    4. the semantic accuracy for all models increases significantly with
       the increase in corpus size.
    5. however, the increase in syntactic accuracy from the increase in
       corpus size for the id165 fasttext model is lower (in both
       relative and absolute terms). this could possibly indicate that
       advantages gained by incorporating morphological information could
       be less significant in case of larger corpus sizes (the corpuses
       used in the original paper seem to indicate this too)
    6. training times for gensim are slightly lower than the fasttext
       no-ngram model, and significantly lower than the id165 variant.
       this is quite impressive considering fasttext is implemented in c++
       and gensim in python (with calls to low-level blas routines for
       much of the heavy lifting). you could read [55]this post for more
       details regarding id97 optimisation in gensim. note that these
       times include importing any dependencies and serializing the models
       to disk, and not just the training times.

conclusions

   these preliminary results seem to indicate fasttext embeddings are
   significantly better than id97 at encoding syntactic information.
   this is expected, since most syntactic analogies are morphology based,
   and the char id165 approach of fasttext takes such information into
   account. the original id97 model seems to perform better on
   semantic tasks, since words in semantic analogies are unrelated to
   their char id165s, and the added information from irrelevant char
   id165s worsens the embeddings. it   d be interesting to see how
   transferable these embeddings are for different kinds of tasks by
   comparing their performance in a downstream supervised task.

   [56]fasttext[57]gensim[58]id27s[59]id97

author of post

jayant jain's bio:

   r&d engineer at rare technologies, interested in applying nlp and text
   analysis to a variety of domains, especially news. graduate from iit-r.
   loves board games and reading sci-fi. [60]github profile

need expert consulting in ml and nlp?

   ________________________________________

   ________________________________________


   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   please leave this field empty. ________________________________________

   send

categories

   categories[select category___________]

archives

   archives [select month__]

recent posts

     * [61]export pii drill-down reports
     * [62]personal data analytics
     * [63]scanning office 365 for sensitive pii information
     * [64]pivoted document length normalisation
     * [65]sent2vec: an unsupervised approach towards learning sentence
       embeddings

stay ahead of the curve

get our latest tutorials, updates and insights delivered straight to your
inbox.

   ____________________

   ____________________

   subscribe
   ____________________
   1-2 times a month, if lucky. your information will not be shared.

   [66][footer-logo.png]
     * [67]services
     * [68]careers
     * [69]our team
     * [70]corporate training
     * [71]blog
     * [72]incubator
     * [73]contact
     * [74]competitions
     * [75]site map

   rare technologies [76][email protected] sv  tova 5, prague, czech
   republic [77](eu) +420 776 288 853
   type and press    enter    to search ____________________

references

   visible links
   1. https://rare-technologies.com/feed/
   2. https://rare-technologies.com/comments/feed/
   3. https://rare-technologies.com/fasttext-and-gensim-word-embeddings/feed/
   4. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/fasttext-and-gensim-word-embeddings/
   5. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/fasttext-and-gensim-word-embeddings/&format=xml
   6. https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld
   7. https://rare-technologies.com/
   8. https://rare-technologies.com/fasttext-and-gensim-word-embeddings/
   9. https://rare-technologies.com/services/
  10. https://rare-technologies.com/fasttext-and-gensim-word-embeddings/
  11. https://pii-tools.com/
  12. https://scaletext.com/
  13. https://rare-technologies.com/corporate-training/
  14. https://rare-technologies.com/corporate-training/
  15. https://rare-technologies.com/python-best-practices/
  16. https://rare-technologies.com/practical-machine-learning/
  17. https://rare-technologies.com/topic-modelling-training/
  18. https://rare-technologies.com/deep_learning_training/
  19. https://rare-technologies.com/incubator
  20. https://github.com/rare-technologies/
  21. https://rare-technologies.com/incubator/
  22. https://rare-technologies.com/competitions/
  23. https://rare-technologies.com/#braintrust
  24. https://rare-technologies.com/careers/
  25. https://rare-technologies.com/our-team/
  26. https://rare-technologies.com/blog/
  27. https://rare-technologies.com/contact/
  28. https://rare-technologies.com/fasttext-and-gensim-word-embeddings/
  29. https://rare-technologies.com/services/
  30. https://rare-technologies.com/fasttext-and-gensim-word-embeddings/
  31. https://pii-tools.com/
  32. https://scaletext.com/
  33. https://rare-technologies.com/corporate-training/
  34. https://rare-technologies.com/corporate-training/
  35. https://rare-technologies.com/python-best-practices/
  36. https://rare-technologies.com/practical-machine-learning/
  37. https://rare-technologies.com/topic-modelling-training/
  38. https://rare-technologies.com/deep_learning_training/
  39. https://rare-technologies.com/incubator
  40. https://github.com/rare-technologies/
  41. https://rare-technologies.com/incubator/
  42. https://rare-technologies.com/competitions/
  43. https://rare-technologies.com/#braintrust
  44. https://rare-technologies.com/careers/
  45. https://rare-technologies.com/our-team/
  46. https://rare-technologies.com/blog/
  47. https://rare-technologies.com/contact/
  48. https://rare-technologies.com/fasttext-and-gensim-word-embeddings/
  49. https://rare-technologies.com/author/jayant/
  50. https://rare-technologies.com/category/gensim/
  51. https://github.com/facebookresearch/fasttext
  52. https://arxiv.org/pdf/1301.3781v3.pdf
  53. https://github.com/jayantj/gensim/blob/9f3e275ddad22afd54b7986654f3033f9baf8983/docs/notebooks/id97_fasttext_comparison.ipynb
  54. https://arxiv.org/pdf/1607.04606v1.pdf
  55. https://rare-technologies.com/id97-in-python-part-two-optimizing/
  56. https://rare-technologies.com/tag/fasttext/
  57. https://rare-technologies.com/tag/gensim/
  58. https://rare-technologies.com/tag/word-embeddings/
  59. https://rare-technologies.com/tag/id97/
  60. https://github.com/jayantj/
  61. https://rare-technologies.com/personal-data-reports/
  62. https://rare-technologies.com/pii_analytics/
  63. https://rare-technologies.com/pii-scan-o365-connector/
  64. https://rare-technologies.com/pivoted-document-length-normalisation/
  65. https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/
  66. https://rare-technologies.com/fasttext-and-gensim-word-embeddings/
  67. https://rare-technologies.com/services/
  68. https://rare-technologies.com/careers/
  69. https://rare-technologies.com/our-team/
  70. https://rare-technologies.com/corporate-training/
  71. https://rare-technologies.com/blog/
  72. https://rare-technologies.com/incubator/
  73. https://rare-technologies.com/contact/
  74. https://rare-technologies.com/competitions/
  75. https://rare-technologies.com/sitemap
  76. https://rare-technologies.com/cdn-cgi/l/email-protection#bed7d0d8d1feccdfccdb93cadbddd6d0d1d2d1d9d7dbcd90ddd1d3
  77. tel:+420 776 288 853

   hidden links:
  79. https://rare-technologies.com/fasttext-and-gensim-word-embeddings/#top
  80. https://www.facebook.com/raretechnologies
  81. https://twitter.com/raretechteam
  82. https://www.linkedin.com/company/6457766
  83. https://github.com/piskvorky/
  84. https://rare-technologies.com/feed/
