   iframe: [1]https://www.googletagmanager.com/ns.html?id=gtm-mp366cc

gensim logo

   [2]gensim
   gensim tagline

get expert help from the gensim authors

       [3]consulting in machine learning & nlp

       commercial document similarity engine: [4]scaletext.ai

       [5]corporate trainings in python data science and deep learning
     * [6]home
     * [7]tutorials
     * [8]install
     * [9]support
     * [10]api
     * [11]about

   models.doc2vec     doc2vec paragraph embeddings

models.doc2vec     doc2vec paragraph embeddings[12]  

   learn paragraph and document embeddings via the distributed memory and
   distributed id159s from [13]quoc le and tomas mikolov:
      distributed representations of sentences and documents   .

   the algorithms use either hierarchical softmax or negative sampling;
   see [14]tomas mikolov, kai chen, greg corrado, and jeffrey dean:
      efficient estimation of word representations in vector space, in
   proceedings of workshop at iclr, 2013    and [15]tomas mikolov, ilya
   sutskever, kai chen, greg corrado, and jeffrey dean:    distributed
   representations of words and phrases and their compositionality. in
   proceedings of nips, 2013   .

   for a usage example, see the [16]doc2vec tutorial.

   make sure you have a c compiler before installing gensim, to use the
   optimized doc2vec routines (70x speedup compared to plain numpy
   implementation,
   [17]https://rare-technologies.com/parallelizing-id97-in-python/).

usage examples[18]  

   initialize & train a model:
>>> from gensim.test.utils import common_texts
>>> from gensim.models.doc2vec import doc2vec, taggeddocument
>>>
>>> documents = [taggeddocument(doc, [i]) for i, doc in enumerate(common_texts)]
>>> model = doc2vec(documents, vector_size=5, window=2, min_count=1, workers=4)

   persist a model to disk:
>>> from gensim.test.utils import get_tmpfile
>>>
>>> fname = get_tmpfile("my_doc2vec_model")
>>>
>>> model.save(fname)
>>> model = doc2vec.load(fname)  # you can continue training with the loaded mod
el!

   if you   re finished training a model (=no more updates, only querying,
   reduce memory usage), you can do:
>>> model.delete_temporary_training_data(keep_doctags_vectors=true, keep_inferen
ce=true)

   infer vector for a new document:
>>> vector = model.infer_vector(["system", "response"])

   class gensim.models.doc2vec.doc2vec(documents=none, corpus_file=none,
          dm_mean=none, dm=1, dbow_words=0, dm_concat=0, dm_tag_count=1,
          docvecs=none, docvecs_mapfile=none, comment=none,
          trim_rule=none, callbacks=(), **kwargs)[19]  
          bases: [20]gensim.models.base_any2vec.basewordembeddingsmodel

          class for training, using and evaluating neural networks
          described in [21]distributed representations of sentences and
          documents.

          some important internal attributes are the following:

        wv[22]  
                this object essentially contains the mapping between words
                and embeddings. after training, it can be used directly to
                query those embeddings in various ways. see the module
                level docstring for examples.

                type: [23]id97keyedvectors

        docvecs[24]  
                this object contains the paragraph vectors. remember that
                the only difference between this model and [25]id97 is
                that besides the word vectors we also include paragraph
                embeddings to capture the paragraph.

                in this way we can capture the difference between the same
                word used in a different context. for example we now have
                a different representation of the word    leaves    in the
                following two sentences

1. manos leaves the office every day at 18:00 to catch his train
2. this season is called fall, because leaves fall from the trees.

                in a plain [26]id97 model the word would have exactly
                the same representation in both sentences, in [27]doc2vec
                it will not.

                type: [28]doc2veckeyedvectors

        vocabulary[29]  
                this object represents the vocabulary (sometimes called
                dictionary in gensim) of the model. besides keeping track
                of all unique words, this object provides extra
                functionality, such as sorting words by frequency, or
                discarding extremely rare words.

                type: [30]doc2vecvocab

        trainables[31]  
                this object represents the inner shallow neural network
                used to train the embeddings. the semantics of the network
                differ slightly in the two available training modes (cbow
                or sg) but you can think of it as a nn with a single
                projection and hidden layer which we train on the corpus.
                the weights are then used as our embeddings the only
                addition to the underlying nn used in [32]id97 is that
                the input includes not only the word vectors of each word
                in the context, but also the paragraph vector.

                type: [33]doc2vectrainables

   parameters:
          + documents (iterable of list of [34]taggeddocument, optional)    
            input corpus, can be simply a list of elements, but for larger
            corpora,consider an iterable that streams the documents
            directly from disk/network. if you don   t supply documents (or
            corpus_file), the model is left uninitialized     use if you
            plan to initialize it in some other way.
          + corpus_file (str, optional)     path to a corpus file in
            [35]linesentence format. you may use this argument instead of
            documents to get performance boost. only one of documents or
            corpus_file arguments need to be passed (or none of them, in
            that case, the model is left uninitialized). documents    tags
            are assigned automatically and are equal to line number, as in
            [36]taggedlinedocument.
          + dm ({1,0}, optional)     defines the training algorithm. if
            dm=1,    distributed memory    (pv-dm) is used. otherwise,
            distributed bag of words (pv-dbow) is employed.
          + vector_size (int, optional)     dimensionality of the feature
            vectors.
          + window (int, optional)     the maximum distance between the
            current and predicted word within a sentence.
          + alpha (float, optional)     the initial learning rate.
          + min_alpha (float, optional)     learning rate will linearly drop
            to min_alpha as training progresses.
          + seed (int, optional)     seed for the random number generator.
            initial vectors for each word are seeded with a hash of the
            concatenation of word + str(seed). note that for a fully
            deterministically-reproducible run, you must also limit the
            model to a single worker thread (workers=1), to eliminate
            ordering jitter from os thread scheduling. in python 3,
            reproducibility between interpreter launches also requires use
            of the pythonhashseed environment variable to control hash
            randomization.
          + min_count (int, optional)     ignores all words with total
            frequency lower than this.
          + max_vocab_size (int, optional)     limits the ram during
            vocabulary building; if there are more unique words than this,
            then prune the infrequent ones. every 10 million word types
            need about 1gb of ram. set to none for no limit.
          + sample (float, optional)     the threshold for configuring which
            higher-frequency words are randomly downsampled, useful range
            is (0, 1e-5).
          + workers (int, optional)     use these many worker threads to
            train the model (=faster training with multicore machines).
          + epochs (int, optional)     number of iterations (epochs) over
            the corpus.
          + hs ({1,0}, optional)     if 1, hierarchical softmax will be used
            for model training. if set to 0, and negative is non-zero,
            negative sampling will be used.
          + negative (int, optional)     if > 0, negative sampling will be
            used, the int for negative specifies how many    noise words   
            should be drawn (usually between 5-20). if set to 0, no
            negative sampling is used.
          + ns_exponent (float, optional)     the exponent used to shape the
            negative sampling distribution. a value of 1.0 samples exactly
            in proportion to the frequencies, 0.0 samples all words
            equally, while a negative value samples low-frequency words
            more than high-frequency words. the popular default value of
            0.75 was chosen by the original id97 paper. more recently,
            in [37]https://arxiv.org/abs/1804.04212, caselles-dupr  ,
            lesaint, & royo-letelier suggest that other values may perform
            better for recommendation applications.
          + dm_mean ({1,0}, optional)     if 0 , use the sum of the context
            word vectors. if 1, use the mean. only applies when dm is used
            in non-concatenative mode.
          + dm_concat ({1,0}, optional)     if 1, use concatenation of
            context vectors rather than sum/average; note concatenation
            results in a much-larger model, as the input is no longer the
            size of one (sampled or arithmetically combined) word vector,
            but the size of the tag(s) and all words in the context strung
            together.
          + dm_tag_count (int, optional)     expected constant number of
            document tags per document, when using dm_concat mode.
          + dbow_words ({1,0}, optional)     if set to 1 trains word-vectors
            (in skip-gram fashion) simultaneous with dbow doc-vector
            training; if 0, only trains doc-vectors (faster).
          + trim_rule (function, optional)    
            vocabulary trimming rule, specifies whether certain words
            should remain in the vocabulary, be trimmed away, or handled
            using the default (discard if word count < min_count). can be
            none (min_count will be used, look to [38]keep_vocab_item()),
            or a callable that accepts parameters (word, count, min_count)
            and returns either gensim.utils.rule_discard,
            gensim.utils.rule_keep or gensim.utils.rule_default. the rule,
            if given, is only used to prune vocabulary during current
            method call and is not stored as part of the model.

              the input parameters are of the following types:

                    # word (str) - the word we are examining
                    # count (int) - the word   s frequency count in the
                      corpus
                    # min_count (int) - the minimum count threshold.

          + callbacks     list of callbacks that need to be executed/run at
            specific stages during training.

        build_vocab(documents=none, corpus_file=none, update=false,
                progress_per=10000, keep_raw_vocab=false, trim_rule=none,
                **kwargs)[39]  
                build vocabulary from a sequence of documents (can be a
                once-only generator stream).

   parameters:
               o documents (iterable of list of [40]taggeddocument,
                 optional)     can be simply a list of [41]taggeddocument
                 elements, but for larger corpora, consider an iterable
                 that streams the documents directly from disk/network.
                 see [42]taggedbrowncorpus or [43]taggedlinedocument
               o corpus_file (str, optional)     path to a corpus file in
                 [44]linesentence format. you may use this argument
                 instead of documents to get performance boost. only one
                 of documents or corpus_file arguments need to be passed
                 (not both of them). documents    tags are assigned
                 automatically and are equal to a line number, as in
                 [45]taggedlinedocument.
               o update (bool)     if true, the new words in documents will
                 be added to model   s vocab.
               o progress_per (int)     indicates how many words to process
                 before showing/updating the progress.
               o keep_raw_vocab (bool)     if not true, delete the raw
                 vocabulary after the scaling is done and free up ram.
               o trim_rule (function, optional)    
                 vocabulary trimming rule, specifies whether certain words
                 should remain in the vocabulary, be trimmed away, or
                 handled using the default (discard if word count <
                 min_count). can be none (min_count will be used, look to
                 [46]keep_vocab_item()), or a callable that accepts
                 parameters (word, count, min_count) and returns either
                 gensim.utils.rule_discard, gensim.utils.rule_keep or
                 gensim.utils.rule_default. the rule, if given, is only
                 used to prune vocabulary during current method call and
                 is not stored as part of the model.

                    the input parameters are of the following types:

                         @ word (str) - the word we are examining
                         @ count (int) - the word   s frequency count in the
                           corpus
                         @ min_count (int) - the minimum count threshold.

               o **kwargs     additional key word arguments passed to the
                 internal vocabulary construction.

        build_vocab_from_freq(word_freq, keep_raw_vocab=false,
                corpus_count=none, trim_rule=none, update=false)[47]  
                build vocabulary from a dictionary of word frequencies.

                build model vocabulary from a passed dictionary that
                contains a (word -> word count) mapping. words must be of
                type unicode strings.

   parameters:
               o word_freq (dict of (str, int))     word <-> count mapping.
               o keep_raw_vocab (bool, optional)     if not true, delete the
                 raw vocabulary after the scaling is done and free up ram.
               o corpus_count (int, optional)     even if no corpus is
                 provided, this argument can set corpus_count explicitly.
               o trim_rule (function, optional)    
                 vocabulary trimming rule, specifies whether certain words
                 should remain in the vocabulary, be trimmed away, or
                 handled using the default (discard if word count <
                 min_count). can be none (min_count will be used, look to
                 [48]keep_vocab_item()), or a callable that accepts
                 parameters (word, count, min_count) and returns either
                 gensim.utils.rule_discard, gensim.utils.rule_keep or
                 gensim.utils.rule_default. the rule, if given, is only
                 used to prune vocabulary during [49]build_vocab() and is
                 not stored as part of the model.

                    the input parameters are of the following types:

                         @ word (str) - the word we are examining
                         @ count (int) - the word   s frequency count in the
                           corpus
                         @ min_count (int) - the minimum count threshold.

               o update (bool, optional)     if true, the new provided words
                 in word_freq dict will be added to model   s vocab.

        clear_sims()[50]  
                resets the current word vectors.

        cum_table[51]  

        dbow[52]  
                indicates whether    distributed bag of words    (pv-dbow)
                will be used, else    distributed memory    (pv-dm) is used.

        delete_temporary_training_data(keep_doctags_vectors=true,
                keep_id136=true)[53]  
                discard parameters that are used in training and score.
                use if you   re sure you   re done training a model.

   parameters:
               o keep_doctags_vectors (bool, optional)     set to false if
                 you don   t want to save doctags vectors. in this case you
                 will not be able to use [54]most_similar(),
                 [55]similarity(), etc methods.
               o keep_id136 (bool, optional)     set to false if you
                 don   t want to store parameters that are used for
                 [56]infer_vector() method.

        dm[57]  
                indicates whether    distributed memory    (pv-dm) will be
                used, else    distributed bag of words    (pv-dbow) is used.

        doesnt_match(**kwargs)[58]  
                deprecated, use self.wv.doesnt_match() instead.

                refer to the documentation for [59]doesnt_match().

        estimate_memory(vocab_size=none, report=none)[60]  
                estimate required memory for a model using current
                settings.

   parameters:
               o vocab_size (int, optional)     number of raw words in the
                 vocabulary.
               o report (dict of (str, int), optional)     a dictionary from
                 string representations of the specific model   s memory
                 consuming members to their size in bytes.

   returns:

   a dictionary from string representations of the model   s memory
   consuming members to their size in bytes. includes members from the
   base classes as well as weights and tag lookup memory estimation
   specific to the class.
                return type:

   dict of (str, int), optional

        estimated_lookup_memory()[61]  
                get estimated memory for tag lookup, 0 if using pure int
                tags.

                returns:   the estimated ram required to look up a tag in bytes.
              return type: int

        evaluate_word_pairs(**kwargs)[62]  
                deprecated, use self.wv.evaluate_word_pairs() instead.

                refer to the documentation for [63]evaluate_word_pairs().

        hashfxn[64]  

        infer_vector(doc_words, alpha=none, min_alpha=none, epochs=none,
                steps=none)[65]  
                infer a vector for given post-bulk training document.

                notes

                subsequent calls to this function may infer different
                representations for the same document. for a more stable
                representation, increase the number of steps to assert a
                stricket convergence.

   parameters:
               o doc_words (list of str)     a document for which the vector
                 representation will be inferred.
               o alpha (float, optional)     the initial learning rate. if
                 unspecified, value from model initialization will be
                 reused.
               o min_alpha (float, optional)     learning rate will linearly
                 drop to min_alpha over all id136 epochs. if
                 unspecified, value from model initialization will be
                 reused.
               o epochs (int, optional)     number of times to train the new
                 document. larger values take more time, but may improve
                 quality and run-to-run stability of inferred vectors. if
                 unspecified, the epochs value from model initialization
                 will be reused.
               o steps (int, optional, deprecated)     previous name for
                 epochs, still available for now for backward
                 compatibility: if epochs is unspecified but steps is, the
                 steps value will be used.

                  returns:

   the inferred paragraph vector for the new document.
                return type:

   np.ndarray

        init_sims(replace=false)[66]  
                pre-compute l2-normalized vectors.

   parameters: replace (bool)     if true - forget the original vectors and
   only keep the normalized ones to saved ram (also you can   t continue
   training if call it with replace=true).

        iter[67]  

        layer1_size[68]  

        classmethod load(*args, **kwargs)[69]  
                load a previously saved [70]doc2vec model.

   parameters:
               o fname (str)     path to the saved file.
               o *args (object)     additional arguments, see
                 ~gensim.models.base_any2vec.basewordembeddingsmodel.load.
               o **kwargs (object)     additional arguments, see
                 ~gensim.models.base_any2vec.basewordembeddingsmodel.load.

                see also

              [71]save()
                      save [72]doc2vec model.

                  returns:   loaded model.
                return type: [73]doc2vec

        min_count[74]  

        most_similar(**kwargs)[75]  
                deprecated, use self.wv.most_similar() instead.

                refer to the documentation for [76]most_similar().

        most_similar_cosmul(**kwargs)[77]  
                deprecated, use self.wv.most_similar_cosmul() instead.

                refer to the documentation for [78]most_similar_cosmul().

        n_similarity(**kwargs)[79]  
                deprecated, use self.wv.n_similarity() instead.

                refer to the documentation for [80]n_similarity().

        reset_from(other_model)[81]  
                copy shareable data structures from another (possibly
                pre-trained) model.

   parameters: other_model ([82]doc2vec)     other model whose internal data
   structures will be copied over to the current object.

        sample[83]  

        save(fname_or_handle, **kwargs)[84]  
                   save the object to file.

   parameters:
               o fname_or_handle ({str, file-like object})     path to file
                 where the model will be persisted.
               o **kwargs (object)     key word arguments propagated to
                 [85]save().

                see also

              [86]load()
                      method for load model after current method.

        save_id97_format(fname, doctag_vec=false, word_vec=true,
                prefix='*dt_', fvocab=none, binary=false)[87]  
                store the input-hidden weight matrix in the same format
                used by the original c id97-tool.

   parameters:
               o fname (str)     the file path used to save the vectors in.
               o doctag_vec (bool, optional)     indicates whether to store
                 document vectors.
               o word_vec (bool, optional)     indicates whether to store
                 word vectors.
               o prefix (str, optional)     uniquely identifies doctags from
                 word vocab, and avoids collision in case of repeated
                 string in doctag and word vocab.
               o fvocab (str, optional)     optional file path used to save
                 the vocabulary.
               o binary (bool, optional)     if true, the data will be saved
                 in binary id97 format, otherwise - will be saved in
                 plain text.

        similar_by_vector(**kwargs)[88]  
                deprecated, use self.wv.similar_by_vector() instead.

                refer to the documentation for [89]similar_by_vector().

        similar_by_word(**kwargs)[90]  
                deprecated, use self.wv.similar_by_word() instead.

                refer to the documentation for [91]similar_by_word().

        similarity(**kwargs)[92]  
                deprecated, use self.wv.similarity() instead.

                refer to the documentation for [93]similarity().

        syn0_lockf[94]  

        syn1[95]  

        syn1neg[96]  

        train(documents=none, corpus_file=none, total_examples=none,
                total_words=none, epochs=none, start_alpha=none,
                end_alpha=none, word_count=0, queue_factor=2,
                report_delay=1.0, callbacks=())[97]  
                update the model   s neural weights.

                to support linear learning-rate decay from (initial) alpha
                to min_alpha, and accurate progress-percentage logging,
                either total_examples (count of documents) or total_words
                (count of raw words in documents) must be provided. if
                documents is the same corpus that was provided to
                [98]build_vocab() earlier, you can simply use
                total_examples=self.corpus_count.

                to avoid common mistakes around the model   s ability to do
                multiple training passes itself, an explicit epochs
                argument must be provided. in the common and recommended
                case where [99]train() is only called once, you can set
                epochs=self.iter.

   parameters:
               o documents (iterable of list of [100]taggeddocument,
                 optional)     can be simply a list of elements, but for
                 larger corpora,consider an iterable that streams the
                 documents directly from disk/network. if you don   t supply
                 documents (or corpus_file), the model is left
                 uninitialized     use if you plan to initialize it in some
                 other way.
               o corpus_file (str, optional)     path to a corpus file in
                 [101]linesentence format. you may use this argument
                 instead of documents to get performance boost. only one
                 of documents or corpus_file arguments need to be passed
                 (not both of them). documents    tags are assigned
                 automatically and are equal to line number, as in
                 [102]taggedlinedocument.
               o total_examples (int, optional)     count of documents.
               o total_words (int, optional)     count of raw words in
                 documents.
               o epochs (int, optional)     number of iterations (epochs)
                 over the corpus.
               o start_alpha (float, optional)     initial learning rate. if
                 supplied, replaces the starting alpha from the
                 constructor, for this one call to train. use only if
                 making multiple calls to train, when you want to manage
                 the alpha learning-rate yourself (not recommended).
               o end_alpha (float, optional)     final learning rate. drops
                 linearly from start_alpha. if supplied, this replaces the
                 final min_alpha from the constructor, for this one call
                 to [103]train(). use only if making multiple calls to
                 [104]train(), when you want to manage the alpha
                 learning-rate yourself (not recommended).
               o word_count (int, optional)     count of words already
                 trained. set this to 0 for the usual case of training on
                 all words in documents.
               o queue_factor (int, optional)     multiplier for size of
                 queue (number of workers * queue_factor).
               o report_delay (float, optional)     seconds to wait before
                 reporting progress.
               o callbacks     list of callbacks that need to be
                 executed/run at specific stages during training.

        wmdistance(**kwargs)[105]  
                deprecated, use self.wv.wmdistance() instead.

                refer to the documentation for [106]wmdistance().

   class gensim.models.doc2vec.doc2vectrainables(dm=1, dm_concat=0,
          dm_tag_count=1, vector_size=100, seed=1, hashfxn=<built-in
          function hash>, window=5)[107]  
          bases: [108]gensim.models.id97.id97trainables

          represents the inner shallow neural network used to train
          [109]doc2vec.

        get_doctag_trainables(doc_words, vector_size)[110]  

        classmethod load(fname, mmap=none)[111]  
                load an object previously saved using [112]save() from a
                file.

   parameters:
               o fname (str)     path to file that contains needed object.
               o mmap (str, optional)     memory-map option. if the object
                 was saved with large arrays stored separately, you can
                 load these arrays via mmap (shared memory) using
                 mmap=   r   . if the file being loaded is compressed (either
                    .gz    or    .bz2   ), then `mmap=none must be set.

                see also

              [113]save()
                      save object to file.

   returns: object loaded from fname.
   return type: object
   raises: attributeerror     when called on an object instance instead of
   class (this is a class method).

        prepare_weights(hs, negative, wv, docvecs, update=false)[114]  
                build tables and model weights based on final vocabulary
                settings.

        reset_doc_weights(docvecs)[115]  

        reset_weights(hs, negative, wv, docvecs, vocabulary=none)[116]  
                reset all projection weights to an initial (untrained)
                state, but keep the existing vocabulary.

        save(fname_or_handle, separately=none, sep_limit=10485760,
                ignore=frozenset([]), pickle_protocol=2)[117]  
                save the object to a file.

   parameters:
               o fname_or_handle (str or file-like)     path to output file
                 or already opened file-like object. if the object is a
                 file handle, no special array handling will be performed,
                 all attributes will be saved to the same file.
               o separately (list of str or none, optional)    
                 if none, automatically detect large numpy/scipy.sparse
                 arrays in the object being stored, and store them into
                 separate files. this prevent memory errors for large
                 objects, and also allows [118]memory-mapping the large
                 arrays for efficient loading and sharing the large arrays
                 in ram between multiple processes.
                 if list of str: store these attributes into separate
                 files. the automated size check is not performed in this
                 case.
               o sep_limit (int, optional)     don   t store arrays smaller
                 than this separately. in bytes.
               o ignore (frozenset of str, optional)     attributes that
                 shouldn   t be stored at all.
               o pickle_protocol (int, optional)     protocol number for
                 pickle.

                see also

              [119]load()
                      load object from file.

        seeded_vector(seed_string, vector_size)[120]  
                get a random vector (but deterministic by seed_string).

        update_weights(hs, negative, wv)[121]  
                copy all the existing weights, and reset the weights for
                the newly added vocabulary.

   class gensim.models.doc2vec.doc2vecvocab(max_vocab_size=none,
          min_count=5, sample=0.001, sorted_vocab=true, null_word=0,
          ns_exponent=0.75)[122]  
          bases: [123]gensim.models.id97.id97vocab

          vocabulary used by [124]doc2vec.

          this includes a mapping from words found in the corpus to their
          total frequency count.

   parameters:
          + max_vocab_size (int, optional)     maximum number of words in
            the vocabulary. used to limit the ram during vocabulary
            building; if there are more unique words than this, then prune
            the infrequent ones. every 10 million word types need about
            1gb of ram, set to none for no limit.
          + min_count (int)     words with frequency lower than this limit
            will be discarded from the vocabulary.
          + sample (float, optional)     the threshold for configuring which
            higher-frequency words are randomly downsampled, useful range
            is (0, 1e-5).
          + sorted_vocab (bool)     if true, sort the vocabulary by
            descending frequency before assigning word indexes.
          + null_word ({0, 1})     if true, a null pseudo-word will be
            created for padding when using concatenative l1
            (run-of-words). this word is only ever input     never predicted
                so count, huffman-point, etc doesn   t matter.
          + ns_exponent (float, optional)     the exponent used to shape the
            negative sampling distribution. a value of 1.0 samples exactly
            in proportion to the frequencies, 0.0 samples all words
            equally, while a negative value samples low-frequency words
            more than high-frequency words. the popular default value of
            0.75 was chosen by the original id97 paper. more recently,
            in [125]https://arxiv.org/abs/1804.04212, caselles-dupr  ,
            lesaint, & royo-letelier suggest that other values may perform
            better for recommendation applications.

        add_null_word(wv)[126]  

        create_binary_tree(wv)[127]  
                create a [128]binary huffman tree using stored vocabulary
                word counts. frequent words will have shorter binary
                codes. called internally from build_vocab().

        indexed_doctags(doctag_tokens, docvecs)[129]  
                get the indexes and backing-arrays used in training
                examples.

   parameters:
               o doctag_tokens (list of {str, int})     a list of tags for
                 which we want the index.
               o docvecs (list of [130]doc2veckeyedvectors)     vector
                 representations of the documents in the corpus. each
                 vector has size == vector_size

                  returns:

   indices of the provided tag keys.
                return type:

   list of int

        classmethod load(fname, mmap=none)[131]  
                load an object previously saved using [132]save() from a
                file.

   parameters:
               o fname (str)     path to file that contains needed object.
               o mmap (str, optional)     memory-map option. if the object
                 was saved with large arrays stored separately, you can
                 load these arrays via mmap (shared memory) using
                 mmap=   r   . if the file being loaded is compressed (either
                    .gz    or    .bz2   ), then `mmap=none must be set.

                see also

              [133]save()
                      save object to file.

   returns: object loaded from fname.
   return type: object
   raises: attributeerror     when called on an object instance instead of
   class (this is a class method).

        make_cum_table(wv, domain=2147483647)[134]  
                create a cumulative-distribution table using stored
                vocabulary word counts for drawing random words in the
                negative-sampling training routines.

                to draw a word index, choose a random integer up to the
                maximum value in the table (cum_table[-1]), then finding
                that integer   s sorted insertion point (as if by
                bisect_left or ndarray.searchsorted()). that insertion
                point is the drawn index, coming up in proportion equal to
                the increment at that slot.

                called internally from build_vocab().

        prepare_vocab(hs, negative, wv, update=false,
                keep_raw_vocab=false, trim_rule=none, min_count=none,
                sample=none, dry_run=false)[135]  
                apply vocabulary settings for min_count (discarding
                less-frequent words) and sample (controlling the
                downsampling of more-frequent words).

                calling with dry_run=true will only simulate the provided
                settings and report the size of the retained vocabulary,
                effective corpus length, and estimated memory
                requirements. results are both printed via logging and
                returned as a dict.

                delete the raw vocabulary after the scaling is done to
                free up ram, unless keep_raw_vocab is set.

        save(fname_or_handle, separately=none, sep_limit=10485760,
                ignore=frozenset([]), pickle_protocol=2)[136]  
                save the object to a file.

   parameters:
               o fname_or_handle (str or file-like)     path to output file
                 or already opened file-like object. if the object is a
                 file handle, no special array handling will be performed,
                 all attributes will be saved to the same file.
               o separately (list of str or none, optional)    
                 if none, automatically detect large numpy/scipy.sparse
                 arrays in the object being stored, and store them into
                 separate files. this prevent memory errors for large
                 objects, and also allows [137]memory-mapping the large
                 arrays for efficient loading and sharing the large arrays
                 in ram between multiple processes.
                 if list of str: store these attributes into separate
                 files. the automated size check is not performed in this
                 case.
               o sep_limit (int, optional)     don   t store arrays smaller
                 than this separately. in bytes.
               o ignore (frozenset of str, optional)     attributes that
                 shouldn   t be stored at all.
               o pickle_protocol (int, optional)     protocol number for
                 pickle.

                see also

              [138]load()
                      load object from file.

        scan_vocab(documents=none, corpus_file=none, docvecs=none,
                progress_per=10000, trim_rule=none)[139]  
                create the models vocabulary: a mapping from unique words
                in the corpus to their frequency count.

   parameters:
               o documents (iterable of [140]taggeddocument, optional)    
                 the tagged documents used to create the vocabulary. their
                 tags can be either str tokens or ints (faster).
               o corpus_file (str, optional)     path to a corpus file in
                 [141]linesentence format. you may use this argument
                 instead of documents to get performance boost. only one
                 of documents or corpus_file arguments need to be passed
                 (not both of them).
               o docvecs (list of [142]doc2veckeyedvectors)     the vector
                 representations of the documents in our corpus. each of
                 them has a size == vector_size.
               o progress_per (int)     progress will be logged every
                 progress_per documents.
               o trim_rule (function, optional)    
                 vocabulary trimming rule, specifies whether certain words
                 should remain in the vocabulary, be trimmed away, or
                 handled using the default (discard if word count <
                 min_count). can be none (min_count will be used, look to
                 [143]keep_vocab_item()), or a callable that accepts
                 parameters (word, count, min_count) and returns either
                 gensim.utils.rule_discard, gensim.utils.rule_keep or
                 gensim.utils.rule_default. the rule, if given, is only
                 used to prune vocabulary during [144]build_vocab() and is
                 not stored as part of the model.

                    the input parameters are of the following types:

                         @ word (str) - the word we are examining
                         @ count (int) - the word   s frequency count in the
                           corpus
                         @ min_count (int) - the minimum count threshold.

                  returns:

   tuple of (total words in the corpus, number of documents)
                return type:

   (int, int)

        sort_vocab(wv)[145]  
                sort the vocabulary so the most frequent words have the
                lowest indexes.

   class gensim.models.doc2vec.doctag[146]  
          bases: [147]gensim.models.doc2vec.doctag

          a string document tag discovered during the initial vocabulary
          scan. the document-vector equivalent of a vocab object.

          will not be used if all presented document tags are ints.

          the offset is only the true index into the
          doctags_syn0/doctags_syn0_lockf if-and-only-if no raw-int tags
          were used. if any raw-int tags were used, string [148]doctag
          vectors begin at index (max_rawint + 1), so the true index is
          (rawint_index + 1 + offset).

          see also

          _index_to_doctag()

          create new instance of doctag(offset, word_count, doc_count)

        count(value)     integer -- return number of occurrences of
                value[149]  

        doc_count[150]  
                alias for field number 2

        index(value[, start[, stop]])     integer -- return first index of
                value.[151]  
                raises valueerror if the value is not present.

        offset[152]  
                alias for field number 0

        repeat(word_count)[153]  

        word_count[154]  
                alias for field number 1

   gensim.models.doc2vec.labeledsentence(*args, **kwargs)[155]  
          deprecated, use [156]taggeddocument instead.

   class gensim.models.doc2vec.taggedbrowncorpus(dirname)[157]  
          bases: object

          reader for the [158]brown corpus (part of nltk data).

          parameters: dirname (str)     path to folder with brown corpus.

   class gensim.models.doc2vec.taggeddocument[159]  
          bases: [160]gensim.models.doc2vec.taggeddocument

          represents a document along with a tag, input document format
          for [161]doc2vec.

          a single document, made up of words (a list of unicode string
          tokens) and tags (a list of tokens). tags may be one or more
          unicode string tokens, but typical practice (which will also be
          the most memory-efficient) is for the tags list to include a
          unique integer id as the only tag.

          replaces    sentence as a list of words    from
          [162]gensim.models.id97.id97.

          create new instance of taggeddocument(words, tags)

        count(value)     integer -- return number of occurrences of
                value[163]  

        index(value[, start[, stop]])     integer -- return first index of
                value.[164]  
                raises valueerror if the value is not present.

        tags[165]  
                alias for field number 1

        words[166]  
                alias for field number 0

   class gensim.models.doc2vec.taggedlinedocument(source)[167]  
          bases: object

          iterate over a file that contains documents: one line =
          [168]taggeddocument object.

          words are expected to be already preprocessed and separated by
          whitespace. document tags are constructed automatically from the
          document line number (each document gets a unique integer tag).

   parameters: source (string or a file-like object)     path to the file on
   disk, or an already-open file object (must support seek(0)).

          examples

>>> from gensim.test.utils import datapath
>>> from gensim.models.doc2vec import taggedlinedocument
>>>
>>> for document in taggedlinedocument(datapath("head500.noblanks.cor")):
...     pass

   smaller gensim logo [169]gensim footer image
      copyright 2009-now, [170]radim   eh    ek
   last updated on jan 31, 2019.
     * [171]home
     * |
     * [172]tutorials
     * |
     * [173]install
     * |
     * [174]support
     * |
     * [175]api
     * |
     * [176]about

   [177]tweet @gensim_py
   support:
   [178]stay informed via gensim mailing list:
   ____________________________ subscribe

references

   1. https://www.googletagmanager.com/ns.html?id=gtm-mp366cc
   2. https://radimrehurek.com/gensim/index.html
   3. https://rare-technologies.com/
   4. https://scaletext.com/
   5. https://rare-technologies.com/corporate-training/
   6. https://radimrehurek.com/gensim/index.html
   7. https://radimrehurek.com/gensim/tutorial.html
   8. https://radimrehurek.com/gensim/install.html
   9. https://radimrehurek.com/gensim/support.html
  10. https://radimrehurek.com/gensim/apiref.html
  11. https://radimrehurek.com/gensim/about.html
  12. https://radimrehurek.com/gensim/models/doc2vec.html#module-gensim.models.doc2vec
  13. https://arxiv.org/pdf/1405.4053v2.pdf
  14. https://arxiv.org/pdf/1301.3781.pdf
  15. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  16. https://github.com/rare-technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb
  17. https://rare-technologies.com/parallelizing-id97-in-python/
  18. https://radimrehurek.com/gensim/models/doc2vec.html#usage-examples
  19. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec
  20. https://radimrehurek.com/gensim/models/base_any2vec.html#gensim.models.base_any2vec.basewordembeddingsmodel
  21. https://arxiv.org/abs/1405.4053v2
  22. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.wv
  23. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.id97keyedvectors
  24. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.docvecs
  25. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
  26. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
  27. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec
  28. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.doc2veckeyedvectors
  29. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.vocabulary
  30. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vecvocab
  31. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.trainables
  32. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
  33. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vectrainables
  34. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument
  35. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
  36. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggedlinedocument
  37. https://arxiv.org/abs/1804.04212
  38. https://radimrehurek.com/gensim/utils.html#gensim.utils.keep_vocab_item
  39. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.build_vocab
  40. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument
  41. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument
  42. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggedbrowncorpus
  43. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggedlinedocument
  44. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
  45. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggedlinedocument
  46. https://radimrehurek.com/gensim/utils.html#gensim.utils.keep_vocab_item
  47. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.build_vocab_from_freq
  48. https://radimrehurek.com/gensim/utils.html#gensim.utils.keep_vocab_item
  49. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.build_vocab
  50. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.clear_sims
  51. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.cum_table
  52. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.dbow
  53. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.delete_temporary_training_data
  54. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.doc2veckeyedvectors.most_similar
  55. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.doc2veckeyedvectors.similarity
  56. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.infer_vector
  57. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.dm
  58. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.doesnt_match
  59. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.doesnt_match
  60. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.estimate_memory
  61. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.estimated_lookup_memory
  62. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.evaluate_word_pairs
  63. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.evaluate_word_pairs
  64. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.hashfxn
  65. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.infer_vector
  66. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.init_sims
  67. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.iter
  68. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.layer1_size
  69. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.load
  70. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec
  71. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.save
  72. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec
  73. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec
  74. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.min_count
  75. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.most_similar
  76. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.most_similar
  77. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.most_similar_cosmul
  78. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.most_similar_cosmul
  79. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.n_similarity
  80. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.n_similarity
  81. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.reset_from
  82. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec
  83. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.sample
  84. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.save
  85. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.save
  86. https://radimrehurek.com/gensim/models/base_any2vec.html#gensim.models.base_any2vec.baseany2vecmodel.load
  87. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.save_id97_format
  88. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.similar_by_vector
  89. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.similar_by_vector
  90. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.similar_by_word
  91. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.similar_by_word
  92. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.similarity
  93. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.similarity
  94. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.syn0_lockf
  95. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.syn1
  96. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.syn1neg
  97. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.train
  98. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.build_vocab
  99. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.train
 100. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument
 101. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
 102. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggedlinedocument
 103. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.train
 104. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.train
 105. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.wmdistance
 106. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.wordembeddingskeyedvectors.wmdistance
 107. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vectrainables
 108. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97trainables
 109. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec
 110. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vectrainables.get_doctag_trainables
 111. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vectrainables.load
 112. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.save
 113. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.save
 114. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vectrainables.prepare_weights
 115. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vectrainables.reset_doc_weights
 116. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vectrainables.reset_weights
 117. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vectrainables.save
 118. https://en.wikipedia.org/wiki/mmap
 119. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
 120. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vectrainables.seeded_vector
 121. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vectrainables.update_weights
 122. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vecvocab
 123. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97vocab
 124. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec
 125. https://arxiv.org/abs/1804.04212
 126. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vecvocab.add_null_word
 127. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vecvocab.create_binary_tree
 128. https://en.wikipedia.org/wiki/huffman_coding
 129. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vecvocab.indexed_doctags
 130. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.doc2veckeyedvectors
 131. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vecvocab.load
 132. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.save
 133. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.save
 134. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vecvocab.make_cum_table
 135. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vecvocab.prepare_vocab
 136. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vecvocab.save
 137. https://en.wikipedia.org/wiki/mmap
 138. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
 139. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vecvocab.scan_vocab
 140. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument
 141. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
 142. https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.doc2veckeyedvectors
 143. https://radimrehurek.com/gensim/utils.html#gensim.utils.keep_vocab_item
 144. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec.build_vocab
 145. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vecvocab.sort_vocab
 146. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doctag
 147. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doctag
 148. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doctag
 149. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doctag.count
 150. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doctag.doc_count
 151. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doctag.index
 152. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doctag.offset
 153. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doctag.repeat
 154. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doctag.word_count
 155. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.labeledsentence
 156. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument
 157. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggedbrowncorpus
 158. http://www.nltk.org/book/ch02.html#tab-brown-sources
 159. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument
 160. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument
 161. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.doc2vec
 162. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
 163. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument.count
 164. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument.index
 165. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument.tags
 166. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument.words
 167. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggedlinedocument
 168. https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.taggeddocument
 169. https://radimrehurek.com/gensim/index.html
 170. https://radimrehurek.com/cdn-cgi/l/email-protection#f0829194999d8295988582959bb083958a9e919dde938a
 171. https://radimrehurek.com/gensim/index.html
 172. https://radimrehurek.com/gensim/tutorial.html
 173. https://radimrehurek.com/gensim/install.html
 174. https://radimrehurek.com/gensim/support.html
 175. https://radimrehurek.com/gensim/apiref.html
 176. https://radimrehurek.com/gensim/about.html
 177. https://twitter.com/gensim_py
 178. https://groups.google.com/group/gensim
