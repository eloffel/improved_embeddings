   #[1]github [2]recent commits to id4.matlab:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]23
     * [35]star [36]90
     * [37]fork [38]49

[39]lmthang/[40]id4.matlab

   [41]code [42]issues 0 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   code to train state-of-the-art id4 systems.
   [47]http://nlp.stanford.edu/projects/id4/
     * [48]583 commits
     * [49]1 branch
     * [50]0 releases
     * [51]fetching contributors

    1. [52]matlab 52.5%
    2. [53]perl 25.5%
    3. [54]python 17.2%
    4. [55]shell 4.2%
    5. [56]smalltalk 0.6%

   (button) matlab perl python shell smalltalk
   branch: master (button) new pull request
   [57]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/l
   [58]download zip

downloading...

   want to be notified of new releases in lmthang/id4.matlab?
   [59]sign in [60]sign up

launching github desktop...

   if nothing happens, [61]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [62]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [63]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [64]download the github extension for visual studio
   and try again.

   (button) go back
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [65]permalink
   type        name       latest commit message commit time
        failed to load latest commit information.
        [66]code
        [67]data
        [68]scripts
        [69].gitignore
        [70]readme.md
        [71]git_commit.sh

readme.md

effective approaches to attention-based id4sm

   code to train id4 systems as described in our
   emnlp paper [72]effective approaches to attention-based neural machine
   translation.

features:

     * multi-layer bilingual encoder-decoder models: gpu-enabled.
     * attention mechanisms: global and local models.
     * beam-search decoder: can decode multiple models.
     * other features: dropout, train monolingual language models.
     * end-to-end pipeline: scripts to preprocess, compute evaluation
       scores.

citations:

   if you make use of this code in your research, please cite our paper
@inproceedings{luong-pham-manning:2015:emnlp,
  author    = {luong, minh-thang  and  pham, hieu  and  manning, christopher d.}
,
  title     = {effective approaches to attention-based neural machine translatio
n},
  booktitle = {proceedings of the 2015 conference on empirical methods in natura
l language processing},
  year      = {2015},
}

     * thang luong [73]lmthang@stanford.edu, 2014, 2015
     * with contributions from: hieu pham [74]hyhieu@stanford.edu --
       beam-search decoder.

files

readme.md       - this file
code/           - main matlab code
  trainlstm.m: train models
  testlstm.m: decode models
data/           - toy data
scripts/        - utility scripts

   the code directory further divides into sub-directories:
  basic/: define basic functions like sigmoid, prime. it also has an efficient w
ay to aggreate embeddings.
  layers/: we define various layers like attention, lstm, etc.
  misc/: things that we haven't categorized yet.
  preprocess/: deal with data.
  print/: print results, logs for debugging purposes.

examples

     * prepare the data

./scripts/run_prepare_data.sh ./data/train.10k.en ./data/valid.100.en ./data/tes
t.100.en 1000 ./output/id.1000
./scripts/run_prepare_data.sh ./data/train.10k.de ./data/valid.100.de ./data/tes
t.100.de 1000 ./output/id.1000

   here, we convert train/valid/test files in text format into integer
   format that can be handled efficiently in matlab. the script syntax is:
run_prepare_data.sh <trainfile> <validfile> <testfile> <vocabsize> <outdir>

     * train a bilingual, encoder-decoder model

   in matlab, go into the code/ directory and run:
trainlstm('../output/id.1000/train.10k', '../output/id.1000/valid.100', '../outp
ut/id.1000/test.100', 'de', 'en', '../output/id.1000/train.10k.de.vocab.1000', '
../output/id.1000/train.10k.en.vocab.1000', '../output/basic', 'isresume', 0)

   this trains a very basic model with all the default settings. we set
   'isresume' to 0 so that it will train a new model each time you run the
   command instead of loading existing models. see trainlstm.m for more.

   (to run directly from your terminal, checkout scripts/train.sh)

   the syntax is:
trainlstm(trainprefix,validprefix,testprefix,srclang,tgtlang,srcvocabfile,tgtvoc
abfile,outdir,varargin)
% arguments:
%   trainprefix, validprefix, testprefix: expect files trainprefix.srclang,
%     trainprefix.tgtlang. similarly for validprefix and testprefix.
%     these data files contain sequences of integers one per line.
%   srclang, tgtlang: languages, e.g. en, de.
%   srcvocabfile, tgtvocabfile: one word per line.
%   outdir: output directory.
%   varargin: other optional arguments.

   the trainer code outputs logs such as:
1, 20, 6.38k, 1, 6.52, gn=11.62

   which means: at epoch 1, mini-batches 20, training speed is 6.38k
   words/s, learning rate is 1, train cost is 6.52, and grad norm is
   11.62.

   once in a while, the code will evaluate on the valid and test sets:
# eval 34.40, 2, 144, 6.19k, 1.00, train=4.76, valid=3.63, test=3.54, w_src{1}=0
.050 w_tgt{1}=0.081 w_emb_src=0.050 w_emb_tgt=0.056 w_soft=0.076, time=0.57s

   which tells us additional information such as the current test
   perplexity, 34.40, the valid / test costs, and the average abs values
   of the model paramters.
     * decode

testlstm('../output/basic/modelrecent.mat', 2, 10, 1, '../output/basic/translati
ons.txt')

   decode with beamsize 2, collect maximum 10 translations, batchsize 1.

   note that the testlstm implicitly decodes the test file specified
   during training. to specifiy a different test file, use 'testprefix',
   see testlstm.m for more.
testlstm('../output/basic/modelrecent.mat', 2, 10, 1, '../output/basic/translati
ons.txt', 'testprefix', '../output/id.1000/valid.100')

   (to run directly from your terminal, checkout scripts/test.sh)

   syntax:
testlstm(modelfiles, beamsize, stacksize, batchsize, outputfile,varargin)
% arguments:
%   modelfiles: single or multiple models to decode. multiple models are
%     separated by commas.
%   beamsize: number of hypotheses kept at each time step.
%   stacksize: number of translations retrieved.
%   batchsize: number of sentences decoded simultaneously. we only ensure
%     accuracy of batchsize = 1 for now.
%   outputfile: output translation file.
%   varargin: other optional arguments.

     * grad check

trainlstm('', '', '', '', '', '', '', '../output/gradcheck', 'isgradcheck', 1)

advanced examples

     * profiling

trainlstm('../output/id.1000/train.10k', '../output/id.1000/valid.100', '../outp
ut/id.1000/test.100', 'de', 'en', '../output/id.1000/train.10k.de.vocab.1000', '
../output/id.1000/train.10k.en.vocab.1000', '../output/basic', 'isprofile', 1, '
isresume', 0)

     * train a monolingual language model

trainlstm('../output/id.1000/train.10k', '../output/id.1000/valid.100','../outpu
t/id.1000/test.100', '', 'en', '','../output/id.1000/train.10k.en.vocab.1000', '
../output/mono', 'isbi', 0, 'isresume', 0)

     * train multi-layer model with dropout

trainlstm('../output/id.1000/train.10k', '../output/id.1000/valid.100', '../outp
ut/id.1000/test.100', 'de', 'en', '../output/id.1000/train.10k.de.vocab.1000', '
../output/id.1000/train.10k.en.vocab.1000', '../output/advanced', 'numlayers', 2
, 'lstmsize', 100, 'dropout', 0.8, 'isresume', 0)

   we train a 2 stacking lstm layers with 100 lstm cells and 100-dim
   embeddings. here, 0.8 means the dropout "keep" id203.
     * more control of hyperparameters

trainlstm('../output/id.1000/train.10k', '../output/id.1000/valid.100', '../outp
ut/id.1000/test.100', 'de', 'en', '../output/id.1000/train.10k.de.vocab.1000', '
../output/id.1000/train.10k.en.vocab.1000', '../output/hypers', 'learningrate',
1.0, 'maxgradnorm', 5, 'initrange', 0.1, 'batchsize', 128, 'numepoches', 10, 'fi
netuneepoch', 5, 'isresume', 0)

   apart from "obvious" hyperparameters, we want to scale our gradients
   whenever its norm averaged by batch size (128) is greater than 5. after
   training for 5 epochs, we start halving our learning rate each epoch.
   to have further control of learning rate schedule, see 'epochfraction'
   and 'finetunerate' options in trainlstm.m
     * train attention model:

trainlstm('../output/id.1000/train.10k', '../output/id.1000/valid.100', '../outp
ut/id.1000/test.100', 'de', 'en', '../output/id.1000/train.10k.de.vocab.1000', '
../output/id.1000/train.10k.en.vocab.1000', '../output/attn', 'attnfunc', 1, 'at
tnopt', 1, 'isreverse', 1, 'feedinput', 1, 'isresume', 0)

   here, we also use source reversing 'isreverse' and the input feeding
   approach 'feedinput' as described in the paper. other attention
   architectures can be specified as follows:
  % attnfunc=0: no attention.
  %          1: global attention
  %          2: local attention + monotonic alignments
  %          4: local attention  + regression for absolute pos (multiplied distw
eights)
  % attnopt: decide how we generate the alignment weights:
  %          0: location-based
  %          1: content-based, dot product
  %          2: content-based, general dot product
  %          3: content-based, concat montreal style

   'isresume' is set to 0 to avoid loading existing models (done by
   default), so that you can try different attention architectures.
     * more grad checks:

./scripts/run_grad_checks.sh > output/grad_checks.txt 2>&1

   then compare with the provided grad check outputs data/grad_checks.txt.
   they should look similar.

   note: many different configurations will be run with the
   run_grad_checks.sh script. for many configuration, we set the
   'initrange' to a large value 10, so you will notice the total gradient
   differences are large. this is to debug subtle mistakes; and if the
   total diff < 10, you can mostly be assured. we do note that with
   attnfunc=4, attnopt=1, the diff is quite large; this is something to be
   checked though the model seems to work in practice.

     *    2019 github, inc.
     * [75]terms
     * [76]privacy
     * [77]security
     * [78]status
     * [79]help

     * [80]contact github
     * [81]pricing
     * [82]api
     * [83]training
     * [84]blog
     * [85]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [86]reload to refresh your
   session. you signed out in another tab or window. [87]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/lmthang/id4.matlab/commits/master.atom
   3. https://github.com/lmthang/id4.matlab#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/lmthang/id4.matlab
  32. https://github.com/join
  33. https://github.com/login?return_to=/lmthang/id4.matlab
  34. https://github.com/lmthang/id4.matlab/watchers
  35. https://github.com/login?return_to=/lmthang/id4.matlab
  36. https://github.com/lmthang/id4.matlab/stargazers
  37. https://github.com/login?return_to=/lmthang/id4.matlab
  38. https://github.com/lmthang/id4.matlab/network/members
  39. https://github.com/lmthang
  40. https://github.com/lmthang/id4.matlab
  41. https://github.com/lmthang/id4.matlab
  42. https://github.com/lmthang/id4.matlab/issues
  43. https://github.com/lmthang/id4.matlab/pulls
  44. https://github.com/lmthang/id4.matlab/projects
  45. https://github.com/lmthang/id4.matlab/pulse
  46. https://github.com/join?source=prompt-code
  47. http://nlp.stanford.edu/projects/id4/
  48. https://github.com/lmthang/id4.matlab/commits/master
  49. https://github.com/lmthang/id4.matlab/branches
  50. https://github.com/lmthang/id4.matlab/releases
  51. https://github.com/lmthang/id4.matlab/graphs/contributors
  52. https://github.com/lmthang/id4.matlab/search?l=matlab
  53. https://github.com/lmthang/id4.matlab/search?l=perl
  54. https://github.com/lmthang/id4.matlab/search?l=python
  55. https://github.com/lmthang/id4.matlab/search?l=shell
  56. https://github.com/lmthang/id4.matlab/search?l=smalltalk
  57. https://github.com/lmthang/id4.matlab/find/master
  58. https://github.com/lmthang/id4.matlab/archive/master.zip
  59. https://github.com/login?return_to=https://github.com/lmthang/id4.matlab
  60. https://github.com/join?return_to=/lmthang/id4.matlab
  61. https://desktop.github.com/
  62. https://desktop.github.com/
  63. https://developer.apple.com/xcode/
  64. https://visualstudio.github.com/
  65. https://github.com/lmthang/id4.matlab/tree/32f8564e39d4a3d30fa57038e44f4a3dbdc2068c
  66. https://github.com/lmthang/id4.matlab/tree/master/code
  67. https://github.com/lmthang/id4.matlab/tree/master/data
  68. https://github.com/lmthang/id4.matlab/tree/master/scripts
  69. https://github.com/lmthang/id4.matlab/blob/master/.gitignore
  70. https://github.com/lmthang/id4.matlab/blob/master/readme.md
  71. https://github.com/lmthang/id4.matlab/blob/master/git_commit.sh
  72. https://aclweb.org/anthology/d/d15/d15-1166.pdf
  73. mailto:lmthang@stanford.edu
  74. mailto:hyhieu@stanford.edu
  75. https://github.com/site/terms
  76. https://github.com/site/privacy
  77. https://github.com/security
  78. https://githubstatus.com/
  79. https://help.github.com/
  80. https://github.com/contact
  81. https://github.com/pricing
  82. https://developer.github.com/
  83. https://training.github.com/
  84. https://github.blog/
  85. https://github.com/about
  86. https://github.com/lmthang/id4.matlab
  87. https://github.com/lmthang/id4.matlab

   hidden links:
  89. https://github.com/
  90. https://github.com/lmthang/id4.matlab
  91. https://github.com/lmthang/id4.matlab
  92. https://github.com/lmthang/id4.matlab
  93. https://help.github.com/articles/which-remote-url-should-i-use
  94. https://github.com/lmthang/id4.matlab#effective-approaches-to-attention-based-neural-machine-translationsm
  95. https://github.com/lmthang/id4.matlab#features
  96. https://github.com/lmthang/id4.matlab#citations
  97. https://github.com/lmthang/id4.matlab#files
  98. https://github.com/lmthang/id4.matlab#examples
  99. https://github.com/lmthang/id4.matlab#advanced-examples
 100. https://github.com/
