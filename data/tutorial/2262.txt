   #[1]github [2]recent commits to id4-tips:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]27
     * [35]star [36]323
     * [37]fork [38]73

[39]neubig/[40]id4-tips

   [41]code [42]issues 1 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   a tutorial about id4 including tips on building
   practical systems
     * [47]12 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors

    1. [51]perl 93.6%
    2. [52]shell 6.4%

   (button) perl shell
   branch: master (button) new pull request
   [53]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/n
   [54]download zip

downloading...

   want to be notified of new releases in neubig/id4-tips?
   [55]sign in [56]sign up

launching github desktop...

   if nothing happens, [57]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [59]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [60]download the github extension for visual studio
   and try again.

   (button) go back
   [61]@neubig
   [62]neubig [63]fixed literal tab in readme
   latest commit [64]c81d8b4 nov 16, 2016
   [65]permalink
   type      name      latest commit message commit time
        failed to load latest commit information.
        [66]data
        [67]scripts
        [68].gitignore
        [69]readme.md

readme.md

tips on building id4 systems

   by [70]graham neubig (nara institute of science and technology/carnegie
   mellon university)

   this tutorial will explain some practical tips about how to train a
   id4 system. it is partly based around examples
   using the [71]lamtram toolkit. note that this will not cover the theory
   behind id4 in detail, nor is it a survey meant to cover all the work on
   neural mt, but it will show you how to use lamtram, and also
   demonstrate some things that you have to do in order to make a system
   that actually works well (focusing on ones that are implemented in my
   toolkit).

   this tutorial will assume that you have already installed lamtram (and
   the [72]dynet backend library that it depends on) on linux or mac.
   then, use git to pull this tutorial and the corresponding data.
git clone http://github.com/neubig/id4-tips

   the data in the data/ directory is japanese-english data that i have
   prepared doing some language-specific preprocessing (id121,
   lowercasing, etc.). enter the id4-tips directory
cd id4-tips

   and make a link to the directory in which you installed lamtram:
ln -s /full/path/to/lamtram/directory lamtram

machine translation

   machine translation is a method for translating from a source sequence
   f with words f_1, ..., f_j to a target sequence e with words e_1, ...,
   e_i. this usually means that we translate between a sentence in a
   source language (e.g. japanese) to a sentence in a target language
   (e.g. english). machine translation can be used for other applications
   as well.

   in recent years, the most prominent method is statistical machine
   translation (smt; brown et al. (1993)), which builds a probabilistic
   model of the target sequence given the source sequence p(e|f). this
   probabilistic model is trained using a large set of training data
   containing pairs of source and target sequences.

   a good resource on machine translation in general, including a number
   of more traditional (non-neural) methods is koehn (2009)'s book
   "id151".

id4 (id4) and encoder-decoder models

   id4 is a particular variety of smt that learns
   the probabilistic model p(e|f) using neural networks. i will assume
   that readers already know basic concepts about neural networks: what a
   neural network is, particularly what a recurrent neural network is, and
   how they are trained. if you don't, a good tutorial is goldberg
   (2015)'s primer on neural networks for natural language processing.

   encoder-decoder models (kalchbrenner & blunsom 2013, sutskever et al.
   2014) are the simplest version of id4. the idea is relatively simple:
   we read in the words of a target sentence one-by-one using a recurrent
   neural network, then predict the words in the target sentence.

   first, we encode the source sentence. to do so, we convert the source
   word into a fixed-length word representation, parameterized by   _wr:
wf_j = wordrep(f_j;   _fwr)

   then, we map this into a hidden state using a recurrent neural network,
   parameterized by   _fid56. we assume h_0 is a zero vector.
h_j = id56(h_{j-1}, wf_j;   _fid56)

   it is also common to generate h_j using bidirectional neural networks,
   where we run one forward id56 that reads from left-to-right, and another
   backward id56 that reads from right to left, then concatenate the
   representations for each word. this is the default setting in lamtram
   (specified by --encoder_types "for|rev").

   next, we decode to generate the target sentence, one word at a time.
   this is done by initializing the first hidden state of the decoder g_0
   to be equal to the last hidden state of the encoder: g_0 = h_j. next,
   we generate a word in the output by performing a softmax over the
   target vocabulary to predict the id203 of each word in the
   output, parameterized by   _esm:
pe_i = softmax(g_{i-1};   _esm)

   we then pick the word that has highest id203:
e'_i = argmax_k(pe_i[k])

   we then update the hidden state with this predicted value:
we'_i = wordrep(e'_i;   _ewr)
g_i = id56(g_{i-1}, we'_i;   _eid56)

   this process is continued until a special "end of sentence" symbol is
   chosen for e'_i.

training id4 models with maximum likelihood

   note that the various elements of the model explained in the previous
   model have parameters   . these need to be learned in order to generate
   high-quality translations. the standard way of training neural networks
   is by using maximum likelihood. this is done by maximizing the log
   likelihood of the training data:
  ' = argmax_{  }(   _{e,f} log p(e|f;  ) )

   or equivalently minimizing the negative log likelihood:
  ' = argmin_{  }( -   _{e,f} log p(e|f;  ) )

   the standard way we do this minimization is through stochastic gradient
   descent (sgd), where we calculate the gradient of the negative log
   id203 for a single example <f,e>
   _{  } -log p(e|f;  )

   then update the parameters based on an update rule:
       update(  ,    _{  } -log p(e|f;  ))

   the most standard update rule simply subtracts the gradient of the
   negative log likelihood multiplied by a learning rate   
sgd_update(  ,    _{  } -log p(e|f;  ),   ) :=    -    *    _{  } -log p(e|f;  )

   let's try to do this with lamtram. first make a directory to hold the
   model:
mkdir models

   then train the model with the following commands:
lamtram/src/lamtram/lamtram-train \
  --model_type encdec \
  --train_src data/train.ja \
  --train_trg data/train.en \
  --trainer sgd \
  --learning_rate 0.1 \
  --rate_decay 1.0 \
  --epochs 10 \
  --model_out models/encdec.mod

   here, model_type indicates that we want to train an encoder-decoder,
   train_src and train_trg indicate the source and target training data
   files. trainer specifies that we will use the standard update rule, and
   learning_rate specifies   . rate_decay will be explained later. epochs
   is the number of passes over the training data, and model_out is the
   place where the model is written out to.

   if training is going well, we will be able to see the following log
   output:
epoch 1 sent 100: ppl=1122.07, unk=0, rate=0.1, time=2.04832 (502.852 w/s)
epoch 1 sent 200: ppl=737.81, unk=0, rate=0.1, time=4.08551 (500.305 w/s)
epoch 1 sent 300: ppl=570.027, unk=0, rate=0.1, time=6.07408 (501.311 w/s)
epoch 1 sent 400: ppl=523.924, unk=0, rate=0.1, time=8.23374 (502.566 w/s)

   ppl is reporting perplexity on the training set, which is equal to the
   exponent of the per-word negative log id203:
ppl(  ) = exp( -(  _{e,f} log p(e|f;  ))/(  _e |e|) )

   for this perplexity, lower is better, so if it's decreasing we're
   learning something.

   one thing you'll notice is that training is really slow... there are
   10,000 sentences in our small training corpus, but you're probably
   tired of waiting already. the next section will explain how we speed
   things up, so let's let it run for a while, and then when you get tired
   of waiting hit ctrl+c to stop training.

speeding up training

minibatches

   one powerful tool to speed up training of neural networks is
   mini-batching. the idea behind minibatching is that instead of
   calculating the gradient for a single example <e,f>
   _{  } log p(e|f;  )

   we calculate the gradients for multiple examples at one time
   _{  }   _{<e,f> in minibatch} log p(e|f;  )

   then perform the update of the model's parameters using this aggregated
   gradient. this has several advantages:
     * gradient updates take time, so if we have n sentences in our
       minibatch, we can perform n times fewer gradient updates.
     * more importantly, by sticking sentences together in a batch, we can
       share some of the calculations between them. for example, where
       non-minibatched neural networks might multiply the hidden vector
       h_i by a weight matrix w, when we are mini-batching we can connect
       h_i from different sentences into a single matrix h and do a big
       matrix-id127 w * h, which is much more efficient.
     * also, using mini-batches can make the updates to the parameters
       more stable, as information from multiple sentences is considered
       at one time.

   on the other hand, large minibatches do have disadvantages:
     * if our mini-batch sizes are too big, sometimes we may run out of
       memory by trying to store too many calculated values in memory at
       once.
     * while the calculation of each sentence becomes faster, because the
       total number of updates is fewer, sometimes training can be slower
       than when not using mini-batches.

   anyway, let's try this in lamtram by adding the --minibatch_size
   num_words option, where num_words is the number of words included in
   each mini-batch. if we set num_words to be equal to 256, and re-run the
   previous command, we get the following log:
epoch 1 sent 106: ppl=3970.52, unk=0, rate=0.1, time=0.526336 (2107.02 w/s)
epoch 1 sent 201: ppl=2645.1, unk=0, rate=0.1, time=1.00862 (2071.15 w/s)
epoch 1 sent 316: ppl=1905.16, unk=0, rate=0.1, time=1.48682 (2068.84 w/s)
epoch 1 sent 401: ppl=1574.61, unk=0, rate=0.1, time=1.82187 (2064.91 w/s)

   looking at the w/s (words per second) on the right side of the log, we
   can see that we're processing data 4 times faster than before, nice!
   let's still hit ctrl+c though, as we'll speed up training even more in
   the next section.

other update rules

   in addition to the standard sgd_update rule listed above, there are a
   myriad of additional ways to update the parameters, including "sgd with
   momentum", "adagrad", "adadelta", "rmsprop", "adam", and many others.
   explaining these in detail is beyond the scope of this tutorial, but it
   suffices to say that these will more quickly find a good place in
   parameter space than the standard method above. my current favorite
   optimization method is "adam" (kingma et al. 2014), which can be run by
   setting --trainer adam. we'll also have to change the initial learning
   rate to --learning_rate 0.001, as a learning rate of 0.1 is too big
   when using adam.

   try re-running the following command:
lamtram/src/lamtram/lamtram-train \
  --model_type encdec \
  --train_src data/train.ja \
  --train_trg data/train.en \
  --trainer adam \
  --learning_rate 0.001 \
  --minibatch_size 256 \
  --rate_decay 1.0 \
  --epochs 10 \
  --model_out models/encdec.mod

   you'll probably find that the perplexity drops significantly faster
   than when using the standard sgd update (after the first epoch, i had a
   perplexity of 287 with standard sgd, and 233 with adam).

gpus (advanced)

   if you have access to a machine with a gpu, this can make training much
   faster, particularly when training id4 systems with large vocabularies
   or large hidden layer sizes using minibatches. running lamtram on gpus
   is simple, you just need to compile the dynet library using the cuda
   backend, then link lamtram to it appropriately. however, in our case
   here we are using a small network and small training set, so training
   on cpu is sufficient for now.

attention

basic concept

   one of the major advances in id4 has been the introduction of attention
   (bahdanau et al. 2015). the basic idea behind attention is that when we
   want to generate a particular target word e_i, that we will want to
   focus on a particular source word f_j, or a couple words. in order to
   express this, attention calculates a "context vector" c_i that is used
   as input to the softmax in addition to the decoder state:
pe_i = softmax([g_{i-1}, c_i];   _esm)

   this context vector is defined as the sum of the input sequence vectors
   h_j, weighted by an attention vector a_i as follows:
c_i =   _j a_{i,j} h_j

   there are a number of ways to calculate the attention vector a_i
   (described in detail below), but all follow a basic pattern of
   calculating an attention score   _{i,j} for every word that is a
   function of g_i and h_j:
  _{i,j} = attention(g_i, h_j;   _attn)

   and then use a softmax function to convert score vector   _i into an
   attention vector a_i that adds to one.

   if you want to try to train an attentional model with lamtram, just
   change all mentions of encdec above to encatt (for
   encoder/attentional), and an attentional model will be trained for you.
   for example, we can run the following command:
lamtram/src/lamtram/lamtram-train \
  --model_type encatt \
  --train_src data/train.ja \
  --train_trg data/train.en \
  --trainer adam \
  --learning_rate 0.001 \
  --minibatch_size 256 \
  --rate_decay 1.0 \
  --epochs 10 \
  --model_out models/encatt.mod

   if you compare the perplexities between these two methods you may see
   some difference in the perplexity results after 10 epochs. when i ran
   these, i got a perplexity of 19 for the encoder-decoder, and a
   perplexity of 11 for the attentional model, demonstrating that it's a
   bit easier for the attentional model to model the training corpus
   correctly.

types of attention (advanced)

   there are several ways to calculate the attention scores   _{i,j}, such
   as those investigated by luong et al. (2015). the following ones are
   implemented in lamtram, and can be changed using the --attention_type
   type option as noted below.
     * dot product: calculate the dot product   _{i,j} = g_i *
       transpose(wf_j) (--attention_type dot).
     * bilinear: a bilinear model that puts a parameterized transform
         _bilin between the two vectors   _{i,j} = g_i *   _bilin *
       transpose(wf_j) (--attention_type bilin).
     * multi-layer id88: input the two vectors into a multi-layer
       id88 with a hidden layer of size layernodes,   _{i,j} =
       mlp([g_i, wf_j];   _mlp) (--attention_type mlp:layernodes)

   in practice, i've found that dot product tends to work pretty well, and
   because of this it's the default setting in lamtram. however, the
   multi-layer id88 also performs well in some cases, so sometimes
   it's worth trying.

   in addition, luong et al. (2015) introduced a method called "attention
   feeding," which uses the context vector c_{i-1} of the previous state
   as input to the decoder neural network. this is enabled by default
   using the --attention_feed true option in lamtram, as it seems to help
   somewhat consistently.

testing id4 systems

   now that we have a couple translation models, and know how good they
   are doing on the training set (according to perplexity), we will want
   to test to see how well they will do on data that is not used in
   training. we do this by measuring accuracy on some test data,
   conveniently prepared in data/test.ja and data/test.en.

   the first way we can measure accuracy is by calculating the perplexity
   on this held-out data. this will measure the accuracy of the id4
   systems id203 estimates p(e|f), and see how well they generalize
   to new data. we can do this for the encoder-decoder model using the
   following command:
lamtram/src/lamtram/lamtram \
  --operation ppl \
  --models_in encdec=models/encdec.mod \
  --src_in data/test.ja \
  < data/test.en

   and likewise for the attentional model by replacing encdec with encatt
   (twice) in the command above. note here that we're actually getting
   perplexities that are much worse for the test set than we did on the
   training set (i got train/test perplexities of 19/118 for the encdec
   model and 11/112 for the encatt model). this is for two reasons: lack
   of handling of words that don't occur in the training set, and
   overfitting of the training set. i'll discuss these later.

   next, let's try to actually generate translations of the input using
   the following command (likewise for the attentional model by swapping
   encdec into encatt):
lamtram/src/lamtram/lamtram \
  --operation gen \
  --models_in encdec=models/encdec.mod \
  --src_in data/test.ja \
  > models/encdec.en

   we can then measure the accuracy of this model using a measure called
   id7 score (papineni et al. 2002), which measures the similarity
   between the translation generated by the model and a reference
   translation created by a human (data/test.en):
scripts/multi-id7.pl data/test.en < models/encdec.en

   this gave me a id7 score of 1.76 for encdec and 2.17 for encatt, which
   shows that we're getting something. but generally we need a id7 score
   of at least 15 or so to have something remotely readable, so we're
   going to have to try harder.

thresholding unknown words

   the first problem that we have to tackle is that currently the model
   has no way of handling unknown words that don't exist in the training
   data. the most common way of fixing this problem is by replacing some
   of the words in the training data with a special <unk> symbol, which
   will also be used when we observe an unknown word in the testing data.
   for example, we can replace all words that appear only once in the
   training corpus by performing the following commands.
lamtram/script/unk-single.pl < data/train.en > data/train.unk.en
lamtram/script/unk-single.pl < data/train.ja > data/train.unk.ja

   then we can re-train the attentional model using this new data:
lamtram/src/lamtram/lamtram-train \
  --model_type encatt \
  --train_src data/train.unk.ja \
  --train_trg data/train.unk.en \
  --trainer adam \
  --learning_rate 0.001 \
  --minibatch_size 256 \
  --rate_decay 1.0 \
  --epochs 10 \
  --model_out models/encatt-unk.mod

   this greatly helps our accuracy on the test set: when i measured the
   perplexity and id7 score on the test set, this gave me 58 and 3.32
   respectively, a bit better than before! it also speeds up training
   quite a bit because it reduces the size of the vocabulary.

using a development set

   the second problem, over-fitting, can be fixed somewhat by using a
   development set. the development set is a set of data separate from the
   training and test sets that we use to measure how well the model is
   generalizing during training. there are two simple ways to help use
   this set to prevent overfitting.

early stopping

   the first way we can prevent overfitting is regularly measure the
   accuracy on the development data, and stop training when we get the
   model that has the best accuracy on this data set. this is called
   "early stopping" and used in most neural network models. running this
   in lamtram is easy, just specify the dev_src and dev_trg options as
   follows. (you may also want to increase the number of training epochs
   to 20 or so to really witness how much the model overfits in later
   stages of training.)
lamtram/src/lamtram/lamtram-train \
  --model_type encatt \
  --train_src data/train.unk.ja \
  --train_trg data/train.unk.en \
  --dev_src data/dev.ja \
  --dev_trg data/dev.en \
  --trainer adam \
  --learning_rate 0.001 \
  --minibatch_size 256 \
  --rate_decay 1.0 \
  --epochs 10 \
  --model_out models/encatt-unk-stop.mod

   you'll notice that now after every pass over the training data, we're
   measuring the perplexity on the development set, and the model is
   written out only when the perplexity is its best value yet. in my case,
   the development perplexity reaches its peak on the 8th iteration then
   starts getting worse. in my case, by stopping training on the 8th
   iteration, the perplexity improved a slight bit to 56, but this didn't
   make a big difference in id7.

rate decay

   another trick that is often used is "rate decay" this reduces the
   learning rate    every time the perplexity gets worse on the development
   set. this causes the model to update the parameters a bit more
   conservatively, which as an effect of controlling overfitting. we can
   enable rate decay by setting the rate_decay parameter to 0.5 (which
   will halve the learning rate everytime the development perplexity gets
   worse). this prolongs training a little bit, so let's also set the
   number of epochs to 15, just to make sure that training has run its
   course.
lamtram/src/lamtram/lamtram-train \
  --model_type encatt \
  --train_src data/train.unk.ja \
  --train_trg data/train.unk.en \
  --dev_src data/dev.ja \
  --dev_trg data/dev.en \
  --trainer adam \
  --learning_rate 0.001 \
  --minibatch_size 256 \
  --rate_decay 0.5 \
  --epochs 15 \
  --model_out models/encatt-unk-decay.mod

   in my case, the rate was decayed on every epoch after the 8th. this
   didn't result in an improvement on this particular data set, but in
   many cases this rate decay can be quite helpful.

using an external lexicon

   one way to further help out neural mt systems is to incorporate an
   external lexicon indicating mappings between words (and their
   probabilities).

training the lexicon

   first, we need to create the lexicon. this can be done using a word
   alignment tool that finds the correspondences between words in the
   source and target sentences. here we will use [73]fast_align because it
   is simple to use and fast, but other word alignment tools such as
   [74]giza++ or [75]nile might give you better results.

   first, let's download and build fast_align:
git clone https://github.com/clab/fast_align.git
mkdir fast_align/build
cd fast_align/build
cmake ../
make
cd ../../

   then, we can run fast_align on the training data to build a lexicon,
   and use the convert-cond.pl script to convert it into a format that
   lamtram can use.
mkdir lexicon
paste data/train.{ja,en} | sed $'s/\t/ ||| /g' > lexicon/train.jaen
fast_align/build/fast_align -i lexicon/train.jaen -v -p lexicon/train.jaen.cond
> lexicon/train.jaen.align
lamtram/script/convert-cond.pl < lexicon/train.jaen.cond > lexicon/train.jaen.pr
ob

unknown word replacement

   the first way we can use this lexicon is by using it to map unknown
   words in the source language into the target language. without a
   lexicon, when an unknown word is predicted in the target, the id4
   system will find the word in the source sentence with the highest
   alignment weight a_j and map it into the target as-is. if we have a
   lexicon, if the source word has a lexicon entry, instead of mapping the
   word f_j as-is, the id4 system will output the word with the highest
   id203 p_{lex}(e|f_j) in the lexicon.

   this can be done in lamtram by specifying the map_in function during
   decoding:
lamtram/src/lamtram/lamtram \
  --operation gen \
  --models_in encatt=models/encatt-unk-stop.mod \
  --src_in data/test.ja \
  --map_in lexicon/train.jaen.prob \
  > models/encatt-unk-stop-rep.en

   this helped a little bit, raising the id7 score from 2.58 to 2.63 for
   my model.

improving translation probabilities

   another way we can use lexicons is to use them to bootstrap translation
   probabilities (arthur et al. 2016). this works by calculating a lexicon
   id203 based on the attention weights a_j
p_{lex}(e_i | f, a) =   _j a_j p(e_i | f_j)

   this is then added as an additional information source when calculating
   the softmax probabilities over the output. the advantage of this method
   is that the lexicon is fast to train, and also contains information
   about what words can be translated into others in an efficient manner,
   making it easier for the mt system to learn correct translations,
   particularly of rare words.

   this method can be applied by adding the attention_lex options as
   follows. "alpha" is a parameter to adjust the strength of the lexicon,
   where smaller indicates that more weight will be put on the lexicon
   probabilities:
lamtram/src/lamtram/lamtram-train \
  --model_type encatt \
  --train_src data/train.unk.ja \
  --train_trg data/train.unk.en \
  --dev_src data/dev.ja \
  --dev_trg data/dev.en \
  --attention_lex prior:file=lexicon/train.jaen.prob:alpha=0.0001 \
  --trainer adam \
  --learning_rate 0.001 \
  --minibatch_size 256 \
  --rate_decay 1.0 \
  --epochs 10 \
  --model_out models/encatt-unk-stop-lex.mod

   in my running, this improves our perplexity from 57 to 37, and id7
   score from 2.48 to 8.83, nice!

search

id125

   in the initial explanation of id4, i explained that translations are
   generated by selecting the next word in the target sentence that
   maximizes the id203 pe_i. however, while this gives us a locally
   optimal decision about the next word e'_i, this is a greedy search
   method that won't necessarily give us the sentence e' that maximizes
   the translation id203 p(e|f).

   to improve search (and hopefully translation accuracy), we can use
   "id125," which instead of considering the one best next word,
   considers the k best hypotheses at every time step i. if k is bigger,
   search will be more accurate but slower. k can be set with the --beam
   option during decoding, so let's try this here with our best model so
   far:
lamtram/src/lamtram/lamtram \
  --operation gen \
  --models_in encatt=models/encatt-unk-stop-lex.mod \
  --src_in data/test.ja \
  --map_in lexicon/train.jaen.prob \
  --beam beam \
  > models/encatt-unk-stop-lex-beambeam.en

   where we replace the two instances of beam above with values such as 1,
   2, 3, 5.

   looking at the results
beam=1:  id7 = 8.83, 42.2/14.3/5.5/2.1 (bp=0.973, ratio=0.973, hyp_len=4564, re
f_len=4690)
beam=2:  id7 = 9.23, 45.4/16.2/6.4/2.5 (bp=0.887, ratio=0.893, hyp_len=4186, re
f_len=4690)
beam=3:  id7 = 9.66, 49.4/18.0/7.5/3.1 (bp=0.805, ratio=0.822, hyp_len=3855, re
f_len=4690)
beam=5:  id7 = 9.66, 50.7/18.7/8.0/3.4 (bp=0.765, ratio=0.788, hyp_len=3698, re
f_len=4690)
beam=10: id7 = 9.73, 51.7/19.2/8.5/3.8 (bp=0.726, ratio=0.758, hyp_len=3553, re
f_len=4690)

   we can see that by increasing the beam size, we can get a decent
   improvement in id7.

adjusting for sentence length

   however, there is also something concerning about the previous result.
   "ratio=" is the ratio of "output length"/"reference length" and if this
   is less than 1, our sentences are too short. we can see that as we
   increase the beam size, our sentences are getting to be much shorter
   that the reference. the reason for this is that as sentences get
   longer, their id203 tends to get lower, and when we increase the
   beam size we become more effective at finding these shorter sentences.

   there are a number of ways to fix this problem, but the easiest is
   adding a "word penalty" wp which multiplies the id203 of the
   sentence by the constant "e^{wp}" every time an additional word is
   added. this is equivalent to setting a prior id203 on the length
   of the sentence that follows an exponential distribution. wp can be set
   using the --word_pen option of lamtram, so let's try setting a few
   different values and measure the id7 score for beam width of 10:
wp=0.0: id7 = 9.73, 51.7/19.2/8.5/3.8 (bp=0.726, ratio=0.758, hyp_len=3553, ref
_len=4690)
wp=0.5: id7 = 9.90, 50.5/18.6/8.0/3.5 (bp=0.775, ratio=0.797, hyp_len=3736, ref
_len=4690)
wp=1.0: id7 = 10.00, 48.3/17.9/7.5/3.0 (bp=0.850, ratio=0.860, hyp_len=4033, re
f_len=4690)
wp=1.5: id7 = 9.95, 44.1/16.1/6.5/2.5 (bp=0.963, ratio=0.963, hyp_len=4518, ref
_len=4690)

   we can see that as we increase the word penalty, this gives us more
   reasonably-lengthed hypotheses, which also improves the id7 a little
   bit.

changing network structure

   one thing that we have not considered so far is the size of the network
   that we're training. currently the default for lamtram is that all
   recurrent networks have 100 hidden nodes (or when using
   forward/backward encoders, the encoders will be 50 and decoder will be
   100). in addition, we're using only a single hidden layer, while many
   recent systems use deeper networks with 2-4 hidden layers. these can be
   changed using the --layers option of lamtram, which defaults to
   lstm:100:1, where the first option is using id137 (which tend
   to work pretty well), the second option is the width, and third option
   is the depth. let's try to train a wider network by setting --layers
   lstm:200:1.

   one thing to note is that the dynet toolkit has a default limit of
   using 512mb of memory, but once we start using larger networks this
   might not be sufficient. so we'll also increase the amount of memory to
   1024mb by adding the --dynet_mem 1024 parameter.
lamtram/src/lamtram/lamtram-train \
  --dynet_mem 1024 \
  --model_type encatt \
  --train_src data/train.unk.ja \
  --train_trg data/train.unk.en \
  --dev_src data/dev.ja \
  --dev_trg data/dev.en \
  --attention_lex prior:file=lexicon/train.jaen.prob:alpha=0.0001 \
  --layers lstm:200:1 \
  --trainer adam \
  --learning_rate 0.001 \
  --minibatch_size 256 \
  --rate_decay 1.0 \
  --epochs 10 \
  --model_out models/encatt-unk-stop-lex-w200.mod

   note that this makes training significantly slower, because we need to
   do twice as many calculations in many of our id127s.
   testing this model, the model with 200 nodes reduces perplexity from 37
   to 33, and improves id7 from 10.00 to 10.21. when using larger
   training data we'll get even bigger improvements by making the network
   bigger.

ensembling

   one final technique that is useful for improving final results is
   "ensembling," or combining multiple models together. the way this works
   is that if we have two id203 distributions pe_i^{(1)} and
   pe_i^{(2)} from multiple models, we can calculate the next id203
   by linearly interpolating them together:
pe_i = (pe_i^{(1)} + pe_i^{(2)}) / 2

   or log-linearly interpolating them together:
pe_i = exp( (log(pe_i^{(1)}) + log(pe_i^{(2)})) / 2 )

   performing ensembling at test time in lamtram is simple: in
   --models_in, we simply add two different model options separated by a
   pipe, as follows. the default is linear interpolation, but you can also
   try log-linear interpolation by setting --ensemble_op logsum. let's try
   ensembling our 100-node and 200-node models to measure perplexity:
lamtram/src/lamtram/lamtram \
  --operation ppl \
  --models_in "encatt=models/encatt-unk-stop-lex.mod|encatt=models/encatt-unk-st
op-lex-w200.mod" \
  --src_in data/test.ja \
  < data/test.en

   this reduced the perplexity from 36/33 to 30 for the ensembled model,
   and resulted in a id7 score of 10.99. of course, we can probably
   improve this by ensembling even more models together. it's actually ok
   to just train several models of the same structure with different
   random seeds (if you set the --seed parameter of lamtram you can set a
   different seed, or by default a different one will be chosen randomly
   every time).

final output

   because we're basically done, i'll also list up a few examples from the
   start of the test corpus, where the first line is the input, the second
   line is the correct translation, and the third line is generated
   translation.
                                                     
can you do it in one day ?
you can do it on a day ?

                                                                     
he stared at me with a satirical smile .
he stared at the irony of irony .

                                                                  
it &apos;s time to leave .
our start of our start is we .

                                                        
what do you want to do in the afternoon ?
what did you do for you this afternoon ?

   not great, but actually pretty good considering that we only have
   10,000 sentences of training data, and that japanese-english is a
   pretty difficult language pair to translate!

more advanced (but very useful!) methods

   the following are a few extra methods that can be pretty useful in some
   cases, but i won't be testing here:

id173

   as mentioned before, when dealing with small data we need to worry
   about overfitting, and some ways to fix this are ealy stopping and
   learning rate decay. in addition, we can also reduce the damage of
   overfitting by adding some variety of id173.

   one common way of regularizing neural networks is "dropout" (srivastava
   et al. 2014) which consists of randomly disabling a set fraction of the
   units in the input network. this dropout rate can be set with the
   --dropout rate option. usually we use a rate of 0.5, which has nice
   theoretical properties. i tried this on this data set, and it reduced
   perplexity from 33 to 30 for the 200 node model, but didn't have a
   large effect on id7 scores.

   another way to do this is using l2 id173, which puts a penalty
   on the l2 norm of the parameter vectors in the model. this can be
   applied by adding --dynet_l2 rate to the beginning of the option list.
   i've personally had little luck with getting this to work for neural
   networks, but it might be worth trying.

using subword units

   one problem with neural network models is that as the vocabulary gets
   larger, training time increases, so it's often necessary to replace
   many of the words in the vocabulary with <unk> to ensure that training
   times remain reasonable. there are a number of ways that have been
   proposed to handle the problem of large vocabularies. one simple way to
   do so without sacrificing accuracy on low-frequency words (too much) is
   by splitting rare words into subword units. a method to do so by
   sennrich et al. (2016) discovers good subword units using a method
   called "byte pair encoding", and is implemented in the [76]subword-id4
   package. you can use this as an additional
   pre-processing/post-processing step before learning and using a model
   with lamtram.

training for other evaluation measures

   finally, you may have noticed throughout this tutorial that we are
   training models to maximize the likelihood, but evaluating our models
   using id7 score. there are a number of methods to resolve this
   mismatch between the training and testing criteria by directly
   optimizing id4 systems to improve translation accuracy. in lamtram, a
   method by shen et al. (2016) can be used to optimize id4 systems for
   expected id7 score (or in other words, minimize the risk). in
   particular, i've found that this does a good job of at least ensuring
   that the id4 system generates output that is of the appropriate length.

   there are a number settings that should be changed when using the
   method:
     * --learning_criterion minrisk: this will enable minimum-risk based
       training.
     * --model_in file: because this method is slow to train, it's better
       to first initialize the model using standard maximimum likelihood
       training, then fine-tune the model with id7-based training. this
       method can be used to read in an already-trained model.
     * --minrisk_num_samples num: this method works by generating samples
       from the model, then evaluating these generated samples. increasing
       num improves the stability of the training, but also reduces the
       training efficiency. a value 20-100 should be reasonable.
     * --minrisk_scaling, --minrisk_dedup: parameters of the algorithm
       including the scaling factors for probabilities, and whether to
       include the correct answer in the samples or not.
     * --trainer sgd --learning_rate 0.05: i've found that using more
       advanced optimizers like adam actually reduces stability in
       training, so using vanilla sgd might be a safer choice. slightly
       lowering the learning rate is also sometimes necessary.
     * --eval_every 1000: training is a bit slower than standard id4
       training, so we can evaluate more frequently than when we finish
       the whole corpus.

   the final command will look like this:
lamtram/src/lamtram/lamtram-train \
  --dynet_mem 1024 \
  --model_type encatt \
  --train_src data/train.unk.ja \
  --train_trg data/train.unk.en \
  --dev_src data/dev.ja \
  --dev_trg data/dev.en \
  --trainer sgd \
  --learning_criterion minrisk \
  --learning_rate 0.05 \
  --minrisk_num_samples 20 \
  --minrisk_scaling 0.005 \
  --minrisk_include_ref true \
  --rate_decay 1.0 \
  --epochs 10 \
  --eval_every 1000 \
  --model_in models/encatt-unk-stop-lex-w200.mod \
  --model_out models/encatt-unk-stop-lex-w200-minrisk.mod

preparing data

data size

   up until now, you have just been working with the small data set of
   10,000 that i've provided. having about 10,000 sentences makes training
   relatively fast, but having more data will make accuracy significantly
   higher. fortunately, there is a larger data set of about 140,000
   sentences called train-big.ja and train-big.en, which you can download
   by running the following commands.
wget http://phontron.com/lamtram/download/data-big.tar.gz
tar -xzf data-big.tar.gz

   try re-running experiments with this larger data set, and you will see
   that the accuracy gets significantly higher. in real id4 systems, it's
   common to use several million sentences (or more!) to achieve usable
   accuracies. sometimes in these cases, you'll want to evaluate the
   accuracy of your system more frequently than when you reach the end of
   the corpus, so try specifying the --eval_every num_sentences command,
   where num_sentences is the number of sentences after which you'd like
   to evaluate on the dev set. also, it's highly recommended that you use
   a gpu for training when scaling to larger data and networks.

preprocessing

   also note that up until now, we've taken it for granted that our data
   is split into words and lower-cased. when you build an actual system,
   this will not be the case, so you'll have to perform these processes
   yourself. here, for id121 we're using:
     * english: [77]moses (koehn et al. 2007)
     * japanese: [78]kytea (neubig et al. 2011)

   and for lowercasing we're using:
perl -nle 'print lc'

   make sure that you do id121, and potentially lowercasing, before
   feeding your data into lamtram, or any mt toolkit. you can see an
   example of how we converted the tanaka corpus into the data used for
   the tutorial by looking at scripts/create-data.sh script.

final word

   now, you know a few practical things about making an accurate neural mt
   system. using the methods described here, we were able to improve a
   system trained on only 10,000 sentences from 1.83 id7 to 10.99 id7.
   switching over to larger data should result in much larger increases,
   and may even result in readable translations.

   this is a very fast-moving field, so this guide might be obsolete in a
   few months from the writing (or even already!) but hopefully this
   helped you learn the basics to get started, start reading papers, and
   come up with your own methods/applications.

references

     * philip arthur, graham neubig, satoshi nakamura. incorporating
       discrete translation lexicons in id4. emnlp,
       2016
     * dzmitry bahdanau, kyunghyun cho, yoshua bengio. neural machine
       translation by jointly learning to align and translate. iclr, 2015.
     * peter f. brown, vincent j. della pietra, stephen a. della pietra,
       robert l. mercer. the mathematics of statistical machine
       translation: parameter estimation. computational linguistics, 1993.
     * yoav goldberg. a primer on neural network models for natural
       language processing. arxiv, 2015.
     * nal kalchbrenner, phil blunsom. recurrent continuous translation
       models. emnlp, 2013.
     * diederik kingma, jimmy ba. adam: a method for stochastic
       optimization. arxiv, 2014.
     * philipp koehn et al. moses: open source toolkit for statistical
       machine translation. acl, 2007.
     * philipp koehn. id151. cambridge
       university press, 2009.
     * minh-thang luong, hieu pham, christopher d. manning. effective
       approaches to attention-based id4. emnlp,
       2015.
     * graham neubig, yosuke nakata, shinsuke mori. pointwise prediction
       for robust, adaptable japanese morphological analysis. acl, 2011.
     * rico sennrich, barry haddow, alexandra birch. neural machine
       translation of rare words with subword units. acl, 2016.
     * nitish srivastava, geoffrey e. hinton, alex krizhevsky, ilya
       sutskever, ruslan r. salakhutdinov. dropout: a simple way to
       prevent neural networks from overfitting. jmlr, 2014.
     * shiqi shen, yong cheng, zhongjun he, wei he, hua wu, maosong sun,
       yang liu. minimum risk training for id4.
       acl, 2016.
     * ilya sutskever, oriol vinyals, quoc v. le. sequence to sequence
       learning with neural networks. nips, 2014.

     *    2019 github, inc.
     * [79]terms
     * [80]privacy
     * [81]security
     * [82]status
     * [83]help

     * [84]contact github
     * [85]pricing
     * [86]api
     * [87]training
     * [88]blog
     * [89]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [90]reload to refresh your
   session. you signed out in another tab or window. [91]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/neubig/id4-tips/commits/master.atom
   3. https://github.com/neubig/id4-tips#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/neubig/id4-tips
  32. https://github.com/join
  33. https://github.com/login?return_to=/neubig/id4-tips
  34. https://github.com/neubig/id4-tips/watchers
  35. https://github.com/login?return_to=/neubig/id4-tips
  36. https://github.com/neubig/id4-tips/stargazers
  37. https://github.com/login?return_to=/neubig/id4-tips
  38. https://github.com/neubig/id4-tips/network/members
  39. https://github.com/neubig
  40. https://github.com/neubig/id4-tips
  41. https://github.com/neubig/id4-tips
  42. https://github.com/neubig/id4-tips/issues
  43. https://github.com/neubig/id4-tips/pulls
  44. https://github.com/neubig/id4-tips/projects
  45. https://github.com/neubig/id4-tips/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/neubig/id4-tips/commits/master
  48. https://github.com/neubig/id4-tips/branches
  49. https://github.com/neubig/id4-tips/releases
  50. https://github.com/neubig/id4-tips/graphs/contributors
  51. https://github.com/neubig/id4-tips/search?l=perl
  52. https://github.com/neubig/id4-tips/search?l=shell
  53. https://github.com/neubig/id4-tips/find/master
  54. https://github.com/neubig/id4-tips/archive/master.zip
  55. https://github.com/login?return_to=https://github.com/neubig/id4-tips
  56. https://github.com/join?return_to=/neubig/id4-tips
  57. https://desktop.github.com/
  58. https://desktop.github.com/
  59. https://developer.apple.com/xcode/
  60. https://visualstudio.github.com/
  61. https://github.com/neubig
  62. https://github.com/neubig/id4-tips/commits?author=neubig
  63. https://github.com/neubig/id4-tips/commit/c81d8b4fb8266dd4bd48df2ea4e7aed98bc90021
  64. https://github.com/neubig/id4-tips/commit/c81d8b4fb8266dd4bd48df2ea4e7aed98bc90021
  65. https://github.com/neubig/id4-tips/tree/c81d8b4fb8266dd4bd48df2ea4e7aed98bc90021
  66. https://github.com/neubig/id4-tips/tree/master/data
  67. https://github.com/neubig/id4-tips/tree/master/scripts
  68. https://github.com/neubig/id4-tips/blob/master/.gitignore
  69. https://github.com/neubig/id4-tips/blob/master/readme.md
  70. http://phontron.com/
  71. http://github.com/neubig/lamtram
  72. http://github.com/clab/dynet
  73. https://github.com/clab/fast_align
  74. https://github.com/moses-smt/giza-pp
  75. https://github.com/jasonriesa/nile
  76. http://github.com/rsennrich/subword-id4
  77. http://statmt.org/moses
  78. http://phontron.com/kytea/
  79. https://github.com/site/terms
  80. https://github.com/site/privacy
  81. https://github.com/security
  82. https://githubstatus.com/
  83. https://help.github.com/
  84. https://github.com/contact
  85. https://github.com/pricing
  86. https://developer.github.com/
  87. https://training.github.com/
  88. https://github.blog/
  89. https://github.com/about
  90. https://github.com/neubig/id4-tips
  91. https://github.com/neubig/id4-tips

   hidden links:
  93. https://github.com/
  94. https://github.com/neubig/id4-tips
  95. https://github.com/neubig/id4-tips
  96. https://github.com/neubig/id4-tips
  97. https://help.github.com/articles/which-remote-url-should-i-use
  98. https://github.com/neubig/id4-tips#tips-on-building-neural-machine-translation-systems
  99. https://github.com/neubig/id4-tips#machine-translation
 100. https://github.com/neubig/id4-tips#neural-machine-translation-id4-and-encoder-decoder-models
 101. https://github.com/neubig/id4-tips#training-id4-models-with-maximum-likelihood
 102. https://github.com/neubig/id4-tips#speeding-up-training
 103. https://github.com/neubig/id4-tips#minibatches
 104. https://github.com/neubig/id4-tips#other-update-rules
 105. https://github.com/neubig/id4-tips#gpus-advanced
 106. https://github.com/neubig/id4-tips#attention
 107. https://github.com/neubig/id4-tips#basic-concept
 108. https://github.com/neubig/id4-tips#types-of-attention-advanced
 109. https://github.com/neubig/id4-tips#testing-id4-systems
 110. https://github.com/neubig/id4-tips#thresholding-unknown-words
 111. https://github.com/neubig/id4-tips#using-a-development-set
 112. https://github.com/neubig/id4-tips#early-stopping
 113. https://github.com/neubig/id4-tips#rate-decay
 114. https://github.com/neubig/id4-tips#using-an-external-lexicon
 115. https://github.com/neubig/id4-tips#training-the-lexicon
 116. https://github.com/neubig/id4-tips#unknown-word-replacement
 117. https://github.com/neubig/id4-tips#improving-translation-probabilities
 118. https://github.com/neubig/id4-tips#search
 119. https://github.com/neubig/id4-tips#beam-search
 120. https://github.com/neubig/id4-tips#adjusting-for-sentence-length
 121. https://github.com/neubig/id4-tips#changing-network-structure
 122. https://github.com/neubig/id4-tips#ensembling
 123. https://github.com/neubig/id4-tips#final-output
 124. https://github.com/neubig/id4-tips#more-advanced-but-very-useful-methods
 125. https://github.com/neubig/id4-tips#id173
 126. https://github.com/neubig/id4-tips#using-subword-units
 127. https://github.com/neubig/id4-tips#training-for-other-evaluation-measures
 128. https://github.com/neubig/id4-tips#preparing-data
 129. https://github.com/neubig/id4-tips#data-size
 130. https://github.com/neubig/id4-tips#preprocessing
 131. https://github.com/neubig/id4-tips#final-word
 132. https://github.com/neubig/id4-tips#references
 133. https://github.com/
