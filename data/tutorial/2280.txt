   [1][mainlogowhitethick.jpg]
     * search
     * ____________________
     * [2]home
     * [3]+=1
     * [4]support the content
     * [5]community
     * [6]log in
     * [7]sign up

     * ____________________
     * [8]home
     * [9]+=1
     * [10]support the content
     * [11]community
     * [12]log in
     * [13]sign up

   [machine-learning-support-vector-machine-linear-svc-example-with-python
   -and-sklearn-1024x584.png]

linear svc machine learning id166 example with python

   iframe:
   [14]//www.youtube.com/embed/81zgoib7dtk?list=plqvvvaa0qudd0flggphkcej-9
   jp-qdzz3

   the most applicable machine learning algorithm for our problem is
   [15]linear svc. before hopping into linear svc with our data, we're
   going to show a very simple example that should help solidify your
   understanding of working with linear svc.

   the objective of a linear svc (support vector classifier) is to fit to
   the data you provide, returning a "best fit" hyperplane that divides,
   or categorizes, your data. from there, after getting the hyperplane,
   you can then feed some features to your classifier to see what the
   "predicted" class is. this makes this specific algorithm rather
   suitable for our uses, though you can use this for many situations.
   let's get started.

   first, we're going to need some basic dependencies:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
style.use("ggplot")
from sklearn import id166

   matplotlib here is not truly necessary for linear svc. the reason why
   we're using it here is for the eventual data visualization. typically,
   you wont be able to visualize as many dimensions as you will have
   features, but, it's worth visualizing at least once to understand how
   linear svc works.

   other than the visualization packages we're using, you will just need
   to import id166 from sklearn and numpy for array conversion.

   next, let's consider that we have two features to consider. these
   features will be visualized as axis on our graph. so something like:
x = [1, 5, 1.5, 8, 1, 9]
y = [2, 8, 1.8, 8, 0.6, 11]

   then we can graph this data using:
plt.scatter(x,y)
plt.show()

   the result is:
   [machine-learning-feature-scatter-plot.png]

   now, of course, we can see with our own eyes how these groups should be
   divided, though exactly where we might draw the dividing line might be
   debated:
   [machine-learning-division-example.png]

   so this is with two features, and we see we have a 2d graph. if we had
   three features, we could have a 3d graph. the 3d graph would be a
   little more challenging for us to visually group and divide, but still
   do-able. the problem occurs when we have four features, or
   four-thousand features. now you can start to understand the power of
   machine learning, seeing and analyzing a number of dimensions
   imperceptible to us.

   with that in mind, we're going to go ahead and continue with our
   two-featured example. now, in order to feed data into our machine
   learning algorithm, we first need to compile an array of the features,
   rather than having them as x and y coordinate values.

   generally, you will see the feature list being stored in a capital x
   variable. let's translate our above x and y coordinates into an array
   that is compiled of the x and y coordinates, where x is a feature and y
   is a feature.
x = np.array([[1,2],
             [5,8],
             [1.5,1.8],
             [8,8],
             [1,0.6],
             [9,11]])

   now that we have this array, we need to label it for training purposes.
   there are forms of machine learning called "unsupervised learning,"
   where data labeling isn't used, as is the case with id91, though
   this example is a form of supervised learning.

   for our labels, sometimes referred to as "targets," we're going to use
   0 or 1.
y = [0,1,0,1,0,1]

   just by looking at our data set, we can see we have coordinate pairs
   that are "low" numbers and coordinate pairs that are "higher" numbers.
   we've then assigned 0 to the lower coordinate pairs and 1 to the higher
   feature pairs.

   these are the labels. in the case of our project, we will wind up
   having a list of numerical features that are various statistics about
   stock companies, and then the "label" will be either a 0 or a 1, where
   0 is under-perform the market and a 1 is out-perform the market.

   moving along, we are now going to define our classifier:
clf = id166.svc(kernel='linear', c = 1.0)

   we're going to be using the svc (support vector classifier) id166
   (support vector machine). our kernel is going to be linear, and c is
   equal to 1.0. what is c you ask? don't worry about it for now, but, if
   you must know, c is a valuation of "how badly" you want to properly
   classify, or fit, everything. the machine learning field is relatively
   new, and experimental. there exist many debates about the value of c,
   as well as how to calculate the value for c. we're going to just stick
   with 1.0 for now, which is a nice default parameter.

   next, we call:
clf.fit(x,y)

   note: this is an older tutorial, and scikit-learn has since deprecated
   this method. by version 0.19, this code will cause an error because it
   needs to be a numpy array, and re-shaped. to see an example of
   converting to a numpy array and reshaping, check out this [16]k nearest
   neighbors tutorial, near the end. you do not need to follow along with
   that series to mimic what is done there with the reshaping, and
   continue along with this series.

   from here, the learning is done. it should be nearly-instant, since we
   have such a small data set.

   next, we can predict and test. let's print a prediction:
print(clf.predict([0.58,0.76]))

   we're hoping this predicts a 0, since this is a "lower" coordinate
   pair.

   sure enough, the prediction is a classification of 0. next, what if we
   do:
print(clf.predict([10.58,10.76]))

   and again, we have a theoretically correct answer of 1 as the
   classification. this was a blind prediction, though it was really a
   test as well, since we knew what the hopeful target was.
   congratulations, you have 100% accuracy!

   now, to visualize your data:
w = clf.coef_[0]
print(w)

a = -w[0] / w[1]

xx = np.linspace(0,12)
yy = a * xx - clf.intercept_[0] / w[1]

h0 = plt.plot(xx, yy, 'k-', label="non weighted div")

plt.scatter(x[:, 0], x[:, 1], c = y)
plt.legend()
plt.show()

   the result:
   [machine-learning-support-vector-machine-linear-svc-example-with-python
   -and-sklearn-1024x584.png]

   if you'd like a bit more explanation on how the graphing code works,
   watch the second-half of the embedded video. visualizing the data is
   somewhat useful to see what the program is doing in the background, but
   is not really necessary to understand how to visualize it specifically
   at this point. you will likely find that the problems you are trying to
   solve simply cannot be visualized due to having too many features and
   thus too many dimensions to graph.
   [17]there exists 1 challenge(s) for this tutorial. (button) sign up to
   +=1 for access to these, video downloads, and no ads.
   [18]there exists 2 quiz/question(s) for this tutorial. (button) sign up
   to +=1 for access to these, video downloads, and no ads.

   the next tutorial: [19](button) getting more features from our data
   [plus-equals-1-v7.png]
     * intro to machine learning with scikit learn and python
       [20](button) go
     * simple support vector machine (id166) example with character
       recognition
       [21](button) go
     * our method and where we will be getting our data
       [22](button) go
     * parsing data
       [23](button) go
     * more parsing
       [24](button) go
     * structuring data with pandas
       [25](button) go
     * getting more data and meshing data sets
       [26](button) go
     * labeling of data part 1
       [27](button) go
     * labeling data part 2
       [28](button) go
     * finally finishing up the labeling
       [29](button) go
     * linear svc machine learning id166 example with python
     * getting more features from our data
       [30](button) go
     * linear svc machine learning and testing our data
       [31](button) go
     * scaling, normalizing, and machine learning with many features
       [32](button) go
     * shuffling our data to solve a learning issue
       [33](button) go
     * using quandl for more data
       [34](button) go
     * improving our analysis with a more accurate measure of performance
       in relation to fundamentals
       [35](button) go
     * learning and testing our machine learning algorithm
       [36](button) go
     * more testing, this time including n/a data
       [37](button) go
     * back-testing the strategy
       [38](button) go
     * pulling current data from yahoo
       [39](button) go
     * building our new data-set
       [40](button) go
     * searching for investment suggestions
       [41](button) go
     * raising investment requirement standards
       [42](button) go
     * testing raised standards
       [43](button) go
     * streamlining the changing of standards
       [44](button) go

you've reached the end!

   contact: harrison@pythonprogramming.net.
     * [45]support this website!
     * [46]hire me
     * [47]facebook
     * [48]twitter
     * [49]google+

legal stuff:

     * [50]terms and conditions
     * [51]privacy policy

   [52]

   programming is a superpower.

      over 9000! pythonprogramming.net

references

   visible links
   1. https://pythonprogramming.net/
   2. https://pythonprogramming.net/
   3. https://pythonprogramming.net/+=1/
   4. https://pythonprogramming.net/support/
   5. https://goo.gl/7zgavq
   6. https://pythonprogramming.net/login/
   7. https://pythonprogramming.net/register/
   8. https://pythonprogramming.net/
   9. https://pythonprogramming.net/+=1/
  10. https://pythonprogramming.net/support/
  11. https://goo.gl/7zgavq
  12. https://pythonprogramming.net/login/
  13. https://pythonprogramming.net/register/
  14. https://www.youtube.com/embed/81zgoib7dtk?list=plqvvvaa0qudd0flggphkcej-9jp-qdzz3
  15. http://scikit-learn.org/stable/modules/generated/sklearn.id166.linearsvc.html
  16. https://pythonprogramming.net/k-nearest-neighbors-application-machine-learning-tutorial/
  17. https://pythonprogramming.net/+=1/
  18. https://pythonprogramming.net/+=1/
  19. https://pythonprogramming.net/collecting-features-machine-learning/?completed=/linear-svc-example-scikit-learn-id166-python/
  20. https://pythonprogramming.net/machine-learning-python-sklearn-intro/
  21. https://pythonprogramming.net/support-vector-machine-id166-example-tutorial-scikit-learn-python/
  22. https://pythonprogramming.net/data-acquisition-machine-learning/
  23. https://pythonprogramming.net/getting-data-machine-learning/
  24. https://pythonprogramming.net/parsing-data-website-machine-learning/
  25. https://pythonprogramming.net/using-pandas-structure-process-data/
  26. https://pythonprogramming.net/getting-data-sp-500-index-value-comparison/
  27. https://pythonprogramming.net/labeling-data-machine-learning/
  28. https://pythonprogramming.net/labeling-data-machine-learning-part-2/
  29. https://pythonprogramming.net/label-data-machine-learning/
  30. https://pythonprogramming.net/collecting-features-machine-learning/
  31. https://pythonprogramming.net/linear-svc-machine-learning-testing-data/
  32. https://pythonprogramming.net/preprocessing-machine-learning/
  33. https://pythonprogramming.net/shuffling-data-learning/
  34. https://pythonprogramming.net/using-quandl-data/
  35. https://pythonprogramming.net/improving-analysis-machine-learning/
  36. https://pythonprogramming.net/learning-and-testing-id166/
  37. https://pythonprogramming.net/machine-learning-testing-with-na/
  38. https://pythonprogramming.net/back-testing-machine-learning-investing-strategy/
  39. https://pythonprogramming.net/current-yahoo-data-for-machine-learning/
  40. https://pythonprogramming.net/building-yahoo-data-machine-learning/
  41. https://pythonprogramming.net/investment-suggestions-machine-learning/
  42. https://pythonprogramming.net/raising-investment-suggestion-requirements/
  43. https://pythonprogramming.net/machine-learning-testing-new-algo/
  44. https://pythonprogramming.net/streamlining-changing-machine-learning/
  45. https://pythonprogramming.net/support-donate/
  46. https://pythonprogramming.net/consulting/
  47. https://www.facebook.com/pythonprogramming.net/
  48. https://twitter.com/sentdex
  49. https://plus.google.com/+sentdex
  50. https://pythonprogramming.net/about/tos/
  51. https://pythonprogramming.net/about/privacy-policy/
  52. https://xkcd.com/353/

   hidden links:
  54. https://pythonprogramming.net/linear-svc-example-scikit-learn-id166-python/
  55. https://pythonprogramming.net/+=1/?a=13&t=/linear-svc-example-scikit-learn-id166-python/
  56. https://pythonprogramming.net/+=1/?a=13&t=/linear-svc-example-scikit-learn-id166-python/
