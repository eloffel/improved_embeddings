   #[1]machine learning, statistics, systems

   [c4ade7596c93b909699000666b47bc53?s=200]

[2]andrew tulloch   machine learning, statistics, systems

   [3]about | [4]academic | [5]github | [6]cv

[7]a basic soft-margin kernel id166 implementation in python

   26 november 2013

   [8]support vector machines (id166s) are a family of nice supervised
   learning algorithms that can train classification and regression models
   efficiently and with very good performance in practice.

   id166s are also rooted in id76 and hilbert space theory,
   and there is a lot of beautiful mathematics in the derivation of
   various aspects of the training algorithm, which we will go into in
   subsequent posts.

   for now, we'll just give an introduction to the basic theory of
   soft-margin kernel id166s. the classical treatment is to start with
   hard-margin linear id166s, then introduce the kernel trick and the
   soft-margin formulation, so this is somewhat faster-moving than other
   presentations.

mathematical formulation

   we consider our training set to be

   \begin{equation} d = { (\mathbf{x}_{i}, y_{i}), \mathbf{x} \in
   \mathbb{r}^d, y \in \{ -1, 1 \} }. \end{equation}

   the key idea is that we seek to find a hyperplane $w$ separating our
   data - and maximimize the margin of this hyperplane to optimize
   decision-theoretic metrics.

   let $\kappa$ be a id81 on $\mathbb{r}^d \times \mathbb{r}^d$
   - a function such that the matrix $k$ with $k_{ij} = \kappa(x_i, x_j)$
   is positive semidefinite. a key property of such id81s is
   that there exists a map $\nu$ such that $\langle \nu(x), \nu(y) \rangle
   = \kappa(x, y)$. one can think of $\nu$ as mapping our input features
   into a higher dimensional output space.

   we can show that for a given feature mapping $\nu$ satisfying the
   previous condition, the lagrangian for the problem of finding the
   maximum margin hyperplane takes the form:

   \begin{equation} \inf_{z \in \mathbb{r}^n} \frac{1}{2} \left|
   \sum_{i=1}^{n} y_i \nu(x_i) z_i \right|_2^2 - e^t z \end{equation}
   subject to $z \geq 0$ and $\langle z, y \rangle = 0$.

   given a resulting vector of lagrange multipliers $z$, we find that most
   $z$ are zero. this comes from the complementary slackness conditions in
   our optimization problem - either $(x_i, y_i)$ is on the maximum margin
   (and so corresponding lagrange multiplier is nonzero), or it is not on
   the margin (and so the lagrange multiplier is zero).

   the prediction of a given feature vector $x$ takes the form
   \begin{align} \label{eq:1} \langle w, \nu(x) \rangle &= \sum_{i=1}^{n}
   z_{i} y_{i} \langle \nu(x_{i}), \nu(x) \rangle \ &= \sum_{i=1}^{n}
   z_{i} y_{i} \kappa(x_{i}, x) \end{align} where we can take the sum over
   only the non-zero $z_{i}$.

   this yields a very efficient prediction algorithm - once we have
   trained our id166, a large amount of the training data (those samples
   with zero lagrangian multipliers) can be removed.

   there are more complications (handling the bias term, handling
   non-separable datasets), but this is the gist of the algorithm.

implementation

   the full implementation of the training (using [9]cvxopt as a quadratic
   program solver) in python is given below:

   the code is fairly self-explanatory, and follows the given training
   algorithm quite closely. to compute our lagrange multipliers, we simply
   construct the gram matrix and solve the given qp. we then pass our
   trained support vectors and their corresponding lagrange multipliers
   and weights to the id166predictor, whose implementation is given below.

   this simply implements the above prediction equation.

   a sample list of id81s are given in

demonstration

   we demonstrate drawing pairs of independent standard normal variables
   as features, and label $y_i = sign(\sum x)$. this is trivially linearly
   seperable, so we train a linear id166 (where $\kappa(x_i, x_j) = \langle
   x_i, x_j \rangle$) on the sample data.

   we then visualize the samples and decision boundary of the id166 on this
   dataset, using [10]matplotlib. see [11]this gist for details on the
   implementation.

   an example output of this demonstration is given below:

   id166 demonstration

more information

   see the [12]id166py library on github for all code used in this post.
   please enable javascript to view the [13]comments powered by disqus.
   [14]blog comments powered by disqus

     2014 andrew tulloch

references

   1. http://tullo.ch/feed.xml
   2. http://tullo.ch/
   3. http://tullo.ch/about/
   4. http://tullo.ch/academic/
   5. https://github.com/ajtulloch
   6. http://tullo.ch/static/cv.pdf
   7. http://tullo.ch/articles/id166-py/
   8. http://en.wikipedia.org/wiki/support_vector_machine
   9. http://cvxopt.org/
  10. http://matplotlib.org/
  11. https://gist.github.com/ajtulloch/7655467
  12. https://github.com/ajtulloch/id166py
  13. http://disqus.com/?ref_noscript
  14. http://disqus.com/
