   #[1]quora [2]next

   [3]quora
   ____________________

   sign in
   [4]support vector machines
   [5]layman's explanations

what does support vector machine (id166) mean in layman's terms?

   updatecancel

   andjnfn zlbhlxuyjftkk plpxkuaooqnmqnbqxdooacanfl blfdaltalhxbtsc
   machine learning is faster when you collaborate with your team.
   our servers make that possible. we build hardware for ml, and we're
   trusted by amazon research and mit.
   [6]lrejosrvaqmurxvndsga lmmhoiarwugewzwja eazzntkgd
   ujmlyaeedtmmjzzobmsydhvqtasghlyt.hpcxzkluotgmwpq
   you dismissed this ad.
   the feedback you provide will help us show you more relevant content in
   the future.
   undo

answer wiki

   21 answers
   [7]happy mittal
   [8]happy mittal, phd scholar, cse, iitd
   [9]updated 124w ago

   in layman   s terms, support vector machine is a generalization of
   [10]nearest neighbor (nn) algorithm. so let us understand nn first (if
   you already know about nn, you can directly jump to id166 part below).

   nearest neighbor algorithm

   it is a very simple algorithm. you are given a training data consisting
   of [math]m[/math] training instances
   [math]\{(x^1,y^1),(x^2,y^2),\ldots[/math][math],(x^m,y^m)\}[/math].

   here [math]y^i[/math] is the class label of [math]i^\text{th}[/math]
   instance. for example, in the figure below, each instance can have
   either label [math]1[/math] (blue points), or label [math]-1[/math]
   (red points).
   [main-qimg-ee0419f7e2aa4543d0840edffa26af25]

   for now, let us assume we have two possible classes [math]1[/math] or
   [math]-1[/math].

   now you are given a test point (black point above), and you have to
   predict its class, whether it belongs to red class or blue class. in nn
   algorithm, you find the nearest training point to this test point (by
   measuring distance of test point from every training point), and the
   class predicted of test point is same as of nearest training point.
   from now on, we will use the word similarity instead of distance. lower
   the distance, higher the similarity between two points. this will be
   useful to understand id166s. for example, the circled point is closest to
   test point, and hence class of test point is blue.

   so some of the observations from nn algorithm are :
    1. there is no training phase in this algorithm. we don   t do any
       computation alone with training points. only when a test point
       comes, we compute similarity from every training point. this is a
       big disadvantage of nn algorithm. consider having millions of
       training points, and for every test point, we have to calculate
       millions of similarities from test point.
    2. we calculate similarities from the training points which are very
       very far from test points, which are not really required.
    3. we don   t give any importance to other training points except the
       nearest one.

   support vector machine

   in id166, we remove these disadvantages as :
    1. instead of finding similarity from every training point, we
       calculate similarity from only a subset of training points, which
       we compute in the training phase. so in the training phase, we fix
       a subset of training points from which we compute the similarity of
       any test point. these selected training points are called support
       vectors, since only these points will support our decision of
       selecting the class of a test point. our hope is that our training
       phase finds as few as support vectors so that we have to compute
       fewer number of similarities.
    2. now once we have selected support vectors, we assign a weight to
       each support vector, which basically tells how much importance we
       want to give to that support vector while making our decision. so
       unlike nn, we don   t give importance to only a single training
       point, instead we give each support vector a separate importance.

   now how do we take the decision in this setting? we just compute a
   weighted sum of similarities from test point to each of the support
   vector and compute the class based on this weighted sum.

   more concretely, suppose out of [math]m[/math] training instances, we
   have selected [math]k[/math] instances as support vectors. let these
   support vectors are
   [math]\{(s^1,y^1),(s^2,y^2),\ldots,(s^k,y^k)\}[/math], and let their
   weights are [math]\{\alpha^1,\alpha^2,\ldots,\alpha^k\}[/math]. let
   [math]x[/math] is a test point. now we compute weighted sum of
   similarities from [math]x[/math] to each of the support vectors :

   [math]d=\sum_{j=1}^k y^j*\alpha^j*sim(x,s^j)[/math]

   here [math]sim(x,s^j)[/math] is the similarity between test point
   [math]x[/math] and support vector [math]s^j[/math]. note that we have
   also multiplied each weighted similarity with the class of that support
   vector i.e. by [math]1[/math] or [math]-1[/math], so that
   [math]d[/math] is favored towards the class of that support vector.

   finally, if [math]d > 0[/math], we predict class of x to be
   [math]1[/math], else [math]-1[/math].

   now having understood what id166s are, we have following questions :
    1. how do we choose support vectors ? it is actually a by-product of
       finding weights (the alphas). so in training phase, we find weight
       for each training point, and those points whose weight becomes zero
       are not support vectors i.e. their importance is zero during test
       time, and rest are support vectors.
    2. so then how to learn alphas ? its at this point, where all the math
       comes. since the question asks for layman   s explanation, i will
       skip the math here (you can read the math [11]here). intuitively,
       we try to find that separating hyperplane, from which distance of
       closest training points is maximum (also known as max-margin
       classifier), and those closest training points then become support
       vectors. during the math, while optimizing the function, we get the
       weights of the support vectors. pictorially, this looks like :

   [main-qimg-307b5437e0e060f6563ded9d5391ba2e]

   in the above figure, there are only 3 support vectors, so at the test
   time, we will compute similarity test point and only these 3 support
   vectors. generally, there are very few support vectors out of all
   training points, and hence at test time, we need to calculate very few
   similarities.

   3. what exactly the similarity function [math]sim(x,s^j)[/math] is ?
   during the math we do in above step, it turns out that the similarity
   function is dot product i.e. [math]sim(x,s^j)=\langle
   x,s^j\rangle[/math] (remember dot product measures similarity between
   two vectors).

   the fun part : now here comes the fun part, instead of taking dot
   product between two simple vectors [math]x[/math] and [math]s[/math]
   (where [math]s[/math] is one of the support vectors), we can transform
   each of the vector into some other higher dimensional vector, and then
   take dot product between those two complex, higher dimensional vectors,
   and whole thing still works, and in fact works better since we have
   improved the similarity function. in particular, we can apply some
   transformation function [math]\phi[/math] to [math]x[/math] and
   [math]s[/math] to get two big vectors [math]\phi(x)[/math] and
   [math]\phi(s)[/math] and then we can take their dot product to get a
   quantity which is called kernel :

   [math]k(x,s)=\langle\phi(x),\phi(s)\rangle[/math]

   so kernel is nothing but a similarity measure between two transformed
   vectors.

   so our equation for computing [math]d[/math] becomes :

   [math]d=\sum_{j=1}^k y^j*\alpha^j*k(x,s^j)[/math]

   the kernel trick :

   so now the question arises whether we need to actually transform the
   vectors into higher dimension to get the kernel value ? it turns out
   that the answer is not necessarily. see, in above equation of
   calculating [math]d[/math], we require only the final answer of
   [math]k(x,s^j)[/math]. we actually don   t care, which transformation
   function was applied to each of the vectors. so if there is a
   similarity function (a kernel), which can be factorized into dot
   product of two transformed vectors, then that id81 can be
   used in above equation. we don   t even need to compute exact
   transformation, as long as we can prove above thing. this is called the
   kernel trick.

   so earlier, we used a very simple kernel, which was just the dot
   product between [math]x[/math] and [math]s[/math] (in their original
   dimension). now we may ask the question whether square of that dot
   product is a kernel ? i.e. whether

   [math]k(x,s)=(\langle x,s\rangle)^2[/math]

   is a kernel or not. for simplicity, let both vectors [math]x[/math] and
   [math]s[/math] are two dimensional vectors. we can prove that it is in
   fact a valid kernel by noting that

   [math](\langle
   x,s\rangle)^2=(x_1s_1+x_2s_2)^2=(x_1^2s_1^2+x_2^2s_2^2+2x_1s_1x_2s_2)[/
   math]

   [math]=\langle
   \{x_1^2,x_2^2,\sqrt{2}x_1x_2\},\{s_1^2,s_2^2,\sqrt{2}s_1s_2\}\rangle[/m
   ath]

   so as we see here, this kernel is a dot product between two vectors :

   [math]\phi(x)=\{x_1^2,x_2^2,\sqrt{2}x_1x_2\},
   \phi(s)=\{s_1^2,s_2^2,\sqrt{2}s_1s_2\}[/math]

   which are nothing but transformation of vectors [math]x[/math] and
   [math]s[/math].

   note that we don   t have to actually calculate transformed vectors, we
   just require id81, which is very easy to compute, for
   example, here, its just the square of dot product.

   4. which id81 to use ? we have 3 kinds of common kernel
   functions :
     * linear kernel : [math] k(x,s)=\langle x,s\rangle[/math]. this is
       just the dot product between original vectors.
     * polynomial kernel : [math] k(x,s)=(\langle x,s\rangle)^p[/math]. we
       saw an example of this above, in which [math]p=2[/math].
     * gaussian kernel : [math]k(x,s)=\exp(-||x-s||/\sigma)[/math]. this
       is just an unnormalized gaussian function (we don   t need to
       normalize it, since we need just similarity measure, and not a
       distribution). this is super powerful kernel since it is the dot
       product between two infinite dimensional vectors. so the amazing
       part is that we are calculating dot product between two infinite
       dimensional vectors without actually having those vectors !!!.

   so to summarize, id166 is the generalization of nn algorithm with weights
   assigned to some of the training instances (support vectors) which
   decide the class of test instance. moreover, instead of using simple
   similarity functions, we can apply a variety of complex similarity
   functions.

   there is a lot of math behind it, but i think you get the big picture.
   9.4k views    [12]view 154 upvoters
   thank you for your feedback!
   your feedback is private.
   is this answer still relevant and up to date?
   [button input] (not implemented)___ [button input] (not implemented)__
   upvote    154
   share    0

   pvoarmqukyodmwooxtvkotfesrcgbdumu gbqypelq
   aadbauqvytcgmczdkxddqzzvmusotoycbsjkhcnpgizjzkoxmiz
   what is the revenue generation model for duckduckgo?
   [13]gabriel weinberg
   [14]gabriel weinberg, ceo & founder at duckduckgo.com (2008-present)
   [15]updated 42w ago

   duckduckgo has been a profitable company since 2014 without storing or
   sharing any personal information on people using our search engine. as
   we like to say, what you search on duckduckgo is private, even from us!
   we   re proud to have a business model for a web-based business that   s
   prof...
   [16](continue reading)
   you dismissed this ad.
   the feedback you provide will help us show you more relevant content in
   the future.
   undo

related questions[17]more answers below

     * [18]what is a support vector machine?
     * [19]in layman's terms, how does id166 work?
     * [20]how exactly can the id166 (support vector machine) be solved?
     * [21]how do you teach support vector machine to a beginner in
       machine learning?
     * [22]what is the explanation of the support vector machine algorithm
       in layman language?

   view more

related questions

     * [23]how does support vector machine (id166) work in text
       classification? assume the words have been converted to vectors.
     * [24]how much functional analysis do i need to know in order to
       fully understand support vector machine (id166)?
     * [25]what is the mathematical definition of margin in support vector
       machine (id166)?
     * [26]how does id166 "support vector machine" work and what are the
       alternative ways?
     * [27]what is a kernlab support vector machine?
     * [28]how effective is support vector machine algorithm?
     * [29]how can you find support vectors when programming id166?
     * [30]how do support vector machines work?
     * [31]how are vectors used in machine learning?
     * [32]what is a vector machine?
     * [33]how does a id166 choose its support vectors?
     * [34]what is a hierarchical  support vector machine (id166)?
     * [35]what is the explanation behind the name "support vector
       machine"?
     * [36]what is id166 support?
     * [37]what is a support vector machine? why don   t the researchers use
       id166 instead of ann?

related questions

     * [38]what is a support vector machine?
     * [39]in layman's terms, how does id166 work?
     * [40]how exactly can the id166 (support vector machine) be solved?
     * [41]how do you teach support vector machine to a beginner in
       machine learning?
     * [42]what is the explanation of the support vector machine algorithm
       in layman language?
     * [43]how does support vector machine (id166) work in text
       classification? assume the words have been converted to vectors.
     * [44]how much functional analysis do i need to know in order to
       fully understand support vector machine (id166)?
     * [45]what is the mathematical definition of margin in support vector
       machine (id166)?
     * [46]how does id166 "support vector machine" work and what are the
       alternative ways?
     * [47]what is a kernlab support vector machine?

   [48]about    [49]careers    [50]privacy    [51]terms    [52]contact

references

   visible links
   1. https://www.quora.com/opensearch/description.xml
   2. https://www.quora.com/what-does-support-vector-machine-id166-mean-in-laymans-terms?page_id=2
   3. https://www.quora.com/
   4. https://www.quora.com/topic/support-vector-machines
   5. https://www.quora.com/topic/laymans-explanations
   6. https://lambdal.com/
   7. https://www.quora.com/profile/happy-mittal
   8. https://www.quora.com/profile/happy-mittal
   9. https://www.quora.com/what-does-support-vector-machine-id166-mean-in-laymans-terms/answer/happy-mittal
  10. https://en.wikipedia.org/wiki/nearest_neighbor_search
  11. http://cs229.stanford.edu/notes/cs229-notes3.pdf
  12. https://www.quora.com/what-does-support-vector-machine-id166-mean-in-laymans-terms
  13. https://www.quora.com/profile/gabriel-weinberg
  14. https://www.quora.com/profile/gabriel-weinberg
  15. https://www.quora.com/what-is-the-revenue-generation-model-for-duckduckgo/answers/67404040
  16. https://www.quora.com/what-does-support-vector-machine-id166-mean-in-laymans-terms
  17. https://www.quora.com/what-does-support-vector-machine-id166-mean-in-laymans-terms#moreanswers
  18. https://www.quora.com/what-is-a-support-vector-machine
  19. https://www.quora.com/in-laymans-terms-how-does-id166-work
  20. https://www.quora.com/how-exactly-can-the-id166-support-vector-machine-be-solved
  21. https://www.quora.com/how-do-you-teach-support-vector-machine-to-a-beginner-in-machine-learning
  22. https://www.quora.com/what-is-the-explanation-of-the-support-vector-machine-algorithm-in-layman-language
  23. https://www.quora.com/how-does-support-vector-machine-id166-work-in-text-classification-assume-the-words-have-been-converted-to-vectors
  24. https://www.quora.com/how-much-functional-analysis-do-i-need-to-know-in-order-to-fully-understand-support-vector-machine-id166
  25. https://www.quora.com/what-is-the-mathematical-definition-of-margin-in-support-vector-machine-id166
  26. https://www.quora.com/how-does-id166-support-vector-machine-work-and-what-are-the-alternative-ways
  27. https://www.quora.com/what-is-a-kernlab-support-vector-machine
  28. https://www.quora.com/how-effective-is-support-vector-machine-algorithm
  29. https://www.quora.com/how-can-you-find-support-vectors-when-programming-id166
  30. https://www.quora.com/how-do-support-vector-machines-work
  31. https://www.quora.com/how-are-vectors-used-in-machine-learning
  32. https://www.quora.com/what-is-a-vector-machine
  33. https://www.quora.com/how-does-a-id166-choose-its-support-vectors
  34. https://www.quora.com/what-is-a-hierarchical-support-vector-machine-id166
  35. https://www.quora.com/what-is-the-explanation-behind-the-name-support-vector-machine
  36. https://www.quora.com/unanswered/what-is-id166-support
  37. https://www.quora.com/what-is-a-support-vector-machine-why-don-t-the-researchers-use-id166-instead-of-ann
  38. https://www.quora.com/what-is-a-support-vector-machine
  39. https://www.quora.com/in-laymans-terms-how-does-id166-work
  40. https://www.quora.com/how-exactly-can-the-id166-support-vector-machine-be-solved
  41. https://www.quora.com/how-do-you-teach-support-vector-machine-to-a-beginner-in-machine-learning
  42. https://www.quora.com/what-is-the-explanation-of-the-support-vector-machine-algorithm-in-layman-language
  43. https://www.quora.com/how-does-support-vector-machine-id166-work-in-text-classification-assume-the-words-have-been-converted-to-vectors
  44. https://www.quora.com/how-much-functional-analysis-do-i-need-to-know-in-order-to-fully-understand-support-vector-machine-id166
  45. https://www.quora.com/what-is-the-mathematical-definition-of-margin-in-support-vector-machine-id166
  46. https://www.quora.com/how-does-id166-support-vector-machine-work-and-what-are-the-alternative-ways
  47. https://www.quora.com/what-is-a-kernlab-support-vector-machine
  48. https://www.quora.com/about
  49. https://www.quora.com/careers
  50. https://www.quora.com/about/privacy
  51. https://www.quora.com/about/tos
  52. https://www.quora.com/contact

   hidden links:
  54. https://lambdal.com/
  55. https://lambdal.com/
  56. https://www.quora.com/what-does-support-vector-machine-id166-mean-in-laymans-terms
  57. https://www.quora.com/what-does-support-vector-machine-id166-mean-in-laymans-terms
  58. https://www.quora.com/what-is-the-revenue-generation-model-for-duckduckgo/answers/67404040?pa_story=mja2njk0nduzmzmxnjczmteynxwymtexmdyymzmyote3mtl8ma**
