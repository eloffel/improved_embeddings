   #[1]edwin chen's blog atom feed [2]edwin chen's blog rss feed

[3]introduction to id49

   imagine you have a sequence of snapshots from a day in justin bieber   s
   life, and you want to label each image with the activity it represents
   (eating, sleeping, driving, etc.). how can you do this?

   one way is to ignore the sequential nature of the snapshots, and build
   a per-image classifier. for example, given a month   s worth of labeled
   snapshots, you might learn that dark images taken at 6am tend to be
   about sleeping, images with lots of bright colors tend to be about
   dancing, images of cars are about driving, and so on.

   by ignoring this sequential aspect, however, you lose a lot of
   information. for example, what happens if you see a close-up picture of
   a mouth     is it about singing or eating? if you know that the previous
   image is a picture of justin bieber eating or cooking, then it   s more
   likely this picture is about eating; if, however, the previous image
   contains justin bieber singing or dancing, then this one probably shows
   him singing as well.

   thus, to increase the accuracy of our labeler, we should incorporate
   the labels of nearby photos, and this is precisely what a conditional
   random field does.

part-of-speech tagging

   let   s go into some more detail, using the more common example of
   part-of-speech tagging.

   in id52, the goal is to label a sentence (a sequence of words or
   tokens) with tags like adjective, noun, preposition, verb, adverb,
   article.

   for example, given the sentence    bob drank coffee at starbucks   , the
   labeling might be    bob (noun) drank (verb) coffee (noun) at
   (preposition) starbucks (noun)   .

   so let   s build a conditional random field to label sentences with their
   parts of speech. just like any classifier, we   ll first need to decide
   on a set of feature functions \(f_i\).

feature functions in a crf

   in a crf, each feature function is a function that takes in as input:
     * a sentence s
     * the position i of a word in the sentence
     * the label \(l_i\) of the current word
     * the label \(l_{i-1}\) of the previous word

   and outputs a real-valued number (though the numbers are often just
   either 0 or 1).

   (note: by restricting our features to depend on only the current and
   previous labels, rather than arbitrary labels throughout the sentence,
   i   m actually building the special case of a linear-chain crf. for
   simplicity, i   m going to ignore general crfs in this post.)

   for example, one possible feature function could measure how much we
   suspect that the current word should be labeled as an adjective given
   that the previous word is    very   .

features to probabilities

   next, assign each feature function \(f_j\) a weight \(\lambda_j\) (i   ll
   talk below about how to learn these weights from the data). given a
   sentence s, we can now score a labeling l of s by adding up the
   weighted features over all words in the sentence:

   \(score(l | s) = \sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l_i,
   l_{i-1})\)

   (the first sum runs over each feature function \(j\), and the inner sum
   runs over each position \(i\) of the sentence.)

   finally, we can transform these scores into probabilities \(p(l | s)\)
   between 0 and 1 by exponentiating and normalizing:

   \(p(l | s) = \frac{exp[score(l|s)]}{\sum_{l   } exp[score(l   |s)]} =
   \frac{exp[\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s, i, l_i,
   l_{i-1})]}{\sum_{l   } exp[\sum_{j = 1}^m \sum_{i = 1}^n \lambda_j f_j(s,
   i, l   _i, l   _{i-1})]}\)

example feature functions

   so what do these feature functions look like? examples of id52
   features could include:
     * \(f_1(s, i, l_i, l_{i-1}) = 1\) if \(l_i =\) adverb and the ith
       word ends in    -ly   ; 0 otherwise. ** if the weight \(\lambda_1\)
       associated with this feature is large and positive, then this
       feature is essentially saying that we prefer labelings where words
       ending in -ly get labeled as adverb.
     * \(f_2(s, i, l_i, l_{i-1}) = 1\) if \(i = 1\), \(l_i =\) verb, and
       the sentence ends in a question mark; 0 otherwise. ** again, if the
       weight \(\lambda_2\) associated with this feature is large and
       positive, then labelings that assign verb to the first word in a
       question (e.g.,    is this a sentence beginning with a verb?   ) are
       preferred.
     * \(f_3(s, i, l_i, l_{i-1}) = 1\) if \(l_{i-1} =\) adjective and
       \(l_i =\) noun; 0 otherwise. ** again, a positive weight for this
       feature means that adjectives tend to be followed by nouns.
     * \(f_4(s, i, l_i, l_{i-1}) = 1\) if \(l_{i-1} =\) preposition and
       \(l_i =\) preposition. ** a negative weight \(\lambda_4\) for this
       function would mean that prepositions don   t tend to follow
       prepositions, so we should avoid labelings where this happens.

   and that   s it! to sum up: to build a conditional random field, you just
   define a bunch of feature functions (which can depend on the entire
   sentence, a current position, and nearby labels), assign them weights,
   and add them all together, transforming at the end to a id203 if
   necessary.

   now let   s step back and compare crfs to some other common machine
   learning techniques.

smells like id28   

   the form of the crf probabilities \(p(l | s) = \frac{exp[\sum_{j = 1}^m
   \sum_{i = 1}^n f_j(s, i, l_i, l_{i-1})]}{\sum_{l   } exp[\sum_{j = 1}^m
   \sum_{i = 1}^n f_j(s, i, l   _i, l   _{i-1})]}\) might look [4]familiar.

   that   s because crfs are indeed basically the sequential version of
   id28: whereas id28 is a log-linear model
   for classification, crfs are a log-linear model for sequential labels.

looks like id48s   

   recall that id48 are another model for part-of-speech
   tagging (and sequential labeling in general). whereas crfs throw any
   bunch of functions together to get a label score, id48s take a
   generative approach to labeling, defining

   \(p(l,s) = p(l_1) \prod_i p(l_i | l_{i-1}) p(w_i | l_i)\)

   where
     * \(p(l_i | l_{i-1})\) are transition probabilities (e.g., the
       id203 that a preposition is followed by a noun);
     * \(p(w_i | l_i)\) are emission probabilities (e.g., the id203
       that a noun emits the word    dad   ).

   so how do id48s compare to crfs? crfs are more powerful     they can model
   everything id48s can and more. one way of seeing this is as follows.

   note that the log of the id48 id203 is \(\log p(l,s) = \log p(l_0)
   + \sum_i \log p(l_i | l_{i-1}) + \sum_i \log p(w_i | l_i)\). this has
   exactly the log-linear form of a crf if we consider these
   log-probabilities to be the weights associated to binary transition and
   emission indicator features.

   that is, we can build a crf equivalent to any id48 by   
     * for each id48 transition id203 \(p(l_i = y | l_{i-1} = x)\),
       define a set of crf transition features of the form \(f_{x,y}(s, i,
       l_i, l_{i-1}) = 1\) if \(l_i = y\) and \(l_{i-1} = x\). give each
       feature a weight of \(w_{x,y} = \log p(l_i = y | l_{i-1} = x)\).
     * similarly, for each id48 emission id203 \(p(w_i = z | l_{i} =
       x)\), define a set of crf emission features of the form
       \(g_{x,y}(s, i, l_i, l_{i-1}) = 1\) if \(w_i = z\) and \(l_i = x\).
       give each feature a weight of \(w_{x,z} = \log p(w_i = z | l_i =
       x)\).

   thus, the score \(p(l|s)\) computed by a crf using these feature
   functions is precisely proportional to the score computed by the
   associated id48, and so every id48 is equivalent to some crf.

   however, crfs can model a much richer set of label distributions as
   well, for two main reasons:
     * crfs can define a much larger set of features. whereas id48s are
       necessarily local in nature (because they   re constrained to binary
       transition and emission feature functions, which force each word to
       depend only on the current label and each label to depend only on
       the previous label), crfs can use more global features. for
       example, one of the features in our pos tagger above increased the
       id203 of labelings that tagged the first word of a sentence
       as a verb if the end of the sentence contained a question mark.
     * crfs can have arbitrary weights. whereas the probabilities of an
       id48 must satisfy certain constraints (e.g., \(0 <= p(w_i | l_i) <=
       1, \sum_w p(w_i = w | l_1) = 1)\), the weights of a crf are
       unrestricted (e.g., \(\log p(w_i | l_i)\) can be anything it
       wants).

learning weights

   let   s go back to the question of how to learn the feature weights in a
   crf. one way, unsurprisingly, is to use id119.

   assume we have a bunch of training examples (sentences and associated
   part-of-speech labels). randomly initialize the weights of our crf
   model. to shift these randomly initialized weights to the correct ones,
   for each training example   
     * go through each feature function \(f_i\), and calculate the
       gradient of the log id203 of the training example with
       respect to \(\lambda_i\): \(\frac{\partial}{\partial w_j} \log p(l
       | s) = \sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) - \sum_{l   } p(l    | s)
       \sum_{j = 1}^m f_i(s, j, l   _j, l   _{j-1})\)
     * note that the first term in the gradient is the contribution of
       feature \(f_i\) under the true label, and the second term in the
       gradient is the expected contribution of feature \(f_i\) under the
       current model. this is exactly the form you   d expect gradient
       ascent to take.
     * move \(\lambda_i\) in the direction of the gradient: \(\lambda_i =
       \lambda_i + \alpha [\sum_{j = 1}^m f_i(s, j, l_j, l_{j-1}) -
       \sum_{l   } p(l    | s) \sum_{j = 1}^m f_i(s, j, l   _j, l   _{j-1})]\)
       where \(\alpha\) is some learning rate.
     * repeat the previous steps until some stopping condition is reached
       (e.g., the updates fall below some threshold).

   in other words, every step takes the difference between what we want
   the model to learn and the model   s current state, and moves
   \(\lambda_i\) in the direction of this difference.

finding the optimal labeling

   suppose we   ve trained our crf model, and now a new sentence comes in.
   how do we do label it?

   the naive way is to calculate \(p(l | s)\) for every possible labeling
   l, and then choose the label that maximizes this id203. however,
   since there are \(k^m\) possible labels for a tag set of size k and a
   sentence of length m, this approach would have to check an exponential
   number of labels.

   a better way is to realize that (linear-chain) crfs satisfy an
   [5]optimal substructure property that allows us to use a
   (polynomial-time) id145 algorithm to find the optimal
   label, similar to the [6]viterbi algorithm for id48s.

a more interesting application

   okay, so part-of-speech tagging is kind of boring, and there are plenty
   of existing pos taggers out there. when might you use a crf in real
   life?

   suppose you want to mine twitter for the types of presents people
   received for christmas:

     what people on twitter wanted for christmas, and what they got:
     [7]twitter.com/edchedch/statu   
         edwin chen (@echen) [8]january 2, 2012

   how can you figure out which words refer to gifts?

   to gather data for the graphs above, i simply looked for phrases of the
   form    i want xxx for christmas    and    i got xxx for christmas   . however,
   a more sophisticated crf variant could use a gift part-of-speech-like
   tag (even adding other tags like gift-giver and gift-receiver, to get
   even more information on who got what from whom) and treat this like a
   id52 problem. features could be based around things like    this
   word is a gift if the previous word was a gift-receiver and the word
   before that was    gave       or    this word is a gift if the next two words
   are    for christmas      .

   [9]edwin chen

   ai, stats, data science, human computation.

   math/cs/linguistics at mit, id103 at msr, quant trading at
   clarium, ads at twitter, data science at dropbox, stats/ml at google.
   [10]quora
   [11]twitter
   [12]github
   [13]linkedin
   [14]email

recent posts

   [15]exploring lstms
   [16]moving beyond ctr: better recommendations through human evaluation
   [17]propensity modeling, causal id136, and discovering drivers of
   growth
   [18]product insights for airbnb
   [19]improving twitter search with real-time human computation
   [20]edge prediction in a social graph: my solution to facebook's user
   recommendation contest on kaggle
   [21]soda vs. pop with twitter
   [22]infinite mixture models with nonparametric bayes and the dirichlet
   process
   [23]instant interactive visualization with d3 + ggplot2
   [24]movie recommendations and more via mapreduce and scalding
   [25]quick introduction to ggplot2
   [26]introduction to id49
   [27]winning the netflix prize: a summary
   [28]stuff harvard people like
   [29]information transmission in a social network: dissecting the spread
   of a quora post


    proudly powered by [30]pelican, which takes great advantage of
    [31]python.

references

   1. http://blog.echen.me/feeds/all.atom.xml
   2. http://blog.echen.me/feeds/all.rss.xml
   3. http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/
   4. http://en.wikipedia.org/wiki/logistic_regression
   5. http://en.wikipedia.org/wiki/optimal_substructure
   6. http://en.wikipedia.org/wiki/viterbi_algorithm
   7. http://t.co/egektbgf
   8. https://twitter.com/edchedch/status/153683967315419136
   9. http://blog.echen.me/
  10. https://quora.com/edwin-chen-1
  11. https://twitter.com/echen
  12. https://github.com/echen
  13. https://www.linkedin.com/in/edwinchen1
  14. mailto:hello[  ]echen.me
  15. http://blog.echen.me/2017/05/30/exploring-lstms/
  16. http://blog.echen.me/2014/10/07/moving-beyond-ctr-better-recommendations-through-human-evaluation/
  17. http://blog.echen.me/2014/08/15/propensity-modeling-causal-id136-and-discovering-drivers-of-growth/
  18. http://blog.echen.me/2014/08/14/product-insights-for-airbnb/
  19. http://blog.echen.me/2013/01/08/improving-twitter-search-with-real-time-human-computation
  20. http://blog.echen.me/2012/07/31/edge-prediction-in-a-social-graph-my-solution-to-facebooks-user-recommendation-contest-on-kaggle/
  21. http://blog.echen.me/2012/07/06/soda-vs-pop-with-twitter/
  22. http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/
  23. http://blog.echen.me/2012/03/05/instant-interactive-visualization-with-d3-and-ggplot2/
  24. http://blog.echen.me/2012/02/09/movie-recommendations-and-more-via-mapreduce-and-scalding/
  25. http://blog.echen.me/2012/01/17/quick-introduction-to-ggplot2/
  26. http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/
  27. http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/
  28. http://blog.echen.me/2011/09/29/stuff-harvard-people-like/
  29. http://blog.echen.me/2011/09/07/information-transmission-in-a-social-network-dissecting-the-spread-of-a-quora-post/
  30. http://getpelican.com/
  31. http://python.org/
