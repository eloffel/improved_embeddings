   #[1]wildml    feed [2]wildml    comments feed [3]wildml    understanding
   convolutional neural networks for nlp comments feed [4]recurrent neural
   network tutorial, part 4     implementing a gru/lstm id56 with python and
   theano [5]implementing a id98 for text classification in tensorflow
   [6]alternate [7]alternate

   [8]skip to content

   [9]wildml

   artificial intelligence, deep learning, and nlp

   (button) menu
     * [10]home
     * [11]ai newsletter
     * [12]deep learning glossary
     * [13]contact
     * [14]about

   posted on [15]november 7, 2015january 10, 2016 by [16]denny britz

understanding convolutional neural networks for nlp

   when we hear about convolutional neural network (id98s), we typically
   think of id161. id98s were responsible for major breakthroughs
   in image classification and are the core of most id161
   systems today, from facebook   s automated photo tagging to self-driving
   cars.

   more recently we   ve also started to apply id98s to problems in natural
   language processing and gotten some interesting results. in this post
   i   ll try to summarize what id98s are, and how they   re used in nlp. the
   intuitions behind id98s are somewhat easier to understand for
   the id161 use case, so i   ll start there, and then slowly move
   towards nlp.

what is convolution?

   the for me easiest way to understand a convolution is by thinking of it
   as a sliding window function applied to a matrix. that   s a mouthful,
   but it becomes quite clear looking at a visualization:
   convolution with 3  3 filter. source:
   http://deeplearning.stanford.edu/wiki/index.php/feature_extraction_usin
   g_convolution

   imagine that the matrix on the left represents an black and white
   image. each entry corresponds to one pixel, 0 for black and 1 for white
   (typically it   s between 0 and 255 for grayscale images). the sliding
   window is called a kernel, filter, or feature detector. here we use a
   3  3 filter, multiply its values element-wise with the original matrix,
   then sum them up. to get the full convolution we do this for each
   element by sliding the filter over the whole matrix.

   you may be wondering wonder what you can actually do with this. here
   are some intuitive examples.

averaging each pixel with its neighboring values blurs an image:




taking the difference between a pixel and its neighbors detects edges:

   (to understand this one intuitively, think about what happens in parts
   of the image that are smooth, where a pixel color equals that of its
   neighbors: the additions cancel and the resulting value is 0, or black.
   if there   s a sharp edge in intensity, a transition from white to black
   for example, you get a large difference and a resulting white value)




   the[17] gimp manual has a few other examples. to understand more about
   how convolutions work i also recommend checking out [18]chris olah   s
   post on the topic.

what are convolutional neural networks?

   now you know what convolutions are. but what about id98s? id98s are
   basically just several layers of convolutions with nonlinear activation
   functions like [19]relu or [20]tanh applied to the results. in a
   traditional feedforward neural network we connect each input neuron to
   each output neuron in the next layer. that   s also called a fully
   connected layer, or affine layer. in id98s we don   t do that. instead, we
   use convolutions over the input layer to compute the output. this
   results in local connections, where each region of the input is
   connected to a neuron in the output. each layer applies different
   filters, typically hundreds or thousands like the ones showed above,
   and combines their results. there   s also something something called
   pooling (subsampling) layers, but i   ll get into that later. during the
   training phase, a id98 automatically learns the values of its filters
   based on the task you want to perform. for example, in image
   classification a id98 may learn to detect edges from raw pixels in the
   first layer, then use the edges to detect simple shapes in the second
   layer, and then use these shapes to deter higher-level features, such
   as facial shapes in higher layers. the last layer is then a classifier
   that uses these high-level features.

   [21]convolutional neural network (clarifai)

   there are two aspects of this computation worth paying attention to:
   location invariance and compositionality. let   s say you want to
   classify whether or not there   s an elephant in an image. because you
   are sliding your filters over the whole image you don   t really care
   where the elephant occurs. in practice,  pooling also gives
   you invariance to translation, rotation and scaling, but more on that
   later. the second key aspect is (local) compositionality. each filter
   composes a local patch of lower-level features into higher-level
   representation. that   s why id98s are so powerful in id161. it
   makes intuitive sense that you build edges from pixels, shapes from
   edges, and more complex objects from shapes.

so, how does any of this apply to nlp?

   instead of image pixels, the input to most nlp tasks are sentences or
   documents represented as a matrix. each row of the matrix corresponds
   to one token, typically a word, but it could be a character. that is,
   each row is vector that represents a word. typically, these vectors are
   id27s (low-dimensional representations) like [22]id97 or
   [23]glove, but they could also be one-hot vectors that index the word
   into a vocabulary. for a 10 word sentence using a 100-dimensional
   embedding we would have a 10  100 matrix as our input. that   s our
      image   .

   in vision, our filters slide over local patches of an image, but in nlp
   we typically use filters that slide over full rows of the matrix
   (words). thus, the    width    of our filters is usually the same as the
   width of the input matrix. the height, or region size, may vary, but
   sliding windows over 2-5 words at a time is typical. putting all the
   above together, a convolutional neural network for nlp may look like
   this (take a few minutes and try understand this picture and how the
   dimensions are computed. you can ignore the pooling for now, we   ll
   explain that later):
   [24]illustration of a convolutional neural network (id98) architecture
   for sentence classification. here we depict three filter region sizes:
   2, 3 and 4, each of which has 2 filters. every filter performs
   convolution on the sentence matrix and generates (variable-length)
   feature maps. then 1-max pooling is performed over each map, i.e., the
   largest number from each feature map is recorded. thus a univariate
   feature vector is generated from all six maps, and these 6 features are
   concatenated to form a feature vector for the penultimate layer. the
   final softmax layer then receives this feature vector as input and uses
   it to classify the sentence; here we assume binary classification and
   hence depict two possible output states. source: hang, y., & wallace,
   b. (2015). a sensitivity analysis of (and practitioners    guide to)
   convolutional neural networks for sentence classification illustration
   of a convolutional neural network (id98) architecture for sentence
   classification. here we depict three filter region sizes: 2, 3 and 4,
   each of which has 2 filters. every filter performs convolution on the
   sentence matrix and generates (variable-length) feature maps. then
   1-max pooling is performed over each map, i.e., the largest number from
   each feature map is recorded. thus a univariate feature vector is
   generated from all six maps, and these 6 features are concatenated to
   form a feature vector for the penultimate layer. the final softmax
   layer then receives this feature vector as input and uses it to
   classify the sentence; here we assume binary classification and hence
   depict two possible output states. source: zhang, y., & wallace, b.
   (2015). a sensitivity analysis of (and practitioners    guide to)
   convolutional neural networks for sentence classification.

   what about the nice intuitions we had for id161? location
   invariance and local compositionality made intuitive sense for images,
   but not so much for nlp. you probably do care a lot where in the
   sentence a word appears. pixels close to each other are likely to be
   semantically related (part of the same object), but the same
   isn   t always true for words. in many languages, parts of
   phrases could be separated by several other words. the compositional
   aspect isn   t obvious either. clearly, words compose in some ways, like
   an adjective modifying a noun, but how exactly this works what higher
   level representations actually    mean    isn   t as obvious as in the
   id161 case.

   given all this, it seems like id98s wouldn   t be a good fit for nlp
   tasks. [25]recurrent neural networks make more intuitive sense. they
   resemble how we process language (or at least how we think we process
   language): reading sequentially from left to right. fortunately,
   this doesn   t mean that id98s don   t work.  [26]all models are wrong, but
   some are useful. it turns out that id98s applied to nlp problems perform
   quite well. the simple [27]id159 is an obvious
   oversimplification with incorrect assumptions, but has nonetheless been
   the standard approach for years and lead to pretty good results.

   a big argument for id98s is that they are fast. very fast. convolutions
   are a central part of computer graphics and implemented on a hardware
   level on gpus. compared to something like [28]id165s, id98s are also
   efficient in terms of representation. with a large vocabulary,
   computing anything more than 3-grams can quickly become expensive. even
   google doesn   t provide anything beyond 5-grams. convolutional filters
   learn good representations automatically, without needing to represent
   the whole vocabulary. it   s completely reasonable to have filters of
   size larger than 5. i like to think that many of the learned filters in
   the first layer are capturing features quite similar (but not limited)
   to id165s, but represent them in a more compact way.

id98 hyperparameters

   before explaining at how id98s are applied to nlp tasks, let   s look at
   some of the choices you need to make when building a id98. hopefully
   this will help you better understand the literature in the field.

narrow vs. wide convolution

   when i explained convolutions above i neglected a little detail of how
   we apply the filter. applying a 3  3 filter at the center of the matrix
   works fine, but what about the edges? how would you apply the filter to
   the first element of a matrix that doesn   t have any neighboring
   elements to the top and left? you can use zero-padding. all elements
   that would fall outside of the matrix are taken to be zero. by doing
   this you can apply the filter to every element of your input matrix,
   and get a larger or equally sized output. adding zero-padding is also
   called wide convolution, and not using zero-padding would be a narrow
   convolution. an example in 1d looks like this:
   [29]narrow vs. wide convolution. source: a convolutional neural network
   for modelling sentences (2014) narrow vs. wide convolution. filter size
   5, input size 7. source: a convolutional neural network for modelling
   sentences (2014)

   you can see how wide convolution is useful, or even necessary, when you
   have a large filter relative to the input size. in the above, the
   narrow convolution yields  an output of size (7-5) + 1=3 , and a wide
   convolution an output of size (7+2*4 - 5) + 1 =11 . more generally, the
   formula for the output size is n_{out}=(n_{in} + 2*n_{padding} -
   n_{filter}) + 1 .

stride size

   another hyperparameter for your convolutions is the stride size,
   defining by how much you want to shift your filter at each step.  in
   all the examples above the stride size was 1, and consecutive
   applications of the filter overlapped. a larger stride size leads to
   fewer applications of the filter and a smaller output size. the
   following from the [30]stanford cs231 website shows stride sizes of 1
   and 2 applied to a one-dimensional input:
   [31]convolution stride size. left: stride size 1. right: stride size 2.
   source: http://cs231n.github.io/convolutional-networks/ convolution
   stride size. left: stride size 1. right: stride size 2. source:
   http://cs231n.github.io/convolutional-networks/

   in the literature we typically see stride sizes of 1, but a larger
   stride size may allow you to build a model that behaves somewhat
   similarly to a [32]id56, i.e. looks like a tree.

pooling layers

   a key aspect of convolutional neural networks are pooling
   layers, typically applied after the convolutional layers. pooling
   layers subsample their input. the most common way to do pooling it to
   apply a max operation to the result of each filter. you don   t
   necessarily need to pool over the complete matrix, you could also pool
   over a window. for example, the following shows max pooling for a 2  2
   window (in nlp we typically are apply pooling over the complete output,
   yielding just a single number for each filter):
   [33]max pooling in id98. source:
   http://cs231n.github.io/convolutional-networks/#pool max pooling in
   id98. source: http://cs231n.github.io/convolutional-networks/#pool

   why pooling? there are a couple of reasons. one property of pooling is
   that it provides a fixed size output matrix, which typically is
   required for classification. for example, if you have 1,000 filters and
   you apply max pooling to each, you will get a 1000-dimensional output,
   regardless of the size of your filters, or the size of your input. this
   allows you to use variable size sentences, and variable size filters,
   but always get the same output dimensions to feed into a classifier.

   pooling also reduces the output dimensionality but (hopefully) keeps
   the most salient information. you can think of each filter as detecting
   a specific feature, such as detecting if the sentence contains a
   negation like    not amazing    for example. if this phrase occurs
   somewhere in the sentence, the result of applying the filter to that
   region will yield a large value, but a small value in other regions. by
   performing the max operation you  are keeping information about whether
   or not the feature appeared in the sentence, but you are losing
   information about where exactly it appeared. but isn   t this information
   about locality really useful? yes, it  is and it   s a bit similar to
   what a bag of id165s model is doing. you are losing global information
   about locality (where in a sentence something happens), but you are
   keeping local information captured by your filters, like    not amazing   
   being very different from    amazing not   .

   in imagine recognition, pooling also provides basic invariance to
   translating (shifting) and rotation. when you are pooling over a
   region, the output will stay approximately the same even if you
   shift/rotate the image by a few pixels, because the max operations will
   pick out the same value regardless.

channels

   the last concept we need to understand are channels. channels are
   different    views    of your input data. for example, in image recognition
   you typically have rgb (red, green, blue) channels. you can apply
   convolutions across channels, either with different or equal weights.
   in nlp you could imagine having various channels as well: you could
   have a separate channels for different id27s ([34]id97
   and [35]glove for example), or you could have a channel for the same
   sentence represented in different languages, or phrased in different
   ways.

convolutional neural networks applied to nlp

   let   s now look at some of the applications of id98s to natural language
   processing. i   ll try it summarize some of the research results.
   invariably i   ll miss many interesting applications (do let me know in
   the comments), but i hope to cover at least some of the more popular
   results.

   the most natural fit for id98s seem to be classifications tasks, such as
   id31, spam detection or topic categorization.
   convolutions and pooling operations lose information about the local
   order of words, so that sequence tagging as in id52 or entity
   extraction is a bit harder to fit into a pure id98 architecture (though
   not impossible, you can add positional features to the input).

   [1] evaluates a id98 architecture on various classification datasets,
   mostly comprised of id31 and topic categorization tasks.
   the id98 architecture achieves very good performance across datasets,
   and new state-of-the-art on a few. surprisingly, the network used in
   this paper is quite simple, and that   s what makes it powerful.the input
   layer is a sentence comprised of concatenated [36]id97 word
   embeddings. that   s followed by a convolutional layer with multiple
   filters, then a max-pooling layer, and finally a softmax classifier.
   the paper also experiments with two different channels in the form of
   static and dynamic id27s, where one channel is
   adjusted during training and the other isn   t. a similar, but somewhat
   more complex, architecture was previously proposed in [2]. [6] adds an
   additional layer that performs    semantic id91    to this network
   architecture.


   [37]kim, y. (2014). convolutional neural networks for sentence
   classification kim, y. (2014). convolutional neural networks for
   sentence classification

   [4] trains a id98 from scratch, without the need for for pre-trained
   word vectors like id97 or glove. it applies convolutions directly
   to one-hot vectors. the author also proposes a space-efficient
   bag-of-words-like representation for the input data, reducing the
   number of parameters the network needs to learn. in [5] the author
   extends the model with an additional unsupervised    region embedding   
   that is learned using a id98  predicting the context of text regions.
   the approach in these papers seems to work well for long-form texts
   (like movie reviews), but their performance on short texts (like
   tweets) isn   t clear. intuitively, it makes sense that using pre-trained
   id27s for short texts would yield larger gains than using
   them for long texts.

   building a id98 architecture means that there are many hyperparameters
   to choose from, some of which i presented above: input represenations
   (id97, glove, one-hot), number and sizes of convolution filters,
   pooling strategies (max, average), and id180 (relu,
   tanh). [7] performs an empirical evaluation on the effect of varying
   hyperparameters in id98 architectures, investigating their impact on
   performance and variance over multiple runs. if you are looking to
   implement your own id98 for text classification, using the results of
   this paper as a starting point would be an excellent idea. a few
   results that stand out are that max-pooling always beat average
   pooling, that the ideal filter sizes are important but task-dependent,
   and that id173 doesn   t seem to make a big different in the nlp
   tasks that were considered. a caveat of this research is that all the
   datasets were quite similar in terms of their document length, so the
   same guidelines may not apply to data that looks considerably
   different.

   [8] explores id98s for  id36 and relation classification
   tasks. in addition to the word vectors, the authors use the relative
   positions of words to the entities of interest as an input to the
   convolutional layer. this models assumes that the positions of the
   entities are given, and that each example input contains one relation.
   [9] and [10] have explored similar models.

   another interesting use case of id98s in nlp can be found in [11] and
   [12], coming out of microsoft research. these papers describe how to
   learn semantically meaningful representations of sentences that can be
   used for information retrieval. the example given in the papers
   includes recommending potentially interesting documents to users based
   on what they are currently reading. the sentence representations are
   trained based on search engine log data.

   most id98 architectures learn embeddings (low-dimensional
   representations) for words and sentences in one way or another as part
   of their training procedure. not all papers though focus on this aspect
   of training or investigate how meaningful the learned embeddings are.
   [13] presents a id98 architecture to predict hashtags for facebook
   posts, while at the same time generating meaningful embeddings for
   words and sentences. these learned embeddings are then successfully
   applied to another task     recommending potentially interesting
   documents to users, trained based on clickstream data.

character-level id98s

   so far, all of the models presented were based on words. but there has
   also been research in applying id98s directly to characters. [14] learns
   character-level embeddings, joins them with pre-trained word
   embeddings, and uses a id98 for id52. [15][16]
   explores the use of id98s to learn directly from characters, without the
   need for any pre-trained embeddings. notably, the authors use a
   relatively deep network with a total of 9 layers, and apply it to
   id31 and text categorization tasks. results show that
   learning directly from character-level input works very well on large
   datasets (millions of examples), but underperforms simpler models on
   smaller datasets (hundreds of thousands of examples). [17] explores to
   application of character-level convolutions to id38, using
   the output of the character-level id98 as the input to an lstm at each
   time step. the same model is applied to various languages.

   what   s amazing is that essentially all of the papers above were
   published  in the past 1-2 years. obviously there has been excellent
   work with id98s on nlp before, as in[38] natural language processing
   (almost) from scratch, but the pace of new results and state of the art
   systems being published is clearly accelerating.

questions or feedback? let me know in the comments. thanks for reading!


paper references


     * [1] [39]kim, y. (2014). convolutional neural networks for sentence
       classification. proceedings of the 2014 conference on empirical
       methods in natural language processing (emnlp 2014), 1746   1751.
     * [2] [40]kalchbrenner, n., grefenstette, e., & blunsom, p. (2014). a
       convolutional neural network for modelling sentences. acl, 655   665.
     * [3] [41]santos, c. n. dos, & gatti, m. (2014). deep convolutional
       neural networks for id31 of short texts. in
       coling-2014 (pp. 69   78).
     * [4] [42]johnson, r., & zhang, t. (2015). effective use of word
       order for text categorization with convolutional neural networks.
       to appear: naacl-2015, (2011).
     * [5] [43]johnson, r., & zhang, t. (2015). semi-supervised
       convolutional neural networks for text categorization via region
       embedding.
     * [6] [44]wang, p., xu, j., xu, b., liu, c., zhang, h., wang, f., &
       hao, h. (2015). semantic id91 and convolutional neural
       network for short text categorization. proceedings acl 2015,
       352   357.
     * [7] [45]zhang, y., & wallace, b. (2015). a sensitivity analysis of
       (and practitioners    guide to) convolutional neural networks for
       sentence classification,
     * [8] [46]nguyen, t. h., & grishman, r. (2015). id36:
       perspective from convolutional neural networks. workshop on vector
       modeling for nlp, 39   48.
     * [9] [47]sun, y., lin, l., tang, d., yang, n., ji, z., & wang, x.
       (2015). modeling mention , context and entity with neural networks
       for entity disambiguation, (ijcai), 1333   1339.
     * [10] [48]zeng, d., liu, k., lai, s., zhou, g., & zhao, j. (2014).
       relation classification via convolutional deep neural network.
       coling, (2011), 2335   2344.
     * [11] [49]gao, j., pantel, p., gamon, m., he, x., & deng, l. (2014).
       modeling interestingness with deep neural networks.
     * [12] [50]shen, y., he, x., gao, j., deng, l., & mesnil, g. (2014).
       a latent semantic model with convolutional-pooling structure for
       information retrieval. proceedings of the 23rd acm international
       conference on conference on information and knowledge management    
       cikm    14, 101   110.
     * [13] [51]weston, j., & adams, k. (2014). # t ag s pace : semantic
       embeddings from hashtags, 1822   1827.
     * [14] [52]santos, c., & zadrozny, b. (2014). learning
       character-level representations for part-of-speech tagging.
       proceedings of the 31st international conference on machine
       learning, icml-14(2011), 1818   1826.
     * [15] [53]zhang, x., zhao, j., & lecun, y. (2015). character-level
       convolutional networks for text classification, 1   9.
     * [16] [54]zhang, x., & lecun, y. (2015). text understanding from
       scratch. arxiv e-prints, 3, 011102.
     * [17] [55]kim, y., jernite, y., sontag, d., & rush, a. m. (2015).
       character-aware neural language models.


   categories[56]convolutional neural networks, [57]deep learning,
   [58]neural networks

post navigation

   [59]previous postprevious recurrent neural network tutorial, part 4    
   implementing a gru/lstm id56 with python and theano
   [60]next postnext implementing a id98 for text classification in
   tensorflow

subscribe to blog via email

   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

recent posts

     * [61]introduction to learning to trade with id23
     * [62]ai and deep learning in 2017     a year in review
     * [63]hype or not? some perspective on openai   s dota 2 bot
     * [64]learning id23 (with code, exercises and
       solutions)
     * [65]id56s in tensorflow, a practical guide and undocumented features
     * [66]deep learning for chatbots, part 2     implementing a
       retrieval-based model in tensorflow
     * [67]deep learning for chatbots, part 1     introduction
     * [68]attention and memory in deep learning and nlp

archives

     * [69]february 2018
     * [70]december 2017
     * [71]august 2017
     * [72]october 2016
     * [73]august 2016
     * [74]july 2016
     * [75]april 2016
     * [76]january 2016
     * [77]december 2015
     * [78]november 2015
     * [79]october 2015
     * [80]september 2015

categories

     * [81]conversational agents
     * [82]convolutional neural networks
     * [83]deep learning
     * [84]gpu
     * [85]id38
     * [86]memory
     * [87]neural networks
     * [88]news
     * [89]nlp
     * [90]recurrent neural networks
     * [91]id23
     * [92]id56s
     * [93]tensorflow
     * [94]trading
     * [95]uncategorized

meta

     * [96]log in
     * [97]entries rss
     * [98]comments rss
     * [99]wordpress.org

   [100]proudly powered by wordpress

references

   1. http://www.wildml.com/feed/
   2. http://www.wildml.com/comments/feed/
   3. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/feed/
   4. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/
   5. http://www.wildml.com/2015/12/implementing-a-id98-for-text-classification-in-tensorflow/
   6. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
   7. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/&format=xml
   8. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/#content
   9. http://www.wildml.com/
  10. http://www.wildml.com/
  11. https://www.getrevue.co/profile/wildml
  12. http://www.wildml.com/deep-learning-glossary/
  13. mailto:dennybritz@gmail.com
  14. http://www.wildml.com/about/
  15. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
  16. http://www.wildml.com/author/dennybritz/
  17. http://docs.gimp.org/en/plug-in-convmatrix.html
  18. http://colah.github.io/posts/2014-07-understanding-convolutions/
  19. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  20. https://reference.wolfram.com/language/ref/tanh.html
  21. http://www.wildml.com/wp-content/uploads/2015/11/screen-shot-2015-11-07-at-7.26.20-am.png
  22. https://code.google.com/p/id97/
  23. http://nlp.stanford.edu/projects/glove/
  24. http://www.wildml.com/wp-content/uploads/2015/11/screen-shot-2015-11-06-at-12.05.40-pm.png
  25. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  26. https://en.wikipedia.org/wiki/all_models_are_wrong
  27. https://en.wikipedia.org/wiki/bag-of-words_model
  28. https://en.wikipedia.org/wiki/id165
  29. http://www.wildml.com/wp-content/uploads/2015/11/screen-shot-2015-11-05-at-9.47.41-am.png
  30. http://cs231n.github.io/convolutional-networks/
  31. http://www.wildml.com/wp-content/uploads/2015/11/screen-shot-2015-11-05-at-10.18.08-am.png
  32. https://en.wikipedia.org/wiki/recursive_neural_network
  33. http://www.wildml.com/wp-content/uploads/2015/11/screen-shot-2015-11-05-at-2.18.38-pm.png
  34. https://code.google.com/p/id97/
  35. http://nlp.stanford.edu/projects/glove/
  36. https://code.google.com/p/id97/
  37. http://www.wildml.com/wp-content/uploads/2015/11/screen-shot-2015-11-06-at-8.03.47-am.png
  38. http://arxiv.org/abs/1103.0398
  39. http://arxiv.org/abs/1408.5882
  40. http://arxiv.org/abs/1404.2188
  41. http://www.aclweb.org/anthology/c14-1008
  42. http://arxiv.org/abs/1412.1058v1
  43. http://arxiv.org/abs/1504.01255
  44. http://www.aclweb.org/anthology/p15-2058
  45. http://arxiv.org/abs/1510.03820
  46. http://www.cs.nyu.edu/~thien/pubs/vector15.pdf
  47. http://ijcai.org/papers15/papers/ijcai15-192.pdf
  48. http://www.aclweb.org/anthology/c14-1220
  49. http://research.microsoft.com/pubs/226584/604_paper.pdf
  50. http://research.microsoft.com/pubs/226585/cikm2014_cdssm_final.pdf
  51. http://emnlp2014.org/papers/pdf/emnlp2014194.pdf
  52. http://jmlr.org/proceedings/papers/v32/santos14.pdf
  53. http://arxiv.org/abs/1509.01626
  54. http://arxiv.org/abs/1502.01710
  55. http://arxiv.org/abs/1508.06615
  56. http://www.wildml.com/category/neural-networks/convolutional-neural-networks/
  57. http://www.wildml.com/category/deep-learning/
  58. http://www.wildml.com/category/neural-networks/
  59. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/
  60. http://www.wildml.com/2015/12/implementing-a-id98-for-text-classification-in-tensorflow/
  61. http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/
  62. http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/
  63. http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/
  64. http://www.wildml.com/2016/10/learning-reinforcement-learning/
  65. http://www.wildml.com/2016/08/id56s-in-tensorflow-a-practical-guide-and-undocumented-features/
  66. http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/
  67. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  68. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  69. http://www.wildml.com/2018/02/
  70. http://www.wildml.com/2017/12/
  71. http://www.wildml.com/2017/08/
  72. http://www.wildml.com/2016/10/
  73. http://www.wildml.com/2016/08/
  74. http://www.wildml.com/2016/07/
  75. http://www.wildml.com/2016/04/
  76. http://www.wildml.com/2016/01/
  77. http://www.wildml.com/2015/12/
  78. http://www.wildml.com/2015/11/
  79. http://www.wildml.com/2015/10/
  80. http://www.wildml.com/2015/09/
  81. http://www.wildml.com/category/conversational-agents/
  82. http://www.wildml.com/category/neural-networks/convolutional-neural-networks/
  83. http://www.wildml.com/category/deep-learning/
  84. http://www.wildml.com/category/gpu/
  85. http://www.wildml.com/category/language-modeling/
  86. http://www.wildml.com/category/memory/
  87. http://www.wildml.com/category/neural-networks/
  88. http://www.wildml.com/category/news/
  89. http://www.wildml.com/category/nlp/
  90. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  91. http://www.wildml.com/category/reinforcement-learning/
  92. http://www.wildml.com/category/id56s/
  93. http://www.wildml.com/category/tensorflow/
  94. http://www.wildml.com/category/trading/
  95. http://www.wildml.com/category/uncategorized/
  96. http://www.wildml.com/wp-login.php
  97. http://www.wildml.com/feed/
  98. http://www.wildml.com/comments/feed/
  99. https://wordpress.org/
 100. https://wordpress.org/
