   #[1]cross validated [2]feed for question 'what is the difference
   between convolutional neural networks, restricted id82s,
   and auto-encoders?'

stack exchange network

   stack exchange network consists of 175 q&a communities including
   [3]stack overflow, the largest, most trusted online community for
   developers to learn, share their knowledge, and build their careers.
   [4]visit stack exchange
   ____________________ (button)
    1.
    2.
    3.
    4.
    5.
    6. [5]log in [6]sign up
    7.

[7]current community
          + cross validated
            [8]help [9]chat
          + cross validated meta

your communities
       [10]sign up or [11]log in to customize your list.

[12]more stack exchange communities
       [13]company blog
          + [14]tour start here for a quick overview of the site
          + [15]help center detailed answers to any questions you might
            have
          + [16]meta discuss the workings and policies of this site
          + [17]about us learn more about stack overflow the company
          + [18]business learn more about hiring developers or posting ads
            with us

   by using our site, you acknowledge that you have read and understand
   our [19]cookie policy, [20]privacy policy, and our [21]terms of
   service.

   cross validated is a question and answer site for people interested in
   statistics, machine learning, data analysis, data mining, and data
   visualization. join them; it only takes a minute:
   [22]sign up
   here's how it works:
   anybody can ask a question
   anybody can answer
   the best answers are voted up and rise to the top

   [23]cross validated

    1.
    2. [24]home
    3.
         1. [25]questions
         2. [26]tags
         3. [27]users
         4. [28]unanswered

[29]what is the difference between convolutional neural networks, restricted
id82s, and auto-encoders?

   [30]ask question
   (button)
   122
   (button) (button)
   163
   $\begingroup$

   recently i have been reading about deep learning and i am confused
   about the terms (or say technologies). what is the difference between
     * convolutional neural networks (id98),
     * restricted id82s (rbm) and
     * auto-encoders?

   [31]neural-networks [32]deep-learning [33]conv-neural-network
   [34]autoencoders [35]rbm
   [36]share|cite|[37]improve this question
   [38]edited sep 12 '17 at 11:15
   [39]ferdi
   3,86742355
   asked sep 4 '14 at 20:52
   [40]rockthestarrockthestar
   4,128195175
   $\endgroup$

   add a comment |

4 answers 4

   [41]active [42]oldest [43]votes
   (button)
   235
   (button)
   +50
   $\begingroup$

   autoencoder is a simple 3-layer neural network where output units are
   directly connected back to input units. e.g. in a network like this:

   enter image description here

   output[i] has edge back to input[i] for every i. typically, number of
   hidden units is much less then number of visible (input/output) ones.
   as a result, when you pass data through such a network, it first
   compresses (encodes) input vector to "fit" in a smaller representation,
   and then tries to reconstruct (decode) it back. the task of training is
   to minimize an error or reconstruction, i.e. find the most efficient
   compact representation (encoding) for input data.

   rbm shares similar idea, but uses stochastic approach. instead of
   deterministic (e.g. logistic or relu) it uses stochastic units with
   particular (usually binary of gaussian) distribution. learning
   procedure consists of several steps of id150 (propagate:
   sample hiddens given visibles; reconstruct: sample visibles given
   hiddens; repeat) and adjusting the weights to minimize reconstruction
   error.

   enter image description here

   intuition behind rbms is that there are some visible random variables
   (e.g. film reviews from different users) and some hidden variables
   (like film genres or other internal features), and the task of training
   is to find out how these two sets of variables are actually connected
   to each other (more on this example may be found [44]here).

   convolutional neural networks are somewhat similar to these two, but
   instead of learning single global weight matrix between two layers,
   they aim to find a set of locally connected neurons. id98s are mostly
   used in image recognition. their name comes from [45]"convolution"
   operator or simply "filter". in short, filters are an easy way to
   perform complex operation by means of simple change of a convolution
   kernel. apply gaussian blur kernel and you'll get it smoothed. apply
   canny kernel and you'll see all edges. apply gabor kernel to get
   gradient features.

   enter image description here

   (image from [46]here)

   the goal of convolutional neural networks is not to use one of
   predefined kernels, but instead to learn data-specific kernels. the
   idea is the same as with autoencoders or rbms - translate many
   low-level features (e.g. user reviews or image pixels) to the
   compressed high-level representation (e.g. film genres or edges) - but
   now weights are learned only from neurons that are spatially close to
   each other.

   enter image description here

   all three models have their use cases, pros and cons, but probably the
   most important properties are:
    1. autoencoders are simplest ones. they are intuitively
       understandable, easy to implement and to reason about (e.g. it's
       much easier to find good meta-parameters for them than for rbms).
    2. rbms are generative. that is, unlike autoencoders that only
       discriminate some data vectors in favour of others, rbms can also
       generate new data with given joined distribution. they are also
       considered more feature-rich and flexible.
    3. id98s are very specific model that is mostly used for very specific
       task (though pretty popular task). most of the top-level algorithms
       in image recognition are somehow based on id98s today, but outside
       that niche they are hardly applicable (e.g. what's the reason to
       use convolution for film review analysis?).

   upd.

   id84

   when we represent some object as a vector of $n$ elements, we say that
   this is a vector in $n$-dimensional space. thus, dimensionality
   reduction refers to a process of refining data in such a way, that each
   data vector $x$ is translated into another vector $x'$ in an
   $m$-dimensional space (vector with $m$ elements), where $m < n$.
   probably the most common way of doing this is [47]pca. roughly
   speaking, pca finds "internal axes" of a dataset (called "components")
   and sorts them by their importance. first $m$ most important components
   are then used as new basis. each of these components may be thought of
   as a high-level feature, describing data vectors better than original
   axes.

   both - autoencoders and rbms - do the same thing. taking a vector in
   $n$-dimensional space they translate it into an $m$-dimensional one,
   trying to keep as much important information as possible and, at the
   same time, remove noise. if training of autoencoder/rbm was successful,
   each element of resulting vector (i.e. each hidden unit) represents
   something important about the object - shape of an eyebrow in an image,
   genre of a film, field of study in scientific article, etc. you take
   lots of noisy data as an input and produce much less data in a much
   more efficient representation.

   deep architectures

   so, if we already had pca, why the hell did we come up with
   autoencoders and rbms? it turns out that pca only allows linear
   transformation of a data vectors. that is, having $m$ principal
   components $c_1..c_m$, you can represent only vectors
   $x=\sum_{i=1}^{m}w_ic_i$. this is pretty good already, but not always
   enough. no matter, how many times you will apply pca to a data -
   relationship will always stay linear.

   autoencoders and rbms, on other hand, are non-linear by the nature, and
   thus, they can learn more complicated relations between visible and
   hidden units. moreover, they can be stacked, which makes them even more
   powerful. e.g. you train rbm with $n$ visible and $m$ hidden units,
   then you put another rbm with $m$ visible and $k$ hidden units on top
   of the first one and train it too, etc. and exactly the same way with
   autoencoders.

   but you don't just add new layers. on each layer you try to learn best
   possible representation for a data from the previous one:

   enter image description here

   on the image above there's an example of such a deep network. we start
   with ordinary pixels, proceed with simple filters, then with face
   elements and finally end up with entire faces! this is the essence of
   deep learning.

   now note, that at this example we worked with image data and
   sequentially took larger and larger areas of spatially close pixels.
   doesn't it sound similar? yes, because it's an example of deep
   convolutional network. be it based on autoencoders or rbms, it uses
   convolution to stress importance of locality. that's why id98s are
   somewhat distinct from autoencoders and rbms.

   classification

   none of models mentioned here work as classification algorithms per se.
   instead, they are used for pretraining - learning transformations from
   low-level and hard-to-consume representation (like pixels) to a
   high-level one. once deep (or maybe not that deep) network is
   pretrained, input vectors are transformed to a better representation
   and resulting vectors are finally passed to real classifier (such as
   id166 or id28). in an image above it means that at the
   very bottom there's one more component that actually does
   classification.
   [48]share|cite|[49]improve this answer
   [50]edited sep 29 '14 at 20:19
   answered sep 29 '14 at 13:45
   [51]ffriendffriend
   5,20541827
   $\endgroup$
     * 2
       $\begingroup$ great explanation, thanks! few questions: for
       autoencoder, it can be more than 3 layers, depending on how far the
       dimension reduction you want to have, right? also, as far as i
       know, autoencoder is for dimension reduction and rbms&id98s are more
       on classification. so what factors determine whether rbms or id98s
       have to be used? $\endgroup$     [52]rockthestar sep 29 '14 at 15:25
     * 1
       $\begingroup$ @rockthestar: see my update. $\endgroup$
           [53]ffriend sep 29 '14 at 19:14
     * $\begingroup$ great explanation. i have much better idea now, thank
       you very much. $\endgroup$     [54]rockthestar sep 30 '14 at 0:16
     * 3
       $\begingroup$ @ffriend very short and clear explanation. there is a
       lot written about these topics but your briefing is fantastic.
       "rockthestar" if you want more detailed explanation you can read
       this 100 pages book of yoshua bengio
       (www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf) $\endgroup$
           [55]javierfdr may 20 '15 at 10:36
     * 2
       $\begingroup$ @skan: take a look at [56]id56s. specifically,
       [57]lstm is quite popular now. you may also want to ask a separate
       question with more details to get a qualified answer, because id56s
       aren't quite my field of expertise. $\endgroup$     [58]ffriend jun
       23 '17 at 12:01

    |  [59]show 1 more comment
   (button)
   15
   (button)
   $\begingroup$

   all of these architectures can be interpreted as a neural network. the
   main difference between autoencoder and convolutional network is the
   level of network hardwiring. convolutional nets are pretty much
   hardwired. convolution operation is pretty much local in image domain,
   meaning much more sparsity in the number of connections in neural
   network view. pooling(subsampling) operation in image domain is also a
   hardwired set of neural connections in neural domain. such topological
   constraints on network structure. given such constraints, training of
   id98 learns best weights for this convolution operation (in practice
   there are multiple filters). id98s are usually used for image and speech
   tasks where convolutional constraints are a good assumption.

   in contrast, autoencoders almost specify nothing about the topology of
   the network. they are much more general. the idea is to find good
   neural transformation to reconstruct the input. they are composed of
   encoder (projects the input to hidden layer) and decoder (reprojects
   hidden layer to output). the hidden layer learns a set of latent
   features or latent factors. linear autoencoders span the same subspace
   with pca. given a dataset, they learn number of basis to explain the
   underlying pattern of the data.

   rbms are also a neural network. but interpretation of the network is
   totally different. rbms interpret the network as not a feedforward, but
   a bipartite graph where the idea is to learn joint id203
   distribution of hidden and input variables. they are viewed as a
   graphical model. remember that both autoencoder and id98 learns a
   deterministic function. rbms, on the other hand, is generative model.
   it can generate samples from learned hidden representations. there are
   different algorithms to train rbms. however, at the end of the day,
   after learning rbms, you can use its network weights to interpret it as
   a feedforward network.
   [60]share|cite|[61]improve this answer
   answered sep 29 '14 at 15:04
   [62]cylonmathcylonmath
   25113
   $\endgroup$

   add a comment |
   (button)
   5
   (button)
   $\begingroup$

   rbms can be seen as some kind of probabilistic auto encoder. actually,
   it has been shown that under certain conditions they become equivalent.

   nevertheless, it is much harder to show this equivalency than to just
   believe they are different beasts. indeed, i find it hard to find a lot
   of similarities among the three, as soon as i start to look closely.

   e.g. if you write down the functions implemented by an auto encoder, an
   rbm and a id98, you get three completely different mathematical
   expressions.
   [63]share|cite|[64]improve this answer
   [65]edited aug 6 '16 at 0:58
   [66]kenorb
   3741618
   answered sep 29 '14 at 13:11
   [67]bayerjbayerj
   9,36512549
   $\endgroup$

   add a comment |
   (button)
   1
   (button)
   $\begingroup$

   i can't tell you much about rbms, but autoencoders and id98s are two
   different kinds of things. an autoencoder is a neural network that is
   trained in an unsupervised fashion. the goal of an autoencoder is to
   find a more compact representation of the data by learning an encoder,
   which transforms the data to their corresponding compact
   representation, and a decoder, which reconstructs the original data.
   the encoder part of autoencoders (and originally rbms) have been used
   to learn good initial weights of a deeper architecture, but there are
   other applications. essentially, an autoencoder learns a id91 of
   the data. in contrast, the term id98 refers to a type of neural network
   which uses the convolution operator (often the 2d convolution when it
   is used for image processing tasks) to extract features from the data.
   in image processing, filters, that are convoluted with images, are
   learned automatically to solve the task at hand, e.g. a classification
   task. whether the training criterion is a regression/classification
   (supervised) or a reconstruction (unsupervised) is unrelated to idea of
   convolutions as an alternative to affine transformations. you can also
   have a id98-autoencoder.
   [68]share|cite|[69]improve this answer
   answered sep 29 '14 at 10:09
   [70]sigurdsigurd
   111
   $\endgroup$
     * $\begingroup$ autoencoder is supervised training, as you train to
       generate the same information as output as the given input, only
       with a smaller hiddenlayer, so that the network has to find an
       appropriate encoding, that transforms the input to the size of the
       hidden layer and back. in unsuperwised training, as used in
       statistical networks, like som or neural gas, you don't have a
       target information that is presented to the network as output.
       $\endgroup$     [71]sci aug 16 '16 at 7:14

   add a comment |

your answer


   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________
   _______________________________________________________________________

   thanks for contributing an answer to cross validated!
     * please be sure to answer the question. provide details and share
       your research!

   but avoid    
     * asking for help, clarification, or responding to other answers.
     * making statements based on opinion; back them up with references or
       personal experience.

   use mathjax to format equations. [72]mathjax reference.

   to learn more, see our [73]tips on writing great answers.
   (button)
   draft saved
   draft discarded
   ____________________

sign up or [74]log in

   sign up using google
   sign up using facebook
   sign up using email and password
   [button input] (not implemented)______

post as a guest

   name
   ____________________
   email

   required, but never shown
   ________________________________________

post as a guest

   name
   ____________________
   email

   required, but never shown
   ________________________________________
   (button) post your answer (button) discard

   by clicking "post your answer", you agree to our [75]terms of service,
   [76]privacy policy and [77]cookie policy

not the answer you're looking for? browse other questions tagged
[78]neural-networks [79]deep-learning [80]conv-neural-network
[81]autoencoders [82]rbm or [83]ask your own question.

   asked

   4 years, 7 months ago

   viewed

   97,838 times

   active

   [84]1 year, 6 months ago

linked

   8
   [85]receptive field of neurons in lenet
   3
   [86]variational auto-encoders vs restricted id82s
   0
   [87]computation differences of rbms and id98s

related

   10
   [88]good tutorial for restricted id82s (rbm)
   7
   [89]are restricted id82s better than stacked auto encoders
   and why?
   28
   [90]id50 or deep id82s?
   1
   [91]what is a convolutional neural network
   3
   [92]what's the difference between convolutional neural network and
   convolutional auto encoders?
   5
   [93]difference between neural network architectures
   4
   [94]advantages of convolutional neural networks over    simple   
   feed-forward networks?
   16
   [95]modern use cases of restricted id82s (rbm's)?
   0
   [96]how do auto-encoders or restricted id82s find high
   variance components for non-linear pca?
   3
   [97]variational auto-encoders vs restricted id82s

[98]hot network questions

     * [99]how does one intimidate enemies without having the capacity for
       violence?
     * [100]how bulky would the original autograph of the torah been?
     * [101]horror movie about the virus at the prom; beginning and end
       are stylized as a cartoon
     * [102]meaning of     in          ?
     * [103]is it inappropriate for a student to attend their mentor's
       dissertation defense?
     * [104]why am i being followed by a political opponent in twitter?
     * [105]important resources for dark age civilizations?
     * [106]what does the "remote control" for a qf-4 look like?
     * [107]how to source a part of a file
     * [108]dc-dc converter from low voltage at high current, to high
       voltage at low current
     * [109]can a cauchy sequence converge for one metric while not
       converging for another?
     * [110]how much ram could one put in a typical 80386 setup?
     * [111]java casting: java 11 throws lambdaconversionexception while
       1.8 does not
     * [112]linear path optimization with two dependent variables
     * [113]does otiluke's resilient sphere beat magic circle?
     * [114]i accidentally deleted a stock terminal theme
     * [115]definition of observer and time measured by different
       observers in general relativity
     * [116]do i have a twin with permutated remainders?
     * [117]were any external disk drives stacked vertically?
     * [118]is it legal for company to use my work email to pretend i
       still work there?
     * [119]how is it possible to have an ability score that is less than
       3?
     * [120]question on branch cuts and branch points
     * [121]how can i prevent hyper evolved versions of regular creatures
       from wiping out their cousins?
     * [122]how old can references or sources in a thesis be?

   [123]more hot questions
   [124]question feed

[125]cross validated

     * [126]tour
     * [127]help
     * [128]chat
     * [129]contact
     * [130]feedback
     * mobile

[131]company

     * [132]stack overflow
     * [133]stack overflow business
     * [134]developer jobs
     * [135]about
     * [136]press
     * [137]legal
     * [138]privacy policy

[139]stack exchange
network

     * [140]technology
     * [141]life / arts
     * [142]culture / recreation
     * [143]science
     * [144]other

     * [145]stack overflow
     * [146]server fault
     * [147]super user
     * [148]web applications
     * [149]ask ubuntu
     * [150]webmasters
     * [151]game development

     * [152]tex - latex
     * [153]software engineering
     * [154]unix & linux
     * [155]ask different (apple)
     * [156]wordpress development
     * [157]geographic information systems
     * [158]electrical engineering

     * [159]android enthusiasts
     * [160]information security
     * [161]database administrators
     * [162]drupal answers
     * [163]sharepoint
     * [164]user experience
     * [165]mathematica

     * [166]salesforce
     * [167]expressionengine   answers
     * [168]stack overflow em portugu  s
     * [169]blender
     * [170]network engineering
     * [171]cryptography
     * [172]code review

     * [173]magento
     * [174]software recommendations
     * [175]signal processing
     * [176]emacs
     * [177]raspberry pi
     * [178]stack overflow                    
     * [179]programming puzzles & code golf

     * [180]stack overflow en espa  ol
     * [181]ethereum
     * [182]data science
     * [183]arduino
     * [184]bitcoin
     * [185]more (31)

     * [186]photography
     * [187]science fiction & fantasy
     * [188]graphic design
     * [189]movies & tv
     * [190]music: practice & theory
     * [191]worldbuilding
     * [192]seasoned advice (cooking)

     * [193]home improvement
     * [194]personal finance & money
     * [195]academia
     * [196]law
     * [197]more (15)

     * [198]english language & usage
     * [199]skeptics
     * [200]mi yodeya (judaism)
     * [201]travel
     * [202]christianity
     * [203]english language learners
     * [204]japanese language

     * [205]arqade (gaming)
     * [206]bicycles
     * [207]role-playing games
     * [208]anime & manga
     * [209]puzzling
     * [210]motor vehicle maintenance & repair
     * [211]more (33)

     * [212]mathoverflow
     * [213]mathematics
     * [214]cross validated (stats)
     * [215]theoretical computer science
     * [216]physics
     * [217]chemistry
     * [218]biology

     * [219]computer science
     * [220]philosophy
     * [221]more (10)

     * [222]meta stack exchange
     * [223]stack apps
     * [224]api
     * [225]data

     * [226]blog
     * [227]facebook
     * [228]twitter
     * [229]linkedin

   site design / logo    2019 stack exchange inc; user contributions
   licensed under [230]cc by-sa 3.0 with [231]attribution required.
   rev 2019.4.5.33234

   cross validated works best with javascript enabled

references

   visible links
   1. https://stats.stackexchange.com/opensearch.xml
   2. https://stats.stackexchange.com/feeds/question/114385
   3. https://stackoverflow.com/
   4. https://stackexchange.com/
   5. https://stats.stackexchange.com/users/login?ssrc=head&returnurl=https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
   6. https://stats.stackexchange.com/users/signup?ssrc=head&returnurl=https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
   7. https://stats.stackexchange.com/
   8. https://stats.stackexchange.com/help
   9. https://chat.stackexchange.com/?tab=site&host=stats.stackexchange.com
  10. https://stats.stackexchange.com/users/signup?ssrc=site_switcher&returnurl=https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
  11. https://stats.stackexchange.com/users/login?ssrc=site_switcher&returnurl=https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
  12. https://stackexchange.com/sites
  13. https://stackoverflow.blog/
  14. https://stats.stackexchange.com/tour
  15. https://stats.stackexchange.com/help
  16. https://stats.meta.stackexchange.com/
  17. https://stackoverflow.com/company/about
  18. https://www.stackoverflowbusiness.com/?ref=topbar_help
  19. https://stackoverflow.com/legal/cookie-policy
  20. https://stackoverflow.com/legal/privacy-policy
  21. https://stackoverflow.com/legal/terms-of-service/public
  22. https://stats.stackexchange.com/users/signup?ssrc=hero&returnurl=https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
  23. https://stats.stackexchange.com/
  24. https://stats.stackexchange.com/
  25. https://stats.stackexchange.com/questions
  26. https://stats.stackexchange.com/tags
  27. https://stats.stackexchange.com/users
  28. https://stats.stackexchange.com/unanswered
  29. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
  30. https://stats.stackexchange.com/questions/ask
  31. https://stats.stackexchange.com/questions/tagged/neural-networks
  32. https://stats.stackexchange.com/questions/tagged/deep-learning
  33. https://stats.stackexchange.com/questions/tagged/conv-neural-network
  34. https://stats.stackexchange.com/questions/tagged/autoencoders
  35. https://stats.stackexchange.com/questions/tagged/rbm
  36. https://stats.stackexchange.com/q/114385
  37. https://stats.stackexchange.com/posts/114385/edit
  38. https://stats.stackexchange.com/posts/114385/revisions
  39. https://stats.stackexchange.com/users/128677/ferdi
  40. https://stats.stackexchange.com/users/41749/rockthestar
  41. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma?answertab=active#tab-top
  42. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma?answertab=oldest#tab-top
  43. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma?answertab=votes#tab-top
  44. http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/
  45. http://en.wikipedia.org/wiki/convolution
  46. https://developer.apple.com/library/ios/documentation/performance/conceptual/vimage/convolutionoperations/convolutionoperations.html
  47. http://en.wikipedia.org/wiki/principal_component_analysis
  48. https://stats.stackexchange.com/a/117188
  49. https://stats.stackexchange.com/posts/117188/edit
  50. https://stats.stackexchange.com/posts/117188/revisions
  51. https://stats.stackexchange.com/users/3305/ffriend
  52. https://stats.stackexchange.com/users/41749/rockthestar
  53. https://stats.stackexchange.com/users/3305/ffriend
  54. https://stats.stackexchange.com/users/41749/rockthestar
  55. https://stats.stackexchange.com/users/45573/javierfdr
  56. https://en.wikipedia.org/wiki/recurrent_neural_network
  57. https://en.wikipedia.org/wiki/long_short-term_memory
  58. https://stats.stackexchange.com/users/3305/ffriend
  59. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
  60. https://stats.stackexchange.com/a/117199
  61. https://stats.stackexchange.com/posts/117199/edit
  62. https://stats.stackexchange.com/users/56624/cylonmath
  63. https://stats.stackexchange.com/a/117184
  64. https://stats.stackexchange.com/posts/117184/edit
  65. https://stats.stackexchange.com/posts/117184/revisions
  66. https://stats.stackexchange.com/users/12989/kenorb
  67. https://stats.stackexchange.com/users/2860/bayerj
  68. https://stats.stackexchange.com/a/117157
  69. https://stats.stackexchange.com/posts/117157/edit
  70. https://stats.stackexchange.com/users/56629/sigurd
  71. https://stats.stackexchange.com/users/127819/sci
  72. http://www.math.harvard.edu/texman/
  73. https://stats.stackexchange.com/help/how-to-answer
  74. https://stats.stackexchange.com/users/login?ssrc=question_page&returnurl=https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma#new-answer
  75. https://stackoverflow.com/legal/terms-of-service/public
  76. https://stackoverflow.com/legal/privacy-policy
  77. https://stackoverflow.com/legal/cookie-policy
  78. https://stats.stackexchange.com/questions/tagged/neural-networks
  79. https://stats.stackexchange.com/questions/tagged/deep-learning
  80. https://stats.stackexchange.com/questions/tagged/conv-neural-network
  81. https://stats.stackexchange.com/questions/tagged/autoencoders
  82. https://stats.stackexchange.com/questions/tagged/rbm
  83. https://stats.stackexchange.com/questions/ask
  84. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma?lastactivity
  85. https://stats.stackexchange.com/questions/142606/receptive-field-of-neurons-in-lenet?noredirect=1
  86. https://stats.stackexchange.com/questions/254035/variational-auto-encoders-vs-restricted-boltzmann-machines?noredirect=1
  87. https://stats.stackexchange.com/questions/250393/computation-differences-of-rbms-and-id98s?noredirect=1
  88. https://stats.stackexchange.com/questions/48162/good-tutorial-for-restricted-boltzmann-machines-rbm
  89. https://stats.stackexchange.com/questions/94483/are-restricted-boltzmann-machines-better-than-stacked-auto-encoders-and-why
  90. https://stats.stackexchange.com/questions/95428/deep-belief-networks-or-deep-boltzmann-machines
  91. https://stats.stackexchange.com/questions/116728/what-is-a-convolutional-neural-network
  92. https://stats.stackexchange.com/questions/189325/whats-the-difference-between-convolutional-neural-network-and-convolutional-aut
  93. https://stats.stackexchange.com/questions/195494/difference-between-neural-network-architectures
  94. https://stats.stackexchange.com/questions/215681/advantages-of-convolutional-neural-networks-over-simple-feed-forward-networks
  95. https://stats.stackexchange.com/questions/231988/modern-use-cases-of-restricted-boltzmann-machines-rbms
  96. https://stats.stackexchange.com/questions/251604/how-do-auto-encoders-or-restricted-boltzmann-machines-find-high-variance-compone
  97. https://stats.stackexchange.com/questions/254035/variational-auto-encoders-vs-restricted-boltzmann-machines
  98. https://stackexchange.com/questions?tab=hot
  99. https://worldbuilding.stackexchange.com/questions/143288/how-does-one-intimidate-enemies-without-having-the-capacity-for-violence
 100. https://judaism.stackexchange.com/questions/101613/how-bulky-would-the-original-autograph-of-the-torah-been
 101. https://scifi.stackexchange.com/questions/208568/horror-movie-about-the-virus-at-the-prom-beginning-and-end-are-stylized-as-a-ca
 102. https://japanese.stackexchange.com/questions/66448/meaning-of-   -in-         
 103. https://academia.stackexchange.com/questions/127430/is-it-inappropriate-for-a-student-to-attend-their-mentors-dissertation-defense
 104. https://webapps.stackexchange.com/questions/127191/why-am-i-being-followed-by-a-political-opponent-in-twitter
 105. https://worldbuilding.stackexchange.com/questions/143300/important-resources-for-dark-age-civilizations
 106. https://aviation.stackexchange.com/questions/62047/what-does-the-remote-control-for-a-qf-4-look-like
 107. https://vi.stackexchange.com/questions/19446/how-to-source-a-part-of-a-file
 108. https://electronics.stackexchange.com/questions/430917/dc-dc-converter-from-low-voltage-at-high-current-to-high-voltage-at-low-current
 109. https://math.stackexchange.com/questions/3175604/can-a-cauchy-sequence-converge-for-one-metric-while-not-converging-for-another
 110. https://retrocomputing.stackexchange.com/questions/9579/how-much-ram-could-one-put-in-a-typical-80386-setup
 111. https://stackoverflow.com/questions/55532055/java-casting-java-11-throws-lambdaconversionexception-while-1-8-does-not
 112. https://cs.stackexchange.com/questions/106508/linear-path-optimization-with-two-dependent-variables
 113. https://rpg.stackexchange.com/questions/144612/does-otilukes-resilient-sphere-beat-magic-circle
 114. https://apple.stackexchange.com/questions/356048/i-accidentally-deleted-a-stock-terminal-theme
 115. https://physics.stackexchange.com/questions/470705/definition-of-observer-and-time-measured-by-different-observers-in-general-relat
 116. https://codegolf.stackexchange.com/questions/182669/do-i-have-a-twin-with-permutated-remainders
 117. https://retrocomputing.stackexchange.com/questions/9569/were-any-external-disk-drives-stacked-vertically
 118. https://workplace.stackexchange.com/questions/133270/is-it-legal-for-company-to-use-my-work-email-to-pretend-i-still-work-there
 119. https://rpg.stackexchange.com/questions/144591/how-is-it-possible-to-have-an-ability-score-that-is-less-than-3
 120. https://mathematica.stackexchange.com/questions/194668/question-on-branch-cuts-and-branch-points
 121. https://worldbuilding.stackexchange.com/questions/143180/how-can-i-prevent-hyper-evolved-versions-of-regular-creatures-from-wiping-out-th
 122. https://academia.stackexchange.com/questions/127607/how-old-can-references-or-sources-in-a-thesis-be
 123. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 124. https://stats.stackexchange.com/feeds/question/114385
 125. https://stats.stackexchange.com/
 126. https://stats.stackexchange.com/tour
 127. https://stats.stackexchange.com/help
 128. https://chat.stackexchange.com/?tab=site&host=stats.stackexchange.com
 129. https://stats.stackexchange.com/contact
 130. https://stats.meta.stackexchange.com/
 131. https://stackoverflow.com/company/about
 132. https://stackoverflow.com/
 133. https://www.stackoverflowbusiness.com/
 134. https://stackoverflow.com/jobs
 135. https://stackoverflow.com/company/about
 136. https://stackoverflow.com/company/press
 137. https://stackoverflow.com/legal
 138. https://stackoverflow.com/legal/privacy-policy
 139. https://stackexchange.com/
 140. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 141. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 142. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 143. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 144. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 145. https://stackoverflow.com/
 146. https://serverfault.com/
 147. https://superuser.com/
 148. https://webapps.stackexchange.com/
 149. https://askubuntu.com/
 150. https://webmasters.stackexchange.com/
 151. https://gamedev.stackexchange.com/
 152. https://tex.stackexchange.com/
 153. https://softwareengineering.stackexchange.com/
 154. https://unix.stackexchange.com/
 155. https://apple.stackexchange.com/
 156. https://wordpress.stackexchange.com/
 157. https://gis.stackexchange.com/
 158. https://electronics.stackexchange.com/
 159. https://android.stackexchange.com/
 160. https://security.stackexchange.com/
 161. https://dba.stackexchange.com/
 162. https://drupal.stackexchange.com/
 163. https://sharepoint.stackexchange.com/
 164. https://ux.stackexchange.com/
 165. https://mathematica.stackexchange.com/
 166. https://salesforce.stackexchange.com/
 167. https://expressionengine.stackexchange.com/
 168. https://pt.stackoverflow.com/
 169. https://blender.stackexchange.com/
 170. https://networkengineering.stackexchange.com/
 171. https://crypto.stackexchange.com/
 172. https://codereview.stackexchange.com/
 173. https://magento.stackexchange.com/
 174. https://softwarerecs.stackexchange.com/
 175. https://dsp.stackexchange.com/
 176. https://emacs.stackexchange.com/
 177. https://raspberrypi.stackexchange.com/
 178. https://ru.stackoverflow.com/
 179. https://codegolf.stackexchange.com/
 180. https://es.stackoverflow.com/
 181. https://ethereum.stackexchange.com/
 182. https://datascience.stackexchange.com/
 183. https://arduino.stackexchange.com/
 184. https://bitcoin.stackexchange.com/
 185. https://stackexchange.com/sites#technology
 186. https://photo.stackexchange.com/
 187. https://scifi.stackexchange.com/
 188. https://graphicdesign.stackexchange.com/
 189. https://movies.stackexchange.com/
 190. https://music.stackexchange.com/
 191. https://worldbuilding.stackexchange.com/
 192. https://cooking.stackexchange.com/
 193. https://diy.stackexchange.com/
 194. https://money.stackexchange.com/
 195. https://academia.stackexchange.com/
 196. https://law.stackexchange.com/
 197. https://stackexchange.com/sites#lifearts
 198. https://english.stackexchange.com/
 199. https://skeptics.stackexchange.com/
 200. https://judaism.stackexchange.com/
 201. https://travel.stackexchange.com/
 202. https://christianity.stackexchange.com/
 203. https://ell.stackexchange.com/
 204. https://japanese.stackexchange.com/
 205. https://gaming.stackexchange.com/
 206. https://bicycles.stackexchange.com/
 207. https://rpg.stackexchange.com/
 208. https://anime.stackexchange.com/
 209. https://puzzling.stackexchange.com/
 210. https://mechanics.stackexchange.com/
 211. https://stackexchange.com/sites#culturerecreation
 212. https://mathoverflow.net/
 213. https://math.stackexchange.com/
 214. https://stats.stackexchange.com/
 215. https://cstheory.stackexchange.com/
 216. https://physics.stackexchange.com/
 217. https://chemistry.stackexchange.com/
 218. https://biology.stackexchange.com/
 219. https://cs.stackexchange.com/
 220. https://philosophy.stackexchange.com/
 221. https://stackexchange.com/sites#science
 222. https://meta.stackexchange.com/
 223. https://stackapps.com/
 224. https://api.stackexchange.com/
 225. https://data.stackexchange.com/
 226. https://stackoverflow.blog/?blb=1
 227. https://www.facebook.com/officialstackoverflow/
 228. https://twitter.com/stackoverflow
 229. https://linkedin.com/company/stack-overflow
 230. https://creativecommons.org/licenses/by-sa/3.0/
 231. https://stackoverflow.blog/2009/06/25/attribution-required/

   hidden links:
 233. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 234. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 235. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 236. https://stackexchange.com/users/?tab=inbox
 237. https://stackexchange.com/users/?tab=reputation
 238. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 239. https://stackexchange.com/
 240. https://stats.stackexchange.com/
 241. https://stats.meta.stackexchange.com/
 242. https://stats.stackexchange.com/users/128677/ferdi
 243. https://stats.stackexchange.com/users/41749/rockthestar
 244. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 245. https://stats.stackexchange.com/users/3305/ffriend
 246. https://stats.stackexchange.com/users/56624/cylonmath
 247. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 248. https://stats.stackexchange.com/users/12989/kenorb
 249. https://stats.stackexchange.com/users/2860/bayerj
 250. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 251. https://stats.stackexchange.com/users/56629/sigurd
 252. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
 253. https://stats.stackexchange.com/q/142606
 254. https://stats.stackexchange.com/q/254035
 255. https://stats.stackexchange.com/q/250393
 256. https://stats.stackexchange.com/q/48162
 257. https://stats.stackexchange.com/q/94483
 258. https://stats.stackexchange.com/q/95428
 259. https://stats.stackexchange.com/q/116728
 260. https://stats.stackexchange.com/q/189325
 261. https://stats.stackexchange.com/q/195494
 262. https://stats.stackexchange.com/q/215681
 263. https://stats.stackexchange.com/q/231988
 264. https://stats.stackexchange.com/q/251604
 265. https://stats.stackexchange.com/q/254035
 266. https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma
