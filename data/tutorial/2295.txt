   (button) toggle navigation [1]colah's blog
     * [2]blog
     * [3]about
     * [4]contact

understanding convolutions

   posted on july 13, 2014

   [5]neural networks, [6]convolutional neural networks, [7]convolution,
   [8]math, [9]id203

   in a [10]previous post, we built up an understanding of convolutional
   neural networks, without referring to any significant mathematics. to
   go further, however, we need to understand convolutions.

   if we just wanted to understand convolutional neural networks, it might
   suffice to roughly understand convolutions. but the aim of this series
   is to bring us to the frontier of convolutional neural networks and
   explore new options. to do that, we   re going to need to understand
   convolutions very deeply.

   thankfully, with a few examples, convolution becomes quite a
   straightforward idea.

lessons from a dropped ball

   imagine we drop a ball from some height onto the ground, where it only
   has one dimension of motion. how likely is it that a ball will go a
   distance \(c\) if you drop it and then drop it again from above the
   point at which it landed?

   let   s break this down. after the first drop, it will land \(a\) units
   away from the starting point with id203 \(f(a)\), where \(f\) is
   the id203 distribution.

   now after this first drop, we pick the ball up and drop it from another
   height above the point where it first landed. the id203 of the
   ball rolling \(b\) units away from the new starting point is \(g(b)\),
   where \(g\) may be a different id203 distribution if it   s dropped
   from a different height.

   if we fix the result of the first drop so we know the ball went
   distance \(a\), for the ball to go a total distance \(c\), the distance
   traveled in the second drop is also fixed at \(b\), where \(a+b=c\). so
   the id203 of this happening is simply \(f(a) \cdot g(b)\).[11]^1

   let   s think about this with a specific discrete example. we want the
   total distance \(c\) to be 3. if the first time it rolls, \(a=2\), the
   second time it must roll \(b=1\) in order to reach our total distance
   \(a+b=3\). the id203 of this is \(f(2) \cdot g(1)\).

   however, this isn   t the only way we could get to a total distance of 3.
   the ball could roll 1 units the first time, and 2 the second. or 0
   units the first time and all 3 the second. it could go any \(a\) and
   \(b\), as long as they add to 3.

   the probabilities are \(f(1) \cdot g(2)\) and \(f(0) \cdot g(3)\),
   respectively.

   in order to find the total likelihood of the ball reaching a total
   distance of \(c\), we can   t consider only one possible way of reaching
   \(c\). instead, we consider all the possible ways of partitioning \(c\)
   into two drops \(a\) and \(b\) and sum over the id203 of each
   way.

   \[...~~ f(0)\!\cdot\! g(3) ~+~ f(1)\!\cdot\! g(2) ~+~ f(2)\!\cdot\!
   g(1)~~...\]

   we already know that the id203 for each case of \(a+b=c\) is
   simply \(f(a) \cdot g(b)\). so, summing over every solution to
   \(a+b=c\), we can denote the total likelihood as:

   \[\sum_{a+b=c} f(a) \cdot g(b)\]

   turns out, we   re doing a convolution! in particular, the convolution of
   \(f\) and \(g\), evluated at \(c\) is defined:

   \[(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)~~~~\]

   if we substitute \(b = c-a\), we get:

   \[(f\ast g)(c) = \sum_a f(a) \cdot g(c-a)\]

   this is the standard definition[12]^2 of convolution.

   to make this a bit more concrete, we can think about this in terms of
   positions the ball might land. after the first drop, it will land at an
   intermediate position \(a\) with id203 \(f(a)\). if it lands at
   \(a\), it has id203 \(g(c-a)\) of landing at a position \(c\).

   to get the convolution, we consider all intermediate positions.

visualizing convolutions

   there   s a very nice trick that helps one think about convolutions more
   easily.

   first, an observation. suppose the id203 that a ball lands a
   certain distance \(x\) from where it started is \(f(x)\). then,
   afterwards, the id203 that it started a distance \(x\) from where
   it landed is \(f(-x)\).

   if we know the ball lands at a position \(c\) after the second drop,
   what is the id203 that the previous position was \(a\)?

   so the id203 that the previous position was \(a\) is \(g(-(a-c))
   = g(c-a)\).

   now, consider the id203 each intermediate position contributes to
   the ball finally landing at \(c\). we know the id203 of the first
   drop putting the ball into the intermediate position a is \(f(a)\). we
   also know that the id203 of it having been in \(a\), if it lands
   at \(c\) is \(g(c-a)\).

   summing over the \(a\)s, we get the convolution.

   the advantage of this approach is that it allows us to visualize the
   evaluation of a convolution at a value \(c\) in a single picture. by
   shifting the bottom half around, we can evaluate the convolution at
   other values of \(c\). this allows us to understand the convolution as
   a whole.

   for example, we can see that it peaks when the distributions align.

   and shrinks as the intersection between the distributions gets smaller.

   by using this trick in an animation, it really becomes possible to
   visually understand convolutions.

   below, we   re able to visualize the convolution of two box functions:
   from wikipedia

   armed with this perspective, a lot of things become more intuitive.

   let   s consider a non-probabilistic example. convolutions are sometimes
   used in audio manipulation. for example, one might use a function with
   two spikes in it, but zero everywhere else, to create an echo. as our
   double-spiked function slides, one spike hits a point in time first,
   adding that signal to the output sound, and later, another spike
   follows, adding a second, delayed copy.

higher dimensional convolutions

   convolutions are an extremely general idea. we can also use them in a
   higher number of dimensions.

   let   s consider our example of a falling ball again. now, as it falls,
   it   s position shifts not only in one dimension, but in two.

   convolution is the same as before:

   \[(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)\]

   except, now \(a\), \(b\) and \(c\) are vectors. to be more explicit,

   \[(f\ast g)(c_1, c_2) =
   \sum_{\begin{array}{c}a_1+b_1=c_1\\a_2+b_2=c_2\end{array}} f(a_1,a_2)
   \cdot g(b_1,b_2)\]

   or in the standard definition:

   \[(f\ast g)(c_1, c_2) = \sum_{a_1, a_2} f(a_1, a_2) \cdot g(c_1-a_1,~
   c_2-a_2)\]

   just like one-dimensional convolutions, we can think of a
   two-dimensional convolution as sliding one function on top of another,
   multiplying and adding.

   one common application of this is image processing. we can think of
   images as two-dimensional functions. many important image
   transformations are convolutions where you convolve the image function
   with a very small, local function called a    kernel.   
   from the [13]river trail documentation

   the kernel slides to every position of the image and computes a new
   pixel as a weighted sum of the pixels it floats over.

   for example, by averaging a 3x3 box of pixels, we can blur an image. to
   do this, our kernel takes the value \(1/9\) on each pixel in the box,
   derived from the [14]gimp documentation

   we can also detect edges by taking the values \(-1\) and \(1\) on two
   adjacent pixels, and zero everywhere else. that is, we subtract two
   adjacent pixels. when side by side pixels are similar, this is gives us
   approximately zero. on edges, however, adjacent pixels are very
   different in the direction perpendicular to the edge.
   derived from the [15]gimp documentation

   the gimp documentation has [16]many other examples.

convolutional neural networks

   so, how does convolution relate to convolutional neural networks?

   consider a 1-dimensional convolutional layer with inputs \(\{x_n\}\)
   and outputs \(\{y_n\}\), like we discussed in the [17]previous post:

   as we observed, we can describe the outputs in terms of the inputs:

   \[y_n = a(x_{n}, x_{n+1}, ...)\]

   generally, \(a\) would be multiple neurons. but suppose it is a single
   neuron for a moment.

   recall that a typical neuron in a neural network is described by:

   \[\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~...~ + b)\]

   where \(x_0\), \(x_1\)    are the inputs. the weights, \(w_0\), \(w_1\),
       describe how the neuron connects to its inputs. a negative weight
   means that an input inhibits the neuron from firing, while a positive
   weight encourages it to. the weights are the heart of the neuron,
   controlling its behavior.[18]^3 saying that multiple neurons are
   identical is the same thing as saying that the weights are the same.

   it   s this wiring of neurons, describing all the weights and which ones
   are identical, that convolution will handle for us.

   typically, we describe all the neurons in a layer at once, rather than
   individually. the trick is to have a weight matrix, \(w\):

   \[y = \sigma(wx + b)\]

   for example, we get:

   \[y_0 = \sigma(w_{0,0}x_0 + w_{0,1}x_1 + w_{0,2}x_2 ...)\]

   \[y_1 = \sigma(w_{1,0}x_0 + w_{1,1}x_1 + w_{1,2}x_2 ...)\]

   each row of the matrix describes the weights connecting a neuron to its
   inputs.

   returning to the convolutional layer, though, because there are
   multiple copies of the same neuron, many weights appear in multiple
   positions.

   which corresponds to the equations:

   \[y_0 = \sigma(w_0x_0 + w_1x_1 -b)\]

   \[y_1 = \sigma(w_0x_1 + w_1x_2 -b)\]

   so while, normally, a weight matrix connects every input to every
   neuron with different weights:

   \[w = \left[\begin{array}{ccccc} w_{0,0} & w_{0,1} & w_{0,2} & w_{0,3}
   & ...\\ w_{1,0} & w_{1,1} & w_{1,2} & w_{1,3} & ...\\ w_{2,0} & w_{2,1}
   & w_{2,2} & w_{2,3} & ...\\ w_{3,0} & w_{3,1} & w_{3,2} & w_{3,3} &
   ...\\ ... & ... & ... & ... & ...\\ \end{array}\right]\]

   the matrix for a convolutional layer like the one above looks quite
   different. the same weights appear in a bunch of positions. and because
   neurons don   t connect to many possible inputs, there   s lots of zeros.

   \[w = \left[\begin{array}{ccccc} w_0 & w_1 & 0 & 0 & ...\\ 0 & w_0 &
   w_1 & 0 & ...\\ 0 & 0 & w_0 & w_1 & ...\\ 0 & 0 & 0 & w_0 & ...\\ ... &
   ... & ... & ... & ...\\ \end{array}\right]\]

   multiplying by the above matrix is the same thing as convolving with
   \([...0, w_1, w_0, 0...]\). the function sliding to different positions
   corresponds to having neurons at those positions.

   what about two-dimensional convolutional layers?

   the wiring of a two dimensional convolutional layer corresponds to a
   two-dimensional convolution.

   consider our example of using a convolution to detect edges in an
   image, above, by sliding a kernel around and applying it to every
   patch. just like this, a convolutional layer will apply a neuron to
   every patch of the image.

conclusion

   we introduced a lot of mathematical machinery in this blog post, but it
   may not be obvious what we gained. convolution is obviously a useful
   tool in id203 theory and computer graphics, but what do we gain
   from phrasing convolutional neural networks in terms of convolutions?

   the first advantage is that we have some very powerful language for
   describing the wiring of networks. the examples we   ve dealt with so far
   haven   t been complicated enough for this benefit to become clear, but
   convolutions will allow us to get rid of huge amounts of unpleasant
   book-keeping for us.

   secondly, convolutions come with significant implementational
   advantages. many libraries provide highly efficient convolution
   routines. further, while convolution naively appears to be an
   \(o(n^2)\) operation, using some rather deep mathematical insights, it
   is possible to create a \(o(n\log(n))\) implementation. we will discuss
   this in much greater detail in a future post.

   in fact, the use of highly-efficient parallel convolution
   implementations on gpus has been essential to recent progress in
   id161.

next posts in this series

   this post is part of a series on convolutional neural networks and
   their generalizations. the first two posts will be review for those
   familiar with deep learning, while later ones should be of interest to
   everyone. to get updates, subscribe to my [19]rss feed!

   please comment below or on the side. pull requests can be made on
   [20]github.

acknowledgments

   i   m extremely grateful to eliana lorch, for extensive discussion of
   convolutions and help writing this post.

   i   m also grateful to michael nielsen and dario amodei for their
   comments and support.
     __________________________________________________________________

    1. we want the id203 of the ball rolling \(a\) units the first
       time and also rolling \(b\) units the second time. the
       distributions \(p(a) = f(a)\) and \(p(b) = g(b)\) are independent,
       with both distributions centered at 0. so \(p(a,b) = p(a) * p(b) =
       f(a) \cdot g(b)\).[21]   
    2. the non-standard definition, which i haven   t previously seen, seems
       to have a lot of benefits. in future posts, we will find this
       definition very helpful because it lends itself to generalization
       to new algebraic structures. but it also has the advantage that it
       makes a lot of algebraic properties of convolutions really obvious.
       for example, convolution is a commutative operation. that is,
       \(f\ast g = g\ast f\). why?
       \[\sum_{a+b=c} f(a) \cdot g(b) ~~=~ \sum_{b+a=c} g(b) \cdot f(a)\]
       convolution is also associative. that is, \((f\ast g)\ast h = f\ast
       (g\ast h)\). why?
       \[\sum_{(a+b)+c=d} (f(a) \cdot g(b)) \cdot h(c) ~~=~
       \sum_{a+(b+c)=d} f(a) \cdot (g(b) \cdot h(c))\][22]   
    3. there   s also the bias, which is the    threshold    for whether the
       neuron fires, but it   s much simpler and i don   t want to clutter
       this section talking about it.[23]   

   subscribe to the [24]rss feed. built by [25]oinkina with [26]hakyll
   using [27]bootstrap, [28]mathjax, and [29]disqus.

   enable javascript for footnotes, disqus comments, and other cool stuff.

references

   1. http://colah.github.io/
   2. http://colah.github.io/
   3. http://colah.github.io/about.html
   4. http://colah.github.io/contact.html
   5. http://colah.github.io/posts/tags/neural_networks.html
   6. http://colah.github.io/posts/tags/convolutional_neural_networks.html
   7. http://colah.github.io/posts/tags/convolution.html
   8. http://colah.github.io/posts/tags/math.html
   9. http://colah.github.io/posts/tags/id203.html
  10. http://colah.github.io/posts/2014-07-conv-nets-modular/
  11. http://colah.github.io/posts/2014-07-understanding-convolutions/#fn1
  12. http://colah.github.io/posts/2014-07-understanding-convolutions/#fn2
  13. http://intellabs.github.io/rivertrail/tutorial/
  14. http://docs.gimp.org/en/plug-in-convmatrix.html
  15. http://docs.gimp.org/en/plug-in-convmatrix.html
  16. http://docs.gimp.org/en/plug-in-convmatrix.html
  17. http://colah.github.io/posts/2014-07-conv-nets-modular/
  18. http://colah.github.io/posts/2014-07-understanding-convolutions/#fn3
  19. http://colah.github.io/rss.xml
  20. https://github.com/colah/conv-nets-series
  21. http://colah.github.io/posts/2014-07-understanding-convolutions/#fnref1
  22. http://colah.github.io/posts/2014-07-understanding-convolutions/#fnref2
  23. http://colah.github.io/posts/2014-07-understanding-convolutions/#fnref3
  24. http://colah.github.io/rss.xml
  25. https://github.com/oinkina
  26. http://jaspervdj.be/hakyll
  27. http://getbootstrap.com/
  28. http://www.mathjax.org/
  29. http://disqus.com/
