   (button) toggle navigation [1]colah's blog
     * [2]blog
     * [3]about
     * [4]contact

conv nets: a modular perspective

   posted on july 8, 2014

   [5]neural networks, [6]deep learning, [7]convolutional neural networks,
   [8]modular neural networks

introduction

   in the last few years, deep neural networks have lead to breakthrough
   results on a variety of pattern recognition problems, such as computer
   vision and voice recognition. one of the essential components leading
   to these results has been a special kind of neural network called a
   convolutional neural network.

   at its most basic, convolutional neural networks can be thought of as a
   kind of neural network that uses many identical copies of the same
   neuron.[9]^1 this allows the network to have lots of neurons and
   express computationally large models while keeping the number of actual
   parameters     the values describing how neurons behave     that need to be
   learned fairly small.
   a 2d convolutional neural network

   this trick of having multiple copies of the same neuron is roughly
   analogous to the abstraction of functions in mathematics and computer
   science. when programming, we write a function once and use it in many
   places     not writing the same code a hundred times in different places
   makes it faster to program, and results in fewer bugs. similarly, a
   convolutional neural network can learn a neuron once and use it in many
   places, making it easier to learn the model and reducing error.

structure of convolutional neural networks

   suppose you want a neural network to look at audio samples and predict
   whether a human is speaking or not. maybe you want to do more analysis
   if someone is speaking.

   you get audio samples at different points in time. the samples are
   evenly spaced.

   the simplest way to try and classify them with a neural network is to
   just connect them all to a fully-connected layer. there are a bunch of
   different neurons, and every input connects to every neuron.

   a more sophisticated approach notices a kind of symmetry in the
   properties it   s useful to look for in the data. we care a lot about
   local properties of the data: what frequency of sounds are there around
   a given time? are they increasing or decreasing? and so on.

   we care about the same properties at all points in time. it   s useful to
   know the frequencies at the beginning, it   s useful to know the
   frequencies in the middle, and it   s also useful to know the frequencies
   at the end. again, note that these are local properties, in that we
   only need to look at a small window of the audio sample in order to
   determine them.

   so, we can create a group of neurons, \(a\), that look at small time
   segments of our data.[10]^2 \(a\) looks at all such segments, computing
   certain features. then, the output of this convolutional layer is fed
   into a fully-connected layer, \(f\).

   in the above example, \(a\) only looked at segments consisting of two
   points. this isn   t realistic. usually, a convolution layer   s window
   would be much larger.

   in the following example, \(a\) looks at 3 points. that isn   t realistic
   either     sadly, it   s tricky to visualize \(a\) connecting to lots of
   points.

   one very nice property of convolutional layers is that they   re
   composable. you can feed the output of one convolutional layer into
   another. with each layer, the network can detect higher-level, more
   abstract features.

   in the following example, we have a new group of neurons, \(b\). \(b\)
   is used to create another convolutional layer stacked on top of the
   previous one.

   convolutional layers are often interweaved with pooling layers. in
   particular, there is a kind of layer called a max-pooling layer that is
   extremely popular.

   often, from a high level perspective, we don   t care about the precise
   point in time a feature is present. if a shift in frequency occurs
   slightly earlier or later, does it matter?

   a max-pooling layer takes the maximum of features over small blocks of
   a previous layer. the output tells us if a feature was present in a
   region of the previous layer, but not precisely where.

   max-pooling layers kind of    zoom out   . they allow later convolutional
   layers to work on larger sections of the data, because a small patch
   after the pooling layer corresponds to a much larger patch before it.
   they also make us invariant to some very small transformations of the
   data.

   in our previous examples, we   ve used 1-dimensional convolutional
   layers. however, convolutional layers can work on higher-dimensional
   data as well. in fact, the most famous successes of convolutional
   neural networks are applying 2d convolutional neural networks to
   recognizing images.

   in a 2-dimensional convolutional layer, instead of looking at segments,
   \(a\) will now look at patches.

   for each patch, \(a\) will compute features. for example, it might
   learn to detect the presence of an edge. or it might learn to detect a
   texture. or perhaps a contrast between two colors.

   in the previous example, we fed the output of our convolutional layer
   into a fully-connected layer. but we can also compose two convolutional
   layers, as we did in the one dimensional case.

   we can also do max pooling in two dimensions. here, we take the maximum
   of features over a small patch.

   what this really boils down to is that, when considering an entire
   image, we don   t care about the exact position of an edge, down to a
   pixel. it   s enough to know where it is to within a few pixels.

   three-dimensional convolutional networks are also sometimes used, for
   data like videos or volumetric data (eg. 3d medical scans). however,
   they are not very widely used, and much harder to visualize.

   now, we previously said that \(a\) was a group of neurons. we should be
   a bit more precise about this: what is \(a\) exactly?

   in traditional convolutional layers, \(a\) is a bunch of neurons in
   parallel, that all get the same inputs and compute different features.

   for example, in a 2-dimensional convolutional layer, one neuron might
   detect horizontal edges, another might detect vertical edges, and
   another might detect green-red color contrasts.

   that said, in the recent paper    network in network    ([11]lin et al.
   (2013)), a new    mlpconv    layer is proposed. in this model, \(a\) would
   have multiple layers of neurons, with the final layer outputting higher
   level features for the region. in the paper, the model achieves some
   very impressive results, setting new state of the art on a number of
   benchmark datasets.

   that said, for the purposes of this post, we will focus on standard
   convolutional layers. there   s already enough for us to consider there!

results of convolutional neural networks

   earlier, we alluded to recent breakthroughs in id161 using
   convolutional neural networks. before we go on, i   d like to briefly
   discuss some of these results as motivation.

   in 2012, alex krizhevsky, ilya sutskever, and geoff hinton blew
   existing image classification results out of the water ([12]krizehvsky
   et al. (2012)).

   their progress was the result of combining together a bunch of
   different pieces. they used gpus to train a very large, deep, neural
   network. they used a new kind of neuron (relus) and a new technique to
   reduce a problem called    overfitting    (dropout). they used a very large
   dataset with lots of image categories ([13]id163). and, of course,
   it was a convolutional neural network.

   their architecture, illustrated below, was very deep. it has 5
   convolutional layers,[14]^3 with pooling interspersed, and three
   fully-connected layers. the early layers are split over the two gpus.
   from [15]krizehvsky et al. (2012)

   they trained their network to classify images into a thousand different
   categories.

   randomly guessing, one would guess the correct answer 0.1% of the time.
   krizhevsky, et al.   s model is able to give the right answer 63% of the
   time. further, one of the top 5 answers it gives is right 85% of the
   time!
   top: 4 correctly classified examples. bottom: 4 incorrectly classified
   examples. each example has an image, followed by its label, followed by
   the top 5 guesses with probabilities. from [16]krizehvsky et al.
   (2012).

   even some of its errors seem pretty reasonable to me!

   we can also examine what the first layer of the network learns to do.

   recall that the convolutional layers were split between the two gpus.
   information doesn   t go back and forth each layer, so the split sides
   are disconnected in a real way. it turns out that, every time the model
   is run, the two sides specialize.
   filters learned by the first convolutional layer. the top half
   corresponds to the layer on one gpu, the bottom on the other. from
   [17]krizehvsky et al. (2012)

   neurons in one side focus on black and white, learning to detect edges
   of different orientations and sizes. neurons on the other side
   specialize on color and texture, detecting color contrasts and
   patterns.[18]^4 remember that the neurons are randomly initialized. no
   human went and set them to be edge detectors, or to split in this way.
   it arose simply from training the network to classify images.

   these remarkable results (and other exciting results around that time)
   were only the beginning. they were quickly followed by a lot of other
   work testing modified approaches and gradually improving the results,
   or applying them to other areas. and, in addition to the neural
   networks community, many in the id161 community have adopted
   deep convolutional neural networks.

   convolutional neural networks are an essential tool in id161
   and modern pattern recognition.

formalizing convolutional neural networks

   consider a 1-dimensional convolutional layer with inputs \(\{x_n\}\)
   and outputs \(\{y_n\}\):

   it   s relatively easy to describe the outputs in terms of the inputs:

   \[y_n = a(x_{n}, x_{n+1}, ...)\]

   for example, in the above:

   \[y_0 = a(x_0, x_1)\] \[y_1 = a(x_1, x_2)\]

   similarly, if we consider a 2-dimensional convolutional layer, with
   inputs \(\{x_{n,m}\}\) and outputs \(\{y_{n,m}\}\):

   we can, again, write down the outputs in terms of the inputs:

   \[y_{n,m} = a\left(\begin{array}{ccc} x_{n,~m}, & x_{n+1,~m},& ...,~\\
   x_{n,~m+1}, & x_{n+1,~m+1}, & ..., ~\\ &...\\\end{array}\right)\]

   for example:

   \[y_{0,0} = a\left(\begin{array}{cc} x_{0,~0}, & x_{1,~0},~\\ x_{0,~1},
   & x_{1,~1}~\\\end{array}\right)\] \[y_{1,0} = a\left(\begin{array}{cc}
   x_{1,~0}, & x_{2,~0},~\\ x_{1,~1}, & x_{2,~1}~\\\end{array}\right)\]

   if one combines this with the equation for \(a(x)\),

   \[a(x) = \sigma(wx + b)\]

   one has everything they need to implement a convolutional neural
   network, at least in theory.

   in practice, this is often not best way to think about convolutional
   neural networks. there is an alternative formulation, in terms of a
   mathematical operation called convolution, that is often more helpful.

   the convolution operation is a powerful tool. in mathematics, it comes
   up in diverse contexts, ranging from the study of partial differential
   equations to id203 theory. in part because of its role in pdes,
   convolution is very important in the physical sciences. it also has an
   important role in many applied areas, like computer graphics and signal
   processing.

   for us, convolution will provide a number of benefits. firstly, it will
   allow us to create much more efficient implementations of convolutional
   layers than the naive perspective might suggest. secondly, it will
   remove a lot of messiness from our formulation, handling all the
   bookkeeping presently showing up in the indexing of \(x\)s     the
   present formulation may not seem messy yet, but that   s only because we
   haven   t got into the tricky cases yet. finally, convolution will give
   us a significantly different perspective for reasoning about
   convolutional layers.

     i admire the elegance of your method of computation; it must be nice
     to ride through these fields upon the horse of true mathematics
     while the like of us have to make our way laboriously on foot.       
     albert einstein

next posts in this series

   [19]read the next post!

   this post is part of a series on convolutional neural networks and
   their generalizations. the first two posts will be review for those
   familiar with deep learning, while later ones should be of interest to
   everyone. to get updates, subscribe to my [20]rss feed!

   please comment below or on the side. pull requests can be made on
   [21]github.

acknowledgments

   i   m grateful to eliana lorch, aaron courville, and sebastian zany for
   their comments and support.
     __________________________________________________________________

    1. it should be noted that not all neural networks that use multiple
       copies of the same neuron are convolutional neural networks.
       convolutional neural networks are just one type of neural network
       that uses the more general trick, weight-tying. other kinds of
       neural network that do this are recurrent neural networks and
       id56s.[22]   
    2. groups of neurons, like \(a\), that appear in multiple places are
       sometimes called modules, and networks that use them are sometimes
       called modular neural networks.[23]   
    3. they also test using 7 in the paper.[24]   
    4. this seems to have interesting analogies to rods and cones in the
       retina.[25]   

   subscribe to the [26]rss feed. built by [27]oinkina with [28]hakyll
   using [29]bootstrap, [30]mathjax, and [31]disqus.

   enable javascript for footnotes, disqus comments, and other cool stuff.

references

   1. http://colah.github.io/
   2. http://colah.github.io/
   3. http://colah.github.io/about.html
   4. http://colah.github.io/contact.html
   5. http://colah.github.io/posts/tags/neural_networks.html
   6. http://colah.github.io/posts/tags/deep_learning.html
   7. http://colah.github.io/posts/tags/convolutional_neural_networks.html
   8. http://colah.github.io/posts/tags/modular_neural_networks.html
   9. http://colah.github.io/posts/2014-07-conv-nets-modular/#fn1
  10. http://colah.github.io/posts/2014-07-conv-nets-modular/#fn2
  11. http://arxiv.org/abs/1312.4400
  12. http://www.cs.toronto.edu/~fritz/absps/id163.pdf
  13. http://www.image-net.org/
  14. http://colah.github.io/posts/2014-07-conv-nets-modular/#fn3
  15. http://www.cs.toronto.edu/~fritz/absps/id163.pdf
  16. http://www.cs.toronto.edu/~fritz/absps/id163.pdf
  17. http://www.cs.toronto.edu/~fritz/absps/id163.pdf
  18. http://colah.github.io/posts/2014-07-conv-nets-modular/#fn4
  19. http://colah.github.io/posts/2014-07-understanding-convolutions/
  20. http://colah.github.io/rss.xml
  21. https://github.com/colah/conv-nets-series
  22. http://colah.github.io/posts/2014-07-conv-nets-modular/#fnref1
  23. http://colah.github.io/posts/2014-07-conv-nets-modular/#fnref2
  24. http://colah.github.io/posts/2014-07-conv-nets-modular/#fnref3
  25. http://colah.github.io/posts/2014-07-conv-nets-modular/#fnref4
  26. http://colah.github.io/rss.xml
  27. https://github.com/oinkina
  28. http://jaspervdj.be/hakyll
  29. http://getbootstrap.com/
  30. http://www.mathjax.org/
  31. http://disqus.com/
