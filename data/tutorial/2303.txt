   find the rest of the how neural networks work video series [1]in this
   free online course.

   iframe: [2]https://www.youtube.com/embed/fmpdiaimiea

   [3]slides         [4]pdf [2mb]         [5]ppt [6mb]
   [6]in french by [7]charles crouspeyre
   [8]in japanese
   [9]in simplified mandarin by [10]jimmy lin
   [11]in traditional mandarin by [12]jimmy lin
   [13]in persian by [14]elham khanchebemehr
           [15]related presentation by [16]mohammad khalooei
   [17]matlab and caffe implementations for nvidia gpus by [18]alexander
   hanuschkin [19]top kdnuggets blogger - gold, september 2016
   [20]mybridge machine learning top 10, september 2016

   nine times out of ten, when you hear about deep learning breaking a new
   technological barrier, convolutional neural networks are involved. also
   called id98s or convnets, these are the workhorse of the deep neural
   network field. they have learned to sort images into categories even
   better than humans in some cases. if there   s one method out there that
   justifies the hype, it is id98s.

   what   s especially cool about them is that they are easy to understand,
   at least when you break them down into their basic parts. i   ll walk you
   through it. there's a video that talks through these images in greater
   detail. if at any point you get a bit lost, just click on an image and
   you'll jump to that part of the video.

x's and o's

   [21][id981.png]

   to help guide our walk through a convolutional neural network, we   ll
   stick with a very simplified example: determining whether an image is
   of an x or an o. this example is just rich enough to illustrate the
   principles behind id98s, but still simple enough to avoid getting bogged
   down in non-essential details. our id98 has one job. each time we hand
   it a picture, it has to decide whether it has an x or an o. it assumes
   there is always one or the other.
   [22][id982.png]

   a na  ve approach to solving this problem is to save an image of an x
   and an o and compare every new image to our exemplars to see which is
   the better match. what makes this task tricky is that computers are
   extremely literal. to a computer, an image looks like a two-dimensional
   array of pixels (think giant checkerboard) with a number in each
   position. in our example a pixel value of 1 is white, and -1 is black.
   when comparing two images, if any pixel values don   t match, then the
   images don   t match, at least to the computer. ideally, we would like to
   be able to see x   s and o   s even if they   re shifted, shrunken, rotated
   or deformed. this is where id98s come in.

features

   [23][id983.png]

   id98s compare images piece by piece. the pieces that it looks for are
   called features. by finding rough feature matches in roughly the same
   positions in two images, id98s get a lot better at seeing similarity
   than whole-image matching schemes.
   [24][id984.png]

   each feature is like a mini-image   a small two-dimensional array of
   values. features match common aspects of the images. in the case of x
   images, features consisting of diagonal lines and a crossing capture
   all the important characteristics of most x   s. these features will
   probably match up to the arms and center of any image of an x.

convolution

   [25][id985.png]

   when presented with a new image, the id98 doesn   t know exactly where
   these features will match so it tries them everywhere, in every
   possible position. in calculating the match to a feature across the
   whole image, we make it a filter. the math we use to do this is called
   convolution, from which convolutional neural networks take their name.

   the math behind convolution is nothing that would make a sixth-grader
   uncomfortable. to calculate the match of a feature to a patch of the
   image, simply multiply each pixel in the feature by the value of the
   corresponding pixel in the image. then add up the answers and divide by
   the total number of pixels in the feature. if both pixels are white (a
   value of 1) then 1 * 1 = 1. if both are black, then (-1) * (-1) = 1.
   either way, every matching pixel results in a 1. similarly, any
   mismatch is a -1. if all the pixels in a feature match, then adding
   them up and dividing by the total number of pixels gives a 1.
   similarly, if none of the pixels in a feature match the image patch,
   then the answer is a -1.
   [26][id986.png]

   to complete our convolution, we repeat this process, lining up the
   feature with every possible image patch. we can take the answer from
   each convolution and make a new two-dimensional array from it, based on
   where in the image each patch is located. this map of matches is also a
   filtered version of our original image. it   s a map of where in the
   image the feature is found. values close to 1 show strong matches,
   values close to -1 show strong matches for the photographic negative of
   our feature, and values near zero show no match of any sort.
   [27][id987.png]

   the next step is to repeat the convolution process in its entirety for
   each of the other features. the result is a set of filtered images, one
   for each of our filters. it   s convenient to think of this whole
   collection of convolution operations as a single processing step. in
   id98s this is referred to as a convolution layer, hinting at the fact
   that it will soon have other layers added to it.

   it   s easy to see how id98s get their reputation as computation hogs.
   although we can sketch our id98 on the back of a napkin, the number of
   additions, multiplications and divisions can add up fast. in math
   speak, they scale linearly with the number of pixels in the image, with
   the number of pixels in each feature and with the number of features.
   with so many factors, it   s easy to make this problem many millions of
   times larger without breaking a sweat. small wonder that microchip
   manufacturers are now making specialized chips in an effort to keep up
   with the demands of id98s.

pooling

   [28][id988.png]

   another power tool that id98s use is called pooling. pooling is a way to
   take large images and shrink them down while preserving the most
   important information in them. the math behind pooling is second-grade
   level at most. it consists of stepping a small window across an image
   and taking the maximum value from the window at each step. in practice,
   a window 2 or 3 pixels on a side and steps of 2 pixels work well.

   after pooling, an image has about a quarter as many pixels as it
   started with. because it keeps the maximum value from each window, it
   preserves the best fits of each feature within the window. this means
   that it doesn   t care so much exactly where the feature fit as long as
   it fit somewhere within the window. the result of this is that id98s can
   find whether a feature is in an image without worrying about where it
   is. this helps solve the problem of computers being hyper-literal.
   [29][id989.png]

   a pooling layer is just the operation of performing pooling on an image
   or a collection of images. the output will have the same number of
   images, but they will each have fewer pixels. this is also helpful in
   managing the computational load. taking an 8 megapixel image down to a
   2 megapixel image makes life a lot easier for everything downstream.

rectified linear units

   [30][id9810.png]

   a small but important player in this process is the rectified linear
   unit or relu. it   s math is also very simple   wherever a negative number
   occurs, swap it out for a 0. this helps the id98 stay mathematically
   healthy by keeping learned values from getting stuck near 0 or blowing
   up toward infinity. it   s the axle grease of id98s   not particularly
   glamorous, but without it they don   t get very far.
   [31][id9811.png]

   the output of a relu layer is the same size as whatever is put into it,
   just with all the negative values removed.

deep learning

   [32][id9812.png]

   you   ve probably noticed that the input to each layer (two-dimensional
   arrays) looks a lot like the output (two-dimensional arrays). because
   of this, we can stack them like lego bricks. raw images get filtered,
   rectified and pooled to create a set of shrunken, feature-filtered
   images. these can be filtered and shrunken again and again. each time,
   the features become larger and more complex, and the images become more
   compact. this lets lower layers represent simple aspects of the image,
   such as edges and bright spots. higher layers can represent
   increasingly sophisticated aspects of the image, such as shapes and
   patterns. these tend to be readily recognizable. for instance, in a id98
   trained on human faces, the highest layers represent patterns that are
   clearly face-like.
   [33][id9818.png]

fully connected layers

   [34][id9813.png]

   id98s have one more arrow in their quiver. fully connected layers take
   the high-level filtered images and translate them into votes. in our
   case, we only have to decide between two categories, x and o. fully
   connected layers are the primary building block of traditional neural
   networks. instead of treating inputs as a two-dimensional array, they
   are treated as a single list and all treated identically. every value
   gets its own vote on whether the current image is an x or and o.
   however, the process isn   t entirely democratic. some values are much
   better than others at knowing when the image is an x, and some are
   particularly good at knowing when the image is an o. these get larger
   votes than the others. these votes are expressed as weights, or
   connection strengths, between each value and each category.

   when a new image is presented to the id98, it percolates through the
   lower layers until it reaches the fully connected layer at the end.
   then an election is held. the answer with the most votes wins and is
   declared the category of the input.
   [35][id9814.png]

   fully connected layers, like the rest, can be stacked because their
   outputs (a list of votes) look a whole lot like their inputs (a list of
   values). in practice, several fully connected layers are often stacked
   together, with each intermediate layer voting on phantom    hidden   
   categories. in effect, each additional layer lets the network learn
   ever more sophisticated combinations of features that help it make
   better decisions.

id26

   [36][id9815.png]

   our story is filling in nicely, but it still has a huge hole   where do
   features come from? and how do we find the weights in our fully
   connected layers? if these all had to be chosen by hand, id98s would be
   a good deal less popular than they are. luckily, a bit of machine
   learning magic called id26 does this work for us.

   to make use of id26, we need a collection of images that we
   already know the answer for. this means that some patient soul flipped
   through thousands of images and assigned them a label of x or o. we use
   these with an untrained id98, which means that every pixel of every
   feature and every weight in every fully connected layer is set to a
   random value. then we start feeding images through it, one after other.

   each image the id98 processes results in a vote. the amount of wrongness
   in the vote, the error, tells us how good our features and weights are.
   the features and weights can then be adjusted to make the error less.
   each value is adjusted a little higher and a little lower, and the new
   error computed each time. whichever adjustment makes the error less is
   kept. after doing this for every feature pixel in every convolutional
   layer and every weight in every fully connected layer, the new weights
   give an answer that works slightly better for that image. this is then
   repeated with each subsequent image in the set of labeled images.
   quirks that occur in a single image are quickly forgotten, but patterns
   that occur in lots of images get baked into the features and connection
   weights. if you have enough labeled images, these values stabilize to a
   set that works pretty well across a wide variety of cases.

   as is probably apparent, id26 is another expensive computing
   step, and another motivator for specialized computing hardware.

hyperparameters

   unfortunately, not every aspect of id98s can be learned in so
   straightforward a manner. there is still a long list of decisions that
   a id98 designer must make.
     * for each convolution layer, how many features? how many pixels in
       each feature?
     * for each pooling layer, what window size? what stride?
     * for each extra fully connected layer, how many hidden neurons?

   in addition to these there are also higher level architectural
   decisions to make: how many of each layer to include? in what order?
   some deep neural networks can have over a thousand layers, which opens
   up a lot of possibilities.

   with so many combinations and permutations, only a small fraction of
   the possible id98 configurations have been tested. id98 designs tend to
   be driven by accumulated community knowledge, with occasional
   deviations showing surprising jumps in performance. and while we   ve
   covered the building blocks of vanilla id98s, there are lots of other
   tweaks that have been tried and found effective, such as new layer
   types and more complex ways to connect layers with each other.

beyond images

   [37][id9816.png]

   while our x and o example involves images, id98s can be used to
   categorize other types of data too. the trick is, whatever data type
   you start with, to transform it to make it look like an image. for
   instance, audio signals can be chopped into short time chunks, and then
   each chunk broken up into bass, midrange, treble, or finer frequency
   bands. this can be represented as a two-dimensional array where each
   column is a time chunk and each row is a frequency band.    pixels    in
   this fake picture that are close together are closely related. id98s
   work well on this. researchers have gotten quite creative. they have
   adapted text data for natural language processing and even chemical
   data for drug discovery.
   [38][id9817.png]

   an example of data that doesn   t fit this format is customer data, where
   each row in a table represents a customer, and each column represents
   information about them, such as name, address, email, purchases and
   browsing history. in this case, the location of rows and columns
   doesn   t really matter. rows can be rearranged and columns can be
   re-ordered without losing any of the usefulness of the data. in
   contrast, rearranging the rows and columns of an image makes it largely
   useless.

   a rule of thumb: if your data is just as useful after swapping any of
   your columns with each other, then you can   t use convolutional neural
   networks.

   however if you can make your problem look like finding patterns in an
   image, then id98s may be exactly what you need.

learn more

   [39][deep_learning.png]

   for more details on how to build this x-and-o example, check out
   [40]dr. alexander hanuschkin's excellent [41]matlab and caffe
   implementations for nvidia gpus. if you'd like to dig deeper into deep
   learning, check out my [42]demystifying deep learning post. i also
   recommend the [43]notes from the stanford cs 231 course by justin
   johnson and andrej karpathy that provided inspiration for this post, as
   well as the writings of [44]christopher olah, an exceptionally clear
   writer on the subject of neural networks.

   if you are one who loves to learn by doing, there are a number of
   popular deep learning tools available. try them all! and then tell us
   what you think.
     * [45]caffe
     * [46]cntk
     * [47]deeplearning4j
     * [48]tensorflow
     * [49]theano
     * [50]torch
     * [51]many others

   i hope you've enjoyed our walk through the neighborhood of
   convolutional neural networks. feel free to start up a conversation.

references

   1. https://end-to-end-machine-learning.teachable.com/p/how-deep-neural-networks-work
   2. https://www.youtube.com/embed/fmpdiaimiea
   3. https://pytorch.org/
   4. https://github.com/brohrer/public-hosting/raw/master/how_id98s_work.pdf
   5. https://github.com/brohrer/public-hosting/blob/master/how_id98s_work.pptx?raw=true
   6. https://medium.com/@charlescrouspeyre/comment-les-r  seaux-de-neurones-  -convolution-fonctionnent-b288519dbcf8
   7. https://www.linkedin.com/in/ccrouspeyre/
   8. http://postd.cc/how-do-convolutional-neural-networks-work/
   9. https://brohrer.mcknote.com/zh-hans/how_machine_learning_works/how_convolutional_neural_networks_work.html
  10. https://brohrer.mcknote.com/
  11. https://brohrer.mcknote.com/zh-hant/how_machine_learning_works/how_convolutional_neural_networks_work.html
  12. https://brohrer.mcknote.com/
  13. https://elham-khanche.github.io/blog/howid98swork/
  14. https://www.linkedin.com/in/elham-khanchebemehr-b679547b/
  15. http://www.slideshare.net/khalooei/ss-70486910
  16. https://www.linkedin.com/in/khalooei/
  17. http://www.optophysiology.uni-freiburg.de/research/research_dl/id98swithmatlabandcaffe
  18. http://www.optophysiology.uni-freiburg.de/labmembers/hanuschkin
  19. http://www.kdnuggets.com/2016/09/top-news-week-0829-0904.html
  20. https://medium.mybridge.co/machine-learning-top-10-in-september-6838169e9ee7#.nq7k47zce
  21. https://youtu.be/fmpdiaimiea?t=1m43s
  22. https://youtu.be/fmpdiaimiea?t=3m05s
  23. https://youtu.be/fmpdiaimiea?t=3m59s
  24. https://youtu.be/fmpdiaimiea?t=4m20s
  25. https://youtu.be/fmpdiaimiea?t=4m55s
  26. https://youtu.be/fmpdiaimiea?t=7m20s
  27. https://youtu.be/fmpdiaimiea?t=8m20s
  28. https://youtu.be/fmpdiaimiea?t=9m20s
  29. https://youtu.be/fmpdiaimiea?t=11m31s
  30. https://youtu.be/fmpdiaimiea?t=11m46s
  31. https://youtu.be/fmpdiaimiea?t=12m37s
  32. https://youtu.be/fmpdiaimiea?t=12m51s
  33. http://web.eecs.umich.edu/~honglak/icml09-convolutionaldeepbeliefnetworks.pdf
  34. https://youtu.be/fmpdiaimiea?t=14m03s
  35. https://youtu.be/fmpdiaimiea?t=16m32s
  36. https://youtu.be/fmpdiaimiea?t=18m13s
  37. https://youtu.be/fmpdiaimiea?t=21m26s
  38. https://youtu.be/fmpdiaimiea?t=23m36s
  39. http://brohrer.github.io/deep_learning_demystified.html
  40. http://www.optophysiology.uni-freiburg.de/labmembers/hanuschkin
  41. http://www.optophysiology.uni-freiburg.de/research/research_dl/id98swithmatlabandcaffe
  42. http://brohrer.github.io/deep_learning_demystified.html
  43. http://cs231n.github.io/convolutional-networks/
  44. http://colah.github.io/archive.html
  45. http://caffe.berkeleyvision.org/
  46. https://github.com/microsoft/cntk
  47. https://en.wikipedia.org/wiki/deeplearning4j
  48. http://www.tensorflow.org/
  49. https://en.wikipedia.org/wiki/theano_(software)
  50. https://en.wikipedia.org/wiki/torch_(machine_learning)
  51. http://deeplearning.net/software_links/
