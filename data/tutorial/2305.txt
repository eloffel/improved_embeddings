   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    deep learning for id161     introduction
   to convolution neural networks comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]deep learning [94]deep learning for id161    
   introduction to convolution neural networks

   [95]deep learning[96]machine learning[97]python

deep learning for id161     introduction to convolution neural
networks

   [98]aarshay jain, april 4, 2016

introduction

   the power of artificial intelligence is beyond our imagination. we all
   know robots have already reached a testing phase in some of the
   powerful countries of the world. governments, large companies are
   spending billions in developing this ultra-intelligence creature. the
   recent existence of robots have gained attention of many research
   houses across the world.

   does it excite you as well ? personally for me, learning about robots &
   developments in ai started with a deep curiosity and excitement in me!
   let   s learn about id161 today.

   the earliest research in id161 started way back in 1950s.
   since then, we have come a long way but still find ourselves far from
   the ultimate objective. but with neural networks and deep learning, we
   have become empowered like never before.

   applications of deep learning in vision have taken this technology to a
   different level and made sophisticated things like self-driven cars
   possible in near future. in this article, i will also introduce you to
   convolution neural networks which form the crux of deep learning
   applications in id161.

   note: this article is inspired by [99]stanford   s class on visual
   recognition. understanding this article requires prior knowledge of
   neural networks. if you are new to neural networks, you can [100]start
   here. another useful resource on basics of deep learning can be
   found [101]here.

table of contents

    1. challenges in id161
    2. overview of traditional approaches
    3. review of neural networks fundamentals
    4. introduction to convolution neural networks
    5. case study: increasing power of of id98s in id163 competition
    6. implementing id98s using graphlab (practical in python)


1. challenges in id161 (cv)

   as the name suggests, the aim of id161 (cv) is to imitate the
   functionality of human eye and brain components responsible for your
   sense of sight.

   doing actions such as recognizing an animal, describing a view,
   differentiating among visible objects are really a cake-walk for
   humans. you   d be surprised to know that it took decades of research to
   discover and impart the ability of detecting an object to a computer
   with reasonable accuracy.

   the field of id161 has witnessed continual advancements in
   the past 5 years. one of the most stated advancement is convolution
   neural networks (id98s). today, deep id98s form the crux of
   most sophisticated fancy id161 application, such as
   self-driving cars, auto-tagging of friends in our facebook pictures,
   facial security features, gesture recognition, automatic number plate
   recognition, etc.

   let   s get familiar with it a bit more:

   id164 is considered to be the most basic application of
   id161. rest of the other developments in id161 are
   achieved by making small enhancements on top of this. in real life,
   every time we(humans) open our eyes, we unconsciously detect objects.

   since it is super-intuitive for us, we fail to appreciate the key
   challenges involved when we try to design systems similar to our eye.
   lets start by looking at some of the key roadblocks:
    1. variations in viewpoint
          + the same object can have different positions and angles in an
            image depending on the relative position of the object and the
            observer.
          + there can also be different positions. for instance look at
            the following images:[102] cat_poses
          + though its obvious to know that these are the same object, it
            is not very easy to teach this aspect to a computer (robots or
            machines).
    2. difference in illumination
          + different images can have different light conditions. for
            instance:
            [103]cat_illum
          + though this image is so dark, we can still recognize that it
            is a cat. teaching this to a computer is another challenge.
    3. hidden parts of images
          + images need not necessarily be complete. small or large
            proportions of the images might be hidden which makes the
            detection task difficult. for instance:
            [104]hidden_dog
          + here, only the face of the puppy is visible and that too
            partially, posing another challenge for the computer to
            recognize.
    4. background clutter
          + some images might blend into the background. for instance:
            [105]background_clutter
          + if you observe carefully, you can find a man in this image. as
            simple as it looks, it   s an uphill task for a computer to
            learn.

   these are just some of the challenges which i brought up so that you
   can appreciate the complexity of the tasks which your eye and brain duo
   does with such utter ease. breaking up all these challenges and solving
   individually is still possible today in id161. but we   re
   still decades away from a system which can get anywhere close to our
   human eye (which can do everything!).

   this brilliance of our human body is the reason why researchers have
   been trying to break the enigma of id161 by analyzing the
   visual mechanics of humans or other animals. some of the earliest work
   in this direction was done by hubel and weisel with their famous cat
   experiment in 1959. read more about it [106]here.

   this was the first study which emphasized the importance of edge
   detection for solving the id161 problem. they were rewarded
   the nobel prize for their work.

   before diving into convolutional neural networks, lets take a quick
   overview of the traditional or rather elementary techniques used in
   id161 before deep learning became popular.


2. overview of traditional approaches

   various techniques, other than deep learning are available enhancing
   id161. though, they work well for simpler problems, but as
   the data become huge and the task becomes complex, they are no
   substitute for deep id98s. let   s briefly discuss two simple approaches.
    1. knn (k-nearest neighbours)
          + each image is matched with all images in training data. the
            top k with minimum distances are selected. the majority class
            of those top k is predicted as output class of the image.
          + various distance metrics can be used like l1 distance (sum of
            absolute distance), l2 distance (sum of squares), etc.
          + drawbacks:
               o even if we take the image of same object with same
                 illumination and orientation, the object might lie in
                 different locations of image, i.e. left, right or center
                 of image. for instance:
                 [107]dog_location
               o here the same dog is on right side in first image and
                 left side in second. though its the same image, knn would
                 give highly non-zero distance for the 2 images.
               o similar to above, other challenges mentioned in section 1
                 will be faced by knn.
    2. linear classifiers
          + they use a parametric approach where each pixel value is
            considered as a parameter.
          + it   s like a weighted sum of the pixel values with the
            dimension of the weights matrix depending on the number of
            outcomes.
          + intuitively, we can understand this in terms of a template.
            the weighted sum of pixels forms a template image which is
            matched with every image. this will also face difficulty in
            overcoming the challenges discussed in section 1 as single
            template is difficult to design for all the different cases.

   i hope this gives some intuition into the challenges faced by
   approaches other than deep learning. please note that more
   sophisticated techniques can be used than the ones discussed above but
   they would rarely beat a deep learning model.


3. review of neural networks fundamentals

   let   s discuss some properties of a neural networks. i will skip the
   basics of neural networks here as i have already covered that in my
   previous article     [108]fundamentals of deep learning     starting with
   neural networks.

   once your fundamentals are sorted, let   s learn in detail some important
   concepts such as id180, id174, initializing
   weights and dropouts.


id180

   there are various id180 which can be used and this is an
   active area of research. let   s discuss some of the popular options:
    1. sigmoid function
          + equation:   (x) = 1/(1+e^-x)
            [109]sigmoid act
          + sigmoid activation, also used in id28
            regression, squashes the input space from (-inf,inf) to (0,1)
          + but it has various problems and it is almost never used in
            id98s:
              1. saturated neurons kill the gradient
                    # if you observe the above graph carefully, if the
                      input is beyond -5 or 5, the output will be very
                      close to 0 and 1 respectively. also, in this region
                      the gradients are almost zero. notice that the
                      tangents in this region will be almost parallel to
                      x-axis thus ~0 slope.
                    # as we know that gradients get multiplied in
                      back-propogation, so this small gradient will
                      virtually stop back-propogation into further layers,
                      thus killing the gradient.
              2. outputs are not zero-centered
                    # as you can see that all the outputs are between 0
                      and 1. as these become inputs to the next layer, all
                      the gradients of the next layer will be either
                      positive or negative. so the path to optimum will be
                      zig-zag. i will skip the mathematics here. please
                      refer the stanford class referred above for details.
              3. taking the exp() is computationally expensive
                    # though not a big drawback, it has a slight negative
                      impact
    2. tanh activation
          + it is simply the hyperbolic tangent function with form:
            [110]tanh act
          + it is always preferred over sigmoid because it solved problem
            #2, i.e. the outputs are in range (-1,1).
          + but it will still result in killing the gradient and thus not
            recommended choice.
    3.  relu (rectified linear unit)
          + equation: f(x) = max( 0 , x )
            [111]relu actv
          + it is the most commonly used activation function for id98s. it
            has following advantages:
               o gradient won   t saturate in the positive region
               o computationally very efficient as simple thresholding is
                 required
               o empirically found to converge faster than sigmoid or
                 tanh.
          + but still it has the following disadvantages:
               o output is not zero-centered and always positive
               o gradient is killed for x<0. few techniques like leaky
                 relu and parametric relu are used to overcome this and i
                 encourage you to find these
               o gradient is not defined at x=0. but this can be easily
                 catered using sub-gradients and posts less practical
                 challenges as x=0 is generally a rare case

   to summarize, relu is mostly the activation function of choice. if the
   caveats are kept in mind, these can be used very efficiently.


id174

   for images, generally the following preprocessing steps are done:
    1. same size images: all images are converted to the same size and
       generally in square shape.
    2. mean centering: for each pixel, its mean value among all images can
       be subtracted from each pixel. sometimes (but rarely) mean
       centering along red, green and blue channels can also be done

   note that id172 is generally not done in images.


weight initialization

   there can be various techniques for initializing weights. lets consider
   a few of them:
    1. all zeros
          + this is generally a bad idea because in this case all the
            neuron will generate the same output initially and similar
            gradients would flow back in back-propagation
          + the results are generally undesirable as network won   t train
            properly.
    2. gaussian random variables
          + the weights can be initialized with random gaussian
            distribution of 0 mean and small standard deviation (0.1 to
            1e-5)
          + this works for shallow networks, i.e. ~5 hidden layers but not
            for deep networks
          + in case of deep networks, the small weights make the outputs
            small and as you move towards the end, the values become even
            smaller. thus the gradients will also become small resulting
            in gradient killing at the end.
          + note that you need to play with the standard deviation of the
            gaussian distribution which works well for your network.
    3. xavier initialization
          + it suggests that variance of the gaussian distribution of
            weights for each neuron should depend on the number of inputs
            to the layer.
          + the recommended variance is square root of inputs. so the
            numpy code for initializing the weights of layer with n inputs
            is: np.random.randn(n_in, n_out)*sqrt(1/n_in)
          + a recent research suggested that for relu neurons, the
            recommended update is: np.random.randn(n_in,
            n_out)*sqrt(2/n_in). read this [112]blog post for more
            details.

   one more thing must be remembered while using relu as activation
   function. it is that the weights initialization might be such that some
   of the neurons might not get activated because of negative input. this
   is something that should be checked. you might be surprised to know
   that 10-20% of the relus might be dead at a particular time while
   training and even in the end.

   these were just some of the concepts i discussed here. some more
   concepts can be of importance like batch id172, stochastic
   id119, dropouts which i encourage you to read on your own.


4. introduction to convolution neural networks

   before going into the details, lets first try to get some intuition
   into why deep networks work better.

   as we learned from the drawbacks of earlier approaches, they are unable
   to cater to the vast amount of variations in images. deep id98s work by
   consecutively modeling small pieces of information and combining them
   deeper in network.

   one way to understand them is that the first layer will try to detect
   edges and form templates for edge detection. then subsequent layers
   will try to combine them into simpler shapes and eventually into
   templates of different object positions, illumination, scales, etc. the
   final layers will match an input image with all the templates and the
   final prediction is like a weighted sum of all of them. so, deep id98s
   are able to model complex variations and behaviour giving highly
   accurate predictions.

   there is an interesting paper on visualization of deep features in id98s
   which you can go through to get more intuition     [113]understanding
   neural networks through deep visualization.

   for the purpose of explaining id98s and finally showing an example, i
   will be using the cifar-10 dataset for explanation here and you can
   download the data set from [114]here. this dataset has 60,000 images
   with 10 labels and 6,000 images of each type. each image is colored and
   32  32 in size.

   a id98 typically consists of 3 types of layers:
    1. convolution layer
    2. pooling layer
    3. fully connected layer

   you might find some batch id172 layers in some old id98s but
   they are not used these days. we   ll consider these one by one.


convolution layer

   since convolution layers form the crux of the network, i   ll consider
   them first. each layer can be visualized in the form of a block or a
   cuboid. for instance in the case of cifar-10 data, the input layer
   would have the following form:

   [115]fig-1

   here you can see, this is the original image which is 32  32 in height
   and width. the depth here is 3 which corresponds to the red, green and
   blue colors, which form the basis of colored images. now a convolution
   layer is formed by running a filter over it. a filter is another block
   or cuboid of smaller height and width but same depth which is swept
   over this base block. let   s consider a filter of size 5x5x3.

   [116]fig_2

   we start this filter from the top left corner and sweep it till the
   bottom left corner. this filter is nothing but a set of eights, i.e.
   5x5x3=75 + 1 bias = 76 weights. at each position, the weighted sum of
   the pixels is calculated as w^tx + b and a new value is obtained. a
   single filter will result in a volume of size 28x28x1 as shown above.

   note that multiple filters are generally run at each step. therefore,
   if 10 filters are used, the output would look like:

   [117]fig-3

   here the filter weights are parameters which are learned during the
   back-propagation step. you might have noticed that we got a 28  28 block
   as output when the input was 32  32. why so? let   s look at a simpler
   case.

   suppose the initial image had size 6x6xd and the filter has size 3x3xd.
   here i   ve kept the depth as d because it can be anything and it   s
   immaterial as it remains the same in both. since depth is same, we can
   have a look at the front view of how filter would work:

   [118]fig-4

   here we can see that the result would be 4x4x1 volume block. notice
   there is a single output for entire depth of the each location of
   filter. but you need not do this visualization all the time. let   s
   define a generic case where image has dimension nxnxd and filter has
   fxfxd. also, lets define another term stride (s) here which is the
   number of cells (in above matrix) to move in each step. in the above
   case, we had a stride of 1 but it can be a higher value as well. so the
   size of the output will be:

   output size = (n     f)/s + 1

   you can validate the first case where n=32, f=5, s=1. the output had 28
   pixels which is what we get from this formula as well. please note that
   some s values might result in non-integer result and we generally don   t
   use such values.

   let   s consider an example to consolidate our understanding. starting
   with the same image as before of size 32  32, we need to apply 2 filters
   consecutively, first 10 filters of size 7, stride 1 and next 6 filters
   of size 5, stride 2. before looking at the solution below, just think
   about 2 things:
    1. what should be the depth of each filter?
    2. what will the resulting size of the images in each step.

   here is the answer:

   [119]fig-5


   notice here that the size of the images is getting shrunk
   consecutively. this will be undesirable in case of deep networks where
   the size would become very small too early. also, it would restrict the
   use of large size filters as they would result in faster size
   reduction.

   to prevent this, we generally use a stride of 1 along with zero-padding
   of size (f-1)/2. zero-padding is nothing but adding additional
   zero-value pixels towards the border of the image.

   consider the example we saw above with 6  6 image and 3  3 filter. the
   required padding is (3-1)/2=1. we can visualize the padding as:

   [120]fig-6

   here you can see that the image now becomes 8  8 because of padding of 1
   on each side. so now the output will be of size 6  6 same as the
   original image.

   now let   s summarize a convolution layer as following:
     * input size: w[1] x h[1] x d[1]
     * hyper-parameters:
          + k: #filters
          + f: filter size (fxf)
          + s: stride
          + p: amount of padding
     * output size: w[2] x h[2] x d[2]
          + w[21]
          + h[21]
          + d[2 ]
     * #parameters = (f.f.d).k + k
          + f.f.d : number of parameters for each filter (analogous to
            volume of the cuboid)
          + (f.f.d).k : volume of each filter multiplied by the number of
            filters
          + +k: adding k parameters for the bias term

   some additional points to be taken into consideration:
     * k should be set as powers of 2 for computational efficiency
     * f is generally taken as odd number
     * f=1 might sometimes be used and it makes sense because there is a
       depth component involved
     * filters might be called kernels sometimes

   having understood the convolution layer, lets move on to pooling layer.


pooling layer

   when we use padding in convolution layer, the image size remains same.
   so, pooling layers are used to reduce the size of image. they work by
   sampling in each layer using filters. consider the following 4  4 layer.
   so if we use a 2  2 filter with stride 2 and max-pooling, we get the
   following response:

   [121]fig-7

   here you can see that 4 2  2 matrix are combined into 1 and their
   maximum value is taken. generally, max-pooling is used but other
   options like average pooling can be considered.


fully connected layer

   at the end of convolution and pooling layers, networks generally use
   fully-connected layers in which each pixel is considered as a separate
   neuron just like a regular neural network. the last fully-connected
   layer will contain as many neurons as the number of classes to be
   predicted. for instance, in cifar-10 case, the last fully-connected
   layer will have 10 neurons.


5. case study: alexnet

   i recommend reading the prior section multiple times and getting a hang
   of the concepts before moving forward.

   in this section, i will discuss the alexnet architecture in detail. to
   give you some background, alexnet is the winning solution of
   [122]id163 challenge 2012. this is one of the most reputed computer
   vision challenge and 2012 was the first time that a deep learning
   network was used for solving this problem.

   also, this resulted in a significantly better result as compared to
   previous solutions. i will share the network architecture here and
   review all the concepts learned above.

   the detailed solution has been explained in this [123]paper. i will
   explain the overall architecture of the network here. the alexnet
   consists of a 11 layer id98 with the following architecture:

   [124]fig-8

   here you can see 11 layers between input and output. lets discuss each
   one of them individually. note that the output of each layer will be
   the input of next layer. so you should keep that in mind.
     * layer 0: input image
          + size: 227 x 227 x 3
          + note that in the paper referenced above, the network diagram
            has 224x224x3 printed which appears to be a typo.
     * layer 1: convolution with 96 filters, size 11  11, stride 4, padding
       0
          + size: 55 x 55 x 96
          + (227-11)/4 + 1 = 55 is the size of the outcome
          + 96 depth because 1 set denotes 1 filter and there are 96
            filters
     * layer 2: max-pooling with 3  3 filter, stride 2
          + size: 27 x 27 x 96
          + (55     3)/2 + 1 = 27 is size of outcome
          + depth is same as before, i.e. 96 because pooling is done
            independently on each layer
     * layer 3: convolution with 256 filters, size 5  5, stride 1, padding
       2
          + size: 27 x 27 x 256
          + because of padding of (5-1)/2=2, the original size is restored
          + 256 depth because of 256 filters
     * layer 4: max-pooling with 3  3 filter, stride 2
          + size: 13 x 13 x 256
          + (27     3)/2 + 1 = 13 is size of outcome
          + depth is same as before, i.e. 256 because pooling is done
            independently on each layer
     * layer 5: convolution with 384 filters, size 3  3, stride 1, padding
       1
          + size: 13 x 13 x 384
          + because of padding of (3-1)/2=1, the original size is restored
          + 384 depth because of 384 filters
     * layer 6: convolution with 384 filters, size 3  3, stride 1, padding
       1
          + size: 13 x 13 x 384
          + because of padding of (3-1)/2=1, the original size is restored
          + 384 depth because of 384 filters
     * layer 7: convolution with 256 filters, size 3  3, stride 1, padding
       1
          + size: 13 x 13 x 256
          + because of padding of (3-1)/2=1, the original size is restored
          + 256 depth because of 256 filters
     * layer 8: max-pooling with 3  3 filter, stride 2
          + size: 6 x 6 x 256
          + (13     3)/2 + 1 = 6 is size of outcome
          + depth is same as before, i.e. 256 because pooling is done
            independently on each layer
     * layer 9: fully connected with 4096 neuron
          + in this later, each of the 6x6x256=9216 pixels are fed into
            each of the 4096 neurons and weights determined by
            back-propagation.
     * layer 10: fully connected with 4096 neuron
          + similar to layer #9
     * layer 11: fully connected with 1000 neurons
          + this is the last layer and has 1000 neurons because id163
            data has 1000 classes to be predicted.

   i understand this is a complicated structure but once you understand
   the layers, it   ll give you a much better understanding of the
   architecture. note that you fill find a different representation of the
   structure if you look at the alexnet paper. this is because at that
   gpus were not very powerful and they used 2 gpus for training the
   network. so the work processing was divided between the two.

   i highly encourage you to go through the other advanced solutions of
   id163 challenges after 2012 to get more ideas of how people design
   these networks. some of interesting solutions are:
     * [125]zfnet: winner of 2013 challenge
     * [126]googlenet: winner of 2014 challenge
     * [127]vggnet: a good solution from 2014 challenge
     * [128]resnet: winner of 2015 challenge designed by microsoft
       research team

   [129]this video gives a brief overview and comparison of these
   solutions towards the end.


6. implementing id98s using graphlab

   having understood the theoretical concepts, lets move on to the fun
   part (practical) and make a basic id98 on the cifar-10 dataset which
   we   ve downloaded before.

   i   ll be using graphlab for the purpose of running algorithms. instead
   of graphlab, you are free to use alternatives tools such as torch,
   theano, keras, caffe, tensorflow, etc. but graphlab allows a quick and
   dirty implementation as it takes care of the weights initializations
   and network architecture on its own.

   we   ll work on the cifar-10 dataset which you can
   download from [130]here. the first step is to load the data. this data
   is packed in a specific format which can be loaded using the following
   code:
import pandas as pd
import numpy as np
import cpickle

#define a function to load each batch as dictionary:
def unpickle(file):
    fo = open(file, 'rb')
    dict = cpickle.load(fo)
    fo.close()
    return dict

#make dictionaries by calling the above function:
batch1 = unpickle('data/data_batch_1')
batch2 = unpickle('data/data_batch_2')
batch3 = unpickle('data/data_batch_3')
batch4 = unpickle('data/data_batch_4')
batch5 = unpickle('data/data_batch_5')
batch_test = unpickle('data/test_batch')

#define a function to convert this dictionary into dataframe with image pixel ar
ray and labels:
def get_dataframe(batch):
    df = pd.dataframe(batch['data'])
    df['image'] = df.as_matrix().tolist()
    df.drop(range(3072),axis=1,inplace=true)
    df['label'] = batch['labels']
    return df

#define train and test files:
train = pd.concat([get_dataframe(batch1),get_dataframe(batch2),get_dataframe(bat
ch3),get_dataframe(batch4),get_dataframe(batch5)],ignore_index=true)
test = get_dataframe(batch_test)

   we can verify this data by looking at the head and shape of data as
   follow:
print train.head()

   [131]1. train head
print train.shape, test.shape

   [132]2. train test shape

   since we   ll be using graphlab, the next step is to convert this into a
   graphlab sframe and run neural network. let   s convert the data first:
import graphlab as gl
gltrain = gl.sframe(train)
gltest = gl.sframe(test)

   graphlab has a functionality of automatically creating a neural network
   based on the data. lets run that as a baseline model before going into
   an advanced model.
model = gl.neuralnet_classifier.create(gltrain, target='label', validation_set=n
one)

   [133]3. model1

   here it used a simple fully connected network with 2 hidden layers and
   10 neurons each. let   s evaluate this model on test data.
model.evaluate(gltest)

   [134]4. model1 test evaluate

   as you can see that we have a pretty low accuracy of ~15%. this is
   because it is a very fundamental network. lets try to make a id98 now.
   but if we go about training a deep id98 from scratch, we will face the
   following challenges:
    1. the available data is very less to capture all the required
       features
    2. training deep id98s generally requires a gpu as a cpu is not
       powerful enough to perform the required calculations. thus we won   t
       be able to run it on our system. we can probably rent an amazom aws
       instance.

   to overcome these challenges, we can use pre-trained networks. these
   are nothing but networks like alexnet which are pre-trained on many
   images and the weights for deep layers have been determined. the only
   challenge is to find a pre-trianed network which has been trained on
   images similar to the one we want to train. if the pre-trained network
   is not made on images of similar domain, then the features will not
   exactly make sense and classifier will not be of higher accuracy.

   before proceeding further, we need to convert these images into the
   size used in id163 which we   re using for classification. the
   graphlab model is based on 256  256 size images. so we need to convert
   our images to that size. lets do it using the following code:
#convert pixels to graphlab image format
gltrain['glimage'] = gl.sarray(gltrain['image']).pixel_array_to_image(32, 32, 3,
 allow_rounding = true)
gltest['glimage'] = gl.sarray(gltest['image']).pixel_array_to_image(32, 32, 3, a
llow_rounding = true)
#remove the original column
gltrain.remove_column('image')
gltest.remove_column('image')
gltrain.head()

   [135]5. train image orig

   here we can see that a new column of type graphlab image has been
   created but the images are in 32  32 size. so we convert them to 256  256
   using following code:
#convert into 256x256 size
gltrain['image'] = gl.image_analysis.resize(gltrain['glimage'], 256, 256, 3)
gltest['image'] = gl.image_analysis.resize(gltest['glimage'], 256, 256, 3)
#remove old column:
gltrain.remove_column('glimage')
gltest.remove_column('glimage')
gltrain.head()

   [136]6. train image conv

   now we can see that the image has been converted into the desired size.
   next, we will load the id163 pre-trained model in graphlab and
   use the features created in its last layer into a simple classifier and
   make predictions.

   lets start by loading the pre-trained model.
#load the pre-trained model:
pretrained_model = gl.load_model('http://s3.amazonaws.com/graphlab-datasets/deep
learning/id163_model_iter45')

   now we have to use this model and extract features which will be passed
   into a classifier. note that the following operations may take a lot of
   computing time. i use a macbook pro 15    and i had to leave it for whole
   night!
gltrain['features'] = pretrained_model.extract_features(gltrain)
gltest['features'] = pretrained_model.extract_features(gltest)

   lets have a look at the data to make sure we have the features:
gltrain.head()

   [137]7. dtrain head

   though, we have the features with us, notice here that lot of them are
   zeros. you can understand this as a result of smaller data set.
   id163 was created on 1.2mn images. so there would be many features
   in those images that don   t make sense for this data, thus resulting in
   zero outcome.

   now lets create a classifier using graphlab. the advantage with
      classifier    function is that it will automatically create various
   classifiers and chose the best model.
simple_classifier = graphlab.classifier.create(gltrain, features = ['features'],
 target = 'label')

   the various outputs are:
    1. boosted trees classifier
       [138]8. boosted o:p
    2. id79 classifier
       [139]9. rf o:p
    3. decision tree classifier
       [140]10. dec tree op
    4. id28 classifier
       [141]11. log ref op

   the final model selection is based on a validation set with 5% of the
   data. the results are:

   [142]12. final selection

   so we can see that boosted trees classifier has been chosen as the
   final model. let   s look at the results on test data:
simple_classifier.evaluate(gltest)

   [143]13. test result

   so we can see that the test accuracy is now ~50%. it   s a decent jump
   from 15% to 50% but there is still huge potential to do better. the
   idea here was to get you started and i will skip the next steps. here
   are some things which you can try:
    1. remove the redundant features in the data
    2. perform hyper-parameter tuning in models
    3. search for pre-trained models which are trained on images similar
       to this dataset

   you can find many open-source solutions for this dataset which give
   >95% accuracy. you should check those out. please feel free to try them
   and post your solutions in comments below.


projects

   now, its time to take the plunge and actually play with some other real
   datasets. so are you ready to take on the challenge? accelerate your
   deep learning journey with the following practice problems:
   [144]practice problem: identify the apparels identify the type of
   apparel for given images
   [145]practice problem: identify the digits identify the digit in given
   images

end notes

   in this article, we covered the basics of id161 using deep
   convolution neural networks (id98s). we started by appreciating the
   challenges involved in designing artificial systems which mimic the
   eye. then, we looked at some of the traditional techniques, prior to
   deep learning, and got some intuition into their drawbacks.

   we moved on to understanding the some aspects of tuning a neural
   networks such as id180, weights initialization and
   data-preprocessing. next, we got some intuition into why deep id98s
   should work better than traditional approaches and we understood
   the different elements present in a general deep id98.

   subsequently, we consolidated our understanding by analyzing the
   architecture of alexnet, the winning solution of id163 2012
   challenge. finally, we took the cifar-10 data and implemented a id98 on
   it using a pre-trained alexnet deep network.

   i hope you liked this article. did you find this article useful ?
   please feel free to share your feedback through comments below. and to
   gain expertise in working in neural network try out the deep learning
   practice problem     [146]identify the digits.

you can test your skills and knowledge. check out [147]live competitions and
compete with best data scientists from all over the world.

   you can also read this article on analytics vidhya's android app
   [148]get it on google play

share this:

     * [149]click to share on linkedin (opens in new window)
     * [150]click to share on facebook (opens in new window)
     * [151]click to share on twitter (opens in new window)
     * [152]click to share on pocket (opens in new window)
     * [153]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [154]activation function, [155]artificial intelligence,
   [156]id158s, [157]batch id172,
   [158]id161, [159]convolution layer, [160]convolution neural
   networks, [161]deep learning, [162]fully connected layer,
   [163]graphlab, [164]image recognition, [165]machine learning,
   [166]neural networks, [167]object recognition, [168]pooling layer,
   [169]stochastic gradient, [170]weight initialization
   next article

gartner business intelligence, analytics and information management summit
2016, 7-8 june, mumbai, india

   previous article

new case study for analytics interviews: dawn of taxi aggregators

[171]aarshay jain

   aarshay is a ml enthusiast, pursuing ms in data science at columbia
   university, graduating in dec 2017. he is currently exploring the
   various ml techniques and writes articles for av to share his knowledge
   with the community.
     *
     *
     *
     *

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [172]discussion portal to get your queries resolved

38 comments

     * phil wilson says:
       [173]april 4, 2016 at 7:14 am
       thanks it   s a great article. i just had one doubt i.e. in the
       example where you are applying two consecutive convolutional
       filters first 10 filters of size 7, stride 1 and next 6 filters of
       size 5, stride 2 on the image of size 32x32x3 the diagram shows
       that after the first set of filters, the size of activation map
       should be 26x26x10 instead of 25x25x10. is there a mistake in my
       understanding?
       [174]reply
          + aarshay jain says:
            [175]april 4, 2016 at 2:55 pm
            hi phil,
            thanks for reaching out. yes you   re correct. i guess there is
            some mix-up with graphics guys. i   ll fix it.     
            [176]reply
     * [177]ranjeet singh says:
       [178]april 4, 2016 at 8:09 am
       can u help us building a gender classifier??
       [179]reply
          + aarshay jain says:
            [180]april 4, 2016 at 2:56 pm
            i   m myself pretty new to this. but i can try to help you guys.
            please feel free to connect with me at [181][email protected]
            [182]reply
     * nico says:
       [183]april 4, 2016 at 10:43 am
       hi aarshay! great article, thank you for posting!
       where could i find a few pre trained id98s for me to try out a few
       on my data?
       thank you!
       [184]reply
          + aarshay jain says:
            [185]april 4, 2016 at 2:58 pm
            hi nico,
            different packages provide pre-trained networks. for instance
            in this article i have used an alexnet pre-trained network
            available in graphlab. if you   re working on a vision problem,
            caffe is a very good option. you   ll find pre defined
            pre-trained networks there.
            [186]reply
     * srk says:
       [187]april 4, 2016 at 5:53 pm
       excellent one aarshay. thank you.!
       [188]reply
          + aarshay jain says:
            [189]april 4, 2016 at 5:59 pm
            thanks srk     
            [190]reply
     * abhishek singh rathore says:
       [191]april 5, 2016 at 2:42 am
       amazing article; thanks     
       [192]reply
          + aarshay jain says:
            [193]april 5, 2016 at 7:26 am
            glad you liked it     
            [194]reply
     * panovr says:
       [195]april 5, 2016 at 9:21 am
       you wrote:    this filter is nothing but a set of eights, i.e.
       5x5x3=75 + 1 bias = 76 weights. at each position, the weighted sum
       of the pixels is calculated as wtx + b and a new value is obtained.
          
       if the filter is 5x5x3, for each pixel, one can get three values
       for this pixel   s red, green and blue. what is w and x? and how can
       these three values to become one?
       thanks!
       [196]reply
          + aarshay jain says:
            [197]april 5, 2016 at 10:06 am
            hi panovr,
            the way it works is that the same filter coefficients w are
            used to slide over the entire image space. so w is just an
            expanded version of the 3 pixel value sets. if filter is 5x5x3
            then x would be [ 25 entires for red .. 25 entries for green
            .. 25 entries for blue .. 1 bias term ] and w would be [ 25
            coefficients for red .. 25 coefficients for green .. 25
            coefficients for blue .. 1 coefficients for bias term ]. this
            way it becomes an array and we can take a sum-product to get a
            single value.
            i hope this makes sense. please feel free to discuss further.
            [198]reply
               o panovr says:
                 [199]april 5, 2016 at 10:40 am
                 hi aarshay, as my understanding, if the filter is 5x5x3,
                 for red channel of the pixel, the weight is w1, and for
                 green channel of the pixel, the weight is w2, and w3 for
                 the blue channel.
                 one can create x1 for the red channel of the pixel and
                 its 24 neighbors, and x2, x3 etc.
                 so, for red channel, v1 = w1^t * x1 + b1, and v2, v3 etc.
                 (after transpose, w1 is a row vector and x1 is a column
                 vector)
                 thus we will get three values v1, v2 and v3 after that
                 computation, however, the depth of the result is 1 in the
                 figure. i just wonder this.
                 [200]reply
                    # aarshay jain says:
                      [201]april 5, 2016 at 10:53 am
                      you understanding for individual rgb channels aligns
                      with mine. but the v1, v2 and v3 you are mentioning
                      are not used as separate values but combined (added)
                      into 1. the depth component in resultant layers
                      comes from the fact that we use multiple filters of
                      same kind but different weights. this is the way i
                      understand id98s.
                      can you refer some literature where your approach is
                      being followed? i am basing my statement as per the
                      learnings from the stanford class i mentioned above.
                      [202]reply
                         @ panovr says:
                           [203]april 5, 2016 at 11:14 am
                           i have not reference other literature, and just
                           want to understand your article.
                           thanks for writing this!
                           [204]reply
                              - aarshay jain says:
                                [205]april 5, 2016 at 11:17 am
                                in that case, i would say would say go
                                with my understanding unless you find
                                otherwise. you can refer to the stanford
                                class for more details     
                                [206]reply
     * vivek ruhela says:
       [207]april 5, 2016 at 12:30 pm
       thanks for this article. can you explain the layer 9 in the example
       shown here. why 4096 is chosen and how to calculate layer 9
       weights.? thanks again.
       [208]reply
          + aarshay jain says:
            [209]april 5, 2016 at 12:33 pm
            i   m not sure why they chose 4096. there is not clear reason
            mentioned in their research article. they typically take
            powers of 2 and i suppose this is a hyper-parameter so they
            would have tested different values.
            [210]reply
     * shan says:
       [211]april 5, 2016 at 6:11 pm
       nice and informative. thanks.
       [212]reply
          + aarshay jain says:
            [213]april 5, 2016 at 6:52 pm
                
            [214]reply
     * ayush rai says:
       [215]april 7, 2016 at 12:09 pm
       hi aarshay its a great article. i have a query that in some places
       i find there is a dropout layers after fc6 layers i.e after fully
       connected layer. what is the use of having a dropout layer?
       thanks
       [216]reply
          + aarshay jain says:
            [217]april 7, 2016 at 12:44 pm
            dropout layers can be understood analogous to subsampling.
            randomly some neurons are removed from the model in different
            ff and bp runs. intuitively, it helps enhancing the predictive
            power of each neuron because now the network should give
            reasonable results even when some of the neurons are removed.
            hope this makes some sense atleast.
            [218]reply
               o ayush rai says:
                 [219]april 7, 2016 at 3:15 pm
                 yeah i understand it now. also by removing neurons in
                 various forward and backward propagation runs also
                 reduces the risk of overfitting. thus these layers act as
                 regularisers.
                 [220]reply
                    # aarshay jain says:
                      [221]april 7, 2016 at 3:25 pm
                      exactly..
                      [222]reply
     * lenka vrana says:
       [223]april 7, 2016 at 12:21 pm
       hi. the theoretic part was great, so i wanted to try your code as
       well. however i get this error when i try to load the cifar10 files
       with unpickle function:    ascii    codec can   t decode byte 0x8b in
       position 6: ordinal not in range(128).
       i   m new to python, so i would appreciate your help.
       [224]reply
          + aarshay jain says:
            [225]april 7, 2016 at 12:47 pm
            which version of python are you using? 2.7 or 3.4? have you
            tried searching for this error online?
            [226]reply
               o lenka vrana says:
                 [227]april 7, 2016 at 1:01 pm
                 version 3.5.1. i tried to google it, but no results were
                 connected to the    pickled    documents.
                 [228]reply
                    # aarshay jain says:
                      [229]april 7, 2016 at 1:08 pm
                      i used 2.7. i took this code from the cifar-10
                      webpage mentioned in the article. probably this code
                      is designed for 2.7. you have search for similar
                      code for 3.5. you might want to check whether the
                         pickled    package has been transported to the 3.5
                      environment and what changes are there in code.
                      [230]reply
                         @ lenka says:
                           [231]april 7, 2016 at 1:54 pm
                           i   ll try. thank you.
                           [232]reply
     * waqas ullah khan says:
       [233]april 8, 2016 at 3:10 am
       nice work sir aarshay jain. keep it up.
       [234]reply
          + aarshay jain says:
            [235]april 8, 2016 at 4:02 am
            thank you waqas     
            [236]reply
     * praveen gupta sanka says:
       [237]april 12, 2016 at 1:13 pm
       hi aarshay,
       a generic question     which is the best module/library available to
       develop neural network model in python.
       [238]reply
          + aarshay jain says:
            [239]april 12, 2016 at 1:47 pm
            there are various tools available which serve slightly
            different purposes. i wouldn   t say there is one best solution
            but you should use the one which suits your case.
            you can search for caffe, torch, theano, tensor flow, keras,
            lasagne.
            [240]reply
     * yona says:
       [241]may 8, 2016 at 2:45 pm
       hi aarshay
       good article.
       do you happen to know some thing on how to detect face with id98?
       [242]reply
          + aarshay jain says:
            [243]may 9, 2016 at 5:15 am
            thanks yona.. i   m sorry i haven   t worked on facial recognition
            but i   m sure you   ll find ample resources online.     
            [244]reply
     * poornachandra sandur says:
       [245]june 11, 2016 at 3:01 pm
       hi aarshay,
       very informative article. aarshay , i found in many of the video
       lectures on deep learning, in which the python program
       automatically downloads the mnist or cifar etc datasets , processes
       them and gives the required results , but how to prepare the data
       which are in the form of images for processing them in the id98   s .
       i request you to enlighten us with this concept on how to prepare
       the data sets for processing in convolutional neural networks .
       please do the needful.
       [246]reply
     * anuj priyadarshi says:
       [247]may 16, 2018 at 12:26 pm
       hi, i am new to machine learning, can anyone advise me on how to
       start and with ml.
       i currently know pandas, numpy, and have seen machine learning
       introduction on coursera.
       [248]reply
          + aishwarya singh says:
            [249]may 16, 2018 at 5:41 pm
            hi anuj,
            you can go through this article :    [250]the ultimate learning
            path to becoming a data scientist in 2018   
            [251]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [252]srk       3924
   2    [2.jpg?date=2019-04-05] [253]mark12    3510
   3    [3.jpg?date=2019-04-05] [254]nilabha   3261
   4    [4.jpg?date=2019-04-05] [255]nitish007 3237
   5    [5.jpg?date=2019-04-05] [256]tezdhar   3082
   [257]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [258]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [259]understanding support vector machine algorithm from examples
       (along with code)
     * [260]essentials of machine learning algorithms (with python and r
       codes)
     * [261]a complete tutorial to learn data science with python from
       scratch
     * [262]7 types of regression techniques you should know!
     * [263]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [264]a simple introduction to anova (with applications in excel)
     * [265]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [266]top 5 machine learning github repositories and reddit discussions
   from march 2019

[267]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [268]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[269]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [270]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[271]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [272]16 opencv functions to start your id161 journey (with
   python code)

[273]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [274][ds-finhack.jpg]

   [275][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [276]about us
     * [277]our team
     * [278]career
     * [279]contact us
     * [280]write for us

   [281]about us
   [282]   
   [283]our team
   [284]   
   [285]careers
   [286]   
   [287]contact us

data scientists

     * [288]blog
     * [289]hackathon
     * [290]discussions
     * [291]apply jobs
     * [292]leaderboard

companies

     * [293]post jobs
     * [294]trainings
     * [295]hiring hackathons
     * [296]advertising
     * [297]reach us

   don't have an account? [298]sign up here.

join our community :

   [299]46336 [300]followers
   [301]20224 [302]followers
   [303]followers
   [304]7513 [305]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [306]privacy policy
     * [307]terms of use
     * [308]refund policy

   don't have an account? [309]sign up here

   iframe: [310]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [311](button) join now

   subscribe!

   iframe: [312]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [313](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/deep-learning/
  94. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
  95. https://www.analyticsvidhya.com/blog/category/deep-learning/
  96. https://www.analyticsvidhya.com/blog/category/machine-learning/
  97. https://www.analyticsvidhya.com/blog/category/python-2/
  98. https://www.analyticsvidhya.com/blog/author/aarshay/
  99. http://cs231n.stanford.edu/syllabus.html
 100. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
 101. http://blog.algorithmia.com/introduction-to-deep-learning-2016/
 102. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/cat_poses.jpg
 103. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/cat_illum.jpg
 104. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/hidden_dog.jpg
 105. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/background_clutter.jpg
 106. https://computervisionblog.wordpress.com/2013/06/01/cats-and-vision-is-vision-acquired-or-innate/
 107. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/dog_location.png
 108. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
 109. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/sigmoid-act.png
 110. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/tanh-act.png
 111. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/relu-actv.png
 112. http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization
 113. http://yosinski.com/media/papers/yosinski__2015__icml_dl__understanding_neural_networks_through_deep_visualization__.pdf
 114. https://www.cs.toronto.edu/~kriz/cifar.html
 115. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/fig-1.png
 116. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/fig_2.png
 117. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/fig-3.png
 118. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/fig-4.png
 119. https://www.analyticsvidhya.com/wp-content/uploads/2016/04/fig-5.png
 120. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/fig-6.png
 121. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/fig-7.png
 122. http://www.image-net.org/challenges/lsvrc/2012/
 123. http://www.cs.toronto.edu/~fritz/absps/id163.pdf
 124. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/fig-8.png
 125. http://arxiv.org/abs/1311.2901
 126. http://arxiv.org/abs/1409.4842
 127. http://www.robots.ox.ac.uk/~vgg/research/very_deep/
 128. http://arxiv.org/abs/1512.03385
 129. https://www.youtube.com/watch?v=lxfughug-iq&feature=youtu.be
 130. https://www.cs.toronto.edu/~kriz/cifar.html
 131. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/1.-train-head.png
 132. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/2.-train-test-shape.png
 133. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/3.-model1.png
 134. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/4.-model1-test-evaluate.png
 135. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/5.-train-image-orig.png
 136. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/6.-train-image-conv.png
 137. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/7.-dtrain-head.png
 138. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/8.-boosted-op.png
 139. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/9.-rf-op.png
 140. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/10.-dec-tree-op.png
 141. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/11.-log-ref-op.png
 142. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/12.-final-selection.png
 143. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/13.-test-result.png
 144. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-apparels/?utm_source=deep-learning-computer-vision-introduction-convolution-neural-networks&utm_medium=blog
 145. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/?utm_source=deep-learning-computer-vision-introduction-convolution-neural-networks&utm_medium=blog
 146. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/
 147. http://datahack.analyticsvidhya.com/contest/all
 148. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 149. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/?share=linkedin
 150. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/?share=facebook
 151. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/?share=twitter
 152. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/?share=pocket
 153. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/?share=reddit
 154. https://www.analyticsvidhya.com/blog/tag/activation-function/
 155. https://www.analyticsvidhya.com/blog/tag/artificial-intelligence/
 156. https://www.analyticsvidhya.com/blog/tag/artificial-neural-networks/
 157. https://www.analyticsvidhya.com/blog/tag/batch-id172/
 158. https://www.analyticsvidhya.com/blog/tag/computer-vision/
 159. https://www.analyticsvidhya.com/blog/tag/convolution-layer/
 160. https://www.analyticsvidhya.com/blog/tag/convolution-neural-networks/
 161. https://www.analyticsvidhya.com/blog/tag/deep-learning/
 162. https://www.analyticsvidhya.com/blog/tag/fully-connected-layer/
 163. https://www.analyticsvidhya.com/blog/tag/graphlab/
 164. https://www.analyticsvidhya.com/blog/tag/image-recognition/
 165. https://www.analyticsvidhya.com/blog/tag/machine-learning/
 166. https://www.analyticsvidhya.com/blog/tag/neural-networks/
 167. https://www.analyticsvidhya.com/blog/tag/object-recognition/
 168. https://www.analyticsvidhya.com/blog/tag/pooling-layer/
 169. https://www.analyticsvidhya.com/blog/tag/stochastic-gradient/
 170. https://www.analyticsvidhya.com/blog/tag/weight-initialization/
 171. https://www.analyticsvidhya.com/blog/author/aarshay/
 172. https://discuss.analyticsvidhya.com/
 173. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108913
 174. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108913
 175. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108939
 176. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108939
 177. http://l.p.u. id161 team/
 178. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108915
 179. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108915
 180. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108940
 181. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection#cdacacbfbea5acb4a7aca4a38daaa0aca4a1e3aea2a0
 182. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108940
 183. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108922
 184. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108922
 185. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108941
 186. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108941
 187. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108944
 188. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108944
 189. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108945
 190. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108945
 191. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108953
 192. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108953
 193. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108961
 194. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108961
 195. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108964
 196. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108964
 197. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108971
 198. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108971
 199. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108976
 200. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108976
 201. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108977
 202. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108977
 203. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108980
 204. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108980
 205. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108981
 206. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108981
 207. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108988
 208. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108988
 209. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108989
 210. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-108989
 211. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109014
 212. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109014
 213. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109016
 214. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109016
 215. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109113
 216. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109113
 217. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109117
 218. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109117
 219. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109133
 220. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109133
 221. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109135
 222. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109135
 223. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109115
 224. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109115
 225. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109118
 226. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109118
 227. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109119
 228. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109119
 229. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109121
 230. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109121
 231. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109126
 232. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109126
 233. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109161
 234. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109161
 235. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109163
 236. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109163
 237. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109355
 238. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109355
 239. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109358
 240. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-109358
 241. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-110624
 242. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-110624
 243. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-110662
 244. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-110662
 245. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-112120
 246. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-112120
 247. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-153306
 248. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-153306
 249. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-153314
 250. https://www.analyticsvidhya.com/blog/2018/01/ultimate-learning-path-becoming-data-scientist-2018/
 251. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/#comment-153314
 252. https://datahack.analyticsvidhya.com/user/profile/srk
 253. https://datahack.analyticsvidhya.com/user/profile/mark12
 254. https://datahack.analyticsvidhya.com/user/profile/nilabha
 255. https://datahack.analyticsvidhya.com/user/profile/nitish007
 256. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 257. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 258. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 259. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 260. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 261. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 262. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 263. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 264. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 265. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 266. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 267. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 268. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 269. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 270. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 271. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 272. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 273. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 274. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 275. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 276. http://www.analyticsvidhya.com/about-me/
 277. https://www.analyticsvidhya.com/about-me/team/
 278. https://www.analyticsvidhya.com/career-analytics-vidhya/
 279. https://www.analyticsvidhya.com/contact/
 280. https://www.analyticsvidhya.com/about-me/write/
 281. http://www.analyticsvidhya.com/about-me/
 282. https://www.analyticsvidhya.com/about-me/team/
 283. https://www.analyticsvidhya.com/about-me/team/
 284. https://www.analyticsvidhya.com/about-me/team/
 285. https://www.analyticsvidhya.com/career-analytics-vidhya/
 286. https://www.analyticsvidhya.com/about-me/team/
 287. https://www.analyticsvidhya.com/contact/
 288. https://www.analyticsvidhya.com/blog
 289. https://datahack.analyticsvidhya.com/
 290. https://discuss.analyticsvidhya.com/
 291. https://www.analyticsvidhya.com/jobs/
 292. https://datahack.analyticsvidhya.com/users/
 293. https://www.analyticsvidhya.com/corporate/
 294. https://trainings.analyticsvidhya.com/
 295. https://datahack.analyticsvidhya.com/
 296. https://www.analyticsvidhya.com/contact/
 297. https://www.analyticsvidhya.com/contact/
 298. https://datahack.analyticsvidhya.com/signup/
 299. https://www.facebook.com/analyticsvidhya/
 300. https://www.facebook.com/analyticsvidhya/
 301. https://twitter.com/analyticsvidhya
 302. https://twitter.com/analyticsvidhya
 303. https://plus.google.com/+analyticsvidhya
 304. https://in.linkedin.com/company/analytics-vidhya
 305. https://in.linkedin.com/company/analytics-vidhya
 306. https://www.analyticsvidhya.com/privacy-policy/
 307. https://www.analyticsvidhya.com/terms/
 308. https://www.analyticsvidhya.com/refund-policy/
 309. https://id.analyticsvidhya.com/accounts/signup/
 310. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 311. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 312. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 313. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 315. https://www.facebook.com/analyticsvidhya
 316. https://twitter.com/analyticsvidhya
 317. https://plus.google.com/+analyticsvidhya/posts
 318. https://in.linkedin.com/company/analytics-vidhya
 319. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-apparels/?utm_source=deep-learning-computer-vision-introduction-convolution-neural-networks&utm_medium=blog
 320. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/?utm_source=deep-learning-computer-vision-introduction-convolution-neural-networks&utm_medium=blog
 321. https://www.analyticsvidhya.com/blog/2016/04/gartner-business-intelligence/
 322. https://www.analyticsvidhya.com/blog/2016/04/case-study-analytics-interviews-dawn-taxi-aggregators/
 323. https://www.analyticsvidhya.com/blog/author/aarshay/
 324. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection#50313122233831293a31393e10373d31393c7e333f3d
 325. https://in.linkedin.com/in/aarshayjain
 326. https://github.com/aarshayj
 327. https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/aarshay
 328. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 329. https://www.facebook.com/analyticsvidhya/
 330. https://twitter.com/analyticsvidhya
 331. https://plus.google.com/+analyticsvidhya
 332. https://plus.google.com/+analyticsvidhya
 333. https://in.linkedin.com/company/analytics-vidhya
 334. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 335. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 336. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 337. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 338. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 339. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 340. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 341. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 342. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 343. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 344. javascript:void(0);
 345. javascript:void(0);
 346. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 347. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 348. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 349. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 350. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 351. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 352. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 353. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 354. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 355. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f04%2fdeep-learning-computer-vision-introduction-convolution-neural-networks%2f&linkname=deep%20learning%20for%20computer%20vision%20-%20introduction%20to%20convolution%20neural%20networks
 356. javascript:void(0);
 357. javascript:void(0);
