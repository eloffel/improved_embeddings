   #[1]the data science blog    feed [2]the data science blog    comments
   feed [3]the data science blog    an intuitive explanation of
   convolutional neural networks comments feed [4]a quick introduction to
   neural networks [5]alternate [6]alternate [7]the data science blog
   [8]wordpress.com

   [9]skip to content

[10]the data science blog

machine learning, deep learning, nlp, data science

   (button) menu
     * [11]about
     * [12]the data science blog
     * [13]reading material
     * [14]contact me

an intuitive explanation of convolutional neural networks

   posted on [15]august 11, 2016may 29, 2017 by [16]ujjwalkarn

what are convolutional neural networks and why are they important?

   convolutional neural networks (convnets or id98s) are a category of
   [17]neural networks that have proven very effective in areas such
   as image recognition and classification. convnets have been successful
   in identifying faces, objects and traffic signs apart from powering
   vision in robots and self driving cars.

   screen shot 2017-05-28 at 11.41.55 pm.png

figure 1: source [[18]1]

   in figure 1 above, a convnet is able to recognize scenes and the system
   is able to suggest relevant captions (   a soccer player is kicking a
   soccer ball   ) while figure 2 shows an example of convnets being used
   for recognizing everyday objects, humans and animals. lately,
   convnets have been effective in several natural language processing
   tasks (such as sentence classification) as well.

   screen shot 2016-08-07 at 4.17.11 pm.png

figure 2: source [[19]2]

   convnets, therefore, are an important tool for most machine learning
   practitioners today. however, understanding convnets and learning to
   use them for the first time can sometimes be an intimidating
   experience. the primary purpose of this blog post is to develop an
   understanding of how convolutional neural networks work on images.

   if you are new to neural networks in general, i would recommend reading
   [20]this short tutorial on multi layer id88s to get an idea about
   how they work, before proceeding. multi layer id88s are referred
   to as    fully connected layers    in this post.

the lenet architecture (1990s)

   lenet was one of the very first convolutional neural networks
   which helped propel the field of deep learning. this pioneering work by
   yann lecun was named [21]lenet5 after many previous successful
   iterations since the year 1988 [[22]3]. at that time the lenet
   architecture was used mainly for character recognition tasks such as
   reading zip codes, digits, etc.

   below, we will develop an intuition of how the lenet architecture
   learns to recognize images. there have been several new architectures
   proposed in the recent years which are improvements over the lenet, but
   they all use the main concepts from the lenet and are relatively easier
   to understand if you have a clear understanding of the former.

   screen shot 2016-08-07 at 4.59.29 pm.png

figure 3: a simple convnet. source [[23]5]

   the convolutional neural network in figure 3 is similar in architecture
   to the original lenet and classifies an input image into
   four categories: dog, cat, boat or bird (the original lenet was used
   mainly for character recognition tasks). as evident from the figure
   above, on receiving a boat image as input, the network correctly
   assigns the highest id203 for boat (0.94) among all
   four categories. the sum of all probabilities in the output layer
   should be one (explained later in this post).

   there are four main operations in the convnet shown in figure 3 above:
    1. convolution
    2. non linearity (relu)
    3. pooling or sub sampling
    4. classification (fully connected layer)

   these operations are the basic building blocks of every convolutional
   neural network, so understanding how these work is an important step to
   developing a sound understanding of convnets. we will try to understand
   the intuition behind each of these operations below.

an image is a matrix of pixel values

   essentially, every image can be represented as a matrix of pixel
   values.

   8-gif.gif

figure 4: every image is a matrix of pixel values. source [[24]6]

   [25]channel is a conventional term used to refer to a certain component
   of an image. an image from a standard digital camera will have three
   channels     red, green and blue     you can imagine those as
   three 2d-matrices stacked over each other (one for each color), each
   having pixel values in the range 0 to 255.

   a [26]grayscale image, on the other hand, has just one channel. for the
   purpose of this post, we will only consider grayscale images, so we
   will have a single 2d matrix representing an image. the value of each
   pixel in the matrix will range from 0 to 255     zero indicating
   black and 255 indicating white.

the convolution step

   convnets derive their name from the [27]   convolution    operator. the
   primary purpose of convolution in case of a convnet is to extract
   features from the input image. convolution preserves the spatial
   relationship between pixels by learning image features using small
   squares of input data. we will not go into the mathematical details of
   convolution here, but will try to understand how it works over images.

   as we discussed above, every image can be considered as a matrix of
   pixel values. consider a 5 x 5 image whose pixel values are only 0 and
   1 (note that for a grayscale image, pixel values range from 0 to 255,
   the green matrix below is a special case where pixel values are only 0
   and 1):

   screen shot 2016-07-24 at 11.25.13 pm

   also, consider another 3 x 3 matrix as shown below:

   screen shot 2016-07-24 at 11.25.24 pm

   then, the convolution of the 5 x 5 image and the 3 x 3 matrix can be
   computed as shown in the animation in figure 5 below:
   convolution_schematic

figure 5: the convolution operation. the output matrix is called convolved
feature or feature map. source [[28]7]

   take a moment to understand how the computation above is being done.
   we slide the orange matrix over our original image (green) by 1
   pixel (also called    stride   ) and for every position, we compute element
   wise multiplication (between the two matrices) and add the
   multiplication outputs to get the final integer which forms a single
   element of the output matrix (pink). note that the 3  3 matrix    sees   
   only a part of the input image in each stride.

   in id98 terminology, the 3  3 matrix is called a    filter    or    kernel    or
      feature detector    and the matrix formed by sliding the filter over the
   image and computing the dot product is called the    convolved feature   
   or    activation map    or the    feature map   . it is important to note that
   filters acts as feature detectors from the original input image.

   it is evident from the animation above that different values of the
   filter matrix will produce different feature maps for the same input
   image. as an example, consider the following input image:

   111.png

   in the table below, we can see the effects of convolution of the above
   image with different filters. as shown, we can perform operations such
   as edge detection, sharpen and blur just by changing the numeric values
   of our filter matrix before the convolution operation [[29]8]     this
   means that different filters can detect different features from an
   image, for example edges, curves etc. more such examples are available
   in section 8.2.4 [30]here.

   screen shot 2016-08-05 at 11.03.00 pm.png

   another good way to understand the convolution operation is by looking
   at the animation in figure 6 below:

   giphy.gif

figure 6: the convolution operation. source [[31]9]

   a filter (with red outline) slides over the input image (convolution
   operation) to produce a feature map. the convolution of another filter
   (with the green outline), over the same image gives a different feature
   map as shown. it is important to note that the convolution operation
   captures the local dependencies in the original image. also notice how
   these two different filters generate different feature maps from the
   same original image. remember that the image and the two filters above
   are just numeric matrices as we have discussed above.

   in practice, a id98 learns the values of these filters on its own during
   the training process (although we still need to specify parameters such
   as number of filters, filter size, architecture of the network etc.
   before the training process). the more number of filters we have, the
   more image features get extracted and the better our network becomes at
   recognizing patterns in unseen images.

   the size of the feature map (convolved feature) is controlled by three
   parameters [[32]4] that we need to decide before the convolution step
   is performed:
     * depth: depth corresponds to the number of filters we use for the
       convolution operation. in the network shown in figure 7, we
       are performing convolution of the original boat image using
       three distinct filters, thus producing three different feature maps
       as shown. you can think of these three feature maps as stacked 2d
       matrices, so, the    depth    of the feature map would be three.

   screen shot 2016-08-10 at 3.42.35 am

figure 7

     * stride: stride is the number of pixels by which we slide our filter
       matrix over the input matrix. when the stride is 1 then we move the
       filters one pixel at a time. when the stride is 2, then the filters
       jump 2 pixels at a time as we slide them around. having a larger
       stride will produce smaller feature maps.

     * zero-padding: sometimes, it is convenient to pad the input
       matrix with zeros around the border, so that we can apply the
       filter to bordering elements of our input image matrix. a nice
       feature of zero padding is that it allows us to control the size of
       the feature maps. adding zero-padding is also called wide
       convolution, and not using zero-padding would be a narrow
       convolution. this has been explained clearly in [[33]14].

introducing non linearity (relu)

   an additional operation called relu has been used after every
   convolution operation in figure 3 above. relu stands for rectified
   linear unit and is a non-linear operation. its output is given by:

   screen shot 2016-08-10 at 2.23.48 am.png

figure 8: the relu operation

   relu is an element wise operation (applied per pixel) and replaces all
   negative pixel values in the feature map by zero. the purpose of relu
   is to introduce non-linearity in our convnet, since most of the
   real-world data we would want our convnet to learn would be non-linear
   (convolution is a linear operation     element wise id127
   and addition, so we account for non-linearity by introducing a
   non-linear function like relu).

   the relu operation can be understood clearly from figure 9 below. it
   shows the relu operation applied to one of the feature maps obtained in
   figure 6 above. the output feature map here is also referred to as the
      rectified    feature map.

   screen shot 2016-08-07 at 6.18.19 pm.png

figure 9: relu operation. source [[34]10]

   other non linear functions such as tanh or sigmoid can also be used
   instead of relu, but relu has been found to perform better in most
   situations.

the pooling step

   spatial pooling (also called subsampling or downsampling) reduces the
   dimensionality of each feature map but retains the most
   important information. spatial pooling can be of different types: max,
   average, sum etc.

   in case of max pooling, we define a spatial neighborhood (for example,
   a 2  2 window) and take the largest element from the rectified feature
   map within that window. instead of taking the largest element we could
   also take the average (average pooling) or sum of all elements in that
   window. in practice, max pooling has been shown to work better.

   figure 10 shows an example of max pooling operation on a
   rectified feature map (obtained after convolution + relu operation) by
   using a 2  2 window.

   screen shot 2016-08-10 at 3.38.39 am.png

figure 10: max pooling. source [[35]4]

   we slide our 2 x 2 window by 2 cells (also called    stride   ) and take
   the maximum value in each region. as shown in figure 10, this reduces
   the dimensionality of our feature map.

   in the network shown in figure 11, pooling operation is applied
   separately to each feature map (notice that, due to this, we
   get three output maps from three input maps).

   screen shot 2016-08-07 at 6.19.37 pm.png

figure 11: pooling applied to rectified feature maps

   figure 12 shows the effect of pooling on the rectified feature map we
   received after the relu operation in figure 9 above.

   screen shot 2016-08-07 at 6.11.53 pm.png

figure 12: pooling. source [[36]10]

   the function of pooling is to progressively reduce the spatial size of
   the input representation [[37]4]. in particular, pooling
     * makes the input representations (feature dimension) smaller and
       more manageable
     * reduces the number of parameters and computations in the network,
       therefore, controlling [38]overfitting [[39]4]
     * makes the network invariant to small transformations, distortions
       and translations in the input image (a small distortion in input
       will not change the output of pooling     since we take the maximum /
       average value in a local neighborhood).
     * helps us arrive at an almost scale invariant representation of our
       image (the exact term is    equivariant   ). this is very powerful
       since we can detect objects in an image no matter where they are
       located (read [[40]18] and [[41]19] for details).

story so far

   screen shot 2016-08-08 at 2.26.09 am.png

figure 13

   so far we have seen how convolution, relu and pooling work. it is
   important to understand that these layers are the basic building blocks
   of any id98. as shown in figure 13, we have two sets of convolution,
   relu & pooling layers     the 2nd convolution layer performs
   convolution on the output of the first pooling layer using six filters
   to produce a total of six feature maps. relu is then applied
   individually on all of these six feature maps. we then perform max
   pooling operation separately on each of the six rectified feature maps.

   together these layers extract the useful features from the
   images, introduce non-linearity in our network and reduce feature
   dimension while aiming to make the features somewhat equivariant to
   scale and translation [[42]18].

   the output of the 2nd pooling layer acts as an input to the fully
   connected layer, which we will discuss in the next section.

fully connected layer

   the fully connected layer is a traditional multi layer id88 that
   uses a softmax activation function in the output layer (other
   classifiers like id166 can also be used, but will stick to softmax in
   this post). the term    fully connected    implies that every neuron in the
   previous layer is connected to every neuron on the next layer. i
   recommend [43]reading this post if you are unfamiliar with multi layer
   id88s.

   the output from the convolutional and pooling layers represent
   high-level features of the input image. the purpose of the fully
   connected layer is to use these features for classifying the input
   image into various classes based on the training dataset. for example,
   the image classification task we set out to perform has four possible
   outputs as shown in figure 14 below (note that figure 14 does not show
   connections between the nodes in the fully connected layer)

   screen shot 2016-08-06 at 12.34.02 am.png

figure 14: fully connected layer -each node is connected to every other node
in the adjacent layer

   apart from classification, adding a fully-connected layer is also a
   (usually) cheap way of learning non-linear combinations of these
   features. most of the features from convolutional and pooling layers
   may be good for the classification task, but combinations of those
   features might be even better [[44]11].

   the sum of output probabilities from the fully connected layer is 1.
   this is ensured by using the [45]softmax as the activation function in
   the output layer of the fully connected layer. the softmax
   function takes a vector of arbitrary real-valued scores and squashes it
   to a vector of values between zero and one that sum to one.

putting it all together     training using id26

   as discussed above, the convolution + pooling layers act as feature
   extractors from the input image while fully connected layer acts as a
   classifier.

   note that in figure 15 below, since the input image is a boat, the
   target id203 is 1 for boat class and 0 for other three classes,
   i.e.
     * input image = boat
     * target vector = [0, 0, 1, 0]

   screen shot 2016-08-07 at 9.15.21 pm.png

figure 15: training the convnet

   the overall training process of the convolution network may be
   summarized as below:
     * step1: we initialize all filters and parameters / weights with
       random values

     * step2: the network takes a training image as input, goes through
       the forward propagation step (convolution, relu and pooling
       operations along with forward propagation in the fully connected
       layer) and finds the output probabilities for each class.
          + lets say the output probabilities for the boat image above
            are [0.2, 0.4, 0.1, 0.3]
          + since weights are randomly assigned for the first training
            example, output probabilities are also random.

     * step3: calculate the total error at the output layer (summation
       over all 4 classes)
          +  total error =         (target id203     output
            id203)   

     * step4: use id26 to calculate the gradients of the error
       with respect to all weights in the network and use id119
       to update all filter values / weights and parameter values to
       minimize the output error.
          + the weights are adjusted in proportion to their contribution
            to the total error.
          + when the same image is input again, output probabilities might
            now be [0.1, 0.1, 0.7, 0.1], which is closer to the target
            vector [0, 0, 1, 0].
          + this means that the network has learnt to classify this
            particular image correctly by adjusting its weights / filters
            such that the output error is reduced.
          + parameters like number of filters, filter sizes, architecture
            of the network etc. have all been fixed before step 1 and do
            not change during training process     only the values of the
            filter matrix and connection weights get updated.

     * step5: repeat steps 2-4 with all images in the training set.

   the above steps train the convnet     this essentially means that all the
   weights and parameters of the convnet have now been optimized to
   correctly classify images from the training set.

   when a new (unseen) image is input into the convnet, the network would
   go through the forward propagation step and output a id203 for
   each class (for a new image, the output probabilities are calculated
   using the weights which have been optimized to correctly classify all
   the previous training examples). if our training set is large enough,
   the network will (hopefully) generalize well to new images and classify
   them into correct categories.

   note 1: the steps above have been oversimplified and mathematical
   details have been avoided to provide intuition into the training
   process. see [[46]4] and [[47]12] for a mathematical formulation and
   thorough understanding.

   note 2: in the example above we used two sets of alternating
   convolution and pooling layers. please note however, that these
   operations can be repeated any number of times in a single convnet. in
   fact, some of the best performing convnets today have tens of
   convolution and pooling layers! also, it is not necessary to have a
   pooling layer after every convolutional layer. as can be seen in the
   figure 16 below, we can have multiple convolution + relu operations in
   succession before having a pooling operation. also notice how each
   layer of the convnet is visualized in the figure 16 below.

   car.png

figure 16: source [[48]4]

visualizing convolutional neural networks

   in general, the more convolution steps we have, the more complicated
   features our network will be able to learn to recognize. for example,
   in image classification a convnet may learn to detect edges from raw
   pixels in the first layer, then use the edges to detect simple shapes
   in the second layer, and then use these shapes to deter higher-level
   features, such as facial shapes in higher layers [[49]14]. this is
   demonstrated in figure 17 below     these features were learnt using
   a [50]convolutional deep belief network and the figure is included here
   just for demonstrating the idea (this is only an example: real life
   convolution filters may detect objects that have no meaning to humans).

   screen shot 2016-08-10 at 12.58.30 pm.png

figure 17: learned features from a convolutional deep belief network. source
[[51]21]

   [52]adam harley created amazing visualizations of a convolutional
   neural network trained on the mnist database of handwritten digits
   [[53]13]. i highly recommend [54]playing around with it to understand
   details of how a id98 works.

   we will see below how the network works for an input    8   . note that the
   visualization in figure 18 does not show the relu operation separately.

   conv_all.png

figure 18: visualizing a convnet trained on handwritten digits. source
[[55]13]

   the input image contains 1024 pixels (32 x 32 image) and the first
   convolution layer (convolution layer 1) is formed by convolution of six
   unique 5    5 (stride 1) filters with the input image. as seen, using
   six different filters produces a feature map of depth six.

   convolutional layer 1 is followed by pooling layer 1 that does 2    2
   max pooling (with stride 2) separately over the six feature maps in
   convolution layer 1. you can move your mouse pointer over any pixel in
   the pooling layer and observe the 2 x 2 grid it forms in the previous
   convolution layer (demonstrated in figure 19). you   ll notice that the
   pixel having the maximum value (the brightest one) in the 2 x 2 grid
   makes it to the pooling layer.

   screen shot 2016-08-06 at 12.45.35 pm.png

figure 19: visualizing the pooling operation. source [[56]13]

   pooling layer 1 is followed by sixteen 5    5 (stride 1) convolutional
   filters that perform the convolution operation. this is followed
   by pooling layer 2 that does 2    2 max pooling (with stride 2). these
   two layers use the same concepts as described above.

   we then have three fully-connected (fc) layers. there are:
     * 120 neurons in the first fc layer
     * 100 neurons in the second fc layer
     * 10 neurons in the third fc layer corresponding to the 10 digits    
       also called the output layer

   notice how in figure 20, each of the 10 nodes in the output layer are
   connected to all 100 nodes in the 2nd fully connected layer (hence the
   name fully connected).

   also, note how the only bright node in the output layer corresponds to
      8        this means that the network correctly classifies our handwritten
   digit (brighter node denotes that the output from it is higher, i.e. 8
   has the highest id203 among all other digits).

   final.png

figure 20: visualizing the filly connected layers. source [[57]13]

   the 3d version of the same visualization is available [58]here.

other convnet architectures

   convolutional neural networks have been around since early 1990s. we
   discussed the lenet above which was one of the very first convolutional
   neural networks. some other influential architectures are listed below
   [[59]3] [[60]4].
     * lenet (1990s): already covered in this article.

     * 1990s to 2012: in the years from late 1990s to early 2010s
       convolutional neural network were in incubation. as more and more
       data and computing power became available, tasks that convolutional
       neural networks could tackle became more and more interesting.

     * alexnet (2012)     in 2012, alex krizhevsky (and others) released
       [61]alexnet which was a deeper and much wider version of the lenet
       and won by a large margin the difficult id163 large scale visual
       recognition challenge (ilsvrc) in 2012. it was a significant
       breakthrough with respect to the previous approaches and
       the current widespread application of id98s can be attributed to
       this work.

     * zf net (2013)     the ilsvrc 2013 winner was a convolutional network
       from matthew zeiler and rob fergus. it became known as the
       [62]zfnet (short for zeiler & fergus net). it was an improvement on
       alexnet by tweaking the architecture hyperparameters.

     * googlenet (2014)     the ilsvrc 2014 winner was a convolutional
       network from [63]szegedy et al. from google. its main contribution
       was the development of an inception module that dramatically
       reduced the number of parameters in the network (4m, compared to
       alexnet with 60m).

     * vggnet (2014)     the runner-up in ilsvrc 2014 was the network that
       became known as the [64]vggnet. its main contribution was in
       showing that the depth of the network (number of layers) is a
       critical component for good performance.

     * resnets (2015)     [65]residual network developed by kaiming he (and
       others) was the winner of ilsvrc 2015. resnets are currently by far
       state of the art convolutional neural network models and are the
       default choice for using convnets in practice (as of may 2016).

     * densenet (august 2016)     recently published by gao huang (and
       others), the [66]densely connected convolutional network has each
       layer directly connected to every other layer in a feed-forward
       fashion. the densenet has been shown to obtain significant
       improvements over previous state-of-the-art architectures on five
       highly competitive object recognition benchmark tasks. check out
       the torch implementation [67]here.

conclusion

   in this post, i have tried to explain the main concepts
   behind convolutional neural networks in simple terms. there are several
   details i have oversimplified / skipped, but hopefully this post gave
   you some intuition around how they work.

   this post was originally inspired from [68]understanding convolutional
   neural networks for nlp by denny britz (which i would recommend
   reading) and a number of explanations here are based on that post. for
   a more thorough understanding of some of these concepts, i would
   encourage you to go through the [69]notes from [70]stanford   s course on
   convnets as well as other excellent resources mentioned
   under references below. if you face any issues understanding any of the
   above concepts or have questions / suggestions, feel free to leave a
   comment below.

   all images and animations used in this post belong to their respective
   authors as listed in references section below.

references

    1. [71]karpathy/neuraltalk2: efficient image captioning code in torch,
       [72]examples
    2. shaoqing ren, et al,    faster r-id98: towards real-time object
       detection with region proposal networks   ,
       2015, [73]arxiv:1506.01497
    3. [74]neural network architectures, eugenio culurciello   s blog
    4. [75]cs231n convolutional neural networks for visual recognition,
       stanford
    5. [76]clarifai / technology
    6. [77]machine learning is fun! part 3: deep learning and
       convolutional neural networks
    7. [78]feature extraction using convolution, stanford
    8. [79]wikipedia article on kernel (image processing)
    9. [80]deep learning methods for vision, cvpr 2012 tutorial
   10. [81]neural networks by rob fergus, machine learning summer school
       2015
   11. [82]what do the fully connected layers do in id98s?
   12. [83]convolutional neural networks, andrew gibiansky
   13. a. w. harley,    an interactive node-link visualization of
       convolutional neural networks,    in isvc, pages 867-877, 2015
       ([84]link). [85]demo
   14. [86]understanding convolutional neural networks for nlp
   15. [87]id26 in convolutional neural networks
   16. [88]a beginner   s guide to understanding convolutional neural
       networks
   17. vincent dumoulin, et al,    a guide to convolution arithmetic for
       deep learning   , 2015, [89]arxiv:1603.07285
   18. [90]what is the difference between deep learning and usual machine
       learning?
   19. [91]how is a convolutional neural network able to learn invariant
       features?
   20. [92]a taxonomy of deep convolutional neural nets for computer
       vision
   21. honglak lee, et al,    convolutional id50 for
       scalable unsupervised learning of hierarchical representations   
       ([93]link)

share this:

     * [94]tweet
     *
     *
     *

       iframe:
       [95]https://www.reddit.com/static/button/button1.html?newwindow=tru
       e&width=120&url=https%3a%2f%2fujjwalkarn.me%2f2016%2f08%2f11%2fintu
       itive-explanation-convnets%2f&title=an%20intuitive%20explanation%20
       of%20convolutional%20neural%20networks

     *

related

   posted in [96]deep learning

post navigation

   [97]a quick introduction to neural networks
     __________________________________________________________________

171 thoughts on    an intuitive explanation of convolutional neural networks   

comment navigation

   [98]    older comments
    1.
   hannah says:
       [99]january 30, 2019 at 9:54 am
       this is best article that helped me understand id98. reading on
       google tensor flow page, i felt very confused about id98. i am so
       glad that i read this article.
       [100]likelike
       [101]reply
    2.
   e.praneeth says:
       [102]february 11, 2019 at 12:41 pm
       everything explained from scratch. one of the best site i came
       across. thank you, author, for writing this.
       [103]likelike
       [104]reply
    3.
   [105]sacha schutz (@dridk) says:
       [106]february 19, 2019 at 4:02 am
       in the handwritten digit example, i don   t understand how the second
       convolution layer is connected.
       in first layer, you apply 6 filters to one picture. but in the
       second layer, you apply 16 filters to different regions of
       differents features images. i cannot understand how it   s computed.
       does all output images are combined and then filter is applied ?
       [107]likelike
       [108]reply
    4.
   [109]insightfulexplanations says:
       [110]march 5, 2019 at 12:35 am
       i admire such articles. these explanations motivated me also to
       write in a clear way [111]https://mathintuitions.blogspot.com/.
       [112]likelike
       [113]reply
    5.
   [114]david smith says:
       [115]march 8, 2019 at 5:25 pm
       it was very exciting how convnets build from pixels to numbers then
       recognize the image. thank you for your explanation.
       [116]likelike
       [117]reply
    6.
   [118]david rama says:
       [119]march 31, 2019 at 3:22 pm
       nice write up ujuwal! there   s been a few more conv net
       infrastructures since then but this article is still very relevant.
       [120]likelike
       [121]reply

comment navigation

   [122]    older comments

leave a reply [123]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [124]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [125]log out /
   [126]change )
   google photo

   you are commenting using your google account. ( [127]log out /
   [128]change )
   twitter picture

   you are commenting using your twitter account. ( [129]log out /
   [130]change )
   facebook photo

   you are commenting using your facebook account. ( [131]log out /
   [132]change )
   [133]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

connect

     * [134]view thedatascienceblog   s profile on facebook
     * [135]view ujwlkarn   s profile on twitter
     * [136]view ujjwalkarn   s profile on linkedin
     * [137]view ujjwalkarn   s profile on github

categories

     * [138]data science
     * [139]deep learning
     * [140]machine learning
     * [141]python
     * [142]r language

recent posts

     * [143]an intuitive explanation of convolutional neural networks
     * [144]a quick introduction to neural networks
     * [145]introducing xda: r package for exploratory data analysis
     * [146]curated list of r tutorials for data science
     * [147]common operations on pandas dataframe

follow me on twitter

   [148]my tweets

[149]like facebook page for updates

     [150]like facebook page for updates

follow blog via email

   join 531 other followers

   ____________________

   (button) subscribe

blog view count

     * 807,375 hits

search this blog

   search for: ____________________ search
   [151]create a website or blog at wordpress.com


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [152]cancel reblog post

references

   visible links
   1. https://ujjwalkarn.me/feed/
   2. https://ujjwalkarn.me/comments/feed/
   3. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/feed/
   4. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
   5. https://public-api.wordpress.com/oembed/?format=json&url=https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/&for=wpcom-auto-discovery
   6. https://public-api.wordpress.com/oembed/?format=xml&url=https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/&for=wpcom-auto-discovery
   7. https://ujjwalkarn.me/osd.xml
   8. https://s1.wp.com/opensearch.xml
   9. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/#content
  10. https://ujjwalkarn.me/
  11. https://ujjwalkarn.me/
  12. https://ujjwalkarn.me/blog/
  13. https://ujjwalkarn.me/blogroll/
  14. https://ujjwalkarn.me/contact-me/
  15. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
  16. https://ujjwalkarn.me/author/ujwlkarn/
  17. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
  18. http://cs.stanford.edu/people/karpathy/neuraltalk2/demo.html
  19. https://arxiv.org/pdf/1506.01497v3.pdf
  20. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
  21. http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf
  22. https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba
  23. https://www.clarifai.com/technology
  24. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721
  25. https://en.wikipedia.org/wiki/channel_(digital_image)
  26. https://en.wikipedia.org/wiki/grayscale
  27. http://en.wikipedia.org/wiki/convolution
  28. http://deeplearning.stanford.edu/wiki/index.php/feature_extraction_using_convolution
  29. https://en.wikipedia.org/wiki/kernel_(image_processing)
  30. http://docs.gimp.org/en/plug-in-convmatrix.html
  31. http://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12/
  32. http://cs231n.github.io/convolutional-networks/
  33. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
  34. http://mlss.tuebingen.mpg.de/2015/slides/fergus/fergus_1.pdf
  35. http://cs231n.github.io/convolutional-networks/
  36. http://mlss.tuebingen.mpg.de/2015/slides/fergus/fergus_1.pdf
  37. http://cs231n.github.io/convolutional-networks/
  38. https://en.wikipedia.org/wiki/overfitting
  39. http://cs231n.github.io/convolutional-networks/
  40. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md
  41. https://www.quora.com/how-is-a-convolutional-neural-network-able-to-learn-invariant-features
  42. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md
  43. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
  44. https://stats.stackexchange.com/questions/182102/what-do-the-fully-connected-layers-do-in-id98s/182122#182122
  45. http://cs231n.github.io/linear-classify/#softmax
  46. http://cs231n.github.io/convolutional-networks/
  47. http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/
  48. http://cs231n.github.io/convolutional-networks/
  49. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
  50. http://web.eecs.umich.edu/~honglak/icml09-convolutionaldeepbeliefnetworks.pdf
  51. http://web.eecs.umich.edu/~honglak/icml09-convolutionaldeepbeliefnetworks.pdf
  52. http://scs.ryerson.ca/~aharley/
  53. http://scs.ryerson.ca/~aharley/vis/harley_vis_isvc15.pdf
  54. http://scs.ryerson.ca/~aharley/vis/conv/flat.html
  55. http://scs.ryerson.ca/~aharley/vis/conv/flat.html
  56. http://scs.ryerson.ca/~aharley/vis/conv/flat.html
  57. http://scs.ryerson.ca/~aharley/vis/conv/flat.html
  58. http://scs.ryerson.ca/~aharley/vis/conv/
  59. https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba
  60. http://cs231n.github.io/convolutional-networks/
  61. https://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks.pdf
  62. http://arxiv.org/abs/1311.2901
  63. http://arxiv.org/abs/1409.4842
  64. http://www.robots.ox.ac.uk/~vgg/research/very_deep/
  65. http://arxiv.org/abs/1512.03385
  66. http://arxiv.org/abs/1608.06993
  67. https://github.com/liuzhuang13/densenet
  68. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
  69. http://cs231n.github.io/
  70. http://cs231n.stanford.edu/
  71. https://github.com/karpathy/neuraltalk2
  72. http://cs.stanford.edu/people/karpathy/neuraltalk2/demo.html
  73. http://arxiv.org/pdf/1506.01497v3.pdf
  74. https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba
  75. http://cs231n.github.io/convolutional-networks/
  76. https://www.clarifai.com/technology
  77. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.2gfx5zcw3
  78. http://deeplearning.stanford.edu/wiki/index.php/feature_extraction_using_convolution
  79. https://en.wikipedia.org/wiki/kernel_(image_processing)
  80. http://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12
  81. http://mlss.tuebingen.mpg.de/2015/slides/fergus/fergus_1.pdf
  82. http://stats.stackexchange.com/a/182122/53914
  83. http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/
  84. http://scs.ryerson.ca/~aharley/vis/harley_vis_isvc15.pdf
  85. http://scs.ryerson.ca/~aharley/vis/conv/flat.html
  86. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
  87. http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/
  88. https://adeshpande3.github.io/adeshpande3.github.io/a-beginner's-guide-to-understanding-convolutional-neural-networks-part-2/
  89. http://arxiv.org/pdf/1603.07285v1.pdf
  90. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md
  91. https://www.quora.com/how-is-a-convolutional-neural-network-able-to-learn-invariant-features
  92. http://journal.frontiersin.org/article/10.3389/frobt.2015.00036/full
  93. http://web.eecs.umich.edu/~honglak/icml09-convolutionaldeepbeliefnetworks.pdf
  94. https://twitter.com/share
  95. https://www.reddit.com/static/button/button1.html?newwindow=true&width=120&url=https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/&title=an intuitive explanation of convolutional neural networks
  96. https://ujjwalkarn.me/category/deep-learning/
  97. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
  98. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/comment-page-3/#comments
  99. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/comment-page-4/#comment-8755
 100. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/?like_comment=8755&_wpnonce=0067499200
 101. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/?replytocom=8755#respond
 102. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/comment-page-4/#comment-11203
 103. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/?like_comment=11203&_wpnonce=69c2de4d2f
 104. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/?replytocom=11203#respond
 105. http://twitter.com/dridk
 106. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/comment-page-4/#comment-12973
 107. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/?like_comment=12973&_wpnonce=e623317c8c
 108. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/?replytocom=12973#respond
 109. http://insightfulexplanations.wordpress.com/
 110. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/comment-page-4/#comment-15017
 111. https://mathintuitions.blogspot.com/
 112. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/?like_comment=15017&_wpnonce=0127965d85
 113. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/?replytocom=15017#respond
 114. https://www.answerminer.com/
 115. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/comment-page-4/#comment-15501
 116. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/?like_comment=15501&_wpnonce=a2c36be378
 117. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/?replytocom=15501#respond
 118. https://www.nsemba.com/
 119. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/comment-page-4/#comment-19593
 120. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/?like_comment=19593&_wpnonce=37e756b052
 121. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/?replytocom=19593#respond
 122. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/comment-page-3/#comments
 123. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/#respond
 124. https://gravatar.com/site/signup/
 125. javascript:highlandercomments.doexternallogout( 'wordpress' );
 126. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
 127. javascript:highlandercomments.doexternallogout( 'googleplus' );
 128. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
 129. javascript:highlandercomments.doexternallogout( 'twitter' );
 130. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
 131. javascript:highlandercomments.doexternallogout( 'facebook' );
 132. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
 133. javascript:highlandercomments.cancelexternalwindow();
 134. https://www.facebook.com/thedatascienceblog/
 135. https://twitter.com/ujwlkarn/
 136. https://www.linkedin.com/in/ujjwalkarn/
 137. https://github.com/ujjwalkarn/
 138. https://ujjwalkarn.me/category/data-science/
 139. https://ujjwalkarn.me/category/deep-learning/
 140. https://ujjwalkarn.me/category/machine-learning/
 141. https://ujjwalkarn.me/category/python/
 142. https://ujjwalkarn.me/category/r-language/
 143. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
 144. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
 145. https://ujjwalkarn.me/2016/06/17/introducing-xda-r-package-for-exploratory-data-analysis/
 146. https://ujjwalkarn.me/2016/06/04/curated-list-of-r-tutorials-for-data-science/
 147. https://ujjwalkarn.me/2016/05/30/common-operations-on-pandas-dataframe/
 148. https://twitter.com/ujwlkarn
 149. https://www.facebook.com/thedatascienceblog/
 150. https://www.facebook.com/thedatascienceblog/
 151. https://wordpress.com/?ref=footer_custom_svg
 152. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/

   hidden links:
 154. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
 155. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/#comment-form-guest
 156. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/#comment-form-load-service:wordpress.com
 157. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/#comment-form-load-service:twitter
 158. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/#comment-form-load-service:facebook
