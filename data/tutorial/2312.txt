   #[1]adit deshpande - cs undergrad at ucla ('19)

   [2][pic.jpg]

[3]adit deshpande

   cs undergrad at ucla ('19)

   [4]blog [5]about [6]github [7]projects [8]resume

a beginner's guide to understanding convolutional neural networks

   [cover.png]

introduction

                   convolutional neural networks. sounds like a weird
   combination of biology and math with a little cs sprinkled in, but
   these networks have been some of the most influential innovations in
   the field of id161. 2012 was the first year that neural nets
   grew to prominence as alex krizhevsky used them to win that year   s
   id163 competition (basically, the annual olympics of computer
   vision), dropping the classification error record from 26% to 15%, an
   astounding improvement at the time.ever since then, a host of companies
   have been using deep learning at the core of their services. facebook
   uses neural nets for their automatic tagging algorithms, google for
   their photo search, amazon for their product recommendations, pinterest
   for their home feed personalization, and instagram for their search
   infrastructure.
   [companies.png]

   however, the classic, and arguably most popular, use case of these
   networks is for image processing. within image processing, let   s take a
   look at how to use these id98s for image classification.

the problem space

                   image classification is the task of taking an input
   image and outputting a class (a cat, dog, etc) or a id203 of
   classes that best describes the image. for humans, this task of
   recognition is one of the first skills we learn from the moment we are
   born and is one that comes naturally and effortlessly as adults.
   without even thinking twice, we   re able to quickly and seaid113ssly
   identify the environment we are in as well as the objects that surround
   us. when we see an image or just when we look at the world around us,
   most of the time we are able to immediately characterize the scene and
   give each object a label, all without even consciously noticing. these
   skills of being able to quickly recognize patterns, generalize from
   prior knowledge, and adapt to different image environments are ones
   that we do not share with our fellow machines.
   [corgi3.png]

inputs and outputs

                   when a computer sees an image (takes an image as
   input), it will see an array of pixel values. depending on the
   resolution and size of the image, it will see a 32 x 32 x 3 array of
   numbers (the 3 refers to rgb values). just to drive home the point,
   let's say we have a color image in jpg form and its size is 480 x 480.
   the representative array will be 480 x 480 x 3. each of these numbers
   is given a value from 0 to 255 which describes the pixel intensity at
   that point. these numbers, while meaningless to us when we perform
   image classification, are the only inputs available to the computer.
   the idea is that you give the computer this array of numbers and it
   will output numbers that describe the id203 of the image being a
   certain class (.80 for cat, .15 for dog, .05 for bird, etc).

what we want the computer to do

                   now that we know the problem as well as the inputs and
   outputs, let   s think about how to approach this. what we want the
   computer to do is to be able to differentiate between all the images
   it   s given and figure out the unique features that make a dog a dog or
   that make a cat a cat. this is the process that goes on in our minds
   subconsciously as well. when we look at a picture of a dog, we can
   classify it as such if the picture has identifiable features such as
   paws or 4 legs. in a similar way, the computer is able perform image
   classification by looking for low level features such as edges and
   curves, and then building up to more abstract concepts through a series
   of convolutional layers. this is a general overview of what a id98 does.
   let   s get into the specifics.

biological connection

                   but first, a little background. when you first heard of
   the term convolutional neural networks, you may have thought of
   something related to neuroscience or biology, and you would be right.
   sort of. id98s do take a biological inspiration from the visual cortex.
   the visual cortex has small regions of cells that are sensitive to
   specific regions of the visual field. this idea was expanded upon by a
   fascinating experiment by hubel and wiesel in 1962 ([9]video) where
   they showed that some individual neuronal cells in the brain responded
   (or fired) only in the presence of edges of a certain orientation. for
   example, some neurons fired when exposed to vertical edges and some
   when shown horizontal or diagonal edges. hubel and wiesel found out
   that all of these neurons were organized in a columnar architecture and
   that together, they were able to produce visual perception. this idea
   of specialized components inside of a system having specific tasks (the
   neuronal cells in the visual cortex looking for specific
   characteristics) is one that machines use as well, and is the basis
   behind id98s.

structure

                   back to the specifics. a more detailed overview of what
   id98s do would be that you take the image, pass it through a series of
   convolutional, nonlinear, pooling (downsampling), and fully connected
   layers, and get an output. as we said earlier, the output can be a
   single class or a id203 of classes that best describes the image.
   now, the hard part is understanding what each of these layers do. so
   let   s get into the most important one.

first layer     math part

                   the first layer in a id98 is always a convolutional
   layer. first thing to make sure you remember is what the input to this
   conv (i   ll be using that abbreviation a lot) layer is. like we
   mentioned before, the input is a 32 x 32 x 3 array of pixel values.
   now, the best way to explain a conv layer is to imagine a flashlight
   that is shining over the top left of the image. let   s say that the
   light this flashlight shines covers a 5 x 5 area. and now, let   s
   imagine this flashlight sliding across all the areas of the input
   image. in machine learning terms, this flashlight is called a filter(or
   sometimes referred to as a neuron or a kernel) and the region that it
   is shining over is called the receptive field. now this filter is also
   an array of numbers (the numbers are called weights or parameters). a
   very important note is that the depth of this filter has to be the same
   as the depth of the input (this makes sure that the math works out), so
   the dimensions of this filter is 5 x 5 x 3. now, let   s take the first
   position the filter is in for example.  it would be the top left
   corner. as the filter is sliding, or convolving, around the input
   image, it is multiplying the values in the filter with the original
   pixel values of the image (aka computing element wise multiplications).
   these multiplications are all summed up (mathematically speaking, this
   would be 75 multiplications in total). so now you have a single number.
   remember, this number is just representative of when the filter is at
   the top left of the image. now, we repeat this process for every
   location on the input volume. (next step would be moving the filter to
   the right by 1 unit, then right again by 1, and so on). every unique
   location on the input volume produces a number. after sliding the
   filter over all the locations, you will find out that what you   re left
   with is a 28 x 28 x 1 array of numbers, which we call an activation map
   or feature map. the reason you get a 28 x 28 array is that there are
   784 different locations that a 5 x 5 filter can fit on a 32 x 32 input
   image. these 784 numbers are mapped to a 28 x 28 array.
   [activationmap.png]

   (quick note: some of the images, including the one above, i used came
   from this terrific book, [10]"neural networks and deep learning" by
   michael nielsen. strongly recommend.)

   let   s say now we use two 5 x 5 x 3 filters instead of one. then our
   output volume would be 28 x 28 x 2. by using more filters, we are able
   to preserve the spatial dimensions better. mathematically, this is
   what   s going on in a convolutional layer.

first layer     high level perspective

                   however, let   s talk about what this convolution is
   actually doing from a high level. each of these filters can be thought
   of as feature identifiers. when i say features, i   m talking about
   things like straight edges, simple colors, and curves. think about the
   simplest characteristics that all images have in common with each
   other. let   s say our first filter is 7 x 7 x 3 and is going to be a
   curve detector. (in this section, let   s ignore the fact that the filter
   is 3 units deep and only consider the top depth slice of the filter and
   the image, for simplicity.)as a curve detector, the filter will have a
   pixel structure in which there will be higher numerical values along
   the area that is a shape of a curve (remember, these filters that we   re
   talking about as just numbers!).
   [filter.png]

   now, let   s go back to visualizing this mathematically. when we have
   this filter at the top left corner of the input volume, it is computing
   multiplications between the filter and pixel values at that region. now
   let   s take an example of an image that we want to classify, and let   s
   put our filter at the top left corner.
   [originalandfilter.png]

   remember, what we have to do is multiply the values in the filter with
   the original pixel values of the image.
   [firstpixelmulitiplication.png]

   basically, in the input image, if there is a shape that generally
   resembles the curve that this filter is representing, then all of the
   multiplications summed together will result in a large value! now let   s
   see what happens when we move our filter.
   [secondmultiplication.png]

   the value is much lower! this is because there wasn   t anything in the
   image section that responded to the curve detector filter. remember,
   the output of this conv layer is an activation map. so, in the simple
   case of a one filter convolution (and if that filter is a curve
   detector), the activation map will show the areas in which there at
   mostly likely to be curves in the picture. in this example, the top
   left value of our 26 x 26 x 1 activation map (26 because of the 7x7
   filter instead of 5x5) will be 6600. this high value means that it is
   likely that there is some sort of curve in the input volume that caused
   the filter to activate. the top right value in our activation map will
   be 0 because there wasn   t anything in the input volume that caused the
   filter to activate (or more simply said, there wasn   t a curve in that
   region of the original image). remember, this is just for one filter.
   this is just a filter that is going to detect lines that curve outward
   and to the right. we can have other filters for lines that curve to the
   left or for straight edges. the more filters, the greater the depth of
   the activation map, and the more information we have about the input
   volume.

   disclaimer: the filter i described in this section was simplistic for
   the main purpose of describing the math that goes on during a
   convolution. in the picture below, you   ll see some examples of actual
   visualizations of the filters of the first conv layer of a trained
   network. nonetheless, the main argument remains the same. the filters
   on the first layer convolve around the input image and    activate    (or
   compute high values) when the specific feature it is looking for is in
   the input volume.
   [firstlayers.png]

   (quick note: the above image came from stanford's [11]cs 231n course
   taught by andrej karpathy and justin johnson. recommend for anyone
   looking for a deeper understanding of id98s.)

going deeper through the network

                   now in a traditional convolutional neural network
   architecture, there are other layers that are interspersed between
   these conv layers. i   d strongly encourage those interested to read up
   on them and understand their function and effects, but in a general
   sense, they provide nonlinearities and preservation of dimension that
   help to improve the robustness of the network and control overfitting.
   a classic id98 architecture would look like this.
   [table.png]

   the last layer, however, is an important one and one that we will go
   into later on. let   s just take a step back and review what we   ve
   learned so far. we talked about what the filters in the first conv
   layer are designed to detect. they detect low level features such as
   edges and curves. as one would imagine, in order to predict whether an
   image is a type of object, we need the network to be able to recognize
   higher level features such as hands or paws or ears. so let   s think
   about what the output of the network is after the first conv layer. it
   would be a 28 x 28 x 3 volume (assuming we use three 5 x 5 x 3
   filters).  when we go through another conv layer, the output of the
   first conv layer becomes the input of the 2^nd conv layer.  now, this
   is a little bit harder to visualize. when we were talking about the
   first layer, the input was just the original image. however, when we   re
   talking about the 2^nd conv layer, the input is the activation map(s)
   that result from the first layer. so each layer of the input is
   basically describing the locations in the original image for where
   certain low level features appear. now when you apply a set of filters
   on top of that (pass it through the 2^nd conv layer), the output will
   be activations that represent higher level features. types of these
   features could be semicircles (combination of a curve and straight
   edge) or squares (combination of several straight edges). as you go
   through the network and go through more conv layers, you get activation
   maps that represent more and more complex features. by the end of the
   network, you may have some filters that activate when there is
   handwriting in the image, filters that activate when they see pink
   objects, etc. if you want more information about visualizing filters in
   convnets, matt zeiler and rob fergus had an excellent [12]research
   paper discussing the topic. jason yosinski also has a [13]video on
   youtube that provides a great visual representation. another
   interesting thing to note is that as you go deeper into the network,
   the filters begin to have a larger and larger receptive field, which
   means that they are able to consider information from a larger area of
   the original input volume (another way of putting it is that they are
   more responsive to a larger region of pixel space).

fully connected layer

                   now that we can detect these high level features, the
   icing on the cake is attaching a fully connected layer to the end of
   the network. this layer basically takes an input volume (whatever the
   output is of the conv or relu or pool layer preceding it) and outputs
   an n dimensional vector where n is the number of classes that the
   program has to choose from. for example, if you wanted a digit
   classification program, n would be 10 since there are 10 digits. each
   number in this n dimensional vector represents the id203 of a
   certain class. for example, if the resulting vector for a digit
   classification program is [0 .1 .1 .75 0 0 0 0 0 .05], then this
   represents a 10% id203 that the image is a 1, a 10% id203
   that the image is a 2, a 75% id203 that the image is a 3, and a
   5% id203 that the image is a 9 (side note: there are other ways
   that you can represent the output, but i am just showing the softmax
   approach). the way this fully connected layer works is that it looks at
   the output of the previous layer (which as we remember should represent
   the activation maps of high level features) and determines which
   features most correlate to a particular class. for example, if the
   program is predicting that some image is a dog, it will have high
   values in the activation maps that represent high level features like a
   paw or 4 legs, etc. similarly, if the program is predicting that some
   image is a bird, it will have high values in the activation maps that
   represent high level features like wings or a beak, etc. basically, a
   fc layer looks at what high level features most strongly correlate to a
   particular class and has particular weights so that when you compute
   the products between the weights and the previous layer, you get the
   correct probabilities for the different classes.
   [lenet.png]

training (aka:what makes this stuff work)

                   now, this is the one aspect of neural networks that i
   purposely haven   t mentioned yet and it is probably the most important
   part. there may be a lot of questions you had while reading. how do the
   filters in the first conv layer know to look for edges and curves? how
   does the fully connected layer know what activation maps to look at?
   how do the filters in each layer know what values to have? the way the
   computer is able to adjust its filter values (or weights) is through a
   training process called id26.

   before we get into id26, we must first take a step back and
   talk about what a neural network needs in order to work. at the moment
   we all were born, our minds were fresh. we didn   t know what a cat or
   dog or bird was. in a similar sort of way, before the id98 starts, the
   weights or filter values are randomized. the filters don   t know to look
   for edges and curves. the filters in the higher layers don   t know to
   look for paws and beaks. as we grew older however, our parents and
   teachers showed us different pictures and images and gave us a
   corresponding label. this idea of being given an image and a label is
   the training process that id98s go through. before getting too into it,
   let   s just say that we have a training set that has thousands of images
   of dogs, cats, and birds and each of the images has a label of what
   animal that picture is. back to backprop.

   so id26 can be separated into 4 distinct sections, the
   forward pass, the id168, the backward pass, and the weight
   update. during the forward pass, you take a training image which as we
   remember is a 32 x 32 x 3 array of numbers and pass it through the
   whole network. on our first training example, since all of the weights
   or filter values were randomly initialized, the output will probably be
   something like [.1 .1 .1 .1 .1 .1 .1 .1 .1 .1], basically an output
   that doesn   t give preference to any number in particular. the network,
   with its current weights, isn   t able to look for those low level
   features or thus isn   t able to make any reasonable conclusion about
   what the classification might be. this goes to the id168 part
   of id26. remember that what we are using right now is
   training data. this data has both an image and a label. let   s say for
   example that the first training image inputted was a 3. the label for
   the image would be [0 0 0 1 0 0 0 0 0 0]. a id168 can be
   defined in many different ways but a common one is mse (mean squared
   error), which is    times (actual - predicted) squared.
   [equation.png]

   let   s say the variable l is equal to that value. as you can imagine,
   the loss will be extremely high for the first couple of training
   images. now, let   s just think about this intuitively. we want to get to
   a point where the predicted label (output of the convnet) is the same
   as the training label (this means that our network got its prediction
   right).in order to get there, we want to minimize the amount of loss we
   have. visualizing this as just an optimization problem in calculus, we
   want to find out which inputs (weights in our case) most directly
   contributed to the loss (or error) of the network.
   [loss.png]

   this is the mathematical equivalent of a dl/dw where w are the weights
   at a particular layer. now, what we want to do is perform a backward
   pass through the network, which is determining which weights
   contributed most to the loss and finding ways to adjust them so that
   the loss decreases. once we compute this derivative, we then go to the
   last step which is the weight update. this is where we take all the
   weights of the filters and update them so that they change in the
   opposite direction of the gradient.
   [weight.png]

   the learning rate is a parameter that is chosen by the programmer. a
   high learning rate means that bigger steps are taken in the weight
   updates and thus, it may take less time for the model to converge on an
   optimal set of weights. however, a learning rate that is too high could
   result in jumps that are too large and not precise enough to reach the
   optimal point.
   [highlr.png]

                   the process of forward pass, id168, backward
   pass, and parameter update is one training iteration. the program will
   repeat this process for a fixed number of iterations for each set of
   training images (commonly called a batch). once you finish the
   parameter update on the last training example, hopefully the network
   should be trained well enough so that the weights of the layers are
   tuned correctly.

testing

                   finally, to see whether or not our id98 works, we have a
   different set of images and labels (can   t double dip between training
   and test!) and pass the images through the id98. we compare the outputs
   to the ground truth and see if our network works!

how companies use id98s

                   data, data, data. the companies that have lots of this
   magic 4 letter word are the ones that have an inherent advantage over
   the rest of the competition. the more training data that you can give
   to a network, the more training iterations you can make, the more
   weight updates you can make, and the better tuned to the network is
   when it goes to production. facebook (and instagram) can use all the
   photos of the billion users it currently has, pinterest can use
   information of the 50 billion pins that are on its site, google can use
   search data, and amazon can use data from the millions of products that
   are bought every day. and now you know the magic behind how they use
   it.

disclaimer

                   while this post should be a good start to understanding
   id98s, it is by no means a comprehensive overview. things not discussed
   in this post include the nonlinear and pooling layers as well as
   hyperparameters of the network such as filter sizes, stride, and
   padding. topics like network architecture, batch id172,
   vanishing gradients, dropout, initialization techniques, non-convex
   optimization,biases, choices of id168s, data
   augmentation,id173 methods, computational considerations,
   modifications of id26, and more were also not discussed (yet
   ).

   [14]link to part 2

   dueces.
   [15]sources

   [16]tweet

   written on july 20, 2016

   please enable javascript to view the [17]comments powered by disqus.

references

   visible links
   1. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
   2. https://adeshpande3.github.io/adeshpande3.github.io/
   3. https://adeshpande3.github.io/adeshpande3.github.io/
   4. https://adeshpande3.github.io/adeshpande3.github.io/
   5. https://adeshpande3.github.io/adeshpande3.github.io/about
   6. https://github.com/adeshpande3
   7. https://adeshpande3.github.io/adeshpande3.github.io/projects
   8. https://adeshpande3.github.io/adeshpande3.github.io/resume.pdf
   9. https://www.youtube.com/watch?v=cw5pkv9rj3o
  10. http://neuralnetworksanddeeplearning.com/
  11. http://cs231n.stanford.edu/
  12. https://cs.nyu.edu/~fergus/papers/zeilereccv2014.pdf
  13. https://www.youtube.com/watch?v=agkfiq4igam
  14. https://adeshpande3.github.io/adeshpande3.github.io/a-beginner's-guide-to-understanding-convolutional-neural-networks-part-2/
  15. https://adeshpande3.github.io/assets/sources.txt
  16. https://twitter.com/share
  17. http://disqus.com/?ref_noscript

   hidden links:
  19. mailto:adeshpande3@g.ucla.edu
  20. https://www.facebook.com/adit.deshpande.5
  21. https://github.com/adeshpande3
  22. https://instagram.com/thejugglinguy
  23. https://www.linkedin.com/in/aditdeshpande
  24. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
  25. https://www.twitter.com/aditdeshpande3
