   #[1]adit deshpande - cs undergrad at ucla ('19)

   [2][pic.jpg]

[3]adit deshpande

   cs undergrad at ucla ('19)

   [4]blog [5]about [6]github [7]projects [8]resume

a beginner's guide to understanding convolutional neural networks part 2

   [cover2nd.png]

   introduction

   [9]link to part 1

                  in this post, we   ll go into a lot more of the specifics
   of convnets. disclaimer: now, i do realize that some of these topics
   are quite complex and could be made in whole posts by themselves. in an
   effort to remain concise yet retain comprehensiveness, i will provide
   links to research papers where the topic is explained in more detail.

   stride and padding

                   alright, let   s look back at our good old conv layers.
   remember the filters, the receptive fields, the convolving? good. now,
   there are 2 main parameters that we can change to modify the behavior
   of each layer. after we choose the filter size, we also have to choose
   the stride and the padding.

   stride controls how the filter convolves around the input volume. in
   the example we had in part 1, the filter convolves around the input
   volume by shifting one unit at a time. the amount by which the filter
   shifts is the stride. in that case, the stride was implicitly set at 1.
   stride is normally set in a way so that the output volume is an integer
   and not a fraction. let   s look at an example. let   s imagine a 7 x 7
   input volume, a 3 x 3 filter (disregard the 3^rd dimension for
   simplicity), and a stride of 1. this is the case that we   re accustomed
   to.
   [stride1.png]

   same old, same old, right? see if you can try to guess what will happen
   to the output volume as the stride increases to 2.
   [stride2.png]

   so, as you can see, the receptive field is shifting by 2 units now and
   the output volume shrinks as well. notice that if we tried to set our
   stride to 3, then we   d have issues with spacing and making sure the
   receptive fields fit on the input volume. normally, programmers will
   increase the stride if they want receptive fields to overlap less and
   if they want smaller spatial dimensions.

   now, let   s take a look at padding. before getting into that, let   s
   think about a scenario. what happens when you apply three 5 x 5 x 3
   filters to a 32 x 32 x 3 input volume? the output volume would be 28 x
   28 x 3. notice that the spatial dimensions decrease. as we keep
   applying conv layers, the size of the volume will decrease faster than
   we would like. in the early layers of our network, we want to preserve
   as much information about the original input volume so that we can
   extract those low level features. let   s say we want to apply the same
   conv layer but we want the output volume to remain 32 x 32 x 3. to do
   this, we can apply a zero padding of size 2 to that layer. zero padding
   pads the input volume with zeros around the border. if we think about a
   zero padding of two, then this would result in a 36 x 36 x 3 input
   volume.
   [pad.png]

   if you have a stride of 1 and if you set the size of zero padding to


   [zeropad.png]

   where k is the filter size, then the input and output volume will
   always have the same spatial dimensions.

   the formula for calculating the output size for any given conv layer is


   [output.png]

   where o is the output height/length, w is the input height/length, k is
   the filter size, p is the padding, and s is the stride.

   choosing hyperparameters

                   how do we know how many layers to use, how many conv
   layers, what are the filter sizes, or the values for stride and
   padding? these are not trivial questions and there isn   t a set standard
   that is used by all researchers. this is because the network will
   largely depend on the type of data that you have. data can vary by
   size, complexity of the image, type of image processing task, and more.
   when looking at your dataset, one way to think about how to choose the
   hyperparameters is to find the right combination that creates
   abstractions of the image at a proper scale.

   relu (rectified linear units) layers

                   after each conv layer, it is convention to apply a
   nonlinear layer (or activation layer) immediately afterward.the purpose
   of this layer is to introduce nonlinearity to a system that basically
   has just been computing linear operations during the conv layers (just
   element wise multiplications and summations).in the past, nonlinear
   functions like tanh and sigmoid were used, but researchers found out
   that relu layers work far better because the network is able to train a
   lot faster (because of the computational efficiency) without making a
   significant difference to the accuracy. it also helps to alleviate the
   vanishing gradient problem, which is the issue where the lower layers
   of the network train very slowly because the gradient decreases
   exponentially through the layers (explaining this might be out of the
   scope of this post, but see [10]here and [11]here for good
   descriptions). the relu layer applies the function f(x) = max(0, x) to
   all of the values in the input volume. in basic terms, this layer just
   changes all the negative activations to 0.this layer increases the
   nonlinear properties of the model and the overall network without
   affecting the receptive fields of the conv layer.

   [12]paper by the great geoffrey hinton (aka the father of deep
   learning).

   pooling layers

                   after some relu layers, programmers may choose to apply
   a pooling layer. it is also referred to as a downsampling layer. in
   this category, there are also several layer options, with maxpooling
   being the most popular. this basically takes a filter (normally of size
   2x2) and a stride of the same length. it then applies it to the input
   volume and outputs the maximum number in every subregion that the
   filter convolves around.
   [maxpool.png]

   other options for pooling layers are average pooling and l2-norm
   pooling. the intuitive reasoning behind this layer is that once we know
   that a specific feature is in the original input volume (there will be
   a high activation value), its exact location is not as important as its
   relative location to the other features. as you can imagine, this layer
   drastically reduces the spatial dimension (the length and the width
   change but not the depth) of the input volume. this serves two main
   purposes. the first is that the amount of parameters or weights is
   reduced by 75%, thus lessening the computation cost. the second is that
   it will control overfitting. this term refers to when a model is so
   tuned to the training examples that it is not able to generalize well
   for the validation and test sets. a symptom of overfitting is having a
   model that gets 100% or 99% on the training set, but only 50% on the
   test data.

   dropout layers

                  now, dropout layers have a very specific function in
   neural networks. in the last section, we discussed the problem of
   overfitting, where after training, the weights of the network are so
   tuned to the training examples they are given that the network doesn   t
   perform well when given new examples. the idea of dropout is simplistic
   in nature. this layer    drops out    a random set of activations in that
   layer by setting them to zero. simple as that. now, what are the
   benefits of such a simple and seemingly unnecessary and
   counterintuitive process? well, in a way, it forces the network to be
   redundant. by that i mean the network should be able to provide the
   right classification or output for a specific example even if some of
   the activations are dropped out. it makes sure that the network isn   t
   getting too    fitted    to the training data and thus helps alleviate the
   overfitting problem. an important note is that this layer is only used
   during training, and not during test time.

   [13]paper by geoffrey hinton.

   network in network layers

                   a network in network layer refers to a conv layer where
   a 1 x 1 size filter is used. now, at first look, you might wonder why
   this type of layer would even be helpful since receptive fields are
   normally larger than the space they map to. however, we must remember
   that these 1x1 convolutions span a certain depth, so we can think of it
   as a 1 x 1 x n convolution where n is the number of filters applied in
   the layer. effectively, this layer is performing a n-d element-wise
   multiplication where n is the depth of the input volume into the layer.

   [14]paper by min lin.

   classification, localization, detection, segmentation

                   in the example we used in part 1 of this series, we
   looked at the task of image classification. this is the process of
   taking an input image and outputting a class number out of a set of
   categories. however, when we take a task like object localization, our
   job is not only to produce a class label but also a bounding box that
   describes where the object is in the picture.
   [localization.png]

   we also have the task of id164, where localization needs to
   be done on all of the objects in the image. therefore, you will have
   multiple bounding boxes and multiple class labels.

   finally, we also have object segmentation where the task is to output a
   class label as well as an outline of every object in the input image.
   [detection.png]

   more detail on how these are implemented to come in part 3, but for
   those who can   t wait   

   detection/ localization: [15]rid98, [16]fast rid98, [17]faster rid98,
   [18]multibox, [19]bayesian optimization, [20]multi-region, [21]rid98
   minus r, [22]image windows
   segmentation: [23]semantic seg, [24]unconstrained video, [25]shape
   guided, [26]object regions, [27]shape sharing

   yeah, there   s a lot more.

   id21

   now, a common misconception in the dl community is that without a
   google-esque amount of data, you can   t possibly hope to create
   effective deep learning models. while data is a critical part of
   creating the network, the idea of id21 has helped to
   lessen the data demands. id21 is the process of taking a
   pre-trained model (the weights and parameters of a network that has
   been trained on a large dataset by somebody else) and    fine-tuning    the
   model with your own dataset. the idea is that this pre-trained model
   will act as a feature extractor. you will remove the last layer of the
   network and replace it with your own classifier (depending on what your
   problem space is). you then freeze the weights of all the other layers
   and train the network normally (freezing the layers means not changing
   the weights during id119/optimization).

   let   s investigate why this works. let   s say the pre-trained model that
   we   re talking about was trained on id163 (for those that aren   t
   familiar, id163 is a dataset that contains 14 million images with
   over 1,000 classes). when we think about the lower layers of the
   network, we know that they will detect features like edges and curves.
   now, unless you have a very unique problem space and dataset, your
   network is going to need to detect curves and edges as well. rather
   than training the whole network through a random initialization of
   weights, we can use the weights of the pre-trained model (and freeze
   them) and focus on the more important layers (ones that are higher up)
   for training. if your dataset is quite different than something like
   id163, then you   d want to train more of your layers and freeze only
   a couple of the low layers.

   [28]paper by yoshua bengio (another deep learning pioneer).
   [29]paper by ali sharif razavian.
   [30]paper by jeff donahue.
   [31]paper and [32]subsequent paper by dario garcia-gasulla.

   data augmentation techniques

   by now, we   re all probably numb to the importance of data in convnets,
   so let   s talk about ways that you can make your existing dataset even
   larger, just with a couple easy transformations. like we   ve mentioned
   before, when a computer takes an image as an input, it will take in an
   array of pixel values. let   s say that the whole image is shifted left
   by 1 pixel. to you and me, this change is imperceptible. however, to a
   computer, this shift can be fairly significant as the classification or
   label of the image doesn   t change, while the array does. approaches
   that alter the training data in ways that change the array
   representation while keeping the label the same are known as data
   augmentation techniques. they are a way to artificially expand your
   dataset. some popular augmentations people use are grayscales,
   horizontal flips, vertical flips, random crops, color jitters,
   translations, rotations, and much more. by applying just a couple of
   these transformations to your training data, you can easily double or
   triple the number of training examples.

   [33]link to part 3

   dueces.
   [34]sources

   [35]tweet

   written on july 29, 2016

   please enable javascript to view the [36]comments powered by disqus.

references

   visible links
   1. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
   2. https://adeshpande3.github.io/adeshpande3.github.io/
   3. https://adeshpande3.github.io/adeshpande3.github.io/
   4. https://adeshpande3.github.io/adeshpande3.github.io/
   5. https://adeshpande3.github.io/adeshpande3.github.io/about
   6. https://github.com/adeshpande3
   7. https://adeshpande3.github.io/adeshpande3.github.io/projects
   8. https://adeshpande3.github.io/adeshpande3.github.io/resume.pdf
   9. https://adeshpande3.github.io/adeshpande3.github.io/a-beginner's-guide-to-understanding-convolutional-neural-networks/
  10. https://en.wikipedia.org/wiki/vanishing_gradient_problem
  11. https://www.quora.com/what-is-the-vanishing-gradient-problem
  12. http://www.cs.toronto.edu/~fritz/absps/reluicml.pdf
  13. https://www.cs.toronto.edu/~hinton/absps/jmlrdropout.pdf
  14. https://arxiv.org/pdf/1312.4400v3.pdf
  15. https://arxiv.org/pdf/1311.2524v5.pdf
  16. https://arxiv.org/pdf/1504.08083.pdf
  17. http://arxiv.org/pdf/1506.01497v3.pdf
  18. http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/erhan_scalable_object_detection_2014_cvpr_paper.pdf
  19. http://web.eecs.umich.edu/~honglak/cvpr15-id98-detection.pdf
  20. http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/gidaris_object_detection_via_iccv_2015_paper.pdf
  21. http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/lenc15rid98.pdf
  22. http://calvin.inf.ed.ac.uk/wp-content/uploads/publications/alexe12pami.pdf
  23. http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/long_fully_convolutional_networks_2015_cvpr_paper.pdf
  24. http://calvin.inf.ed.ac.uk/wp-content/uploads/publications/papazoglouiccv2013-camera-ready.pdf
  25. https://www.cs.cmu.edu/~efros/courses/lbmv07/papers/borenstein06.pdf
  26. http://crcv.ucf.edu/papers/cvpr2013/videoobjectsegmentation.pdf
  27. http://www.cs.utexas.edu/~grauman/papers/shape-sharing-eccv2012.pdf
  28. https://arxiv.org/pdf/1411.1792v1.pdf
  29. http://arxiv.org/pdf/1403.6382.pdf
  30. https://arxiv.org/pdf/1310.1531.pdf
  31. https://arxiv.org/pdf/1705.07706.pdf
  32. https://arxiv.org/pdf/1707.09872.pdf
  33. https://adeshpande3.github.io/adeshpande3.github.io/the-9-deep-learning-papers-you-need-to-know-about.html
  34. https://adeshpande3.github.io/assets/sources2.txt
  35. https://twitter.com/share
  36. http://disqus.com/?ref_noscript

   hidden links:
  38. mailto:adeshpande3@g.ucla.edu
  39. https://www.facebook.com/adit.deshpande.5
  40. https://github.com/adeshpande3
  41. https://instagram.com/thejugglinguy
  42. https://www.linkedin.com/in/aditdeshpande
  43. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
  44. https://www.twitter.com/aditdeshpande3
