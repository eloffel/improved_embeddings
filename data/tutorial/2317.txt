scalable large-margin 
structured learning: 
theory and algorithms

x

x

y=+1

y=-1

the   man   bit    the    dog

dt   nn   vbd  dt   nn

x

y

the   man   hit    the    dog

              (cid:20277)              

liang huang   kai zhao  lemao liu
the city university of new york (cuny)

slides at: http://acl.cs.qc.edu/~lhuang/

examples of bad id170

what is id170?

x

x

y=+1

y=-1

the   man   bit    the    dog

dt   nn   vbd  dt   nn

x

y

the   man   bit    the    dog x

the   man   bit    the    dog x

s

np

vp

dt

nn

vb

np

y

bit

the

dt

nn

man

              (cid:20277)               y
    binary classi   cation: output is binary
nlp is all about id170!
    multiclass classi   cation: output is a (small) number
    structured classi   cation: output is a structure (seq., tree, graph)
    part-of-speech tagging, parsing, summarization, translation
    exponentially many classes: search (id136) ef   ciency is crucial! 2

dog

the

learning: unstructured vs. structured
binary/multiclass
naive 
bayes

structured learning

generative

id48s

(count & divide)

conditional

conditional

logistic 
regression
(maxent)

online+
viterbi

id88
max
margin

id166

3

discriminative

(expectations)

crfs

online+
viterbi

structured id88

(argmax)

max
margin

structured id166

(loss-augmented 

argmax)

4

why id88 (online learning)?
    because we want scalability on big data!
    learning time has to be linear in the number of examples
    can make only constant number of passes over training data
    only online learning (id88/mira) can guarantee this!
    id166 scales between o(n2) and o(n3); crf no guarantee
    and id136 on each example must be super fast
    another advantage of id88: just need argmax

id166

crf

    . . .

5

id88: from binary to structured

binary id88
(rosenblatt, 1959)

x

x

y=+1

y=-1
multiclass id88
(freund/schapire, 1999)

trivial
x

w
exact
id136

y

2 classes

constant

# of classes

easy
x

w
exact
id136

y

structured id88

(collins, 2002)

the   man   bit    the    dog

exponential 
# of classes
x

x

                (cid:20277)                

y

y

hard
w
exact
id136

z

z

z

update weights

if y     z

update weights

if y     z

update weights

if y     z

scalability challenges

tutorial outline

    id136 (on one example) is too slow (even w/ dp)
    can we sacri   ce search exactness for faster learning?
    would inexact search interfere with learning?
    if so, how should we modify learning?
    even fastest inexact id136 is still too slow
    due to too many training examples
    can we parallelize online learning?

5
update
6
update
7
update
8
update

1
update
2
update
3
update
4
update

    . . .

    overview of structured learning
    challenges in scalability
    structured id88
    convergence proof
    structured id88 with inexact search

9
update
10
update
11
update
12
update

13
update
14
update
15
update
16
update

   

    latent-variable structured id88
    parallelizing online learning (id88 & mira)

7

6

8

generic id88

    id88 is the simplest machine learning algorithm
    online-learning: one example at a time
    learning by doing
       nd the best output under the current weights
    update weights at mistakes
yi

update weights

xi

id136
w

zi

structured id88

the man bit the dog

xi

id136
w

dt nn  vbd dt nn

yi

zi

dt nn  nn dt nn

update weights

   

9

example:  pos  tagging

id136: id145

    gold-standard:    dt   nn   vbd  dt   nn
                          the   man   bit    the    dog
    current output:  dt   nn   nn   dt   nn
                          the   man   bit    the    dog
    assume only two feature classes
    tag bigrams          ti-1    ti
    word/tag pairs            wi
    weights ++:  (nn, vbd)    (vbd, dt)     (vbd   bit)
    weights --:  (nn, nn)     (nn, dt)      (nn   bit)

y

x

z

x

  (x, y)

  (x, z)

11

w
exact
id136

z

x

y

update weights

if y     z

tagging: o(nt3)

cky parsing: o(n3)

10

12

ef   ciency vs. expressiveness

averaged id88

xi

argmax

y   gen(x)
id136
w

yi

zi

update weights

    the id136 (argmax) must be ef   cient
    either the search space gen(x) is small, or factored
    features must be local to y (but can be global to x)
    e.g. bigram tagger, but look at all input words (cf. crfs)

    much more stable and accurate results
    approximation of voted id88 (large-margin) 

(freund & schapire, 1999)

j

0

y

x

13

   

j + 1

j

= !

wj

j

14

averaging => large margin

    much more stable and accurate results
    approximation of voted id88 (large-margin) 

(freund & schapire, 1999)

ef   cient implementation of averaging
    naive implementation (running sum) doesn   t scale
    very clever trick from daume (2006, phd thesis)

wt

 wt

r
o
r
r
e
 
t
s
e

t

15

16

w(0)=w(1)=w(2)=w(3)=w(4)= w(1) w(1) w(2) w(1) w(2) w(3) w(1) w(2) w(3) w(4)id88 vs. crfs

    id88 is online and viterbi approximation of crf
    simpler to code; faster to converge; ~same accuracy

e

nlin

o

crfs

(lafferty et al, 2001)

vite

r
bi

x(x,y)2d xz2gen(x)

exp(w     (x, z))

z(x)

stochastic gradient 

descent (sgd)

hard/viterbi crfs

vite

r
bi

for (x, y) 2 d, argmax
z2gen(x)

w     (x, z)

structured id88

(collins, 2002)

e

nlin

o

id88 convergence proof
    binary classi   cation:    converges iff. data is separable
    id170: converges iff. data is separable
    there is an oracle vector that correctly labels all examples
    one vs the rest (correct label better than all incorrect labels)
    theorem: if separable, then # of updates     r2 /   2     r: diameter
r: diameter

x100

r: diameter

x100
x3012

  

y=-1

x111

x2000

y=+1

y100

  

z     y100

geometry of convergence proof pt 1

geometry of convergence proof pt 2

17

novikoff => freund & schapire => collins
  1962                 1999                  2002 

18

z

exact
1-best

 

u

 

(

p

d

x

,

a

t

e

y

correct 

label

y

,

z

)

  

separation

unit oracle 
vector u

w
exact
id136

z

x

y

update weights

if y     z

id88 update:

margin

   

(by induction)

kwk+1k   k 

(part 1: lowerbound)
19

u

p

d

a

t

e

)
k
(
w

 
t
n
e
r
r
u
c

l

e
d
o
m

(k+1)
new 
model
w

x

summary: the proof uses 3 facts:
1. separation (margin)
2. diameter (always    nite)
3. violation (guaranteed by exact search)

w
exact
id136

update weights

if y     z

y

z

violation: incorrect label scored higher

id88 update:

z

exact
1-best

 

u

 

(

p

d

x

,

a

t

e

y

,

z

)

  m a x   d i a m e t e r

r :

y

correct 

label

u

p

d

a

t

e

p

k r

(k+1)
new 
model
w

by induction: 

combine with: 

)
k
(
w

 
t
n
e
r
r
u
c

l

e
d
o
m

<
9
0
  

    r2
diameter
k 
kwk+1k2     kr2
pkr
kwk+1k    
kwk+1k   k 
bound on # of updates:

violation
(part 2: upperbound)

(part 1: lowerbound)
20

k     r2/ 2

tutorial outline

    overview of structured learning
    challenges in scalability
    structured id88
    convergence proof
    structured id88 with inexact search

    latent-variable id88
    parallelizing online learning (id88 & mira)

scalability challenge 1: id136
x

x

binary classi   cation

trivial
x
# of classes
y

constant

w
exact
id136

z

update weights

if y     z

y=+1

y=-1

structured classi   cation

exponential 
# of classes

the   man   bit    the    dog

dt   nn   vbd  dt   nn

x

y

x

y

hard
w
exact
id136

z

update weights

if y     z

    challenge: search ef   ciency (exponentially many classes)
    often use id145 (dp)
    but dp is still too slow for repeated use, e.g. parsing o(n3)
    q: can we sacri   ce search exactness for faster learning?

22

21

id88 w/ inexact id136

bad news and good news

the   man   bit    the    dog

dt   nn   vbd  dt   nn

x

y

x

y

w

inexact
id136

z

update weights

if y     z

the   man   bit    the    dog

dt   nn   vbd  dt   nn

x

y

x

y

w

inexact
id136

z

update weights

if y     z

greedy search

id125

greedy search

id125

q: does id88 still work???

a: it no longer works as is,
but we can make it work

by some magic.

    routine use of inexact id136 in nlp (e.g. id125)
    how does structured id88 work with inexact search?
    so far most structured learning theory assume exact search
    would search errors break these learning properties?
    if so how to modify learning to accommodate inexact search?

23

    bad news: no more guarantee of convergence
    in practice id88 degrades a lot due to search errors
    good news: new update methods guarantee convergence
    new id88 variants that    live with    search errors
    in practice they work really well w/ inexact search

24

convergence with exact search

no convergence w/ greedy search

current 
model

w(k+1)

w

(

k

)

update

v v

v

v n

n

n v
correct
label

training example 

time        ies
n         v

output space
{n,v} x {n, v}

structured id88 

converges with 
exact search

w (

w ( k )

current 
model
w ( k )

v v

v

new 
model
)

k + 1

v v

n

v

n

n v
correct
label

n v

training example 

time        ies
n         v

output space
{n,v} x {n, v}

structured id88 

does not converge 
with greedy search

v n

update

  (x,y,z)

 

v n

which part of the convergence proof no longer holds?

the proof only uses 3 facts:
1. separation (margin)
2. diameter (always    nite)
3. violation (guaranteed by exact search)

26

25

geometry of convergence proof pt 2

w
exact
id136

z

x

y

update weights

if y     z

violation: incorrect label scored higher

observation: violation is all we need!
    exact search is not really required by the proof
    rather, it is only used to ensure violation!
all
 violations

violation: incorrect label scored higher

exact
1-best

z

 

u

p

id88 update:

 

(

d

x

,

a

t

e

  m a x   d i a m e t e r

r :

y

,

z

)

y

correct 

label

    r2
diameter

violation

inexact search doesn   t guarantee violation!

u

p

d

a

t

e

<
9
0
  

)
k
(
w

 
t
n
e
r
r
u
c

l

e
d
o
m

(k+1)
new 
model
w

27

28

the proof only uses 3 facts:

1. separation (margin)
2. diameter (always    nite)
3. violation (but no need for exact)

z

exact
1-best

 

u

 

(

p

d

x

,

a

t

e

  m a x   d i a m e t e r

r :

z

y

,

z

)

y

correct 

label

inexact
1-best

u

p

d

a

t

e

)
k
(
w

 
t
n
e
r
r
u
c

l

e
d
o
m

<
9
0
  

(k+1)
new 
model
w

violation-fixing id88

    if we guarantee violation, we don   t care about exactness!
    violation is good b/c we can at least    x a mistake
all

same mistake bound as before!

 violations

y

standard id88

violation-   xing id88

w
exact
id136

x

y

z

update weights

if y     z

x

y

w
   nd 

violation

z

y   

update weights

if y        z

29

what if can   t guarantee violation
    this is why id88 doesn   t work well w/ inexact search
    because not every update is guaranteed to be a violation
    thus the proof breaks; no convergence guarantee
    example: beam or greedy search
    the model might prefer the correct label (if exact search)
    but the search prunes it away
    such a non-violation update is    bad   
because it doesn   t    x any mistake
    the new model still misguides the search
    q: how can we always guarantee violation?

current
model

d  
a

t

d

b

u

p

e

a

30

solution 1: early update (collins/roark 2004)

early update: guarantees violation

current 
model
w ( k )

v v

v

new 
model
)

k + 1

w (

w ( k )

w ( k )

w(k+1)
new 
model

n

v

v

n

 
 
(
x
,
y
,
z
)

n v
correct
label

n v

training example 

time        ies
n         v

output space
{n,v} x {n, v}

standard id88 
does not converge 
with greedy search

v n

update

  (x,y,z)

 

v n

stop and update at the    rst mistake

31

w ( k )

)

k + 1

w (

w ( k )

w ( k )

w(k+1)

v v

v

n

v

v

n

 
 
(
x
,
y
,
z
)

n v
correct
label

n v

training example 

time        ies
n         v

output space
{n,v} x {n, v}

standard update 
doesn   t converge 

b/c it  doesn   t 

guarantee violation

v n

  (x,y,z)

 

v n

early update: incorrect pre   x 

scores higher: a violation!

32

all possible updatesbeamnv vearly update: from greedy to beam
    id125 is a generalization of greedy (where b=1)
    at each stage we keep top b hypothesis
    widely used: tagging, parsing, translation...
    early update -- when correct label    rst falls off the beam
    up to this point the incorrect pre   x should score higher
    standard update (full update) -- no guarantee!

correct
i n c o r r e c t

 
y
l
r
a
e

e
t
a
d
p
u

violation guaranteed: 
incorrect pre   x scores 
higher up to this point

correct label
falls off beam

(pruned)

standard update
(no guarantee!)
33

early update as violation-fixing
also new de   nition of
   beam separability   :

w

a correct pre   x should 

score higher than 
any incorrect pre   x 
of the same length
(maybe too strong)

x

y

cf. kulesza and pereira,2007

   nd 

violation

update weights

if y        z

z

y   

pre   x violations

y

   beam

z

 
y
l
r
a
e

e
t
a
d
p
u

y   

correct label
falls off beam

(pruned)

standard update

(bad!)

34

solution 2: max-violation  (huang et al 2012)

  beam

early

latest

standard
(bad!)

max-violation

    we now established a theory for early update (collins/roark)
    but it learns too slowly due to partial updates
    max-violation: use the pre   x where violation is maximum
       worst-mistake    in the search space
    all these update methods are violation-   xing id88s

35

four experiments

part-of-speech tagging

the   man   bit    the    dog

dt   nn   vbd  dt   nn

x

y

incremental parsing

the   man   bit    the    dog

the

man

bit

the

dog

bottom-up parsing w/ cube pruning

machine translation

the   man   bit    the    dog

the

man

bit

the

dog

x

y

the   man   bit    the    dog

              (cid:20277)              

x

y

x

y

max-violation > early >> standard
    exp 1 on part-of-speech tagging w/ id125 (on ctb5)
    early and max-violation >> standard update at smallest beams
    this advantage shrinks as beam size increases
    max-violation converges faster than early (and slightly better)

max-violation > early >> standard
    exp 1 on part-of-speech tagging w/ id125 (on ctb5)
    early and max-violation >> standard update at smallest beams
    this advantage shrinks as beam size increases
    max-violation converges faster than early (and slightly better)

t

l

 

u
o
-
d
e
h
n
o
 
y
c
a
r
u
c
c
a
g
n
g
g
a

 

i

t

 94

 93.5

 93

 92.5

 92

 0

t

l

u
o
-
d
e
h
n
o

 

beam=1 (greedy)

 
y
c
a
r
u
c
c
a
g
n
g
g
a

 

i

 94

 93.5

 93

 92.5

 0.05

 0.1

 0.15

training time (hours)

 0.2

 0.25

t
 
t
s
e
b

 92

 1

best accuracy 
vs. beam size

max-violation
early
standard

 2

 3

 4

beam size

 5

 6

 7

37

t

l

 

u
o
-
d
e
h
n
o
 
y
c
a
r
u
c
c
a
g
n
g
g
a

 

i

t

 94

 93.5

 93

 92.5

 92

 0

t

l

u
o
-
d
e
h
n
o

 

beam=2

 
y
c
a
r
u
c
c
a
g
n
g
g
a

 

i

 94

 93.5

 93

 92.5

 0.05

 0.1

 0.15

training time (hours)

 0.2

 0.25

t
 
t
s
e
b

 92

 1

best accuracy 
vs. beam size

max-violation
early
standard

 2

 3

 4

beam size

 5

 6

 7

38

max-violation > early >> standard
    exp 2 on my incremental dependency parser (huang & sagae 10)
    standard update is horrible due to search errors
    early update:   38 iterations, 15.4 hours  (92.24)
    max-violation: 10 iterations,   4.6 hours  (92.25)

t

u
o
-
d
e
h

l

 

n
o

 
y
c
a
r
u
c
c
a

 

i

g
n
s
r
a
p

 92
 90
 88
 86
 84
 82
 80
 78

beam=8

max-violation
early
standard

 0

 2

 4

 6

 8

 10  12  14  16

training time (hours)

t

u
o
-
d
e
h

l

 

n
o

 
y
c
a
r
u
c
c
a

 

i

g
n
s
r
a
p

 92.25

 92

 91.75

 91.5

 91.25

 91

 0

beam=8

max-violation
early
standard (79.0, omitted)
 2

 4
training time (hours)

 6

 8  10  12  14  16

max-violation is 3.3x faster than early update

39

why standard update so bad for parsing

    standard update works horribly with severe search error
    due to large number of invalid updates (non-violation)

t

u
o
-
d
e
h

l

 

n
o

 
y
c
a
r
u
c
c
a

 

i

g
n
s
r
a
p

 92
 90
 88
 86
 84
 82
 80
 78

o(n11) => o(nb)

max-violation
early
standard

parsing

b=8

 0

 2

 4

 6

 8

 10  12  14  16

take-home message:

 0

t

 0.05

 0.1

 0.15

 0.2

 0.25

training time (hours)

early/max-violation more helpful 

training time (hours)

for harder search problems!

parsing
tagging

o(nt3) => o(nb)

tagging

b=1

t

u
o
-
d
e
h

l

 

n
o

 
y
c
a
r
u
c
c
a

 

i

g
n
g
g
a

 94
 93.5
 93
 92.5
 92
 91.5
 91

 100

 75

s
e

t

a
d
p
u

 

d

i
l

a
v
n

i
 
f

 

o
%

 50

 25

 0

% of invalid updates 
in standard update

 2

 4

 6

 8

 10  12  14  16

beam size

40

exp 3: bottom-up parsing

exp 4: machine translation

    cky parsing with cube pruning for higher-order features
    we extended our framework from graphs to hypergraphs

uas on penn-ym dev

s
a
u

 94
 93.8
 93.6
 93.4
 93.2
 93
 92.8
 92.6
 92.4
 92.2
 92

s-max
p-max
skip
standard
 2
 3

 1

(zhang et al 2013)

 4
 5
epochs

 6

 7

 8

41

comparison of four exps
    the harder your search, the more advantageous

t

u
o
-
d
e
h

l

 

n
o

 
y
c
a
r
u
c
c
a

 

i

g
n
g
g
a

t

 94
 93.5
 93
 92.5
 92
 91.5
 91

s
a
u

 94
 93.8
 93.6
 93.4
 93.2
 93
 92.8
 92.6
 92.4
 92.2
 92

tagging

b=1

 0

 0.05

 0.1

 0.15

 0.2

 0.25

training time (hours)
uas on penn-ym dev

s-max
p-max
skip
standard
 2
 3

 1

bottom-up 

parsing

 4
 5
epochs

 6

 7

 8

t

u
o
-
d
e
h

l

 

n
o

 
y
c
a
r
u
c
c
a

 

i

g
n
s
r
a
p

u
e
l
b

 92
 90
 88
 86
 84
 82
 80
 78

26
25
24
23
22
21
20
19
18
17
16
15

max-violation
early
standard

incremental 
parsing b=8

 0

 2

 4

 6

 8

 10  12  14  16

training time (hours)

machine translation

max-violation
early
standard
local

 2

 4

 6

 8  10  12  14  16  18  20

number of iteration

43

    standard id88 works poorly for machine translation
    b/c invalid update ratio is very high (search quality is low)
    max-violation converges faster than early update
       rst truly successful effort in 
large-scale training for translation

u
e
l
b

26
25
24
23
22
21
20
19
18
17
16
15

max-violation
early
standard
local

 2

 4

 6

 8  10  12  14  16  18  20

90%

80%

o

i
t

a
r

70%

60%

50%

ratio of invalid updates
+non-local feature

(standard id88)

 2  4  6  8  10 12 14 16 18 20 22 24 26 28 30

number of iteration

(yu et al 2013)

beam size

related work and discussions
    our    violation-   xing    framework include as special cases
    early-update (collins and roark, 2004)
    laso (daume and marcu, 2005)
    not sure about searn (daume et al, 2009)
       beam-separability    or    greedy-separability    related to:
       algorithmic-separability    of (kulesza and pereira, 2007)
    but these conditions are too strong to hold in practice
    under-generating (beam) vs. over-generating (lp-relax.)
    kulesza & pereira and martins et al (2011): lp-relaxation
    finley and joachims (2008): both under and over for id166

44

conclusions so far

tutorial outline

    structured id88 is simple, scalable, and powerful
    (almost) same convergence proof from multiclass id88
    but it doesn   t work very well with inexact search
    solution: violation-   xing id88 framework
    convergence under new defs of separability
    learn to    live with    search errors
    in particular,    max-violation    works great
    converges fast, and results in high accuracy
    they are more helpful to harder search problems!

    overview of structured learning
    challenges in scalability
    structured id88
    convergence proof
    structured id88 with inexact search

    latent-variable id88
    parallelizing online learning (id88 & mira)

45

46

learning with latent variables

learning latent structures

    aka    weakly-supervised    or    partially-observed    learning
    learning from    natural annotations   ; more scalable
    examples: translation, id68, id29...
                (cid:1)     (cid:2)

what is the largest state?

 computer

x

binary/multiclass
naive 
bayes

structured learning

latent structures

id48s

em

(forward-backward)

conditional

conditional

latent

derivation

bush  talked  with  sharon

y

latent

derivation

                     
ko n py u :  ta :

parallel text

id68

(liang et al 2006; 

yu et al 2013; 

xiao and xiong 2013)

(knight & graehl, 1998;
kondrak et al 2007, etc.)

argmax(state, size)

alaska

qa pairs

(clark et al 2010;
liang et al 2013;

kwiatkowski et al 2013)

47

logistic 
regression
(maxent)

online+
viterbi

id88
max
margin

id166

crfs

latent crfs

online+
viterbi

structured id88

latent id88

max
margin

structured id166

latent structured id166
48

latent structured id88

    no explicit positive signal
    hallucinate the    correct    derivation by current weights

training example

              (cid:20277)              

x

during online learning...
              (cid:20277)              

x

forced
decoding

space

d   

highest-scoring
gold derivation

highest-scoring 

derivation

beam

  d

the   man   bit    the    dog

y

   

the   dog   bit    the    man z

unconstrained search
    example: id125 phrase-based decoding

bushi yu shalong juxing le huitan

(cid:2)_ _(cid:2)(cid:2)(cid:2)... meeting

(cid:2)_ _ _ _ _ (cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)bush

(cid:2)_ _(cid:2)(cid:2)(cid:2)(cid:1)(cid:1)(cid:1)(cid:1)... talks

???
                    ... shalong
                    ... sharon

(cid:2)_ _(cid:2)(cid:2)(cid:2)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)... talk

w   w +  (x, d   )    (x,   d)

reward
correct

penalize
wrong

(liang et al 2006;

yu et al 2013)

49

                       

50

constrained search

    forced decoding: must produce the exact reference translation

bushi yu shalong juxing le huitan

bush held talks with sharon

(cid:2)_ _(cid:2)(cid:2)(cid:2)... meeting

(cid:2)_ _ _ _ _ (cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)bush

(cid:2)_ _(cid:2)(cid:2)(cid:2)(cid:1)(cid:1)(cid:1)(cid:1)... talks

one gold derivation

(cid:2)_ _(cid:2)(cid:2)(cid:2)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)... talk

gold derivation lattice

held talks

                    ... shalong
                    ... sharon

search errors in decoding

    no explicit positive signal
    hallucinate the    correct    derivation by current weights

training example

              (cid:20277)              

x

during online learning...
              (cid:20277)              

x

forced
decoding

space

d   

highest-scoring
gold derivation

highest-scoring 

derivation

beam

  d

with sharon

the   man   bit    the    dog

y

   

the   dog   bit    the    man z

held

bush

_ _ _ _ _ _

    _ _ _ _ _

talks

with

sharon

    _ _         _

    _ _            

        _            

                       

0

1

2

3

4

5

6

w   w +  (x, d   )    (x,   d)

reward
correct

penalize
wrong

(liang et al 2006;

yu et al 2013)

problem: 

search errors

52

fullsearchspacefullsearchspacesearch error: gold derivations pruned

gold derivation lattice

held talks

with sharon

bush

held

talks

with

sharon

_ _ _ _ _ _

    _ _ _ _ _

    _ _         _

    _ _            

        _            

                       

0

1

2

3

4

5

6

real decoding id125

should address search errors here!

    _ _ _ _ _
_ _ _ _ _    
_ _ _ _     _
_ _    _ _ _

_ _     _     _
_ _        _ _
_         _ _ _
_     _ _     _

_ _ _ _ _ _

0

1

2

_             _ _
_ _            _
        _     _ _
_             _ _
    _ _         _

3

_     _            
    _         _    
    _ _            
    _ _            

fixing search error 1: early update 
    early update (collins/roark   04) when the correct falls off beam
    up to this point the incorrect pre   x should score higher
    that   s a    violation    we want to    x;   proof in (huang et al 2012)
    standard id88 does not guarantee violation
    the correct sequence (pruned) might score higher at the end!
       invalid    update b/c it reinforces the model error

correct

i n c o r r e c t

 
y
l
r
a
e

e
t
a
d
p
u

violation guaranteed: 
incorrect pre   x scores 
higher up to this point

4

5

6

53

model

correct sequence

falls off beam

(pruned)

standard update
(no guarantee!)
54

21

early update w/ latent variable
    the gold-standard derivations are not annotated
    we treat any reference-producing derivation as good

gold derivation lattice

held talks

with sharon

bush

held

talks

with

sharon

_ _ _ _ _ _

    _ _ _ _ _

    _ _         _

    _ _            

        _            

                       

0

1

2

3

4

5

6

correct
i n c o r r e c t

model

 
y
l
r
a
e

e
t
a
d
p
u

violation guaranteed: 
incorrect pre   x scores 
higher up to this point

all correct derivations fall off
stop decoding

 fixing search error 2: max-violation

best in the beam

worst in the beam

w

l

e
d
o
m

d i

y
l
r
a
e

d+
i

d i   

-
x
a
m

n
o

i
t

l

a
o
v

i

d+
|x|

dy
|x|

d
t
s

d 
|x|

l

a
c
o

l

d+
i   

standard update 

is invalid

    early update works but learns slowly due to partial updates
    max-violation: use the pre   x where violation is maximum
       worst-mistake    in the search space
    now extended to handle latent-variable

56

55

21

latent-variable id88

roadmap of techniques 

best in the beam

correct sequence

y
l
r
a
e

worst in the beam

best in the beam

worst in the beam

w

l

e
d
o
m

falls off 
the beam

d i

y
l
r
a
e

d+
i

 
l
l

u

f

)
d
r
a
d
n
a
t
s
(

t
s
e

t

a

l

last valid 
update

invalid
update!

d+
|x|

dy
|x|

d
t
s

d 
|x|

l

a
c
o

l

structured 
id88
(collins, 2002)

latent-variable 

id88

(zettlemoyer and collins, 

2005; sun et al., 2009)

id88 w/ 
inexact search
(collins & roark, 2004;

huang et al 2012)

latent-variable  id88 

w/ inexact search

(yu et al 2013; zhao et al 2014)

-
x
a
m

n
o

i
t

l

a
o
v

i

biggest
violation

d i   

-
x
a
m

n
o
i
t
a
o
v

i

l

d+
i   

standard update 

is invalid

mt

syntactic parsing

id29

id68
58

experiments: discriminative training for mt

    standard update (liang et al   s    bold   ) works poorly
    b/c invalid update ratio is very high (search quality is low)
    max-violation converges faster than early update

best in the beam

d i

d i   

this explains why liang et al    06 failed

std ~    bold   ; local ~    local   

u
e
l
b

26
25
24
23
22
21
20
19
18
17

c e

m a x f o r

early

local

standard

mert

 2

 4

 6

 8  10  12  14  16  18  20

number of iteration

w

l

e
d
o
m

90%

80%

o

i
t

a
r

70%

60%

50%

worst in the beam

y
l
r
a
e

d+
i

d+
|x|

d
t
s

d 
|x|

l

a
c
o

l

-
x
a
m

n
o

i
t

l

a
o
v

i

dy
|x|

d+
i   

standard update 

is invalid

ratio of invalid updates
+non-local feature

standard latent-variable 

id88

 2  4  6  8  10 12 14 16 18 20 22 24 26 28 30
59

beam size

open problems in theory

    latent-variable structured id88:
    does it converge? under what conditions?
    latent-variable structured id88 with inexact search
    does it converge? under what conditions?
x100

r

  

x100
x3012

x111

r

y100

  

y=-1

oracle 
vector

x2000

y=+1

(novikoff, 1962)

#updates    

r2
 2

z     y100

oracle 
vector

(collins, 2002)

60

open problems in theory

    latent-variable structured id88:
    does it converge? under what conditions?
    latent-variable structured id88 with inexact search
    does it converge? under what conditions?
x100

easy to prove but unrealistic

open problems in theory

    latent-variable structured id88:
    does it converge? under what conditions?
    latent-variable structured id88 with inexact search
    does it converge? under what conditions?

easy to prove but unrealistic

ideal situation but hard to prove

+

  

-

r

y100

  

+

  

-

+

  

-

oracle 
vector

(sun et al, 2009)

#updates    

r2
 2

z     y100

oracle 
vector

(collins, 2002)

61

oracle 
vector

(sun et al, 2009)

#updates    

r2
 2

???

62

tutorial outline

    overview of structured learning
    challenges in scalability
    structured id88
    convergence proof
    structured id88 with inexact search

    latent-variable id88
    parallelizing online learning (id88 & mira)

online learning from big data

    online learning has linear-time guarantee
    contrast: popular methods such as id166/crf are superlinear
    but online learning can still be too slow if data is too big
    even with the fastest inexact search (e.g. greedy)
    how to parallelize online learning for big data?

id166

crf

    . . .

1

update

2

update

3

update

4

5

update

6

update

7

update

8

9

update
10
update
11
update
12
update

update

update

13
update
14
update
15
update
16
update

   

63

64

aside: id88 => mira

geometry of 1-best & k-best mira

    id88 is simple but...
    only    aims to       xes one mistake (violation) on each example
    but id170 may have many violations
    may under- or over-correct on a violation
    mira (margin infused relaxation algorithm)
    1-best mira: corrects one violation    just enough    (crammer 03)
    k-best mira: corrects k violations at one time (mcdonald et al 05)

all

 violations

l

e
d
o
m

 
t
n
e
r
r
u
c

exact
1-best

z

 

u

 

(

p

d

x

,

a

t

e

wt+1 =

y

,

z

)

y correct 
label

z = k-best

z2gen(x)

wt     (x, z)

w0:8z2z,w0    (x,y,z) `(y,z)kw0   wtk2

argmin

can we parallelize online learning?

    can we parallelize online learning?
    harder than parallelizing batch (crf)
    losing dependency b/w examples
    each iteration faster, but accuracy lower
    method 1: ipm (mcdonald et al 2010) 
only ~3-4x faster on 10+ cpus
    can we do (a lot) better?

w
search

x
y

z

update weights w 

if y     z

1
update
2
update
3
update
4
update

5
update
6
update
7
update
8
update

9
update
10
update
11
update
12
update

13
update
14
update
15
update
16
update

   

iterative parameter mixing (ipm)

(mcdonald et al 2010)

wt+1 =

1-best mira

w0:8z2z,w0    (x,y,z) `(y,z)kw0   wtk2

argmin

k-best mira

single constraint

=> analytic solution

up to k constraints

=> id76

w

w

w0

oracle

w0

oracle

65

66

method 2: minibatch parallelization
    decode a minibatch in parallel, update in serial

1
update
2
update
3
update
4
update

5
update
6
update
7
update
8
update

9
update
10
update
11
update
12
update

13
update
14
update
15
update
16
update

   

1
2

9
10

5
6

3

4

   

update
11
12

13
14

   

update

7

8

15
16

iterative parameter mixing  (ipm)

(mcdonald et al 2010)

67

minibatch 

(zhao and huang, 2012)

68

method 1: iterative parameter mixingwhy minibatch?

    minibatch also helps in serial mode!
    id88: use average updates within minibatch
       averaging effect    (cf. mcdonald et al 2010)
    easy to prove convergence (still r2/  2)

update

w(k+1) = w(k) +

1

+

lower bound
u    w(k+1) = u    w(k)
axi
u    w(k+1)   k 
kw(k+1)k   k 

u      (x, y, z)
   
margin

(by induction)

1

1

  (x, y, z)

upper bound

axi
axi
kw(k+1)k2 = kw(k)k2 + k
    r2
w(k)   xi
    0

2
a

+

kw(k+1)k2     kr2

  (x, y, z)k2
jensen   s

  (x, y, z)

violation
(by induction)

69

why minibatch?

    minibatch also helps in serial mode!
    id88: use average updates within minibatch
       averaging effect    (cf. mcdonald et al 2010)
    easy to prove convergence (still r2/  2)
    mira: optimization over more constraints
    mira is an online approximation of id166
    minibatch mira: better approximation of id166
    approaches id166 at maximum batch size

minibatch mira
minibatch size

1

pure online mira

all
id166

8x constrains 
in each update

geometry of minibatch mira

load balancing

z2

y3

y2

z3

w

w0

oracle

z1

y1

gold
incorrect

    rearrange each minibatch to minimize wasted time

1
update
2
update
3
update
4
update

5
update
6
update
7
update
8
update

9
update
10
update
11
update
12
update

13
update
14
update
15
update
16
update

   

1

2

9
10

7
8

15

16

3
4

5
6

   
update

11
12

13
14

   
update

3
1

12
9

7
2

10
11

4
6

5
8

   
update

14
15

13
16

   
update

iterative parameter mixing

mcdonald et al 2010

71

minibatch

load balanced minibatch

70

72

experiments

x

y

the   man   bit    the    dog

bit

man

dog

the

the

incremental parsing

the   man   bit    the    dog

dt   nn   vbd  dt   nn

x

y

part-of-speech tagging

the   man   bit    the    dog

                (cid:20277)                

x

y

phrase-based translation

experiment 1: parsing with mira
    id125 incremental parsing (huang et al 2012) with mira
    minibatch learns better even in the serial setting (1 cpu)

t

l

u
o
-
d
e
h
n
o

 

 
y
c
a
r
u
c
c
a

 92.5

 92.25

 92

 91.75

 91.5

 91.25

 91

 90.75

 0

 1

baseline: minibatch=1
minibatch=4
minibatch=24

 4

 3

 2
wall-clock time (hours)

 5

 6

 7

 8

74

parallelized minibatch faster than ipm
    minibatch is much faster than iterative parameter mixing

experiment 2: tagging (id88)

minibatch:

9x on 12 cores

mcdonald et al:
3x on 12 cores

75

76

experiment 3: machine translation
    minibatch leads to 7x speedup on 24 cores

 24

mert

pro-dense

roadmap of techniques 

structured 
id88
(collins, 2002)

latent-variable 

id88

(zettlemoyer and collins, 2005; 

sun et al., 2009)

minibatch

parallelization
(zhao & huang, 2013)

id88 w/ 
inexact search
(collins & roark, 2004;

huang et al 2012)

u
e
l
b

 23

 22

minibatch-24 (24-core)
minibatch-24 (6-core)
minibatch-24 (1-core)
pure online baseline

latent-variable id88 

w/ inexact search & 

parallelization

(yu et al 2013; zhao et al 2014)

 0

 0.5

 1

 1.5

 2

 2.5

 3

 3.5

 4

mt

syntactic parsing

id29

time

77

(zhao et al 2014)

id68
78

final conclusions

annotated references (1)

    online structured learning is simple and powerful
    search ef   ciency is the key challenge
    search errors do interfere with learning
    but we can use violation-   xing id88 w/ inexact search
    we can extend id88 to learn latent structures
    we can parallelize online learning using minibatch

best in the beam

correct sequence

y
l
r
a
e

-
x
a
m

n
o

i
t

l

a
o
v

i

worst in the beam

falls off 
the beam

biggest
violation

)
d
r
a
d
n
a

 
l
l

u

f

t
s
(

t
s
e

t

a

l

last valid 
update

invalid
update!

79

    binary id88
    original: rosenblatt, 1959
    convergence proof: novikoff, 1962
    multiclass id88 (and voted/average id88)
    freund and schapire, 1999
    structured id88 (and inexact search extensions)
    original: collins, 2002 (also contains generalization bounds; proofs 
    early-update: collins and roark, 2004 (but no justi   cation)
    max-violation: huang et al, 2012 (also de   nes violation-   xing 
id88 framework, where early/max-violation are instances)
    hypergraph inexact search: zhang et al, 2013 (cky-style parsing)

mostly verbatim from freund/schapire, 1999)

80

annotated references (2)

annotated references (3)

    latent variable id88 (and inexact search extensions)
    id29: zettlemoyer and collins, 2005
    machine translation: liang et al, 2006
    separability condition: sun et al, 2009
    inexact search: yu et al, 2013
    hiero w/ inexact search: zhao et al, 2014
    mira
    1-best mira: crammer and singer, 2003
    k-best mira: mcdonald et al, 2005

    parallelizing online learning
    iterative parameter mixing: mcdonald et al, 2010
    minibatch: zhao and huang, 2013
    other references
    crf: lafferty et al, 2001
    m3n (structured id166): taskar et al, 2003
    laso: daum   and marcu, 2005
    averaging trick: daum  , 2006, ph.d. thesis

backup slides: convergence

    if data is separable in representation, and exact search

81

83

separation and violation

82

84

what if not separable?
    a weaker theorem, and generalization bounds

proof

86

proof

85

87

