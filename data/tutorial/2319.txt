matrix and tensor

factorization methods for

natural language processing

guillaume bouchard,   jason naradowsky,   sebastian riedel,

tim rockt  schel,   andreas vlachos

computer science department

university college london

id105 in a nutshell

nlp and id105

we already use it!

topic modelling: probabilistic lsa, lda...
id27 learning: id97, glove...
knowledge base population

factorization can improve id170 models:

smoothing of model parameters

id105 supports logical reasoning:

recent work unites factorization and id85 rules

motivation

matrix and tensor factorization

encompass a variety of learning problems
well-studied in ml/stats literature
state-of-the-art performance in many tasks

this tutorial

part 1 

seeing the matrix
id105 basics
non-negative id105
binary id105
inclusion of prior knowledge

this tutorial

part 2 

tensor factorization basics
factorization with a predictive model
collective id105
convexification

outcomes

recognizing potential applications
understanding of the mathematical concepts
clarification of the connections among models
familiarization with existing nlp applications

seeing the matrix

binary classification

train
train

test

label
label

1
1

1
1

0
0

0
0

???

???

features
features

f1,  f3,  f4,  f6
f1,  f3,  f4,  f6

f3,  f6
f3,  f6

f1,  f2,  f5
f1,  f2,  f5

f1,  f2
f1,  f2

f1,  f3,  f4

f2

matrix completion

task1
task1
1
1

task2
task2
label
label
1
1
1
1

feat1
feat1
feat1
feat1
1
1
1
1

feat2
feat2
feat2
feat2
0
0
0
0

feat3
feat3
feat3
feat3
1
1
1
1

feat4
feat4
feat4
feat4
1
1
1

feat5
feat5
feat5
feat5
0
0
0
0

feat6
feat6
feat6
feat6
1
1
1

0
0

1
1

0
0

???
???

???
???

1
1
1
1

0
0
0
0

0
0
0
0

???
???
???
???

???
???
???
???

0
0
0

1
1
1

1
1
1
1

1
1
1
1

0
0
0
0

1
1
1

0
0
0

0
0
0

1
1
1
1

1
1
1

0
0
0
0

1
1
1
1

0
0
0

1
1
1

1
1
1
1

0
0
0
0

0
0
0

1
1
1

0
0
0

0
0

0
0
0

0
0
0
0

0
0
0

0
0
0
0

1
1
1
1

0
0
0
0

1
1
1

0
0
0

0
0
0
0

1
1
1
1

0
0
0
0

0
0
0
0

0
0
0

0
0
0

1
1

1
1
1
1

0
0
0
0

0
0
0
0

0
0
0
0

0
0
0

0
0

0
0

train
train
train
train

test
test
test
test

unlab
unlab
unlab

binary classification
semi-supervised (transductive) learning
id72
missing data

notation

scalars: lower-case letters 
vectors: boldface lower-case letters 

matrices: boldface capital letters 

th row of 
: 
th column of 

:

vector operations

the inner product between vectors 

 and 

the outer product between vectors 

 and 

matrix operations

the inner product for matrices 

 and 

matrix operations

the outer product for matrices 

 and 

matrix operations
the element-wise hadamard product for 

norms

commonly used to define:

id168s (how far we are from our objective)
regularizers (how large we allow our parameters to grow)

element-wise  -norms:

frobenius norm 

norm example

matrix rank

the maximum number of linearly independent columns/rows

for matrix 
 then 

:

if 
else 

 ( full rank )

hint: 

vanilla id105

matrix completion via low-rank

factorization

given 

find 

 and 

so that 
low rank assumption: 

why low-rank

low-rank assumption usually does not hold
reconstruction unlikely to be perfect
if full-rank then perfect reconstruction is trivial: 

key insight:

original matrix exhibits redundancy and noise,
low-rank reconstruction exploits the former to

remove the latter

doc1
doc1
doc1
doc1
doc1
doc1
doc1

doc2
doc2
doc2
doc2
doc2
doc2
doc2

doc3
doc3
doc3
doc3
doc3
doc3
doc3

doc4
doc4
doc4
doc4
doc4
doc4
doc4

doc5
doc5
doc5
doc5
doc5
doc5
doc5

greece
greece
greece
greece
greece
greece
greece

1.00
0.98

0.00
0.04

0.00
  0.02

0.00
  0.02

1.00
1.03

0.32
0.32
0.32

tsipras
tsipras
tsipras
tsipras
tsipras
tsipras
tsipras

2.00
1.96

0.00
0.07

0.00
  0.06

0.00
  0.08

2.00
2.04

0.61
0.61
0.61

  0.61
  0.61
  0.61

  1.23
  1.23
  1.23

germany
germany
germany
germany
germany
germany
germany

0.00
0.14

1.00
0.45

0.00
y
0.75

1.00
1.03

1.00
0.86

0.73
0.73
0.73

v
0.21
0.21
0.21

crisis
crisis
crisis
crisis
crisis
crisis
crisis
0.00
0.17

0.00
0.46

1.00
0.76

1.00
1.04

1.00
0.90

0.75
0.75
0.75

0.19
0.19
0.19

economy
economy
economy
economy
economy
economy
economy

0.00
  0.38

0.00
0.32

1.00
0.58

1.00
0.79

0.00
0.14

0.40
0.40
0.40

0.46
0.46
0.46

document-term matrix 

document-term matrix 
document-term matrix 
document-term matrix 
document-term matrix 
reconstructed 
reconstructed 
document-topic 
document-topic 
document-topic 
document-topic 

rank?/topics?
rank?/topics?

, topic-term 
, topic-term 
, topic-term 
, topic-term 

 
 

 
 

  1.31
  1.31
  1.31

0.22
0.22
0.22

u
0.49
0.49
0.49

0.67
0.67
0.67

  0.94
  0.94
  0.94

0.56
0.56
0.56

0.55
0.55
0.55

0.89
0.89
0.89

1.22
1.22
1.22

1.44
1.44
1.44

 

why low-rank

because we want a model on matrices with:

1.  invariance by permutation of rows and columns

the model depends only on the
singular values

2.  a measure to control complexity

a (pseudo-)norm on the singular
values

3.  fast to compute and memory efficient

a sparsity-inducing norm

three ways to factorize

singular value decomposition (svd)

given 

 there exists

where 

 are orthogonal:

and 

 is diagonal

truncated svd

if we truncate 

 to its 

 largest values

then 

is the rank-  minimizer of 

eckart-young theorem

svd demo

1
2
3
4
5

val  (u,s,v)  =  breezesvd(m)

val  eigen  =  numbers(rowvector(s))
val  (ea,ev)  =  toembeddings(u,s,v,4)

val  m1  =  opacity(matrix(dots(ea,ev)),0,2)

render(seq(opacity(matrix(m),0,2),numbers(rowvector(s)),m1),

(cid:0)

3.41

2.19

1.18

0.44

0.00

mf with stochastic id119

objective: 

: missing values

algorithm:

1.  pick 
2.  gradient steps

 uniformly at random

where 

 is the gradient step size (can be adaptive)

sgd code

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

def  optimizel2(m:  mat,  k:  int,  iters:  int,  alpha:double  =  0.1,  

initscale:double  =  1.0):  (seq[vect],  seq[vect])  =  {
    val  av  =  initialav(k,  m.dim1,  m.dim2,  initscale)

    val  rand  =  new  scala.util.random(0)

    val  a  =  av._1;  val  v  =  av._2
    def  update(i:  int,  j:  int)  =  {

        val  a  =  a(i).copy
        val  v  =  v(j).copy
        val  y  =  a  dot  v

        a(i)  +=  v  *  alpha  *  (m(i,  j)  -  y)
        v(j)  +=  a  *  alpha  *  (m(i,  j)  -  y)
    for  (i  <-  range(0,iters).tolist)  {
        update(rand.nextint(m.dim1),  

                      rand.nextint(m.dim2))

    }

    }
}

    (a,  v)
(cid:0)

val  models  =  for  (i  <-  x.map(_.toint))  yield  optimizel2(m,  k,  i  *  100

val  x  =  range(0,10).map(_.todouble)

val  ysgd  =  models.map(m  =>  l2loss(m,dots(m._1,m._2)))

val  c  =  plot(x,ysgd,"l2-loss  sgd",seq(0.0,10.0),seq(ysvd,ysvd

sgd converges to svd

val  k  =  2  //rank  

val  (u,s,v)  =  breezesvd(m)

val  (ea,ev)  =  toembeddings(u,s,v,k)
val  ysvd  =  l2loss(m,dots(ea,ev))

c.size  =  (500.0  ->  200.0)

c//ml.wolfe.ui.d3plotter.lineplot(c)

0.1)

(cid:0)

1
2
3
4
5
6
7
8
9
10

8
7
6
5
4
3
2
1

l2  loss  sgd
svd

0

2

4

6

8

10

alternating least squares
objective: 

block-coordinate descent:

least square problems     unique solutions!

each step can be parallelized efficiently

id173

objective:

common choices for 

 and 

:

sparsity-inducing 1-norm: 
overfitting-avoiding squared 2-norm: 
1-norm not continuously differentiable
optimization harder but still possible

regularized id45

document-term matrix 

, term-topic matrix 

readability compactness retrieval

document-topic matrix 
   
   
   
   

1-norm    2-norm       
2-norm 1-norm    
1-norm 1-norm    
2-norm 2-norm    

   
   
   
   

wang et al. (2011)

learning id27s

context1
context1

context2
context2

context3
context3

context4
context4

context5
context5

greece
greece

1.00
1.00

0.00
0.00

0.00
0.00

0.00
0.00

1.00
1.00

0.32

tsipras
tsipras

2.00
2.00

0.00
0.00

0.00
0.00

0.00
0.00

2.00
2.00

0.61

  0.61

  1.23

germany
germany

0.00
0.00

1.00
1.00

0.00
0.00

1.00
1.00

1.00
1.00

0.73

0.21

crisis
crisis
0.00
0.00

0.00
0.00

1.00
1.00

1.00
1.00

1.00
1.00

0.75

0.19

economy
economy

0.00
0.00

0.00
0.00

1.00
1.00

1.00
1.00

0.00
0.00

0.40

0.46

0.56

0.55

0.89

1.22

1.44

  1.31

0.22

0.49

0.67

  0.94

implicit mf: skipgram (mikolov et al. 2013)
explicit mf: glove (pennington et al. 2014) and s-ppmi (levy
and goldberg, 2014)

non-negative matrix

factorization

non-negativity

low-rank factors are commonly interpreted as:

topics
image features

http://nimfa.biolab.si/nimfa.examples.orl_images.html

mf is commomly used to find how 

each instance is composed of its parts; 
subtraction is often counter-intuitive

non-negative mf vs mf

greece
greece
greece
greece
greece

tsipras
tsipras
tsipras
tsipras
tsipras

germany
germany
germany
germany
germany

crisis
crisis
crisis
crisis
crisis

economy
economy
economy
economy
economy

1.40
0.56

0.00
0.55

0.00
0.89

0.00
1.22

1.26
1.44

0.07
  1.31

0.50
0.22

0.98
0.49

1.47
0.67

1.07
  0.94

doc1
doc1
doc1
doc1
doc1

doc2
doc2
doc2
doc2
doc2

doc3
doc3
doc3
doc3
doc3

doc4
doc4
doc4
doc4
doc4

doc5
doc5
doc5
doc5
doc5

0.70
0.32

0.08
  0.61

1.41
0.61

0.16
  1.23

0.05
0.73

0.65
0.21

0.03
0.75

0.76
0.19

0.00
0.40

0.53
0.46

document-term matrix
nmf reconstruction
mf reconsruction
rank-4 nmf reconstruction
rank-4 mf reconstruction

definition

find 

 

    and    

 

given 
 

so that 

nmf is additive mixture/(soft) id91

basic nmf algorithm

multiplicative update rules to minimize 

:

recent work have switched to constrained

alternating least squares optimization

lee and seung (2001)

kim and park (2008)

nmf with kl divergence

assume that 

 represents a id203 distribution

minimize the id181:

results in similar multiplicative updates
ensures that 

 and 

 also represent id203 distributions

probabilistic latent semantic analysis

where 

 is a document, 

 a word and   a component.

where 

 and 

probabilistic latent semantic analysis

where 
and 

plsa uses em for parameter estimation and can
converge to different local optima of the same

objective as nmf-kl.

gaussier and goutte (2005)

id44

generative story

for each topic 

draw multinomial over words 

for each document 

draw multinomial over topics 
for each word 

sample a topic assignment 
sample a word 

 from 

 from 

 from 

 from 

given the row-normalized document-term
 that
matrix 

 and 

, find the factors 
reconstruct it.

arora et al (2012)

summary

saw the matrix
refresher on matrix rank and norms
standard approaches to mf
non-negative mf and connections to plsa and lda

binary id105

binary data

worksfor
the

3.56
0

0.88
1

1.59
0

1.66
1

lecturerat
black
1.39
1

0.34
1

2.29
0

1.01
1

affiliatedwith

cat

0.91
1

0.12
0

2.45
1

2.61
0

profat
sat
1.38
0

2.06
0

1.04
1

2.96
0

doc1
(blunsom,oxford)

doc2
(clark,cambridge)

doc3
(riedel,ucl)

doc4
(vlachos,sheffield)

logistic loss

objective

id203 of 
id203 of 
gradient steps (for 

)

equivalent view: generalized pca

1
2
3
4
5

(blunsom,oxford)
(blunsom,oxford)
(blunsom,oxford)
(blunsom,oxford)
(blunsom,oxford)

(clark,cambridge)
(clark,cambridge)
(clark,cambridge)
(clark,cambridge)
(clark,cambridge)

(riedel,ucl)
(riedel,ucl)
(riedel,ucl)
(riedel,ucl)
(riedel,ucl)

(vlachos,sheffield)
(vlachos,sheffield)
(vlachos,sheffield)
(vlachos,sheffield)
(vlachos,sheffield)

val  k  =  2

val  m1  =  opacity(matrix(m),0,1)  +  names  

val  (a,v)  =  optimizel2(m,  k,  1000)

val  m2  =  opacity(matrix(dots(a,v)),0,1)  +  names

render(seq(m1,m2),layout)  

(cid:0)

worksfor
worksfor
worksfor
worksfor
worksfor

  1.55
0.18
0

  1.06
0.26
0

  2.83
0.06
0

  2.99
0.05
0

1.03
1.03
1.03
1.03

1.73
1.73
1.73
1.73

lecturerat
lecturerat
lecturerat
lecturerat
lecturerat

  0.76
0.32
0

1.14
0.76
1

  0.03
0.49
0

  0.18
0.45
0

  0.83
  0.83
  0.83
  0.83

0.33
0.33
0.33
0.33

affiliatedwith
affiliatedwith
affiliatedwith
affiliatedwith
affiliatedwith

  0.26
0.44
0

2.80
0.94
1

1.97
0.88
1

1.82
0.86
1

  2.22
  2.22
  2.22
  2.22

  0.65
  0.65
  0.65
  0.65

profat
profat
profat
profat
profat
  0.11
0.47
0

2.42
0.92
1

1.84
0.86
1

1.73
0.85
1

  1.94
  1.94
  1.94
  1.94

  0.66
  0.66
  0.66
  0.66

0.46
0.46
0.46
0.46

  1.30
  1.30
  1.30
  1.30

  0.50
  0.50
  0.50
  0.50

  0.38
  0.38
  0.38
  0.38

  1.16
  1.16
  1.16
  1.16

0.16
0.16
0.16
0.16

  1.33
  1.33
  1.33
  1.33

  1.50
  1.50
  1.50
  1.50

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

def  optimizelogistic(m:  mat,  k:  int,  iters:  int,  alpha:double  =  0.1

initscale:double  =  1.0):  (seq[vect],  seq[vect])  =  {
    val  av  =  initialav(k,  m.dim1,  m.dim2,  initscale)

    val  rand  =  new  scala.util.random(0)

    val  a  =  av._1;  val  v  =  av._2
    def  update(i:  int,  j:  int)  =  {

        val  a  =  a(i).copy
        val  v  =  v(j).copy

        val  p  =  sigmoid(a  dot  v)

        a(i)  +=  v  *  alpha  *  (m(i,  j)  -  p)
        v(j)  +=  a  *  alpha  *  (m(i,  j)  -  p)
    for  (i  <-  range(0,iters).tolist)  {
        update(rand.nextint(m.dim1),  

                      rand.nextint(m.dim2))

    }

    (a,  v)

    }
}
""
(cid:0)

#slide3

(anna,ucl)
(anna,ucl)
(anna,ucl)
(anna,ucl)

(bob,msr)
(bob,msr)
(bob,msr)
(bob,msr)

(andreas,ucl)
(andreas,ucl)
(andreas,ucl)
(andreas,ucl)

(james,oxford)
(james,oxford)
(james,oxford)
(james,oxford)

(bob,sheffield)
(bob,sheffield)
(bob,sheffield)
(bob,sheffield)

worksfor
worksfor
worksfor
worksfor

lecturerat
lecturerat
lecturerat
lecturerat

professorat
professorat
professorat
professorat

studiedat
studiedat
studiedat
studiedat

0.0

0.0
0.0
0.0

0.0
0.0

0.0
0.0
0.0

0.0

1.0
1.0
1.0
1.0

0.0
0.0
0.0

1.0
1.0
1.0
1.0

0.0
0.0
0.0

1.0
1.0
1.0
1.0

1.0
1.0
1.0
1.0

1.0
1.0
1.0
1.0

1.0
1.0
1.0
1.0

0.0
0.0
0.0

0.0
0.0

0.0
0.0

1.0
1.0
1.0
1.0

1.0
1.0
1.0
1.0

0.0
0.0

0.0
0.0

sign-rank

(linial et al., 2007; bouchard et al., 2015)

sign-rank

(linial et al., 2007; bouchard et al., 2015)

sign-rank

(linial et al., 2007; bouchard et al., 2015)

negative data

application: id36

(riedel et al, 2013)

often only positive data is observed
sample unobserved cells and treat as negative
leads to bias, requires many samples

implicit negative training data

bayesian personalized ranking

(rendle et al, 2009) 

sample during sgd

application: id36

(riedel et al, 2013)

results

(riedel et al, 2013)

inclusion of prior knowledge in

factorization models

drawbacks of openie and embeddings

drawbacks of openie and embeddings

drawbacks of openie and embeddings

drawbacks of openie and embeddings

drawbacks of openie and embeddings

factual prior knowledge

collective matrix factoriztion (section 7)
discriminative factorial models (section 6)

hard constraints
(e.g. chang et al. 2014)

id85 prior knowledge

representation learning: hard to fix mistakes

distant supervision: fails for no or little alignment

pros: formulae are easy to modify and improve
cons: brittle, no generalization and id136 can become
intractable
markov logic networks: often intractable in practice

low-rank logic embeddings

combining logic and id105

combining logic and id105

combining logic and id105

combining logic and id105

combining logic and id105

combining logic and id105

combining logic and id105

combining logic and id105

combining logic and id105

combining logic and id105

combining logic and id105

combining logic and id105

combining logic and id105

combining logic and id105

combining logic and id105

id105 is    injecting    atomic

formulae

training facts are ground atoms:

let [   ] denote mapping from symbolic formulae to
expectations

training objective:

can we do this for any propositional formulae?

differentiable logic formulae

from negation and conjunction we can build any
propositional formula

disjunction:

implication:

jointly maximize the log-likelihood of atomic and
propositional formulae

grounding

grounding based on all observed facts for premise 
and consequent 
sample unobserved facts 
add propositional formulae to the id105
training objective

 and 

zero-shot relation learning

zero-shot relation learning

zero-shot relation learning

zero-shot relation learning

zero-shot relation learning

full dataset

tensor factorization

tensors

in many real-world data more then two variables are
interacting

subject, verb, object triplets 
binary relation triplets 
tag-recommendation: user, item, tag triplets 
user-movie-song-book recommendation: 

2d matrix representation is an unnatural choice
tensors are the higher-order generalization of matrices
can be used to model 

-way interactions

notation

(kolda and bader, 2009)

vectors: boldface lower-case letters 
matrices: boldface capital letters 
tensors: euler script letters 
order-

 tensor is a multidimensional array with n indices

example order-3 tensor

order-3 tensor fibers

 is used to denote the mode-1 column fiber when fixing

mode 2 to j and mode 3 to k

 mode-2 row fiber
 mode-3 tube fiber

order-3 tensor slices

 denotes the horizontal slice when fixing the mode-1 index

to i

 vertical slice
 lateral slice

tensor operations
 tensor-vector product 

mode-

dot product of each mode-

 fiber in 

 with 

resulting tensor has 

 modes

tensor operations

mode-

 tensor-matrix product 

matrix-vector product of each mode-

 fiber in 

 with 

tensor operations

mode-

 matricization 

arrange all mode-

 fibers as columns

tensor rank

rank-  tensor: outer product of three vectors:

rank of a tensor 
up to 

: smallest number of rank-  tensor that add

tucker decomposition

(tucker, 1966) 

given: 

 is called core tensor

models linear interactions between all three variables

, 

 and 

 are called loading matrices

, 

, 

, 

example

 

problem: in practice hard to estimate parameters

sgd expensive for core tensor 
tucker decomposition is not unique
often one mode is kept fixed (tucker2)

tucker2 decomposition

full tucker decomposition is called tucker3

factorizes along all three modes

tucker2 factorizes only along two modes

one of the loading matrices is the identity matrix
for instance: 

example: 

id105 as tucker1

decomposition

tucker1 factorizes only along one mode
is an instance of id105

learns vectors for pairs of variables of two modes
two of the loading matrices are the identity
for instance: 

example:

candecomp/parafac

(carroll and chang, 1970; harshman, 1970)
approximate 

 with a sum of 

 rank-  tensors

loss

rescal

(nickel et al, 2011; 2012) 

rescal

(nickel et al, 2011; 2012)

is an instance of tucker2: 

 holds one slice 

 for each relation 
 is the dictonary-matrix for all entity

embeddings
symmetry assumption: an entity has the same embedding
whether used as first or second argument

per relation view (  slices of 
id168

 and 

): 

optimized using alternating least squares

results

factorization model of semantic

compositionality
(van de cruys et al, 2013)

factorization model of semantic

compositionality
(van de cruys et al, 2013)

captures three-way interaction of subject, verb, object triplets
instance of tucker2 decomposition

 is the dictionary matrix of noun embeddings

 holds slice 

 for each verb 

relationship to neural networks

id105: 

relationship to neural networks

(nickel, 2015)

rescal: 

combining matrix and tensor

factorization

(e.g. chang et al. 2014, singh et al. 2015)

collective id105

arbitrary database factorization
objective: a probabilistic model for any database

example: health data

(cid:0)

modeling two relations

economy

g

g

p

p

e

e

greece

tsipras

germany

greece

crisis

tsipras

economy

germany

d1

d2

d3

d4

d5

d1

d2

d3

d4

d5

crisis
d1

d2

d3

d4

d5

third relation: feature labelling

economy

g

g

p

p

e

e

crisis
d1

d2

d3

d4

d5

greece

tsipras

germany

greece

crisis

tsipras

economy

germany

g

p

e

d1

d2

d3

greece

d4

tsipras

d5

germany

crisis

economy

d1

d2

d3

d4

d5

g

p

e

greece

tsipras

germany

crisis

economy

g

p

e

d1

d2

d3

d4

d5

greece

tsipras

germany

crisis

economy

g

p

e

d1

d2

d3

d4

d5

modeling two relations

assume we have 2 relations:

1.  document - word matrix 
2.  document - label matrix 

representing is a a 2-slices tensor is overly complicated and not

needed. a simple and effective way to represent it is by

concatenating the two matrices into a big 

 matrix:

we can estimate a probabilitic model on 

 by factorizing it.

handles missing data in the input
can work with partially labelled documents
takes advantage of unlabelled data

concatenation

document - word matrix 
document - label matrix 
word - label matrix 

concatenation:

 naive

we can estimate a probabilitic model on 

 by factorizing it. this

additional relation enables feature labelling and increase

prediction capabilities when similar rare words share similar

labels

symmetric block matrix

document - word matrix 
document - label matrix 
word - label matrix 

 a 
 symmetric block-matrix:

let factorize it!

collective id105
 types of entity with embeddings 
we have 

 and we observe relations between all

possible entity types 

general id168s 

:

let's try sgd!

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

      seq("greece","tsipras","germany","crisis","economy"),

val  entitynames  =  seq(

      seq("g","p","e"),

      seq("d1","d2","d3","d4","d5"))
val  n1  =  entitynames(0).length  
val  n2  =  entitynames(1).length  
val  n3  =  entitynames(2).length  

val  tuples31  =  totuples(parsematrix(

    """1  2  0  0  0
          0  0  1  0  0
          0  0  0  1  1
          0  0  1  1  1

    """1  0  0
          0  1  1
          0  1  1
          0  1  0

    """1  1  0  0  0
          1  1  0  1  1

          1  2  1  1  1"""),  n1  +  n2,  0)

val  tuples32  =  totuples(parsematrix(

          1  0  1"""),  n1  +  n2,  n1)

val  tuples21  =  totuples(parsematrix(

          1  1  1  1  1"""),  n1,  0)

val  t1  =  new  table(l2_type,  tuples31)
val  t2  =  new  table(l2_type,  tuples32)
val  t3  =  new  table(l2_type,  tuples21)

(cid:0)

1
2

3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

def  optimizecmf(database:  seq[table],  tables_weights:  seq[double
int,  iters:  int,  step_size:double  =  0.01,  initscale:double  =  1.0):  
        val  u  =  initial_embeddings(num_of_entities(database),  rk,  

seq[vect]  =  {

initscale)  //  n*rank  embedding  matrix

        for  (it  <-  0  until  iters)  {        //  loop  over  sgd  iterations

            val  relation  =  sample_discrete(tables_weights)

            //  sampling  an  observation
            val  table  =  database(relation)

            val  observation  =  sample_seq(table.observations)

            val  dloss  =  loss_gradient(table.loss_type,  v1  dot  v2,  gold)

            val  (i,  j,  gold)  =  observation

            //gradient  step
            val  v1  =  u(i).copy
            val  v2  =  u(j).copy

            u(i)  -=  v2  *  step_size  *  dloss
            u(j)  -=  v1  *  step_size  *  dloss

        u  //output  the  embedding  matrix}

        }
    }

(cid:0)

#result

1
2

val  u  =  optimizecmf0(seq(t1,  t2,  t3),  seq(.3,  .3,  .4),  3,  20000,.02

showcmf(u)

(cid:0)

#generalization

generalization to arbitrary databases

example: augmented multi-view

objective: predict one view given the other

pixel similarity should help. we create a binary matrix linking

pixels.

relation 1: pixel intensity value in the first image (real)
relation 2: pixel intensity value in the second image (real)
relation 3: relative similarity between pixels (binary)

 

example: social media data (flickr)

bayesian modelling

cmf: many id173 parameters.

tuning is painful

bayesian learning: principled automatic tuning

1.  choice of a prior 
2.  find (approximate!) the posterior 

algorithms: - sampling - id150: salakhutdinov et al.,
2008 - hmc: mohamed et al., 2009 - variational id136 -

id58: raiko et al., 2008

- expectation propagation: stern et al., 2009

bayesian id105

independent gaussian priors: 

homoscedastic gaussian likelihood: 

algorithms for bayesian matrix

factorization
block id150

sample 
sample 

 for 
 for 

id58 : fully factorized approximation: 

compute 
compute 

 based on 
 based on 

 and 
 and 

bayesian collective id105

we have 

 types of entity with embeddings 

 and we observe relations between all

possible entity types 

independent gaussian priors: 

per-relation homoscedastic gaussian likelihood: 

algorithms for bayesian cmf

block id150

for each type 

sample 

 for 

id58 : fully factorized approximation: 

 -
now we can maximize the likelihood with respect to 

compute 

 based on 

 and 

 and 

with 

gene expression data experiment

40 patients with breast cancer

views: 2 measurements of 4287 genes.
task: predicting random missing entries

conclusion to collective matrix

factorization

collective id105 and tensor factorization:

learn   more by fusing multiple databases
flexible and generic model for relational data
bayesian learning : automatic tuning of parameters and
complexity

collective id105 vs. tensor factorization:
cmf is easier because it deals only with matrices
cmf = typed data. tensor = multi-relational
augmented multi-view: common setup

discriminative factorial models

discriminative factorial models

factorization is an efficient way of reducing the effective number

of parameters in discriminative/predictive models

we introduce:

factorization machines
multi-task/multi-label learning with id105
id170 with factorized parameters

factorization machines

binary id105 as a id28 matrix 

. for any 

:
 where 

 and 

is equivalent to:

u.c. rank(

) = k

 : standard mf

 called factorization machine [rendle 2010]

 : mf with row and columns bias

reduced rank regression

multiple id75:
d inputs: 
k outputs: matrix 

id75:

parameters: matrix 

reduced rank regression assumes 

id72
inputs: 
outputs: 

 feature 
 labels 

can be interpreted as label embedding: 

 is the 

-dimensional

embedding of the  -th label.

id170

goal: predict 
 is a structured output

 is all possible structures
 is often a vector, but it can be a matrix or tensor.

in nlp 

 is a score function

examples of nlp id170

lei et al.: low-rank tensors for scoring dependency (acl 2014)

standard vector features

high order features

difficulties of manual feature selection

few templates:
poor performance
many templates:

high performance 
useful feature interactions hard to know a priori

 many parameters

alternatives? automatic feature selection:

effective, but ''messy'' (zhao, 2009)
computationally expensive

solution: learn templates from data using id105

step 1: identify simple features
feature vectors for each type of information:

, vector 
, vector 

, vector 

 for head token
for child token

 for arc information

step 2: define composite features

, matrix 
, matrix 
, matrix 

, matrix 

step 3: formulate model parameters

tensor

a tensor 

, describes the concatenation of all three

feature vectors, so replace 

 with tensor 

:

can be huge! a better option? low rank approximation.

calculate a as:

where 

 and 

 are dense low-dimensional representations 

learning

training objective:

non-id76 with id173

can optimize with variant of passive aggressive (crammer,

2006)

learning

online method tailored for tensors [lei et al., 2014]

iterate over data and for each instance, update unary feature
weights 
 and choose one of the feature tensors as follows

(assuming 

 was chosen):

with sub-problem (solvable with closed-form solution):

performance - parsing

rbg + mf model trained with rank 
results averaged over 14 languages (conll data)

 tensors

performance - srl

baselines have many features and automatic feature selection
outperforms previous best system across many languages

conclusion

discriminative models can be parameterized with matrices or
tensors
low-rank asumption enables parameter sharing

id72: corresponds to label-embedding
vanilla id105 can be expressed as a 
dimensional id28 with rank constraint
id170 with low rank parameters can
improve performance over heavily engineered state-of-the-
art systems

-

convexification

convexification
. for any 

matrix 

,

(1) 

(2) 

(3) 

is equivalent to:

which is relaxed into:

(3) is convex, (2) is not

what do we gain by making the problem

convex?

we have access to all the id76 tools!

theoretical guarantees
more importantly, new algorithms

theory

convergence to the global optimum, no need for smart
initialization
speed of convergences
polynomial time guarantees for a fixed accuracy

new algorithms

proximal methods (ista and fista)

frank-wolfe (conjugate gradient) algorithms
augmented lagrangian approaches (admm, splitting
methods)

stochastic variants (e.g. sag)

combining id173s

convex penalties can be added to get better models
sparse plus low-rank

sum of trace norms

e.g.: convex cannonical correlation analysis

very popular in image processing ==> should also be useful in

nlp!

some pointers to convex factorization
2001: fazel, hindi & boyd. "a rank minimization heuristic with
application to minimum order system approximation.
2009: cand  s & recht. "exact matrix completion via convex
optimization.".
2009: wright, ganesh, rao, peng & ma. "robust principal
component analysis".
2009: hsu, kakade and zhang. a spectral algorithm for
learning id48
2011: tamioka & suzu. statistical performance of convex
tensor decomposition.
2013: bouchard, guo & yin. convex collective matrix
factorization.
2014: bailly, carreras & quattoni: unsupervised spectral
learning of finite-state transducers

application: spectral learning in nlp
a promising application of convex factorization: polynomial-

time learning of

id48s
wfsas
grammars
even nmf!

idea: factorization of redundant moment matrices.

example: empirical hankel matrix of a

string

matrix of counts of all possible prefixes and suffixes

summary of convex factorization

trace norm is great

from intractable to tractable problems

spectral learning combines well with trace norm

learning latent variable with computational guarantees

summary

before the break

1.  matrices/tensor = natural concepts for relations
2.  low-rank = natural id173
3.  non-negativity => id91
4.  binary loss: much better than squared loss

after the break

1.  prior knowledge included using logic-based loss models
2.  multi-relational learning

tensors: many different possible factorizations
collective id105 for typed data

3.  discriminative factoriation

reduced rank regression/id72
factorization machines
id170

4.  convexification

the trace norm is a real breakthrough
theoretical guarantees, new algorithms, structured
id173

take-home messages

nlp is about learning relations. low-rank factorization simply

works

in theory: well understood model with nearly linear-time
complexity
in practice: many large scale examples

the real reason why it works:

replaces combinatorics by id202

toolkits

r package for collective id105 (klami)
https://cran.r-project.org/web/packages/cmf
mymedialite (ganter & rendle):
http://mymedialite.net/news/index.html
scikit-tensor (nickel): 
nmf-matlab (yi & ngom):
https://sites.google.com/site/nmftool/
theano (bengio et al.):
http://deeplearning.net/software/theano/
http://www.wolfe.ml
wolfe (riedel et al.): 

http://github.com/mnick/scikit-tensor

used in this presentation via moro by sameer singh

annotated bibliography

kolda, tamara g., and brett w. bader. "tensor decompositions
and applications." siam review 51.3 (2009): 455-500. 
extensive overview of the topic, we adopted their notation in the
tutorial 

goldberg, andrew b., zhu, xiaojin, recht, benjamin, xu, jun-
ming, nowak, robert. "transduction with matrix completion:
three birds with one stone." in advances in neural information
processing systems (nips) 24. 2010. 
first work to formulate semi-supervised id72 with
missing data explicitly as matrix completion 

koren, yehuda, robert bell, and chris volinsky. "matrix
factorization techniques for recommender systems." computer
8 (2009): 30-37. 
description of sgd and als algoirthms for id105

wang, quan, et al. "regularized id45."
proceedings of the 34th international acm sigir conference on
research and development in information retrieval. acm, 2011.
introduces regularized id45 

lee, daniel d., and h. sebastian seung. "algorithms for non-
negative id105." advances in neural information
processing systems (nips) 15. 2001. 
multiplicative updates algorithm for nmf 

kim, hyunsoo, and haesun park. "nonnegative matrix
factorization based on alternating nonnegativity constrained
least squares and active set method." siam journal on matrix
analysis and applications 30.2 (2008): 713-730. 
alternating least squares for nmf

gaussier, eric, and cyril goutte. "relation between plsa and
nmf and implications." proceedings of the 28th annual
international acm sigir conference on research and
development in information retrieval. acm, 2005. 
first paper to point out the connection between plsa and nmf 

arora, sanjeev, rong ge, and ankur moitra. "learning topic
models--going beyond svd." foundations of computer science
(focs), 2012 ieee 53rd annual symposium on. ieee, 2012. 
theoretical justification of id96 as nmf

tucker, ledyard r. "some mathematical notes on three-mode
factor analysis." psychometrika 31.3 (1966): 279-311. 

carroll, j. douglas, and jih-jie chang. "analysis of individual
differences in multidimensional scaling via an n-way
generalization of    eckart-young    decomposition."
psychometrika 35.3 (1970): 283-319. 

harshman, richard a. "foundations of the parafac procedure:
models and conditions for an" explanatory" multi-modal factor
analysis." (1970): 1-84. apa

nickel, maximilian, volker tresp, and hans-peter kriegel. "a
three-way model for collective learning on multi-relational data."
proceedings of the 28th international conference on machine
learning (icml-11). 2011. 

nickel, maximilian, volker tresp, and hans-peter kriegel.
"factorizing yago: scalable machine learning for linked data."
proceedings of the 21st international conference on world wide
web. acm, 2012. apa 

goller, christoph, and andreas kuchler. "learning task-
dependent distributed representations by id26
through structure." neural networks, 1996., ieee international
conference on. vol. 1. ieee, 1996.

koby crammer, ofer dekel, joseph keshet, shia shalev-shwartz,
and yoram singer. "online passive-aggressive
algorithms".jmlr. 2006. 

tao lei, yu xin, yuan zhang, regina barzilay and tommi
jaakkola. " low-rank tensors for scoring dependency
structures."in proceedings of the 52nd annual meeting of the
association for computational linguistics (volume 1: long
papers), pp 1381--1391, baltimore, maryland, june. 

hai zhao, wenliang chen, chunyu kit, guodong zhou.
"multilingual dependency learning: a huge feature engineering
method to semantic id33", conll, 2009.

rendle, steffen. "factorization machines." data mining (icdm),
2010 ieee 10th international conference on. ieee, 2010. 

cohen, s. b., stratos, k., collins, m., foster, d. p., and ungar, l.
2014. spectral learning of latent-variable pid18s: algorithms and
sample complexity. journal of machine learning research. 

bailly, r., carreras, x., luque, f., and quattoni, a. 2013.
unsupervised spectral learning of wid18 as low-rank matrix
completion. 

klami, arto, guillaume bouchard, and abhishek tripathi. "group-
sparse embeddings in collective id105."
proceedings of international conference on learning
representations (iclr) 2014. 

salakhutdinov, ruslan, and andriy mnih. "bayesian probabilistic
id105 using id115."
proceedings of the 25th international conference on machine
learning. acm, 2008. 

mohamed, shakir, zoubin ghahramani, and katherine a. heller.
"bayesian exponential family pca." advances in neural
information processing systems. 2009. 

stern, david h., ralf herbrich, and thore graepel. "matchbox:
large scale online bayesian recommendations." proceedings of
the 18th international conference on world wide web. acm,
2009. 

raiko, tapani, alexander ilin, and juha karhunen. "principal
component analysis for sparse high-dimensional data." neural
information processing. springer berlin heidelberg, 2008.

