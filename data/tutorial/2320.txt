building a phrase-based smt system

building a phrase-based smt system

graham neubig & kevin duh

nara institute of science and technology (naist)

5/10/2012

 

 

1

building a phrase-based smt system

phrase-based

id151 (smt)
    divide sentence into patterns, reorder, combine

today i will give a lecture on machine translation .
today
.
            
   

i will give
               

machine translation

a lecture on

            

         

today
            

machine translation

            

a lecture on

         

i will give
               

.
   

                                                   

 

    statistical translation models, reordering models, 

language models learned from text

 

2

building a phrase-based smt system

this talk

1) what are the steps required to build a phrase-based 

machine translation translation system?

2) what tools implement these steps in moses* (an 

open-source statistical mt system)?

3) what are some research problems related to each of 

these components? 

 

 
* http://www.statmt.org/moses

3

building a phrase-based smt system

steps in training a

phrase-based smt system

    collecting data
    id121
    id38
    alignment
    phrase extraction/scoring
    reordering models
    decoding
    evaluation
    tuning

 

 

building a phrase-based smt system

collecting data

 

 

building a phrase-based smt system

collecting data

    sentence parallel data

    used in: translation model/reordering model

                                 this is a pen.
                                i ate with my friend yesterday.
                                   elephants' trunks are long. 

    monolingual data (in the target language)

    used in: language model

this is a pen.
i ate with my friend yesterday.
elephants' trunks are long. 

 

 

building a phrase-based smt system

good data is

    big!    

y
c
a
r
u
c
c
a
 
n
o
i
t
a
s
n
a
r
t

l

lm data size (million words) [brants 2007]

    clean
    in the same domain as test data

 

 

building a phrase-based smt system

collecting data

    for academic workshops, data is prepared for us!

e.g.
iwslt 2011    

name
ted
news commentary
europarl
un
giga

type
lectures
news
political
political
web

words
1.76m
2.52m
45.7m
301m
576m

    in real systems

    data from government organizations, newspapers
    crawl the web
    merge several data sources

 

 

building a phrase-based smt system

research

    finding bilingual pages [resnik 03]

 

 

[image: mainichi shimbun]

building a phrase-based smt system

research

    finding bilingual pages [resnik 03]
    sentence alignment [moore 02]

 

 

building a phrase-based smt system

research

    finding bilingual pages [resnik 03]
    sentence alignment [moore 02]

    crowd-sourcing data creation [ambati 10]

    mechanical turk, duolingo, etc.

 

 

building a phrase-based smt system

id121

 

 

building a phrase-based smt system

id121

    example: divide japanese into words

                                 

                                             

    example: make english lowercase, split punctuation

taro visited hanako.

taro visited hanako .

 

 

building a phrase-based smt system

tools for id121

    most european languages

tokenize.perl  en  <  input.en  >  output.en
tokenize.perl  fr  <  input.fr  >  output.fr

    japanese

mecab: mecab    o  wakati  <  input.ja  >  output.ja
kytea: kytea    notags  <  input.ja  >  output.ja
juman, etc.

    chinese

stanford segmenter, ldc, kytea, etc...

 

 

building a phrase-based smt system

research

    what is good id121 for machine translation?

    accuracy? consistency? [chang 08]
    matching target language words? [sudoh 11]

                                             

taro <arg1> visited <arg2> hanako .

    morphology (korean, arabic, russian) [niessen 01]

   

   

   

      

   

   

      

   

   

   

    ?

   

?
    unsupervised learning [chung 09, neubig 12]

   

  

 

   
  

   
  

      

   

   

   

   

   

   

   

 

 

building a phrase-based smt system

id38

 

 

building a phrase-based smt system

id38

    assign a id203 to each sentence

e1: taro visited hanako

e2: the taro visited the hanako

lm

e3: taro visited the bibliography

p(e1)

p(e2)

p(e3)

    more fluent sentences get higher id203

p(e1) > p(e2)

p(e1) > p(e3)

 

 

building a phrase-based smt system

id165 models

    we want the id203 of 

p(w =    taro visited hanako   )

    id165 model calculates one word at a time

    condition on n-1 previous words

e.g. 2-gram model

p(w1=   taro   ) * p(w2=   visited    | w1=   taro   )

* p(w3=   hanako    | w2=   visited   )
* p(w4=   </s>    | w3=   hanako   )

 

note:
sentence ending symbol </s>

 

18

building a phrase-based smt system

tools

    srilm toolkit:

train:

ngram  count    order  5    interpolate    kndiscount    unk
            text  input.txt    lm  lm.arpa

test:

ngram    lm  lm.arpa    ppl  test.txt

    others: kenlm, randlm, irstlm

 

 

building a phrase-based smt system

research problems
    is there anything that can beat id165s?

[goodman 01]
    fast to compute
    easy to integrate into decoding
    surprisingly strong

    other methods

    syntactic lms [charniak 03] 
    neural networks [bengio 06]
    model m [chen 09]
    etc...

 

 

building a phrase-based smt system

alignment

 

 

building a phrase-based smt system

alignment

    find which words correspond to each-other

                                       

                                       

taro visited hanako  .

taro visited hanako  .

    done automatically with probabilistic methods

         
         
         
         
         
         
         
         
         
         
         
         

english
english
english
english
english
english
english
english
english
english
english
english

 

p(        |hanako) = 0.99
p(        |taro) = 0.97
p(visited|        ) = 0.46
p(visited|        ) = 0.04
p(        |taro) = 0.0001

 

22

building a phrase-based smt system

ibm/id48 models

    one-to-many alignment model
                      

the hotel front desk

x

x

the hotel front desk

                      

    ibm model 1: no structure (   bag of words   )
    ibm models 2-5, id48: add more structure

 

 

23

building a phrase-based smt system

combining one-to-many alignments

                      

x

the hotel front desk

x

the hotel front desk

                      

combine

the hotel front desk

                      

    several different heuristics
 

 

24

building a phrase-based smt system

tools

    mkcls: find bilingual classes

                      

the hotel front desk

    35     49    12

23    35    12     19

    giza++: find alignments using ibm models (uses 

classes from mkcls for smoothing)
                      

    35  49  12

                      

+

23  35  12  19

the hotel front desk
    symal: combine alignments in both directions
    (included in train-model.perl of moses)

the hotel front desk

 

 

building a phrase-based smt system

research problems

    does alignment actually matter? [aryan 06]
    supervised alignment models [fraser 06, haghighi 09]
    alignment using syntactic structure [denero 07]
    phrase-based alignment models [marcu 02, denero 

08]

 

 

building a phrase-based smt system

phrase extraction

 

 

building a phrase-based smt system

phrase extraction

    use alignments to find phrase pairs

   
         
         

the
hotel
front
desk

                   hotel
                   the hotel
            front desk
                        hotel front desk
                        the hotel front desk

 

 

building a phrase-based smt system

phrase scoring

    calculate 5 standard features

    phrase translation probabilities:

p(f|e) = c(f,e)/c(e)           p(e|f) = c(f,e)/c(f)

e.g. c(               , the hotel) / c(the hotel)

    lexical translation probabilities

    use word-based translation probabilities (ibm model 1)
    helps with sparsity
p(f|e) =   f 1/|e|    e p(f|e)

e.g. 
(p(           |the)+p(           |hotel))/2 * (p(     |the)+p(     |hotel))/2

 

    phrase penalty: 1 for each phrase

 

building a phrase-based smt system

tools

    extract: extract all the phrases
    phrase-extract/score: score the phrases
    (included in train-model.perl)

 

 

building a phrase-based smt system

research 

    id20 of translation models [koehn 07, 

matsoukas 09]

    reducing phrase table size [johnson 07]
    generalized phrase extraction (geppetto toolkit) [ling 

10] 

    phrase sense disambiguation [carpuat 07]

 

 

building a phrase-based smt system

reordering models

 

 

building a phrase-based smt system

lexicalized reordering

    id203 of monotone, swap, discontinuous

                     
                     

mono

disc.

swap

the
thin
man
visited
taro

                 the thin                                     taro
high monotone id203         high swap id203

    conditioning on input/output, left/right, or both

 

 

building a phrase-based smt system

tools

    extract: same as phrase extraction
    lexical-reordering/score: scores lexical reordering
    (included in train-model.perl)

 

 

building a phrase-based smt system

research

    still a very open research area (especially en   ja)
    change the translation model

    hierarchical phrase-based [chiang 07]
    syntax-based translation [yamada 01, galley 06]

    pre-ordering [xia 04, isozaki 10]

f

f'

e

 

        

           

           

        

                       

he

ate

 

rice

building a phrase-based smt system

decoding

 

 

building a phrase-based smt system

decoding

    given the models, find the best answer (or n-best)

model

                  
            

decoder

taro visited hanako               4.5
the taro visited the hanako   3.2
taro met hanako                   2.4
hanako visited taro               -2.9

    exact search is np-hard! [knight 99]
    decoding uses beam-search to find an approximate 

solution [koehn 03]

 

 

building a phrase-based smt system

tools

    moses!

moses    f  moses.ini  <  input.txt  >  output.txt

    also: moses_chart, cdec (for hiero, syntax-based 

models)

 

 

building a phrase-based smt system

research

    decoding for lattice input [dyer 08]
    decoding for syntax models [mi 08]
    minimum bayes risk decoding [kumar 04]
    exact decoding [germann 01]

 

 

building a phrase-based smt system

evaluation

 

 

building a phrase-based smt system

human evaluation

    adequacy: is the meaning correct?
    fluency: is the sentence natural?
    pairwise: is x a better translation than y?

                              

taro visited hanako the taro visited the hanako hanako visited taro

adequate?                          
fluent?          
better?

b, c

 

   
    
c

 

   
   

building a phrase-based smt system

automatic evaluation

    how well does the translation match a reference?

    (or multiple references: more than one correct translation)
    id7: id165 precision, brevity penalty [papineni 03]

reference: taro visited hanako
system: the taro visited the hanako

brevity: min(1, |system|/|reference|) = min(1, 5/3)

1-gram: 3/5
2-gram: 1/4

brevity penalty = 1.0

id7-2 = (3/5*1/4)1/2 * 1.0

     = 0.387        

    also meteor (normalizes synonyms), ter (# of 

changes), ribes (reordering)

 

 

building a phrase-based smt system

research

    metrics with focus on a particular thing

    reordering [isozaki 10]
    accuracy of meaning [lo 11]

    tunable metrics [cer 10]
    metric aggregation [albrecht 07]
    id104 human evaluation [callison-burch 11]

 

 

building a phrase-based smt system

tuning

 

 

building a phrase-based smt system

tuning

    scores of translation, reordering, and language models

    taro visited hanako
     the taro visited the hanako
     hanako visited taro

 

lm       tm      rm
-4
-1
-1
-5
-2
-2

-3
-4
-3

    if we add weights, we can get better answers:

    taro visited hanako
     the taro visited the hanako
     hanako visited taro

 

lm       tm      rm
-1
-4
-5
-1
-2
-2

0.3*
0.3*
0.3*

0.5*
0.5*
0.5*

-3
-4
-3

0.2*
0.2*
0.2*

-8
-10
-7 best

score    

best
score    

-2.2
-2.7
-2.3

 

    tuning finds these weights: wlm=0.2 wtm=0.3 wrm=0.5

 

building a phrase-based smt system

tuning methods

    minimum error rate training: mert [och 03]

source (dev)

                              

decode

n-best (dev)

the taro visited the hanako

hanako visited taro
taro visited hanako

...

model

weights

find better

weights

reference (dev)
taro visited hanako

    others: mira [watanabe 07] (online update), pro 

 

(ranking) [hopkins 11]

 

building a phrase-based smt system

research

    tuning with millions of features (e.g. mira, pro)
    tuning with lattices [macherey 08]
    speeding up tuning [suzuki 11]
    tuning with multiple metrics [duh 12]

 

 

building a phrase-based smt system

last words

 

 

building a phrase-based smt system

last words

    mt is fun! join us.
    improving very quickly, but still many problems.
    system is big, but you can focus on one problem.

thank you

mt

 

 

                              
danke
      
gracias
   
   
terima kasih

   

   

   

building a phrase-based smt system

bibliography

 

 

building a phrase-based smt system

   

   

   

   

   

   

j. albrecht and r. hwa. a re-examination of machine learning approaches for sentence-level mt evaluation. 
in proc. acl, pages 880-887, 2007.
v. ambati, s. vogel, and j. carbonell. active learning and id104 for machine translation. proc. 
lrec, 7:2169-2174, 2010.
n. ayan and b. dorr. going beyond aer: an extensive analysis of word alignments and their impact on mt. 
in proc. acl, 2006.
y. bengio, h. schwenk, j.-s. sencal, f. morin, and j.-l. gauvain. neural probabilistic language models. in 
innovations in machine learning, volume 194, pages 137-186. 2006.
t. brants, a. c. popat, p. xu, f. j. och, and j. dean. large language models in machine translation. in proc. 
emnlp, pages 858-867, 2007.
c. callison-burch, p. koehn, c. monz, and o. zaidan. findings of the 2011 workshop on statistical machine 
translation. in proc. wmt, pages 22-64, 2011.

    m. carpuat and d. wu. how phrase sense disambiguation outperforms id51 for 

id151. in proc. tmi, pages 43-52, 2007.
d. cer, c. manning, and d. jurafsky. the best lexical metric for phrasebased statistical mt system 
optimization. in naacl hlt, 2010.
p.-c. chang, m. galley, and c. d. manning. optimizing chinese id40 for machine translation 
performance. in proc. wmt, 2008.
e. charniak, k. knight, and k. yamada. syntax-based language models for id151. in 
mt summit ix, pages 40-46, 2003.
s. chen. shrinking exponential language models. in proc. naacl, pages 468-476, 2009.
d. chiang. hierarchical phrase-based translation. computational linguistics, 33(2), 2007.
t. chung and d. gildea. unsupervised id121 for machine translation. in proc. emnlp, 2009.
j. denero, a. bouchard-c^ote, and d. klein. sampling alignment structure under a bayesian translation 
model. in proc. emnlp, 2008.
j. denero and d. klein. tailoring word alignments to syntactic machine translation. in proc. acl, volume 45, 
2007.
k. duh, k. sudoh, x. wu, h. tsukada, and m. nagata. learning to translate with multiple objectives. in proc. 
acl, 2012.
c. dyer, s. muresan, and p. resnik. generalizing word lattice translation. in proc. acl, 2008.

 

   

   

   

   

   

   

   

   

   

   

 

   

a. fraser and d. marcu. semi-supervised training for statistical word alignment. in proc. acl, pages 769-
776, 2006.

building a phrase-based smt system

    m. galley, j. graehl, k. knight, d. marcu, s. deneefe, w. wang, and i. thayer. scalable id136 and 

   

   

   

training of context-rich syntactic translation models. in proc. acl, pages 961-968, 2006.
u. germann, m. jahr, k. knight, d. marcu, and k. yamada. fast decoding and optimal decoding for machine 
translation. in proc. acl, pages 228-235, 2001.
j. t. goodman. a bit of progress in id38. computer speech & language, 15(4), 2001.
a. haghighi, j. blitzer, j. denero, and d. klein. better word alignments with supervised itg models. in proc. 
acl, 2009.

    m. hopkins and j. may. tuning as ranking. in proc. emnlp, 2011.

   

   

   

   

h. isozaki, t. hirao, k. duh, k. sudoh, and h. tsukada. automatic evaluation of translation quality for distant 
language pairs. in proc. emnlp, pages 944-952, 2010.
h. isozaki, k. sudoh, h. tsukada, and k. duh. head nalization: a simple reordering rule for sov languages. in 
proc. wmt and metricsmatr, 2010.
j. h. johnson, j. martin, g. foster, and r. kuhn. improving translation quality by discarding most of the 
phrasetable. in proc. emnlp, pages 967-975, 2007.
k. knight. decoding complexity in word-replacement translation models. computational linguistics, 25(4), 
1999.
p. koehn, f. j. och, and d. marcu. statistical phrase-based translation. in proc. hlt, pages 48-54, 2003.
p. koehn and j. schroeder. experiments in id20 for id151. in proc. 
wmt, 2007.
s. kumar and w. byrne. minimum bayes-risk decoding for id151. in proc. hlt, 2004.
    w. ling, t. lus, j. graca, l. coheur, and i. trancoso. towards a general and extensible phrase-extraction 

   

   

   

algorithm. in m. federico, i. lane, m. paul, and f. yvon, editors, proc. iwslt, pages 313-320, 2010.
c.-k. lo and d. wu. meant: an inexpensive, high-accuracy, semiautomatic metric for evaluating translation 
utility based on semantic roles. in proc. acl, pages 220-229, 2011.

    w. macherey, f. och, i. thayer, and j. uszkoreit. lattice-based minimum error rate training for statistical 

machine translation. in proc. emnlp, 2008.
d. marcu and w. wong. a phrase-based, joint id203 model for id151. in proc. 
emnlp, 2002.

 

   

   

 

building a phrase-based smt system

   

   

   

s. matsoukas, a.-v. i. rosti, and b. zhang. discriminative corpus weight estimation for machine translation. 
in proc. emnlp, pages 708717, 2009.
h. mi, l. huang, and q. liu. forest-based translation. in proc. acl, pages 192-199, 2008.
r. moore. fast and accurate sentence alignment of bilingual corpora. machine translation: from research 
to real users, pages 135-144, 2002.

    g. neubig, t. watanabe, s. mori, and t. kawahara. machine translation without words through substring 

alignment. in proc. acl, jeju, korea, 2012.
s. niessen, h. ney, et al. morpho-syntactic analysis for reordering in id151. in proc. 
mt summit, 2001.
f. j. och. minimum error rate training in id151. in proc. acl, 2003.
k. papineni, s. roukos, t. ward, and w.-j. zhu. id7: a method for automatic evaluation of machine 
translation. in proc. coling, pages 311-318, 2002.
p. resnik and n. a. smith. the web as a parallel corpus. computational linguistics, 29(3):349-380, 2003.
j. suzuki, k. duh, and m. nagata. distributed minimum error rate training of smt using particle swarm 
optimization. in proc. ijcnlp, pages 649-657, 2011.
t. watanabe, j. suzuki, h. tsukada, and h. isozaki. online largemargin training for statistical machine 
translation. in proc. emnlp, pages 764-773, 2007.
f. xia and m. mccord. improving a statistical mt system with automatically learned rewrite patterns. in proc. 
coling, 2004.
k. yamada and k. knight. a syntax-based statistical translation model. in proc. acl, 2001.

   

   

   

   

   

   

   

   

    o. f. zaidan and c. callison-burch. id104 translation: professional quality from non-professionals. 

in proc. acl, pages 1220-1229, 2011.

 

 

