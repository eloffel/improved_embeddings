deep learning for nlp:

word vectors and lexical semantics 

ed grefenstette
etg@google.com

how to represent words

natural language text = sequences of discrete symbols (e.g. words).

naive representation: one hot vectors in r|vocabulary| (very large).

classical ir: document and query vectors are superpositions of word vectors.

similarly for word classification problems (e.g. document classification).

issues: sparse, orthogonal representations, semantically weak.

how to represent words

we want richer representations expressing semantic similarity.

id65:
"you shall know a word by the company it keeps."     j.r. firth (1957)

idea: produce dense vector representations based on the context/use of words.

three main approaches: count-based, predictive, and task-based.

count-based methods

define a basis vocabulary c of context words.

define a word window size w.

count the basis vocabulary words occurring w words to the left or right of each 
instance of a target word in the corpus.

form a vector representation of the target word based on these counts.

count-based methods

... and the cute kitten purred and then ...
... the cute furry cat purred and miaowed ...
... that the small kitten miaowed and she ...
... the loud furry dog ran and bit ...

example basis vocabulary: {bit, cute, furry, loud, miaowed, purred, ran, small}.

kitten context words: {cute, purred, small, miaowed}.
cat context words: {cute, furry, miaowed}.
dog context words: {loud, furry, ran, bit}.

count-based methods

... and the cute kitten purred and then ...
... the cute furry cat purred and miaowed ...
... that the small kitten miaowed and she ...
... the loud furry dog ran and bit ...

example basis vocabulary: {bit, cute, furry, loud, miaowed, purred, ran, small}.

count-based methods

use inner product or cosine as similarity kernel. e.g.:

reminder:

cosine has the advantage that it's a norm-invariant metric.

count-based methods

not all features are equal: we must distinguish counts that are high because the 
are informative from those that are just independently frequent contexts.

many normalisation methods: tf-idf, pmi, etc.

some remove the need for norm-invariant similarity metrics.

but... perhaps there are easier ways to address this problem of count-based 
methods (and others, e.g. choice of basis context).

neural embedding models

learning count based vectors produces an embedding matrix in r|vocab|  |context|:

rows are word vectors, so we can retrieve them with one hot vectors in {0,1}|vocab|:

symbols = unique vectors. representation = embedding symbols with e.

neural embedding models

(one) generic idea behind embedding learning:
1. collect instances ti     inst(t) of a word t of vocab v.
2. for each instance, collect its context words c(ti ) (e.g. k-word window).
3. define some score function score(ti , c(ti );   , e) with upper bound on output.
4. define a loss:

5. estimate:

6. use the estimated e as your embedding matrix.

neural embedding models

scoring function matters! 
easy to design a useless scorer (e.g. ignore input, output upper bound).
ideally, scorer:
    embeds ti with e (obviously).
    produces a score which is a function of how well ti is accounted for by c(ti ), 
    requires the word to account for the context (or the reverse) more than 

and/or vice versa.

another word in the same place.

    produces a loss which is differentiable w.r.t.    and e.

neural embedding models: c&w (collobert et al. 2011)

embed all words in a sentence with e.

shallow convolution over embeddings.

mlp projects output of convolution to a 
scalar score.

convolutions and mlp are parameterised by 
a set of weights   .

overall network models a function over 
sentences s: g  ,e(s) = f  (embede(s))

neural embedding models: c&w (collobert et al. 2011) 

what prevents the network from ignoring 
input and outputting high scores?

during training, for each sentence s we 
sample a distractor sentence z by randomly 
corrupting words of s.

minimise hinge loss:

neural embedding models: c&w (collobert et al. 2011)

interpretation: representations carry 
information about what neighbouring 
representations should look like.

a lot like the distributional hypothesis?

a sensible model but:
fairly deep, so not cheap to train.
convolutions capture very local information.

neural embedding models: cbow (mikolov et al. 2013)

embed context words. add them.
project back to vocabulary size. softmax.

minimize negative log likelihood:

neural embedding models: cbow (mikolov et al. 2013)

all linear, so very fast. basically a cheap way 
of applying one matrix to all inputs.

historically, negative sampling used instead 
of expensive softmax.

nll minimisation is more stable and is fast 
enough today.

variants: position specific matrix per input 
(ling et al. 2015).

neural embedding models: skip-gram (mikolov et al. 2013)

target word predicts context words.

embed target word.

project into vocabulary. softmax.

learn to estimate likelihood of context words.

neural embedding models: skip-gram (mikolov et al. 2013)

fast: one embedding versus |c| embeddings.

just read off probabilities from softmax.

similar variants to cbow possible: position 
specific projections.

trade off between efficiency and more structured 
notion of context.

comparison with count-based models

count based and objective-based models: same general idea.

id97 == pmi id105 of count based models 
(levy and goldberg, 2014)

count based and most neural models have equivalent performance when 
properly hyperoptimized (levy et al. 2015)

specific benefits of neural approaches

easy to learn, especially with good id202 libraries.

highly parallel problem: minibatching, gpus, distributed models.

can predict other discrete aspects of context (dependencies, pos tags, etc). can 
estimate these probabilities with counts, but sparsity quickly becomes a problem.

can predict/condition on continuous contexts: e.g. images.

evaluating word representations

intrinsic evaluation:

    wordsim-353 (finkelstein et al. 2003)
    siid113x-999 (hill et al. 2016, but has been around since 2014)
    word analogy task (mikolov et al. 2013), queen = king - man + woman
    embedding visualisation (nearest neighbours, id167 projection)

source: http://nlp.yvespeirsman.be/blog/visualizing-word-embeddings-with-tsne/

source: http://colah.github.io/posts/2014-07-nlp-id56s-representations/

evaluating word representations

extrinsic evaluation:

    simply: do your embeddings improve performance on other task(s).
    more on this later...

task-based embedding learning

just saw methods for learning e through minimising a loss.

one use for e is to get input features to a neural network from words.

neural network parameters are updated using gradients on loss l(x, y,   ):

if e        then this update can modify e (if we let it):

task-based embedding learning

we can therefore directly train embeddings jointly with the parameters of the 
network which uses them.

general intuition: learn to classify/predict/generate based on features, but also 
the features themselves.

embeddings matrix can be learned from scratch, or initialised with pre-learned 
embeddings (fine-tuning).

task-based features: bow classifiers

we want to classify sentences/documents based on a variable number of word 
representations.

simplest option: bag of vectors.

projection into logits (input to softmax) can be arbitrarily complex. e.g.:

task-based features: bow classifiers

simple to implement and train.

example tasks:
    id31 (e.g. tweets, movie reviews).
    document classification (e.g. 20 newsgroups)
    author identification, etc...

we learn task-specific features: e.g. notion of positive/negative words in 
id31.

we can think of word meaning as being grounded in the task.

task-based features: bow classifiers

but, no notion of words in context (ambiguity, polysemy).

cannot capture saliency of individual words. everything contributes to the 
decision, so more words = more noise.

grounding in classification tasks can yield quite shallow semantics. e.g.:
    there is more to the word "good" than the sentiment expressed.
    there is more to "cpu" than the fact that it predicts the topic "computer".

task-based features: bilingual features

simple objectives can yield better grounding for word representations.

example: recognising cross-lingual sentence alignment based on word vectors 
(hermann and blunsom 2014). consider a dataset of english sentences and their 
german translations, d={(ei, gi)}i   |d| 

we want to produce representations ei and gi of the english and german sentence 
such the similarity between the vectors is maximised.

we use this objective to train embedding matrices ee and eg, for english and 
german words.

task-based features: bilingual features

sentence representations are produced with a simple composition function. you 
could do bag of words, or some aspect of word order, e.g.

an obvious loss would be:

obvious degenerate solution to this objective is:

task-based features: bilingual features

so we avoid the degenerate case with a noise-contrastive margin-based loss:

sample a random german sentence per data point as noise. impose the 
constraint that the margin of similarity between a paired pair of sentences and an 
unpaired pair of sentences be at least m (some hyperparameter).

intuition: aligned sentences share high-level meaning, so embeddings should 
reflect the high-level meaning in order to minimise loss.

task-based features: other models

these models are all very simple (this is not a bad thingtm). there are many other 
options.

how do we capture the relation between words? disambiguation? the context 
they occur in? how do we use these embeddings effectively?

this is a recurring topic for the rest of this course.

task-based features: interpretation

task-based embeddings capture information salient to the task. again, no 
guarantee this will capture "general" meaning beyond features useful for the task.

this can be overcome by using a multi-task objective but this comes with its own 
difficulties.

alternatively, embeddings can be pretrained and fixed, relying on task-specific 
projections into the network, but is the pretraining objective general enough?

e.g. it might project "cat" and "kitten" into a similar part of the embedding space, 
but a task might need to radically differentiate these concepts.

final words

learning and re-using word vectors is a form of id21. it is particularly 
useful if you have little task-specific training data, or poor coverage of the 
vocabulary (in which case you might not want to fine-tune embeddings).

generally speaking, if you have enough training data (and vocabulary coverage) 
you will benefit from training embeddings on the task, at the cost of reusability.

take home message of this lecture:
inputs to neural networks over text are embeddings.
we can train them separately, within a particular task, or both.

thank you

credits

oxford machine learning and computational linguistics groups

deepmind natural language research group

