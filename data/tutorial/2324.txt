deep learning for 
natural language 

processing

practicals overview

chris dyer

chris dyer

deepmind
where do i come from?

carnegie mellon

phd linguistics (u of maryland, usa, 2010)
postdoc cs (carnegie mellon, 2010   2012)
faculty cs (carnegie mellon, 2012   )
deepmind (2016   )

chris dyer

deepmind
where do i come from?

carnegie mellon

phd linguistics (u of maryland, usa, 2010)
postdoc cs (carnegie mellon, 2010   2012)
faculty cs (carnegie mellon, 2012   )
deepmind (2016   )
what do i work on?

linguistic structure in statistical/deep models
   better generalization, better sample complexity, more consistent in 
more languages 

discovering linguistic structure/units in data
   what kinds of linguistic knowledge can be inferred from the data? 
machine translation, (syntactic|semantic) parsing, representation   
learning, morphology, user-generated content, generation    

practicals

ground rules

    one introductory practical (more in a minute) 

    turn in at beginning of week 3 or at the end of term 

    then, a big multi-part practical that you can do over the course of the term 

       choose your own adventure    

    we provide a list of projects that you can choose from based on your 

interests (subject to some constraints) 

    you work at your own pace throughout the course 

    one report submitted at the end of term 

    each student responsible for their own work

practical topics i

deep learning for nlp
    perceiving and representing text (and speech):    percepts    vs. 

   features    

    text categorisation (   text cat   ) 
    id86 

    language modelling 
    conditional language modelling 

    conditional on a representation of context, generate 

appropriate text 

    examples: id103, id134 

perceiving text

deep learning for nlp
    will you have the same    perceptual units    at test time as you have at 

training time?
    how do you handle unknown words? 
    do you tokenize (segment) text into tokens? what are tokens? is    new york 

city    one token? or three? 

    do you lowercase? attempt to correct spelling? 

    do you just treat text as a stream of characters? bytes? 

    trade offs 

    smaller perceptual units need more complex models, but have fewer oovs 

    larger perceptual units can get away with simpler models, but oovs are a 

problem

perceiving text

deep learning for nlp
    will you have the same    perceptual units    at test time as you have at 

training time?
    how do you handle unknown words? 
    do you tokenize (segment) text into tokens? what are tokens? is    new york 

city    one token? or three? 

    do you lowercase? attempt to correct spelling? 

    do you just treat text as a stream of characters? bytes? 

    trade offs 

    smaller perceptual units need more complex models, but have fewer oovs 

    larger perceptual units can get away with simpler models, but oovs are a 

problem

singletons

34.3%

70.0%

   
2.7

1.5

wsj
twitter

log f (w) = log c       log(r(w)   b)

representing text

    once we   ve engineered some percepts we can turn to    
    representation learning 

    we want to compute representations of text by using it to predict 

something 

    as we saw in part i of today   s lecture, context is a pretty good 

representation of the lexical semantics of words 

   

in practical 1
    you will explore representation learning based on predicting 

contexts 

    to do so, you will need to establish the vocabulary you operate on

practical topics i

deep learning for nlp
    perceiving and representing text (and speech):    percepts    vs. 

1
al 
tic
c
a
r

{p

   features    

    text categorisation (   text cat   ) 
    id86 

    language modelling 
    conditional language modelling 

    conditional on a representation of context, generate 

appropriate text 

    examples: id103, id134 

practical topics i

deep learning for nlp
    perceiving and representing text (and speech):    percepts    vs. 

   features    

    text categorisation (   text cat   ) 
    id86 

    language modelling 
    conditional language modelling 

    conditional on a representation of context, generate 

appropriate text 

    examples: id103, id134 

m
e

ainin

1
al 
tic
c
a
r

{p

al
tic
c
a
r
p
 
g

{r

practical topics i

deep learning for nlp
    perceiving and representing text (and speech):    percepts    vs. 

   features    

    text categorisation (   text cat   ) 
    id86 

    language modelling 
    conditional language modelling 

    conditional on a representation of context, generate 

appropriate text 

    examples: id103, id134 

m
e

ainin

1
al 
tic
c
a
r

{p

al
tic
c
a
r
p
 
g

{r

practical topics ii

deep learning for nlp
    natural language understanding

    conditional id38 applications (+id86) 
    translation, summarisation, conversational agents 

    following instructions 

    id53, structured knowledge-base population 
    dialogue 

    analytic applications 

    id96 
    linguistic analysis (discourse, semantics, syntax, morphology)

m
e

ainin

al
tic
c
a
r
p
 
g

{r

practical topics ii

deep learning for nlp
    natural language understanding

    conditional id38 applications (+id86) 
    translation, summarisation, conversational agents 

    following instructions 

    id53, structured knowledge-base population 
    dialogue 

    analytic applications 

    id96 
    linguistic analysis (discourse, semantics, syntax, morphology)

m
e

ainin

al
tic
c
a
r
p
 
g

{r

practicals

deep learning for nlp
    we are going to work with a single dataset that lets us 

explore many of these high level themes 

    you will have the opportunity to transform a single 

dataset into a lot of different problems 

    this is an important skill in real-world ml 

    data has lots of value if you can imagine 

interesting things to do with it 

    outside of intro ml courses

dataset

deep learning for nlp

dataset

deep learning for nlp

    appealing properties 

    talks on many different topics but with a similar style 

    enough data to learn interesting things, small enough to 

work on limited computational resources 

    interesting data of data associated with each talk 

    topic labels, titles, summaries, topic labels, video, video 

alignments, translations into  

    who doesn   t want to hear a computer-generated ted talk?

dataset

deep learning for nlp

  <head>
    <url>http://www.ted.com/talks/rajiv_maheswaran_the_math_behind_basketball_s_wildest_moves</url>
    <keywords>talks, math, sports, technology, visualizations</keywords>
    <speaker>rajiv maheswaran</speaker>
    <videourl>http://download.ted.com/talks/rajivmaheswaran_2015.mp4</videourl>
    <date>2015/03/17</date>
    <title>rajiv maheswaran: the math behind basketball's wildest moves</title>
    <description>ted talk subtitles and transcript: basketball is a fast-moving game of improvisation, 
contact and, ahem, spatio-temporal pattern recognition. rajiv maheswaran and his colleagues are analyzing 
the movements behind the key plays of the game, to help coaches and players combine intuition with new 
data. bonus: what they're learning could help us understand how humans move everywhere.</description>
    <transcription>
      <seekvideo id="954">my colleagues and i are fascinated by the science of moving dots.</seekvideo>
      <seekvideo id="4927">so what are these dots?</seekvideo>
      <seekvideo id="6101">well, it's all of us.</seekvideo>
      <seekvideo id="7412">and we're moving in our homes, in our offices, as we shop and travel</seekvideo>
      . . . 

dataset

deep learning for nlp

    the ted dataset contains 2,085 talks, each containing: 

    multiple topic labels (e.g.,    talks, business, creativity, curiosity, goal-setting, innovation, 

motivation, potential, success, work   ) 

    a title (12,766 words in all titles, excluding author names, which are always included in 

them) 

    a brief content summary (109,880 words in all summaries) 
    a content transcript with alignments to frames of the video recording (574,794 aligned 

segments) 

    differences between subsequent video segments video frames give durations 

    total content length is 5,201,252 tokens when tokenized 
    annotations for applause and laughter 
    translations into over 100 languages (although only a few languages have all talks)

dataset

deep learning for nlp

talk lengths (words)

# of topic labels

(most talks are 2k-3k words)

(most talks have 4 keywords)

practical topics i

deep learning for nlp
    perceiving and representing text (and speech):    percepts    vs. 

   features    

    text categorisation (   text cat   ) 
    id86 

    language modelling 
    conditional language modelling 

    conditional on a representation of context, generate 

appropriate text 

    examples: id103, id134 

m
e

ainin

1
al 
tic
c
a
r

{p

al
tic
c
a
r
p
 
g

{r

practical topics i

deep learning for nlp
    perceiving and representing text (and speech):    percepts    vs. 

build id27s from ted talks

   features    

    text categorisation (   text cat   ) 
    id86 

    language modelling 
    conditional language modelling 

    conditional on a representation of context, generate 

appropriate text 

    examples: id103, id134 

m
e

ainin

1
al 
tic
c
a
r

{p

al
tic
c
a
r
p
 
g

{r

practical topics i

deep learning for nlp
    perceiving and representing text (and speech):    percepts    vs. 

build id27s from ted talks

   features    

    text categorisation (   text cat   ) 
    id86 

predict talk labels

    language modelling 
    conditional language modelling 

    conditional on a representation of context, generate 

appropriate text 

    examples: id103, id134 

m
e

ainin

1
al 
tic
c
a
r

{p

al
tic
c
a
r
p
 
g

{r

practical topics i

deep learning for nlp
    perceiving and representing text (and speech):    percepts    vs. 

build id27s from ted talks

   features    

    text categorisation (   text cat   ) 
    id86 

    language modelling 
    conditional language modelling 

predict talk labels
generate random ted talks

    conditional on a representation of context, generate 

appropriate text 

    examples: id103, id134 

m
e

ainin

1
al 
tic
c
a
r

{p

al
tic
c
a
r
p
 
g

{r

practical topics i

deep learning for nlp
    perceiving and representing text (and speech):    percepts    vs. 

build id27s from ted talks

   features    

    text categorisation (   text cat   ) 
    id86 

    language modelling 
    conditional language modelling 

predict talk labels
generate random ted talks

    conditional on a representation of context, generate 

generate a ted talks about a topic

appropriate text 

    examples: id103, id134 

m
e

ainin

1
al 
tic
c
a
r

{p

al
tic
c
a
r
p
 
g

{r

practical topics ii

deep learning for nlp
    natural language understanding

    conditional id38 applications (+id86) 
    translation, summarisation, conversational agents 

    following instructions 

    id53, structured knowledge-base population 
    dialogue 

    analytic applications 

    id96 
    linguistic analysis (discourse, semantics, syntax, morphology)

m
e

ainin

al
tic
c
a
r
p
 
g

{r

practical topics ii

deep learning for nlp
    natural language understanding

    conditional id38 applications (+id86) 
    translation, summarisation, conversational agents 

build a ted talk translator

    following instructions 

    id53, structured knowledge-base population 
    dialogue 

    analytic applications 

    id96 
    linguistic analysis (discourse, semantics, syntax, morphology)

m
e

ainin

al
tic
c
a
r
p
 
g

{r

practical topics ii

deep learning for nlp
    natural language understanding

    conditional id38 applications (+id86) 
    translation, summarisation, conversational agents 

build a ted talk translator

generate summaries from ted talks

    following instructions 

    id53, structured knowledge-base population 
    dialogue 

    analytic applications 

    id96 
    linguistic analysis (discourse, semantics, syntax, morphology)

m
e

ainin

al
tic
c
a
r
p
 
g

{r

practical topics ii

deep learning for nlp
    natural language understanding

    conditional id38 applications (+id86) 
    translation, summarisation, conversational agents 

build a ted talk translator

generate summaries from ted talks

    following instructions 

    id53, structured knowledge-base population 
    dialogue 

    analytic applications 

    id96 
predict speaking for sentences
    linguistic analysis (discourse, semantics, syntax, morphology)

m
e

ainin

al
tic
c
a
r
p
 
g

{r

practical topics ii

deep learning for nlp
    natural language understanding

    conditional id38 applications (+id86) 
    translation, summarisation, conversational agents 

build a ted talk translator

generate summaries from ted talks

    following instructions 

    id53, structured knowledge-base population 
    dialogue 

    analytic applications 

    id96 
predict speaking for sentences
    linguistic analysis (discourse, semantics, syntax, morphology)

when will the audience laugh?

m
e

ainin

al
tic
c
a
r
p
 
g

{r

software in practicals

software toolkits

    deep learning operate on basic features with complex models that consist of 

common components stacked together 

    work    ow 

    design model     implement     test     analyse     repeat  

    implementation of models is non-trivial 

    computations must be fast 
    derivative calculations are easy to get wrong 

    solution: toolkits that simplify implementation of models 

    standard component building blocks 
    facilities for automatic differentiation

software toolkits

    deep learning operate on basic features with complex models that consist of 

common components stacked together 

    work    ow 

    design model     implement     test     analyse     repeat  

    implementation of models is non-trivial 

    computations must be fast 
    derivative calculations are easy to get wrong 

    solution: toolkits that simplify implementation of models 

    standard component building blocks 
    facilities for automatic differentiation

software toolkits

    deep learning operate on basic features with complex models that consist of 

common components stacked together 

    work    ow 

    design model     implement     test     analyse     repeat  

    implementation of models is non-trivial 

    computations must be fast 
    derivative calculations are easy to get wrong 

    solution: toolkits that simplify implementation of models 

    standard component building blocks (lin alg, nonlinearities, convolutions, etc.) 
    facilities for automatic differentiation

software toolkits

    this course: you are free to use your own toolkit 

    however, demonstrators know 

    torch (and interested in pytorch) 

    tensorflow 

    many other options 

    theano 

    dynet (similar to pytorch, designed for language, fastest 

toolkit on cpu), written by me

understanding toolkits

    how do you declare computation graphs? 

    static (e.g. tensorflow, theano) 

    write down a symbolic expression representing all 

calculations you will carry out for different training instances 

    toolkit optimizes it and gives you training/prediction code 

    dynamic (e.g., dynet, pytorch) 

    write code that computes predictions 

    symbolic representation of computation is written down 

implicitly (based on operator overloading) by toolkit

understanding toolkits

    how do you declare computation graphs? 

    static (e.g. tensorflow, theano) 

    write down a symbolic expression representing all 

calculations you will carry out for different training instances 

    toolkit optimizes it and gives you training/prediction code 

    dynamic (e.g., dynet, pytorch) 

    write code that computes predictions 

    symbolic representation of computation is written down 

implicitly (based on operator overloading) by toolkit

static vs. dynamic

    static

    pros: toolkits can optimize the computation graph (think: 

compilers) 

    cons: you write code to write a symbolic program that the toolkit 

executes. the symbolic language is sometimes impoverished 

    dynamic

    pros: you write code in your favorite language (as long as your 

favorite language is c++ or python) 

    cons: toolkit has fewer opportunities to optimize

static vs. dynamic

    static

    pros: toolkits can optimize the computation graph (think: 

compilers) 

    cons: you write code to write a symbolic program that the toolkit 

executes. the symbolic language is sometimes impoverished 

    dynamic

    pros: you write code in your favorite language (as long as your 

favorite language is c++ or python) 

    cons: toolkit has fewer opportunities to optimize

static vs. dynamic

model

dynamic

}|

static

}|

{

{

z

z

enjoy the practicals ;)

