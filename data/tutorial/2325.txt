recurrent neural networks and language

modelling: part 1

phil blunsom

phil.blunsom@cs.ox.ac.uk

language models

such that(cid:80)

w         p(w ) = 1:

a language model assigns a id203 to a sequence of words,

given the observed training text, how probable is this new
utterance?

thus we can compare di   erent orderings of words
(e.g. translation):

p(he likes apples) > p(apples likes he)

or choice of words (e.g. id103):

p(he likes apples) > p(he licks apples)

history: cryptography

language models

much of natural language processing can be structured as
(conditional) language modelling:

translation

plm(les chiens aiment les os ||| dogs love bones)

id53

dialogue

plm(what do dogs love? ||| bones . |   )

plm(how are you? ||| fine thanks. and you? |   )

language models

most language models employ the chain rule to decompose the
joint id203 into a sequence of conditional probabilities:

p(w1, w2, w3, . . . , wn ) =

p(w1) p(w2|w1) p(w3|w1, w2)    . . .    p(wn|w1, w2, . . . wn   1)

note that this decomposition is exact and allows us to model
complex joint distributions by learning conditional distributions
over the next word (wn) given the history of words observed
(w1, . . . , wn   1).

language models

the simple objective of modelling the next word given the
observed history contains much of the complexity of natural
language understanding.

consider predicting the extension of the utterance:

p(  | there she built a)

with more context we are able to use our knowledge of both
language and the world to heavily constrain the distribution over
the next word:

p(  | alice went to the beach. there she built a)

there is evidence that human id146 partly relies on
future prediction.

evaluating a language model

a good model assigns real utterances w n
id203. this can be measured with cross id178:

1 from a language a high

h(w n

1 ) =    

1
n

log2 p(w n
1 )

intuition 1: cross id178 is a measure of how many bits are
needed to encode text with our model.

alternatively we can use perplexity:

perplexity(w n

1 ) = 2h(w n
1 )

intuition 2: perplexity is a measure of how surprised our model is
on seeing each word.

language modelling data

language modelling is a time series prediction problem in which we
must be careful to train on the past and test on the future.

if the corpus is composed of articles, it is best to ensure the test
data is drawn from a disjoint set of articles to the training data.

language modelling data

two popular data sets for language modelling evaluation are a
preprocessed version of the id32,1 and the billion word
corpus.2 both are    awed:

    the ptb is very small and has been heavily processed. as

such it is not representative of natural language.

    the billion word corpus was extracted by    rst randomly

permuting sentences in news articles and then splitting into
training and test sets. as such train and test sentences come
from the same articles and overlap in time.

the recently introduced wikitext datasets3 are a better option.

1

www.fit.vutbr.cz/~imikolov/id56lm/simple-examples.tgz
2
code.google.com/p/1-billion-word-language-modeling-benchmark/
3pointer sentinel mixture models. merity et al., arxiv 2016

lecture overview

the rest of this lecture will survey three approaches to
parametrising language models:

    with count based id165 models we approximate the history

of observed words with just the previous n words.

    neural id165 models embed the same    xed id165 history in

a continues space and thus better capture correlations
between histories.

    with recurrent neural networks we drop the    xed id165

history and compress the entire history in a    xed length
vector, enabling long range correlations to be captured.

outline

count based id165 language models

neural id165 language models

recurrent neural network language models

id165 models: the markov chain assumption

markov assumption:

    only previous history matters
    limited memory: only last k     1 words are included in history

(older words less relevant)
    kth order markov model

for instance 2-gram language model:

p(w1, w2, w3,

, wn)

. . .
= p(w1) p(w2|w1) p(w3|w1, w2)    . . .

  p(wn|w1, w2, . . . wn   1)

    p(w1) p(w2|w1) p(w3|w2)    . . .    p(wn|wn   1)

the conditioning context, wi   1, is called the history.

id165 models: estimating probabilities

id113 for 3-grams:

p(w3|w1, w2) =

count(w1, w2, w3)

count(w1, w2)

collect counts over a large text corpus. billions to trillions of
words are easily available by scraping the web.

id165 models: back-o   

in our training corpus we may never observe the trigrams:

    oxford pimm   s eater
    oxford pimm   s drinker

if both have count 0 our smoothing methods will assign the same
id203 to them.

a better solution is to interpolate with the bigram id203:

    pimm   s eater
    pimm   s drinker

id165 models: interpolated back-o   

by recursively interpolating the id165 probabilities with the
(n     1)-gram probabilities we can smooth our language model and
ensure all words have non-zero id203 in a given context.

a simple approach is linear interpolation:

pi (wn|wn   2, wn   1) =   3p(wn|wn   2, wn   1) +

  2p(wn|wn   1) +
  1p(wn).

where   3 +   2 +   1 = 1.

a number of more advanced smoothing and interpolation schemes
have been proposed, with kneser-ney being the most common.4

4an empirical study of smoothing techniques for id38.

stanley chen and joshua goodman. harvard university, 1998.
research. microsoft. com/ en-us/ um/ people/ joshuago/ tr-10-98. pdf

provisional summary

good

    count based id165 models are exceptionally scalable and are able

to be trained on trillions of words of data,

    fast constant time evaluation of probabilities at test time,
    sophisticated smoothing techniques match the empirical distribution

of language.5

bad

    large ngrams are sparse, so hard to capture long dependencies,
    symbolic nature does not capture correlations between semantically

similary word distributions, e.g. cat     dog,

    similarly morphological regularities, running     jumping, or gender.

5heaps    law: en.wikipedia.org/wiki/heaps   _law

outline

count based id165 language models

neural id165 language models

recurrent neural network language models

neural language models

feed forward network

h = g (vx + c)

  y = wh + b

xh  yneural language models

trigram nn language model

hn = g (v [wn   1; wn   2] + c)
  pn = softmax(whn + b)

exp ui(cid:80)

j exp uj

softmax(u)i =

    wi are one hot vetors and   pi are

distributions,

    |wi| = |   pi| = v (words in the

vocabulary),

    v is usually very large > 1e5.

wn 1hn  pnwn 2neural language models: sampling

wn|wn   1, wn   2       pn

wn 1hna~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarkwn 2hebuilt  pnneural language models: sampling

wn|wn   1, wn   2       pn

w0there~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarkw 1<s><s>  p1h1neural language models: sampling

wn|wn   1, wn   2       pn

w0there~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarkw 1<s><s>  p1he~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvark  p2w1w0h1h2neural language models: sampling

wn|wn   1, wn   2       pn

w0there~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarkw 1<s><s>  p1he~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarkbuilt~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvark  p2  p3w1w0w1w2h1h2h3neural language models: sampling

wn|wn   1, wn   2       pn

w0there~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarkw 1<s><s>  p1he~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarkbuilt~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarka~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvark  p2  p3  p4w1w0w1w2w2w3h1h2h3h4neural language models: training

the usual training objective is
the cross id178 of the data
given the model (id113):

f =    

1
n

costn(wn,   pn)

(cid:88)

n

the cost function is simply the
model   s estimated log-id203
of wn:

cost(a, b) = at log b

(assuming wi is a one hot
encoding of the word)

wncostnwn 1hn  pnwn 2neural language models: training

calculating the gradients is
straightforward with back
propagation:

(cid:80)
(cid:80)
=     1
=     1

n

n

n

   f
   w
   f
   v

   costn

      pn

      pn
   w

n

   costn

      pn

      pn
   hn

   hn
   v

wncostnwn 1hn  pnwn 2neural language models: training

calculating the gradients is straightforward with back propagation:

   f
   w

1
4

=    

   costn

      pn

      pn
   w

,

   f
   v

1
4

=    

   costn

      pn

      pn
   hn

   hn
   v

4(cid:88)

n=1

4(cid:88)

n=1

note that calculating the gradients for each time step n is independent of
all other timesteps, as such they are calculated in parallel and summed.

w1w2w3w4cost1cost2cost3cost4w0h1h2h3h4w1w2w3  p1  p2  p3  p4fw 1w0w1w2comparison with count based id165 lms

good

    better generalisation on unseen id165s, poorer on seen id165s.

solution: direct (linear) ngram features.

    simple nlms are often an order magnitude smaller in memory

footprint than their vanilla id165 cousins (though not if you use
the linear features suggested above!).

bad

    the number of parameters in the model scales with the id165 size

and thus the length of the history captured.

    the id165 history is    nite and thus there is a limit on the longest

dependencies that an be captured.

    mostly trained with maximum likelihood based objectives which do

not encode the expected frequencies of words a priori.

outline

count based id165 language models

neural id165 language models

recurrent neural network language models

recurrent neural network language models

feed forward

recurrent network

h = g (vx + c)

  y = wh + b

hn = g (v [xn; hn   1] + c)
  yn = whn + b

xh  yxnhn  ynrecurrent neural network language models

hn = g (v [xn; hn   1] + c)

w0there~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvark<s>  p1h1recurrent neural network language models

hn = g (v [xn; hn   1] + c)

w0there~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvark<s>  p1he~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvark  p2w1h1h2recurrent neural network language models

hn = g (v [xn; hn   1] + c)

w0there~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvark<s>  p1he~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarkbuilt~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvark  p2  p3w1w2h1h2h3recurrent neural network language models

hn = g (v [xn; hn   1] + c)

w0there~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvark<s>  p1he~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarkbuilt~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarka~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvark  p2  p3  p4w1w2w3h1h2h3h4recurrent neural network language models

feed forward

recurrent network

h = g (vx + c)

  y = wh + b

hn = g (v [xn; hn   1] + c)
  yn = whn + b

ycostxh  yxnhn  ynycostnrecurrent neural network language models

the unrolled recurrent network is a directed acyclic computation
graph. we can run id26 as usual:

4(cid:88)

n=1

f =    

1
4

costn(wn,   pn)

w1w2w3w4cost1cost2cost3cost4w0h0h1h2h3h4w1w2w3  p1  p2  p3  p4frecurrent neural network language models

this algorithm is called back propagation through time (bptt).
note the dependence of derivatives at time n with those at time
n +   :
   f
   h2

   f
   cost2

   f
   cost3

   f
   cost4

      p2
   h2

      p3
   h3

   h3
   h2

      p4
   h4

   h4
   h3

   cost2

   cost4

   cost3

      p2

      p4

      p3

   h3
   h2

=

+

+

w1w2w3w4cost1cost2cost3cost4w0h0h1h2h3h4w1w2w3  p1  p2  p3  p4frecurrent neural network language models

if we break these depdencies after a    xed number of timesteps we
get truncated back propagation through time (tbptt):

4(cid:88)

n=1

f =    

1
4

costn(wn,   pn)

w1w2w3w4cost1cost2cost3cost4w0h0h1h2h3h4w1w2w3  p1  p2  p3  p4frecurrent neural network language models

if we break these depdencies after a    xed number of timesteps we
get truncated back propagation through time (tbptt):

   f
   h2    

   f
   cost2

   cost2

      p2

      p2
   h2

w1w2w3w4cost1cost2cost3cost4w0h0h1h2h3h4w1w2w3  p1  p2  p3  p4fid56s: minibatching and complexity

mini-batching on a gpu is an e   ective way of speeding up big
matrix vector products. id56lms have two such products that
dominate their computation: the recurrent matrix v and the
softmax matrix w .

complexity:

bptt linear in the length of the longest sequence.

minibatching can be ine   cient as the sequences in a
batch may have di   erent lengths.

tbptt constant in the truncation length t .

e   cient for mini-batching as all sequences have
length t .

more on this in the gpu lecture next week.

comparison with id165 lms

good

    id56s can represent unbounded dependencies, unlike models with a

   xed id165 order.

    id56s compress histories of words into a    xed size hidden vector.
    the number of parameters does not grow with the length of
dependencies captured, but they do grow with the amount of
information stored in the hidden layer.

bad

    id56s are hard to learn and often will not discover long range
dependencies present in the data (more on this next lecture).

    increasing the size of the hidden layer, and thus memory, increases

the computation and memory quadratically.

    mostly trained with maximum likelihood based objectives which do

not encode the expected frequencies of words a priori.

bias vs variance in lm approximations

the main issue in language modelling is compressing the history (a
string). this is useful beyond language modelling in classi   cation and
representation tasks (more next week).

    with id165 models we approximate the history with only the last n

words.

    with recurrent models (id56s, next) we compress the unbounded

history into a    xed sized vector.

we can view this progression as the classic bias vs. variance tradeo    in
ml. id165 models are biased but low variance. id56s decrease the bias
considerably, hopefully at a small cost to variance.

consider predicting the id203 of a sentence by how many times you
have seen it before. this is an unbiased estimator with (extremely) high
variance.

references

textbook

blog posts

deep learning, chapter 10.
www.deeplearningbook.org/contents/id56.html

andrej karpathy: the unreasonable e   ectiveness of
recurrent neural networks
karpathy.github.io/2015/05/21/id56-effectiveness/

yoav goldberg: the unreasonable e   ectiveness of
character-level language models
nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139

stephen merity: explaining and illustrating orthogonal
initialization for recurrent neural networks.
smerity.com/articles/2016/orthogonal_init.html

in the next lecture i will discuss the architechtural

and algorithmic solutions to issues encountered

when training id56s.

