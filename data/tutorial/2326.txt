recurrent neural networks and language

modelling: part 2

phil blunsom

phil.blunsom@cs.ox.ac.uk

language modelling: review

language models aim to represent the history of observed text
(w1, . . . , wt   1) succinctly in order to predict the next word (wt):

    with count based id165 lms we approximate the history with just

the previous n words.

    neural id165 lms embed the same    xed id165 history in a
continues space and thus capture correlations between histories.
    with recurrent neural network lms we drop the    xed id165
history and compress the entire history in a    xed length vector,
enabling long range correlations to be captured.

w0there~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvark<s>  p1he~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarkbuilt~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvarka~theitifwasandallherhecatrockdogyeswetensunofaiyoutherebuilt...........aardvark  p2  p3  p4w1w2w3h1h2h3h4capturing long range dependencies

if an id56 language model is to outperform an id165 model it
must discover and represent long range dependencies:

p(sandcastle | alice went to the beach. there she built a)
while a simple id56 lm can represent such dependencies in
theory, can it learn them?

wncostnw0h0h1h2hn 1hnw1wn 2wn 1  pn   id56s: exploding and vanishing gradients

consider the path of partial derivatives linking a change in cost4 to
changes in h1:

hn = g (v [xn; hn   1] + c)
  pn = softmax(whn + b)

   cost4

   h1

=

   cost4

      p4

      p4
   h4

   h4
   h3

   h3
   h2

   h2
   h1

w4cost4w0h0h1h2h3h4w1w2w3  p4id56s: exploding and vanishing gradients

consider the path of partial derivatives linking a change in costn
to changes in h1:

hn = g (v [xn; hn   1] + c)
  pn = softmax(whn + b)

   costn

   h1

=

   costn

      pn

      pn
   hn

       (cid:89)

n   {n,...,2}

      

   hn
   hn   1

wncostnw0h0h1h2hn 1hnw1wn 2wn 1  pn   id56s: exploding and vanishing gradients

consider the path of partial derivatives linking a change in costn
to changes in h1:

(cid:16)(cid:81)

(cid:17)

hn = g (v [xn; hn   1] + c),

   costn

   h1

=    costn
      pn

      pn
   hn

n   {n,...,2}    hn
   hn   1

wncostnw0h0h1h2hn 1hnw1wn 2wn 1  pn   id56s: exploding and vanishing gradients

consider the path of partial derivatives linking a change in costn
to changes in h1:

(cid:16)(cid:81)

(cid:17)

hn = g (vx xn + vhhn   1 + c

),

   costn

   h1

=    costn
      pn

      pn
   hn

n   {n,...,2}    hn
   zn

   zn
   hn   1

(cid:124)

(cid:123)(cid:122)

zn

(cid:125)

wncostnw0h0h1h2hn 1hnw1wn 2wn 1  pn   id56s: exploding and vanishing gradients

consider the path of partial derivatives linking a change in costn
to changes in h1:

hn = g (vx xn + vhhn   1 + c

   costn

   h1

=    costn
      pn

      pn
   hn

n   {n,...,2}    hn
   zn

   zn
   hn   1

(cid:17)

(cid:124)

   hn
   zn

(cid:123)(cid:122)
(cid:125)
= diag(cid:0)g(cid:48)(zn)(cid:1)

),

zn

(cid:16)(cid:81)

   zn
   hn   1

= vh

wncostnw0h0h1h2hn 1hnw1wn 2wn 1  pn   id56s: exploding and vanishing gradients

consider the path of partial derivatives linking a change in costn
to changes in h1:

(cid:16)(cid:81)

(cid:17)

hn = g (vx xn + vhhn   1 + c

   costn

   h1

=    costn
      pn

      pn
   hn

n   {n,...,2}    hn
   zn

   zn
   hn   1

(cid:124)

   hn
   zn

(cid:123)(cid:122)
(cid:125)
= diag(cid:0)g(cid:48)(zn)(cid:1)

),

zn

   hn
   hn   1

=

   hn
   zn

   zn
   hn   1

   zn
   hn   1

= vh

= diag(cid:0)g(cid:48)(zn)(cid:1) vh

wncostnw0h0h1h2hn 1hnw1wn 2wn 1  pn   id56s: exploding and vanishing gradients

       (cid:89)

n   {n,...,2}

diag(cid:0)g(cid:48)(zn)(cid:1) vh

      

   costn

   h1

=

   costn

      pn

      pn
   hn

the core of the recurrent product is the repeated multiplication of
vh. if the largest eigenvalue of vh is:
    1, then gradient will propagate,
    > 1, the product will grow exponentially (explode),
    < 1, the product shrinks exponentially (vanishes).

id56s: exploding and vanishing gradients

most of the time the spectral radius of vh is small. the result is that the
gradient vanishes and long range dependencies are not learnt.

many non-linearities (g (  )) can also shrink
the gradient.

second order optimisers ((quasi-)newtonian methods) can overcome
this, but they are di   cult to scale. careful initialisation of the recurrent
weights can help.1

here we will consider the most popular solution, changing the network
architecture.

1stephen merity: explaining and illustrating orthogonal initialization for recurrent

neural networks. smerity.com/articles/2016/orthogonal_init.html

 0(x)   1 0(x)   0 0(x)   0long short term memory (lstm)

hn = g (v [wn   1; hn   1] + c)

wn 2hnwn 1  pn 1  pnhn 1long short term memory (lstm)

hn = tanh(v [wn   1; hn   1] + c)

hnwn 1  pn 1  pnhn 1tanhlong short term memory (lstm)

cn = cn   1 + tanh(v [wn   1; hn   1] + bc )

hnwn 1  pn 1  pn+hn 1cn 1cntanhlong short term memory (lstm)

cn = cn   1 + in     tanh(v [wn   1; hn   1] + bc )

in =    (wi [wn   1; ht   1] + bi ) .

hnwn 1  pn 1  pn+hn 1cn 1cntanhin   long short term memory (lstm)

cn = fn     cn   1 + in     tanh(v [wn   1; hn   1] + bc )

in =    (wi [wn   1; ht   1] + bi ) ,
fn =    (wf [wn   1; ht   1] + bf ) .

hnwn 1  pn 1  pn+   hn 1cn 1cntanhin   fnlong short term memory (lstm)

cn = fn     cn   1 + in     tanh(v [wn   1; hn   1] + bc )
hn = on     tanh (whcn + bh) .

in =    (wi [wn   1; ht   1] + bi ) ,
fn =    (wf [wn   1; ht   1] + bf ) ,
on =    (wo[wn   1; ht   1] + bo) .

hnwn 1  pn 1  pn+   hn 1cn 1cntanhin      onfnlong short term memory (lstm)

christopher olah: understanding id137
colah.github.io/posts/2015-08-understanding-lstms/

lstm lm

the lstm cell,2

cn = fn     cn   1 + in       cn,

  cn = tanh (wc [wn   1; ht   1] + bc ) ,
hn = on     tanh (whcn + bh) .
in =    (wi [wn   1; ht   1] + bi ) ,
fn =    (wf [wn   1; ht   1] + bf ) ,
on =    (wo[wn   1; ht   1] + bo) ,

where hn is the hidden state at time n, and i, f , o are the input,
forget, and output gates respectively.3

2

3

long short-term memory. hochreiter and schmidhuber, neural computation 1997.

optimizing performance of recurrent neural networks on gpus. appleyard et al., arxiv 2016.

lstm lm

the lstm cell,2

cn = fn     cn   1 + (1     fn)       cn,

  cn = tanh (wc [wn   1; ht   1] + bc ) ,
hn = on     tanh (whcn + bh) .
in=    (wi [wn   1; ht   1] + bi ),
fn =    (wf [wn   1; ht   1] + bf ) ,
on =    (wo[wn   1; ht   1] + bo) ,

where hn is the hidden state at time n, and i, f , o are the input,
forget, and output gates respectively.3

2

3

long short-term memory. hochreiter and schmidhuber, neural computation 1997.

optimizing performance of recurrent neural networks on gpus. appleyard et al., arxiv 2016.

gated recurrent unit (gru)

christopher olah: understanding id137
colah.github.io/posts/2015-08-understanding-lstms/

gated recurrent unit (gru)

the gru cell,4

hn = (1     zn)     hn   1 + zn       hn.

zn =    (wz [xn; ht   1] + bz ) ,
rn =    (wr [xn; ht   1] + br ) ,

  hn = tanh(cid:0)w  h[xn; rn     hn   1] + b  h

(cid:1) .

4learning phrase representations using id56 encoderdecoder for

id151. cho et al, emnlp 2014.

lstms and grus

good

    careful initialisation and optimisation of vanilla id56s can

enable them to learn long(ish) dependencies, but gated
additive cells, like the lstm and gru, often just work.

    the (re)introduction of lstms has been key to many recent

developments, e.g. id4, speech
recognition, tts, etc.

bad

    lstms and grus have considerably more parameters and
computation per memory cell than a vanilla id56, as such
they have less memory capacity per parameter.5

5capcity and trainability in recurrent neural networks. collins et al., arxiv

2016.

deep id56 lms

the memory capacity of an id56 can be increased by employing a
larger hidden layer hn, but a linear increase in hn results in a
quadratic increase in model size and computation.

a deep id56 increases the memory and representational ability
with linear scaling.

w0h0h1h2h3h4w1w2w3  p1  p2  p3  p4deep id56 lms

the memory capacity of an id56 can be increased by employing a
larger hidden layer hn, but a linear increase in hn results in a
quadratic increase in model size and computation.

a deep id56 increases the memory and representational ability
with linear scaling.

w0h1,1h1,2h1,3h1,4w1w2w3h2,1h2,2h2,3h2,4  p1  p2  p3  p4h1,0h2,0deep id56 lms

the memory capacity of an id56 can be increased by employing a
larger hidden layer hn, but a linear increase in hn results in a
quadratic increase in model size and computation.

a deep id56 increases the memory and representational ability
with linear scaling.

w0h1,1h1,2h1,3h1,4w1w2w3h2,1h2,2h2,3h2,4  p1  p2  p3  p4h3,4h3,3h3,2h3,1h1,0h2,0h3,0deep id56 lms

the memory capacity of an id56 can be increased by employing a
larger hidden layer hn, but a linear increase in hn results in a
quadratic increase in model size and computation.

a deep id56 increases the memory and representational ability
with linear scaling.

w0h1,1h1,2h1,3h1,4w1w2w3h2,1h2,2h2,3h2,4  p1  p2  p3  p4h3,4h3,3h3,2h3,1h1,0h2,0h3,0deep id56 lm

alternatively we can increase depth in the time dimension. this
improves the representational ability, but not the memory capacity.

w0h0h1h2h3h4w1w2w3  p1  p2  p3  p4deep id56 lm

alternatively we can increase depth in the time dimension. this
improves the representational ability, but not the memory capacity.

the recently proposed recurrent highway network6 employs a
deep-in-time gru-like cell with untied weights, and reports strong
results on language modelling.

6recurrent id199. zilly et al., arxiv 2016.

w0h0h1,1h2,1h3,1h4,1w1w2w3h1,2h2,2h3,2  p1  p2  p3  p4h4,2scaling: large vocabularies

much of the computational cost of a neural lm is a function of the size
of the vocabulary and is dominated by calculating:

  pn = softmax (whn + b)

scaling: large vocabularies

much of the computational cost of a neural lm is a function of the size
of the vocabulary and is dominated by calculating:

  pn = softmax (whn + b)

solutions
short-lists: use the neural lm for the most frequent words, and a
traditional ngram lm for the rest. while easy to implement, this nulli   es
the neural lm   s main advantage, i.e. generalisation to rare events.
batch local short-lists: approximate the full partition function for data
instances from a segment of the data with a subset of the vocabulary
chosen for that segment.7

7on using very large target vocabulary for id4. jean et

al., acl 2015

scaling: large vocabularies

much of the computational cost of a neural lm is a function of the size
of the vocabulary and is dominated by calculating:

  pn = softmax (whn + b)

solutions
approximate the gradient/change the objective: if we did not have
to sum over the vocabulary to normalise during training it would be much
faster. it is tempting to consider maximising likelihood by making the log
partition function an independent parameter c, but this leads to an ill
de   ned objective.

  pn     exp (whn + b)    exp(c)

scaling: large vocabularies

much of the computational cost of a neural lm is a function of the size
of the vocabulary and is dominated by calculating:

  pn = softmax (whn + b)

solutions
approximate the gradient/change the objective: mnih and teh use
noise contrastive estimation (nce). this amounts to learning a binary
classi   er to distinguish data samples from (k) samples from a noise
distribution (a unigram is a good choice):

p(data = 1|   pn) =

  pn

  pn + kpnoise(wn)

now parametrising the log partition function as c does not degenerate.
this is very e   ective for speeding up training, but has no impact on
testing time.7

7

in practice    xing c = 0 is e   ective. it is tempting to believe that this noise contrastive objective justi   es

using unnormalised scores at test time. this is not the case and leads to high variance results.

scaling: large vocabularies

much of the computational cost of a neural lm is a function of the size
of the vocabulary and is dominated by calculating:

  pn = softmax (whn + b)

solutions
approximate the gradient/change the objective: nce de   nes a
binary classi   cation task between true or noise words with a logistic loss.
an alternative proposed by jozefowicz et al.7, called importance
sampling (is), de   nes a multiclass classi   cation problem between the
true word and noise samples, with a softmax and cross id178 loss.

7exploring the limits of id38. jozefowicz et al., arxiv 2016.

scaling: large vocabularies

much of the computational cost of a neural lm is a function of the size
of the vocabulary and is dominated by calculating:

  pn = softmax (whn + b)

solutions
factorise the output vocabulary: one level factorisation works well
(brown id91 is a good choice, frequency binning is not):

n

n

,   pword

p(wn|   pclass

)    p(wn|class(wn),   pword
) = p(class(wn)|   pclass
where the function class(  ) maps each word to one class. assuming
balanced classes, this gives a    v speedup.

n

n

),

scaling: large vocabularies

much of the computational cost of a neural lm is a function of the size
of the vocabulary and is dominated by calculating:

  pn = softmax (whn + b)

solutions
factorise the output vocabulary: by extending the factorisation to a
binary tree (or code) we can get a log v speedup,7 but choosing a tree is
hard (frequency based hu   man coding is a poor choice):

(cid:89)

i

p(wn|hn) =

p(di|ri , hn),

where di is i th digit in the code for word wn, and ri is the parameter
vector for the i th node in the path corresponding to that code.

recently grave et al. proposed optimising an n-ary factorisation tree for
both perplexity and gpu throughput.8

7a scalable hierarchical distributed language model. mnih and hinton, nips   09.
8e   cient softmax approximation for gpus. grave et al., arxiv 2016

scaling: large vocabularies

full softmax

training: computation and memory o(v ),
evaluation: computation and memory o(v ),
sampling: computation and memory o(v ).

balanced class factorisation

   
   
v ) and memory o(v ),
v ) and memory o(v ),

training: computation o(
evaluation: computation o(
sampling: computation and memory o(v ) (but average case
is better).

balanced tree factorisation

training: computation o(log v ) and memory o(v ),
evaluation: computation o(log v ) and memory o(v ),
sampling: computation and memory o(v ) (but average case
is better).

nce / is

training: computation o(k) and memory o(v ),
evaluation: computation and memory o(v ),
sampling: computation and memory o(v ).

sub-word level language modelling

an alternative to changing the softmax is to change the input
granularity and model text at the morpheme or character level.

this results in a much smaller softmax and no unknown words, but
the downsides are longer sequences and longer dependencies.

this also allows the model to capture subword structure and
morphology: disunited     disinherited     disinterested.
charater lms lag word based models in perplexity, but are clearly
the future of language modelling.

w0a~abcdefghijklmnopqrstu..........._<s>_~w1h1h2abcdefghijklmnopqrstu..........._h~w1h2abcdefghijklmnopqrstu..........._a~abcdefghijklmnopqrstu..........._p~abcdefghijklmnopqrstu..........._h~abcdefghijklmnopqrstu..........._p~abcdefghijklmnopqrstu..........._y~abcdefghijklmnopqrstu...........__~abcdefghijklmnopqrstu..........._c~abcdefghijklmnopqrstu..........._a~abcdefghijklmnopqrstu..........._t~abcdefghijklmnopqrstu..........._w2w3w4w5w6w7w8w9w10h10h9h8h7h6h5h4h3h11regularisation: dropout

large recurrent networks often over   t their training data by
memorising the sequences observed. such models generalise poorly
to novel sequences.

a common approach in deep learning is to overparametrise a
model, such that it could easily memorise the training data, and
then heavily regularise it to facilitate generalisation.

the regularisation method of choice is often dropout.9

9dropout: a simple way to prevent neural networks from over   tting.

srivastava et al. jmlr 2014.

regularisation: dropout

dropout is ine   ective when applied to recurrent connections, as
repeated random masks zero all hidden units in the limit. the
most common solution is to only apply dropout to non-recurrent
connections.10

10recurrent neural network id173. zaremba et al., arxiv 2014.

w1w2w3w4cost1cost2cost3cost4w0h0h1h2h3h4w1w2w3  p1  p2  p3  p4fdropoutdropoutdropoutdropoutdropoutdropoutdropoutdropoutregularisation: bayesian dropout (gal)

gal and ghahramani11 advocate tying the recurrent dropout mask
and sampling at evaluation time:

11a theoretically grounded application of dropout in recurrent neural

networks. gal and ghahramani, nips 2016.

w1w2w3w4cost1cost2cost3cost4w0h0h1h2h3h4w1w2w3  p1  p2  p3  p4fdropoutdropoutdropoutdropoutdropoutdropoutdropoutdropoutdropoutsummary

long range dependencies

lead to vanishing (or exploding) gradients,

    the repeated multiplication of the recurrent weights v
    additive gated architectures, such as lstms, signi   cantly

reduce this issue.

deep id56s

memory capacity with a quadratic slow down,

    increasing the size of the recurrent layer increases
    deepening networks in both dimensions can improve their

representational e   ciency and memory capacity with a
linear complexity cost.

large vocabularies

calculations,

    large vocabularies, v > 104, lead to slow softmax
    reducing the number of vector matrix products evaluated,

by factorising the softmax or sampling, reduces the
training overhead signi   cantly.

    di   erent optimisations have di   erent training and
evaluation complexities which should be considered.

references

papers

on the di   culty of training recurrent neural networks. pascanu et al., icml 2013.

long short-term memory. hochreiter and schmidhuber, neural computation 1997.

learning phrase representations using id56 encoderdecoder for statistical machine
translation. cho et al, emnlp 2014

a scalable hierarchical distributed language model. mnih and hinton, nips 2009.

a fast and simple algorithm for training neural probabilistic language models. mnih and
teh, icml 2012.

on using very large target vocabulary for id4. jean et al., acl
2015

exploring the limits of id38. jozefowicz et al., arxiv 2016.

e   cient softmax approximation for gpus. grave et al., arxiv 2016

notes on noise contrastive estimation and negative sampling. dyer, arxiv 2014.

pragmatic neural language modelling in machine translation. baltescu and blunsom,
naacl 2015

a theoretically grounded application of dropout in recurrent neural networks. gal and
ghahramani, nips 2016.

recurrent id199. zilly et al., arxiv 2016.

capcity and trainability in recurrent neural networks. collins et al., arxiv 2016.

optimizing performance of recurrent neural networks on gpus. appleyard et al., arxiv

blog posts christopher olah: understanding id137

2016.

colah.github.io/posts/2015-08-understanding-lstms/

yarin gal: uncertainty in deep learning mlg.eng.cam.ac.uk/yarin/blog_2248.html

the end

next week we will cover representation learning for classi   cation

and accelerating deep networks using gpus.

