deep learning for natural language processing

text classi   cation

karl moritz hermann

kmh@google.com

7 feb 2017

overview

this lecture discusses text classi   cation.
as part of that we   ll discuss the following

    generative and discriminative models
    na    ve bayes
    id28
    text representation (bow, features, id56s, convolutions)
    softmax classi   ers
    practical aspects of classi   er training

important message

good day,

my name is dr william monroe, a sta    in the private clients section of
a well-known bank, here in london, england. one of our accounts, with
holding balance of   15,000,000 has been dormant and last operated
three years ago. from my investigations, the owner of the said account,
john shumejda died on the 4th of january 2002 in a plane crash.

i have decided to    nd a reliable foreign partner to deal with. i therefore
propose to do business with you, standing in as the next of kin of these
funds from the deceased.

this transaction is totally free of risk and troubles as the fund is
legitimate and does not originate from drug, money laundry or terrorism.

on your interest, let me hear from you urgently.

best regards,
dr william monroe financial analysis and remittance manager

why classify?

classi   cation tasks

    is this e-mail spam?
    positive or negative review?
    what is the topic of this article?
    predict hashtags for a tweet
    age/gender identi   cation
    language identi   cation
    id31
    ...

types of classi   cation tasks

    binary classi   cation (true, false)
    multi-class classi   cation (politics, sports, gossip)
    multi-label classi   cation (#party #friday #fail)
    id91 (labels unknown)

classi   cation methods

1 by hand

2 rule-based

3 statistical

classi   cation methods

1 by hand

e.g. yahoo in the old days
  very accurate and consistent assuming experts
  super slow, expensive, does not scale

2 rule-based

e.g. advanced search criteria (   site:ox.ac.uk   )
  accuracy high if rule is suitable
  need to manually build and maintain rule-based system.

3 statistical
this lecture
  scales well, can be very accurate, automatic
  requires classi   ed training data. sometimes a lot!

statistical text classi   cation

assume some text represented by d and some class c. we want to
learn the id203 of d being of class c:

p(c|d)

key questions:

    how to represent d.
    how to calculate p(c|d).

text classi   cation in two parts

think of text classi   cation as a two stage process:

representation

process text into some (   xed) representation.
how to learn d

classi   cation

classify document given that representation.
how to learn p(c|d)

possible representations for text

    bag of words (bow)

    easy, no e   ort required.
    variable size, ignores sentential structure.

    hand-crafted features

    full control, can use of nlp pipeline, class-speci   c features
    over-speci   c, incomplete, makes use of nlp pipeline.

    learned feature representation

    can learn to contain all relevant information.
    needs to be learned.

generative vs. discriminative models

generative (joint) models

p(c, d)

    model the distribution of individual classes and place

probabilities over both observed data and hidden variables
(such as labels)

    e.g. id165 models, id48, probabilistic
context-free grammars, ibm machine translation models,
na    ve bayes, ...

discriminative (conditional) models

p(c|d)
    learn boundaries between classes. take data as given and put

id203 over the hidden structure given the data.

    e.g. id28, maximum id178 models, conditional

random    elds, support-vector machines, ...

na    ve bayes classi   er (1/4)

bayes    rule:

p(c|d) =

p(c)p(d|c)

p(d)

we can simplify this in a classi   cation scenario and ignore the
denominator p(d) because it is independent of class choice c:

p(c|d)     p(c)p(d|c)

    p(c)

p(ti|c)

(cid:89)

1   i   nd

this estimates the id203 of document d being in class c,
assuming document length nd and tokens t.

na    ve bayes classi   er (2/4)

p(c|d)     p(c)

(cid:89)

1   i   nd

p(ti|c)

p(c) and p(t|c) can be estimated from labelled training data:

dc
d

p(c) =

tct(cid:80)

t(cid:48)   v tct(cid:48)

p(t|c) =

independence assumptions
note that we assume p(ti|c) = p(tj|c) independent of token
position. this is the na    ve part of na    ve bayes.

na    ve bayes classi   er (3/4)

the best class is the maximum a posteriori (map) class:
p(ti|c)

cmap = argmax

p(c)

p(c|d) = argmax
c   c

c   c

(cid:89)

1   i   nd

multiplying tons of small probabilities is tricky, so log space it:

      log p(c) +

(cid:88)

1   i   nd

      

log p(ti|c)

cmap = argmax

c   c

finally: zero probabilities are bad. add smoothing:

p(t|c) =

tct(cid:80)

t(cid:48)   v tct(cid:48)

    p(t|c) =

tct + 1

t(cid:48)   v tct(cid:48) + |v|

(cid:80)

this is laplace or add-1 smoothing. there are many alternatives.

na    ve bayes classi   er (4/4)

advantages
    simple
    interpretable
    fast (linear in size of training set and test document)
    text representation trivial (bag of words)

drawbacks

    independence assumptions often too strong
    sentence/document structure not taken into account
    na    ve classi   er has zero probabilities; smoothing is awkward

quiz

question
is na    ve bayes a generative or a discriminative model?

quiz

question
is na    ve bayes a generative or a discriminative model?

na    ve bayes is a generative model!

p(c|d) =

p(d|c)p(c)

p(d)

p(c|d)p(d) = p(d|c)p(c)

= p(d, c)

    while we use a id155 p(c|d) for

classi   cation, we model the joint id203 of c and d.

    this means it is trivial to invert the process and generate new

text given a class label.

feature representations

a feature representation (of text) can be viewed as a vector where
each element indicates the presence or absence of a given feature
in a document.
note: features can be binary (presence/absence), multinomial
(count) or continuous (eg. tf-idf weighted).

feature

coe   cient weight

bias
  0
   prozac   
  1
   school   
  2
   dear friend      3
   nigeria   
  4
   homework   
  5

0.2
1.4
-0.4
1.8
2.0
-1.7

what does this feature representation classify?

id28 (1/7)

if we only want to classify text, we do not need the full power of a
generative model, but a discriminative model is su   cient.
we only want to learn p(c|d).

a general framework for this is id28.

logistic because is uses a logistic function

regression combines a feature vector (d) with weights (  ) to

compute an answer

id28 (2/7)

binary case:

p(true|d) =

p(false|d) =

1

1 + exp(  0 +(cid:80)
exp(  0 +(cid:80)
1 + exp(  0 +(cid:80)
exp(  c,0 +(cid:80)
c(cid:48) exp(  c(cid:48),0 +(cid:80)
(cid:80)

i   i xi )

i   i xi )

i   i xi )

i   c,i xi )

i   c(cid:48),i xi )

multinomial case:

p(c|d) =

where x are the features contained in d.

id28 (3/7)

the binary and general functions(*) for the id28

p(true|d) =

1 + exp((cid:80)

1

can be simpli   ed as follows

p(c|d) =

1

1 + exp(   z)

p(c|d) =

i   i xi )

i   c,i xi )

i   c(cid:48),i xi )

exp((cid:80)
(cid:80)
c(cid:48) exp((cid:80)
(cid:80)

exp(zc )
c(cid:48) exp(zc(cid:48))

p(c|d) =

which are referred to as the logistic and softmax function.

* this is not the most general form for the id28, but a
suitable speci   cation for the purposes of this course.

id28 (4/7)

logistic function

softmax function

    multinomial generalisation of logistic function
    takes output of k distinct linear functions and returns a

id203 distribution over those outputs

id28 (5/7)

    given this model formulation, we want to learn parameters   
that maximise the conditional likelihood of the data according
to the model.

    due to the softmax function we not only construct a classi   er,

but learn id203 distributions over classi   cations.

    there are many ways to chose weights   :

id88 find misclassi   ed examples and move weights in

the direction of their correct class

margin-based methods such as support vector machines

can be used for learning weights

id28 directly maximise the conditional

log-likelihood via id119 (next slide).

id28 (6/7)

the conditional log-likelihood of the multinomial id28
is:

log p(c|d,   ) = log

p(cn|dn,   )

(cid:89)

c,d   (c ,d)

(cid:88)
(cid:88)

c,d   (c ,d)

c,d   (c ,d)

=

log p(cn|dn,   ) =

log p(cn|dn,   )

exp((cid:80)
(cid:80)
c(cid:48) exp((cid:80)

log

i   c,i xi )

i   c(cid:48),i xi )

learning weights
  derivative with respect to    is concave
  no closed-form solution
(calculation left as excercise - or look up in the earlier lecture)

id28 (7/7)

advantages

    still reasonably simple
    results are very interpretable
    do not assume statistical independence between features!

drawbacks

    harder to learn than na    ve bayes
    manually designing features can be expensive
    will not necessarily generalise well due to hand-crafted

features

break

this was the broad introduction to text classi   cation

we will cover deep learning approaches next

recap: recurrent neural networks

recurrent neural network language model

    agnostic to actual recurrent function (lstm, gru, ..)
    reads inputs xi to accumulate state hi and predict outputs yi

information contained in an id56

representing text with an id56

    hi is a function of x{0:i} and h{0:i   1}
    it contains information about all text read up to point i.
    the    rst half of this lecture was focused on learning a

representation x for a given text

    h is precisely that

id28 revisited

e   ectively, x = hn where n is the length of a given input
document.

so in order to classify text we can simply take a trained language
model (last week) and extract text representations from the    nal
hidden state cn.
classi   cation as before using a id28:

exp((cid:80)
c(cid:48) exp((cid:80)
(cid:80)

i   c,i hni )

i   c(cid:48),i hni )

p(c|d) =

  can use id56 + id28 out of the box
  can in fact use any other classi   er on top of h!
  how to ensure that h pays attention to relevant aspects of data?

text classi   cation with an id56

move the classi   cation function inside the network

text classi   cation with an id56

we do not need output layers y as we only care about p(c|d)!

(cid:88)

take id56 state as input x = hn

compute class weights

fc =

  c,i xi

apply nonlinearity

apply softmax function

i

mc =   (fc )
p(c|d) =

(cid:80)

exp(mc )
i exp(mi )

id168 for an id56 classi   er

you will have encountered this classi   er already in the    rst lecture.
this is a simple multilayer id88 (mlp).
we can train the model using the cross-id178 loss:

li =    (cid:88)

c

yc log p(c|di ) =     log

(cid:33)

(cid:32)

(cid:80)

exp(mc )
j exp(mj )

where yc = 1 if c = i, 0 otherwise.
    cross-id178 is designed to deal with errors on probabilities.
    optimizing means minimizing the cross-id178 between the
estiated class probabilities (p(c|d) and the true distribution.
    there are many alternative losses (hinge-loss, square error, l1

loss) not discussed today.

extension: multi-label classi   cation

what if a single data point may have multiple correct labels?
  the cross-id178 loss assumes there is only one correct label.

option 1: binary classi   ers
for each possible label class (c) de   ne a binary classi   er
(true/false) on class feature weights mc . assume yic = 1 if
example i is in class c and 0 otherwise. the loss per example i is:

li =

yic log(  (fc )) + (1     yic )log (1       (fc ))

(cid:88)

c

= yic       (fc )

   li
   fc

how else could you formulate a multi-label classi   er?

dual objective id56

in practice it may make sense to combine an lm objective with
classi   er training and to optimise the two losses jointly.

j =   jclass + (1       )jlm

such a joint loss enables making use of text beyond labelled data.

bi-directional id56s

another way to add signal is to process the input text both in a
forward and in a backward sequence.

the update rules for this directly follow the regular forward-facing
id56 architecture. in practice, bidirectional networks have shown
to be more robust than unidirectional networks.

bi-directional id56s

a bidirectional network can be used as a classi   er simply by
rede   ning d to be the concatenation of both    nal hidden states:

d = (h   

n (cid:107)h   
0 )

quiz

question
is a simple id56 classi   er a generative or discriminative model?

quiz

question
is a simple id56 classi   er a generative or discriminative model?

id56 classi   ers can be either!

generative model

encoder: discriminative (it does not model the id203 of the
text)
joint-model: generative (learns both p(c) and p(d))

non-sequential neural networks

outside of natural language a lot of data is not sequential. there
are di   erent architectures designed to deal with such data.

convolutional neural networks were designed for image
classi   cation but can be adapted to work for language, too.

id56s are a language-centric variant that have
found success particularly for classi   cation tasks.

id56s

composition follows syntactic structure
    accumulation of state follows syntax
    can chose any form of tree structure

composition function

y = g (wl xl + wr xr )

a simple composition function as
above is often su   cient.
however, it is also possible to
substitute this for a recurrent
cell.

id56s

while recursive networks have no simple generative counterpart, it
is possible to improve classi   er training by adding an additional
autoencoder loss.

autoencoder signals

reconstruction error

erec (i,   ) =

1
2

(cid:13)(cid:13)(cid:13)xi     x(cid:48)

i

(cid:13)(cid:13)(cid:13)2

this learns a compression
function over the input space.

convolutional network networks

for images convolutional neural networks are very useful:

convolution iteratives takes a matrix of inputs and computes a

(smaller) matrix of outputs

subsampling takes a matrix of inputs and pools those into a

single element , e.g. by taking the maximum

convolutional neural networks

convolutional window acts as a classifer for local features.

   

stacked convolutional layers learn higher-level features.

8fully connected layerconvolutional layer88raw imagefirst order local featureshigher order featurespredictionconvolutional neural networks

reasons to consider id98s for text
  really fast (gpus! see thursday)
  bow is often su   cient (see na    ve bayes)
  actually can take some structure into account
  not sequential in its processing of input data
  easier to discriminate than to generate variably sized data

convolutional neural networks

assume your data is of shape rm  n  k , where m is the number of
input words, n the size of the input embeddings and k the number
of feature maps (initially 1). let xi denote a layer in the id98.
convolutional layer
each    lter fl,m is an dot product applied across the input data
(xl     rml  nl  kl ). s denotes a given segment of the input map.

ofl,m,s = wfl,m (cid:12) xl,s

key decisions are the number of    lters (k ), size of the    lter (wf )
and stride with which the    lter moves across the input.

max-pooling layer

ijk = max{xi(cid:48)j(cid:48)k : i     i(cid:48) < i + p, j     j(cid:48) < j + p}
x(cid:48)

actual maths will look di   erent for e   ciency purposes. gradients will get
covered in thursday   s lecture or calculate yourself - reasonably simple.

end
thursday   s lecture will discuss gpus for neural networks and give
more details about convolutional networks.
going forward we will build on the basic classi   ers discussed today
for more complex architectures and problems.

sources and further reading
http://cs231n.github.io/convolutional-networks/
http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-
nlp/
http://karpathy.github.io/2015/05/21/id56-e   ectiveness/

lai et al. (2015)    recurrent convolutional neural networks for text classi   cation   
hermann (2014)    distributional representations for id152   
kalchbrenner et al. (2014)    a convolutional neural network for modelling sentences   
socher et al. (2012)    semantic compositionality through recursive matrix-vector
spaces   

