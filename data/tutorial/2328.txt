hardware and software for nlp

jeremy appleyard, nvidia

performance
motivation

decreasing time to solution is very useful:

    for training it allows you to experiment with more model architectures

   

in production environments providing a quick result to the end-user is crucial

not everything can be done automatically: some things are up to the user

2

accelerated computing

cpu

optimized for 

serial tasks

gpu accelerator

optimized for 
parallel tasks

3

low latency or high throughput?

cpu
    optimized for low-latency access 

gpu
    optimized for data-parallel, 

to cached data sets

throughput computation

    control logic for out-of-order 

    architecture tolerant of memory 

and speculative execution

latency

    more transistors dedicated to 

computation

4

low latency or high throughput
design leads to performance

cpu architecture must minimize latency within each thread

gpu architecture hides latency with computation (10+k threads)

gpu     high throughput processor

computation thread

t4

t3

t2

t1

cpu core     low latency processor

t1

t2

t3

t4

tn processing

waiting for data

ready to be processed

5

100000

10000

1000

100

10

1
2003

theoretical single precision gflop/s at base clock

nvidia gpu

intel cpu

2005

2007

2009

2011

2013

2015

6

1000

100

10

1
2003

theoretical  peak gb/s

nvidia gpu

intel cpu

2005

2007

2009

2011

2013

2015

7

roofline model
useful analysis tool

floating point throughput is often the quantity we want to maximize. if we double 
achieved floating point throughput we double the amount of useful work we do in a 
given time.

a roofline model is a plot of the computational intensity of an algorithm against it   s 
expected floating point throughput for a given piece of hardware

computational intensity is defined as the number of floating point operations per
byte: flops/byte

8

roofline model
current hardware

roofline model for gpu and cpu

100000

10000

1000

s
/
p
o
l
f
g

100

10

0.25

1

4

16

64

256

1024

arithmetic intensity (ai)

gpu

cpu

operation bytes

flops

ai

c=a+b

12

1

0.083

mat-vec

o(n2)

o(n2)

o(1)

1d fft

o(n) o(nlogn) o(logn)

mat-mat

o(n2)

o(n3)

o(n)

id56

?

?

?

9

performance analysis of lstm

10

lstm 
computation required

equations:

operations:

         =     (                 ;           1 +         )
         =     (                 ;           1 +         )
         =     (                 ;           1 +         )

           = tanh(                 ;           1 +         )

         =                         1 +                        

        =              tanh(                +        )

for a batch of 1, wt, ht are vectors of size h

4x matrix-vector multiplications (input 2h, 
output h)

1x matrix-vector multiplication (input h, 
output h)

2x pointwise tanh (size h)

3x pointwise sigmoid (size h)

6x pointwise add (size h)

2x pointwise multiplication (size h)

11

lstm 
computation required

equations:

operations:

         =     (                 ;           1 +         )
         =     (                 ;           1 +         )
         =     (                 ;           1 +         )

           = tanh(                 ;           1 +         )

         =                         1 +                        

        =              tanh(        )

for a batch of 1, wt, ht are vectors of size h

4x matrix-vector multiplications (input 2h, 
output h)

1x matrix-vector multiplication (input h, 
output h)

2x pointwise tanh (size h)

3x pointwise sigmoid (size h)

5x pointwise add (size h)

2x pointwise multiplication (size h)

12

lstm 
computation required

equations:

operations:

         =     (                 ;           1 +         )
         =     (                 ;           1 +         )
         =     (                 ;           1 +         )

           = tanh(                 ;           1 +         )

         =                         1 +                        

        =              tanh(        )

for a batch of b, wt, ht are matrices of size 
hxb.

4x matrix-id127s (input 2hxb, 
output hxb)

1x matrix-vector multiplication (input h, 
output h)

2x pointwise tanh (size hxb)

3x pointwise sigmoid (size hxb)

5x pointwise add (size hxb)

2x pointwise multiplication (size hxb)

13

increasing parallelism
an aside

[a1][h] = [x1]

[a2][h] = [x2]

[a3][h] = [x3]

[a4][h] = [x4]

a    [h] =   x

as the id127s share inputs we can combine them

14

lstm 
computation required

equations:

operations:

         =     (                 ;           1 +         )
         =     (                 ;           1 +         )
         =     (                 ;           1 +         )

           = tanh(                 ;           1 +         )

         =                         1 +                        

        =              tanh(        )

for a batch of b, wt, ht are matrices of size 
hxb.

1x matrix-id127 (input 2hxb, 
output 4hxb)

1x matrix-vector multiplication (input h, 
output h)

2x pointwise tanh (size hxb)

3x pointwise sigmoid (size hxb)

5x pointwise add (size hxb)

2x pointwise multiplication (size hxb)

15

lstm 
computation required

equations:

operations:

         =     (                 ;           1 +         )
         =     (                 ;           1 +         )
         =     (                 ;           1 +         )

           = tanh(                 ;           1 +         )

         =                         1 +                        

        =              tanh(        )

for a batch of b, wt, ht are matrices of size 
hxb.

1x matrix-id127 (input 
2hxb, output 4hxb)

2x pointwise tanh (size hxb)

3x pointwise sigmoid (size hxb)

5x pointwise add (size hxb)

2x pointwise multiplication (size hxb)

16

matrix-id127s 
known as gemm

a matrix-id127 (or gemm) c=ab can be parameterized by three matrix 
dimensions: m, n and k.

floating point ops = mn(2k-1)     2mnk

bytes through memory = sizeof(datatype) * (mk + kn + mn)

n

c

m

=

m

k

a

n

b

k

17

lstm matrices
flop:byte ratio

from before, data input is shape 2hxb, data output is shape 4hxb.

if a is our parameter matrix, this gives m=4h, n=b, k=2h

    floating point ops     2*4h*b*2h = 16hhb

    bytes through memory (fp32) = 4(8hh + 2hb + 4hb)

this gives a flops:byte ratio of 16hhb:4(8hh + 2hb + 4hb) = 2hb:3b+4h

18

memory vs flop bound
expected batch size

batch size (b) can vary for a given model while giving a similarly performing model.

when training memory capacity required scales roughly linearly with batch size. 

convergence can be poor if batch size is too large or too small. 

in deployment it may be only a few samples are available to be processed at once which 
potentially limits batching

19

roofline model for lstm
selecting an efficient minibatch size

roofline model for lstm gemm. h=2048, nvidia p100

s
/
p
o
l
f
g

16384

8192

4096

2048

1024

512

256

1

2

4

8

16

32

64

128

256

512 1024 2048 4096

minibatch

theory

20

roofline model for lstm
selecting an efficient minibatch size

roofline model for lstm gemm. h=2048, nvidia p100

s
/
p
o
l
f
g

16384

8192

4096

2048

1024

512

256

1

2

4

8

16

32

64

128

256

512 1024 2048 4096

minibatch

theory

achieved

21

lstm network level optimizations

22

network level optimizations

problem statement: given a fixed h and b, how can i make my network execute 
faster?

i will talk about three optimizations that could be performed:

1. reducing memory traffic

2. reducing overheads

3.

increasing parallelism

see 1 for other possible optimizations

1optimizing performance of recurrent neural networks on gpus. appleyard et al., arxiv 2016.

23

reducing memory traffic
optimization #1

for small (unchangeable) minibatch we are bandwidth limited. the majority of the 
bandwidth is loading the a matrix, which is constant over time.

if we can reduce the amount of times we load a, the faster we expect to go.

24

unrolling over time
before

    perform the same operation repeatedly on a combination of:

   

previous state

    new input

          1

       

       

lstm 
cell

        

25

unrolling over time
after

    perform the same operation repeatedly on a combination of:

   

previous state

    new input

   0

   1

   2

   3

   4

   5

   6

   .

    0

    1

    2

    3

    4

    5

    6

26

freeing dependencies
previous layer input vs recurrent input

the lstm equations have many id127s of the form:                 ;           1

here wt is the input from the previous layer, ht-1 the input from the previous 
recurrent step

we can isolate these two parts of the id127:

                ;           1 =                      +                       1

this results in one id127 operating on the output from the previous 
layer and one operating on the output from the previous step. 

functionally this is the same.

27

fusing operations over time
effective minibatch increase

each arrow can be seen as a id127. because the vertical arrows are 
independent, they can be grouped

   0

   1

   2

   3

   4

   5

   .

    0

    1

    2

    3

    4

    5

28

fusing operations over time
effective minibatch increase

each arrow can be seen as a id127. because the vertical arrows are 
independent, they can be grouped

with x times as many input elements to process, grouping by x is equivalent to 
increasing minibatch by x (parameter matrix is time-invarient).

parallelism can be preseved by task parallelism or    streaming   

   0

   1

   2

   3

   4

   5

   .

    0

    1

    2

    3

    4

    5

29

persistent id56s
advanced technique

fusing operations over time only helps some of our id127s: the 
recurrent operations are still bandwidth limited.

diamos et al.2  have proposed a method to keep the recurrent matrix in on-chip at a 
very high read bandwidth

some constraints to implementation due to size limits of on-chip memory, but 
impressive speedups for small batches (>10x)

2persistent id56s: stashing recurrent weights on-chip, diamos et al., icml 2016

30

overhead reduction
optimization #2

equations:

operations:

         =     (                 ;           1 +         )
         =     (                 ;           1 +         )
         =     (                 ;           1 +         )

           = tanh(                 ;           1 +         )

         =                         1 +                        

        =              tanh(        )

for a batch of b, wt, ht are matrices of size 
hxb.

1x matrix-id127 (input 2hxb, 
output 4hxb)

2x pointwise tanh (size hxb)

3x pointwise sigmoid (size hxb)

5x pointwise add (size hxb)

2x pointwise multiplication (size hxb)

31

overhead reduction
optimization #2

equations:

operations:

         =     (                 ;           1 +         )
         =     (                 ;           1 +         )
         =     (                 ;           1 +         )

           = tanh(                 ;           1 +         )

         =                         1 +                        

        =              tanh(        )

for a batch of b, wt, ht are matrices of size 
hxb.

1x matrix-id127 (input 2hxb, 
output 4hxb)

2x pointwise tanh (input hxb)

2x pointwise sigmoid (input hxb)

5x pointwise add (input hxb)

2x pointwise multiplication (input hxb)

32

pointwise operations
optimization #2

a na  ve implementation of these pointwise operations on the gpu would implement
each as a separate gpu kernel

a kernel essentially means:

    cpu launches the kernel to the gpu with some small overhead

    for each pointwise element, launch one thread

    this thread reads the value it is responsible for, does a simple operation and 

writes its result

there are two problems with this: overhead of kernel setup and bandwidth

33

pointwise operations
solution

a solution to this is to fuse the pointwise operations into one kernel

instead of launching on kernel per operation, launch one to do the entire series

source: https://devblogs.nvidia.com/parallelforall/optimizing-recurrent-neural-networks-cudnn-5/

in the above case, this is more than 2x speedup!

34

increasing parallelism
optimization #3

   .

   .

   .

35

increasing parallelism
optimization #3

   .

   .

   .

36

increasing parallelism
optimization #3

   .

   .

   .

37

increasing parallelism
optimization #3

   .

   .

   .

38

increasing parallelism
optimization #3

   .

   .

   .

39

increasing parallelism
optimization #3

   .

   .

   .

40

increasing parallelism
optimization #3

   .

   .

   .

41

increasing parallelism
optimization #3

   .

   .

   .

42

increasing parallelism
optimization #3

   .

   .

   .

43

performance
using streams + layers

output of the nvidia visual profiler:

44

three optimizations
speedup

for an lstm with the following properties:

   

   

512 hidden units

100 recurrent iterations

    minibatch 64

   

four layers

before: 83.6ms/pass

after: 23.8ms/pass

45

cudnn
library for neural networks

nvidia provides a free library for accelerating neural network operations: cudnn

cudnn is integrated into all major frameworks and provides optimized routines for 
many neural network architectures, including basic id56s, grus and lstms.

more info and download here: developer.nvidia.com/cudnn

other libraries are available for blas operations, fft, random number generation, 
and many more operations.

46

final words

both hardware and software choices can greatly reduce time to solution. 

it   s important to be aware of performance trade-offs being made when designing and 
executing a network.

libraries and frameworks are intended to do as much of this as possible for you, but 
it may be you have to do a little extra work to get best performance, especially 
when straying off the beaten path

47

