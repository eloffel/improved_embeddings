conditional 

id38

chris dyer

review: unconditional lms
a language model assigns probabilities to sequences of   
words,                                  .
w = (w1, w2, . . . , w`)
we saw that it is helpful to decompose this id203   
using the chain rule, as follows:

p(w) = p(w1)     p(w2 | w1)     p(w3 | w1, w2)              

p(w` | w1, . . . , w` 1)

p(wt | w1, . . . , wt 1)

=

|w|yt=1

this reduces the id38 problem to modeling   
the id203 of the next word, given the history of   
preceding words.

unconditional lms with id56s

random variable

p(w5|w1,w2,w3,w4)

z }| {

h4

softmax

id56 hidden state

vector, length=|vocab|

h1

w1

h0

h2

w2

w1

observed   
context word

w2

vector   
(id27)

h3

w3

w3

w4

w4

conditional lms
a conditional language model assigns probabilities to 
sequences of words,                                  , given some 
w = (w1, w2, . . . , w`)
conditioning context,    .
x
as with unconditional models, it is again helpful to use   
the chain rule to decompose this id203:

p(w | x) =

p(wt | x, w1, w2, . . . , wt 1)

what is the id203 of the next word, given the history of   
previously generated words and conditioning context    ?
x

`yt=1

conditional lms

         input   
x
an author
a topic label
{spam, not_spam}
a sentence in french
a sentence in english
a sentence in english
an image
a document
a document
meterological measurements
acoustic signal
conversational history + database dialogue system response
a question + a document
a question + an image

         text output   
w
a document written by that author
an article about that topic
an email
its english translation
its french translation
its chinese translation
a text description of the image
its summary
its translation
a weather report
transcription of speech

its answer
its answer

conditional lms

         input   
x
an author
a topic label
{spam, not_spam}
a sentence in french
a sentence in english
a sentence in english
an image
a document
a document
meterological measurements
acoustic signal
conversational history + database dialogue system response
a question + a document
a question + an image

         text output   
w
a document written by that author
an article about that topic
an email
its english translation
its french translation
its chinese translation
a text description of the image
its summary
its translation
a weather report
transcription of speech

its answer
its answer

conditional lms

         input   
x
an author
a topic label
{spam, not_spam}
a sentence in french
a sentence in english
a sentence in english
an image
a document
a document
meterological measurements
acoustic signal
conversational history + database dialogue system response
a question + a document
a question + an image

         text output   
w
a document written by that author
an article about that topic
an email
its english translation
its french translation
its chinese translation
a text description of the image
its summary
its translation
a weather report
transcription of speech

its answer
its answer

k
e
e

his w

t

n ext w eek
tw o w eeks

data for training conditional lms
to train conditional language models, we need paired   
samples,                     .
{(xi, wi)}n
data availability varies. it   s easy to think of tasks that   
could be solved by conditional language models, but the   
data just doesn   t exist.

i=1

relatively large amounts of data for:

translation, summarisation, id134,   
id103

algorithmic challenges
we often want to    nd the most likely      given some    . this   
is unfortunately generally an intractable problem.

w

x

w    = arg max
w

p(w | x)

we therefore approximate it using a id125 or with   
monte carlo methods since                           is often   
computationally easy. 
improving search/id136 is an open research question.

w(i)     p(w | x)

how can we search more effectively?
can we get guarantees that we have found the max?
can we limit the model a bit to make search easier?

evaluating conditional lms
how good is our conditional language model?
these are language models, we can use cross-id178   
or perplexity.
task-speci   c evaluation. compare the model   s most likely 
output to human-generated expected output using a   
task-speci   c evaluation metric    .
l

okay to implement, hard to interpret

w    = arg max
w

l(w   , wref)
examples of    : id7, meteor, wer, id8.

p(w | x)

l

easy to implement, okay to interpret

human evaluation.

hard to implement, easy to interpret

evaluating conditional lms
how good is our conditional language model?
these are language models, we can use cross-id178   
or perplexity.
task-speci   c evaluation. compare the model   s most likely 
output to human-generated expected output using a   
task-speci   c evaluation metric    .
l

okay to implement, hard to interpret

w    = arg max
w

l(w   , wref)
examples of    : id7, meteor, wer, id8.

p(w | x)

l

easy to implement, okay to interpret

human evaluation.

hard to implement, easy to interpret

lecture overview
the rest of this lecture will look at    encoder-decoder      
models that learn a function that maps     into a    xed-size   
vector and then uses a language model to    decode      
that vector into a sequence of words,    .w

x

x

kunst kann nicht gelehrt werden   

w

artistry can   t be taught   

lecture overview
the rest of this lecture will look at    encoder-decoder      
models that learn a function that maps     into a    xed-size   
vector and then uses a language model to    decode      
that vector into a sequence of words,    .w

x

x

w

a dog is playing on the beach.

lecture overview
    two questions 

    how do we encode    as a    xed-size vector,   ? 

x

c

- problem (or at least modality) speci   c
- think about assumptions
c

    how do we condition on    in the decoding 

model?
- less problem speci   c
- we will review solution/architectures

kalchbrenner and blunsom 2013
encoder

c = embed(x)
s = vc

recurrent decoder

recurrent connection
embedding of  wt 1

source sentence

ht = g(w[ht 1; wt 1] + s + b])
ut = pht + b0

learnt bias

p(wt | x, w<t) = softmax(ut)
recall unconditional id56

ht = g(w[ht 1; wt 1] + b])

k&b 2013: encoder
how should we de   ne                        ?
c = embed(x)
the simplest model possible:

c =xi

xi

x1

x2

x3

x4

x5

x6

x1

x2

x3

x4

x5

x6

what do you think of this model?

k&b 2013: csm encoder
how should we de   ne                        ?
c = embed(x)
convolutional sentence model (csm)

k&b 2013: csm encoder

    good

    convolutions learn interactions among features in a local 

context 

    by stacking them, longer range dependencies can be learnt 

    deep convnets have a branching structure similar to trees, 

but no parser is required 

    bad 

    sentences have different lengths, need different depth trees; 

convnets are not usually so dynamic, but see*

* kalchbrenner et al. (2014). a convolutional neural network for modelling sentences. in proc. acl.

k&b 2013: id56 decoder
encoder

c = embed(x)
s = vc

recurrent decoder

recurrent connection
embedding of  wt 1

source sentence

ht = g(w[ht 1; wt 1] + s + b])
ut = pht + b0

learnt bias

p(wt | x, w<t) = softmax(ut)
recall unconditional id56

ht = g(w[ht 1; wt 1] + b])

k&b 2013: id56 decoder

s

h0

h1

x1

<s>

k&b 2013: id56 decoder

s

  p1

h0

softmax

h1

x1

<s>

k&b 2013: id56 decoder
p(tom | s,hsi)

s

  p1

h0

tom

   

softmax

h1

x1

<s>

k&b 2013: id56 decoder
p(tom | s,hsi)

   p(likes | s,hsi, tom)

s

  p1

tom

   

likes

   

softmax

softmax

h2

x2

h0

h1

x1

<s>

k&b 2013: id56 decoder
p(tom | s,hsi)

   p(likes | s,hsi, tom)

   p(beer | s,hsi, tom, likes)

s

  p1

tom

   

likes

   

beer

   

softmax

softmax

softmax

h2

x2

h3

x3

h0

h1

x1

<s>

k&b 2013: id56 decoder
p(tom | s,hsi)

   p(likes | s,hsi, tom)

   p(beer | s,hsi, tom, likes)

   p(h\si | s,hsi, tom, likes, beer)

likes

   

beer

   

</s>

   

s

  p1

tom

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

h0

h1

x1

<s>

sutskever et al. (2014)
lstm encoder

(c0, h0) are parameters
(ci, hi) = lstm(xi, ci 1, hi 1)

the encoding is               where             .
` = |x|

(c`, h`)

lstm decoder

w0 = hsi

(ct+`, ht+`) = lstm(wt 1, ct+` 1, ht+` 1)

ut = pht+` + b
p(wt | x, w<t) = softmax(ut)

sutskever et al. (2014)

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014)

beginnings

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014)

beginnings

are

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014)

beginnings

are

dif   cult

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014)

beginnings

are

dif   cult

stop

         

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014)

    good

    id56s deal naturally with sequences of various lengths 
    lstms in principle can propagate gradients a long 

distance 

    very simple architecture! 

    bad 

    the hidden state has to remember a lot of information! 

(we will return to this problem on thursday.)

sutskever et al. (2014): tricks

beginnings

are

dif   cult

stop

         

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014): tricks
read the input sequence    backwards   : +4 id7

beginnings

are

dif   cult

stop

         

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014): tricks
use an ensemble of j independently trained models.

ensemble of 2 models: +3 id7
ensemble of 5 models: +4.5 id7

decoder:

(c(j)

t+`) = lstm(j)(wt 1, c(j)
t+`, h(j)
u(j)
t = ph(j)

t + b(j)

t+` 1, h(j)

t+` 1)

ut =

1
j

u(j0)

jxj0=1

p(wt | x, w<t) = softmax(ut)

a word about decoding
in general, we want to    nd the most probable (map) output   
given the input, i.e.

w    = arg max
w

p(w | x)

= arg max

w

|w|xt=1

log p(wt | x, w<t)

a word about decoding
in general, we want to    nd the most probable (map) output   
given the input, i.e.

w    = arg max
w

p(w | x)

= arg max

w

log p(wt | x, w<t)

|w|xt=1

this is, for general id56s, a hard problem. we therefore 
approximate it with a greedy search:
p(w1 | x)
p(w2 | x, w   1)

w   1 = arg max
w1
w   2 = arg max
w2

...

w   t = arg max
w2

p(wt | x, w   <t)

a word about decoding
in general, we want to    nd the most probable (map) output   
given the input, i.e.

w    = arg max
w

p(w | x)

|w|xt=1

= arg max

w

log p(wt | x, w<t)

undecidable :(
this is, for general id56s, a hard problem. we therefore 
approximate it with a greedy search:
p(w1 | x)
p(w2 | x, w   1)

w   1 = arg max
w1
w   2 = arg max
w2

...

w   t = arg max
w2

p(wt | x, w   <t)

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink

beer

hsilogprob=0

w0

w1

w2

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

w2

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink
logprob=-6.93

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

i
logprob=-5.80

w0

w1

w2

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink
logprob=-6.93

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

i
logprob=-5.80

beer
logprob=-8.66

drink
logprob=-2.87

w2

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink
logprob=-6.93

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

i
logprob=-5.80

beer
logprob=-8.66

drink
logprob=-2.87

w2

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink
logprob=-6.93

drink
logprob=-6.28

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

i
logprob=-5.80

beer
logprob=-8.66

drink
logprob=-2.87

w2

like
logprob=-7.31

beer
logprob=-3.04

wine
logprob=-5.12

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink
logprob=-6.93

drink
logprob=-6.28

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

i
logprob=-5.80

beer
logprob=-8.66

drink
logprob=-2.87

w2

like
logprob=-7.31

beer
logprob=-3.04

wine
logprob=-5.12

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink
logprob=-6.93

drink
logprob=-6.28

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

i
logprob=-5.80

beer
logprob=-8.66

drink
logprob=-2.87

w2

like
logprob=-7.31

beer
logprob=-3.04

wine
logprob=-5.12

w3

sutskever et al. (2014): tricks

use id125: +1 id7

x = bier trinke ich
i

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

drink
logprob=-6.93

drink
logprob=-6.28

i
logprob=-5.80

beer
logprob=-8.66

drink
logprob=-2.87

w2

like
logprob=-7.31

beer
logprob=-3.04

wine
logprob=-5.12

w3

image id134

    neural networks are great for working with multiple 

modalities   everything is a vector! 

    image id134 can therefore use the same 

techniques as translation modeling 

    a word about data 

    relatively few captioned images are available 

    pre-train image embedding model using another 

task, like image identi   cation (e.g., id163)

kiros et al. (2013)

    looks a lot like kalchbrenner and blunsom (2013) 

    convolutional network on the input 

    id165 language model on the output 

    innovation: multiplicative interactions in the 

decoder id165 model

kiros et al. (2013)

encoder

x = embed(x)

kiros et al. (2013)

encoder
unconditional id165 lm:

x = embed(x)

embedding of wt 1

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

p(wt | x, wt 1

t n+1) = softmax(ut)

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

p(wt | x, wt 1

t n+1) = softmax(ut)

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj

how big is this tensor?

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj
what   s the intuition here?

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj

how big is this tensor?

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj
(u 2 r|v |   d, v 2 rd   k)
wi = uw,ivi,j
rt = w[wt n+1; wt n+2; . . . ; wt 1] + cx

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

p(wt | x, wt 1
multiplicative id165 lm:

t n+1) = softmax(ut)

(u 2 r|v |   d, v 2 rd   k)
wi = uw,ivi,j
rt = w[wt n+1; wt n+2; . . . ; wt 1] + cx
ht = (wf rrt)   (wf xx)
ut = pht + b

p(wt | x, w<t) = softmax(ut)

kiros et al. (2013)

    two take-home messages: 

    feed-forward id165 models can be used in 

place of id56s in conditional models 

    modeling interactions between input modalities 

holds a lot of promise 

    although mlp-type models can approximate 

higher order tensors, multiplicative models 
appear to make learning interactions easier

questions?

