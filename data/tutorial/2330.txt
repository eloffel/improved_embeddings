conditional language 

modeling   

with attention

chris dyer

review: conditional lms
a conditional language model assigns probabilities to 
sequences of words,                                  , given some 
w = (w1, w2, . . . , w`)
conditioning context,    .
x
as with unconditional models, it is again helpful to use   
the chain rule to decompose this id203:

p(w | x) =

p(wt | x, w1, w2, . . . , wt 1)

what is the id203 of the next word, given the history of   
previously generated words and conditioning context    ?
x

`yt=1

sutskever et al. (2014)

            

            

            

sutskever et al. (2014)

            

            

            

akiko

   

  p1

likes

   

pimm   s

   

</s>

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

sutskever et al. (2014)

h1

x1

<s>

            

            

            

akiko

   

  p1

likes

   

pimm   s

   

</s>

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

sutskever et al. (2014)

h1

x1

<s>

conditiong with vectors
we are compressing a lot of information in a    nite-sized   
vector.

conditiong with vectors
we are compressing a lot of information in a    nite-sized   
vector.

   you can't cram the meaning of a whole %&!$#   

sentence into a single $&!#* vector!   

prof. ray mooney

conditiong with vectors
we are compressing a lot of information in a    nite-sized   
vector.

gradients have a long way to travel. even lstms forget!

conditiong with vectors
we are compressing a lot of information in a    nite-sized   
vector.

gradients have a long way to travel. even lstms forget!

what is to be done?

outline of lecture

    machine translation with attention 

    image id134 with attention

solving the vector problem 

in translation

    represent a source sentence as a matrix 

    generate a target sentence from a matrix 

    this will 

    solve the capacity problem 

    solve the gradient    ow problem

sentences as matrices

    problem with the    xed-size vector model 

    sentences are of different sizes but vectors are of 

the same size 

    solution: use matrices instead 

    fixed number of rows, but number of columns 

depends on the number of words 

    usually |f| = #cols

sentences as matrices

ich m  ochte ein bier

sentences as matrices

ich m  ochte ein bier

mach   s gut

sentences as matrices

ich m  ochte ein bier

mach   s gut

die wahrheiten der menschen sind die unwiderlegbaren irrt  umer

sentences as matrices

ich m  ochte ein bier

mach   s gut

die wahrheiten der menschen sind die unwiderlegbaren irrt  umer

question: how do we build these matrices?

with concatenation

    each word type is represented by an n-dimensional 

vector 

    take all of the vectors for the sentence and 

concatenate them into a matrix 

    simplest possible model 

    so simple, no one has bothered to publish how 

well/badly it works!

x1

x2

x3

x4

ich m  chte ein

bier

fi = xi

x1

x2

x3

x4

ich m  chte ein

bier

fi = xi

x1

x2

x3

x4

ich m  chte ein

bier

f 2 rn   |f|

ich m  ochte ein bier

with convolutional nets

    apply convolutional networks to transform the naive 

concatenated matrix to obtain a context-dependent matrix 

    explored in a recent iclr submission by gehring et al., 

2016 (from fair) 
    closely related to the neural translation model 
proposed by kalchbrenner and blunsom, 2013 

    note: convnets usually have a    pooling    operation at the 
top level that results in a    xed-sized representation. for 
sentences, leave this out.

x1

x2

x3

x4

ich m  chte ein

bier

x1

x2

x3

x4

ich m  chte ein

bier

   

filter 1

x1

x2

x3

x4

ich m  chte ein

bier

   

   

filter 1

filter 2

x1

x2

x3

x4

ich m  chte ein

bier

   

   

filter 1

filter 2

x1

x2

x3

x4

f 2 rf (n)   g(|f|)

ich m  ochte ein bier

ich m  chte ein

bier

with bidirectional id56s

    by far the most widely used matrix representation, due to 

bahdanau et al (2015)  

    one column per word 

    each column (word) has two halves concatenated together: 

    a    forward representation   , i.e., a word and its left context 

    a    reverse representation   , i.e., a word and its right context 
    implementation: bidirectional id56s (grus or lstms) to read f 

from left to right and right to left, concatenate representations

x1

x2

x3

x4

ich m  chte ein

bier

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

  h 1

  h 2

  h 3

  h 4

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

f 2 r2n   |f|

x1

x2

x3

x4

ich m  ochte ein bier

ich m  chte ein

bier

where are we in 2017?

    there are lots of ways to construct f 

    very little systematic work comparing them 

    there are many more undiscovered things out there 

    convolutions are particularly interesting and under-explored 

    syntactic information can help (sennrich & haddow, 2016; nadejde 

et al., 2017), but many more integration strategies are possible 

    try something with phrase types instead of word types?
multi-word expressions are a pain in the neck .

generation from matrices

    we have a matrix f representing the input, now we need to generate from it 
    bahdanau et al. (2015) were the    rst to propose using attention for translating from matrix-

encoded sentences 

    high-level idea 

    generate the output sentence word by word using an id56 
    at each output position t, the id56 receives two inputs (in addition to any recurrent inputs) 

    a    xed-size vector embedding of the previously generated output symbol et-1 
    a    xed-size vector encoding a    view    of the input matrix 

    how do we get a    xed-size vector from a matrix that changes over time? 

    bahdanau et al: do a weighted sum of the columns of f (i.e., words) based on how 

important they are at the current time step. (i.e., just a matrix-vector product fat) 

    the weighting of the input columns at each time-step (at) is called attention

recall id56s   

    

recall id56s   

    

recall id56s   

i'd

    

recall id56s   

i'd

    

i'd

recall id56s   

i'd

like

    

i'd

recall id56s   

    

    

ich m  ochte ein bier

    

ich m  ochte ein bier

    

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

    

c1 = fa1

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

    

c1 = fa1

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

i'd

    

c1 = fa1

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

i'd

    

i'd

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

    

i'd

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

    

i'd

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

z

}|

{

    

i'd

c2 = fa2

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

z

}|

{

    

i'd

c2 = fa2

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

a

    

i'd

like

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

i'd

like

a

beer

z

}|

{

    

i'd

like

a

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

a

beer

stopstop

    

i'd

like

a

beer

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

attention

    how do we know what to attend to at each time-

step? 

    that is, how do we compute     ?at

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every column: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the expected input embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)

(called     in the paper)

ct = fat

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every column: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the expected input embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)

(called     in the paper)

ct = fat

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every column: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the expected input embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)

(called     in the paper)

ct = fat

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every column: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the expected input embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)

(called     in the paper)

ct = fat

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every column: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the expected input embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(called     in the paper)

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)
ct = fat

nonlinear attention-energy 

model

    in the actual model, bahdanau et al. replace the dot 

product between the columns of f and rt with an mlp: 

ut = f>rt
ut = tanh (wf + rt) v

(simple model)
(bahdanau et al)

    here, w and v are learned parameters of appropriate 

dimension and +    broadcasts    over the |f| columns in wf 

    this can learn more complex interactions 

    it is unclear if the added complexity is necessary for 

good performance

nonlinear attention-energy 

model

    in the actual model, bahdanau et al. replace the dot 

product between the columns of f and rt with an mlp: 

ut = f>rt
ut = v> tanh(wf + rt)

(simple model)
(bahdanau et al)

    here, w and v are learned parameters of appropriate 

dimension and +    broadcasts    over the |f| columns in wf 

    this can learn more complex interactions 

    it is unclear if the added complexity is necessary for 

good performance

nonlinear attention-energy 

model

    in the actual model, bahdanau et al. replace the dot 

product between the columns of f and rt with an mlp: 

ut = f>rt
ut = v> tanh(wf + rt)

(simple model)
(bahdanau et al)

    here, w and v are learned parameters of appropriate 

dimension and +    broadcasts    over the |f| columns in wf 

    this can learn more complex interactions 

    it is unclear if the added complexity is necessary for 

good performance

putting it all together

(part 1 of lecture)

f = encodeasmatrix(f )
e0 = hsi
s0 = w (learned initial state; bahdanau uses           )u  h 1
t = 0
while et 6= h/si :

t = t + 1
rt = vst 1
ut = v> tanh(wf + rt)
at = softmax(ut)
ct = fat
st = id56(st 1, [et 1; ct])
yt = softmax(pst + b)
et | e<t     categorical(yt)

}(compute attention; part 2 of lecture)

(        is a learned embedding of    )
et 1
et
(    and    are learned parameters)
p

b

putting it all together

(part 1 of lecture)

f = encodeasmatrix(f )
e0 = hsi
s0 = w (learned initial state; bahdanau uses           )u  h 1
t = 0
while et 6= h/si :

doesn   t depend on output decisions

}(compute attention; part 2 of lecture)

(        is a learned embedding of    )
et 1
et
(    and    are learned parameters)
p

b

t = t + 1
rt = vst 1
ut = v> tanh(wf + rt)
at = softmax(ut)
ct = fat
st = id56(st 1, [et 1; ct])
yt = softmax(pst + b)
et | e<t     categorical(yt)

putting it all together

(part 1 of lecture)

f = encodeasmatrix(f )
e0 = hsi
s0 = w (learned initial state; bahdanau uses           )u  h 1
t = 0
x = wf
while et 6= h/si :

x

t = t + 1
rt = vst 1
ut = v> tanh(wf + rt)
at = softmax(ut)
ct = fat
st = id56(st 1, [et 1; ct])
yt = softmax(pst + b)
et | e<t     categorical(yt)

}(compute attention; part 2 of lecture)

(        is a learned embedding of    )
et
et 1
(    and    are learned parameters)
p

b

putting it all together

(part 1 of lecture)

f = encodeasmatrix(f )
e0 = hsi
s0 = w (learned initial state; bahdanau uses           )u  h 1
t = 0
x = wf
while et 6= h/si :

t = t + 1
rt = vst 1
ut = v> tanh(x + rt)
at = softmax(ut)
ct = fat
st = id56(st 1, [et 1; ct])
yt = softmax(pst + b)
et | e<t     categorical(yt)

}(compute attention; part 2 of lecture)

(        is a learned embedding of    )
et
et 1
(    and    are learned parameters)
p

b

attention in mt

add attention to id195 translation: +11 id7

model variant

   early binding   

i'd

like

a

beer

stopstop

   late binding   

    
i'd

i'd
like

like
a

a
beer

beer

stopstop

    

i'd

like

a

beer

model variant

    what are the relative advantages of early binding 

versus late binding?

summary

    attention is closely related to    pooling    operations in convnets 

(and other architectures) 

    bahdanau   s attention model seems to only cares about 

   content    

    no obvious bias in favor of diagonals, short jumps, fertility, 

etc. 

    some work has begun to add other    structural    biases 

(luong et al., 2015; cohn et al., 2016), but there are lots more 
opportunities 

    attention weights provide interpretation you can look at

a word about gradients

i'd

like

a

beer

z

}|

{

    

i'd

like

a

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

a

beer

z

}|

{

    

i'd

like

a

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

a

beer

z

}|

{

    

i'd

like

a

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

attention and translation

    cho   s question: does a translator read and memorize 
the input sentence/document and then generate the 
output? 

    compressing the entire input sentence into a vector 

basically says    memorize the sentence    

    common sense experience says translators refer 

back and forth to the input. (also backed up by eye-
tracking studies) 

    should humans be a model for machines?

outline of lecture

    machine translation with attention 

    image id134 with attention

            

            

            

akiko

   

  p1

likes

   

pimm   s

   

</s>

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

sutskever et al. (2014)

h1

x1

<s>

a

   

  p1

man

   

is

   

rowing

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

vinyals et al. (2014) show and tell: a neural image caption generator

h1

x1

<s>

image id134

    can attention help caption modeling?

xu et al. (2015, icml)

regions in convnets

each point in a    higher    level of a convnet    
de   nes spatially localised feature vectors(/matrices).
xu et al. calls these    annotation vectors   , ai, i 2 {1, . . . , l}

a1

f =

a1h

i

a2

f =

a1h

a2

i

a3

f =

a1h

a2 a3       

i

attention

    attention    weights    (    ) are computed using 

exactly the same technique as discussed above 
    deterministic soft attention (bahdanau et al., 2014)   

at

(weighted average)

ct = fat

    stochastic hard attention (xu et al., 2015)

st     categorical(at)
ct = f:,st

(sample a column)

    what are the bene   ts of this model? 

    what are the challenges of this model?

attention

    attention    weights    (    ) are computed using 

exactly the same technique as discussed above 
    deterministic soft attention (bahdanau et al., 2014)   

at

ct = fat

(weighted average)

    stochastic hard attention (xu et al., 2015)

st     categorical(at)
ct = f:,st

(sample a column)

    what are the bene   ts of this model? 

    what are the challenges of this model?

attention

    attention    weights    (    ) are computed using 

exactly the same technique as discussed above 
    deterministic soft attention (bahdanau et al., 2014)   

at

ct = fat

(weighted average)

    stochastic hard attention (xu et al., 2015)

st     categorical(at)
ct = f:,st

(sample a column)

    what are the bene   ts of this model? 

    what are the challenges of learning the 

parameters of this model?

learning hard attention

p(w, s | x)

p(s | x)p(w | x, s)

l =   log p(w | x)

=   logxs
=   logxs
     xs

mc

     

1
n

nxi=1

p(s | x) log p(w | x, s)

(jensen   s inequality)

p(s(i) | x) log p(w | x, s)

learning hard attention

p(w, s | x)

p(s | x)p(w | x, s)

l =   log p(w | x)

=   logxs
=   logxs
     xs

mc

     

1
n

nxi=1

p(s | x) log p(w | x, s)

(jensen   s inequality)

p(s(i) | x) log p(w | x, s)

learning hard attention

p(w, s | x)

p(s | x)p(w | x, s)

l =   log p(w | x)

=   logxs
=   logxs
     xs

mc

     

1
n

nxi=1

p(s | x) log p(w | x, s)

(jensen   s inequality)

p(s(i) | x) log p(w | x, s)

learning hard attention

    sample n sequences of attention decisions from the 

model 

    the gradient is the id203 of the gradient of the 

id203 of this sequence scaled by the log id203 
of generating the target words using that sequence of 
attention decisions 

    this is equivalent to using the reinforce algorithm 

(williams, 1992) using the log id203 of the observed 
words as a    reward function   . reinforce a policy 
gradient algorithm used for id23.

attention in captioning

add soft attention to image captioning: +2 id7
add hard attention to image captioning: +4 id7

summary

    signi   cant performance improvements 

    better performance over vector-based encodings 

    better performance with smaller training data sets 

    model interpretability 
    better gradient    ow 

    better capacity (especially obvious for translation)

questions?

