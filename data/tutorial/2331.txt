id103

andrew senior
(deepmind london)
many thanks for slides to vincent vanhoucke, heiga
zen, jun song & andrew zisserman
february 21st, 2017. oxford university

outline

id103

acoustic representation
phonetic representation
history
probabilistic id103

neural network id103

hybrid neural networks
training losses
sequence discriminative training
new architectures

other topics

id103 problem

automatic id103 (asr)

       ok google, directions home   

text-to-id133 (tts)
   take the    rst left       

andrew senior

id103

1 of 63

speech problems

    automatic id103
    spontaneous vs read speech
    large vocabulary
    in noise
    low resource
    far-   eld
    accent-independent
    speaker-adaptive

    text to speech
    low resource
    realistic id144
    speaker identi   cation
    speech enhancement
    speech separation

andrew senior

id103

2 of 63

outline

id103

acoustic representation
phonetic representation
history
probabilistic id103

neural network id103

hybrid neural networks
training losses
sequence discriminative training
new architectures

other topics

what is speech     physical realisation

    waves of changing air pressure.
    realised through excitation from the vocal cords
    modulated by the vocal tract.
    modulated by the articulators (tongue, teeth, lips).
    vowels produced with an open vocal tract (stationary)
    consonants are constrictions of vocal tract.
    converted to voltage with a microphone.
    sampled with an analogue to digital converter

    can be parameterized by position of tongue.

andrew senior

id103

4 of 63

timeamplitudesampling & quantizationspeech representation

    human hearing is ~50hz-20khz
    human speech is ~85hz   8khz
    telephone speech has 8khz sampling: 300hz   4khz bandwidth
    1 bit per sample can be intelligible
    cd is 44.1khz 16 bits per sample
    contemporary speech processing mostly around 16khz 16bits/sample

andrew senior

id103

5 of 63

speech representation

we want a low-dimensionality representation, invariant to speaker,
background noise, rate of speaking etc.
    fourier analysis shows energy in di   erent frequency bands.
    windowed short-term fast fourier transform
    e.g. fft on overlapping 25ms windows (400 samples) taken every

10ms
    energy vs frequency [discrete] vs time [discrete]

andrew senior

id103

6 of 63

mel frequency representation

    fft is still too high-dimensional.
    downsample by local weighted averages on mel scale non-linear

spacing, and take a log. m = 1127 ln(1 + f

700 )

    result in log-mel features (default for neural network speech

modelling.)

    40+ dimensional features per frame

andrew senior

id103

7 of 63

mfccs

    mel frequency cepstral coe   cients - mfccs are the discrete cosine

transformation of the mel    lterbank energies. whitened and
low-dimensional.

    similar to principal components of log spectra.
    gmm id103 systems may use 13 mfccs
    perceptual linear prediction     a common alternative representation.
    frame stacking- it   s common to concatenate several consecutive

frames.

    e.g. 26 for fully-connected dnn. 8 for lstm.
    gmms used local di   erences (deltas) and second-order di   erences
(delta-deltas) to capture dynamics. (13 + 13 + 13 dimensional)

    ultimately use ~39 dimensional id156
(~class-aware pca) projection of 9 stacked mfcc vectors.

andrew senior

id103

8 of 63

outline

id103

acoustic representation
phonetic representation
history
probabilistic id103

neural network id103

hybrid neural networks
training losses
sequence discriminative training
new architectures

other topics

speech as communication

    speech evolved as communication to convey information.
    consists of sentences (in asr we usually talk about    utterances   )
    sentences composed of words
    minimal unit is a    phoneme   

    minimal unit that distinguishes one word from another.
    set of 40   60 distinct sounds.
    vary per language,
    universal representations.

    ipa: international phonetic alphabet,
    x-sampa (ascii)

    distinct words with the same pronunciation:    there    vs    their   

    homophones

    id144

    how something is said can convey meaning.

andrew senior

id103

10 of 63

datasets

    timit

    hand-marked phone boundaries given
    630 speakers    10 utterances

    wall street journal (wsj) 1986 read speech. wsj0 1991, 30k vocab
    broadcast news (bn) 1996 104 hours
    switchboard (swb) 1992. 2000 hours spontaneous telephone speech

500 speakers

    google voice search

    anonymized live tra   c 3m utterances 2000 hours

hand-transcribed 4m vocabulary. constantly refreshed, synthetic
reverberation + additive noise

    deepspeech 5000h read (lombard) speech + swb with additive

noise.

    youtube 125,000 hours aligned captions (soltau et al., 2016)

andrew senior

id103

11 of 63

outline

id103

acoustic representation
phonetic representation
history
probabilistic id103

neural network id103

hybrid neural networks
training losses
sequence discriminative training
new architectures

other topics

rough history

    1960s dynamic time warping
    1970s id48
    multi-layer id88 1986
    id103 with neural networks 1987   1995
    superseded by gmms 1995   2009
    neural network features 2002   
    deep networks 2006    (hinton, 2002)
    deep networks for id103

    good results on timit (mohamed et al., 2009)
    results on large vocabulary systems 2010 (dahl et al., 2011)
    google launches dnn asr product 2011
    dominant paradigm for asr 2012 (hinton et al., 2012)

    recurrent networks for id103 1990, 2012   

    new models (attention, las, neural transducer)

andrew senior

id103

13 of 63

outline

id103

acoustic representation
phonetic representation
history
probabilistic id103

neural network id103

hybrid neural networks
training losses
sequence discriminative training
new architectures

other topics

probabilistic id103

    speech signal represented as an observation sequence o = {ot}.
    we want to    nd the most likely word sequence   w
    we model this with a hidden markov model.

(markovian: memoryless)

    the system has a set of discrete states,
    transitions from state to state according to transition probabilities
    acoustic observation when making a transition is conditioned on
    we seek to recover the state sequence and consequently the word

state alone. p (ot|ct)
sequence.

andrew senior

id103

15 of 63

/k//ae//t//e//th/<s>fundamental equation of id103

we choose the decoder output as the most likely sequence   w from all
possible sequences,      , for an observation sequence o:

  w = arg max

w         p (w|o)
w         p (o|w)p (w)

= arg max

a product of acoustic model and language model scores.

(cid:88)

d,c,p

p (o|w) =

p (o|c)p (c|p)p (p|w)

(1)

(2)

(3)

where p is the phone sequence and c is the state sequence.

andrew senior

id103

16 of 63

    we can model word sequences with a language model.

p (w1, w2, . . . , wn ) = p (w0)

p (wi|w0, . . . , wi   1)

(cid:89)

andrew senior

id103

17 of 63

id103 as transduction
from signal to language.

andrew senior

id103

18 of 63

id103 as transduction     lexicon

construct graph using weighted finite state transducers (wfst)

andrew senior

id103

19 of 63

id103 as transduction

compose lexicon fst with grammar fst l     g

andrew senior

id103

20 of 63

phonetic units

    phonemes:    cat        /k/, /ae/, /t/
    context independent id48 states k1, k2, ae1 . . .

    model onset / middle / end separately.

    context dependent states k1.17, . . .
    context dependent phones
    diphones (pairs of half-phones)
    syllables
    word-parts cf machine translation (wu et al., 2016)
    characters (graphemes)
    whole words sak et al. (2014a, 2015); soltau et al. (2016)

    hard to generalize to rare words.

choice depends on language, size of dataset, task, resources available.

andrew senior

id103

21 of 63

context dependent phonetic id91

    a phone   s realization depends on the preceding and following context
    could improve discrimination if we model di   erent contextual

realizations separately:
e.g ae preceded by k, followed by t: ae+t-k

    but, if we have 42 phones, and 3 states per phone, there are 3    423

context-dependent phones.

    most of these won   t be observed
    so cluster     group together similar distributions and train a joint

model.

    have a    back-o       rule to determine which model to use for

unobserved contexts.

    usually a decision tree.

andrew senior

id103

22 of 63

gaussian mixture models

state.

p (ot|ci) =(cid:80)

    dominant paradigm for asr from 1990 to 2010
    model the id203 distribution of the acoustic features for each

j wijn (ot;   ij,   ij)

parameters under control.

sequence for each utterance

    often use diagonal covariance gaussians to keep number of
    train by the e-m algorithm (dempster et al., 1977) alternating:
    m: forced alignment computing the maximum-likelihood state
    e: parameter (  ,   ) estimation
of components per mixture.
    more components, better    t. 79 parameters / component.
parallelizable by state.

    given an alignment mapping audio frames to states, this is
    hard to share parameters / data across states.

    complex training procedures to incrementally    t increasing numbers

andrew senior

id103

23 of 63

forced alignment

    forced alignment uses a model to compute the maximum likelihood

alignment between speech features and phonetic states.

    for each training utterance, construct the set of phonetic states for

the ground truth transcription.

    use viterbi algorithm to    nd ml monotonic state sequence
    under constraints such as at least one frame per state.
    results in a phonetic label for each frame.
    can give hard or soft segmentation.

andrew senior

id103

24 of 63

 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1silhaukouldizitautsaidsilsilhaukouldiztsaiforced alignment

with a transducer with states ci:

compute state likelihoods at time t

p (o1,...,t|ci) =

p (ot|cj)p (o1,...,t|cj)p (ci|cj)

(cid:88)

j

with transition probabilities: p (ci|cj)
to    nd best path;

p (o1,...,t|ci) = max

j

p (ot|cj)p (o1,...,t|cj)p (ci|cj)

andrew senior

id103

25 of 63

/k//ae//t//e//th/<s>forced alignment t = 0

observation likelihoods p (ot|ci)

start distribution pt=0(ci)

andrew senior

id103

26 of 63

/k//ae//t//e//th/<s>t->/th//e//k//ae//t/.........0.10.60.10.10.10.20.50.10.10.10.30.10.10.30.10.20.10.20.30.10.10.20.50.10.20.30.10.10.40.1t->/th//e//k//ae//t/.........01.00000<s>forced alignment t = 1

observation likelihoods p (ot|ci)

state likelihoods p (o1,...,t|ci)

andrew senior

id103

27 of 63

/k//ae//t//e//th/<s>t->/th//e//k//ae//t/.........0.10.60.10.10.10.20.50.10.10.10.30.10.10.30.10.20.10.20.30.10.10.20.50.10.20.30.10.10.40.1t->/th//e//k//ae//t/.........01.00000<s>0.61forced alignment t = 1

observation likelihoods p (ot|ci)

state likelihoods p (o1,...,t|ci)

andrew senior

id103

28 of 63

/k//ae//t//e//th/<s>t->/th//e//k//ae//t/.........0.10.60.10.10.10.20.50.10.10.10.30.10.10.30.10.20.10.20.30.10.10.20.50.10.20.30.10.10.40.1t->/th//e//k//ae//t/.........01.00000<s>0.61.15.032forced alignment t = t

observation likelihoods p (ot|ci)

state likelihoods p (o1,...,t|ci)

andrew senior

id103

29 of 63

/k//ae//t//e//th/<s>t->/th//e//k//ae//t/.........0.10.60.10.10.10.20.50.10.10.10.30.10.10.30.10.20.10.20.30.10.10.20.50.10.20.30.10.10.40.1t->/th//e//k//ae//t/.........01.0000<s>00.61.15.032decoding

id103 unfolds in much the same way.
now we have a graph instead of a
straight-through path.
optional silences between words
alternative pronunciation paths.
typically use max id203, and work in the log
domain.
hypothesis space is huge, so we only keep a
   beam    of the best paths, and can lose what
would end up being the true best path.

andrew senior

id103

30 of 63

thecataoncehellodogcatdogtwo main paradigms for neural networks for speech

    use neural networks to compute nonlinear feature representations.
       bottleneck    or    tandem    features (hermansky et al., 2000)
    low-dimensional representation is modelled conventionally with
    allows all the gmm machinery and tricks to be exploited.
    use neural networks to estimate phonetic unit probabilities.

gmms.

andrew senior

id103

31 of 63

neural network features

train a neural network to discriminate classes.
use output or a low-dimensional bottleneck layer representation as
features.

hidden
layers

input
layer

output

layer

bottleneck

layer

x1

x2

x3

x4

y1

y2

y3

y4

y5

andrew senior

id103

32 of 63

neural network features

    trap: concatenate plp-hlda features and nn features.
    bottleneck outperforms posterior features (grezl et al., 2007)
    generally dnn features + gmms reach about the same performance

as hybrid dnn-id48 systems, but are much more complex.

andrew senior

id103

33 of 63

outline

id103

acoustic representation
phonetic representation
history
probabilistic id103

neural network id103

hybrid neural networks
training losses
sequence discriminative training
new architectures

other topics

hybrid networks

    train the network as a classi   er with a softmax across the phonetic

units.

    train with cross-id178.
    softmax

(cid:80)n

y (i) =

exp (a (i,   ))
j=1 exp (a (j,   ))
will converge to posterior across phonetic states:
p (ci|ot)

andrew senior

id103

35 of 63

hybrid neural network decoding

now we model p (o|c) with a neural network instead of a gaussian
mixture model. everything else stays the same.

(cid:89)

t

p (o|c) =

p (ot|ct) =

   

p (ot|ct)

p (ct|ot)p (ot)

p (ct)
p (ct|ot)
p (ct)

for observations ot at time t and a cd state sequence ct.
we can ignore p (ot) since it is the same for all decoding paths.
the last term is called the    scaled posterior   :

log p (ot|ct) = log p (ct|ot)        log p (ct)

(4)

(5)

(6)

(7)

empirically (by cross validation) we actually    nd better results with a
   prior smoothing    term        0.8.

id103

andrew senior

36 of 63

input features

neural networks can handle high-dimensional features with correlated
features.
use (26) stacked    lterbank inputs. (40-dimensional mel-spaced
   lterbanks)
example    lters learned in the    rst layer of a fully-connected network:

(33 x 8    lters. each subimage 40 frequency vs 26 time.)

andrew senior

id103

37 of 63

neural network architectures for id103

    fully connected
    convolutional networks (id98s)
    recurrent neural networks (id56s)

    lstms
    grus

andrew senior

id103

38 of 63

convolutional neural networks

    time delay neural networks

    waibel et al. (1989)
    dilated convolutions (peddinti et al., 2015)

    id98s in time or frequency domain. abdel-hamid et al. (2014);

sainath et al. (2013)

    wavenet (van den oord et al., 2016)

andrew senior

id103

39 of 63

recurrent neural networks

    id56s

    id56 (robinson and fallside, 1991)
    lstm graves et al. (2013)
    deep lstm-p sak et al. (2014b)
    cldnn (right) (sainath et al., 2015a)
    gru. deepspeech 1/2 (amodei et al., 2015)

    bidirectional (schuster and paliwal, 1997)

helps, but introduces latency.

    dependencies not long at speech frame rates

(100hz).

    frame stacking and down-sampling help.

andrew senior

id103

40 of 63

human parity in id103 (xiong et al.,
2016)

    ensemble of blstms
    i-vectors for speaker id172

    i-vector is an embedding of audio trained to discriminate between

speakers. (speaker id)

    interpolated id165 + lstm language model.
    5.8% wer on swb (vs 5.9% for human).

andrew senior

id103

41 of 63

outline

id103

acoustic representation
phonetic representation
history
probabilistic id103

neural network id103

hybrid neural networks
training losses
sequence discriminative training
new architectures

other topics

cross id178 training

    gmms were trained with maximum likelihood
    conventional training uses cross-id178 loss.

lxen t (ot,   ) =

yt (i) log

yt (i)
  yt (i)

n(cid:88)

i=1

    with large data we can use viterbi (binary) targets: yt     {0, 1}

    i.e. a hard alignment.

    can also use a soft (baum-welch) alignment (senior and robinson,

1994)

andrew senior

id103

43 of 63

connectionist temporal classi   cation (graves et al.,
2006)

    ctc is a bundle of alternatives to conventional system:

labels.

    ctc introduces an optional blank symbol between the    real   
    simple to implement in the fst framework -an optional

    continuous realignment     no need for a bootstrap model
    always use soft targets.
    don   t scale by posterior.

    similar results to conventional training.

andrew senior

id103

44 of 63

/k//ae//t/----ctc alignments

andrew senior

id103

45 of 63

 0 0.2 0.4 0.6 0.8 1silmjuzi@mzins@kagousil<b>sil.1m.25j.35u.46z.69i.113@.320m.227z.87i.17n.350s.41@.133k.75a.22g.18ou.68outline

id103

acoustic representation
phonetic representation
history
probabilistic id103

neural network id103

hybrid neural networks
training losses
sequence discriminative training
new architectures

other topics

sequence discriminative training

    conventional training uses cross-id178 loss

    tries to maximize id203 of the true state sequence given the

data.

    we care about word error rate of the complete system.
    design a loss that   s di   erentiable and closer to what we care about.
    applied to neural networks (kingsbury, 2009)
    posterior scaling gets learnt by the network.
    improves conventional training and ctc by ~15% relative.
    bmmi, smbr(povey et al., 2008)
p (xr, sr)
s p (xr, s)

p (sr|xr) =

p (xr|sr) p (sr)
s p (xr|s) p (s)

(cid:80)

(cid:80)

=

r(cid:88)

r=1

r(cid:88)

(cid:88)

lmmi (  ) =    

log p (sr|xr)

r(cid:88)

andrew senior

lsm br (  ) =

lsm br (xr,   ) =

id103

p (sj|xr) e (sj, sr)

47 of 63

sequence discriminative training

andrew senior

id103

48 of 63

sequence discriminative training

andrew senior

id103

49 of 63

outline

id103

acoustic representation
phonetic representation
history
probabilistic id103

neural network id103

hybrid neural networks
training losses
sequence discriminative training
new architectures

other topics

sequence2sequence

    basic sequence2sequence not that good for speech

    utterances are too long to memorize
    monotonicity of audio (vs machine translation)

    attention + id195 for speech (chorowski et al., 2015)
    listen, attend and spell (chan et al., 2015)
    output characters until eos
    incorporates language model of training set.
    harder to incorporate a separately-trained language model. (e.g.

trained on trillions of tokens)

andrew senior

id103

51 of 63

watch listen, attend and spell (chung et al., 2016)

apply las to audio and video streams simultaneously.

train with scheduled sampling (bengio et al., 2015)

andrew senior

id103

52 of 63

watch listen, attend and spell (chung et al., 2016)

andrew senior

id103

53 of 63

neural transducer (jaitly et al., 2015)

    id195 models require the whole sequence to be available.
    introduce latency compared to unidirectional.
    solution: transcribe monotonic chunks at a time with attention.

andrew senior

id103

54 of 63

neural transducer

andrew senior

id103

55 of 63

raw waveform id103

    we typically train on a much-reduced dimensional signal.
    would like to train end-to-end.
    learn    lterbanks, instead of hand-crafting.
    a conventional id56 at audio sample rate can   t learn long-enough

cldnn (sainath et al., 2015b)

dependencies.
    add a convolutional    lter to a conventional system e.g.
    wavenet-style architecture. [see tts talk on thursday]
    clockwork id56 (koutn    k et al., 2014) run a hierarchical id56 at

multiple rates.

andrew senior

id103

56 of 63

raw waveform id103

frequency distribution of learned    lters di   ers from hand-initialization:

andrew senior

id103

57 of 63

id103 in noise

    multi-style training (   mts   )

training.

    collect noisy data.
    or, add realistic but randomized noise to utterances during
    e.g. through a    room simulator    to add reverberation.
    optionally add a clean-reconstruction loss in training.

    train a denoiser.
    nb lombard e   ect     voice changes in noise.

andrew senior

id103

58 of 63

multi-microphone id103

    multiple microphones give a richer representation
       closest to the speaker    has better snr
    beamforming

    given geometry of microphone array and speed of sound
    compute time delay of arrival at each microphone
    delay-and-sum: constructive interference of signal in chosen
    destructive interference depends on direction / frequency of noise.

direction.

    more features for a neural network to exploit.

    important to preserve phase information to enable beam-forming

andrew senior

id103

59 of 63

factored multichannel raw waveform cldnn (sainath
et al., 2016)

andrew senior

id103

60 of 63

fconvlstmlstmlstmdnnoutput targetsx2[t]2<mpool + nonlincldnnx1[t]2<m. . h112<nh212<n. . h122<nh222<ntconv2z[t]2<1   f   py[t]2<m   1   php12<nhp22<ntconv1dnnclean featuresdnid4lg2<l   f   1w[t]2<m l+1   f   p420246p=1impulse responseschannel 0channel 1060120180doabeampattern06121824300.40.20.00.20.4p=2060120180doa0.40.20.00.20.4p=3060120180doa21012p=4060120180doa0.20.00.2p=5060120180doa1.00.50.00.51.0p=6060120180doa0.20.10.00.10.2p=7060120180doa505p=8060120180doa0.40.20.00.20.4p=9060120180doa012345time (milliseconds)1.00.50.00.51.0p=10012345678frequency (khz)060120180doareferences i

abdel-hamid, o., mohamed, a.-r., jiang, h., deng, l., penn, g., and yu, d. (2014). convolutional neural networks for

id103. ieee/acm trans. audio, speech and lang. proc., 22(10):1533   1545.

amodei, d., anubhai, r., battenberg, e., case, c., casper, j., catanzaro, b., chen, j., chrzanowski, m., coates, a.,
diamos, g., elsen, e., engel, j., fan, l., fougner, c., han, t., hannun, a. y., jun, b., legresley, p., lin, l.,
narang, s., ng, a. y., ozair, s., prenger, r., raiman, j., satheesh, s., seetapun, d., sengupta, s., wang, y.,
wang, z., wang, c., xiao, b., yogatama, d., zhan, j., and zhu, z. (2015). deep speech 2: end-to-end speech
recognition in english and mandarin. corr, abs/1512.02595.

bengio, s., vinyals, o., jaitly, n., and shazeer, n. (2015). scheduled sampling for sequence prediction with recurrent

neural networks. corr, abs/1506.03099.

chan, w., jaitly, n., le, q. v., and vinyals, o. (2015). listen, attend and spell. corr, abs/1508.01211.

chorowski, j., bahdanau, d., serdyuk, d., cho, k., and bengio, y. (2015). attention-based models for id103.

corr, abs/1506.07503.

chung, j. s., senior, a. w., vinyals, o., and zisserman, a. (2016). lip reading sentences in the wild. corr,

abs/1611.05358.

dahl, g., yu, d., li, d., and acero, a. (2011). large vocabulary continuous id103 with context-dependent

dbn-id48s. in icassp.

dempster, a., laird, n., and rubin, d. (1977). maximum likelihood from incomplete data via the em algorithm. journal of

the royal statistical society, 39(b):1     38.

graves, a., fern  andez, s., gomez, f., and schmidhuber, j. (2006). connectionist temporal classi   cation: labelling

unsegmented sequence data with recurrent neural networks. in proceedings of the 23rd international conference on
machine learning, pages 369   376. acm.

graves, a., jaitly, n., and mohamed, a. (2013). hybrid id103 with deep bidirectional lstm. in asru.

grezl, kara   at, and cernocky (2007). neural network topologies and bottleneck features. id103.

hermansky, h., ellis, d., and sharma, s. (2000). tandem connectionist feature extraction for conventional id48 systems.

in icassp.

andrew senior

id103

61 of 63

references ii

hinton, g., deng, l., yu, d., dahl, g., mohamed, a., jaitly, n., senior, a., vanhoucke, v., nguyen, p., sainath, t. n., and
kingsbury, b. (2012). deep neural networks for acoustic modeling in id103. ieee signal processing
magazine, 29(6):82   97.

hinton, g. e. (2002). training products of experts by minimizing contrastive divergence. neural computation.

jaitly, n., le, q. v., vinyals, o., sutskever, i., and bengio, s. (2015). an online sequence-to-sequence model using partial

conditioning. corr, abs/1511.04868.

kingsbury, b. (2009). lattice-based optimization of sequence classi   cation criteria for neural-network acoustic modeling. in

ieee international conference on acoustics, speech, and signal processing (icassp), pages 3761   3764, taipei,
taiwan.

koutn    k, j., gre   , k., gomez, f. j., and schmidhuber, j. (2014). a clockwork id56. corr, abs/1402.3511.

mohamed, a., dahl, g., and hinton, g. (2009). id50 for phone recognition. in nips.

peddinti, v., povey, d., and khudanpur, s. (2015). a time delay neural network architecture for e   cient modeling of long

temporal contexts. in interspeech.

povey, d., kanevsky, d., kingsbury, b., ramabhadran, b., saon, g., and visweswariah, k. (2008). boosted mmi for model

and feature-space discriminative training. in proc. icassp.

robinson, a. and fallside, f. (1991). a recurrent error propagation network id103 system. computer speech

and language, 5(3):259   274.

sainath, t. n., mohamed, a.-r., kingsbury, b., and ramabhadran, b. (2013). deep convolutional neural networks for lvcsr.

in ieee international conference on acoustics, speech and signal processing.

sainath, t. n., vinyals, o., senior, a., and sak, h. (2015a). convolutional, long short-term memory, fully connected deep

neural networks. in ieee international conference on acoustics, speech, and signal processing (icassp).

sainath, t. n., weiss, r., senior, a., wilson, k., and vinyals, o. (2015b). raw waveform cldnns. in submitted to

interspeech.

sainath, t. n., weiss, r. j., wilson, k. w., narayanan, a., and bacchiani, m. (2016). factored spatial and spectral

multichannel raw waveform cldnns. in to appear in proc. icassp.

andrew senior

id103

62 of 63

references iii

sak, h., senior, a., and beaufays, f. (2014a). long short-term memory based recurrent neural network architectures for

large vocabulary id103. arxiv e-prints.

sak, h., senior, a., and beaufays, f. (2014b). long short-term memory recurrent neural network architectures for large

scale acoustic modeling. in interspeech 2014.

sak, h., senior, a., rao, k., irsoy, o., graves, a., beaufays, f., and schalkwyk, j. (2015). learning acoustic frame labeling
for id103 with recurrent neural networks. in ieee international conference on acoustics, speech, and
signal processing (icassp).

schuster, m. and paliwal, k. k. (1997). id182. signal processing, ieee transactions on,

45(11):2673   2681.

senior, a. and robinson, a. (1994). forward-backward retraining of recurrent neural networks. in nips.

soltau, h., liao, h., and sak, h. (2016). neural speech recognizer: acoustic-to-word lstm model for large vocabulary

id103. corr, abs/1610.09975.

van den oord, a., dieleman, s., zen, h., simonyan, k., vinyals, o., graves, a., kalchbrenner, n., senior, a. w., and

kavukcuoglu, k. (2016). wavenet: a generative model for raw audio. corr, abs/1609.03499.

waibel, a., hanazawa, t., hinton, g., shikano, k., and lang, k. (1989). phoneme recognition using time-delay neural

networks. ieee transactions on acoustics, speech and signal processing, 37(3).

wu, y., schuster, m., chen, z., le, q. v., norouzi, m., macherey, w., krikun, m., cao, y., gao, q., macherey, k.,

klingner, j., shah, a., johnson, m., liu, x., kaiser, l., gouws, s., kato, y., kudo, t., kazawa, h., stevens, k.,
kurian, g., patil, n., wang, w., young, c., smith, j., riesa, j., rudnick, a., vinyals, o., corrado, g., hughes, m.,
and dean, j. (2016). google   s id4 system: bridging the gap between human and machine
translation. corr, abs/1609.08144.

xiong, w., droppo, j., huang, x., seide, f., seltzer, m., stolcke, a., yu, d., and zweig, g. (2016). achieving human

parity in conversational id103. corr, abs/1610.05256.

andrew senior

id103

63 of 63

