generative model-based
text-to-id133
andrew senior
heiga zen
february 23rd, 2017@oxford

(deepmind london) many thanks to

outline

generative tts

generative acoustic models for parametric tts

id48 (id48s)
neural networks

beyond parametric tts

learned features
wavenet
end-to-end

conclusion & future topics

text-to-speech as sequence-to-sequence mapping

automatic id103 (asr)

       ok google, directions home   

text-to-id133 (tts)
   take the    rst left       

andrew senior

generative model-based text-to-id133

1 of 50

speech production process

andrew senior

generative model-based text-to-id133

2 of 50

text(concept)frequencytransfercharacteristicsmagnitudestart--endfundamentalfrequencymodulation of carrier waveby speech informationfundamental freqvoiced/unvoicedfreq transfer charair flowsound sourcevoiced: pulseunvoiced: noisespeechtypical    ow of tts system

andrew senior

generative model-based text-to-id133

3 of 50

sentence segmentationid40text id172part-of-speech taggingpronunciationid144 predictionwaveform generationtexttext analysissynthesizedseechid133discrete     discretediscrete     continuousnlpspeechfrontendbackendid133 approaches

andrew senior

generative model-based text-to-id133

4 of 50

| text=   hello, my name is heiga zen.   )p(speech=id133 approaches

rule-based, formant synthesis
[1]

andrew senior

generative model-based text-to-id133

4 of 50

| text=   hello, my name is heiga zen.   )p(speech=id133 approaches

rule-based, formant synthesis
[1]

sample-based, concatenative
synthesis [2]

andrew senior

generative model-based text-to-id133

4 of 50

| text=   hello, my name is heiga zen.   )p(speech=id133 approaches

rule-based, formant synthesis
[1]

sample-based, concatenative
synthesis [2]

model-based, generative synthesis

andrew senior

generative model-based text-to-id133

4 of 50

| text=   hello, my name is heiga zen.   )p(speech=unit selection concatenative id133

    build a database with wide linguistic diversity.
    forced align and chop up into diphones.
    for a new utterance, choose units matching the diphone sequence.
    minimize total cost by greedy search.

    cost =(cid:80)

i u (i) + j(i, i     1)

    splice together adjacent units matching up last pitch period.

andrew senior

generative model-based text-to-id133

5 of 50

tts databases

    want high quality,
    studio recording
    controlled, consistent conditions
    no background noise
    single (professional) speaker

    typically read speech

andrew senior

generative model-based text-to-id133

6 of 50

tts databases

    vctk (voice cloning tool kit)

    109 native speakers of english 400 sentences. 96khz 24 bits
    intended for adaptation of an average voice.

    google tts 10s of hours
    edingburgh merlin system

https://github.com/cstr-edinburgh/merlin

andrew senior

generative model-based text-to-id133

7 of 50

tts performance metrics

    tts performance is subjective.
    intelligibility (in noise)
    naturalness

    mean opinion score (5 point scale)
    a/b preference tests.
    e.g. amazon mechanical turk 100 utterances 5   7 tests per sample
    care needed to control for human factors.

    objective measures

    pesq
    robust mos

andrew senior

generative model-based text-to-id133

8 of 50

probabilistic formulation of tts

random variables

x
w
w

x

speech waveforms (data)

transcriptions (data)

given text

observed

observed

observed

synthesized speech

unobserved

andrew senior

generative model-based text-to-id133

9 of 50

xwwxprobabilistic formulation of tts

random variables

x
w
w

x

speech waveforms (data)

transcriptions (data)

given text

observed

observed

observed

synthesized speech

unobserved

synthesis
    estimate posterior predictive distribution

    p(x | w,x ,w)

    sample   x from the posterior distribution

andrew senior

generative model-based text-to-id133

9 of 50

xwwxprobabilistic formulation

introduce auxiliary variables (representation) + factorize dependency

(cid:88)

(cid:8)p(x | o)p(o | l,   )p(l | w)

(cid:90)(cid:90)(cid:90) (cid:88)
p(x | o)p(o | l,   )p(  )p(l | w)/ p(x )(cid:9)dodod  

   l

   l

p(x | w,x ,w) =

where

o, o: acoustic features
l, l: linguistic features

  : model

andrew senior

generative model-based text-to-id133

10 of 50

xwwxllo  oapproximation (1)

approximate {sum & integral} by best point estimates (like map) [3]

p(x | w,x ,w)     p(x |   o)

where

{   o,   l,   o,   l,     } = arg max
o,l,o,l,  
p(x | o)p(o | l,   )p(l | w)

(cid:8)
p(x | o)p(o | l,   )p(  )p(l | w)(cid:9)

andrew senior

generative model-based text-to-id133

11 of 50

  oxwwxllo  oapproximation (2)

joint     step-by-step maximization [3]

extract acoustic features

extract linguistic features

  o = arg maxo
  l = arg maxl
     = arg max

p(x | o)
p(l | w)
p(   o |   l,   )p(  ) learn mapping
p(l | w)
p(o |   l,     )
  x     fx(   o) = p(x |   o)

  l = arg max

  o = arg max

  

o

l

predict linguistic features

predict acoustic features

synthesize waveform

andrew senior

generative model-based text-to-id133

12 of 50

xowlwl    o  lo      lx  oapproximation (2)

joint     step-by-step maximization [3]

extract acoustic features

  o = arg maxo
  l = arg maxl
     = arg max

p(x | o)
p(l | w)
p(   o |   l,   )p(  )
p(l | w)
p(o |   l,     )
  x     fx(   o) = p(x |   o)

  l = arg max

  o = arg max

  

o

l

andrew senior

generative model-based text-to-id133

12 of 50

xowlwl    o  lo      lx  oapproximation (2)

joint     step-by-step maximization [3]

extract acoustic features

extract linguistic features

  o = arg maxo
  l = arg maxl
     = arg max

p(x | o)
p(l | w)
p(   o |   l,   )p(  )
p(l | w)
p(o |   l,     )
  x     fx(   o) = p(x |   o)

  l = arg max

  o = arg max

  

o

l

andrew senior

generative model-based text-to-id133

12 of 50

xowlwl    o  lo      lx  oapproximation (2)

joint     step-by-step maximization [3]

extract acoustic features

extract linguistic features

  o = arg maxo
  l = arg maxl
     = arg max

p(x | o)
p(l | w)
p(   o |   l,   )p(  ) learn mapping
p(l | w)
p(o |   l,     )
  x     fx(   o) = p(x |   o)

  l = arg max

  o = arg max

  

o

l

andrew senior

generative model-based text-to-id133

12 of 50

xowlwl    o  lo      lx  oapproximation (2)

joint     step-by-step maximization [3]

extract acoustic features

extract linguistic features

  o = arg maxo
  l = arg maxl
     = arg max

p(x | o)
p(l | w)
p(   o |   l,   )p(  ) learn mapping
p(l | w)
p(o |   l,     )
  x     fx(   o) = p(x |   o)

  l = arg max

  o = arg max

  

o

l

predict linguistic features

andrew senior

generative model-based text-to-id133

12 of 50

xowlwl    o  lo      lx  oapproximation (2)

joint     step-by-step maximization [3]

extract acoustic features

extract linguistic features

  o = arg maxo
  l = arg maxl
     = arg max

p(x | o)
p(l | w)
p(   o |   l,   )p(  ) learn mapping
p(l | w)
p(o |   l,     )
  x     fx(   o) = p(x |   o)

  l = arg max

  o = arg max

  

o

l

predict linguistic features

predict acoustic features

andrew senior

generative model-based text-to-id133

12 of 50

xowlwl    o  lo      lx  oapproximation (2)

joint     step-by-step maximization [3]

extract acoustic features

extract linguistic features

  o = arg maxo
  l = arg maxl
     = arg max

p(x | o)
p(l | w)
p(   o |   l,   )p(  ) learn mapping
p(l | w)
p(o |   l,     )
  x     fx(   o) = p(x |   o)

  l = arg max

  o = arg max

  

o

l

predict linguistic features

predict acoustic features

synthesize waveform

andrew senior

generative model-based text-to-id133

12 of 50

xowlwl    o  lo      lx  oapproximation (2)

joint     step-by-step maximization [3]

extract acoustic features

extract linguistic features

  

  o = arg maxo
  l = arg maxl
     = arg max

p(x | o)
p(l | w)
p(   o |   l,   )p(  ) learn mapping
p(l | w)
p(o |   l,     )
  x     fx(   o) = p(x |   o)
representations: acoustic, linguistic, mapping

  l = arg max

  o = arg max

o

l

synthesize waveform

predict linguistic features

predict acoustic features

andrew senior

generative model-based text-to-id133

12 of 50

xowlwl    o  lo      lx  orepresentation     linguistic features

andrew senior

generative model-based text-to-id133

13 of 50

hello, world.hello,world.helloworldh-e2l-ou1w-er1-l-dhelouwerldphone: voicing, manner, ...syllable: stress, tone, ...word: pos, grammar, ...phrase: intonation, ...sentence: length, ...representation     linguistic features

    based on knowledge about spoken language
    lexicon, letter-to-sound rules
    tokenizer, tagger, parser
    phonology rules

andrew senior

generative model-based text-to-id133

13 of 50

hello, world.hello,world.helloworldh-e2l-ou1w-er1-l-dhelouwerldphone: voicing, manner, ...syllable: stress, tone, ...word: pos, grammar, ...phrase: intonation, ...sentence: length, ...representation     acoustic features

duration model
    typically run a parametric synthesizer on frames (e.g. 5ms windows)
    need to know how many frames each phonetic unit lasts.
    model this separately e.g. ffnn linguistic features     duration.

andrew senior

generative model-based text-to-id133

14 of 50

representation     acoustic features

piece-wise stationary, source-   lter generative model p(x | o)

andrew senior

generative model-based text-to-id133

15 of 50

pulse train (voiced)white noise (unvoiced)speechvocal sourcevocal tract filterfundamentalfrequencyaperiodicity,voicing, ...+ 0 800[db]8 [khz]cepstrum, lpc, ...e(n)x(n) = h(n)*e(n)h(n)overlap/shiftwindowingrepresentation     acoustic features

piece-wise stationary, source-   lter generative model p(x | o)

    needs to solve inverse problem
    estimate parameters from signals
    use estimated parameters (e.g., cepstrum) as acoustic features

andrew senior

generative model-based text-to-id133

15 of 50

pulse train (voiced)white noise (unvoiced)speechvocal sourcevocal tract filterfundamentalfrequencyaperiodicity,voicing, ...+ 0 800[db]8 [khz]cepstrum, lpc, ...e(n)x(n) = h(n)*e(n)h(n)overlap/shiftwindowingrepresentation     mapping

rule-based, formant synthesis [1]

vocoder analysis

text analysis

  o = arg maxo
  l = arg maxl
     = arg max

p(x | o)
p(l | w)
p(   o |   l,   )p(  ) extract rules
text analysis
p(l | w)
p(o |   l,     )
  x     fx(   o) = p(x |   o)

  l = arg max

  o = arg max

apply rules

  

o

l

vocoder synthesis

andrew senior

generative model-based text-to-id133

16 of 50

xowlwl    o  lo      lx  orepresentation     mapping

rule-based, formant synthesis [1]

  

text analysis

  l = arg max

vocoder analysis

  o = arg maxo
  l = arg maxl
     = arg max

p(x | o)
p(l | w)
p(   o |   l,   )p(  ) extract rules
text analysis
p(l | w)
p(o |   l,     )
  x     fx(   o) = p(x |   o)
    hand-crafted rules on knowledge-based features

vocoder synthesis

  o = arg max

apply rules

o

l

andrew senior

generative model-based text-to-id133

16 of 50

xowlwl    o  lo      lx  orepresentation     mapping

id48-based [4], statistical parametric synthesis [5]

vocoder analysis

text analysis

  o = arg maxo
  l = arg maxl
     = arg max

p(x | o)
p(l | w)
p(   o |   l,   )p(  ) train id48s
text analysis
p(l | w)
p(o |   l,     )
  x     fx(   o) = p(x |   o)

  l = arg max

  o = arg max

  

o

l

vocoder synthesis

parameter generation

andrew senior

generative model-based text-to-id133

17 of 50

xowlwl    o  lo      lx  orepresentation     mapping

id48-based [4], statistical parametric synthesis [5]

  

text analysis

  l = arg max

vocoder analysis

  o = arg maxo
  l = arg maxl
     = arg max

p(x | o)
p(l | w)
p(   o |   l,   )p(  ) train id48s
text analysis
p(l | w)
p(o |   l,     )
  x     fx(   o) = p(x |   o)
    replace rules by id48-based generative acoustic model

parameter generation

vocoder synthesis

  o = arg max

o

l

andrew senior

generative model-based text-to-id133

17 of 50

xowlwl    o  lo      lx  ooutline

generative tts

generative acoustic models for parametric tts

id48 (id48s)
neural networks

beyond parametric tts

learned features
wavenet
end-to-end

conclusion & future topics

id48-based generative acoustic model for tts

    context-dependent subword id48s
    id90 to cluster & tie id48 states     interpretable

(cid:88)
(cid:88)

   q

   q

t(cid:89)
t(cid:89)

t=1

t=1

p(o | l,   ) =

=

p(ot | qt,   )p (q | l,   )

qt: hidden state at t

n (ot;   qt,   qt)p (q | l,   )

andrew senior

generative model-based text-to-id133

19 of 50

q1o1q2o2q3o3q4o4ll1lno1o2o3too2............o4o2o6o5......: discrete: continuousid48-based generative acoustic model for tts

    non-smooth, step-wise statistics

    smoothing is essential

    di   cult to use high-dimensional acoustic features (e.g., raw spectra)

    use low-dimensional features (e.g., cepstra)

    data fragmentation

    ine   ective, local representation

a lot of research work have been done to address these issues

andrew senior

generative model-based text-to-id133

20 of 50

outline

generative tts

generative acoustic models for parametric tts

id48 (id48s)
neural networks

beyond parametric tts

learned features
wavenet
end-to-end

conclusion & future topics

alternative acoustic model

id48: handle variable length & alignment
decision tree: map linguistic     acoustic

regression tree: linguistic features     stats. of acoustic features

andrew senior

generative model-based text-to-id133

22 of 50

yesnoyesno...yesnoyesnoyesnostatistics of acoustic features olinguistic features lalternative acoustic model

id48: handle variable length & alignment
decision tree: map linguistic     acoustic

regression tree: linguistic features     stats. of acoustic features
replace the tree w/ a general-purpose regression model
    arti   cial neural network

andrew senior

generative model-based text-to-id133

22 of 50

yesnoyesno...yesnoyesnoyesnostatistics of acoustic features olinguistic features lffnn-based acoustic model for tts [6]

ht = g (whllt + bh)

(cid:88)
t (cid:107)ot       ot(cid:107)2

     = arg min

  

  ot = wohht + bo

   = {whl, woh, bh, bo}
  ot     e [ot | lt]     replace id90 & gaussian distributions

generative model-based text-to-id133

andrew senior

23 of 50

htltframe-level linguistic featureotframe-level acoustic featureinputtargetltotot+1ot   1lt+1lt   1id56-based acoustic model for tts [7]

ht = g (whllt + whhht   1 + bh)

     = arg min

  

(cid:88)
t (cid:107)ot       ot(cid:107)2

  ot = wohht + bo

   = {whl, whh, woh, bh, bo}

ffnn:   ot     e [ot | lt] id56:   ot     e [ot | l1, . . . , lt]

generative model-based text-to-id133

andrew senior

24 of 50

recurrentconnectionsltotot+1ot   1lt+1lt   1ltframe-level linguistic featureotframe-level acoustic featureinputtargetnn-based generative acoustic model for tts

    non-smooth, step-wise statistics

    id56 predicts smoothly varying acoustic features [7, 8]

    di   cult to use high-dimensional acoustic features (e.g., raw spectra)

    layered architecture can handle high-dimensional features [9]

    data fragmentation

    distributed representation [10]

andrew senior

generative model-based text-to-id133

25 of 50

nn-based generative acoustic model for tts

    non-smooth, step-wise statistics

    id56 predicts smoothly varying acoustic features [7, 8]

    di   cult to use high-dimensional acoustic features (e.g., raw spectra)

    layered architecture can handle high-dimensional features [9]

    data fragmentation

    distributed representation [10]

nn-based approach is now mainstream in research & products
    models: ffnn [6], mdn [11], id56 [7], highway network [12], gan

[13]

    products: e.g., google [14]

andrew senior

generative model-based text-to-id133

25 of 50

nn-based generative model for tts

text     linguistic     (articulatory)     acoustic     waveform

andrew senior

generative model-based text-to-id133

26 of 50

text(concept)frequencytransfercharacteristicsmagnitudestart--endfundamentalfrequencymodulation of carrier waveby speech informationfundamental freqvoiced/unvoicedfreq transfer charair flowsound sourcevoiced: pulseunvoiced: noisespeechoutline

generative tts

generative acoustic models for parametric tts

id48 (id48s)
neural networks

beyond parametric tts

learned features
wavenet
end-to-end

conclusion & future topics

knowledge-based features     learned features

unsupervised id171

    speech: auto-encoder at fft spectra [9, 15]     positive results
    text: word [16], phone & syllable [17]     less positive

andrew senior

generative model-based text-to-id133

28 of 50

x(t) (raw fft spectrum) acousticfeatureo(t)   0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0hello0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0worldw(n)(1-hot representation of word)w(n+1)~x(t)linguisticfeaturel(n)   l(n-1)relax approximation
joint acoustic feature extraction & model training

two-step optimization     joint optimization

                  o = arg maxo

     = arg max

  

p(x | o)
p(   o |   l,   )p(  )

   

{    ,   o} = arg max

  ,o

p(x | o)p(o |   l,   )p(  )

joint source-   lter & acoustic model optimization
    id48 [18, 19, 20]
    nn [21, 22]

andrew senior

generative model-based text-to-id133

29 of 50

xowlwl    lo      lx  orelax approximation
joint acoustic feature extracion & model training

mixed-phase cepstral analysis + lstm-id56 [22]

andrew senior

generative model-based text-to-id133

30 of 50

linguisticfeatures.........back propagationforwardpropagationz-1z-1-1derivativesz-1z-1zz-...speechcepstrum......pulse trainltpng(z)h-1u(z)fseo(v)td(u)td(v)to(u)txnnnnoutline

generative tts

generative acoustic models for parametric tts

id48 (id48s)
neural networks

beyond parametric tts

learned features
wavenet
end-to-end

conclusion & future topics

relax approximation
direct mapping from linguistic to waveform

no explicit acoustic features

{    ,   o} = arg max

  ,o

p(x | o)p(o |   l,   )p(  )

   
     = arg max

  

p(x |   l,   )p(  )

generative models for raw audio
    lpc [23]
    wavenet [24]
    sampleid56 [25]

andrew senior

generative model-based text-to-id133

32 of 50

xwlwl    l      lxwavenet: a generative model for raw audio

autoregressive (ar) modelling of speech signals

x = {x0, x1, . . . , xn   1} : raw waveform

p(x |   ) = p(x0, x1, . . . , xn   1 |   ) =

p(xn | x0, . . . , xn   1,  )

n   1(cid:89)

n=0

andrew senior

generative model-based text-to-id133

33 of 50

wavenet: a generative model for raw audio

autoregressive (ar) modelling of speech signals

x = {x0, x1, . . . , xn   1} : raw waveform

p(x |   ) = p(x0, x1, . . . , xn   1 |   ) =

p(xn | x0, . . . , xn   1,  )

n   1(cid:89)

n=0

wavenet [24]
    p(xn | x0, . . . , xn   1,   ) is modeled by convolutional nn

andrew senior

generative model-based text-to-id133

33 of 50

wavenet: a generative model for raw audio

autoregressive (ar) modelling of speech signals

x = {x0, x1, . . . , xn   1} : raw waveform

p(x |   ) = p(x0, x1, . . . , xn   1 |   ) =

p(xn | x0, . . . , xn   1,  )

n   1(cid:89)

n=0

wavenet [24]
    p(xn | x0, . . . , xn   1,   ) is modeled by convolutional nn
key components
    causal dilated convolution: capture long-term dependency
    gated convolution + residual + skip: powerful non-linearity
    softmax at output: classi   cation rather than regression

andrew senior

generative model-based text-to-id133

33 of 50

wavenet     causal dilated convolution

100ms in 16khz sampling = 1,600 time steps
    too long to be captured by normal id56/lstm
dilated convolution

exponentially increase receptive    eld size w.r.t. # of layers

andrew senior

generative model-based text-to-id133

34 of 50

inputoutputhiddenlayer3hiddenlayer2hiddenlayer1. . .p(x  | x   ,..., x   )nn-16n-1xn-1xn-2xn-3xn-16wavenet     non-linearity

andrew senior

generative model-based text-to-id133

35 of 50

residual blockresidual blockresidual blockresidual block......1x11x11x11x151251251251251251251251251225630relu1x1256relu1x1256softmax256   256256256256256skip connectionsresidual block: 2x1 dilated conv     gated activation     1x1 conv + residual connection1x1: 1x1 conv0p(x  | x ,..., x   )nn-1wavenet     softmax

andrew senior

generative model-based text-to-id133

36 of 50

timeamplitudeanalog audio signalwavenet     softmax

andrew senior

generative model-based text-to-id133

36 of 50

timeamplitudesampling & quantizationwavenet     softmax

andrew senior

generative model-based text-to-id133

36 of 50

timeamplitude116category indexcategorical distribution     histogram  - unimodal  - multimodal  - skewed  ...wavenet     softmax

andrew senior

generative model-based text-to-id133

36 of 50

timeamplitude116category indexcategorical distribution     histogram  - unimodal  - multimodal  - skewed  ...prof. d. jurafsky -    now tts is the same problem as languagemodeling!   wavenet     conditional modelling

andrew senior

generative model-based text-to-id133

37 of 50

res. block...res. blockres. blockres. blockres. blockres. blockembeddingat time n hnlinguistic featuresxn-1xn-2xn-3xn-4p(x  | x   ,..., x    , h ,   )nn-32n-1n  wavenet vs conventional audio generative models

assumptions in conventional audio generative models
[23, 26, 27, 22]
    stationary process w/    xed-length analysis window

    estimate model within 20   30ms window w/ 5   10 shift

    linear, time-invariant    lter within a frame

    relationship between samples can be non-linear

    gaussian process

    assumes speech signals are normally distributed

wavenet
    sample-by-saple, non-linear, capable to take additional inputs
    arbitrary-shaped signal distribution
sota subjective naturalness w/ wavenet-based tts [24]

id48

lstm

concatenative

wavenet

andrew senior

generative model-based text-to-id133

38 of 50

outline

generative tts

generative acoustic models for parametric tts

id48 (id48s)
neural networks

beyond parametric tts

learned features
wavenet
end-to-end

conclusion & future topics

relax approximation
towards bayesian end-to-end tts

integrated end-to-end

                  l = arg maxl

     = arg max

  

p(l | w)
p(x |   l,   )p(  )

   

     = arg max

  

p(x | w,   )p(  )

text analysis is integrated to model

andrew senior

generative model-based text-to-id133

40 of 50

xww      xrelax approximation
towards bayesian end-to-end tts

bayesian end-to-end

  

              = arg max
p(x | w,   )p(  )
  x     fx(w,     ) = p(x | w,     )
(cid:90)

  x     fx(w,x ,w) = p(x | w,x ,w)
=

p(x | w,   )p(   | x ,w)d  

   

k(cid:88)

k=1

1
k

   

p(x | w,     k)     ensemble

marginalize model parameters & architecture

andrew senior

generative model-based text-to-id133

41 of 50

xww  xgenerative model-based text-to-id133

    bayes formulation + factorization + approximations
    representation: acoustic features, linguistic features, mapping

    mapping: rules     id48     nn
    feature: engineered     unsupervised, learned

    less approximations

    joint training, direct waveform modelling
    moving towards integrated & bayesian end-to-end tts

naturalness: concatenative     generative
flexibility: concatenative (cid:28) generative (e.g., multiple speakers)

andrew senior

generative model-based text-to-id133

42 of 50

beyond    text   -to-id133

tts on conversational assistants

    texts aren   t fully contained
    need more context

    location to resolve homographs
    user query to put right emphasis

andrew senior

generative model-based text-to-id133

43 of 50

beyond    text   -to-id133

tts on conversational assistants

    texts aren   t fully contained
    need more context

    location to resolve homographs
    user query to put right emphasis

we need representation that can

organize the world information & make it accessible & useful

from tts generative models

andrew senior

generative model-based text-to-id133

43 of 50

beyond    generative    tts

generative model-based tts
    model represents process behind speech production

    trained to minimize error against human-produced speech
    learned model     speaker

andrew senior

generative model-based text-to-id133

44 of 50

beyond    generative    tts

generative model-based tts
    model represents process behind speech production

    trained to minimize error against human-produced speech
    learned model     speaker
    speech is for communication

    goal: maximize the amount of information to be received

missing    listener   
       listener    in training / model itself?

andrew senior

generative model-based text-to-id133

44 of 50

thanks!

andrew senior

generative model-based text-to-id133

45 of 50

references i

[1] d. klatt.

real-time id133 by rule.
journal of asa, 68(s1):s18   s18, 1980.

[2]

a. hunt and a. black.
unit selection in a concatenative id133 system using a large speech database.
in proc. icassp, pages 373   376, 1996.

[3] k. tokuda.

id133 as a statistical machine learning problem.
https://www.sp.nitech.ac.jp/~tokuda/tokuda_asru2011_for_pdf.pdf.
invited talk given at asru 2011.

[4] t. yoshimura, k. tokuda, t. masuko, t. kobayashi, and t. kitamura.

simultaneous modeling of spectrum, pitch and duration in id48-based id133.
ieice trans. inf. syst., j83-d-ii(11):2099   2107, 2000.
(in japanese).

[5] h. zen, k. tokuda, and a. black.

statistical parametric id133.
speech commn., 51(11):1039   1064, 2009.

[6] h. zen, a. senior, and m. schuster.

statistical parametric id133 using deep neural networks.
in proc. icassp, pages 7962   7966, 2013.

[7]

y. fan, y. qian, f.-l. xie, and f. soong.
tts synthesis with bidirectional lstm based recurrent neural networks.
in proc. interspeech, pages 1964   1968, 2014.

andrew senior

generative model-based text-to-id133

46 of 50

references ii

[8] h. zen.

acoustic modeling for id133: from id48 to id56.
http://research.google.com/pubs/pub44630.html.
invited talk given at asru 2015.

[9]

s. takaki and j. yamagishi.
a deep auto-encoder based low-dimensional feature extraction from fft spectral envelopes for statistical parametric
id133.
in proc. icassp, pages 5535   5539, 2016.

[10] g. hinton, j. mcclelland, and d. rumelhart.

distributed representation.
in d. rumelhart, j. mcclelland, and the pdp research group, editors, parallel distributed processing: explorations in
the microstructure of cognition. mit press, 1986.

[11] h. zen and a. senior.

deep mixture density networks for acoustic modeling in statistical parametric id133.
in proc. icassp, pages 3872   3876, 2014.

[12] x. wang, s. takaki, and j. yamagishi.

investigating very deep id199 for parametric id133.
in proc. isca ssw9, 2016.

[13] y. saito, s. takamichi, and saruwatari.

training algorithm to deceive anti-spoo   ng veri   cation for dnn-based id133.
in proc. icassp, 2017.

[14] h. zen, y. agiomyrgiannakis, n. egberts, f. henderson, and p. szczepaniak.

fast, compact, and high quality lstm-id56 based statistical parametric speech synthesizers for mobile devices.
in proc. interspeech, 2016.

andrew senior

generative model-based text-to-id133

47 of 50

references iii

[15] p. muthukumar and a. black.

a deep learning approach to data-driven parameterizations for statistical parametric id133.
arxiv:1409.8558, 2014.

[16] p. wang, y. qian, f. soong, l. he, and h. zhao.

id27 for recurrent neural network based tts synthesis.
in proc. icassp, pages 4879   4883, 2015.

[17] x. wang, s. takaki, and j. yamagishi.

investigation of using continuous representation of various linguistic units in neural network-based text-to-speech
synthesis.
ieice trans. inf. syst., e90-d(12):2471   2480, 2016.

[18] t. toda and k. tokuda.

statistical approach to vocal tract transfer function estimation based on factor analyzed trajectory id48.
in proc. icassp, pages 3925   3928, 2008.

[19] y.-j. wu and k. tokuda.

minimum generation error training with direct log spectral distortion on lsps for id48-based id133.
in proc. interspeech, pages 577   580, 2008.

[20] r. maia, h. zen, and m. gales.

statistical parametric id133 with joint estimation of acoustic and excitation model parameters.
in proc. isca ssw7, pages 88   93, 2010.

[21] k. tokuda and h. zen.

directly modeling speech waveforms by neural networks for statistical parametric id133.
in proc. icassp, pages 4215   4219, 2015.

[22] k. tokuda and h. zen.

directly modeling voiced and unvoiced components in speech waveforms by neural networks.
in proc. icassp, pages 5640   5644, 2016.

andrew senior

generative model-based text-to-id133

48 of 50

references iv

[23] f. itakura and s. saito.

a statistical method for estimation of speech spectral density and formant frequencies.
trans. ieice, j53a:35   42, 1970.

[24] a. van den oord, s. dieleman, h. zen, k. simonyan, o. vinyals, a. graves, n. kalchbrenner, a. senior, and

k. kavukcuoglu.
wavenet: a generative model for raw audio.
arxiv:1609.03499, 2016.

[25] s. mehri, k. kumar, i. gulrajani, r. kumar, s. jain, j. sotelo, a. courville, and y. bengio.

sampleid56: an unconditional end-to-end neural audio generation model.
arxiv:1612.07837, 2016.

[26] s. imai and c. furuichi.

unbiased estimation of log spectrum.
in proc. eurasip, pages 203   206, 1988.

[27] h. kameoka, y. ohishi, d. mochihashi, and j. le roux.

speech analysis with multi-kernel linear prediction.
in proc. spring conference of asj, pages 499   502, 2010.
(in japanese).

andrew senior

generative model-based text-to-id133

49 of 50

andrew senior

generative model-based text-to-id133

50 of 50

xo    lx  oxwwx(1) bayesianxwwxllo  o(2) auxiliary variables     + factorization(5) joint acoustic feature  extraction + model training(6) conditional wavenet-based tts(7) integrated end-to-end(8) bayesian end-to-endxwwllo  o(3) joint maximizationx  owlwlo      lx    lxwlwl      lx  xww    x  xwwxowlwl    o  lo      lx  o(4) step-by-step maximizatione.g., statistical parametric tts