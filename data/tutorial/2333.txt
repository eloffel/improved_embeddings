deep learning for natural language processing

id53

karl moritz hermann

kmh@google.com

28 feb 2017

questions

question

when were the    rst pyramids built?

jean-claude juncker

how old is keir starmer?

what is the current price for aapl?

what   s the weather like in london?

whom did juncker meet with?

when did you get to this lecture?

why do we yawn?

questions

question

answer

when were the    rst pyramids built?

2630 bc

jean-claude juncker

jean-claude juncker is a luxembourgish
politician. since 2014, juncker has been
president of the european commission.

how old is keir starmer?

54 years

what is the current price for aapl?

136.50 usd

what   s the weather like in london?

7 degrees celsius. clear with some clouds.

whom did juncker meet with?

the european commission president was
speaking after meeting with irish taoiseach
enda kenny in brussels.

when did you get to this lecture?

five minutes after it started.

why do we yawn?

when we   re bored or tired we don   t breathe
as deeply as we normally do. this causes a
drop in our blood-oxygen levels and yawning
helps us counter-balance that.

why do we care about qa?

because qa is awesome

1 qa is an ai-complete problem.

if we solve qa, we have solved every other problem, too.

2 many immediate and obvious applications

search, dialogue, information extraction, summarisation, ...

3 some pretty nice results already

ibm watson and jeopardy!, siri, google search ...

4 lots left to do!

plenty of interesting research and hard problems as well as
low-hanging fruit.

questions (again)

question

when were the    rst pyramids built?

jean-claude juncker

how old is keir starmer?

what is the current price for aapl?

what   s the weather like in london?

whom did juncker meet with?

when did you get to this lecture?

why do we yawn?

questions (again)

question

answer source

when were the    rst pyramids built?

encyclopedia

jean-claude juncker

recent encyclopedia / wikipedia

how old is keir starmer?

very recent encyclopedia (i.e. wikipedia).
extrapolate from date of birth.

what is the current price for aapl?

nasdaq ticker

what   s the weather like in london?

met o   ce

whom did juncker meet with?

independent

the
   jean-claude
juncker doesn   t want northern ireland and
republic to have post-brexit hard border   

article

when did you get to this lecture?

personal observation

why do we yawn?

various studies on the matter.

id53 depends on three kinds of data
and this gives us a good system for thinking about various qa tasks

question
factual questions   
complex/narrative
questions
information retrieval

context/source
sets of documents
(corpus)

a single document

library reference

knowledge base

non-linguistic types
of data (gps, images,
sensors, ...)

answer
a single fact

an explanation

a document

a sentence or
paragraph extracted
from somewhere

an image or other
type of object

another question

    factual question     factoid question, but that doesn   t work in b.e.

question taxonomy

many possible taxonomies for questions 1

    wh- words
    subject of question
    the form of expected answers
    types of sources from which answers may be drawn

for the purposes of building qa systems it is useful to start by
considering the sources an answer may be drawn from.

focus on the answer rather than the question.

1see e.g. pomerantz (2005), a linguistic analysis of question taxonomies

qa taxonomy discovery

three questions for building a qa system

    what do the answers look like?
    where can i get the answers from?
    what does my training data look like?

by the end of this lecture, being able to answer these questions
should allow you to devise a qa system for any given task.

areas in id53

reading
comprehension

id29

visual qa

information
retrieval

library reference

    answer based on a document
    context is a speci   c document
    answer is a logical form,
possible executed against a kb
    context is a knowledge base
    answer is simple and factual
    context is one/multiple image(s)
    answer is a document/paragraph/sentence
    context is a corpus of documents
    answer is another question
    context is the structured knowledge

available in the library and the librarians
view of it.

remainder of this lecture
for the remainder of this lecture, we will cover four areas of id53 in
more depth.

    id29
    answer sentence selection
    reading comprehension
    visual id53

id29

id29 is the process of mapping natural language into a
formal representation of its meaning. depending on the chosen
formalism this logical representation can be used to query a
structured knowledge base.

question     logical form     kb query     answer

id29 is question   logical form.
we (often mistakenly) then assume that lf   answer is trivial.

knowledge bases for qa with id29

knowledge bases typically represent their data as triples
(married-to, michelle obama, barack obama)
(member-of, united kingdom, european union)
generally: (relation, entity1, entity2)

there are several (large) databases freely available to use, e.g.:

freebase 1.9 billion triples on general knowledge. defunct as

of 2016 and replaced by google id13

wikidata information on 25 million entities
openstreetmap 3 billion triples on geography

geoquery 700 facts about us geography. tiny dataset, but

frequently used in id29 work.

kbs are cheap     supervised data is expensive!

free9172 917 freebase annotated questions

geoquery3 880 questions on us geography

nlmaps4 2,380 natural language queries on the osm data

these kinds of datasets are incredibly expensive to create as they
require experts for the manual annotation process, who are trained
in using a given database schema:

   where are kindergartens in hamburg?   

query(area(keyval(name,hamburg)),

nwr(keyval(amenity,kindergarten)),
qtype(latlong))

2cai and yates, 2013
3zelle and mooney, 1996
4hass and riezler, 2016

a deep learning approach to id29

id29 can be viewed as a sequence to sequence model,
not unlike machine translation.

details
  encode sentence with sequence models
  decode with standard mechanisms from mt

  supervised training data hard to come by
  depending on formalism used, highly complex target side
  how to deal with proper nouns and numbers?

one solution to sparsity: avoid logical forms

id29 frequently reduce the reliance on supervised data
(language-logical form) by exploiting other types of data such as
question-answer pairs or corpora of questions only.

berant et al. (2013): id29 on freebase from qa pairs
reddy et al. (2014): large-scale id29 without qa pairs

improved neural id29

we can apply the same idea to neural id29, and
further take mechanisms from machine translation to improve
performance and data e   ciency:

    like in mt, using attention can be helpful

dong and lapata (2016): language to logical form with neural attention

    exploit the highly rigid structure in the target side to

constrain generation
liang et al. (2016): neural symbolic machines

ling et al. (2016): latent predictor networks for code generation

    make use of semi-supervised training to counter sparsity

kocisky et al. (2016): id29 with semi-supervised sequential

autoencoders

generation with multiple sources

ling et al. (2016): latent predictor networks for code generation

generation with multiple sources

ling et al. (2016): latent predictor networks for code generation

generation with multiple sources

ling et al. (2016): latent predictor networks for code generation

id29 summary

  lf instead of answer
makes system robust
  answer independent of
question and parsing
mechanism
  can deal with rapidly
changing information

  constrained to queriable
questions in db schema
  no database is large enough
  training data hard to    nd

  when were the pyramids built?
? jean-claude juncker
  how old is keir starmer?
  what is the price for aapl?
  what   s the weather in london?
  whom did juncker meet with?
  when did you get here?
  why do we yawn?

caveat: each of these examples
requires a di   erent underlying kb!

reading comprehension
answer a question related to a given document

corpora for reading comprehension

id98/dailymail5 over 1 million cloze form qa pairs with articles

from id98 and mail online for context. pick an
anonymised entity.

cbt6 700k qa pairs, children   s books as context. pick one

of 10 candidates.

squad7 100k manual qa pairs with 500 wikipedia articles for

context. answer is a span.

assumptions made in all of the above tasks

    context is read on the    y and unknown during training phase
    answer is contained in the context as a single word or span
    this constraint does not hold for reading comprehension in general!

5hermann et al., 2015
6hill et al., 2015
7rajpurkar et al., 2016

id98/dailymail dataset example

id98 article

document the bbc producer allegedly struck by jeremy clarkson will not

press charges against the    top gear    host, his lawyer said
friday. clarkson, who hosted one of the most-watched
television shows in the world, was dropped by the bbc
wednesday after an internal investigation by the british
broadcaster found he had subjected producer oisin tymon    to
an unprovoked physical and verbal attack.    . . .

query producer x will not press charges against jeremy clarkson, his

lawyer says.

answer oisin tymon

we formulate cloze style queries from the story paraphrases.

oov and proper nouns are dealt with by replacing all entities with
anonymised markers. this greatly reduces the vocabulary size.

id98/dailymail dataset example

id98 article

document the ent381 producer allegedly struck by ent212 will not press
charges against the     ent153     host , his lawyer said friday .
ent212 , who hosted one of the most - watched television
shows in the world , was dropped by the ent381 wednesday
after an internal investigation by the ent180 broadcaster found
he had subjected producer ent193     to an unprovoked physical
and verbal attack .     . . .

query producer x will not press charges against ent212 , his lawyer

says .

answer ent193

we formulate cloze style queries from the story paraphrases.

oov and proper nouns are dealt with by replacing all entities with
anonymised markers. this greatly reduces the vocabulary size.

a generic neural model for reading comprehension

given context d and question q, the id203 of an answer a can
be represented as:

p(a|q, d)     exp(w (a)g (q, d)),

s.t.a     v

fill in the details
  encode question and context with sequence models
  combine q and c with an mlp or attention
  select answer from attention map, by using a classi   er, or with
generative setup

  how to deal with out of vocabulary (oov) terms?
  how to deal with proper nouns and numbers?

reading comprehension with attention

read (encode) context document and question
use question to attend to context
use joint representation to generate answer

    predict based on attention map
    generate conditioned on joint representation
    classify over set of candidate answers

reading comprehension with attention

denote the outputs of a bidirectional lstm as       y (t) and       y (t).

form two encodings, one for the query and one for each token in
the document,

u =       yq (|q|) ||       yq (1),

yd (t) =       yd (t) ||       yd (t).

the representation r of the document d is formed by a weighted
sum of the token vectors. the weights are interpreted as the
model   s attention,

m(t) = tanh (wymyd (t) + wumu) ,
s(t)     exp (w

(cid:124)
ms m(t)) ,

r = yd s.

de   ne the joint document and query embedding via a non-linear
combination:

g ar(d, q) = tanh (wrg r + wug u) .

attentive reader training

models were trained using asynchronous minibatch stochastic
id119 (rmsprop) on approximately 25 gpus.

attention sum reader

the model can be modi   ed to make use of the fact that the
answer is a word from the context document. now we calculate
the id203 of the answer being in position i of the context8:

p(i|q, d)     exp(fi (d)    g (q))

positional probabilities can then be summed to form token-based
probabilities:

p(w|q, d)     (cid:88)

p(i|q, d)

i(w ,d)

the rest of the model is equivalent to the attentive reader model
presented before.

8kadlec et al. (2016), text understanding with the attention sum reader

network

reading comprehension summary

  ask questions in context
  easily used in
discriminative and generative
fashion
  large datasets available

  constraint on context often
arti   cial
  many types of questions
unanswerable

? when were the pyramids built?
? jean-claude juncker
? how old is keir starmer?
? what is the price for aapl?
? what   s the weather in london?
  whom did juncker meet with?
  when did you get here?
  why do we yawn?

caveat: need context for any of
these, and incredibly up-to-date
context for some of these.

answer sentence selection

answer sentence selection

answer sentence selection describes the task of picking a suitable
sentence from a corpus that can be used to answer a question.

questions factual questions, possibly with context

data source    the web    or the output of some ir system

answer one or several excerpts pertinent to the answer

the answer is guaranteed to be extracted, while in reading
comprehension it could be either generated or extracted.

corpora for answer sentence selection

trec qa track (8-13) several hundred manually-annotated
question answer pairs with around 20 candidates per instance.

ms marco 100k question-answer pairs with 10 contextual
passages each. can also be used as a qa dataset for reading
comprehension.

likewise, answer sentence selection plays a role in any information
retrieval setup, and datasets from ir and other qa tasks can easily
be converted into answer selection style datasets.

a neural model for answer sentence selection

we need to compute the id203 of an answer candidate a and
a question q matching. note that this is di   erent from the
previous task as we now calculate that score independently of all
other candidates:

p(y = 1|q, a) =   (qt ma + b)

yu et al., 2014

evaluation

unlike single entity style qa where we can use a simple accuracy
measure, tasks such as answer sentence selection require more
specialised metrics for evaluating model performance.

measure

description

formula

accuracy

binary measure

mean recipro-
cal rank

measures position of    rst rel-
evant document in return set.

1|q|

#true/#total

(cid:80)|q|

i=1

1

ranki

id7 score

machine translation measure
for translation accuracy

complicated

answer selection summary

  designed to deal with large
amounts of context
  more robust than    true    qa
systems as it turns provides
context with its answers
  obvious pipeline step
between ir and qa

  does not provide answers,
provides context only
  real-world use depends on
underlying ir pipeline

  when were the pyramids built?
  jean-claude juncker
  how old is keir starmer?
  what is the price for aapl?
  what   s the weather in london?
? whom did juncker meet with?
  when did you get here?
  why do we yawn?

note: things like age or stock price
may produce answers, but with no
guarantee of accuracy (any mention
of any aapl price might be a good
   t).

visual id53

sometimes questions require context outside of pure language.

visual qa: task and corpora

in recent years a number of visual qa datasets have sprung up.
some of the more popular ones include:

visualqa agrawal et al. (2015)
vqa 2.0 goyal et al. (2016)

coco-qa ren et al. (2015)

details between these datasets vary, but the basic organisation
remains the same of images paired with simple questions and
answers (either free form or from a list of options).

all of these are reasonably large (100ks of images, over 1m
questions).

visual qa is quite straight-forward

    question is language     some encoder
    context is a single picture     convolutional network
    answer is a single word     classi   er function

we have covered all the components already:

blind model

ignoring the images is a good baseline!

    what colour is the cat?
    how many chairs are around the table?
    what furniture is in the bedroom?
    where is the person sleeping?

we can get reasonably good guesses in at many of these
questions without seeing an image for context. see goyal et al.
(2016)

attention methods for visual qa

viewing vqa from the perspective of our default qa paradigm,
there is signi   cant overlap with reading comprehension style
models. we use similar techniques to improve performance.

we can use attention on visual representations:

yang et al. (2015): stacked attention networks for image question

answering

question:what are sitting in the basket on a bicycle?id98/lstmsoftmaxdogsanswer:id98+query+attention layer 1attention layer 2feature vectors of di   erentparts of imageattention methods for visual qa

viisual id53 summary

  extra modality    for free   
  plenty of training data
available as of recently

  currently quite gimmicky
  still a long way to go

  when were the pyramids built?
  jean-claude juncker
  how old is keir starmer?
  what is the price for aapl?
? what   s the weather in london?
  whom did juncker meet with?
  when did you get here?
  why do we yawn?

summary (how to build your own qa system)

build a qa model in seven questions

    what is the task?
    what do question, answer and context look like?
    where does the data come from?
    can you augment the data?
    how to encode question and context?
    how to combine question and context?
    how to predict or generate an answer?

there are plenty of open questions left in qa.
just remember to start with the data!

end

sources and further reading
id53 theory and datasets
pomerantz (2005), a linguistic analysis of question taxonomies
nguyen et al. (2016), ms marco: a human generated machine reading
comprehension dataset
haas and riezler (2016), a corpus and semantic parser for multilingual natural
language querying of openstreetmap

id29
artzi et al. (2013), id29 with id35
berant et al. (2013), id29 on freebase from question-answer pairs
http://nlp.stanford.edu/software/sempre/

reading comprehension
hermann et al. (2015), teaching machines to read and comprehend
kadlec et al. (2016), text understanding with the attention sum reader network

visual qa
yang et al. (2015), stacked attention networks for image id53
ren et al. (2015), exploring models and data for image id53
goyal et al. (2016), making the v in vqa matter: elevating the role of image
understanding in visual id53.

https://avisingh599.github.io/deeplearning/visual-qa/

