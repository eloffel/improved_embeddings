deep learning for nlp:

memory

ed grefenstette
etg@google.com

the plan

1. the transduction bottleneck
2. limitations of id56s
3. id56s revisited
4. attention
5. register machines
6. pushdown automata

the bottleneck

revisiting some preliminaries: id56s

    recurrent hidden layer outputs 

distribution over next 
symbol/label/nil

    connects "back to itself"

    conceptually: hidden layer 

models history of the sequence.

revisiting some preliminaries: id56s

    id56s fit variable width problems 

well

    unfold to feedforward nets with 

shared weights

    can capture long(ish) range 

dependencies

some obvious id56 applications

    language modelling: chain rule gives us p(s) from    t   (1,t)p(st|s1, ..., st-1).
    sequence labelling: p(lt|s1, ..., st)
    sentence classification: model p(l|s) from cell state, mean pooling, etc.

but... simpler/better models exist for most of these.

transduction with conditional models

transduction with conditional models

sequence to sequence mapping with id56s

many nlp (and other!) tasks are castable as transduction problems. e.g.:

translation: english to french transduction

parsing: string to tree transduction

computation(?!): input data to output data transduction

sequence to sequence mapping with id56s

generally, goal is to transform some source sequence

into some target sequence

sequence to sequence mapping with id56s

represent s and model p(ti+1|t1...tn; s) with id56s:
1. read in source sequence to produce s.
2. train model to maximise the likelihood of t given s.
3. test time: generate target sequence t (greedily, id125, etc) from s.

a simple encoder-decoder model

    concatenate source and target sequences into joint sequences:

s1 s2 ... sm ||| t1 t2 ... tn

    train a single id56 or pair of id56s over joint sequences
    ignore id56 output until separator symbol (e.g. "|||")
    jointly learn to compose source and generate target sequences

deep lstms for translation

(sutskever et al. nips 2014)

learning to execute

task (zaremba and sutskever, 2014):

    read simple python scripts character-by-character
    output numerical result character-by-character.

the bottleneck for simple id56s

    non-adaptive capacity

    target sequence modelling 

dominates training

    gradient-starved encoder

    fixed size considered harmful?

limitations of id56s: 

a computational perspective

computational hierarchy

turing machines (computable functions)

         

pushdown automata (context free languages)

         

finite state machines (regular languages)

id56s and turing machines

sieglemann & sontag (1995)

any turing machine {q,   ,   , ...} can be translated into an id56:

    one-hot states as hidden layer, size |q|
    one-hot encoding of symbols of    as input
    one-hot encoding of      {l, r} as outputs
    identity as recurrence matrix,    as update matrix

by extension, id56s can express/approximate a set of turing machines.

but expressivity     learnability!

id56s and turing machines

simple id56s (basic, gru, lstm) cannot* learn turing machines:
    id56s do not control the "tape". sequence exposed in forced order.

    maximum likelihood objective (p(x|  ), p(x,y|  ), ...) produces model close to 

training data distribution.

    insane to expect regularisation to yield structured computational model as 

an out-of-sample generalisation mechanism.

*  through "normal" sequence-based maximum likelihood training.

id56s and finite state machines

not a proof, but think of simple id56s as approximations of id122s:
    effectively order-n markov chains, but n need not be specified

    memoryless in theory, but can simulate memory through dependencies:

    e.g. ".*a...a"     p(x="a"|"a" was seen four symbols ago)

    very limited, bounded form of memory

    no incentive under ml objectives to learn dependencies beyond the sort and 

range observed during training

id56s and finite state machines

some problems:
    id56 state acts as both controller and "memory"

    longer dependencies require more "memory"

    tracking more dependencies requires more "memory"

    more complex/structured dependencies require more "memory"

    ultimately, id122s are pretty basic.

why more than id122?

natural language is arguably at least context free (need at least a pda)

even if it's not, rule parsimony matters!

e.g. model anbn, if in practice n is never more than n.

regular language (n+1 rules)
  |(ab)|(aabb)|(aaabbb)|...

id18 (2 rules)

s     a s b

s       

computational hierarchy

   [

we we 
want to 
be here

turing machines (computable functions)

         

pushdown automata (context free languages)

         

we are here    

finite state machines (regular languages)

the plan

1. the transduction bottleneck
2. limitations of id56s
3. id56s revisited
4. attention
5. register machines
6. pushdown automata

questions?

id56s revisited

id56s: more api than model

id56s: more api than model

id56s: more api than model

id56s: more api than model

id56s: more api than model

for a recurrent cell, we are modelling a function 

id56: x     p     y     n

where xt   x is an input, pt   p is the previous recurrent state, yt   y is an output, 
and nt   n is an updated recurrent state, all possibly nested sets of vectors, 
scalars, ...

typically p = n, and pt+1 = nt for pt+1, nt     p.

sometimes x=y, and xt+1 = f(yt) where f is not necessarily differentiable.

id56s: more api than model

we aim to satisfy the following constraint (with some exceptions):

where the bar operator 
indicates flattened sets.

the controller-memory split

    summary: keep things differentiable, use p/n to track state, do whatever you 

want within the cell.

    cells can nest/stack/be sequenced, as long as everything "type checks".

    we can address some of the issues discussed before with this api by 

separating memory and controller.

    that's just the beginning!

the controller-memory split

attention: rom

attention

    you have an array of vectors representing some data.
    your controller deals with i/o logic.
    you want to read your data array at each timestep.
    you want to accumulate gradients in your memory.

attention (early fusion)

id56: x     p     y     n

pt = (ht-1, m)

yt , ht= controller(xs, ht-1)

nt = (ht, m)

xs = xt     fatt(ht-1, m)

e.g. fatt(h, m) = m     softmax(h     watt     m)

attention (late fusion)

id56: x     p     y     n

pt = (ht-1, m)

nt = (ht, m)

wt , ht= controller(xt, ht-1)

yt = fcomp(wt, fatt(ht, m))

alternatively:
yt = fatt(ht, m) if fatt yields a distribution 
over memory positions.

rom for encoder-decoder models

    encoder produces array of representations, e.g. one vector per token.

    representations are packed into an attention matrix.

    decoder is a controller + memory model with attention matrix as memory.

    gradients of error w.r.t. memory provide gradients of error w.r.t. encoder.

rom for encoder-decoder models

    encoder is gradient starved no more!

    compute soft alignments between sequences.

    search for information in larger sequences.

    memory isn't touched, so operations can be done in place.

skipping the bottleneck

skipping the bottleneck

recognizing id123 (rte)

a man is crowd surfing at a concert
the man is at a football game
the man is drunk
the man is at a concert

-
-
-

a wedding party is taking pictures

-
-
-

there is a funeral
they are outside
someone got married

contradiction
neutral
entailment

contradiction
neutral
entailment

44

word-by-word attention

45

girl + boy = kids

46

large-scale supervised reading comprehension

the bbc producer allegedly struck by jeremy clarkson will not press charges against the 
   top gear    host, his lawyer said friday. clarkson, who hosted one of the most-watched 
television shows in the world, was dropped by the bbc wednesday after an internal 
investigation by the british broadcaster found he had subjected producer oisin tymon    to
an unprovoked physical and verbal attack.        

cloze-style question:
query: 
answer:  oisin tymon

producer x will not press charges against jeremy clarkson, his lawyer says.

machine reading with attention

the impatient reader

    create embedding for 

each token in document
   
read query word by word
    attend over document at 
each step through query
iteratively combine 
attention distribution
predict answer with 
increased accuracy

   

   

example qa heatmap

correct prediction (ent49) - requires id2

attention summary

    attention successfully mitigates limitations of original id195
    versatile, adaptable to many problems
    can be tailored to specific sorts of processes, e.g. id193
    helpful for learning good source representations, although these are strongly 

tied to the end-task

    read-only puts strong onus on controller to track what's been read
    read-only means encoder needs to do a lot of the legwork

the plan

1. the transduction bottleneck
2. limitations of id56s
3. id56s revisited
4. attention
5. register machines
6. pushdown automata

questions?

register machines: ram

computational hierarchy

turing machines (computable functions)

         

pushdown automata (context free languages)

         

finite state machines (regular languages)

attention as rom

register memory as ram

neural ram: general idea

    controller deals with i/o
    controller also produces distributions over memory registers for 

reading/writing

    controller decides what to write, how much to erase, etc.
    controller and memory state are updated, and recurrent cell produces output, 

next state

id56: x     p     y     n (an example of)

pt = (ht-1, mt-1, rt-1)
xs =xt     rt-1
yt, kread, kwrite, v, ht = controller(xs, ht-1)
rt = fread(kread, mt-1) where e.g. fread = fatt
mt = fwrite(v, kwrite, mt-1)
e.g.  mt[i] = a[i]  v + (1-a[i])  mt-1[i] 
where a = softmax(kwrite  wwrite  mt-1)

extensions

    location based addressing: e.g. state tracks an initially one-hot key which is 

shifted by the controller.

    mix of location and content based.
    hard addressing with reinforce
    more esoteric/heuristic addressing mechanisms (better to be kept diff.)
    memory key/content factorization (e.g. npi)
    memory prefixes

relation to actual turing machines

    part of the "tape" is internalised
    controller can control tape motion via various mechanisms
    id56 could model state transitions
    weird split between external and internal tape in id195 paradigm
    in ml-based training, number of computational steps is tied to data
    unlikely to learn a general algorithm, but experiments (e.g. graves et al. 

2014) show better generalisation on symbolic tasks.

register machines and nlu

    complex reasoning probably requires something both more expressive and 

more structured than id56s + attention.

    these architectures are complex, hard to train, many design choices.

    not always an immediate mapping of problems to purported capabilities of 

these architectures.

    fascinating research to be done here, but don't forget about simpler models!

stacks: neural pdas

computational hierarchy

turing machines (computable functions)

         

pushdown automata (context free languages)

         

finite state machines (regular languages)

the controller-memory split

controlling a neural stack

controller api

controller + stack interaction

example: a continuous stack

example: a continuous stack

example: a continuous stack

synthetic transduction tasks

copy

reversal

a1a2a3...an     a1a2a3...an

a1a2a3...an     an...a3a2a1

synthetic itg transduction tasks

subject-verb-object to subject-object-verb reordering

si1 vi28 oi5 oi7 si15 rpi si19 vi16 oi10 oi24     so1 oo5 oo7 so15 rpo so19 vo16 oo10 oo24 vo28

genderless to gendered grammar

we11 the en19 and the em17     wg11 das gn19 und der gm17

results

experiment

copy

reversal

svo-sov

stack

poor

solved

solved

conjugation

converges

queue

solved

poor

solved

solved

deque

solved

solved

solved

solved

deep lstm

poor

poor

converges

converges

every neural stack/queue/deque that solves a problem preserves the solution for 
longer sequences (tested up to 2x length of training sequences).

rapid convergence

regular language (n+1 rules)
  |(ab)|(aabb)|(aaabbb)|...

id18 (2 rules)

s     a s b

s       

differentiable stacks / queues / etc

    grefenstette et al. 2015: stacks, queues, double ended queues

    presented here

    joulin and mikolov 2015: stacks, lists

    more lightweight stack semantics
    check out the experiments!
    dyer et al. 2015: id200s

    supervision needed for push/pop
    doesn't fit the controller/memory framework, but worth reading!

neural pda summary

    decent approximations of classical pda
    architectural bias towards recursive/nested dependencies
    should be useful for syntactically rich natural language

    parsing
    compositionality
    but little work on applying these architectures

    does going up in the computational hierarchy help? we need to find out.

conclusions

    easy to design an overly complex model. not always worth it.
    better to understand the limits of current architectures within the context of 

a problem.

    by understanding the limitations and their nature, often better solutions pop 

out of analysis. best example: chapters 1-3 of felix gers' thesis (2001).

    think not just about the model, but about the complexity of the problem you 

want to solve.

    nonetheless, be creative. plenty of problems to solve in nlu and elsewhere.

thank you

credits

oxford machine learning and computational linguistics groups

deepmind natural language research group

