natural language processing:

algorithms and applications, old and new

noah smith

carnegie mellon university 2015       university of washington

wsdm winter school, january 31, 2015

outline

i. introduction to nlp

ii. algorithms for nlp

iii. example applications

part i

introduction to nlp

why nlp?

analysisgeneration?text/speechwhat does it mean to    know    a language?

levels of linguistic knowledge

phonologyorthographymorphologysyntaxsemanticspragmaticsdiscourseid102"shallower""deeper"speechtextorthographic knowledge required

                                                                                                                                                                                                                                                                                                                                                                                            .                                                                                                                                                                                                                                                         15          ....morphological knowledge required

uygarla  st  ramad  klar  m  zdanm    ss  n  zcas  na    (behaving) as if you are
among those whom we could not civilize   

a ship-shipping ship, shipping shipping-ships.
(syntactic knowledge required.)

analysisgeneration?text/speechexample: part-of-speech tagging
(gimpel et al., 2011; owoputi et al., 2013)

ikr

smh

he

asked

   r

yo

last

name

so

he

can

add

u

on

fb

lololol

example: part-of-speech tagging
(gimpel et al., 2011; owoputi et al., 2013)

i know, right

shake my head

ikr

smh

he

asked

for
   r

your
yo

last

name

so

he

can

add

you
u

facebook

laugh out loud

on

fb

lololol

example: part-of-speech tagging
(gimpel et al., 2011; owoputi et al., 2013)

i know, right

shake my head

ikr
!

smh

g

he
o

asked

v

for
   r
p

your
yo
d

last
a

name

n

interjection

acronym

pronoun

verb

prep.

det.

adj.

noun

so
p

he
o

can
v

add
v

you
u
on
o p

facebook

laugh out loud

fb
   

lololol

!

preposition

proper noun

part ii

algorithms for nlp

a starting point: categorizing texts

mosteller and wallace (1963) automatically inferred the authors of
the disputed federalist papers.
many other examples:

(cid:73) news: politics vs. sports vs. business vs. technology ...
(cid:73) reviews of    lms, restaurants, products: postive vs. negative
(cid:73) email: spam vs. not
(cid:73) what is the reading level of a piece of text?
(cid:73) how in   uential will a scienti   c paper be?
(cid:73) will a piece of proposed legislation pass?

categorizing texts: a standard line of attack

1. human experts label some data.

2. feed the data to a learning algorithm l that constructs an

automatic labeling function (classi   er) c .

3. apply that function to as much data as you want!

categorizing texts: notation

(cid:73) training examples: x = (cid:104)x1, x2, . . . , xn(cid:105)
(cid:73) their categorical labels: y = (cid:104)y1, y2, . . . , yn(cid:105), each yn     y
(cid:73) a classi   er c seeks to map any x to the    correct    y

x     c     y

(cid:73) a learner l infers c from x and y

x    
y     l     c

categorizing texts: c

first,    maps (cid:104)x, y(cid:105) into rd (feature vector).

then c uses the vector to map into y.

(cid:73) linear models de   ne:

c (x) = argmax

y   y

w(cid:62)  (x, y )

where w     rd is a vector of coe   cients.

(cid:73) many non-linear options available as well (id90,

neural networks, . . . ).

categorizing texts
example from yano et al. (2012)

be it enacted by the senate and house of representatives of the united

states of america in congress assembled, section 1.

compensation for

work-related injury. (a) authorization of payment- the secretary of the

treasury shall pay, out of money in the treasury not otherwise

appropriated, the sum of $46,726.30 to john m. ragsdale as compensation

for injuries sustained by john m. ragsdale in june and july of 1952

while john m. ragsdale was employed by the national bureau of standards.

(b) settlement of claims- the payment made under subsection (a) shall be

a full settlement of all claims by john m. ragsdale against the united

states for the injuries referred to in subsection (a).

sec. 2.

limitation on agents and attorneys    fees. it shall be unlawful for an

amount that exceeds 10 percent of the amount authorized by section 1 to

be paid to or received by any agent or attorney in consideration of

services rendered in connection with this act. any person who violates

this section shall be guilty of an infraction and shall be subject to a

fine in the amount provided in title 18, united states code.

example of a linear model

probabilistic models de   ne p(y = y |   (x, y ) = f):

c (x) = argmax

y   y

= argmax

y   y

p(y = y |   (x, y ) = f)
p(y = y )    p(  (x, y ) = f | y = y )

p(  (x, y ) = f)

na    ve bayes makes a strong assumption:

p([  (x, y )]d = fd | y = y )

. . . = argmax

y   y

p(y = y )

= argmax

y   y

log p(y = y )

+

(cid:124)

(cid:123)(cid:122)

wy =y

d(cid:88)

d=1

(cid:124)

log p([  (x, y )]d = fd | y = y )

(cid:123)(cid:122)

wy =y ,  d =fd

(cid:125)

d(cid:89)
(cid:125)

d=1

note

(cid:73) na    ve bayes is a linear model and a probabilistic model.

(cid:73) another example that is both linear and probabilistic:

(multinomial) id28
(cid:73) not all linear models are probabilistic!
(cid:73) not all probabilistic models are linear!

c as linear model

c (x) = argmax

y   y

w(cid:62)  (x, y )

   x, y3      x, y1      x, y2      x, y4   f   f2   x, y3      x, y1      x, y2      x, y4   f   f2w   x, y3      x, y1      x, y2      x, y4   f   f2wcategorizing texts: l

usually learning l involves choosing w.
often set up as an optimization problem:

n(cid:88)

n=1

  w = argmin
w:   (w)     

1
n

(cid:124)

loss(xn, yn; w)

(cid:123)(cid:122)

loss(w)

(cid:125)

example: classic multi-class support vector machine,

   (w) = (cid:107)w(cid:107)2

2

loss(x, y ; w) =    w(cid:62)  (x, y ) + max

y(cid:48)   y w(cid:62)  (x, y(cid:48)) +

(cid:26) 0 if y = y(cid:48)

1 otherwise

categorizing texts: l

usually learning l involves choosing w.
often set up as an optimization problem:

n(cid:88)

n=1

  w = argmin
w:   (w)     

1
n

(cid:124)

loss(xn, yn; w)

(cid:123)(cid:122)

loss(w)

(cid:125)

example: multinomial id28 with (cid:96)2 id173,

   (w) = (cid:107)w(cid:107)2

2

loss(x, y ; w) =    w(cid:62)  (x, y ) + log

exp w(cid:62)  (x, y(cid:48))

(cid:88)

y(cid:48)   y

what about    (w)?

we usually constrain w to fall in an (cid:96)2 ball:

min
w:(cid:107)w(cid:107)2

2     

loss(w)

   

loss(w) + c(cid:107)w(cid:107)2

2

min
w

what about    (w)?

we usually constrain w to fall in an (cid:96)2 ball:

min
w:(cid:107)w(cid:107)2

2     

loss(w)

   

loss(w) + c(cid:107)w(cid:107)2

2

min
w

newer idea: use (cid:96)1 ball instead (lasso; tibshirani, 1996).

min
w

loss(w) + c (cid:107)w(cid:107)1

(cid:124)(cid:123)(cid:122)(cid:125)
d(cid:88)

|wd|

d=1

what about    (w)?

we usually constrain w to fall in an (cid:96)2 ball:

min
w:(cid:107)w(cid:107)2

2     

loss(w)

   

loss(w) + c(cid:107)w(cid:107)2

2

min
w

newer idea: use (cid:96)1 ball instead (lasso; tibshirani, 1996).

min
w

loss(w) + c (cid:107)w(cid:107)1

(cid:124)(cid:123)(cid:122)(cid:125)
d(cid:88)

|wd|

even newer idea: use    (cid:96)1 of (cid:96)2    (group lasso; yuan and lin, 2006).

d=1

visualizing the lasso and group lasso

see our tutorial from eacl (martins et al., 2014).

visualizing the lasso and group lasso

see our tutorial from eacl (martins et al., 2014).

using data to create group lasso   s groups
(yogatama and smith, 2014)

(cid:73) in categorizing a document, only some sentences are relevant.
(cid:73) groups: one group for every sentence in every training-set

document.

(cid:73) all of the features (words) occurring in the sentence are in its

group.

(cid:73) special algorithms are required to learn with

thousands/millions of overlapping groups.

see    making the most of bag of words: sentence id173
with alternating direction method of multipliers,    yogatama and
smith (2014).

text categorization example
ibm vs. mac

id31
amazon dvds (blitzer et al., 2007)

categorizing texts: choosing a learner l

(cid:73) do you want posterior probabilities, or just labels?
(cid:73) how interpretable does your model need to be?
(cid:73) what background knowledge do you have about the data that

can help?

(cid:73) what methods do you understand well enough to explain to

others?

(cid:73) what methods will your team/boss/reader understand?
(cid:73) what implementations are available?
(cid:73) cost, scalability, programming language, compatibility with

your work   ow, ...

(cid:73) how well does it work (on held-out data)?

categorizing texts: recipe

1. obtain a pool of correctly categorized texts d.
2. de   ne a feature function    from hypothetically-labeled texts

to feature vectors.

3. select a parameterized function c from feature vectors to

categories.

4. select c    s parameters w using training set (cid:104)x, y(cid:105)     d and

learner l.

5. predict labels using c on a held-out sample from d; estimate

quality.

from categorization to id170

instead of a    nite, discrete set y, each input x has its own yx .

(cid:73) e.g., yx is the set of pos sequences that could go with

sentence x.

|yx| depends on |x|, often exponentially!

(cid:73) our 25-pos tagset gives as many as 25|x| outputs.

yx can usually be de   ned as a set of interdependent
categorization problems.

(cid:73) each word   s pos depends on the pos tags of nearby words!

decoding a sequence

abstract problem:

x = (cid:104)x[1], x[2], . . . , x[l](cid:105)
   
c
   
y = (cid:104)y [1], y [2], . . . , y [l](cid:105)

simple solution: categorize each x[(cid:96)] separately.

but what if y [(cid:96)] and y [(cid:96) + 1] depend on each other?

linear models, generalized to sequences

  y = argmax

y   yx

w(cid:62)  (x, y [1], . . . , y [l])

linear models, generalized to sequences

  y = argmax

y   yx

w(cid:62)  (x, y [1], . . . , y [l])

(cid:32) l(cid:88)

(cid:33)
  local (x, (cid:96), y [(cid:96)     1], y [(cid:96)])

  y = argmax

y   yx

w(cid:62)

(cid:96)=2

special case: hidden markov model

id48s are probabilistic; they de   ne:

p(x, y ) = p(stop | y [l])

p(x[(cid:96)] | y [(cid:96)])

   p(y [(cid:96)] | y [(cid:96)     1])

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

emission

transition

l(cid:89)

(cid:96)=1

(cid:124)

(where y [0] is de   ned to be a special start symbol).

emission and transition counts can be treated as features, with
coe   cients equal to their log-probabilities.
w(cid:62)  local (x, (cid:96), y [(cid:96)     1], y [(cid:96)]) = log p(x[(cid:96)] | y [(cid:96)]) + log p(y [(cid:96)] | y [(cid:96)     1])

the probabilistic view is sometimes useful (we will see this later).

finding the best sequence y : intuition

if we knew y [1 : l     1], picking y [l] would be easy:

w(cid:62)  local (x, l, y [l     1],   )+ w(cid:62)

argmax

  

(cid:96)=2

(cid:32)l   1(cid:88)

(cid:33)
  local (x, (cid:96), y [(cid:96)     1], y [(cid:96)])

finding the best sequence y : notation

let:

v [l     1,   ] = max
y [1:l   2]

w(cid:62)

(cid:32)l   2(cid:88)

(cid:33)
  local (x, (cid:96), y [(cid:96)     1], y [(cid:96)])

our choice for y [l] is then:

(cid:96)=2

+ w(cid:62)  local (x, l     1, y [l     2],   )
(cid:19)

(cid:18)

argmax

  

max

  (cid:48) w(cid:62)  local (x, l,   (cid:48),   ) + v [l     1,   (cid:48)]

finding the best sequence y : notation

let:

v [l     1,   ] = max
y [1:l   2]

w(cid:62)

(cid:32)l   2(cid:88)

(cid:33)
  local (x, (cid:96), y [(cid:96)     1], y [(cid:96)])

(cid:96)=2

+ w(cid:62)  local (x, l     1, y [l     2],   )

note that:
v [l     1,   ] = max

  (cid:48) v [l     2,   (cid:48)] + w(cid:62)  local (x, l     1,   (cid:48),   )

and more generally:
   (cid:96)     {2, . . .}, v [(cid:96),   ] = max

  (cid:48) v [(cid:96)     1,   (cid:48)] + w(cid:62)  local (x, (cid:96),   (cid:48),   )

visualization

n
o
   
v
a
!
...

ikr

smh

he

asked

   r

yo

. . .

finding the best sequence y : algorithm

input: x, w,   local (  ,  ,  ,  )
(cid:73)      , v [1,   ] = 0.
(cid:73) for (cid:96)     {2, . . . , l}:
     , v [(cid:96),   ] = max

  (cid:48) v [(cid:96)     1,   (cid:48)] + w(cid:62)  local (x, (cid:96),   (cid:48),   )

store the    argmax      (cid:48) as b[(cid:96),   ].

(cid:73) y [l] = argmax   v [l,   ].
(cid:73) backtrack. for (cid:96)     {l     1, . . . , 1}:

y [(cid:96)] = b[(cid:96) + 1, y [(cid:96) + 1]]

(cid:73) return (cid:104)y [1], . . . , y [l](cid:105).

visualizing and analyzing viterbi

n
o
   
v
a
!
...

ikr

smh

he

asked

   r

yo

. . .

sequence labeling: what   s next?

1. what is sequence labeling useful for?

2. what are the features   ?
3. how we learn the parameters w?

part-of-speech tagging

ikr
!

smh
g

he
o

asked

v

   r
p

interjection

acronym

pronoun

verb

prep.

yo
d

det.

last
a

name

n

adj.

noun

so
p

he
o

can
v

add
v

on
u
o p

fb
   

lololol

!

preposition

proper noun

supersense tagging

ikr
   

smh

   

he
   

asked

communication

   r
   

yo
   

last
   

name

cognition

so
   

he
   

can
   

add
stative

u
   

on
   

fb

lololol

group

   

see:    coarse lexical semantic annotation with supersenses: an
arabic case study,    schneider et al. (2012).

id39

with commander chris ferguson at the helm ,

person

atlantis touched down at kennedy space center .
spacecraft

location

id39

with commander chris ferguson at the helm ,

person

o

b

i

i

o o o o

atlantis touched down at kennedy space center .
spacecraft

location

b

o

o o

b

i

i

o

id39: another example

1

2

3

4

5

6

7

8

9

10

x = britain sent warships across the english channel monday to rescue
y = b
y(cid:48) = o

o o
o o

o o
o o

o
o

o
o

b
b

b
b

i
i

12

14

11

13

19
britons stranded by eyjafjallaj  okull    s volcanic ash cloud .
o o o
o o o

o
o

o
o

o
o

o
o

b
b

b
b

16

17

18

15

id39: features

  
bias:
count of i s.t. y [i] = b
count of i s.t. y [i] = i
count of i s.t. y [i] = o
lexical:
count of i s.t. x[i] = britain and y [i] = b
count of i s.t. x[i] = britain and y [i] = i
count of i s.t. x[i] = britain and y [i] = o
downcased:
count of i s.t. lc(x[i]) = britain and y [i] = b
count of i s.t. lc(x[i]) = britain and y [i] = i
count of i s.t. lc(x[i]) = britain and y [i] = o
count of i s.t. lc(x[i]) = sent and y [i] = o
count of i s.t. lc(x[i]) = warships and y [i] = o

  (x, y)   (x, y(cid:48))

5
1
14

1
0
0

1
0
0
1
1

4
1
15

0
0
1

0
0
1
1
1

id39: features

  
shape:
count of i s.t. shape(x[i]) = aaaaaaa and y [i] = b
count of i s.t. shape(x[i]) = aaaaaaa and y [i] = i
count of i s.t. shape(x[i]) = aaaaaaa and y [i] = o
pre   x:
count of i s.t. pre1(x[i]) = b and y [i] = b
count of i s.t. pre1(x[i]) = b and y [i] = i
count of i s.t. pre1(x[i]) = b and y [i] = o
count of i s.t. pre1(x[i]) = s and y [i] = o
count of i s.t. shape(pre1(x[i])) = a and y [i] = b
count of i s.t. shape(pre1(x[i])) = a and y [i] = i
count of i s.t. shape(pre1(x[i])) = a and y [i] = o
i{shape(pre1(x[1])) = a     y1 = b}
i{shape(pre1(x[1])) = a     y [1] = o}
gazetteer:
count of i s.t. x[i] is in the gazetteer and y [i] = b
count of i s.t. x[i] is in the gazetteer and y [i] = i
count of i s.t. x[i] is in the gazetteer and y [i] = o
count of i s.t. x[i] = sent and y [i] = o

  (x, y)   (x, y(cid:48))

3
1
0

2
0
0
2
5
1
0
1
0

2
0
0
1

2
1
1

1
0
1
2
4
1
1
0
1

1
0
1
1

multiword expressions

he was willing to budge a little on

the price which means a lot to me .

see:    discriminative lexical semantic segmentation with gaps:
running the mwe gamut,    schneider et al. (2014).

multiword expressions

he was willing to budge a little on

o o

o o o b i o

the price which means a lot to me .

o o

o

b i

i

i

i o

a little; means a lot to me

see:    discriminative lexical semantic segmentation with gaps:
running the mwe gamut,    schneider et al. (2014).

multiword expressions

he was willing to budge a little on

o o

o o b b

i

i

the price which means a lot to me .

o o

o

b i

i

i

i o

a little; means a lot to me; budge . . . on

see:    discriminative lexical semantic segmentation with gaps:
running the mwe gamut,    schneider et al. (2014).

cross-lingual word alignment

dyer et al. (2013): a single    diagonal-ness    feature leads gains in
translation (id7 score).

chinese     english
french     english
arabic     english

model 4
34.1
27.4
54.5

fast align
34.7
27.7
55.7

speedup
13  
10  
10  

mr president , noah's ark was    lled not with production factors , but with living creatures .noahs arche war nicht voller produktionsfaktoren , sondern gesch  pfe .other sequence decoding problems

(cid:73) word id68
(cid:73) id103
(cid:73) music transcription
(cid:73) gene identi   cation

add dimensions:

(cid:73) image segmentation
(cid:73) object recognition
(cid:73) id42

sequence decoding: l

recall that for categorization, we set up learning as empirical risk
minimization:

  w = argmin
w:   (w)     

1
n

loss(xn, yn; w)

n(cid:88)

n=1

example loss:

loss(x, y ; w) =    w(cid:62)  (x, y ) + max
y(cid:48)   yx

w(cid:62)  (x, y(cid:48))

structured id88 (collins, 2002)

input: x, y, t , step size sequence (cid:104)  1, . . . ,   t(cid:105)

(cid:73) w = 0
(cid:73) for t     {1, . . . , t}:

(cid:73) draw n uniformly at random from {1, . . . , n}.
(cid:73) decode xn:

  y = argmax
y   yxn
(cid:73) if   y (cid:54)= yn, update parameters:

w(cid:62)  (xn, y )

w = w +   t (  (xn, yn)       (xn,   y ))

(cid:73) return w

variations on the structured id88

change loss:

(cid:73) conditional random    elds: use    softmax    instead of max in

loss; generalizes id28

(cid:73) max-margin markov networks: use cost-augmented max in

loss; generalizes support vector machine

incorporate id173    (w), as previously discussed.

change the optimization algorithm:

(cid:73) automatic step-size scaling (e.g., mira, adagrad)
(cid:73) batch and    mini-batch    updating
(cid:73) averaging and voting

id170: lines of attack

1. transform into a sequence of classi   cation problems.

2. transform into a sequence labeling problem and use a variant

of the viterbi algorithm.

3. design a representation, prediction algorithm, and learning

algorithm for your particular problem.

beyond sequences

(cid:73) can all linguistic structure be captured with sequence

labeling?

(cid:73) some representations are more elegantly handled using other

kinds of output structures.

(cid:73) syntax: trees
(cid:73) semantics: graphs

(cid:73) id145 and other combinatorial algorithms are

central.

(cid:73) always useful: features    that decompose into local parts

dependency tree

see:    a dependency parser for tweets,    kong et al. (2014)

i     the biebs & want to have his babies !    > la times : teen pop star heartthrob is all the rage on social media omg    #belieberrootcoordsemantic graph

the boy wants to visit new york city.

see:    a discriminative graph-based parser for the abstract
meaning representation,    flanigan et al. (2014)

wantboyvisitcitynamenewyorkcitynameagentagentthemethemepart iii

example applications

machine translation

translation from analytic to synthetic languages

how to generate well-formed words in a morphologically rich target
language?

useful tool: morphological lexicon

   translating into morphologically rich languages with synthetic
phrases,    chahuneau et al. (2013)

y   =                 y   = {verb, main, ind,past, sing, fem,medial, perf}                deterministichigh-level approach

contemporary translation is performed by mapping
source-language    phrases    to target-language    phrases.   

a phrase is a sequence of one or more words.

in addition, let a phrase be a sequence of one or more stems.

our approach automatically in   ects stems in context, and lets
these synthetic phrases compete with traditional ones.

predicting in   ection in multilingual context

  (x, y  ) =(cid:10)  source(x)       target(y  ),   target(y  )       target(y  )(cid:11)

y   =                 y   = {verb, main, ind,past, sing, fem,medial, perf}                deterministic                                                                              she had attempted to cross the road on her bikeprp   vbd         vbn          to    vb       dt     nn    in  prp$   nnnsubjauxxcomp  :                _v,+,  :mis2sfm2ec50   c473        c28          c8    c275   c37   c43  c82 c94   c331root-1+1                                                                              she had attempted to cross the road on her bikeprp   vbd         vbn          to    vb       dt     nn    in  prp$   nnnsubjauxxcomp  :                _v,+,  :mis2sfm2ec50   c473        c28          c8    c275   c37   c43  c82 c94   c331root-1+1translation results (out of english)

    russian     hebrew     swahili
14.7  0.1
18.3  0.1
baseline
18.7  0.2
+class lm 15.7  0.1
16.2  0.1
19.0  0.1
+synthetic

15.8  0.3
16.8  0.4
17.6  0.1

translation quality (id7 score; higher is better), averaged across
three runs.

something completely di   erent

measuring ideological proportions

   well, i think you hit a reset button for the fall campaign.
everything changes. it   s almost like an etch-a-sketch. you can
kind of shake it up and restart all over again.   

   eric fehrnstrom, mitt romney   s spokesman, 2012

measuring ideological proportions

   well, i think you hit a reset button for the fall campaign.
everything changes. it   s almost like an etch-a-sketch. you can
kind of shake it up and restart all over again.   

   eric fehrnstrom, mitt romney   s spokesman, 2012

measuring ideological proportions: motivation

(cid:73) hypothesis: primary candidates    move to the center    before a

general election.

(cid:73) in primary elections, voters tend to be ideologically

concentrated.

(cid:73) in general elections, voters are now more widely dispersed

across the ideological spectrum.

(cid:73) do obama, mccain, and romney use more    extreme   

ideological rhetoric in the primaries than the general election?

can we measure candidates    ideological positions from the text of
their speeches at di   erent times?

see:    measuring ideological proportions in political speeches,    sim
et al. (2013).

operationalizing    ideology   

leftrightcenterprogressivereligious leftfar leftreligious rightcenter leftfar rightcenter rightlibertarianpopulistcue-lag representation of a speech

instead of putting more limits on your earnings and your options, we need
to place clear and    rm limits on government spending. as a start, i will
lower federal spending to 20 percent of gdp within four years    time    
down from the 24.3 percent today.
the president   s plan assumes an endless expansion of government, with
costs rising and rising with the spread of obamacare.
i will halt the ex-
pansion of government, and repeal obamacare.
working together, we can save social security without making any changes
in the system for people in or nearing retirement. we have two basic
options for future retirees: a tax increase for high-income retirees, or a
decrease in the bene   t growth rate for high-income retirees.
i favor the
second option; it protects everyone in the system and it avoids higher taxes
that will drag down the economy
i have proposed a medicare plan that improves the program, keeps it sol-
vent, and slows the rate of growth in health care costs.
   excerpt from speech by romney on 5/25/12 in des moines, ia

cue-lag representation of a speech

instead of putting more limits on your earnings and your options, we need
to place clear and    rm limits on government spending. as a start, i will
lower federal spending to 20 percent of gdp within four years    time    
down from the 24.3 percent today.
the president   s plan assumes an endless expansion of government, with
costs rising and rising with the spread of obamacare.
i will halt the ex-
pansion of government, and repeal obamacare.
working together, we can save social security without making any changes
in the system for people in or nearing retirement. we have two basic
options for future retirees: a tax increase for high-income retirees, or a
decrease in the bene   t growth rate for high-income retirees.
i favor the
second option; it protects everyone in the system and it avoids higher taxes
that will drag down the economy.
i have proposed a medicare plan that improves the program, keeps it sol-
vent, and slows the rate of growth in health care costs.
   excerpt from speech by romney on 5/25/12 in des moines, ia

cue-lag representation of a speech

government spending 8 federal spending 47 repeal obamacare 7

social security 24 tax increase 13 growth rate 21 higher taxes 29

health care costs

line of attack

1. build a    dictionary    of cues.

2. infer ideological proportions from the cue-lag representation of

speeches.

ideological books corpus

ideological books corpus

example cues

center-right d.
frum, m. mccain,
c. t. whitman
(1,450)

libertarian rand
paul, john stossel,
reason (2,268)

religious right
(960)

governor bush; class voter; health care; republican president;
george bush; state police; move forward; miss america; mid-
dle eastern; water bu   alo; fellow citizens; sam   s club; amer-
ican life; working class; general election; culture war; status
quo; human dignity; same-sex marriage
medical marijuana; raw milk; rand paul; economic freedom;
health care; government intervention; market economies;
commerce clause; military spending; government agency;
due process; drug war; minimum wage; federal
law; ron
paul; private property
daily saint; holy spirit; matthew [c/v]; john [c/v]; jim wallis;
modern liberals; individual liberty; god   s word; jesus christ;
elementary school; natural law; limited government; emerg-
ing church; private property; planned parenthood; christian
nation; christian faith

browse results at http://www.ark.cs.cmu.edu/clip/.

cue-lag ideological proportions model

libertarian (r)

libertarian (r)

right

progressive (l)

government

spending

federal
spending

repeal

obamacare

social
security

(cid:73) each speech is modeled as a sequence:

(cid:73) ideologies are labels (y )
(cid:73) cue terms are observed (x)

id48    with a twist   

right

repeal

obamacare

progressive (l)

social
security

id48    with a twist   

right

repeal

obamacare

progressive (l)

social
security

w(cid:62)  local (x, (cid:96), right, prog.) = log p(right (cid:32) prog.) + . . .

backgroundleftrightcenterprogressivereligious leftfar leftreligious rightmainstreamfar rightnon radicallibertarianpopulistid48    with a twist   

right

repeal

obamacare

progressive (l)

lag=7

social
security

also considers id203 of restarting the walk through a
   noisy-or    model.

learning and id136

we do not have labeled examples (cid:104)x, y(cid:105) to learn from!

instead, labels are    hidden.   

we sample from the posterior over labels, p(y | x).

this is sometimes called approximate bayesian id136.

measuring ideological proportions in speeches

(cid:73) campaign speeches from 21 candidates, separated into

primary and general elections in 2008 and 2012.

(cid:73) run model on each candidate separately with

(cid:73) independent transition parameters for each epoch, but
(cid:73) shared emission parameters for a candidate.

mitt romney

primaries 2012general 2012religious (l)centercenter-rightlibertarian (r)religious (r)far leftprogressive (l)leftcenter-leftrightpopulist (r)far rightmitt romney

primaries 2012general 2012religious (l)centercenter-rightlibertarian (r)religious (r)far leftprogressive (l)leftcenter-leftrightpopulist (r)far rightbarack obama

primaries 2008general 2008far leftreligious (l)leftcenter-leftcenter-rightlibertarian (r)populist (r)religious (r)progressive (l)centerrightfar rightbarack obama

primaries 2008general 2008far leftreligious (l)leftcenter-leftcenter-rightlibertarian (r)populist (r)religious (r)progressive (l)centerrightfar rightjohn mccain

primaries 2008general 2008far leftreligious (l)center-leftcenter-rightlibertarian (r)religious (r)progressive (l)leftcenterrightpopulist (r)far rightjohn mccain

primaries 2008general 2008far leftreligious (l)center-leftcenter-rightlibertarian (r)religious (r)progressive (l)leftcenterrightpopulist (r)far rightobjective evaluation?

pre-registered hypothesis
a statement by a domain expert about his/her expectations of the
model   s output.

preregistered hypotheses

hypotheses
sanity checks (strong):
s1. republican primary candidates should tend to draw more from right

than from left.

s2. democratic primary candidates should tend to draw more from left

s3.

than from right.
in general elections, democrats should draw more from the left than
the republicans and vice versa for the right.

primary hypotheses (strong):
p1. romney, mccain and other republicans should almost never draw from

far left, and extremely rarely from progressive.

p2. romney should draw more heavily from the right than obama in both

stages of the 2012 campaign.

primary hypotheses (moderate):
p3. romney should draw more heavily on words from the libertarian,
populist, religious right, and far right in the primary compared
to the general election.
in the general election, romney should draw
more heavily on center, center-right and left vocabularies.

baselines

compare against    simpli   ed    versions of the model:

(cid:73) id48: traditional id48 without ideological tree structure

(cid:73) nores: weaker assumptions (never restart)

(cid:73) mix: stronger assumptions (always restart)

results

sanity checks
strong hypotheses
moderate hypotheses
total

clip id48 mix nores
20/21
17/22
31/34
30/34
14/17
11/17
65/72
58/73

19/22
23/33
14/17
56/72

21/22
28/34
12/17
61/73

summary

i introduction to nlp
ii algorithms for nlp

(cid:73) categorizing texts

(cid:73) sparsity and group sparsity

(cid:73) decoding sequences

(cid:73) viterbi
(cid:73) structured id88

(cid:73) many examples of tasks

iii example applications

(cid:73) a translation problem
(cid:73) a political science problem

some current research directions in nlp

(cid:73) representations for semantics

(cid:73) distributed
(cid:73) denotational
(cid:73) non-propositional
(cid:73) hybrids of all of the above
(cid:73) broad-coverage as well as domain-speci   c

(cid:73) alternatives to annotating data:

(cid:73) constraints and bias
(cid:73) id173 and priors
(cid:73) semisupervised learning
(cid:73) feature/representation learning     unsupervised discovery

(cid:73) multilinguality
(cid:73) approximate id136 algorithms for learning and decoding

thank you!

references i

blitzer, j., dredze, m., and pereira, f. (2007). biographies, bollywood, boom-boxes and blenders: domain

adaptation for sentiment classi   cation. in proceedings of the annual meeting of the association for
computational linguistics.

chahuneau, v., schlinger, e., dyer, c., and smith, n. a. (2013). translating into morphologically rich languages

with synthetic phrases. in proceedings of the conference on empirical methods in natural language
processing.

collins, m. (2002). discriminative training methods for id48: theory and experiments with

id88 algorithms. in proceedings of the conference on empirical methods in natural language
processing.

dyer, c., chahuneau, v., and smith, n. a. (2013). a simple, fast, and e   ective reparameterization of ibm model

2. in proceedings of the conference of the north american chapter of the association for computational
linguistics.

flanigan, j., thomson, s., carbonell, j., dyer, c., and smith, n. a. (2014). a discriminative graph-based parser

for the id15. in proceedings of the annual meeting of the association for
computational linguistics.

gimpel, k., schneider, n., o   connor, b., das, d., mills, d., eisenstein, j., heilman, m., yogatama, d., flanigan,

j., and smith, n. a. (2011). part-of-speech tagging for twitter: annotation, features, and experiments. in
proceedings of the annual meeting of the association for computational linguistics, companion volume.

kong, l., schneider, n., swayamdipta, s., bhatia, a., dyer, c., and smith, n. a. (2014). a dependency parser for

tweets. in proceedings of the conference on empirical methods in natural language processing.

martins, a. f. t., yogatama, d., smith, n. a., and figueiredo, m. a. t. (2014). structured sparsity in natural

language processing: models, algorithms, and applications. eacl tutorial available at
http://www.cs.cmu.edu/~afm/home_files/eacl2014tutorial.pdf.

mosteller, f. and wallace, d. l. (1963). id136 in an authorship problem: a comparative study of discrimination

methods applied to the authorship of the disputed federalist papers. journal of the american statistical
association, 58(302):275   309.

owoputi, o., o   connor, b., dyer, c., gimpel, k., schneider, n., and smith, n. a. (2013). improved part-of-speech

tagging for online conversational text with word clusters. in proceedings of the conference of the north
american chapter of the association for computational linguistics.

references ii

schneider, n., danchik, e., dyer, c., and smith, n. a. (2014). discriminative lexical semantic segmentation with

gaps: running the mwe gamut. transactions of the association for computational linguistics, 2:193   206.

schneider, n., mohit, b., o   azer, k., and smith, n. a. (2012). coarse lexical semantic annotation with

supersenses: an arabic case study. in proceedings of the annual meeting of the association for computational
linguistics.

sim, y., acree, b. d. l., gross, j. h., and smith, n. a. (2013). measuring ideological proportions in political

speeches. in proceedings of the conference on empirical methods in natural language processing, seattle,
wa.

tibshirani, r. (1996). regression shrinkage and selection via the lasso. journal of the royal statistical society,

series b, 58(1):267   288.

yano, t., smith, n. a., and wilkerson, j. d. (2012). textual predictors of bill survival in congressional
committees. in proceedings of the conference of the north american chapter of the association for
computational linguistics.

yogatama, d. and smith, n. a. (2014). making the most of bag of words: sentence id173 with alternating

direction method of multipliers. in proceedings of the international conference on machine learning.

yuan, m. and lin, y. (2006). model selection and estimation in regression with grouped variables. journal of the

royal statistical society (b), 68(1):49.

