cmu scs

large graph mining: power tools 

and a practitioner   s guide

task 1: node importance

faloutsos, miller, tsourakakis

cmu

cmu scs

outline

introduction     motivation
   
    task 1: node importance
    task 2: community detection
    task 3: recommendations
    task 4: connection sub-graphs
    task 5: mining graphs over time
    task 6: virus/influence propagation
    task 7: spectral id207
    task 8: tera/peta graph mining: hadoop
    observations     patterns of real graphs
    conclusions

kdd'09

faloutsos, miller, tsourakakis

p1-2

cmu scs

node importance - motivation:

    given a graph (eg., web pages containing 

the desirable query word)

    q: which node is the most important?

kdd'09

faloutsos, miller, tsourakakis

p1-3

cmu scs

node importance - motivation:

    given a graph (eg., web pages containing 

the desirable query word)

    q: which node is the most important?

    a1: hits (svd = singular value 

decomposition)

    a2: eigenvector (id95)

kdd'09

faloutsos, miller, tsourakakis

p1-4

cmu scs

node importance - motivation

    svd and eigenvector analysis: very closely 

related

    see    theory task   , later

kdd'09

faloutsos, miller, tsourakakis

p1-5

cmu scs

svd - detailed outline

    motivation
    definition - properties
    interpretation
    complexity
    case studies

kdd'09

faloutsos, miller, tsourakakis

p1-6

cmu scs

svd - motivation

    problem #1: text - lsi: find    concepts   
    problem #2: compression / dim. reduction

kdd'09

faloutsos, miller, tsourakakis

p1-7

cmu scs

svd - motivation

    problem #1: text - lsi: find    concepts   

kdd'09

faloutsos, miller, tsourakakis

p1-8

cmu scs

svd - motivation

    customer-product, for recommendation 

system:

vegetarians

meat eaters

to m atos
chicken
lettuce
bread
beef

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

kdd'09

faloutsos, miller, tsourakakis

p1-9

cmu scs

svd - motivation

    problem #2: compress / reduce 

dimensionality

kdd'09

faloutsos, miller, tsourakakis

p1-10

cmu scs

problem - specs

    ~10**6 rows; ~10**3 columns; no updates;

    random access to any cell(s) ; small error: ok

kdd'09

faloutsos, miller, tsourakakis

p1-11

cmu scs

svd - motivation

kdd'09

faloutsos, miller, tsourakakis

p1-12

cmu scs

svd - motivation

kdd'09

faloutsos, miller, tsourakakis

p1-13

cmu scs

svd - detailed outline

    motivation
    definition - properties
    interpretation
    complexity
    case studies
    additional properties

kdd'09

faloutsos, miller, tsourakakis

p1-14

cmu scs

svd - definition

(reminder: id127

1 2

3 4
5 6

x

 1

-1

=

3 x 2

2 x 1

kdd'09

faloutsos, miller, tsourakakis

p1-15

cmu scs

svd - definition

(reminder: id127

1 2

3 4
5 6

x

 1

-1

=

3 x 2

2 x 1

3 x 1

kdd'09

faloutsos, miller, tsourakakis

p1-16

cmu scs

svd - definition

(reminder: id127

1 2

3 4
5 6

x

 1

-1

-1

=

3 x 2

2 x 1

3 x 1

kdd'09

faloutsos, miller, tsourakakis

p1-17

cmu scs

svd - definition

(reminder: id127

1 2

3 4
5 6

x

 1

-1

-1

-1

=

3 x 2

2 x 1

3 x 1

kdd'09

faloutsos, miller, tsourakakis

p1-18

cmu scs

svd - definition

(reminder: id127

1 2

3 4
5 6

x

 1

-1

=

-1

-1
-1

kdd'09

faloutsos, miller, tsourakakis

p1-19

cmu scs

svd - definition

a[n x m] = u[n x r]             [ [ [ [ r x r] (v[m x r])t
    a: n x m matrix (eg., n documents, m 

terms)

    u: n x r matrix (n documents, r concepts)
            : r x r diagonal matrix (strength of each 

   concept   ) (r : rank of the matrix)

    v: m x r matrix (m terms, r concepts)

kdd'09

faloutsos, miller, tsourakakis

p1-20

cmu scs

svd - definition

    a = u          vt - example:

kdd'09

faloutsos, miller, tsourakakis

p1-21

cmu scs

svd - properties

theorem [press+92]: always possible to 

decompose matrix a into a = u          vt , where

    u,   ,  ,  ,  , v: unique (*)

    u, v: column orthonormal (ie., columns are unit 

vectors, orthogonal to each other)
    ut u = i; vt v = i (i: identity matrix)

            : singular are positive, and sorted in decreasing 

order

kdd'09

faloutsos, miller, tsourakakis

p1-22

cmu scs

svd - example

    a = u          vt - example:

retrieval

inf.

data

lung

brain

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

cs

md

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-23

cmu scs

svd - example

    a = u          vt - example:
cs-concept

retrieval

lung

md-concept

inf.

data

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

cs

md

brain

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-24

cmu scs

svd - example

    a = u          vt - example:
cs-concept

retrieval

doc-to-concept 
similarity matrix

lung

md-concept

inf.

data

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

cs

md

brain

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-25

cmu scs

svd - example

    a = u          vt - example:

retrieval

inf.

data

lung

brain

   strength    of cs-concept

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

cs

md

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-26

cmu scs

svd - example

    a = u          vt - example:

retrieval

inf.

data

lung

brain

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

cs

md

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

term-to-concept
similarity matrix

cs-concept

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-27

cmu scs

svd - example

    a = u          vt - example:

retrieval

inf.

data

lung

brain

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

cs

md

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

term-to-concept
similarity matrix

cs-concept

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-28

cmu scs

svd - detailed outline

    motivation
    definition - properties
    interpretation
    complexity
    case studies
    additional properties

kdd'09

faloutsos, miller, tsourakakis

p1-29

cmu scs

svd - interpretation #1

   documents   ,    terms    and    concepts   :
    u: document-to-concept similarity matrix
    v: term-to-concept sim. matrix
            : its diagonal elements:    strength    of each 

concept

kdd'09

faloutsos, miller, tsourakakis

p1-30

cmu scs

svd     interpretation #1

   documents   ,    terms    and    concepts   :
q: if a is the document-to-term matrix, what 

is at a?

a:
q: a at ?
a:

kdd'09

faloutsos, miller, tsourakakis

p1-31

cmu scs

svd     interpretation #1

   documents   ,    terms    and    concepts   :
q: if a is the document-to-term matrix, what 

is at a?

a: term-to-term ([m x m]) similarity matrix
q: a at ?
a: document-to-document ([n x n]) similarity 

matrix

kdd'09
icde   09

copyright: faloutsos, tong (2009)

faloutsos, miller, tsourakakis

p1-32
2-32

cmu scs

svd properties

    v are the eigenvectors of the covariance 

matrix ata

    u are the eigenvectors of the gram (inner-

product) matrix aat

further reading:
1. ian t. jolliffe, principal component analysis (2nd ed), springer, 2002.
p1-33
2-33
2. gilbert strang, id202 and its applications (4th ed), brooks cole, 2005.

copyright: faloutsos, tong (2009)

faloutsos, miller, tsourakakis

kdd'09

cmu scs

svd - interpretation #2

    best axis to project on: (   best    = min sum of 

squares of projection errors)

kdd'09

faloutsos, miller, tsourakakis

p1-34

cmu scs

svd - motivation

kdd'09

faloutsos, miller, tsourakakis

p1-35

cmu scs

svd - interpretation #2

svd: gives
best axis to project

v1

    minimum rms error

first singular

vector

kdd'09

faloutsos, miller, tsourakakis

p1-36

cmu scs

svd - interpretation #2

kdd'09

faloutsos, miller, tsourakakis

p1-37

cmu scs

svd - interpretation #2

    a = u          vt - example:

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

v1

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-38

cmu scs

svd - interpretation #2

    a = u          vt - example:

variance (   spread   ) on the v1 axis

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-39

cmu scs

svd - interpretation #2

    a = u          vt - example:

    u          gives the coordinates of the points in the 

projection axis

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-40

cmu scs

svd - interpretation #2

    more details
    q: how exactly is dim. reduction done?

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-41

cmu scs

svd - interpretation #2

    more details
    q: how exactly is dim. reduction done?
    a: set the smallest singular values to zero:

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-42

cmu scs

svd - interpretation #2

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

~

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

0

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-43

cmu scs

svd - interpretation #2

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

~

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

0

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-44

cmu scs

svd - interpretation #2

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

~

0.18

0.36

0.18

0.90

0

0
0

9.64

x

x

0.58 0.58 0.58 0

0

kdd'09

faloutsos, miller, tsourakakis

p1-45

cmu scs

svd - interpretation #2

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

~

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

0

0
0

0

0

0

0

0

0
0

kdd'09

faloutsos, miller, tsourakakis

p1-46

cmu scs

svd - interpretation #2

exactly equivalent:
   spectral decomposition    of the matrix:

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-47

cmu scs

svd - interpretation #2

exactly equivalent:
   spectral decomposition    of the matrix:

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

u1

u2

  1

x

  2

x

v1
v2

kdd'09

faloutsos, miller, tsourakakis

p1-48

cmu scs

svd - interpretation #2

exactly equivalent:
   spectral decomposition    of the matrix:

m
1

2

1

5

0

0
0

1 1

2 2

1 1

5 5

0 0

0 0
0 0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

n

=

  1

u1

vt

1

+

  2

u2

vt

2

+...

kdd'09

faloutsos, miller, tsourakakis

p1-49

cmu scs

svd - interpretation #2

exactly equivalent:
   spectral decomposition    of the matrix:

m
1

2

1

5

0

0
0

1 1

2 2

1 1

5 5

0 0

0 0
0 0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

n

r terms

=

  1

u1

vt

1

+

  2

u2

vt

2

+...

n x 1

1 x m

kdd'09

faloutsos, miller, tsourakakis

p1-50

cmu scs

svd - interpretation #2

approximation / dim. reduction:
by keeping the first few terms (q: how many?)

m
1

2

1

5

0

0
0

1 1

2 2

1 1

5 5

0 0

0 0
0 0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

n

=

  1

u1

vt

1

+

  2

u2

vt

2

+...

assume:   1 >=   2 >= ...

kdd'09

faloutsos, miller, tsourakakis

p1-51

cmu scs

svd - interpretation #2

a (heuristic - [fukunaga]): keep 80-90% of 

   energy    (= sum of squares of   i    s)

m
1

2

1

5

0

0
0

1 1

2 2

1 1

5 5

0 0

0 0
0 0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

n

=

  1

u1

vt

1

+

  2

u2

vt

2

+...

assume:   1 >=   2 >= ...

kdd'09

faloutsos, miller, tsourakakis

p1-52

cmu scs

svd - detailed outline

    motivation
    definition - properties
    interpretation

    #1: documents/terms/concepts
    #2: dim. reduction
    #3: picking non-zero, rectangular    blobs   

    complexity
    case studies
    additional properties

kdd'09

faloutsos, miller, tsourakakis

p1-53

cmu scs

svd - interpretation #3

    finds non-zero    blobs    in  a data matrix

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-54

cmu scs

svd - interpretation #3

    finds non-zero    blobs    in  a data matrix

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

=

0.18 0

0.36 0

0.18 0

0.90 0

0

0
0

0.53

0.80
0.27

x

9.64 0

0

5.29

x

0.58 0.58 0.58 0

0

0

0

0

0.71 0.71

kdd'09

faloutsos, miller, tsourakakis

p1-55

cmu scs

svd - interpretation #3

    finds non-zero    blobs    in  a data matrix =
       communities    (bi-partite cores, here)

1 1

2 2

1 1

5 5

0 0

0 0
0 0

1

2

1

5

0

0
0

0

0

0

0

2

3
1

0

0

0

0

2

3
1

row 1

row 4

row 5

row 7

col 1

col 3

col 4

kdd'09

faloutsos, miller, tsourakakis

p1-56

cmu scs

svd - detailed outline

    motivation
    definition - properties
    interpretation
    complexity
    case studies
    additional properties

kdd'09

faloutsos, miller, tsourakakis

p1-57

cmu scs

svd - complexity

    o( n * m * m) or o( n * n * m) (whichever 

is less)

or if we want first k singular vectors
or if the matrix is sparse [berry]

    less work, if we just want singular values
   
   
    implemented: in any id202 package 
(linpack, matlab, splus, mathematica ...)

kdd'09

faloutsos, miller, tsourakakis

p1-58

cmu scs

svd - conclusions so far

    svd: a= u          vt : unique (*)
   
   
   
    dim. reduction: keep the first few strongest 

u: document-to-concept similarities
v: term-to-concept similarities
        : strength of each concept

singular values (80-90% of    energy   )
    svd: picks up linear correlations
    svd: picks up non-zero    blobs   

kdd'09

faloutsos, miller, tsourakakis

p1-59

cmu scs

svd - detailed outline

    motivation

    definition - properties

    interpretation

    complexity

    svd properties

    case studies

    conclusions

kdd'09

faloutsos, miller, tsourakakis

p1-60

cmu scs

svd - other properties - summary

    can produce orthogonal basis (obvious) 

(who cares?)

    can solve over- and under-determined linear 

problems (see c(1) property)

    can compute    fixed points    (=    steady state 

prob. in markov chains   ) (see c(4) 
property)

kdd'09

faloutsos, miller, tsourakakis

p1-61

cmu scs

svd -outline of properties

    (a): obvious
    (b): less obvious
    (c): least obvious (and most powerful!)

kdd'09

faloutsos, miller, tsourakakis

p1-62

cmu scs

properties - by defn.:

a(0): a[n x m] = u [ n x r ]          [ r x r ] vt

[ r x m] 

a(1): ut
a(2): vt
a(3):         k = diag(   1

[r x n] u [n x r ] = i [r x r ] (identity matrix)
[r x n] v [n x r ] = i [r x r ] 
k, ...   r

k ) (k: any real 

k,   2

number)

a(4): at = v          ut

kdd'09

faloutsos, miller, tsourakakis

p1-63

cmu scs

less obvious properties

a(0): a[n x m] = u [ n x r ]          [ r x r ] vt

[ r x m] 

b(1): a [n x m] (at) [m x n] = ??

kdd'09

faloutsos, miller, tsourakakis

p1-64

cmu scs

less obvious properties

a(0): a[n x m] = u [ n x r ]          [ r x r ] vt
b(1): a [n x m] (at) [m x n] = u         2 ut

[ r x m] 

symmetric; intuition?

kdd'09

faloutsos, miller, tsourakakis

p1-65

cmu scs

less obvious properties

a(0): a[n x m] = u [ n x r ]          [ r x r ] vt
b(1): a [n x m] (at) [m x n] = u         2 ut

[ r x m] 

symmetric; intuition?
   document-to-document    similarity matrix

b(2): symmetrically, for    v   

(at) [m x n] a [n x m] = v l2 vt

intuition?

kdd'09

faloutsos, miller, tsourakakis

p1-66

cmu scs

less obvious properties

a: term-to-term similarity matrix

b(3): ( (at) [m x n] a [n x m] ) k= v         2k vt
and
b(4): (at a ) k  ~ v1   1

t for k>>1

2k v1

where
v1: [m x 1] first column (singular-vector) of v
  1: strongest singular value

kdd'09

faloutsos, miller, tsourakakis

p1-67

cmu scs

less obvious properties

b(4): (at a ) k  ~ v1   1
b(5): (at a ) k  v    ~ (constant) v1
ie., for (almost) any v   , it converges to a 

t for k>>1

2k v1

vector parallel to v1

thus, useful to compute first singular 

vector/value (as well as the next ones, too...)

kdd'09

faloutsos, miller, tsourakakis

p1-68

cmu scs

less obvious properties -

repeated:

a(0): a[n x m] = u [ n x r ]          [ r x r ] vt

[ r x m] 

b(1): a [n x m] (at) [m x n] = u         2 ut
b(2): (at) [m x n] a [n x m] = v         2 vt
b(3): ( (at) [m x n] a [n x m] ) k= v         2k vt
b(4): (at a ) k  ~ v1   1
b(5): (at a ) k  v    ~ (constant) v1

t
2k v1

kdd'09

faloutsos, miller, tsourakakis

p1-69

cmu scs

least obvious properties - cont   d

a(0): a[n x m] = u [ n x r ]          [ r x r ] vt

[ r x m] 

c(2): a [n x m] v1 [m x 1] =         1 u1 [n x 1] 
where v1 , u1 the first (column) vectors of  v, u. (v1

== right-singular-vector)

c(3): symmetrically: u1
u1 == left-singular-vector
therefore:

t a =         1 v1

t

kdd'09

faloutsos, miller, tsourakakis

p1-70

cmu scs

least obvious properties - cont   d

a(0): a[n x m] = u [ n x r ]          [ r x r ] vt

[ r x m] 

c(4): at a v1 =         1

2 v1

(fixed point - the dfn of eigenvector for a 

symmetric matrix)

kdd'09

faloutsos, miller, tsourakakis

p1-71

cmu scs

least obvious properties -

altogether

a(0): a[n x m] = u [ n x r ]          [ r x r ] vt

[ r x m] 

c(1): a [n x m] x [m x 1] = b [n x 1] 

then, x0 = v         (-1) ut b: shortest, actual or least-
squares solution

c(2): a [n x m] v1 [m x 1] =         1 u1 [n x 1] 
t
t a =         1 v1
c(3): u1
c(4): at a v1 =         1

2 v1

kdd'09

faloutsos, miller, tsourakakis

p1-72

cmu scs

properties - conclusions

a(0): a[n x m] = u [ n x r ]          [ r x r ] vt

[ r x m] 

b(5): (at a ) k  v    ~ (constant) v1
c(1): a [n x m] x [m x 1] = b [n x 1] 

then, x0 = v         (-1) ut b: shortest, actual or least-
squares solution
c(4): at a v1 =         1

2 v1

kdd'09

faloutsos, miller, tsourakakis

p1-73

cmu scs

svd - detailed outline

    ...
    svd properties
    case studies

    kleinberg   s algorithm
    google   s algorithm

    conclusions

kdd'09

faloutsos, miller, tsourakakis

p1-74

cmu scs

kleinberg   s algo (hits)

kleinberg, jon (1998). 
authoritative sources in a 
hyperlinked environment. 
proc. 9th acm-siam 
symposium on discrete 
algorithms.

kdd'09

faloutsos, miller, tsourakakis

p1-75

cmu scs

recall: problem dfn

    given a graph (eg., web pages containing 

the desirable query word)

    q: which node is the most important?

kdd'09

faloutsos, miller, tsourakakis

p1-76

cmu scs

kleinberg   s algorithm

    problem dfn: given the web and a query
    find the most    authoritative    web pages for 

this query

step 0: find all pages containing the query 

terms

step 1: expand by one move forward and 

backward

kdd'09

faloutsos, miller, tsourakakis

p1-77

cmu scs

kleinberg   s algorithm

    step 1: expand by one move forward and 

backward

kdd'09

faloutsos, miller, tsourakakis

p1-78

cmu scs

kleinberg   s algorithm

    on the resulting graph, give high score (= 

   authorities   ) to nodes that many important 
nodes point to

    give high importance score (   hubs   ) to 

nodes that point to good    authorities   )

hubs

authorities

kdd'09

faloutsos, miller, tsourakakis

p1-79

cmu scs

kleinberg   s algorithm

observations

    recursive definition!

    each node (say,    i   -th node) has both an 
authoritativeness score ai and a hubness
score hi

kdd'09

faloutsos, miller, tsourakakis

p1-80

cmu scs

kleinberg   s algorithm

let e be the set of edges and a be the 

adjacency matrix: 

the (i,j) is 1 if the edge from i to j exists

let h and a be  [n x 1] vectors with the 

   hubness    and    authoritativiness    scores.

then:

kdd'09

faloutsos, miller, tsourakakis

p1-81

cmu scs

kleinberg   s algorithm

then:

ai = hk + hl + hm

k

l

m

i

that is
ai = sum (hj)     over all j that 

(j,i) edge exists

or
a = at h

kdd'09

faloutsos, miller, tsourakakis

p1-82

cmu scs

i

kleinberg   s algorithm

symmetrically, for the    hubness   :

n

p

q

hi = an + ap + aq

that is
hi = sum (qj)     over all j that 

(i,j) edge exists

or

h = a a

kdd'09

faloutsos, miller, tsourakakis

p1-83

cmu scs

kleinberg   s algorithm

in conclusion, we want vectors h and a such 

that:

h = a a
a = at h

=

recall properties:
c(2): a [n x m] v1 [m x 1] =   1 u1 [n x 1] 
c(3): u1

t
t a =   1 v1

kdd'09

faloutsos, miller, tsourakakis

p1-84

cmu scs

kleinberg   s algorithm

in short, the solutions to

h = a a
a = at h

are the left- and right- singular-vectors of the 

adjacency matrix a.

starting from random a    and iterating, we   ll 

eventually converge

(q: to which of all the singular-vectors? why?)

kdd'09

faloutsos, miller, tsourakakis

p1-85

cmu scs

kleinberg   s algorithm

(q: to which of all the singular-vectors? 

why?)

a: to the ones of the strongest singular-value, 

because of property b(5):

b(5): (at a ) k  v    ~ (constant) v1

kdd'09

faloutsos, miller, tsourakakis

p1-86

cmu scs

kleinberg   s algorithm - results

eg., for the query    java   :

0.328 www.gamelan.com

0.251 java.sun.com

0.190 www.digitalfocus.com (   the java 

developer   )

kdd'09

faloutsos, miller, tsourakakis

p1-87

cmu scs

kleinberg   s algorithm - discussion

       authority    score can be used to find    similar 

pages    (how?)

kdd'09

faloutsos, miller, tsourakakis

p1-88

cmu scs

svd - detailed outline

    ...
    complexity
    svd properties
    case studies

    kleinberg   s algorithm (hits)
    google   s algorithm

    conclusions

kdd'09

faloutsos, miller, tsourakakis

p1-89

cmu scs

id95 (google)

   brin, sergey and lawrence 
page (1998). anatomy of a 
large-scale hypertextual
web search engine. 7th intl 
world wide web conf.

larry
page

sergey

brin

kdd'09

faloutsos, miller, tsourakakis

p1-90

cmu scs

problem: id95

given a directed graph, find its most 

interesting/central node

a node is important,
if it is connected 
with important nodes
(recursive, but ok!)

kdd'09

faloutsos, miller, tsourakakis

p1-91

cmu scs

problem: id95 - solution

given a directed graph, find its most 

interesting/central node

proposed solution: random walk; spot most 
   popular    node (-> steady state prob. (ssp))

a node has high ssp,
if it is connected 
with high ssp nodes
(recursive, but ok!)

kdd'09

faloutsos, miller, tsourakakis

p1-92

cmu scs

(simplified) id95 algorithm

    let a be the adjacency matrix;

   

let b be the transition matrix: transpose, column-normalized - then

to

3

from

b

1

1

1

1/2

1/2

1/2

1/2

p1

p2

p3

p4

p5

=

p1

p2

p3

p4

p5

1

4

2

5

kdd'09

faloutsos, miller, tsourakakis

p1-93

cmu scs

(simplified) id95 algorithm

    b p = p

b                     p    =      p

1

1

1

1/2

1/2

1/2

1/2

p1

p2

p3

p4

p5

=

p1

p2

p3

p4

p5

1

4

3

2

5

kdd'09

faloutsos, miller, tsourakakis

p1-94

cmu scs

a

d

b

definitions

adjacency matrix (from-to)

degree matrix = (diag ( d1, d2,    , dn) )

transition matrix: to-from, column 
normalized
b = at d-1

kdd'09

faloutsos, miller, tsourakakis

p1-95

cmu scs

(simplified) id95 algorithm

    b p = 1 * p

    thus, p is the eigenvector that corresponds 
to the highest eigenvalue (=1, since the matrix is 
column-normalized)

    why does such a p exist? 

    p exists if b is nxn, nonnegative, irreducible 

[perron   frobenius theorem]

kdd'09

faloutsos, miller, tsourakakis

p1-96

cmu scs

(simplified) id95 algorithm

    in short: imagine a particle randomly 

moving along the edges

    compute its steady-state probabilities (ssp)

full version of algo:  with occasional random 

jumps

why? to make the matrix irreducible

kdd'09

faloutsos, miller, tsourakakis

p1-97

cmu scs

full algorithm

    with id203 1-c, fly-out to a random 

node

    then, we have

p = c b p + (1-c)/n 1 =>
p = (1-c)/n  [i - c b] -1 1

kdd'09

faloutsos, miller, tsourakakis

p1-98

cmu scs

m

then

alternative notation

modified transition matrix
m = c b + (1-c)/n  1  1t

p = m p

that is: the steady state probabilities =

id95 scores form the first eigenvector of 

the    modified transition matrix   

kdd'09

faloutsos, miller, tsourakakis

p1-99

cmu scs

parenthesis: intuition behind 

eigenvectors

kdd'09

faloutsos, miller, tsourakakis

p1-100

cmu scs

formal definition

if a is a (n x n) square matrix
(   , x) is an eigenvalue/eigenvector pair 
of a if

a x =    x

closely related to singular values:

kdd'09

faloutsos, miller, tsourakakis

p1-101

cmu scs

property #1: eigen- vs singular-values

if 

b[n x m] = u[n x r]             [ [ [ [ r x r] (v[m x r])t

then a = (btb) is symmetric and
2 vi

c(4): bt b vi =         i

ie, v1 , v2 , ...: eigenvectors of  a = (btb) 

kdd'09

faloutsos, miller, tsourakakis

p1-102

cmu scs

property #2

    if a[nxn] is a real, symmetric matrix
    then it has n real eigenvalues

(if a is not symmetric, some eigenvalues may 

be complex)

kdd'09

faloutsos, miller, tsourakakis

p1-103

cmu scs

property #3

    if a[nxn] is a real, symmetric matrix
    then it has n real eigenvalues

    and they agree with its n singular values, 

except possibly for the sign

kdd'09

faloutsos, miller, tsourakakis

p1-104

cmu scs

intuition

    a as vector transformation

x   

2
1

=

a

2
1

1
3

x

1
0

x   

x

1

1

2

kdd'09

3

faloutsos, miller, tsourakakis

p1-105

cmu scs

intuition

    by defn., eigenvectors remain parallel to 

themselves (   fixed points   )

  1

v1
0.52

3.62 *

0.85

=

a

2
1

1
3

v1
0.52

0.85

kdd'09

faloutsos, miller, tsourakakis

p1-106

cmu scs

convergence

    usually, fast:

kdd'09

faloutsos, miller, tsourakakis

p1-107

cmu scs

convergence

    usually, fast:

kdd'09

faloutsos, miller, tsourakakis

p1-108

cmu scs

convergence

    usually, fast:
    depends on ratio

  1 :   2

  2

  1

kdd'09

faloutsos, miller, tsourakakis

p1-109

cmu scs

kleinberg/google - conclusions

svd helps in graph analysis:

hub/authority scores: strongest left- and right-

singular-vectors of the adjacency matrix

random walk on a graph: steady state 

probabilities are given by the strongest 
eigenvector of the (modified) transition 
matrix

kdd'09

faloutsos, miller, tsourakakis

p1-110

cmu scs

conclusions

    svd: a valuable tool

    given a document-term matrix, it finds 

   concepts    (lsi)

    ... and can find fixed-points or steady-state 

probabilities (google/ kleinberg/ markov 
chains)

kdd'09

faloutsos, miller, tsourakakis

p1-111

cmu scs

conclusions cont   d

(we didn   t discuss/elaborate, but, svd

    ...  can reduce dimensionality (kl)

    ... and can find rules (pca; ratiorules)

    ... and can solve optimally over- and under-

constraint linear systems (least squares / 
query feedbacks)

kdd'09

faloutsos, miller, tsourakakis

p1-112

cmu scs

references

    berry, michael: http://www.cs.utk.edu/~lsi/

    brin, s. and l. page (1998). anatomy of a 

large-scale hypertextual web search 
engine. 7th intl world wide web conf.

kdd'09

faloutsos, miller, tsourakakis

p1-113

cmu scs

references

    christos faloutsos, searching multimedia 

databases by content, springer, 1996. (app. 
d)

    fukunaga, k. (1990). introduction to 

statistical pattern recognition, academic 
press.

    i.t. jolliffe principal component analysis

springer, 2002 (2nd ed.)

kdd'09

faloutsos, miller, tsourakakis

p1-114

cmu scs

references cont   d

    kleinberg, j. (1998). authoritative sources 

in a hyperlinked environment. proc. 9th 
acm-siam symposium on discrete 
algorithms.

    press, w. h., s. a. teukolsky, et al. (1992). 

numerical recipes in c, cambridge 
university press. www.nr.com

kdd'09

faloutsos, miller, tsourakakis

p1-115

