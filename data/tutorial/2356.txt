frame-id29
nathan schneider, university of edinburgh 
may 31, 2015     framenet tutorial at naacl-hlt

1

framenet + nlp = <3

    we want to develop systems that 

understand text 

    frame semantics and framenet o   er a 
linguistically & computationally satisfying 
theory/representation for semantic relations

2

frame-id29

semeval task 19 [baker, ellsworth, & erk 2007]

    given a text sentence, analyze its frame 
semantics. mark: 
    words/phrases that are lexical units 
    frame evoked by each lu 
    frame elements (role   argument pairings) 
    analysis is in terms of groups of tokens.    
no assumption that we know the syntax.

3

framenet srl, parsing:    

early work

    the original srl paper actually used framenet 
(gildea & jurafsky 2002).    
also thompson et al. 2003 (w/ frame id), 
fleischman et al. 2003, pad   & lapata 2005,    
erk & pad   2006, matsubayashi et al. 2009, 
f  rstenau & lapata 2009. 
    semeval 2007 shared task (baker et al. 2007): full-
text annotations.    
best system by johansson & nugues.

4

semafor

[das, schneider, chen, & smith 2010]

5

semafor

[das, schneider, chen, & smith 2010]

   

6

the semafor pipeline

[das, schneider, chen, & smith 2010]

preprocessing: syntactic dependency parser

target 
detection

frame 

disambiguation

argument 
detection + 

labeling

heuristic    
(whitelist)

statistical 

(supervised, feature-based) 
trained on full-text annotations

7

full-text annotations

https://framenet.icsi.berkeley.edu/fndrupal/index.php?q=fulltextindex

8

full-text annotations

9

semafor

[das, schneider, chen, & smith 2010]

    semafor   s models consist of features over 
observable parts of the sentence (words, 
lemmas, pos tags, dependency edges & paths) 
that may be predictive of frame/role labels 
    full-text annotations as training data for 

(semi)supervised learning 

    extensive body of work on semantic role 
labeling [starting with gildea & jurafsky 2002 for 
framenet; also much work for propbank]

10

semafor

[das, schneider, chen, & smith 2010]

    state-of-the-art performance on semeval   07 

evaluation (outperforms the best system 
from the task, johansson & nugues 2007) 

    on se07:   [f] 74% [a] 68% [f   a] 46%   
on fn1.5: [f] 91% [a] 80% [f   a] 69% 
    but: this task is really hard. room for 

improvement at all stages.

[das et al. 2014]

11

semafor demo

http://demo.ark.cs.cmu.edu/parse

12

13

how to improve?

    better modeling with current resources 
    ways to use non-framenet resources 
    create new resources?

karl moritz 
hermann

oscar 

t  ckstr  m

dipanjan das

sam 

thomson

meghana 
kshirsagar

14

advances in modeling

target 
detection

frame 

disambiguation

argument 
detection + 

labeling

15

advances in modeling

frame 

disambiguation

16

target detectionargument detection + labelingunknown predicates

    problem: many frame-evoking predicates 

are seen neither in lexicon nor training data. 
how, then, to assign the correct frame? 
    solution: propagate frame labels from 

known predicates to unknown predicates in 
a similarity graph. [das & smith 2011, 2012 / 2014]

17

framenet data. for a pair of lus, we measured the euclidean distance between their
frame distributions. this distance was next converted to a similarity score and inter-
polated with the similarity score from lin   s dependency thesaurus. we omit further
details about the interpolation and refer the reader to full details given in das and smith
(2011).

unknown predicates

for each lu, we create a vertex and link it to the k nearest neighbor lus under the
interpolated similarity metric. the resulting graph has 64,480 vertices, 9,263 of which
are labeled seeds from framenet 1.5 and 55,217 of which are unlabeled. each vertex has
a possible set of labels corresponding to the 877 frames de   ned in the lexicon. figure 4
shows an excerpt from the constructed graph.

figure 4
excerpt from our constructed graph over lus. green lus are observed in the framenet 1.5 data.
above/below them are shown the most frequently observed frame that these lus associate
with. the black lus are unobserved and graph propagation produces a distribution over most
likely frames that they could evoke as target instances.

18

word representations

    problem: with little training data, many 
features are too infrequent to be useful   
particularly for rare/unseen words. 

    solution: learn id27s that are 
predictive of frame labels (neural network). 
[hermann et al. 2014]

19

advances in modeling

argument 
detection + 

labeling

20

target detectionframe disambiguationconstraints on argument 

combinations

constraints over role pairs. 

    problem: a frame   s arguments should not overlap, but 
this means classi   cation decisions are not independent. 
    also, some frames de   ne hard requires/excludes 
    solution 1: id125 (approximate).    
    solution 2: id209 (exact).    
    solution 3 (google   s variant of semafor): label arguments 

[das et al. 2010 / 2014] 

[das et al. 2012 / 2014] 

with id145. [t  ckstr  m et al. 2015]

21

constraints on argument 

combinations

figure 1: an example sentence from the annotations released as part of framenet 1.5 with three predicates marked in
bold. each predicate has its evoked semantic frame marked above it, in a distinct color. for each frame, its semantic
roles are shown in the same color, and the spans ful   lling the roles are underlined. for example, manner evokes the
conduct frame, and has the agent and manner roles ful   lled by austria and most un-viennese respectively.
2 collective argument identi   cation
here, we take a declarative approach to modeling
argument identi   cation using an ilp and relate our
formulation to prior work in shallow semantic pars-
ing. we show how knowledge speci   ed in a lin-
guistic resource can be used to derive the constraints
used in our ilp. finally, we draw connections of our
speci   cation to id114, a popular formal-
ism in ai, and describe how the constraints can be
treated as factors in a factor graph.

we de   ne a vector z of binary variables zr,s  
{0, 1} for every role and span pair. we have that:
z   {0, 1}d, where d = |rf|   |st|. zr,s = 1 means
that role r is    lled by span s. given the binary z vec-
tor, it is straightforward to recover the collection of

r and s. the srl literature provides many feature
functions of this form and many ways to use ma-
chine learning to acquire  . our presented method
does not make any assumptions about the score ex-
cept that it has the form in eq. 1.

22

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:8)(cid:10)(cid:11)(cid:12)(cid:13)(cid:8)(cid:13)(cid:14)(cid:15)(cid:13)(cid:12)(cid:4)(cid:13)(cid:16)(cid:8)(cid:4)(cid:10)(cid:8)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:8)(cid:3)(cid:17)(cid:10)(cid:10)(cid:4)(cid:18)(cid:19)(cid:20)(cid:8)(cid:6)(cid:11)(cid:4)(cid:10)(cid:8)(cid:4)(cid:18)(cid:13)(cid:8)(cid:21)(cid:2)(cid:5)(cid:10)(cid:15)(cid:13)(cid:7)(cid:11)(cid:8)(cid:22)(cid:11)(cid:6)(cid:10)(cid:11)(cid:8)(cid:9)(cid:8)(cid:6)(cid:3)(cid:8)(cid:13)(cid:19)(cid:23)(cid:10)(cid:24)(cid:6)(cid:11)(cid:25)(cid:8)(cid:6)(cid:4)(cid:3)(cid:8)(cid:6)(cid:2)(cid:7)(cid:4)(cid:8)(cid:9)(cid:7)(cid:10)(cid:8)(cid:9)(cid:8)(cid:4)(cid:5)(cid:13)(cid:7)(cid:16)(cid:6)(cid:11)(cid:25)(cid:8)(cid:10)(cid:11)(cid:8)(cid:4)(cid:10)(cid:13)(cid:3)(cid:8)(cid:7)(cid:11)(cid:16)(cid:8)(cid:15)(cid:10)(cid:25)(cid:10)(cid:26)(cid:16)(cid:7)(cid:11)(cid:12)(cid:6)(cid:11)(cid:25)(cid:8)(cid:6)(cid:11)(cid:8)(cid:7)(cid:8)(cid:17)(cid:10)(cid:3)(cid:4)(cid:8)(cid:2)(cid:11)(cid:26)(cid:27)(cid:6)(cid:13)(cid:11)(cid:11)(cid:13)(cid:3)(cid:13)(cid:8)(cid:11)(cid:2)(cid:8)(cid:8)(cid:9)(cid:7)(cid:8)(cid:28)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:7)(cid:3)(cid:3)(cid:12)(cid:13)(cid:7)(cid:14)(cid:12)(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:7)(cid:10)(cid:15)(cid:16)(cid:11)(cid:8)(cid:17)(cid:18)(cid:19)(cid:20)(cid:6)(cid:19)(cid:21)(cid:21)(cid:22)(cid:23)(cid:24)(cid:19)(cid:23)(cid:25)(cid:21)(cid:22)(cid:23)(cid:5)(cid:26)(cid:24)(cid:19)(cid:23)(cid:25)(cid:21)(cid:22)(cid:23)(cid:5)(cid:27)(cid:6)(cid:19)(cid:21)(cid:21)(cid:22)(cid:23)(cid:12)(cid:28)(cid:22)(cid:21)(cid:25)(cid:1)(cid:22)(cid:20)(cid:29)(cid:5)(cid:30)(cid:18)(cid:31)(cid:22)(cid:23)conclusion

    semafor system from cmu has been applied to 
tasks as diverse as stock prediction and spoken 
dialogue segmentation   

http://www.ark.cs.cmu.edu/semafor/

http://demo.ark.cs.cmu.edu/parse

    ongoing research at cmu, google, & elsewhere!

23

   
baker, c., ellsworth, m., & erk, k. (2007). semeval-2007 task 19: frame semantic 
structure extraction. semeval. 

das, d., chen, d., martins, a. f. t., schneider, n., & smith, n. a. (2014). frame-
id29. computational linguistics 40(1), 9   56. 

das, d., martins, a. f. t., & smith, n. a. (2012). an exact id209 
algorithm for shallow id29 with constraints. *sem. 

das, d., schneider, n., chen, d., & smith, n. a. (2010). probabilistic frame-
id29. naacl-hlt. 

das, d., & smith, n. a. (2011). semi-supervised frame-id29 for 
unknown predicates. acl. 

das, d., & smith, n. a. (2012). graph-based lexicon expansion with sparsity-
inducing penalties. naacl-hlt. 

erk, k., & pad  , s. (2006). shalmaneser - a toolchain for shallow semantic 
parsing. lrec. 

fleischman, m., kwon, n., & hovy, e. (2003). maximum id178 models for 
framenet classification. emnlp. 

f  rstenau, h., & lapata, m. (2009). semi-supervised id14. eacl.

24

gildea, d., & jurafsky, d. (2002). automatic labeling of  semantic roles. 
computational linguistics, 28(3), 245   288. 

hermann, k. m., das, d., weston, j., & ganchev, k. (2014). semantic frame 
identification with distributed word representations. acl. 

johansson, r., & nugues, p. (2007). lth: semantic structure extraction using 
nonprojective dependency trees. semeval. 

kwon, n., fleischman, m., & hovy, e. (2004). framenet-based id29 
using maximum id178 models. coling. 

matsubayashi, y., okazaki, n., & tsujii, j. (2009). a comparative study on 
generalization of  semantic roles in framenet. acl-ijcnlp. 

pad  , s., & lapata, m. (2005). cross-linguistic projection of  role-semantic 
information. hlt/emnlp. 

t  ckstr  m, o., ganchev, k., & das, d. (2015). efficient id136 and structured 
learning for id14. tacl, 3, 29   41. 

thompson, c. a., levy, r., & manning, c. d. (2003). a generative model for 
id14. ecml.

25

