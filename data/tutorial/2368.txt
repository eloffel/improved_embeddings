   #[1]github [2]recent commits to lstm-char-id98:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]62
     * [35]star [36]767
     * [37]fork [38]215

[39]yoonkim/[40]lstm-char-id98

   [41]code [42]issues 11 [43]pull requests 3 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   lstm language model with id98 over characters
     * [47]243 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors
     * [51]mit

    1. [52]lua 95.4%
    2. [53]shell 4.6%

   (button) lua shell
   branch: master (button) new pull request
   [54]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/y
   [55]download zip

downloading...

   want to be notified of new releases in yoonkim/lstm-char-id98?
   [56]sign in [57]sign up

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [60]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [61]download the github extension for visual studio
   and try again.

   (button) go back
   [62]@yoonkim
   [63]yoonkim [64]fix squeeze issue
   latest commit [65]ff8e317 mar 15, 2016
   [66]permalink
   type        name        latest commit message  commit time
        failed to load latest commit information.
        [67]data/ptb
        [68]model         [69]clean up           nov 23, 2015
        [70]util          [71]delete squeeze.lua mar 15, 2016
        [72].gitignore
        [73]licence
        [74]readme.md
        [75]evaluate.lua
        [76]get_data.sh
        [77]main.lua
        [78]run_models.sh [79]further clean-up   nov 23, 2015

readme.md

character-aware neural language models

   code for the paper [80]character-aware neural language models (aaai
   2016).

   a neural language model (nlm) built on character inputs only.
   predictions are still made at the word-level. the model employs a
   convolutional neural network (id98) over characters to use as inputs
   into an long short-term memory (lstm) recurrent neural network language
   model (id56-lm). also optionally passes the output from the id98 through
   a [81]highway network, which improves performance.

   much of the base code is from [82]andrej karpathy's excellent character
   id56 implementation.

requirements

   code is written in lua and requires torch. it also requires the nngraph
   and the luautf8 packages, which can be installed via:
luarocks install nngraph
luarocks install luautf8

   gpu usage will additionally require cutorch and cunn packages:
luarocks install cutorch
luarocks install cunn

   cudnn will result in a good (8x-10x) speed-up for convolutions, so it
   is highly recommended. this will make the training time of a
   character-level model be somewhat competitive against a word-level
   model (1500 tokens/sec vs 3000 tokens/sec for the large
   character/word-level models described below).
git clone https://github.com/soumith/cudnn.torch.git
cd cudnn.torch
luarocks make cudnn-scm-1.rockspec

data

   data should be put into the data/ directory, split into train.txt,
   valid.txt, and test.txt

   each line of the .txt file should be a sentence. the english penn
   treebank (ptb) data (tomas mikolov's pre-processed version with vocab
   size equal to 10k, widely used by the id38 community) is
   given as the default.

   the paper also runs the models on non-english data (czech, french,
   german, russian, and spanish), from the icml 2014 paper
   [83]compositional morphology for word representations and language
   modelling by jan botha and phil blunsom. this can be downloaded from
   [84]jan's website.

   for ease of use, we provide a script to download the non-english data
   (get_data.sh). the script also saves the downloaded data into the
   relevant folders.

note on ptb

   the ptb data above does not have end-of-sentence tokens for each
   sentence, and hence these must be manually appended. this can be done
   by adding -eos '+' to the script (obviously you can use other
   characters than + to represent an end-of-sentence token---we recommend
   a single unused character).

   the non-english data already have end-of-sentence tokens for each line
   so, you want to add -eos '' to the command line.

unicode in lua

   lua is unicode-agnostic (each string is just a sequence of bytes) so we
   use the luautf8 package to deal with languages where a character can be
   more than one byte (e.g. russian). many thanks to [85]vseledkin for
   alerting us to the fact that previous version of the code did not take
   this account!

model

   here are some example scripts. add -gpuid 0 to each line to use a gpu
   (which is required to get any reasonable speed with the id98), and
   -cudnn 1 to use the cudnn package. scripts to reproduce the results of
   the paper can be found under run_models.sh

character-level models

   large character-level model (lstm-charid98-large in the paper). this is
   the default: should get ~82 on valid and ~79 on test. takes ~5 hours
   with cudnn.
th main.lua -savefile char-large -eos '+'

   small character-level model (lstm-charid98-small in the paper). this
   should get ~96 on valid and ~93 on test. takes ~2 hours with cudnn.
th main.lua -savefile char-small -id56_size 300 -highway_layers 1
-kernels '{1,2,3,4,5,6}' -feature_maps '{25,50,75,100,125,150}' -eos '+'

word-level models

   large word-level model (lstm-word-large in the paper). this should get
   ~89 on valid and ~85 on test.
th main.lua -savefile word-large -word_vec_size 650 -highway_layers 0
-use_chars 0 -use_words 1 -eos '+'

   small word-level model (lstm-word-small in the paper). this should get
   ~101 on valid and ~98 on test.
th main.lua -savefile word-small -word_vec_size 200 -highway_layers 0
-use_chars 0 -use_words 1 -id56_size 200 -eos '+'

combining both

   note that if -use_chars and -use_words are both set to 1, the model
   will concatenate the output from the id98 with the id27. we've
   found this model to underperform a purely character-level model,
   though.

evaluation

   by default main.lua will evaluate the model on test data after
   training, but this will use the last epoch's model, and also will be
   slow due to the way the data is set up.

   evaluation on test can be performed via the following script:
th evaluate.lua -model model_file.t7 -data_dir data/ptb -savefile model_results.
t7

   where model_file.t7 is the path to the best performing (on validation)
   model. this will also save some basic statistics (e.g. perplexity by
   token) in model_results.t7.

hierarchical softmax

   training on a larger vocabulary (e.g. 100k+) will require hierarchical
   softmax (hsm) to train at a reasonable speed. you can use the -hsm
   option to do this. for example -hsm 500 will randomly split the
   vocabulary into 500 clusters of (approximately) equal size. -hsm 0 is
   the default and will not use hsm. -hsm -1 will automatically choose the
   number of clusters for you, by choosing the integer closest to
   sqrt(|v|).

batch size

   if training on bigger datasets you should probably use a larger batch
   size (e.g. -batch_size 100).

licence

   mit

     *    2019 github, inc.
     * [86]terms
     * [87]privacy
     * [88]security
     * [89]status
     * [90]help

     * [91]contact github
     * [92]pricing
     * [93]api
     * [94]training
     * [95]blog
     * [96]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [97]reload to refresh your
   session. you signed out in another tab or window. [98]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/yoonkim/lstm-char-id98/commits/master.atom
   3. https://github.com/yoonkim/lstm-char-id98#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/yoonkim/lstm-char-id98
  32. https://github.com/join
  33. https://github.com/login?return_to=/yoonkim/lstm-char-id98
  34. https://github.com/yoonkim/lstm-char-id98/watchers
  35. https://github.com/login?return_to=/yoonkim/lstm-char-id98
  36. https://github.com/yoonkim/lstm-char-id98/stargazers
  37. https://github.com/login?return_to=/yoonkim/lstm-char-id98
  38. https://github.com/yoonkim/lstm-char-id98/network/members
  39. https://github.com/yoonkim
  40. https://github.com/yoonkim/lstm-char-id98
  41. https://github.com/yoonkim/lstm-char-id98
  42. https://github.com/yoonkim/lstm-char-id98/issues
  43. https://github.com/yoonkim/lstm-char-id98/pulls
  44. https://github.com/yoonkim/lstm-char-id98/projects
  45. https://github.com/yoonkim/lstm-char-id98/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/yoonkim/lstm-char-id98/commits/master
  48. https://github.com/yoonkim/lstm-char-id98/branches
  49. https://github.com/yoonkim/lstm-char-id98/releases
  50. https://github.com/yoonkim/lstm-char-id98/graphs/contributors
  51. https://github.com/yoonkim/lstm-char-id98/blob/master/licence
  52. https://github.com/yoonkim/lstm-char-id98/search?l=lua
  53. https://github.com/yoonkim/lstm-char-id98/search?l=shell
  54. https://github.com/yoonkim/lstm-char-id98/find/master
  55. https://github.com/yoonkim/lstm-char-id98/archive/master.zip
  56. https://github.com/login?return_to=https://github.com/yoonkim/lstm-char-id98
  57. https://github.com/join?return_to=/yoonkim/lstm-char-id98
  58. https://desktop.github.com/
  59. https://desktop.github.com/
  60. https://developer.apple.com/xcode/
  61. https://visualstudio.github.com/
  62. https://github.com/yoonkim
  63. https://github.com/yoonkim/lstm-char-id98/commits?author=yoonkim
  64. https://github.com/yoonkim/lstm-char-id98/commit/ff8e3176f0348a5e36165b5ea42033e748c31150
  65. https://github.com/yoonkim/lstm-char-id98/commit/ff8e3176f0348a5e36165b5ea42033e748c31150
  66. https://github.com/yoonkim/lstm-char-id98/tree/ff8e3176f0348a5e36165b5ea42033e748c31150
  67. https://github.com/yoonkim/lstm-char-id98/tree/master/data/ptb
  68. https://github.com/yoonkim/lstm-char-id98/tree/master/model
  69. https://github.com/yoonkim/lstm-char-id98/commit/3d549015a31f3d7565703d196509e83fe8cd26e5
  70. https://github.com/yoonkim/lstm-char-id98/tree/master/util
  71. https://github.com/yoonkim/lstm-char-id98/commit/c71988117204d0205a0fe67eedc069e3d4fb44f6
  72. https://github.com/yoonkim/lstm-char-id98/blob/master/.gitignore
  73. https://github.com/yoonkim/lstm-char-id98/blob/master/licence
  74. https://github.com/yoonkim/lstm-char-id98/blob/master/readme.md
  75. https://github.com/yoonkim/lstm-char-id98/blob/master/evaluate.lua
  76. https://github.com/yoonkim/lstm-char-id98/blob/master/get_data.sh
  77. https://github.com/yoonkim/lstm-char-id98/blob/master/main.lua
  78. https://github.com/yoonkim/lstm-char-id98/blob/master/run_models.sh
  79. https://github.com/yoonkim/lstm-char-id98/commit/1e6a5c06c0777faed640af456c5a19639025ff84
  80. http://arxiv.org/abs/1508.06615
  81. http://arxiv.org/abs/1507.06228
  82. https://github.com/karpathy/char-id56
  83. http://arxiv.org/abs/1405.4273
  84. https://bothameister.github.io/
  85. https://github.com/vseledkin
  86. https://github.com/site/terms
  87. https://github.com/site/privacy
  88. https://github.com/security
  89. https://githubstatus.com/
  90. https://help.github.com/
  91. https://github.com/contact
  92. https://github.com/pricing
  93. https://developer.github.com/
  94. https://training.github.com/
  95. https://github.blog/
  96. https://github.com/about
  97. https://github.com/yoonkim/lstm-char-id98
  98. https://github.com/yoonkim/lstm-char-id98

   hidden links:
 100. https://github.com/
 101. https://github.com/yoonkim/lstm-char-id98
 102. https://github.com/yoonkim/lstm-char-id98
 103. https://github.com/yoonkim/lstm-char-id98
 104. https://help.github.com/articles/which-remote-url-should-i-use
 105. https://github.com/yoonkim/lstm-char-id98#character-aware-neural-language-models
 106. https://github.com/yoonkim/lstm-char-id98#requirements
 107. https://github.com/yoonkim/lstm-char-id98#data
 108. https://github.com/yoonkim/lstm-char-id98#note-on-ptb
 109. https://github.com/yoonkim/lstm-char-id98#unicode-in-lua
 110. https://github.com/yoonkim/lstm-char-id98#model
 111. https://github.com/yoonkim/lstm-char-id98#character-level-models
 112. https://github.com/yoonkim/lstm-char-id98#word-level-models
 113. https://github.com/yoonkim/lstm-char-id98#combining-both
 114. https://github.com/yoonkim/lstm-char-id98#evaluation
 115. https://github.com/yoonkim/lstm-char-id98#hierarchical-softmax
 116. https://github.com/yoonkim/lstm-char-id98#batch-size
 117. https://github.com/yoonkim/lstm-char-id98#licence
 118. https://github.com/
