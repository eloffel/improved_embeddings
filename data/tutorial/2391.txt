   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 2 kernel
     * [6]view on gist
     * [7]execute on binder
     * [8]download notebook

the unreasonable effectiveness of character-level language models[9]  

(and why id56s are still cool)[10]  

[11]yoav goldberg[12]  

   id56s, lstms and deep learning are all the rage, and a recent [13]blog
   post by andrej karpathy is doing a great job explaining what these
   models are and how to train them. it also provides some very impressive
   results of what they are capable of. this is a great post, and if you
   are interested in natural language, machine learning or neural networks
   you should definitely read it.

   go read it now, then come back here.

   you're back? good. impressive stuff, huh? how could the network learn
   to immitate the input like that? indeed. i was quite impressed as well.

   however, it feels to me that most readers of the post are impressed by
   the wrong reasons. this is because they are not familiar with
   unsmoothed maximum-liklihood id186 and their
   unreasonable effectiveness at generating rather convincing natural
   language outputs.

   in what follows i will briefly describe these character-level
   maximum-likelihood langauge models, which are much less magical than
   id56s and lstms, and show that they too can produce a rather convincing
   shakespearean prose. i will also show about 30 lines of python code
   that take care of both training the model and generating the output.
   compared to this baseline, the id56s may seem somehwat less impressive.
   so why was i impressed? i will explain this too, below.

unsmoothed maximum likelihood character level language model[14]  

   the name is quite long, but the idea is very simple. we want a model
   whose job is to guess the next character based on the previous $n$
   letters. for example, having seen ello, the next characer is likely to
   be either a commma or space (if we assume is is the end of the word
   "hello"), or the letter w if we believe we are in the middle of the
   word "mellow". humans are quite good at this, but of course seeing a
   larger history makes things easier (if we were to see 5 letters instead
   of 4, the choice between space and w would have been much easier).

   we will call $n$, the number of letters we need to guess based on, the
   order of the language model.

   id56s and lstms can potentially learn infinite-order language model
   (they guess the next character based on a "state" which supposedly
   encode all the previous history). we here will restrict ourselves to a
   fixed-order language model.

   so, we are seeing $n$ letters, and need to guess the $n+1$th one. we
   are also given a large-ish amount of text (say, all of shakespear
   works) that we can use. how would we go about solving this task?

   mathematiacally, we would like to learn a function $p(c | h)$. here,
   $c$ is a character, $h$ is a $n$-letters history, and $p(c|h)$ stands
   for how likely is it to see $c$ after we've seen $h$.

   perhaps the simplest approach would be to just count and divide (a.k.a
   maximum likelihood estimates). we will count the number of times each
   letter $c'$ appeared after $h$, and divide by the total numbers of
   letters appearing after $h$. the unsmoothed part means that if we did
   not see a given letter following $h$, we will just give it a
   id203 of zero.

   and that's all there is to it.

training code[15]  

   here is the code for training the model. fname is a file to read the
   characters from. order is the history size to consult. note that we pad
   the data with leading ~ so that we also learn how to start.
   in [41]:
from collections import *

def train_char_lm(fname, order=4):
    data = file(fname).read()
    lm = defaultdict(counter)
    pad = "~" * order
    data = pad + data
    for i in xrange(len(data)-order):
        history, char = data[i:i+order], data[i+order]
        lm[history][char]+=1
    def normalize(counter):
        s = float(sum(counter.values()))
        return [(c,cnt/s) for c,cnt in counter.iteritems()]
    outlm = {hist:normalize(chars) for hist, chars in lm.iteritems()}
    return outlm

   let's train it on andrej's shakespears's text:
   in [42]:
!wget http://cs.stanford.edu/people/karpathy/char-id56/shakespeare_input.txt

--2015-05-23 02:05:18--  http://cs.stanford.edu/people/karpathy/char-id56/shakesp
eare_input.txt
resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64
connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.
http request sent, awaiting response... 200 ok
length: 4573338 (4.4m) [text/plain]
saving to:    shakespeare_input.txt   

shakespeare_input.t 100%[=====================>]   4.36m   935kb/s   in 8.8s

2015-05-23 02:05:49 (507 kb/s) -    shakespeare_input.txt    saved [4573338/4573338]


   in [43]:
lm = train_char_lm("shakespeare_input.txt", order=4)

   ok. now let's do some queries:
   in [44]:
lm['ello']

   out[44]:
[('!', 0.0068143100511073255),
 (' ', 0.013628620102214651),
 ("'", 0.017035775127768313),
 (',', 0.027257240204429302),
 ('.', 0.0068143100511073255),
 ('r', 0.059625212947189095),
 ('u', 0.03747870528109029),
 ('w', 0.817717206132879),
 ('n', 0.0017035775127768314),
 (':', 0.005110732538330494),
 ('?', 0.0068143100511073255)]

   in [45]:
lm['firs']

   out[45]:
[('t', 1.0)]

   in [46]:
lm['rst ']

   out[46]:
[("'", 0.0008025682182985554),
 ('a', 0.0056179775280898875),
 ('c', 0.09550561797752809),
 ('b', 0.009630818619582664),
 ('e', 0.0016051364365971107),
 ('d', 0.0032102728731942215),
 ('g', 0.0898876404494382),
 ('f', 0.012038523274478331),
 ('i', 0.009630818619582664),
 ('h', 0.0040128410914927765),
 ('k', 0.008025682182985553),
 ('m', 0.0593900481540931),
 ('l', 0.10674157303370786),
 ('o', 0.018459069020866775),
 ('n', 0.0008025682182985554),
 ('p', 0.014446227929373997),
 ('s', 0.16292134831460675),
 ('r', 0.0008025682182985554),
 ('t', 0.0032102728731942215),
 ('w', 0.033707865168539325),
 ('a', 0.02247191011235955),
 ('c', 0.012841091492776886),
 ('b', 0.024879614767255216),
 ('e', 0.0032102728731942215),
 ('d', 0.015248796147672551),
 ('g', 0.011235955056179775),
 ('f', 0.011235955056179775),
 ('i', 0.016853932584269662),
 ('h', 0.019261637239165328),
 ('k', 0.0040128410914927765),
 ('m', 0.02247191011235955),
 ('l', 0.01043338683788122),
 ('o', 0.030497592295345103),
 ('n', 0.020064205457463884),
 ('q', 0.0016051364365971107),
 ('p', 0.00882825040128411),
 ('s', 0.03290529695024077),
 ('r', 0.0072231139646869984),
 ('u', 0.0016051364365971107),
 ('t', 0.05377207062600321),
 ('w', 0.024077046548956663),
 ('v', 0.002407704654895666),
 ('y', 0.002407704654895666)]

   in [ ]:


   so ello is followed by either space, punctuation or w (or r, u, n),
   firs is pretty much deterministic, and the word following ist can start
   with pretty much every letter.

generating from the model[16]  

   generating is also very simple. to generate a letter, we will take the
   history, look at the last $order$ characteters, and then sample a
   random letter based on the corresponding distribution.
   in [47]:
from random import random

def generate_letter(lm, history, order):
        history = history[-order:]
        dist = lm[history]
        x = random()
        for c,v in dist:
            x = x - v
            if x <= 0: return c

   to generate a passage of $k$ characters, we just seed it with the
   initial history and run letter generation in a loop, updating the
   history at each turn.
   in [48]:
def generate_text(lm, order, nletters=1000):
    history = "~" * order
    out = []
    for i in xrange(nletters):
        c = generate_letter(lm, history, order)
        history = history[-order:] + c
        out.append(c)
    return "".join(out)

generated shakespeare from different order models[17]  

   let's try to generate text based on different language-model orders.
   let's start with something silly:

order 2:[18]  

   in [35]:
lm = train_char_lm("shakespeare_input.txt", order=2)
print generate_text(lm, 2)

fif thad yourty
fare sid on che as al my he sheace ing.

thy your thy ove dievest sord wit whand of sold iset?

commet laund hant.

kincesargant:
out aboy tur pome you musicell losts, blover.

how difte quainge to sh,
and usbas ey will chor bacterea, and mens grou:
princeser,
'tis a but be;
i hends ing noth much?

lo, withiell thicest to an, se nourink of a gray that, the's ge, fat a to to and
 requand pink my menis of lat sall favere, i whathews be frevisars.
flaviiiii:
whout les: your
macus:
o,--
hie an an thout nown mis yought, the phimne, shappy bley
sirs,--ha!

hart, frow mas gen the me?

sey:
herfe, inese
vereat a voter'd
theave, shashall er, ist hem thdre of
mare to.

lovenat
me bree shatteed.

besat's the a giverve se.

flany:
whis i'll-volover to of you man hitinut,
to thadarthopeatund me wing of pourisforniners dinguent my so liked withe brave
heiry, and fore ist. fain
thess kno st, will be witund nothousto yesty,
art to stry all son ford bas sood cal love thys; as of th tund

   in [ ]:


   not so great.. but what if we increase the order to 4?

order 4[19]  

   in [36]:
lm = train_char_lm("shakespeare_input.txt", order=4)
print generate_text(lm, 4)

first, the devishin it son?

montano:
'tis true as full squellen the rest me, my passacre. and nothink my fairs,' done
 to vision of actious to thy to love, brings gods!

thur:
will comfited our flight offend make thy love;
brothere is oats at on thes:'--why, cross and so
her shouldestruck at one their hearina in all go to lives of costag,
to his he tyrant of you our the fill we hath trouble an over me?

king john:
great though i gain; for talk to mine and to the christ: a right him out
to kiss;
and to a kindness not of loves you gower and to the stray
than hers of ever in this flight?
i do me,
after, wild,
or, if i into ebbs, by fair too me knowned worship asider thyself-skin ever is a
gain, and eat behold speak imposed thy hand. give and cours not sweet you of sor
row then; for they are gone! then the prince, i
see your likewis, is thee; and him for is them hearts, we have a kiss,
and it is the come, some an eanly; you that am fire: prince when 'twixt young pi
ece, that honourish we fort

   in [37]:
lm = train_char_lm("shakespeare_input.txt", order=4)
print generate_text(lm, 4)

first office, masters
to part at that she may direct my brance
i would he dead. pleaseth profit,
then we last awaked you to again,
far that night i'll courteous herneath,
of circle off.

speed:
not you.

don pedro:
how to your preferment.

duchess quickly:
now rome
such other's chamber tears.
a head.

virgilia:
o, we show the bowls thouse two hones, if you loved: a proned speaking shrought
upon that shall affect, onest, that i am a man is at milford's worth.
am boundeserts are you, or woman great that's noble upon me burth one of the wel
l surfew-begot of thy daughed with trib, trumpet they the sever heave down?

first what down, on for truth of marry, which i have troilus' mouth'd
to rever hang that cond malvolio?

exeter:
blists: but speak morn back; would your soverdities, fatherefore the pate rever
mirth, let her thoughts:
orsino's heard make methink, being of an oxford or a name.

gonzalo:
what i reason,
his known:
yet i care the moor-worm.

duchess:
o, partles their father not our

   in [ ]:


   this is already quite reasonable, and reads like english. just 4
   letters history! what if we increase it to 7?

order 7[20]  

   in [38]:
lm = train_char_lm("shakespeare_input.txt", order=7)
print generate_text(lm, 7)

first citizen:
one graves
within rich pisa walls,
your noses snapper-up of uncurrent roar'd!

hotspur:
hath he call you i bear; the admiration; but young.

biron:
one word to all!

falstaff:
ay, my good lord,
sir?

octavius:
philarmonus!

soothsayer
that worthy's thumb-ring: all the green-a box.

mistress quickly:
ay, sir.

cade:
i would unstate
myself. vexed i am one of your royal cheer yon strangers from boast:
and god speed?

chiron:
and our virtue of your years, prodigious, the farthing whether deigned him alrea
dy.

widow:
your master's pleasure.

countess:
why me, timon:
if he were else this do?

falstaff:
prithee, be gone.

constance:
you have compiled iniquity have walked?

gentleman:
ay, at philip of madam
juliet, go and thou shall forth.
silvia, silvia--witness to come
to know in heart-string to you picked. i must nothing but valour. do you put me
to all the opening it.

widow:
thus we met
my wife' there's bohemia: who, if i were much more than my bosom
be as we stay, her brav

   in [ ]:


how about 10?[21]  

   in [39]:
lm = train_char_lm("shakespeare_input.txt", order=10)
print generate_text(lm, 10)

first citizen:
nay, then, that was hers,
it speaks against your other service:
but since the
youth of the circumstance be spoken:
your uncle and one baptista's daughter.

sebastian:
do i stand till the break off.

biron:
hide thy head.

ventidius:
he purposeth to athens: whither, with the vow
i made to handle you.

falstaff:
my good knave.

malvolio:
sad, lady! i could be forgiven you, you're welcome. give ear, sir, my doublet an
d hose and leave this present death.

second gentleman:
who may that she confess it is my lord enraged and forestalled ere we come to be
 a man. drown thyself?

apemantus:
ho, ho! i laugh to see your beard!

boyet:
madam, in great extremes of passion as she
discovers it.

parolles:
by my white head and her wit
values itself: to the sepulchre!'
with this, my lord,
that i have some business: let's away.

first keeper:
forbear to murder: and wilt thou not say he lies,
and lies, and let the devil would have said, sir, their speed
hath been balm to heal their woes,
b

this works pretty well[22]  

   with an order of 4, we already get quite reasonable results. increasing
   the order to 7 (~word and a half of history) or 10 (~two short words of
   history) already gets us quite passable shakepearan text. i'd say it is
   on par with the examples in andrej's post. and how simple and
   un-mystical the model is!

so why am i impressed with the id56s after all?[23]  

   generating english a character at a time -- not so impressive in my
   view. the id56 needs to learn the previous $n$ letters, for a rather
   small $n$, and that's it.

   however, the code-generation example is very impressive. why? because
   of the context awareness. note that in all of the posted examples, the
   code is well indented, the braces and brackets are correctly nested,
   and even the comments start and end correctly. this is not something
   that can be achieved by simply looking at the previous $n$ letters.

   if the examples are not cherry-picked, and the output is generally that
   nice, then the lstm did learn something not trivial at all.

   just for the fun of it, let's see what our simple language model does
   with the linux-kernel code:
   in [49]:
!wget http://cs.stanford.edu/people/karpathy/char-id56/linux_input.txt

--2015-05-23 02:07:59--  http://cs.stanford.edu/people/karpathy/char-id56/linux_i
nput.txt
resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64
connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.
http request sent, awaiting response... 200 ok
length: 6206996 (5.9m) [text/plain]
saving to:    linux_input.txt   

linux_input.txt     100%[=====================>]   5.92m  1.10mb/s   in 9.3s

2015-05-23 02:08:09 (654 kb/s) -    linux_input.txt    saved [6206996/6206996]


   in [50]:
lm = train_char_lm("linux_input.txt", order=10)
print generate_text(lm, 10)

~~/*
 * linux/kernel/time.c
 * please report this on hardware.
 */
void irq_mark_irq(unsigned long old_entries, eval);

                /*
                 * divide only 1000 for ns^2 -> us^2 conversion values don't ove
rflow:
                seq_puts(m, "\ttramp: %ps",
                                        (void *)class->contending_point]++;
        if (likely(t->flags & wq_unbound)) {
                /*
                 * update inode information. if the
                 * slowpath and sleep time (abs or rel)
 * @rmtp: remaining (either due
 * to consume the state of ring buffer size. */
        header_size - size, in bytes, of the chain.
                 */
                bug_on(!error);
                } while (cgrp) {
                if (old) {
                if (kdb_continue_catastrophic;
#endif

/*
 * for the deadlock.\n");
                return 0;
}
#endif

        if (!info->hdr)))
                return diag;
                }
                /* we are sharing problem where roundup (the collection is
                 * better readable */
        for (i = 0; i < rp->maxactive = max_t(u64, delay, 10000ll);
        __hrtimer_get_res - get the timer
 * @timer:      hrtimer to sched_clock_data *my_rdp)
{
        bool oneshot = tick_oneshot_mask, gfp_kernel)) {
                free_cpumask_v

   in [51]:
lm = train_char_lm("linux_input.txt", order=15)
print generate_text(lm, 15)

~/*
 * linux/kernel/power/snapshot.c
 *
 * this file is licensed under the terms of the gnu general public license for m
ore detailed information
 * on memory ordering guarantees
 * cgroups with bigger numbers are newer than those with smaller numbers.
 * also, as csses are always appended to the parent, and put the ref when
                         * this cgroup is being freed, so let's make sure that
 * every task struct that event->ctx->task could possibly point to
 * remains valid.  this condition is satisfied when called through
 * perf_event_init_context(child, ctxn);
                if (ret) {
                        pr_err("module len %lu truncated\n", info->len);
                        return -enomem;

        env->prog = *prog;

        /* grab the mutex to protect coming/going of the the jump_label table */
static const struct user_regset *
find_regset(const struct cpumask *cpu_map)
{
        int i;

        if (diag >= 0) {
                kdb_printf("go must execute on the entry cpu, "
                           "please use \"cpu %d\" and then execute go\n",
                           kdb_initial_cpu.  used to
 * single threaded,

   in [52]:
lm = train_char_lm("linux_input.txt", order=20)
print generate_text(lm, 20)

/*
 * linux/kernel/irq/spurious.c
 *
 * copyright (c) 2004 nadia yvette chambers
 */

#include <linux/irq.h>
#include <linux/mutex.h>
#include <linux/capability.h>
#include <linux/suspend.h>
#include <linux/shm.h>

#include <asm/uaccess.h>
#include <linux/interrupt.h>
#include "kdb_private.h"

/*
 * table of kdb_breakpoints
 */
kdb_bp_t kdb_breakpoints[kdb_maxbpt];

static void kdb_setsinglestep(struct pt_regs *regs)
{
        struct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);

        mutex_lock(&swhash->hlist_mutex);
        swhash->online = true;
        if (swhash->hlist_refcount)
                swevent_hlist_release(swhash);

        mutex_unlock(&show_mutex);

        return 0;
}

/*
 * unshare file descriptor table if it is being shared
 */
static int unshare_fs(unsigned long unshare_flags, struct cred **new_cred)
{
        struct cred *cred = current_cred();

        retval = -eperm;
        if (rgid != (gid_t) -1) {
                if (gid_eq(old->gid, kegid) ||
                    gid_eq(old->sgid, kegid) ||
                    gid_eq(old->sgid, kegid) ||
                    gid_eq(old->egid,

   in [53]:
print generate_text(lm, 20)

/*
 * linux/kernel/irq/chip.c
 *
 * copyright 2003-2004 red hat inc., durham, north carolina.
 * all rights reserved.
 * copyright (c) 2009 wind river systems, inc.
 * copyright (c) 2008 thomas gleixner <[24][email protected]>
 *
 * this code is based on david mills's reference nanokernel
 * implementation. it was mostly rewritten but keeps the same idea.
 */
void __hardpps(const struct timespec *tp)
{
        ktime_get_real_ts(tp);
        return 0;
}

/*
 * walks through iomem resources and calls func() with matching resource
 * ranges. this walks through whole tree and not just first level children.
 * all the memory ranges which overlap start,end and also match flags and
 * name are valid candidates.
 *
 * @name: name of resource
 * @flags: resource flags
 * @start: start addr
 * @end: end addr
 */
int walk_iomem_res(char *name, unsigned long val);

static int alloc_snapshot(struct trace_array *tr)
{
        struct dentry *d_tracer;

        d_tracer = tracing_init_dentry(void)
{
        struct trace_array *tr = wakeup_t

   in [55]:
print generate_text(lm, 20, nletters=5000)

/*
 * linux/kernel/irq/resend.c
 *
 * copyright (c) 2008 steven rostedt <[25][email protected]>
 * copyright (c) 2002 khalid aziz <[26][email protected]>
 * copyright (c) 2002 richard henderson
   copyright (c) 2001 rusty russell, 2002, 2010 rusty russell ibm.

    this program is distributed in the hope that it will be useful,
 * but without any warranty; without even the implied warranty of
* merchantability or fitness for a particular purpose.  see the
 * gnu general public license as published by the free software foundation, inc.
,
 * 51 franklin st - fifth floor, boston, ma  02110-1301  usa
 *
 */
#include <linux/cpuset.h>
#include <linux/sched/deadline.h>
#include <linux/ioport.h>
#include <linux/fs.h>
#include <linux/export.h>
#include <linux/mm.h>
#include <linux/ptrace.h>
#include <linux/profile.h>
#include <linux/smp.h>
#include <linux/proc_fs.h>
#include <linux/interrupt.h>
#include "kdb_private.h"

/*
 * table of kdb_breakpoints
 */
kdb_bp_t kdb_breakpoints[kdb_maxbpt];

static void kdb_setsinglestep(struct pt_regs *regs);
static int uretprobe_dispatcher(struct uprobe_consumer *con;
        int ret = -enoent;

        do {
                spin_lock(&hash_lock);
        if (tree->goner) {
                spin_unlock(&hash_lock);
                        fsnotify_put_mark(&parent->mark);
}

static void cpu_cgroup_css_offline,
        .fork           = cpu_cgroup_fork,
        .can_attach     = cpu_cgroup_can_attach(struct cgroup_subsys_state *last
;

        do {
                last = pos;
                /* ->prev isn't rcu safe, walk ->next till the end */
                pos = null;
                css_for_each_child(pos, css) {
                struct freezer *parent = parent_freezer(freezer);

        mutex_lock(&freezer_mutex);
        rcu_read_lock();
        list_for_each_entry_safe(owatch, nextw, &parent->watches, wlist) {
                if (audit_compare_dname_path(const char *dname, const char *path
, int parentlen)
{
        int dlen, pathlen;
        const char *p;

        dlen = strlen(dname);
        pathlen = strlen(path);
        if (pathlen < dlen)
                return 1;

        parentlen = parentlen == audit_name_full ? parent_len(path) : parentlen;
        if (pathlen - parentlen != dlen)
                return 1;

        p = path + parentlen;

        return strncmp(p, dname, dlen);
}

static int audit_log_pid_context(context, context->target_pid,
                                  context->target_sessionid,
                                  context->target_auid, context->target_uid,
                                  context->target_sessionid,
                                  context->target_sid, context->target_comm, t->
comm, task_comm_len);
                return 0;
        }

        spin_lock_mutex(&lock->wait_lock, flags);
                schedule();
                raw_spin_lock_init(&rq->lock);
                rq->nr_running = 0;
                rq->calc_load_active = nr_active;
        }

        return delta;
}

/*
 * a1 = a0 * e + a * (1 - e)) * e + a * (1 - e)
 *    = (a0 * e^2 + a * (1 - e) * (1 - e^n)/(1 - e)
 *    = a0 * e^2 + a * (1 - e) * (1 + e + ... + e^n-1) [1]
 *    = a0 * e^n + a * (1 - e) * (1 + e + e^2)
 *
 *  ...
 *
 * an = a0 * e^n + a * (1 - e) * (1 + e + ... + e^n-1) [1]
 *    = a0 * e^n + a * (1 - e) * (1 + e)
 *
 * a3 = a2 * e + a * (1 - e)
 *
 * a2 = a1 * e + a * (1 - e)
 *    = (a0 * e^2 + a * (1 - e) * (1 - e^n)/(1 - e)
 *    = a0 * e^2 + a * (1 - e) * (1 + e + e^2)
 *
 *  ...
 *
 * an = a0 * e^n + a * (1 - e^n)
 *
 * [1] application of the geometric series:
 *
 *                        is not a '0' or '1')\n");
}

static void *l_start(struct seq_file *file, void *v, loff_t *offset)
{
        unsigned long flags;

        spin_lock_irqsave(&timekeeper_lock, flags);
        if (global_trace.stop_count;
}

/**
 * tracing_is_enabled - show if global_trace has been disabled
 *
 * shows if the global trace has been enabled or not. it uses the
 * mirror flag "buffer_disabled" to be used in fast paths such as for
 * the irqsoff tracer. but it may be inaccurate due to races. if you
 * need to know the accurate state, use tracing_is_on() which is a little
 * slower, but accurate.
 */
int tracing_is_enabled())
                tracer_enabled = 0;

        unregister_wakeup_function(tr, graph, 0);

        if (!ret && tracing_is_enabled())
                return;

        local_irq_save(flags);
        gdbstub_msg_write(s, count);
        local_irq_restore(flags);
        }

        /* -enoent from try_to_grab_pending(work, is_dwork, &flags);
                /*
                 * if someone else is already canceling, wait for it to
                 * finish.  flush_work() doesn't work for preempt_none
                 * because we may get scheduled between @work's completion
                 * and the other canceling task resuming and clearing
                 * canceling - flush_work() will return false immediately
                 * as @work is no longer busy, try_to_grab_pending(struct work_s
truct *work)
{
        unsigned long data = atomic_long_read(&rsp->expedited_done);
                if (ulong_cmp_ge(jiffies,
                         rdp->rsp->gp_start + 2, jiffies))
                return 0;  /* grace period is not old enough. */
        barrier();
        if (local_read(&cpu_buffer_a->committing))
                goto out_dec;
        if (local_read(&cpu_buffer->overrun);
                        local_sub(buf_page_size, &cpu_buffer->entries_bytes);

                /*
                 * the entries will be zeroed out when we move the
                 * tail page.
                 */

                /* still more to do */
                break;

        case rb_page_update:
                /*
                 * this is not really a fixup. the work struct was
                 * statically initialized. we just make sure that it
                 * is tracked in the object tracker.
                         */
                        debug

   in [ ]:


   order 10 is pretty much junk. in order 15 things sort-of make sense,
   but we jump abruptly between the and by order 20 we are doing quite
   nicely -- but are far from keeping good indentation and brackets.

   how could we? we do not have the memory, and these things are not
   modeled at all. while we could quite easily enrich our model to support
   also keeping track of brackets and indentation (by adding information
   such as "have i seen ( but not )" to the conditioning history), this
   requires extra work, non-trivial human reasoning, and will make the
   model significantly more complex.

   the lstm, on the other hand, seemed to have just learn it on its own.
   and that's impressive.
   in [ ]:


   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [27]fastly, rendered by [28]rackspace

   nbviewer github [29]repository.

   nbviewer version: [30]33c4683

   nbconvert version: [31]5.4.0

   rendered (fri, 05 apr 2019 18:02:27 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/gist/yoavg/d76121dfde2618422139
   5. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139
   6. https://gist.github.com/d76121dfde2618422139
   7. https://mybinder.org/v2/gist/yoavg/d76121dfde2618422139/master?filepath=lm_example
   8. https://gist.githubusercontent.com/yoavg/d76121dfde2618422139/raw/56eaf865d5765ab20413e5abfa00c5c3aed179ee/lm_example
   9. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#the-unreasonable-effectiveness-of-character-level-language-models
  10. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#(and-why-id56s-are-still-cool)
  11. http://www.cs.biu.ac.il/~yogo
  12. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#yoav-goldberg
  13. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  14. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#unsmoothed-maximum-likelihood-character-level-language-model
  15. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#training-code
  16. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#generating-from-the-model
  17. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#generated-shakespeare-from-different-order-models
  18. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#order-2:
  19. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#order-4
  20. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#order-7
  21. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#how-about-10?
  22. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#this-works-pretty-well
  23. https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139#so-why-am-i-impressed-with-the-id56s-after-all?
  24. https://nbviewer.jupyter.org/cdn-cgi/l/email-protection
  25. https://nbviewer.jupyter.org/cdn-cgi/l/email-protection
  26. https://nbviewer.jupyter.org/cdn-cgi/l/email-protection
  27. http://www.fastly.com/
  28. https://developer.rackspace.com/?nbviewer=awesome
  29. https://github.com/jupyter/nbviewer
  30. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  31. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
