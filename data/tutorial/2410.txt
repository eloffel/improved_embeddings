   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    holding
   your hand like a small child through a neural network     part 1 comments
   feed [5]metrics gone wrong        how companies are optimizing the wrong
   way [6]dupont: sr. software developer     bioinformatics/genomics
   (abt00001643)

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2016    [28]apr    [29]tutorials,
   overviews    holding your hand like a small child through a neural
   network     part 1 ( [30]16:n15 )

holding your hand like a small child through a neural network     part 1

   [31][prv.gif] previous post
   [32]next post [nxt.gif]


   tags: [33]neural networks

   the first part of this 2 part series expands upon a now-classic neural
   network blog post and demonstration, guiding the reader through the
   foundational building blocks of a simple neural network.
     __________________________________________________________________

   by paul singman, freelance data scientist.

   for those who do not get the reference in the title: [34]wedding
   crashers.

   neural
   for those trying to deepen their understanding of neural nets,
   iamtrask   s    [35]a neural network in 11 lines of python    is a staple
   piece. while it does a good job   a great job even   of helping people
   understand neural nets better, it still takes significant effort on the
   reader   s part to truly follow along.

   my goal is to do more of the work for you and make it even easier.
   (note: you still have to exert mental effort if you actually want to
   learn this stuff, no one can replace that process for you.) however i
   will try to make it as easy as possible. how will i do that? primarily
   by taking his code and printing things, printing all the things. and
   renaming some of the variables to clearer names. i   ll do that too.

   link to my code: what i call the [36]annotated 11 line neural network.

   first, let   s take a look at the inputs for out neural network, and the
   output we are trying to train it to predict:

  +--------+---------+
  | inputs | outputs |
  +--------+---------+
  |  0,0,1 |       0 |
  |  0,1,1 |       0 |
  |  1,0,1 |       1 |
  |  1,1,1 |       1 |
  +--------+---------+


   those are our inputs. as mr. trask points out in his article, notice
   the first column of the input data corresponds perfectly to the output.
   this does make this    classification task    trivial since there   s a
   perfect correlation between one of the inputs and the output, but that
   doesn   t mean we can   t learn from this example. it just means we should
   expect the weight corresponding to the first input column to be very
   large at the end of training. we   ll have to wait and see what the
   weights on the other inputs end up being.

   in mr. trask   s 1-layer nn, the weights are held in the variable syn0
   (synapse 0). read about the brain to learn why he calls them synapses.
   i   m going to refer to them as the weights, however.  notice that we
   initialize the weights with random numbers that are supposed to have
   mean 0.

   let   s take a look at the initial values of the weights:

  weights:
   [[ 0.39293837 ]
   [-0.42772133]
   [-0.54629709]]


   we see that they, in fact, do not have a mean of zero. oh well, c   est
   la vie, the average won   t come out to be exactly zero every time.

   generating our first predictions

   let   s view the output of the nn variable-by-variable,
   iteration-by-iteration. we start with iteration #0.

  ---------iteration #0-------------
  inputs:
   [[0 0 1]
   [0 1 1]
   [1 0 1]
   [1 1 1]]


   those are our inputs, same as from the chart above. they represent four
   training examples.

   the first calculation we perform is the dot product of the inputs and
   the weights. i   m a crazy person (did i mention that?) so i   m going to
   follow along and perform this calculation by hand.

   we   ve got a 4  3 matrix  (the inputs) and a 3  1 matrix (the weights), so
   the result of the id127 will be a 4  1 matrix.

   we have:

  (0 * .3929) + (0 * -.4277) + (1 * -.5463) = -.5463

  (0 * .3929) + (1 * -.4277) + (1 * -.5463) = -.9740

  (1 * .3929) + (0 * -.4277) + (1 * -.5463) = -.1534

  (1 * .3929) + (1 * -.4277) + (1 * -.5463) = -.5811


   i   m sorry i can   t create fancy graphics to show why those are the
   calculations you perform for this dot product. if you   re actually
   following along with this article, i trust you   ll figure it out. read
   about [37]id127 if you need more background.

   okay! we   ve got a 4  1 matrix of dot product results and if you   re like
   me, you probably have no idea why we got to where we   ve gotten, and
   where we   re going with this. have patience for a couple more steps and
   i promise i   ll guide us to a reasonable    mini-result    and explain what
   just happened.

   the next step according to the code is to send the four values through
   the sigmoid function. the purpose of this is to convert the raw numbers
   into probabilities (values between 0 and 1). this is the same step
   id28 takes to provide its classification probabilities.

                              sigmoid function

   element-wise, as it   s called in the world of matrix operations, we
   apply to the sigmoid function to each of the four results we got from
   the id127*. large values should be transformed close to
   1. large negative values should be transformed to something close to 0.
   and numbers in between should take on a value in between 0 and 1!

   *i   m using the terms id127 and dot product
   interchangeably here.

   sigmoid

   although i calculated the results of applying the sigmoid function
      manually    in excel, i   ll defer to the code results for this one:

  dot product results:
   [[-0.54629709]
   [-0.97401842]
   [-0.15335872]
   [-0.58108005]]

  id203 predictions (sigmoid):
   [[ 0.36672394]
   [ 0.27408027]
   [ 0.46173529]
   [ 0.35868411]]


   so we take the results of the dot product (of the initial inputs and
   weights) and send them through the sigmoid function. the result is the
      mini-result    i promised earlier and represents the model   s first
   predictions.

   to be overly explicit, if you take the first dot product result, -.5463
   and input it as the    x    in the sigmoid function, the output is 0.3667.

   this means that the neural network   s first    guesses    are that the first
   input has a 36.67% chance of being a 1. the second input has a 27%
   chance, the third a 46.17% chance, and the final and fourth input a
   35.86% chance.

   all of our dot product results were negative, so it makes sense that
   all of our predictions were under 50% (a dot product result of 0 would
   correspond to a 50% prediction, meaning the model has absolutely no
   idea whether to guess 0 or 1).

   to provide some context, the sigmoid is far from the only function we
   could use to transform the dot product into probabilities, though it is
   the one with the nicest mathematical properties, namely that it is
   differentiable and its derivative, as we   ll see later, is
   mind-numbingly simple.

   pages: 1 [38]2
     __________________________________________________________________

   [39][prv.gif] previous post
   [40]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [41]another 10 free must-read books for machine learning and data
       science
    2. [42]9 must-have skills you need to become a data scientist, updated
    3. [43]who is a typical data scientist in 2019?
    4. [44]the pareto principle for data scientists
    5. [45]what no one will tell you about data science job applications
    6. [46]19 inspiring women in ai, big data, data science, machine
       learning
    7. [47]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [48]id158s optimization using genetic algorithm
       with python
    2. [49]who is a typical data scientist in 2019?
    3. [50]8 reasons why you should get a microsoft azure certification
    4. [51]the pareto principle for data scientists
    5. [52]r vs python for data visualization
    6. [53]how to work in data science, ai, big data
    7. [54]the deep learning toolset     an overview

[55]latest news

     * [56]statistical thinking for industrial problem solving (st...
     * [57]from business intelligence to machine intelligence
     * [58]what is missing when ai makes a decision?
     * [59]spatio-temporal statistics: a primer
     * [60]another 10 free must-see courses for machine learning a...
     * [61]download your datax guide to ai in marketing

   [62]kdnuggets home    [63]news    [64]2016    [65]apr    [66]tutorials,
   overviews    holding your hand like a small child through a neural
   network     part 1 ( [67]16:n15 )
      2019 kdnuggets. [68]about kdnuggets.  [69]privacy policy. [70]terms
   of service

   [71]subscribe to kdnuggets news
   [72][tw_c48.png] [73]facebook [74]linkedin
   x

   [envelope.png] [75]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2016/04/holding-your-hand-neural-network-part-1.html/feed
   5. https://www.kdnuggets.com/2016/04/metrics-gone-wrong-optimizing-wrong-way.html
   6. https://www.kdnuggets.com/jobs/16/04-20-dupont-sr-software-developer-bioinformatics-genomics.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2016/index.html
  28. https://www.kdnuggets.com/2016/04/index.html
  29. https://www.kdnuggets.com/2016/04/tutorials.html
  30. https://www.kdnuggets.com/2016/n15.html
  31. https://www.kdnuggets.com/2016/04/metrics-gone-wrong-optimizing-wrong-way.html
  32. https://www.kdnuggets.com/jobs/16/04-20-dupont-sr-software-developer-bioinformatics-genomics.html
  33. https://www.kdnuggets.com/tag/neural-networks
  34. http://www.youtube.com/watch?v=-cprq-gqpju&t=2m30s
  35. https://iamtrask.github.io/2015/07/12/basic-python-network/
  36. https://github.com/peacing/datascience-tutorials/blob/master/annotate_11nn.ipynb
  37. http://tutorial.math.lamar.edu/classes/calcii/dotproduct.aspx
  38. https://www.kdnuggets.com/2016/04/holding-your-hand-neural-network-part-1.html/2
  39. https://www.kdnuggets.com/2016/04/metrics-gone-wrong-optimizing-wrong-way.html
  40. https://www.kdnuggets.com/jobs/16/04-20-dupont-sr-software-developer-bioinformatics-genomics.html
  41. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  42. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  43. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  44. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  45. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  46. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  47. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  48. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  49. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  50. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  51. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  52. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  53. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  54. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  55. https://www.kdnuggets.com/news/index.html
  56. https://www.kdnuggets.com/2019/04/jmp-statistical-thinking-free-online-course.html
  57. https://www.kdnuggets.com/2019/04/datarobot-from-business-intelligence-machine-intelligence.html
  58. https://www.kdnuggets.com/2019/04/ai-makes-decision.html
  59. https://www.kdnuggets.com/2019/04/spatio-temporal-statistics-primer.html
  60. https://www.kdnuggets.com/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html
  61. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  62. https://www.kdnuggets.com/
  63. https://www.kdnuggets.com/news/index.html
  64. https://www.kdnuggets.com/2016/index.html
  65. https://www.kdnuggets.com/2016/04/index.html
  66. https://www.kdnuggets.com/2016/04/tutorials.html
  67. https://www.kdnuggets.com/2016/n15.html
  68. https://www.kdnuggets.com/about/index.html
  69. https://www.kdnuggets.com/news/privacy-policy.html
  70. https://www.kdnuggets.com/terms-of-service.html
  71. https://www.kdnuggets.com/news/subscribe.html
  72. https://twitter.com/kdnuggets
  73. https://facebook.com/kdnuggets
  74. https://www.linkedin.com/groups/54257
  75. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
  77. https://www.kdnuggets.com/
  78. https://www.kdnuggets.com/
