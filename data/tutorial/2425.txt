   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    a
   statistical view of deep learning comments feed [5]a community event
   for innovative spark apps: a datapalooza dispatch [6]getting started
   with python and apache flink

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2015    [28]nov    [29]tutorials,
   overviews, how-tos    a statistical view of deep learning ( [30]15:n38 )

a statistical view of deep learning

   [31][prv.gif] previous post
   [32]next post [nxt.gif]


   tags: [33]deep learning, [34]recurrent neural networks, [35]statistical
   learning

   a statistical overview of deep learning, with a focus on testing
   wide-held beliefs, highlighting statistical connections, and the unseen
   implications of deep learning. the post links to 6 articles covering a
   number of related topics.
     __________________________________________________________________

   by [36]shakir mohamed, google deepmind.
   over the past 6 months, i've taken to writing a series of posts (one
   each month) on a statistical view of deep learning with two principal
   motivations in mind. the first was as a personal exercise to make
   concrete and to test the limits of the way that i think about, and use
   deep learning in my every day work. the second, was to highlight
   important statistical connections and implications of deep learning
   that i do not see being made in the popular courses, reviews and books
   on deep learning, but which are extremely important to keep in mind.
   post links and summary
   links to each post with a short summary and as a single pdf are
   collected here.
   [37]recursive glms. we constructed a view of deep feedforward networks
   as a natural extension of generalised id75 formed by
   recursive application of the the generalised linear form. maximum
   likelihood was shown to be the underlying method for parameter
   learning.
   [38]auto-encoders and free energy. denoising auto-encoders were shown
   to be a way of implemented an amortised id136 in models with latent
   variables. adding noise to the observations and denoising turns out not
   to be what we are interested in. instead, we can add noise to the
   hidden variables, turning auto-encoders into deep generative models and
   providing one way of implementing variational id136.
   [39]memory and kernels. where deep networks are archetypical parametric
   models, kernel machines are archetypical non-parametric models. we
   showed that these seemingly different approaches are very closely
   connected, being duals of each other. their differences have
   interesting implications for our machine learning practice, and over
   time we will see these methods being used more closely together.
   [40]recurrent networks and dynamical systems. whereas models with
   shared parameters that are recursively applied are called recurrent
   networks in deep learning, in statistics, models of this type are
   called state-space models or dynamical systems. recurrent networks
   assume that the hidden states are deterministic, whereas state-space
   models have stochastic hidden states. they can easily be connected by
   maximum likelihood reasoning, and there are great deal of innovative
   models that can be exchanged between the two areas.
   equivalent models: recurrent networks and state-space models.
   equivalent models: recurrent networks and state-space models.
   [41]generalisation and regularisation. maximum likelihood is not
   sufficient for the scalable and accurate deep learning we require and
   we can ameliorate this situation by using various forms of statistical
   regularisation. and thus, most deep learning now uses some form of map
   estimation. using statistical reasoning, we can then reason about the
   limitations of map estimation, which will lead us to think about
   invariant map estimators wherefrom the connections to the natural
   gradient, minimum message length methods, and bayesian id136
   emerges.
   contours showing the shrinkage effects of different priors.
   contours showing the shrinkage effects of different priors.
   [42]what is deep? as a closing post, it seemed appropriate to
   interrogate the key word that has been used throughout this series.
   many people new to deep learning often ask what is deep, and this post
   characterises deep models as probabilistic id187 where
   the hierarchical dependency is through the means of the random
   variables within the hierarchy. when using maximum likelihood or map we
   can recover familiar solutions. but other types of hierarchies can be
   considered as well, and have an important role to play and will emerge
   as we continue research.
   deep and id187 are abound in machine learning.
   deep and id187 are abound in machine learning.
   [43]combined pdf: i've created a combined pdf of the posts in case they
   are of interest. find it here [[44]svdl.pdf]
   discussion
   each post is necessarily short since my aim was to test how concrete i
   could frame my thinking within around 1200 words (posts are on average
   1500 words though). thus there are many more discussions, references
   and connections that could have been added, and is the limitation of
   these posts. i do not explicitly discuss convolutional networks
   anywhere. since convolution is a special linear operation we will not
   need any special reasoning to form a statistical view. what does
   require more reasoning is the statistical connections to pooling
   operations and something i'll hopefully cement in the future. the
   invariant map estimators discussed in part 5 show that you could get an
   update rule that will involve the inverse fisher which is different
   from that obtained using the natural gradient, and is a connection that
   i was unable to establish directly. i did not provide many examples of
   the ways that popular deep and statistical methods can be combined.
   kernel methods (in part 3) and deep learning can easily be combined by
   parameterising the kernel with a neural network, giving the best of
   both worlds. i have chosen to view dropout (in part 5) as a prior
   assumption that does not require id136, and connected this to
   spike-and-slab priors. but there are many other views that are
   complementary and valid for this, making a longer discussion of just
   this topic something for the future.
   i have enjoyed this series, and it has been a wonderful exploration and
   fun to write. thanks to the many people who have read, shared and sent
   me feedback. i do have a new series of posts to come: next time on
   neuroscience-inspired machine learning :)
   bio: [45]shakir mohamed is a researcher in statistical machine learning
   and artificial intelligence, and currently a senior research scientist
   at google deepmind in london. before that he was a cifar scholar with
   nando de freitas at the ubc. he has a phd from university of cambridge.
   [46]original.
   related:
     * [47]does deep learning come from the devil?
     * [48]will deep learning take over machine learning, make other
       algorithms obsolete?
     * [49]top 5 arxiv deep learning papers, explained
     __________________________________________________________________

   [50][prv.gif] previous post
   [51]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [52]another 10 free must-read books for machine learning and data
       science
    2. [53]9 must-have skills you need to become a data scientist, updated
    3. [54]who is a typical data scientist in 2019?
    4. [55]the pareto principle for data scientists
    5. [56]what no one will tell you about data science job applications
    6. [57]19 inspiring women in ai, big data, data science, machine
       learning
    7. [58]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [59]id158s optimization using genetic algorithm
       with python
    2. [60]who is a typical data scientist in 2019?
    3. [61]8 reasons why you should get a microsoft azure certification
    4. [62]the pareto principle for data scientists
    5. [63]r vs python for data visualization
    6. [64]how to work in data science, ai, big data
    7. [65]the deep learning toolset     an overview

[66]latest news

     * [67]download your datax guide to ai in marketing
     * [68]kdnuggets offer: save 20% on strata in london
     * [69]training a champion: building deep neural nets for big ...
     * [70]building a recommender system
     * [71]predict age and gender using convolutional neural netwo...
     * [72]top tweets, mar 27     apr 02: here is a great ex...

   [73]kdnuggets home    [74]news    [75]2015    [76]nov    [77]tutorials,
   overviews, how-tos    a statistical view of deep learning ( [78]15:n38 )
      2019 kdnuggets. [79]about kdnuggets.  [80]privacy policy. [81]terms
   of service

   [82]subscribe to kdnuggets news
   [83][tw_c48.png] [84]facebook [85]linkedin
   x

   [envelope.png] [86]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2015/11/statistical-view-deep-learning.html/feed
   5. https://www.kdnuggets.com/2015/11/datapalooza-dispatch-kobielus.html
   6. https://www.kdnuggets.com/2015/11/getting-started-python-apache-flink.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2015/index.html
  28. https://www.kdnuggets.com/2015/11/index.html
  29. https://www.kdnuggets.com/2015/11/tutorials.html
  30. https://www.kdnuggets.com/2015/n38.html
  31. https://www.kdnuggets.com/2015/11/datapalooza-dispatch-kobielus.html
  32. https://www.kdnuggets.com/2015/11/getting-started-python-apache-flink.html
  33. https://www.kdnuggets.com/tag/deep-learning
  34. https://www.kdnuggets.com/tag/recurrent-neural-networks
  35. https://www.kdnuggets.com/tag/statistical-learning
  36. http://blog.shakirm.com/about-me/
  37. http://blog.shakirm.com/2015/01/a-statistical-view-of-deep-learning-i-recursive-glms/
  38. http://blog.shakirm.com/2015/03/a-statistical-view-of-deep-learning-ii-auto-encoders-and-free-energy/
  39. http://blog.shakirm.com/2015/04/a-statistical-view-of-deep-learning-iii-memory-and-kernels/
  40. http://blog.shakirm.com/2015/05/a-statistical-view-of-deep-learning-iv-recurrent-nets-and-dynamical-systems/
  41. http://blog.shakirm.com/2015/05/a-statistical-view-of-deep-learning-v-generalisation-and-regularisation/
  42. http://blog.shakirm.com/2015/06/a-statistical-view-of-deep-learning-vi-what-is-deep/
  43. http://blog.shakirm.com/wp-content/uploads/2015/07/svdl.pdf
  44. http://blog.shakirm.com/wp-content/uploads/2015/07/svdl.pdf
  45. http://blog.shakirm.com/about-me/
  46. http://blog.shakirm.com/ml-series/a-statistical-view-of-deep-learning/
  47. https://www.kdnuggets.com/2015/10/deep-learning-vapnik-einstein-devil-yandex-conference.html
  48. https://www.kdnuggets.com/2014/10/deep-learning-make-machine-learning-algorithms-obsolete.html
  49. https://www.kdnuggets.com/2015/10/top-arxiv-deep-learning-papers-explained.html
  50. https://www.kdnuggets.com/2015/11/datapalooza-dispatch-kobielus.html
  51. https://www.kdnuggets.com/2015/11/getting-started-python-apache-flink.html
  52. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  53. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  54. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  55. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  56. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  57. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  58. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  59. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  60. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  61. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  62. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  63. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  64. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  65. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  66. https://www.kdnuggets.com/news/index.html
  67. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  68. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  69. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  70. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  71. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  72. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  73. https://www.kdnuggets.com/
  74. https://www.kdnuggets.com/news/index.html
  75. https://www.kdnuggets.com/2015/index.html
  76. https://www.kdnuggets.com/2015/11/index.html
  77. https://www.kdnuggets.com/2015/11/tutorials.html
  78. https://www.kdnuggets.com/2015/n38.html
  79. https://www.kdnuggets.com/about/index.html
  80. https://www.kdnuggets.com/news/privacy-policy.html
  81. https://www.kdnuggets.com/terms-of-service.html
  82. https://www.kdnuggets.com/news/subscribe.html
  83. https://twitter.com/kdnuggets
  84. https://facebook.com/kdnuggets
  85. https://www.linkedin.com/groups/54257
  86. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
  88. https://www.kdnuggets.com/
  89. https://www.kdnuggets.com/
