   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    top 10
   data mining algorithms, explained comments feed [5]how to reduce data
   hoarding, get better visualizations and decisions [6]trifacta    
   wrangling us flight data, part 2

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2015    [28]may    [29]tutorials,
   overviews, how-tos    top 10 data mining algorithms, explained
   ( [30]15:n17 )

top 10 data mining algorithms, explained

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 1364
   tags: [33]algorithms, [34]apriori, [35]bayesian, [36]boosting,
   [37]c4.5, [38]cart, [39]data mining, [40]explained, [41]id116,
   [42]k-nearest neighbors, [43]naive bayes, [44]page rank, [45]support
   vector machines, [46]top 10

   top 10 data mining algorithms, selected by top researchers, are
   explained here, including what do they do, the intuition behind the
   algorithm, available implementations of the algorithms, why use them,
   and interesting applications.
     __________________________________________________________________

                                                            c [47]comments

   by [48]raymond li.

   today, i   m going to explain in plain english the top 10 most
   influential data mining algorithms as voted on by 3 separate panels in
   this [49]survey paper.

   once you know what they are, how they work, what they do and where you
   can find them, my hope is you   ll have this blog post as a springboard
   to learn even more about data mining.

   what are we waiting for? let   s get started!

   top-10-data-mining-algorithms

   here are the algorithms:
     * 1. c4.5
     * 2. id116
     * 3. support vector machines
     * 4. apriori
     * 5. em
     * 6. id95
     * 7. adaboost
     * 8. knn
     * 9. naive bayes
     * 10. cart

   we also provide interesting resources at the end.

1. c4.5

   what does it do? c4.5 constructs a classifier in the form of a decision
   tree. in order to do this, c4.5 is given a set of data representing
   things that are already classified.

   wait, what   s a classifier? a classifier is a tool in data mining that
   takes a bunch of data representing things we want to classify and
   attempts to predict which class the new data belongs to.

   what   s an example of this? sure, suppose a dataset contains a bunch of
   patients. we know various things about each patient like age, pulse,
   blood pressure, vo[2]max, family history, etc. these are called
   attributes.

   now:

   given these attributes, we want to predict whether the patient will get
   cancer. the patient can fall into 1 of 2 classes: will get cancer or
   won   t get cancer. c4.5 is told the class for each patient.

   and here   s the deal:

   using a set of patient attributes and the patient   s corresponding
   class, c4.5 constructs a decision tree that can predict the class for
   new patients based on their attributes.

   cool, so what   s a decision tree? decision tree learning creates
   something similar to a flowchart to classify new data. using the same
   patient example, one particular path in the flowchart could be:
     * patient has a history of cancer
     * patient is expressing a gene highly correlated with cancer patients
     * patient has tumors
     * patient   s tumor size is greater than 5cm

   the bottom line is:

   at each point in the flowchart is a question about the value of some
   attribute, and depending on those values, he or she gets classified.
   you can find lots of [50]examples of id90.

   is this supervised or unsupervised? this is supervised learning, since
   the training dataset is labeled with classes. using the patient
   example, c4.5 doesn   t learn on its own that a patient will get cancer
   or won   t get cancer. we told it first, it generated a decision tree,
   and now it uses the decision tree to classify.

   you might be wondering how c4.5 is different than other decision tree
   systems?
     * first, c4.5 uses [51]information gain when generating the decision
       tree.
     * second, although other systems also incorporate pruning, c4.5 uses
       a [52]single-pass pruning process to mitigate over-fitting. pruning
       results in [53]many improvements.
     * third, c4.5 can work with both continuous and discrete data. my
       understanding is it does this by specifying ranges or thresholds
       for continuous data thus turning continuous data into discrete
       data.
     * finally, incomplete data is dealt with in [54]its own ways.

   why use c4.5? arguably, the best selling point of id90 is
   their ease of interpretation and explanation. they are also quite fast,
   quite popular and the output is [55]human readable.

   where is it used? a popular open-source java implementation can be
   found over at [56]opentox. [57]orange, an open-source data
   visualization and analysis tool for data mining, implements c4.5 in
   their decision tree classifier.

   classifiers are great, but make sure to checkout the next algorithm
   about id91   

2. id116

   what does it do? id116 creates k groups from a set of objects so that
   the members of a group are more similar. it   s a popular cluster
   analysis technique for exploring a dataset.

   hang on, what   s cluster analysis? cluster analysis is a family of
   algorithms designed to form groups such that the group members are more
   similar versus non-group members. clusters and groups are synonymous in
   the world of cluster analysis.

   is there an example of this? definitely, suppose we have a dataset of
   patients. in cluster analysis, these would be called observations. we
   know various things about each patient like age, pulse, blood pressure,
   vo[2]max, cholesterol, etc. this is a vector representing the patient.

   look:

   you can basically think of a vector as a list of numbers we know about
   the patient. this list can also be interpreted as coordinates in
   multi-dimensional space. pulse can be one dimension, blood pressure
   another dimension and so forth.

   you might be wondering:

   given this set of vectors, how do we cluster together patients that
   have similar age, pulse, blood pressure, etc?

   want to know the best part?

   you tell id116 how many clusters you want. id116 takes care of the
   rest.

   how does id116 take care of the rest? id116 has lots of variations
   to optimize for certain types of data.

   at a high level, they all do something like this:
    1. id116 picks points in multi-dimensional space to represent each
       of the k clusters. these are called centroids.
    2. every patient will be closest to 1 of these k centroids. they
       hopefully won   t all be closest to the same one, so they   ll form a
       cluster around their nearest centroid.
    3. what we have are k clusters, and each patient is now a member of a
       cluster.
    4. id116 then finds the center for each of the k clusters based on
       its cluster members (yep, using the patient vectors!).
    5. this center becomes the new centroid for the cluster.
    6. since the centroid is in a different place now, patients might now
       be closer to other centroids. in other words, they may change
       cluster membership.
    7. steps 2-6 are repeated until the centroids no longer change, and
       the cluster memberships stabilize. this is called convergence.

   is this supervised or unsupervised? it depends, but most would classify
   id116 as unsupervised. other than specifying the number of clusters,
   id116    learns    the clusters on its own without any information about
   which cluster an observation belongs to. id116 can be
   [58]semi-supervised.

   why use id116? i don   t think many will have an issue with this:

   the key selling point of id116 is its simplicity. its simplicity
   means it   s generally faster and more efficient than other algorithms,
   especially over large datasets.

   it gets better:

   id116 can be used to [59]pre-cluster a massive dataset followed by a
   more expensive cluster analysis on the sub-clusters. id116 can also
   be used to rapidly    play    with k and explore whether there are
   overlooked patterns or relationships in the dataset.

   it   s not all smooth sailing:

   two key weaknesses of id116 are its sensitivity to outliers, and its
   sensitivity to the initial choice of centroids. one final thing to keep
   in mind is id116 is designed to operate on continuous data     you   ll
   need to do some [60]tricks to get it to work on discrete data.

   where is it used? a ton of implementations for id116 id91 are
   available online:
     * [61]apache mahout
     * [62]julia
     * [63]r
     * [64]scipy
     * [65]weka
     * [66]matlab
     * [67]sas

   if id90 and id91 didn   t impress you, you   re going to
   love the next algorithm.

3. support vector machines

   what does it do? support vector machine (id166) learns a hyperplane to
   classify data into 2 classes. at a high-level, id166 performs a similar
   task like c4.5 except id166 doesn   t use id90 at all.

   whoa, a hyper-what? a hyperplane is a function like the equation for a
   line, y = mx + b. in fact, for a simple classification task with just 2
   features, the hyperplane can be a line.

   as it turns out   

   id166 can perform a trick to project your data into higher dimensions.
   once projected into higher dimensions   

      id166 figures out the best hyperplane which separates your data into the
   2 classes.

   do you have an example? absolutely, the simplest example i found starts
   with a bunch of red and blue balls on a table. if the balls aren   t too
   mixed together, you could take a stick and without moving the balls,
   separate them with the stick.

   you see:

   when a new ball is added on the table, by knowing which side of the
   stick the ball is on, you can predict its color.

   what do the balls, table and stick represent? the balls represent data
   points, and the red and blue color represent 2 classes. the stick
   represents the simplest hyperplane which is a line.

   and the coolest part?

   id166 figures out the function for the hyperplane.

   what if things get more complicated? right, they frequently do. if the
   balls are mixed together, a straight stick won   t work.

   here   s the work-around:

   quickly lift up the table throwing the balls in the air. while the
   balls are in the air and thrown up in just the right way, you use a
   large sheet of paper to divide the balls in the air.

   you might be wondering if this is cheating:

   nope, lifting up the table is the equivalent of mapping your data into
   higher dimensions. in this case, we go from the 2 dimensional table
   surface to the 3 dimensional balls in the air.

   how does id166 do this? by using a kernel we have a nice way to operate
   in higher dimensions. the large sheet of paper is still called a
   hyperplane, but it is now a function for a plane rather than a line.
   note from yuval that once we   re in 3 dimensions, the hyperplane must be
   a plane rather than a line.

   i found this visualization super helpful:

   iframe: [68]https://www.youtube.com/embed/3licbrzprza?rel=0

   reddit also has 2 great threads on this in the [69]eli5 and [70]ml
   subreddits.

   how do balls on a table or in the air map to real-life data? a ball on
   a table has a location that we can specify using coordinates. for
   example, a ball could be 20cm from the left edge and 50cm from the
   bottom edge. another way to describe the ball is as (x, y) coordinates
   or (20, 50). x and y are 2 dimensions of the ball.

   here   s the deal:

   if we had a patient dataset, each patient could be described by various
   measurements like pulse, cholesterol level, blood pressure, etc. each
   of these measurements is a dimension.

   the bottom line is:

   id166 does its thing, maps them into a higher dimension and then finds
   the hyperplane to separate the classes.

   margins are often associated with id166? what are they? the margin is the
   distance between the hyperplane and the 2 closest data points from each
   respective class. in the ball and table example, the distance between
   the stick and the closest red and blue ball is the margin.

   the key is:

   id166 attempts to maximize the margin, so that the hyperplane is just as
   far away from red ball as the blue ball. in this way, it decreases the
   chance of misclassification.

   where does id166 get its name from? using the ball and table example, the
   hyperplane is equidistant from a red ball and a blue ball. these balls
   or data points are called support vectors, because they support the
   hyperplane.

   is this supervised or unsupervised? this is a supervised learning,
   since a dataset is used to first teach the id166 about the classes. only
   then is the id166 capable of classifying new data.

   why use id166? id166 along with c4.5 are generally the [71]2 classifiers to
   try first. no classifier will be the best in all cases due to the
   [72]no free lunch theorem. in addition, kernel selection and
   interpretability are some weaknesses.

   where is it used? there are many implementations of id166. a few of the
   popular ones are [73]scikit-learn, [74]matlab and of course [75]libid166.

   the next algorithm is one of my favorites   

   pages: 1 [76]2 [77]3
     __________________________________________________________________

   [78][prv.gif] previous post
   [79]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [80]another 10 free must-read books for machine learning and data
       science
    2. [81]9 must-have skills you need to become a data scientist, updated
    3. [82]who is a typical data scientist in 2019?
    4. [83]the pareto principle for data scientists
    5. [84]what no one will tell you about data science job applications
    6. [85]19 inspiring women in ai, big data, data science, machine
       learning
    7. [86]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [87]id158s optimization using genetic algorithm
       with python
    2. [88]who is a typical data scientist in 2019?
    3. [89]8 reasons why you should get a microsoft azure certification
    4. [90]the pareto principle for data scientists
    5. [91]r vs python for data visualization
    6. [92]how to work in data science, ai, big data
    7. [93]the deep learning toolset     an overview

[94]latest news

     * [95]download your datax guide to ai in marketing
     * [96]kdnuggets offer: save 20% on strata in london
     * [97]training a champion: building deep neural nets for big ...
     * [98]building a recommender system
     * [99]predict age and gender using convolutional neural netwo...
     * [100]top tweets, mar 27     apr 02: here is a great ex...

more recent stories

     * [101]top tweets, mar 27     apr 02: here is a great explanat...
     * [102]odsc east is selling out; odsc india announced
     * [103]accelerate ai and data science career expo, may 4, 2019
     * [104]grow your data career at datasciencego, san diego, sep 27-29
     * [105]getting started with nlp using the pytorch framework
     * [106]how to diy your data science education
     * [107]top 8 data science use cases in gaming
     * [108]kdnuggets 19:n13, apr 3: top 10 data scientist coding mista...
     * [109]make better data-driven business decisions
     * [110]top stories, mar 25-31: r vs python for data visualization;
       th...
     * [111]two predictive analytics world events in europe this fall
     * [112]7 qualities your big data visualization tools absolutely must
       ...
     * [113]yeshiva university: tenure-track faculty in ai and machine
       lea...
     * [114]which face is real?
     * [115]yeshiva university: program director / tenure track faculty
       me...
     * [116]top 10 coding mistakes made by data scientists
     * [117]uber   s case study at paw industry 4.0: machine learning ...
     * [118]xai     a data scientist   s mouthpiece
     * [119]what does gpt-2 think about the ai arms race?
     * [120]openclassrooms: data freelance online course creator
       [telecomm...

   [121]kdnuggets home    [122]news    [123]2015    [124]may   
   [125]tutorials, overviews, how-tos    top 10 data mining algorithms,
   explained ( [126]15:n17 )
      2019 kdnuggets. [127]about kdnuggets.  [128]privacy policy.
   [129]terms of service

   [130]subscribe to kdnuggets news
   [131][tw_c48.png] [132]facebook [133]linkedin
   x
   [envelope.png] [134]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2015/05/top-10-data-mining-algorithms-explained.html/feed
   5. https://www.kdnuggets.com/2015/05/reduce-data-hoarding-pca-visualization-decisions.html
   6. https://www.kdnuggets.com/2015/05/trifacta-wrangling-us-flight-data-part2.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2015/index.html
  28. https://www.kdnuggets.com/2015/05/index.html
  29. https://www.kdnuggets.com/2015/05/tutorials.html
  30. https://www.kdnuggets.com/2015/n17.html
  31. https://www.kdnuggets.com/2015/05/reduce-data-hoarding-pca-visualization-decisions.html
  32. https://www.kdnuggets.com/2015/05/trifacta-wrangling-us-flight-data-part2.html
  33. https://www.kdnuggets.com/tag/algorithms
  34. https://www.kdnuggets.com/tag/apriori
  35. https://www.kdnuggets.com/tag/bayesian
  36. https://www.kdnuggets.com/tag/boosting
  37. https://www.kdnuggets.com/tag/c4-5
  38. https://www.kdnuggets.com/tag/cart
  39. https://www.kdnuggets.com/tag/data-mining
  40. https://www.kdnuggets.com/tag/explained
  41. https://www.kdnuggets.com/tag/id116
  42. https://www.kdnuggets.com/tag/k-nearest-neighbors
  43. https://www.kdnuggets.com/tag/naive-bayes
  44. https://www.kdnuggets.com/tag/page-rank
  45. https://www.kdnuggets.com/tag/support-vector-machines
  46. https://www.kdnuggets.com/tag/top-10
  47. https://www.kdnuggets.com/2015/05/top-10-data-mining-algorithms-explained.html/3#comments
  48. http://rayli.net/
  49. http://www.cs.uvm.edu/~icdm/algorithms/10algorithms-08.pdf
  50. https://www.google.com/search?q=c4.5+decision+tree&tbm=isch
  51. http://en.wikipedia.org/wiki/id178_(information_theory)
  52. http://www.cs.bc.edu/~alvarez/ml/statpruning.html
  53. http://stackoverflow.com/questions/10865372/why-does-the-c4-5-algorithm-use-pruning-in-order-to-reduce-the-decision-tree-and
  54. http://stats.stackexchange.com/questions/96025/how-do-decision-tree-learning-algorithms-deal-with-missing-values-under-the-hoo
  55. https://www.google.com/search?q=j4.8+output&tbm=isch
  56. http://www.opentox.org/dev/documentation/components/j48
  57. http://orange.biolab.si/
  58. https://www.google.com/search?q=semi+supervised+k+means
  59. http://stats.stackexchange.com/questions/58855/why-do-we-use-id116-instead-of-other-algorithms
  60. http://stats.stackexchange.com/questions/28170/id91-a-dataset-with-both-discrete-and-continuous-variables
  61. https://mahout.apache.org/users/id91/id116-commandline.html
  62. https://github.com/juliastats/id91.jl
  63. https://stat.ethz.ch/r-manual/r-devel/library/stats/html/kmeans.html
  64. http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.cluster.vq.kmeans.html
  65. http://weka.sourceforge.net/doc.dev/weka/clusterers/simplekmeans.html
  66. http://www.mathworks.com/help/stats/kmeans.html
  67. http://support.sas.com/documentation/cdl/en/statugid91/61759/pdf/default/statugid91.pdf
  68. https://www.youtube.com/embed/3licbrzprza?rel=0
  69. http://www.reddit.com/r/explainlikeimfive/comments/rkmjp/what_is_support_vector_machine/
  70. http://www.reddit.com/r/machinelearning/comments/15zrpp/please_explain_support_vector_machines_id166_like_i
  71. http://www.researchgate.net/post/what_are_pros_and_cons_of_decision_tree_versus_other_classifier_as_knn_id166_nn
  72. http://en.wikipedia.org/wiki/no_free_lunch_theorem
  73. http://scikit-learn.org/stable/modules/id166.html
  74. http://www.mathworks.com/help/stats/support-vector-machines-id166.html
  75. http://www.csie.ntu.edu.tw/~cjlin/libid166/
  76. https://www.kdnuggets.com/2015/05/top-10-data-mining-algorithms-explained.html/2
  77. https://www.kdnuggets.com/2015/05/top-10-data-mining-algorithms-explained.html/3
  78. https://www.kdnuggets.com/2015/05/reduce-data-hoarding-pca-visualization-decisions.html
  79. https://www.kdnuggets.com/2015/05/trifacta-wrangling-us-flight-data-part2.html
  80. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  81. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  82. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  83. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  84. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  85. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  86. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  87. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  88. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  89. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  90. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  91. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  92. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  93. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  94. https://www.kdnuggets.com/news/index.html
  95. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  96. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  97. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  98. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  99. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
 100. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
 101. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
 102. https://www.kdnuggets.com/2019/04/odsc-east-selling-out-india-announced.html
 103. https://www.kdnuggets.com/jobs/19/04-03-accelerate-ai-data-science-career-expo-2019.html
 104. https://www.kdnuggets.com/2019/04/formulated-data-career-datasciencego-san-diego.html
 105. https://www.kdnuggets.com/2019/04/nlp-pytorch.html
 106. https://www.kdnuggets.com/2019/04/diy-your-data-science-education.html
 107. https://www.kdnuggets.com/2019/04/top-8-data-science-use-cases-gaming.html
 108. https://www.kdnuggets.com/2019/n13.html
 109. https://www.kdnuggets.com/2019/04/psu-make-better-data-driven-business-decisions.html
 110. https://www.kdnuggets.com/2019/04/top-news-week-0325-0331.html
 111. https://www.kdnuggets.com/2019/04/paw-two-predictive-analytics-world-events-europe-fall.html
 112. https://www.kdnuggets.com/2019/04/7-qualities-big-data-visualization-tools.html
 113. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-faculty-ai-machine-learning.html
 114. https://www.kdnuggets.com/2019/04/which-face-real-stylegan.html
 115. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-program-director-faculty-artificial-intelligence-machine-learning.html
 116. https://www.kdnuggets.com/2019/04/top-10-coding-mistakes-data-scientists.html
 117. https://www.kdnuggets.com/2019/04/paw-uber-case-study-machine-learning-enforce-mobile-performance.html
 118. https://www.kdnuggets.com/2019/04/xai-data-scientist.html
 119. https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html
 120. https://www.kdnuggets.com/jobs/19/04-01-openclassrooms-data-freelance-online-course-creator-b.html
 121. https://www.kdnuggets.com/
 122. https://www.kdnuggets.com/news/index.html
 123. https://www.kdnuggets.com/2015/index.html
 124. https://www.kdnuggets.com/2015/05/index.html
 125. https://www.kdnuggets.com/2015/05/tutorials.html
 126. https://www.kdnuggets.com/2015/n17.html
 127. https://www.kdnuggets.com/about/index.html
 128. https://www.kdnuggets.com/news/privacy-policy.html
 129. https://www.kdnuggets.com/terms-of-service.html
 130. https://www.kdnuggets.com/news/subscribe.html
 131. https://twitter.com/kdnuggets
 132. https://facebook.com/kdnuggets
 133. https://www.linkedin.com/groups/54257
 134. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
 136. https://www.kdnuggets.com/
 137. https://www.kdnuggets.com/
