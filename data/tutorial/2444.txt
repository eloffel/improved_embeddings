   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    deep
   learning in a nutshell     what it is, how it works, why care? comments
   feed [5]fundamental methods of data science: classification, regression
   and similarity matching [6]december 2014 analytics, big data, data
   mining acquisitions and startups activity

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2015    [28]jan    [29]opinions,
   interviews, reports    deep learning in a nutshell     what it is, how it
   works, why care? ( [30]15:n02 )

deep learning in a nutshell     what it is, how it works, why care?

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 410
   tags: [33]brain, [34]deep learning, [35]deepmind, [36]neural networks,
   [37]nikhil buduma

   deep learning and neural networks are increasingly important concepts
   in computer science with great strides being made by large companies
   like google and startups like deepmind.
     __________________________________________________________________

   by nikhil buduma (musings of an mit student).
   deep learning. neural networks. id26. over the past year or
   two, i've heard these buzz words being tossed around a lot, and it's
   something that has definitely seized my curiosity recently. deep
   learning is an area of active research these days, and if you've kept
   up with the field of computer science, i'm sure you've come across at
   least some of these terms at least once.
   deep learning can be an intimidating concept, but it's becoming
   increasingly important these days. google's already making huge strides
   in the space with the [38]google brain project and its recent
   acquisition of the london-based deep learning startup [39]deepmind.
   moreover, deep learning methods are beating out traditional machine
   learning approaches on virtually every single metric.
   so what exactly is deep learning? how does it work? and most
   importantly, why should you even care?
   note to the reader
   if you're new to computer science, and you've followed me up till this
   point, please stick with me. certain optional sections of this article
   may get a little math heavy (marked with a *), but i want to make this
   subject accessible to everyone, computer science major or not. in fact,
   if you are reading through this article and, at any point, you find
   yourself confused about the material, please email me. i will make
   whatever edits and clarifications that are necessary to make the
   article clearer.
   what is machine learning?
   before we dive into deep learning, i want to take a step back and talk
   a little bit about the broader field of "machine learning" and what it
   means when we say that we're programming machines to learn.
   sometimes we encounter problems for which it's really hard to write a
   computer program to solve. for example, let's say we wanted to program
   a computer to recognize hand-written digits:
   handwritten digits

              image provided by the mnist handwritten database
   you could imagine trying to devise a set of rules to distinguish each
   individual digit. zeros, for instance, are basically one closed loop.
   but what if the person didn't perfectly close the loop. or what if the
   right top of the loop closes below where the left top of the loop
   starts?
   handwritten zero or six

      a zero that's difficult to distinguish from a six algorithmically
   in this case, we have difficulty differentiating zeros from sixes. we
   could establish some sort of cutoff, but how would you decide the
   cutoff in the first place? as you can see, it quickly becomes quite
   complicated to compile a list of heuristics (i.e., rules and guesses)
   that accurately classifies handwritten digits.
   and there are so many more classes of problems that fall into this
   category. recognizing objects, understanding concepts, comprehending
   speech. we don't know what program to write because we still don't know
   how it's done by our own brains. and even if we did have a good idea
   about how to do it, the program might be horrendously complicated.
   so instead of trying to write a program, we try to develop an algorithm
   that a computer can use to look at hundreds or thousands of examples
   (and the correct answers), and then the computer uses that experience
   to solve the same problem in new situations. essentially, our goal is
   to teach the computer to solve by example, very similar to how we might
   teach a young child to distinguish a cat from a dog.
   over the past few decades, computer scientists have developed a number
   of algorithms that try to allow computers to learn to solve problems
   through examples. deep learning, which was first theorized in the early
   80's (and perhaps even earlier), is one paradigm for performing machine
   learning. and because of a flurry of modern research, deep learning is
   again on the rise because it's been shown to be quite good at teaching
   computers to do what our brains can do naturally.
   one of the big challenges with traditional machine learning models is a
   process called feature extraction. specifically, the programmer needs
   to tell the computer what kinds of things it should be looking for that
   will be informative in making a decision. feeding the algorithm raw
   data rarely ever works, so feature extraction is a critical part of the
   traditional machine learning workflow. this places a huge burden on the
   programmer, and the algorithm's effectiveness relies heavily on how
   insightful the programmer is. for complex problems such as object
   recognition or handwriting recognition, this is a huge challenge.
   deep learning is one of the only methods by which we can circumvent the
   challenges of feature extraction. this is because deep learning models
   are capable of learning to focus on the right features by themselves,
   requiring little guidance from the programmer. this makes deep learning
   an extremely powerful tool for modern machine learning.
   a first look at neural networks
   deep learning is a form of machine learning that uses a model of
   computing that's very much inspired by the structure of the brain.
   hence we call this model a neural network. the basic foundational unit
   of a neural network is the neuron, which is actually conceptually quite
   simple.
   neuron

                   schematic for a neuron in a neural net
   each neuron has a set of inputs, each of which is given a specific
   weight. the neuron computes some function on these weighted inputs. a
   linear neuron takes a linear combination of the weighted inputs. a
   sigmoidal neuron does something a little more complicated:
   sigmoid

                     the function of a sigmoidal neuron
   it feeds the weighted sum of the inputs into the logistic function. the
   logistic function returns a value between 0 and 1. when the weighted
   sum is very negative, the return value is very close to 0. when the
   weighted sum is very large and positive, the return value is very close
   to 1. for the more mathematically inclined, the logistic function is a
   good choice because it has a nice looking derivative, which makes
   learning a simpler process. but technical details aside, whatever
   function the neuron uses, the value it computes is transmitted to other
   neurons as its output. in practice, sigmoidal neurons are used much
   more often than linear neurons because they enable much more versatile
   learning algorithms compared to linear neurons.
   a neural network comes about when we start hooking up neurons to each
   other, to the input data, and to the "outlets," which correspond to the
   network's answer to the learning problem. to make this structure easier
   to visualize, i've included a simple example of a neural net below. we
   let w(k)i,j be the weight of the link connecting the ith neuron in
   the kth layer with the jth neuron in the k+1st layer:
   neural net

      an example of a neural net with 3 layers and 3 neurons per layer
   similar to how neurons are generally organized in layers in the human
   brain, neurons in neural nets are often organized in layers as well,
   where neurons on the bottom layer receive signals from the inputs,
   where neurons in the top layers have their outlets connected to the
   "answer," and where are usually no connections between neurons in the
   same layer (although this is an optional restriction, more complex
   connectivities require more involved mathematical analysis). we also
   note that in this example, there are no connections that lead from a
   neuron in a higher layer to a neuron in a lower layer (i.e., no
   directed cycles). these neural networks are called feed-forward neural
   networks as opposed to their counterparts, which are
   called recursiveneural networks (again these are much more complicated
   to analyze and train). for the sake of simplicity, we focus only on
   feed-forward networks throughout this discussion. here's a set of some
   more important notes to keep in mind:
   1) although every layer has the same number of neurons in this example,
   this is not necessary.
   2) it is not required that a neuron has its outlet connected to the
   inputs of every neuron in the next layer. in fact, selecting which
   neurons to connect to which other neurons in the next layer is an art
   that comes from experience. allowing maximal connectivity will more
   often than not result in overfitting, a concept which we will discuss
   in more depth later.
   3) the inputs and outputs are vectorized representations. for example,
   you might imagine a neural network where the inputs are the individual
   pixel rgb values in an image represented as a vector. the last layer
   might have 2 neurons which correspond to the answer to our
   problem: [0,1] if the image contains a dog, [1,0] if the image contains
   a cat, [0,0] if it contains neither, and [1,1] if it contains both.
   4) the layers of neurons that lie sandwiched between the first layer of
   neurons (input layer) and the last layer of neurons (output layer), are
   called hidden layers. this is because this is where most of the magic
   is happening when the neural net tries to solve problems. taking a look
   at the activities of hidden layers can tell you a lot about the
   features the network has learned to extract from the data.
   training a single neuron
   well okay, things are starting to get interesting, but we're still
   missing a big chunk of the picture. we know how a neural net can
   compute answers from inputs, but we've been assuming that we know what
   weights to use to begin with. finding out what those weights should be
   is the hard part of the problem, and that's done through a process
   called training. during training, we show the neural net a large number
   of training examples and iteratively modify the weights to minimize the
   errors we make on the training examples.
   let's start off with a toy example involving a single linear neuron to
   motivate the process. every day you grab lunch in the dining hall where
   your meal consists completely of burgers, fries, and soda. you buy some
   number of servings of each item. you want to be able to predict how
   much your meal will cost you, but you don't know the prices of each
   individual item. the only thing the cashier will tell you is the total
   price of the meal.
   how do we solve this problem? well, we could begin by being smart about
   picking our training cases, right? for one meal we could buy only a
   single serving of burgers, for another we could only buy a single
   serving of fries, and then for our last meal we could buy a single
   serving of soda. in general, choosing smart training cases is a very
   good idea. there's lots of research that shows that by engineering a
   clever training set, you can make your neural net a lot more effective.
   the issue with this approach is that in real situations, this rarely
   ever gets you even 10% of the way to the solution. for example, what's
   the analog of this strategy in image recognition?
   let's try to motivate a solution that works in general. take a look at
   the single neuron we want to train:
   dining hall neuron

           the neuron we want to train for the dining hall problem
   let's say we have a bunch of training examples. then we can calculate
   what the neural network will output on the ith training example using
   the simple formula in the diagram. we want to train the neuron so that
   we pick the optimal weights possible - the weights that minimize the
   errors we make on the training examples. in this case, let's say we
   want to minimize the square error over all of the training examples
   that we encounter. more formally, if we know that t(i) is the true
   answer for the ith training example and y(i) is the value computed by
   the neural network, we want to minimize the value of the error
   function e:
   [nikhil-eq1.png]
   now at this point you might be thinking, wait up... why do we need to
   bother ourselves with this error function nonsense when we have a bunch
   of variables (weights) and we have a set of equations (one for each
   training example)? couldn't we just solve this problem by setting up a
   system of linear system of equations? that would automatically give us
   an error of zero assuming that we have a consistent set of training
   examples, right?
   that's a smart observation, but the insight unfortunately doesn't
   generalize well. remember that although we're using a linear neuron
   here, linear neurons aren't used very much in practice because they're
   constrained in what they can learn. and the moment you start using
   nonlinear neurons like the sigmoidal neurons we talked about, we can no
   longer set up a system of linear equations!
   so maybe we can use an iterative approach instead that generalizes to
   nonlinear examples. let's try to visualize how we might minimize the
   squared error over all of the training examples by simplifying the
   problem. let's say we're dealing with a linear neuron with only two
   inputs (and thus only two weights, w1 and w2). then we can imagine a
   3-dimensional space where the horizontal dimensions correspond to the
   weights w1 and w2, and there is one vertical dimension that corresponds
   to the value of the error function e. so in this space, points in the
   horizontal plane correspond to different settings of the weights, and
   the height at those points corresponds to the error that we're
   incurring, summed over all training cases. if we consider the errors we
   make over all possible weights, we get a surface in this 3-dimensional
   space, in particular a quadratic bowl:
   quadratic error surface

               the quadratic error surface for a linear neuron
   we can also conveniently visualize this surface as a set of elliptical
   contours, where the minimum error is at the center of the ellipses:
   contour error surface

             visualizing the error surface as a set of contours
   so now let's say we find ourselves somewhere on the horizontal plane
   (by picking a random initialization for the weights). how would we get
   ourselves to the point on the horizontal plane with the smallest error
   value? one strategy is to always move perpendicularly to the contour
   lines. take a look, for instance, at the path denoted by the red
   arrows. quite clearly, you can see that following this strategy will
   eventually get us to the point of minimum error.
   what's particularly interesting is that moving perpendicularly to the
   contour lines is equivalent to taking the path of steepest descent down
   the parabolic bowl. this is a pretty amazing result from calculus, and
   it gives us the name of this general strategy for training neural
   nets: id119.
   learning rates and the delta rule
   in practice at each step of moving perpendicular to the contour, we
   need to determine how far we want to walk before recalculating our new
   direction. this distance needs to depend on the steepness of the
   surface. why? the closer we are to the minimum, the shorter we want to
   step forward. we know we are close to the minimum, because the surface
   is a lot flatter, so we can use the steepness as an indicator of how
   close we are to the minimum. we multiply this measure of steepness with
   a pre-determined constant factor   , the learning rate. picking the
   learning rate is a hard problem. if we pick a learning rate that's too
   small, we risk taking too long during the training process. if we pick
   a learning rate that's too big, we'll mostly likely start diverging
   away from the minimum (this pretty easy to visualize). modern training
   algorithms adapt the learning rate to overcome this difficult
   challenge.
   for those who are interested, putting all the pieces results in what is
   called the delta rule for training the linear neuron. the delta rule
   states that given a learning rate   , we ought to change the
   weight wk at each iteration of training by   wk=   i  xk(t(i)   y(i)).
   deriving this formula is left as an exercise for the experienced
   reader. for a hint, study our derivation for a sigmoidal neuron in the
   next section.
   unfortunately, just taking the path of steepest descent doesn't always
   do the trick when we have nonlinear neurons. the error surface can get
   complicated and there could be multiple local minimum. as a result,
   using this procedure could potentially get us to a bad local minimum
   that isn't the global minimum. as a result, in practice, training
   neural nets involves a modification of id119
   called stochastic id119, that tries to use randomization and
   noise to find the global minimum with high id203 on a complex
   error surface.
   moving onto the sigmoidal neuron *
   this section and the next will get a little heavy with the math, so
   just be forewarned. if you're not comfortable with multivariate
   calculus, feel free to skip them and move onto the remaining sections.
   otherwise, let's just dive right into it!
   let's recall the mechanism by which logistic neurons compute their
   output value from their inputs:
   [nikhil-eq2.png]
   the neuron computes the weighted sum of its inputs, the logit, z. it
   then feeds z into the input function to compute y, its final output.
   these functions have very nice derivatives, which makes learning easy!
   for learning, we want to compute the gradient of the error function
   with respect to the weights. to do so, we start by taking the
   derivative of the logit, z, with respect to the inputs and the weights.
   by linearity of the logit:
   [nikhil-eq3.png]
   also, quite surprisingly, the derivative of the output with respect to
   the logit is quite simple if you express it in terms of the output.
   verifying this is left as an exercise for the reader:
   [nikhil-eq4.png]
   we then use the chain rule to get the derivative of the output with
   respect to each weight:
   [nikhil-eq5.png]
   putting all of this together, we can now compute the derivative of the
   error function with respect to each weight:
   [nikhil-eq6.png]
   thus, the final rule for modifying the weights becomes:
   [nikhil-eq7.png]
   as you may notice, the new modification rule is just like the delta
   rule, except with extra multiplicative terms included to account for
   the logistic component of the sigmoidal neuron.
   the id26 algorithm *
   now we're finally ready to tackle the problem of training multilayer
   neural networks (instead of just single neurons). so what's the idea
   behind id26? we don't know what the hidden units ought to be
   doing, but what we can do is compute how fast the error changes as we
   change a hidden activity. essentially we'll be trying to find the path
   of steepest descent!
   each hidden unit can affect many output units. thus, we'll have to
   combine many separate effects on the error in an informative way. our
   strategy will be one of id145. once we have the error
   derivatives for one layer of hidden units, we'll use them to compute
   the error derivatives for the activities of the layer below. and once
   we find the error derivatives for the activities of the hidden units,
   it's quite easy to get the error derivatives for the weights leading
   into a hidden unit. we'll redefine some notation for ease of discussion
   and refer to the following diagram:
   id26 diagram

    reference diagram for the derivation of the id26 algorithm
   the subscript we use will refer to the layer of the neuron. the
   symbol y will refer to the activity of a neuron, as usual. similarly
   the symbol z will refer to the logit of a neuron. we start by taking a
   look at the base case of the id145 problem, the error
   function derivatives at the output layer:
   [nikhil-eq8.png]
   now we tackle the inductive step. let's presume we have the error
   derivatives for layer j. we now aim to calculate the error derivatives
   for the layer below it, layer i. to do so, we must accumulate
   information for how the output of a neuron in layer i affects the
   logits of every neuron in layer j. this can be done as follows, using
   the fact that the partial derivative of the logit with respect to the
   incoming output data from the layer beneath is merely the weight of the
   connection w[ij]:
   [nikhil-eq9.png]
   now we can use the following to complete the inductive step:
   [nikhil-eq10.png]
   combining these two together, we can finally express the partial
   derivatives of layer i in terms of the partial derivatives of layer j.
   [nikhil-eq11.png]
   then once we've gone through the whole id145 routine,
   having filled up the table appropriately with all of our partial
   derivatives (of the error function with respect to the hidden unit
   activities), we can then determine how the error changes with respect
   to the weights. this gives us how to modify the weights after each
   training example:
   [nikhil-eq12.png]
   in order to do id26 with batching of training examples, we
   merely sum up the partial derivatives over all the training examples in
   the batch. this gives us the following modification formula:
   [nikhil-eq13.png]
   we have succeeded in deriving the id26 algorithm for a
   feed-forward neural net utilizing sigmoidal neurons!
   the problem of overfitting
   now let's say you decide you're very excited about deep learning and so
   you want to try to train a neural network of your own to identify
   objects in an image. you know this is a complicated problem so you use
   a huge neural network (let's say 20 layers) and you have 1,000 training
   examples. you train your neural network using the algorithm we
   describe, but something's clearly wrong. your neural net performs
   virtually perfectly on your training examples, but when you put it in
   practice, it performs very poorly! what's going on here?
   the problem we've encountered is called overfitting, and it happens
   when you have way too many parameters in your model and not enough
   training data. to visualize this, let's consider the figure below,
   where you want to fit a model to the data points:
   overfitting

           an example that illustrates the concept of overfitting
   which curve would you trust? the line which gets almost no training
   example exactly? or the complicated curve that hits every single point
   in the training set? most likely you would trust the linear fit instead
   of the complicated curve because it seems less contrived. this
   situation is analogous to the neural network we trained. we have way
   too many parameters, over 100 trillion trillion (or 100 septillion)
   parameters. it's no wonder that we're overfitting!
   so how do we prevent overfitting? the two simplest ways are:
   1) limit the connectivities of neurons in your model. architecting a
   good neural network requires a lot of experience and intuition, and it
   boils down to giving your model freedom to discover relationships while
   also constraining it so it doesn't overfit.
   2) adding more training examples! often times you can cleverly add
   amplify your existing training set (changing illumination, applying
   shifts and other transformations, etc.).
   there are more sophisticated methods of training that try to directly
   solve overfitting such as including a dropout layers/neurons, but these
   methods are beyond the scope of this article
   conclusions
   we've covered a lot of ground, but there's still a lot more that's
   going on in deep learning research. in future articles, i will probably
   talk more about different kinds of neural architectures (convolutional
   networks, soft max layers, etc.). i'll probably also write an article
   about how to train your own neural network using some of the awesome
   open source libraries out there, such as caffe (which allows you to gpu
   accelerate the training of neural networks). those of you who are
   interested in pursuing deep learning further, please get in touch! i
   love talking about new ideas and projects.
   author: [40]nikhil buduma blog publishes a collection of adventures,
   thoughts, and random tidbits from an mit computer science student.
   original:
   [41]http://nikhilbuduma.com/2014/12/29/deep-learning-in-a-nutshell/
   related:
     * [42]how deep learning analytics mimic the mind
     * [43]geoff hinton ama: neural networks, the brain, and machine
       learning
     * [44]research leaders on data science and big data key trends, top
       papers
         ______________________________________________________________

       [45][prv.gif] previous post
       [46]next post [nxt.gif]
         ______________________________________________________________

top stories past 30 days

                                             most popular
         1. [47]another 10 free must-read books for machine learning and
            data science
         2. [48]9 must-have skills you need to become a data scientist,
            updated
         3. [49]who is a typical data scientist in 2019?
         4. [50]the pareto principle for data scientists
         5. [51]what no one will tell you about data science job
            applications
         6. [52]19 inspiring women in ai, big data, data science, machine
            learning
         7. [53]my favorite mind-blowing machine learning/ai breakthroughs

                                             most shared
         1. [54]id158s optimization using genetic
            algorithm with python
         2. [55]who is a typical data scientist in 2019?
         3. [56]8 reasons why you should get a microsoft azure
            certification
         4. [57]the pareto principle for data scientists
         5. [58]r vs python for data visualization
         6. [59]how to work in data science, ai, big data
         7. [60]the deep learning toolset     an overview

[61]latest news
          + [62]download your datax guide to ai in marketing
          + [63]kdnuggets offer: save 20% on strata in london
          + [64]training a champion: building deep neural nets for big ...
          + [65]building a recommender system
          + [66]predict age and gender using convolutional neural netwo...
          + [67]top tweets, mar 27     apr 02: here is a great ex...

more recent stories
          + [68]top tweets, mar 27     apr 02: here is a great explanat...
          + [69]odsc east is selling out; odsc india announced
          + [70]accelerate ai and data science career expo, may 4, 2019
          + [71]grow your data career at datasciencego, san diego, sep
            27-29
          + [72]getting started with nlp using the pytorch framework
          + [73]how to diy your data science education
          + [74]top 8 data science use cases in gaming
          + [75]kdnuggets 19:n13, apr 3: top 10 data scientist coding
            mista...
          + [76]make better data-driven business decisions
          + [77]top stories, mar 25-31: r vs python for data
            visualization; th...
          + [78]two predictive analytics world events in europe this fall
          + [79]7 qualities your big data visualization tools absolutely
            must ...
          + [80]yeshiva university: tenure-track faculty in ai and machine
            lea...
          + [81]which face is real?
          + [82]yeshiva university: program director / tenure track
            faculty me...
          + [83]top 10 coding mistakes made by data scientists
          + [84]uber   s case study at paw industry 4.0: machine learning
            ...
          + [85]xai     a data scientist   s mouthpiece
          + [86]what does gpt-2 think about the ai arms race?
          + [87]openclassrooms: data freelance online course creator
            [telecomm...
       [88]kdnuggets home    [89]news    [90]2015    [91]jan    [92]opinions,
       interviews, reports    deep learning in a nutshell     what it is, how
       it works, why care? ( [93]15:n02 )
          2019 kdnuggets. [94]about kdnuggets.  [95]privacy policy.
       [96]terms of service

       [97]subscribe to kdnuggets news
       [98][tw_c48.png] [99]facebook [100]linkedin
       x
       [envelope.png] [101]get kdnuggets, a leading newsletter on ai, data
       science, and machine learning
       email: ______________________________
       sign up
       leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2015/01/deep-learning-explanation-what-how-why.html/feed
   5. https://www.kdnuggets.com/2015/01/fundamental-methods-data-science-classification-regression-similarity-matching.html
   6. https://www.kdnuggets.com/2015/01/december-analytics-big-data-science-company-activity.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2015/index.html
  28. https://www.kdnuggets.com/2015/01/index.html
  29. https://www.kdnuggets.com/2015/01/opinions-interviews.html
  30. https://www.kdnuggets.com/2015/n02.html
  31. https://www.kdnuggets.com/2015/01/fundamental-methods-data-science-classification-regression-similarity-matching.html
  32. https://www.kdnuggets.com/2015/01/december-analytics-big-data-science-company-activity.html
  33. https://www.kdnuggets.com/tag/brain
  34. https://www.kdnuggets.com/tag/deep-learning
  35. https://www.kdnuggets.com/tag/deepmind
  36. https://www.kdnuggets.com/tag/neural-networks
  37. https://www.kdnuggets.com/tag/nikhil-buduma
  38. http://www.wired.com/2014/07/google_brain/
  39. http://deepmind.com/
  40. http://nikhilbuduma.com/
  41. http://nikhilbuduma.com/2014/12/29/deep-learning-in-a-nutshell/
  42. https://www.kdnuggets.com/2014/03/how-deep-learning-analytics-mimic-mind.html
  43. https://www.kdnuggets.com/2014/12/geoff-hinton-ama-neural-networks-brain-machine-learning.html
  44. https://www.kdnuggets.com/2015/01/research-leaders-data-science-big-data-key-trends-top-papers.html
  45. https://www.kdnuggets.com/2015/01/fundamental-methods-data-science-classification-regression-similarity-matching.html
  46. https://www.kdnuggets.com/2015/01/december-analytics-big-data-science-company-activity.html
  47. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  48. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  49. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  50. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  51. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  52. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  53. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  54. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  55. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  56. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  57. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  58. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  59. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  60. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  61. https://www.kdnuggets.com/news/index.html
  62. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  63. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  64. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  65. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  66. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  67. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  68. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  69. https://www.kdnuggets.com/2019/04/odsc-east-selling-out-india-announced.html
  70. https://www.kdnuggets.com/jobs/19/04-03-accelerate-ai-data-science-career-expo-2019.html
  71. https://www.kdnuggets.com/2019/04/formulated-data-career-datasciencego-san-diego.html
  72. https://www.kdnuggets.com/2019/04/nlp-pytorch.html
  73. https://www.kdnuggets.com/2019/04/diy-your-data-science-education.html
  74. https://www.kdnuggets.com/2019/04/top-8-data-science-use-cases-gaming.html
  75. https://www.kdnuggets.com/2019/n13.html
  76. https://www.kdnuggets.com/2019/04/psu-make-better-data-driven-business-decisions.html
  77. https://www.kdnuggets.com/2019/04/top-news-week-0325-0331.html
  78. https://www.kdnuggets.com/2019/04/paw-two-predictive-analytics-world-events-europe-fall.html
  79. https://www.kdnuggets.com/2019/04/7-qualities-big-data-visualization-tools.html
  80. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-faculty-ai-machine-learning.html
  81. https://www.kdnuggets.com/2019/04/which-face-real-stylegan.html
  82. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-program-director-faculty-artificial-intelligence-machine-learning.html
  83. https://www.kdnuggets.com/2019/04/top-10-coding-mistakes-data-scientists.html
  84. https://www.kdnuggets.com/2019/04/paw-uber-case-study-machine-learning-enforce-mobile-performance.html
  85. https://www.kdnuggets.com/2019/04/xai-data-scientist.html
  86. https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html
  87. https://www.kdnuggets.com/jobs/19/04-01-openclassrooms-data-freelance-online-course-creator-b.html
  88. https://www.kdnuggets.com/
  89. https://www.kdnuggets.com/news/index.html
  90. https://www.kdnuggets.com/2015/index.html
  91. https://www.kdnuggets.com/2015/01/index.html
  92. https://www.kdnuggets.com/2015/01/opinions-interviews.html
  93. https://www.kdnuggets.com/2015/n02.html
  94. https://www.kdnuggets.com/about/index.html
  95. https://www.kdnuggets.com/news/privacy-policy.html
  96. https://www.kdnuggets.com/terms-of-service.html
  97. https://www.kdnuggets.com/news/subscribe.html
  98. https://twitter.com/kdnuggets
  99. https://facebook.com/kdnuggets
 100. https://www.linkedin.com/groups/54257
 101. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
 103. https://www.kdnuggets.com/
 104. https://www.kdnuggets.com/
