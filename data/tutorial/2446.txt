   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 3 kernel
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]data-analysis-and-machine-learning-projects
    2. [10]example-data-science-notebook

an example machine learning notebook[11]  

notebook by [12]randal s. olson[13]  

supported by [14]jason h. moore[15]  

[16]university of pennsylvania institute for bioinformatics[17]  

   it is recommended to [18]view this notebook in nbviewer for the best
   viewing experience.

   you can also [19]execute the code in this notebook on binder - no local
   installation required.

table of contents[20]  

    1. [21]introduction
    2. [22]license
    3. [23]required libraries
    4. [24]the problem domain
    5. [25]step 1: answering the question
    6. [26]step 2: checking the data
    7. [27]step 3: tidying the data
          + [28]bonus: testing our data
    8. [29]step 4: exploratory analysis
    9. [30]step 5: classification
          + [31]cross-validation
          + [32]parameter tuning
   10. [33]step 6: reproducibility
   11. [34]conclusions
   12. [35]further reading
   13. [36]acknowledgements

introduction[37]  

   [38][ go back to the top ]

   in the time it took you to read this sentence, terabytes of data have
   been collectively generated across the world     more data than any of us
   could ever hope to process, much less make sense of, on the machines
   we're using to read this notebook.

   in response to this massive influx of data, the field of data science
   has come to the forefront in the past decade. cobbled together by
   people from a diverse array of fields     statistics, physics, computer
   science, design, and many more     the field of data science represents
   our collective desire to understand and harness the abundance of data
   around us to build a better world.

   in this notebook, i'm going to go over a basic python data analysis
   pipeline from start to finish to show you what a typical data science
   workflow looks like.

   in addition to providing code examples, i also hope to imbue in you a
   sense of good practices so you can be a more effective     and more
   collaborative     data scientist.

   i will be following along with the data analysis checklist from [39]the
   elements of data analytic style, which i strongly recommend reading as
   a free and quick guidebook to performing outstanding data analysis.

   this notebook is intended to be a public resource. as such, if you see
   any glaring inaccuracies or if a critical topic is missing, please feel
   free to point it out or (preferably) submit a pull request to improve
   the notebook.

license[40]  

   [41][ go back to the top ]

   please see the [42]repository readme file for the licenses and usage
   terms for the instructional material and code in this notebook. in
   general, i have licensed this material so that it is as widely usable
   and shareable as possible.

required libraries[43]  

   [44][ go back to the top ]

   if you don't have python on your computer, you can use the [45]anaconda
   python distribution to install most of the python packages you need.
   anaconda provides a simple double-click installer for your convenience.

   this notebook uses several python packages that come standard with the
   anaconda python distribution. the primary libraries that we'll be using
   are:
     * numpy: provides a fast numerical array structure and helper
       functions.
     * pandas: provides a dataframe structure to store data in memory and
       work with it easily and efficiently.
     * scikit-learn: the essential machine learning package in python.
     * matplotlib: basic plotting library in python; most other python
       plotting libraries are built on top of it.
     * seaborn: advanced statistical plotting library.
     * watermark: a jupyter notebook extension for printing timestamps,
       version numbers, and hardware information.

   to make sure you have all of the packages you need, install them with
   conda:
conda install numpy pandas scikit-learn matplotlib seaborn

conda install -c conda-forge watermark


   conda may ask you to update some of them if you don't have the most
   recent version. allow it to do so.

   note: i will not be providing support for people trying to run this
   notebook outside of the anaconda python distribution.

the problem domain[46]  

   [47][ go back to the top ]

   for the purposes of this exercise, let's pretend we're working for a
   startup that just got funded to create a smartphone app that
   automatically identifies species of flowers from pictures taken on the
   smartphone. we're working with a moderately-sized team of data
   scientists and will be building part of the data analysis pipeline for
   this app.

   we've been tasked by our company's head of data science to create a
   demo machine learning model that takes four measurements from the
   flowers (sepal length, sepal width, petal length, and petal width) and
   identifies the species based on those measurements alone.

   [petal_sepal.jpg]

   we've been given a [48]data set from our field researchers to develop
   the demo, which only includes measurements for three types of iris
   flowers:

iris setosa[49]  

   [iris_setosa.jpg]

iris versicolor[50]  

   [iris_versicolor.jpg]

iris virginica[51]  

   [iris_virginica.jpg]

   the four measurements we're using currently come from hand-measurements
   by the field researchers, but they will be automatically measured by an
   image processing model in the future.

   note: the data set we're working with is the famous [52]iris data set    
   included with this notebook     which i have modified slightly for
   demonstration purposes.

step 1: answering the question[53]  

   [54][ go back to the top ]

   the first step to any data analysis project is to define the question
   or problem we're looking to solve, and to define a measure (or set of
   measures) for our success at solving that task. the data analysis
   checklist has us answer a handful of questions to accomplish that, so
   let's work through those questions.

     did you specify the type of data analytic question (e.g.
     exploration, association causality) before touching the data?

   we're trying to classify the species (i.e., class) of the flower based
   on four measurements that we're provided: sepal length, sepal width,
   petal length, and petal width.

     did you define the metric for success before beginning?

   let's do that now. since we're performing classification, we can use
   [55]accuracy     the fraction of correctly classified flowers     to
   quantify how well our model is performing. our company's head of data
   has told us that we should achieve at least 90% accuracy.

     did you understand the context for the question and the scientific
     or business application?

   we're building part of a data analysis pipeline for a smartphone app
   that will be able to classify the species of flowers from pictures
   taken on the smartphone. in the future, this pipeline will be connected
   to another pipeline that automatically measures from pictures the
   traits we're using to perform this classification.

     did you record the experimental design?

   our company's head of data has told us that the field researchers are
   hand-measuring 50 randomly-sampled flowers of each species using a
   standardized methodology. the field researchers take pictures of each
   flower they sample from pre-defined angles so the measurements and
   species can be confirmed by the other field researchers at a later
   point. at the end of each day, the data is compiled and stored on a
   private company github repository.

     did you consider whether the question could be answered with the
     available data?

   the data set we currently have is only for three types of iris flowers.
   the model built off of this data set will only work for those iris
   flowers, so we will need more data to create a general flower
   classifier.
     __________________________________________________________________

   notice that we've spent a fair amount of time working on the problem
   without writing a line of code or even looking at the data.

   thinking about and documenting the problem we're working on is an
   important step to performing effective data analysis that often goes
   overlooked. don't skip it.

step 2: checking the data[56]  

   [57][ go back to the top ]

   the next step is to look at the data we're working with. even curated
   data sets from the government can have errors in them, and it's vital
   that we spot these errors before investing too much time in our
   analysis.

   generally, we're looking to answer the following questions:
     * is there anything wrong with the data?
     * are there any quirks with the data?
     * do i need to fix or remove any of the data?

   let's start by reading the data into a pandas dataframe.
   in [1]:
import pandas as pd

iris_data = pd.read_csv('iris-data.csv')
iris_data.head()

   out[1]:
     sepal_length_cm sepal_width_cm petal_length_cm petal_width_cm    class
   0 5.1             3.5            1.4             0.2            iris-setosa
   1 4.9             3.0            1.4             0.2            iris-setosa
   2 4.7             3.2            1.3             0.2            iris-setosa
   3 4.6             3.1            1.5             0.2            iris-setosa
   4 5.0             3.6            1.4             0.2            iris-setosa

   we're in luck! the data seems to be in a usable format.

   the first row in the data file defines the column headers, and the
   headers are descriptive enough for us to understand what each column
   represents. the headers even give us the units that the measurements
   were recorded in, just in case we needed to know at a later point in
   the project.

   each row following the first row represents an entry for a flower: four
   measurements and one class, which tells us the species of the flower.

   one of the first things we should look for is missing data. thankfully,
   the field researchers already told us that they put a 'na' into the
   spreadsheet when they were missing a measurement.

   we can tell pandas to automatically identify missing values if it knows
   our missing value marker.
   in [2]:
iris_data = pd.read_csv('iris-data.csv', na_values=['na'])

   voil  ! now pandas knows to treat rows with 'na' as missing values.

   next, it's always a good idea to look at the distribution of our data    
   especially the outliers.

   let's start by printing out some summary statistics about the data set.
   in [3]:
iris_data.describe()

   out[3]:
         sepal_length_cm sepal_width_cm petal_length_cm petal_width_cm
   count 150.000000      150.000000     150.000000      145.000000
   mean  5.644627        3.054667       3.758667        1.236552
    std  1.312781        0.433123       1.764420        0.755058
    min  0.055000        2.000000       1.000000        0.100000
    25%  5.100000        2.800000       1.600000        0.400000
    50%  5.700000        3.000000       4.350000        1.300000
    75%  6.400000        3.300000       5.100000        1.800000
    max  7.900000        4.400000       6.900000        2.500000

   we can see several useful values from this table. for example, we see
   that five petal_width_cm entries are missing.

   if you ask me, though, tables like this are rarely useful unless we
   know that our data should fall in a particular range. it's usually
   better to visualize the data in some way. visualization makes outliers
   and errors immediately stand out, whereas they might go unnoticed in a
   large table of numbers.

   since we know we're going to be plotting in this section, let's set up
   the notebook so we can plot inside of it.
   in [4]:
# this line tells the notebook to show plots inside of the notebook
%matplotlib inline

import matplotlib.pyplot as plt
import seaborn as sb

   next, let's create a scatterplot matrix. scatterplot matrices plot the
   distribution of each column along the diagonal, and then plot a
   scatterplot matrix for the combination of each variable. they make for
   an efficient tool to look for errors in our data.

   we can even have the plotting package color each entry by its class to
   look for trends within the classes.
   in [5]:
# we have to temporarily drop the rows with 'na' values
# because the seaborn plotting function does not know
# what to do with them
sb.pairplot(iris_data.dropna(), hue='class')
;

   out[5]:
''

   [noyaaaaasuvork5cyii= ]

   from the scatterplot matrix, we can already see some issues with the
   data set:
    1. there are five classes when there should only be three, meaning
       there were some coding errors.
    2. there are some clear outliers in the measurements that may be
       erroneous: one sepal_width_cm entry for iris-setosa falls well
       outside its normal range, and several sepal_length_cm entries for
       iris-versicolor are near-zero for some reason.
    3. we had to drop those rows with missing values.

   in all of these cases, we need to figure out what to do with the
   erroneous data. which takes us to the next step...

step 3: tidying the data[58]  

   [59][ go back to the top ]

   now that we've identified several errors in the data set, we need to
   fix them before we proceed with the analysis.

   let's walk through the issues one-by-one.

     there are five classes when there should only be three, meaning
     there were some coding errors.

   after talking with the field researchers, it sounds like one of them
   forgot to add iris- before their iris-versicolor entries. the other
   extraneous class, iris-setossa, was simply a typo that they forgot to
   fix.

   let's use the dataframe to fix these errors.
   in [6]:
iris_data.loc[iris_data['class'] == 'versicolor', 'class'] = 'iris-versicolor'
iris_data.loc[iris_data['class'] == 'iris-setossa', 'class'] = 'iris-setosa'

iris_data['class'].unique()

   out[6]:
array(['iris-setosa', 'iris-versicolor', 'iris-virginica'], dtype=object)

   much better! now we only have three class types. imagine how
   embarrassing it would've been to create a model that used the wrong
   classes.

     there are some clear outliers in the measurements that may be
     erroneous: one sepal_width_cm entry for iris-setosa falls well
     outside its normal range, and several sepal_length_cm entries for
     iris-versicolor are near-zero for some reason.

   fixing outliers can be tricky business. it's rarely clear whether the
   outlier was caused by measurement error, recording the data in improper
   units, or if the outlier is a real anomaly. for that reason, we should
   be judicious when working with outliers: if we decide to exclude any
   data, we need to make sure to document what data we excluded and
   provide solid reasoning for excluding that data. (i.e., "this data
   didn't fit my hypothesis" will not stand peer review.)

   in the case of the one anomalous entry for iris-setosa, let's say our
   field researchers know that it's impossible for iris-setosa to have a
   sepal width below 2.5 cm. clearly this entry was made in error, and
   we're better off just scrapping the entry than spending hours finding
   out what happened.
   in [7]:
# this line drops any 'iris-setosa' rows with a separal width less than 2.5 cm
iris_data = iris_data.loc[(iris_data['class'] != 'iris-setosa') | (iris_data['se
pal_width_cm'] >= 2.5)]
iris_data.loc[iris_data['class'] == 'iris-setosa', 'sepal_width_cm'].hist()
;

   out[7]:
''

   [fr3k9yngnt7y97f6jjvg9dofpf+lf2obkw3zjxwunmm4zszq58qfcst93ccripgmq39b5b
   hmfcqifj0ibu+ner9wruq8w+qrwap8jj8m+d4ftaafi4gni2lufw+msczxa5lxuebrefe0c
   at4xcz8wofyzqnvjohpax9mtcthfhvch+zkxx2bk32ncq545aqkfxohhhfl0rcni1usirg4
   jakyi1usirg4jakyi1usirg4jakyi1usivlf8vzbkg9ozk8aaaaasuvork5cyii= ]

   excellent! now all of our iris-setosa rows have a sepal width greater
   than 2.5.

   the next data issue to address is the several near-zero sepal lengths
   for the iris-versicolor rows. let's take a look at those rows.
   in [8]:
iris_data.loc[(iris_data['class'] == 'iris-versicolor') &
              (iris_data['sepal_length_cm'] < 1.0)]

   out[8]:
   sepal_length_cm sepal_width_cm petal_length_cm petal_width_cm      class
77 0.067           3.0            5.0             1.7            iris-versicolor
78 0.060           2.9            4.5             1.5            iris-versicolor
79 0.057           2.6            3.5             1.0            iris-versicolor
80 0.055           2.4            3.8             1.1            iris-versicolor
81 0.055           2.4            3.7             1.0            iris-versicolor

   how about that? all of these near-zero sepal_length_cm entries seem to
   be off by two orders of magnitude, as if they had been recorded in
   meters instead of centimeters.

   after some brief correspondence with the field researchers, we find
   that one of them forgot to convert those measurements to centimeters.
   let's do that for them.
   in [9]:
iris_data.loc[(iris_data['class'] == 'iris-versicolor') &
              (iris_data['sepal_length_cm'] < 1.0),
              'sepal_length_cm'] *= 100.0

iris_data.loc[iris_data['class'] == 'iris-versicolor', 'sepal_length_cm'].hist()
;

   out[9]:
''

   [aaur2uqaaaaasuvork5cyii= ]

   phew! good thing we fixed those outliers. they could've really thrown
   our analysis off.

     we had to drop those rows with missing values.

   let's take a look at the rows with missing values:
   in [10]:
iris_data.loc[(iris_data['sepal_length_cm'].isnull()) |
              (iris_data['sepal_width_cm'].isnull()) |
              (iris_data['petal_length_cm'].isnull()) |
              (iris_data['petal_width_cm'].isnull())]

   out[10]:
      sepal_length_cm sepal_width_cm petal_length_cm petal_width_cm    class
   7  5.0             3.4            1.5             nan            iris-setosa
   8  4.4             2.9            1.4             nan            iris-setosa
   9  4.9             3.1            1.5             nan            iris-setosa
   10 5.4             3.7            1.5             nan            iris-setosa
   11 4.8             3.4            1.6             nan            iris-setosa

   it's not ideal that we had to drop those rows, especially considering
   they're all iris-setosa entries. since it seems like the missing data
   is systematic     all of the missing values are in the same column for
   the same iris type     this error could potentially bias our analysis.

   one way to deal with missing data is mean imputation: if we know that
   the values for a measurement fall in a certain range, we can fill in
   empty values with the average of that measurement.

   let's see if we can do that here.
   in [11]:
iris_data.loc[iris_data['class'] == 'iris-setosa', 'petal_width_cm'].hist()
;

   out[11]:
''

   [pqu7puctszqaaaabjru5erkjggg== ]

   most of the petal widths for iris-setosa fall within the 0.2-0.3 range,
   so let's fill in these entries with the average measured petal width.
   in [12]:
average_petal_width = iris_data.loc[iris_data['class'] == 'iris-setosa', 'petal_
width_cm'].mean()

iris_data.loc[(iris_data['class'] == 'iris-setosa') &
              (iris_data['petal_width_cm'].isnull()),
              'petal_width_cm'] = average_petal_width

iris_data.loc[(iris_data['class'] == 'iris-setosa') &
              (iris_data['petal_width_cm'] == average_petal_width)]

   out[12]:
      sepal_length_cm sepal_width_cm petal_length_cm petal_width_cm    class
   7  5.0             3.4            1.5             0.25           iris-setosa
   8  4.4             2.9            1.4             0.25           iris-setosa
   9  4.9             3.1            1.5             0.25           iris-setosa
   10 5.4             3.7            1.5             0.25           iris-setosa
   11 4.8             3.4            1.6             0.25           iris-setosa
   in [13]:
iris_data.loc[(iris_data['sepal_length_cm'].isnull()) |
              (iris_data['sepal_width_cm'].isnull()) |
              (iris_data['petal_length_cm'].isnull()) |
              (iris_data['petal_width_cm'].isnull())]

   out[13]:
   sepal_length_cm sepal_width_cm petal_length_cm petal_width_cm class

   great! now we've recovered those rows and no longer have missing data
   in our data set.

   note: if you don't feel comfortable imputing your data, you can drop
   all rows with missing data with the dropna() call:
iris_data.dropna(inplace=true)


   after all this hard work, we don't want to repeat this process every
   time we work with the data set. let's save the tidied data file as a
   separate file and work directly with that data file from now on.
   in [14]:
iris_data.to_csv('iris-data-clean.csv', index=false)

iris_data_clean = pd.read_csv('iris-data-clean.csv')

   now, let's take a look at the scatterplot matrix now that we've tidied
   the data.
   in [15]:
sb.pairplot(iris_data_clean, hue='class')
;

   out[15]:
''

   [azqyfhvf7dw0aaaaaelftksuqmcc ]

   of course, i purposely inserted numerous errors into this data set to
   demonstrate some of the many possible scenarios you may face while
   tidying your data.

   the general takeaways here should be:
     * make sure your data is encoded properly
     * make sure your data falls within the expected range, and use domain
       knowledge whenever possible to define that expected range
     * deal with missing data in one way or another: replace it if you can
       or drop it
     * never tidy your data manually because that is not easily
       reproducible
     * use code as a record of how you tidied your data
     * plot everything you can about the data at this stage of the
       analysis so you can visually confirm everything looks correct

bonus: testing our data[60]  

   [61][ go back to the top ]

   at scipy 2015, i was exposed to a great idea: we should test our data.
   just how we use unit tests to verify our expectations from code, we can
   similarly set up unit tests to verify our expectations about a data
   set.

   we can quickly test our data using assert statements: we assert that
   something must be true, and if it is, then nothing happens and the
   notebook continues running. however, if our assertion is wrong, then
   the notebook stops running and brings it to our attention. for example,
assert 1 == 2

   will raise an assertionerror and stop execution of the notebook because
   the assertion failed.

   let's test a few things that we know about our data set now.
   in [16]:
# we know that we should only have three classes
assert len(iris_data_clean['class'].unique()) == 3

   in [17]:
# we know that sepal lengths for 'iris-versicolor' should never be below 2.5 cm
assert iris_data_clean.loc[iris_data_clean['class'] == 'iris-versicolor', 'sepal
_length_cm'].min() >= 2.5

   in [18]:
# we know that our data set should have no missing measurements
assert len(iris_data_clean.loc[(iris_data_clean['sepal_length_cm'].isnull()) |
                               (iris_data_clean['sepal_width_cm'].isnull()) |
                               (iris_data_clean['petal_length_cm'].isnull()) |
                               (iris_data_clean['petal_width_cm'].isnull())]) ==
 0

   and so on. if any of these expectations are violated, then our analysis
   immediately stops and we have to return to the tidying stage.

step 4: exploratory analysis[62]  

   [63][ go back to the top ]

   now after spending entirely too much time tidying our data, we can
   start analyzing it!

   exploratory analysis is the step where we start delving deeper into the
   data set beyond the outliers and errors. we'll be looking to answer
   questions such as:
     * how is my data distributed?
     * are there any correlations in my data?
     * are there any confounding factors that explain these correlations?

   this is the stage where we plot all the data in as many ways as
   possible. create many charts, but don't bother making them pretty    
   these charts are for internal use.

   let's return to that scatterplot matrix that we used earlier.
   in [19]:
sb.pairplot(iris_data_clean)
;

   out[19]:
''

   [t6wdbs5svc+e2azs3lbdxwoaqw5epizmttjzgvgkecgkpqk+rujvqcjyksd7yfmtsmaoqm
   xiekjym2mzs9exgdl1qzrribgrxpbzm1rd3xiwkt+dvvvmyhytxt3r4otydmzmzpro14yz2
   zmzmz9x0mwmzmzmfudj8fmzmzm1necbjuzmzlz33esbgzmzmz951+rdxdp64ps8qaaaabjr
   u5erkjggg== ]

   our data is normally distributed for the most part, which is great news
   if we plan on using any modeling methods that assume the data is
   normally distributed.

   there's something strange going on with the petal measurements. maybe
   it's something to do with the different iris types. let's color code
   the data by the class again to see if that clears things up.
   in [20]:
sb.pairplot(iris_data_clean, hue='class')
;

   out[20]:
''

   [azqyfhvf7dw0aaaaaelftksuqmcc ]

   sure enough, the strange distribution of the petal measurements exist
   because of the different species. this is actually great news for our
   classification task since it means that the petal measurements will
   make it easy to distinguish between iris-setosa and the other iris
   types.

   distinguishing iris-versicolor and iris-virginica will prove more
   difficult given how much their measurements overlap.

   there are also correlations between petal length and petal width, as
   well as sepal length and sepal width. the field biologists assure us
   that this is to be expected: longer flower petals also tend to be
   wider, and the same applies for sepals.

   we can also make violin plots of the data to compare the measurement
   distributions of the classes. violin plots contain the same information
   as [64]box plots, but also scales the box according to the density of
   the data.
   in [21]:
plt.figure(figsize=(10, 10))

for column_index, column in enumerate(iris_data_clean.columns):
    if column == 'class':
        continue
    plt.subplot(2, 2, column_index + 1)
    sb.violinplot(x='class', y=column, data=iris_data_clean)

   [p6xqnegr2pyaaaaasuvork5cyii= ]

   enough flirting with the data. let's get to modeling.

step 5: classification[65]  

   [66][ go back to the top ]

   wow, all this work and we still haven't modeled the data!

   as tiresome as it can be, tidying and exploring our data is a vital
   component to any data analysis. if we had jumped straight to the
   modeling step, we would have created a faulty classification model.

   remember: bad data leads to bad models. always check your data first.
     __________________________________________________________________

   assured that our data is now as clean as we can make it     and armed
   with some cursory knowledge of the distributions and relationships in
   our data set     it's time to make the next big step in our analysis:
   splitting the data into training and testing sets.

   a training set is a random subset of the data that we use to train our
   models.

   a testing set is a random subset of the data (mutually exclusive from
   the training set) that we use to validate our models on unforseen data.

   especially in sparse data sets like ours, it's easy for models to
   overfit the data: the model will learn the training set so well that it
   won't be able to handle most of the cases it's never seen before. this
   is why it's important for us to build the model with the training set,
   but score it with the testing set.

   note that once we split the data into a training and testing set, we
   should treat the testing set like it no longer exists: we cannot use
   any information from the testing set to build our model or else we're
   cheating.

   let's set up our data first.
   in [22]:
iris_data_clean = pd.read_csv('iris-data-clean.csv')

# we're using all four measurements as inputs
# note that scikit-learn expects each entry to be a list of values, e.g.,
# [ [val1, val2, val3],
#   [val1, val2, val3],
#   ... ]
# such that our input data set is represented as a list of lists

# we can extract the data in this format from pandas like this:
all_inputs = iris_data_clean[['sepal_length_cm', 'sepal_width_cm',
                             'petal_length_cm', 'petal_width_cm']].values

# similarly, we can extract the class labels
all_labels = iris_data_clean['class'].values

# make sure that you don't mix up the order of the entries
# all_inputs[5] inputs should correspond to the class in all_labels[5]

# here's what a subset of our inputs looks like:
all_inputs[:5]

   out[22]:
array([[ 5.1,  3.5,  1.4,  0.2],
       [ 4.9,  3. ,  1.4,  0.2],
       [ 4.7,  3.2,  1.3,  0.2],
       [ 4.6,  3.1,  1.5,  0.2],
       [ 5. ,  3.6,  1.4,  0.2]])

   now our data is ready to be split.
   in [23]:
from sklearn.model_selection import train_test_split

(training_inputs,
 testing_inputs,
 training_classes,
 testing_classes) = train_test_split(all_inputs, all_labels, test_size=0.25, ran
dom_state=1)

   with our data split, we can start fitting models to our data. our
   company's head of data is all about decision tree classifiers, so let's
   start with one of those.

   decision tree classifiers are incredibly simple in theory. in their
   simplest form, decision tree classifiers ask a series of yes/no
   questions about the data     each time getting closer to finding out the
   class of each entry     until they either classify the data set perfectly
   or simply can't differentiate a set of entries. think of it like a game
   of [67]twenty questions, except the computer is much, much better at
   it.

   here's an example decision tree classifier:

   [iris_dtc.png]

   notice how the classifier asks yes/no questions about the data    
   whether a certain feature is <= 1.75, for example     so it can
   differentiate the records. this is the essence of every decision tree.

   the nice part about decision tree classifiers is that they are
   scale-invariant, i.e., the scale of the features does not affect their
   performance, unlike many machine learning models. in other words, it
   doesn't matter if our features range from 0 to 1 or 0 to 1,000;
   decision tree classifiers will work with them just the same.

   there are several [68]parameters that we can tune for decision tree
   classifiers, but for now let's use a basic decision tree classifier.
   in [24]:
from sklearn.tree import decisiontreeclassifier

# create the classifier
decision_tree_classifier = decisiontreeclassifier()

# train the classifier on the training set
decision_tree_classifier.fit(training_inputs, training_classes)

# validate the classifier on the testing set using classification accuracy
decision_tree_classifier.score(testing_inputs, testing_classes)

   out[24]:
0.97368421052631582

   heck yeah! our model achieves 97% classification accuracy without much
   effort.

   however, there's a catch: depending on how our training and testing set
   was sampled, our model can achieve anywhere from 80% to 100% accuracy:
   in [25]:
model_accuracies = []

for repetition in range(1000):
    (training_inputs,
     testing_inputs,
     training_classes,
     testing_classes) = train_test_split(all_inputs, all_labels, test_size=0.25)

    decision_tree_classifier = decisiontreeclassifier()
    decision_tree_classifier.fit(training_inputs, training_classes)
    classifier_accuracy = decision_tree_classifier.score(testing_inputs, testing
_classes)
    model_accuracies.append(classifier_accuracy)

plt.hist(model_accuracies)
;

   out[25]:
''

   [ac40cbqbqg9xaaaaaelftksuqmcc ]

   it's obviously a problem that our model performs quite differently
   depending on the subset of the data it's trained on. this phenomenon is
   known as overfitting: the model is learning to classify the training
   set so well that it doesn't generalize and perform well on data it
   hasn't seen before.

cross-validation[69]  

   [70][ go back to the top ]

   this problem is the main reason that most data scientists perform
   k-fold cross-validation on their models: split the original data set
   into k subsets, use one of the subsets as the testing set, and the rest
   of the subsets are used as the training set. this process is then
   repeated k times such that each subset is used as the testing set
   exactly once.

   10-fold cross-validation is the most common choice, so let's use that
   here. performing 10-fold cross-validation on our data set looks
   something like this:

   (each square is an entry in our data set)
   in [26]:
import numpy as np
from sklearn.model_selection import stratifiedkfold

def plot_cv(cv, features, labels):
    masks = []
    for train, test in cv.split(features, labels):
        mask = np.zeros(len(labels), dtype=bool)
        mask[test] = 1
        masks.append(mask)

    plt.figure(figsize=(15, 15))
    plt.imshow(masks, interpolation='none', cmap='gray_r')
    plt.ylabel('fold')
    plt.xlabel('row #')

plot_cv(stratifiedkfold(n_splits=10), all_inputs, all_labels)

   [lzzk9lj+waaaabjru5erkjggg== ]

   you'll notice that we used stratified k-fold cross-validation in the
   code above. stratified k-fold keeps the class proportions the same
   across all of the folds, which is vital for maintaining a
   representative subset of our data set. (e.g., so we don't have 100%
   iris setosa entries in one of the folds.)

   we can perform 10-fold cross-validation on our model with the following
   code:
   in [27]:
from sklearn.model_selection import cross_val_score

decision_tree_classifier = decisiontreeclassifier()

# cross_val_score returns a list of the scores, which we can visualize
# to get a reasonable estimate of our classifier's performance
cv_scores = cross_val_score(decision_tree_classifier, all_inputs, all_labels, cv
=10)
plt.hist(cv_scores)
plt.title('average score: {}'.format(np.mean(cv_scores)))
;

   out[27]:
''

   [ancoyzrrxehraaaaaelftksuqmcc ]

   now we have a much more consistent rating of our classifier's general
   classification accuracy.

parameter tuning[71]  

   [72][ go back to the top ]

   every machine learning model comes with a variety of parameters to
   tune, and these parameters can be vitally important to the performance
   of our classifier. for example, if we severely limit the depth of our
   decision tree classifier:
   in [28]:
decision_tree_classifier = decisiontreeclassifier(max_depth=1)

cv_scores = cross_val_score(decision_tree_classifier, all_inputs, all_labels, cv
=10)
plt.hist(cv_scores)
plt.title('average score: {}'.format(np.mean(cv_scores)))
;

   out[28]:
''

   [d9qljqvseluwaaaaaelftksuqmcc ]

   the classification accuracy falls tremendously.

   therefore, we need to find a systematic method to discover the best
   parameters for our model and data set.

   the most common method for model parameter tuning is grid search. the
   idea behind grid search is simple: explore a range of parameters and
   find the best-performing parameter combination. focus your search on
   the best range of parameters, then repeat this process several times
   until the best parameters are discovered.

   let's tune our decision tree classifier. we'll stick to only two
   parameters for now, but it's possible to simultaneously explore dozens
   of parameters if we want.
   in [29]:
from sklearn.model_selection import gridsearchcv

decision_tree_classifier = decisiontreeclassifier()

parameter_grid = {'max_depth': [1, 2, 3, 4, 5],
                  'max_features': [1, 2, 3, 4]}

cross_validation = stratifiedkfold(n_splits=10)

grid_search = gridsearchcv(decision_tree_classifier,
                           param_grid=parameter_grid,
                           cv=cross_validation)

grid_search.fit(all_inputs, all_labels)
print('best score: {}'.format(grid_search.best_score_))
print('best parameters: {}'.format(grid_search.best_params_))

best score: 0.959731543624161
best parameters: {'max_depth': 3, 'max_features': 3}

   now let's visualize the grid search to see how the parameters interact.
   in [30]:
grid_visualization = grid_search.cv_results_['mean_test_score']
grid_visualization.shape = (5, 4)
sb.heatmap(grid_visualization, cmap='blues', annot=true)
plt.xticks(np.arange(4) + 0.5, grid_search.param_grid['max_features'])
plt.yticks(np.arange(5) + 0.5, grid_search.param_grid['max_depth'])
plt.xlabel('max_features')
plt.ylabel('max_depth')
;

   out[30]:
''

   [e8spzu8n4nwaaaaasuvork5cyii= ]

   now we have a better sense of the parameter space: we know that we need
   a max_depth of at least 2 to allow the decision tree to make more than
   a one-off decision.

   max_features doesn't really seem to make a big difference here as long
   as we have 2 of them, which makes sense since our data set has only 4
   features and is relatively easy to classify. (remember, one of our data
   set's classes was easily separable from the rest based on a single
   feature.)

   let's go ahead and use a broad grid search to find the best settings
   for a handful of parameters.
   in [31]:
decision_tree_classifier = decisiontreeclassifier()

parameter_grid = {'criterion': ['gini', 'id178'],
                  'splitter': ['best', 'random'],
                  'max_depth': [1, 2, 3, 4, 5],
                  'max_features': [1, 2, 3, 4]}

cross_validation = stratifiedkfold(n_splits=10)

grid_search = gridsearchcv(decision_tree_classifier,
                           param_grid=parameter_grid,
                           cv=cross_validation)

grid_search.fit(all_inputs, all_labels)
print('best score: {}'.format(grid_search.best_score_))
print('best parameters: {}'.format(grid_search.best_params_))

best score: 0.9664429530201343
best parameters: {'criterion': 'id178', 'max_depth': 3, 'max_features': 3, 'sp
litter': 'best'}

   now we can take the best classifier from the grid search and use that:
   in [32]:
decision_tree_classifier = grid_search.best_estimator_
decision_tree_classifier

   out[32]:
decisiontreeclassifier(class_weight=none, criterion='id178', max_depth=3,
            max_features=3, max_leaf_nodes=none, min_impurity_decrease=0.0,
            min_impurity_split=none, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=false, random_state=none, splitter='best')

   we can even visualize the decision tree with [73]graphviz to see how
   it's making the classifications:
   in [33]:
import sklearn.tree as tree
from sklearn.externals.six import stringio

with open('iris_dtc.dot', 'w') as out_file:
    out_file = tree.export_graphviz(decision_tree_classifier, out_file=out_file)

   [iris_dtc.png]

   (this classifier may look familiar from earlier in the notebook.)

   alright! we finally have our demo classifier. let's create some visuals
   of its performance so we have something to show our company's head of
   data.
   in [34]:
dt_scores = cross_val_score(decision_tree_classifier, all_inputs, all_labels, cv
=10)

sb.boxplot(dt_scores)
sb.stripplot(dt_scores, jitter=true, color='black')
;

   out[34]:
''

   [8f1nyqck+nzg8aaaaasuvork5cyii= ]

   id48m... that's a little boring by itself though. how about we compare
   another classifier to see how they perform?

   we already know from previous projects that id79 classifiers
   usually work better than individual id90. a common problem
   that id90 face is that they're prone to overfitting: they
   complexify to the point that they classify the training set
   near-perfectly, but fail to generalize to data they have not seen
   before.

   id79 classifiers work around that limitation by creating a
   whole bunch of id90 (hence "forest")     each trained on random
   subsets of training samples (drawn with replacement) and features
   (drawn without replacement)     and have the id90 work together
   to make a more accurate classification.

   let that be a lesson for us: even in machine learning, we get better
   results when we work together!

   let's see if a id79 classifier works better here.

   the great part about scikit-learn is that the training, testing,
   parameter tuning, etc. process is the same for all models, so we only
   need to plug in the new classifier.
   in [35]:
from sklearn.ensemble import randomforestclassifier

random_forest_classifier = randomforestclassifier()

parameter_grid = {'n_estimators': [10, 25, 50, 100],
                  'criterion': ['gini', 'id178'],
                  'max_features': [1, 2, 3, 4]}

cross_validation = stratifiedkfold(n_splits=10)

grid_search = gridsearchcv(random_forest_classifier,
                           param_grid=parameter_grid,
                           cv=cross_validation)

grid_search.fit(all_inputs, all_labels)
print('best score: {}'.format(grid_search.best_score_))
print('best parameters: {}'.format(grid_search.best_params_))

grid_search.best_estimator_

best score: 0.9664429530201343
best parameters: {'criterion': 'gini', 'max_features': 1, 'n_estimators': 25}

   out[35]:
randomforestclassifier(bootstrap=true, class_weight=none, criterion='gini',
            max_depth=none, max_features=1, max_leaf_nodes=none,
            min_impurity_decrease=0.0, min_impurity_split=none,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=1,
            oob_score=false, random_state=none, verbose=0,
            warm_start=false)

   now we can compare their performance:
   in [36]:
random_forest_classifier = grid_search.best_estimator_

rf_df = pd.dataframe({'accuracy': cross_val_score(random_forest_classifier, all_
inputs, all_labels, cv=10),
                       'classifier': ['id79'] * 10})
dt_df = pd.dataframe({'accuracy': cross_val_score(decision_tree_classifier, all_
inputs, all_labels, cv=10),
                      'classifier': ['decision tree'] * 10})
both_df = rf_df.append(dt_df)

sb.boxplot(x='classifier', y='accuracy', data=both_df)
sb.stripplot(x='classifier', y='accuracy', data=both_df, jitter=true, color='bla
ck')
;

   out[36]:
''

   [azikyjar1spoaaaaaelftksuqmcc ]

   how about that? they both seem to perform about the same on this data
   set. this is probably because of the limitations of our data set: we
   have only 4 features to make the classification, and id79
   classifiers excel when there's hundreds of possible features to look
   at. in other words, there wasn't much room for improvement with this
   data set.

step 6: reproducibility[74]  

   [75][ go back to the top ]

   ensuring that our work is reproducible is the last and     arguably    
   most important step in any analysis. as a rule, we shouldn't place much
   weight on a discovery that can't be reproduced. as such, if our
   analysis isn't reproducible, we might as well not have done it.

   notebooks like this one go a long way toward making our work
   reproducible. since we documented every step as we moved along, we have
   a written record of what we did and why we did it     both in text and
   code.

   beyond recording what we did, we should also document what software and
   hardware we used to perform our analysis. this typically goes at the
   top of our notebooks so our readers know what tools to use.

   [76]sebastian raschka created a handy [77]notebook tool for this:
   in [38]:
%watermark -a 'randal s. olson' -nmv --packages numpy,pandas,sklearn,matplotlib,
seaborn

randal s. olson thu jul 12 2018

cpython 3.6.6
ipython 6.4.0

numpy 1.12.1
pandas 0.23.1
sklearn 0.19.1
matplotlib 2.2.2
seaborn 0.8.1

compiler   : gcc 4.2.1 compatible clang 4.0.1 (tags/release_401/final)
system     : darwin
release    : 17.6.0
machine    : x86_64
processor  : i386
cpu cores  : 8
interpreter: 64bit

   finally, let's extract the core of our work from steps 1-5 and turn it
   into a single pipeline.
   in [39]:
%matplotlib inline
import pandas as pd
import seaborn as sb
from sklearn.ensemble import randomforestclassifier
from sklearn.model_selection import train_test_split, cross_val_score

# we can jump directly to working with the clean data because we saved our clean
ed data set
iris_data_clean = pd.read_csv('iris-data-clean.csv')

# testing our data: our analysis will stop here if any of these assertions are w
rong

# we know that we should only have three classes
assert len(iris_data_clean['class'].unique()) == 3

# we know that sepal lengths for 'iris-versicolor' should never be below 2.5 cm
assert iris_data_clean.loc[iris_data_clean['class'] == 'iris-versicolor', 'sepal
_length_cm'].min() >= 2.5

# we know that our data set should have no missing measurements
assert len(iris_data_clean.loc[(iris_data_clean['sepal_length_cm'].isnull()) |
                               (iris_data_clean['sepal_width_cm'].isnull()) |
                               (iris_data_clean['petal_length_cm'].isnull()) |
                               (iris_data_clean['petal_width_cm'].isnull())]) ==
 0

all_inputs = iris_data_clean[['sepal_length_cm', 'sepal_width_cm',
                             'petal_length_cm', 'petal_width_cm']].values

all_labels = iris_data_clean['class'].values

# this is the classifier that came out of grid search
random_forest_classifier = randomforestclassifier(criterion='gini', max_features
=3, n_estimators=50)

# all that's left to do now is plot the cross-validation scores
rf_classifier_scores = cross_val_score(random_forest_classifier, all_inputs, all
_labels, cv=10)
sb.boxplot(rf_classifier_scores)
sb.stripplot(rf_classifier_scores, jitter=true, color='black')

# ...and show some of the predictions from the classifier
(training_inputs,
 testing_inputs,
 training_classes,
 testing_classes) = train_test_split(all_inputs, all_labels, test_size=0.25)

random_forest_classifier.fit(training_inputs, training_classes)

for input_features, prediction, actual in zip(testing_inputs[:10],
                                              random_forest_classifier.predict(t
esting_inputs[:10]),
                                              testing_classes[:10]):
    print('{}\t-->\t{}\t(actual: {})'.format(input_features, prediction, actual)
)

[ 7.2  3.6  6.1  2.5]   -->     iris-virginica  (actual: iris-virginica)
[ 7.4  2.8  6.1  1.9]   -->     iris-virginica  (actual: iris-virginica)
[ 5.8  2.7  5.1  1.9]   -->     iris-virginica  (actual: iris-virginica)
[ 6.1  2.6  5.6  1.4]   -->     iris-virginica  (actual: iris-virginica)
[ 6.9  3.1  5.4  2.1]   -->     iris-virginica  (actual: iris-virginica)
[ 4.9  3.   1.4  0.2]   -->     iris-setosa     (actual: iris-setosa)
[ 5.1  3.7  1.5  0.4]   -->     iris-setosa     (actual: iris-setosa)
[ 5.   3.2  1.2  0.2]   -->     iris-setosa     (actual: iris-setosa)
[ 5.8  2.6  4.   1.2]   -->     iris-versicolor (actual: iris-versicolor)
[ 5.5  2.6  4.4  1.2]   -->     iris-versicolor (actual: iris-versicolor)

   [aairfrri6qcdaaaaaelftksuqmcc ]

   there we have it: we have a complete and reproducible machine learning
   pipeline to demo to our company's head of data. we've met the success
   criteria that we set from the beginning (>90% accuracy), and our
   pipeline is flexible enough to handle new inputs or flowers when that
   data set is ready. not bad for our first week on the job!

conclusions[78]  

   [79][ go back to the top ]

   i hope you found this example notebook useful for your own work and
   learned at least one new trick by reading through it.

   if you've spotted any errors or would like to contribute to this
   notebook, please don't hestitate to get in touch. i can be reached in
   the following ways:
     * [80]email me
     * [81]tweet at me
     * [82]submit an issue on github
     * fork the [83]notebook repository, make the fix/addition yourself,
       then send over a pull request

further reading[84]  

   [85][ go back to the top ]

   this notebook covers a broad variety of topics but skips over many of
   the specifics. if you're looking to dive deeper into a particular
   topic, here's some recommended reading.

   data science: william chen compiled a [86]list of free books for
   newcomers to data science, ranging from the basics of r & python to
   machine learning to interviews and advice from prominent data
   scientists.

   machine learning: /r/machinelearning has a useful [87]wiki page
   containing links to online courses, books, data sets, etc. for machine
   learning. there's also a [88]curated list of machine learning
   frameworks, libraries, and software sorted by language.

   unit testing: dive into python 3 has a [89]great walkthrough of unit
   testing in python, how it works, and how it should be used

   pandas has [90]several tutorials covering its myriad features.

   scikit-learn has a [91]bunch of tutorials for those looking to learn
   machine learning in python. andreas mueller's [92]scikit-learn workshop
   materials are top-notch and freely available.

   matplotlib has many [93]books, videos, and tutorials to teach plotting
   in python.

   seaborn has a [94]basic tutorial covering most of the statistical
   plotting features.

acknowledgements[95]  

   [96][ go back to the top ]

   many thanks to [97]andreas mueller for some of his [98]examples in the
   machine learning section. i drew inspiration from several of his
   excellent examples.

   the photo of a flower with annotations of the petal and sepal was taken
   by [99]eric guinther.

   the photos of the various iris flower types were taken by [100]ken
   walker and [101]barry glick.

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [102]fastly, rendered by [103]rackspace

   nbviewer github [104]repository.

   nbviewer version: [105]33c4683

   nbconvert version: [106]5.4.0

   rendered (fri, 05 apr 2019 18:11:42 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb
   5. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb
   6. https://github.com/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb
   7. https://mybinder.org/v2/gh/rhiever/data-analysis-and-machine-learning-projects/master?filepath=example-data-science-notebook/example machine learning notebook.ipynb
   8. https://raw.githubusercontent.com/rhiever/data-analysis-and-machine-learning-projects/master/example-data-science-notebook/example machine learning notebook.ipynb
   9. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/tree/master
  10. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/tree/master/example-data-science-notebook
  11. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#an-example-machine-learning-notebook
  12. http://www.randalolson.com/
  13. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#notebook-by-randal-s.-olson
  14. http://www.epistasis.org/
  15. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#supported-by-jason-h.-moore
  16. http://upibi.org/
  17. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#university-of-pennsylvania-institute-for-bioinformatics
  18. http://nbviewer.ipython.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb
  19. https://mybinder.org/v2/gh/rhiever/data-analysis-and-machine-learning-projects/master?filepath=example-data-science-notebook/example machine learning notebook.ipynb
  20. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  21. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#introduction
  22. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#license
  23. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#required-libraries
  24. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#the-problem-domain
  25. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#step-1:-answering-the-question
  26. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#step-2:-checking-the-data
  27. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#step-3:-tidying-the-data
  28. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#bonus:-testing-our-data
  29. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#step-4:-exploratory-analysis
  30. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#step-5:-classification
  31. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#cross-validation
  32. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#parameter-tuning
  33. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#step-6:-reproducibility
  34. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#conclusions
  35. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#further-reading
  36. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#acknowledgements
  37. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#introduction
  38. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  39. https://leanpub.com/datastyle
  40. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#license
  41. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  42. https://github.com/rhiever/data-analysis-and-machine-learning-projects#license
  43. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#required-libraries
  44. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  45. http://continuum.io/downloads
  46. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#the-problem-domain
  47. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  48. https://github.com/rhiever/data-analysis-and-machine-learning-projects/raw/master/example-data-science-notebook/iris-data.csv
  49. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#iris-setosa
  50. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#iris-versicolor
  51. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#iris-virginica
  52. https://archive.ics.uci.edu/ml/datasets/iris
  53. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#step-1:-answering-the-question
  54. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  55. https://en.wikipedia.org/wiki/accuracy_and_precision
  56. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#step-2:-checking-the-data
  57. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  58. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#step-3:-tidying-the-data
  59. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  60. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#bonus:-testing-our-data
  61. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  62. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#step-4:-exploratory-analysis
  63. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  64. https://en.wikipedia.org/wiki/box_plot
  65. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#step-5:-classification
  66. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  67. https://en.wikipedia.org/wiki/twenty_questions
  68. http://scikit-learn.org/stable/modules/generated/sklearn.tree.decisiontreeclassifier.html
  69. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#cross-validation
  70. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  71. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#parameter-tuning
  72. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  73. http://www.graphviz.org/
  74. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#step-6:-reproducibility
  75. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  76. http://sebastianraschka.com/
  77. https://github.com/rasbt/watermark
  78. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#conclusions
  79. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  80. http://www.randalolson.com/contact/
  81. https://twitter.com/randal_olson/
  82. https://github.com/rhiever/data-analysis-and-machine-learning-projects/issues
  83. https://github.com/rhiever/data-analysis-and-machine-learning-projects/
  84. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#further-reading
  85. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  86. http://www.wzchen.com/data-science-books/
  87. https://www.reddit.com/r/machinelearning/wiki/index
  88. https://github.com/josephmisiti/awesome-machine-learning
  89. http://www.diveintopython3.net/unit-testing.html
  90. http://pandas.pydata.org/pandas-docs/stable/tutorials.html
  91. http://scikit-learn.org/stable/tutorial/index.html
  92. https://github.com/amueller/scipy_2015_sklearn_tutorial
  93. http://matplotlib.org/resources/index.html
  94. http://stanford.edu/~mwaskom/software/seaborn/tutorial.html
  95. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#acknowledgements
  96. https://nbviewer.jupyter.org/github/rhiever/data-analysis-and-machine-learning-projects/blob/master/example-data-science-notebook/example machine learning notebook.ipynb#table-of-contents
  97. http://amueller.github.io/
  98. https://github.com/amueller/scipy_2015_sklearn_tutorial
  99. https://commons.wikimedia.org/wiki/file:petal-sepal.jpg
 100. http://www.signa.org/index.pl?display+iris-setosa+2
 101. http://www.signa.org/index.pl?display+iris-virginica+3
 102. http://www.fastly.com/
 103. https://developer.rackspace.com/?nbviewer=awesome
 104. https://github.com/jupyter/nbviewer
 105. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
 106. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
