   #[1]andrej karpathy blog posts

   [2][rssicon.svg]
   [3]andrej karpathy blog

   [4]about [5]hacker's guide to neural networks

what a deep neural network thinks about your #selfie

   oct 25, 2015

   [teaser.jpeg]

   convolutional neural networks are great: they recognize things, places
   and people in your personal photos, signs, people and lights in
   self-driving cars, crops, forests and traffic in aerial imagery,
   various anomalies in medical images and all kinds of other useful
   things. but once in a while these powerful visual recognition models
   can also be warped for distraction, fun and amusement. in this fun
   experiment we   re going to do just that: we   ll take a powerful,
   140-million-parameter state-of-the-art convolutional neural network,
   feed it 2 million selfies from the internet, and train it to classify
   good selfies from bad ones. just because it   s easy and because we can.
   and in the process we might learn how to take better selfies :)
   [6](reference)

     yeah, i   ll do real work. but first, let me tag a #selfie.

convolutional neural networks

   before we dive in i thought i should briefly describe what
   convolutional neural networks (or convnets for short) are in case a
   slightly more general audience reader stumbles by. basically, convnets
   are a very powerful hammer, and id161 problems are very
   nails. if you   re seeing or reading anything about a computer
   recognizing things in images or videos, in 2015 it almost certainly
   involves a convnet. some examples:
   [useful.jpg]
   few of many examples of convnets being useful. from top left and
   clockwise: classifying house numbers in street view images, recognizing
   bad things in medical images, recognizing chinese characters, traffic
   signs, and faces.

   a bit of history. convnets happen to have an interesting background
   story. they were first developed by [7]yann lecun et al. in 1980   s
   (building on some earlier work, e.g. from [8]fukushima). as a fun early
   example see this demonstration of lenet 1 (that was the convnet   s name)
   [9]recognizing digits back in 1993. however, these models remained
   mostly ignored by the id161 community because it was thought
   that they would not scale to    real-world    images. that turned out to be
   only true until about 2012, when we finally had enough compute (in form
   of gpus specifically, thanks nvidia) and enough data (thanks
   [10]id163) to actually scale these models, as was first demonstrated
   when alex krizhevsky, ilya sutskever and geoff hinton won the [11]2012
   id163 challenge (think: the world cup of id161), crushing
   their competition (16.4% error vs. 26.2% of the second best entry).

   i happened to witness this critical juncture in time first hand because
   the id163 challenge was over the last few years organized by
   [12]fei-fei li   s lab (my lab), so i remember when my labmate gasped in
   disbelief as she noticed the (very strong) convnet submission come up
   in the submission logs. and i remember us pacing around the room trying
   to digest what had just happened. in the next few months convnets went
   from obscure models that were shrouded in skepticism to rockstars of
   id161, present as a core building block in almost every new
   id161 paper. the id163 challenge reflects this trend - in
   the 2012 id163 challenge there was only one convnet entry, and since
   then in 2013 and 2014 almost all entries used convnets. also, fun fact,
   the winning team each year immediately incorporated into a company.

   over the next few years we had perfected, simplified, and scaled up the
   original 2012    [13]alexnet    architecture (yes, we give them names). in
   2013 there was the    [14]zfnet   , and then in 2014 the    [15]googlenet   
   (get it? because it   s like lenet but from google? hah) and
      [16]vggnet   . anyway, what we know now is that convnets are:
     * simple: one operation is repeated over and over few tens of times
       starting with the raw image.
     * fast, processing an image in few tens of milliseconds
     * they work very well (e.g. see [17]this post where i struggle to
       classify images better than the googlenet)
     * and by the way, in some ways they seem to work similar to our own
       visual cortex (see e.g. [18]this paper)

under the hood

   so how do they work? when you peek under the hood you   ll find a very
   simple computational motif repeated over and over. the gif below
   illustrates the full computational process of a small convnet:
   [gif2.gif]
   illustration of the id136 process.

   on the left we feed in the raw image pixels, which we represent as a
   3-dimensional grid of numbers. for example, a 256x256 image would be
   represented as a 256x256x3 array (last 3 for red, green, blue). we then
   perform convolutions, which is a fancy way of saying that we take small
   filters and slide them over the image spatially. different filters get
   excited over different features in the image: some might respond
   strongly when they see a small horizontal edge, some might respond
   around regions of red color, etc. if we suppose that we had 10 filters,
   in this way we would transform the original (256,256,3) image to a
   (256,256,10)    image   , where we   ve thrown away the original image
   information and only keep the 10 responses of our filters at every
   position in the image. it   s as if the three color channels (red, green,
   blue) were now replaced with 10 filter response channels (i   m showing
   these along the first column immediately on the right of the image in
   the gif above).

   now, i explained the first column of activations right after the image,
   so what   s with all the other columns that appear over time? they are
   the exact same operation repeated over and over, once to get each new
   column. the next columns will correspond to yet another set of filters
   being applied to the previous column   s responses, gradually detecting
   more and more complex visual patterns until the last set of filters is
   computing the id203 of entire visual classes (e.g. dog/toad) in
   the image. clearly, i   m skimming over some parts but that   s the basic
   gist: it   s just convolutions from start to end.

   training. we   ve seen that a convnet is a large collection of filters
   that are applied on top of each other. but how do we know what the
   filters should be looking for? we don   t - we initialize them all
   randomly and then train them over time. for example, we feed an image
   to a convnet with random filters and it might say that it   s 54% sure
   that   s a dog. then we can tell it that it   s in fact a toad, and there
   is a mathematical process for changing all filters in the convnet a
   tiny amount so as to make it slightly more likely to say toad the next
   time it sees that same image. then we just repeat this process
   tens/hundreds of millions of times, for millions of images.
   automagically, different filters along the computational pathway in the
   convnet will gradually tune themselves to respond to important things
   in the images, such as eyes, then heads, then entire bodies etc.
   [id98vis.jpg]
   examples of what 12 randomly chosen filters in a trained convnet get
   excited about, borrowed from [19]matthew zeiler's [20]visualizing and
   understanding convolutional networks. filters shown here are in the 3rd
   stage of processing and seem to look for honey-comb like patterns, or
   wheels/torsos/text, etc. again, we don't specify this; it emerges by
   itself and we can inspect it.

   another nice set of visualizations for a fully trained convnet can be
   found in jason yosinski et al. project [21]deepvis. it includes a fun
   live demo of a convnet running in real time on your computer   s camera,
   as explained nicely by jason in this video:

   iframe: [22]https://www.youtube.com/embed/agkfiq4igam

   in summary, the whole training process resembles showing a child many
   images of things, and him/her having to gradually figure out what to
   look for in the images to tell those things apart. or if you prefer
   your explanations technical, then convnet is just expressing a function
   from image pixels to class probabilities with the filters as
   parameters, and we run stochastic id119 to optimize a
   classification id168. or if you   re into ai/brain/singularity
   hype then the function is a    deep neural network   , the filters are
   neurons, and the full convnet is a piece of adaptive, simulated visual
   cortical tissue.

training a convnet

   the nice thing about convnets is that you can feed them images of
   whatever you like (along with some labels) and they will learn to
   recognize those labels. in our case we will feed a convnet some good
   and bad selfies, and it will automagically find the best things to look
   for in the images to tell those two classes apart. so lets grab some
   selfies:
    1. i wrote a quick script to gather images tagged with #selfie. i
       ended up getting about 5 million images (with convnets it   s the
       more the better, always).
    2. i narrowed that down with another convnet to about 2 million images
       that contain at least one face.
    3. now it is time to decide which ones of those selfies are good or
       bad. intuitively, we want to calculate a proxy for how many people
       have seen the selfie, and then look at the number of likes as a
       function of the audience size. i took all the users and sorted them
       by their number of followers. i gave a small bonus for each
       additional tag on the image, assuming that extra tags bring more
       eyes. then i marched down this sorted list in groups of 100, and
       sorted those 100 selfies based on their number of likes. i only
       used selfies that were online for more than a month to ensure a
       near-stable like count. i took the top 50 selfies and assigned them
       as positive selfies, and i took the bottom 50 and assigned those to
       negatives. we therefore end up with a binary split of the data into
       two halves, where we tried to normalize by the number of people who
       have probably seen each selfie. in this process i also filtered
       people with too few followers or too many followers, and also
       people who used too many tags on the image.
    4. take the resulting dataset of 1 million good and 1 million bad
       selfies and train a convnet.

   at this point you may object that the way i   m deciding if a selfie is
   good or bad is wrong - e.g. what if someone posted a very good selfie
   but it was late at night, so perhaps not as many people saw it and it
   got less likes? you   re right - it almost definitely is wrong, but it
   only has to be right more often that not and the convnet will manage.
   it does not get confused or discouraged, it just does its best with
   what it   s been given. to get an idea about how difficult it is to
   distinguish the two classes in our data, have a look at some example
   training images below. if i gave you any one of these images could you
   tell which category it belongs to?
   [grid_render_posneg.jpg]
   example images showing good and bad selfies in our training data. these
   will be given to the convnet as teaching material.

   training details. just to throw out some technical details, i used
   [23]caffe to train the convnet. i used a vggnet pretrained on id163,
   and finetuned it on the selfie dataset. the model trained overnight on
   an nvidia k40 gpu. i disabled dropout because i had better results
   without it. i also tried a vggnet pretrained on a dataset with faces
   but did not obtain better results than starting from an id163
   checkpoint. the final model had 60% accuracy on my validation data
   split (50% is guessing randomly).

what makes a good #selfie ?

   okay, so we collected 2 million selfies, decided which ones are
   probably good or bad based on the number of likes they received
   (controlling for the number of followers), fed all of it to caffe and
   trained a convnet. the convnet    looked    at every one of the 2 million
   selfies several tens of times, and tuned its filters in a way that best
   allows it to separate good selfies from bad ones. we can   t very easily
   inspect exactly what it found (it   s all jumbled up in 140 million
   numbers that together define the filters). however, we can set it loose
   on selfies that it has never seen before and try to understand what
   it   s doing by looking at which images it likes and which ones it does
   not.

   i took 50,000 selfies from my test data (i.e. the convnet hasn   t seen
   these before). as a first visualization, in the image below i am
   showing a continuum visualization, with the best selfies on the top
   row, the worst selfies on the bottom row, and every row in between is a
   continuum:
   [grid_render_continuum.jpg]
   a continuum from best (top) to worst (bottom) selfies, as judged by the
   convnet.

   that was interesting. lets now pull up the top 100 selfies (out of
   50,000), according to the convnet:
   [grid_render_best.jpg]
   best 100 out of 50,000 selfies, as judged by the convolutional neural
   network.

   if you   d like to see more here is a link to [24]top 1000 selfies
   (3.5mb). are you noticing a pattern in what the convnet has likely
   learned to look for? a few patterns stand out for me, and if you notice
   anything else i   d be happy to hear about in the comments. to take a
   good selfie, do:
     * be female. women are consistently ranked higher than men. in
       particular, notice that there is not a single guy in the top 100.
     * face should occupy about 1/3 of the image. notice that the position
       and pose of the face is quite consistent among the top images. the
       face always occupies about 1/3 of the image, is slightly tilted,
       and is positioned in the center and at the top. which also brings
       me to:
     * cut off your forehead. what   s up with that? it looks like a popular
       strategy, at least for women.
     * show your long hair. notice the frequent prominence of long strands
       of hair running down the shoulders.
     * oversaturate the face. notice the frequent occurrence of
       over-saturated lighting, which often makes the face look much more
       uniform and faded out. related to that,
     * put a filter on it. black and white photos seem to do quite well,
       and most of the top images seem to contain some kind of a filter
       that fades out the image and decreases the contrast.
     * add a border. you will notice a frequent appearance of
       horizontal/vertical white borders.

   interestingly, not all of these rules apply to males. i manually went
   through the top 2000 selfies and picked out the top males, here   s what
   we get:
   [males.jpg]
   best few male selfies taken from the top 2,000 selfies.

   in this case we see don   t see any cut off foreheads. instead, most
   selfies seem to be a slightly broader shot with head fully in the
   picture, and shoulders visible. it also looks like many of them have a
   fancy hair style with slightly longer hair combed upwards. however, we
   still do see the prominance of faded facial features.

   lets also look at some of the worst selfies, which the convnet is quite
   certain would not receive a lot of likes. i am showing the images in a
   much smaller and less identifiable format because my intention is for
   us to learn about the broad patterns that decrease the selfie   s
   quality, not to shine light on people who happened to take a bad
   selfie. here they are:
   [grid_render_worst.jpg]
   worst 300 out of 50,000 selfies, as judged by the convolutional neural
   network.

   even at this small resolution some patterns clearly emerge. don   t:
     * take selfies in low lighting. very consistently, darker photos
       (which usually include much more noise as well) are ranked very low
       by the convnet.
     * frame your head too large. presumably no one wants to see such an
       up-close view.
     * take group shots. it   s fun to take selfies with your friends but
       this seems to not work very well. keep it simple and take up all
       the space yourself. but not too much space.

   as a last point, note that a good portion of the variability between
   what makes a good or bad selfies can be explained by the style of the
   image, as opposed to the raw attractiveness of the person. also, with
   some relief, it seems that the best selfies do not seem to be the ones
   that show the most skin. i was quite concerned for a moment there that
   my fancy 140-million convnet would turn out to be a simple
   amount-of-skin-texture-counter.

   celebrities. as a last fun experiment, i tried to run the convnet on a
   few famous celebrity selfies, and sorted the results with the continuum
   visualization, where the best selfies are on the top and the convnet
   score decreases to the right and then towards the bottom:
   [celebs_grid_render.jpg]
   celebrity selfies as judged by a convolutional neural network. most
   attractive selfies: top left, then deceasing in quality first to the
   right then towards the bottom. right click > open image in new tab on
   this image to see it in higher resolution.

   amusingly, note that the general rule of thumb we observed before (no
   group photos) is broken with the famous group selfie of ellen degeneres
   and others from the oscars, yet the convnet thinks this is actually a
   very good selfie, placing it on the 2nd row! nice! :)

   another one of our rules of thumb (no males) is confidently defied by
   chris pratt   s body (also 2nd row), and honorable mentions go to justin
   beiber   s raised eyebrows and stephen collbert / jimmy fallon duo (3rd
   row). james franco   s selfie shows quite a lot more skin than chris   ,
   but the convnet is not very impressed (4th row). neither was i.

   lastly, notice again the importance of style. there are several
   uncontroversially-good-looking people who still appear on the bottom of
   the list, due to bad framing (e.g. head too large possibly for j lo),
   bad lighting, etc.

exploring the #selfie space

   another fun visualization we can try is to lay out the selfies with
   [25]id167. id167 is a wonderful algorithm that i like to run on nearly
   anything i can because it   s both very general and very effective - it
   takes some number of things (e.g. images in our case) and lays them out
   in such way that nearby things are similar. you can in fact lay out
   many things with id167, such as [26]netflix movies, [27]words,
   [28]twitter profiles, [29]id163 images, or really anything where you
   have some number of things and a way of comparing how similar two
   things are. in our case we will lay out selfies based on how similar
   the convnet perceives them. in technical terms, we are doing this based
   on l2 norms of the fc7 activations in the last fully-connected layer.
   here is the visualization:
   [grid_render_tsne_reduced.jpg]
   selfie id167 visualization. here is a link to a [30]higher-resolution
   version. (9mb)

   you can see that selfies cluster in some fun ways: we have group
   selfies on top left, a cluster of selfies with sunglasses/glasses in
   middle left, closeups bottom left, a lot of mirror full-body shots top
   right, etc. well, i guess that was kind of fun.

finding the optimal crop for a selfie

   another fun experiment we can run is to use the convnet to
   automatically find the best selfie crops. that is, we will take an
   image, randomly try out many different possible crops and then select
   the one that the convnet thinks looks best. below are four examples of
   the process, where i show the original selfies on the left, and the
   convnet-cropped selfies on the right:
   [crops1.jpg]
   each of the four pairs shows the original image (left) and the crop
   that was selected by the convnet as looking best (right). </a>

   notice that the convnet likes to make the head take up about 1/3 of the
   image, and chops off the forehead. amusingly, in the image on the
   bottom right the convnet decided to get rid of the    self    part of
   selfie, entirely missing the point :) you can find many more fun
   examples of these    rude    crops:
   [crop2.jpg]
   same visualization as above, with originals on left and best crops on
   right. the one on the right is my favorite.</a>

   before any of the more advanced users ask: yes, i did try to insert a
   [31]spatial transformer layer right after the image and before the
   convnet. then i backpropped into the 6 parameters that define an
   arbitrary affine crop. unfortunately i could not get this to work well
   - the optimization would sometimes get stuck, or drift around somewhat
   randomly. i also tried constraining the transform to scale/translation
   but this did not help. luckily, when your transform has 3 bounded
   parameters then we can afford to perform global search (as seen above).

how good is yours?

   curious about what the network thinks of your selfies? i   ve packaged
   the network into a twitter bot so that you can easily find out. (the
   bot turns out to be onyl ~150 lines of python, including all
   caffe/tweepy code). attach your image to a tweet (or include a link)
   and mention the bot [32]@deepselfie anywhere in the tweet. the bot will
   take a look at your selfie and then pitch in with its opinion! for best
   results link to a square image, otherwise the bot will have to squish
   it to a square, which deteriorates the results. the bot should reply
   within a minute or something went wrong (try again later).
   [selfiebot2.png]
   example interaction with the selfie bot ([33]@deepselfie).

   before anyone asks, i also tried to port a smaller version of this
   convnet to run on ios so you could enjoy real-time feedback while
   taking your selfies, but this turned out to be quite involved for a
   quick side project - e.g. i first tried to write my own fragment
   shaders since there is no cuda-like support, then looked at some
   threaded cpu-only versions, but i couldn   t get it to work nicely and in
   real time. and i do have real work to do.

conclusion

   i hope i   ve given you a taste of how powerful convolutional neural
   networks are. you give them example images with some labels, they learn
   to recognize those things automatically, and it all works very well and
   is very fast (at least at test time, once it   s trained). of course,
   we   ve only barely scratched the surface - convnets are used as a basic
   building block in many neural networks, not just to classify
   images/videos but also to segment, detect, and describe, both in the
   cloud or in robots.

   if you   d liked to learn more, the best place to start for a beginner
   right now is probably [34]michael nielsen   s tutorials. from there i
   would encourage you to first look at [35]andrew ng   s coursera class,
   and then next i would go through course notes/assignments for
   [36]cs231n. this is a class specifically on convnets that i taught
   together with fei-fei at stanford last winter quarter. we will also be
   offering the class again starting january 2016 and you   re free to
   follow along. for more advanced material i would look into [37]hugo
   larochelle   s neural networks class or the [38]deep learning book
   currently being written by yoshua bengio, ian goodfellow and aaron
   courville.

   of course you   ll learn much more by doing than by reading, so i   d
   recommend that you play with [39]101 kaggle challenges, or that you
   develop your own side projects, in which case i warmly recommend that
   you not only do but also write about it, and post it places for all of
   us to read, for example on [40]/r/machinelearning which has accumulated
   a nice community. as for recommended tools, the three common options
   right now are:
     * [41]caffe (c++, python/matlab wrappers), which i used in this post.
       if you   re looking to do basic image classification then caffe is
       the easiest way to go, in many cases requiring you to write no
       code, just invoking included scripts.
     * theano-based deep learning libraries (python) such as [42]keras or
       [43]lasagne, which allow more flexibility.
     * [44]torch (c++, lua), which is what i currently use in my research.
       i   d recommend torch for the most advanced users, as it offers a lot
       of freedom, flexibility, speed, all with quite simple abstractions.

   some other slightly newer/less proven but promising libraries include
   [45]nervana   s neon, [46]cgt, or [47]mocha in julia.

   lastly, there are a few companies out there who aspire to bring deep
   learning to the masses. one example is [48]metamind, who offer web
   interface that allows you to drag and drop images and train a convnet
   (they handle all of the details in the cloud). metamind and
   [49]clarifai also offer convnet rest apis.

   that   s it, see you next time!
   please enable javascript to view the [50]comments powered by disqus.
   [51]comments powered by disqus

     * andrej karpathy blog

     * [52]karpathy
     * [53]karpathy

   musings of a computer scientist.

references

   visible links
   1. http://karpathy.github.io/feed.xml
   2. http://karpathy.github.io/feed.xml
   3. http://karpathy.github.io/
   4. http://karpathy.github.io/about/
   5. http://karpathy.github.io/neuralnets/
   6. https://www.youtube.com/watch?v=kdemffbs5h0
   7. https://www.facebook.com/yann.lecun
   8. https://en.wikipedia.org/wiki/neocognitron
   9. https://www.youtube.com/watch?v=fwfdura_l6q
  10. http://www.image-net.org/
  11. http://image-net.org/challenges/lsvrc/2012/results.html
  12. http://vision.stanford.edu/
  13. http://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks
  14. http://arxiv.org/abs/1311.2901
  15. http://arxiv.org/abs/1409.4842
  16. http://www.robots.ox.ac.uk/~vgg/research/very_deep/
  17. http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-id163/
  18. http://arxiv.org/abs/1406.3284
  19. http://www.matthewzeiler.com/
  20. http://arxiv.org/abs/1311.2901
  21. http://yosinski.com/deepvis
  22. https://www.youtube.com/embed/agkfiq4igam
  23. http://caffe.berkeleyvision.org/
  24. http://cs.stanford.edu/people/karpathy/grid_render_top.jpg
  25. http://lvdmaaten.github.io/tsne/
  26. http://lvdmaaten.github.io/tsne/examples/netflix_tsne.jpg
  27. http://lvdmaaten.github.io/tsne/examples/semantic_tsne.jpg
  28. http://cs.stanford.edu/people/karpathy/tsnejs/
  29. http://cs.stanford.edu/people/karpathy/id98embed/
  30. http://cs.stanford.edu/people/karpathy/grid_render_tsne_big.jpg
  31. http://torch.ch/blog/2015/09/07/spatial_transformers.html
  32. https://twitter.com/deepselfie
  33. https://twitter.com/deepselfie
  34. http://neuralnetworksanddeeplearning.com/index.html
  35. https://www.coursera.org/learn/machine-learning
  36. http://cs231n.stanford.edu/
  37. https://www.youtube.com/playlist?list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh
  38. http://www.iro.umontreal.ca/~bengioy/dlbook/
  39. https://www.kaggle.com/competitions
  40. https://www.reddit.com/r/machinelearning
  41. http://caffe.berkeleyvision.org/
  42. http://keras.io/
  43. https://github.com/lasagne/lasagne
  44. http://torch.ch/
  45. https://github.com/nervanasystems/neon
  46. http://rll.berkeley.edu/cgt/
  47. http://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/
  48. https://www.metamind.io/
  49. http://www.clarifai.com/
  50. http://disqus.com/?ref_noscript
  51. http://disqus.com/
  52. https://github.com/karpathy
  53. https://twitter.com/karpathy

   hidden links:
  55. http://karpathy.github.io/2015/10/25/selfie/
