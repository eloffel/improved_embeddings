   #[1]triangleinequality    feed [2]triangleinequality    comments feed
   [3]triangleinequality    neural networks part 1: feedingforward and
   id26 comments feed [4]enter the id88 [5]neural
   networks part 2: python implementation [6]alternate [7]alternate
   [8]triangleinequality [9]wordpress.com

     * [10]home
     * [11]about
     * [12]contents

[13]triangleinequality

   for matters mathematical
   search ____________________ (button) search
     __________________________________________________________________

   [14]home    [15]machine learning    neural networks part 1:
   feedingforward and id26

neural networks part 1: feedingforward and id26

   introduction

   last time we looked at the [16]id88 model for classification,
   which is similar to [17]id28, in that it uses a
   nonlinear transformation of a linear combination of the input features
   to give an output class.

   a neural network generalizes this idea by conceiving of a network of
   nodes which each take as an input a linear combination of the outputs
   of other nodes, then use some [18]activation function to turn this into
   an output, such as:
     * the sigmoidal [19]logistic function, as in [20]id28,
     * the binary step function seen in the [21]id88,
     * the identity function as seen in [22]id75,
     * the sigmoidal tanh .

   a feedforward neural network

   a neural network is just a directed graph. we will be looking at
   [23]feedforward neural networks, which are [24]acyclic.

   each node is called a neuron. there are three main kinds of neurons:
     * input neurons, one neuron for each feature, so if our input were
       vectors in \mathbb{r}^3 , we would have 3 input neurons. these
       neurons have trivial id180 and no incoming edges.
     * output neurons, one neuron for each output feature, so if our so if
       our output were vectors in \mathbb{r}^3 , we would have 3 output
       neurons. these will typically have a nonlinear activation function,
       chosen so that the output range matches our training data. these
       neurons have no outgoing edges.
     * hidden neurons are in between input and output neurons and have
       both incoming and outgoing edges.

   since the graph is acyclic, we can think of it as being composed of
   layers, as in the following picture taken from [25]wikipedia:

   although note there can be many hidden layers. each edge has a weight,
   a real number.

   the neural network makes a prediction by feeding forward the input. to
   describe this we develop some notation. the discussion that follows is
   indebted to this[26] very helpful page.

   notation

   let n be a neural network and g index the vertices of n so that each
   n_i is a vertex for i \in g .

   for each i \in g let t_i denote the set of indices of the targets of
   n_i . that is the set of indices x \subset g such for each j \in x
   there exists a directed edge e_{ij} from n_i \to n_j . we may also
   conflate t_i with the set of indexed neurons if the context makes this
   unambiguous.

   similarly, for each i \in g let s_i denote the set of indices of the
   sources of n_i .

   for each index i , let f_i: \mathbb{r} \to \mathbb{r} denote the
   activation function of n_i .

   for each index i , let o_i(x) \in \mathbb{r}:=f_i(x) be the output of
   n_i . when the argument x is understood, we may simply write o_i .

   for each directed edge e_{ij} define w_{ij} \in \mathbb{r} to be the
   weight of the edge.

   where the outputs are understood, for each index i , define the input
   i_i to be \sum \limits _{j \in s_i}w_{ji}o_j , the weighted sum of the
   outputs of the previous layer.

   feeding forward

   feeding forward refers to how an input becomes an output or prediction.

   as you might have guessed: feeding forward an input vector x , the
   input and output to each input neuron is just the corresponding
   component of the vector x .

   from there the input to a neuron in the first hidden layer is simply
   the sum of the components of x , weighted by the weights of the
   incoming edges to that hidden node.

   you repeat this for each hidden node. then the inputs to the hidden
   nodes are transformed via their id180 to their outputs.
   the weighted sums of these outputs are then passed on as inputs to the
   next layer, and so on.

   this is probably best understood through a simple example.

   [27]nn1 here we have 4 neurons. the input x is two dimensional, so we
   have two input neurons shown in blue. we have a single hidden layer
   with just one hidden neuron, shown in green. finally we have a single
   output neuron shown in orange. hence the output y is one dimensional.

   choosing the weights: id26

   so we have seen how a neural network can make predictions, but how can
   we choose the weights so that the predictions are any good? the answer
   is the same as in id28, we use id119. in
   order to do this we need to use differentiable id180 and
   choose an appropriate error function to minimize.

   if x \in \mathbb{r}^k is the input vector, and y \in \mathbb{r}^n is
   the target vector, let f_n: \mathbb{r}^k \to \mathbb{r}^n be the
   prediction of the neural network. then we define the error to be e(x)=
   \frac{1}{2}(f_n(x)-y)^2 . this is the natural euclidean norm of the
   difference between the target and the prediction. the factor of a half
   is there for aesthetical reasons, since we will be differentiating and
   gaining a factor of 2.

   the trick to the algorithm is to figure out how the weights contribute
   to the error, and id26 is an elegant way of assigning
   credit/blame to each node and edge. we fix an input x , and write e for
   e(x) , and for each each edge e_{ij} we are seeking \frac{\partial
   e}{\partial w_{ij}} .  once we have found this, we set w_{ij} \mapsto
   w_{ij} - \lambda \frac{\partial e}{\partial w_{ij}} , where \lambda is
   the learning rate.  so we are doing stochastic id119 as in
   the id88.

   it is clear that an edge e_{ij} and its weight w_{ij} contribute to the
   error through the target n_j    s input. so it makes sense to divide and
   conquer using the chain rule, that is

   \frac{\partial e}{\partial w_{ij}} = \frac{\partial e}{\partial i_j}
   \cdot \frac{\partial i_j}{\partial w_{ij}} .

   the first part of the product is worth giving a name,  we denote it by
   e_j:=\frac{\partial e}{\partial i_j} and call it the error at the node.

   the second part of the product is simple to calculate:

   \frac{\partial e_j}{\partial w_{ij}} = \frac{\partial}{\partial w_{ij}}
   \sum \limits _{k \in s_j}w_{kj}o_k = o_i

   returning to e_j , we can calculate it recursively, assuming the errors
   for the neurons in t_j have already been calculated.

   observe that e can be thought of as a function of the variables o_k for
   k indexing any layer, and so using the chain rule we have

   e_j = \frac{\partial e}{\partial i_j}

   = \sum \limits_{k \in t_j} \frac{\partial e}{\partial i_k}\cdot
   \frac{\partial i_k}{\partial i_j}

   = \sum \limits_{k \in t_j} e_k \cdot \frac{\partial i_k}{\partial i_j}

   =\sum \limits_{k \in t_j} e_k \cdot \frac{\partial i_k}{\partial o_j}
   \cdot \frac{\partial o_j}{\partial i_j}

   =f_j'(i_j)\sum \limits_{k \in t_j} e_k \cdot \frac{\partial
   i_k}{\partial o_j}

   =f_j'(i_j)\sum \limits_{k \in t_j} e_k \cdot \left (
   \frac{\partial}{\partial o_j} \sum \limits _{l \in s_k}w_{lk}o_l \right
   )

   =f_j'(i_j)\sum \limits_{k \in t_j} e_k \cdot w_{jk}

   notice that if we assume that each layer is fully-connected to the
   next, then we could write the above succinctly as an inner product, and
   inner products can be calculated very efficiently, for instance using a
   [28]gpu, so it is often faster to have more connections!

   together this gives

   \frac{\partial e}{\partial w_{ij}}=o_i \cdot f_j'(i_j)\cdot\sum
   \limits_{k \in t_j} e_k \cdot w_{jk} .

   since we have calculated the error at a node recursively, we had better
   deal with the base case      the output neurons. but this follows simply
   from the definition of  e to be e_i=f'(i_i)(o_i-y_i) , with the factor
   of 2 cancelling with the half as promised.

   bias neurons

   the final piece of the puzzle are called bias neurons, which i had left
   out before to simplify the discussion. each layer, except the output
   layer, has a bias neuron. a bias neuron has no inputs, and its output
   is always 1. effectively it just adds the weight of its outgoing edges
   to the inputs of the next layer.

   these behave just like regular neurons, only when updating their
   weights, we note that since their input is effectively constant, they
   have \frac{\partial i_j}{\partial w_{ij}}=1 , and so we need only
   calculate the error at a bias neuron to update its weights.

   next time

   in the next installment i will show how to compactly represent the
   process of feeding forward and id26 using vector and matrix
   notation assuming the layers are fully connected. we will then use this
   to give a python implementation.

   advertisements

share this:

     * [29]twitter
     * [30]facebook
     *

like this:

   like loading...

related

   tags: [31]id26, [32]feedforward, [33]machine learning,
   [34]neural net, [35]neural network
   by [36]triangleinequality in [37]machine learning on [38]march 27,
   2014.
   [39]    enter the id88 [40]neural networks part 2:
   python implementation    
     __________________________________________________________________

2 comments

    1. [41]neural networks part 2: python implementation   
       triangleinequality says:
       [42]march 31, 2014 at 3:04 pm
       [   ] so last time we introduced the feedforward neural network. we
       discussed how input gets fed forward to become output, and the
       id26 algorithm for [   ]
       [43]reply
    2. [44]theano, autoencoders and mnist    triangleinequality says:
       [45]august 12, 2014 at 11:01 am
       [   ] don   t want to go into too much detail here, but an autoencoder
       is a feedforward neural network with a single hidden layer with the
       twist being that its target output is exactly the same as its [   ]
       [46]reply

leave a reply [47]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [48]googleplus-sign-in

     *
     *

   [49]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [50]log out /
   [51]change )
   google photo

   you are commenting using your google account. ( [52]log out /
   [53]change )
   twitter picture

   you are commenting using your twitter account. ( [54]log out /
   [55]change )
   facebook photo

   you are commenting using your facebook account. ( [56]log out /
   [57]change )
   [58]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

   [59]create a free website or blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [60]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [61]cookie policy

   iframe: [62]likes-master

   %d bloggers like this:

references

   visible links
   1. https://triangleinequality.wordpress.com/feed/
   2. https://triangleinequality.wordpress.com/comments/feed/
   3. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/feed/
   4. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/
   5. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/&for=wpcom-auto-discovery
   8. https://triangleinequality.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://triangleinequality.wordpress.com/
  11. https://triangleinequality.wordpress.com/about/
  12. https://triangleinequality.wordpress.com/contents/
  13. https://triangleinequality.wordpress.com/
  14. https://triangleinequality.wordpress.com/
  15. https://triangleinequality.wordpress.com/category/machine-learning/
  16. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/
  17. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/
  18. http://en.wikipedia.org/wiki/activation_function
  19. http://en.wikipedia.org/wiki/logistic_function
  20. https://triangleinequality.wordpress.com/2013/12/02/logistic-regression/
  21. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/
  22. https://triangleinequality.wordpress.com/2013/11/17/linear-regression-the-maths/
  23. http://en.wikipedia.org/wiki/feedforward_neural_network
  24. http://en.wikipedia.org/wiki/directed_acyclic_graph
  25. http://en.wikipedia.org/wiki/feedforward_neural_network
  26. http://www.willamette.edu/~gorr/classes/cs449/backprop.html
  27. https://triangleinequality.files.wordpress.com/2014/03/nn1.png
  28. http://en.wikipedia.org/wiki/graphics_processing_unit
  29. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/?share=twitter
  30. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/?share=facebook
  31. https://triangleinequality.wordpress.com/tag/id26/
  32. https://triangleinequality.wordpress.com/tag/feedforward/
  33. https://triangleinequality.wordpress.com/tag/machine-learning/
  34. https://triangleinequality.wordpress.com/tag/neural-net/
  35. https://triangleinequality.wordpress.com/tag/neural-network/
  36. https://triangleinequality.wordpress.com/author/triangleinequality/
  37. https://triangleinequality.wordpress.com/tag/machine-learning/
  38. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  39. https://triangleinequality.wordpress.com/2014/02/24/enter-the-id88/
  40. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/
  41. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/
  42. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/#comment-85
  43. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/?replytocom=85#respond
  44. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/
  45. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/#comment-147
  46. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/?replytocom=147#respond
  47. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/#respond
  48. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://triangleinequality.wordpress.com&color_scheme=light
  49. https://gravatar.com/site/signup/
  50. javascript:highlandercomments.doexternallogout( 'wordpress' );
  51. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  52. javascript:highlandercomments.doexternallogout( 'googleplus' );
  53. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  54. javascript:highlandercomments.doexternallogout( 'twitter' );
  55. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  56. javascript:highlandercomments.doexternallogout( 'facebook' );
  57. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  58. javascript:highlandercomments.cancelexternalwindow();
  59. https://wordpress.com/?ref=footer_website
  60. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  61. https://automattic.com/cookies
  62. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  64. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/#comment-form-guest
  65. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/#comment-form-load-service:wordpress.com
  66. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/#comment-form-load-service:twitter
  67. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/#comment-form-load-service:facebook
