   #[1]triangleinequality    feed [2]triangleinequality    comments feed
   [3]triangleinequality    neural networks part 2: python implementation
   comments feed [4]neural networks part 1: feedingforward and
   id26 [5]id178: expect a surprise [6]alternate [7]alternate
   [8]triangleinequality [9]wordpress.com

     * [10]home
     * [11]about
     * [12]contents

[13]triangleinequality

   for matters mathematical
   search ____________________ (button) search
     __________________________________________________________________

   [14]home    [15]machine learning    neural networks part 2:
   python implementation

neural networks part 2: python implementation

   ok so [16]last time we introduced the feedforward neural network. we
   discussed how input gets fed forward to become output, and the
   id26 algorithm for learning the weights of the edges.

   today we will begin by showing how the model can be expressed using
   matrix notation, under the assumption that the neural network is fully
   connected, that is each neuron is connected to all the neurons in the
   next layer.

   once this is done we will give a python implementation and test it out.

   matrix notation for neural networks

   most of  this i learned from [17]here.

   in what follows, vectors are always thought of as columns, and so the
   transpose a row.

   so first off we have x \in \mathbb{r}^k , our input vector, and y \in
   \mathbb{r}^m our output vector.

   our neural network n will have n layers l_1, \dots , l_n with l_1   the
   input layer, and l_n the output layer.

   for each layer l_i :
     * write n_i:=|l_i| for the number of neurons in that layer (not
       including the bias neurons);
     * write f_i: \mathbb{r}^{n_i} \to \mathbb{r}^{n_i} for the layer   s
       activation function;
     * write f_i': \mathbb{r}^{n_i} \to \mathbb{r}^{n_i} for vector of
       derivatives of the id180;
     * write i_i \in \mathbb{r}^{n_i} for the vector of inputs to the ith
       layer;
     * write b_i \in \mathbb{r}^{n_{i}} for the vector of biases;
     * write o_i \in \mathbb{r}^{n_i}= f_i(i_i) for the vector of outputs
       from the ith layer;
     * write o_i' \in \mathbb{r}^{n_i}= f_i'(i_i) ;
     * write w_i for the n_{i+1} \times n_i real matrix containing the
       weights of the edges going from l_i \to l_{i+1} (not including bias
       neurons), so that w_io_i + b_i = i_{i+1} .

   feeding forward in vector notation

   so to feedforward the input we have

   x \mapsto w_1x +b_1 \mapsto f_2(w_1x+b_1) \mapsto w_2f_2(w_1x)+b_2
   \mapsto \dots \mapsto f_n(w_{n-1} f_{n-1}( \dots w_2f_2(w_1x) )) .

   and we call this output f_n(x) .

   id26 in vector notation

   now write e_i \in \mathbb{r}^{n_i} for the error at the layer i defined
   e_i:=\frac{\partial e}{\partial i_i} .

   then e_n = o_n' \cdot (o_n-y) , and e_{i-1} = \left (w_{i-1}^te_i\right
   ) \cdot o_{i-1}' , where the multiplication \cdot is component wise.

   write \triangle w_i for the matrix with entries (\frac{\partial
   e}{\partial w_{ij}})_{ij} , for non-bias weights.

   then we have \triangle w_i = \cdot e_{i+1} \cdot o_i^t , the outer
   product of e_{i+1} and o_i .

   and for the biases write \triangle b_i \in \mathbb{r}^{n_i} for the
   vector with jth component \frac{\partial e}{\partial (b_i)_j} .

   then \triangle b_i =e_{i+1} .

   python implementation

   we will create a class neuralnetwork and perform our calculations using
   matrices in numpy.
import numpy as np

class neuralnetwork(object):
    def __init__(self, x, y, parameters):
        #input data
        self.x=x
        #output data
        self.y=y
        #expect parameters to be a tuple of the form:
        #    ((n_input,0,0), (n_hidden_layer_1, f_1, f_1'), ...,
        #     (n_hidden_layer_k, f_k, f_k'), (n_output, f_o, f_o'))
        self.n_layers = len(parameters)
        #counts number of neurons without bias neurons in each layer.
        self.sizes = [layer[0] for layer in parameters]
        #id180 for each layer.
        self.fs =[layer[1] for layer in parameters]
        #derivatives of id180 for each layer.
        self.fprimes = [layer[2] for layer in parameters]
        self.build_network()

    def build_network(self):
        #list of weight matrices taking the output of one layer to the input of
the next.
        self.weights=[]
        #bias vector for each layer.
        self.biases=[]
        #input vector for each layer.
        self.inputs=[]
        #output vector for each layer.
        self.outputs=[]
        #vector of errors at each layer.
        self.errors=[]
        #we initialise the weights randomly, and fill the other vectors with 1s.
        for layer in range(self.n_layers-1):
            n = self.sizes[layer]
            m = self.sizes[layer+1]
            self.weights.append(np.random.normal(0,1, (m,n)))
            self.biases.append(np.random.normal(0,1,(m,1)))
            self.inputs.append(np.zeros((n,1)))
            self.outputs.append(np.zeros((n,1)))
            self.errors.append(np.zeros((n,1)))
        #there are only n-1 weight matrices, so we do the last case separately.
        n = self.sizes[-1]
        self.inputs.append(np.zeros((n,1)))
        self.outputs.append(np.zeros((n,1)))
        self.errors.append(np.zeros((n,1)))

    def feedforward(self, x):
        #propagates the input from the input layer to the output layer.
        k=len(x)
        x.shape=(k,1)
        self.inputs[0]=x
        self.outputs[0]=x
        for i in range(1,self.n_layers):
            self.inputs[i]=self.weights[i-1].dot(self.outputs[i-1])+self.biases[
i-1]
            self.outputs[i]=self.fs[i](self.inputs[i])
        return self.outputs[-1]

    def update_weights(self,x,y):
        #update the weight matrices for each layer based on a single input x and
 target y.
        output = self.feedforward(x)
        self.errors[-1]=self.fprimes[-1](self.outputs[-1])*(output-y)

        n=self.n_layers-2
        for i in xrange(n,0,-1):
            self.errors[i] = self.fprimes[i](self.inputs[i])*self.weights[i].t.d
ot(self.errors[i+1])
            self.weights[i] = self.weights[i]-self.learning_rate*np.outer(self.e
rrors[i+1],self.outputs[i])
            self.biases[i] = self.biases[i] - self.learning_rate*self.errors[i+1
]
        self.weights[0] = self.weights[0]-self.learning_rate*np.outer(self.error
s[1],self.outputs[0])
        self.biases[0] = self.biases[0] - self.learning_rate*self.errors[1]
    def train(self,n_iter, learning_rate=1):
        #updates the weights after comparing each input in x with y
        #repeats this process n_iter times.
        self.learning_rate=learning_rate
        n=self.x.shape[0]
        for repeat in range(n_iter):
            #we shuffle the order in which we go through the inputs on each iter
.
            index=list(range(n))
            np.random.shuffle(index)
            for row in index:
                x=self.x[row]
                y=self.y[row]
                self.update_weights(x,y)

    def predict_x(self, x):
        return self.feedforward(x)

    def predict(self, x):
        n = len(x)
        m = self.sizes[-1]
        ret = np.ones((n,m))
        for i in range(len(x)):
            ret[i,:] = self.feedforward(x[i])
        return ret

   and we   re done! now to test it we generate some synthetic data and
   supply the network with some id180.
def logistic(x):
    return 1.0/(1+np.exp(-x))

def logistic_prime(x):
    ex=np.exp(-x)
    return ex/(1+ex)**2

def identity(x):
    return x

def identity_prime(x):
    return 1

   first we will try to get it to approximate a sine curve.
#expit is a fast way to compute logistic using precomputed exp.
from scipy.special import expit
def test_regression(plots=false):
    #first create the data.
    n=200
    x=np.linspace(0,3*np.pi,num=n)
    x.shape=(n,1)
    y=np.sin(x)
    #we make a neural net with 2 hidden layers, 20 neurons in each, using logist
ic activation
    #functions.
    param=((1,0,0),(20, expit, logistic_prime),(20, expit, logistic_prime),(1,id
entity, identity_prime))
    #set learning rate.
    rates=[0.05]
    predictions=[]
    for rate in rates:
        n=neuralnetwork(x,y,param)
        n.train(4000, learning_rate=rate)
        predictions.append([rate,n.predict(x)])
    import matplotlib.pyplot as plt
    fig, ax=plt.subplots(1,1)
    if plots:
        ax.plot(x,y, label='sine', linewidth=2, color='black')
        for data in predictions:
            ax.plot(x,data[1],label="learning rate: "+str(data[0]))
        ax.legend()
test_regression(true)

   when i ran this it produced the following:
   [18]nn_approx_sine

   next we will try a classification problem with a nonlinear decision
   boundary, how about being above and below the sine curve?
def test_classification(plots=false):
    #number samples
    n=700

    n_iter=1500
    learning_rate=0.05

    #samples for true decision boundary plot
    l=np.linspace(0,3*np.pi,num=n)
    l = np.sin(l)

    #data inputs, training
    x = np.random.uniform(0, 3*np.pi, size=(n,2))
    x[:,1] *= 1/np.pi
    x[:,1]-= 1

    #data inputs, testing
    t = np.random.uniform(0, 3*np.pi, size=(n,2))
    t[:,1] *= 1/np.pi
    t[:,1] -= 1

    #data outputs
    y = np.sin(x[:,0]) <= x[:,1]

    #fitting
    param=((2,0,0),(30, expit, logistic_prime),(30, expit, logistic_prime),(1,ex
pit, logistic_prime))
    n=neuralnetwork(x,y, param)
    #training
    n.train(n_iter, learning_rate)
    predictions_training=n.predict(x)
    predictions_training= predictions_training <0.5
    predictions_training= predictions_training[:,0]
    #testing
    predictions_testing=n.predict(t)
    predictions_testing= predictions_testing <0.5
    predictions_testing= predictions_testing[:,0]

    #plotting
    import matplotlib.pyplot as plt
    fig, ax=plt.subplots(2,1)

    #training plot
    #we plot the predictions of the neural net blue for class 0, red for 1.
    ax[0].scatter(x[predictions_training,0], x[predictions_training,1], color='b
lue')
    not_index = np.logical_not(predictions_training)
    ax[0].scatter(x[not_index,0], x[not_index,1], color='red')
    ax[0].set_xlim(0, 3*np.pi)
    ax[0].set_ylim(-1,1)
    #true decision boundary
    ax[0].plot(l,l, color='black')
    #shade the areas according to how to they should be classified.
    ax[0].fill_between(l, l,y2=-1, alpha=0.5)
    ax[0].fill_between(l, l, y2=1, alpha=0.5, color='red')

    #testing plot
    ax[1].scatter(t[predictions_testing,0], t[predictions_testing,1], color='blu
e')
    not_index = np.logical_not(predictions_testing)
    ax[1].scatter(t[not_index,0], t[not_index,1], color='red')
    ax[1].set_xlim(0, 3*np.pi)
    ax[1].set_ylim(-1,1)
    ax[1].plot(l,l, color='black')
    ax[1].fill_between(l, l,y2=-1, alpha=0.5)
    ax[1].fill_between(l, l, y2=1, alpha=0.5, color='red')

test_classification()

   when i ran this it looked like this.
   [19]nn_sine_classification
   the top plot shows the performance on the training data, and the bottom
   the performance of on the testing data. the points are colored
   according to the net   s predictions, whereas the areas are shaded
   according to the true classifications.

   as you can see neural networks are capable of giving very good models,
   but the number of iterations and hidden nodes may be large. less
   iterations are possible using good configurations of the network i
   terms of sizes and numbers of hidden layers, and choosing the learning
   rate well, but it is difficult to know how to choose these well.

   it ended up being quicker to choose a larger number of hidden nodes, a
   large number of iterations, and a small learning rate, than to
   experiment finding a good choice. in the future i may write about how
   to use momentum to speed up training.

   the code for this post is available [20]here, please comment if
   anything is unclear or incorrect!
   advertisements

share this:

     * [21]twitter
     * [22]facebook
     *

like this:

   like loading...

related

   tags: [23]classification, [24]machine, [25]neural net, [26]neural
   network, [27]regression
   by [28]triangleinequality in [29]machine learning, [30]python on
   [31]march 31, 2014.
   [32]    neural networks part 1: feedingforward and id26
   [33]id178: expect a surprise    
     __________________________________________________________________

2 comments

    1. [34]lkafle says:
       [35]april 2, 2014 at 7:42 am
       reblogged this on [36]healthcare software solutions lava kafle
       kathmandu nepal lava prasad kafle lava kafle on google+ <a
       href="[37]https://plus.google.com/102726194262702292606&quot;
       rel="publisher">google+</a>.
       [38]reply
    2. [39]theano, autoencoders and mnist    triangleinequality says:
       [40]august 12, 2014 at 11:01 am
       [   ] with theano, and thanks to gpu computation, much faster also! i
       recommend having a look at my python implementation of
       id26 just so you can see the effort saved. this
       implementation benefited greatly from this [   ]
       [41]reply

leave a reply [42]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [43]googleplus-sign-in

     *
     *

   [44]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [45]log out /
   [46]change )
   google photo

   you are commenting using your google account. ( [47]log out /
   [48]change )
   twitter picture

   you are commenting using your twitter account. ( [49]log out /
   [50]change )
   facebook photo

   you are commenting using your facebook account. ( [51]log out /
   [52]change )
   [53]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

   [54]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [55]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [56]cookie policy

   iframe: [57]likes-master

   %d bloggers like this:

references

   visible links
   1. https://triangleinequality.wordpress.com/feed/
   2. https://triangleinequality.wordpress.com/comments/feed/
   3. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/feed/
   4. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
   5. https://triangleinequality.wordpress.com/2014/04/19/id178-expect-a-surprise/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/&for=wpcom-auto-discovery
   8. https://triangleinequality.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://triangleinequality.wordpress.com/
  11. https://triangleinequality.wordpress.com/about/
  12. https://triangleinequality.wordpress.com/contents/
  13. https://triangleinequality.wordpress.com/
  14. https://triangleinequality.wordpress.com/
  15. https://triangleinequality.wordpress.com/category/machine-learning/
  16. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  17. http://www.willamette.edu/~gorr/classes/cs449/backprop.html
  18. https://triangleinequality.files.wordpress.com/2014/03/nn_approx_sine.png
  19. https://triangleinequality.files.wordpress.com/2014/03/nn_sine_classification.png
  20. https://sourceforge.net/projects/triangleinequal/files/machine learning/nn.py/download
  21. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/?share=twitter
  22. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/?share=facebook
  23. https://triangleinequality.wordpress.com/tag/classification/
  24. https://triangleinequality.wordpress.com/tag/machine/
  25. https://triangleinequality.wordpress.com/tag/neural-net/
  26. https://triangleinequality.wordpress.com/tag/neural-network/
  27. https://triangleinequality.wordpress.com/tag/regression/
  28. https://triangleinequality.wordpress.com/author/triangleinequality/
  29. https://triangleinequality.wordpress.com/category/machine-learning/
  30. https://triangleinequality.wordpress.com/category/python/
  31. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/
  32. https://triangleinequality.wordpress.com/2014/03/27/neural-networks-part-1/
  33. https://triangleinequality.wordpress.com/2014/04/19/id178-expect-a-surprise/
  34. http://lkafle.wordpress.com/
  35. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/#comment-86
  36. http://lkafle.wordpress.com/2014/04/02/neural-networks-part-2-python-implementation/
  37. https://plus.google.com/102726194262702292606&quot
  38. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/?replytocom=86#respond
  39. https://triangleinequality.wordpress.com/2014/08/12/theano-autoencoders-and-mnist/
  40. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/#comment-148
  41. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/?replytocom=148#respond
  42. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/#respond
  43. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://triangleinequality.wordpress.com&color_scheme=light
  44. https://gravatar.com/site/signup/
  45. javascript:highlandercomments.doexternallogout( 'wordpress' );
  46. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/
  47. javascript:highlandercomments.doexternallogout( 'googleplus' );
  48. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/
  49. javascript:highlandercomments.doexternallogout( 'twitter' );
  50. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/
  51. javascript:highlandercomments.doexternallogout( 'facebook' );
  52. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/
  53. javascript:highlandercomments.cancelexternalwindow();
  54. https://wordpress.com/?ref=footer_blog
  55. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/
  56. https://automattic.com/cookies
  57. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  59. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/#comment-form-guest
  60. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/#comment-form-load-service:wordpress.com
  61. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/#comment-form-load-service:twitter
  62. https://triangleinequality.wordpress.com/2014/03/31/neural-networks-part-2/#comment-form-load-service:facebook
