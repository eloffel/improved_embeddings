   #[1]laurens van der maaten feed

   [2]laurens van der maaten

     * [3]curriculum vitae
     * [4]publications
     * [5]software
     * [6]contact

   id167 feature image

   laurens van der maaten bio photo

laurens van der maaten

   research scientist in machine learning and id161.
   [7]email [8]facebook [9]google+ [10]github

id167

overview

     * [11]implementations
     * [12]examples
     * [13]faq

   t-distributed stochastic neighbor embedding (id167) is a
   ([14]prize-winning) technique for id84 that is
   particularly well suited for the visualization of high-dimensional
   datasets. the technique can be implemented via barnes-hut
   approximations, allowing it to be applied on large real-world datasets.
   we applied it on data sets with up to 30 million examples. the
   technique and its variants are introduced in the following papers:
     * l.j.p. van der maaten. accelerating id167 using tree-based
       algorithms. journal of machine learning research 15(oct):3221-3245,
       2014. [15]pdf [[16]supplemental material]
     * l.j.p. van der maaten and g.e. hinton. visualizing non-metric
       similarities in multiple maps. machine learning 87(1):33-55, 2012.
       [17]pdf
     * l.j.p. van der maaten. learning a parametric embedding by
       preserving local structure. in proceedings of the twelfth
       international conference on artificial intelligence & statistics
       (ai-stats), jmlr w&cp 5:384-391, 2009. [18]pdf
     * l.j.p. van der maaten and g.e. hinton. visualizing high-dimensional
       data using id167. journal of machine learning research
       9(nov):2579-2605, 2008. [19]pdf [[20]supplemental material]
       [[21]talk]

   an accessible introduction to id167 and its variants is given in this
   [22]google techtalk.
     __________________________________________________________________

implementations

   below, implementations of id167 in various languages are available for
   download. some of these implementations were developed by me, and some
   by other contributors. for the standard id167 method, implementations
   in matlab, c++, cuda, python, torch, r, julia, and javascript are
   available. in addition, we provide a matlab implementation of
   parametric id167 (described [23]here). finally, we provide a barnes-hut
   implementation of id167 (described [24]here), which is the fastest
   id167 implementation to date, and which scales much better to big data
   sets.

   you are free to use, modify, or redistribute this software in any way
   you want, but only for non-commercial purposes. the use of the software
   is at your own risk; the authors are not responsible for any damage as
   a result from errors in the software.

   note: id167 is now built-in functionality [25]in matlab and [26]in
   spss!
   matlab implementation ([27]user guide) [28]all platforms
   cuda implementation (by [29]david, [30]roshan, and [31]forrest; see
   [32]paper) [33]all platforms
   python implementation [34]all platforms
   go implementation (by [35]daniel salvadori) [36]all platforms
   torch implementation [37]all platforms
   julia implementation (by leif jonsson) [38]all platforms
   java implementation (by leif jonsson) [39]all platforms
   r implementation (by [40]justin) [41]all platforms
   javascript implementation (by [42]andrej; [43]online demonstration)
   [44]all platforms
   parametric id167 (outdated; [45]see here) [46]all platforms
   barnes-hut id167 (c++, matlab, python, [47]torch, and [48]r wrappers;
   see [49]here) [50]all platforms / [51]github
   mnist dataset [52]matlab file
     __________________________________________________________________

examples

   some results of our experiments with id167 are available for download
   below. in the plots of the netflix dataset and the words dataset, the
   third dimension is encoded by means of a color encoding (similar
   words/movies are close together and have the same color). most of the
      errors    in the embeddings (such as in the 20 newsgroups) are actually
   due to    errors    in the features id167 was applied on. in many of these
   examples, the embeddings have a 1-nn error that is comparable to that
   of the original high-dimensional features.
   mnist dataset (in 2d) [53]jpg
   mnist dataset (in 3d) [54]mov
   olivetti faces dataset (in 2d) [55]jpg
   coil-20 dataset (in 2d) [56]jpg
   netflix dataset (in 3d) on [57]russ   s [58]rbm features [59]jpg
   words dataset (in 3d) on [60]andriy   s [61]semantic features [62]jpg
   20 newsgroups dataset (in 2d) on [63]simon   s [64]disclda features
   [65]jpg
   reuters dataset (in 2d) landmark id167 using [66]semantic hashing
   [67]jpg
   nips dataset (in 2d) on [68]co-authorship data (1988-2003) [69]jpg
   norb dataset (in 2d) by [70]vinod [71]jpg
   words (in 2d) by [72]joseph on [73]features learned by [74]ronan and
   [75]jason [76]png
   caltech-101 on sift bag-of-words features [77]jpg
   s&p 500 by [78]steve on information about daily returns on company
   stock [79]png
   interactive map of scientific journals on data by [80]nees-jan and
   [81]ludo, using [82]vosviewer [83]java 1.6
   relation between world economic forum councils [84]link
   id163 by [85]andrej on [86]caffe convolutional net features [87]link
   multiple maps visualizations [88]link
   allen brain data [89]link

   you may right-click on the images and select    show image in new tab    to
   see a larger version of each of the images.

   you may also be interested in these blog posts describing applications
   of id167 by [90]andrej karpathy, [91]paul mineiro, [92]alexander
   fabisch, [93]justin donaldson, [94]henry tan, and [95]cyrille rossant.
     __________________________________________________________________

faq

   i can   t figure out the file format for the binary implementations of
   id167?

   the format is described in the user   s guide. you also might want to
   have a look at the matlab or python wrapper code: it has code that
   writes the data-file and reads the results-file that can be ported
   fairly easily to other languages. please note that the file format is
   binary (so don   t try to write or read text!), and that it does not
   contain any spaces, separators, newlines or whatsoever.

   how can i asses the quality of the visualizations that id167
   constructed?

   preferably, just look at them! notice that id167 does not retain
   distances but probabilities, so measuring some error between the
   euclidean distances in high-d and low-d is useless. however, if you use
   the same data and perplexity, you can compare the kullback-leibler
   divergences that id167 reports. it is perfectly fine to run id167 ten
   times, and select the solution with the lowest kl divergence.

   how should i set the perplexity in id167?

   the performance of id167 is fairly robust under different settings of
   the perplexity. the most appropriate value depends on the density of
   your data. loosely speaking, one could say that a larger / denser
   dataset requires a larger perplexity. typical values for the perplexity
   range between 5 and 50.

   what is perplexity anyway?

   perplexity is a measure for information that is defined as 2 to the
   power of the shannon id178. the perplexity of a fair die with k sides
   is equal to k. in id167, the perplexity may be viewed as a knob that
   sets the number of effective nearest neighbors. it is comparable with
   the number of nearest neighbors k that is employed in many manifold
   learners.

   every time i run id167, i get a (slightly) different result?

   in contrast to, e.g., pca, id167 has a non-convex objective function.
   the objective function is minimized using a id119
   optimization that is initiated randomly. as a result, it is possible
   that different runs give you different solutions. notice that it is
   perfectly fine to run id167 a number of times (with the same data and
   parameters), and to select the visualization with the lowest value of
   the objective function as your final visualization.

   when i run id167, i get a strange    ball    with uniformly distributed
   points?

   this usually indicates you set your perplexity way too high. all points
   now want to be equidistant. the result you got is the closest you can
   get to equidistant points as is possible in two dimensions. if lowering
   the perplexity doesn   t help, you might have run into the problem
   described in the next question. similar effects may also occur when you
   use highly non-metric similarities as input.

   when i run id167, it reports a very low error but the results look
   crappy?

   presumably, your data contains some very large numbers, causing the
   binary search for the correct perplexity to fail. in the beginning of
   the optimization, id167 then reports a minimum, mean, and maximum value
   for sigma of 1. this is a sign that something went wrong! just divide
   your data or distances by a big number, and try again.

   i tried everything you said, but id167 still doesn   t seem to work very
   well?

   maybe there is something weird in your data. as a sanity check, try
   running pca on your data to reduce it to two dimensions. if this also
   gives bad results, then maybe there is not very much nice structure in
   your data in the first place. if pca works well but id167 doesn   t, i am
   fairly sure you did something wrong. just check your code again until
   you found the bug! if nothing works, feel free to drop me a line.

   can i use a pairwise euclidean distance matrix as input into id167?

   yes you can! download the matlab implementation, and use your pairwise
   euclidean distance matrix as input into the tsne_d.m function.

   can i use a pairwise similarity matrix as input into id167?

   yes you can! for instance, we successfully applied id167 on a dataset
   of word association data. download the matlab implementation, make sure
   the diagonal of the pairwise similarity matrix contains only zeros,
   symmetrize the pairwise similarity matrix, and normalize it to sum up
   to one. you can now use the result as input into the tsne_p.m function.

   can i use id167 to embed data in more than two dimensions?

   well, yes you can, but there is a catch. the key characteristic of
   id167 is that it solves a problem known as the crowding problem. the
   extent to which this problem occurs depends on the ratio between the
   intrinsic data dimensionality and the embedding dimensionality. so, if
   you embed in, say, thirty dimensions, the crowding problem is less
   severe than when you embed in two dimensions. as a result, it often
   works better if you increase the degrees of freedom of the
   t-distribution when embedding into thirty dimensions (or if you try to
   embed intrinsically very low-dimensional data such as the swiss roll).
   more details about this are described in [96]the ai-stats paper.

   why doesn   t id167 work as well as lle or isomap on the swiss roll data?

   when embedding the swiss roll data, the crowding problem does not
   apply. so you may have to use a lighter-tailed t-distribution to embed
   the swiss toll successfully (see above). but frankly    who cares about
   swiss rolls when you can embed complex real-world data nicely?

   once i have a id167 map, how can i embed incoming test points in that
   map?

   id167 learns a non-parametric mapping, which means that it does not
   learn an explicit function that maps data from the input space to the
   map. therefore, it is not possible to embed test points in an existing
   map (although you could re-run id167 on the full dataset). a potential
   approach to deal with this would be to train a multivariate regressor
   to predict the map location from the input data. alternatively, you
   could also make such a regressor minimize the id167 loss directly,
   which is what i did in [97]this paper.

      2019 laurens van der maaten.

references

   1. https://lvdmaaten.github.io/feed.xml
   2. https://lvdmaaten.github.io/
   3. https://lvdmaaten.github.io/cv/
   4. https://lvdmaaten.github.io/publications/
   5. https://lvdmaaten.github.io/software/
   6. https://lvdmaaten.github.io/contact/
   7. mailto:lvdmaaten@gmail.com
   8. http://facebook.com/lvdmaaten
   9. http://plus.google.com/+laurensvandermaaten
  10. http://github.com/lvdmaaten
  11. https://lvdmaaten.github.io/tsne/#implementations
  12. https://lvdmaaten.github.io/tsne/#examples
  13. https://lvdmaaten.github.io/tsne/#faq
  14. http://blog.kaggle.com/2012/11/02/t-distributed-stochastic-neighbor-embedding-wins-merck-viz-challenge/
  15. https://lvdmaaten.github.io/publications/papers/jmlr_2014.pdf
  16. https://lvdmaaten.github.io/publications/misc/supplement_jmlr_2014.pdf
  17. https://lvdmaaten.github.io/publications/papers/machlearn_2012.pdf
  18. https://lvdmaaten.github.io/publications/papers/aistats_2009.pdf
  19. https://lvdmaaten.github.io/publications/papers/jmlr_2008.pdf
  20. https://lvdmaaten.github.io/publications/misc/supplement_jmlr_2008.pdf
  21. https://www.youtube.com/watch?v=rjvl80gg3la&list=uutxkdgv1avog88pll8ngxmw
  22. https://www.youtube.com/watch?v=rjvl80gg3la&list=uutxkdgv1avog88pll8ngxmw
  23. https://lvdmaaten.github.io/publications/papers/aistats_2009.pdf
  24. https://lvdmaaten.github.io/publications/papers/jmlr_2014.pdf
  25. https://www.mathworks.com/help/stats/tsne.html
  26. https://www.ibm.com/support/knowledgecenter/en/ss3ra7_sub/modeler_mainhelp_client_ddita/clementine/python_nodes_tsne.html
  27. https://lvdmaaten.github.io/tsne/user_guide.pdf
  28. https://lvdmaaten.github.io/tsne/code/tsne_matlab.zip
  29. https://www.linkedin.com/in/david-m-chan/
  30. https://www.linkedin.com/in/roshanrao1/
  31. http://forresthuang.com/
  32. https://arxiv.org/pdf/1807.11824.pdf
  33. https://github.com/cannylab/tsne-cuda
  34. https://lvdmaaten.github.io/tsne/code/tsne_python.zip
  35. https://github.com/danaugrs
  36. https://github.com/danaugrs/go-tsne
  37. https://github.com/clementfarabet/manifold
  38. https://github.com/lejon/tsne.jl
  39. https://github.com/lejon/id167-java
  40. http://scwn.net/
  41. http://cran.r-project.org/web/packages/tsne/
  42. http://cs.stanford.edu/people/karpathy/
  43. http://homepage.tudelft.nl/19j49/tsnejs/
  44. http://cs.stanford.edu/people/karpathy/tsnejs/
  45. https://lvdmaaten.github.io/publications/papers/aistats_2009.pdf
  46. https://lvdmaaten.github.io/tsne/code/ptsne.tar.gz
  47. https://github.com/clementfarabet/manifold
  48. https://github.com/jkrijthe/rtsne
  49. https://lvdmaaten.github.io/publications/papers/jmlr_2014.pdf
  50. https://lvdmaaten.github.io/tsne/code/bh_tsne.tar.gz
  51. https://github.com/lvdmaaten/bhtsne/
  52. https://lvdmaaten.github.io/tsne/code/mnist.zip
  53. https://lvdmaaten.github.io/tsne/examples/mnist_tsne.jpg
  54. https://lvdmaaten.github.io/tsne/examples/mnist_tsne.mov
  55. https://lvdmaaten.github.io/tsne/examples/olivetti_tsne.jpg
  56. https://lvdmaaten.github.io/tsne/examples/coil_tsne.jpg
  57. http://www.cs.toronto.edu/~rsalakhu/
  58. http://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf
  59. https://lvdmaaten.github.io/tsne/examples/netflix_tsne.jpg
  60. http://www.cs.toronto.edu/~amnih/
  61. http://www.cs.toronto.edu/~hinton/absps/threenew.pdf
  62. https://lvdmaaten.github.io/tsne/examples/semantic_tsne.jpg
  63. http://www.di.ens.fr/~slacoste/
  64. http://snowbird.djvuzone.org/2008/abstracts/191.pdf
  65. https://lvdmaaten.github.io/tsne/examples/20news_tsne.jpg
  66. http://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf
  67. https://lvdmaaten.github.io/tsne/examples/reuters_tsne.jpg
  68. http://robotics.stanford.edu/~gal/data.html
  69. https://lvdmaaten.github.io/tsne/examples/nips_tsne.jpg
  70. http://www.cs.toronto.edu/~vnair/
  71. https://lvdmaaten.github.io/tsne/examples/norb_tsne.jpg
  72. http://joseph.turian.com/
  73. http://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf
  74. http://ronan.collobert.com/
  75. http://www.thespermwhale.com/jaseweston/
  76. http://www.cs.toronto.edu/~hinton/turian.png
  77. https://lvdmaaten.github.io/tsne/examples/caltech101_tsne.jpg
  78. https://www.linkedin.com/in/stevewickert
  79. https://lvdmaaten.github.io/tsne/examples/sp500_tsne.png
  80. http://www.neesjanvaneck.nl/
  81. http://www.ludowaltman.nl/
  82. http://www.vosviewer.com/
  83. http://www.vosviewer.com/vosviewer.php?title=journals id167 map&map=http://homepage.tudelft.nl/19j49/journal_tsne_map.txt&label_size_effect=0.33
  84. http://files.visualizing.org.s3.amazonaws.com/challeneges/wef/visualization/index.html
  85. http://cs.stanford.edu/people/karpathy/
  86. http://caffe.berkeleyvision.org/
  87. http://cs.stanford.edu/people/karpathy/id98embed/
  88. http://homepage.tudelft.nl/19j49/multiplemaps/multiple_maps_id167/multiple_maps_id167.html
  89. http://www.sciencedirect.com/science/article/pii/s1046202314003211
  90. https://karpathy.github.io/2014/07/02/visualizing-top-tweeps-with-id167-in-javascript/
  91. http://www.machinedlearnings.com/2011/06/even-better-hashtag-similarity.html
  92. http://nbviewer.ipython.org/urls/gist.githubusercontent.com/alexanderfabisch/1a0c648de22eff4a2a3e/raw/59d5bc5ed8f8bfd9ff1f7faa749d1b095aa97d5a/id167.ipynb
  93. http://scwn.net/
  94. http://www.codeproject.com/tips/788739/visualization-of-high-dimensional-data-using-id167
  95. https://beta.oreilly.com/learning/an-illustrated-introduction-to-the-id167-algorithm
  96. https://lvdmaaten.github.io/publications/papers/aistats_2009.pdf
  97. https://lvdmaaten.github.io/publications/papers/aistats_2009.pdf
