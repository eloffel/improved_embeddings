   #[1]curious insight

[2]curious insight
     __________________________________________________________________

   technology, software, data science, machine learning, entrepreneurship,
   investing, and various other topics

tags
     __________________________________________________________________

   [3]about [4]machine learning [5]data visualization [6]data science
   [7]big data [8]software development [9]emerging technology
   [10]entrepreneurship [11]investing [12]strategy [13]book review
   [14]random thoughts [15]curious insights

   copyright    curious insight. 2019     all rights reserved.

   proudly published with [16]ghost.
     *
     *
     *
     *
     *
     *

[17]curious insight

   [18]machine learning[19]data science[20]data visualization

machine learning exercises in python, part 1

   [21]5th december 2014
     __________________________________________________________________

   this post is part of a series covering the exercises from andrew ng's
   [22]machine learning class on coursera. the original code, exercise
   text, and data files for this post are available [23]here.

   [24]part 1 - simple id75
   [25]part 2 - multivariate id75
   [26]part 3 - id28
   [27]part 4 - multivariate id28
   [28]part 5 - neural networks
   [29]part 6 - support vector machines
   [30]part 7 - id116 id91 & pca
   [31]part 8 - anomaly detection & recommendation

   one of the pivotal moments in my professional development this year
   came when i discovered coursera. i'd heard of the "mooc" phenomenon but
   had not had the time to dive in and take a class. earlier this year i
   finally pulled the trigger and signed up for andrew ng's [32]machine
   learning class. i completed the whole thing from start to finish,
   including all of the programming exercises. the experience opened my
   eyes to the power of this type of education platform, and i've been
   hooked ever since.

   this blog post will be the first in a series covering the programming
   exercises from andrew's class. one aspect of the course that i didn't
   particularly care for was the use of octave for assignments. although
   octave/matlab is a fine platform, most real-world "data science" is
   done in either r or python (certainly there are other languages and
   tools being used, but these two are unquestionably at the top of the
   list). since i'm trying to develop my python skills, i decided to start
   working through the exercises from scratch in python. the full source
   code is available at [33]my ipython repo on github. you'll also find
   the data used in these exercises and the original exercise pdfs in
   sub-folders off the root directory if you're interested.

   while i can explain some of the concepts involved in this exercise
   along the way, it's impossible for me to convey all the information you
   might need to fully comprehend it. if you're really interested in
   machine learning but haven't been exposed to it yet, i encourage you to
   check out the class (it's completely free and there's no commitment
   whatsoever). with that, let's get started!

examining the data

   in the first part of exercise 1, we're tasked with implementing simple
   id75 to predict profits for a food truck. suppose you are
   the ceo of a restaurant franchise and are considering different cities
   for opening a new outlet. the chain already has trucks in various
   cities and you have data for profits and populations from the cities.
   you'd like to figure out what the expected profit of a new food truck
   might be given only the population of the city that it would be placed
   in.

   let's start by examining the data which is in a file called
   "ex1data1.txt" in the "data" directory of my repository above. first we
   need to import a few libraries.
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

   now let's get things rolling. we can use pandas to load the data into a
   data frame and display the first few rows using the "head" function.
path = os.getcwd() + '\data\ex1data1.txt'
data = pd.read_csv(path, header=none, names=['population', 'profit'])
data.head()

     population profit
   0 6.1101     17.5920
   1 5.5277     9.1302
   2 8.5186     13.6620
   3 7.0032     11.8540
   4 5.8598     6.8233

   another useful function that pandas provides out-of-the-box is the
   "describe" function, which calculates some basic statistics on a data
   set. this is helpful to get a "feel" for the data during the
   exploratory analysis stage of a project.
data.describe()

         population  profit
   count 97.000000  97.000000
   mean  8.159800   5.839135
    std  3.869884   5.510262
    min  5.026900   -2.680700
    25%  5.707700   1.986900
    50%  6.589400   4.562300
    75%  8.578100   7.046700
    max  22.203000  24.147000

   examining stats about your data can be helpful, but sometimes you need
   to find ways to visualize it too. fortunately this data set only has
   one dependent variable, so we can toss it in a scatter plot to get a
   better idea of what it looks like. we can use the "plot" function
   provided by pandas for this, which is really just a wrapper for
   matplotlib.
data.plot(kind='scatter', x='population', y='profit', figsize=(12,8))

   population vs. profit

   it really helps to actually look at what's going on, doesn't it? we can
   clearly see that there's a cluster of values around cities with smaller
   populations, and a somewhat linear trend of increasing profit as the
   size of the city increases. now let's get to the fun part -
   implementing a id75 algorithm in python from scratch!

implementing simple id75

   if you're not familiar with id75, it's an approach to
   modeling the relationship between a dependent variable and one or more
   independent variables (if there's one independent variable then it's
   called simple id75, and if there's more than one
   independent variable then it's called multiple id75).
   there are lots of different types and variances of id75
   that are outside the scope of this discussion so i won't go into that
   here, but to put it simply - we're trying to create a linear model of
   the data x, using some number of parameters theta, that describes the
   variance of the data such that given a new data point that's not in x,
   we could accurately predict what the outcome y would be without
   actually knowing what y is.

   in this implementation we're going to use an optimization technique
   called id119 to find the parameters theta. if you're
   familiar with id202, you may be aware that there's another way
   to find the optimal parameters for a linear model called the "normal
   equation" which basically solves the problem at once using a series of
   matrix calculations. however, the issue with this approach is that it
   doesn't scale very well for large data sets. in contrast, we can use
   variants of id119 and other optimization methods to scale to
   data sets of unlimited size, so for machine learning problems this
   approach is more practical.

   okay, that's enough theory. let's write some code. the first thing we
   need is a cost function. the cost function evaluates the quality of our
   model by calculating the error between our model's prediction for a
   data point, using the model parameters, and the actual data point. for
   example, if the population for a given city is 4 and we predicted that
   it was 7, our error is (7-4)^2 = 3^2 = 9 (assuming an l2 or "least
   squares" id168). we do this for each data point in x and sum
   the result to get the cost. here's the function:
def computecost(x, y, theta):
    inner = np.power(((x * theta.t) - y), 2)
    return np.sum(inner) / (2 * len(x))

   notice that there are no loops. we're taking advantage of numpy's
   linear algrebra capabilities to compute the result as a series of
   matrix operations. this is far more computationally efficient than an
   unoptimizted "for" loop.

   in order to make this cost function work seaid113ssly with the pandas
   data frame we created above, we need to do some manipulating. first, we
   need to insert a column of 1s at the beginning of the data frame in
   order to make the matrix operations work correctly (i won't go into
   detail on why this is needed, but it's in the exercise text if you're
   interested - basically it accounts for the intercept term in the linear
   equation). second, we need to separate our data into independent
   variables x and our dependent variable y.
# append a ones column to the front of the data set
data.insert(0, 'ones', 1)

# set x (training data) and y (target variable)
cols = data.shape[1]
x = data.iloc[:,0:cols-1]
y = data.iloc[:,cols-1:cols]

   finally, we're going to convert our data frames to numpy matrices and
   instantiate a parameter matirx.
# convert from data frames to numpy matrices
x = np.matrix(x.values)
y = np.matrix(y.values)
theta = np.matrix(np.array([0,0]))

   one useful trick to remember when debugging matrix operations is to
   look at the shape of the matrices you're dealing with. it's also
   helpful to remember when walking through the steps in your head that
   id127s look like (i x j) * (j x k) = (i x k), where i,
   j, and k are the shapes of the relative dimensions of the matrix.
x.shape, theta.shape, y.shape

   ((97l, 2l), (1l, 2l), (97l, 1l))

   okay, so now we can try out our cost function. remember the parameters
   were initialized to 0 so the solution isn't optimal yet, but we can see
   if it works.
computecost(x, y, theta)

   32.072733877455676

   so far so good. now we need to define a function to perform gradient
   descent on the parameters theta using the update rules defined in the
   exercise text. here's the function for id119:
def gradientdescent(x, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape))
    parameters = int(theta.ravel().shape[1])
    cost = np.zeros(iters)

    for i in range(iters):
        error = (x * theta.t) - y

        for j in range(parameters):
            term = np.multiply(error, x[:,j])
            temp[0,j] = theta[0,j] - ((alpha / len(x)) * np.sum(term))

        theta = temp
        cost[i] = computecost(x, y, theta)

    return theta, cost

   the idea with id119 is that for each iteration, we compute
   the gradient of the error term in order to figure out the appropriate
   direction to move our parameter vector. in other words, we're
   calculating the changes to make to our parameters in order to reduce
   the error, thus bringing our solution closer to the optimal solution
   (i.e best fit).

   this is a fairly complex topic and i could easily devote a whole blog
   post just to discussing id119. if you're interested in
   learning more, i would recommend starting with [34]this article and
   branching out from there.

   once again we're relying on numpy and id202 for our solution.
   you may notice that my implementation is not 100% optimal. in
   particular, there's a way to get rid of that inner loop and update all
   of the parameters at once. i'll leave it up to the reader to figure it
   out for now (i'll cover it in a later post).

   now that we've got a way to evaluate solutions, and a way to find a
   good solution, it's time to apply this to our data set.
# initialize variables for learning rate and iterations
alpha = 0.01
iters = 1000

# perform id119 to "fit" the model parameters
g, cost = gradientdescent(x, y, theta, alpha, iters)
g

   matrix([[-3.24140214, 1.1272942 ]])

   note that we've initialized a few new variables here. if you look
   closely at the id119 function, it has parameters called
   alpha and iters. alpha is the learning rate - it's a factor in the
   update rule for the parameters that helps determine how quickly the
   algorithm will converge to the optimal solution. iters is just the
   number of iterations. there is no hard and fast rule for how to
   initialize these parameters and typically some trial-and-error is
   involved.

   we now have a parameter vector descibing what we believe is the optimal
   linear model for our data set. one quick way to evaluate just how good
   our regression model is might be to look at the total error of our new
   solution on the data set:
computecost(x, y, g)

   4.5159555030789118

   that's certainly a lot better than 32, but it's not a very intuitive
   way to look at it. fortunately we have some other techniques at our
   disposal.

viewing the results

   we're now going to use matplotlib to visualize our solution. remember
   the scatter plot from before? let's overlay a line representing our
   model on top of a scatter plot of the data to see how well it fits. we
   can use numpy's "linspace" function to create an evenly-spaced series
   of points within the range of our data, and then "evaluate" those
   points using our model to see what the expected profit would be. we can
   then turn it into a line graph and plot it.
x = np.linspace(data.population.min(), data.population.max(), 100)
f = g[0, 0] + (g[0, 1] * x)

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(x, f, 'r', label='prediction')
ax.scatter(data.population, data.profit, label='traning data')
ax.legend(loc=2)
ax.set_xlabel('population')
ax.set_ylabel('profit')
ax.set_title('predicted profit vs. population size')

   predicted profit vs. population size

   not bad! our solution looks like and optimal linear model of the data
   set. since the gradient decent function also outputs a vector with the
   cost at each training iteration, we can plot that as well.
fig, ax = plt.subplots(figsize=(12,8))
ax.plot(np.arange(iters), cost, 'r')
ax.set_xlabel('iterations')
ax.set_ylabel('cost')
ax.set_title('error vs. training epoch')

   error vs. training epoch

   notice that the cost always decreases - this is an example of what's
   called a id76 problem. if you were to plot the entire
   solution space for the problem (i.e. plot the cost as a function of the
   model parameters for every possible value of the parameters) you would
   see that it looks like a "bowl" shape with a "basin" representing the
   optimal solution.

   that's all for now! in part 2 we'll finish off the first exercise by
   extending this example to more than 1 variable. i'll also show how the
   above solution can be reached by using a popular machine learning
   library called scikit-learn.

   follow me on twitter to get new post updates.
   [35]follow @jdwittenauer
   [36]machine learning[37]data science[38]data visualization
   author

[39]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

[40]curious insight

   author

[41]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

share article
     __________________________________________________________________

   [42]twitter [43]facebook [44]google+ [45]reddit [46]linkedin
   [47]pinterest

   copyright    curious insight. 2014     all rights reserved.

   proudly published with [48]ghost.

references

   visible links
   1. https://www.johnwittenauer.net/rss/
   2. https://www.johnwittenauer.net/
   3. https://www.johnwittenauer.net/about/
   4. https://www.johnwittenauer.net/tag/machine-learning/
   5. https://www.johnwittenauer.net/tag/data-visualization/
   6. https://www.johnwittenauer.net/tag/data-science/
   7. https://www.johnwittenauer.net/tag/big-data/
   8. https://www.johnwittenauer.net/tag/software-development/
   9. https://www.johnwittenauer.net/tag/emerging-technology/
  10. https://www.johnwittenauer.net/tag/entrepreneurship/
  11. https://www.johnwittenauer.net/tag/investing/
  12. https://www.johnwittenauer.net/tag/strategy/
  13. https://www.johnwittenauer.net/tag/book-review/
  14. https://www.johnwittenauer.net/tag/random-thoughts/
  15. https://www.johnwittenauer.net/tag/curious-insights/
  16. https://ghost.org/
  17. https://www.johnwittenauer.net/
  18. https://www.johnwittenauer.net/tag/machine-learning/
  19. https://www.johnwittenauer.net/tag/data-science/
  20. https://www.johnwittenauer.net/tag/data-visualization/
  21. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  22. https://www.coursera.org/course/ml
  23. https://github.com/jdwittenauer/ipython-notebooks
  24. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  25. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
  26. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
  27. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
  28. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
  29. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
  30. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
  31. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
  32. https://www.coursera.org/course/ml
  33. https://github.com/jdwittenauer/ipython-notebooks
  34. http://en.wikipedia.org/wiki/gradient_descent
  35. https://twitter.com/jdwittenauer
  36. https://www.johnwittenauer.net/tag/machine-learning/
  37. https://www.johnwittenauer.net/tag/data-science/
  38. https://www.johnwittenauer.net/tag/data-visualization/
  39. https://www.johnwittenauer.net/author/john/
  40. https://www.johnwittenauer.net/
  41. https://www.johnwittenauer.net/author/john/
  42. http://twitter.com/share?text=machine learning exercises in python, part 1&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  43. https://www.facebook.com/sharer/sharer.php?u=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  44. https://plus.google.com/share?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  45. http://www.reddit.com/submit?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/&title=machine learning exercises in python, part 1
  46. http://www.linkedin.com/sharearticle?mini=true&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/&title=machine learning exercises in python, part 1
  47. http://pinterest.com/pin/create/button/?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/&description=machine learning exercises in python, part 1
  48. https://ghost.org/

   hidden links:
  50. mailto:jdwittenauer@gmail.com
  51. https://twitter.com/jdwittenauer
  52. http://www.linkedin.com/in/jdwittenauer
  53. https://github.com/jdwittenauer
  54. http://www.kaggle.com/jdwittenauer
  55. https://www.johnwittenauer.net/rss/
  56. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
