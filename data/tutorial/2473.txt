   #[1]curious insight

[2]curious insight
     __________________________________________________________________

   technology, software, data science, machine learning, entrepreneurship,
   investing, and various other topics

tags
     __________________________________________________________________

   [3]about [4]machine learning [5]data visualization [6]data science
   [7]big data [8]software development [9]emerging technology
   [10]entrepreneurship [11]investing [12]strategy [13]book review
   [14]random thoughts [15]curious insights

   copyright    curious insight. 2019     all rights reserved.

   proudly published with [16]ghost.
     *
     *
     *
     *
     *
     *

[17]curious insight

   [18]machine learning[19]data science[20]data visualization

machine learning exercises in python, part 3

   [21]14th july 2015
     __________________________________________________________________

   this post is part of a series covering the exercises from andrew ng's
   [22]machine learning class on coursera. the original code, exercise
   text, and data files for this post are available [23]here.

   [24]part 1 - simple id75
   [25]part 2 - multivariate id75
   [26]part 3 - id28
   [27]part 4 - multivariate id28
   [28]part 5 - neural networks
   [29]part 6 - support vector machines
   [30]part 7 - id116 id91 & pca
   [31]part 8 - anomaly detection & recommendation

   in part 2 of the series we wrapped up our implementation of
   multivariate id75 using id119 and applied it to
   a simple housing prices data set. in this post we   re going to switch
   our objective from predicting a continuous value (regression) to
   classifying a result into two or more discrete buckets (classification)
   and apply it to a student admissions problem. suppose that you are the
   administrator of a university department and you want to determine each
   applicant's chance of admission based on their results on two exams.
   you have historical data from previous applicants that you can use as a
   training set. for each training example, you have the applicant's
   scores on two exams and the admissions decision. to accomplish this,
   we're going to build a classification model that estimates the
   id203 of admission based on the exam scores using a somewhat
   confusingly-named technique called id28.

id28

   you may be wondering     why are we using a    regression    algorithm on a
   classification problem? although the name seems to indicate otherwise,
   id28 is actually a classification algorithm. i suspect
   it   s named as such because it   s very similar to id75 in
   its learning approach, but the cost and gradient functions are
   formulated differently. in particular, id28 uses a
   sigmoid or    logit    activation function instead of the continuous output
   in id75 (hence the name). we   ll see more later on when we
   dive into the implementation.

   to get started, let   s import and examine the data set we   ll be working
   with.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

import os
path = os.getcwd() + '\data\ex2data1.txt'
data = pd.read_csv(path, header=none, names=['exam 1', 'exam 2', 'admitted'])
data.head()

      exam 1    exam 2   admitted
   0 34.623660 78.024693 0
   1 30.286711 43.894998 0
   2 35.847409 72.902198 0
   3 60.182599 86.308552 1
   4 79.032736 75.344376 1

   there are two continuous independent variables in the data -    exam 1   
   and    exam 2   . our prediction target is the    admitted    label, which is
   binary-valued. a value of 1 means the student was admitted and a value
   of 0 means the student was not admitted. let   s see this graphically
   with a scatter plot of the two scores and use color coding to visualize
   if the example is positive or negative.
positive = data[data['admitted'].isin([1])]
negative = data[data['admitted'].isin([0])]

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(positive['exam 1'], positive['exam 2'], s=50, c='b', marker='o', labe
l='admitted')
ax.scatter(negative['exam 1'], negative['exam 2'], s=50, c='r', marker='x', labe
l='not admitted')
ax.legend()
ax.set_xlabel('exam 1 score')
ax.set_ylabel('exam 2 score')

   from this plot we can see that there   s a nearly linear decision
   boundary. it curves a bit so we can   t classify all of the examples
   correctly using a straight line, but we should be able to get pretty
   close. now we need to implement id28 so we can train a
   model to find the optimal decision boundary and make class predictions.
   the first step is to implement the sigmoid function.
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

   this function is the    activation    function for the output of logistic
   regression. it converts a continuous input into a value between zero
   and one. this value can be interpreted as the class id203, or the
   likelihood that the input example should be classified positively.
   using this id203 along with a threshold value, we can obtain a
   discrete label prediction. it helps to visualize the function   s output
   to see what it   s really doing.
nums = np.arange(-10, 10, step=1)

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(nums, sigmoid(nums), 'r')

   our next step is to write the cost function. remember that the cost
   function evaluates the performance of the model on the training data
   given a set of model parameters. here   s the cost function for logistic
   regression.
def cost(theta, x, y):
    theta = np.matrix(theta)
    x = np.matrix(x)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(x * theta.t)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(x * theta.t)))
    return np.sum(first - second) / (len(x))

   note that we reduce the output down to a single scalar value, which is
   the sum of the    error    quantified as a function of the difference
   between the class id203 assigned by the model and the true label
   of the example. the implementation is completely vectorized     it   s
   computing the model   s predictions for the whole data set in one
   statement (sigmoid(x * theta.t)). if the math here isn   t making sense,
   refer to the exercise text i linked to above for a more detailed
   explanation.

   we can test the cost function to make sure it   s working, but first we
   need to do some setup.
# add a ones column - this makes the id127 work out easier
data.insert(0, 'ones', 1)

# set x (training data) and y (target variable)
cols = data.shape[1]
x = data.iloc[:,0:cols-1]
y = data.iloc[:,cols-1:cols]

# convert to numpy arrays and initalize the parameter array theta
x = np.array(x.values)
y = np.array(y.values)
theta = np.zeros(3)

   i like to check the shape of the data structures i   m working with
   fairly often to convince myself that their values are sensible. this
   technique if very useful when implementing id127.
x.shape, theta.shape, y.shape

   ((100l, 3l), (3l,), (100l, 1l))

   now let   s compute the cost for our initial solution given zeros for the
   model parameters, here represented as    theta   .
cost(theta, x, y)

   0.69314718055994529

   now that we have a working cost function, the next step is to write a
   function that computes the gradient of the model parameters to figure
   out how to change the parameters to improve the outcome of the model on
   the training data. recall that with id119 we don   t just
   randomly jigger around the parameter values and see what works best. at
   each training iteration we update the parameters in a way that   s
   guaranteed to move them in a direction that reduces the training error
   (i.e. the    cost   ). we can do this because the cost function is
   differentiable. the calculus involved in deriving the equation is well
   beyond the scope of this blog post, but the full equation is in the
   exercise text. here   s the function.
def gradient(theta, x, y):
    theta = np.matrix(theta)
    x = np.matrix(x)
    y = np.matrix(y)

    parameters = int(theta.ravel().shape[1])
    grad = np.zeros(parameters)

    error = sigmoid(x * theta.t) - y

    for i in range(parameters):
        term = np.multiply(error, x[:,i])
        grad[i] = np.sum(term) / len(x)

    return grad

   note that we don't actually perform id119 in this function -
   we just compute a single gradient step. in the exercise, an octave
   function called "fminunc" is used to optimize the parameters given
   functions to compute the cost and the gradients. since we're using
   python, we can use scipy's optimization api to do the same thing.
import scipy.optimize as opt
result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(x, y))
cost(result[0], x, y)

   0.20357134412164668

   we now have the optimal model parameters for our data set. next we need
   to write a function that will output predictions for a dataset x using
   our learned parameters theta. we can then use this function to score
   the training accuracy of our classifier.
def predict(theta, x):
    id203 = sigmoid(x * theta.t)
    return [1 if x >= 0.5 else 0 for x in id203]

theta_min = np.matrix(result[0])
predictions = predict(theta_min, x)
correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) i
n zip(predictions, y)]
accuracy = (sum(map(int, correct)) % len(correct))
print 'accuracy = {0}%'.format(accuracy)

   accuracy = 89%

   our id28 classifer correctly predicted if a student was
   admitted or not 89% of the time. not bad! keep in mind that this is
   training set accuracy though. we didn't keep a hold-out set or use
   cross-validation to get a true approximation of the accuracy so this
   number is likely higher than its true performance (this topic is
   covered in a later exercise).

regularized id28

   now that we have a working implementation of id28, we'll
   going to improve the algorithm by adding id173. id173
   is a term in the cost function that causes the algorithm to prefer
   "simpler" models (in this case, models will smaller coefficients). the
   theory is that this helps to minimize overfitting and improve the
   model's ability to generalize. we   ll apply our regularized version of
   id28 to a slightly more challenging problem. suppose you
   are the product manager of the factory and you have the test results
   for some microchips on two different tests. from these two tests, you
   would like to determine whether the microchips should be accepted or
   rejected. to help you make the decision, you have a data set of test
   results on past microchips, from which you can build a logistic
   regression model.

   let's start by visualizing the data.
path = os.getcwd() + '\data\ex2data2.txt'
data2 = pd.read_csv(path, header=none, names=['test 1', 'test 2', 'accepted'])

positive = data2[data2['accepted'].isin([1])]
negative = data2[data2['accepted'].isin([0])]

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(positive['test 1'], positive['test 2'], s=50, c='b', marker='o', labe
l='accepted')
ax.scatter(negative['test 1'], negative['test 2'], s=50, c='r', marker='x', labe
l='rejected')
ax.legend()
ax.set_xlabel('test 1 score')
ax.set_ylabel('test 2 score')

   this data looks a bit more complicated than the previous example. in
   particular, you'll notice that there is no linear decision boundary
   that will perform well on this data. one way to deal with this using a
   linear technique like id28 is to construct features that
   are derived from polynomials of the original features. we can try
   creating a bunch of polynomial features to feed into the classifier.
degree = 5
x1 = data2['test 1']
x2 = data2['test 2']

data2.insert(3, 'ones', 1)

for i in range(1, degree):
    for j in range(0, i):
        data2['f' + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)

data2.drop('test 1', axis=1, inplace=true)
data2.drop('test 2', axis=1, inplace=true)

data2.head()

     accepted ones    f10      f20       f21       f30      f31       f32
   0 1        1    0.051267  0.002628 0.035864  0.000135  0.001839 0.025089
   1 1        1    -0.092742 0.008601 -0.063523 -0.000798 0.005891 -0.043509
   2 1        1    -0.213710 0.045672 -0.147941 -0.009761 0.031616 -0.102412
   3 1        1    -0.375000 0.140625 -0.188321 -0.052734 0.070620 -0.094573
   4 1        1    -0.513250 0.263426 -0.238990 -0.135203 0.122661 -0.111283

   now we need to modify the cost and gradient functions to include the
   id173 term. in each case, the regularizer is added on to the
   previous calculation. here   s the updated cost function.
def costreg(theta, x, y, learningrate):
    theta = np.matrix(theta)
    x = np.matrix(x)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(x * theta.t)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(x * theta.t)))
    reg = (learningrate / 2 * len(x)) * np.sum(np.power(theta[:,1:theta.shape[1]
], 2))
    return np.sum(first - second) / (len(x)) + reg

   notice that we   ve added a new variable called    reg    that is a function
   of the parameter values. as the parameters get larger, the penalization
   added to the cost function increases. also note that we   ve added a new
      learning rate    parameter to the function. this is also part of the
   id173 term in the equation. the learning rate gives us a new
   hyper-parameter that we can use to tune how much weight the
   id173 holds in the cost function.

   next we   ll add id173 to the gradient function.
def gradientreg(theta, x, y, learningrate):
    theta = np.matrix(theta)
    x = np.matrix(x)
    y = np.matrix(y)

    parameters = int(theta.ravel().shape[1])
    grad = np.zeros(parameters)

    error = sigmoid(x * theta.t) - y

    for i in range(parameters):
        term = np.multiply(error, x[:,i])

        if (i == 0):
            grad[i] = np.sum(term) / len(x)
        else:
            grad[i] = (np.sum(term) / len(x)) + ((learningrate / len(x)) * theta
[:,i])

    return grad

   just as with the cost function, the id173 term is added on to
   the original calculation. however, unlike the cost function, we
   included logic to make sure that the first parameter is not
   regularized. the intuition behind this decision is that the first
   parameter is considered the    bias    or    intercept    of the model and
   shouldn   t be penalized.

   we can test out the new functions just as we did before.
# set x and y (remember from above that we moved the label to column 0)
cols = data2.shape[1]
x2 = data2.iloc[:,1:cols]
y2 = data2.iloc[:,0:1]

# convert to numpy arrays and initalize the parameter array theta
x2 = np.array(x2.values)
y2 = np.array(y2.values)
theta2 = np.zeros(11)

learningrate = 1

costreg(theta2, x2, y2, learningrate)

   0.6931471805599454

   we can also re-use the optimization code from earlier to find the
   optimal model parameters.
result2 = opt.fmin_tnc(func=costreg, x0=theta2, fprime=gradientreg, args=(x2, y2
, learningrate))
result2

   (array([ 0.35872309, -3.22200653, 18.97106363, -4.25297831,
   18.23053189, 20.36386672, 8.94114455, -43.77439015,
   -17.93440473, -50.75071857, -2.84162964]), 110, 1)

   finally, we can use the same method we applied earlier to create label
   predictions for the training data and evaluate the performance of the
   model.
theta_min = np.matrix(result2[0])
predictions = predict(theta_min, x2)
correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) i
n zip(predictions, y2)]
accuracy = (sum(map(int, correct)) % len(correct))
print 'accuracy = {0}%'.format(accuracy)

   accuracy = 91%

   that's all for part 3! in the next post in the series we'll expand on
   our implementation of id28 to tackle multi-class image
   classification.

   follow me on twitter to get new post updates.
   [32]follow @jdwittenauer
   [33]machine learning[34]data science[35]data visualization
   author

[36]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

[37]curious insight

   author

[38]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

share article
     __________________________________________________________________

   [39]twitter [40]facebook [41]google+ [42]reddit [43]linkedin
   [44]pinterest

   copyright    curious insight. 2015     all rights reserved.

   proudly published with [45]ghost.

references

   visible links
   1. https://www.johnwittenauer.net/rss/
   2. https://www.johnwittenauer.net/
   3. https://www.johnwittenauer.net/about/
   4. https://www.johnwittenauer.net/tag/machine-learning/
   5. https://www.johnwittenauer.net/tag/data-visualization/
   6. https://www.johnwittenauer.net/tag/data-science/
   7. https://www.johnwittenauer.net/tag/big-data/
   8. https://www.johnwittenauer.net/tag/software-development/
   9. https://www.johnwittenauer.net/tag/emerging-technology/
  10. https://www.johnwittenauer.net/tag/entrepreneurship/
  11. https://www.johnwittenauer.net/tag/investing/
  12. https://www.johnwittenauer.net/tag/strategy/
  13. https://www.johnwittenauer.net/tag/book-review/
  14. https://www.johnwittenauer.net/tag/random-thoughts/
  15. https://www.johnwittenauer.net/tag/curious-insights/
  16. https://ghost.org/
  17. https://www.johnwittenauer.net/
  18. https://www.johnwittenauer.net/tag/machine-learning/
  19. https://www.johnwittenauer.net/tag/data-science/
  20. https://www.johnwittenauer.net/tag/data-visualization/
  21. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
  22. https://www.coursera.org/course/ml
  23. https://github.com/jdwittenauer/ipython-notebooks
  24. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  25. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
  26. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
  27. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
  28. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
  29. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
  30. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
  31. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
  32. https://twitter.com/jdwittenauer
  33. https://www.johnwittenauer.net/tag/machine-learning/
  34. https://www.johnwittenauer.net/tag/data-science/
  35. https://www.johnwittenauer.net/tag/data-visualization/
  36. https://www.johnwittenauer.net/author/john/
  37. https://www.johnwittenauer.net/
  38. https://www.johnwittenauer.net/author/john/
  39. http://twitter.com/share?text=machine learning exercises in python, part 3&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
  40. https://www.facebook.com/sharer/sharer.php?u=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
  41. https://plus.google.com/share?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
  42. http://www.reddit.com/submit?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/&title=machine learning exercises in python, part 3
  43. http://www.linkedin.com/sharearticle?mini=true&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/&title=machine learning exercises in python, part 3
  44. http://pinterest.com/pin/create/button/?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/&description=machine learning exercises in python, part 3
  45. https://ghost.org/

   hidden links:
  47. mailto:jdwittenauer@gmail.com
  48. https://twitter.com/jdwittenauer
  49. http://www.linkedin.com/in/jdwittenauer
  50. https://github.com/jdwittenauer
  51. http://www.kaggle.com/jdwittenauer
  52. https://www.johnwittenauer.net/rss/
  53. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
