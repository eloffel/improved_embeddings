   #[1]curious insight

[2]curious insight
     __________________________________________________________________

   technology, software, data science, machine learning, entrepreneurship,
   investing, and various other topics

tags
     __________________________________________________________________

   [3]about [4]machine learning [5]data visualization [6]data science
   [7]big data [8]software development [9]emerging technology
   [10]entrepreneurship [11]investing [12]strategy [13]book review
   [14]random thoughts [15]curious insights

   copyright    curious insight. 2019     all rights reserved.

   proudly published with [16]ghost.
     *
     *
     *
     *
     *
     *

[17]curious insight

   [18]machine learning[19]data science[20]data visualization

machine learning exercises in python, part 4

   [21]13th march 2016
     __________________________________________________________________

   this post is part of a series covering the exercises from andrew ng's
   [22]machine learning class on coursera. the original code, exercise
   text, and data files for this post are available [23]here.

   [24]part 1 - simple id75
   [25]part 2 - multivariate id75
   [26]part 3 - id28
   [27]part 4 - multivariate id28
   [28]part 5 - neural networks
   [29]part 6 - support vector machines
   [30]part 7 - id116 id91 & pca
   [31]part 8 - anomaly detection & recommendation

   in part three of this series we implemented both simple and regularized
   id28, completing our python implementation of the second
   exercise from andrew ng's machine learning class. there's a limitation
   with our solution though - it only works for binary classification. in
   this post we'll extend our solution from the previous exercise to
   handle multi-class classification. in doing so, we'll cover the first
   half of exercise 3 and set ourselves up for the next big topic, neural
   networks.

   just a quick note on syntax - to show the output of a statement i'm
   appending the result in the code block with a ">" to indicate that it's
   the result of running the previous statement. if the result is very
   long (more than 1-2 lines) then i'm sticking it below the code block in
   a separate block of its own. hopefully it's clear which statements are
   input and which are output.

   our task in this exercise is to use id28 to recognize
   hand-written digits (0 to 9). let's get started by loading the data
   set. unlike the previous examples, our data file is in a format native
   to matlab and not automatically recognizable by pandas, so to load it
   in python we need to use a scipy utility (remember the data files are
   available at the link at the top of the post).
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.io import loadmat
%matplotlib inline

data = loadmat('data/ex3data1.mat')
data

{'x': array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
        [ 0.,  0.,  0., ...,  0.,  0.,  0.],
        [ 0.,  0.,  0., ...,  0.,  0.,  0.],
        ...,
        [ 0.,  0.,  0., ...,  0.,  0.,  0.],
        [ 0.,  0.,  0., ...,  0.,  0.,  0.],
        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),
 '__globals__': [],
 '__header__': 'matlab 5.0 mat-file, platform: glnxa64, created on: sun oct 16 1
3:09:09 2011',
 '__version__': '1.0',
 'y': array([[10],
        [10],
        [10],
        ...,
        [ 9],
        [ 9],
        [ 9]], dtype=uint8)}

   let's quickly examine the shape of the arrays we just loaded into
   memory.
data['x'].shape, data['y'].shape

> ((5000l, 400l), (5000l, 1l))

   great, we've got our data loaded. the images are represented in martix
   x as a 400-dimensional vector (of which there are 5,000 of them). the
   400 "features" are grayscale intensities of each pixel in the original
   20 x 20 image. the class labels are in the vector y as a numeric class
   representing the digit that's in the image. there's an illustration
   below gives an example of what some of the digits look like. each grey
   box with a white hand-drawn digit represents on 400-dimensional row in
   our dataset.

   our first task is to modify our id28 implementation to
   be completely vectorized (i.e. no "for" loops). this is because
   vectorized code, in addition to being short and concise, is able to
   take advantage of id202 optimizations and is typically much
   faster than iterative code. however if you look at our cost function
   implementation from exercise 2, it's already vectorized! so we can
   re-use the same implementation here. note we're skipping straight to
   the final, regularized version.
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def cost(theta, x, y, learningrate):
    theta = np.matrix(theta)
    x = np.matrix(x)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(x * theta.t)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(x * theta.t)))
    reg = (learningrate / 2 * len(x)) * np.sum(np.power(theta[:,1:theta.shape[1]
], 2))
    return np.sum(first - second) / (len(x)) + reg

   again, this cost function is identical to the one we created in the
   previous exercise so if you're not sure how we got to this point, check
   out the previous post before moving on.

   next we need the function that computes the gradient. we already
   defined this in the previous exercise, only in this case we do have a
   "for" loop in the update step that we need to get rid of. here's the
   original code for reference:
def gradient_with_loop(theta, x, y, learningrate):
    theta = np.matrix(theta)
    x = np.matrix(x)
    y = np.matrix(y)

    parameters = int(theta.ravel().shape[1])
    grad = np.zeros(parameters)

    error = sigmoid(x * theta.t) - y

    for i in range(parameters):
        term = np.multiply(error, x[:,i])

        if (i == 0):
            grad[i] = np.sum(term) / len(x)
        else:
            grad[i] = (np.sum(term) / len(x)) + ((learningrate / len(x)) * theta
[:,i])

    return grad

   let's take a step back and remind ourselves what's going on here. we
   just defined a cost function that says, in a nutshell, "given some
   candidate solution theta applied to input x, how far off is the result
   from the true desired outcome y". this is our method that evaluates a
   set of parameters. by contrast, the gradient function specifies how to
   change those parameters to get an answer that's slightly better than
   the one we've already got. the math behind how this all works can get a
   little hairy if you're not comfortable with id202, but it's
   laid out pretty well in the exercise text, and i would encourage the
   reader to get comfortable with it to better understand why these
   functions work.

   now we need to create a version of the gradient function that doesn't
   use any loops. in our new version we're going to pull out the "for"
   loop and compute the gradient for each parameter at once using linear
   algebra (except for the intercept parameter, which is not regularized
   so it's computed separately).

   also note that we're converting the data structures to numpy matrices
   (which i've used for the most part throughout these exercises). this is
   done in an attempt to make the code look more similar to octave than it
   would using arrays because matrices automatically follow matrix
   operation rules vs. element-wise operations, which is the default for
   arrays. there is some debate in the community over whether or not the
   matrix class should be used at all, but it's there so we're using it in
   these examples.
def gradient(theta, x, y, learningrate):
    theta = np.matrix(theta)
    x = np.matrix(x)
    y = np.matrix(y)

    parameters = int(theta.ravel().shape[1])
    error = sigmoid(x * theta.t) - y

    grad = ((x.t * error) / len(x)).t + ((learningrate / len(x)) * theta)

    # intercept gradient is not regularized
    grad[0, 0] = np.sum(np.multiply(error, x[:,0])) / len(x)

    return np.array(grad).ravel()

   now that we've defined our cost and gradient functions, it's time to
   build a classifier. for this task we've got 10 possible classes, and
   since id28 is only able to distiguish between 2 classes
   at a time, we need a strategy to deal with the multi-class scenario. in
   this exercise we're tasked with implementing a one-vs-all
   classification approach, where a label with k different classes results
   in k classifiers, each one deciding between "class i" and "not class i"
   (i.e. any class other than i). we're going to wrap the classifier
   training up in one function that computes the final weights for each of
   the 10 classifiers and returns the weights as a k x (n + 1) array,
   where n is the number of parameters.
from scipy.optimize import minimize

def one_vs_all(x, y, num_labels, learning_rate):
    rows = x.shape[0]
    params = x.shape[1]

    # k x (n + 1) array for the parameters of each of the k classifiers
    all_theta = np.zeros((num_labels, params + 1))

    # insert a column of ones at the beginning for the intercept term
    x = np.insert(x, 0, values=np.ones(rows), axis=1)

    # labels are 1-indexed instead of 0-indexed
    for i in range(1, num_labels + 1):
        theta = np.zeros(params + 1)
        y_i = np.array([1 if label == i else 0 for label in y])
        y_i = np.reshape(y_i, (rows, 1))

        # minimize the objective function
        fmin = minimize(fun=cost, x0=theta, args=(x, y_i, learning_rate), method
='tnc', jac=gradient)
        all_theta[i-1,:] = fmin.x

    return all_theta

   a few things to note here...first, we're adding an extra parameter to
   theta (along with a column of ones to the training data) to account for
   the intercept term. second, we're transforming y from a class label to
   a binary value for each classifier (either is class i or is not class
   i). finally, we're using scipy's newer optimization api to minimize the
   cost function for each classifier. the api takes an objective function,
   an initial set of parameters, an optimization method, and a jacobian
   (gradient) function if specified. the parameters found by the
   optimization routine are then assigned to the parameter array.

   one of the more challenging parts of implementing vectorized code is
   getting all of the matrix interactions written correctly, so i find it
   useful to do some sanity checks by looking at the shapes of the
   arrays/matrices i'm working with and convincing myself that they're
   sensible. let's look at some of the data structures used in the above
   function.
rows = data['x'].shape[0]
params = data['x'].shape[1]

all_theta = np.zeros((10, params + 1))

x = np.insert(data['x'], 0, values=np.ones(rows), axis=1)

theta = np.zeros(params + 1)

y_0 = np.array([1 if label == 0 else 0 for label in data['y']])
y_0 = np.reshape(y_0, (rows, 1))

x.shape, y_0.shape, theta.shape, all_theta.shape

> ((5000l, 401l), (5000l, 1l), (401l,), (10l, 401l))

   these all appear to make sense. note that theta is a one-dimensional
   array, so when it gets converted to a matrix in the code that computes
   the gradient, it turns into a (1 x 401) matrix. let's also check the
   class labels in y to make sure they look like what we're expecting.
np.unique(data['y'])

> array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=uint8)

   let's make sure that our training function actually runs, and we get
   some sensible outputs, before going any further.
all_theta = one_vs_all(data['x'], data['y'], 10, 1)
all_theta

array([[ -5.79312170e+00,   0.00000000e+00,   0.00000000e+00, ...,
          1.22140973e-02,   2.88611969e-07,   0.00000000e+00],
       [ -4.91685285e+00,   0.00000000e+00,   0.00000000e+00, ...,
          2.40449128e-01,  -1.08488270e-02,   0.00000000e+00],
       [ -8.56840371e+00,   0.00000000e+00,   0.00000000e+00, ...,
         -2.59241796e-04,  -1.12756844e-06,   0.00000000e+00],
       ...,
       [ -1.32641613e+01,   0.00000000e+00,   0.00000000e+00, ...,
         -5.63659404e+00,   6.50939114e-01,   0.00000000e+00],
       [ -8.55392716e+00,   0.00000000e+00,   0.00000000e+00, ...,
         -2.01206880e-01,   9.61930149e-03,   0.00000000e+00],
       [ -1.29807876e+01,   0.00000000e+00,   0.00000000e+00, ...,
          2.60651472e-04,   4.22693052e-05,   0.00000000e+00]])

   we're now ready for the final step - using the trained classifiers to
   predict a label for each image. for this step we're going to compute
   the class id203 for each class, for each training instance (using
   vectorized code of course!) and assign the output class label as the
   class with the highest id203.
def predict_all(x, all_theta):
    rows = x.shape[0]
    params = x.shape[1]
    num_labels = all_theta.shape[0]

    # same as before, insert ones to match the shape
    x = np.insert(x, 0, values=np.ones(rows), axis=1)

    # convert to matrices
    x = np.matrix(x)
    all_theta = np.matrix(all_theta)

    # compute the class id203 for each class on each training instance
    h = sigmoid(x * all_theta.t)

    # create array of the index with the maximum id203
    h_argmax = np.argmax(h, axis=1)

    # because our array was zero-indexed we need to add one for the true label p
rediction
    h_argmax = h_argmax + 1

    return h_argmax

   now we can use the predict_all function to generate class predictions
   for each instance and see how well our classifier works.
y_pred = predict_all(data['x'], all_theta)
correct = [1 if a == b else 0 for (a, b) in zip(y_pred, data['y'])]
accuracy = (sum(map(int, correct)) / float(len(correct)))
print 'accuracy = {0}%'.format(accuracy * 100)

> accuracy = 97.58%

   close to 98% is actually pretty good for a relatively simple method
   like id28. we can get much, much better though. in the
   next post, we'll see how to improve on this result by implementing a
   feed-forward neural network from scratch and applying it to the same
   image classification task.

   follow me on twitter to get new post updates.
   [32]follow @jdwittenauer
   [33]machine learning[34]data science[35]data visualization
   author

[36]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

[37]curious insight

   author

[38]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

share article
     __________________________________________________________________

   [39]twitter [40]facebook [41]google+ [42]reddit [43]linkedin
   [44]pinterest

   copyright    curious insight. 2016     all rights reserved.

   proudly published with [45]ghost.

references

   visible links
   1. https://www.johnwittenauer.net/rss/
   2. https://www.johnwittenauer.net/
   3. https://www.johnwittenauer.net/about/
   4. https://www.johnwittenauer.net/tag/machine-learning/
   5. https://www.johnwittenauer.net/tag/data-visualization/
   6. https://www.johnwittenauer.net/tag/data-science/
   7. https://www.johnwittenauer.net/tag/big-data/
   8. https://www.johnwittenauer.net/tag/software-development/
   9. https://www.johnwittenauer.net/tag/emerging-technology/
  10. https://www.johnwittenauer.net/tag/entrepreneurship/
  11. https://www.johnwittenauer.net/tag/investing/
  12. https://www.johnwittenauer.net/tag/strategy/
  13. https://www.johnwittenauer.net/tag/book-review/
  14. https://www.johnwittenauer.net/tag/random-thoughts/
  15. https://www.johnwittenauer.net/tag/curious-insights/
  16. https://ghost.org/
  17. https://www.johnwittenauer.net/
  18. https://www.johnwittenauer.net/tag/machine-learning/
  19. https://www.johnwittenauer.net/tag/data-science/
  20. https://www.johnwittenauer.net/tag/data-visualization/
  21. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
  22. https://www.coursera.org/course/ml
  23. https://github.com/jdwittenauer/ipython-notebooks
  24. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  25. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
  26. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
  27. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
  28. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
  29. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
  30. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
  31. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
  32. https://twitter.com/jdwittenauer
  33. https://www.johnwittenauer.net/tag/machine-learning/
  34. https://www.johnwittenauer.net/tag/data-science/
  35. https://www.johnwittenauer.net/tag/data-visualization/
  36. https://www.johnwittenauer.net/author/john/
  37. https://www.johnwittenauer.net/
  38. https://www.johnwittenauer.net/author/john/
  39. http://twitter.com/share?text=machine learning exercises in python, part 4&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
  40. https://www.facebook.com/sharer/sharer.php?u=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
  41. https://plus.google.com/share?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
  42. http://www.reddit.com/submit?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/&title=machine learning exercises in python, part 4
  43. http://www.linkedin.com/sharearticle?mini=true&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/&title=machine learning exercises in python, part 4
  44. http://pinterest.com/pin/create/button/?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/&description=machine learning exercises in python, part 4
  45. https://ghost.org/

   hidden links:
  47. mailto:jdwittenauer@gmail.com
  48. https://twitter.com/jdwittenauer
  49. http://www.linkedin.com/in/jdwittenauer
  50. https://github.com/jdwittenauer
  51. http://www.kaggle.com/jdwittenauer
  52. https://www.johnwittenauer.net/rss/
  53. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
