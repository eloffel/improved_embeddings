   #[1]curious insight

[2]curious insight
     __________________________________________________________________

   technology, software, data science, machine learning, entrepreneurship,
   investing, and various other topics

tags
     __________________________________________________________________

   [3]about [4]machine learning [5]data visualization [6]data science
   [7]big data [8]software development [9]emerging technology
   [10]entrepreneurship [11]investing [12]strategy [13]book review
   [14]random thoughts [15]curious insights

   copyright    curious insight. 2019     all rights reserved.

   proudly published with [16]ghost.
     *
     *
     *
     *
     *
     *

[17]curious insight

   [18]machine learning[19]data science

machine learning exercises in python, part 5

   [20]30th may 2016
     __________________________________________________________________

   this post is part of a series covering the exercises from andrew ng's
   [21]machine learning class on coursera. the original code, exercise
   text, and data files for this post are available [22]here.

   [23]part 1 - simple id75
   [24]part 2 - multivariate id75
   [25]part 3 - id28
   [26]part 4 - multivariate id28
   [27]part 5 - neural networks
   [28]part 6 - support vector machines
   [29]part 7 - id116 id91 & pca
   [30]part 8 - anomaly detection & recommendation

   in part four we wrapped up our implementation of id28 by
   extending our solution to handle multi-class classification and testing
   it on the hand-written digits data set. using just id28
   we were able to hit a classification accuracy of about 97.5%, which is
   reasonably good but pretty much maxes out what we can achieve with a
   linear model. in this blog post we'll again tackle the hand-written
   digits data set, but this time using a feed-forward neural network with
   id26. we'll implement un-regularized and regularized
   versions of the neural network cost function and compute gradients via
   the id26 algorithm. finally, we'll run the algorithm through
   an optimizer and evaluate the performance of the network on the
   handwritten digits data set

   i'll note up front that the math (and code) in this exercise gets a bit
   hairy. implementing a neural network from scratch is not for the faint
   of heart. for those that venture on, be warned - here be dragons. i
   also strongly encourage the reader to skim through the corresponding
   exercise text from andrew ng's class. i uploaded a copy [31]here. this
   text contains all of the equations that we'll be implementing. with
   that, let's dive in!

   since the data set is the same one we used in the last exercise, we'll
   re-use the code from last time to load the data.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.io import loadmat
%matplotlib inline

data = loadmat('data/ex3data1.mat')
data

{'x': array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
        [ 0.,  0.,  0., ...,  0.,  0.,  0.],
        [ 0.,  0.,  0., ...,  0.,  0.,  0.],
        ...,
        [ 0.,  0.,  0., ...,  0.,  0.,  0.],
        [ 0.,  0.,  0., ...,  0.,  0.,  0.],
        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),
 '__globals__': [],
 '__header__': 'matlab 5.0 mat-file, platform: glnxa64, created on: sun oct 16 1
3:09:09 2011',
 '__version__': '1.0',
 'y': array([[10],
        [10],
        [10],
        ...,
        [ 9],
        [ 9],
        [ 9]], dtype=uint8)}

   since we're going to need these later (and will use them often), let's
   create some useful variables up-front.
x = data['x']
y = data['y']
x.shape, y.shape

((5000l, 400l), (5000l, 1l))

   we're also going to need to one-hot encode our labels. one-hot encoding
   turns a class label \(n\) (out of \(k\) classes) into a vector of
   length \(k\) where index \(n\) is "hot" (1) while the rest are zero.
   scikit-learn has a built in utility we can use for this.
from sklearn.preprocessing import onehotencoder
encoder = onehotencoder(sparse=false)
y_onehot = encoder.fit_transform(y)
y_onehot.shape

(5000l, 10l)

y[0], y_onehot[0,:]

(array([10], dtype=uint8),
 array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]))

   the neural network we're going to build for this exercise has an input
   layer matching the size of our instance data (400 + the bias unit), a
   hidden layer with 25 units (26 with the bias unit), and an output layer
   with 10 units corresponding to our one-hot encoding for the class
   labels. the first piece we need to implement is a cost function to
   evaluate the loss for a given set of network parameters. the source
   mathematical function is in the exercise text, and looks pretty
   intimidating, but it helps to really break it down into pieces. here
   are the functions required to compute the cost.
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def forward_propagate(x, theta1, theta2):
    m = x.shape[0]

    a1 = np.insert(x, 0, values=np.ones(m), axis=1)
    z2 = a1 * theta1.t
    a2 = np.insert(sigmoid(z2), 0, values=np.ones(m), axis=1)
    z3 = a2 * theta2.t
    h = sigmoid(z3)

    return a1, z2, a2, z3, h

def cost(params, input_size, hidden_size, num_labels, x, y, learning_rate):
    m = x.shape[0]
    x = np.matrix(x)
    y = np.matrix(y)

    # reshape the parameter array into parameter matrices for each layer
    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidd
en_size, (input_size + 1))))
    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_
labels, (hidden_size + 1))))

    # run the feed-forward pass
    a1, z2, a2, z3, h = forward_propagate(x, theta1, theta2)

    # compute the cost
    j = 0
    for i in range(m):
        first_term = np.multiply(-y[i,:], np.log(h[i,:]))
        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))
        j += np.sum(first_term - second_term)

    j = j / m

    return j

   we've used the sigmoid function before so that's not new. the
   forward-propagate function computes the hypothesis for each training
   instance given the current parameters (in other words, given some
   current state of the network and a set of inputs, it calculates the
   outputs at each layer in the network). the shape of the hypothesis
   vector (denoted by \(h\)), which contains the prediction probabilities
   for each class, should match our one-hot encoding for y. finally, the
   cost function runs the forward-propagation step and calculates the
   error of the hypothesis (predictions) vs. the true label for the
   instance.

   we can test this real quick to convince ourselves that it's working as
   expected. seeing the output from intermediate steps is also useful to
   understand what's going on.
# initial setup
input_size = 400
hidden_size = 25
num_labels = 10
learning_rate = 1

# randomly initialize a parameter array of the size of the full network's parame
ters
params = (np.random.random(size=hidden_size * (input_size + 1) + num_labels * (h
idden_size + 1)) - 0.5) * 0.25

m = x.shape[0]
x = np.matrix(x)
y = np.matrix(y)

# unravel the parameter array into parameter matrices for each layer
theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_s
ize, (input_size + 1))))
theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labe
ls, (hidden_size + 1))))

theta1.shape, theta2.shape

((25l, 401l), (10l, 26l))

a1, z2, a2, z3, h = forward_propagate(x, theta1, theta2)
a1.shape, z2.shape, a2.shape, z3.shape, h.shape

((5000l, 401l), (5000l, 25l), (5000l, 26l), (5000l, 10l), (5000l, 10l))

   the cost function, after computing the hypothesis matrix \(h\), applies
   the cost equation to compute the total error between \(y\) and \(h\).
cost(params, input_size, hidden_size, num_labels, x, y_onehot, learning_rate)

6.8228086634127862

   our next step is to add id173 to the cost function, which adds
   a penalty term to the cost that scales with the magnitude of the
   parameters. the equation for this looks pretty ugly, but it can be
   boiled down to just one line of code added to the original cost
   function. just add the following right before the return statement.
j += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.
sum(np.power(theta2[:,1:], 2)))

   next up is the id26 algorithm. id26 computes the
   parameter updates that will reduce the error of the network on the
   training data. the first thing we need is a function that computes the
   gradient of the sigmoid function we created earlier.
def sigmoid_gradient(z):
    return np.multiply(sigmoid(z), (1 - sigmoid(z)))

   now we're ready to implement id26 to compute the gradients.
   since the computations required for id26 are a superset of
   those required in the cost function, we're actually going to extend the
   cost function to also perform id26 and return both the cost
   and the gradients. if you're wondering why i'm not just calling the
   existing cost function from within the backprop function to make the
   design more modular, it's because backprop uses a number of other
   variables calculated inside the cost function. here's the full
   implementation. i skipped ahead and added gradient id173
   rather than first create an un-regularized version.
def backprop(params, input_size, hidden_size, num_labels, x, y, learning_rate):
    ##### this section is identical to the cost function logic we already saw ##
###
    m = x.shape[0]
    x = np.matrix(x)
    y = np.matrix(y)

    # reshape the parameter array into parameter matrices for each layer
    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidd
en_size, (input_size + 1))))
    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_
labels, (hidden_size + 1))))

    # run the feed-forward pass
    a1, z2, a2, z3, h = forward_propagate(x, theta1, theta2)

    # initializations
    j = 0
    delta1 = np.zeros(theta1.shape)  # (25, 401)
    delta2 = np.zeros(theta2.shape)  # (10, 26)

    # compute the cost
    for i in range(m):
        first_term = np.multiply(-y[i,:], np.log(h[i,:]))
        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))
        j += np.sum(first_term - second_term)

    j = j / m

    # add the cost id173 term
    j += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) +
 np.sum(np.power(theta2[:,1:], 2)))

    ##### end of cost function logic, below is the new part #####

    # perform id26
    for t in range(m):
        a1t = a1[t,:]  # (1, 401)
        z2t = z2[t,:]  # (1, 25)
        a2t = a2[t,:]  # (1, 26)
        ht = h[t,:]  # (1, 10)
        yt = y[t,:]  # (1, 10)

        d3t = ht - yt  # (1, 10)

        z2t = np.insert(z2t, 0, values=np.ones(1))  # (1, 26)
        d2t = np.multiply((theta2.t * d3t.t).t, sigmoid_gradient(z2t))  # (1, 26
)

        delta1 = delta1 + (d2t[:,1:]).t * a1t
        delta2 = delta2 + d3t.t * a2t

    delta1 = delta1 / m
    delta2 = delta2 / m

    # add the gradient id173 term
    delta1[:,1:] = delta1[:,1:] + (theta1[:,1:] * learning_rate) / m
    delta2[:,1:] = delta2[:,1:] + (theta2[:,1:] * learning_rate) / m

    # unravel the gradient matrices into a single array
    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))

    return j, grad

   there's a lot going on here so let's try to unpack it a bit. the first
   half of the function is calculating the error by running the data plus
   current parameters through the "network" (the forward-propagate
   function) and comparing the output to the true labels. the total error
   across the whole data set is represented as \(j\). this is the part we
   discussed earlier as the cost function.

   the rest of the function is essentially answering the question "how can
   i adjust my parameters to reduce the error the next time i run through
   the network"? it does this by computing the contributions at each layer
   to the total error and adjusting appropriately by coming up with a
   "gradient" matrix (or, how much to change each parameter and in what
   direction).

   the hardest part of the backprop computation (other than understanding
   why we're doing all these calculations) is getting the matrix
   dimensions right, which is why i annotated some of the calculations
   with comments showing the resulting shape of the computation. by the
   way, if you find it confusing when to use a * b vs. np.multiply(a, b),
   you're not alone. basically the former is a id127 and
   the latter is an element-wise multiplication (unless a or b is a scalar
   value, in which case it doesn't matter). i wish there was a more
   concise syntax for this (maybe there is and i'm just not aware of it).

   anyway, let's test it out to make sure the function returns what we're
   expecting it to.
j, grad = backprop(params, input_size, hidden_size, num_labels, x, y_onehot, lea
rning_rate)
j, grad.shape

(6.8281541822949299, (10285l,))

   we're finally ready to train our network and use it to make
   predictions. this is roughly similar to the previous exercise with
   multi-class id28.
from scipy.optimize import minimize

# minimize the objective function
fmin = minimize(fun=backprop, x0=params, args=(input_size, hidden_size, num_labe
ls, x, y_onehot, learning_rate),
                method='tnc', jac=true, options={'maxiter': 250})
fmin

 status: 3
 success: false
    nfev: 250
     fun: 0.33900736818312283
       x: array([ -8.85740564e-01,   2.57420350e-04,  -4.09396202e-04, ...,
         1.44634791e+00,   1.68974302e+00,   7.10121593e-01])
 message: 'max. number of function evaluations reach'
     jac: array([ -5.11463703e-04,   5.14840700e-08,  -8.18792403e-08, ...,
        -2.48297749e-04,  -3.17870911e-04,  -3.31404592e-04])
     nit: 21

   we put a bound on the number of iterations since the objective function
   is not likely to completely converge. our total cost has dropped below
   0.5 though so that's a good indicator that the algorithm is working.
   let's use the parameters it found and forward-propagate them through
   the network to get some predictions. we have to reshape the output from
   the optimizer to match the parameter matrix shapes that our network is
   expecting, then run the forward propagation to generate a hypothesis
   for the input data.
x = np.matrix(x)
theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_s
ize, (input_size + 1))))
theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):], (num_labe
ls, (hidden_size + 1))))

a1, z2, a2, z3, h = forward_propagate(x, theta1, theta2)
y_pred = np.array(np.argmax(h, axis=1) + 1)
y_pred

array([[10],
       [10],
       [10],
       ...,
       [ 9],
       [ 9],
       [ 9]], dtype=int64)

   finally we can compute the accuracy to see how well our trained network
   is doing.
correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]
accuracy = (sum(map(int, correct)) / float(len(correct)))
print 'accuracy = {0}%'.format(accuracy * 100)

accuracy = 99.22%

   and we're done! we've successfully implemented a rudimentary
   feed-forward neural network with id26 and used it to
   classify images of handwritten digits. it might seem surprising at
   first that we managed to do this without implementing any classes or
   data structures resembling a network in any way. isn't that what neural
   networks are all about? this was one of the biggest surprises to me
   when i took the class - how the whole thing basically boils down to a
   series of id127s. it turns out that this is by far the
   most efficient way to solve the problem. in fact if you look at any of
   the popular deep learning frameworks such as tensorflow, they're
   essentially graphs of id202 computations. it's a very useful
   and practical way to think about machine learning algorithms.

   that concludes the exercise on neural networks. in the next exercise
   we'll look at another popular supervised learning algorithm, support
   vector machines.

   follow me on twitter to get new post updates.
   [32]follow @jdwittenauer
   [33]machine learning[34]data science
   author

[35]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

[36]curious insight

   author

[37]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

share article
     __________________________________________________________________

   [38]twitter [39]facebook [40]google+ [41]reddit [42]linkedin
   [43]pinterest

   copyright    curious insight. 2016     all rights reserved.

   proudly published with [44]ghost.

references

   visible links
   1. https://www.johnwittenauer.net/rss/
   2. https://www.johnwittenauer.net/
   3. https://www.johnwittenauer.net/about/
   4. https://www.johnwittenauer.net/tag/machine-learning/
   5. https://www.johnwittenauer.net/tag/data-visualization/
   6. https://www.johnwittenauer.net/tag/data-science/
   7. https://www.johnwittenauer.net/tag/big-data/
   8. https://www.johnwittenauer.net/tag/software-development/
   9. https://www.johnwittenauer.net/tag/emerging-technology/
  10. https://www.johnwittenauer.net/tag/entrepreneurship/
  11. https://www.johnwittenauer.net/tag/investing/
  12. https://www.johnwittenauer.net/tag/strategy/
  13. https://www.johnwittenauer.net/tag/book-review/
  14. https://www.johnwittenauer.net/tag/random-thoughts/
  15. https://www.johnwittenauer.net/tag/curious-insights/
  16. https://ghost.org/
  17. https://www.johnwittenauer.net/
  18. https://www.johnwittenauer.net/tag/machine-learning/
  19. https://www.johnwittenauer.net/tag/data-science/
  20. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
  21. https://www.coursera.org/course/ml
  22. https://github.com/jdwittenauer/ipython-notebooks
  23. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  24. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
  25. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
  26. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
  27. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
  28. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
  29. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
  30. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
  31. https://github.com/jdwittenauer/ipython-notebooks/blob/master/exercises/ml/ex4.pdf
  32. https://twitter.com/jdwittenauer
  33. https://www.johnwittenauer.net/tag/machine-learning/
  34. https://www.johnwittenauer.net/tag/data-science/
  35. https://www.johnwittenauer.net/author/john/
  36. https://www.johnwittenauer.net/
  37. https://www.johnwittenauer.net/author/john/
  38. http://twitter.com/share?text=machine learning exercises in python, part 5&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
  39. https://www.facebook.com/sharer/sharer.php?u=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
  40. https://plus.google.com/share?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
  41. http://www.reddit.com/submit?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/&title=machine learning exercises in python, part 5
  42. http://www.linkedin.com/sharearticle?mini=true&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/&title=machine learning exercises in python, part 5
  43. http://pinterest.com/pin/create/button/?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/&description=machine learning exercises in python, part 5
  44. https://ghost.org/

   hidden links:
  46. mailto:jdwittenauer@gmail.com
  47. https://twitter.com/jdwittenauer
  48. http://www.linkedin.com/in/jdwittenauer
  49. https://github.com/jdwittenauer
  50. http://www.kaggle.com/jdwittenauer
  51. https://www.johnwittenauer.net/rss/
  52. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
