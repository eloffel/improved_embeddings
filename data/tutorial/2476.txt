   #[1]curious insight

[2]curious insight
     __________________________________________________________________

   technology, software, data science, machine learning, entrepreneurship,
   investing, and various other topics

tags
     __________________________________________________________________

   [3]about [4]machine learning [5]data visualization [6]data science
   [7]big data [8]software development [9]emerging technology
   [10]entrepreneurship [11]investing [12]strategy [13]book review
   [14]random thoughts [15]curious insights

   copyright    curious insight. 2019     all rights reserved.

   proudly published with [16]ghost.
     *
     *
     *
     *
     *
     *

[17]curious insight

   [18]machine learning[19]data science[20]data visualization

machine learning exercises in python, part 6

   [21]26th june 2016
     __________________________________________________________________

   this post is part of a series covering the exercises from andrew ng's
   [22]machine learning class on coursera. the original code, exercise
   text, and data files for this post are available [23]here.

   [24]part 1 - simple id75
   [25]part 2 - multivariate id75
   [26]part 3 - id28
   [27]part 4 - multivariate id28
   [28]part 5 - neural networks
   [29]part 6 - support vector machines
   [30]part 7 - id116 id91 & pca
   [31]part 8 - anomaly detection & recommendation

   we're now hitting the home stretch of both the course content and this
   series of blog posts. in this exercise, we'll be using support vector
   machines (id166s) to build a spam classifier. we'll start with id166s on
   some simple 2d data sets to see how they work. then we'll look at a set
   of email data and build a classifier on the processed emails using a
   id166 to determine if they are spam or not. as always, it helps to follow
   along using the exercise text for the course (posted [32]here).

   before jumping into the code, let's quickly recap what an id166 is and
   why it's worth learning about. broadly speaking, id166s are a class of
   supervised learning algorithm that builds a representation of the
   examples in the training data as points in space, mapped so that the
   examples belonging to each class in the data are divided by a clear gap
   that is as wide as possible. new examples are then mapped into that
   same space and predicted to belong to a class based on which side of
   the gap they fall on. id166s are a binary classifcation tool by default,
   although there are ways to use them in multi-class scenarios. id166s can
   also handle non-linear classification using something called the
   [33]kernel trick to project the data into a high-dimensional space
   before attempting to find a hyperplane. id166s are a powerful class of
   algorithms and are used often in practical machine learning
   applications.

   the first thing we're going to do is look at a simple 2-dimensional
   data set and see how a linear id166 works on the data set for varying
   values of c (similar to the id173 term in linear/logistic
   regression). let's load the data.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from scipy.io import loadmat
%matplotlib inline

raw_data = loadmat('data/ex6data1.mat')
raw_data

{'x': array([[ 1.9643  ,  4.5957  ],
        [ 2.2753  ,  3.8589  ],
        [ 2.9781  ,  4.5651  ],
        ...,
        [ 0.9044  ,  3.0198  ],
        [ 0.76615 ,  2.5899  ],
        [ 0.086405,  4.1045  ]]),
 '__globals__': [],
 '__header__': 'matlab 5.0 mat-file, platform: glnxa64, created on: sun nov 13 1
4:28:43 2011',
 '__version__': '1.0',
 'y': array([[1],
        [1],
        [1],
        ...,
        [0],
        [0],
        [1]], dtype=uint8)}

   we'll visualize it as a scatter plot where the class label is denoted
   by a symbol ('+' for positive, 'o' for negative).
data = pd.dataframe(raw_data['x'], columns=['x1', 'x2'])
data['y'] = raw_data['y']

positive = data[data['y'].isin([1])]
negative = data[data['y'].isin([0])]

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(positive['x1'], positive['x2'], s=50, marker='x', label='positive')
ax.scatter(negative['x1'], negative['x2'], s=50, marker='o', label='negative')
ax.legend()

   notice that there is one outlier positive example that sits apart from
   the others. the classes are still linearly separable but it's a very
   tight fit. we're going to train a linear support vector machine to
   learn the class boundary. in this exercise we're not tasked with
   implementing an id166 from scratch, so i'm going to use the one built
   into scikit-learn.
from sklearn import id166
svc = id166.linearsvc(c=1, loss='hinge', max_iter=1000)
svc

linearsvc(c=1, class_weight=none, dual=true, fit_intercept=true,
     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
     penalty='l2', random_state=none, tol=0.0001, verbose=0)

   for the first experiment we'll use c=1 and see how it performs.
svc.fit(data[['x1', 'x2']], data['y'])
svc.score(data[['x1', 'x2']], data['y'])

0.98039215686274506

   it appears that it mis-classified the outlier. let's see what happens
   with a larger value of c.
svc2 = id166.linearsvc(c=100, loss='hinge', max_iter=1000)
svc2.fit(data[['x1', 'x2']], data['y'])
svc2.score(data[['x1', 'x2']], data['y'])

1.0

   this time we got a perfect classification of the training data, however
   by increasing the value of c we've created a decision boundary that is
   no longer a natural fit for the data. we can visualize this by looking
   at the confidence level for each class prediction, which is a function
   of the point's distance from the hyperplane.
data['id166 1 confidence'] = svc.decision_function(data[['x1', 'x2']])

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(data['x1'], data['x2'], s=50, c=data['id166 1 confidence'], cmap='seism
ic')
ax.set_title('id166 (c=1) decision confidence')

data['id166 2 confidence'] = svc2.decision_function(data[['x1', 'x2']])

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(data['x1'], data['x2'], s=50, c=data['id166 2 confidence'], cmap='seism
ic')
ax.set_title('id166 (c=100) decision confidence')

   the difference is a bit subtle but look at the color of the points near
   the boundary. in the first image the points near the boundary are a
   strong red or blue, indicating that they're a solid distance from the
   hyperplane. this is not the case in the second image, where a number of
   points are nearly white, indicating that they are directly adjacent to
   the hyperplane.

   now we're going to move from a linear id166 to one that's capable of
   non-linear classification using kernels. we're first tasked with
   implementing a gaussian id81. although scikit-learn has a
   gaussian kernel built in, for transparency we'll implement one from
   scratch.
def gaussian_kernel(x1, x2, sigma):
    return np.exp(-(np.sum((x1 - x2) ** 2) / (2 * (sigma ** 2))))

x1 = np.array([1.0, 2.0, 1.0])
x2 = np.array([0.0, 4.0, -1.0])
sigma = 2
gaussian_kernel(x1, x2, sigma)

0.32465246735834974

   that result matches the expected value from the exercise. next we're
   going to examine another data set, this time with a non-linear decision
   boundary.
raw_data = loadmat('data/ex6data2.mat')

data = pd.dataframe(raw_data['x'], columns=['x1', 'x2'])
data['y'] = raw_data['y']

positive = data[data['y'].isin([1])]
negative = data[data['y'].isin([0])]

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(positive['x1'], positive['x2'], s=30, marker='x', label='positive')
ax.scatter(negative['x1'], negative['x2'], s=30, marker='o', label='negative')
ax.legend()

   for this data set we'll build a support vector machine classifier using
   the built-in rbf kernel and examine its accuracy on the training data.
   to visualize the decision boundary, this time we'll shade the points
   based on the predicted id203 that the instance has a negative
   class label. we'll see from the result that it gets most of them right.
svc = id166.svc(c=100, gamma=10, id203=true)
svc.fit(data[['x1', 'x2']], data['y'])
data['id203'] = svc.predict_proba(data[['x1', 'x2']])[:,0]

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(data['x1'], data['x2'], s=30, c=data['id203'], cmap='reds')

   for the third data set we're given both training and validation sets
   and tasked with finding optimal hyper-parameters for an id166 model based
   on validation set performance. although we could use scikit-learn's
   built-in grid search to do this quite easily, in the spirit of
   following the exercise directions we'll implement a simple grid search
   from scratch.
raw_data = loadmat('data/ex6data3.mat')

x = raw_data['x']
xval = raw_data['xval']
y = raw_data['y'].ravel()
yval = raw_data['yval'].ravel()

c_values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]
gamma_values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]

best_score = 0
best_params = {'c': none, 'gamma': none}

for c in c_values:
    for gamma in gamma_values:
        svc = id166.svc(c=c, gamma=gamma)
        svc.fit(x, y)
        score = svc.score(xval, yval)

        if score > best_score:
            best_score = score
            best_params['c'] = c
            best_params['gamma'] = gamma

best_score, best_params

(0.96499999999999997, {'c': 0.3, 'gamma': 100})

   now we'll move on to the last part of the exercise. in this part our
   objective is to use id166s to build a spam filter. in the exercise text,
   there's a task involving some text pre-processing to get our data in a
   format suitable for an id166 to handle. however, the task is pretty
   trivial (mapping words to an id from a dictionary that's provided for
   the exercise) and the rest of the pre-processing steps such as html
   removal, id30, id172 etc. are already done. rather than
   reproduce these pre-processing steps, i'm going to skip ahead to the
   machine learning task which involves building a classifier from
   pre-processed train and test data sets consisting of spam and non-spam
   emails transformed to word occurance vectors.
spam_train = loadmat('data/spamtrain.mat')
spam_test = loadmat('data/spamtest.mat')

spam_train

{'x': array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 1, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 '__globals__': [],
 '__header__': 'matlab 5.0 mat-file, platform: glnxa64, created on: sun nov 13 1
4:27:25 2011',
 '__version__': '1.0',
 'y': array([[1],
        [1],
        [0],
        ...,
        [1],
        [0],
        [0]], dtype=uint8)}

x = spam_train['x']
xtest = spam_test['xtest']
y = spam_train['y'].ravel()
ytest = spam_test['ytest'].ravel()

x.shape, y.shape, xtest.shape, ytest.shape

((4000l, 1899l), (4000l,), (1000l, 1899l), (1000l,))

   each document has been converted to a vector with 1,899 dimensions
   corresponding to the 1,899 words in the vocabulary. the values are
   binary, indicating the presence or absence of the word in the document.
   at this point, training and evaluation are just a matter of fitting the
   testing the classifer.
svc = id166.svc()
svc.fit(x, y)
print('test accuracy = {0}%'.format(np.round(svc.score(xtest, ytest) * 100, 2)))

test accuracy = 95.3%

   this result is with the default parameters. we could probably get it a
   bit higher using some parameter tuning, but 95% accuracy still isn't
   bad.

   that concludes exercise 6! in the next exercise we'll perform
   id91 and image compression with id116 and principal component
   analysis.

   follow me on twitter to get new post updates.
   [34]follow @jdwittenauer
   [35]machine learning[36]data science[37]data visualization
   author

[38]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

[39]curious insight

   author

[40]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

share article
     __________________________________________________________________

   [41]twitter [42]facebook [43]google+ [44]reddit [45]linkedin
   [46]pinterest

   copyright    curious insight. 2016     all rights reserved.

   proudly published with [47]ghost.

references

   visible links
   1. https://www.johnwittenauer.net/rss/
   2. https://www.johnwittenauer.net/
   3. https://www.johnwittenauer.net/about/
   4. https://www.johnwittenauer.net/tag/machine-learning/
   5. https://www.johnwittenauer.net/tag/data-visualization/
   6. https://www.johnwittenauer.net/tag/data-science/
   7. https://www.johnwittenauer.net/tag/big-data/
   8. https://www.johnwittenauer.net/tag/software-development/
   9. https://www.johnwittenauer.net/tag/emerging-technology/
  10. https://www.johnwittenauer.net/tag/entrepreneurship/
  11. https://www.johnwittenauer.net/tag/investing/
  12. https://www.johnwittenauer.net/tag/strategy/
  13. https://www.johnwittenauer.net/tag/book-review/
  14. https://www.johnwittenauer.net/tag/random-thoughts/
  15. https://www.johnwittenauer.net/tag/curious-insights/
  16. https://ghost.org/
  17. https://www.johnwittenauer.net/
  18. https://www.johnwittenauer.net/tag/machine-learning/
  19. https://www.johnwittenauer.net/tag/data-science/
  20. https://www.johnwittenauer.net/tag/data-visualization/
  21. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
  22. https://www.coursera.org/course/ml
  23. https://github.com/jdwittenauer/ipython-notebooks
  24. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  25. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
  26. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
  27. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
  28. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
  29. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
  30. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
  31. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
  32. https://github.com/jdwittenauer/ipython-notebooks/blob/master/exercises/ml/ex6.pdf
  33. https://en.wikipedia.org/wiki/kernel_method
  34. https://twitter.com/jdwittenauer
  35. https://www.johnwittenauer.net/tag/machine-learning/
  36. https://www.johnwittenauer.net/tag/data-science/
  37. https://www.johnwittenauer.net/tag/data-visualization/
  38. https://www.johnwittenauer.net/author/john/
  39. https://www.johnwittenauer.net/
  40. https://www.johnwittenauer.net/author/john/
  41. http://twitter.com/share?text=machine learning exercises in python, part 6&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
  42. https://www.facebook.com/sharer/sharer.php?u=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
  43. https://plus.google.com/share?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
  44. http://www.reddit.com/submit?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/&title=machine learning exercises in python, part 6
  45. http://www.linkedin.com/sharearticle?mini=true&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/&title=machine learning exercises in python, part 6
  46. http://pinterest.com/pin/create/button/?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/&description=machine learning exercises in python, part 6
  47. https://ghost.org/

   hidden links:
  49. mailto:jdwittenauer@gmail.com
  50. https://twitter.com/jdwittenauer
  51. http://www.linkedin.com/in/jdwittenauer
  52. https://github.com/jdwittenauer
  53. http://www.kaggle.com/jdwittenauer
  54. https://www.johnwittenauer.net/rss/
  55. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
