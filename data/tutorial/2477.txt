   #[1]curious insight

[2]curious insight
     __________________________________________________________________

   technology, software, data science, machine learning, entrepreneurship,
   investing, and various other topics

tags
     __________________________________________________________________

   [3]about [4]machine learning [5]data visualization [6]data science
   [7]big data [8]software development [9]emerging technology
   [10]entrepreneurship [11]investing [12]strategy [13]book review
   [14]random thoughts [15]curious insights

   copyright    curious insight. 2019     all rights reserved.

   proudly published with [16]ghost.
     *
     *
     *
     *
     *
     *

[17]curious insight

   [18]machine learning[19]data science[20]data visualization

machine learning exercises in python, part 7

   [21]14th july 2016
     __________________________________________________________________

   this post is part of a series covering the exercises from andrew ng's
   [22]machine learning class on coursera. the original code, exercise
   text, and data files for this post are available [23]here.

   [24]part 1 - simple id75
   [25]part 2 - multivariate id75
   [26]part 3 - id28
   [27]part 4 - multivariate id28
   [28]part 5 - neural networks
   [29]part 6 - support vector machines
   [30]part 7 - id116 id91 & pca
   [31]part 8 - anomaly detection & recommendation

   we're now down to the last two posts in this series! in this
   installment we'll cover two fascinating topics: id116 id91 and
   principal component analysis (pca). id116 and pca are both examples
   of [32]unsupervised learning techniques. unsupervised learning problems
   do not have any label or target for us to learn from to make
   predictions, so unsupervised algorithms instead attempt to learn some
   interesting structure in the data itself. we'll first implement id116
   and see how it can be used it to compress an image. we'll also
   experiment with pca to find a low-dimensional representation of images
   of faces. as always, it helps to follow along using the exercise text
   for the course (posted [33]here).

id116 id91

   to start out we're going to implement and apply id116 to a simple
   2-dimensional data set to gain some intuition about how it works.
   id116 is an iterative, unsupervised id91 algorithm that groups
   similar instances together into clusters. the algorithm starts by
   guessing the initial centroids for each cluster, and then repeatedly
   assigns instances to the nearest cluster and re-computes the centroid
   of that cluster. the first piece that we're going to implement is a
   function that finds the closest centroid for each instance in the data.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from scipy.io import loadmat
%matplotlib inline

def find_closest_centroids(x, centroids):
    m = x.shape[0]
    k = centroids.shape[0]
    idx = np.zeros(m)

    for i in range(m):
        min_dist = 1000000
        for j in range(k):
            dist = np.sum((x[i,:] - centroids[j,:]) ** 2)
            if dist < min_dist:
                min_dist = dist
                idx[i] = j

    return idx

   let's test the function to make sure it's working as expected. we'll
   use the test case provided in the exercise.
data = loadmat('data/ex7data2.mat')
x = data['x']
initial_centroids = initial_centroids = np.array([[3, 3], [6, 2], [8, 5]])

idx = find_closest_centroids(x, initial_centroids)
idx[0:3]

array([ 0.,  2.,  1.])

   the output matches the expected values in the text (remember our arrays
   are zero-indexed instead of one-indexed so the values are one lower
   than in the exercise). next we need a function to compute the centroid
   of a cluster. the centroid is simply the mean of all of the examples
   currently assigned to the cluster.
def compute_centroids(x, idx, k):
    m, n = x.shape
    centroids = np.zeros((k, n))

    for i in range(k):
        indices = np.where(idx == i)
        centroids[i,:] = (np.sum(x[indices,:], axis=1) / len(indices[0])).ravel(
)

    return centroids

compute_centroids(x, idx, 3)

array([[ 2.42830111,  3.15792418],
       [ 5.81350331,  2.63365645],
       [ 7.11938687,  3.6166844 ]])

   this output also matches the expected values from the exercise. so far
   so good. the next part involves actually running the algorithm for some
   number of iterations and visualizing the result. this step was
   implmented for us in the exercise, but since it's not that complicated
   i'll build it here from scratch. in order to run the algorithm we just
   need to alternate between assigning examples to the nearest cluster and
   re-computing the cluster centroids.
def run_k_means(x, initial_centroids, max_iters):
    m, n = x.shape
    k = initial_centroids.shape[0]
    idx = np.zeros(m)
    centroids = initial_centroids

    for i in range(max_iters):
        idx = find_closest_centroids(x, centroids)
        centroids = compute_centroids(x, idx, k)

    return idx, centroids

idx, centroids = run_k_means(x, initial_centroids, 10)

   we can now plot the result using color coding to indicate cluster
   membership.
cluster1 = x[np.where(idx == 0)[0],:]
cluster2 = x[np.where(idx == 1)[0],:]
cluster3 = x[np.where(idx == 2)[0],:]

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(cluster1[:,0], cluster1[:,1], s=30, color='r', label='cluster 1')
ax.scatter(cluster2[:,0], cluster2[:,1], s=30, color='g', label='cluster 2')
ax.scatter(cluster3[:,0], cluster3[:,1], s=30, color='b', label='cluster 3')
ax.legend()

   one step we skipped over is a process for initializing the centroids.
   this can affect the convergence of the algorithm. we're tasked with
   creating a function that selects random examples and uses them as the
   initial centroids.
def init_centroids(x, k):
    m, n = x.shape
    centroids = np.zeros((k, n))
    idx = np.random.randint(0, m, k)

    for i in range(k):
        centroids[i,:] = x[idx[i],:]

    return centroids

init_centroids(x, 3)

array([[ 1.15354031,  4.67866717],
       [ 6.27376271,  2.24256036],
       [ 2.20960296,  4.91469264]])

   our next task is to apply id116 to image compression. the intuition
   here is that we can use id91 to find a small number of colors
   that are most representative of the image, and map the original 24-bit
   colors to a lower-dimensional color space using the cluster
   assignments. here's the image we're going to compress.

   the raw pixel data has been pre-loaded for us so let's pull it in.
image_data = loadmat('data/bird_small.mat')
image_data

{'a': array([[[219, 180, 103],
         [230, 185, 116],
         [226, 186, 110],
         ...,
         [ 14,  15,  13],
         [ 13,  15,  12],
         [ 12,  14,  12]],
        ...,
        [[ 15,  19,  19],
         [ 20,  20,  18],
         [ 18,  19,  17],
         ...,
         [ 65,  43,  39],
         [ 58,  37,  38],
         [ 52,  39,  34]]], dtype=uint8),
 '__globals__': [],
 '__header__': 'matlab 5.0 mat-file, platform: glnxa64, created on: tue jun  5 0
4:06:24 2012',
 '__version__': '1.0'}

   we can quickly look at the shape of the data to validate that it looks
   like what we'd expect for an image.
a = image_data['a']
a.shape

(128l, 128l, 3l)

   now we need to apply some pre-processing to the data and feed it into
   the id116 algorithm.
# normalize value ranges
a = a / 255.

# reshape the array
x = np.reshape(a, (a.shape[0] * a.shape[1], a.shape[2]))

# randomly initialize the centroids
initial_centroids = init_centroids(x, 16)

# run the algorithm
idx, centroids = run_k_means(x, initial_centroids, 10)

# get the closest centroids one last time
idx = find_closest_centroids(x, centroids)

# map each pixel to the centroid value
x_recovered = centroids[idx.astype(int),:]

# reshape to the original dimensions
x_recovered = np.reshape(x_recovered, (a.shape[0], a.shape[1], a.shape[2]))

plt.imshow(x_recovered)

   cool! you can see that we created some artifacts in the compression but
   the main features of the image are still there despite mapping the
   original image to only 16 colors. that's it for id116. we'll now move
   on to principal component analysis.

principal component analysis

   pca is a linear transformation that finds the "principal components",
   or directions of greatest variance, in a data set. it can be used for
   dimension reduction among other things. in this exercise we're first
   tasked with implementing pca and applying it to a simple 2-dimensional
   data set to see how it works. let's start off by loading and
   visualizing the data set.
data = loadmat('data/ex7data1.mat')
x = data['x']

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(x[:, 0], x[:, 1])

   the algorithm for pca is fairly simple. after ensuring that the data is
   normalized, the output is simply the singular value decomposition of
   the covariance matrix of the original data. since numpy already has
   built-in functions to calculate the covariance and svd of a matrix,
   we'll use those rather than build from scratch.
def pca(x):
    # normalize the features
    x = (x - x.mean()) / x.std()

    # compute the covariance matrix
    x = np.matrix(x)
    cov = (x.t * x) / x.shape[0]

    # perform svd
    u, s, v = np.linalg.svd(cov)

    return u, s, v

u, s, v = pca(x)
u, s, v

(matrix([[-0.79241747, -0.60997914],
         [-0.60997914,  0.79241747]]),
 array([ 1.43584536,  0.56415464]),
 matrix([[-0.79241747, -0.60997914],
         [-0.60997914,  0.79241747]]))

   now that we have the principal components (matrix u), we can use these
   to project the original data into a lower-dimensional space. for this
   task we'll implement a function that computes the projection and
   selects only the top k components, effectively reducing the number of
   dimensions.
def project_data(x, u, k):
    u_reduced = u[:,:k]
    return np.dot(x, u_reduced)

z = project_data(x, u, 1)
z

matrix([[-4.74689738],
        [-7.15889408],
        [-4.79563345],
        [-4.45754509],
        [-4.80263579],
        ...,
        [-6.44590096],
        [-2.69118076],
        [-4.61386195],
        [-5.88236227],
        [-7.76732508]])

   we can also attempt to recover the original data by reversing the steps
   we took to project it.
def recover_data(z, u, k):
    u_reduced = u[:,:k]
    return np.dot(z, u_reduced.t)

x_recovered = recover_data(z, u, 1)
x_recovered

matrix([[ 3.76152442,  2.89550838],
        [ 5.67283275,  4.36677606],
        [ 3.80014373,  2.92523637],
        [ 3.53223661,  2.71900952],
        [ 3.80569251,  2.92950765],
        ...,
        [ 5.10784454,  3.93186513],
        [ 2.13253865,  1.64156413],
        [ 3.65610482,  2.81435955],
        [ 4.66128664,  3.58811828],
        [ 6.1549641 ,  4.73790627]])

   if we then attempt to visualize the recovered data, the intuition
   behind how the algorithm works becomes really obvious.
fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(x_recovered[:, 0], x_recovered[:, 1])

   notice how the points all seem to be compressed down to an invisible
   line. that invisible line is essentially the first principal component.
   the second principal component, which we cut off when we reduced the
   data to one dimension, can be thought of as the variation orthogonal to
   that line. since we lost that information, our reconstruction can only
   place the points relative to the first principal component.

   our last task in this exercise is to apply pca to images of faces. by
   using the same dimension reduction techniques we can capture the
   "essence" of the images using much less data than the original images.
faces = loadmat('data/ex7faces.mat')
x = faces['x']
x.shape

(5000l, 1024l)

   the exercise code includes a function that will render the first 100
   faces in the data set in a grid. rather than try to re-produce that
   here, you can look in the exercise text for an example of what they
   look like. we can at least render one image fairly easily though.
face = np.reshape(x[3,:], (32, 32))
plt.imshow(face)

   yikes, that looks awful! these are only 32 x 32 grayscale images though
   (it's also rendering sideways, but we can ignore that for now). our
   next step is to run pca on the faces data set and take the top 100
   principal components.
u, s, v = pca(x)
z = project_data(x, u, 100)

   now we can attempt to recover the original structure and render it
   again.
x_recovered = recover_data(z, u, 100)
face = np.reshape(x_recovered[3,:], (32, 32))
plt.imshow(face)

   notice that we lost some detail, though not as much as you might expect
   for a 10x reduction in the number of dimensions.

   that concludes exercise 7! in the final exercise we'll implement
   algorithms for anomaly detection and build a id126
   using id185.

   follow me on twitter to get new post updates.
   [34]follow @jdwittenauer
   [35]machine learning[36]data science[37]data visualization
   author

[38]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

[39]curious insight

   author

[40]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

share article
     __________________________________________________________________

   [41]twitter [42]facebook [43]google+ [44]reddit [45]linkedin
   [46]pinterest

   copyright    curious insight. 2016     all rights reserved.

   proudly published with [47]ghost.

references

   visible links
   1. https://www.johnwittenauer.net/rss/
   2. https://www.johnwittenauer.net/
   3. https://www.johnwittenauer.net/about/
   4. https://www.johnwittenauer.net/tag/machine-learning/
   5. https://www.johnwittenauer.net/tag/data-visualization/
   6. https://www.johnwittenauer.net/tag/data-science/
   7. https://www.johnwittenauer.net/tag/big-data/
   8. https://www.johnwittenauer.net/tag/software-development/
   9. https://www.johnwittenauer.net/tag/emerging-technology/
  10. https://www.johnwittenauer.net/tag/entrepreneurship/
  11. https://www.johnwittenauer.net/tag/investing/
  12. https://www.johnwittenauer.net/tag/strategy/
  13. https://www.johnwittenauer.net/tag/book-review/
  14. https://www.johnwittenauer.net/tag/random-thoughts/
  15. https://www.johnwittenauer.net/tag/curious-insights/
  16. https://ghost.org/
  17. https://www.johnwittenauer.net/
  18. https://www.johnwittenauer.net/tag/machine-learning/
  19. https://www.johnwittenauer.net/tag/data-science/
  20. https://www.johnwittenauer.net/tag/data-visualization/
  21. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
  22. https://www.coursera.org/course/ml
  23. https://github.com/jdwittenauer/ipython-notebooks
  24. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  25. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
  26. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
  27. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
  28. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
  29. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
  30. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
  31. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
  32. https://en.wikipedia.org/wiki/unsupervised_learning
  33. https://github.com/jdwittenauer/ipython-notebooks/blob/master/exercises/ml/ex7.pdf
  34. https://twitter.com/jdwittenauer
  35. https://www.johnwittenauer.net/tag/machine-learning/
  36. https://www.johnwittenauer.net/tag/data-science/
  37. https://www.johnwittenauer.net/tag/data-visualization/
  38. https://www.johnwittenauer.net/author/john/
  39. https://www.johnwittenauer.net/
  40. https://www.johnwittenauer.net/author/john/
  41. http://twitter.com/share?text=machine learning exercises in python, part 7&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
  42. https://www.facebook.com/sharer/sharer.php?u=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
  43. https://plus.google.com/share?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
  44. http://www.reddit.com/submit?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/&title=machine learning exercises in python, part 7
  45. http://www.linkedin.com/sharearticle?mini=true&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/&title=machine learning exercises in python, part 7
  46. http://pinterest.com/pin/create/button/?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/&description=machine learning exercises in python, part 7
  47. https://ghost.org/

   hidden links:
  49. mailto:jdwittenauer@gmail.com
  50. https://twitter.com/jdwittenauer
  51. http://www.linkedin.com/in/jdwittenauer
  52. https://github.com/jdwittenauer
  53. http://www.kaggle.com/jdwittenauer
  54. https://www.johnwittenauer.net/rss/
  55. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
