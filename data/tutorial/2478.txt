   #[1]curious insight

[2]curious insight
     __________________________________________________________________

   technology, software, data science, machine learning, entrepreneurship,
   investing, and various other topics

tags
     __________________________________________________________________

   [3]about [4]machine learning [5]data visualization [6]data science
   [7]big data [8]software development [9]emerging technology
   [10]entrepreneurship [11]investing [12]strategy [13]book review
   [14]random thoughts [15]curious insights

   copyright    curious insight. 2019     all rights reserved.

   proudly published with [16]ghost.
     *
     *
     *
     *
     *
     *

[17]curious insight

   [18]machine learning[19]data science[20]data visualization

machine learning exercises in python, part 8

   [21]12th august 2016
     __________________________________________________________________

   this post is part of a series covering the exercises from andrew ng's
   [22]machine learning class on coursera. the original code, exercise
   text, and data files for this post are available [23]here.

   [24]part 1 - simple id75
   [25]part 2 - multivariate id75
   [26]part 3 - id28
   [27]part 4 - multivariate id28
   [28]part 5 - neural networks
   [29]part 6 - support vector machines
   [30]part 7 - id116 id91 & pca
   [31]part 8 - anomaly detection & recommendation

   we've now reached the last post in this series! it's been an
   interesting journey. andrew's class was really well-done and
   translating it all to python has been a fun experience. in this final
   installment we'll cover the last two topics in the course - anomaly
   detection and id126s. we'll implement an anomaly
   detection algorithm using a gaussian model and apply it to detect
   failing servers on a network. we'll also see how to build a
   id126 using id185 and apply it to a
   movie recommendations data set. as always, it helps to follow along
   using the exercise text for the course (posted [32]here).

anomaly detection

   our first task is to use a gaussian model to detect if an unlabeled
   example from a data set should be considered an anomaly. we have a
   simple 2-dimensional data set to start off with so we can easily
   visualize what the algorithm is doing. let's pull in and plot the data.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from scipy.io import loadmat
%matplotlib inline

data = loadmat('data/ex8data1.mat')
x = data['x']
x.shape

(307l, 2l)

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(x[:,0], x[:,1])

   it appears that there's a pretty tight cluster in the center with
   several values further out away from the cluster. in this simple
   example, these could be considered anomalies. to find out, we're tasked
   with estimating a gaussian distribution for each feature in the data.
   you may recall that to define a id203 distribution we need two
   things - mean and variance. to accomplish this we'll create a simple
   function that calculates the mean and variance for each feature in our
   data set.
def estimate_gaussian(x):
    mu = x.mean(axis=0)
    sigma = x.var(axis=0)

    return mu, sigma

mu, sigma = estimate_gaussian(x)
mu, sigma

(array([ 14.11222578,  14.99771051]), array([ 1.83263141,  1.70974533]))

   now that we have our model parameters, we need to determine a
   id203 threshold which indicates that an example should be
   considered an anomaly. to do this, we need to use a set of labeled
   validation data (where the true anomalies have been marked for us) and
   test the model's performance at identifying those anomalies given
   different threshold values.
xval = data['xval']
yval = data['yval']

xval.shape, yval.shape

((307l, 2l), (307l, 1l))

   we also need a way to calculate the id203 that a data point
   belongs to a normal distribution given some set of parameters.
   fortunately scipy has this built-in.
from scipy import stats
dist = stats.norm(mu[0], sigma[0])
dist.pdf(x[:,0])[0:50]

array([ 0.183842  ,  0.20221694,  0.21746136,  0.19778763,  0.20858956,
        0.21652359,  0.16991291,  0.15123542,  0.1163989 ,  0.1594734 ,
        0.21716057,  0.21760472,  0.20141857,  0.20157497,  0.21711385,
        0.21758775,  0.21695576,  0.2138258 ,  0.21057069,  0.1173018 ,
        0.20765108,  0.21717452,  0.19510663,  0.21702152,  0.17429399,
        0.15413455,  0.21000109,  0.20223586,  0.21031898,  0.21313426,
        0.16158946,  0.2170794 ,  0.17825767,  0.17414633,  0.1264951 ,
        0.19723662,  0.14538809,  0.21766361,  0.21191386,  0.21729442,
        0.21238912,  0.18799417,  0.21259798,  0.21752767,  0.20616968,
        0.21520366,  0.1280081 ,  0.21768113,  0.21539967,  0.16913173])

   in case it isn't clear, we just calculated the id203 that each of
   the first 50 instances of our data set's first dimension belong to the
   distribution that we defined earlier by calculating the mean and
   variance for that dimension. essentially it's computing how far each
   instance is from the mean and how that compares to the "typical"
   distance from the mean for this data.

   let's compute and save the id203 density of each of the values in
   our data set given the gaussian model parameters we calculated above.
p = np.zeros((x.shape[0], x.shape[1]))
p[:,0] = stats.norm(mu[0], sigma[0]).pdf(x[:,0])
p[:,1] = stats.norm(mu[1], sigma[1]).pdf(x[:,1])

p.shape

(307l, 2l)

   we also need to do this for the validation set (using the same model
   parameters). we'll use these probabilities combined with the true label
   to determine the optimal id203 threshold to assign data points as
   anomalies.
pval = np.zeros((xval.shape[0], xval.shape[1]))
pval[:,0] = stats.norm(mu[0], sigma[0]).pdf(xval[:,0])
pval[:,1] = stats.norm(mu[1], sigma[1]).pdf(xval[:,1])

   next, we need a function that finds the best threshold value given the
   id203 density values and true labels. to do this we'll calculate
   the f1 score for varying values of epsilon. f1 is a function of the
   number of true positives, false positives, and false negatives.
def select_threshold(pval, yval):
    best_epsilon = 0
    best_f1 = 0
    f1 = 0

    step = (pval.max() - pval.min()) / 1000

    for epsilon in np.arange(pval.min(), pval.max(), step):
        preds = pval < epsilon

        tp = np.sum(np.logical_and(preds == 1, yval == 1)).astype(float)
        fp = np.sum(np.logical_and(preds == 1, yval == 0)).astype(float)
        fn = np.sum(np.logical_and(preds == 0, yval == 1)).astype(float)

        precision = tp / (tp + fp)
        recall = tp / (tp + fn)
        f1 = (2 * precision * recall) / (precision + recall)

        if f1 > best_f1:
            best_f1 = f1
            best_epsilon = epsilon

    return best_epsilon, best_f1

epsilon, f1 = select_threshold(pval, yval)
epsilon, f1

(0.0095667060059568421, 0.7142857142857143)

   finally, we can apply the threshold to the data set and visualize the
   results.
# indexes of the values considered to be outliers
outliers = np.where(p < epsilon)

fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(x[:,0], x[:,1])
ax.scatter(x[outliers[0],0], x[outliers[0],1], s=50, color='r', marker='o')

   not bad! the points in red are the ones that were flagged as outliers.
   visually these seem pretty reasonable. the top right point that has
   some separation (but was not flagged) may be an outlier too, but it's
   fairly close. there's another example in the text of applying this to a
   higher-dimensional data set, but since it's a trivial extension of the
   two-dimensional example we'll move on to the last section.

id185

   recommendation engines use item and user-based similarity measures to
   examine a user's historical preferences to make recommendations for new
   "things" the user might be interested in. in this exercise we'll
   implement a particular recommendation algorithm called collaborative
   filtering and apply it to a data set of movie ratings. let's first load
   and examine the data we'll be working with.
data = loadmat('data/ex8_movies.mat')
data

{'r': array([[1, 1, 0, ..., 1, 0, 0],
        [1, 0, 0, ..., 0, 0, 1],
        [1, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 'y': array([[5, 4, 0, ..., 5, 0, 0],
        [3, 0, 0, ..., 0, 0, 5],
        [4, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 '__globals__': [],
 '__header__': 'matlab 5.0 mat-file, platform: glnxa64, created on: thu dec  1 1
7:19:26 2011',
 '__version__': '1.0'}

   y is a (number of movies x number of users) array containing ratings
   from 1 to 5. r is an "indicator" array containing binary values
   indicating if a user has rated a movie or not. both should have the
   same shape.
y = data['y']
r = data['r']
y.shape, r.shape

((1682l, 943l), (1682l, 943l))

   we can look at the average rating for a movie by averaging over a row
   in y for indexes where a rating is present.
y[1,r[1,:]].mean()

2.5832449628844114

   we can also try to "visualize" the data by rendering the matrix as if
   it were an image. we can't glean too much from this but it does give us
   an idea of a relative density of ratings across users and movies.
fig, ax = plt.subplots(figsize=(12,12))
ax.imshow(y)
ax.set_xlabel('users')
ax.set_ylabel('movies')
fig.tight_layout()

   next we're going to implement a cost function for collaborative
   filtering. intuitively, the "cost" is the degree to which a set of
   movie rating predictions deviate from the true predictions. the cost
   equation is given in the exercise text. it is based on two sets of
   parameter matrices called x and theta in the text. these are "unrolled"
   into the "params" input so that we can use scipy's optimization package
   later on. note that i've included the array/matrix shapes in comments
   to help illustrate how the matrix interactions work.
def cost(params, y, r, num_features):
    y = np.matrix(y)  # (1682, 943)
    r = np.matrix(r)  # (1682, 943)
    num_movies = y.shape[0]
    num_users = y.shape[1]

    # reshape the parameter array into parameter matrices
    x = np.matrix(np.reshape(params[:num_movies * num_features], (num_movies, nu
m_features)))  # (1682, 10)
    theta = np.matrix(np.reshape(params[num_movies * num_features:], (num_users,
 num_features)))  # (943, 10)

    # initializations
    j = 0

    # compute the cost
    error = np.multiply((x * theta.t) - y, r)  # (1682, 943)
    squared_error = np.power(error, 2)  # (1682, 943)
    j = (1. / 2) * np.sum(squared_error)

    return j

   in order to test this, we're provided with a set of pre-trained
   parameters that we can evaluate. to keep the evaluation time down,
   we'll look at just a small sub-set of the data.
users = 4
movies = 5
features = 3

params_data = loadmat('data/ex8_movieparams.mat')
x = params_data['x']
theta = params_data['theta']

x_sub = x[:movies, :features]
theta_sub = theta[:users, :features]
y_sub = y[:movies, :users]
r_sub = r[:movies, :users]

params = np.concatenate((np.ravel(x_sub), np.ravel(theta_sub)))

cost(params, y_sub, r_sub, features)

22.224603725685675

   this answer matches what the exercise text said we're supposed to get.
   next we need to implement the gradient computations. just like we did
   with the neural networks implementation in exercise 4, we'll extend the
   cost function to also compute the gradients.
def cost(params, y, r, num_features):
    y = np.matrix(y)  # (1682, 943)
    r = np.matrix(r)  # (1682, 943)
    num_movies = y.shape[0]
    num_users = y.shape[1]

    # reshape the parameter array into parameter matrices
    x = np.matrix(np.reshape(params[:num_movies * num_features], (num_movies, nu
m_features)))  # (1682, 10)
    theta = np.matrix(np.reshape(params[num_movies * num_features:], (num_users,
 num_features)))  # (943, 10)

    # initializations
    j = 0
    x_grad = np.zeros(x.shape)  # (1682, 10)
    theta_grad = np.zeros(theta.shape)  # (943, 10)

    # compute the cost
    error = np.multiply((x * theta.t) - y, r)  # (1682, 943)
    squared_error = np.power(error, 2)  # (1682, 943)
    j = (1. / 2) * np.sum(squared_error)

    # calculate the gradients
    x_grad = error * theta
    theta_grad = error.t * x

    # unravel the gradient matrices into a single array
    grad = np.concatenate((np.ravel(x_grad), np.ravel(theta_grad)))

    return j, grad

j, grad = cost(params, y_sub, r_sub, features)
j, grad

(22.224603725685675,
 array([ -2.52899165,   7.57570308,  -1.89979026,  -0.56819597,
          3.35265031,  -0.52339845,  -0.83240713,   4.91163297,
         -0.76677878,  -0.38358278,   2.26333698,  -0.35334048,
         -0.80378006,   4.74271842,  -0.74040871, -10.5680202 ,
          4.62776019,  -7.16004443,  -3.05099006,   1.16441367,
         -3.47410789,   0.        ,   0.        ,   0.        ,
          0.        ,   0.        ,   0.        ]))

   our next step is to add id173 to both the cost and gradient
   calculations. we'll create one final regularized version of the
   function (note that this version includes an additional learning rate
   parameter called "lambda").
def cost(params, y, r, num_features, learning_rate):
    y = np.matrix(y)  # (1682, 943)
    r = np.matrix(r)  # (1682, 943)
    num_movies = y.shape[0]
    num_users = y.shape[1]

    # reshape the parameter array into parameter matrices
    x = np.matrix(np.reshape(params[:num_movies * num_features], (num_movies, nu
m_features)))  # (1682, 10)
    theta = np.matrix(np.reshape(params[num_movies * num_features:], (num_users,
 num_features)))  # (943, 10)

    # initializations
    j = 0
    x_grad = np.zeros(x.shape)  # (1682, 10)
    theta_grad = np.zeros(theta.shape)  # (943, 10)

    # compute the cost
    error = np.multiply((x * theta.t) - y, r)  # (1682, 943)
    squared_error = np.power(error, 2)  # (1682, 943)
    j = (1. / 2) * np.sum(squared_error)

    # add the cost id173
    j = j + ((learning_rate / 2) * np.sum(np.power(theta, 2)))
    j = j + ((learning_rate / 2) * np.sum(np.power(x, 2)))

    # calculate the gradients with id173
    x_grad = (error * theta) + (learning_rate * x)
    theta_grad = (error.t * x) + (learning_rate * theta)

    # unravel the gradient matrices into a single array
    grad = np.concatenate((np.ravel(x_grad), np.ravel(theta_grad)))

    return j, grad

j, grad = cost(params, y_sub, r_sub, features, 1.5)
j, grad

(31.344056244274221,
 array([ -0.95596339,   6.97535514,  -0.10861109,   0.60308088,
          2.77421145,   0.25839822,   0.12985616,   4.0898522 ,
         -0.89247334,   0.29684395,   1.06300933,   0.66738144,
          0.60252677,   4.90185327,  -0.19747928, -10.13985478,
          2.10136256,  -6.76563628,  -2.29347024,   0.48244098,
         -2.99791422,  -0.64787484,  -0.71820673,   1.27006666,
          1.09289758,  -0.40784086,   0.49026541]))

   this result again matches up with the expected output from the exercise
   code, so it looks like the id173 is working. before we train
   the model, we have one final step. we're tasked with creating our own
   movie ratings so we can use the model to generate personalized
   recommendations. a file is provided for us that links the movie index
   to its title. let's load the file into a dictionary and use some sample
   ratings provided in the exercise.
movie_idx = {}
f = open('data/movie_ids.txt')
for line in f:
    tokens = line.split(' ')
    tokens[-1] = tokens[-1][:-1]
    movie_idx[int(tokens[0]) - 1] = ' '.join(tokens[1:])

ratings = np.zeros((1682, 1))

ratings[0] = 4
ratings[6] = 3
ratings[11] = 5
ratings[53] = 4
ratings[63] = 5
ratings[65] = 3
ratings[68] = 5
ratings[97] = 2
ratings[182] = 4
ratings[225] = 5
ratings[354] = 5

print('rated {0} with {1} stars.'.format(movie_idx[0], str(int(ratings[0]))))
print('rated {0} with {1} stars.'.format(movie_idx[6], str(int(ratings[6]))))
print('rated {0} with {1} stars.'.format(movie_idx[11], str(int(ratings[11]))))
print('rated {0} with {1} stars.'.format(movie_idx[53], str(int(ratings[53]))))
print('rated {0} with {1} stars.'.format(movie_idx[63], str(int(ratings[63]))))
print('rated {0} with {1} stars.'.format(movie_idx[65], str(int(ratings[65]))))
print('rated {0} with {1} stars.'.format(movie_idx[68], str(int(ratings[68]))))
print('rated {0} with {1} stars.'.format(movie_idx[97], str(int(ratings[97]))))
print('rated {0} with {1} stars.'.format(movie_idx[182], str(int(ratings[182])))
)
print('rated {0} with {1} stars.'.format(movie_idx[225], str(int(ratings[225])))
)
print('rated {0} with {1} stars.'.format(movie_idx[354], str(int(ratings[354])))
)

rated toy story (1995) with 4 stars.
rated twelve monkeys (1995) with 3 stars.
rated usual suspects, the (1995) with 5 stars.
rated outbreak (1995) with 4 stars.
rated shawshank redemption, the (1994) with 5 stars.
rated while you were sleeping (1995) with 3 stars.
rated forrest gump (1994) with 5 stars.
rated silence of the lambs, the (1991) with 2 stars.
rated alien (1979) with 4 stars.
rated die hard 2 (1990) with 5 stars.
rated sphere (1998) with 5 stars.

   we can add this custom ratings vector to the data set so it gets
   included in the model.
r = data['r']
y = data['y']

y = np.append(y, ratings, axis=1)
r = np.append(r, ratings != 0, axis=1)

   we're now ready to train the id185 model. we're going
   to normalize the ratings and then run the optimization routine using
   our cost function, parameter vector, and data matrices at inputs.
from scipy.optimize import minimize

movies = y.shape[0]
users = y.shape[1]
features = 10
learning_rate = 10.

x = np.random.random(size=(movies, features))
theta = np.random.random(size=(users, features))
params = np.concatenate((np.ravel(x), np.ravel(theta)))

ymean = np.zeros((movies, 1))
ynorm = np.zeros((movies, users))

for i in range(movies):
    idx = np.where(r[i,:] == 1)[0]
    ymean[i] = y[i,idx].mean()
    ynorm[i,idx] = y[i,idx] - ymean[i]

fmin = minimize(fun=cost, x0=params, args=(ynorm, r, features, learning_rate),
                method='cg', jac=true, options={'maxiter': 100})
fmin

  status: 1
 success: false
    njev: 149
    nfev: 149
     fun: 38953.88249706676
       x: array([-0.07177334, -0.08315075,  0.1081135 , ...,  0.1817828 ,
        0.16873062,  0.03383596])
 message: 'maximum number of iterations has been exceeded.'
     jac: array([ 0.01833555,  0.07377974,  0.03999323, ..., -0.00970181,
        0.00758961, -0.01181811])

   since everything was "unrolled" for the optimization routine to work
   properly, we need to reshape our matrices back to their original
   dimensions.
x = np.matrix(np.reshape(fmin.x[:movies * features], (movies, features)))
theta = np.matrix(np.reshape(fmin.x[movies * features:], (users, features)))

x.shape, theta.shape

((1682l, 10l), (944l, 10l))

   our trained parameters are now in x and theta. we can use these to
   create some recommendations for the user we added earlier.
predictions = x * theta.t
my_preds = predictions[:, -1] + ymean
sorted_preds = np.sort(my_preds, axis=0)[::-1]
sorted_preds[:10]

matrix([[ 5.00000264],
        [ 5.00000249],
        [ 4.99999831],
        [ 4.99999671],
        [ 4.99999659],
        [ 4.99999253],
        [ 4.99999238],
        [ 4.9999915 ],
        [ 4.99999019],
        [ 4.99998643]]

   that gives us an ordered list of the top ratings, but we lost what
   index those ratings are for. we actually need to use argsort so we know
   what movie the predicted rating corresponds to.
idx = np.argsort(my_preds, axis=0)[::-1]
print("top 10 movie predictions:")
for i in range(10):
    j = int(idx[i])
    print('predicted rating of {0} for movie {1}.'.format(str(float(my_preds[j])
), movie_idx[j]))

top 10 movie predictions:
predicted rating of 5.00000264002 for movie prefontaine (1997).
predicted rating of 5.00000249142 for movie santa with muscles (1996).
predicted rating of 4.99999831018 for movie marlene dietrich: shadow and light (
1996) .
predicted rating of 4.9999967124 for movie saint of fort washington, the (1993).
predicted rating of 4.99999658864 for movie they made me a criminal (1939).
predicted rating of 4.999992533 for movie someone else's america (1995).
predicted rating of 4.99999238336 for movie great day in harlem, a (1994).
predicted rating of 4.99999149604 for movie star kid (1997).
predicted rating of 4.99999018592 for movie aiqing wansui (1994).
predicted rating of 4.99998642746 for movie entertaining angels: the dorothy day
 story (1996).

   the recommended movies don't actually line up that well with what's in
   the exercise text. the reason why isn't too clear and i haven't found
   anything to account for it, but it's possible there's a mistake in the
   code somewhere. bonus points if someone spots an error and points it
   out! still, even if there's some minor difference the bulk of the
   example is accurate.

   that concludes the last exercise! when i started this series, my goal
   was to become more proficient in python as well as refine the machine
   learning knowledge i'd gained from taking andrew's class. i feel
   confident that i accomplished that goal. my hope though is that it's
   just as valuable to read as it was for me to create.

   follow me on twitter to get new post updates.
   [33]follow @jdwittenauer
   [34]machine learning[35]data science[36]data visualization
   author

[37]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

[38]curious insight

   author

[39]john wittenauer

   data scientist, engineer, author, investor, entrepreneur

share article
     __________________________________________________________________

   [40]twitter [41]facebook [42]google+ [43]reddit [44]linkedin
   [45]pinterest

   copyright    curious insight. 2016     all rights reserved.

   proudly published with [46]ghost.

references

   visible links
   1. https://www.johnwittenauer.net/rss/
   2. https://www.johnwittenauer.net/
   3. https://www.johnwittenauer.net/about/
   4. https://www.johnwittenauer.net/tag/machine-learning/
   5. https://www.johnwittenauer.net/tag/data-visualization/
   6. https://www.johnwittenauer.net/tag/data-science/
   7. https://www.johnwittenauer.net/tag/big-data/
   8. https://www.johnwittenauer.net/tag/software-development/
   9. https://www.johnwittenauer.net/tag/emerging-technology/
  10. https://www.johnwittenauer.net/tag/entrepreneurship/
  11. https://www.johnwittenauer.net/tag/investing/
  12. https://www.johnwittenauer.net/tag/strategy/
  13. https://www.johnwittenauer.net/tag/book-review/
  14. https://www.johnwittenauer.net/tag/random-thoughts/
  15. https://www.johnwittenauer.net/tag/curious-insights/
  16. https://ghost.org/
  17. https://www.johnwittenauer.net/
  18. https://www.johnwittenauer.net/tag/machine-learning/
  19. https://www.johnwittenauer.net/tag/data-science/
  20. https://www.johnwittenauer.net/tag/data-visualization/
  21. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
  22. https://www.coursera.org/course/ml
  23. https://github.com/jdwittenauer/ipython-notebooks
  24. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/
  25. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
  26. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/
  27. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
  28. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/
  29. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/
  30. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/
  31. http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
  32. https://github.com/jdwittenauer/ipython-notebooks/blob/master/exercises/ml/ex8.pdf
  33. https://twitter.com/jdwittenauer
  34. https://www.johnwittenauer.net/tag/machine-learning/
  35. https://www.johnwittenauer.net/tag/data-science/
  36. https://www.johnwittenauer.net/tag/data-visualization/
  37. https://www.johnwittenauer.net/author/john/
  38. https://www.johnwittenauer.net/
  39. https://www.johnwittenauer.net/author/john/
  40. http://twitter.com/share?text=machine learning exercises in python, part 8&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
  41. https://www.facebook.com/sharer/sharer.php?u=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
  42. https://plus.google.com/share?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
  43. http://www.reddit.com/submit?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/&title=machine learning exercises in python, part 8
  44. http://www.linkedin.com/sharearticle?mini=true&url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/&title=machine learning exercises in python, part 8
  45. http://pinterest.com/pin/create/button/?url=https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/&description=machine learning exercises in python, part 8
  46. https://ghost.org/

   hidden links:
  48. mailto:jdwittenauer@gmail.com
  49. https://twitter.com/jdwittenauer
  50. http://www.linkedin.com/in/jdwittenauer
  51. https://github.com/jdwittenauer
  52. http://www.kaggle.com/jdwittenauer
  53. https://www.johnwittenauer.net/rss/
  54. https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/
