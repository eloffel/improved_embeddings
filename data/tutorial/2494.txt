   #[1]rss

   [2]about scott fortmann-roe
   essays
   [3]accurately measuring model prediction error
   [4]understanding the id160
   [5]subscribe [rss.png]
   understanding the id160

   june 2012

   when we discuss prediction models, prediction errors can be decomposed
   into two main subcomponents we care about: error due to "bias" and
   error due to "variance". there is a tradeoff between a model's ability
   to minimize bias and variance. understanding these two types of error
   can help us diagnose model results and avoid the mistake of over- or
   under-fitting.

bias and variance

   understanding how different sources of error lead to bias and variance
   helps us improve the data fitting process resulting in more accurate
   models. we define bias and variance in three ways: conceptually,
   graphically and mathematically.

conceptual definition

     * error due to bias: the error due to bias is taken as the difference
       between the expected (or average) prediction of our model and the
       correct value which we are trying to predict. of course you only
       have one model so talking about expected or average prediction
       values might seem a little strange. however, imagine you could
       repeat the whole model building process more than once: each time
       you gather new data and run a new analysis creating a new model.
       due to randomness in the underlying data sets, the resulting models
       will have a range of predictions. bias measures how far off in
       general these models' predictions are from the correct value.
     * error due to variance: the error due to variance is taken as the
       variability of a model prediction for a given data point. again,
       imagine you can repeat the entire model building process multiple
       times. the variance is how much the predictions for a given point
       vary between different realizations of the model.

graphical definition

   we can create a graphical visualization of bias and variance using a
   bulls-eye diagram. imagine that the center of the target is a model
   that perfectly predicts the correct values. as we move away from the
   bulls-eye, our predictions get worse and worse. imagine we can repeat
   our entire model building process to get a number of separate hits on
   the target. each hit represents an individual realization of our model,
   given the chance variability in the training data we gather. sometimes
   we will get a good distribution of training data so we predict very
   well and we are close to the bulls-eye, while sometimes our training
   data might be full of outliers or non-standard values resulting in
   poorer predictions. these different realizations result in a scatter of
   hits on the target.

   we can plot four different cases representing combinations of both high
   and low bias and variance.
             low variance high variance
   low bias
   high bias
   graphical illustration of bias and variance.

mathematical definition

   after hastie, et al. 2009 ^[6]1

   if we denote the variable we are trying to predict as $y$ and our
   covariates as $x$, we may assume that there is a relationship relating
   one to the other such as $ y = f(x) + \epsilon $ where the error term $
   \epsilon $ is normally distributed with a mean of zero like so $
   \epsilon \sim \mathcal{n}(0,\sigma_\epsilon) $.

   we may estimate a model $ \hat{f}(x) $ of $ f(x) $ using linear
   regressions or another modeling technique. in this case, the expected
   squared prediction error at a point $ x $ is:

   $$ err(x) = e\left[(y-\hat{f}(x))^2\right] $$

   this error may then be decomposed into bias and variance components:

   $$err(x) = \left(e[\hat{f}(x)]-f(x)\right)^2 +
   e\left[\left(\hat{f}(x)-e[\hat{f}(x)]\right)^2\right] +\sigma_e^2 $$

   $$err(x) = \mathrm{bias}^2 + \mathrm{variance} + \mathrm{irreducible\
   error}$$

   that third term, irreducible error, is the noise term in the true
   relationship that cannot fundamentally be reduced by any model. given
   the true model and infinite data to calibrate it, we should be able to
   reduce both the bias and variance terms to 0. however, in a world with
   imperfect models and finite data, there is a tradeoff between
   minimizing the bias and minimizing the variance.

an illustrative example: voting intentions

   let's undertake a simple model building task. we wish to create a model
   for the percentage of people who will vote for a republican president
   in the next election. as models go, this is conceptually trivial and is
   much simpler than what people commonly envision when they think of
   "modeling", but it helps us to cleanly illustrate the difference
   between bias and variance.

   a straightforward, if flawed (as we will see below), way to build this
   model would be to randomly choose 50 numbers from the phone book, call
   each one and ask the responder who they planned to vote for in the next
   election. imagine we got the following results:
   voting republican voting democratic non-respondent total
   13                16                21             50

   from the data, we estimate that the id203 of voting republican is
   13/(13+16), or 44.8%. we put out our press release that the democrats
   are going to win by over 10 points; but, when the election comes
   around, it turns out they actually lose by 10 points. that certainly
   reflects poorly on us. where did we go wrong in our model?

   clearly, there are many issues with the trivial model we built. a list
   would include that we only sample people from the phone book and so
   only include people with listed numbers, we did not follow up with
   non-respondents and they might have different voting patterns from the
   respondents, we do not try to weight responses by likeliness to vote
   and we have a very small sample size.

   it is tempting to lump all these causes of error into one big box.
   however, they can actually be separate sources causing bias and those
   causing variance.

   for instance, using a phonebook to select participants in our survey is
   one of our sources of bias. by only surveying certain classes of
   people, it skews the results in a way that will be consistent if we
   repeated the entire model building exercise. similarly, not following
   up with respondents is another source of bias, as it consistently
   changes the mixture of responses we get. on our bulls-eye diagram these
   move us away from the center of the target, but they would not result
   in an increased scatter of estimates.

   on the other hand, the small sample size is a source of variance. if we
   increased our sample size, the results would be more consistent each
   time we repeated the survey and prediction. the results still might be
   highly inaccurate due to our large sources of bias, but the variance of
   predictions will be reduced. on the bulls-eye diagram, the low sample
   size results in a wide scatter of estimates. increasing the sample size
   would make the estimates clump closers together, but they still might
   miss the center of the target.

   again, this voting model is trivial and quite removed from the modeling
   tasks most often faced in practice. in general the data set used to
   build the model is provided prior to model construction and the modeler
   cannot simply say, "let's increase the sample size to reduce variance."
   in practice an explicit tradeoff exists between bias and variance where
   decreasing one increases the other. minimizing the total error of the
   model requires a careful balancing of these two forms of error.

an applied example: voter party registration

   let's look at a bit more realistic example. assume we have a training
   data set of voters each tagged with three properties: voter party
   registration, voter wealth, and a quantitative measure of voter
   religiousness. these simulated data are plotted below ^[7]2. the x-axis
   shows increasing wealth, the y-axis increasing religiousness and the
   red circles represent republican voters while the blue circles
   represent democratic votes. we want to predict voter registration using
   wealth and religiousness as predictors.
   religiousness    
                   wealth    
   hypothetical party registration. plotted on religiousness (y-axis)
   versus wealth (x-axis).

the k-nearest neighbor algorithm

   there are many ways to go about this modeling task. for binary data
   like ours, id28s are often used. however, if we think
   there are non-linearities in the relationships between the variables, a
   more flexible, data-adaptive approach might be desired. one very
   flexible machine-learning technique is a method called k-nearest
   neighbors.

   in k-nearest neighbors, the party registration of a given voter will be
   found by plotting him or her on the plane with the other voters. the
   nearest k other voters to him or her will be found using a geographic
   measure of distance and the average of their registrations will be used
   to predict his or her registration. so if the nearest voter to him (in
   terms of wealth and religiousness) is a democrat, s/he will also be
   predicted to be a democrat.

   the following figure shows the nearest neighborhoods for each of the
   original voters. if k was specified as 1, a new voter's party
   registration would be determined by whether they fall within a red or
   blue region
   nearest neighborhoods for each point of the training data set.

   if we sampled new voters we can use our existing training data to
   predict their registration. the following figure plots the wealth and
   religiousness for these new voters and uses the nearest neighborhood
   algorithm to predict their registration. you can hover the mouse over a
   point to see the neighborhood that was used to create the point's
   prediction.
   nearest neighbor predictions for new data. hover over a point to see
   the neighborhood used to predict it.

   we can also plot the full prediction regions for where individuals will
   be classified as either democrats or republicans. the following figure
   shows this.

   a key part of the k-nearest neighborhood algorithm is the choice of k.
   up to now, we have been using a value of 1 for k. in this case, each
   new point is predicted by its nearest neighbor in the training set.
   however, k is an adjustable value, we can set it to anything from 1 to
   the number of data points in the training set. below you can adjust the
   value of k used to generate these plots. as you adjust it, both the
   following and the preceding plots will be updated to show how
   predictions change when k changes.
   k-nearest neighbors: 1
   nearest neighbor prediction regions. lighter colors indicate less
   certainty about predictions. you can adjust the value of k.

   [8]generate new training data what is the best value of k? in this
   simulated case, we are fortunate in that we know the actual model that
   was used to classify the original voters as republicans or democrats. a
   simple split was used and the dividing line is plotted in the above
   figure. voters north of the line were classified as republicans, voters
   south of the line democrats. some stochastic noise was then added to
   change a random fraction of voters' registrations. you can also
   generate new training data to see how the results are sensitive to the
   original training data.

   try experimenting with the value of k to find the best prediction
   algorithm that matches up well with the black boundary line.

bias and variance

   increasing k results in the averaging of more voters in each
   prediction. this results in smoother prediction curves. with a k of 1,
   the separation between democrats and republicans is very jagged.
   furthermore, there are "islands" of democrats in generally republican
   territory and vice versa. as k is increased to, say, 20, the transition
   becomes smoother and the islands disappear and the split between
   democrats and republicans does a good job of following the boundary
   line. as k becomes very large, say, 80, the distinction between the two
   categories becomes more blurred and the boundary prediction line is not
   matched very well at all.

   at small k's the jaggedness and islands are signs of variance. the
   locations of the islands and the exact curves of the boundaries will
   change radically as new data is gathered. on the other hand, at large
   k's the transition is very smooth so there isn't much variance, but the
   lack of a match to the boundary line is a sign of high bias.

   what we are observing here is that increasing k will decrease variance
   and increase bias. while decreasing k will increase variance and
   decrease bias. take a look at how variable the predictions are for
   different data sets at low k. as k increases this variability is
   reduced. but if we increase k too much, then we no longer follow the
   true boundary line and we observe high bias. this is the nature of the
   id160.

analytical bias and variance

   in the case of k-nearest neighbors we can derive an explicit analytical
   expression for the total error as a summation of bias and variance:

   $$ err(x) = \left(f(x)-\frac{1}{k}\sum\limits_{i=1}^k
   f(x_i)\right)^2+\frac{\sigma_\epsilon^2}{k} + \sigma_\epsilon^2 $$

   $$ err(x) = \mathrm{bias}^2 + \mathrm{variance} + \mathrm{irreducible\
   error} $$

   the variance term is a function of the irreducible error and k with the
   variance error steadily falling as k increases. the bias term is a
   function of how rough the model space is (e.g. how quickly in reality
   do values change as we move through the space of different wealths and
   religiosities). the rougher the space, the faster the bias term will
   increase as further away neighbors are brought into estimates.

managing bias and variance

   there are some key things to think about when trying to manage bias and
   variance.

fight your instincts

   a gut feeling many people have is that they should minimize bias even
   at the expense of variance. their thinking goes that the presence of
   bias indicates something basically wrong with their model and
   algorithm. yes, they acknowledge, variance is also bad but a model with
   high variance could at least predict well on average, at least it is
   not fundamentally wrong.

   this is mistaken logic. it is true that a high variance and low bias
   model can preform well in some sort of long-run average sense. however,
   in practice modelers are always dealing with a single realization of
   the data set. in these cases, long run averages are irrelevant, what is
   important is the performance of the model on the data you actually have
   and in this case bias and variance are equally important and one should
   not be improved at an excessive expense to the other.

id112 and resampling

   id112 and other resampling techniques can be used to reduce the
   variance in model predictions. in id112 (bootstrap aggregating),
   numerous replicates of the original data set are created using random
   selection with replacement. each derivative data set is then used to
   construct a new model and the models are gathered together into an
   ensemble. to make a prediction, all of the models in the ensemble are
   polled and their results are averaged.

   one powerful modeling algorithm that makes good use of id112 is
   [9]id79s. id79s works by training numerous decision
   trees each based on a different resampling of the original training
   data. in id79s the bias of the full model is equivalent to the
   bias of a single decision tree (which itself has high variance). by
   creating many of these trees, in effect a "forest", and then averaging
   them the variance of the final model can be greatly reduced over that
   of a single tree. in practice the only limitation on the size of the
   forest is computing time as an infinite number of trees could be
   trained without ever increasing bias and with a continual (if
   asymptotically declining) decrease in the variance.

asymptotic properties of algorithms

   academic statistical articles discussing prediction algorithms often
   bring up the ideas of asymptotic consistency and asymptotic efficiency.
   in practice what these imply is that as your training sample size grows
   towards infinity, your model's bias will fall to 0 (asymptotic
   consistency) and your model will have a variance that is no worse than
   any other potential model you could have used (asymptotic efficiency).

   both these are properties that we would like a model algorithm to have.
   we, however, do not live in a world of infinite sample sizes so
   asymptotic properties generally have very little practical use. an
   algorithm that may have close to no bias when you have a million
   points, may have very significant bias when you only have a few hundred
   data points. more important, an asymptotically consistent and efficient
   algorithm may actually perform worse on small sample size data sets
   than an algorithm that is neither asymptotically consistent nor
   efficient. when working with real data, it is best to leave aside
   theoretical properties of algorithms and to instead focus on their
   actual accuracy in a given scenario.

understanding over- and under-fitting

   at its root, dealing with bias and variance is really about dealing
   with over- and under-fitting. bias is reduced and variance is increased
   in relation to model complexity. as more and more parameters are added
   to a model, the complexity of the model rises and variance becomes our
   primary concern while bias steadily falls. for example, as more
   polynomial terms are added to a id75, the greater the
   resulting model's complexity will be ^[10]3. in other words, bias has a
   negative first-order derivative in response to model complexity ^[11]4
   while variance has a positive slope.
   [biasvariance.png]
   bias and variance contributing to total error.

   understanding bias and variance is critical for understanding the
   behavior of prediction models, but in general what you really care
   about is overall error, not the specific decomposition. the sweet spot
   for any model is the level of complexity at which the increase in bias
   is equivalent to the reduction in variance. mathematically:

   $$ \frac{dbias}{dcomplexity} = - \frac{dvariance}{dcomplexity} $$

   if our model complexity exceeds this sweet spot, we are in effect
   over-fitting our model; while if our complexity falls short of the
   sweet spot, we are under-fitting the model. in practice, there is not
   an analytical way to find this location. instead we must use an
   accurate measure of prediction error and explore differing levels of
   model complexity and then choose the complexity level that minimizes
   the overall error. a key to this process is the selection of an
   accurate error measure as often grossly inaccurate measures are used
   which can be deceptive. the topic of accuracy measures is [12]discussed
   here but generally resampling based measures such as cross-validation
   should be preferred over theoretical measures such as aikake's
   information criteria.

      

   scott fortmann-roe
     __________________________________________________________________

    1. [13]the elements of statistical learning. a superb book. [14]   
    2. the simulated data are purely for demonstration purposes. there is
       no reason to expect it to accurately reflect what would be observed
       if this experiment was actually carried out. [15]   
    3. the k-nearest neighbors algorithm described above is a little bit
       of a paradoxical case. it might seem that the "simplest" and least
       complex model is the one where k is 1. this reasoning is based on
       the conceit that having more neighbors be involved in calculating
       the value of a point results in greater complexity. from a
       computational standpoint this might be true, but it is more
       accurate to think of complexity from a model standpoint. the
       greater the variability of the resulting model, the greater its
       complexity. from this standpoint, we can see that larger values of
       k result in lower model complexity. [16]   
    4. it is important to note that this is only true if you have a robust
       means of properly utilizing each added covariate or piece of
       complexity. if you have an algorithm that involves non-convex
       optimizations (e.g. ones with potential local minima and maxima)
       adding additional covariates could make it harder to find the best
       set of model parameters and could actually result in higher bias.
       however, for algorithms like id75 with efficient and
       precise machinery, additional covariates will always reduce
       bias. [17]   

   all contents copyright 2012. all rights reserved. css from
   substance.io.

references

   1. http://scott.fortmann-roe.com/docs/docs/feed.rss
   2. http://scott.fortmann-roe.com/
   3. http://scott.fortmann-roe.com/docs/measuringerror.html
   4. http://scott.fortmann-roe.com/docs/biasvariance.html
   5. http://scott.fortmann-roe.com/docs/feed.rss
   6. http://scott.fortmann-roe.com/docs/biasvariance.html#fn:1
   7. http://scott.fortmann-roe.com/docs/biasvariance.html#fn:2
   8. javascript:generatetraining()
   9. http://oz.berkeley.edu/users/breiman/randomforest2001.pdf
  10. http://scott.fortmann-roe.com/docs/biasvariance.html#fn:3
  11. http://scott.fortmann-roe.com/docs/biasvariance.html#fn:4
  12. http://scott.fortmann-roe.com/docs/measuringerror.html
  13. http://www-stat.stanford.edu/~tibs/elemstatlearn/
  14. http://scott.fortmann-roe.com/docs/biasvariance.html#fnref:1
  15. http://scott.fortmann-roe.com/docs/biasvariance.html#fnref:2
  16. http://scott.fortmann-roe.com/docs/biasvariance.html#fnref:3
  17. http://scott.fortmann-roe.com/docs/biasvariance.html#fnref:4
