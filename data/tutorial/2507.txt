   #[1]leon.bottou.org [2]start [3]sitemap [4]recent changes [5]current
   namespace [6]plain html [7]wiki markup

     * [8]skip to content

   [9]leon.bottou.org

user tools

site tools

   ____________________ (1) search
     __________________________________________________________________

sidebar

   [10]home

   [11]research

   [12]publications

   [13]talks

   [14]projects

   [15]vitae

table of contents

     * [16]stochastic id119 (v.2)
          + [17]download and compilation
          + [18]stochastic gradient id166
          + [19]stochastic gradient crfs
          + [20]references
          + [21]changelog
          + [22]related projects

stochastic id119 (v.2)

   learning algorithms based on [23]stochastic gradient approximations are
   known for their poor performance on optimization tasks and their
   extremely good performance on machine learning tasks (bottou and
   bousquet, 2008). despite these proven capabilities, there were
   lingering concerns about the difficulty of setting the adaptation gains
   and achieving robust performance. stochastic gradient algorithms have
   been historically associated with back-propagation algorithms in
   multilayer neural networks, which can be very challenging non-convex
   problems. stochastic gradient algorithms are also notoriously hard to
   debug because they often appear to somehow work despite the bugs.
   experimenters are then led to believe, incorrectly, that the algorithm
   itself is flawed.

   therefore it is useful to see how stochastic id119 performs
   on simple linear and convex problems such as linear [24]support vector
   machines (id166s) or [25]id49 (crfs). this page
   proposes simple code examples illustrating the good properties of
   stochastic id119 algorithms. the provided source code values
   clarity over speed.
     * the first release of this code (2007) was written to accompany my
       [26]2007 nips tutorial on [27]large scale learning.

     * the second major release of this code (2011) adds a robust
       implementation of the averaged stochastic id119
       algorithm (ruppert, 1988) which consists of performing stochastic
       id119 iterations and simultaneously averaging the
       parameter vectors over time. when the stochastic gradient gains
       decrease with an appropriately slow schedule, polyak and juditsky
       (1992) have shown that the convergence is asymptotically as
       efficient as second-order stochastic id119 with much
       smaller computational costs. one can therefore hope to match the
       batch optimization performance after a single pass on the randomly
       shuffled training set (fabian, 1978; bottou and lecun, 2004).
       achieving one-pass learning in practice remains difficult because
       one often needs more than one pass to simply reach this favorable
       asymptotic regime. the gain schedule has a deep impact on this
       convergence. finer analyses (xu, 2010; bach and moulines, 2011)
       reveal useful guidelines to set these learning rates. xu (2010)
       also describe a wonderful way to efficiently perform the averaging
       operation when the training data is sparse. on problems with
       moderate dimension, the resulting algorithm reaches near-optimal
       test set performance in one or two passes. when the dimension
       increases, the advantage erodes, but the algorithm remains
       competitive.

download and compilation

downloading the programs

     * create a clone of the [28]git repository with command:

     git clone http://leon.bottou.org/git/sgd.git
     * as an alternative, you can still download the tarball
       [29]sgd-2.1.tar.gz released in september 2012. this is no longer
       the recommended approach but should still work.
     * make sure to read the files    readme.txt    and    data/readme.txt   .

downloading the datasets

     * download the following files from the [30]rcv1-v2 web site and
       install them in directory    data/rcv1   .
          + [31]lyrl2004_tokens_test_pt0.dat.gz,
            [32]lyrl2004_tokens_test_pt1.dat.gz
          + [33]lyrl2004_tokens_test_pt2.dat.gz,
            [34]lyrl2004_tokens_test_pt3.dat.gz
          + [35]lyrl2004_tokens_train.dat.gz, [36]rcv1-v2.topics.qrels.gz

     * download the following files from the [37]pascal large-scale
       challenge server and install them in directory    data/pascal   .
          + [38]convert.py, [39]alpha_train.lab.bz2,
            [40]alpha_train.dat.bz2 (860mb)
          + optionally, [41]webspam_train.lab.bz2,
            [42]webspam_train.dat.bz2 (1.5gb)

     * download the following files from the [43]conll2000 chunking web
       site and install them in directory    data/conll2000   .
          + [44]train.txt.gz, [45]test.txt.gz

     * run the conversion script for the pascal datasets. this requires
       python and takes some time.

   $ cd data/pascal
   $ ./convert.py -o alpha.txt alpha train
   $ ./convert.py -o webspam.txt webspam train    # (optional)

compiling

     * to compile under linux, simply type    make    in the toplevel
       directory. the compilation requires the [46]zlib library. most
       linux distributions provide this by default. things should be very
       similar under other unix operating systems.
     * to compile under windows, see the file    win/readme.txt   .

stochastic gradient id166

   the id166 programs are located in directory    sgd/id166   . the file
      id166/readme.txt    provides the full details and describes how to run
   various experiments.

id166sgd

   program    id166sgd    implements a straightforward stochastic gradient
   descent algorithm.
    1. the learning rate has the form   [0] / (1 +      [0] t) where    is the
       id173 constant.
    2. the constant   [0] is determined by performing preliminary
       experiments on a data subsample.
    3. the learning rate for the bias is multiplied by 0.01 because this
       frequently improves the condition number.
    4. the weights are represented as the product of a scalar and a vector
       in order to easily implement the weight decay operation that
       results from the id173 term.

   points (1) and (4) follow (shalev-schwartz et al., 2007).

id166asgd

   program    id166asgd    implements the averaged stochastic id119
   algorithm.
    1. the learning rate has the form   [0] / (1 +      [0] t)^ 0.75 where   
       is the id173 constant.
    2. the constant   [0] is determined by performing preliminary
       experiments on a data subsample.
    3. averaging starts only at the second epoch. the first epoch is a
       simple stochastic id119. this provides more robust
       convergence at the expense of one additional epoch. you can use
       option -avgstart to change this.
    4. both the stochastic gradient weights and the averaged weights are
       represented using a linear transformation that yields efficiency
       gains for sparse training data.

   points (1) and (4) follow (xu, 2010).

rcv1 benchmark

   the benchmark task is the recognition of rcv1 documents belonging to
   the class ccat. program    prep_rcv1    reads the rcv1-v2 token files from
   directory    data/rcv1    and computes the tf/idf features on the basis of
   the our training set. this programs produces the two files
      rcv1.train.bin.gz    and    rcv1.test.bin.gz    containing our training and
   testing sets. this short program also generates files in id166light
   format when compiled with option -dprep_id166light.
   benchmark features training examples testing examples
   rcv1      47152    781265            23149

   algorithm (hinge loss,   =1e-4) training time^* primal cost test error
   smo ([47]id166light)     16000 secs^1 0.2275 6.02%
   cutting plane ([48]id166perf)     45 secs^2 0.2278 6.03%
   hinge loss sdca ([49]liblinear -s 3 -b 1) 2.5 secs - 6.02%
   sgd (id166sgd) < 1 sec. 0.2275 6.02%
   asgd (id166asgd) < 1 sec. 0.2275 6.02%

   ^1 extrapolated from a 23642 seconds run on a 30% slower machine.
   ^2 extrapolated from a 66 seconds run on a 30% slower machine.
   ^* all timings exclude data loading time.
    algorithm (log loss,   =5e-7)  training time primal cost test error
   tron ([50]liblinear -s 0 -b 1) 33 secs       -           5.14%
   sdca ([51]liblinear -s 7 -b 1) 15 secs       -           5.13%
   sgd (id166sgd)                   4 secs        0.1283      5.14%
   asgd (id166asgd)                 5 secs        0.1281      5.13%

alpha benchmark

   you must first convert the original pascal files using the python
   script convert.py as explained above. program    prep_alpha    then reads
   the converted pascal file    data/pascal/alpha.txt    and produces the two
   files    rcv1.train.bin.gz    and    rcv1.test.bin.gz    containing our
   training and testing sets. the alpha dataset illustrates how asgd
   sometimes vastly outperforms plain sgd. the plot below shows how asgd
   reaches near-optimal test performance only one pass after the
   activation of the averaging code (epoch 2.)
   benchmark features training examples testing examples
   alpha     500      250000            250000

    algorithm (log loss,   =1e-6)  training time primal cost test error
   tron ([52]liblinear -s 0 -b 1) 115 secs      -           21.86%
   sdca ([53]liblinear -s 7 -b 1) 19 secs       -           21.86%
   sgd (id166sgd)                   51 secs       0.4717      21.95%
   asgd (id166asgd)                 4 secs        0.4713      21.86%

stochastic gradient crfs

   the crf programs    crfsgd    and    crfasgd    respectively use the sgd and
   asgd algorithms to train a [54]conditional random field. the algorithms
   are setup exactly as the id166 variants, but the implementation accounts
   for the greater structural complexity of id49.
   both programs accept the same input files as the well known [55]crf++
   software by [56]taku kudo. they produce the same tagging files which
   can be analyzed using the conll perl script    conlleval   . they use the
   same template files to specify the features and accept the same data
   files. for convenience,    crfsgd    and    crfasgd    can also directly read
   gzipped data files. see the crf++ documentation for details.

conll 2000 benchmark

   our crf benchmark is the [57]conll2000 text chunking task, which
   consists of dividing a text in syntactically correlated segments (noun
   phrases, verb phrases, etc.). we use the feature template that comes
   with crf++.
   benchmark parameters training sentences training chunks testing
   sentences testing chunks
   conll2000 1679700 8936 106978 2012 23852

   algorithm (-f 3 -c 1) training time primal cost test f1 score
   lbfgs ([58]crf++)         3000 secs^3 9042        93.74%
   sgd (crfsgd)          303 secs      9096        93.73%
   asgd (crfasgd)        134 secs      9204        93.79%

   ^3 extrapolated from a 4335 seconds run on a 30% slower machine.

references

   francis bach, eric moulines. non-asymptotic analysis of stochastic
   approximation algorithms for machine learning. technical report, to
   appear in advances in neural information processing systems (nips),
   2011.

   [59]hal link (hal-00608041).
   l  on bottou: stochastic gradient learning in neural networks,
   proceedings of neuro-n  mes 91, ec2, nimes, france, 1991.

   [60]more...
   l  on bottou: online algorithms and stochastic approximations, online
   learning and neural networks, edited by david saad, cambridge
   university press, cambridge, uk, 1998.

   [61]more...
   l  on bottou and yann lecun: large scale online learning, advances in
   neural information processing systems 16, edited by sebastian thrun,
   lawrence saul and bernhard sch  lkopf, mit press, cambridge, ma, 2004.

   [62]more...
   l  on bottou and olivier bousquet: the tradeoffs of large scale
   learning, advances in neural information processing systems, 20, mit
   press, cambridge, ma, 2008.

   [63]more...
   l  on bottou: large-scale machine learning with stochastic gradient
   descent, proceedings of the 19th international conference on
   computational statistics (compstat'2010), 177   187, edited by yves
   lechevallier and gilbert saporta, paris, france, august 2010, springer.

   [64]more...
   boris t. polyak, anatoly b. juditsky: acceleration of stochastic
   approximation by averaging, siam journal on control and optimization,
   30(4):838-855, 1992.

   [65]acm link
   david ruppert. efficient estimations from a slowly convergent
   robbins-monro process. technical report 781, cornell university
   operations research and industrial engineering, 1988.
   shai shalev-shwartz, yoram singer, nathan srebro: pegasos: primal
   estimated subgradient solver for id166, proceedings of the 24th
   international conference on machine learning, 2007.

   [66]pdf.
   s. v. n. vishwanathan, nicol n. schraudolph, mark schmidt, kevin
   murphy: accelerated training of id49 with
   stochastic gradient methods. proceedings of the 23rd international
   conference on machine learning, 969   976, acm press, 2006.

   [67]pdf
   wei xu: towards optimal one pass large scale learning with averaged
   stochastic id119, technical report, 2010.

   [68]arxiv link (arxiv:1107.2490v1)

changelog

   version date comments
   sgd-1.0.tar.gz 2007-08-22 initial release.
   [69]sgd-1.1.tar.gz 2007-10-04 relicensed under lgpl
   [70]sgd-1.2.tar.gz 2008-04-07 bug fix in id166/reprocess.cpp. thanks to
   wei xu.
   [71]sgd-1.3.tar.gz 2009-01-31 added sse2 code for dense vectors.
   [72]sgd-2.0.tar.gz 2011-10-12 major release featuring sgd and asgd.
   [73]sgd-2.1.tar.gz 2012-09-14 bug fix: istream::unget() does not clear
   eof bit
   latest git clone http:leon.bottou.org/git/sgd.git

related projects

   rather than emphasizing performance or generality, this code favors
   readability. i am therefore glad to see that many authors of machine
   learning projects have found it useful, sometimes directly, sometimes
   as a source of inspiration. here are a few links:
     * [74]implicit sgd uses the id166sgd code as a starting point.
     * [75]scikit contains a [76]sgd optimizer inspired by id166sgd.
     * [77]shogun contains a [78]sgd module derived from id166sgd.
     * [79]vowpal wabbit goes to [80]great [81]lengths to go faster than
       id166sgd ;-)
     * [82]stanford's javanlp project [83]found [84]inspiration in crfsgd.
     *    

   projects/sgd.txt    last modified: 2018/09/03 17:04 by leonb
     __________________________________________________________________

page tools

references

   visible links
   1. https://leon.bottou.org/lib/exe/opensearch.php
   2. https://leon.bottou.org/
   3. https://leon.bottou.org/projects/sgd?do=index
   4. https://leon.bottou.org/feed.php
   5. https://leon.bottou.org/feed.php?mode=list&ns=projects
   6. https://leon.bottou.org/_export/xhtml/projects/sgd
   7. https://leon.bottou.org/_export/raw/projects/sgd
   8. https://leon.bottou.org/projects/sgd#dokuwiki__content
   9. https://leon.bottou.org/start
  10. https://leon.bottou.org/start
  11. https://leon.bottou.org/research
  12. https://leon.bottou.org/papers
  13. https://leon.bottou.org/talks
  14. https://leon.bottou.org/projects
  15. https://leon.bottou.org/biography
  16. https://leon.bottou.org/projects/sgd#stochastic_gradient_descent_v2
  17. https://leon.bottou.org/projects/sgd#download_and_compilation
  18. https://leon.bottou.org/projects/sgd#stochastic_gradient_id166
  19. https://leon.bottou.org/projects/sgd#stochastic_gradient_crfs
  20. https://leon.bottou.org/projects/sgd#references
  21. https://leon.bottou.org/projects/sgd#changelog
  22. https://leon.bottou.org/projects/sgd#related_projects
  23. https://leon.bottou.org/research/stochastic
  24. https://en.wikipedia.org/wiki/support_vector_machine
  25. https://en.wikipedia.org/wiki/conditional_random_field
  26. https://leon.bottou.org/talks/largescale
  27. https://leon.bottou.org/research/largescale
  28. http://git-scm.org/
  29. https://leon.bottou.org/_media/projects/sgd-2.1.tar.gz
  30. http://jmlr.csail.mit.edu/papers/volume5/lewis04a/lyrl2004_rcv1v2_readme.htm
  31. http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a12-token-files/lyrl2004_tokens_test_pt0.dat.gz
  32. http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a12-token-files/lyrl2004_tokens_test_pt1.dat.gz
  33. http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a12-token-files/lyrl2004_tokens_test_pt2.dat.gz
  34. http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a12-token-files/lyrl2004_tokens_test_pt3.dat.gz
  35. http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a12-token-files/lyrl2004_tokens_train.dat.gz
  36. http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a08-topic-qrels/rcv1-v2.topics.qrels.gz
  37. ftp://largescale.ml.tu-berlin.de/largescale/
  38. ftp://largescale.ml.tu-berlin.de/largescale/scripts/convert.py
  39. ftp://largescale.ml.tu-berlin.de/largescale/alpha/alpha_train.lab.bz2
  40. ftp://largescale.ml.tu-berlin.de/largescale/alpha/alpha_train.dat.bz2
  41. ftp://largescale.ml.tu-berlin.de/largescale/webspam/webspam_train.lab.bz2
  42. ftp://largescale.ml.tu-berlin.de/largescale/webspam/webspam_train.dat.bz2
  43. http://www.cnts.ua.ac.be/conll2000/chunking
  44. http://www.cnts.ua.ac.be/conll2000/chunking/train.txt.gz
  45. http://www.cnts.ua.ac.be/conll2000/chunking/test.txt.gz
  46. http://www.zlib.net/
  47. http://id166light.joachims.org/
  48. http://www.cs.cornell.edu/people/tj/id166_light/id166_perf.html
  49. http://www.csie.ntu.edu.tw/~cjlin/liblinear
  50. http://www.csie.ntu.edu.tw/~cjlin/liblinear
  51. http://www.csie.ntu.edu.tw/~cjlin/liblinear
  52. http://www.csie.ntu.edu.tw/~cjlin/liblinear
  53. http://www.csie.ntu.edu.tw/~cjlin/liblinear
  54. https://en.wikipedia.org/wiki/conditional_random_field
  55. http://crfpp.sourceforge.net/
  56. http://www.chasen.org/~taku
  57. http://www.cnts.ua.ac.be/conll2000/chunking
  58. http://crfpp.sourceforge.net/
  59. http://hal.archives-ouvertes.fr/hal-00608041/en
  60. https://leon.bottou.org/papers/bottou-91c
  61. https://leon.bottou.org/papers/bottou-98x
  62. https://leon.bottou.org/papers/bottou-lecun-2004
  63. https://leon.bottou.org/papers/bottou-bousquet-2008
  64. https://leon.bottou.org/papers/bottou-2010
  65. http://dl.acm.org/citation.cfm?id=131098
  66. http://www.machinelearning.org/proceedings/icml2007/papers/587.pdf
  67. http://www.stat.purdue.edu/~vishy/papers/visschschmur06.pdf
  68. http://arxiv.org/abs/1107.2490
  69. https://leon.bottou.org/_media/projects/sgd-1.1.tar.gz
  70. https://leon.bottou.org/_media/projects/sgd-1.2.tar.gz
  71. https://leon.bottou.org/_media/projects/sgd-1.3.tar.gz
  72. https://leon.bottou.org/_media/projects/sgd-2.0.tar.gz
  73. https://leon.bottou.org/_media/projects/sgd-2.1.tar.gz
  74. http://faculty.chicagobooth.edu/panagiotis.toulis/implicitsgd.html
  75. http://scikit-learn.org/
  76. http://scikit-learn.org/0.11/modules/sgd.html
  77. http://www.shogun-toolbox.org/
  78. http://www.shogun-toolbox.org/doc/en/current/classshogun_1_1cid166sgd.html
  79. http://hunch.net/~vw/
  80. http://hunch.net/?p=309
  81. https://github.com/johnlangford/vowpal_wabbit/wiki/rcv1-example
  82. http://nlp.stanford.edu/nlp/javadoc/javanlp/
  83. http://www-nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/optimization/sgdminimizer.html
  84. http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ie/crf/crffeatureexporter.html

   hidden links:
  86. https://leon.bottou.org/_detail/projects/alpha-cost.png?id=projects%3asgd
  87. https://leon.bottou.org/_detail/projects/crf-f1.png?id=projects%3asgd
