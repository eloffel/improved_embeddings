   27 april, 2016

announcements

     * starting preparation for a2-dlearn 2016
          + looking for speakers, especially outside of nlp!
     * we are going to do a deep learning fair this year
          + want to participate?

about this talk

     * extend what we did last time to deep neural nets
          + relies a bit on the last talk, otherwise too much to cover
     * still too much for two sessions, way too much material
          + if interested, see references, lots of great material from
            others
          + wanted to talk about computation graphs, but no time, good
            refs. included
     * every one will have different knowledge, difficult to make all
       happy
          + some math, try to keep it minimal
          + some code as a guide for navigating subject matter, mostly
            torch/lua this time
     * not a diagram guy, borrowed most images (except a few)
          + you can find all original sources at the end

summary of last session

     * optimization framework with id168s for shallow models
          + model
          + data, feature vectors
          + learning through sgd with chain rule
          + id168s (hinge, log loss, square)
     * sparse optimizations
     * feature hashing
     * application of id27s for linear bow models
     * large dataset handling
     * nbid166 - a discriminative and generative linear bow model
     * slides online, and backups explaining details of particular losses

highlights

     * empirical risk minimization

   \[r_l: f \in \mathcal{f} \longmapsto e_n(f) = \frac{1}{n}
   \sum_{n=0}^{n} q(z_n, f)\]
     * performed with stochastic id119

   \[w^{(t+1)} = w^{(t)} - \eta \nabla q(y_t, f(x_t))\]

review: concrete example with l2 loss:

     * id168 \[q = \frac{1}{2} (f_w(x) - y)^2\]
     * derivative of loss with respect to params (chain rule) \[(f_w(x) -
       y) \frac{\partial}{\partial w_j}(f_w(x) - y)\] \[(f_w(x) - y)
       \frac{\partial}{\partial w_j} \sum_{d=0}^{d} w_d x_d - y\]
     * finally wrt parameter j, we get a simple update \[(f_w(x) - y)
       x_j\]
     * use chain rule to make cost derivative modular
     * note that first part of function is just the derivative of loss
       function wrt f(x)

multinomial id28

     * multinomial id28 yields a id203 distribution
       over labels
          + generalization of id28 to multi-class
          + highly convenient for our purposes
          + can be thought of as a linear application of weights, followed
            by a softmax \[p(y=i|x,w,b) = softmax_i(wx + b) = \frac{e^{w_i
            x + b_i}}{\sum_{j=1}^{c} e^{w_j x + b_j}}\]
     * building a model in torch
local model = nn.sequential()
model:add(nn.linear(xsz, nlabels))
model:add(nn.logsoftmax())
...
-- evaluate the model over fv x
local pred = model:forward(x)
    cross-id178 id168
     * we often use cross id178 loss aka negative log likelihood

   \[z = w x\] \[f(x) = softmax_i(z)\] - cross id178 definition: \[q =
   \frac{1}{d} \sum_{n=1}^{d} \mathbf{h}(p_n, q_n)\] \[\mathcal{l}(f(x),
   y) = \mathbf{h}(y, f(x)) = -\sum_{c=1}^{c} y_c log f(x)_c\]
   \[\mathcal{l}(f(x), y) = -\sum_{c=1}^{c} \mathbf{1}_{(y=c)} log
   f(x)_c\] \[= -log f(x)_{y}\]

gradient of cross-id178 loss

     * gradient turns out to be simple, though getting there takes some
       work
          + [1]https://www.youtube.com/watch?v=1n837i4s1t8
          + also in bishop book

   \[= -1 (\mathbf{1}{(y=c)} - f(x)_c)\] \[= -1 (actuals - predicted)\] -
   code in torch for evaluating
-- use nll since our model has logsoftmax otherwise use nn.crossid178criterion
()
local crit = nn.classnllcriterion()
...
-- evaluate model for fv x
local pred = model:forward(x)
-- forward computes the loss
local err = crit:forward(pred, y)
-- gradient
local grad = crit:backward(pred, y)

multinomial id28 graph

   [a2j8lcqm6v8baaaaaelftksuqmcc]

multi-layer id88 graph

   [m8kg1xlmxdiaaaaasuvork5cyii=]

forward prop in multi-layer id88

     * preactivations \[z^{(l+1)} = w^{(l)} h^{(l)} + b^{(l)}\]
     * activations (\(f^{(l)}\) is a tanh or sigmoid usually) \[h^{(l)} =
       f^{(l)}(z^{(l)})\]
     * inputs \[h^{(0)} = x\]
     * outputs \[y = h^{(l)}\]
     * if we use our softmax for final layer \[f^{(l)} = softmax\]

backprop in multi-layer id88

     * updating the pre-activations

   \[\frac{\partial e}{\partial z^{(l)}_{j}} = \frac{\partial e}{\partial
   h^{(l)}_{j}} \frac{\partial h^{(l)}_{j}}{\partial z^{(l)}_{j}} =
   \frac{\partial e}{\partial h^{(l)}_{j}} f'(z^{(l)}_{j})\] - updating
   activations (final state) \[\frac{\partial e}{\partial h^{(l)}_{i}} =
   \frac{\partial e}{\partial y_{i}} = -1 (\mathbf{1}_{(y=c)} - f(x)_c)\]
     * updating activations (non-final) \[\frac{\partial e}{\partial
       h^{(l)}_{i}} = \sum \frac{\partial e}{\partial z^{(l+1)}_{j}}
       \frac{\partial z^{(l+1)}_{j}}{\partial h^{(l)}_{i}} = \sum
       \frac{\partial e}{\partial z^{(l+1)}_{j}} w_{ij}\]

multi-layer id88 (backward propagation)

     * updating the weights

   \[\frac{\partial e}{\partial w^{(l)}_{ij}} = \frac{\partial e}{\partial
   z^{(l+1)}_{j}} \frac{\partial z^{(l+1)}_{j}} {\partial w^{(l)}_{ij}} =
   \frac{\partial e}{\partial z^{(l+1)}_{j}} h^{(l)}_{i}\]
     * we can define a layers producing activations or pre-activations
          + can define a activation and weight gradients only in terms of
            next step and forward input to layer
          + forward function to get us through the layer to our outputs
          + backward function to propagate using chain rule to our inputs

pattern of backprop

     * we saw forward() and backward() functions already. here is the
       chain rule at work

function sequential:backward(input, gradoutput, scale)
  scale = scale or 1
  local currentgradoutput = gradoutput
  local currentmodule = self.modules[#self.modules]
  for i=#self.modules-1,1,-1 do
    local previousmodule = self.modules[i]
    currentgradoutput = self:rethrowerrors(currentmodule, i+1, 'backward', previ
ousmodule.output,
        currentgradoutput, scale)
    currentmodule.gradinput = currentgradoutput
    currentmodule = previousmodule
  end
  currentgradoutput = self:rethrowerrors(currentmodule, 1, 'backward', input, cu
rrentgradoutput, scale)
  self.gradinput = currentgradoutput
  return currentgradoutput
end

mini-batch optimization: utilize parallel computation

   \[w^{(t+1)} = w^{(t)} - \eta \frac{1}{b} \sum_{b=bt + 1}^{b(t+1)}
   \frac{\partial l(z, \theta)}{\partial \theta}\]
     * bengio: when b increases, more multiply-add operations per second
       by taking advantage of parallelism or efficient matrix-matrix
       multipli- cations (instead of separate matrix-vector
       multiplications),
          + often gaining a factor of 2 in practice in overall training
            time.
          + however, as b increases, number of updates per computation
            done decreases, convergence slows
     * need to tweak learning rate hyper-params when we do this

problem motivation

     * we spend hours doing feature engineering, hopefully with good
       results
          + live or die by good feature engineering, clever nlpers
          + some kitchen-sink action as well
     * what if we could avoid this and let the machine figure it out!
          + goal of deep learning, system learns to extract its own
            features
          + feature maps

conll2000 pos chunker

# unigram
u00:%x[-2,0]
u01:%x[-1,0]
u02:%x[0,0]
u03:%x[1,0]
u04:%x[2,0]
u05:%x[-1,0]/%x[0,0]
u06:%x[0,0]/%x[1,0]

u10:%x[-2,1]
u11:%x[-1,1]
u12:%x[0,1]
u13:%x[1,1]
u14:%x[2,1]
u15:%x[-2,1]/%x[-1,1]
u16:%x[-1,1]/%x[0,1]
u17:%x[0,1]/%x[1,1]
u18:%x[1,1]/%x[2,1]

u20:%x[-2,1]/%x[-1,1]/%x[0,1]
u21:%x[-1,1]/%x[0,1]/%x[1,1]
u22:%x[0,1]/%x[1,1]/%x[2,1]

# bigram
b

partial snippet of features for dep. parsing

        heads = dict((arc[1], arc[0]) for arc in conf.arcs)
        ...
        # features from the stack
        if len(conf.stack) > 0:
        s0_pos = conf.stack[-1]
        if s0_pos < len(sentence):
            s0w = sentence[s0_pos][word]
            s0p = sentence[s0_pos][pos]
        else:
            s0w = "root"
            s0p = "root"

        if len(conf.stack) > 1:
            s1_pos = conf.stack[-2]
            s1w = sentence[s1_pos][word]
            s1p = sentence[s1_pos][pos]

        hc = head_children(conf.arcs, s0_pos, sentence)
        left_most = hc[0]
        s10p = left_most[pos]
        right_most = hc[1]
        sr0p = right_most[pos]

        sh0p = "null"
        if s0_pos in heads:
            sh0p = sentence[heads[s0_pos]][pos]

bow revisited

     * a lot of times, we use (shingled) id159s
          + id31
          + text categorization
     * looked at nbid166 last time, elegant idea, good performance, shingled
       bow
     * we can often use an mlp as a drop in for a shallow model
          + this is often called an neural bow

bow is not nlp at its best

     * bow models seem deeply flawed! don't seem to model how humans
       process text
          + they dont preserve or utilize structure or time
     * our text is deeply structured
          + articles <- sections <- paragraphs <- sentences <- words
     * at a bare minimum, most nlp seems like interpreting words (or even
       characters) over time
          + tagging is temporal where a label predicted for each word
          + id38 is temporal where a new word is predicted at
            each state
          + classification, reading a post, producing a label
     * can we do better?

nlp as temporal problem

   [z]

nlp as temporal problem

     * so is it a signal processing problem?
     * convolution: identifies, preserves locally important features
          + can be stacked building a hierarchy of features
          + key for image recognition, invariant to rotation, scaling,
            shares weights
          + takes into account ngrams, but learns to handle them
            dynamically, can preserve word orders
     * kalchbrenner: trained weights in filter correspond to a linguistic
       feature detector
          + learns to recognize a specific class of id165s
     * collobert: tagging a word considers the whole sentence
          + produces local features around each word of the sentence,
            combine into a global feature vector fed to standard layers

application of a 1d conv layer

-- this is just to demonstrate how it works, normally use temporalconvolution
-- our signal, t words, each with depth kl
local sig = torch.tensor(t, kl):normal()
convlen = sig:size(1) - kw+1
-- the internal representation of weights (wgt) is a cube: nk x kl x kw
-- note this is slightly different from temporalconvolution banding
local output = torch.zeros(convlen, nk)
-- each output feature map
for k=1,nk do
    for i=1,convlen do
        output[{i,k}] = tconv.bias[k] -- set bias in output
    end
    -- each input feature map of k: wgt[k][l]
    for l=1,kl do -- each
        local line = sig[{{},l}] -- full signal for lth input feature map
        for i=1,convlen do -- each lag
            local elem = line:sub(i, i+kw-1) -- valid region only
            output[{i,k}] = output[{i,k}] + elem:dot(wgt[{k,{},l}])
        end
    end
end

simple convolution/max-over-time (cmot) architecture

     * here is a simple id98 that works very well on short texts
          + convolutional layer generating between 60 and 500 feature
            maps, followed by non-linearity
          + max over time pooling generating a hidden output of same size
            as # feature maps
          + linear layer to number of classes
          + softmax output
     * use id97 embeddings as pretraining for a lookup table
          + a lookup table is a multiply of 1-hot vector against shared
            weight matrix, yielding a single column
          + we have an array of temporal 1-hot vectors to convert
          + we can initialize our weights with id97 model
     * for very sparse data, or when morphology important, use character
       level embeddings
          + simple idea: use id97 on char-split data (small context
            windows), average together for a word
          + note: better ways exist to combine letters to words (id98, dos
            santos model, blstm, ling model)

cmot model

   [2q==]

cmot model (torch)

function createmodel(lookuptable, cmotsz, filtsz, gpu, nc)
    local dsz = lookuptable.dsz
    local seq = nn.sequential()
    seq:add(lookuptable)
    seq:add(nn.temporalconvolution(dsz, cmotsz, filtsz))
    seq:add(nn.relu())
    seq:add(nn.max(2))
    seq:add(nn.dropout(0.5))
    seq:add(nn.linear(cmotsz, nc))
    seq:add(nn.logsoftmax())
    return gpu and seq:cuda() or seq
end
...
local w2v = id97lookuptable(opt.embed, vocab)
local f2i = {}
ts,f2i = loadtemporalindices(opt.train, w2v, f2i, opt)
es,f2i = loadtemporalindices(opt.eval, w2v, f2i, opt)
local i2f = revlut(f2i)
local model = createmodel(w2v, opt.hsz, opt.filtsz, opt.gpu, #i2f)

sgd variants using optim

w,dedw = model:getparameters()

local si = shuffle[i]
local x = options.gpu and xt[si]:cuda() or xt[si]
local y = options.gpu and yt[si]:cuda() or yt[si]
local thisbatchsz = x:size(1)

local evalf = function(wt)

  if wt ~= w then w:copy(wt) end
  -- zero weight grads
  dedw:zero()

  local pred = model:forward(x)
  local err = crit:forward(pred, y)
  ...
  -- backprop
  local grad = crit:backward(pred, y)
  model:backward(x, grad)
  -- return error weights to optimizer
  return err, dedw
end

optmeth(evalf, w, state) -- e.g. optim.adagrad

even simpler cmot algorithm

     * many times we can just avoid "fine-tuning" and read training data
       in as embedding vectors
          + off-the-shelf can work very well
     * when fixing weights, don't bother using a lookuptable, its slow and
       unneeded
     * just load feature vectors of dense embeddings (very cheap)
     * we covered preloading vectors last time for shallow classifiers

cmot model with fixed weights (torch)

function createmodel(dsz, cmotsz, filtsz, gpu, nc)
    local seq = nn.sequential()
    seq:add(nn.temporalconvolution(dsz, cmotsz, filtsz))
    seq:add(nn.relu())
    seq:add(nn.max(2))
    seq:add(nn.dropout(0.5))
    seq:add(nn.linear(cmotsz, nc))
    seq:add(nn.logsoftmax())
    return gpu and seq:cuda() or seq
end

setting up the model

   load models, not layers
local w2v = id97model(opt.embed)
local dsz = w2v.dsz
if opt.cembed ~= 'none' then
   opt.w2cv = id97model(opt.cembed)
   dsz = dsz + opt.w2cv.dsz
end

   now use to make features
local y = torch.tensor({f2i[label]})
local toks = text:split(' ')
local mx = math.min(#toks, mxlen)
local siglen = mx + (2*halffiltsz)
local x = torch.zeros(siglen, dsz)
for i=1,mx do
  local w = toks[i]
  local z = w2v:lookup(w)
  if options.w2cv then
    local q = word2chvec(options.w2cv, w)
    z = torch.cat(z, q)
  end
  x[{i + halffiltsz}] = z
end

other viewpoints

     * maybe we should think of nlp as a combination of temporal signal
       and memory
          + read words, remember context (short term memory buffer)
          + implies a recurrent neural network
     * this is prevalent approach for id38 with dnns
          + also commonly applied to other problems: tagging, parsing,
            everything?

"vanilla" id56 (elman network)

   [33jmytga07gbw7gb47gca7gc87gde7gdw7hes7hiheqaaa7]

unrolling an id56

   [of9nfsi7vvpnuaaaaaelftksuqmcc]
     * one for each timestep, shared weights
     * image from
       [2]http://colah.github.io/posts/2015-08-understanding-lstms
     * problem due to vanishing/exploding gradients
     * graves: sensitivity decays over time as new inputs overwrite
       activations of hidden layer and network 'forgets' first inputs

in practice: lstm (unrolled)

   [jf8d+skjt+a44aaaaaaasuvork5cyii=]

lstm cell (no peephole connections)

   \[i_t = \sigma( w_{x,i} x_t + w_{h,i} h_{t-1} + b_i)\] \[f_t = \sigma(
   w_{x,f} x_t + w_{h,f} h_{t-1} + b_f)\] \[z_t = tanh( w_{x,c} x_t +
   w_{h,c} h_{t-1} + b_c)\] \[c_t = f_t \cdot c_{t-1} + i_t \cdot z_t\]
   \[o_t = \sigma( w_{x,o} x_t + w_{h,o} h_{t-1} + b_o)\] \[h_t = o_t
   \cdot tanh(c_t)\]

blstms for tagging (ling et al. 2015)

   [pp1trpehlnftiaibd2r4axerguageojdqiaqcfrycaqcgucfhuagekiweageahuwaofaif
   bhiraibcosbakbqiwfqcaqcfrycaqcgqolguagugeheageahuwaofaomjcibaivfgibakbq
   iwfqcaqqlaqcaqysvgff5a4eeslxq0aaaaasuvork5cyii=]

characters and morphology

     * characters and morphology important for most tagging tasks
          + collobert used separate handcrafted features for this
          + dos santos improves on collobert model using character
            compositions learned using cmot architecture
     * would we get an improvement just by averaging char vectors?
       (obviously wouldnt likely be as good, but quick and dirty)
          + seems to be a yes, slight improvement on twitter pos!
          + keep it simple for these examples
          + code is very similar to what was shown for cmot

model

    -- make two id56 units, one for forward direction, one for backward
      local tsz = 2 * hsz
      local id56fwd = nn.lstm(dsz, hsz)
      local id56bwd = nn.lstm(dsz, hsz)

      -- this will feed the same input, and will join
      -- results along the 3rd dimension
      local concat = nn.concat(3)
      concat:add(id56fwd)

      -- create a sub-chain for reverse
      local subseq = nn.sequential()

      -- flip the signal so time is descending
      subseq:add(nn.reversedcopy())
      subseq:add(id56bwd)

      -- flip the signal again when done
      subseq:add(nn.reversedcopy())
      concat:add(subseq)
      seq:add(concat)
    seq:add(nn.dropout(0.5))
    seq:add(nn.temporalconvolution(tsz, nc, 1))

references - 1

     * full unabbreviated code
          + [3]https://github.com/dpressel/baseline
          + [4]https://github.com/dpressel/sgdtk
     * practical recommendations for gradient-based training of deep
       architectures - bengio
          + [5]http://arxiv.org/pdf/1206.5533v2.pdf
     * andrew gibiansky (blog)
          + [6]http://andrew.gibiansky.com/blog/machine-learning/fully-con
            nected-neural-networks/
     * hugo larochelle (watch every single one!)
          + [7]https://www.youtube.com/watch?v=_kowtd8t45q
     * neural architectures for id39 - lample,
       ballesteros, subramanian, kawakami, dyer
          + [8]http://arxiv.org/pdf/1603.01360v1.pdf
     * finding function in form: compositional character models for open
       vocabulary word representation - ling et. al.
          + [9]http://www.cs.cmu.edu/~lingwang/papers/emnlp2015.pdf
          + [10]https://github.com/wlin12/jnn

references - 2

     * tutorial on id28 in theano (other good ones too!)
          + [11]http://deeplearning.net/tutorial/logreg.html
     * [12]http://stats.stackexchange.com/questions/140811/how-large-shoul
       d-the-batch-size-be-for-stochastic-gradient-descent?rq=1
     * convolutional neural networks for sentence classification - kim
       (2015)
          + [13]http://arxiv.org/pdf/1408.5882v2.pdf
     * natural language processing (almost) from scratch - collobert et.
       al.
          + [14]http://jmlr.org/papers/volume12/collobert11a/collobert11a.
            pdf
     * a convolutional neural network for modelling sentences -
       kalchbrenner, grefenstette, blunsom
          + [15]http://arxiv.org/pdf/1404.2188v1.pdf
     * boosting id39 with neural character embeddings
       - dos santos, guimaraes
          + [16]http://www.aclweb.org/anthology/w15-3904
     * learning character-level representations for part-of-speech tagging
       - dos santos, zadrozny
          + [17]http://jmlr.csail.mit.edu/proceedings/papers/v32/santos14.
            pdf
     * summary of dos santos models
          + [18]https://rawgit.com/dpressel/meetups/master/nlp-reading-gro
            up-2016-03-14/presentation.html

references - 3

     * torch id56 module
          + [19]https://github.com/element-research/id56
     * justin johnson's torch-id56 code
          + [20]https://github.com/jcjohnson/torch-id56
     * [21]http://mnemstudio.org/neural-networks-elman.htm
     * [22]http://colah.github.io/posts/2015-08-understanding-lstms/
     * supervised sequence labelling with recurrent neural networks -
       graves
          + [23]https://www.cs.toronto.edu/~graves/preprint.pdf
     * [24]http://blog.aidangomez.ca/2016/04/17/backpropogating-an-lstm-a-
       numerical-example/
     * microsoft cntk book - nice computational network section
          + [25]http://research.microsoft.com/pubs/226641/cntkbook-2016021
            7..pdf
     * theano manual graph section
          + [26]http://deeplearning.net/software/theano/extending/graphstr
            uctures.html
     * presentation discussing implementing convolution in time and
       frequency domain (in c++ and r)
          + [27]https://cdn.rawgit.com/annarborrusergroup/presentations/ma
            ster/2015-11/rjava-rcpp/presentation.html#1

references

   1. https://www.youtube.com/watch?v=1n837i4s1t8
   2. http://colah.github.io/posts/2015-08-understanding-lstms
   3. https://github.com/dpressel/baseline
   4. https://github.com/dpressel/sgdtk
   5. http://arxiv.org/pdf/1206.5533v2.pdf
   6. http://andrew.gibiansky.com/blog/machine-learning/fully-connected-neural-networks/
   7. https://www.youtube.com/watch?v=_kowtd8t45q
   8. http://arxiv.org/pdf/1603.01360v1.pdf
   9. http://www.cs.cmu.edu/~lingwang/papers/emnlp2015.pdf
  10. https://github.com/wlin12/jnn
  11. http://deeplearning.net/tutorial/logreg.html
  12. http://stats.stackexchange.com/questions/140811/how-large-should-the-batch-size-be-for-stochastic-gradient-descent?rq=1
  13. http://arxiv.org/pdf/1408.5882v2.pdf
  14. http://jmlr.org/papers/volume12/collobert11a/collobert11a.pdf
  15. http://arxiv.org/pdf/1404.2188v1.pdf
  16. http://www.aclweb.org/anthology/w15-3904
  17. http://jmlr.csail.mit.edu/proceedings/papers/v32/santos14.pdf
  18. https://rawgit.com/dpressel/meetups/master/nlp-reading-group-2016-03-14/presentation.html
  19. https://github.com/element-research/id56
  20. https://github.com/jcjohnson/torch-id56
  21. http://mnemstudio.org/neural-networks-elman.htm
  22. http://colah.github.io/posts/2015-08-understanding-lstms/
  23. https://www.cs.toronto.edu/~graves/preprint.pdf
  24. http://blog.aidangomez.ca/2016/04/17/backpropogating-an-lstm-a-numerical-example/
  25. http://research.microsoft.com/pubs/226641/cntkbook-20160217
  26. http://deeplearning.net/software/theano/extending/graphstructures.html
  27. https://cdn.rawgit.com/annarborrusergroup/presentations/master/2015-11/rjava-rcpp/presentation.html#1
