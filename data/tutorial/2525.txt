   #[1]alternate

   [me-face.png] brandon amos
   (button)
     * [2]about
     * [3]blog

     *
     *
     *
     *

image completion with deep learning in tensorflow

   august 9, 2016
     __________________________________________________________________
     __________________________________________________________________

introduction

   content-aware fill is a powerful tool designers and photographers use
   to fill in unwanted or missing parts of images. image completion and
   [4]inpainting are closely related technologies used to fill in missing
   or corrupted parts of images. there are many ways to do content-aware
   fill, image completion, and inpainting. in this blog post, i present
   raymond yeh and chen chen et al.   s paper    [5]semantic image inpainting
   with perceptual and contextual losses,    which was just posted on arxiv
   on july 26, 2016. this paper shows how to use deep learning for image
   completion with a [6]dcgan. this blog post is meant for a general
   technical audience with some deeper portions for people with a machine
   learning background. i   ve added [ml-heavy] tags to sections to indicate
   that the section can be skipped if you don   t want too many details. we
   will only look at the constrained case of completing missing pixels
   from images of faces. i have released all of the [7]tensorflow source
   code behind this post on github at
   [8]bamos/dcgan-completion.tensorflow.

   we   ll approach image completion in three steps.
    1. [9]we   ll first interpret images as being samples from a id203
       distribution.
    2. [10]this interpretation lets us learn how to generate fake images.
    3. [11]then we   ll find the best fake image for completion.

   photoshop example of automatically filling in missing image parts.
   (image cc licensed, [12]source.)

   photoshop example of automatically removing unwanted image parts.
   (image cc licensed, [13]source.)

   completions generated by what we   ll cover in this blog post. the
   centers of these images are being automatically generated. the source
   code to create this is available [14]here. these are not curated! i
   selected a random subset of images from the lfw dataset.
     __________________________________________________________________

step 1: interpreting images as samples from a id203 distribution

how would you fill in the missing information?

   in the examples above, imagine you   re building a system to fill in the
   missing pieces. how would you do it? how do you think the human brain
   does it? what kind of information would you use?

   in this post we will focus on two types of information:
    1. contextual information: you can infer what missing pixels are based
       on information provided by surrounding pixels.
    2. perceptual information: you interpret the filled in portions as
       being    normal,    like from what you   ve seen in real life or from
       other pictures.

   both of these are important. without contextual information, how do you
   know what to fill in? without perceptual information, there are many
   valid completions for a context. something that looks    normal    to a
   machine learning system might not look normal to humans.

   it would be nice to have an exact, intuitive algorithm that captures
   both of these properties that says step-by-step how to complete an
   image. creating such an algorithm may be possible for specific cases,
   but in general, nobody knows how. today   s best approaches use
   statistics and machine learning to learn an approximate technique.

but where does statistics fit in? these are images.

   to motivate the problem, let   s start by looking at a [15]id203
   distribution that is well-understood and can be represented concisely
   in closed form: a [16]normal distribution. here   s the [17]id203
   density function (pdf) for a normal distribution. you can interpret the
   pdf as going over the input space horizontally with the vertical axis
   showing the id203 that some value occurs. (if you   re interested,
   the code to create these plots is available at
   [18]bamos/dcgan-completion.tensorflow:simple-distributions.py.)

   pdf for a normal distribution.

   let   s sample from the distribution to get some data. make sure you
   understand the connection between the pdf and the samples.

   samples from a normal distribution.

   this is a 1d id203 distribution because the input only goes along
   a single dimension. we can do the same thing in two dimensions.

   pdf and samples from a 2d normal distribution. the pdf is shown as a
   contour plot and the samples are overlaid.

   the key relationship between images and statistics is that we can
   interpret images as samples from a high-dimensional id203
   distribution. the id203 distribution goes over the pixels of
   images. imagine you   re taking a picture with your camera. this picture
   will have some finite number of pixels. when you take an image with
   your camera, you are sampling from this complex id203
   distribution. this distribution is what we   ll use to define what makes
   an image normal or not. with images, unlike with the normal
   distributions, we don   t know the true id203 distribution and we
   can only collect samples.

   in this post, we   ll use color images represented by the [19]rgb color
   model. our images will be 64 pixels wide and 64 pixels high, so our
   id203 distribution has $64\cdot 64\cdot 3 \approx 12k$
   dimensions.

so how can we complete images?

   let   s first consider the multivariate normal distribution from before
   for intuition. given $x=1$, what is the most probable $y$ value? we can
   find this by maximizing the value of the pdf over all possible $y$
   values with $x=1$ fixed.

   finding the most probable $y$ value given some fixed $x$ in a
   multivariate normal distribution.

   this concept naturally extends to our image id203 distribution
   when we know some values and want to complete the missing values. just
   pose it as a maximization problem where we search over all of the
   possible missing values. the completion will be the most probable
   image.

   visually looking at the samples from the normal distribution, it seems
   reasonable that we could find the pdf given only samples. just pick
   your favorite [20]statistical model and fit it to the data.

   however we don   t use this method in practice. while the pdf is easy to
   recover for simple distributions, it   s difficult and often intractable
   for more complex distributions over images. the complexity partly comes
   from intricate [21]conditional dependencies: the value of one pixel
   depends on the values of other pixels in the image. also, maximizing
   over a general pdf is an extremely difficult and often intractable
   non-id76 problem.

step 2: quickly generating fake images

learning to generate new samples from an unknown id203 distribution

   instead of learning how to compute the pdf, another well-studied idea
   in statistics is to learn how to generate new (random) samples with a
   [22]generative model. generative models can often be difficult to train
   or intractable, but lately the deep learning community has made some
   amazing progress in this space. [23]yann lecun gives a great
   introduction to one way of training generative models (adversarial
   training) in [24]this quora post, describing the idea as the most
   interesting idea in the last 10 years in machine learning:

   [25]yann lecun   s introduction to adversarial training from [26]this
   quora post.

   street fighter analogy for adversarial networks from the [27]eyescream
   post. the networks fight each other and improve together, like two
   humans playing against each other in a game. [28]image source.

   there are other ways to train generative models with deep learning,
   like [29]id5 (vaes). in this post we   ll only focus
   on generative adversarial nets (gans).

[ml-heavy] generative adversarial net (gan) building blocks

   these ideas started with ian goodfellow et al.   s landmark paper
      [30]generative adversarial nets    (gans), published at the [31]neural
   information processing systems (nips) conference in 2014. the idea is
   that we define a simple, well-known distribution and represent it as
   $p_z$. for the rest of this post, we   ll use $p_z$ as a uniform
   distribution between -1 and 1 (inclusively). we represent sampling a
   number from this distribution as $z\sim p_z$. if $p_z$ is
   5-dimensional, we can sample it with one line of python with [32]numpy:
z = np.random.uniform(-1, 1, 5)
array([ 0.77356483,  0.95258473, -0.18345086,  0.69224724, -0.34718733])

   now this we have a simple distribution we can easily sample from, we   d
   like to define a function $g(z)$ that produces samples from our
   original id203 distribution.
def g(z):
   ...
   return imagesample

z = np.random.uniform(-1, 1, 5)
imagesample = g(z)

   so how do we define $g(z)$ so that it takes a vector on input and
   returns an image? we   ll use a deep neural network. there are many great
   introductions to deep neural network basics, so i won   t cover them
   here. some great references that i recommend are [33]stanford   s cs231n
   course, ian goodfellow et al.   s [34]deep learning book, [35]image
   kernels explained visually, and [36]convolution arithmetic guide.

   there are many ways we can structure $g(z)$ with deep learning. the
   original gan paper proposed the idea, a training procedure, and
   preliminary experimental results. the idea has been greatly built on
   and improved. one of the most recent ideas was presented in the paper
      [37]unsupervised representation learning with deep convolutional
   id3    by alec radford, luke metz, and
   soumith chintala at the [38]international conference on learning
   representations (iclr, pronounced    eye-clear   ) in 2016. this paper
   presents deep convolutional gans (called dcgans) that use
   fractionally-strided convolutions to upsample images.

   what is a fractionally-strided convolution and how do they upsample
   images? vincent dumoulin and francesco visin   s paper    [39]a guide to
   convolution arithmetic for deep learning    and [40]conv_arithmetic
   project is a very well-written introduction to convolution arithmetic
   in deep learning. the visualizations are amazing and give great
   intuition into how fractionally-strided convolutions work. first, make
   sure you understand how a normal convolution slides a kernel over a
   (blue) input space to produce the (green) output space. here, the
   output is smaller than the input. (if you don   t, go through [41]the
   cs231n id98 section or the [42]convolution arithmetic guide.)

   illustration of a convolution from the input (blue) to output (green).
   this image is from [43]vdumoulin/conv_arithmetic.

   next, suppose that you have a 3x3 input. our goal is to upsample so
   that the output is larger. you can interpret a fractionally-strided
   convolution as expanding the pixels so that there are zeros in-between
   the pixels. then the convolution over this expanded space will result
   in a larger output space. here, it   s 5x5.

   illustration of a fractionally-strided convolution from the input
   (blue) to output (green). this image is from
   [44]vdumoulin/conv_arithmetic.

   as a side-note, there are many names for convolutional layers that
   upsample: [45]full convolution, in-network upsampling,
   fractionally-strided convolution, backwards convolution, deconvolution,
   upconvolution, or transposed convolution. using the term
      deconvolution    for this is strongly discouraged because it   s an
   over-loaded term: [46]the mathematical operation or [47]other uses in
   id161 have a completely different meaning.

   now that we have fractionally-strided convolutions as building blocks,
   we can finally represent $g(z)$ so that it takes a vector $z\sim p_z$
   on input and outputs a 64x64x3 rgb image.

   one way to structure the generator $g(z)$ with a dcgan. this image is
   from the [48]dcgan paper.

   the [49]dcgan paper also presents other tricks and modifications for
   training dcgans like using batch id172 or leaky relus if you   re
   interested.

using $g(z)$ to produce fake images

   let   s pause and appreciate how powerful this formulation of $g(z)$ is.
   the [50]dcgan paper showed how a dcgan can be trained on a dataset of
   bedroom images. then sampling $g(z)$ will produce the following fake
   images of what the generator thinks bedrooms looks like. none of these
   images are in the original dataset!

   generating bedroom images with a dcgan. this image is from the
   [51]dcgan paper.

   also, you can perform vector arithmetic on the $z$ input space. the
   following is on a network trained to produce faces.

   face arithmetic with dcgans. this image is from the [52]dcgan paper.

[ml-heavy] training dcgans

   now that we have defined $g(z)$ and have seen how powerful the
   formulation is, how do we train it? we have a lot of [53]latent
   variables (or parameters) that we need to find. this is where using
   adversarial networks comes in.

   first let   s define some notation. let the (unknown) id203
   distribution of our data be $p_{\rm data}$. also we can interpret
   $g(z)$ (where $z\sim p_z$) as drawing samples from a id203
   distribution, let   s call it the generative id203 distribution,
   $p_g$.
   id203 distribution notation meaning
   $p_z$ the (known, simple) distribution $z$ goes over
   $p_{\rm data}$ the (unknown) distribution over our images. this is
   where our images are sampled from.
   $p_g$ the generative distribution that the generator $g$ samples from.
   we would like for $p_g=p_{\rm data}$

   the discriminator network $d(x)$ takes some image $x$ on input and
   returns the id203 that the image $x$ was sampled from $p_{\rm
   data}$. the discriminator should return a value closer to 1 when the
   image is from $p_{\rm data}$ and a value closer to 0 when the image is
   fake, like an image sampled from $p_g$. in dcgans, $d(x)$ is a
   traditional convolutional network.

   the discriminator convolutional network. this image is from the
   [54]inpainting paper.

   the goal of training the discriminator $d(x)$ is:
    1. maximize $d(x)$ for every image from the true data distribution
       $x\sim p_{\rm data}$.
    2. minimize $d(x)$ for every image not from the true data distribution
       $x\not\sim p_{\rm data}$.

   the goal of training the generator $g(z)$ is to produce samples that
   fool $d$. the output of the generator is an image and can be used as
   the input to the discriminator. therefore, the generator wants to to
   maximize $d(g(z))$, or equivalently minimize $1-d(g(z))$ because $d$ is
   a id203 estimate and only ranges between 0 and 1.

   as presented in the paper, training adversarial networks is done with
   the following [55]minimax game. the [56]expectations in the first term
   go over the samples from the true data distribution and over samples
   from $p_z$ in the second term, which goes over $g(z)\sim p_g$.

   we will train $d$ and $g$ by taking the gradients of this expression
   with respect to their parameters. we know how to quickly compute every
   part of this expression. the expectations are approximated in
   minibatches of size $m,$ and the inner maximization can be approximated
   with $k$ gradient steps. it turns out $k=1$ works well for training.

   let $\theta_d$ be the parameters of the discriminator and $\theta_g$ be
   the parameters the generator. the gradients of the loss with respect to
   $\theta_d$ and $\theta_g$ can be computed with [57]id26
   because $d$ and $g$ are defined by well-understood neural network
   components. here   s the training algorithm from the [58]gan paper.
   ideally once this is finished, $p_g=p_{\rm data}$, so $g(z)$ will be
   able to produce new samples from $p_{\rm data}$.

   gan training algorithm from the [59]gan paper.

existing gan and dcgan implementations

   there are many great gan and dcgan implementations on github you can
   browse:
     * [60]goodfeli/adversarial: theano gan implementation released by the
       authors of the gan paper.
     * [61]tqchen/mxnet-gan: unofficial mxnet gan implementation.
     * [62]newmu/dcgan_code: theano dcgan implementation released by the
       authors of the dcgan paper.
     * [63]soumith/dcgan.torch: torch dcgan implementation by one of the
       authors (soumith chintala) of the dcgan paper.
     * [64]carpedm20/dcgan-tensorflow: unofficial tensorflow dcgan
       implementation.
     * [65]openai/improved-gan: code behind [66]openai   s first paper.
       extensively modifies carpedm20/dcgan-tensorflow.
     * [67]mattya/chainer-dcgan: unofficial chainer dcgan implementation.
     * [68]jacobgil/keras-dcgan: unofficial (and incomplete) keras dcgan
       implementation.

   moving forward, we will build on [69]carpedm20/dcgan-tensorflow.

[ml-heavy] dcgans in tensorflow

   the implementation for this portion is in my
   [70]bamos/dcgan-completion.tensorflow github repository. i strongly
   emphasize that the code in this portion is from taehoon kim   s
   [71]carpedm20/dcgan-tensorflow repository. we   ll use my repository here
   so that we can easily use the image completion portions in the next
   section.

   the implementation is mostly in a python class called dcgan in
   [72]model.py. it   s helpful to have everything in a class like this so
   that intermediate states can be saved after training and then loaded
   for later use.

   first let   s define the generator and discriminator architectures. the
   linear, conv2d_transpose, conv2d, and lrelu functions are defined in
   [73]ops.py.
def generator(self, z):
    self.z_, self.h0_w, self.h0_b = linear(z, self.gf_dim*8*4*4,
                                           'g_h0_lin', with_w=true)

    self.h0 = tf.reshape(self.z_, [-1, 4, 4, self.gf_dim * 8])
    h0 = tf.nn.relu(self.g_bn0(self.h0))

    self.h1, self.h1_w, self.h1_b = conv2d_transpose(h0,
        [self.batch_size, 8, 8, self.gf_dim*4], name='g_h1', with_w=true)
    h1 = tf.nn.relu(self.g_bn1(self.h1))

    h2, self.h2_w, self.h2_b = conv2d_transpose(h1,
        [self.batch_size, 16, 16, self.gf_dim*2], name='g_h2', with_w=true)
    h2 = tf.nn.relu(self.g_bn2(h2))

    h3, self.h3_w, self.h3_b = conv2d_transpose(h2,
        [self.batch_size, 32, 32, self.gf_dim*1], name='g_h3', with_w=true)
    h3 = tf.nn.relu(self.g_bn3(h3))

    h4, self.h4_w, self.h4_b = conv2d_transpose(h3,
        [self.batch_size, 64, 64, 3], name='g_h4', with_w=true)

    return tf.nn.tanh(h4)

def discriminator(self, image, reuse=false):
    if reuse:
        tf.get_variable_scope().reuse_variables()

    h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv'))
    h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv')))
    h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv')))
    h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv')))
    h4 = linear(tf.reshape(h3, [-1, 8192]), 1, 'd_h3_lin')

    return tf.nn.sigmoid(h4), h4

   when we   re initializing this class, we   ll use these functions to create
   the models. we need two versions of the discriminator that shares (or
   reuses) parameters. one for the minibatch of images from the data
   distribution and the other for the minibatch of images from the
   generator.
self.g = self.generator(self.z)
self.d, self.d_logits = self.discriminator(self.images)
self.d_, self.d_logits_ = self.discriminator(self.g, reuse=true)

   next, we   ll define the id168s. instead of using the sums, we   ll
   use the [74]cross id178 between $d$   s predictions and what we want
   them to be because it works better. the discriminator wants the
   predictions on the    real    data to be all ones and the predictions on
   the    fake    data from the generator to be all zeros. the generator wants
   the discriminator   s predictions to be all ones.
self.d_loss_real = tf.reduce_mean(
    tf.nn.sigmoid_cross_id178_with_logits(self.d_logits,
                                            tf.ones_like(self.d)))
self.d_loss_fake = tf.reduce_mean(
    tf.nn.sigmoid_cross_id178_with_logits(self.d_logits_,
                                            tf.zeros_like(self.d_)))
self.d_loss = self.d_loss_real + self.d_loss_fake

self.g_loss = tf.reduce_mean(
    tf.nn.sigmoid_cross_id178_with_logits(self.d_logits_,
                                            tf.ones_like(self.d_)))

   gather the variables for each of the models so they can be trained
   separately.
t_vars = tf.trainable_variables()

self.d_vars = [var for var in t_vars if 'd_' in var.name]
self.g_vars = [var for var in t_vars if 'g_' in var.name]

   now we   re ready to optimize the parameters and we   ll use [75]adam,
   which is an adaptive non-id76 method commonly used in
   modern deep learning. adam is often competitive with sgd and (usually)
   doesn   t require hand-tuning of the learning rate, momentum, and other
   hyper-parameters.
d_optim = tf.train.adamoptimizer(config.learning_rate, beta1=config.beta1) \
                    .minimize(self.d_loss, var_list=self.d_vars)
g_optim = tf.train.adamoptimizer(config.learning_rate, beta1=config.beta1) \
                    .minimize(self.g_loss, var_list=self.g_vars)

   we   re ready to go through our data. in each epoch, we sample some
   images in a minibatch and run the optimizers to update the networks.
   interestingly if $g$ is only updated once, the discriminator   s loss
   does not go to zero. also, i think the additional calls at the end to
   d_loss_fake and d_loss_real are causing a little bit of unnecessary
   computation and are redundant because these values are computed as part
   of d_optim and g_optim. as an exercise in tensorflow, you can try
   optimizing this part and send a pr to the original repo. (if you do,
   ping me and i   ll update it in mine too.)
for epoch in xrange(config.epoch):
    ...
    for idx in xrange(0, batch_idxs):
        batch_images = ...
        batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \
                    .astype(np.float32)

        # update d network
        _, summary_str = self.sess.run([d_optim, self.d_sum],
            feed_dict={ self.images: batch_images, self.z: batch_z })

        # update g network
        _, summary_str = self.sess.run([g_optim, self.g_sum],
            feed_dict={ self.z: batch_z })

        # run g_optim twice to make sure that d_loss does not go to zero
        # (different from paper)
        _, summary_str = self.sess.run([g_optim, self.g_sum],
            feed_dict={ self.z: batch_z })

        errd_fake = self.d_loss_fake.eval({self.z: batch_z})
        errd_real = self.d_loss_real.eval({self.images: batch_images})
        errg = self.g_loss.eval({self.z: batch_z})

   that   s it! of course the full code has a little more book-keeping that
   you can check out in [76]model.py.

running dcgan on your images

   if you skipped the last section, but are interested in running some
   code: the implementation for this portion is in my
   [77]bamos/dcgan-completion.tensorflow github repository. i strongly
   emphasize that the code in this portion is from taehoon kim   s
   [78]carpedm20/dcgan-tensorflow repository. we   ll use my repository here
   so that we can easily use the image completion portions in the next
   section. as a warning, if you don   t have a cuda-enabled gpu, training
   the network in this portion may be prohibitively slow.

   please message me if the following doesn   t work for you!

   first let   s clone my [79]bamos/dcgan-completion.tensorflow and
   [80]openface repositories. we   ll use openface   s python-only portions to
   pre-process images. don   t worry, you won   t have to install openface   s
   torch dependency. create a new working directory for this and clone the
   repositories:
git clone https://github.com/cmusatyalab/openface.git
git clone https://github.com/bamos/dcgan-completion.tensorflow.git

   next, install [81]opencv and [82]dlib for python 2. (openface currently
   uses python 2, but if you   re interested, i   d be happy if you make it
   python 3 compatible and [83]send in a pr mentioning this issue.) these
   can a little tricky to get set up and i   ve included a few notes on what
   versions i use and how i install in the [84]openface setup guide. next,
   install openface   s python library so we can preprocess images. if
   you   re not using a virtual environment, you should use sudo when
   running setup.py to globally install openface. (if you have trouble
   setting up this portion, you can also use our openface docker build as
   described in the [85]openface setup guide.)
cd openface
pip2 install -r requirements.txt
python2 setup.py install
models/get-models.sh
cd ..

   next download a dataset of face images. it doesn   t matter if they have
   labels or not, we   ll get rid of them. a non-exhaustive list of options
   are: [86]ms-celeb-1m, [87]celeba, [88]casia-webface, [89]facescrub,
   [90]lfw, and [91]megaface. place the dataset in
   dcgan-completion.tensorflow/data/your-dataset/raw to indicate it   s the
   dataset   s raw images.

   now we   ll use openface   s alignment tool to pre-process the images to be
   64x64.
./openface/util/align-dlib.py data/dcgan-completion.tensorflow/data/your-dataset
/raw align innereyesandbottomlip data/dcgan-completion.tensorflow/data/your-data
set/aligned --size 64

   and finally we   ll flatten the aligned images directory so that it just
   contains images and no sub-directories.
cd dcgan-completion.tensorflow/data/your-dataset/aligned
find . -name '*.png' -exec mv {} . \;
find . -type d -empty -delete
cd ../../..

   we   re ready to train the dcgan. after [92]installing tensorflow, start
   the training.
./train-dcgan.py --dataset ./data/your-dataset/aligned --epoch 20

   you can check what randomly sampled images from the generator look like
   in the samples directory. i   m training on the casia-webface and
   facescrub datasets because i had them on hand. after 14 epochs, the
   samples from mine look like:

   samples from my dcgan after training for 14 epochs with the combined
   casia-webface and facescrub dataset.

   you can also view the tensorflow graphs and id168s with
   [93]tensorboard.
tensorboard --logdir ./logs

   tensorboard loss visualizations. will be updated in real-time when
   training.

   tensorboard visualization of dcgan networks.

step 3: finding the best fake image for image completion

image completion with dcgans

   now that we have a trained discriminator $d(x)$ and generator $g(z)$,
   how can we use them to complete images? in this section i present the
   techniques in raymond yeh and chen chen et al.   s paper    [94]semantic
   image inpainting with perceptual and contextual losses,    which was just
   posted on arxiv on july 26, 2016.

   to do completion for some image $y$, something reasonable that doesn   t
   work is to maximize $d(y)$ over the missing pixels. this will result in
   something that   s neither from the data distribution ($p_{\rm data}$)
   nor the generative distribution ($p_g$). what we want is a reasonable
   projection of $y$ onto the generative distribution.

   (a): ideal reconstruction of $y$ onto the generative distribution (the
   blue manifold). (b): failure example of trying to reconstruct $y$ by
   only maximizing $d(y)$. this image is from the [95]inpainting paper.

[ml-heavy] id168 for projecting onto $p_g$

   to define a reasonable projection, let   s first define some notation for
   completing images. we use a binary mask $m$ that has values 0 or 1. a
   value of 1 represents the parts of the image we want to keep and a
   value of 0 represents the parts of the image we want to complete. we
   can now define how to complete an image $y$ given the binary mask $m$.
   multiply the elements of $y$ by the elements of $m$. the element-wise
   product between two matrices is sometimes called the [96]hadamard
   product and is represented as $m\odot y$. $m\odot y$ gives the original
   part of the image.

   illustration of a binary mask.

   next, suppose we   ve found an image from the generator $g(\hat z)$ for
   some $\hat z$ that gives a reasonable reconstruction of the missing
   portions. the completed pixels $(1-m)\odot g(\hat z)$ can be added to
   the original pixels to create the reconstructed image:

   now all we need to do is find some $g(\hat z)$ that does a good job at
   completing the image. to find $\hat z$, let   s revisit our goals of
   recovering contextual and perceptual information from the beginning of
   this post and pose them in the context of dcgans. we   ll do this by
   defining [97]id168s for an arbitrary $z\sim p_z$. a smaller
   value of these id168s means that $z$ is more suitable for
   completion than a larger value.

   contextual loss: to keep the same context as the input image, make sure
   the known pixel locations in the input image $y$ are similar to the
   pixels in $g(z)$. we need to penalize $g(z)$ for not creating a similar
   image for the pixels that we know about. formally, we do this by
   element-wise subtracting the pixels in $y$ from $g(z)$ and looking at
   how much they differ:

   where $||x||_1=\sum_i |x_i|$ is the [98]$\ell_1$ norm of some vector
   $x$. the [99]$\ell_2$ norm is another reasonable choice, but the
   inpainting paper says that the $\ell_1$ norm works better in practice.

   in the ideal case, all of the pixels at known locations are the same
   between $y$ and $g(z)$. then $g(z)_i - y_i = 0$ for the known pixels
   $i$ and thus ${\mathcal l}_{\rm contextual}(z) = 0$.

   perceptual loss: to recover an image that looks real, let   s make sure
   the discriminator is properly convinced that the image looks real.
   we   ll do this with the same criterion used in training the dcgan:
     __________________________________________________________________

   we   re finally ready to find $\hat z$ with a combination of the
   contextual and perceptual losses:

   where $\lambda$ is a hyper-parameter that controls how import the
   contextual loss is relative to the perceptual loss. (i use
   $\lambda=0.1$ by default and haven   t played with it too much.) then as
   before, the reconstructed image fills in the missing values of $y$ with
   $g(\hat z)$:

   the inpainting paper also uses [100]poisson blending ([101]see chris
   traile   s post for an introduction to it) to smooth the reconstructed
   image.

[ml-heavy] tensorflow implementation of image completion with dcgans

   this section presents the changes i   ve added to
   [102]bamos/dcgan-completion.tensorflow that modifies taehoon kim   s
   [103]carpedm20/dcgan-tensorflow for image completion.

   we can re-use a lot of the existing variables for completion. the only
   new variable we   ll add is a mask for completion:
self.mask = tf.placeholder(tf.float32, [none] + self.image_shape, name='mask')

   we   ll solve ${\rm arg} \min_z {\mathcal l}(z)$ iteratively with
   id119 with the gradients $\nabla_z {\mathcal l}(z)$.
   tensorflow   s [104]automatic differentiation can compute this for us
   once we   ve defined the id168s! so the entire idea of completion
   with dcgans can be implemented by just adding four lines of tensorflow
   code to an existing dcgan implementation. (of course implementing this
   also involves some non-tensorflow code.)
self.contextual_loss = tf.reduce_sum(
    tf.contrib.layers.flatten(
        tf.abs(tf.mul(self.mask, self.g) - tf.mul(self.mask, self.images))), 1)
self.perceptual_loss = self.g_loss
self.complete_loss = self.contextual_loss + self.lam*self.perceptual_loss
self.grad_complete_loss = tf.gradients(self.complete_loss, self.z)

   next, let   s define a mask. i   ve only added one for the center portions
   of images, but feel free to add something else like a random mask and
   send in a pull request.
if config.masktype == 'center':
    scale = 0.25
    assert(scale <= 0.5)
    mask = np.ones(self.image_shape)
    l = int(self.image_size*scale)
    u = int(self.image_size*(1.0-scale))
    mask[l:u, l:u, :] = 0.0

   for id119, we   ll use [105]projected id119 with
   minibatches and momentum to project $z$ to be in $[-1,1]$.
for idx in xrange(0, batch_idxs):
    batch_images = ...
    batch_mask = np.resize(mask, [self.batch_size] + self.image_shape)
    zhats = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))

    v = 0
    for i in xrange(config.niter):
        fd = {
            self.z: zhats,
            self.mask: batch_mask,
            self.images: batch_images,
        }
        run = [self.complete_loss, self.grad_complete_loss, self.g]
        loss, g, g_imgs = self.sess.run(run, feed_dict=fd)

        v_prev = np.copy(v)
        v = config.momentum*v - config.lr*g[0]
        zhats += -config.momentum * v_prev + (1+config.momentum)*v
        zhats = np.clip(zhats, -1, 1)

completing your images

   select some images to complete and place them in
   dcgan-completion.tensorflow/your-test-data/raw. align them as before as
   dcgan-completion.tensorflow/your-test-data/aligned. i randomly selected
   images from the lfw for this. my dcgan wasn   t trained on any of the
   identities in the lfw.

   you can run the completion on your images with:
./complete.py ./data/your-test-data/aligned/* --outdir outputimages

   this will run and periodically output the completed images to --outdir.
   you can create create a gif from these with imagemagick:
cd outputimages
convert -delay 10 -loop 0 completed/*.png completion.gif

   final image completions. the centers of these images are being
   automatically generated. the source code to create this is available
   [106]here. these are not curated! i selected a random subset of images
   from the lfw dataset.

conclusion

   thanks for reading, we made it! in this blog post, we covered one
   method of completing images that:
    1. [107]interprets images as being samples from a id203
       distribution.
    2. [108]generates fake images.
    3. [109]finds the best fake image for completion.

   my examples were on faces, but dcgans can be trained on other types of
   images too. in general, gans are difficult to train and we don   t yet
   know how to train them on certain classes of objects, [110]nor on large
   images. however they   re a promising model and i   m excited to see where
   gan research takes us!

   feel free to ping me on twitter [111]@brandondamos, github [112]@bamos,
   or [113]elsewhere if you have any comments or suggestions on this post.
   thanks!

   dcgan samples (left) and improved gan samples (right, not covered in
   this post) on id163 showing that we don   t yet understand how to use
   gans on every type of image. this image is from the [114]improved gan
   paper.

citation for this article/project

   please consider citing this project in your publications if it helps
   your research. the following is a [115]bibtex and plaintext reference.
   the bibtex entry requires the url latex package.
@misc{amos2016image,
    title        = {{image completion with deep learning in tensorflow}},
    author       = {amos, brandon},
    howpublished = {\url{http://bamos.github.io/2016/08/09/deep-completion}},
    note         = {accessed: [insert date here]}
}

brandon amos. image completion with deep learning in tensorflow.
http://bamos.github.io/2016/08/09/deep-completion.
accessed: [insert date here]

partial bibliography for further reading

     * raymond yeh and chen chen et al.    [116]semantic image inpainting
       with perceptual and contextual losses:    paper this post was based
       on.
     * d. pathak et al. [117]context encoders: id171 by
       inpainting at cvpr 2016: another recent method for inpainting that
       use similar id168s and have released code on github at
       [118]pathak22/context-encoder. this method is less computationally
       expensive than yeh and chen et al. because they use a single
       forward network pass instead of solving an optimization problem
       that involves many forward and backward passes.
     * ian goodfellow et al.    [119]generative adversarial nets   
     * vincent dumoulin and francesco visin.    [120]a guide to convolution
       arithmetic for deep learning   
     * alec radford, luke metz, and soumith chintala.    [121]unsupervised
       representation learning with deep convolutional generative
       adversarial networks   
     * emily denton et al.    [122]deep generative image models using a
       laplacian pyramid of adversarial networks:    paper behind [123]the
       eyescream project.
     * tim salimans et al.    [124]improved techniques for training gans:   
       openai   s first paper. (not discussed here.)

bonus: incomplete thoughts on tensorflow and torch

   as a machine learning researcher, i mostly use numpy, torch, and
   tensorflow in my programs. [125]a few [126]years [127]ago, i used
   fortran. i implemented [128]openface as a python library in numpy that
   calls into networks trained with torch. over the past few months, i   ve
   been using tensorflow more seriously and have a few thoughts comparing
   torch and tensorflow. these are non-exhaustive and from my personal
   experiences as a user.

   if i am misunderstanding something here, please message me and i   ll add
   a correction. due to the fast-paced nature of these frameworks, it   s
   easy to not have references to everything.
     * there are many great collections of tutorials, pre-trained models,
       and technologies for both torch and tensorflow in projects like
       [129]awesome-torch and [130]awesome-tensorflow.
     * i haven   t used [131]torchnet, but it seems promising.
     * torch   s repl is very nice and i always have it open when i   m
       developing in torch to quickly try out operations. tensorflow   s
       interactivesession is nice, but i find that trying things out
       interactively is a little slower since everything has to be defined
       symbolically and initialized in the session. i much prefer trying
       quick numpy operations in python   s repl over tensorflow operations.
     * as with a lot of other programming, error messages in tensorflow
       and torch have their own learning curves. debugging tensorflow
       usually involves reasoning about the symbolic constructions while
       debugging torch is more concrete. [132]sometimes the error messages
       are confusing for me and i send in an issue only to find out that
       my error was obvious.
     * having python support is critical for my research. i love the
       scientific python programming stack and libraries like
       [133]matplotlib and [134]cvxpy that don   t have an equivalent in
       lua. this is why i wrote [135]openface to use torch for training
       the neural network, but python for everything else.
       i can easily convert tensorflow arrays to numpy format and use them
       with other python code, but i have to work hard to do this with
       torch. when i tried using [136]npy4th, i found a bug (that i
       haven   t reported, sorry) that caused incorrect data to be saved.
       [137]torch   s hdf5 bindings seem to work well and can easily be
       loaded in python. and for smaller things, i just manually write out
       logs to a csv file.
       torch has some equivalents to python, like [138]gnuplot wrappers
       for some plotting, but i prefer the python alternatives. there are
       some python torch wrappers like [139]pytorch or [140]lutorpy that
       might make this easier, but i haven   t tried them and my impression
       is that they   re not able to cover every torch feature that can be
       done in lua.
     * in torch, it   s easy to do very detailed operations on tensors that
       can execute on the cpu or gpu. with tensorflow, gpu operations need
       to be implemented symbolically. in torch, it   s easy to drop into
       native c or cuda if i need to add calls to a c or cuda library. for
       example, i   ve created (cpu-only) torch wrappers around the
       [141]gurobi and [142]ecos c optimization libraries.
       in tensorflow, dropping into c or cuda is definitely possible (and
       easy) on the cpu through numpy conversions, but i   m not sure how i
       would make a native cuda call. it   s probably possible, but there
       are no documentation or examples on this.
     * tensorflow (built-in) and torch   s [143]nngraph package graph
       constructions are both nice. in my experiences for complex graphs,
       tensorflow is able to optimize the computations and executes about
       twice as fast as torch. i love nngraph   s visualizations, they   re
       much clearer than tensorboard   s in my experiences.
     * [144]tensorboard is convenient and okay, but i am currently not
       using it for a few reasons. (in this post, i only used it because
       [145]carpedm20/dcgan-tensorflow uses it.) the plots are not
       publication-quality and modifications are very limited. for
       example, it   s (currently) impossible to add a rolling average. the
       tensorboard data is stored in a protobuf format and [146]there   s
       currently no documentation or examples on loading the data in my
       own script. my current solution is to just write out data to csv
       files and load and plot them with another script.
     * i am not surprised to find bugs or missing features in torch/lua
       code. [147]here   s my pr removing an incorrect rank check to the
       lapack potrs call. [148]i also had to add a potrs wrapper for cuda.
       [149]sometimes i find minor bugs when using tensorflow/tflearn, but
       not as frequently and they   re usually minor.
     * automatic differentiation in tensorflow is nice. i can define my
       loss with one line of code and then get the gradients with one more
       line. i haven   t used [150]torch   s autograd package.
     * the torch and tensorflow communities are great at keeping up with
       the latest deep learning techniques. if a popular idea is released,
       torch and tensorflow implementations are quickly released.
     * batch id172 is easier to use in torch and in general it   s
       nice to not worry about explicitly defining all of my trainable
       variables like in tensorflow. [151]tflearn makes this a little
       easier in tensorflow, but i still prefer torch   s way.
     * i am not surprised to learn about useful under-documented features
       in torch and tensorflow by reading through somebody else   s source
       code on github. i described in my [152]last blog post that i found
       a under-documented way to reduce torch model sizes in openface.
     * i prefer how models can be saved and loaded in torch by passing the
       object to a function that serializes it and saves it to a single
       file on disk. in tensorflow, saving and loading the graph is still
       functionally the same, but a little more involved. [153]loading
       torch models on arm is possible, but tricky. i don   t have
       experience loading tensorflow models on arm.
     * when using multiple gpus, setting cutorch.setdevice
       programmatically in torch is slightly easier than exporting the
       cuda_visible_devices environment variable in tensorflow.
     * tensorflow   s long startup time is a slight annoyance if i want to
       quickly debug my code on small examples. in torch, the startup time
       is negligible.
     * in python, i like overriding the process name for long-running
       experiments with [154]setproctitle so that i can remember what   s
       running when i look at the running processes on my gpus or cpus.
       however in torch/lua, [155]nobody on the mailing list was able to
       help me do this. i also asked on a lua irc channel and somebody
       tried to help me, but we weren   t able to figure it out.
     * torch   s [156]argcheck has a [157]serious limitation when functions
       have more than 9 optional arguments. in python, i   d like to start
       using [158]mypy for simple argument checking, but i   m not sure how
       well it works in practice with scientific python.
     * i wrote [159]gurobi and [160]ecos wrappers because there weren   t
       any lp or qp solvers in torch. to my knowledge there still aren   t
       any other options.

references

   visible links
   1. http://bamos.github.io/atom.xml
   2. http://bamos.github.io/
   3. http://bamos.github.io/blog/
   4. https://en.wikipedia.org/wiki/inpainting
   5. https://arxiv.org/abs/1607.07539
   6. https://arxiv.org/abs/1511.06434
   7. https://www.tensorflow.org/
   8. https://github.com/bamos/dcgan-completion.tensorflow
   9. http://bamos.github.io/2016/08/09/deep-completion/#step-1-interpreting-images-as-samples-from-a-id203-distribution
  10. http://bamos.github.io/2016/08/09/deep-completion/#step-2-quickly-generating-fake-images
  11. http://bamos.github.io/2016/08/09/deep-completion/#step-3-using-fake-image-generation-for-image-completion
  12. https://flic.kr/p/7ye6sj
  13. https://flic.kr/p/8fh3vb
  14. http://github.com/bamos/dcgan-completion.tensorflow
  15. https://en.wikipedia.org/wiki/id203_distribution
  16. https://en.wikipedia.org/wiki/normal_distribution
  17. https://en.wikipedia.org/wiki/id203_density_function
  18. https://github.com/bamos/dcgan-completion.tensorflow/blob/master/simple-distributions.py
  19. https://en.wikipedia.org/wiki/rgb_color_model
  20. https://en.wikipedia.org/wiki/statistical_model
  21. https://en.wikipedia.org/wiki/conditional_dependence
  22. https://en.wikipedia.org/wiki/generative_model
  23. http://yann.lecun.com/
  24. https://www.quora.com/what-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning/answer/yann-lecun?srid=nzuy
  25. http://yann.lecun.com/
  26. https://www.quora.com/what-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning/answer/yann-lecun?srid=nzuy
  27. http://soumith.ch/eyescream/
  28. https://en.wikipedia.org/wiki/street_fighter#/media/file:street_fighter_ii_(arcade)_screenshot.png
  29. http://arxiv.org/abs/1312.6114
  30. http://papers.nips.cc/paper/5423-generative-adversarial
  31. https://nips.cc/
  32. http://www.numpy.org/
  33. http://cs231n.github.io/
  34. http://www.deeplearningbook.org/
  35. http://setosa.io/ev/image-kernels/
  36. https://arxiv.org/abs/1603.07285
  37. https://arxiv.org/abs/1511.06434
  38. http://www.iclr.cc/
  39. https://arxiv.org/abs/1603.07285
  40. https://github.com/vdumoulin/conv_arithmetic
  41. http://cs231n.github.io/convolutional-networks/
  42. https://arxiv.org/abs/1603.07285
  43. https://github.com/vdumoulin/conv_arithmetic
  44. https://github.com/vdumoulin/conv_arithmetic
  45. https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf
  46. https://en.wikipedia.org/wiki/deconvolution
  47. http://www.matthewzeiler.com/pubs/iccv2011/iccv2011.pdf
  48. https://arxiv.org/abs/1511.06434
  49. https://arxiv.org/abs/1511.06434
  50. https://arxiv.org/abs/1511.06434
  51. https://arxiv.org/abs/1511.06434
  52. https://arxiv.org/abs/1511.06434
  53. https://en.wikipedia.org/wiki/latent_variable
  54. https://arxiv.org/abs/1607.07539
  55. https://en.wikipedia.org/wiki/minimax
  56. https://en.wikipedia.org/wiki/expected_value
  57. https://en.wikipedia.org/wiki/id26
  58. http://papers.nips.cc/paper/5423-generative-adversarial
  59. http://papers.nips.cc/paper/5423-generative-adversarial
  60. https://github.com/goodfeli/adversarial
  61. https://github.com/tqchen/mxnet-gan
  62. https://github.com/newmu/dcgan_code
  63. https://github.com/soumith/dcgan.torch
  64. https://github.com/carpedm20/dcgan-tensorflow
  65. https://github.com/openai/improved-gan
  66. https://arxiv.org/abs/1606.03498
  67. https://github.com/mattya/chainer-dcgan
  68. https://github.com/jacobgil/keras-dcgan
  69. https://github.com/carpedm20/dcgan-tensorflow
  70. https://github.com/bamos/dcgan-completion.tensorflow
  71. https://github.com/carpedm20/dcgan-tensorflow
  72. https://github.com/bamos/dcgan-completion.tensorflow/blob/master/model.py
  73. https://github.com/bamos/dcgan-completion.tensorflow/blob/master/ops.py
  74. https://en.wikipedia.org/wiki/cross_id178
  75. https://arxiv.org/abs/1412.6980
  76. https://github.com/bamos/dcgan-completion.tensorflow/blob/master/model.py
  77. https://github.com/bamos/dcgan-completion.tensorflow
  78. https://github.com/carpedm20/dcgan-tensorflow
  79. https://github.com/bamos/dcgan-completion.tensorflow
  80. http://cmusatyalab.github.io/openface
  81. http://opencv.org/
  82. http://dlib.net/
  83. https://github.com/cmusatyalab/openface/issues/172
  84. http://cmusatyalab.github.io/openface/setup/
  85. http://cmusatyalab.github.io/openface/setup/
  86. https://www.microsoft.com/en-us/research/project/msr-image-recognition-challenge-irc/
  87. http://mmlab.ie.cuhk.edu.hk/projects/celeba.html
  88. http://www.cbsr.ia.ac.cn/english/casia-webface-database.html
  89. http://vintage.winklerbros.net/facescrub.html
  90. http://vis-www.cs.umass.edu/lfw/
  91. http://megaface.cs.washington.edu/
  92. https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#download-and-setup
  93. https://www.tensorflow.org/versions/r0.10/how_tos/summaries_and_tensorboard/index.html
  94. https://arxiv.org/abs/1607.07539
  95. https://arxiv.org/abs/1607.07539
  96. https://en.wikipedia.org/wiki/hadamard_product_(matrices)
  97. https://en.wikipedia.org/wiki/loss_function
  98. https://en.wikipedia.org/wiki/norm_(mathematics)#taxicab_norm_or_manhattan_norm
  99. https://en.wikipedia.org/wiki/norm_(mathematics)#euclidean_norm
 100. http://dl.acm.org/citation.cfm?id=882269
 101. http://www.ctralie.com/teaching/poissonimageediting/
 102. https://github.com/bamos/dcgan-completion.tensorflow
 103. https://github.com/carpedm20/dcgan-tensorflow
 104. https://en.wikipedia.org/wiki/automatic_differentiation
 105. http://www.stats.ox.ac.uk/~lienart/blog_opti_pgd.html
 106. http://github.com/bamos/dcgan-completion.tensorflow
 107. http://bamos.github.io/2016/08/09/deep-completion/#step-1-interpreting-images-as-samples-from-a-id203-distribution
 108. http://bamos.github.io/2016/08/09/deep-completion/#step-2-quickly-generating-fake-images
 109. http://bamos.github.io/2016/08/09/deep-completion/#step-3-using-fake-image-generation-for-image-completion
 110. https://www.quora.com/do-generative-adversarial-networks-always-converge
 111. https://twitter.com/brandondamos
 112. https://github.com/bamos
 113. http://bamos.github.io/index.html
 114. https://arxiv.org/abs/1606.03498
 115. http://www.bibtex.org/
 116. https://arxiv.org/abs/1607.07539
 117. https://people.eecs.berkeley.edu/~pathak/context_encoder/
 118. https://github.com/pathak22/context-encoder
 119. http://papers.nips.cc/paper/5423-generative-adversarial
 120. https://arxiv.org/abs/1603.07285
 121. https://arxiv.org/abs/1511.06434
 122. http://arxiv.org/abs/1506.05751
 123. http://soumith.ch/eyescream/
 124. https://arxiv.org/abs/1606.03498
 125. https://vtechworks.lib.vt.edu/bitstream/handle/10919/49672/qntoms14.pdf
 126. http://dl.acm.org/citation.cfm?id=2663525
 127. http://dl.acm.org/citation.cfm?id=2685662
 128. https://cmusatyalab.github.io/openface
 129. https://github.com/carpedm20/awesome-torch
 130. https://github.com/jtoy/awesome-tensorflow
 131. https://github.com/torchnet/torchnet
 132. https://github.com/torch/nngraph/issues/107
 133. http://matplotlib.org/
 134. http://www.cvxpy.org/en/latest/
 135. http://cmusatyalab.github.io/openface
 136. https://github.com/htwaijry/npy4th
 137. https://github.com/deepmind/torch-hdf5
 138. https://github.com/torch/gnuplot
 139. https://github.com/hughperkins/pytorch
 140. https://github.com/imodpasteur/lutorpy
 141. https://github.com/bamos/gurobi.torch
 142. https://github.com/bamos/ecos.torch
 143. https://github.com/torch/nngraph
 144. https://www.tensorflow.org/versions/r0.10/how_tos/summaries_and_tensorboard/index.html
 145. https://github.com/carpedm20/dcgan-tensorflow
 146. http://stackoverflow.com/questions/36700404/tensorflow-opening-log-data-written-by-summarywriter
 147. https://github.com/torch/torch7/pull/591
 148. https://github.com/torch/cutorch/pull/364
 149. https://github.com/tflearn/tflearn/pull/221
 150. https://github.com/twitter/torch-autograd
 151. http://tflearn.org/
 152. http://bamos.github.io/2016/01/19/openface-0.2.0/
 153. https://github.com/cmusatyalab/openface/issues/42
 154. https://pypi.python.org/pypi/setproctitle
 155. https://groups.google.com/forum/#!topic/torch7/nxzcyinc-i8
 156. https://github.com/torch/argcheck
 157. https://github.com/torch/argcheck/issues/3
 158. http://www.mypy-lang.org/
 159. https://github.com/bamos/gurobi.torch
 160. https://github.com/bamos/ecos.torch

   hidden links:
 162. http://bamos.github.io/
 163. http://github.com/bamos
 164. http://twitter.com/brandondamos
 165. https://scholar.google.com/citations?user=d8gdzr4aaaaj
 166. http://bamos.github.io/atom.xml
 167. https://twitter.com/intent/tweet/?text=image%20completion%20with%20deep%20learning%20in%20tensorflow%20by%20@brandondamos%20&url=http://bamos.github.io/2016/08/09/deep-completion/
 168. https://facebook.com/sharer/sharer.php?u=http://bamos.github.io/2016/08/09/deep-completion/
 169. https://plus.google.com/share?url=http://bamos.github.io/2016/08/09/deep-completion/
 170. https://www.linkedin.com/sharearticle?mini=true&url=http://bamos.github.io/2016/08/09/deep-completion/&title=image%20completion%20with%20deep%20learning%20in%20tensorflow&summary=image%20completion%20with%20deep%20learning%20in%20tensorflow&source=http://bamos.github.io/2016/08/09/deep-completion/
 171. https://reddit.com/submit/?url=http://bamos.github.io/2016/08/09/deep-completion/
 172. mailto:?subject=image%20completion%20with%20deep%20learning%20in%20tensorflow&body=http://bamos.github.io/2016/08/09/deep-completion/
