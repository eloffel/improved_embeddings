   ram

         memory networks for language understanding, icml tutorial 2016

   speaker: jason weston
   time: 11am-1pm, june 19 @ crown plaza broadway + breakout room

   there has been a recent resurgence in interest in the use of the
   combination of reasoning, attention and memory for solving tasks,
   particularly in the field of language understanding. i will review some
   of these recent efforts, as well as focusing on one of my own group   s
   contributions, memory networks, an architecture that we have applied to
   id53, id38 and general dialog. as we try to
   move towards the goal of true language understanding, i will also
   discuss recent datasets and tests that have been built to assess these
   models abilities to see how far we have come.

slides

   slides are here: [1]ppt or [2]pdf (some animation isn't quite right).

resources

  publications on memory networks:

     j. weston, s. chopra, a. bordes. [3]memory networks. iclr 2015 (and
   arxiv:1410.3916).

     s. sukhbaatar, a. szlam, j. weston, r. fergus. [4]end-to-end memory
   networks. nips 2015 (and arxiv:1503.08895). [5][code]

     j. weston, a. bordes, s. chopra, a. m. rush, b. van merri  nboer, a.
   joulin, t. mikolov. [6]towards ai-complete id53: a set of
   prerequisite toy tasks. arxiv:1502.05698. [7][data & code]

     a. bordes, n. usunier, s. chopra, j. weston. [8]large-scale simple
   id53 with memory networks. arxiv:1506.02075. [9][data]

     j. dodge, a. gane, x. zhang, a. bordes, s. chopra, a. miller, a.
   szlam, j. weston. [10]evaluating prerequisite qualities for learning
   end-to-end id71. arxiv:1511.06931. [11][data]

     f. hill, a. bordes, s. chopra, j. weston. [12]the goldilocks
   principle: reading children's books with explicit memory
   representations. arxiv:1511.02301. [13][data]

     j. weston. [14]dialog-based language learning. arxiv:1604.06045.
   [15][data]

     a. bordes, jason weston. [16]learning end-to-end goal-oriented
   dialog. arxiv:1605.07683. [17][data]

  related publications

   see the related [18]ram workshop from nips 2015 and references therein,
   here are some of them:
   [1] [19]id4 by jointly learning to align and
   translate. d. bahdanau, k. cho, y. bengio; international conference on
   representation learning 2015.
   [2] [20]id63s. alex graves, greg wayne, ivo danihelka.
   arxiv pre-print, 2014
   [3] [21]teaching machines to read and comprehend. karl moritz hermann
   et. al. arxiv pre-print, 2015.
   [4] [22]ask me anything: dynamic memory networks for natural language
   processing. kumar et al. arxiv pre-print, 2015
   [5] [23]show, attend and tell: neural image id134 with
   visual attention. kelvin xu et. al.. arxiv pre-print, 2015.
   [6] [24]attention-based models for id103. jan chorowski,
   dzmitry bahdanau, dmitriy serdyuk, kyunghyun cho, yoshua bengio. arxiv
   pre-print, 2015.
   [7] learning context-free grammars: capabilities and limitations of a
   recurrent neural network with an external stack memory. s. das, c. l.
   giles, and g. z. sun. in accss, 1992.
   [8] neural net architectures for temporal sequence processing. michael
   c mozer. in santa fe institute studies in the sciences of complexity,
   volume 15.
   [9] [25]inferring algorithmic patterns with stack augmented recurrent
   nets. armand joulin and tomas mikolov. arxiv pre-print, 2015.
   [10] [26]id23 turing machine. wojciech zaremba and
   ilya sutskever. arxiv pre-print, 2015.
   [11] [27]generating sequences with recurrent neural networks. alex
   graves. arxiv preprint, 2013.
   [12] [28]long short-term memory. sepp hochreiter, j  rgen schmidhuber.
   neural computation, 9(8): 1735-1780, 1997.
   [13] learning to control fast-weight memories: an alternative to
   dynamic recurrent networks. j  rgen schmidhuber. neural computation,
   4(1):131-139, 1992.
   [14] a self-referential weight matrix. j  rgen schmidhuber. in icann93,
   pp. 446-450. springer, 1993.
   [15][29] learning to combine foveal glimpses with a third-order
   id82. hugo larochelle and geoffrey e. hinton. in nips, pp.
   1243-1251, 2010.
   [16] [30]learning where to attend with deep architectures for image
   tracking. denil et. al. neural computation, 2012.
   [17] [31]recurrent models of visual attention. v. mnih, n. hees, a.
   graves and k. kavukcuoglu. in nips, 2014.
   [18] [32]a neural attention model for abstractive sentence
   summarization. a. m. rush, s. chopra and j. weston. emnlp 2015.

  data

   qa and dialog benchmarks (from my group @ facebook):
     * qa babi tasks: [33]fb.ai/babi
     * simplequestions dataset (100k questions): [34]fb.ai/babi
     * children   s book test dataset: [35]fb.ai/babi
     * dialog babi tasks: [36]fb.ai/babi
     * dialog-based language learning dataset: [37]fb.ai/babi
     * movie dialog datast: [38]fb.ai/babi

   other benchmarks (external to my group):
     * deepmind q&a dataset: [39]http://cs.nyu.edu/~kcho/dmqa/
     * ubuntu dialogue corpus:
       [40]https://github.com/rkadlec/ubuntu-ranking-dataset-creator

  code

     * memory networks (torch/matlab):
       [41]https://github.com/facebook/memnn
     * [42]python-babi: memn2n implemenation on babi tasks with very nice
       interactive demo.
     * [43]theano-babi: memn2n implementation in theano for babi tasks.
     * [44]tf-lang: memn2n language model implementation in tensorflow.
     * [45]tf-babi: another memn2n implementation of memn2n in tensorflow,
       but for babi tasks.
     * simulation tasks generator:
       [46]https://github.com/facebook/babi-tasks

speaker bio

   [jason_chess.jpg] jason weston, facebook ai research
   [47](http://www.jaseweston.com/)
   jason weston is a research scientist at facebook, ny, since feb 2014.
   he earned his phd in machine learning at royal holloway, university of
   london and at at&t research in red bank, nj (advisors: alex gammerman,
   volodya vovk and vladimir vapnik) in 2000. from 2000 to 2002, he was a
   researcher at biowulf technologies, new york. from 2002 to 2003 he was
   a research scientist at the max planck institute for biological
   cybernetics, tuebingen, germany. from 2003 to 2009 he was a research
   staff member at nec labs america, princeton. from 2009 to 2014 he was a
   research scientist at google, ny. his interests lie in statistical
   machine learning and its application to text, audio and images. jason
   has published over 100 papers, including best paper awards at icml and
   ecml. he was part of the youtube team that won a national academy of
   television arts & sciences emmy award for technology and engineering
   for personalized recommendation engines for video discovery. he was
   listed as one of the top 50 authors in computer science in [48]science.

references

   1. http://www.thespermwhale.com/jaseweston/icml2016/icml2016-memnn-tutorial.pptx
   2. http://www.thespermwhale.com/jaseweston/icml2016/icml2016-memnn-tutorial.pdf
   3. http://arxiv.org/abs/1410.3916
   4. http://arxiv.org/abs/1503.08895
   5. https://github.com/facebook/memnn
   6. http://arxiv.org/abs/1502.05698
   7. http://fb.ai/babi
   8. http://arxiv.org/abs/1506.02075
   9. http://fb.ai/babi
  10. http://arxiv.org/abs/1511.06931
  11. http://fb.ai/babi
  12. http://arxiv.org/abs/1511.02301
  13. http://fb.ai/babi
  14. http://arxiv.org/abs/1604.06045
  15. http://fb.ai/babi
  16. http://arxiv.org/abs/1605.07683
  17. http://fb.ai/babi
  18. http://www.thespermwhale.com/jaseweston/ram
  19. http://arxiv.org/pdf/1409.0473.pdf
  20. http://arxiv.org/pdf/1410.5401v2.pdf
  21. http://arxiv.org/abs/1506.03340
  22. http://arxiv.org/abs/1506.07285
  23. http://arxiv.org/pdf/1502.03044
  24. http://arxiv.org/pdf/1506.07503
  25. http://arxiv.org/pdf/1503.01007.pdf
  26. http://arxiv.org/pdf/1505.00521.pdf
  27. http://arxiv.org/abs/1308.0850
  28. http://deeplearning.cs.cmu.edu/pdfs/hochreiter97_lstm.pdf
  29. http://papers.nips.cc/paper/4089-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine
  30. http://arxiv.org/abs/1109.3737
  31. http://arxiv.org/abs/1406.6247
  32. http://arxiv.org/abs/1509.00685
  33. https://fb.ai/babi
  34. https://fb.ai/babi
  35. https://fb.ai/babi
  36. https://fb.ai/babi
  37. https://fb.ai/babi
  38. https://fb.ai/babi
  39. http://cs.nyu.edu/~kcho/dmqa/
  40. https://github.com/rkadlec/ubuntu-ranking-dataset-creator
  41. https://github.com/facebook/memnn
  42. https://github.com/vinhkhuc/memn2n-babi-python
  43. https://github.com/npow/memn2n
  44. https://github.com/carpedm20/memn2n-tensorflow
  45. https://github.com/domluna/memn2n
  46. https://github.com/facebook/babi-tasks
  47. http://www.jaseweston.com/
  48. http://www.sciencemag.org/news/2016/04/who-s-michael-jordan-computer-science-new-tool-ranks-researchers-influence
