   (button) toggle navigation [1]colah's blog
     * [2]blog
     * [3]about
     * [4]contact

neural networks, manifolds, and topology

   posted on april 6, 2014
   topology, neural networks, deep learning, manifold hypothesis

   recently, there   s been a great deal of excitement and interest in deep
   neural networks because they   ve achieved breakthrough results in areas
   such as id161.[5]^1

   however, there remain a number of concerns about them. one is that it
   can be quite challenging to understand what a neural network is really
   doing. if one trains it well, it achieves high quality results, but it
   is challenging to understand how it is doing so. if the network fails,
   it is hard to understand what went wrong.

   while it is challenging to understand the behavior of deep neural
   networks in general, it turns out to be much easier to explore
   low-dimensional deep neural networks     networks that only have a few
   neurons in each layer. in fact, we can create visualizations to
   completely understand the behavior and training of such networks. this
   perspective will allow us to gain deeper intuition about the behavior
   of neural networks and observe a connection linking neural networks to
   an area of mathematics called topology.

   a number of interesting things follow from this, including fundamental
   lower-bounds on the complexity of a neural network capable of
   classifying certain datasets.

a simple example

   let   s begin with a very simple dataset, two curves on a plane. the
   network will learn to classify points as belonging to one or the other.

   the obvious way to visualize the behavior of a neural network     or any
   classification algorithm, for that matter     is to simply look at how it
   classifies every possible data point.

   we   ll start with the simplest possible class of neural network, one
   with only an input layer and an output layer. such a network simply
   tries to separate the two classes of data by dividing them with a line.

   that sort of network isn   t very interesting. modern neural networks
   generally have multiple layers between their input and output, called
      hidden    layers. at the very least, they have one.
   diagram of a simple network from wikipedia

   as before, we can visualize the behavior of this network by looking at
   what it does to different points in its domain. it separates the data
   with a more complicated curve than a line.

   with each layer, the network transforms the data, creating a new
   representation.[6]^2 we can look at the data in each of these
   representations and how the network classifies them. when we get to the
   final representation, the network will just draw a line through the
   data (or, in higher dimensions, a hyperplane).

   in the previous visualization, we looked at the data in its    raw   
   representation. you can think of that as us looking at the input layer.
   now we will look at it after it is transformed by the first layer. you
   can think of this as us looking at the hidden layer.

   each dimension corresponds to the firing of a neuron in the layer.
   the hidden layer learns a representation so that the data is linearly
   separable

continuous visualization of layers

   in the approach outlined in the previous section, we learn to
   understand networks by looking at the representation corresponding to
   each layer. this gives us a discrete list of representations.

   the tricky part is in understanding how we go from one to another.
   thankfully, neural network layers have nice properties that make this
   very easy.

   there are a variety of different kinds of layers used in neural
   networks. we will talk about tanh layers for a concrete example. a tanh
   layer \(\tanh(wx+b)\) consists of:
    1. a linear transformation by the    weight    matrix \(w\)
    2. a translation by the vector \(b\)
    3. point-wise application of tanh.

   we can visualize this as a continuous transformation, as follows:
   gradually applying a neural network layer

   the story is much the same for other standard layers, consisting of an
   affine transformation followed by pointwise application of a monotone
   activation function.

   we can apply this technique to understand more complicated networks.
   for example, the following network classifies two spirals that are
   slightly entangled, using four hidden layers. over time, we can see it
   shift from the    raw    representation to higher level ones it has learned
   in order to classify the data. while the spirals are originally
   entangled, by the end they are linearly separable.

   on the other hand, the following network, also using multiple layers,
   fails to classify two spirals that are more entangled.

   it is worth explicitly noting here that these tasks are only somewhat
   challenging because we are using low-dimensional neural networks. if we
   were using wider networks, all this would be quite easy.

   (andrej karpathy has made a [7]nice demo based on convnetjs that allows
   you to interactively explore networks with this sort of visualization
   of training!)

topology of tanh layers

   each layer stretches and squishes space, but it never cuts, breaks, or
   folds it. intuitively, we can see that it preserves topological
   properties. for example, a set will be connected afterwards if it was
   before (and vice versa).

   transformations like this, which don   t affect topology, are called
   homeomorphisms. formally, they are bijections that are continuous
   functions both ways.

   theorem: layers with \(n\) inputs and \(n\) outputs are homeomorphisms,
   if the weight matrix, \(w\), is non-singular. (though one needs to be
   careful about domain and range.)

   proof: let   s consider this step by step:
    1. let   s assume \(w\) has a non-zero determinant. then it is a
       bijective linear function with a linear inverse. linear functions
       are continuous. so, multiplying by \(w\) is a homeomorphism.
    2. translations are homeomorphisms
    3. tanh (and sigmoid and softplus but not relu) are continuous
       functions with continuous inverses. they are bijections if we are
       careful about the domain and range we consider. applying them
       pointwise is a homeomorphism

   thus, if \(w\) has a non-zero determinant, our layer is a
   homeomorphism.    

   this result continues to hold if we compose arbitrarily many of these
   layers together.

topology and classification

   \(a\) is red, \(b\) is blue

   consider a two dimensional dataset with two classes \(a, b \subset
   \mathbb{r}^2\):

   \[a = \{x | d(x,0) < 1/3\}\]

   \[b = \{x | 2/3 < d(x,0) < 1\}\]

   claim: it is impossible for a neural network to classify this dataset
   without having a layer that has 3 or more hidden units, regardless of
   depth.

   as mentioned previously, classification with a sigmoid unit or a
   softmax layer is equivalent to trying to find a hyperplane (or in this
   case a line) that separates \(a\) and \(b\) in the final represenation.
   with only two hidden units, a network is topologically incapable of
   separating the data in this way, and doomed to failure on this dataset.

   in the following visualization, we observe a hidden representation
   while a network trains, along with the classification line. as we
   watch, it struggles and flounders trying to learn a way to do this.
   for this network, hard work isn   t enough.

   in the end it gets pulled into a rather unproductive local minimum.
   although, it   s actually able to achieve \(\sim 80\%\) classification
   accuracy.

   this example only had one hidden layer, but it would fail regardless.

   proof: either each layer is a homeomorphism, or the layer   s weight
   matrix has determinant 0. if it is a homemorphism, \(a\) is still
   surrounded by \(b\), and a line can   t separate them. but suppose it has
   a determinant of 0: then the dataset gets collapsed on some axis. since
   we   re dealing with something homeomorphic to the original dataset,
   \(a\) is surrounded by \(b\), and collapsing on any axis means we will
   have some points of \(a\) and \(b\) mix and become impossible to
   distinguish between.    

   if we add a third hidden unit, the problem becomes trivial. the neural
   network learns the following representation:

   with this representation, we can separate the datasets with a
   hyperplane.

   to get a better sense of what   s going on, let   s consider an even
   simpler dataset that   s 1-dimensional:

   \[a = [-\frac{1}{3}, \frac{1}{3}]\]

   \[b = [-1, -\frac{2}{3}] \cup [\frac{2}{3}, 1]\]

   without using a layer of two or more hidden units, we can   t classify
   this dataset. but if we use one with two units, we learn to represent
   the data as a nice curve that allows us to separate the classes with a
   line:

   what   s happening? one hidden unit learns to fire when \(x >
   -\frac{1}{2}\) and one learns to fire when \(x > \frac{1}{2}\). when
   the first one fires, but not the second, we know that we are in a.

the manifold hypothesis

   is this relevant to real world data sets, like image data? if you take
   the manifold hypothesis really seriously, i think it bears
   consideration.

   the manifold hypothesis is that natural data forms lower-dimensional
   manifolds in its embedding space. there are both theoretical[8]^3 and
   experimental[9]^4 reasons to believe this to be true. if you believe
   this, then the task of a classification algorithm is fundamentally to
   separate a bunch of tangled manifolds.

   in the previous examples, one class completely surrounded another.
   however, it doesn   t seem very likely that the dog image manifold is
   completely surrounded by the cat image manifold. but there are other,
   more plausible topological situations that could still pose an issue,
   as we will see in the next section.

links and homotopy

   another interesting dataset to consider is two linked tori, \(a\) and
   \(b\).

   much like the previous datasets we considered, this dataset can   t be
   separated without using \(n+1\) dimensions, namely a \(4\)th dimension.

   links are studied in knot theory, an area of topology. sometimes when
   we see a link, it isn   t immediately obvious whether it   s an unlink (a
   bunch of things that are tangled together, but can be separated by
   continuous deformation) or not.
   a relatively simple unlink.

   if a neural network using layers with only 3 units can classify it,
   then it is an unlink. (question: can all unlinks be classified by a
   network with only 3 units, theoretically?)

   from this knot perspective, our continuous visualization of the
   representations produced by a neural network isn   t just a nice
   animation, it   s a procedure for untangling links. in topology, we would
   call it an ambient isotopy between the original link and the separated
   ones.

   formally, an ambient isotopy between manifolds \(a\) and \(b\) is a
   continuous function \(f: [0,1] \times x \to y\) such that each \(f_t\)
   is a homeomorphism from \(x\) to its range, \(f_0\) is the identity
   function, and \(f_1\) maps \(a\) to \(b\). that is, \(f_t\)
   continuously transitions from mapping \(a\) to itself to mapping \(a\)
   to \(b\).

   theorem: there is an ambient isotopy between the input and a network
   layer   s representation if: a) \(w\) isn   t singular, b) we are willing
   to permute the neurons in the hidden layer, and c) there is more than 1
   hidden unit.

   proof: again, we consider each stage of the network individually:
    1. the hardest part is the linear transformation. in order for this to
       be possible, we need \(w\) to have a positive determinant. our
       premise is that it isn   t zero, and we can flip the sign if it is
       negative by switching two of the hidden neurons, and so we can
       guarantee the determinant is positive. the space of positive
       determinant matrices is [10]path-connected, so there exists \(p:
       [0,1] \to gl_n(\mathbb{r})\)[11]^5 such that \(p(0) = id\) and
       \(p(1) = w\). we can continually transition from the identity
       function to the \(w\) transformation with the function \(x \to
       p(t)x\), multiplying \(x\) at each point in time \(t\) by the
       continuously transitioning matrix \(p(t)\).

    2. we can continually transition from the identity function to the
       \(b\) translation with the function \(x \to x + tb\).
    3. we can continually transition from the identity function to the
       pointwise use of    with the function: \(x \to (1-t)x + t  (x)\).    

   i imagine there is probably interest in programs automatically
   discovering such ambient isotopies and automatically proving the
   equivalence of certain links, or that certain links are separable. it
   would be interesting to know if neural networks can beat whatever the
   state of the art is there.

   (apparently determining if knots are trivial is np. this doesn   t bode
   well for neural networks.)

   the sort of links we   ve talked about so far don   t seem likely to turn
   up in real world data, but there are higher dimensional
   generalizations. it seems plausible such things could exist in real
   world data.

   links and knots are \(1\)-dimensional manifolds, but we need 4
   dimensions to be able to untangle all of them. similarly, one can need
   yet higher dimensional space to be able to unknot \(n\)-dimensional
   manifolds. all \(n\)-dimensional manifolds can be untangled in \(2n+2\)
   dimensions.[12]^6

   (i know very little about knot theory and really need to learn more
   about what   s known regarding dimensionality and links. if we know a
   manifold can be embedded in n-dimensional space, instead of the
   dimensionality of the manifold, what limit do we have?)

the easy way out

   the natural thing for a neural net to do, the very easy route, is to
   try and pull the manifolds apart naively and stretch the parts that are
   tangled as thin as possible. while this won   t be anywhere close to a
   genuine solution, it can achieve relatively high classification
   accuracy and be a tempting local minimum.

   it would present itself as very high derivatives on the regions it is
   trying to stretch, and sharp near-discontinuities. we know these things
   happen.[13]^7 contractive penalties, penalizing the derivatives of the
   layers at data points, are the natural way to fight this.[14]^8

   since these sort of local minima are absolutely useless from the
   perspective of trying to solve topological problems, topological
   problems may provide a nice motivation to explore fighting these
   issues.

   on the other hand, if we only care about achieving good classification
   results, it seems like we might not care. if a tiny bit of the data
   manifold is snagged on another manifold, is that a problem for us? it
   seems like we should be able to get arbitrarily good classification
   results despite this issue.

   (my intuition is that trying to cheat the problem like this is a bad
   idea: it   s hard to imagine that it won   t be a dead end. in particular,
   in an optimization problem where local minima are a big problem,
   picking an architecture that can   t genuinely solve the problem seems
   like a recipe for bad performance.)

better layers for manipulating manifolds?

   the more i think about standard neural network layers     that is, with
   an affine transformation followed by a point-wise activation function    
   the more disenchanted i feel. it   s hard to imagine that these are
   really very good for manipulating manifolds.

   perhaps it might make sense to have a very different kind of layer that
   we can use in composition with more traditional ones?

   the thing that feels natural to me is to learn a vector field with the
   direction we want to shift the manifold:

   and then deform space based on it:

   one could learn the vector field at fixed points (just take some fixed
   points from the training set to use as anchors) and interpolate in some
   manner. the vector field above is of the form:

   \[f(x) = \frac{v_0f_0(x) + v_1f_1(x)}{1+f_0(x)+f_1(x)}\]

   where \(v_0\) and \(v_1\) are vectors and \(f_0(x)\) and \(f_1(x)\) are
   n-dimensional gaussians. this is inspired a bit by [15]radial basis
   functions.

k-nearest neighbor layers

   i   ve also begun to think that linear separability may be a huge, and
   possibly unreasonable, amount to demand of a neural network. in some
   ways, it feels like the natural thing to do would be to use
   [16]k-nearest neighbors (id92). however, id92   s success is greatly
   dependent on the representation it classifies data from, so one needs a
   good representation before id92 can work well.

   as a first experiment, i trained some mnist networks (two-layer
   convolutional nets, no dropout) that achieved \(\sim 1\%\) test error.
   i then dropped the final softmax layer and used the id92 algorithm. i
   was able to consistently achieve a reduction in test error of 0.1-0.2%.

   still, this doesn   t quite feel like the right thing. the network is
   still trying to do linear classification, but since we use id92 at test
   time, it   s able to recover a bit from mistakes it made.

   id92 is differentiable with respect to the representation it   s acting
   on, because of the 1/distance weighting. as such, we can train a
   network directly for id92 classification. this can be thought of as a
   kind of    nearest neighbor    layer that acts as an alternative to
   softmax.

   we don   t want to feedforward our entire training set for each
   mini-batch because that would be very computationally expensive. i
   think a nice approach is to classify each element of the mini-batch
   based on the classes of other elements of the mini-batch, giving each
   one a weight of 1/(distance from classification target).[17]^9

   sadly, even with sophisticated architecture, using id92 only gets down
   to 5-4% test error     and using simpler architectures gets worse
   results. however, i   ve put very little effort into playing with
   hyper-parameters.

   still, i really aesthetically like this approach, because it seems like
   what we   re    asking    the network to do is much more reasonable. we want
   points of the same manifold to be closer than points of others, as
   opposed to the manifolds being separable by a hyperplane. this should
   correspond to inflating the space between manifolds for different
   categories and contracting the individual manifolds. it feels like
   simplification.

conclusion

   topological properties of data, such as links, may make it impossible
   to linearly separate classes using low-dimensional networks, regardless
   of depth. even in cases where it is technically possible, such as
   spirals, it can be very challenging to do so.

   to accurately classify data with neural networks, wide layers are
   sometimes necessary. further, traditional neural network layers do not
   seem to be very good at representing important manipulations of
   manifolds; even if we were to cleverly set weights by hand, it would be
   challenging to compactly represent the transformations we want. new
   layers, specifically motivated by the manifold perspective of machine
   learning, may be useful supplements.

   (this is a developing research project. it   s posted as an experiment in
   doing research openly. i would be delighted to have your feedback on
   these ideas: you can comment inline or at the end. for typos, technical
   errors, or clarifications you would like to see added, you are
   encouraged to make a pull request [18]on github.)

acknowledgments

   thank you to yoshua bengio, michael nielsen, dario amodei, eliana
   lorch, jacob steinhardt, and tamsyn waterhouse for their comments and
   encouragement.
     __________________________________________________________________

    1. this seems to have really kicked off with [19]krizhevsky et al.,
       (2012), who put together a lot of different pieces to achieve
       outstanding results. since then there   s been a lot of other
       exciting work.[20]   
    2. these representations, hopefully, make the data    nicer    for the
       network to classify. there has been a lot of work exploring
       representations recently. perhaps the most fascinating has been in
       natural language processing: the representations we learn of words,
       called id27s, have interesting properties. see
       [21]mikolov et al. (2013), [22]turian et al. (2010), and,
       [23]richard socher   s work. to give you a quick flavor, there is a
       [24]very nice visualization associated with the turian paper.[25]   
    3. a lot of the natural transformations you might want to perform on
       an image, like translating or scaling an object in it, or changing
       the lighting, would form continuous curves in image space if you
       performed them continuously.[26]   
    4. [27]carlsson et al. found that local patches of images form a klein
       bottle.[28]   
    5. \(gl_n(\mathbb{r})\) is the set of invertible \(n \times n\)
       matrices on the reals, formally called the [29]general linear group
       of degree \(n\).[30]   
    6. this result is mentioned in [31]wikipedia   s subsection on isotopy
       versions.[32]   
    7. see [33]szegedy et al., where they are able to modify data samples
       and find slight modifications that cause some of the best image
       classification neural networks to misclasify the data. it   s quite
       troubling.[34]   
    8. contractive penalties were introduced in contractive autoencoders.
       see [35]rifai et al. (2011).[36]   
    9. i used a slightly less elegant, but roughly equivalent algorithm
       because it was more practical to implement in theano: feedforward
       two different batches at the same time, and classify them based on
       each other.[37]   

   built by [38]oinkina with [39]hakyll using [40]bootstrap, [41]mathjax,
   [42]disqus, [43]mathbox.js, [44]highlight.js, and [45]footnotes.js.

   enable javascript for footnotes, disqus comments, and other cool stuff.

references

   1. http://colah.github.io/
   2. http://colah.github.io/
   3. http://colah.github.io/about.html
   4. http://colah.github.io/contact.html
   5. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fn1
   6. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fn2
   7. http://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html
   8. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fn3
   9. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fn4
  10. http://en.wikipedia.org/wiki/connected_space#path_connectedness
  11. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fn5
  12. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fn6
  13. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fn7
  14. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fn8
  15. http://en.wikipedia.org/wiki/radial_basis_function
  16. http://colah.github.io/posts/2014-03-nn-manifolds-topology/knn
  17. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fn9
  18. https://github.com/colah/nn-topology-post
  19. http://www.cs.toronto.edu/~fritz/absps/id163.pdf
  20. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fnref1
  21. http://research.microsoft.com/pubs/189726/rvecs.pdf
  22. http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf
  23. http://www.socher.org/
  24. http://metaoptimize.s3.amazonaws.com/cw-embeddings-acl2010/embeddings-mostcommon.embedding_size=50.png
  25. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fnref2
  26. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fnref3
  27. http://comptop.stanford.edu/u/preprints/mumford.pdf
  28. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fnref4
  29. http://en.wikipedia.org/wiki/general_linear_group
  30. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fnref5
  31. http://en.wikipedia.org/wiki/whitney_embedding_theorem#isotopy_versions
  32. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fnref6
  33. http://cs.nyu.edu/~zaremba/docs/understanding.pdf
  34. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fnref7
  35. http://www.iro.umontreal.ca/~lisa/pointeurs/icml2011_explicit_invariance.pdf
  36. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fnref8
  37. http://colah.github.io/posts/2014-03-nn-manifolds-topology/#fnref9
  38. https://github.com/oinkina
  39. http://jaspervdj.be/hakyll
  40. http://getbootstrap.com/
  41. http://www.mathjax.org/
  42. http://disqus.com/
  43. https://github.com/unconed/mathbox.js
  44. http://highlightjs.org/
  45. http://ignorethecode.net/blog/2010/04/20/footnotes/
