   (button) toggle navigation [1]colah's blog
     * [2]blog
     * [3]about
     * [4]contact

neural networks, types, and functional programming

   posted on september 3, 2015

an ad-hoc field

   deep learning, despite its remarkable successes, is a young field.
   while models called id158s have been studied for
   decades, much of that work seems only tenuously connected to modern
   results.

   it   s often the case that young fields start in a very ad-hoc manner.
   later, the mature field is understood very differently than it was
   understood by its early practitioners. for example, in taxonomy, people
   have grouped plants and animals for thousands of years, but the way we
   understood what we were doing changed a lot in light of evolution and
   molecular biology. in chemistry, we have explored chemical reactions
   for a long time, but what we understood ourselves to do changed a lot
   with the discovery of irreducible elements, and again later with models
   of the atom. those are grandiose examples, but the history of science
   and mathematics has seen this pattern again and again, on many
   different scales.

   it seems quite likely that deep learning is in this ad-hoc state.

   at the moment, deep learning is held together by an extremely
   successful tool. this tool doesn   t seem fundamental; it   s something
   we   ve stumbled on, with seemingly arbitrary details that change
   regularly. as a field, we don   t yet have some unifying insight or
   shared understanding. in fact, the field has several competing
   narratives!

   i think it is very likely that, reflecting back in 30 years, we will
   see deep learning very differently.

deep learning 30 years in the future

   if we think we   ll probably see deep learning very differently in 30
   years, that suggests an interesting question: how are we going to see
   it? of course, no one can actually know how we   ll come to understand
   the field. but it is interesting to speculate.

   at present, three narratives are competing to be the way we understand
   deep learning. there   s the neuroscience narrative, drawing analogies to
   biology. there   s the representations narrative, centered on
   transformations of data and the manifold hypothesis. finally, there   s a
   probabilistic narrative, which interprets neural networks as finding
   latent variables. these narratives aren   t mutually exclusive, but they
   do present very different ways of thinking about deep learning.

   this essay extends the representations narrative to a new answer: deep
   learning studies a connection between optimization and functional
   programming.

   in this view, the representations narrative in deep learning
   corresponds to type theory in functional programming. it sees deep
   learning as the junction of two fields we already know to be incredibly
   rich. what we find, seems so beautiful to me, feels so natural, that
   the mathematician in me could believe it to be something fundamental
   about reality.

   this is an extremely speculative idea. i am not arguing that it is
   true. i wish to argue only that it is plausible, that one could imagine
   deep learning evolving in this direction. to be clear: i am primarily
   making an aesthetic argument, rather than an argument of fact. i wish
   to show that this is a natural and elegant idea, encompassing what we
   presently call deep learning.

optimization & function composition

   the distinctive property of deep learning is that it studies deep
   neural networks     neural networks with many layers. over the course of
   multiple layers, these models [5]progressively bend data, warping it
   into a form where it is easy to solve the given task.

   the details of these layers change every so often.[6]^1 what has
   remained constant is that there is a sequence of layers.

   each layer is a function, acting on the output of a previous layer. as
   a whole, the network is a chain of composed functions. this chain of
   composed functions is optimized to perform a task.

   every model in deep learning that i am aware of involves optimizing
   composed functions. i believe this is the heart of what we are
   studying.

representations are types

   with every layer, neural networks transform data, molding it into a
   form that makes their task easier to do. we call these transformed
   versions of data    representations.   

   representations correspond to types.

   at their crudest, types in computer science are a way of embedding some
   kind of data in \(n\) bits. similarly, representations in deep learning
   are a way to embed a data manifold in \(n\) dimensions.

   just as two functions can only be composed together if their types
   agree, two layers can only be composed together when their
   representations agree. data in the wrong representation is nonsensical
   to a neural network. over the course of training, adjacent layers
   negotiate the representation they will communicate in; the performance
   of the network depends on data being in the representation the network
   expects.
   a layer \(f_1\) followed by a layer \(f_2\). the output representation
   of \(f_1\) is the input of \(f_2\).

   in the case of very simple neural network architectures, where there   s
   just a linear sequence of layers, this isn   t very interesting. the
   representation of one layer   s output needs to match the representation
   of the next layer   s input     so what? it   s a trivial and boring
   requirement.

   but many neural networks have more complicated architectures where this
   becomes a more interesting constraint. for a very simple example, let   s
   imagine a neural network with multiple similar kinds of inputs, which
   performs multiple, related tasks. perhaps it takes in rgb images and
   also grayscale images. maybe it   s looking at pictures of people, and
   trying to predict age and gender. because the similarities between the
   kinds of inputs and between the kinds of tasks, it can be helpful to do
   all of this in one model, so that training data helps them all. the
   result is multiple input layers mapping into one representation, and
   multiple outputs mapping from the same representation.

   perhaps this example seems a bit contrived, but mapping different kinds
   of data into the same representation can lead to [7]some pretty
   remarkable things. for example, by mapping words from two languages
   into one representation, we can find corresponding words that we didn   t
   know were translations when we started. and by mapping images and words
   into the same representation, we can classify images of classes we   ve
   never seen!

   representations and types can be seen as the basic building blocks for
   deep learning and functional programming respectively. one of the major
   narratives of deep learning, the manifolds and representations
   narrative, is entirely centered on neural networks bending data into
   new representations. the known connection between geometry, logic,
   topology, and functional programming suggests that the connections
   between representations and types may be of fundamental significance.

deep learning & functional programming

   one of the key insights behind modern neural networks is the idea that
   many copies of one neuron can be used in a neural network.

   in programming, the abstraction of functions is essential. instead of
   writing the same code dozens, hundreds, or even thousands of times, we
   can write it once and use it as we need it. not only does this
   massively reduce the amount of code we need to write and maintain,
   speeding the development process, but it also reduces the risk of us
   introducing bugs, and makes the mistakes we do make easier to catch.

   using multiple copies of a neuron in different places is the neural
   network equivalent of using functions. because there is less to learn,
   the model learns more quickly and learns a better model. this technique
       the technical name for it is    weight tying        is essential to the
   phenomenal results we   ve recently seen from deep learning.

   of course, one can   t just arbitrarily put copies of neurons all over
   the place. for the model to work, you need to do it in a principled
   way, exploiting some structure in your data. in practice, there are a
   handful of patterns that are widely used, such as recurrent layers and
   convolutional layers.

   these neural network patterns are just higher order functions     that
   is, functions which take functions as arguments. things like that have
   been studied extensively in functional programming. in fact, many of
   these network patterns correspond to extremely common functions, like
   fold. the only unusual thing is that, instead of receiving normal
   functions as arguments, they receive chunks of neural network.[8]^2
     * encoding recurrent neural networks are just folds. they   re often
       used to allow a neural network to take a variable length list as
       input, for example taking a sentence as input.
       fold = encoding id56
       haskell: foldl a s
     * generating recurrent neural networks are just unfolds. they   re
       often used to allow a neural network to produce a list of outputs,
       such as words in a sentence.
       unfold = generating id56
       haskell: unfoldr a s
     * general recurrent neural networks are accumulating maps. they   re
       often used when we   re trying to make predictions in a sequence. for
       example, in voice recognition, we might wish to predict a phenome
       for every time step in an audio segment, based on past context.
       accumulating map = id56
       haskell: mapaccumr a s
     * bidirectional id56s are a more obscure variant,
       which i mention primarily for flavor. in functional programming
       terms, they are a left and a right accumulating map zipped
       together. they   re used to make predictions over a sequence with
       both past and future context.
       zipped left & right accumulating map = bidirectional id56
       haskell: zip (mapaccumr a s xs) (mapaccuml a` s` xs)
     * convolutional neural networks are a close relative of map. a normal
       map applies a function to every element. convolutional neural
       networks also look at neighboring elements, applying a function to
       a small window around every element.[9]^3
       windowed map = convolutional layer
       haskell: zipwith a xs (tail xs)
       two dimensional convolutional neural networks are particularly
       notable. they have been behind recent successes in id161.
       ([10]more on conv nets.)
       two dimensional convolutional network
     * id56s (   treenets   ) are catamorphisms, a
       generalization of folds. they consume a data structure from the
       bottom up. they   re mostly used for natural language processing, to
       allow neural networks to operate on parse trees.
       catamorphism = treenet
       haskell: cata a
       the above examples demonstrate how common patterns in neural
       networks correspond to very natural, simple functional programs.
       a new kind of programming
       these patterns are building blocks which can be combined together
       into larger networks. like the building blocks, these combinations
       are functional programs, with chunks of neural network throughout.
       the functional program provides high level structure, while the
       chunks are flexible pieces that learn to perform the actual task
       within the framework provided by the functional program.
       these combinations of building blocks can do really, really
       remarkable things. i   d like to look at a few examples.

     * [11]sutskever, et al. (2014) perform state of the art english to
       french translation by combining an encoding id56 and a generating
       id56. in functional programming terms, they essentially fold over
       the (backwards) english sentence and then unfold to produce the
       french translation.
     * [12]vinyals, et al. (2014) generate image captions with a
       convolutional network and a generating id56. essentially, they
       consume the image with a convolutional network, and then unfold the
       resulting vector into a sentence describing it.
       ([13]vinyals, et al. (2014))

   these sorts of models can be seen as a kind of differentiable
   functional programming.

   but it   s not just an abstract thing! they   re imbued with the flavor of
   functional programming, even if people don   t use that language. when i
   hear colleagues talk at a high level about their models, it has a very
   different feeling to it than people talking about more classical
   models. people talk about things in lots of different ways, of course    
   there   s lots of variance in how people see deep learning     but there   s
   often an undercurrent that feels very similar to functional programming
   conversations.

   it feels like a new kind of programming altogether, a kind of
   differentiable functional programming. one writes a very rough
   functional program, with these flexible, learnable pieces, and defines
   the correct behavior of the program with lots of data. then you apply
   id119, or some other optimization algorithm. the result is a
   program capable of doing remarkable things that we have no idea how to
   create directly, like generating captions describing images.

   it   s the natural intersection of functional programming and
   optimization, and i think it   s beautiful.

conclusion

   i find this idea really beautiful. at the same time, this is a pretty
   strange article and i feel a bit weird posting it. i   m very strongly
   presenting a speculative idea with little support behind it besides my
   own enthusiasm. honestly, adopting the most objective perspective i
   can, i expect this idea is wrong, because most untested ideas are
   wrong. but it could be right, and i think it   s worth talking about.

   beyond that, i   m not really the right person to explore a lot of the
   directions this suggests     for example, one of the obvious things to do
   is to analyze neural networks from a homotopy type theory perspective,
   but i don   t have the relevant background. this is an idea that   s
   begging for broader discussion. it really seems like publishing this is
   the right thing to do.

   finally, i   m hoping this essay might stir up more discussion and
   thoughts about what deep learning is really about. i think there   s an
   important discussion waiting to be had.

   besides, what   s the point of writing a blog if i can   t speculate?
   hopefully, i   ve been able to appropriately balance my excitement with
   my uncertainty.

acknowledgments

   firstly, i   m incredibly grateful to [14]greg corrado and [15]aaron
   courville. they are the deep learning researchers i know who most
   strongly empathize with these ideas, and i   m really grateful for their
   intellectual support.

   several other people have had really extended and helpful conversations
   with me. sebastian zany spent several hours talking about type theory
   and neural networks with me. [16]michael nielsen gave thorough feedback
   on a draft of this essay. [17]chas leichner thought really deeply about
   these ideas, and was very encouraging. a big thank you to the three of
   them!

   i   m also thankful for the comments of [18]james koppel, [19]luke
   vilnis, [20]sam bowman, [21]greg brockman and [22]rob gilson. finally,
   i   ve spoken with a number of other people about these ideas in the last
   few months     thanks to all of them!

appendix: functional names of common layers

   deep learning name           functional name
   learned vector     constant
   embedding layer    list indexing
   encoding id56       fold
   generating id56     unfold
   general id56        accumulating map
   bidirectional id56  zipped left/right accumulating maps
   conv layer            window map   
   treenet            catamorphism
   inverse treenet    anamorphism
     __________________________________________________________________

    1. for example, the once ubiquitous sigmoid layer has been
       substantially replaced by relu layers.[23]   
    2. i think it   s actually kind of surprising that these sort of models
       are possible at all, and it   s because of a surprising fact. many
       higher order functions, given differentiable functions as
       arguments, produce a function which is differentiable almost
       everywhere. further, given the derivatives of argument functions,
       you can simply use chain rule to calculate the derivative of the
       resulting function.[24]   
    3. this operation is also closely related to stencil/convolution
       functions, which are their linear version. they   re typically
       implemented using those. however, in modern neural net research,
       where    mlp convolution layers    are becoming more popular, it seems
       preferable to think of this as an arbitrary function. [25]   
     __________________________________________________________________

more posts

   [mikolov-gendervecs.png]

deep learning, nlp, and representations

   [lstm.png]

understanding id137

   [topology.png]

neural networks, manifolds, and topology

   [datalist.png]

data.list recursion illustrated

   built by [26]oinkina with [27]hakyll using [28]bootstrap, [29]mathjax,
   [30]disqus, [31]mathbox.js, [32]highlight.js, and [33]footnotes.js.

   enable javascript for footnotes, disqus comments, and other cool stuff.

references

   visible links
   1. http://colah.github.io/
   2. http://colah.github.io/archive.html
   3. http://colah.github.io/about.html
   4. http://colah.github.io/contact.html
   5. http://colah.github.io/posts/2015-01-visualizing-representations/#neural-networks-transform-space
   6. http://colah.github.io/posts/2015-09-nn-types-fp/#fn1
   7. http://colah.github.io/posts/2014-07-nlp-id56s-representations/
   8. http://colah.github.io/posts/2015-09-nn-types-fp/#fn2
   9. http://colah.github.io/posts/2015-09-nn-types-fp/#fn3
  10. http://colah.github.io/posts/2014-07-conv-nets-modular/
  11. http://arxiv.org/pdf/1409.3215.pdf
  12. http://arxiv.org/pdf/1411.4555.pdf
  13. http://arxiv.org/pdf/1411.4555.pdf
  14. http://research.google.com/pubs/gregcorrado.html
  15. https://aaroncourville.wordpress.com/
  16. http://michaelnielsen.org/
  17. https://twitter.com/flower_snark
  18. http://www.jameskoppel.com/
  19. http://people.cs.umass.edu/~luke/
  20. http://stanford.edu/~sbowman/
  21. https://gregbrockman.com/
  22. https://github.com/d1plo1d
  23. http://colah.github.io/posts/2015-09-nn-types-fp/#fnref1
  24. http://colah.github.io/posts/2015-09-nn-types-fp/#fnref2
  25. http://colah.github.io/posts/2015-09-nn-types-fp/#fnref3
  26. https://github.com/oinkina
  27. http://jaspervdj.be/hakyll
  28. http://getbootstrap.com/
  29. http://www.mathjax.org/
  30. http://disqus.com/
  31. https://github.com/unconed/mathbox.js
  32. http://highlightjs.org/
  33. http://ignorethecode.net/blog/2010/04/20/footnotes/

   hidden links:
  35. http://colah.github.io/posts/2014-07-nlp-id56s-representations/
  36. http://colah.github.io/posts/2015-08-understanding-lstms/
  37. http://colah.github.io/posts/2014-03-nn-manifolds-topology/
  38. http://colah.github.io/posts/2015-02-datalist-illustrated/
