   (button) toggle navigation [1]colah's blog
     * [2]blog
     * [3]about
     * [4]contact

visualizing representations: deep learning and human beings

   posted on january 16, 2015

   [5]data visualization, [6]machine learning, [7]id27s,
   [8]neural networks, [9]deep learning, [10]user interface, [11]wikipedia

   in a [12]previous post, we explored techniques for visualizing
   high-dimensional data. trying to visualize high dimensional data is, by
   itself, very interesting, but my real goal is something else. i think
   these techniques form a set of basic building blocks to try and
   understand machine learning, and specifically to understand the
   internal operations of deep neural networks.

   deep neural networks are an approach to machine learning that has
   revolutionized id161 and id103 in the last few
   years, blowing the previous state of the art results out of the water.
   they   ve also brought promising results to many other areas, including
   language understanding and machine translation. despite this, it
   remains challenging to understand what, exactly, these networks are
   doing.

   i think that id84, thoughtfully applied, can give
   us a lot of traction on understanding neural networks.

   understanding neural networks is just scratching the surface, however,
   because understanding the network is fundamentally tied to
   understanding the data it operates on. the combination of neural
   networks and id84 turns out to be a very
   interesting tool for visualizing high-dimensional data     a much more
   powerful tool than id84 on its own.

   as we dig into this, we   ll observe what i believe to be an important
   connection between neural networks, visualization, and user interface.

neural networks transform space

   not all neural networks are hard to understand. in fact,
   low-dimensional neural networks     networks which have only two or three
   neurons in each layer     are quite easy to understand.

   consider the following dataset, consisting of two curves on the plane.
   given a point on one of the curves, our network should predict which
   curve it came from.

   a network with just an input layer and an output layer tries to divide
   the two classes with a straight line.

   in the case of this dataset, it is not possible to classify it
   perfectly by dividing it with a straight line. and so, a network with
   only an input layer and an output layer can not classify it perfectly.

   but, in practice, neural networks have additional layers in the middle,
   called    hidden    layers. these layers warp and reshape the data to make
   it easier to classify.

   we call the versions of the data corresponding to different layers
   representations.[13]^1 the input layer   s representation is the raw
   data. the middle    hidden    layer   s representation is a warped, easier to
   classify, version of the raw data.

   low-dimensional neural networks are really easy to reason about because
   we can just look at their representations, and at how one
   representation transforms into another. if we have a question about
   what it is doing, we can just look. (there   s quite a bit we can learn
   from low-dimensional neural networks, as explored in my post [14]neural
   networks, manifolds, and topology.)

   unfortunately, neural networks are usually not low-dimensional. the
   strength of neural networks is classifying high-dimensional data, like
   id161 data, which often has tens or hundreds of thousands of
   dimensions. the hidden representations we learn are also of very high
   dimensionality.

   for example, suppose we are trying to classify mnist. the input
   representation, mnist, is a collection of 784-dimensional vectors! and,
   even for a very simple network, we   ll have a high-dimensional hidden
   representation. to be concrete, let   s use one hidden layer with a
   hundred sigmoid neurons.

   while we can   t visualize the high-dimensional representations directly,
   we can visualize them using id84. below, we look at
   nearest neighbor graphs of mnist in its raw form and in a hidden
   representation from a trained mnist network.

   at the input layer, the classes are quite tangled. but, by the next
   layer, because the model has been trained to distinguish the digit
   classes, the hidden layer has learned to transform the data into a new
   representation in which the digit classes are much more separated.

   this approach, visualizing high-dimensional representations using
   id84, is an extremely broadly applicable technique
   for inspecting models in deep learning.

   in addition to helping us understand what a neural network is doing,
   inspecting representations allows us to understand the data itself.
   even with sophisticated id84 techniques, lots of
   real world data is incomprehensible     its structure is too complicated
   and chaotic. but higher level representations tend to be simpler and
   calmer, and much easier for humans to understand.

   (to be clear, using id84 on representations isn   t
   novel. in fact, they   ve become fairly common. one really beautiful
   example is andrej karpathy   s [15]visualizations of a high-level
   id163 representation. my contribution here isn   t the basic idea, but
   taking it really seriously and seeing where it goes.)

example 1: id27s

   id27s are a remarkable kind of representation. they form when
   we try to solve language tasks with neural networks.

   for these tasks, the input to the network is typically a word, or
   multiple words. each word can be thought of as a unit vector in a
   ridiculously high-dimensional space, with each dimension corresponding
   to a word in the vocabulary. the network warps and compresses this
   space, mapping words into a couple hundred dimensions. this is called a
   id27.

   in a id27, every word is a couple hundred dimensional vector.
   these vectors have some really nice properties. the property we will
   visualize here is that words with similar meanings are close together.

   (these embeddings have lots of other interesting properties, besides
   proximity. for example, directions in the embedding space seems to have
   semantic meaning. further, difference vectors between words seem to
   encode analogies. for example, the difference between woman and man is
   approximately the same as the difference between queen and king:
   \(v(``\text{woman}\!") - v(``\text{man}\!") ~\simeq\)
   \(v(``\text{queen}\!") - v(``\text{king}\!")\). for more on word
   embeddings, see my post [16]deep learning, nlp, and representations.)

   to visualize the id27 in two dimensions, we need to choose a
   id84 technique to use. id167 optimizes for keeping
   points close to their neighbors, so it is the natural tool if we want
   to visualize which words are close together in our id27.

   examining the id167 plot, we see that neighboring words tend to be
   related. but there   s so many words! to get a higher-level view, lets
   highlight a few kinds of words.[17]^2 we can see areas corresponding to
   cities, food, body parts, feelings, relatives and different    travel   
   verbs.
   [words-pic.png]

   that   s just scratching the surface. in the following interactive
   visualization, you can choose lots of different categories to color the
   words by. you can also inspect points individually by hovering over
   them, revealing the corresponding word.
   color words by [18]id138 synset (eg. region.n.03):
   a id27 visualized with id167
   (hover over a point to see the word.)
   ([19]see this with 50,000 points!)

   looking at the above visualization, we can see lots of clusters, from
   broad clusters like regions (region.n.03) and people (person.n.01), to
   smaller ones like body parts (body_part.n.01), units of distance
   (linear_unit.n.01) and food (food.n.01). the network successfully
   learned to put similar words close together.

example 2: paragraph vectors of wikipedia

   paragraph vectors, introduced by [20]le & mikolov (2014), are vectors
   that represent chunks of text. paragraph vectors come in a few
   variations but the simplest one, which we are using here, is basically
   some really nice features on top of a [21]bag of words representation.

   with id27s, we learn vectors in order to solve a language
   task involving the word. with paragraph vectors, we learn vectors in
   order to predict which words are in a paragraph.

   concretely, the neural network learns a low-dimensional approximation
   of word statistics for different paragraphs. in the hidden
   representation of this neural network, we get vectors representing each
   paragraph. these vectors have nice properties, in particular that
   similar paragraphs are close together.

   now, google has some pretty awesome people. andrew dai, quoc le, and
   greg corrado decided to create paragraph vectors for some very
   interesting data sets. one of those was wikipedia, creating a vector
   for every english wikipedia article. i was lucky enough to be there at
   the time, and make some neat visualizations. (see [22]dai, et al.
   (2014))

   since there are a very large number of wikipedia articles, we visualize
   a random subset. again, we use id167, because we want to understand
   what is close together.

   the result is that we get a visualization of the entirety of wikipedia.
   a map of wikipedia. a large fraction of wikipedia   s articles fall into
   a few broad topics: sports, music (songs and albums), films, species,
   and science. i wouldn   t have guessed that! why, for example, is sports
   so massive? well, it seems like many individual athletes, teams,
   stadiums, seasons, tournaments and games end up with their own articles
       that adds up to a lot of articles! similar reasons lead to the large
   music, films and species clusters.
   [wiki-pics-both.png]

   this map of wikipedia presents important structure on multiple scales.
   while, there is a large cluster for sports, there are sub-clusters for
   individual sports like tennis. films have a separate cluster for
   non-western films, like bollywood. even very fine grained topics, like
   human proteins, are separated out!

   again, this is only scratching the surface. in the following
   interactive visualization, you can explore for your self. you can color
   points by their wikipedia categories, or inspect individual points by
   hovering to see the article title. clicking on a point will open the
   article.
   color articles by wikipedia category (eg. films):
   wikipedia paragraph vectors visualized with id167
   (hover over a point to see the title. click to open article.)
   ([23]see this with 50,000 points!)

   (note: wikipedia categories can be quite unintuitive and much broader
   than you expect. for example, every human is included in the category
   applied ethics because humans are in people which is in personhood
   which is in issues in ethics which is in applied ethics.)

example 3: translation model

   the previous two examples have been, while fun, kind of strange. they
   were both produced by networks doing simple contrived tasks that we
   don   t actually care about, with the goal of creating nice
   representations. the representations they produce are really cool and
   useful    but they don   t do too much to validate our approach to
   understanding neural networks.

   let   s look at a cutting edge network doing a real task: translating
   english to french.

   [24]sutskever et al. (2014) translate english sentences into french
   sentences using two recurrent neural networks. the first consumes the
   english sentence, word by word, to produce a representation of it, and
   the second takes the representation of the english sentence and
   sequentially outputs translated words. the two are jointly trained, and
   use a multilayered [25]long short term memory architecture.[26]^3
   [translation2-reparrow.png]

   we can look at the representation right after the english    end of
   sentence    (eos) symbol to get a representation of the english sentence.
   this representation is actually quite a remarkable thing. somehow, from
   an english sentence, we   ve formed a vector that encodes the information
   we need to create a french version of that sentence.

   let   s give this representation a closer look with id167.
   color sentences by first word (eg. the):
   translation representation of sentences visualized with id167
   (hover over a point to see the sentence.)

   this visualization revealed something that was fairly surprising to us:
   the representation is dominated by the first word.

   if you look carefully, there   s a bit more structure than just that. in
   some places, we can see subclusters corresponding to the second word
   (for example, in the quotes cluster, we see subclusters for    i    and
      we   ). in other places we can see sentences with similar first words
   mix together (eg.    this    and    that   ). but by and large, the sentence
   representation is controlled by the first word.

   there are a few reasons this might be the case. the first is that, at
   the point we grab this representation, the network is giving the first
   translated word, and so the representation may strongly emphasize the
   information it needs at that instant. it   s also possible that the first
   word is much harder than the other words to translate because, for the
   other words, it is allowed to know what the previous word in the
   translation was and can kind of markov chain along.

   still, while there are reasons for this to be the case, it was pretty
   surprising. i think there must be lots of cases like this, where a
   quick visualization would reveal surprising insights into the models we
   work with. but, because visualization is inconvenient, we don   t end up
   seeing them.

aside: patterns for visualizing high-dimensional data

   there are a lot of established best practices for visualizing low
   dimensional data. many of these are even taught in school.    label your
   axes.       put units on the axes.    and so on. these are excellent
   practices for visualizing and communicating low-dimensional data.

   unfortunately, they aren   t as helpful when we visualize
   high-dimensional data. label the axes of a id167 plot? the axes don   t
   really have any meaning, nor are the units very meaningful. the only
   really meaningful thing, in a id167 plot, is which points are close
   together.

   there are also some unusual challenges when doing id167 plots. consider
   the following id167 visualization of id27s. look at the
   cluster of male names on the left hand side   
   a id27 visualized with id167
   (this visualization is deliberately terrible.)

       but you can   t look at the cluster of male names on the left hand
   side. (it   s frustrating not to be able to hover, isn   t it?) while the
   points are in the exact same positions as in our earlier visualization,
   without the ability to look at which words correspond to points, this
   plot is essentially useless. at best, we can look at it and say that
   the data probably isn   t random.

   the problem is that in dimensionality reduced plots of high-dimensional
   data, position doesn   t explain the data points. this is true even if
   you understand precisely what the plot you are looking at is.

   well, we can fix that. let   s add back in the tooltip. now, by hovering
   over points you can see what word the correspond to. why don   t you look
   at the body part cluster?
   a id27 visualized with id167
   (this visualization is deliberately terrible, but less than the
   previous one.)

   you are forgiven if you didn   t have the patience to look at several
   hundred data points in order to find the body part cluster. and, unless
   you remembered where it was from before, that   s the effort one would
   expect it to take you.

   the ability to inspect points is not sufficient. when dealing with
   thousands of points, one needs a way to quickly get a high-level view
   of the data, and then drill in on the parts that are interesting.

   this brings us to my personal theory of visualizing high dimensional
   data (based on my whole three months of working on visualizing it):
    1. there must be a way to interrogate individual data points.
    2. there must be a way to get a high-level view of the data.

   interactive visualizations are a really easy way to get both of these
   properties. but they aren   t the only way. there   s a really beautiful
   visualization of mnist in the original id167 paper, [27]maaten & hinton
   (2008), on the page labeled 2596:
   [mnist-tsne-digitsinimage.png]
   mnist visualized with id167
   (partial image from [28]maaten & hinton (2008))

   by directly embedding every mnist digit   s image in the visualization,
   maaten and hinton made it very easy to inspect individual points.
   further, from the    texture    of clusters, one can also quickly recognize
   their nature.

   unfortunately, that approach only works because mnist images are small
   and simple. in their exciting paper on phrase representations, [29]cho
   et al. (2014) include some very small subsections of a id167
   visualization of phrases:
   [cho-timephrase-tsne.png ]
   phrases visualized with id167
   (from [30]cho et al. (2014))

   unfortunately, embedding the phrases directly in the visualization just
   doesn   t work. they   re too large and clunky. actually, i just don   t see
   any good way to visualize this data without using interactive media.

geometric fingerprints

   now that we   ve looked at a bunch of exciting representations, let   s
   return to our simple mnist networks and examine the representations
   they form. we   ll use pca for id84 now, since it
   will allow us to observe some interesting geometric properties of these
   representations, and because it is less stochastic than the other
   id84 algorithms we   ve discussed.

   the following network has a 5 unit sigmoid layer. such a network would
   never be used in practice, but is a bit fun to look at.

   then network   s hidden representation looks like a projection of a
   high-dimensional cube. why? well, sigmoid units tend to give values
   close to 0 or 1, and less frequently anything in the middle. if you do
   that in a bunch of dimensions, you end up with concentration at the
   corners of a high-dimensional cube and, to a lesser extent, along its
   edges. pca then projects this down into two dimensions.

   this cube-like structure is a kind of geometric fingerprint of sigmoid
   layers. do other id180 have a similar geometric
   fingerprint? let   s look at a relu layer.

   because relu   s have a high id203 of being zero, lots of points
   concentrate on the origin, and along axes. projected into two
   dimensions, it looks like a bunch of    spouts    shooting out from the
   origin.

   these geometric properties are much more visible when there are only a
   few neurons.

the space of representations

   every time we train a neural net, we get new representations. this is
   true even if we train the same network multiple times. the result is
   that it is very easy to end up with lots of representations of a
   dataset.

   we rarely look at any of these representations, but if we want to, it   s
   pretty easy to make visualizations of all of them. here   s a bunch to
   look at.
   the many representations of mnist

   now, while we can visualize a lot of representations like this, it
   isn   t terribly helpful. what do we learn from it? not much. we have
   lots of particular representations, but it   s hard to compare them or
   get a big picture view.

   let   s focus on comparing representations for a moment. the tricky thing
   about this is that fundamentally similar neural networks can be very
   different in ways we don   t care about. two neurons might be switched.
   the representation could be rotated or flipped.
   [mnist-pca-conv1.png] [mnist-pca-conv2.png]
   two very similar representations, except for a flip

   we want to, somehow, forget about these unimportant differences and
   focus only on the important differences. we want a canonical form for
   representations, that encodes only meaningful differences.

   distance seems fundamental, here. all of these unimportant differences
   are isometries     that is, transformations like rotation or switching
   two dimensions do not change the distances between points. on the other
   hand, distance between points is really important: things being close
   together is a representations way of saying that they are similar, and
   things being far apart is a representation saying they are different.

   thankfully, there   s an easy way to forget about isometries. for a
   representation \(x\), there   s an associated metric function, \(d_x\),
   which gives us the distance between pairs of points within that
   representation. for another representation \(y\), \(d_x = d_y\) if and
   only if \(x\) is isometric to \(y\). the metric functions encode
   precisely the information we want!

   we can   t really work with \(d_x\) because it is actually a function on
   a very high-dimensional continuous space.[31]^4 we need to discretize
   it for it to be useful.

   \[d_x = \left[\begin{array}{cccc} d_x(x_0, x_0) & d_x(x_1, x_0) &
   d_x(x_2, x_0) & ... \\ d_x(x_0, x_1) & d_x(x_1, x_1) & d_x(x_2, x_1) &
   ... \\ d_x(x_0, x_2) & d_x(x_1, x_2) & d_x(x_2, x_2) & ... \\ ... & ...
   & ... & ... \\ \end{array} \right]\]

   one thing we can do with \(d_x\) is flatten it to get a vector encoding
   the properties of the representation \(x\). we can do this for a lot of
   representations, and we get a collection of high-dimensional vectors.

   the natural thing to do, of course, is to apply dimensionality
   reduction, such as id167, to our representations. geoff hinton dubbed
   this use of id167    meta-sne   . but one can also use other kinds of
   id84.[32]^5 [33]^6

   in the following visualization, there are three boxes. the largest one,
   on the left, visualizes the space of representations, with every point
   corresponding to a representation. the points are positioned by
   id84 of the flattened distance matrices, as above.
   one way to think about this that distance between representations in
   the visualization represents how much they disagree on which points are
   similar and which points are different.

   next, the middle box is a regular visualization of a representation of
   mnist, like the many we   ve seen previously. it displays which ever
   representation you hover over in left box. finally, the right most box
   displays particular mnist digits, depending on which point you hover
   over in the middle box.
   the space of mnist representations
   left: visualization of representations with meta-sne, points are
   representations. middle: visualization of a particular representation,
   points are mnist data points. right: image of a particular data point.

   this visualization shifts us from looking at trees to seeing the
   forest. it moves us from looking at representations, to looking at the
   space of representations. it   s a step up [34]the ladder of abstraction.

   imagine training a neural network and watching its representations
   wander through this space. you can see how your representations compare
   to other    landmark    representations from past experiments. if your
   model   s first layer representation is in the same place a really
   successful model   s was during training, that   s a good sign! if it   s
   veering off towards a cluster you know had too high learning rates, you
   know you should lower it. this can give us qualitative feedback during
   neural network training.

   it also allows us to ask whether two models which achieve comparable
   results are doing similar things internally or not.

deep learning for visualization

   all of the examples above visualize not only the neural network, but
   the data it operates on. this is because the network is inextricably
   tied to the data it operates on.[35]^7

   the visualizations are a bit like looking through a telescope. just
   like a telescope transforms the sky into something we can see, the
   neural network transforms the data into a more accessible form. one
   learns about the telescope by observing how it magnifies the night sky,
   but the really remarkable thing is what one learns about the stars.
   similarly, visualizing representations teaches us about neural
   networks, but it teaches us just as much, perhaps more, about the data
   itself.

   (if the telescope is doing a good job, it fades from the consciousness
   of the person looking through it. but if there   s a scratch on one of
   the telescope   s lenses, the scratch is highly visible. if one has an
   example of a better telescope, the flaws in the worse one will suddenly
   stand out. similarly, most of what we learn about neural networks from
   representations is in unexpected behavior, or by comparing
   representations.)

   understanding data and understanding models that work on that data are
   intimately linked. in fact, i think that understanding your model has
   to imply understanding the data it works on. [36]^8

   while the idea that we should try to visualize neural networks has
   existed in our community for a while, this converse idea     that we can
   use neural networks for visualization     seems equally important is
   almost entirely unexplored.

   let   s explore it.

unthinkable thoughts, incomprehensible data

   in his talk    media for thinking the unthinkable   , bret victor raises a
   really beautiful quote from richard hamming:

     just as there are odors that dogs can smell and we cannot, as well
     as sounds that dogs can hear and we cannot, so too there are
     wavelengths of light we cannot see and flavors we cannot taste.

     why then, given our brains wired the way they are, does the remark
        perhaps there are thoughts we cannot think,    surprise you?

     evolution, so far, may possibly have blocked us from being able to
     think in some directions; there could be unthinkable thoughts.

         - richard hamming, [37]the unreasonable effectiveness of
     mathematics

   victor continues with his own thoughts:

     these sounds that we can   t hear, this light that we can   t see, how
     do we even know about these things in the first place? well, we
     built tools. we built tools that adapt these things that are outside
     of our senses, to our human bodies, our human senses.

     we can   t hear ultrasonic sound, but you hook a microphone up to an
     oscilloscope and there it is. you   re seeing that sound with your
     plain old monkey eyes. we can   t see cells and we can   t see galaxies,
     but we build microscopes and telescopes and these tools adapt the
     world to our human bodies, to our human senses.

     when hamming says there could be unthinkable thoughts, we have to
     take that as    yes, but we build tools that adapt these unthinkable
     thoughts to the way that our minds work and allow us to think these
     thoughts that were previously unthinkable.   

         - bret victor, [38]media for thinking the unthinkable

   this quote really resonates with me. as a machine learning researcher,
   my job is basically to struggle with data that is incomprehensible    
   literally impossible for the human mind to comprehend     and try to
   build tools to think about it and work with it.[39]^9

   however, from the representation perspective, there   s a further natural
   step to go with this idea   

representations in human vision

   lets consider human vision for a moment. our ability to see is amazing.
   the amazing part isn   t our eyes detecting photons, though. that   s the
   easy, simple part. the amazing thing is the ability of our brain to
   transform the mess of swirling high-dimensional data into something we
   can understand. to present it to us so well that it seems simple! we
   can do this because our brains have highly specialized pathways for
   processing visual data.

   just as neural networks transform data from the original raw
   representations into nice representations, the brain transforms our
   senses from complicated high-dimensional data into nice
   representations, from the incomprehensible to the comprehensible. my
   eye detects photons, but before i even become consciously aware of what
   my eye sees, the data goes through incredibly sophisticated
   transformations, turning it into something i can reason about.[40]^10
   the brain does such a good job that vision seems easy! it   s only when
   you try to understand visual data without using your visual system that
   you realize how incredibly complicated and difficult to understand it
   is.

senses we don   t have

   unfortunately, for every sense we have, there are countless others we
   don   t. countless modes of experience lost to us. this is a tragedy.
   imagine the senses we could have! there are vast collections of text
   out there: libraries, wikipedia, the internet as a whole     imagine
   having a sense that allowed you to see a whole corpus at once, which
   parts are similar and which are different! every collision at the large
   hadron collider is monitored by a battery of different sensors    
   imagine having a sense that allowed us to    see    collisions as clearly
   as we can see images! the barrier between us and these potential senses
   isn   t getting the data, it   s getting the data to our brain in a nice
   representation.

   the easiest way to get new kinds of data into the brain is to simply
   project it into existing senses. in some very particular cases, this
   works really well. for example, microscopes and telescopes are
   extremely good at making a new kind of data accessible by projecting it
   into our normal visual sense. they work because macroscopic visual data
   and microscopic visual data are just visual data on different scales,
   with very similar structure to normal visual data, and are well handled
   by the same visual processing systems. much more often, projecting data
   into an existing sense (for example, with pca) throws away all but the
   crudest facets of the data. it   s like taking an image and throwing away
   everything except the average color. it   s something    but not much.

   we can also try to get this data to us symbolically. of course,
   rattling off 10,000-dimensional vectors to people is hopeless. but
   traditional statistics gives us some simple models we can fit, and then
   discuss using language of means, variance, covariance and so on.
   unfortunately, fitting gaussians is like describing clouds as ovals.
   talking about the covariance of two variables is like talking about the
   slope, in a particular direction, of a high-dimensional surface. even
   very sophisticated models from statistics seem unable to cope with the
   complicated, swirling, high-dimensional data we see in problems like
   vision.

   deep learning gives us models that can work with this data. more that
   that, it gives us new representations of the data. the representations
   it produces aren   t optimized to be nice representations for the human
   brain     i have no idea how one would optimize for that, or even what it
   would mean     but they are much nicer than the original data. i think
   that learning representations, with deep learning or other powerful
   models, is essential to helping humans understand new forms of data.

a map of wikipedia

   the best example i can give is the visualization of wikipedia from
   earlier. wikipedia is a repository of human knowledge. by combining
   deep learning and id84, we can make a map of it, as
   we saw earlier:
   [wiki-pic-major.png]
   map of wikipedia
   (paragraph vectors and id167     see [41]example 2 above)

   this style of visualization feels important to me. using deep learning,
   we   ve made a visualization, an interface, for humans to interact with
   wikipedia as a whole. i   m not claiming that it   s a great interface. i   m
   not even sure i think it is terribly useful. but it   s a starting point
   and a proof of concept.

   why not just use id84 by itself? if we had just
   used id84, we would be visualizing geometric or
   topological features of the wikipedia data. using deep learning to
   transform the data allows us to visualize the underlying structure, the
   important variations     in some cases, the very meaning of the
   data[42]^11     instead.

   i think that high-quality representations have a lot of potential for
   users interacting with complicated data, going far beyond what is
   explored here. the most natural direction is machine learning: once you
   are in a high-quality representation, many normally difficult tasks can
   be accomplished with very simple techniques and comparatively little
   data.[43]^12 with a curated collection of representations, one could
   make some really exciting machine learning accessible,[44]^13 although
   it would carry with it challenges for end users[45]^14 and the
   producers of representations.[46]^15

quantifying the subjective

   reasoning about data through representations can be useful even for
   kinds of data the human mind understands really well, because it can
   make explicit and quantifiable things that are normally tacit and
   subjective.

   you probably understand english very well, but much of this knowledge
   is subjective. the meaning of words is socially constructed, arising
   from what people mean by them and how they use them. it   s canonicalized
   in dictionaries, but only to a limited extent. but the subtleties in
   usage and meaning are very interesting because of how they reflect
   culture and society. unfortunately, these things are kind of fuzzy, and
   one typically needs to rely on anecdotes and personal impressions.

   one remarkable property of high-quality id27s is that they
   seem to reify these fuzzy properties into concrete mathematical
   structures! as mentioned earlier, directions in id27s
   correspond to particular kinds of differences in meaning. for example,
   there is some direction corresponding to gender. (for more details, see
   my post [47]deep learning, nlp, and representations.)

   by taking the difference of two word vectors, we can get directions for
   gender. for example, we can get a masculine direction (eg.    man    -
      woman   ) or a feminine direction (eg.    woman    -    man   ). we can also get
   age directions. for example, we can get an adult direction (eg.    woman   
   -    girl   ) or a child direction (eg.    boy    -    man   ).
   [wordgender.png]

   once we have these directions, there   s a very natural question to ask:
   which words are furthest in these directions? what are the most
   masculine or feminine words? the most adult, the most childish? well,
   let   s look at the wikipedia [48]glove vectors, from pennington, et al.
   at stanford:
     * masculine words tend to be related to military/extremism (eg.
       aresenal, tactical, al qaeda), leadership (eg. boss, manager), and
       sports (eg. game, midfielder).
     * female words tend to be related to reproduction (eg. pregnancy,
       birth), romance (eg. couples, marriages), healthcare (eg. nurse,
       patients), and entertainment (eg. actress).
     * adult words tend to be related to power (eg. victory, win),
       importance (eg. decisive, formidable), politics (eg. political,
       senate) and tradition (eg. roots, churches).
     * childish words tend to be related to young families (eg. adoption,
       infant), activities (eg. choir, scouts), items (eg. songs, guitar,
       comics) and sometimes inheritance (eg. heir, throne).

   of course, these results depend on a lot of details. [49]^16

   i   d like to emphasize that which words are feminine or masculine, young
   or adult, isn   t intrinsic. it   s a reflection of our culture, through
   our use of language in a cultural artifact. what this might say about
   our culture is beyond the scope of this essay. my hope is that this
   trick, and machine learning more broadly, might be a useful tool in
   sociology, and especially subjects like gender, race, and disability
   studies.

the future

   right now machine learning research is mostly about getting computers
   to be able to understand data that humans do: images, sounds, text, and
   so on. but the focus is going to shift to getting computers to
   understand things that humans don   t. we can either figure out how to
   use this as a bridge to allow humans to understand these things, or we
   can surrender entire modalities     as rich, perhaps more rich, than
   vision or sound     to be the sole domain of computers. i think user
   interface could be the difference between powerful machine learning
   tools     artificial intelligence     being a black box or a cognitive tool
   that extends the human mind.

   there   s actually two kinds of black boxes we need to avoid. two
   slightly different, but closely connected problems. the first problem
   is that deep learning itself is presently a kind of black box. the
   second is that tools using deep learning to solve particular problems
   might be black boxes.

   we need to figure out how to open the deep learning black box. one
   powerful approach is visualizing representations. in this essay, we
   used interactive media to visualize and explore some powerful models
   from google   s deep learning research group. we then observed that
   particular neural network architectures leave geometric signatures in
   their representations. finally, we created the meta-sne algorithm, in
   order to step up the ladder of abstraction, and think about the space
   of neural networks, instead of particular ones.

   the problem of particular tools being black boxes is, in some ways,
   harder, because there   s so much diversity. but a common problem is that
   humans can   t think about the sort of high-dimensional structures
   machine learning problems typically involve. we observed that
   visualizing representations can also be a tool to help humans
   understand and reason about these structures. we saw that
   representations can be helpful even for data we understand really well.

   these are problems we   re only beginning to attack. i think there   s a
   lot more for us to uncover here. it   s an odd kind of work, at the
   intersection of machine learning, mathematics, and user interface. but
   i think it   s important.

   if you enjoyed this post, consider subscribing to my [50]rss feed.

   (i would be delighted to hear your comments and thoughts: you can
   comment inline or at the end. for typos, technical errors, or
   clarifications you would like to see added, you are encouraged to make
   a pull request on [51]github.)

acknowledgements

   i   m grateful for the hospitality of google   s deep learning research
   group, which had me as an intern while i did most of the work this post
   is based on. i   m especially grateful to my internship host, [52]jeff
   dean.

   i was greatly helped by the comments, advice, and encouragement of many
   googlers, both in the deep learning group and outside of it. these
   include: [53]greg corrado, [54]jon shlens, matthieu devin, andrew dai,
   [55]quoc le, anelia angelova, [56]oriol vinyals, [57]ilya sutskever,
   [58]ian goodfellow, jutta degener, and anna goldie.

   i was strongly influenced by the thoughts, comments and notes of
   [59]michael nielsen, especially [60]his notes on [61]bret victor   s
   work. michael   s thoughts persuaded me that i should think seriously
   about interactive visualizations for understanding deep learning. the
   section    unthinkable thoughts, incomprehensible data    was particularily
   influenced by him.

   i was also helped by the support and comments of a number of other
   non-googler friends, including [62]yoshua bengio, [63]laura ball,
   [64]rob gilson, [65]henry de valence, [66]yomna nasser, and [67]james
   koppel.

   this blog post was made possible by a number of wonderful javascript
   libraries, including [68]d3.js, [69]mathjax, and [70]jquery. a big
   thank you to everyone who contributed to these libraries.
     __________________________________________________________________

    1. the representation perspective is an abstraction over inputs.
       instead of trying to understand what the neural network does to a
       single input, we try to understand what it does to the space of
       inputs, to the data manifold. it   s a step up [71]the ladder of
       abstraction. later, we will take a second step, allowing us to look
       at the space of neural networks, instead of a single one.[72]   
    2. we categorize words using id138 synsets. each synset is labeled
       something like    region.n.03    (region, noun, meaning 3) or
          travel.v.01    (travel, verb, meaning 1).[73]   
    3. it should be noted that, later, sutskever et al. switched to
       reversing the order of the input sentence, finding this improved
       their results.
       [translation2-backwards.png]
       [74]   
    4. the natural way to think about distance between functions is to
       consider them as infinite dimensional vectors \((f(0), ~ f(1),~
       f(2)...)\). in the case of a function on the real numbers or on
       \(\mathbb{r}^n\), it   s a \(2^{\aleph_0}\) dimensional vector! while
       we can actually represent the function finitely (because we know
       it   s based on a neural network, which has a finite number of
       paramaters) it   s really hard to actually calculate distances.[75]   
    5. i   ve heard that some similar techniques may be used in
       neuroscience, where one often needs to compare different
       representations of the same data. further, in a previous post, john
       maccuish commented that one could use the mantel test on the
       distance matrices to compare representations     this gets at a very
       similar idea![76]   
    6. there are some variations you can do on the basic meta-sne
       algorithm. for example, meta-sne analyzes how much representations
       agree on which data points are similar, by comparing distance
       between the points in different representations. but we can also
       compare how much representations agree on analogies, on the manner
       in which things are different, by comparing the distance between
       differences of vectors in different representations. in principle,
       this information is encoded in the distances between data points,
       but one can make it much more explicit. it may also be the case
       that we care more about which networks are very similar, in which
       case we could apply some non-linearity pointwise to the distance
       matrix, to exaggerate the difference between close and not-close
       data points.[77]   
    7. the neural network is a function with the domain of the data
       manifold it was trained on.[78]   
    8. people sometimes complain:    neural networks are so hard to
       understand! why can   t we use understandable models, like id166s?   
       well, you understand id166s, and you don   t understand visual pattern
       recognition. if id166s could solve visual pattern recognition, you
       would understand it. therefore, id166s are not capable of this, nor
       is any other model you can really understand. (i don   t mean this to
       be a    proof    obviously, but i am pretty serious about this
       view.)[79]   
    9. you could imagine defining a field this way, as attempting to build
       tools for thinking about and working with the complicated
       high-dimensional id203 distributions we see in the real
       world. the field you get isn   t quite machine learning, but it has a
       lot of overlap. it actually feels more compelling to me. perhaps
       this is    data science   ?[80]   
   10. as someone without a neuroscience background, i feel a bit nervous
       making remarks like this. that said, i think what i   m mostly saying
       is an interpretation, an abstraction, over some fairly basic facts
       about how human vision works. i also know that at least some
       neuroscientists subscribe to this interpretation and seriously look
       at things through this sort of lens. for example, see [81]dicarlo
       and cox   s paper    untangling invariant object recognition   .[82]   
   11. this is a very bold claim. my defense is this: id27
       models seem to encode semantic meaning in directions, creating a
          semantic vector space.    paragraph vectors (at least the kind that
       we   re using) do the same. somehow, these models seem to discover
       human meaning while learning the structure of the space. the
       results of the devise paper suggest that this may be somewhat
       general in good high-level representations.[83]   
   12. this is some combination of id21, pretraining, and
       id72. how well it works varies, but there   s
       certainly a lot of successes. obviously, the ideal is to have a lot
       of data to train a representation specifically for your task. but
       failing that, we can also try to make very transferable
       representations, possibly by training them for a bunch of different
       tasks.[84]   
   13. curating large collections of structured data has lead to some
       really interesting tools (for example, wolfram alpha). my intuition
       is that curating a collection of high-quality representations for
       different kinds of data could also be really interesting. i think
       [85]metamind is the closest thing i know of to this, right
       now.[86]   
   14. it seems like a lot of the problems that exist with proprietary
       file formats could end up happening here. an end user could very
       easily end up tied to a particular representation. do we need open
       or standardized representations?[87]   
   15. the problem with just releasing representations, from the
       perspective of the model producer, is what geoff hinton calls    dark
       knowledge   . representations subtly encode a lot of the knowledge of
       your model. by releasing representations, organizations are
       implicitly releasing a significant amount of information about
       their model.[88]   
   16. the answer depends a lot on the corpus you train on. if you train
       your id27 on a news corpus, that will be different than
       if you train it on wikipedia. and i assume if you trained on a
       corpus of 19th century literature, that would be very different
       again. it also depends on your model, and how well you trained it.
       the precise interpretation is obviously sensitive to the model.
       but, generally, it will be something like this: there is a certain
       difference in how language is used around the word    man    and the
       word    woman   ; which words cause language around them to change most
       in that manner? (replace    man    and    woman    for whatever you
       want.)[89]   

   subscribe to the [90]rss feed. built by [91]oinkina with [92]hakyll
   using [93]bootstrap, [94]mathjax, and [95]disqus.

   enable javascript for footnotes, disqus comments, and other cool stuff.

references

   1. http://colah.github.io/
   2. http://colah.github.io/
   3. http://colah.github.io/about.html
   4. http://colah.github.io/contact.html
   5. http://colah.github.io/posts/tags/data_visualization.html
   6. http://colah.github.io/posts/tags/machine_learning.html
   7. http://colah.github.io/posts/tags/word_embeddings.html
   8. http://colah.github.io/posts/tags/neural_networks.html
   9. http://colah.github.io/posts/tags/deep_learning.html
  10. http://colah.github.io/posts/tags/user_interface.html
  11. http://colah.github.io/posts/tags/wikipedia.html
  12. http://colah.github.io/posts/2014-10-visualizing-mnist/
  13. http://colah.github.io/posts/2015-01-visualizing-representations/#fn1
  14. http://colah.github.io/posts/2014-03-nn-manifolds-topology/
  15. http://cs.stanford.edu/people/karpathy/id98embed/
  16. http://colah.github.io/posts/2014-07-nlp-id56s-representations/
  17. http://colah.github.io/posts/2015-01-visualizing-representations/#fn2
  18. http://id138.princeton.edu/
  19. http://colah.github.io/posts/2015-01-visualizing-representations/big_vis/words.html
  20. http://arxiv.org/pdf/1405.4053.pdf
  21. http://en.wikipedia.org/wiki/bag-of-words_model
  22. https://fb56552f-a-62cb3a1a-s-sites.googlegroups.com/site/deeplearningworkshopnips2014/68.pdf?attachauth=anoy7cq0ey7dqo5oyr2dmzrzlgycubnistwaoo-iucb-yhxfoudjis5kypkp66yzoy4u8qvofq9giauxhmabxtsp2vi5jyupimrhw7gdmcgvyqxxdtnqxc3fitrzp8np8pbhr-q72sk-1p1ltrbryugwu6_pgzdq8a20c6qn16b2fn4ok7rylcj1ipf1joj_ockrsph_bbeir-clksoimidoz-ynohyeckcmje9rjzturimhflzompy=&attredirects=0
  23. http://colah.github.io/posts/2015-01-visualizing-representations/big_vis/wiki.html
  24. http://arxiv.org/pdf/1409.3215v1.pdf
  25. http://en.wikipedia.org/wiki/long_short_term_memory
  26. http://colah.github.io/posts/2015-01-visualizing-representations/#fn3
  27. http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  28. http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  29. http://arxiv.org/pdf/1406.1078v1.pdf
  30. http://arxiv.org/pdf/1406.1078v1.pdf
  31. http://colah.github.io/posts/2015-01-visualizing-representations/#fn4
  32. http://colah.github.io/posts/2015-01-visualizing-representations/#fn5
  33. http://colah.github.io/posts/2015-01-visualizing-representations/#fn6
  34. http://worrydream.com/ladderofabstraction/
  35. http://colah.github.io/posts/2015-01-visualizing-representations/#fn7
  36. http://colah.github.io/posts/2015-01-visualizing-representations/#fn8
  37. https://www.dartmouth.edu/~matc/mathdrama/reading/hamming.html
  38. http://worrydream.com/mediaforthinkingtheunthinkable/
  39. http://colah.github.io/posts/2015-01-visualizing-representations/#fn9
  40. http://colah.github.io/posts/2015-01-visualizing-representations/#fn10
  41. http://colah.github.io/posts/2015-01-visualizing-representations/#example-2-paragraph-vectors-of-wikipedia
  42. http://colah.github.io/posts/2015-01-visualizing-representations/#fn11
  43. http://colah.github.io/posts/2015-01-visualizing-representations/#fn12
  44. http://colah.github.io/posts/2015-01-visualizing-representations/#fn13
  45. http://colah.github.io/posts/2015-01-visualizing-representations/#fn14
  46. http://colah.github.io/posts/2015-01-visualizing-representations/#fn15
  47. http://colah.github.io/posts/2014-07-nlp-id56s-representations/
  48. http://nlp.stanford.edu/projects/glove/
  49. http://colah.github.io/posts/2015-01-visualizing-representations/#fn16
  50. http://colah.github.io/rss.xml
  51. https://github.com/colah/visualizing-representations/
  52. http://research.google.com/people/jeff/
  53. http://research.google.com/pubs/gregcorrado.html
  54. http://research.google.com/pubs/jonathonshlens.html
  55. http://cs.stanford.edu/~quocle/
  56. http://research.google.com/pubs/oriolvinyals.html
  57. http://www.cs.toronto.edu/~ilya/
  58. http://www-etud.iro.umontreal.ca/~goodfeli/
  59. http://michaelnielsen.org/
  60. http://mnielsen.github.io/notes/kill_math/kill_math.html
  61. http://worrydream.com/
  62. http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html
  63. http://www.thielfellowship.org/author/lball/
  64. https://github.com/d1plo1d
  65. https://www.hdevalence.ca/blog/
  66. https://github.com/ynasser
  67. http://www.jameskoppel.com/
  68. http://d3js.org/
  69. http://www.mathjax.org/
  70. http://jquery.com/
  71. http://worrydream.com/ladderofabstraction/
  72. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref1
  73. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref2
  74. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref3
  75. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref4
  76. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref5
  77. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref6
  78. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref7
  79. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref8
  80. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref9
  81. http://dicarlolab.mit.edu/sites/dicarlolab.mit.edu/files/pubs/dicarlo and cox 2007.pdf
  82. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref10
  83. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref11
  84. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref12
  85. https://www.metamind.io/
  86. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref13
  87. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref14
  88. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref15
  89. http://colah.github.io/posts/2015-01-visualizing-representations/#fnref16
  90. http://colah.github.io/rss.xml
  91. https://github.com/oinkina
  92. http://jaspervdj.be/hakyll
  93. http://getbootstrap.com/
  94. http://www.mathjax.org/
  95. http://disqus.com/
