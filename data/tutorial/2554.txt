   (button) toggle navigation [1]colah's blog
     * [2]blog
     * [3]about
     * [4]contact

groups & group convolutions

   posted on december 8, 2014

   [5]group theory, [6]id203, [7]convolution, [8]math

symmetry

   consider a square. is it symmetric? how is it symmetric? how much
   symmetry does it have? what kind of symmetry does it have?

   what do those questions even mean?

   if you ask someone, they might tell you that a square has rotational
   symmetry. if you rotate a square by 90  , it   s the same shape. without
   knowing which corner was which, it would seem the exact same as it was
   before. you could lift it up, rotate it, and set it back down so that
   it covers the exact same space.

   let   s call this rotation transformation \(r\). to be precise, \(r\)
   rotates a square clockwise by 90  . for example, \(r\sq{e} = \sq{r}\).
   (the    f    on the square is there to let us determine orientation and see
   transformations.)

   you might also be told that a square has horizontal symmetry or
   vertical symmetry. you can flip a square horizontally or vertically and
   still have a square. let   s focus on horizontal symmetry for now. we   ll
   call horizontal flips \(s\). \(s\) performs a reflection across a
   vertical line through the middle of the square. for example, \(s\sq{e}
   = \sq{s}\).

   we now have two transformations, \(r\) and \(s\), which transform
   squares into another square of the same shape. it turns out that these
   two transformations form a kind of    basis    for all the others. by using
   them in some pattern, you can build the other transformations, like
   vertical flipping.

   starting with our original square \(\sq{e}\) in the bottom left corner,
   the following graph shows the transformed versions generated by
   combining \(r\) and \(s\) in different ways. \(r\) and \(s\) are
   represented by arrows of different colors. \(r\) arrows are colored
   blue and \(s\) arrows are colored red.

   we can use the graph to investigate what happens if we perform a
   sequence of transformations. for example, what happens if we rotate,
   flip and then rotate again? well, we start at our original square,
   \(\sq{e}\), and trace: \(\sq{e} \xrightarrow{r} \sq{r} \xrightarrow{s}
   \sq{r3s} \xrightarrow{r} \sq{s}\). in the end, we   re left with just
   horizontally flipped version of the original, \(s\sq{e} = \sq{s}\). if
   we want to express this surprising fact, we can use multiplication like
   notation: \(rsr \sq{e} = s \sq{e}\).

   if we want to think about our graph a bit more abstractly, we can
   represent all the squares as the original square transformed by \(r\)
   and \(s\). for example, \(\sq{r2s} = r^2s\sq{e}\).

   here, \(e\) is the identity transformation, which doesn   t transform the
   object at all. for example \(e\sq{e} = \sq{e}\). (why have \(e\), if it
   doesn   t do anything? it   s a lot like having the number zero.)

   we can go a bit further. the original square, \(\sq{e}\), seems a bit
   unnecessary in \(rsr \sq{e} = s \sq{e}\). why not just say \(rsr = s\)?
   we can just drop the factored out \(\sq{e}\), both in equations and our
   graph.

   now, here   s the essential realization: \(r\) and \(s\) could have been
   other things and we would have had the exact same graph. \(r\) could
   have been rotating 90   counterclockwise. \(s\) could have been vertical
   flips. or we could have been transforming an entirely different kind of
   object. all that matters is the relationship between \(r\) and \(s\),
   how they interact. what we saw with the squares was just one particular
   way this graph, this abstract pattern, could appear in the real world.

   mathematicians call these abstract patterns groups. there is an entire
   field of math dedicated to them. connections between a group and an
   object like the square are called group actions.

but    what is a group?

   not all graphs are groups. only a very special kind of graph is. (we
   won   t give a formal definition here, but we will get a good feel for
   it.)

   firstly, the graph is directed (the edges are arrows) and has colored
   edges. at every vertex, exactly one arrow of a given color comes out
   and one goes in.

   but the key property of these graphs is more subtle. we created our
   graph by starting with an original square, \(\sq{e}\). but what if we
   said the original square was \(\sq{s} = s\sq{e}\)?

   which position we say is the    initial    position is arbitrary. no matter
   which position you think of as the initial one, the graph is the same.
   the graph is perfectly symmetrical, in some sense.[9]^1 imagine that
   the edges are paths of different color you can walk on, and you   re
   standing on one of the nodes: from your perspective the graph is the
   same no matter which node you   re standing on. no matter which node
   you   re on, taking a red path, a blue path, and then a red path and then
   a blue path again will bring you back to where you started.

   in euclidean space, we reason about points by their relative position
   to an origin. similarly, in our group, we pick some origin (eg.
   \(\sq{e}\)) and talk about points by their relative positions. we call
   these relative positions (such as \(r\), \(s\), or \(r^3s\)), the
   elements of the group.

   just like we can add difference vectors of points, we can    add   
   elements of a group together. it isn   t actually addition, of course,
   but it is a natural way to combine elements of the group. sometimes we
   talk about it by analogy with addition and write combining two elements
   \(a\) and \(b\) as \(a+b\), while other times we make analogies to
   multiplication and write \(a\cdot b\).

      adding    or    multiplying    two group elements is actually quite similar
   to vector addition. we decide that one point on the graph is our
   identity element (the original position), and find the two elements we
   want to multiply, \(a\) and \(b\). we pick paths from the identity to
   \(a\) and \(b\). then we stick the \(a\) path on to the end of \(b\),
   to bring us to \(a+b\) or \(a\cdot b\) (depending on the chosen
   notation).

the algebraic perspective

   (this section is optional.)

   the above is almost unrecognizable as group theory, from a traditional
   perspective. usually, we think of groups as a kind of abstraction.

   there are lots of kinds of mathematical objects and, as you look at
   more of them, one beings to see patterns. for example, in arithmetic,
   we see \(a\!\cdot\!(b+c) ~=~ a\!\cdot\! b ~+~ a\!\cdot\! c\) and in set
   theory we see \(a\cap (b \cup c) = a\cap b ~\cup~ a\cap c\). there are
   many other examples of this pattern, and many other patterns.

   one also notices that many important results are true for a broad class
   of objects, and they   re all true for the same reason. they   re true
   because all the objects observe a particular pattern. knowing that a
   mathematical object obeys that pattern is sufficient to prove the
   result holds.

   so, we formalize those patterns into what we call mathematical
   structures.[10]^2 there   s a lot of them, and you can find a [11]very
   long list of algebraic ones on wikipedia. we can study a mathematical
   structure and prove results that hold for any instance of that
   structure. (programmers and computer scientists can see this as making
   mathematics polymorphic.[12]^3)

   we can now give the classical definition of a group. don   t worry too
   much if you have trouble following.

   definition: a group \(g = (s, ~\cdot~)\) is a set \(s\) equipped with a
   binary operation \((~\cdot~)\), a function mapping pairs of group
   elements to group elements, with the following properties:
     * there exists an identity element, \(e \in s\), such that \(e\cdot x
       ~=~ x \cdot e ~=~ x\) for all \(x \in s\).
     * for all elements \(x \in s\), there exists an inverse element
       \(x^{-1} \in s\) such that \(x\cdot x^{-1} = x^{-1}\cdot x = e\).
     * the operation \((~\cdot~)\) is associative. that is, \((a\cdot
       b)\cdot c ~=~ a\cdot (b\cdot c)\) for all \(a,b,c \in s\),

   why those rules? why not more or less? well, we could define a group to
   have more or less requirements. if it was weaker, had less
   requirements, more kinds of objects would be groups and the results we
   prove about groups would be more broadly applicable. if it was
   stronger, had more requirements, we would be talking about a more
   specific kind of object and could prove more about them. in mathematics
   one often balances generality and specificity like this.

   mathematicians study both weaker and stronger versions of groups. but,
   somehow, groups are special. they aren   t too hot, they aren   t too cold:
   they   re just right.

   this might seem kind of arbitrary. why should these particular rules be
   a particularly good collection? one thing that i find very helpful and
   motivating is realizing that they   re equivalent to the requirements we
   made when we were thinking of groups as graphs. identity corresponds to
   there being a starting point, inverses to being able to go backwards on
   arrows, and associativity is equivalent to the perfect symmetry of the
   graph.[13]^4

a group from three cards

   consider three cards, \(\cards{123}\). there are some transformations
   that are natural to apply to them. we   ll call the operation of
   switching the first two cards \((12)\). similarly, we   ll call the
   operation of switching the second cards \((23)\). so,

   \[(12)\cards{123} = \cards{213} ~~~~~~~~~~~~~~~~~~~~~~~~
   (23)\cards{123} = \cards{132}\]

   together, these two operations generate a group, the [14]symmetric
   group on 3 symbols, \(s_3\).

   each group element is a particular way to rearrange the cards, a
   permutation.

shuffling cards

   one interesting thing to think about is shuffling. when we shuffle
   cards, we try to put them in a random ordering, a random permutation.
   this means we create a id203 distribution over the group.

   ideally, our shuffle would give us a uniform distribution     every
   permutation would be equally likely. but we can easily imagine an
   imperfect shuffle, where some permutations are more likely than others.

   of course, if the first shuffle doesn   t randomize them, we can shuffle
   again!

   generally, repeated shuffles will cause id203 mass to diffuse,
   bringing us closer to the uniform distribution.[15]^5

   this should feel similar to the falling ball example in the
   [16]understanding convolutions post. fundamentally, they are the same
   thing: convolution.

group convolutions

   the earlier visualizations of id203 distributions on the
   permutations were kind of messy. the natural way to visualize it is on
   the cayley diagram!

   let   s consider a very simple id203 distribution. 40% of the time
   we apply the operation \((12)\), permuting our cards to
   \(\cards{213}\). 60% of the time we apply \((23)\), permuting our cards
   to \(\cards{132}\). that   s a terrible shuffle, but it is easy to think
   about.

   to be a bit more explicit, let   s picture us as starting with all the
   id203 density on the unpermuted cards \(\cards{123}\) (i.e. the
   identity), and then we apply our very silly shuffle.

   when we shuffle, we sample this distribution, getting some permutation
   \(a\) with id203 \(f(a)\).

   what happens when we shuffle a second time?

   well, the first time we shuffled, we got a permutation \(a\) with
   id203 \(f(a)\). the second time we shuffle, we will get another
   permutation \(b\) with id203 \(g(b)\). these two actions happen
   with id203 \(f(a)g(b)\) and result is a permutation \(c = b\cdot
   a\).

   to get the actual id203 of \(c\), though, it is not sufficient to
   just look at one pair of permutations that bring us to \(c\). instead,
   we need to sum over all possible pairs of permutations. this is the
   convolution of \(g\) and \(f\) (like in function composition, the right
   side goes first).

   \[(g\ast f)(c) = \sum_{b \cdot a = c} g(b)f(a)\]

   substituting \(b = ca^{-1}\), we get:

   \[(g\ast f)(c) = \sum_{a} g(ca^{-1})f(a)\]

   this can be nicely thought of as a sum over the intermediate
   permutations, \(a\), looking at the id203 of that intermediate
   permutation, and the id203 of the permutation necessary to bring
   us to \(c\) from there.

   alternatively, we can substitute \(a = b^{-1}c\) to get:

   \[(g\ast f)(c) = \sum_{b} g(b)f(b^{-1}c)\]

   the traditional definition of group convolution. (if you let the group
   operation be addition, this is the normal definition of convolution.)

further generalizations of convolution

   (this section is optional and assumes a stronger background than the
   rest of the article. less mathematically inclined readers might wish to
   skip this section.)

   the traditional definition of convolution requires that you be able to
   take inverses, and multiply every element by every other element. this
   means you need to be working on a group, or perhaps a quasigroup.

   but if you switch to the definition \((g\ast f)(c) = \sum_{b \cdot a =
   c} g(b)f(a)\), which seems much more natural, convolution makes sense
   on just about any algebraic structure with a binary operator.
   certainly, you can talk about convolutions on monoids, groupoids, and
   categories. as far as i can tell, no one   s really considered
   these.[17]^6

   one cute thing about this is that convolution often inherits the
   algebraic properties of the domains of the functions being convolved.
   for example, if you convolve functions on associative domains, the
   convolution operation is associative:

   \[((a\ast b) \ast c)(x) = \sum_{a \cdot b \cdot c = x} a(a)b(b)c(c) =
   (a\ast (b \ast c))(x)\]

   similarly, if the domain is commutative, so is convolution. and if it
   has identity, so does convolution. sadly, convolution doesn   t get
   inverses if the domain has inverses, so the parallel breaks down at
   abelian monoids.

   with the math working out so nicely, you might wonder if there   s any
   reason one might actually use these. well, convolution on monoids seems
   natural in cases where you    can   t go backwards   . and convolution on
   categories allows for a kind of state. in fact, i think you could very
   naturally describe probabilistic automaton in terms of category
   convolutions.

conclusion

   this essay takes an unusual perspective on group theory. cayley
   diagrams have been around for a long time, but, as far as i know,
   taking them seriously as an approach to group theory, as a kind of
   foundation, is a recent idea, engineered by nathan carter in his book
   visual group theory. interested readers are encouraged to look at his
   book.

   group convolutions provide elegant language for talking about lots of
   situations involving id203. but, since this is a series of blog
   posts on convolutional neural networks, you may suspect that i have
   other interests in them. well, you guessed correctly. group
   convolutions naturally extend convolutional neural networks, with
   everything fitting together extremely nicely. since convolutional
   neural networks are one of the most powerful tools in machine learning
   right now, that   s pretty interesting. in our next post, we will explore
   these networks.

next posts in this series

   this post is part of a series on convolutional neural networks and
   their generalizations. the first two posts will be review for those
   familiar with deep learning, while later ones should be of interest to
   everyone. to get updates, subscribe to my [18]rss feed!

   please comment below or on the side. pull requests can be made on
   [19]github.

acknowledgements

   i   m grateful to yomna nasser, harry de valence, sam eisenstat, and
   sebastian zany for taking the time to read and comment on draft version
   of this post     their feedback improved it a lot!

   i   m also grateful to guillaume alain, eliana lorch, dario amodei, aaron
   courville, yoshua bengio, and michael nielsen for discussion of group
   convolution and its potential applications to neural networks.
     __________________________________________________________________

    1. note that the graph embedding isn   t necessarily symmetrical.[20]   
    2. usually people talk about algebraic structures, abstract
       mathematical structures from algebra. there are similar abstract
       mathematical structures in other areas, particularly in analysis.
       for example: [21]metric spaces, [22]topological spaces and
       [23]measure spaces. however, these are rarely lumped together in
       the way that algebraic structures are.[24]   
    3. this is actually a very deep analogy. in programming, we often try
       to write polymorphic functions that can act on many kinds of
       objects. in mathematics, we   re trying to make polymorphic proofs
       that can operate on different kinds of mathematical object. the
       [25]curry   howard correspondence formalizes this connection between
       programs and proofs.
       (some programming languages, like haskell, even have
       implementations of common algebraic structures as classes!)
       it   s also worth noting that, just as most approaches to
       polymorphism in programming give us subclasses and superclasses,
       algebraic structures also kind of have    sub-structures    and
          super-structures   .[26]   
    4. the associativity part is a bit tricky to see, especially because
       we never rigorously defined the    perfect symmetry    of our    group
       graphs.   
       one definition is that, given a loop originating at \(e\) on the
       graph, \(((bc)d)... = e\), that same sequence is also a loop if it
       starts at a point \(a\), that is \((((ab)c)d)... = a\). it   s pretty
       straightforward to see that this follows from associativity, but
       what about the other direction?
       well, we want to prove for all \(a,b,c\), that, \(a(bc) = (ab)c\).
       let \(d = (bc)^{-1}\), the reverse of the path to \(bc\). then
       \((bc)d = e\) is a loop. by the graph symmetry, \(((ab)c)d = a\).
       we now right-mulitply by \(d^{-1} = (bc)\) to get \((ab)c =
       a(bc)\), which is associativity.[27]   
    5. how many times do you have to shuffle a deck of cards to make it
       truly random? this question was explored by the mathematician persi
       diaconis.[28]   
    6. i can   t really find instances of people talking about these
       convolutions as independent things, but the operation seems to be
       implicitly constructed in objects built to study these structures.
       just as multiplication in [29]group rings is group convolution,
       multiplication in [30]monoid rings is monoid convolution,
       multiplication in [31]groupoid algebras is groupoid convolution,
       and multiplication in [32]categorical algebras is category
       convolution.[33]   

   subscribe to the [34]rss feed. built by [35]oinkina with [36]hakyll
   using [37]bootstrap, [38]mathjax, and [39]disqus.

   enable javascript for footnotes, disqus comments, and other cool stuff.

references

   1. http://colah.github.io/
   2. http://colah.github.io/
   3. http://colah.github.io/about.html
   4. http://colah.github.io/contact.html
   5. http://colah.github.io/posts/tags/group_theory.html
   6. http://colah.github.io/posts/tags/id203.html
   7. http://colah.github.io/posts/tags/convolution.html
   8. http://colah.github.io/posts/tags/math.html
   9. http://colah.github.io/posts/2014-12-groups-convolution/#fn1
  10. http://colah.github.io/posts/2014-12-groups-convolution/#fn2
  11. http://en.wikipedia.org/wiki/list_of_algebraic_structures
  12. http://colah.github.io/posts/2014-12-groups-convolution/#fn3
  13. http://colah.github.io/posts/2014-12-groups-convolution/#fn4
  14. http://en.wikipedia.org/wiki/symmetric_group
  15. http://colah.github.io/posts/2014-12-groups-convolution/#fn5
  16. http://colah.github.io/posts/2014-07-understanding-convolutions/
  17. http://colah.github.io/posts/2014-12-groups-convolution/#fn6
  18. http://colah.github.io/rss.xml
  19. https://github.com/colah/group-conv-post
  20. http://colah.github.io/posts/2014-12-groups-convolution/#fnref1
  21. http://en.wikipedia.org/wiki/metric_space
  22. http://en.wikipedia.org/wiki/topological_space
  23. http://en.wikipedia.org/wiki/measure_(mathematics)
  24. http://colah.github.io/posts/2014-12-groups-convolution/#fnref2
  25. http://en.wikipedia.org/wiki/curry   howard_correspondence
  26. http://colah.github.io/posts/2014-12-groups-convolution/#fnref3
  27. http://colah.github.io/posts/2014-12-groups-convolution/#fnref4
  28. http://colah.github.io/posts/2014-12-groups-convolution/#fnref5
  29. http://en.wikipedia.org/wiki/group_ring
  30. http://en.wikipedia.org/wiki/monoid_ring
  31. http://en.wikipedia.org/wiki/groupoid_algebra
  32. http://en.wikipedia.org/wiki/categorical_algebra
  33. http://colah.github.io/posts/2014-12-groups-convolution/#fnref6
  34. http://colah.github.io/rss.xml
  35. https://github.com/oinkina
  36. http://jaspervdj.be/hakyll
  37. http://getbootstrap.com/
  38. http://www.mathjax.org/
  39. http://disqus.com/
