   (button) toggle navigation [1]colah's blog
     * [2]blog
     * [3]about
     * [4]contact

visual id205

   posted on october 14, 2015

   i love the feeling of having a new way to think about the world. i
   especially love when there   s some vague idea that gets formalized into
   a concrete concept. id205 is a prime example of this.

   id205 gives us precise language for describing a lot of
   things. how uncertain am i? how much does knowing the answer to
   question a tell me about the answer to question b? how similar is one
   set of beliefs to another? i   ve had informal versions of these ideas
   since i was a young child, but id205 crystallizes them
   into precise, powerful ideas. these ideas have an enormous variety of
   applications, from the compression of data, to quantum physics, to
   machine learning, and vast fields in between.

   unfortunately, id205 can seem kind of intimidating. i
   don   t think there   s any reason it should be. in fact, many core ideas
   can be explained completely visually!

visualizing id203 distributions

   before we dive into id205, let   s think about how we can
   visualize simple id203 distributions. we   ll need this later on,
   and it   s convenient to address now. as a bonus, these tricks for
   visualizing id203 are pretty useful in and of themselves!

   i   m in california. sometimes it rains, but mostly there   s sun! let   s
   say it   s sunny 75% of the time. it   s easy to make a picture of that:

   most days, i wear a t-shirt, but some days i wear a coat. let   s say i
   wear a coat 38% of the time. it   s also easy to make a picture for that!

   what if i want to visualize both at the same time? we   ll, it   s easy if
   they don   t interact     if they   re what we call independent. for example,
   whether i wear a t-shirt or a raincoat today doesn   t really interact
   with what the weather is next week. we can draw this by using one axis
   for one variable and one for the other:

   notice the straight vertical and horizontal lines going all the way
   through. that   s what independence looks like! [5]^1 the id203 i   m
   wearing a coat doesn   t change in response to the fact that it will be
   raining in a week. in other words, the id203 that i   m wearing a
   coat and that it will rain next week is just the id203 that i   m
   wearing a coat, times the id203 that it will rain. they don   t
   interact.

   when variables interact, there   s extra id203 for particular pairs
   of variables and missing id203 for others. there   s extra
   id203 that i   m wearing a coat and it   s raining because the
   variables are correlated, they make each other more likely. it   s more
   likely that i   m wearing a coat on a day that it rains than the
   id203 i wear a coat on one day and it rains on some other random
   day.

   visually, this looks like some of the squares swelling with extra
   id203, and other squares shrinking because the pair of events is
   unlikely together:

   but while that might look kind of cool, it   s isn   t very useful for
   understanding what   s going on.

   instead, let   s focus on one variable like the weather. we know how
   probable it is that it   s sunny or raining. for both cases, we can look
   at the conditional probabilities. how likely am i to wear a t-shirt if
   it   s sunny? how likely am i to wear a coat if it   s raining?

   there   s a 25% chance that it   s raining. if it is raining, there   s a 75%
   chance that i   d wear a coat. so, the id203 that it is raining and
   i   m wearing a coat is 25% times 75% which is approximately 19%. the
   id203 that it   s raining and i   m wearing a coat is the id203
   that it is raining, times the id203 that i   d wear a coat if it is
   raining. we write this:

   \[p(\text{rain}, \text{coat}) = p(\text{rain}) \cdot p(\text{coat} ~|~
   \text{rain})\]

   this is a single case of one of the most fundamental identities of
   id203 theory:

   \[p(x,y) = p(x)\cdot p(y|x)\]

   we   re factoring the distribution, breaking it down into the product of
   two pieces. first we look at the id203 that one variable, like
   the weather, will take on a certain value. then we look at the
   id203 that another variable, like my clothing, will take on a
   certain value conditioned on the first variable.

   the choice of which variable to start with is arbitrary. we could just
   as easily start by focusing on my clothing and then look at the weather
   conditioned on it. this might feel a bit less intuitive, because we
   understand that there   s a causal relationship of the weather
   influencing what i wear and not the other way around    but it still
   works!

   let   s go through an example. if we pick a random day, there   s a 38%
   chance that i   d be wearing a coat. if we know that i   m wearing a coat,
   how likely is it that it   s raining? well, i   m more likely to wear a
   coat in the rain than in the sun, but rain is kind of rare in
   california, and so it works out that there   s a 50% chance that it   s
   raining. and so, the id203 that it   s raining and i   m wearing a
   coat is the id203 that i   m wearing a coat (38%), times the
   id203 that it would be raining if i was wearing a coat (50%)
   which is approximately 19%.

   \[p(\text{rain}, \text{coat}) = p(\text{coat}) \cdot p(\text{rain} ~|~
   \text{coat})\]

   this gives us a second way to visualize the exact same id203
   distribution.

   note that the labels have slightly different meanings than in the
   previous diagram: t-shirt and coat are now marginal probabilities, the
   id203 of me wearing that clothing without consideration of the
   weather. on the other hand, there are now two rain and sunny labels,
   for the probabilities of them conditional on me wearing a t-shirt and
   me wearing a coat respectively.

   (you may have heard of bayes    theorem. if you want, you can think of it
   as the way to translate between these two different ways of displaying
   the id203 distribution!)

aside: simpson   s paradox

   are these tricks for visualizing id203 distributions actually
   helpful? i think they are! it will be a little while before we use them
   for visualizing id205, so i   d like to go on a little
   tangent and use them to explore simpson   s paradox. simpson   s paradox is
   an extremely unintuitive statistical situation. it   s just really hard
   to understand at an intuitive level. michael nielsen wrote a lovely
   essay, [6]reinventing explanation, which explored different ways to
   explain it. i   d like to try and take my own shot at it, using the
   tricks we developed in the previous section.

   two treatments for kidney stones are tested. half the patients are
   given treatment a while the other half are given treatment b. the
   patients who received treatment b were more likely to survive than
   those who received treatment a.

   however, patients with small kidney stones were more likely to survive
   if they took treatment a. patients with large kidney stones were also
   more likely to survive if they took treatment a! how can this be?

   the core of the issue is that the study wasn   t properly randomized. the
   patients who received treatment a were likely to have large kidney
   stones, while the patients who received treatment b were more likely to
   have small kidney stones.

   as it turns out, patients with small kidney stones are much more likely
   to survive in general.

   to understand this better, we can combine the two previous diagrams.
   the result is a 3-dimensional diagram with the survival rate split
   apart for small and large kidney stones.

   we can now see that in both the small case and the large case,
   treatment a beats treatment b. treatment b only seemed better because
   the patients it was applied to were more likely to survive in the first
   place!

codes

   now that we have ways of visualizing id203, we can dive into
   id205.

   let me tell you about my imaginary friend, bob. bob really likes
   animals. he constantly talks about animals. in fact, he only ever says
   four words:    dog   ,    cat   ,    fish    and    bird   .

   a couple weeks ago, despite being a figment of my imagination, bob
   moved to australia. further, he decided he only wanted to communicate
   in binary. all my (imaginary) messages from bob look like this:

   to communicate, bob and i have to establish a code, a way of mapping
   words into sequences of bits.

   to send a message, bob replaces each symbol (word) with the
   corresponding codeword, and then concatenates them together to form the
   encoded string.

variable-length codes

   unfortunately, communication services in imaginary-australia are
   expensive. i have to pay $5 per bit of every message i receive from
   bob. have i mentioned that bob likes to talk a lot? to prevent me from
   going bankrupt, bob and i decided we should investigate whether there
   was some way we could make our average message length shorter.

   as it turns out bob doesn   t say all words equally often. bob really
   likes dogs. he talks about dogs all the time. on occasion, he   ll talk
   about other animals     especially the cat his dog likes to chase     but
   mostly he talks about dogs. here   s a graph of his word frequency:

   that seems promising. our old code uses codewords that are 2 bits long,
   regardless of how common they are.

   there   s a nice way to visualize this. in the following diagram, we use
   the vertical axis to visualize the id203 of each word, \(p(x)\),
   and the horizontal axis to visualize the length of the corresponding
   codeword, \(l(x)\). notice that the area is the average length of a
   codeword we send     in this case 2 bits.

   perhaps we could be very clever and make a variable-length code where
   codewords for common words are made especially short. the challenge is
   that there   s competition between codewords     making some shorter forces
   us to make others longer. to minimize the message length, we   d ideally
   like all codewords to be short, but we especially want the commonly
   used ones to be. so the resulting code has shorter codewords for common
   words (like    dog   ) and longer codewords for less common words (like
      bird   ).

   let   s visualize this again. notice that the most common codeword became
   shorter, even as the uncommon ones became longer. the result was, on
   net, a smaller amount of area. this corresponds to a smaller expected
   codeword length. on average, the length of a codeword is now 1.75 bits!

   (you may wonder: why not use 1 by itself as a codeword? sadly, this
   would cause ambiguity when we decode encoded strings. we   ll talk about
   this more shortly.)

   it turns out that this code is the best possible code. there is no code
   which, for this distribution, will give us an average codeword length
   of less than 1.75 bits.

   there is simply a fundamental limit. communicating what word was said,
   what event from this distribution occurred, requires us to communicate
   at least 1.75 bits on average. no matter how clever our code, it   s
   impossible to get the average message length to be less. we call this
   fundamental limit the id178 of the distribution     we   ll discuss it in
   much more detail shortly.

   if we want to understand this limit, the crux of the matter is
   understanding the trade off between making some codewords short and
   others long. once we understand that, we   ll be able to understand what
   the best possible codes are like.

the space of codewords

   there are two codes with a length of 1 bit: 0 and 1. there are four
   codes with a length of 2 bits: 00, 01, 10, and 11. every bit you add on
   doubles the number of possible codes.

   we   re interested in variable-length codes, where some codewords are
   longer than others. we might have simple situations where we have eight
   codewords that are 3 bits long. we might also have more complicated
   mixtures, like two codewords of length 2, and four codewords of length
   3. what decides how many codewords we can have of different lengths?

   recall that bob turns his messages into encoded strings by replacing
   each word with its codeword and concatenating them.

   there   s a slightly subtle issue one needs to be careful of, when
   crafting a variable length code. how do we split the encoded string
   back into the codewords? when all the codewords are the same length,
   it   s easy     just split the string every couple of steps. but since
   there are codewords of different lengths, we need to actually pay
   attention to the content.

   we really want our code to be uniquely decodable, with only one way to
   decode an encoded string. we never want it to be ambiguous which
   codewords make up the encoded string. if we had some special    end of
   codeword    symbol, this would be easy.[7]^2 but we don   t     we   re only
   sending 0s and 1s. we need to be able to look at a sequence of
   concatenated codewords and tell where each one stops.

   it   s very possible to make codes that aren   t uniquely decodable. for
   example, imagine that 0 and 01 were both codewords. then it would be
   unclear what the first codeword of the encoded string 0100111 is     it
   could be either! the property we want is that if we see a particular
   codeword, there shouldn   t be some longer version that is also a
   codeword. another way of putting this is that no codeword should be the
   prefix of another codeword. this is called the prefix property, and
   codes that obey it are called prefix codes.

   one useful way to think about this is that every codeword requires a
   sacrifice from the space of possible codewords. if we take the codeword
   01, we lose the ability to use any codewords it   s a prefix of. we can   t
   use 010 or 011010110 anymore because of ambiguity     they   re lost to us.

   since a quarter of all codewords start with 01, we   ve sacrificed a
   quarter of all possible codewords. that   s the price we pay in exchange
   for having one codeword that   s only 2 bits long! in turn this sacrifice
   means that all the other codewords need to be a bit longer. there   s
   always this sort of trade off between the lengths of the different
   codewords. a short codeword requires you to sacrifice more of the space
   of possible codewords, preventing other codewords from being short.
   what we need to figure out is what the right trade off to make is!

optimal encodings

   you can think of this like having a limited budget to spend on getting
   short codewords. we pay for one codeword by sacrificing a fraction of
   possible codewords.

   the cost of buying a codeword of length 0 is 1, all possible codewords
       if you want to have a codeword of length 0, you can   t have any other
   codeword. the cost of a codeword of length 1, like    0   , is 1/2 because
   half of possible codewords start with    0   . the cost of a codeword of
   length 2, like    01   , is 1/4 because a quarter of all possible codewords
   start with    01   . in general, the cost of codewords decreases
   exponentially with the length of the codeword.

   note that if the cost decays as a (natural) exponential, it is both the
   height and the area![8]^3

   we want short codewords because we want short average message lengths.
   each codeword makes the average message length longer by its
   id203 times the length of the codeword. for example, if we need
   to send a codeword that is 4 bits long 50% of the time, our average
   message length is 2 bits longer than it would be if we weren   t sending
   that codeword. we can picture this as a rectangle.

   these two values are related by the length of the codeword. the amount
   we pay decides the length of the codeword. the length of the codeword
   controls how much it adds to the average message length. we can picture
   the two of these together, like so.

   short codewords reduce the average message length but are expensive,
   while long codewords increase the average message length but are cheap.

   what   s the best way to use our limited budget? how much should we spend
   on the codeword for each event?

   just like one wants to invest more in tools that one uses regularly, we
   want to spend more on frequently used codewords. there   s one
   particularly natural way to do this: distribute our budget in
   proportion to how common an event is. so, if one event happens 50% of
   the time, we spend 50% of our budget buying a short codeword for it.
   but if an event only happens 1% of the time, we only spend 1% of our
   budget, because we don   t care very much if the codeword is long.

   that   s a pretty natural thing to do, but is it the optimal thing to do?
   it is, and i   ll prove it!

   the following proof is visual and should be accessible, but will take
   work to get through and is definitely the hardest part of this essay.
   readers should feel free to skip to accept this as a given and jump to
   the next section.

   let   s picture a concrete example where we need to communicate which of
   two possible events happened. event \(a\) happens \(p(a)\) of the time
   and event \(b\) happens \(p(b)\) of the time. we distribute our budget
   in the natural way described above, spending \(p(a)\) of our budget on
   getting \(a\) a short codeword, and \(p(b)\) on getting \(b\) a short
   codeword.

   the cost and length contribution boundaries nicely line up. does that
   mean anything?

   well, consider what happens to the cost and the length contribution if
   we slightly change the length of the codeword. if we slightly increase
   the length of the codeword, the message length contribution will
   increase in proportion to its height at the boundary, while the cost
   will decrease in proportion to its height at the boundary.

   so, the cost to make the codeword for \(a\) shorter is \(p(a)\). at the
   same time, we don   t care about the length of each codeword equally, we
   care about them in proportion to how much we have to use them. in the
   case of \(a\), that is \(p(a)\). the benefit to us of making the
   codeword for \(a\) a bit shorter is \(p(a)\).

   it   s interesting that both derivatives are the same. it means that our
   initial budget has the interesting property that, if you had a bit more
   to spend, it would be equally good to invest in making any codeword
   shorter. what we really care about, in the end, is the benefit/cost
   ratio     that   s what decides what we should invest more in. in this
   case, the ratio is \(\frac{p(a)}{p(a)}\), which is equal to one. this
   is independent of the value of \(p(a)\)     it   s always one. and we can
   apply the same argument to other events. the benefit/cost is always
   one, so it makes equal sense to invest a bit more in any of them.

   infinitesimally, it doesn   t make sense to change the budget. but that
   isn   t a proof that it   s the best budget. to prove that, we   ll consider
   a different budget, where we spend a bit extra on one codeword at the
   expense of another. we   ll invest \(\epsilon\) less in \(b\), and invest
   it in \(a\) instead. this makes the codeword for \(a\) a bit shorter,
   and the codeword for \(b\) a bit longer.

   now the cost of buying a shorter codeword for \(a\) is \(p(a) +
   \epsilon\), and the cost of buying a shorter codeword for \(b\) is
   \(p(b) - \epsilon\). but the benefits are still the same. this leads
   the benefit cost ratio for buying \(a\) to be \(\frac{p(a)}{p(a) +
   \epsilon}\) which is less than one. on the other hand, the benefit cost
   ratio of buying \(b\) is \(\frac{p(b)}{p(b) - \epsilon}\) which is
   greater than one.

   the prices are no longer balanced. \(b\) is a better deal than \(a\).
   the investors scream:    buy \(b\)! sell \(a\)!    we do this, and end back
   at our original budget plan. all budgets can be improved by shifting
   towards our original plan.

   the original budget     investing in each codeword in proportion to how
   often we use it     wasn   t just the natural thing to do, it was the
   optimal thing to do. (while this proof only works for two codewords, it
   easily generalizes to more.)

   (a careful reader may have noticed that it is possible for our optimal
   budget to suggest codes where codewords have fractional lengths. that
   seems pretty concerning! what does it mean? well, of course, in
   practice, if you want to communicate by sending a single codeword, you
   have to round. but as we   ll see later, there   s a very real sense in
   which it is possible to send fractional codewords when we send many at
   a time! i   ll ask you be patient with me on this for now!)

calculating id178

   recall that the cost of a message of length \(l\) is \(\frac{1}{2^l}\).
   we can invert this to get the length of a message that costs a given
   amount: \(\log_2\left(\frac{1}{\text{cost}}\right)\). since we spend
   \(p(x)\) on a codeword for \(x\), the length is
   \(\log_2\left(\frac{1}{p(x)}\right)\). those are the best choices of
   lengths.

   earlier, we discussed how there is a fundamental limit to how short one
   can get the average message to communicate events from a particular
   id203 distribution, \(p\). this limit, the average message length
   using the best possible code, is called the id178 of \(p\), \(h(p)\).
   now that we know the optimal lengths of the codewords, we can actually
   calculate it!

   \[h(p) = \sum_x p(x)\log_2\left(\frac{1}{p(x)}\right)\]

   (people often write id178 as \(h(p) = - \sum p(x)\log_2(p(x))\) using
   the identity \(\log(1/a) = -\log(a)\). i think the first version is
   more intuitive, and will continue to use it in this essay.)

   no matter what i do, on average i need to send at least that number of
   bits if i want to communicate which event occurred.

   the average amount of information needed to communicate something has
   clear implications for compression. but are there other reasons we
   should care about it? yes! it describes how uncertain i am and gives a
   way to quantify information.

   if i knew for sure what was going to happen, i wouldn   t have to send a
   message at all! if there   s two things that could happen with 50%
   id203, i only need to send 1 bit. but if there   s 64 different
   things that could happen with equal id203, i   d have to send 6
   bits. the more concentrated the id203, the more i can craft a
   clever code with short average messages. the more diffuse the
   id203, the longer my messages have to be.

   the more uncertain the outcome, the more i learn, on average, when i
   find out what happened.

cross-id178

   shortly before his move to australia, bob married alice, another
   figment of my imagination. to the surprise of myself, and also the
   other characters in my head, alice was not a dog lover. she was a cat
   lover. despite this, the two of them were able to find common ground in
   their shared obsession with animals and very limited vocabulary size.

   the two of them say the same words, just at different frequencies. bob
   talks about dogs all the time, alice talks about cats all the time.

   initially, alice sent me messages using bob   s code. unfortunately, her
   messages were longer than they needed to be. bob   s code was optimized
   to his id203 distribution. alice has a different id203
   distribution, and the code is suboptimal for it. while the average
   length of a codeword when bob uses his own code is 1.75 bits, when
   alice uses his code it's 2.25. it would be worse if the two weren   t so
   similar!

   this length     the average length of communicating an event from one
   distribution with the optimal code for another distribution     is called
   the cross-id178. formally, we can define cross-id178 as:[9]^4
   \[h_p(q) = \sum_x q(x)\log_2\left(\frac{1}{p(x)}\right)\]

   in this case, it   s the cross-id178 of alice the cat-lovers word
   frequency with respect to the bob the dog-lovers word frequency.

   to keep the cost of our communications down, i asked alice to use her
   own code. to my relief, this pushed down her average message length.
   but it introduced a new problem: sometimes bob would accidentally use
   alice   s code. surprisingly, it   s worse for bob to accidentally use
   alice's code than for alice to use his!

   so, now we have four possibilities:
     * bob using his own code \((h(p) = 1.75 ~\text{bits})\)
     * alice using bob   s code \((h_p(q) = 2.25 ~\text{bits})\)
     * alice using her own code \((h(q) = 1.75 ~\text{bits})\)
     * bob using alice   s code \((h_q(p) = 2.375 ~\text{bits})\)

   this isn   t necessarily as intuitive as one might think. for example, we
   can see that \(h_p(q) \neq h_q(p)\). is there some way we can see how
   these four values relate to each other?

   in the following diagram, each subplot represents one of these 4
   possibilities. each subplot visualizes average message length the same
   way our previous diagrams did. they are organized in a square, so that
   if the messages are coming from the same distribution the plots are
   beside each other, and if they use the same codes they are on top of
   each other. this allows you to kind of visually slide the distributions
   and codes together.

   can you see why \(h_p(q) \neq h_q(p)\)? \(h_q(p)\) is large because
   there is an event (blue) which is very common under \(p\) but gets a
   long code because it is very uncommon under \(q\). on the other hand,
   common events under \(q\) are less common under \(p\), but the
   difference is less drastic, so \(h_p(q)\) isn   t as high.

   cross-id178 isn   t symmetric.

   so, why should you care about cross-id178? well, cross-id178 gives
   us a way to express how different two id203 distributions are.
   the more different the distributions \(p\) and \(q\) are, the more the
   cross-id178 of \(p\) with respect to \(q\) will be bigger than the
   id178 of \(p\).

   similarly, the more different \(p\) is from \(q\), the more the
   cross-id178 of \(q\) with respect to \(p\) will be bigger than the
   id178 of \(q\).

   the really interesting thing is the difference between the id178 and
   the cross-id178. that difference is how much longer our messages are
   because we used a code optimized for a different distribution. if the
   distributions are the same, this difference will be zero. as the
   difference grows, it will get bigger.

   we call this difference the kullback   leibler divergence, or just the kl
   divergence. the kl divergence of \(p\) with respect to \(q\),
   \(d_q(p)\),[10]^5 is defined:[11]^6

   \[d_q(p) = h_q(p) - h(p)\]

   the really neat thing about kl divergence is that it   s like a distance
   between two distributions. it measures how different they are! (if you
   take that idea seriously, you end up with information geometry.)

   cross-id178 and kl divergence are incredibly useful in machine
   learning. often, we want one distribution to be close to another. for
   example, we might want a predicted distribution to be close to the
   ground truth. kl divergence gives us a natural way to do this, and so
   it shows up everywhere.

id178 and multiple variables

   let   s return to our weather and clothing example from earlier:

   my mother, like many parents, sometimes worries that i don   t dress
   appropriately for the weather. (she has reasonable cause for suspicion
       i have often failed to wear coats in winter.) so, she often wants to
   know both the weather and what clothing i   m wearing. how many bits do i
   have to send her to communicate this?

   well, the easy way to think about this is to flatten the id203
   distribution:

   now we can figure out the optimal codewords for events of these
   probabilities and compute the average message length:

   we call this the joint id178 of \(x\) and \(y\), defined

   \[h(x,y) = \sum_{x,y} p(x,y) \log_2\left(\frac{1}{p(x,y)}\right)\]

   this is the exact same as our normal definition, except with two
   variables instead of one.

   a slightly nicer way to think about this is to avoid flattening the
   distribution, and just think of the code lengths as a third dimension.
   now the id178 is the volume!

   but suppose my mom already knows the weather. she can check it on the
   news. now how much information do i need to provide?

   it seems like i need to send however much information i need to
   communicate the clothes i   m wearing. but i actually need to send less,
   because the weather strongly implies what clothing i   ll wear! let   s
   consider the case where it   s raining and where it   s sunny separately.

   in both cases, i don   t need to send very much information on average,
   because the weather gives me a good guess at what the right answer will
   be. when it   s sunny, i can use a special sunny-optimized code, and when
   it   s raining i can use a raining optimized code. in both cases, i send
   less information than if i used a generic code for both. to get the
   average amount of information i need to send my mother, i just put
   these two cases together   

   we call this the conditional id178. if you formalize it into an
   equation, you get:

   \[h(x|y) = \sum_y p(y) \sum_x p(x|y)
   \log_2\left(\frac{1}{p(x|y)}\right)\] \[~~~~ = \sum_{x,y} p(x,y)
   \log_2\left(\frac{1}{p(x|y)}\right)\]

mutual information

   in the previous section, we observed that knowing one variable can mean
   that communicating another variable requires less information.

   one nice way to think about this is to imagine amounts of information
   as bars. these bars overlap if there   s shared information between them.
   for example, some of the information in \(x\) and \(y\) is shared
   between them, so \(h(x)\) and \(h(y)\) are overlapping bars. and since
   \(h(x,y)\) is the information in both, it   s the union of the bars
   \(h(x)\) and \(h(y)\).[12]^7

   once we think about things this way, a lot of things become easier to
   see.

   for example, we previously noted it takes more information to
   communicate both \(x\) and \(y\) (the    joint id178,    \(h(x,y)\)) than
   it takes to just communicate \(x\) (the    marginal id178,    \(h(x)\)).
   but if you already know \(y\), then it takes less information to
   communicate \(x\) (the    conditional id178,    \(h(x|y)\)) than it would
   if you didn   t!

   that sounds a bit complicated, but it   s very simple when we think about
   it from the bar perspective. \(h(x|y)\) is the information we need to
   send to communicate \(x\) to someone who already knows \(y\), the
   information in \(x\) which isn   t also in \(y\). visually, that means
   \(h(x|y)\) is the part of \(h(x)\) bar which doesn   t overlap with
   \(h(y)\).

   you can now read the inequality \(h(x,y) \geq h(x) \geq h(x|y)\) right
   off the following diagram.

   another identity is that \(h(x,y) = h(y) + h(x|y)\). that is, the
   information in \(x\) and \(y\) is the information in \(y\) plus the
   information in \(x\) which is not in \(y\).

   again, it   s difficult to see in the equations, but easy to see if
   you   re thinking in terms of these overlapping bars of information.

   at this point, we   ve broken the information in \(x\) and \(y\) up in
   several ways. we have the information in each variable, \(h(x)\) and
   \(h(y)\). we have the the union of the information in both, \(h(x,y)\).
   we have the information which is in one but not the other, \(h(x|y)\)
   and \(h(y|x)\). a lot of this seems to revolve around the information
   shared between the variables, the intersection of their information. we
   call this    mutual information,    \(i(x,y)\), defined as:[13]^8 \[i(x,y)
   = h(x) + h(y) - h(x,y)\] this definition works because \(h(x) + h(y)\)
   has two copies of the mutual information, since it   s in both \(x\) and
   \(y\), while \(h(x,y)\) only has one. (consider the previous bar
   diagram.)

   closely related to the mutual information is the variation of
   information. the variation of information is the information which
   isn   t shared between the variables. we can define it like so: \[v(x,y)
   = h(x,y) - i(x,y)\] variation of information is interesting because it
   gives us a metric, a notion of distance, between different variables.
   the variation of information between two variables is zero if knowing
   the value of one tells you the value of the other and increases as they
   become more independent.

   how does this relate to kl divergence, which also gave us a notion of
   distance? well, kl divergence gives us a distance between two
   distributions over the same variable or set of variables. in contrast,
   variation of information gives us distance between two jointly
   distributed variables. kl divergence is between distributions,
   variation of information within a distribution.

   we can bring this all together into a single diagram relating all these
   different kinds of information:

fractional bits

   a very unintuitive thing about id205 is that we can have
   fractional numbers of bits. that   s pretty weird. what does it mean to
   have half a bit?

   here   s the easy answer: often, we   re interested in the average length
   of a message rather than any particular message length. if half the
   time one sends a single bit, and half the time one sends two bits, on
   average one sends one and a half bits. there   s nothing strange about
   averages being fractional.

   but that answer is really dodging the issue. often, the optimal lengths
   of codewords are fractional. what do those mean?

   to be concrete, let   s consider a id203 distribution where one
   event, \(a\), happens 71% of the time and another event, \(b\), occurs
   29% of the time.

   the optimal code would use 0.5 bits to represent \(a\), and 1.7 bits to
   represent \(b\). well, if we want to send a single one of these
   codewords, it simply isn   t possible. we   re forced to round to a whole
   number of bits, and send on average 1 bit.

       but if we   re sending multiple messages at once, it turns out that we
   can do better. let   s consider communicating two events from this
   distribution. if we sent them independently, we   d need to send two
   bits. can we do better?

   half the time, we need to communicate \(aa\), \(21\%\) of the time we
   need to send \(ab\) or \(ba\), and \(8\%\) of the time we need to
   communicate \(bb\). again, the ideal code involves fractional numbers
   of bits.

   if we round the codeword lengths, we   ll get something like this:

   this codes give us an average message length of 1.8 bits. that   s less
   than the 2 bits when we send them independently. another way of
   thinking of this is that we   re sending 0.9 bits on average for each
   event. if we were to send more events at once, it would become smaller
   still. as \(n\) tends to infinity, the overhead due to rounding our
   code would vanish, and the number of bits per codeword would approach
   the id178.

   further, notice that the ideal codeword length for \(a\) was 0.5 bits,
   and the ideal codeword length for \(aa\) was 1 bit. ideal codeword
   lengths add, even when they   re fractional! so, if we communicate a lot
   of events at once, the lengths will add.

   there is a very real sense in which one can have fractional numbers of
   bits of information, even if actual codes can only use whole numbers.

   (in practice, people use particular coding schemes which are efficient
   to different extents. [14]huffman coding, which is basically the kind
   of code we've sketched out here, doesn't handle fractional bits very
   gracefully -- you have to group symbols, like we did above, or use more
   complicated tricks to approach the id178 limit. [15]arithmetic coding
   is a bit different, but elegantly handles fractional bits to be
   asymptotically optimal.)

conclusion

   if we care about communicating in a minimum number of bits, these ideas
   are clearly fundamental. if we care about compressing data, information
   theory addresses the core questions and gives us the fundamentally
   right abstractions. but what if we don   t care     are they anything other
   than curiosities?

   ideas from id205 turn up in lots of contexts: machine
   learning, quantum physics, genetics, thermodynamics, and even gambling.
   practitioners in these fields typically don   t care about information
   theory because they want to compress information. they care because it
   has a compelling connection to their field. quantum entanglement can be
   described with id178.[16]^9 many results in statistical mechanics and
   thermodynamics can be derived by assuming maximum id178 about the
   things you don   t know.[17]^10 a gambler   s wins or losses are directly
   connected to kl divergence, in particular iterated setups.[18]^11

   id205 turns up in all these places because it offers
   concrete, principled formalizations for many things we need to express.
   it gives us ways of measuring and expressing uncertainty, how different
   two sets of beliefs are, and how much an answer to one question tells
   us about others: how diffuse id203 is, the distance between
   id203 distributions, and how dependent two variables are. are
   there alternative, similar ideas? sure. but the ideas from information
   theory are clean, they have really nice properties, and a principled
   origin. in some cases, they   re precisely what you care about, and in
   other cases they   re a convenient proxy in a messy world.

   machine learning is what i know best, so let   s talk about that for a
   minute. a very common kind of task in machine learning is
   classification. let   s say we want to look at a picture and predict
   whether it   s a picture of a dog or a cat. our model might say something
   like    there   s a 80% chance this image is a dog, and a 20% chance it   s a
   cat.    let   s say the correct answer is dog     how good or bad is it that
   we only said there was an 80% chance it was a dog? how much better
   would it have been to say 85%?

   this is an important question because we need some notion of how good
   or bad our model is, in order to optimize it to do well. what should we
   optimize? the correct answer really depends on what we   re using the
   model for: do we only care about whether the top guess was right, or do
   we care about how confident we are in the correct answer? how bad is it
   to be confidently wrong? there isn   t one right answer to this. and
   often it isn   t possible to know the right answer, because we don   t know
   how the model will be used in a precise enough way to formalize what we
   ultimately care about. the result is that there are situations where
   cross-id178 really is precisely what we care about, but that isn   t
   always the case. much more often we don   t know exactly what we care
   about and cross-id178 is a really nice proxy.[19]^12

   information gives us a powerful new framework for thinking about the
   world. sometimes it perfectly fits the problem at hand; other times
   it   s not an exact fit, but still extremely useful. this essay has only
   scratched the surface of id205     there are major topics,
   like error-correcting codes, that we haven   t touched at all     but i
   hope i   ve shown that id205 is a beautiful subject that
   doesn   t need to be intimidating.

   to help me become a better writer, please consider filling out this
   [20]feedback form.

further reading

   claude shannon   s original paper on id205, [21]a
   mathematical theory of communication, is remarkably accessible. (this
   seems to be a recurring pattern in early id205 papers. was
   it the era? a lack of page limits? a culture emanating from bell labs?)

   cover & thomas    elements of id205 seems to be the standard
   reference. i found it helpful.

acknowledgments

   i   m very grateful to [22]dan man  , [23]david andersen, [24]emma pierson
   and dario amodei for taking time to give really incredibly detailed and
   extensive comments on this essay. i   m also grateful for the comments of
   [25]michael nielsen, [26]greg corrado, [27]yoshua bengio, [28]aaron
   courville, [29]nick beckstead, [30]jon shlens, andrew dai,
   [31]christian howard, and [32]martin wattenberg.

   thanks also to my first two neural network seminar series for acting as
   guinea pigs for these ideas.

   finally, thanks to the readers who caught errors and omissions. in
   particular, thanks to connor zwick, kai arulkumaran, jonathan heusser,
   otavio good, and an anonymous commenter.
     __________________________________________________________________

more posts

   [fig.png]

understanding convolutions

   [fig.png]

groups & group convolutions

   [topology.png]

neural networks, manifolds, and topology

   [fig.png]

visualizing representations

deep learning and human beings
     __________________________________________________________________

    1. it   s fun to use this to visualize naive bayesian classifiers, which
       assume independence   [33]   
    2. but horribly inefficient! if we have an extra symbol to use in our
       codes, only using it at the end of codewords like this would be a
       terrible waste.[34]   
    3. i   m cheating a little here. i   ve been using an exponential of base
       2 where this is not true, and am going to switch to a natural
       exponential. this saves us having a lot of \(log(2)\)s in our
       proof, and makes it visually a lot nicer.[35]   
    4. note that this notation for cross-id178 is non-standard. the
       normal notation is \(h(p,q)\). this notation is horrible for two
       reasons. firstly, the exact same notation is also used for joint
       id178. secondly, it makes it seem like cross-id178 is
       symmetric. this is ridiculous, and i   ll be writing \(h_q(p)\)
       instead.[36]   
    5. also non-standard notation.[37]   
    6. if you expand the definition of kl divergence, you get: \[d_q(p) =
       \sum_x p(x)\log_2\left(\frac{p(x)}{q(x)} \right)\] that might look
       a bit strange. how should we interpret it? well,
       \(\log_2\left(\frac{p(x)}{q(x)} \right)\) is just the difference
       between how many bits a code optimized for \(q\) and a code
       optimized for \(p\) would use to represent \(x\). the expression as
       a whole is the expected difference in how many bits the two codes
       would use.[38]   
    7. this builds off the set interpretation of id205 layed
       out in raymond w. yeung   s paper [39]a new outlook on shannon   s
       information measures.[40]   
    8. if you expand the definition of mutual information out, you get:
       \[i(x,y) = \sum_{x,y} p(x,y) \log_2\left(\frac{p(x,y)}{p(x)p(y)}
       \right)\]
       that looks suspiciously like kl divergence!
       what   s going on? well, it is kl divergence. it   s the kl divergence
       of p(x,y) and its naive approximation p(x)p(y). that is, it   s the
       number of bits you save representing \(x\) and \(y\) if you
       understand the relationship between them instead of assuming
       they   re independent.
       one cute way to visualize this is to literally picture the ratio
       between a distribution and its naive approximation:
       [41]   
    9. there   s an entire field of quantum id205. i know
       precisely nothing about the subject, but i   d bet with extremely
       high confidence, based on michael   s other work, that michael
       nielsen and issac chuang   s quantum computation and quantum
       information is an excellent introduction.[42]   
   10. as someone who knows nothing about statistical physics, i   ll very
       nervously try to sketch its connection to id205 as i
       understand it.
       after shannon discovered id205, many noted suspicious
       similarities between equations in thermodynamics and equations in
       id205. e.t. jaynes found a very deep and principled
       connection. suppose you have some system, and take some
       measurements like the pressure and temperature. how probable should
       you think a particular state of the system is? jaynes suggested we
       should assume the id203 distribution which, subject to the
       constraints of our measurement, maximizes the id178. (note that
       this    principle of maximum id178    is much more general than
       physics!) that is, we should assume the possibility with the most
       unknown information. many results can be derived from this
       perspective.
       (reading the first few sections of jaynes    papers ([43]part 1,
       [44]part 2) i was impressed by how accessible they seem.)
       if you   re interested in this connection but don   t want to work
       through the original papers, there   s a section in cover & thomas
       which derives a statistical version of the second law of
       thermodynamics from markov chains![45]   
   11. the connection between id205 and gambling was
       originally laid out by john kelly in his paper    [46]a new
       interpretation of information rate.    it   s a remarkably accessible
       paper, although it requires a few ideas we didn   t develop in this
       essay.
       kelly had an interesting motivation for his work. he noticed that
       id178 was being used in many cost functions which had no
       connection to encoding information and wanted some principled
       reason for it. in writing this essay, i   ve been troubled by the
       same thing, and have really appreciated kelly   s work as an
       additional perspective. that said, i don   t find it completely
       convincing: kelly only winds up with id178 because he considers
       iterated betting where one reinvests all their capital each bet.
       different setups do not lead to id178.
       a nice discussion of kelly   s connection between betting and
       id205 can be found in the standard reference on
       id205, cover & thomas       elements of information
       theory.   [47]   
   12. it doesn   t resolve the issue, but i can   t resist offering a small
       further defense of kl divergence.
       there   s a result which cover & thomas call stein   s lemma, although
       it seems unrelated to the result generally called stein   s lemma. at
       a high level, it goes like this:
       suppose you have some data which you know comes from one of two
       id203 distributions. how confidently can you determine which
       of the two distributions it came from? in general, as you get more
       data points, your confidence should increase exponentially. for
       example, on average you might become 1.5 times as confident about
       which distribution is the truth for every data point you see.
       how much your confidence gets multiplied depends on how different
       the distributions are. if they are very different, you might very
       quickly become confident. but if they   re only very slightly
       different, you might need to see lots of data before you have even
       a mildly confident answer.
       stein   s lemma says, roughly, that the amount you multiply by is
       controlled by the kl divergence. (there   s some subtlety about the
       trade off between false-positives and false-negatives.) this seems
       like a really good reason to care about kl divergence![48]   

   built by [49]oinkina with [50]hakyll using [51]bootstrap, [52]mathjax,
   [53]disqus, [54]mathbox.js, [55]highlight.js, and [56]footnotes.js.

   enable javascript for footnotes, disqus comments, and other cool stuff.

references

   visible links
   1. http://colah.github.io/
   2. http://colah.github.io/archive.html
   3. http://colah.github.io/about.html
   4. http://colah.github.io/contact.html
   5. http://colah.github.io/posts/2015-09-visual-information/#fn1
   6. http://michaelnielsen.org/reinventing_explanation/
   7. http://colah.github.io/posts/2015-09-visual-information/#fn2
   8. http://colah.github.io/posts/2015-09-visual-information/#fn3
   9. http://colah.github.io/posts/2015-09-visual-information/#fn4
  10. http://colah.github.io/posts/2015-09-visual-information/#fn5
  11. http://colah.github.io/posts/2015-09-visual-information/#fn6
  12. http://colah.github.io/posts/2015-09-visual-information/#fn7
  13. http://colah.github.io/posts/2015-09-visual-information/#fn8
  14. https://en.wikipedia.org/wiki/huffman_coding
  15. https://en.wikipedia.org/wiki/arithmetic_coding
  16. http://colah.github.io/posts/2015-09-visual-information/#fn9
  17. http://colah.github.io/posts/2015-09-visual-information/#fn10
  18. http://colah.github.io/posts/2015-09-visual-information/#fn11
  19. http://colah.github.io/posts/2015-09-visual-information/#fn12
  20. https://docs.google.com/forms/d/1zamvi-yl04gets7rngplz9tdgo5965gllddd50y2zni/viewform?usp=send_form
  21. http://worrydream.com/refs/shannon - a mathematical theory of communication.pdf
  22. https://github.com/danmane
  23. https://www.cs.cmu.edu/~dga/
  24. http://obsessionwithregression.blogspot.com/
  25. http://michaelnielsen.org/
  26. http://research.google.com/pubs/gregcorrado.html
  27. http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html
  28. https://aaroncourville.wordpress.com/
  29. http://www.nickbeckstead.com/
  30. http://research.google.com/pubs/jonathonshlens.html
  31. http://research.google.com/pubs/christianhoward.html
  32. http://www.bewitched.com/
  33. http://colah.github.io/posts/2015-09-visual-information/#fnref1
  34. http://colah.github.io/posts/2015-09-visual-information/#fnref2
  35. http://colah.github.io/posts/2015-09-visual-information/#fnref3
  36. http://colah.github.io/posts/2015-09-visual-information/#fnref4
  37. http://colah.github.io/posts/2015-09-visual-information/#fnref5
  38. http://colah.github.io/posts/2015-09-visual-information/#fnref6
  39. http://www.cnd.mcgill.ca/~ivan/it_ineq_script/raymond yeung papers/a new outlook on shannon   s information measures 00079902.pdf
  40. http://colah.github.io/posts/2015-09-visual-information/#fnref7
  41. http://colah.github.io/posts/2015-09-visual-information/#fnref8
  42. http://colah.github.io/posts/2015-09-visual-information/#fnref9
  43. http://bayes.wustl.edu/etj/articles/theory.1.pdf
  44. http://bayes.wustl.edu/etj/articles/theory.2.pdf
  45. http://colah.github.io/posts/2015-09-visual-information/#fnref10
  46. http://www.princeton.edu/~wbialek/rome/refs/kelly_56.pdf
  47. http://colah.github.io/posts/2015-09-visual-information/#fnref11
  48. http://colah.github.io/posts/2015-09-visual-information/#fnref12
  49. https://github.com/oinkina
  50. http://jaspervdj.be/hakyll
  51. http://getbootstrap.com/
  52. http://www.mathjax.org/
  53. http://disqus.com/
  54. https://github.com/unconed/mathbox.js
  55. http://highlightjs.org/
  56. http://ignorethecode.net/blog/2010/04/20/footnotes/

   hidden links:
  58. http://colah.github.io/posts/2014-07-understanding-convolutions/
  59. http://colah.github.io/posts/2014-12-groups-convolution/
  60. http://colah.github.io/posts/2014-03-nn-manifolds-topology/
  61. http://colah.github.io/posts/2015-01-visualizing-representations/
