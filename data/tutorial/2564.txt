   #[1]natural language processing blog - atom [2]natural language
   processing blog - rss [3]natural language processing blog - atom

   [4]skip to main | [5]skip to sidebar

[6]natural language processing blog

   my biased thoughts on the fields of natural language processing (nlp),
   computational linguistics (cl) and related topics (machine learning,
   math, funding, etc.)

26 july 2016

[7]decoding (neural?) representations

   i remember back in grad school days some subset of the field was
   thinking about the following question. i train an unsupervised id48 on
   some language data to get something-like-part-of-speech tags out. and
   naturally the question arises: these tags that come out... what are
   they actually encoding?
   at the time, there were essentially three ways of approaching this
   question that i knew about:
    1. do a head-to-head comparison, in which you build an offline
       matching between induced tags and "true" tags, and then evaluate
       the accuracy of that matching. this was the standard evaluation
       strategy for unsupervised id52, but is really just trying to
       get at the question of: how correlated are the induced tags with
       what we hope comes out.
    2. take a system that expects true pos tags and give it induced pos
       tags instead (at both training and test time). see how much it
       suffers (if at all). joshua goodman told me a few times (though i
       can't find his paper on this) that word clusters were just as good
       as pos tags if your task was ner.
    3. do something like #2, but also give the system both pos tags and
       induced tags, and see if the pos tags give you anything above and
       beyond the induced tags.

   now, ten years later since we're in "everything old is new again"
   phase, we're going through the same exercises, but with id27s
   instead of induced tags. this makes things slightly more complicated
   because it means that we have to have mechanisms that deal with
   continuous representations rather than discrete representations, but
   you basically see the same ideas floating around.
   in fact, of the above approaches, the only one that requires any
   modification is #1 because there's not an obvious way to do the
   matching. the alternative is to let a classifier do the matching,
   rather than an offline process. in particular, you take your
   embeddings, and then try to train a classifier that predicts pos tags
   from the embeddings directly. (note: i claim this generalizes #1
   because if you did this with discrete tags, the classifier would simply
   learn to do the matching that we used to compute "by hand" offline.) if
   your classifier can do a good job, then you're happy.
   this approach naturally has flaws (all do), but i think it's worth
   thinking about this seriously. to do so, we have to take a step back
   and ask ourselves: what are we trying to do? typically, it seems we
   want to make an argument that a system that was not (obviously)
   designed to encode some phenomenon (like pos tags) and was not trained
   (specifically) to predict that phenomenon has nonetheless managed to
   infer that structure. (we then typically go on to say something like
   "who needs no pos tags" even though we just demonstrated our belief
   that they're meaningful by evaluating them... but okay.)
   as a first observation, there is an entire field of study dedicated to
   answering questions like this: (psycho)linguists. admittedly they only
   answer questions like this in humans and not in machines, but if you've
   ever posed to yourself the question "do humans encode/represented
   phrase structures in their brains" and don't know the answer (or if
   you've never thought about this question!) then you should go talk to
   some linguists. more classical linguists would answer these questions
   with tests like, for instance, constituency tests or scoping tests. i
   like [8]colin phillips' [9]encyclopedia article on syntax for a gentle
   introduction (and is what i start with for syntax in intro nlp).
   so, as a starting point for "has my system learned x" we might ask our
   linguist friends how they determine if a human has learned x. some
   techniques are difficult to replicate in machine (e.g., eye movement
   experiments, though of course models that have something akin to
   alignment---or "attention" if you must---could be thought of as having
   something like eye movements, though i would be hesitant to take this
   analogy too far). but many are not, for instance behavioral
   experiments, analyzing errors, and, i hesitate somewhat to say it,
   grammaticality judgements.
   my second comment has to do with the notion of "can these encodings be
   used to predict pos tags." suppose the answer is "yes." what does that
   mean? suppose the answer is "no."
   in order to interpret the answer to these questions, we have to get a
   bit more formal. we're going to train a classifier to do something like
   "predict pos given embedding." okay, so what hypothesis space does that
   classifier have access to? perhaps you say it gets a linear hypothesis
   space, in which case i ask: if it fails, why is that useful? it just
   means that pos cannot be decoded linearly from this encoding. perhaps
   you make the hypothesis space outrageously complicated, in which case i
   ask: if it succeeds, what does that tell us?
   the reason i ask these questions is because i think it's useful to
   think about two extreme cases.
    1. we know that we can embed 200k words in about 300 dimensions with
       nearly orthogonal vectors. this means that for all intents and
       purposes, if we wanted, we could consider ourselves to be working
       with a one-hot word representation. we know that, to some degree,
       pos tags are predictable from words, especially if we allow for
       complex hypothesis spaces. but this is uninteresting because by any
       reasonable account, this representation has not encoded anything
       interesting: it's just the output classifier that's doing something
       interesting. that is to say: if your test can do well on the raw
       words as input, then it's dubious as a test.
    2. we also know that some things are just unpredictable. suppose i had
       a representation that perfectly encoded everything i could possibly
       want. but then in the "last layer" it got run through some
       encryption protocol. all of the information is still there, so the
       representation in some sense "contains" the pos tags, but no
       classifier is going to be able to extract it. that is to say, just
       because the encoded isn't on the "surface" doesn't mean it's not
       there. now, one could reasonably argue something like "well if the
       information is there in an impossible-to-decode format then it
       might as well not be there" but this slope gets slippery very
       quickly.

   currently, i much prefer to think about essentially the equivalent of
   "behavioral" experiments. for instance, if you're machine translating
   and want to know if your system can handle scoping, then give it a
   minimal pair to translate that only differs in the scoping of some
   negation. or if you're interesting in knowing whether it knows about
   pos tags, perhaps look at errors in its output and see if they fall
   along pos categories.
   edit 26 jul 2016, 8:24p eastern: it's unclear to a few people so
   clarification. i'm mostly not talking about type-level id27s
   above, but embeddings in context. at a type-level, you could imagine
   evaluating (1) on out of vocabular terms, which would be totally
   reasonable. i'm think more something like: the state of your bilstm in
   a neural mt system. the issue is that if, for instance, this bilstm can
   repredict the input (as in an autoencoder), then it could be that the
   pos tagger is doing all the work. see [10]this conversation thread with
   yoav goldberg for some discussion.

   posted by hal at [11]7/26/2016 02:42:00 pm

6 comments:

   [12]andr   martins said...
          "perhaps you make the hypothesis space outrageously complicated,
          in which case i ask: if it succeeds, what does that tell us?" --
          can't cross-validation solve this in a very simple way? a
          complex hypothesis space would have trouble generalizing to
          unseen id27s. of course, one can't still solve the
          "encrypted" case (which is as hard as solving decryption), but
          it seems feasible to spot simple correlations if we avoid
          overfitting.

          [13]26 july, 2016 17:11 [14][icon_delete13.gif]

   [15]hal said...
          this was apparently unclear so i just posted an update :).

          [16]26 july, 2016 18:26 [17][icon_delete13.gif]

   [18]andr   martins said...
          my argument above still works with token-based (context
          dependent) embeddings. suppose you have a sequence of word
          tokens x and some model (a bilstm if you like) that reads x and
          produces a sequence of embeddings e. in addition you have a
          sequence of gold pos tags y. then if you create splits e', y'
          and e'', y'' you can train a classifier that tries to predict y'
          from e' and evaluate it on the other split. (cv is even better
          since it can give a std deviation.) as long as there are no
          repeated sentences in the two splits this should be fine.

          [19]27 july, 2016 04:35 [20][icon_delete13.gif]

   [21]hal said...
          i think this is roughly what yoav was proposing. i still don't
          buy it, because if e' == x, you'll still do totally reasonably
          according to this metric, but e'==x could not really be claimed
          to "encode" pos.

          [22]27 july, 2016 07:45 [23][icon_delete13.gif]

   [24]andr   martins said...
          i see what you mean. i was thinking that the performance with e'
          := x' should be a lower bound (i'm assuming the pos classifier
          e' -> y' does independent decisions for every token, i.e., is
          not allowed to use the context as typical pos tagger) but this
          might still be a hard baseline to beat (actually, is it? does
          anyone ever compared the performance with e' = x' with that of a
          more reasonable e'? if e' really encodes contextual information
          it could beat this lower bound.)
          in any case, i think the fundamental problem here is exactly the
          same as in classical many-to-1 matching evaluations of
          unsupervised pos induction: unless one bounds the number of
          clusters (e.g. make it equal to the number of pos tags) each
          word might in the extreme case get its own singleton cluster
          (same situation as e' = x') which breaks the evaluation. one
          possibility is to try to emulate 1-to-1 matching -- for your
          example, this could mean something like:
          - having good accuracy predicting y from e
          - having good accuracy predicting e from y (with a squared loss,
          this would be something like the sum of the variances of the
          clusters whose centroids are the average embedding of all words
          for each pos tag).
          [25]27 july, 2016 09:21 [26][icon_delete13.gif]

   [27]hal said...
          yeah, that makes a lot of sense to me!

          [28]28 july, 2016 08:08 [29][icon_delete13.gif]

   [30]post a comment

   [31]newer post [32]older post [33]home
   subscribe to: [34]post comments (atom)

about me

   [35]my photo

   [36]hal

   [37]view my complete profile

labels

     * [38]acl (3)
     * [39]acs (2)
     * [40]advising (1)
     * [41]algorithms (2)
     * [42]bayesian (10)
     * [43]chunking (1)
     * [44]classification (1)
     * [45]id91 (3)
     * [46]community (26)
     * [47]conferences (45)
     * [48]coreference (1)
     * [49]data (2)
     * [50]discourse (3)
     * [51]id20 (5)
     * [52]evaluation (9)
     * [53]finite state methods (1)
     * [54]id114 (1)
     * [55]hiring (7)
     * [56]information retrieval (1)
     * [57]journals (3)
     * [58]id38 (1)
     * [59]linguistics (7)
     * [60]id168s (1)
     * [61]machine learning (45)
     * [62]machine translation (6)
     * [63]mcmc (1)
     * [64]news (4)
     * [65]online learning (2)
     * [66]papers (17)
     * [67]parsing (2)
     * [68]pl (1)
     * [69]poll (1)
     * [70]problems (12)
     * [71]questions (2)
     * [72]random (1)
     * [73]research (12)
     * [74]reviewing (2)
     * [75]sentiment (1)
     * [76]software (1)
     * [77]speech (1)
     * [78]statistics (3)
     * [79]id170 (5)
     * [80]summarization (4)
     * [81]survey (6)
     * [82]teaching (3)
     * [83]theory (1)
     * [84]topic models (1)

my blog list

     * [icon18_wrench_allbkg.png]
       [85]statistical modeling, causal id136, and social science
       [86]treatment interactions can be hard to estimate from data.
       4 hours ago
     * [icon18_wrench_allbkg.png]
       [87]computational complexity
       [88]cuckoo cycles
       1 day ago
     * [icon18_wrench_allbkg.png]
       [89]in theory
       [90]knuth prize to avi wigderson
       2 days ago
     * [icon18_wrench_allbkg.png]
       [91]daniel lemire's blog
       [92]science and technology links (march 30th 2019)
       6 days ago
     * [icon18_wrench_allbkg.png]
       [93]wadler's blog
       [94]pinker's thirteen rules for better writing
       1 week ago
     * [icon18_wrench_allbkg.png]
       [95]talking brains
       [96]postdoc positions in cognitive neuroscience of communication at
       the university of connecticut
       1 week ago
     * [icon18_wrench_allbkg.png]
       [97]the geomblog
       [98]on pc submissions at soda 2020
       1 week ago
     * [icon18_wrench_allbkg.png]
       [99]journal of statistical software
       [100]coclust: a python package for co-id91
       1 week ago
     * [icon18_wrench_allbkg.png]
       [101]what's new
       [102]uhlenbeck   s theorem on connections with small curvature
       2 weeks ago
     * [icon18_wrench_allbkg.png]
       [103]nuit blanche
       [104]ce soir, paris machine learning #5 season 6: explainable ai,
       unity challenge, ethical ai
       3 weeks ago
     * [icon18_wrench_allbkg.png]
       [105]machine learning (theory)
       [106]code submission should be encouraged but not compulsory
       5 weeks ago
     * [icon18_wrench_allbkg.png]
       [107]my biased coin
       [108]analco, sosa, soda post
       2 months ago
     * [icon18_wrench_allbkg.png]
       [109]gowers's weblog
       [110]how craig barton wishes he   d taught maths
       3 months ago
     * [icon18_wrench_allbkg.png]
       [111]the scala programming language
       [112]new course:    programming reactive systems   
       3 months ago
     * [icon18_wrench_allbkg.png]
       [113]my slice of pizza
       [114]thanksgiving hunger
       4 months ago
     * [icon18_wrench_allbkg.png]
       [115]earning my turns
       [116]august-september music
       6 months ago
     * [icon18_wrench_allbkg.png]
       [117]mathematics and computation
       [118]how to implement type theory in an hour
       7 months ago
     * [icon18_wrench_allbkg.png]
       [119]tombone's blog
       [120]deepfakes: ai-powered deception machines
       10 months ago
     * [icon18_wrench_allbkg.png]
       [121]tcs math - some mathematics of theoretical computer science
       [122]two pages of the book, stuck together
       11 months ago
     * [icon18_wrench_allbkg.png]
       [123]geeking with greg
       [124]two decades of amazon.com recommendations
       1 year ago
     * [icon18_wrench_allbkg.png]
       [125]logicomp
       [126]correctness by design vs. formal verification
       1 year ago
     * [icon18_wrench_allbkg.png]
       [127]xor's hammer
       [128]how is it even possible for a sailboat to sail into the wind?
       1 year ago
     * [icon18_wrench_allbkg.png]
       [129]michael nielsen
       [130]is there a tension between creativity and accuracy?
       1 year ago
     * [icon18_wrench_allbkg.png]
       [131]oddhead blog
       [132]algorithmic economics postdoc position at microsoft research,
       nyc
       3 years ago
     * [icon18_wrench_allbkg.png]
       [133]andy's math/cs page
       [134]making academic contacts (some thoughts for new researchers)
       4 years ago
     * [icon18_wrench_allbkg.png]
       [135]the statmt blog
       [136]easy parallel corpora from wikipedia
       4 years ago
     * [icon18_wrench_allbkg.png]
       [137]learning in vision
       [138]dual submissions -- busted!
       5 years ago
     * [icon18_wrench_allbkg.png]
       [139]webdiarios de motocicleta
       [140]presburger award
       6 years ago
     * [icon18_wrench_allbkg.png]
       [141]lingpipe blog
       [142]upgrading java classes with backward-compatible serialization
       8 years ago
     * [icon18_wrench_allbkg.png]
       [143]quantum algorithms
       [144]polynomial-time quantum algorithm for the simulation of
       chemical dynamics
       10 years ago
     * [icon18_wrench_allbkg.png]
       [145]mathematics weblog
       [146]a levels
       11 years ago
     * [icon18_wrench_allbkg.png]
       [147]structured learning
       [148]corrections to acl anthology urls
       11 years ago
     * [icon18_wrench_allbkg.png]
       [149]apperceptual
     * [icon18_wrench_allbkg.png]
       [150]mathematics weblog
     * [icon18_wrench_allbkg.png]
       [151]yw's machine learning blog
     * [icon18_wrench_allbkg.png]
       [152]undirected grad
     * [icon18_wrench_allbkg.png]
       [153]data wrangling
     * [icon18_wrench_allbkg.png]
       [154]ganesh swami
     * [icon18_wrench_allbkg.png]
       [155]mstatbiostat :
     * [icon18_wrench_allbkg.png]
       [156]the astrostat slog
     * [icon18_wrench_allbkg.png]
       [157]mainly data
     * [icon18_wrench_allbkg.png]
       [158]inductio ex machina
     * [icon18_wrench_allbkg.png]
       [159][lowerbounds, upperbounds]
     * [icon18_wrench_allbkg.png]
       [160]information engineering
     * [icon18_wrench_allbkg.png]
       [161]http://groundtruth.info/astrostat/slog/
     * [icon18_wrench_allbkg.png]
       [162]information retrieval
     * [icon18_wrench_allbkg.png]
       [163]bayesian analysis journal :: forthcoming articles

blog archive

     * [164]     [165]2018 (2)
          + [166]     [167]july (1)
          + [168]     [169]june (1)

     * [170]     [171]2017 (10)
          + [172]     [173]august (1)
          + [174]     [175]april (2)
          + [176]     [177]march (7)

     * [178]     [179]2016 (17)
          + [180]     [181]december (2)
          + [182]     [183]november (3)
          + [184]     [185]august (4)
          + [186]     [187]july (4)
               o [188]a quick comment on structured input vs structured
                 ...
               o [189]decoding (neural?) representations
               o [190]some picks from naacl 2016
               o [191]rating the quality of reviews, after the fact
          + [192]     [193]june (2)
          + [194]     [195]may (1)
          + [196]     [197]march (1)

     * [198]     [199]2015 (7)
          + [200]     [201]december (1)
          + [202]     [203]october (3)
          + [204]     [205]september (2)
          + [206]     [207]june (1)

     * [208]     [209]2014 (14)
          + [210]     [211]november (2)
          + [212]     [213]october (2)
          + [214]     [215]september (1)
          + [216]     [217]july (3)
          + [218]     [219]june (2)
          + [220]     [221]may (2)
          + [222]     [223]april (2)

     * [224]     [225]2013 (4)
          + [226]     [227]september (1)
          + [228]     [229]july (1)
          + [230]     [231]june (1)
          + [232]     [233]april (1)

     * [234]     [235]2012 (7)
          + [236]     [237]december (2)
          + [238]     [239]september (2)
          + [240]     [241]june (1)
          + [242]     [243]february (2)

     * [244]     [245]2011 (16)
          + [246]     [247]december (1)
          + [248]     [249]october (2)
          + [250]     [251]september (2)
          + [252]     [253]july (2)
          + [254]     [255]may (1)
          + [256]     [257]april (2)
          + [258]     [259]march (3)
          + [260]     [261]february (1)
          + [262]     [263]january (2)

     * [264]     [265]2010 (29)
          + [266]     [267]november (2)
          + [268]     [269]october (2)
          + [270]     [271]september (4)
          + [272]     [273]august (6)
          + [274]     [275]july (1)
          + [276]     [277]june (2)
          + [278]     [279]april (5)
          + [280]     [281]february (3)
          + [282]     [283]january (4)

     * [284]     [285]2009 (34)
          + [286]     [287]december (2)
          + [288]     [289]november (3)
          + [290]     [291]october (1)
          + [292]     [293]september (3)
          + [294]     [295]august (3)
          + [296]     [297]july (2)
          + [298]     [299]june (8)
          + [300]     [301]may (2)
          + [302]     [303]april (3)
          + [304]     [305]march (3)
          + [306]     [307]february (2)
          + [308]     [309]january (2)

     * [310]     [311]2008 (37)
          + [312]     [313]december (3)
          + [314]     [315]november (2)
          + [316]     [317]september (2)
          + [318]     [319]august (1)
          + [320]     [321]july (4)
          + [322]     [323]june (7)
          + [324]     [325]may (4)
          + [326]     [327]april (4)
          + [328]     [329]march (4)
          + [330]     [331]february (3)
          + [332]     [333]january (3)

     * [334]     [335]2007 (58)
          + [336]     [337]december (3)
          + [338]     [339]november (5)
          + [340]     [341]october (3)
          + [342]     [343]september (4)
          + [344]     [345]august (4)
          + [346]     [347]july (4)
          + [348]     [349]june (5)
          + [350]     [351]may (7)
          + [352]     [353]april (8)
          + [354]     [355]march (2)
          + [356]     [357]february (6)
          + [358]     [359]january (7)

     * [360]     [361]2006 (78)
          + [362]     [363]december (1)
          + [364]     [365]november (5)
          + [366]     [367]october (10)
          + [368]     [369]september (4)
          + [370]     [371]august (6)
          + [372]     [373]july (8)
          + [374]     [375]june (5)
          + [376]     [377]may (11)
          + [378]     [379]april (7)
          + [380]     [381]march (6)
          + [382]     [383]february (6)
          + [384]     [385]january (9)

     * [386]     [387]2005 (6)
          + [388]     [389]december (6)

references

   visible links
   1. https://nlpers.blogspot.com/feeds/posts/default
   2. https://nlpers.blogspot.com/feeds/posts/default?alt=rss
   3. https://nlpers.blogspot.com/feeds/7864834770955534476/comments/default
   4. https://nlpers.blogspot.com/2016/07/decoding-neural-representations.html#main
   5. https://nlpers.blogspot.com/2016/07/decoding-neural-representations.html#sidebar
   6. https://nlpers.blogspot.com/
   7. https://nlpers.blogspot.com/2016/07/decoding-neural-representations.html
   8. http://www.colinphillips.net/
   9. http://hal3.name/courses/2012f_cl1/out/phillips2003-syntax.pdf
  10. https://twitter.com/yoavgo/status/758051733595652096
  11. https://nlpers.blogspot.com/2016/07/decoding-neural-representations.html
  12. https://www.blogger.com/profile/07656348479195514232
  13. https://nlpers.blogspot.com/2016/07/decoding-neural-representations.html?showcomment=1469574670864#c1353095187980214751
  14. https://www.blogger.com/delete-comment.g?blogid=19803222&postid=1353095187980214751
  15. https://www.blogger.com/profile/02162908373916390369
  16. https://nlpers.blogspot.com/2016/07/decoding-neural-representations.html?showcomment=1469579216506#c2712312444421976312
  17. https://www.blogger.com/delete-comment.g?blogid=19803222&postid=2712312444421976312
  18. https://www.blogger.com/profile/07656348479195514232
  19. https://nlpers.blogspot.com/2016/07/decoding-neural-representations.html?showcomment=1469615723837#c1147262532964479630
  20. https://www.blogger.com/delete-comment.g?blogid=19803222&postid=1147262532964479630
  21. https://www.blogger.com/profile/02162908373916390369
  22. https://nlpers.blogspot.com/2016/07/decoding-neural-representations.html?showcomment=1469627114364#c6546775031065549722
  23. https://www.blogger.com/delete-comment.g?blogid=19803222&postid=6546775031065549722
  24. https://www.blogger.com/profile/07656348479195514232
  25. https://nlpers.blogspot.com/2016/07/decoding-neural-representations.html?showcomment=1469632874003#c3388589390317638336
  26. https://www.blogger.com/delete-comment.g?blogid=19803222&postid=3388589390317638336
  27. https://www.blogger.com/profile/02162908373916390369
  28. https://nlpers.blogspot.com/2016/07/decoding-neural-representations.html?showcomment=1469714917606#c4569954558395998304
  29. https://www.blogger.com/delete-comment.g?blogid=19803222&postid=4569954558395998304
  30. https://www.blogger.com/comment.g?blogid=19803222&postid=7864834770955534476
  31. https://nlpers.blogspot.com/2016/07/a-quick-comment-on-structured-input-vs.html
  32. https://nlpers.blogspot.com/2016/07/some-picks-from-naacl-2016.html
  33. https://nlpers.blogspot.com/
  34. https://nlpers.blogspot.com/feeds/7864834770955534476/comments/default
  35. https://www.blogger.com/profile/02162908373916390369
  36. https://www.blogger.com/profile/02162908373916390369
  37. https://www.blogger.com/profile/02162908373916390369
  38. https://nlpers.blogspot.com/search/label/acl
  39. https://nlpers.blogspot.com/search/label/acs
  40. https://nlpers.blogspot.com/search/label/advising
  41. https://nlpers.blogspot.com/search/label/algorithms
  42. https://nlpers.blogspot.com/search/label/bayesian
  43. https://nlpers.blogspot.com/search/label/chunking
  44. https://nlpers.blogspot.com/search/label/classification
  45. https://nlpers.blogspot.com/search/label/id91
  46. https://nlpers.blogspot.com/search/label/community
  47. https://nlpers.blogspot.com/search/label/conferences
  48. https://nlpers.blogspot.com/search/label/coreference
  49. https://nlpers.blogspot.com/search/label/data
  50. https://nlpers.blogspot.com/search/label/discourse
  51. https://nlpers.blogspot.com/search/label/id20
  52. https://nlpers.blogspot.com/search/label/evaluation
  53. https://nlpers.blogspot.com/search/label/finite state methods
  54. https://nlpers.blogspot.com/search/label/id114
  55. https://nlpers.blogspot.com/search/label/hiring
  56. https://nlpers.blogspot.com/search/label/information retrieval
  57. https://nlpers.blogspot.com/search/label/journals
  58. https://nlpers.blogspot.com/search/label/id38
  59. https://nlpers.blogspot.com/search/label/linguistics
  60. https://nlpers.blogspot.com/search/label/id168s
  61. https://nlpers.blogspot.com/search/label/machine learning
  62. https://nlpers.blogspot.com/search/label/machine translation
  63. https://nlpers.blogspot.com/search/label/mcmc
  64. https://nlpers.blogspot.com/search/label/news
  65. https://nlpers.blogspot.com/search/label/online learning
  66. https://nlpers.blogspot.com/search/label/papers
  67. https://nlpers.blogspot.com/search/label/parsing
  68. https://nlpers.blogspot.com/search/label/pl
  69. https://nlpers.blogspot.com/search/label/poll
  70. https://nlpers.blogspot.com/search/label/problems
  71. https://nlpers.blogspot.com/search/label/questions
  72. https://nlpers.blogspot.com/search/label/random
  73. https://nlpers.blogspot.com/search/label/research
  74. https://nlpers.blogspot.com/search/label/reviewing
  75. https://nlpers.blogspot.com/search/label/sentiment
  76. https://nlpers.blogspot.com/search/label/software
  77. https://nlpers.blogspot.com/search/label/speech
  78. https://nlpers.blogspot.com/search/label/statistics
  79. https://nlpers.blogspot.com/search/label/id170
  80. https://nlpers.blogspot.com/search/label/summarization
  81. https://nlpers.blogspot.com/search/label/survey
  82. https://nlpers.blogspot.com/search/label/teaching
  83. https://nlpers.blogspot.com/search/label/theory
  84. https://nlpers.blogspot.com/search/label/topic models
  85. https://statmodeling.stat.columbia.edu/
  86. http://feedproxy.google.com/~r/statisticalmodelingcausalid136andsocialscience/~3/j8rzuoh8s7c/
  87. https://blog.computationalcomplexity.org/
  88. https://blog.computationalcomplexity.org/2019/04/cuckoo-cycles.html
  89. https://lucatrevisan.wordpress.com/
  90. https://lucatrevisan.wordpress.com/2019/04/02/knuth-prize-to-avi-wigderson/
  91. https://lemire.me/blog
  92. http://feedproxy.google.com/~r/daniel-lemire/atom/~3/tixpi8tsv6i/
  93. http://wadler.blogspot.com/
  94. http://wadler.blogspot.com/2019/03/pinkers-thirteen-rules-for-better.html
  95. http://www.talkingbrains.org/
  96. http://feedproxy.google.com/~r/talkingbrains/~3/spyqgdvyz_k/postdoc-positions-in-cognitive.html
  97. http://blog.geomblog.org/
  98. http://feedproxy.google.com/~r/thegeomblog/~3/ptrxnvy99dg/on-pc-submissions-at-soda-2020.html
  99. https://www.jstatsoft.org/index.php/jss
 100. https://www.jstatsoft.org/index.php/jss/article/view/v088i07
 101. https://terrytao.wordpress.com/
 102. https://terrytao.wordpress.com/2019/03/19/uhlenbecks-theorem-on-connections-with-small-curvature/
 103. http://nuit-blanche.blogspot.com/
 104. http://feedproxy.google.com/~r/blogspot/wcedd/~3/9nkbroe_apc/ce-soir-paris-machine-learning-5-season.html
 105. http://hunch.net/
 106. http://hunch.net/?p=11377237
 107. http://mybiasedcoin.blogspot.com/
 108. http://mybiasedcoin.blogspot.com/2019/01/analco-sosa-soda-post.html
 109. https://gowers.wordpress.com/
 110. https://gowers.wordpress.com/2018/12/22/how-craig-barton-wishes-hed-taught-maths/
 111. http://www.scala-lang.org/news/
 112. http://www.scala-lang.org/news/2018/12/20/programming-reactive-systems-course.html
 113. http://mysliceofpizza.blogspot.com/
 114. http://mysliceofpizza.blogspot.com/2018/11/thanksgiving-hunger.html
 115. https://www.earningmyturns.org/
 116. https://www.earningmyturns.org/2018/09/august-september-music.html
 117. http://math.andrej.com/
 118. http://math.andrej.com/2018/08/25/how-to-implement-type-theory-in-an-hour/
 119. http://www.computervisionblog.com/
 120. http://www.computervisionblog.com/2018/05/deepfakes-ai-powered-deception-machines.html
 121. https://tcsmath.wordpress.com/
 122. https://tcsmath.wordpress.com/2018/04/12/two-pages-of-the-book-stuck-together/
 123. http://glinden.blogspot.com/
 124. http://feedproxy.google.com/~r/geekingwithgreg/~3/8hdwr6fyrns/two-decades-of-amazoncom-recommendations.html
 125. http://logicomp.blogspot.com/
 126. http://logicomp.blogspot.com/2017/06/correctness-by-design-vs-formal.html
 127. https://xorshammer.com/
 128. https://xorshammer.com/2017/05/29/how-is-it-even-possible-for-a-sailboat-to-sail-into-the-wind/
 129. http://michaelnielsen.org/blog
 130. http://feedproxy.google.com/~r/michaelnielsen/wmna/~3/lkqmyjndcbs/
 131. http://blog.oddhead.com/
 132. http://blog.oddhead.com/2015/10/25/algorithmic-economics-postdoc-msr-nyc/
 133. http://andysresearch.blogspot.com/
 134. http://andysresearch.blogspot.com/2014/10/making-academic-contacts-some-thoughts.html
 135. http://statmt.blogspot.com/
 136. http://statmt.blogspot.com/2014/09/easy-parallel-corpora-from-wikipedia.html
 137. http://vimsu99.blogspot.com/
 138. http://vimsu99.blogspot.com/2014/03/dual-submissions-busted.html
 139. http://infoweekly.blogspot.com/
 140. http://infoweekly.blogspot.com/2012/05/presburger-award.html
 141. https://lingpipe-blog.com/
 142. http://lingpipe-blog.com/2010/05/04/upgrading-java-classes-backward-compatible-serialization/
 143. http://qualgorithms.blogspot.com/
 144. http://qualgorithms.blogspot.com/2008/12/polynomial-time-quantum-algorithm-for.html
 145. http://www.sixthform.info/maths
 146. http://www.sixthform.info/maths/?p=166
 147. http://structlearn.blogspot.com/
 148. http://structlearn.blogspot.com/2007/07/corrections-to-acl-anthology-urls.html
 149. http://apperceptual.wordpress.com/feed/
 150. http://sixthform.info/maths/b2rss2.php
 151. http://mehve.org/ywml/atom.xml
 152. http://undirectedgrad.blogspot.com/feeds/posts/default
 153. http://www.datawrangling.com/feed/
 154. http://ergodicity.iamganesh.com/feed/atom/
 155. https://www.lists.utah.edu/wws/rss/latest_d_read/mstatbiostat?count=20&for=10
 156. http://groundtruth.info/astrostat/slog/feed/
 157. http://feeds.feedburner.com/mainlydata
 158. http://conflate.net/inductio/feed/
 159. http://magic.aladdin.cs.cmu.edu/feed/atom/
 160. http://clair.si.umich.edu:8080/wordpress/?feed=atom
 161. http://groundtruth.info/astrostat/slog/
 162. http://ciir.cs.umass.edu/~fdiaz/irblog/?feed=atom
 163. http://ba.stat.cmu.edu/forthcoming.xml
 164. javascript:void(0)
 165. https://nlpers.blogspot.com/2018/
 166. javascript:void(0)
 167. https://nlpers.blogspot.com/2018/07/
 168. javascript:void(0)
 169. https://nlpers.blogspot.com/2018/06/
 170. javascript:void(0)
 171. https://nlpers.blogspot.com/2017/
 172. javascript:void(0)
 173. https://nlpers.blogspot.com/2017/08/
 174. javascript:void(0)
 175. https://nlpers.blogspot.com/2017/04/
 176. javascript:void(0)
 177. https://nlpers.blogspot.com/2017/03/
 178. javascript:void(0)
 179. https://nlpers.blogspot.com/2016/
 180. javascript:void(0)
 181. https://nlpers.blogspot.com/2016/12/
 182. javascript:void(0)
 183. https://nlpers.blogspot.com/2016/11/
 184. javascript:void(0)
 185. https://nlpers.blogspot.com/2016/08/
 186. javascript:void(0)
 187. https://nlpers.blogspot.com/2016/07/
 188. https://nlpers.blogspot.com/2016/07/a-quick-comment-on-structured-input-vs.html
 189. https://nlpers.blogspot.com/2016/07/decoding-neural-representations.html
 190. https://nlpers.blogspot.com/2016/07/some-picks-from-naacl-2016.html
 191. https://nlpers.blogspot.com/2016/07/rating-quality-of-reviews-after-fact.html
 192. javascript:void(0)
 193. https://nlpers.blogspot.com/2016/06/
 194. javascript:void(0)
 195. https://nlpers.blogspot.com/2016/05/
 196. javascript:void(0)
 197. https://nlpers.blogspot.com/2016/03/
 198. javascript:void(0)
 199. https://nlpers.blogspot.com/2015/
 200. javascript:void(0)
 201. https://nlpers.blogspot.com/2015/12/
 202. javascript:void(0)
 203. https://nlpers.blogspot.com/2015/10/
 204. javascript:void(0)
 205. https://nlpers.blogspot.com/2015/09/
 206. javascript:void(0)
 207. https://nlpers.blogspot.com/2015/06/
 208. javascript:void(0)
 209. https://nlpers.blogspot.com/2014/
 210. javascript:void(0)
 211. https://nlpers.blogspot.com/2014/11/
 212. javascript:void(0)
 213. https://nlpers.blogspot.com/2014/10/
 214. javascript:void(0)
 215. https://nlpers.blogspot.com/2014/09/
 216. javascript:void(0)
 217. https://nlpers.blogspot.com/2014/07/
 218. javascript:void(0)
 219. https://nlpers.blogspot.com/2014/06/
 220. javascript:void(0)
 221. https://nlpers.blogspot.com/2014/05/
 222. javascript:void(0)
 223. https://nlpers.blogspot.com/2014/04/
 224. javascript:void(0)
 225. https://nlpers.blogspot.com/2013/
 226. javascript:void(0)
 227. https://nlpers.blogspot.com/2013/09/
 228. javascript:void(0)
 229. https://nlpers.blogspot.com/2013/07/
 230. javascript:void(0)
 231. https://nlpers.blogspot.com/2013/06/
 232. javascript:void(0)
 233. https://nlpers.blogspot.com/2013/04/
 234. javascript:void(0)
 235. https://nlpers.blogspot.com/2012/
 236. javascript:void(0)
 237. https://nlpers.blogspot.com/2012/12/
 238. javascript:void(0)
 239. https://nlpers.blogspot.com/2012/09/
 240. javascript:void(0)
 241. https://nlpers.blogspot.com/2012/06/
 242. javascript:void(0)
 243. https://nlpers.blogspot.com/2012/02/
 244. javascript:void(0)
 245. https://nlpers.blogspot.com/2011/
 246. javascript:void(0)
 247. https://nlpers.blogspot.com/2011/12/
 248. javascript:void(0)
 249. https://nlpers.blogspot.com/2011/10/
 250. javascript:void(0)
 251. https://nlpers.blogspot.com/2011/09/
 252. javascript:void(0)
 253. https://nlpers.blogspot.com/2011/07/
 254. javascript:void(0)
 255. https://nlpers.blogspot.com/2011/05/
 256. javascript:void(0)
 257. https://nlpers.blogspot.com/2011/04/
 258. javascript:void(0)
 259. https://nlpers.blogspot.com/2011/03/
 260. javascript:void(0)
 261. https://nlpers.blogspot.com/2011/02/
 262. javascript:void(0)
 263. https://nlpers.blogspot.com/2011/01/
 264. javascript:void(0)
 265. https://nlpers.blogspot.com/2010/
 266. javascript:void(0)
 267. https://nlpers.blogspot.com/2010/11/
 268. javascript:void(0)
 269. https://nlpers.blogspot.com/2010/10/
 270. javascript:void(0)
 271. https://nlpers.blogspot.com/2010/09/
 272. javascript:void(0)
 273. https://nlpers.blogspot.com/2010/08/
 274. javascript:void(0)
 275. https://nlpers.blogspot.com/2010/07/
 276. javascript:void(0)
 277. https://nlpers.blogspot.com/2010/06/
 278. javascript:void(0)
 279. https://nlpers.blogspot.com/2010/04/
 280. javascript:void(0)
 281. https://nlpers.blogspot.com/2010/02/
 282. javascript:void(0)
 283. https://nlpers.blogspot.com/2010/01/
 284. javascript:void(0)
 285. https://nlpers.blogspot.com/2009/
 286. javascript:void(0)
 287. https://nlpers.blogspot.com/2009/12/
 288. javascript:void(0)
 289. https://nlpers.blogspot.com/2009/11/
 290. javascript:void(0)
 291. https://nlpers.blogspot.com/2009/10/
 292. javascript:void(0)
 293. https://nlpers.blogspot.com/2009/09/
 294. javascript:void(0)
 295. https://nlpers.blogspot.com/2009/08/
 296. javascript:void(0)
 297. https://nlpers.blogspot.com/2009/07/
 298. javascript:void(0)
 299. https://nlpers.blogspot.com/2009/06/
 300. javascript:void(0)
 301. https://nlpers.blogspot.com/2009/05/
 302. javascript:void(0)
 303. https://nlpers.blogspot.com/2009/04/
 304. javascript:void(0)
 305. https://nlpers.blogspot.com/2009/03/
 306. javascript:void(0)
 307. https://nlpers.blogspot.com/2009/02/
 308. javascript:void(0)
 309. https://nlpers.blogspot.com/2009/01/
 310. javascript:void(0)
 311. https://nlpers.blogspot.com/2008/
 312. javascript:void(0)
 313. https://nlpers.blogspot.com/2008/12/
 314. javascript:void(0)
 315. https://nlpers.blogspot.com/2008/11/
 316. javascript:void(0)
 317. https://nlpers.blogspot.com/2008/09/
 318. javascript:void(0)
 319. https://nlpers.blogspot.com/2008/08/
 320. javascript:void(0)
 321. https://nlpers.blogspot.com/2008/07/
 322. javascript:void(0)
 323. https://nlpers.blogspot.com/2008/06/
 324. javascript:void(0)
 325. https://nlpers.blogspot.com/2008/05/
 326. javascript:void(0)
 327. https://nlpers.blogspot.com/2008/04/
 328. javascript:void(0)
 329. https://nlpers.blogspot.com/2008/03/
 330. javascript:void(0)
 331. https://nlpers.blogspot.com/2008/02/
 332. javascript:void(0)
 333. https://nlpers.blogspot.com/2008/01/
 334. javascript:void(0)
 335. https://nlpers.blogspot.com/2007/
 336. javascript:void(0)
 337. https://nlpers.blogspot.com/2007/12/
 338. javascript:void(0)
 339. https://nlpers.blogspot.com/2007/11/
 340. javascript:void(0)
 341. https://nlpers.blogspot.com/2007/10/
 342. javascript:void(0)
 343. https://nlpers.blogspot.com/2007/09/
 344. javascript:void(0)
 345. https://nlpers.blogspot.com/2007/08/
 346. javascript:void(0)
 347. https://nlpers.blogspot.com/2007/07/
 348. javascript:void(0)
 349. https://nlpers.blogspot.com/2007/06/
 350. javascript:void(0)
 351. https://nlpers.blogspot.com/2007/05/
 352. javascript:void(0)
 353. https://nlpers.blogspot.com/2007/04/
 354. javascript:void(0)
 355. https://nlpers.blogspot.com/2007/03/
 356. javascript:void(0)
 357. https://nlpers.blogspot.com/2007/02/
 358. javascript:void(0)
 359. https://nlpers.blogspot.com/2007/01/
 360. javascript:void(0)
 361. https://nlpers.blogspot.com/2006/
 362. javascript:void(0)
 363. https://nlpers.blogspot.com/2006/12/
 364. javascript:void(0)
 365. https://nlpers.blogspot.com/2006/11/
 366. javascript:void(0)
 367. https://nlpers.blogspot.com/2006/10/
 368. javascript:void(0)
 369. https://nlpers.blogspot.com/2006/09/
 370. javascript:void(0)
 371. https://nlpers.blogspot.com/2006/08/
 372. javascript:void(0)
 373. https://nlpers.blogspot.com/2006/07/
 374. javascript:void(0)
 375. https://nlpers.blogspot.com/2006/06/
 376. javascript:void(0)
 377. https://nlpers.blogspot.com/2006/05/
 378. javascript:void(0)
 379. https://nlpers.blogspot.com/2006/04/
 380. javascript:void(0)
 381. https://nlpers.blogspot.com/2006/03/
 382. javascript:void(0)
 383. https://nlpers.blogspot.com/2006/02/
 384. javascript:void(0)
 385. https://nlpers.blogspot.com/2006/01/
 386. javascript:void(0)
 387. https://nlpers.blogspot.com/2005/
 388. javascript:void(0)
 389. https://nlpers.blogspot.com/2005/12/

   hidden links:
 391. https://www.blogger.com/email-post.g?blogid=19803222&postid=7864834770955534476
 392. https://www.blogger.com/post-edit.g?blogid=19803222&postid=7864834770955534476&from=pencil
 393. https://www.blogger.com/profile/07656348479195514232
 394. https://www.blogger.com/profile/02162908373916390369
 395. https://www.blogger.com/profile/07656348479195514232
 396. https://www.blogger.com/profile/02162908373916390369
 397. https://www.blogger.com/profile/07656348479195514232
 398. https://www.blogger.com/profile/02162908373916390369
 399. https://nlpers.blogspot.com/2016/07/decoding-neural-representations.html
 400. https://www.blogger.com/rearrange?blogid=19803222&widgettype=profile&widgetid=profile1&action=editwidget&sectionid=sidebar
 401. https://www.blogger.com/rearrange?blogid=19803222&widgettype=label&widgetid=label1&action=editwidget&sectionid=sidebar
 402. https://www.blogger.com/rearrange?blogid=19803222&widgettype=bloglist&widgetid=bloglist1&action=editwidget&sectionid=sidebar
 403. https://www.blogger.com/rearrange?blogid=19803222&widgettype=blogarchive&widgetid=blogarchive1&action=editwidget&sectionid=sidebar
 404. https://www.blogger.com/rearrange?blogid=19803222&widgettype=html&widgetid=html1&action=editwidget&sectionid=footer
