   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

how tensors advance human technology

an exploration of mathematics past and present

   [9]go to the profile of philip jama
   [10]philip jama (button) blockedunblock (button) followfollowing
   aug 5, 2016

   what is a tensor?

   simply put, a tensor is a multi-dimensional matrix, or array of
   numbers. more generally, tensor analysis spans a set of mathematical
   tools used to quantify and model a diversity of systems spanning
   physics, psychology, evolutionary biology, and modern artificial
   intelligence.

   like many, i was curious to understand more about what tensors have to
   do with machine learning.

   how are tensors used in modern approaches to artificial intelligence
   and machine learning?

brief history of tensor analysis

   i first came across tensors in the form of continuum mechanics while
   studying engineering.

   let me tell you a fascinating story about mathematics. a mathematical
   exploration of the meaning and origins of tensors.

   this story tells some of the fascinating applications of tensor
   analysis         ones that have helped shape and revolutionize humanity   s
   technological progress.

solid mechanics

   in the field of solid mechanics, tensors are used to quantify forces
   throughout a material.
   [1*cioyncl6luw8_wnga1ldrg.png]
   illustration of an infinitesimal cube of material

   let   s take, for instance, consider a piece of cylindrical chalk used to
   write on blackboards in school. imagine pulling, bending, and twisting
   this solid and predicting when and how it would break.

   applying a combination of external forces on a simple object (like
   chalk) creates an array of stresses throughout the material.
   [1*i_uhmozkn3kueqzbgax9oq.png]
   illustration of 3d stress components

   these stresses can be modeled using a set of 9 [11]stress forces,
   exerted on virtually every particle within the object.

   the stresses throughout a solid can be modeled at every point using a a
   tensor known as the [12]cauchy stress tensor.
   [0*yhvnnjvjq_rn_1zd.]

   this type of analysis yields some curious consequences.
   [1*ip0tpfre9uoqusbmfo2m2a.jpeg]
   [13]failure in torsion (left) vs bending (right)

   for instance, it turns out that if you apply a purely torsional force
   (twist) a piece of chalk until it breaks, then it will consistently
   snap along a 45-degree angle.

   whereas bending the chalk will break chalk along a 90-degree angle.

   materials such as chalk and metals often break predictably depending
   on:
     * the organization of their molecules, and
     * the (external and internal) forces applied to the object

   stress forces, like in the chalk example, can be modeled by tensors in
   an area of mathematics called [14]continuum mechanics.

   understanding when things will break helps us design things that won   t
   break unexpectedly.

advancements in technology

   the development of mathematics to model physical forces has allowed
   humans to make tremendous leaps in technological progress.

   let   s consider that for most of civilization, the tallest structures
   created by human were piles of stones         pyramids and eventually
   cathedrals.

   the tensor mathematics used to model stresses in materials such as
   steel was developed in the mid 1800   s by [15]augustin-louis cauchy.
   these [16]tensor equations helped advance humanity   s engineering from
   tall piles of stones to the eiffel tower, and far beyond.
   [0*herwceg5irbxqzuz.]

   in fact, for his contributions, cauchy   s name is one of the
   [17]seventy-two names of scientists, engineers, and mathematicians
   engraved near the base of the eiffel tower.

   the consequences of this can be staggering if you think that every time
   you drive over a large bridge or ride a tall elevator, that your life
   relies (at least in some small way) on these equations of mathematical
   physics and continuum mechanics. engineering design calculations apply
   principles of tensors to ensure that the roof doesn   t fall in on our
   heads.

theory of relativity

   after the technological progress of the nineteenth century, another
   mathematician named hermann minkowski, took this direction of
   mathematics further by describing einstein   s theory of special
   relativity in terms of a tensor field representing a four-dimensional
   manifold known as spacetime.

   in general relativity, the riemann curvature tensor associates a tensor
   to each point in space used to describe the bending of spacetime due to
   gravity.

electromagnetism

   tensors also appear in the study of electromagnetism         minkowski
   simplified [18]maxwell   s equations (four vector calculus equations)
   into two [19]tensor field equations.

so forth   

   there are plenty of other examples of tensor mathematics spanning from
   [20]fluid mechanics, to [21]quantum mechanics.

   it   s fascinating to think that tensors appear in both the mathematics
   used to model spacetime (time-dilation due to gravity) as well as
   electromagnetism. this has laid the foundations for further leaps in
   technology, which have allowed humans to create am/fm radio, satellite
   communications (such as gps) and hopefully (one day) interstellar space
   travel.

artificial intelligence

   so what do tensors mean in the context of ai and machine learning?
   particularly deep learning.

   tensor mathematics comes up in a class of machine learning models that
   involve hidden variables.

   in these models, the latent (hidden) state of data cannot be observed
   directly, but instead, their effects are indirectly observed in
   correlated variables.
   [0*mnh11yjobuz6iog9.]

   for instance, let   s consider gaussian mixture models         where random
   samples are drawn from several [22]normal distributions (or bell
   curves) with unknown parameters. the challenge becomes estimating the
   parameters of the id203 distributions that generated the data
   samples.

   traditionally we would solve for these unknown parameters using
   iterative approaches such as the [23]expectation   maximization
   algorithm. taking a random initial estimate of the parameters, the
   algorithm would iterate repeatedly to converge on a solution.
   [0*rai04yvqigcthsos.]
   expectation-minimization id91 of old faithful geyser
   eruption data

   however, for a set of problems with many contributing factors, these
   iterative techniques become prohibitive and computationally expensive.

   enter tensor decomposition.

   the modern approach to estimating these unknown parameters, known as
   the method of moments, was originally developed by a british
   biostatistician, karl pearson, who outlined the technique in his 1894
   paper, [24]contributions to the mathematical theory of evolution.

   pearson recognized that certain processes in nature were the result of
   multiple random factors. he remarks that the statistical distribution
   of various    biological, sociological, and economic measurements    are
   not a result of a single gaussian, but rather    it may happen that we
   have a mixture of 2,3,   ,n homogeneous groups, each of which deviates
   about its own mean symmetrically and in a manner represented with
   sufficient accuracy by the normal curve.   

   what does this have to do with artificial intelligence? it turns out
   that pearson was on the right path over 120 years ago. solving for
   unknown statistical parameters using the method of moments approach has
   lead to breakthroughs in computation of various models in ai research.

   a key research paper on the topic, [25]tensor decompositions for
   learning latent variable models, was published in 2014. one of the
   authors, anandkumar wrote the following in a [26]discussion on quora:

     the key idea is to consider the tensors which are derived from
     multivariate moments of the observed data.

   (think of multivariate moments as a statistical fingerprint that helps
   untangle how the random data was created, and thereby estimate the
   hidden variables.)

   anandkumar continues,

     we show that decomposing these tensors allows us to consistently
     learn the parameters of a wide range of latent variable models    

     it is crucial to use higher order moments (and therefore tensor
     analysis), since it can be shown that for many of these models, just
     pairwise moments (and therefore matrix analysis) is not sufficient
     to learn the model parameters.

   standard iterative approaches, such as expectation-minimization
   mentioned earlier, tend to be computationally prohibitive. the
   breakthrough of these statistical tensor analysis techniques arises
   when we consider the computational efficiency of this approach.

   the process of estimating model parameters is reduced to a problem of
   extracting a decomposition of a symmetric tensor derived from the
   moments. furthermore, these decomposition problems tend to be amenable
   to efficient methods, such as id119 and the [27]power
   iteration method.

   consequently, these computationally-efficient methods lead to
   tremendous advancements in the field of machine learning enabling
   researchers to build much larger models in order to tackle far more
   complex problems.

conclusion

   admittedly, current machine learning applications pale in comparison to
   the complexity of a single human brain. however with the exponential
   pace of improvements in technology and advancements in algorithmic
   techniques, some believe that a [28]technological singularity may occur
   within decades.

   recent years have already yielded exciting results in fields such as
   image recognition by detecting [29]diabetic retinopathy in human eyes,
   brain computer interfacing (bci) by [30]identifying hand motions from
   eeg scans, and a plethora of applications in voice recognition, facial
   recognition, text analysis, and so forth.
   [1*tmhupu1jtwumjctnlv1xrg.jpeg]
   one of the neurons in the id158, trained from still
   frames from unlabeled youtube videos, learned to detect cats

   [31]google research had a remarkable result where an unsupervised
   learning algorithm, trained from still frames from unlabeled youtube
   videos, learned the visual concept of a cat. they managed to scale
   their computation across 16,000 cpu cores to train a model with over 1
   billion connections!

   the exciting feature of these algorithms (beyond the ability to
   identify cats) is their [32]scalability. rather than having computation
   limited to a single sequential calculation, modern approaches allow the
   computation to be shared among many, many processors.

   what applications will arrive with these mathematical and computational
   advancements? what are the limitations of tensor decomposition?

   this feels like, in many ways, like an eiffel tower moment in history
   where all past efforts to build large-scale ai has been the equivalent
   of stacking large piles of stones. now we could be developing the tools
   to make the next technological leap forward.

links & additional resources

    1. [33]tensor decompositions for learning latent variable models
       (a. anandkumar, r. ge, d. hsu, s.m. kakade, m. telgarsky)
       [34]http://arxiv.org/abs/1210.7559
    2. [35]tensor methods in machine learning (r. ge)
       [36]http://www.offconvex.org/2015/12/17/tensor-decompositions/
    3. pearson   s polynomial (m. hardt)
       [37]http://blog.mrtz.org/2014/04/22/pearsons-polynomial.html
    4. [38]contributions to the mathematical theory of evolution (k.
       pearson)
       [39]https://archive.org/details/philtrans02543681
    5. [40]tensors: stress, strain and elasticity
       [41]http://serc.carleton.edu/nagtworkshops/mineralogy/mineral_physi
       cs/tensors.html
    6. [42]how is tensor analysis applied to machine learning (a.
       anandkumar)
       [43]https://www.quora.com/how-is-tensor-analysis-applied-to-machine
       -learning
    7. [44]tight bounds for learning a mixture of two gaussians
       (m. hardt, e. price) [45]http://arxiv.org/abs/1404.4997
    8. [46]analyzing tensor power method dynamics in overcomplete regime
       (a. anandkumar, r. ge, m. janzamin)
       [47]https://arxiv.org/abs/1411.1488

     * [48]machine learning
     * [49]artificial intelligence
     * [50]deep learning
     * [51]mathematics

   (button)
   (button)
   (button) 102 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [52]go to the profile of philip jama

[53]philip jama

   mathematical thinker and code artist. passionate about data science +
   visualization, bci, and id207.

     * (button)
       (button) 102
     * (button)
     *
     *

   [54]go to the profile of philip jama
   never miss a story from philip jama, when you sign up for medium.
   [55]learn more
   never miss a story from philip jama
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/3831bff0906
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@philjama/how-tensors-advance-human-technology-3831bff0906&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@philjama/how-tensors-advance-human-technology-3831bff0906&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@philjama?source=post_header_lockup
  10. https://medium.com/@philjama
  11. https://en.wikipedia.org/wiki/stress_(mechanics)
  12. https://en.wikipedia.org/wiki/cauchy_stress_tensor
  13. http://link.springer.com/chapter/10.1007/978-0-387-49474-6_9#page-1
  14. https://en.wikipedia.org/wiki/continuum_mechanics
  15. https://en.wikipedia.org/wiki/augustin-louis_cauchy
  16. https://en.wikipedia.org/wiki/cauchy_stress_tensor
  17. https://en.wikipedia.org/wiki/list_of_the_72_names_on_the_eiffel_tower
  18. https://en.wikipedia.org/wiki/maxwell's_equations
  19. https://en.wikipedia.org/wiki/electromagnetic_tensor
  20. https://en.wikipedia.org/wiki/viscous_stress_tensor
  21. https://www.quora.com/what-is-a-tensor-product-in-quantum-mechanics
  22. https://en.wikipedia.org/wiki/normal_distribution
  23. https://en.wikipedia.org/wiki/expectation   maximization_algorithm
  24. https://archive.org/details/philtrans02543681
  25. http://arxiv.org/abs/1210.7559
  26. https://www.quora.com/how-is-tensor-analysis-applied-to-machine-learning
  27. https://arxiv.org/pdf/1411.1488v2.pdf
  28. https://en.wikipedia.org/wiki/technological_singularity
  29. https://www.kaggle.com/c/diabetic-retinopathy-detection
  30. https://www.kaggle.com/c/grasp-and-lift-eeg-detection
  31. https://googleblog.blogspot.ca/2012/06/using-large-scale-brain-simulations-for.html
  32. https://research.googleblog.com/2016/04/announcing-tensorflow-08-now-with.html
  33. http://arxiv.org/abs/1210.7559
  34. http://arxiv.org/abs/1210.7559
  35. http://www.offconvex.org/2015/12/17/tensor-decompositions/
  36. http://www.offconvex.org/2015/12/17/tensor-decompositions/
  37. http://blog.mrtz.org/2014/04/22/pearsons-polynomial.html
  38. https://archive.org/details/philtrans02543681
  39. https://archive.org/details/philtrans02543681
  40. http://serc.carleton.edu/nagtworkshops/mineralogy/mineral_physics/tensors.html
  41. http://serc.carleton.edu/nagtworkshops/mineralogy/mineral_physics/tensors.html
  42. https://www.quora.com/how-is-tensor-analysis-applied-to-machine-learning
  43. https://www.quora.com/how-is-tensor-analysis-applied-to-machine-learning
  44. http://arxiv.org/abs/1404.4997
  45. http://arxiv.org/abs/1404.4997
  46. https://arxiv.org/abs/1411.1488
  47. https://arxiv.org/abs/1411.1488
  48. https://medium.com/tag/machine-learning?source=post
  49. https://medium.com/tag/artificial-intelligence?source=post
  50. https://medium.com/tag/deep-learning?source=post
  51. https://medium.com/tag/mathematics?source=post
  52. https://medium.com/@philjama?source=footer_card
  53. https://medium.com/@philjama
  54. https://medium.com/@philjama
  55. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  57. https://medium.com/p/3831bff0906/share/twitter
  58. https://medium.com/p/3831bff0906/share/facebook
  59. https://medium.com/p/3831bff0906/share/twitter
  60. https://medium.com/p/3831bff0906/share/facebook
