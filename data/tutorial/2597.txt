   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    the evolution and core concepts of deep learning
   & neural networks comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]machine learning [94]the evolution and core concepts of
   deep learning & neural networks

   [95]machine learning

the evolution and core concepts of deep learning & neural networks

   [96]guest blog, august 3, 2016

introduction

   with the evolution of neural networks, various tasks which were
   considered unimaginable can be done conveniently now. tasks such as
   image recognition, id103, finding deeper relations in a
   data set have become much easier. a sincere thanks to the eminent
   researchers in this field whose discoveries and findings have helped us
   leverage the true power of neural networks.

   if you are truly interested in pursuing machine learning as a subject,
   a thorough understand of deep learning networks is crucial for you.
   most ml algorithms tend of lose accuracy when given a data set with
   several variables, whereas a deep learning model does wonders in such
   situations.. therefore, it   s important for us to understand how does it
   work!

   in this article, i   ve explained the core concepts used in deep learning
   i.e. what sort of backend calculations result in enhanced model
   accuracy. along side, i   ve also shared various modeling tips and a
   sneak peek into the history of neural networks.

   explaining the evolution and core concepts of deep learning & neural
   networks



table of contents

     * history of neural networks
     * single layer id88
     * multilayer id88
          + initialization of the parameter
          + activation function
          + id26 algorithm
          + id119
          + cost function
          + learning rate
          + momentum
          + softmax
          + summary of multilayer id88 (mlp)

     *  overview of deep learning
          + restricted id82 and id50
          + dropout
          + techniques to deal with class imbalance
          + smote: synthetic minority over-sampling technique
          + cost-sensitive learning in neural networks

     * about the authors


history of neural networks

   neural networks are the building blocks of today   s technological
   breakthrough in the field of deep learning.  a neural network can be
   seen as simple processing unit that is massively parallel, capable to
   store knowledge and apply this knowledge to make predictions.

   a neural network mimics the brain in a way the network acquires
   knowledge from its environment through a learning process. then,
   intervention connection strengths known as synaptic weights are used to
   store the acquired knowledge. in the learning process, the synaptic
   weights of the network are modified in an orderly fashion to attain the
   desired objective. in 1950, the neuro-psychologist karl lashley   s
   thesis was published in which he described the brain as a distributed
   system.

   another reason that the neural network is compared with the human brain
   is that, they operate like non-linear parallel information-processing
   systems which rapidly perform computations such as pattern recognition
   and perception. as a result, these networks perform very well in areas
   like speech, audio and image recognition where the inputs / signals are
   inherently nonlinear.

   mcculloch and pitts were pioneers of neural networks who wrote a
   research article on the model with two inputs and single output in
   1943. the following were the features of that model:

    a neuron would only be activated if:
     * one of the inputs is active
     * the weights for each input is equal
     * the output of the model is binary

   there is a certain threshold level computed by summing up the input
   values for which the output is either zero or one.

   in hebb   s 1949 book    the organization of behaviour   , the idea that the
   connectivity of brain is continuously changing in response to changes
   in tasks was proposed for the first time. this rule implies that the
   connection between two neurons is active at the same time. this soon
   became the source of inspiration for the development of computational
   models of learning and adaptive systems.

   id158s have the ability to learn from supplied
   data, known as adaptive learning, while the ability of a neural network
   to create its own organization or representation of information is
   known as self-organisation.

   after 15 years, the id88 developed by rosenblatt in 1958 emerged
   as the next model of neuron. id88 is the simplest neural network
   that linearly separates the data into two classes. later, he randomly
   interconnected the id88 and used a trial and error method to
   change the weights for the learning.

   after 1969, the research came to a dead end in this area for the next
   15 years after the mathematicians marvin minsky and seymour parpert
   published a mathematical analysis of the id88. they found that
   the id88 was not capable of representing many important problems,
   like the exclusive-or function (xor). secondly, there was an issue that
   the computers did not have enough processing power to effectively
   handle large neural networks.

   in 1986, the development of the back-propagation algorithm was reported
   by rumelhart, hinton, and williams  that can solve problems like xor,
   beginning a second generation of neural networks. in that same year,
   the celebrated two-volume book, parallel distributed processing:
   explorations in the microstructures of cognition, edited by rumelhart
   and mcclelland, was published. that book has been a major influence in
   the use of back-propagation, which has emerged as the most popular
   learning algorithm for the training of multilayer id88s.


single layer id88 (slp)

   the simplest type of id88 has a single layer of weights
   connecting the inputs and output.  in this way, it can be considered
   the simplest kind of feed-forward network. in a feed forward network,
   the information always moves in one direction; it never goes backwards.

   [97]slp

    figure 1

   figure 1 shows a single-layer id88 for easier conceptual
   grounding and clarification into multilayer id88 (explained
   ahead). single layer id88  represents the m weights that are seen
   as a set of synapses or connecting links between one layer and another
   layer within the network. this parameter indicates how important each
   feature  [98] eq 10.5  is. below is the adder function of features of
   the input multiplied by their respective synaptic connection:

   [99]eq 10

   the bias [100]eq 10.1  ,  acts as an affine transformation to the
   output of the adder function [101]eq 10.2   giving  [102] eq 10.3  ,
   the induced local field as:

   [103]eq 10.4


multilayer id88 (mlp)

   moving onwards, multi-layer id88, also known as feed-forward
   neural networks, consists of a sequence of layers each fully connected
   to the next one.

   a multilayer id88 (mlp) has one or more hidden layers along with
   the input and output layers, each layer contains several neurons that
   interconnect with each other by weight links. the number of neurons in
   the input layer will be the number of attributes in the dataset,
   neurons in the output layer will be the number of classes given in the
   dataset.

   [104]mlp   figure 2

   figure 2 shows a multilayer id88 where we have three layers at
   least and each layer is connected to the last one. to make the
   architecture deep, we need to introduce multiple hidden layers.


initialization of the parameters

   initialization of the parameters, weights and biases plays an important
   role in determining the final model. there is a lot of literature on
   initialization strategy.

   a good random initialization strategy can avoid getting stuck at local
   minima. local minima problem is when the network gets stuck in the
   error surface and does not go down while training even when there is
   capacity left for learning.

   doing experiment by using various initialization strategies is out of
   the scope of this research work.

   the initialization strategy should be selected according to the
   activation function used. for tanh the initialization interval should
   be [105]eq4  where [106]eq4.6  is the number of units in the (i-1)-th
   layer, and [107]eq4.7 is the number of units in the ith layer.
   similarly for the sigmoid activation function the initialization
   interval should be [108]eq4.1 . these initialization strategies ensure
   that information propagated upwards and backwards in the network at the
   early stage of training.


activation function

   the activation function defines the output of a neuron in terms of the
   induced local field v as:

   [109]eq4.2

   where   (.) is the activation function. there are various types of
   id180, the following are the commonly used ones:

1. threshold function

   [110]eq4.4

   [111]eq4.5

   figure 2

   figure 2 indicates that either the neuron is fully active or not.
   however, this function is not differentiable which is quite vital when
   using the back-propagation algorithm (explained later).

2. sigmoid function

   the sigmoid function is a logistic function bounded by 0 and 1, as with
   the threshold function, but this activation function is continuous and
   differentiable.

   [112]image 2

   where    is the slope parameter of the above function. moreover, it is
   nonlinear in nature that helps to increase the performance making sure
   that small changes in the weights and bias causes small changes in the
   output of the neuron.

3. hyperbolic tangent function

      (v) = tanh (v)

    this function enables id180 to range from -1 to +1.

4. rectified linear activation function (relu)

   relus are the smooth approximation to the the sum of many logistic
   units and produce sparse activity vectors. below is the equation of the
   function:[113] eq7 [114]image 4

   figure 3

   in figure 3,  [115] eq 6   is the smooth approximation to the
   rectifier).

5. maxout function

   in 2013, goodfellow found out that the maxout network using a new
   activation function is a natural companion to dropout.

   maxout units facilitate optimization by dropout and improve the
   accuracy of dropout   s fast approximate model averaging technique.  a
   single maxout unit can be interpreted as making a piece wise linear
   approximation to an arbitrary convex function.

   maxout networks learn not just the relationship between hidden units,
   but also the activation function of each hidden unit. below is the
   graphical depiction of how this works:

   [116]image 5

   figure 4

   figure 4 shows the maxout network with 5 visible units, 3 hidden units
   and 2 pieces for each hidden unit.

                                   [117] eq8

   [118]eq8.1

   where [119]eq8.2  is the mean vector of size of the input obtained by
   accessing the matrix w     [120]eq8.3   at the second coordinate i and
   third coordinate j . the number of intermediate units ( k ) is called
   the number of pieces used by the maxout nets.


id26 algorithm

   the back-propagation algorithm can be used to train feed forward neural
   networks or multilayer id88s. it is a method to minimize the cost
   function by changing weights and biases in the network. to learn and
   make better predictions, a number of epochs (training cycles) are
   executed where the error determined by the cost function is backward
   propagated by id119 until a sufficiently small error is
   achieved.


id119

1. mini-batch id119

   let   s say in 100-sized mini-batch, 100 training examples are shown to
   the learning algorithm and weights are updated accordingly. after all
   mini-batches are presented sequentially, the average of accuracy levels
   and training cost levels are calculated for each epoch.

2. stochastic id119

   stochastic id119 is used in the real-time on-line
   processing, where the parameters are updated while presenting only one
   training example, and so average of accuracy levels and training costs
   are taken for the entire training dataset at each epoch.

3. full batch id119

   in this method all the training examples are shown to the learning
   algorithm and the weights are updated.


cost function

   there are various cost functions. below are some examples:

1. mean squared error function

   [121]eq9

     where [122]eq9.2  is the predicted output [123] eq9.3  is the actual
   output

2. cross-id178 function

   [124]eq9.1

   where the f  function is the model   s predicted id203 for the
   input  [125] eq9.4  label to be [126]eq9.5 , w are its parameters, and
   n  is the training-batch size.

3. negative log-likelihood loss (nll) function

   nll is the cost function used in all the experiments of the report.

   [127]eq9.6

   where [128]eq9.7  is the value of the output is, [129]eq9.8  is the
   value of the feature input,    is the parameters and d is the training
   set.


learning rate

   learning rate controls the change in the weight from one iteration to
   another. as a general rule, smaller learning rates are considered as
   stable but cause slower learning. on the other hand higher learning
   rates can be unstable causing oscillations and numerical errors but
   speed up the learning.

   [130]eq10


momentum

   momentum provides inertia to escape local minima; the idea is to simply
   add a certain fraction of the previous weight update to the current
   one, helping to avoid becoming stuck in local minima.

   [131]eq11

   where     is the momentum.


softmax

   softmax is a neural transfer function that is generalized form of
   logistic function implemented in the output layer that turns the
   vectors  into the probabilities that add up and constraint to 1.

   [132]eq12.2


summary of multilayer id88 (mlp)

   for classification, a softmax function may be incorporated in the
   output layer that will give the id203 of each occurring class.
   activation function is used to compute the predicted output of each
   neuron in each layer by using inputs, weights and bias.

   the back propagation method trains the multilayer neural network by
   modifying its synaptic connection weights between the layers to improve
   model performance based on the error correction learning function which
   needs to be continuous and differentiable. the following parameters
   have been evaluated in the experiments:
     * number of hidden layers.
     * number of neurons in the hidden layers.
     * learning rate and momentum.
     * types of activation function.


overview of deep learning

   before 2006, various failed attempts at training deep supervised feed
   forward neural networks were made that resulted in over-fitting of the
   performance on the unseen data i.e. training error reduces while
   validation error increases.

   a deep network usually means an id158 that has more
   than one hidden layer. training the deep hidden layers required more
   computational power. having a greater depth seemed to be better because
   intuitively neurons can make the use of the work done by the neuron in
   the layer below resulting in distributed representation of the data.

   bengio suggests that the neurons in the hidden layers are seen as
   feature detectors learned by the neuron in the below layer. this result
   in better generalization that is a subset of neurons learns from data
   in a specific region of the input space.

   moreover deeper architectures can be more efficient as fewer
   computational units are needed to represent the same functions,
   achieving greater efficiency. the core idea behind the distributed
   representation is the sharing of statistical strengths where different
   components of the architecture are re-used for different purposes.

   deep neural architectures are composed of multiple layers utilizing
   non-linear operations, such as in neural nets with many hidden layers.
   there are often various factors of variation in the dataset, like
   aspects of the data separately and often independently may vary.

   deep learning algorithms can capture these factors that explain the
   statistical variations in the data, and how they interact to generate
   the kind of data we observe. lower level abstractions are more directly
   tied to particular observations; on the other hand higher level ones
   are more abstract because their connection to perceived data is more
   remote.

   the focus of deep architecture learning is to automatically discover
   such abstractions, from low level features to the higher level
   concepts. it is desirable for the learning algorithms to enable this
   discovery without manually defining necessary abstractions.

   training samples in the dataset must be at least as numerous as the
   variations in the test set otherwise the learning algorithm cannot
   generalize. deep learning methods aim to learn feature hierarchies,
   composing lower level features into higher level abstractions.

   deep neural nets with a huge number of parameters are very powerful
   machine learning systems. however, over-fitting is a serious problem in
   deep networks. over-fitting is when the validation error starts to go
   up while the training error declines. dropout is one of the
   id173 techniques for addressing this problem which is
   discussed later.

   today one of the most important factors for the increased success of
   deep learning techniques is advancement in the computing power.
   graphical processing units (gpu) and cloud computing are crucial for
   applying deep learning to many problems.

   cloud computing allows id91 of computers and on demand processing
   that helps to reduce the computation time by paralleling the training
   of the neural network. gpu   s, on the other hand, are special purpose
   chips for high performance mathematical calculations, speeding up the
   computation of matrices.

   in 2006-07, three papers revolutionized the deep learning discipline.
   the key principles in their work were that each layer can be
   pre-trained by unsupervised learning, done one layer at a time. finally
   supervised training by back-propagation of the error is used to
   fine-tune all the layers, effectively giving better initialization by
   unsupervised learning than by random initialization.


restricted id82 and id50

   one of the unsupervised algorithms is restricted id82s
   (rbm) that is used to pre-train deep belief network. the rbm is a
   simplified version of the id82, inspired by statistical
   mechanics, which models energy based probabilities for the underlying
   distributions of the given data sets from which conditional
   distributions can be derived.

   id82s are bidirectionally connected networks of stochastic
   processing units of visible units and hidden units. the raw data
   corresponds to the    visible    neurons and samples to observed states and
   the feature detectors correspond to    hidden    neurons. in a boltzmann
   machine, visible neurons provide the input to the network and the
   environment in which it operates. during training visible neurons are
   clamped (set to a defined value) determined by the training
   data. hidden neurons on the other hand operate freely.

   however, id82 are difficult to train because of its
   connectivity. an rbm has restricted connectivity to make learning
   easier; there are no connections between hidden units in a single layer
   forming a bipartite graph, depicted in figure 2. the advantage of this
   is that the hidden units are updated independently and in parallel
   given the visible state.

   these networks are governed by an energy function that determines the
   id203 of the hidden/visible states. each possible joint
   configuration of the visible and hidden units has a hopfield energy
   determined by the weights and biases. the energies of the joint
   configurations are optimized by id150 that learns the
   parameters by minimizing the lowest energy function of the rbm.

   [133]image 6

   figure 5

   in figure 5, left layer represents the visible layer and right layer
   represents the hidden layer.

   in deep belief network (dbn), rbm is trained by input data with
   important features of the input data captured by stochastic neurons in
   the hidden layer. in the second layer the activations of the trained
   features are treated as input data. the learning process in the second
   rbm layer can be viewed as learning feature of features. every time a
   new layer of features is added to the deep belief network, a
   variational lower bound on the log-id203 of the original training
   data is improved.

   [134]image 7

   figure 6

   figure 6 shows rbm converts its data distribution into a posterior
   distribution over its hidden units.

   the weights of the rbm are randomly initialized causing the difference
   in the distribution of p(x) and q(x). during learning, weights are
   iteratively adjusted to minimize the error between p(x) and q(x). in
   figure 2 q(x) is the approximate of the original data and p(x) is the
   original data.

   the rule for adjusting the synaptic weight from neuron one and another
   is independent of whether both the neurons are visible or hidden or one
   of each. the updated parameters by the layers of rbm are used as
   initialization in dbn   s that fine-tunes all the layers by supervised
   training of id26.

   for the ids data of kdd cup 1999, it is appropriate to use multimodal
   (bernoulli-gaussian) rbm as kdd cup 1999 consists of mixed data types,
   specifically continuous and categorical. in multimodal rbm there are
   two different channel input layers used in the rbm, one is gaussian
   input unit used for continuous features and the other one is bernoulli
   input unit layer where binary features are used. using multimodal rbm
   is beyond the scope of this research work.


dropout

   recent developments have introduced powerful regularizers to deep
   networks to reduce over-fitting. in machine learning, id173 is
   additional information usually introduced in the form of a penalty to
   penalize complexity of the model that leads to over-fitting.

   dropout is a id173 technique for deep neural networks
   introduced by hinton which consists of preventing co-adaptation of
   feature detectors by randomly turning o    a portion of neurons at every
   training iteration but using the entire network (with weights scaled
   down) at test time.

   dropout reduces over-   tting by being equivalent to training an
   exponential number of models that share weights. there exists an
   exponential number of di   erent dropout con   gurations for a given
   training iteration, so a di   erent model is almost certainly trained
   every time. at test time, the average of all models is used, which acts
   as a powerful ensemble method.

   [135]image 8

   figure 7

   in figure 7, dropout randomly drops the connections between the neural
   network layer

   [136]image 9

   figure 8

   in figure 8, at training time the connections are dropped with
   id203, while at the test time weights are scaled to   w

   averaging many models usually has been the key for many winner of the
   machine learning competitions. many different types of model are used
   and then combined to make predictions at test time.

   id79 is a very powerful id112 algorithm which is created by
   averaging many id90 giving them different training sample
   sets with replacement. it is well known that the id90 are
   easy to fit to data and fast at test time so averaging different
   individual trees by giving them different training sets is affordable.

   however, using the same approach with deep neural networks will prove
   to be very computationally expensive. it is already costly to train
   individual deep neural networks and training multiple deep neural
   networks and then averaging seems to be impractical. moreover a single
   network that is efficient at test time is needed rather than having
   lots of large neural nets.

   dropout is an efficient way to average many large neural nets. each
   time while training the model hidden units can be omitted with some
   id203 as in figure 8, which is usually   = 0.5, when the training
   example is presented. as a result a    mean network    model that has all
   the outgoing weights halved is used at test time as in figure 4. the
   mean network is equivalent to taking the geometric mean of the
   id203 distributions over labels predicted by all  possible
   networks with a single hidden layer of  units and    softmax    output
   layer.

   as per the is mathematical proof of how dropout can be seen as an
   ensemble method.

   [137]eq13 is the prediction of the    ensemble    using the geometric mean.

   [138]eq13.1 is the prediction of a single sub model.

   d is the binary vector that tells which inputs to include into the
   softmax classifier.

   [139]eq13.2

   suppose there are  different units. there will be 2^n possible
   assignments to d, and so;

   [140]eq13.3    where y is the single and  [141] eq13.4  is the vector
   of the classes index.

   the sum of the probabilities of the output by a single sub-model is
   used to normalize [142] eq13.5

   [143]eq15

   [144]eq13.7 , as per the definition of softmax

    [145] eq13.8 [146]eq13.10

   [147]eq13.11

   [148]eq13.12

   [149]eq13.13

   so, the predicted id203 must be proportional to this. to
   re-normalize the above expression, it is divided by [150]eq13.14  which
   means the predicted id203 distribution is [151] eq13.15

   another way to view dropout is that, it is able to prevent co-adaption
   among the feature detectors. co-adaption of the feature detector means
   that if a hidden unit knows which other hidden units are present, it
   can co-adapt with them on the training data. however, on the test
   dataset complex co-adaptions are likely to fail to generalize.

   dropout can also be used in the input layer at a lower rate, typically
   20% id203. the concept here is the same as de-noising auto
   encoders developed. in this method some of the inputs are omitted. this
   hurts the training accuracy but improves generalization acting in a
   similar way as adding noise to the dataset while training.

   in 2013 a variant of dropout is introduced called drop connect. instead
   of dropping hidden units with certain id203 weights are randomly
   dropped with certain id203. it has been shown that a drop connect
   network seemed to perform better than dropout on the mnist data set.


techniques to deal with class imbalance

   a class imbalance problem arises when one of the classes (minority
   class) is heavily under-represented in comparison to the other classes
   (majority class). this problem has real world significance where it is
   costly to misclassify minority classes such as detecting anomalous
   activities like fraud or intrusion. there are various techniques to
   deal with the class imbalance problem such as explained below:


smote: synthetic minority over-sampling technique

   one widely used approach to address the class imbalance problem is
   resampling of the data set. the sampling method involves pre-processing
   and balances the training data set by adjusting the prior distribution
   for minority and majority classes. smote is an over-sampling approach
   in which the minority class is over-sampled by creating    synthetic   
   examples rather than by over-sampling with replacement.

   it has been suggested that oversampling the minority class by
   replacement does not improve the results significantly rather it tends
   to over-fit the classification of the minority class. instead the smote
   algorithm operates in    feature space    rather than    data space   . it
   creates synthetic samples by oversampling the minority class which
   tends to generalize better.

   the idea is inspired by creating extra training data by operating on
   real data so that there is more data that helps to generalize
   prediction.

   in this algorithm firstly  nearest neighbours are computed for the
   minority class. then, synthetic samples of the minority class are
   computed in the following manner: a random number of nearest neighbours
   is chosen and distance between that neighbour and the original minority
   class data point is taken.

   this distance is multiplied by a random number between 0 and 1 and adds
   the result to the feature vector of the original minority class data as
   an additional sample, thus creating synthetic minority class samples.


cost-sensitive learning in neural networks

   cost sensitivity learning seems to be quite an effective way to address
   the class imbalance for classification problems. three cost sensitive
   methods have been described that are specific to neural networks.

   incorporate the prior probabilities of the class in the output layer of
   the neural network while testing unseen examples

   [152]image 12

   adjusted learning rates based on the costs. higher learning rates
   should be assigned to examples with high misclassifications costs
   making a larger impact on the weight changes for those examples

   [153]image 13

   modifying the mean square error function. as a result, the learning
   done by id26 will minimize misclassification costs. the new
   error function is:

   [154]image 10

   with the cost factor being k[i,j].

   this new error function results in a new delta rule used in the
   updating of the weights of the network:[155] image 14

   where the first equation represents the error function for output
   neurons and the second equation represents the error function for
   hidden neurons.


end notes

   if you are not comfortable with math, the mathematical functions
   explained above might seem intimidating to you. therefore, you are
   advised to undergo online courses on algebra and integrals.

   in this article, we discussed the core concepts of deep learning such
   as id119, id26 algorithm, cost function etc and
   their respective role in building a robust deep learning model. this
   article is a result of our research work done on deep learning. hope
   you found this article helpful. and to gain expertise in working in
   neural network try out the deep learning practice problem
       [156]identify the digits.

   have you done any research  of a similar topics ? let us know your
   suggestions / opinions on building powerful deep learning models.

about the authors

   syed danish ali is a senior consultant at sir consultants, a leading
   actuarial consultancy in the middle east and south asia. he graduated
   from university of london in sociology. he is a career ambassador of
   institute and faculty of actuaries (ifoa uk). he has more than 60
   publications across a range of international platforms. you can connect
   syed at [157][email protected]
   rahul ahuja is a member of institute and faculty of actuaries uk, has
   masters in data science from cass business school of city university uk
   and multiple years of actuarial consulting experience. you can connect
   rahul at [158][email protected]

got expertise in business intelligence  / machine learning / big data / data
science? showcase your knowledge and help analytics vidhya community
by [159]posting your blog.

   you can also read this article on analytics vidhya's android app
   [160]get it on google play

share this:

     * [161]click to share on linkedin (opens in new window)
     * [162]click to share on facebook (opens in new window)
     * [163]click to share on twitter (opens in new window)
     * [164]click to share on pocket (opens in new window)
     * [165]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [166]id26 algorithm, [167]big data, [168]boltzmann
   machine, [169]core analytics, [170]cost function, [171]deep belief
   networks, [172]deep learning, [173]id119, [174]imbalanced
   classification, [175]learning rate, [176]momentum, [177]multilayer
   id88, [178]neural networks, [179]single layer id88,
   [180]smote, [181]softmax
   next article

innovation in analytics education: great lakes using mentored learning for
online courses

   previous article

tutorial     data science at command line with r & python (scikit learn)

[182]guest blog

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [183]discussion portal to get your queries resolved

one comment

     * dr venugopala rao manneni says:
       [184]august 3, 2016 at 5:06 pm
       clearly explained    
       [185]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [186]srk       3924
   2    [2.jpg?date=2019-04-05] [187]mark12    3510
   3    [3.jpg?date=2019-04-05] [188]nilabha   3261
   4    [4.jpg?date=2019-04-05] [189]nitish007 3237
   5    [5.jpg?date=2019-04-05] [190]tezdhar   3082
   [191]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [192]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [193]understanding support vector machine algorithm from examples
       (along with code)
     * [194]essentials of machine learning algorithms (with python and r
       codes)
     * [195]a complete tutorial to learn data science with python from
       scratch
     * [196]7 types of regression techniques you should know!
     * [197]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [198]a simple introduction to anova (with applications in excel)
     * [199]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [200]top 5 machine learning github repositories and reddit discussions
   from march 2019

[201]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [202]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[203]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [204]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[205]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [206]16 opencv functions to start your id161 journey (with
   python code)

[207]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [208][ds-finhack.jpg]

   [209][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [210]about us
     * [211]our team
     * [212]career
     * [213]contact us
     * [214]write for us

   [215]about us
   [216]   
   [217]our team
   [218]   
   [219]careers
   [220]   
   [221]contact us

data scientists

     * [222]blog
     * [223]hackathon
     * [224]discussions
     * [225]apply jobs
     * [226]leaderboard

companies

     * [227]post jobs
     * [228]trainings
     * [229]hiring hackathons
     * [230]advertising
     * [231]reach us

   don't have an account? [232]sign up here.

join our community :

   [233]46336 [234]followers
   [235]20224 [236]followers
   [237]followers
   [238]7513 [239]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [240]privacy policy
     * [241]terms of use
     * [242]refund policy

   don't have an account? [243]sign up here

   iframe: [244]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [245](button) join now

   subscribe!

   iframe: [246]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [247](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/machine-learning/
  94. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/
  95. https://www.analyticsvidhya.com/blog/category/machine-learning/
  96. https://www.analyticsvidhya.com/blog/author/guest-blog/
  97. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/slp.png
  98. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.5.png
  99. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.png
 100. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.1.png
 101. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.2.png
 102. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.3.png
 103. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.4.png
 104. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/mlp-3.png
 105. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.png
 106. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.6.png
 107. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.7.png
 108. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.1.png
 109. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.2.png
 110. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.4.png
 111. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.5.png
 112. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-2.png
 113. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq7.png
 114. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-4.png
 115. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-6.png
 116. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-5.png
 117. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq8.png
 118. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq8.1.png
 119. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq8.2.png
 120. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq8.3.png
 121. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.png
 122. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.2.png
 123. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.3.png
 124. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.1.png
 125. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.4.png
 126. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.5.png
 127. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.6.png
 128. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.7.png
 129. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.8.png
 130. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq10.png
 131. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq11.png
 132. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq12.2.png
 133. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-6.png
 134. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-7.png
 135. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-8.png
 136. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-9.png
 137. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.png
 138. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.1.png
 139. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.2.png
 140. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.3.png
 141. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.4.png
 142. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.5.png
 143. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq15.png
 144. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.7.png
 145. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.8.png
 146. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.10.png
 147. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.11.png
 148. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.12.png
 149. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.13.png
 150. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.14-1.png
 151. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.15.png
 152. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-12.png
 153. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-13.png
 154. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-10.png
 155. https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-14.png
 156. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/
 157. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection
 158. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection
 159. https://www.analyticsvidhya.com/about-me/write/
 160. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 161. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/?share=linkedin
 162. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/?share=facebook
 163. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/?share=twitter
 164. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/?share=pocket
 165. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/?share=reddit
 166. https://www.analyticsvidhya.com/blog/tag/id26-algorithm/
 167. https://www.analyticsvidhya.com/blog/tag/big-data/
 168. https://www.analyticsvidhya.com/blog/tag/boltzmann-machine/
 169. https://www.analyticsvidhya.com/blog/tag/core-analytics/
 170. https://www.analyticsvidhya.com/blog/tag/cost-function/
 171. https://www.analyticsvidhya.com/blog/tag/deep-belief-networks/
 172. https://www.analyticsvidhya.com/blog/tag/deep-learning/
 173. https://www.analyticsvidhya.com/blog/tag/gradient-descent/
 174. https://www.analyticsvidhya.com/blog/tag/imbalanced-classification/
 175. https://www.analyticsvidhya.com/blog/tag/learning-rate/
 176. https://www.analyticsvidhya.com/blog/tag/momentum/
 177. https://www.analyticsvidhya.com/blog/tag/multilayer-id88/
 178. https://www.analyticsvidhya.com/blog/tag/neural-networks/
 179. https://www.analyticsvidhya.com/blog/tag/single-layer-id88/
 180. https://www.analyticsvidhya.com/blog/tag/smote/
 181. https://www.analyticsvidhya.com/blog/tag/softmax/
 182. https://www.analyticsvidhya.com/blog/author/guest-blog/
 183. https://discuss.analyticsvidhya.com/
 184. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/#comment-114411
 185. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/#comment-114411
 186. https://datahack.analyticsvidhya.com/user/profile/srk
 187. https://datahack.analyticsvidhya.com/user/profile/mark12
 188. https://datahack.analyticsvidhya.com/user/profile/nilabha
 189. https://datahack.analyticsvidhya.com/user/profile/nitish007
 190. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 191. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 192. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 193. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 194. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 195. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 196. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 197. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 198. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 199. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 200. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 201. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 202. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 203. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 204. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 205. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 206. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 207. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 208. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 209. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 210. http://www.analyticsvidhya.com/about-me/
 211. https://www.analyticsvidhya.com/about-me/team/
 212. https://www.analyticsvidhya.com/career-analytics-vidhya/
 213. https://www.analyticsvidhya.com/contact/
 214. https://www.analyticsvidhya.com/about-me/write/
 215. http://www.analyticsvidhya.com/about-me/
 216. https://www.analyticsvidhya.com/about-me/team/
 217. https://www.analyticsvidhya.com/about-me/team/
 218. https://www.analyticsvidhya.com/about-me/team/
 219. https://www.analyticsvidhya.com/career-analytics-vidhya/
 220. https://www.analyticsvidhya.com/about-me/team/
 221. https://www.analyticsvidhya.com/contact/
 222. https://www.analyticsvidhya.com/blog
 223. https://datahack.analyticsvidhya.com/
 224. https://discuss.analyticsvidhya.com/
 225. https://www.analyticsvidhya.com/jobs/
 226. https://datahack.analyticsvidhya.com/users/
 227. https://www.analyticsvidhya.com/corporate/
 228. https://trainings.analyticsvidhya.com/
 229. https://datahack.analyticsvidhya.com/
 230. https://www.analyticsvidhya.com/contact/
 231. https://www.analyticsvidhya.com/contact/
 232. https://datahack.analyticsvidhya.com/signup/
 233. https://www.facebook.com/analyticsvidhya/
 234. https://www.facebook.com/analyticsvidhya/
 235. https://twitter.com/analyticsvidhya
 236. https://twitter.com/analyticsvidhya
 237. https://plus.google.com/+analyticsvidhya
 238. https://in.linkedin.com/company/analytics-vidhya
 239. https://in.linkedin.com/company/analytics-vidhya
 240. https://www.analyticsvidhya.com/privacy-policy/
 241. https://www.analyticsvidhya.com/terms/
 242. https://www.analyticsvidhya.com/refund-policy/
 243. https://id.analyticsvidhya.com/accounts/signup/
 244. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 245. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 246. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 247. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 249. https://www.facebook.com/analyticsvidhya
 250. https://twitter.com/analyticsvidhya
 251. https://plus.google.com/+analyticsvidhya/posts
 252. https://in.linkedin.com/company/analytics-vidhya
 253. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/#_ftnref6
 254. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/#_ftnref12
 255. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/#_ftnref25
 256. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/#_ftnref26
 257. https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/#_ftnref35
 258. https://www.analyticsvidhya.com/blog/2016/08/innovation-in-analytics-education-great-lakes-using-mentored-learning-for-online-courses/
 259. https://www.analyticsvidhya.com/blog/2016/08/tutorial-data-science-command-line-scikit-learn/
 260. https://www.analyticsvidhya.com/blog/author/guest-blog/
 261. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 262. https://www.facebook.com/analyticsvidhya/
 263. https://twitter.com/analyticsvidhya
 264. https://plus.google.com/+analyticsvidhya
 265. https://plus.google.com/+analyticsvidhya
 266. https://in.linkedin.com/company/analytics-vidhya
 267. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 268. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 269. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 270. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 271. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 272. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 273. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 274. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 275. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 276. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 277. javascript:void(0);
 278. javascript:void(0);
 279. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 280. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 281. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 282. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 283. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 284. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 285. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 286. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 287. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 288. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f08%2fevolution-core-concepts-deep-learning-neural-networks%2f&linkname=the%20evolution%20and%20core%20concepts%20of%20deep%20learning%20%26amp%3b%20neural%20networks
 289. javascript:void(0);
 290. javascript:void(0);
