   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    40 interview questions asked at startups in
   machine learning / data science comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]career [94]40 interview questions asked at startups in
   machine learning / data science

   [95]career[96]interviews[97]machine learning

40 interview questions asked at startups in machine learning / data science

   [98]analytics vidhya content team, september 16, 2016

introduction

     careful! these questions can make you think thrice!

   machine learning and data science are being looked as the drivers of
   the next industrial revolution happening in the world today. this also
   means that there are numerous [99]exciting startups looking for data
   scientists.  what could be a better start for your aspiring career!

   however, still, getting into these roles is not easy. you obviously
   need to get excited about the idea, team and the vision of the company.
   you might also find some real difficult techincal questions on your
   way. the set of questions asked depend on what does the startup do. do
   they provide consulting? do they build ml products ? you should always
   find this out prior to beginning your interview preparation.

   to help you prepare for your next interview, i   ve prepared a list of 40
   plausible & tricky questions which are likely to come across your way
   in interviews. if you can answer and understand these question, rest
   assured, you will give a tough fight in your job interview.

   note: a key to answer these questions is to have concrete practical
   understanding on ml and related statistical concepts. you can get that
   know-how in our course    [100]introduction to data science   !

   or how about learning how to crack data science interviews from someone
   who has conducted hundreds of them? check out the    [101]ace data
   science interviews    course taught by kunal jain and pranav dar.

   40 interview questions asked at startups in machine learning


interview questions on machine learning

   q1. you are given a train data set having 1000 columns and 1 million
   rows. the data set is based on a classification problem. your manager
   has asked you to reduce the dimension of this data so that model
   computation time can be reduced. your machine has memory constraints.
   what would you do? (you are free to make practical assumptions.)

   answer: processing a high dimensional data on a limited memory
   machine is a strenuous task, your interviewer would be fully aware of
   that. following are the methods you can use to tackle such situation:
    1. since we have lower ram, we should close all other applications in
       our machine, including the web browser, so that most of the memory
       can be put to use.
    2. we can randomly sample the data set. this means, we can create a
       smaller data set, let   s say, having 1000 variables and 300000 rows
       and do the computations.
    3. to reduce dimensionality, we can separate the numerical and
       categorical variables and remove the correlated variables. for
       numerical variables, we   ll use correlation. for categorical
       variables, we   ll use chi-square test.
    4. also, we can use [102]pca and pick the components which can explain
       the maximum variance in the data set.
    5. using online learning algorithms like vowpal wabbit (available in
       python) is a possible option.
    6. building a linear model using stochastic id119 is also
       helpful.
    7. we can also apply our business understanding to estimate which all
       predictors can impact the response variable. but, this is an
       intuitive approach, failing to identify useful predictors might
       result in significant loss of information.

   note: for point 4 & 5, make sure you read about [103]online learning
   algorithms & [104]stochastic id119. these are advanced
   methods.


   q2. is rotation necessary in pca? if yes, why? what will happen if you
   don   t rotate the components?

   answer: yes, rotation (orthogonal) is necessary because it maximizes
   the difference between variance captured by the component. this makes
   the components easier to interpret. not to forget, that   s the motive of
   doing pca where, we aim to select fewer components (than features)
   which can explain the maximum variance in the data set. by doing
   rotation, the relative location of the components doesn   t change, it
   only changes the actual coordinates of the points.

   if we don   t rotate the components, the effect of pca will diminish and
   we   ll have to select more number of components to explain variance in
   the data set.

   know more: [105]pca


   q3. you are given a data set. the data set has missing values which
   spread along 1 standard deviation from the median. what percentage of
   data would remain unaffected? why?

   answer: this question has enough hints for you to start
   thinking! since, the data is spread across median, let   s assume it   s a
   normal distribution. we know, in a normal distribution, ~68% of the
   data lies in 1 standard deviation from mean (or mode, median), which
   leaves ~32% of the data unaffected. therefore, ~32% of the data would
   remain unaffected by missing values.


   q4. you are given a data set on cancer detection. you   ve build a
   classification model and achieved an accuracy of 96%. why shouldn   t you
   be happy with your model performance? what can you do about it?

   answer: if you have worked on enough data sets, you should deduce that
   cancer detection results in imbalanced data. in an imbalanced data set,
   accuracy should not be used as a measure of performance because 96% (as
   given) might only be predicting majority class correctly, but our class
   of interest is minority class (4%) which is the people who actually got
   diagnosed with cancer. hence, in order to evaluate model performance,
   we should use sensitivity (true positive rate), specificity (true
   negative rate), f measure to determine class wise performance of the
   classifier. if the minority class performance is found to to be poor,
   we can undertake the following steps:
    1. we can use undersampling, oversampling or smote to make the data
       balanced.
    2. we can alter the prediction threshold value by doing
       [106]id203 caliberation and finding a optimal threshold using
       auc-roc curve.
    3. we can assign weight to classes such that the minority classes
       gets larger weight.
    4. we can also use anomaly detection.

   know more: [107]imbalanced classification


   q5. why is naive bayes so    naive    ?

   answer: naive bayes is so    naive    because it assumes that all of the
   features in a data set are equally important and independent. as we
   know, these assumption are rarely true in real world scenario.


   q6. explain prior id203, likelihood and marginal likelihood in
   context of naivebayes algorithm?

   answer: prior id203 is nothing but, the proportion of dependent
   (binary) variable in the data set. it is the closest guess you can make
   about a class, without any further information. for example: in a data
   set, the dependent variable is binary (1 and 0). the proportion of 1
   (spam) is 70% and 0 (not spam) is 30%. hence, we can estimate that
   there are 70% chances that any new email would  be classified as spam.

   likelihood is the id203 of classifying a given observation as 1
   in presence of some other variable. for example: the id203 that
   the word    free    is used in previous spam message is likelihood.
   marginal likelihood is, the id203 that the word    free    is used in
   any message.


   q7. you are working on a time series data set. you manager has asked
   you to build a high accuracy model. you start with the decision tree
   algorithm, since you know it works fairly well on all kinds of data.
   later, you tried a time series regression model and got higher accuracy
   than decision tree model. can this happen? why?

   answer: time series data is known to posses linearity. on the other
   hand, a decision tree algorithm is known to work best to detect non    
   linear interactions. the reason why decision tree failed to provide
   robust predictions because it couldn   t map the linear relationship as
   good as a regression model did. therefore, we learned that, a linear
   regression model can provide robust prediction given the data set
   satisfies its [108]linearity assumptions.


   q8. you are assigned a new project which involves helping a food
   delivery company save more money. the problem is, company   s delivery
   team aren   t able to deliver food on time. as a result, their
   customers get unhappy. and, to keep them happy, they end up delivering
   food for free. which machine learning algorithm can save them?

   answer: you might have started hopping through the list of ml
   algorithms in your mind. but, wait! such questions are asked to
   test your machine learning fundamentals.

   this is not a machine learning problem. this is a route optimization
   problem. a machine learning problem consist of three things:
    1. there exist a pattern.
    2. you cannot solve it mathematically (even by writing exponential
       equations).
    3. you have data on it.

   always look for these three factors to decide if machine learning is a
   tool to solve a particular problem.


   q9. you came to know that your model is suffering from low bias and
   high variance. which algorithm should you use to tackle it? why?

   answer:  low bias occurs when the model   s predicted values are near to
   actual values. in other words, the model becomes flexible enough to
   mimic the training data distribution. while it sounds like great
   achievement, but not to forget, a flexible model has no generalization
   capabilities. it means, when this model is tested on an unseen data, it
   gives disappointing results.

   in such situations, we can use id112 algorithm (like id79)
   to tackle high variance problem. id112 algorithms divides a data set
   into subsets made with repeated randomized sampling. then, these
   samples are used to generate  a set of models using a single learning
   algorithm. later, the model predictions are combined using voting
   (classification) or averaging (regression).

   also, to combat high variance, we can:
    1. use id173 technique, where higher model coefficients get
       penalized, hence lowering model complexity.
    2. use top n features from variable importance chart. may be, with all
       the variable in the data set, the algorithm is having difficulty in
       finding the meaningful signal.


   q10. you are given a data set. the data set contains many variables,
   some of which are highly correlated and you know about it. your manager
   has asked you to run pca. would you remove correlated variables first?
   why?

   answer: chances are, you might be tempted to say no, but that would be
   incorrect. discarding correlated variables have a substantial effect
   on pca because, in presence of correlated variables, the variance
   explained by a particular component gets inflated.

   for example: you have 3 variables in a data set, of which 2 are
   correlated. if you run pca on this data set, the first principal
   component would exhibit twice the variance than it would exhibit with
   uncorrelated variables. also, adding correlated variables lets pca put
   more importance on those variable, which is misleading.


   q11. after spending several hours, you are now anxious to build a high
   accuracy model. as a result, you build 5 gbm models, thinking a
   boosting algorithm would do the magic. unfortunately, neither of models
   could perform better than benchmark score. finally, you decided to
   combine those models. though, ensembled models are known to return high
   accuracy, but you are unfortunate. where did you miss?

   answer: as we know, ensemble learners are based on the idea of
   combining weak learners to create strong learners. but, these learners
   provide superior result when the combined models are uncorrelated.
   since, we have used 5 gbm models and got no accuracy improvement,
   suggests that the models are correlated. the problem with correlated
   models is, all the models provide same information.

   for example: if model 1 has classified user1122 as 1, there are high
   chances model 2 and model 3 would have done the same, even if
   its actual value is 0. therefore, ensemble learners are built on the
   premise of combining weak uncorrelated models to obtain
   better predictions.


   q12. how is knn different from kmeans id91?

   answer: don   t get mislead by    k    in their names. you should know that
   the fundamental difference between both these algorithms is, kmeans is
   unsupervised in nature and knn is supervised in nature. kmeans is a
   id91 algorithm. knn is a classification (or regression)
   algorithm.

   kmeans algorithm partitions a data set into clusters such that a
   cluster formed is homogeneous and the points in each cluster are close
   to each other. the algorithm tries to maintain enough separability
   between these clusters. due to unsupervised nature, the clusters have
   no labels.

   knn algorithm tries to classify an unlabeled observation based on its k
   (can be any number ) surrounding neighbors. it is also known as lazy
   learner because it involves minimal training of model. hence, it
   doesn   t use training data to make generalization on unseen data set.


   q13. how is true positive rate and recall related? write the equation.

   answer: true positive rate = recall. yes, they are equal having the
   formula (tp/tp + fn).

   know more: [109]id74


   q14. you have built a multiple regression model. your model r   isn   t as
   good as you wanted. for improvement, your remove the intercept term,
   your model r   becomes 0.8 from 0.3. is it possible? how?

   answer: yes, it is possible. we need to understand the significance of
   intercept term in a regression model. the intercept term shows model
   prediction without any independent variable i.e. mean prediction. the
   formula of r   = 1        (y     y  )  /   (y     ymean)   where y   is predicted
   value.

   when intercept term is present, r   value evaluates your model wrt. to
   the mean model. in absence of intercept term (ymean), the model can
   make no such evaluation, with large denominator,    (y -
   y  )  /   (y)   equation   s value becomes smaller than actual, resulting in
   higher r  .


   q15. after analyzing the model, your manager has informed that your
   regression model is suffering from multicollinearity. how would you
   check if he   s true? without losing any information, can you still build
   a better model?

   answer: to check multicollinearity, we can create a correlation matrix
   to identify & remove variables having correlation above 75% (deciding a
   threshold is subjective). in addition, we can use calculate vif
   (variance inflation factor) to check the presence of
   multicollinearity. vif value <= 4 suggests no multicollinearity whereas
   a value of >= 10 implies serious multicollinearity. also, we can use
   tolerance as an indicator of multicollinearity.

   but, removing correlated variables might lead to loss of information.
   in order to retain those variables, we can use
   penalized regression models like ridge or lasso regression. also, we
   can add some random noise in correlated variable so that the variables
   become different from each other. but, adding noise might affect the
   prediction accuracy, hence this approach should be carefully used.

   know more: [110]regression


   q16. when is ridge regression favorable over lasso regression?

   answer: you can quote islr   s authors hastie, tibshirani who asserted
   that, in presence of few variables with medium / large sized effect,
   use lasso regression. in presence of many variables with small / medium
   sized effect, use ridge regression.

   conceptually, we can say, lasso regression (l1) does both variable
   selection and parameter shrinkage, whereas ridge regression only does
   parameter shrinkage and end up including all the coefficients in the
   model. in presence of correlated variables, ridge regression might be
   the preferred choice. also, ridge regression works best in situations
   where the least square estimates have higher variance. therefore, it
   depends on our model objective.

   know more: [111]ridge and lasso regression


   q17. rise in global average temperature led to decrease in number of
   pirates around the world. does that mean that decrease in number of
   pirates caused the climate change?

   answer: after reading this question, you should have understood that
   this is a classic case of    causation and correlation   . no, we can   t
   conclude that decrease in number of pirates caused the climate change
   because there might be other factors (lurking or confounding variables)
   influencing this phenomenon.

   therefore, there might be a correlation between global average
   temperature and number of pirates, but based on this information we
   can   t say that pirated died because of rise in global average
   temperature.

   know more: [112]causation and correlation


   q18. while working on a data set, how do you select important
   variables? explain your methods.

   answer: following are the methods of variable selection you can use:
    1. remove the correlated variables prior to selecting important
       variables
    2. use id75 and select variables based on p values
    3. use forward selection, backward selection, stepwise selection
    4. use id79, xgboost and plot variable importance chart
    5. use lasso regression
    6. measure information gain for the available set of features and
       select top n features accordingly.


   q19. what is the difference between covariance and correlation?

   answer: correlation is the standardized form of covariance.

   covariances are difficult to compare. for example: if we calculate the
   covariances of salary ($) and age (years), we   ll get different
   covariances which can   t be compared because of having unequal scales.
   to combat such situation, we calculate correlation to get a value
   between -1 and 1, irrespective of their respective scale.


   q20. is it possible capture the correlation between continuous and
   categorical variable? if yes, how?

   answer: yes, we can use ancova (analysis of covariance) technique to
   capture association between continuous and categorical variables.


   q21. both being tree based algorithm, how is id79 different
   from gradient boosting algorithm (gbm)?

   answer: the fundamental difference is, id79 uses id112
   technique to make predictions. gbm uses boosting techniques to make
   predictions.

   in id112 technique, a data set is divided into n samples using
   randomized sampling. then, using a single learning algorithm a model is
   build on all samples. later, the resultant predictions are combined
   using voting or averaging. id112 is done is parallel. in boosting,
   after the first round of predictions, the algorithm weighs
   misclassified predictions higher, such that they can be corrected in
   the succeeding round. this sequential process of giving higher weights
   to misclassified predictions continue until a stopping criterion is
   reached.

   id79 improves model accuracy by reducing variance (mainly).
   the trees grown are uncorrelated to maximize the decrease in variance.
   on the other hand, gbm improves accuracy my reducing both bias and
   variance in a model.

   know more: [113]tree based modeling


   q22. running a binary classification tree algorithm is the easy part.
   do you know how does a tree splitting takes place i.e. how does the
   tree decide which variable to split at the root node and succeeding
   nodes?

   answer: a classification trees makes decision based on gini index and
   node id178. in simple words, the tree algorithm find the best
   possible feature which can divide the data set into purest possible
   children nodes.

   gini index says, if we select two items from a population at random
   then they must be of same class and id203 for this is 1 if
   population is pure. we can calculate gini as following:
    1. calculate gini for sub-nodes, using formula sum of square of
       id203 for success and failure (p^2+q^2).
    2. calculate gini for split using weighted gini score of each node of
       that split

   id178 is the measure of impurity as given by (for binary class):

   id178, decision tree

   here p and q is id203 of success and failure respectively in that
   node. id178 is zero when a node is homogeneous. it is maximum when a
   both the classes are present in a node at 50%     50%.  lower id178 is
   desirable.


   q23. you   ve built a id79 model with 10000 trees. you got
   delighted after getting training error as 0.00. but, the validation
   error is 34.23. what is going on? haven   t you trained your model
   perfectly?

   answer: the model has overfitted. training error 0.00 means the
   classifier has mimiced the training data patterns to an extent, that
   they are not available in the unseen data. hence, when this classifier
   was run on unseen sample, it couldn   t find those patterns and returned
   prediction with higher error. in id79, it happens when we use
   larger number of trees than necessary. hence, to avoid these situation,
   we should tune number of trees using cross validation.


   q24. you   ve got a data set to work having p (no. of variable) > n (no.
   of observation). why is ols as bad option to work with? which
   techniques would be best to use? why?

   answer: in such high dimensional data sets, we can   t use classical
   regression techniques, since their assumptions tend to fail. when p >
   n, we can no longer calculate a unique least square coefficient
   estimate, the variances become infinite, so ols cannot be used at all.

   to combat this situation, we can use penalized regression methods like
   lasso, lars, ridge which can shrink the coefficients to reduce
   variance. precisely, ridge regression works best in situations where
   the least square estimates have higher variance.

   among other methods include subset regression, forward stepwise
   regression.


   11222 q25. what is convex hull ? (hint: think id166)

   answer: in case of linearly separable data, convex hull represents the
   outer boundaries of the two group of data points. once convex hull is
   created, we get maximum margin hyperplane (mmh) as a perpendicular
   bisector between two convex hulls. mmh is the line which attempts to
   create greatest separation between two groups.


   q26. we know that one hot encoding increasing the dimensionality of a
   data set. but, label encoding doesn   t. how ?

   answer: don   t get baffled at this question. it   s a simple question
   asking the difference between the two.

   using one hot encoding, the dimensionality (a.k.a features) in a data
   set get increased because it creates a new variable for each level
   present in categorical variables. for example: let   s say we have a
   variable    color   . the variable has 3 levels namely red, blue and green.
   one hot encoding    color    variable will generate three new variables as
   color.red, color.blue and color.green containing 0 and 1 value.

   in label encoding, the levels of a categorical variables gets encoded
   as 0 and 1, so no new variable is created. label encoding is
   majorly used for binary variables.


   q27. what cross validation technique would you use on time series data
   set? is it k-fold or loocv?

   answer: neither.

   in time series problem, k fold can be troublesome because there might
   be some pattern in year 4 or 5 which is not in year 3. resampling the
   data set will separate these trends, and we might end up validation on
   past years, which is incorrect. instead, we can use forward chaining
   strategy with 5 fold as shown below:
     * fold 1 : training [1], test [2]
     * fold 2 : training [1 2], test [3]
     * fold 3 : training [1 2 3], test [4]
     * fold 4 : training [1 2 3 4], test [5]
     * fold 5 : training [1 2 3 4 5], test [6]

   where 1,2,3,4,5,6 represents    year   .


   q28. you are given a data set consisting of variables having more than
   30% missing values? let   s say, out of 50 variables, 8 variables have
   missing values higher than 30%. how will you deal with them?

   answer: we can deal with them in the following ways:
    1. assign a unique category to missing values, who knows the missing
       values might decipher some trend
    2. we can remove them blatantly.
    3. or, we can sensibly check their distribution with the target
       variable, and if found any pattern we   ll keep those missing
       values and assign them a new category while removing others.


   29.    people who bought this, also bought       recommendations seen on
   amazon is a result of which algorithm?

   answer: the basic idea for this kind of recommendation engine comes
   from id185.

   id185 algorithm considers    user behavior    for
   recommending items. they exploit behavior of other users and items in
   terms of transaction history, ratings, selection and purchase
   information. other users behaviour and preferences over the items are
   used to recommend items to the new users. in this case, features of the
   items are not known.

   know more: [114]recommender system


   q30. what do you understand by type i vs type ii error ?

   answer: type i error is committed when the null hypothesis is true and
   we reject it, also known as a    false positive   . type ii error is
   committed when the null hypothesis is false and we accept it, also
   known as    false negative   .

   in the context of confusion matrix, we can say type i error occurs when
   we classify a value as positive (1) when it is actually negative (0).
   type ii error occurs when we classify a value as negative (0) when it
   is actually positive(1).


   q31. you are working on a classification problem. for validation
   purposes, you   ve randomly sampled the training data set into train and
   validation. you are confident that your model will work incredibly well
   on unseen data since your validation accuracy is high. however, you get
   shocked after getting poor test accuracy. what went wrong?

   answer: in case of classification problem, we should always use
   stratified sampling instead of random sampling. a random sampling
   doesn   t takes into consideration the proportion of target classes. on
   the contrary, stratified sampling helps to maintain the distribution of
   target variable in the resultant distributed samples also.


   q32. you have been asked to evaluate a regression model based on r  ,
   adjusted r   and tolerance. what will be your criteria?

   answer: tolerance (1 / vif) is used as an indicator of
   multicollinearity. it is an indicator of percent of variance in a
   predictor which cannot be accounted by other predictors. large values
   of tolerance is desirable.

   we will consider adjusted r   as opposed to r   to evaluate model fit
   because r   increases irrespective of improvement in prediction
   accuracy as we add more variables. but, adjusted r   would only increase
   if an additional variable improves the accuracy of model, otherwise
   stays same. it is difficult to commit a general threshold value for
   adjusted r   because it varies between data sets. for example: a gene
   mutation data set might result in lower adjusted r   and still provide
   fairly good predictions, as compared to a stock market data
   where lower adjusted r   implies that model is not good.


   q33. in id116 or knn, we use euclidean distance to calculate the
   distance between nearest neighbors. why not manhattan distance ?

   answer: we don   t use manhattan distance because it calculates distance
   horizontally or vertically only. it has dimension restrictions. on the
   other hand, euclidean metric can be used in any space to calculate
   distance. since, the data points can be present in any dimension,
   euclidean distance is a more viable option.

   example: think of a chess board, the movement made by a bishop or a
   rook is calculated by manhattan distance because of their respective
   vertical & horizontal movements.


   q34. explain machine learning to me like a 5 year old.

   answer: it   s simple. it   s just like how babies learn to walk. every
   time they fall down, they learn (unconsciously) & realize that their
   legs should be straight and not in a bend position. the next time they
   fall down, they feel pain. they cry. but, they learn    not to stand like
   that again   . in order to avoid that pain, they try harder. to succeed,
   they even seek support from the door or wall or anything near them,
   which helps them stand firm.

   this is how a machine works & develops intuition from its environment.

   note: the interview is only trying to test if have the ability of
   explain complex concepts in simple terms.


   q35. i know that a id75 model is generally evaluated using
   adjusted r   or f value. how would you evaluate a id28
   model?

   answer: we can use the following methods:
    1. since id28 is used to predict probabilities, we can
       use auc-roc curve along with confusion matrix to determine its
       performance.
    2. also, the analogous metric of adjusted r   in id28 is
       aic. aic is the measure of fit which penalizes model for the number
       of model coefficients. therefore, we always prefer model with
       minimum aic value.
    3. null deviance indicates the response predicted by a model with
       nothing but an intercept. lower the value, better the model.
       residual deviance indicates the response predicted by a model on
       adding independent variables. lower the value, better the model.

   know more: [115]id28


   q36. considering the long list of machine learning algorithm, given a
   data set, how do you decide which one to use?

   answer: you should say, the choice of machine learning algorithm solely
   depends of the type of data. if you are given a data set which is
   exhibits linearity, then id75 would be the best algorithm
   to use. if you given to work on images, audios, then neural network
   would help you to build a robust model.

   if the data comprises of non linear interactions, then a boosting or
   id112 algorithm should be the choice. if the business requirement is
   to build a model which can be deployed, then we   ll use regression or a
   decision tree model (easy to interpret and explain) instead of black
   box algorithms like id166, gbm etc.

   in short, there is no one master algorithm for all situations. we
   must be scrupulous enough to understand which algorithm to use.


   q37. do you suggest that treating a categorical variable as continuous
   variable would result in a better predictive model?

   answer: for better predictions, categorical variable can be considered
   as a continuous variable only when the variable is ordinal in nature.


   q38. when does id173 becomes necessary in machine learning?

   answer: id173 becomes necessary when the model begins to
   ovefit / underfit. this technique introduces a cost term for bringing
   in more features with the objective function. hence, it tries to push
   the coefficients for many variables to zero and hence reduce cost term.
   this helps to reduce model complexity so that the model can become
   better at predicting (generalizing).


   q39. what do you understand by bias variance trade off?

   answer:  the error emerging from any model can be broken down into
   three components mathematically. following are these component :

   [116]error of a model

   bias error is useful to quantify how much on an average are the
   predicted values different from the actual value. a high bias error
   means we have a under-performing model which keeps on missing important
   trends. variance on the other side quantifies how are the prediction
   made on same observation different from each other. a high variance
   model will over-fit on your training population and perform badly on
   any observation beyond training.


   q40. ols is to id75. maximum likelihood is to logistic
   regression. explain the statement.

   answer: ols and maximum likelihood are the methods used by the
   respective regression methods to approximate the unknown parameter
   (coefficient) value. in simple words,

   ordinary least square(ols) is a method used in id75 which
   approximates the parameters resulting in minimum distance between
   actual and predicted values. maximum likelihood helps in choosing the
   the values of parameters which maximizes the likelihood that the
   parameters are most likely to produce observed data.


end notes

   you might have been able to answer all the questions, but the real
   value is in understanding them and generalizing your knowledge on
   similar questions. if you have struggled at these questions, no
   worries, now is the time to learn and not perform. you should right now
   focus on learning these topics scrupulously.

   these questions are meant to give you a wide exposure on the types of
   questions asked at startups in machine learning. i   m sure these
   questions would leave you curious enough to do deeper topic research at
   your end. if you are planning for it, that   s a good sign.

   did you like reading this article? have you appeared in any startup
   interview recently for data scientist profile? do share your experience
   in comments below. i   d love to know your experience.

looking for a job in analytics? check out [117]currently hiring jobs in
machine learning and data science.

   you can also read this article on analytics vidhya's android app
   [118]get it on google play

share this:

     * [119]click to share on linkedin (opens in new window)
     * [120]click to share on facebook (opens in new window)
     * [121]click to share on twitter (opens in new window)
     * [122]click to share on pocket (opens in new window)
     * [123]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [124]id112, [125]bias variance tradeoff, [126]boosting,
   [127]cross-validation, [128]data science in startups, [129]data
   scientist in startups, [130]data scientist interview, [131]interview
   questions, [132]interviews, [133]machine learning, [134]machine
   learning engineer interview question, [135]maximum likelihood,
   [136]ordinary least square, [137]id173
   next article

comprehensive introduction to apache spark, rdds & dataframes (using pyspark)

   previous article

aws / cloud engineer     pune ( 4+ years of experience )

[138]analytics vidhya content team

   analytics vidhya content team

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [139]discussion portal to get your queries resolved

33 comments

     * kavitha says:
       [140]september 16, 2016 at 7:09 am
       thank you so much manish
       [141]reply
          + analytics vidhya content team says:
            [142]september 16, 2016 at 9:17 am
            hi kavitha, i hope these questions help you to prepare for
            forthcoming interview rounds. all the best.
            [143]reply
     * gianni says:
       [144]september 16, 2016 at 7:12 am
       thank you manish, very helpfull to face on the true reality that a
       long long journey wait me     
       [145]reply
          + analytics vidhya content team says:
            [146]september 16, 2016 at 9:15 am
            hi gianni
            good to know, you found them helpful! all the best.
            [147]reply
          + analytics vidhya content team says:
            [148]september 16, 2016 at 9:17 am
            hi gianni, i am happy to know that these question would help
            you in your journey. all the best.
            [149]reply
     * prof ravi vadlamani says:
       [150]september 16, 2016 at 7:46 am
       good collection compiled by you mr manish ! kudos !
       i am sure it will be very useful to the budding data scientists
       whether they face start-ups or established firms.
       [151]reply
          + analytics vidhya content team says:
            [152]september 16, 2016 at 9:20 am
            hi prof ravi, you are right. these questions can be asked
            anywhere. but, with growth in machine learning startups,
            facing off ml algorithm related question have higher chances,
            though i have laid emphasis on statistical modeling as well.
            [153]reply
     * srinivas says:
       [154]september 16, 2016 at 9:05 am
       thank you manish.helpful for beginners like me.
       [155]reply
          + analytics vidhya content team says:
            [156]september 16, 2016 at 9:14 am
            welcome     
            [157]reply
     * chibole says:
       [158]september 16, 2016 at 9:35 am
       it seems stastics is at the centre of machine learning.
       [159]reply
          + chibole says:
            [160]september 16, 2016 at 9:38 am
            * stastics = statistics
            [161]reply
          + analytics vidhya content team says:
            [162]september 16, 2016 at 9:39 am
            hi chibole,
            true, statistics in an inevitable part of machine learning.
            one needs to understand statistical concepts in order to
            master machine learning.
            [163]reply
               o chibole says:
                 [164]september 16, 2016 at 9:55 am
                 i was wondering, do you recommend for somebody to special
                 in a specific field of ml? i mean, it is recommended to
                 choose between supervised learning and unsupervised
                 learning algorithms, and simply say my specialty is this
                 during an interview. shouldn   t organizations recruiting
                 specify their specialty requirements too?
                    .and thank you for the post.
                 [165]reply
                    # analytics vidhya content team says:
                      [166]september 16, 2016 at 10:40 am
                      hi chibole,
                      it   s always a good thing to establish yourself as an
                      expert in a specific field. this helps the recruiter
                      to understand that you are a detailed oriented
                      person. in machine learning, thinking of building
                      your expertise in supervised learning would be good,
                      but companies want more than that. considering, the
                      variety of data these days, they want someone who
                      can deal with unlabeled data also. in short, they
                      look for someone who isn   t just an expert in
                      operating sniper gun, but can use other weapons also
                      if needed.
                      [167]reply
     * chibole says:
       [168]september 16, 2016 at 9:36 am
       * stastics = statistics
       [169]reply
     * [170]karthikeyan sankaran says:
       [171]september 16, 2016 at 1:10 pm
       hi manish     interesting & informative set of questions & answers.
       thanks for compiling the same.
       [172]reply
          + analytics vidhya content team says:
            [173]september 19, 2016 at 12:36 pm
            most welcome !     
            [174]reply
     * nicola says:
       [175]september 16, 2016 at 7:22 pm
       hi, really an interesting collection of answers. from a merely
       statistical point of view there are some imprecisions (e.g. q40),
       but it is surely useful for job interviews in startups and bigger
       firms.
       [176]reply
          + analytics vidhya content team says:
            [177]september 17, 2016 at 4:11 am
            hi nicola,
            thanks for sharing your thoughts. tell me more about q40.
            what   s about it?
            [178]reply
     * [179]raju says:
       [180]september 17, 2016 at 1:45 am
       i think you got q3 wrong.
       it was to calculate from median and not mean.
       how can assume mean and median to be same
       [181]reply
     * [182]raju says:
       [183]september 17, 2016 at 1:53 am
       don   t bother   ..noted    ..you assumed normal distribution   .
       [184]reply
     * amit srivastava says:
       [185]september 17, 2016 at 7:07 am
       great article. it will help in understanding which topics to focus
       on for interview purposes.
       [186]reply
          + analytics vidhya content team says:
            [187]september 19, 2016 at 12:35 pm
            hi amit,
            thanks for your encouraging words! the purpose of this article
            is to help beginners understand the tricky side of ml
            interviews.
            [188]reply
     * chinmaya mishra says:
       [189]september 17, 2016 at 1:13 pm
       dear kunal,
       few queries i have regarding aic
       1)why we multiply -2 to the aic equation
       2)where this equation has been built.
       rgds
       [190]reply
     * sampath says:
       [191]september 18, 2016 at 5:33 pm
       hi manish,
       great job! it is a very good collection of interview questions on
       machine learning. it will be a great help if you can also publish a
       similar article on statistics. thanks in advance
       [192]reply
          + analytics vidhya content team says:
            [193]september 19, 2016 at 12:34 pm
            hi sampath,
            thanks for your suggestion. i   ll surely consider it in my
            forthcoming articles.
            [194]reply
     * [195]karthi v says:
       [196]september 20, 2016 at 3:32 pm
       hi manish,
       kudos to you!!! good collection for beginners
       i have small suggestion on id84,we can also use
       the below mentioned techniques to reduce the dimension of the data.
       1.missing values ratio
       data columns with too many missing values are unlikely to carry
       much useful information. thus data columns with number of missing
       values greater than a given threshold can be removed. the higher
       the threshold, the more aggressive the reduction.
       2.low variance filter
       similarly to the previous technique, data columns with little
       changes in the data carry little information. thus all data columns
       with variance lower than a given threshold are removed. a word of
       caution: variance is range dependent; therefore id172 is
       required before applying this technique.
       3.high correlation filter.
       data columns with very similar trends are also likely to carry very
       similar information. in this case, only one of them will suffice to
       feed the machine learning model. here we calculate the correlation
       coefficient between numerical columns and between nominal columns
       as the pearson   s product moment coefficient and the pearson   s chi
       square value respectively. pairs of columns with correlation
       coefficient higher than a threshold are reduced to only one. a word
       of caution: correlation is scale sensitive; therefore column
       id172 is required for a meaningful correlation comparison.
       4.id79s / ensemble trees
       decision tree ensembles, also referred to as id79s, are
       useful for feature selection in addition to being effective
       classifiers. one approach to id84 is to
       generate a large and carefully constructed set of trees against a
       target attribute and then use each attribute   s usage statistics to
       find the most informative subset of features. specifically, we can
       generate a large set (2000) of very shallow trees (2 levels), with
       each tree being trained on a small fraction (3) of the total number
       of attributes. if an attribute is often selected as best split, it
       is most likely an informative feature to retain. a score calculated
       on the attribute usage statistics in the id79 tells us    
       relative to the other attributes     which are the most predictive
       attributes.
       5.backward feature elimination
       in this technique, at a given iteration, the selected
       classification algorithm is trained on n input features. then we
       remove one input feature at a time and train the same model on n-1
       input features n times. the input feature whose removal has
       produced the smallest increase in the error rate is removed,
       leaving us with n-1 input features. the classification is then
       repeated using n-2 features, and so on. each iteration k produces a
       model trained on n-k features and an error rate e(k). selecting the
       maximum tolerable error rate, we define the smallest number of
       features necessary to reach that classification performance with
       the selected machine learning algorithm.
       6.forward feature construction.
       this is the inverse process to the backward feature elimination. we
       start with 1 feature only, progressively adding 1 feature at a
       time, i.e. the feature that produces the highest increase in
       performance. both algorithms, backward feature elimination and
       forward feature construction, are quite time and computationally
       expensive. they are practically only applicable to a data set with
       an already relatively low number of input columns.
       [197]reply
     * ramit pandey says:
       [198]september 22, 2016 at 7:28 am
       hi manish ,
       after going through these question i feel i am at 10% of knowledge
       required to pursue career in data science . excellent article to
       read. can you please suggest me any book or training online which
       gives this much deep information . waiting for your reply in
       anticipation . thanks a million
       [199]reply
     * rahul jadhav says:
       [200]september 22, 2016 at 5:36 pm
       amazing collection manish!
       thanks a lot.
       [201]reply
     * nikhils says:
       [202]september 25, 2016 at 3:44 pm
       an awesome article for reference. thanks a ton manish sir for the
       share. please share the pdf format of this blog post if possible.
       have also taken note of karthi   s input!
       [203]reply
     * vinaya says:
       [204]october 5, 2016 at 9:19 am
       ty manish   its an awsm reference   plz upload pdf format also   thanks
       again
       [205]reply
     * prasanna says:
       [206]october 7, 2016 at 5:32 am
       great set of questions manish. btw.. i believe the expressions for
       bias and variance in question 39 is incorrect. i believe the
       brackets are messed. following gives the correct expressions.
       [207]https://en.wikipedia.org/wiki/bias%e2%80%93variance_tradeoff
       [208]reply
     * sidd says:
       [209]november 7, 2016 at 6:42 am
       really awesome article thanks. given the influence young, budding
       students of machine learning will likely have in the future, your
       article is of great value.
       [210]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [211]srk       3924
   2    [2.jpg?date=2019-04-05] [212]mark12    3510
   3    [3.jpg?date=2019-04-05] [213]nilabha   3261
   4    [4.jpg?date=2019-04-05] [214]nitish007 3237
   5    [5.jpg?date=2019-04-05] [215]tezdhar   3082
   [216]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [217]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [218]understanding support vector machine algorithm from examples
       (along with code)
     * [219]essentials of machine learning algorithms (with python and r
       codes)
     * [220]a complete tutorial to learn data science with python from
       scratch
     * [221]7 types of regression techniques you should know!
     * [222]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [223]a simple introduction to anova (with applications in excel)
     * [224]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [225]top 5 machine learning github repositories and reddit discussions
   from march 2019

[226]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [227]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[228]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [229]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[230]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [231]16 opencv functions to start your id161 journey (with
   python code)

[232]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [233][ds-finhack.jpg]

   [234][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [235]about us
     * [236]our team
     * [237]career
     * [238]contact us
     * [239]write for us

   [240]about us
   [241]   
   [242]our team
   [243]   
   [244]careers
   [245]   
   [246]contact us

data scientists

     * [247]blog
     * [248]hackathon
     * [249]discussions
     * [250]apply jobs
     * [251]leaderboard

companies

     * [252]post jobs
     * [253]trainings
     * [254]hiring hackathons
     * [255]advertising
     * [256]reach us

   don't have an account? [257]sign up here.

join our community :

   [258]46336 [259]followers
   [260]20222 [261]followers
   [262]followers
   [263]7513 [264]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [265]privacy policy
     * [266]terms of use
     * [267]refund policy

   don't have an account? [268]sign up here

   iframe: [269]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [270](button) join now

   subscribe!

   iframe: [271]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [272](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/career/
  94. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/
  95. https://www.analyticsvidhya.com/blog/category/career/
  96. https://www.analyticsvidhya.com/blog/category/career/interviews/
  97. https://www.analyticsvidhya.com/blog/category/machine-learning/
  98. https://www.analyticsvidhya.com/blog/author/avcontentteam/
  99. https://www.analyticsvidhya.com/blog/2016/09/18-data-science-iot-startups-y-combinator-summer-2016/
 100. https://trainings.analyticsvidhya.com/courses/course-v1:analyticsvidhya+ds101+2018t2/about?utm_source=40startupquestionarticle&utm_medium=blog
 101. https://trainings.analyticsvidhya.com/courses/course-v1:analyticsvidhya+dsi101+dsi101_2018/about?utm_source=40startupquestionarticle&utm_medium=blog
 102. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
 103. https://www.analyticsvidhya.com/blog/2015/01/introduction-online-machine-learning-simplified-2/
 104. https://www.quora.com/whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent
 105. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
 106. https://www.analyticsvidhya.com/blog/2016/07/platt-scaling-isotonic-regression-minimize-logloss-error/
 107. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/
 108. https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/
 109. https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/
 110. https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/
 111. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/
 112. https://www.analyticsvidhya.com/blog/2015/06/establish-causality-events/
 113. https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/
 114. https://www.analyticsvidhya.com/blog/2015/10/recommendation-engines/
 115. https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/
 116. https://www.analyticsvidhya.com/wp-content/uploads/2015/07/error-of-a-model.png
 117. https://www.analyticsvidhya.com/jobs/
 118. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 119. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/?share=linkedin
 120. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/?share=facebook
 121. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/?share=twitter
 122. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/?share=pocket
 123. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/?share=reddit
 124. https://www.analyticsvidhya.com/blog/tag/id112/
 125. https://www.analyticsvidhya.com/blog/tag/bias-variance-tradeoff/
 126. https://www.analyticsvidhya.com/blog/tag/boosting/
 127. https://www.analyticsvidhya.com/blog/tag/cross-validation/
 128. https://www.analyticsvidhya.com/blog/tag/data-science-in-startups/
 129. https://www.analyticsvidhya.com/blog/tag/data-scientist-in-startups/
 130. https://www.analyticsvidhya.com/blog/tag/data-scientist-interview/
 131. https://www.analyticsvidhya.com/blog/tag/interview-questions/
 132. https://www.analyticsvidhya.com/blog/tag/interviews/
 133. https://www.analyticsvidhya.com/blog/tag/machine-learning/
 134. https://www.analyticsvidhya.com/blog/tag/machine-learning-engineer-interview-question/
 135. https://www.analyticsvidhya.com/blog/tag/maximum-likelihood/
 136. https://www.analyticsvidhya.com/blog/tag/ordinary-least-square/
 137. https://www.analyticsvidhya.com/blog/tag/id173/
 138. https://www.analyticsvidhya.com/blog/author/avcontentteam/
 139. https://discuss.analyticsvidhya.com/
 140. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116110
 141. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116110
 142. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116122
 143. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116122
 144. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116111
 145. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116111
 146. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116119
 147. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116119
 148. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116121
 149. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116121
 150. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116113
 151. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116113
 152. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116123
 153. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116123
 154. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116117
 155. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116117
 156. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116118
 157. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116118
 158. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116124
 159. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116124
 160. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116126
 161. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116126
 162. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116127
 163. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116127
 164. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116128
 165. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116128
 166. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116129
 167. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116129
 168. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116125
 169. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116125
 170. http://bit.ly/31kart8
 171. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116135
 172. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116135
 173. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116209
 174. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116209
 175. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116145
 176. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116145
 177. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116150
 178. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116150
 179. http://none/
 180. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116148
 181. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116148
 182. http://none/
 183. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116149
 184. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116149
 185. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116154
 186. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116154
 187. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116208
 188. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116208
 189. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116161
 190. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116161
 191. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116185
 192. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116185
 193. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116207
 194. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116207
 195. https://github.com/vkarthi46
 196. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116253
 197. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116253
 198. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116334
 199. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116334
 200. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116357
 201. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116357
 202. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116458
 203. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116458
 204. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116815
 205. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116815
 206. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116907
 207. https://en.wikipedia.org/wiki/bias   variance_tradeoff
 208. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-116907
 209. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-118048
 210. https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/#comment-118048
 211. https://datahack.analyticsvidhya.com/user/profile/srk
 212. https://datahack.analyticsvidhya.com/user/profile/mark12
 213. https://datahack.analyticsvidhya.com/user/profile/nilabha
 214. https://datahack.analyticsvidhya.com/user/profile/nitish007
 215. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 216. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 217. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 218. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 219. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 220. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 221. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 222. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 223. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 224. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 225. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 226. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 227. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 228. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 229. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 230. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 231. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 232. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 233. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 234. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 235. http://www.analyticsvidhya.com/about-me/
 236. https://www.analyticsvidhya.com/about-me/team/
 237. https://www.analyticsvidhya.com/career-analytics-vidhya/
 238. https://www.analyticsvidhya.com/contact/
 239. https://www.analyticsvidhya.com/about-me/write/
 240. http://www.analyticsvidhya.com/about-me/
 241. https://www.analyticsvidhya.com/about-me/team/
 242. https://www.analyticsvidhya.com/about-me/team/
 243. https://www.analyticsvidhya.com/about-me/team/
 244. https://www.analyticsvidhya.com/career-analytics-vidhya/
 245. https://www.analyticsvidhya.com/about-me/team/
 246. https://www.analyticsvidhya.com/contact/
 247. https://www.analyticsvidhya.com/blog
 248. https://datahack.analyticsvidhya.com/
 249. https://discuss.analyticsvidhya.com/
 250. https://www.analyticsvidhya.com/jobs/
 251. https://datahack.analyticsvidhya.com/users/
 252. https://www.analyticsvidhya.com/corporate/
 253. https://trainings.analyticsvidhya.com/
 254. https://datahack.analyticsvidhya.com/
 255. https://www.analyticsvidhya.com/contact/
 256. https://www.analyticsvidhya.com/contact/
 257. https://datahack.analyticsvidhya.com/signup/
 258. https://www.facebook.com/analyticsvidhya/
 259. https://www.facebook.com/analyticsvidhya/
 260. https://twitter.com/analyticsvidhya
 261. https://twitter.com/analyticsvidhya
 262. https://plus.google.com/+analyticsvidhya
 263. https://in.linkedin.com/company/analytics-vidhya
 264. https://in.linkedin.com/company/analytics-vidhya
 265. https://www.analyticsvidhya.com/privacy-policy/
 266. https://www.analyticsvidhya.com/terms/
 267. https://www.analyticsvidhya.com/refund-policy/
 268. https://id.analyticsvidhya.com/accounts/signup/
 269. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 270. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 271. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 272. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 274. https://www.facebook.com/analyticsvidhya
 275. https://twitter.com/analyticsvidhya
 276. https://plus.google.com/+analyticsvidhya/posts
 277. https://in.linkedin.com/company/analytics-vidhya
 278. https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/
 279. https://www.analyticsvidhya.com/blog/2016/09/aws-cloud-engineer-pune-4-years-of-experience/
 280. https://www.analyticsvidhya.com/blog/author/avcontentteam/
 281. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 282. https://www.facebook.com/analyticsvidhya/
 283. https://twitter.com/analyticsvidhya
 284. https://plus.google.com/+analyticsvidhya
 285. https://plus.google.com/+analyticsvidhya
 286. https://in.linkedin.com/company/analytics-vidhya
 287. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 288. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 289. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 290. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 291. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 292. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 293. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 294. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 295. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 296. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 297. javascript:void(0);
 298. javascript:void(0);
 299. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 300. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 301. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 302. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 303. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 304. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 305. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 306. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 307. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 308. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f09%2f40-interview-questions-asked-at-startups-in-machine-learning-data-science%2f&linkname=40%20interview%20questions%20asked%20at%20startups%20in%20machine%20learning%20%2f%20data%20science
 309. javascript:void(0);
 310. javascript:void(0);
