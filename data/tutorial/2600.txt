   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    fundamentals of deep learning     starting with
   id158 comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]machine learning [94]fundamentals of deep learning    
   starting with id158

   [95]machine learning

fundamentals of deep learning     starting with id158

   [96]aarshay jain, march 16, 2016

introduction

     did you know the first neural network was discovered in early 1950s
     ?

   deep learning (dl) and neural network (nn) is currently driving some of
   the most ingenious inventions in today   s century. their incredible
   ability to learn from data and environment makes them the first choice
   of machine learning scientists.

   deep learning and neural network lies in the heart of products such as
   self driving cars, image recognition software, recommender systems etc.
   evidently, being a powerful algorithm, it is highly adaptive to various
   data types as well.

   people think neural network is an extremely difficult topic to learn.
   therefore, either some of them don   t use it, or the ones who use it,
   use it as a black box. is there any point in doing something without
   knowing how is it done? no!

   in this article, i   ve attempted to explain the concept of neural
   network in simple words. understanding this article requires
   a little bit of biology and lots of patience. by end of this article,
   you would become a confident analyst ready to start working with neural
   networks. in case you don   t understand anything, i   m always available
   in comments section.

   note: this article is best suited for intermediate users in data
   science & machine learning. beginners might find it challenging.

   fundamentals of neural network


table of contents

    1. what is a neural network?
    2. how a single neuron works?
    3. why multi-layer networks are useful?
    4. general structure of a neural network
    5. back-propagation ( really important )


1. what is a neural network?

   neural networks (nn), also called as id158 is named
   after its artificial representation of working of a human being   s
   nervous system. remember this diagram ? most of us have been taught in
   high school !

   flashback recap: lets start by understanding how our nervous system
   works. nervous system comprises of millions of nerve cells or neurons.
   a neuron has the following structure:

   [97]1. neuron

   the major components are:
     * dendrites- it takes input from other neurons in form of an
       electrical impulse
     * cell body    it generate id136s from those inputs and decide what
       action to take
     * axon terminals    it transmit outputs in form of electrical impulse

   in simple terms, each neuron takes input from numerous other neurons
   through the dendrites. it then performs the required processing on the
   input and sends another electrical pulse through the axiom into the
   terminal nodes from where it is transmitted to numerous other neurons.

   ann works in a very similar fashion. the general structure of a neural
   network looks like:[98] 2. ann structure [99]source

   this figure depicts a typical neural network with working of a single
   neuron explained separately. let   s understand this.

   the input to each neuron are like the dendrites. just like in human
   nervous system, a neuron (artificial though!) collates all the inputs
   and performs an operation on them. lastly, it transmits the output to
   all other neurons (of the next layer) to which it is connected. neural
   network is divided into layer of 3 types:
    1. input layer: the training observations are fed through these
       neurons
    2. hidden layers: these are the intermediate layers between input and
       output which help the neural network learn the complicated
       relationships involved in data.
    3. output layer: the final output is extracted from previous two
       layers. for example: in case of a classification problem with 5
       classes, the output later will have 5 neurons.

   lets start by looking into the functionality of each neuron with
   examples.


2. how a single neuron works?

   in this section, we will explore the working of a single neuron
   with easy examples. the idea is to give you some intuition on how a
   neuron compute outputs using the inputs. a typical neuron looks like:

   how single neuron works

   the different components are:
    1. x[1], x[2],   , x[n]: inputs to the neuron. these can either be the
       actual observations from input layer or an intermediate value from
       one of the hidden layers.
    2. x[0]: bias unit. this is a constant value added to the input of the
       activation function. it works similar to an intercept term and
       typically has +1 value.
    3. w[0],w[1], w[2],   ,w[n]: weights on each input. note that even bias
       unit has a weight.
    4. a: output of the neuron which is calculated as:

   [100]eq1 - neuron

   here f is known an activation function. this makes a neural network
   extremely flexible and imparts the capability to estimate complex
   non-linear relationships in data. it can be a gaussian function,
   logistic function, hyperbolic function or even a linear function in
   simple cases.

   lets implement 3 fundamental functions     or, and, not using neural
   networks. this will help us understand how they work. you can assume
   these to be like a classification problem where we   ll predict the
   output (0 or 1) for different combination of inputs.

   we will model these like linear classifiers with the following
   activation function:

   [101]eq2 - activation fn


example 1: and

   the and function can be implemented as:

   2

   the output of this neuron is:

a = f( -1.5 + x1 + x2 )

   the truth table for this implementation is:

   [102]3. tt and

   here we can see that the and function is successfully implemented.
   column    a    complies with    x1 and x2   . note that here the bias unit
   weight is -1.5. but it   s not a fixed value. intuitively, we can
   understand it as anything which makes the total value positive only
   when both x[1] and x[2] are positive. so any value between (-1,-2)
   would work.


example 2: or

   the or function can be implemented as:

   3

   the output of this neuron is:

a = f( -0.5 + x1 + x2 )

   the truth table for this implementation is:

   [103]4. tt or

   column    a    complies with    x1 or x2   . we can see that, just by changing
   the bias unit weight, we can implement an or function. this is very
   similar to the one above. intuitively, you can understand that here,
   the bias unit is such that the weighted sum will be positive if any of
   x1 or x2 becomes positive.


example 3: not

   just like the previous cases, the not function can be implemented as:

   4a

   the output of this neuron is:

a = f( 1     2*x1 )

   the truth table for this implementation is:

   [104]5. tt not

   again, the compliance with desired value proves functionality. i hope
   with these examples, you   re getting some intuition into how a neuron
   inside a neural network works. here i have used a very simple
   activation function.

   note: generally a logistic function will be used in place of what i
   used here because it is differentiable and makes determination of a
   gradient possible. there   s just 1 catch. and, that is, it outputs
   floating value and not exactly 0 or 1.


3. why multi-layer networks are useful?

   after understanding the working of a single neuron, lets try to
   understand how a neural network can model complex relations using
   multiple layers. to understand this further, we will take the example
   of an xnor function. just a recap, the truth table of an xnor function
   looks like:

   [105]6. xnor tt

   here we can see that the output is 1 when both inputs are same,
   otherwise 0. this sort of a relationship cannot be modeled using a
   single neuron. (don   t believe me? give it a try!) thus we will use a
   multi-layer network. the idea behind using multiple layers is that
   complex relations can be broken into simpler functions and combined.

   lets break down the xnor function.
        x1 xnor x2 = not ( x1 xor x2 )
                   = not [ (a+b).(a'+b') ]       (note: here '+' means or and '.
' mean and)
                   = (a+b)' + (a'+b')'
                   = (a'.b') + (a.b)

   now we can implement it using any of the simplified cases. i will show
   you how to implement this using 2 cases.


case 1: x1 xnor x2 = (a   .b   ) + (a.b)

   here the challenge is to design a neuron to model a   .b    . this can be
   easily modeled using the following:

   4b

   the output of this neuron is:

a = f( 0.5     x1     x2 )

   the truth table for this function is:

   [106]7. tt a'.b'

   now that we have modeled the individual components and we can combine
   them using a multi-layer network. first, lets look at the semantic
   diagram of that network:

   5

   here we can see that in layer 1, we will determine a   .b    and a.b
   individually. in layer 2, we will take their output and implement an or
   function on top. this would complete the entire neural network. the
   final network would look like this:

   6

   if you notice carefully, this is nothing but a combination of the
   different neurons which we have already drawn. the different outputs
   represent different units:
    1. a[1]: implements a   .b   
    2. a[2]: implements a.b
    3. a[3]: implements or which works on a1 and a2, thus effectively
       (a   .b    + a.b)

   the functionality can be verified using the truth table:

   [107]8. tt xnor case 1

   i think now you can get some intuition into how multi-layers work. lets
   do another implementation of the same case.


case 2: x1 xnor x2 = not [ (a+b).(a   +b   ) ]

   in the above example, we had to separately calculate a   .b   . what if we
   want to implement the function just using the basic and, or, not
   functions. consider the following semantic:

   7

   here you can see that we had to use 3 hidden layers. the working will
   be similar to what we did before. the network looks like:

   8

   here the neurons perform following actions:
    1. a[1]: same as a
    2. a[2]: implements a   
    3. a[3]: same as b
    4. a[4]: implements b   
    5. a[5]: implements or, effectively a+b
    6. a[6]: implements or, effectively a   +b   
    7. a[7]: implements and, effectively (a+b).(a   +b   )
    8. a[8:] implements not, effectively not [ (a+b).(a   +b   ) ] which is
       the final xnor

   note that, typically a neuron feeds into every other neuron of the next
   layer except the bias unit. in this case, i   ve obviated few connections
   from layer 1 to layer 2. this is because their weights are 0 and adding
   them will make it visually cumbersome to grasp.

   the truth table is:

   [108]9. tt xnor case 2

   finally, we have successfully implemented xnor function. this method is
   more complicated than case 1. hence, you should prefer case 1 always.
   but the idea here is to show how complicated functions can be broken
   down in multiple layers. i hope the advantages of multiple layers are
   clearer now.


4. general structure of a neural network

   now that we had a look at some basic examples, lets define a generic
   structure in which every neural network falls. we will also see the
   equations to be followed to determine the output given an input. this
   is known as forward propagation.

   a generic neural network can be defined as:

   diagram of a neural network structure


   it has l layers with 1 input layer, 1 output layer and l-2 hidden
   layers. terminology:
     * l: number of layers
     * n[i]: number of neuron in i^th layer excluding the bias unit, where
       i=1,2,   ,l
     * a[i]^(j): the output of the j^th neuron in i^th layer, where
       i=1,2   l | j=0,1,2   .n[i]

   since the the output of each layer forms the input of next layer, lets
   define the equation to determine the output of i+1^th layer using
   output of i^th layer as input.

   the input to the i+1^th layer are:
a[i] = [ a[i]^(0), a[i]^(1), ......, a[i]^(n[i]) ]
dimension: 1 x n[i]+1

   the weights matrix from i^th to i+1^th layer is:
w^(i) = [ [ w[01]^(i)    w[11]^(i)  .......   w[n[i]1]^(i) ]
        [ w[02]^(i)    w[12]^(i)  .......   w[n[i]2]^(i) ]
           ...                   ...
           ...                   ...
           ...                   ...
           ...                   ...
        [ w[0n[i+1]]^(i)  w[1n[i+1]]^(i)  ....... w[n[i]n[i+1]]^(i) ] ]

dimension: n[i+1] x n[i]+1

   the output of the i+1^th layer can be calculated as:
 a[i+1] = f( a[i].w^(i) )
dimension: 1 x n[i+1]

   using these equations for each subsequent layer, we can determine the
   final output. the number of neurons in the output layer will depend on
   the type of problem. it can be 1 for regression or binary
   classification problem or multiple for multi-class classification
   problems.

   but this is just determining the output from 1 run. the ultimate
   objective is to update the weights of the model in order to minimize
   the id168. the weights are updated using a back-propogation
   algorithm which we   ll study next.


5. back-propagation

   back-propagation (bp) algorithms works by determining the loss (or
   error) at the output and then propagating it back into the network. the
   weights are updated to minimize the error resulting from each neuron. i
   will not go in details of the algorithm but i will try to give you some
   intuition into how it works.

   the first step in minimizing the error is to determine the gradient of
   each node wrt. the final output. since, it is a multi-layer network,
   determining the gradient is not very straightforward.

   let   s understand the gradients for multi-layer networks. lets take a
   step back from neural networks and consider a very simple system as
   following:

   [109]diagram 10

   here there are 3 inputs which simple processing as:

d = a     b

e = d * c = (a-b)*c

   now we need to determine the gradients of a,b,c,d wrt the output e. the
   following cases are very straight forward:

   [110]3. eq3

   however, for determining the gradients for a and b, we need to apply
   the chain rule.

   [111]4. eq4


   and, this way the gradient can be computed by simply multiplying the
   gradient of the input to a node with that of the output of that node.
   if you   re still confused, just read the equation carefully 5 times and
   you   ll get it!

   but, the actual cases are not that simple. let   s take another example.
   consider a case where a single input is being fed into multiple items
   in the next layer as this is almost always the case with neural
   network.

   [112]diagram 11

   in this case, the gradients of all other will be very similar to the
   above example except for    m    because m is being fed into 2 nodes. here,
   i   ll show how to determine the gradient for m and rest you should
   calculate on your own.

   [113]5. eq5

   here you can see that the gradient is simply the summation of the two
   different gradients. i hope the cloud cover is slowly vanishing and
   things are becoming lucid. just understand these concepts and we   ll
   come back to this.

   before moving forward, let   s sum up the entire process behind
   optimization of a neural network. the various steps involved in each
   iteration are:
    1. select a network architecture, i.e. number of hidden layers,
       number of neurons in each layer and activation function
    2. initialize weights randomly
    3. use forward propagation to determine the output node
    4. find the error of the model using the known labels
    5. back-propogate the error into the network and determine the error
       for each node
    6. update the weights to minimize gradient

   till now we have covered #1     #3 and we have some intuition into #5.
   now lets start from #4     #6. we   ll use the same generic structure of nn
   as described in section 4.

   #4- find the error
e[l]^(i) = y^(i) - a[l]^(i) | i = 1,2,....,n[l]

   here y^(i) is the actual outcome from training data


   #5- back-propogating the error into the network

   the error for layer l-1 should be determined first using the following:

   [114]6. eq6

   where i = 0,1,2,    .., nl-1 (number of nodes in l-1th layer)

   intuition from the concepts discussed in former half of this section:
     * we saw that the gradient of a node is a function of the gradients
       of all nodes from next layer. here, the error at a node is based on
       weighted sum of errors on all the nodes of the next layer which
       take output of this node as input. since errors are calculated
       using gradients of each node, the factor comes into picture.
     * f'(x)^(i) refers to the derivative of the activation function for
       the inputs coming into that node. note that x refers to weighted
       sum of all inputs in present node before application of activation
       function.
     * the chain rule is followed here by multiplication of the gradient
       of current node, i.e. f'(x)^(i) with that of subsequent nodes which
       comes from first half of rhs of the equation.

   this process has to be repeated consecutively from l-1^th layer to 2^nd
   layer. note that the first layer is just the inputs.


   #6- update weights to minimize gradient
use the following update rule for weights:
 w[ik]^(l) = w[ik]^(l) + a^(i).e[l+1]^(k)

   where,
     * l = 1,2,   .., (l-1) | index of layers (excluding the last layer)
     * i = 0,1,   .., n[l] | index of node in l^th layer
     * k = 1,2,   ., n[l+1] | index of node in l+1^th layer
     * w[ik]^(l) refers to the weight from the l^th layer to l+1^th layer
       from i^th node to k^th node

   i hope the convention is clear. i suggest you go through it multiple
   times and if still there are questions, i   ll be happy to take them on
   through comments below.

   with this we have successfully understood how a neural network works.
   please feel free to discuss further if needed.


end notes

   this article is focused on the fundamentals of a neural network and how
   it works. i hope now you understand the working of a neural network and
   wouldn   t use it as a black box ever. it   s really easy once you
   understand doing it practically as well.

   therefore, in my upcoming article, i   ll explain the applications of
   using neural network in python. more than theoretical, i   ll focus on
   practical aspect of neural network. two applications come to my mind
   immediately:
    1. image processing
    2. natural language processing

   i hope you enjoyed this. i would love if you could share your feedback
   through comments below. looking forward to interacting with you further
   on this!

you want to apply your analytical skills and test your potential?
then [115]participate in our hackathons and compete with top data scientists
from all over the world.

   you can also read this article on analytics vidhya's android app
   [116]get it on google play

share this:

     * [117]click to share on linkedin (opens in new window)
     * [118]click to share on facebook (opens in new window)
     * [119]click to share on twitter (opens in new window)
     * [120]click to share on pocket (opens in new window)
     * [121]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [122]backpropogration, [123]deep learning, [124]forward
   propogation, [125]machine learning, [126]neural network
   next article

statistical analyst     ahmedabad (2+ years of experience)

   previous article

senior data engineer     hyderabad (2+ years of experience)

[127]aarshay jain

   aarshay is a ml enthusiast, pursuing ms in data science at columbia
   university, graduating in dec 2017. he is currently exploring the
   various ml techniques and writes articles for av to share his knowledge
   with the community.
     *
     *
     *
     *

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [128]discussion portal to get your queries resolved

42 comments

     * sandeep r diddi says:
       [129]march 16, 2016 at 4:30 am
       excellent explanation , hope to see some practical example using r
       [130]reply
          + aarshay jain says:
            [131]march 16, 2016 at 3:10 pm
            i   ll come up with examples when i have gained sufficient
            experience on this.
            [132]reply
     * arindam says:
       [133]march 16, 2016 at 4:48 am
       in some article, please explain convolution neural network and
       id56 too.
       [134]reply
          + aarshay jain says:
            [135]march 16, 2016 at 3:11 pm
            thanks for reaching out. i   ll keep this in mind     
            [136]reply
     * venu says:
       [137]march 16, 2016 at 6:22 am
       good one
       [138]reply
          + aarshay jain says:
            [139]march 16, 2016 at 3:11 pm
            thanks!
            [140]reply
     * neeraj singh sarwan says:
       [141]march 16, 2016 at 7:42 am
       i guess the images have two errors.
       1- calculating partial derivative of e wrt b     it should be minus
       c.
       2- misprinting of p in the next diagram. in the last hidden layer
       it should be   p    instead of    b   
       [142]reply
          + aarshay jain says:
            [143]march 16, 2016 at 3:19 pm
            thanks for reaching out.
            1- i think it should be    c   . please have a look again.
            2- i   ll update the images. thanks for informing!
            [144]reply
               o neeraj sarwan says:
                 [145]march 17, 2016 at 8:47 am
                 i am afraid it should be minus c (-c) as you can see
                 e=ac-bc .
                 so on partial derivation of e wrt to b , we get -c .
                 please look again and correct me if i am wrong.
                 thanks
                 [146]reply
                    # aarshay jain says:
                      [147]march 18, 2016 at 4:30 am
                      my bad. i guess you   re right. earlier i thought you
                      are referring to the equation above, i.e. e wrt a.
                      i   ll correct it.
                      thanks!
                      [148]reply
     * rajesh kohli says:
       [149]march 16, 2016 at 10:26 am
       thanks aashray for this excellent article. very well presented.
       just wanted to point out that you might consider updating the two
       example neural networks pictures in the back propagation section
       where in the first picture,    e    represents    c    and in the 2nd
       picture,    p    represents    b    just to avoid confusion for the
       readers.
       thanks again for this wonderful post.
       best
       rajesh
       [150]reply
          + aarshay jain says:
            [151]march 16, 2016 at 3:20 pm
            glad you liked it!
            thanks for reporting the bug in the images. i   ll fix these
            tonight.
            [152]reply
     * cleo batista says:
       [153]march 16, 2016 at 8:23 pm
       good article. thanks!
       [154]reply
          + aarshay jain says:
            [155]march 16, 2016 at 9:14 pm
            glad you liked it     
            [156]reply
     * lamia says:
       [157]march 17, 2016 at 8:36 pm
       thank you very much to your clear approach in explaining the
       concepts in your article
       [158]reply
          + aarshay jain says:
            [159]march 18, 2016 at 4:31 am
            thanks     
            [160]reply
     * [161]paul torek says:
       [162]march 21, 2016 at 12:15 pm
       thanks aarshay. to make the error equation #4 more readable, you
       could un-superscript the 2nd half of the rhs.
       can the neural network get trapped in a local optimum, when trained
       in this way? if so, how can we avoid that? i   m pretty new to ml, so
       please forgive if the answers are obvious to almost everyone    
       they   re not obvious to me.
       [163]reply
          + aarshay jain says:
            [164]march 22, 2016 at 6:30 am
            hi paul,
            i didn   t get which equation you are referring to.
            regarding local optima, i guess its possible and it would
            depend on case to case. this is one reason that deep learning
            models require a lot of data to train efficiently.
            [165]reply
               o [166]paul torek says:
                 [167]march 23, 2016 at 11:24 am
                 i meant the equation in section    #4     find the error   .
                 [168]reply
                    # aarshay jain says:
                      [169]march 23, 2016 at 11:44 am
                      oops. that was a typo. it wasn   t meant to be
                      superscript. i have made the correction. thanks!
                      [170]reply
     * abhishek sinha says:
       [171]april 4, 2016 at 9:12 am
       nice article. would love to see a tutorial on deep learning in
       python some day form you.
       thanks.
       [172]reply
          + aarshay jain says:
            [173]april 4, 2016 at 2:48 pm
            thanks! check out my follow-up article-
            [174]http://www.analyticsvidhya.com/blog/2016/04/deep-learning
            -computer-vision-introduction-convolution-neural-networks/
            [175]reply
     * janakiraman sankara narayanan says:
       [176]april 5, 2016 at 5:53 pm
       thank you so much for the article. i   m interested in self-learning
       algorithms that can be used in robots. can you suggests?
       [177]reply
          + aarshay jain says:
            [178]april 5, 2016 at 5:55 pm
            have you tried re-inforcement learning?
            [179]reply
     * janakiraman sankara narayanan says:
       [180]april 5, 2016 at 6:02 pm
       no, i am starting with mdp and pomdps for path planning. i   m
       curious to learn algorithms that can make robots learn and
       configures itself.
       [181]reply
          + aarshay jain says:
            [182]april 5, 2016 at 6:04 pm
            i believe id23 is what you are looking for.
            check it out.
            [183]reply
               o janakiraman sankara narayanan says:
                 [184]april 5, 2016 at 6:06 pm
                 thanks a lot
                 [185]reply
     * auh says:
       [186]april 7, 2016 at 7:13 am
       dear aarshay, it   s pretty excellent explanation comparing to
       any-other website (i visited so far).
       i have a question regarding dimension of weight matrix (may be
       silly question but i am beginner at neural network, hope don   t feel
       irritate).
       let say, i have 4 neurons (x1, x2, x3, x4 and bias x0) and first
       hidden layer have a1, a2,a3 and bias a0. so, weight matrix looks
       like
       w = [w01 w11 w21 w31 w41]
       [w02 w12 w22 w32 w42]
       [w03 w13 w23 w33 w43]
       [w04 w14 w24 w34 w44]
       dimension: ni+1 x ni+1
       here,
       i=1, n=4 (x1, x2, x3, x4 without bias)
       row = 4(1=subscript) + 1 = 5
       col = n (1+1 = both subscript) = n (2) = 3 (a1, a2, a3)
       please could you tell me in which point i made the mistake??
       [187]reply
          + aarshay jain says:
            [188]april 7, 2016 at 1:04 pm
            i   m glad you liked my approach. i just checked and i guess
            there was a typo in the dimension of w matrix. it   s actually
            n(i+1) x n(i)+1.
            so in your example:
            n(i) = 4
            n(i+1) = 3
            dimension of w: 3  5
            w = [w01 w11 w21 w31 w41]
            [w02 w12 w22 w32 w42]
            [w03 w13 w23 w33 w43]
            each row represents weights from neurons of i layer to same
            neuron of i+1 layer. remember that the bias of i+1 layer will
            not receive weights from i layer. hope this makes sense.
            [189]reply
               o mohammad says:
                 [190]november 20, 2016 at 7:52 pm
                 dear aashray,
                 thank you so much for this wonderful post. in the above
                 example, thanks for explaining the dimension of w(i) =
                 3  5. but got a small doubt here. please understand that
                 i   m also a beginner in neural network and this might be a
                 very silly doubt.
                 in the example given by auh, dimension of a(i) = 1  5, as
                 n(i) = 4
                 you have mentioned to compute the output => a(i+1) = f(
                 ai.w(i) ). but we have dimension of a(i) = 1  5 and
                 dimension of w(i) = 3  5. how to calculate the output here
                 as the dimension does not comply with matrix
                 multiplication rule. or am i missing something here.
                 [191]reply
     * james chuter says:
       [192]april 7, 2016 at 1:44 pm
       xnor function:
       here we can see that the output is 1 when both inputs are same,
       otherwise 0. this sort of a relationship cannot be modeled using a
       single neuron. (don   t believe me? give it a try!)
       a = f((1-x1-x2)^2 -0.5)
       x1 x2 a
       0 0 0.5
       0 1 -0.5
       1 0 -0.5
       1 1 0.5
       or have i missed something?
       [193]reply
          + aarshay jain says:
            [194]april 7, 2016 at 1:55 pm
            hi james,
            interesting approach. i believe if you try to model it as a
            nn, you   ll need 2 layers.
            layer 1 will do (1-x1-x2). layer 2 will square it and add
            -0.5.
            you can also define such a complex activation function. but
            this will be not be scalable to large problems. if we go to
            extent of defining custom activations, why not simply define
            the xnor function itself as the activation?
            nice thinking though. great work     
            [195]reply
     * gaurav says:
       [196]may 17, 2016 at 9:35 am
       i haven   t understood the part:    #5- back-propogating the error into
       the network   . can you please explain it in a more simpler way?
       [197]reply
          + aarshay jain says:
            [198]june 16, 2016 at 5:13 am
            check this out     [199]http://cs231n.github.io/optimization-2/.
            it   s a very good explanation. should help you!
            [200]reply
     * charlie says:
       [201]may 25, 2016 at 10:47 am
       hi, i am confused about when the process of update weights comes to
       an end. how can we know the gradient is minimized?
       [202]reply
          + aarshay jain says:
            [203]june 16, 2016 at 5:09 am
            the equations used for updating the weights are derived so
            that the weight is minimized. so you can be rest assured that
            if your implementation is correct, it   ll work out.
            [204]reply
     * r-kitekt says:
       [205]june 16, 2016 at 5:07 am
       thank you very much for such a detailed and clear tutorial on a
       relatively difficult topic. looking forward to the article when all
       this theory is actually applied. you mentioned that you would do it
       in python; however, if you also have some knowledge of r, it would
       be great if you could write next article using both languages.
       thanks once again.
       [206]reply
          + aarshay jain says:
            [207]june 16, 2016 at 5:13 am
            thanks.. i   m working on the application part and will take
            some time. i have 1 article though on theano which you might
            wanna check out    
            [208]http://www.analyticsvidhya.com/blog/2016/04/neural-networ
            ks-python-theano/.
            regarding r, i   m sorry but my current focus is on python and
            honestly, taking up both languages simultaneously is very
            taxing so i might not be able to share the codes for r. but
            you can definitely figure it out once you have the concepts
            handy.
            [209]reply
     * sachin says:
       [210]july 29, 2016 at 8:04 am
       well explained . thanks..
       [211]reply
          + aarshay jain says:
            [212]august 31, 2016 at 4:35 am
            welcome!
            [213]reply
     * hector blandin says:
       [214]may 25, 2017 at 10:45 pm
       very good article !
       [215]reply
     * vikas says:
       [216]april 19, 2018 at 12:09 pm
       wow, amazing work, god bless you.
       [217]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [218]srk       3924
   2    [2.jpg?date=2019-04-05] [219]mark12    3510
   3    [3.jpg?date=2019-04-05] [220]nilabha   3261
   4    [4.jpg?date=2019-04-05] [221]nitish007 3237
   5    [5.jpg?date=2019-04-05] [222]tezdhar   3082
   [223]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [224]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [225]understanding support vector machine algorithm from examples
       (along with code)
     * [226]essentials of machine learning algorithms (with python and r
       codes)
     * [227]a complete tutorial to learn data science with python from
       scratch
     * [228]7 types of regression techniques you should know!
     * [229]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [230]a simple introduction to anova (with applications in excel)
     * [231]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [232]top 5 machine learning github repositories and reddit discussions
   from march 2019

[233]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [234]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[235]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [236]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[237]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [238]16 opencv functions to start your id161 journey (with
   python code)

[239]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [240][ds-finhack.jpg]

   [241][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [242]about us
     * [243]our team
     * [244]career
     * [245]contact us
     * [246]write for us

   [247]about us
   [248]   
   [249]our team
   [250]   
   [251]careers
   [252]   
   [253]contact us

data scientists

     * [254]blog
     * [255]hackathon
     * [256]discussions
     * [257]apply jobs
     * [258]leaderboard

companies

     * [259]post jobs
     * [260]trainings
     * [261]hiring hackathons
     * [262]advertising
     * [263]reach us

   don't have an account? [264]sign up here.

join our community :

   [265]46336 [266]followers
   [267]20224 [268]followers
   [269]followers
   [270]7513 [271]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [272]privacy policy
     * [273]terms of use
     * [274]refund policy

   don't have an account? [275]sign up here

   iframe: [276]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [277](button) join now

   subscribe!

   iframe: [278]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [279](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/machine-learning/
  94. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
  95. https://www.analyticsvidhya.com/blog/category/machine-learning/
  96. https://www.analyticsvidhya.com/blog/author/aarshay/
  97. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/1.-neuron.jpg
  98. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/2.-ann-structure.jpg
  99. http://www.intechopen.com/books/metallurgy-advances-in-materials-and-processes/artificial-intelligence-techniques-for-modelling-of-temperature-in-the-metal-cutting-process
 100. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/eq1-neuron.png
 101. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/eq2-activation-fn.png
 102. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/3.-tt-and-1.png
 103. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/4.-tt-or-1.png
 104. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/5.-tt-not-2.png
 105. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/6.-xnor-tt.png
 106. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/7.-tt-a.b.png
 107. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/8.-tt-xnor-case-1.png
 108. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/9.-tt-xnor-case-2.png
 109. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/diagram-10.jpg
 110. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/3.-eq3.png
 111. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/4.-eq4-1.png
 112. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/diagram-11.jpg
 113. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/5.-eq5.png
 114. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/6.-eq6.png
 115. http://datahack.analyticsvidhya.com/contest/all
 116. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 117. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/?share=linkedin
 118. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/?share=facebook
 119. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/?share=twitter
 120. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/?share=pocket
 121. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/?share=reddit
 122. https://www.analyticsvidhya.com/blog/tag/backpropogration/
 123. https://www.analyticsvidhya.com/blog/tag/deep-learning/
 124. https://www.analyticsvidhya.com/blog/tag/forward-propogation/
 125. https://www.analyticsvidhya.com/blog/tag/machine-learning/
 126. https://www.analyticsvidhya.com/blog/tag/neural-network/
 127. https://www.analyticsvidhya.com/blog/author/aarshay/
 128. https://discuss.analyticsvidhya.com/
 129. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107466
 130. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107466
 131. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107498
 132. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107498
 133. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107467
 134. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107467
 135. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107499
 136. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107499
 137. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107468
 138. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107468
 139. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107500
 140. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107500
 141. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107475
 142. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107475
 143. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107501
 144. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107501
 145. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107583
 146. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107583
 147. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107632
 148. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107632
 149. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107486
 150. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107486
 151. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107502
 152. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107502
 153. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107517
 154. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107517
 155. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107522
 156. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107522
 157. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107615
 158. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107615
 159. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107633
 160. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107633
 161. http://torekp.weebly.com/
 162. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107919
 163. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107919
 164. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107982
 165. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-107982
 166. http://torekp.weebly.com/
 167. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-108082
 168. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-108082
 169. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-108084
 170. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-108084
 171. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-108917
 172. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-108917
 173. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-108938
 174. http://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
 175. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-108938
 176. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109009
 177. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109009
 178. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109010
 179. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109010
 180. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109011
 181. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109011
 182. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109012
 183. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109012
 184. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109013
 185. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109013
 186. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109101
 187. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109101
 188. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109120
 189. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109120
 190. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-118585
 191. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-118585
 192. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109125
 193. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109125
 194. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109127
 195. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-109127
 196. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-111092
 197. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-111092
 198. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-112273
 199. http://cs231n.github.io/optimization-2/
 200. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-112273
 201. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-111428
 202. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-111428
 203. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-112271
 204. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-112271
 205. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-112270
 206. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-112270
 207. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-112272
 208. http://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/
 209. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-112272
 210. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-114239
 211. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-114239
 212. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-115346
 213. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-115346
 214. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-129232
 215. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-129232
 216. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-152676
 217. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/#comment-152676
 218. https://datahack.analyticsvidhya.com/user/profile/srk
 219. https://datahack.analyticsvidhya.com/user/profile/mark12
 220. https://datahack.analyticsvidhya.com/user/profile/nilabha
 221. https://datahack.analyticsvidhya.com/user/profile/nitish007
 222. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 223. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 224. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 225. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 226. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 227. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 228. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 229. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 230. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 231. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 232. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 233. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 234. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 235. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 236. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 237. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 238. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 239. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 240. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 241. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 242. http://www.analyticsvidhya.com/about-me/
 243. https://www.analyticsvidhya.com/about-me/team/
 244. https://www.analyticsvidhya.com/career-analytics-vidhya/
 245. https://www.analyticsvidhya.com/contact/
 246. https://www.analyticsvidhya.com/about-me/write/
 247. http://www.analyticsvidhya.com/about-me/
 248. https://www.analyticsvidhya.com/about-me/team/
 249. https://www.analyticsvidhya.com/about-me/team/
 250. https://www.analyticsvidhya.com/about-me/team/
 251. https://www.analyticsvidhya.com/career-analytics-vidhya/
 252. https://www.analyticsvidhya.com/about-me/team/
 253. https://www.analyticsvidhya.com/contact/
 254. https://www.analyticsvidhya.com/blog
 255. https://datahack.analyticsvidhya.com/
 256. https://discuss.analyticsvidhya.com/
 257. https://www.analyticsvidhya.com/jobs/
 258. https://datahack.analyticsvidhya.com/users/
 259. https://www.analyticsvidhya.com/corporate/
 260. https://trainings.analyticsvidhya.com/
 261. https://datahack.analyticsvidhya.com/
 262. https://www.analyticsvidhya.com/contact/
 263. https://www.analyticsvidhya.com/contact/
 264. https://datahack.analyticsvidhya.com/signup/
 265. https://www.facebook.com/analyticsvidhya/
 266. https://www.facebook.com/analyticsvidhya/
 267. https://twitter.com/analyticsvidhya
 268. https://twitter.com/analyticsvidhya
 269. https://plus.google.com/+analyticsvidhya
 270. https://in.linkedin.com/company/analytics-vidhya
 271. https://in.linkedin.com/company/analytics-vidhya
 272. https://www.analyticsvidhya.com/privacy-policy/
 273. https://www.analyticsvidhya.com/terms/
 274. https://www.analyticsvidhya.com/refund-policy/
 275. https://id.analyticsvidhya.com/accounts/signup/
 276. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 277. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 278. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 279. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 281. https://www.facebook.com/analyticsvidhya
 282. https://twitter.com/analyticsvidhya
 283. https://plus.google.com/+analyticsvidhya/posts
 284. https://in.linkedin.com/company/analytics-vidhya
 285. https://www.analyticsvidhya.com/blog/2016/03/statistical-analyst-ahmedabad-2-years-experience/
 286. https://www.analyticsvidhya.com/blog/2016/03/senior-data-engineer-hyderabad-2-years-experience/
 287. https://www.analyticsvidhya.com/blog/author/aarshay/
 288. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection#f1909083829990889b90989fb1969c90989ddf929e9c
 289. https://in.linkedin.com/in/aarshayjain
 290. https://github.com/aarshayj
 291. https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/aarshay
 292. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 293. https://www.facebook.com/analyticsvidhya/
 294. https://twitter.com/analyticsvidhya
 295. https://plus.google.com/+analyticsvidhya
 296. https://plus.google.com/+analyticsvidhya
 297. https://in.linkedin.com/company/analytics-vidhya
 298. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 299. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 300. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 301. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 302. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 303. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 304. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 305. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 306. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 307. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 308. javascript:void(0);
 309. javascript:void(0);
 310. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 311. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 312. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 313. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 314. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 315. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 316. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 317. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 318. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 319. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f03%2fintroduction-deep-learning-fundamentals-neural-networks%2f&linkname=fundamentals%20of%20deep%20learning%20-%20starting%20with%20artificial%20neural%20network
 320. javascript:void(0);
 321. javascript:void(0);
