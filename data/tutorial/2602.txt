   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    a complete tutorial to learn data science with
   python from scratch comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]machine learning [94]a complete tutorial to learn data
   science with python from scratch

   [95]machine learning[96]pandas[97]python

a complete tutorial to learn data science with python from scratch

   [98]kunal jain, january 14, 2016

introduction

   it happened a few years back. after working on sas for more than 5
   years, i decided to move out of my comfort zone. [99]being a data
   scientist, my hunt for other useful tools was on! fortunately, it
   didn   t take me long to decide     python was my appetizer.

   i always had an inclination for coding. this was the time to do what i
   really loved. code. turned out, coding was actually quite easy!

   i learned the [100]basics of python within a week. and, since then,
   i   ve not only explored this language to the depth, but also have helped
   many other to learn this language. python was originally a general
   purpose language. but, over the years, with strong community support,
   this language got dedicated library for data analysis and predictive
   modeling.

   due to lack of resource on python for [101]data science, i decided to
   create this tutorial to help many others to learn python faster. in
   this tutorial, we will take bite sized information about how to use
   python for data analysis, chew it till we are comfortable and practice
   it at our own end.

   you can also check out the    [102]introduction to data science    course    
   a comprehensive introduction to the world of data science. it includes
   modules on python, statistics and predictive modeling along with
   multiple practical projects to get your hands dirty.


table of contents

    1. basics of python for data analysis
          + why learn python for data analysis?
          + python 2.7 v/s 3.4
          + how to install python?
          + running a few simple programs in python
    2. python libraries and data structures
          + python data structures
          + python iteration and conditional constructs
          + python libraries
    3. exploratory analysis in python using pandas
          + introduction to series and dataframes
          + analytics vidhya dataset- loan prediction problem
    4. data munging in python using pandas
    5. building a predictive model in python
          + id28
          + decision tree
          + id79

   let   s get started!



1. basics of python for data analysis

why learn python for data analysis?

   python has gathered a lot of interest recently as a choice of language
   for data analysis. i had [103]basics of python some time back. here are
   some reasons which go in favour of learning python:
     * open source     free to install
     * awesome online community
     * very easy to learn
     * can become a common language for data science and production of web
       based analytics products.

   needless to say, it still has few drawbacks too:
     * it is an interpreted language rather than compiled language     hence
       might take up more cpu time. however, given the savings in
       programmer time (due to ease of learning), it might still be a good
       choice.


python 2.7 v/s 3.4

   this is one of the most debated topics in python. you will invariably
   cross paths with it, specially if you are a beginner. there is no
   right/wrong choice here. it totally depends on the situation and your
   need to use. i will try to give you some pointers to help you make an
   informed choice.

why python 2.7?

    1. awesome community support! this is something you   d need in your
       early days. python 2 was released in late 2000 and has been in use
       for more than 15 years.
    2. plethora of third-party libraries! though many libraries have
       provided 3.x support but still a large number of modules work only
       on 2.x versions. if you plan to use python for specific
       applications like web-development with high reliance on external
       modules, you might be better off with 2.7.
    3. some of the features of 3.x versions have backward compatibility
       and can work with 2.7 version.


why python 3.4?

    1. cleaner and faster! python developers have fixed some inherent
       glitches and minor drawbacks in order to set a stronger foundation
       for the future. these might not be very relevant initially, but
       will matter eventually.
    2. it is the future! 2.7 is the last release for the 2.x family and
       eventually everyone has to shift to 3.x versions. python 3 has
       released stable versions for past 5 years and will continue the
       same.

   there is no clear winner but i suppose the bottom line is that you
   should focus on learning python as a language. shifting between
   versions should just be a matter of time. stay tuned for a dedicated
   article on python 2.x vs 3.x in the near future!


how to install python?

   there are 2 approaches to install python:
     * you can download python directly from its [104]project site and
       install individual components and libraries you want
     * alternately, you can download and install a package, which comes
       with pre-installed libraries. i would recommend downloading
       [105]anaconda. another option could be [106]enthought canopy
       express.

   second method provides a hassle free installation and hence i   ll
   recommend that to beginners. the imitation of this approach is you have
   to wait for the entire package to be upgraded, even if you are
   interested in the latest version of a single library. it should not
   matter until and unless, until and unless, you are doing cutting edge
   statistical research.


   choosing a development environment

   once you have installed python, there are various options for choosing
   an environment. here are the 3 most common options:
     * terminal / shell based
     * idle (default environment)
     * ipython notebook     similar to markdown in r

   [107]idle editor for python

   idle editor for python

   while the right environment depends on your need, i personally prefer
   ipython notebooks a lot. it provides a lot of good features for
   documenting while writing the code itself and you can choose to run the
   code in blocks (rather than the line by line execution)

   we will use ipython environment for this complete tutorial.


warming up: running your first python program

   you can use python as a simple calculator to start with:

   [108]jupyter1


few things to note

     * you can start ipython notebook by writing    ipython notebook    on
       your terminal / cmd, depending on the os you are working on
     * you can name a ipython notebook by simply clicking on the name    
       untitledo in the above screenshot
     * the interface shows in [*] for inputs and out[*] for output.
     * you can execute a code by pressing    shift + enter    or    alt +
       enter   , if you want to insert an additional row after.

   before we deep dive into problem solving, lets take a step back and
   understand the basics of python. as we know that data structures and
   iteration and conditional constructs form the crux of any language. in
   python, these include lists, strings, tuples, dictionaries, for-loop,
   while-loop, if-else, etc. let   s take a look at some of these.


2. python libraries and data structures

python data structures

   following are some data structures, which are used in python. you
   should be familiar with them in order to use them as appropriate.
     * lists     lists are one of the most versatile data structure in
       python. a list can simply be defined by writing a list of comma
       separated values in square brackets. lists might contain items of
       different types, but usually the items all have the same type.
       python lists are mutable and individual elements of a list can be
       changed.

   here is a quick example to define a list and then access it:

   [109]python_lists
     * strings     strings can simply be defined by use of single (     ),
       double (     ) or triple (        ) inverted commas. strings enclosed in
       tripe quotes (        ) can span over multiple lines and are used
       frequently in docstrings (python   s way of documenting functions). \
       is used as an escape character. please note that python strings are
       immutable, so you can not change part of strings.

   [110]python_strings
     * tuples     a tuple is represented by a number of values separated by
       commas. tuples are immutable and the output is surrounded by
       parentheses so that nested tuples are processed correctly.
       additionally, even though tuples are immutable, they can hold
       mutable data if needed.

   since tuples are immutable and can not change, they are faster in
   processing as compared to lists. hence, if your list is unlikely to
   change, you should use tuples, instead of lists.

   [111]python_tuples
     * dictionary     dictionary is an unordered set of key: value pairs,
       with the requirement that the keys are unique (within one
       dictionary). a pair of braces creates an empty dictionary: {}.

   [112]python_dictionary

python iteration and conditional constructs

   like most languages, python also has a for-loop which is the most
   widely used method for iteration. it has a simple syntax:
for i in [python iterable]:
  expression(i)

   here    python iterable    can be a list, tuple or other advanced data
   structures which we will explore in later sections. let   s take a look
   at a simple example, determining the factorial of a number.
fact=1
for i in range(1,n+1):
  fact *= i

   coming to conditional statements, these are used to execute code
   fragments based on a condition. the most commonly used construct is
   if-else, with following syntax:
if [condition]:
  __execution if true__
else:
  __execution if false__

   for instance, if we want to print whether the number n is even or odd:
if n%2 == 0:
  print ('even')
else:
  print ('odd')

   now that you are familiar with python fundamentals, let   s take a step
   further. what if you have to perform the following tasks:
    1. multiply 2 matrices
    2. find the root of a quadratic equation
    3. plot bar charts and histograms
    4. make statistical models
    5. access web-pages

   if you try to write code from scratch, its going to be a nightmare and
   you won   t stay on python for more than 2 days! but lets not worry about
   that. thankfully, there are many libraries with predefined which we can
   directly import into our code and make our life easy.

   for example, consider the factorial example we just saw. we can do that
   in a single step as:
math.factorial(n)

   off-course we need to import the math library for that. lets explore
   the various libraries next.


python libraries

   lets take one step ahead in our journey to learn python by getting
   acquainted with some useful libraries. the first step is obviously to
   learn to import them into our environment. there are several ways of
   doing so in python:

   [gears-b.png]
import math as m
from math import *

   in the first manner, we have defined an alias m to library math. we can
   now use various functions from math library (e.g. factorial) by
   referencing it using the alias m.factorial().

   in the second manner, you have imported the entire name space in math
   i.e. you can directly use factorial() without referring to math.

   []
   tip: google recommends that you use first style of importing libraries,
   as you will know where the functions have come from.

   following are a list of libraries, you will need for any scientific
   computations and data analysis:
     * numpy stands for numerical python. the most powerful feature of
       numpy is n-dimensional array. this library also contains basic
       id202 functions, fourier transforms,  advanced random
       number capabilities and tools for integration with other low level
       languages like fortran, c and c++
     * scipy stands for scientific python. scipy is built on numpy. it is
       one of the most useful library for variety of high level science
       and engineering modules like discrete fourier transform, linear
       algebra, optimization and sparse matrices.
     * matplotlib for plotting vast variety of graphs, starting from
       histograms to line plots to heat plots.. you can use pylab feature
       in ipython notebook (ipython notebook    pylab = inline) to use these
       plotting features inline. if you ignore the inline option, then
       pylab converts ipython environment to an environment, very similar
       to matlab. you can also use latex commands to add math to your
       plot.
     * pandas for structured data operations and manipulations. it is
       extensively used for data munging and preparation. pandas were
       added relatively recently to python and have been instrumental in
       boosting python   s usage in data scientist community.
     * scikit learn for machine learning. built on numpy, scipy and
       matplotlib, this library contains a lot of effiecient tools for
       machine learning and statistical modeling including classification,
       regression, id91 and id84.
     * statsmodels for statistical modeling. statsmodels is a python
       module that allows users to explore data, estimate statistical
       models, and perform statistical tests. an extensive list of
       descriptive statistics, statistical tests, plotting functions, and
       result statistics are available for different types of data and
       each estimator.
     * seaborn for statistical data visualization. seaborn is a library
       for making attractive and informative statistical graphics in
       python. it is based on matplotlib. seaborn aims to make
       visualization a central part of exploring and understanding data.
     * bokeh for creating interactive plots, dashboards and data
       applications on modern web-browsers. it empowers the user to
       generate elegant and concise graphics in the style of d3.js.
       moreover, it has the capability of high-performance interactivity
       over very large or streaming datasets.
     * blaze for extending the capability of numpy and pandas to
       distributed and streaming datasets. it can be used to access data
       from a multitude of sources including bcolz, mongodb, sqlalchemy,
       apache spark, pytables, etc. together with bokeh, blaze can act as
       a very powerful tool for creating effective visualizations and
       dashboards on huge chunks of data.
     * scrapy for web crawling. it is a very useful framework for getting
       specific patterns of data. it has the capability to start at a
       website home url and then dig through web-pages within the website
       to gather information.
     * sympy for symbolic computation. it has wide-ranging capabilities
       from basic symbolic arithmetic to calculus, algebra, discrete
       mathematics and quantum physics. another useful feature is the
       capability of formatting the result of the computations as latex
       code.
     * requests for accessing the web. it works similar to the the
       standard python library urllib2 but is much easier to code. you
       will find subtle differences with urllib2 but for beginners,
       requests might be more convenient.

   additional libraries, you might need:
     * os for operating system and file operations
     * networkx and igraph for graph based data manipulations
     * id157 for finding patterns in text data
     * beautifulsoup for scrapping web. it is inferior to scrapy as it
       will extract information from just a single webpage in a run.

   now that we are familiar with python fundamentals and additional
   libraries, lets take a deep dive into problem solving through python.
   yes i mean making a predictive model! in the process, we use some
   powerful libraries and also come across the next level of data
   structures. we will take you through the 3 key phases:
    1. data exploration     finding out more about the data we have
    2. data munging     cleaning the data and playing with it to make it
       better suit statistical modeling
    3. predictive modeling     running the actual algorithms and having fun
           


3. exploratory analysis in python using pandas

   in order to explore our data further, let me introduce you to another
   animal (as if python was not enough!)     pandas
   [113]pandas

   image source: wikipedia

   pandas is one of the most useful data analysis library in python (i
   know these names sounds weird, but hang on!). they have been
   instrumental in increasing the use of python in data science community.
   we will now use pandas to read a data set from an analytics vidhya
   competition, perform exploratory analysis and build our first basic
   categorization algorithm for solving this problem.

   before loading the data, lets understand the 2 key data structures in
   pandas     series and dataframes


introduction to series and dataframes

   series can be understood as a 1 dimensional labelled / indexed array.
   you can access individual elements of this series through these labels.

   a dataframe is similar to excel workbook     you have column names
   referring to columns and you have rows, which can be accessed with use
   of row numbers. the essential difference being that column names and
   row numbers are known as column and row index, in case of dataframes.

   series and dataframes form the core data model for pandas in python.
   the data sets are first read into these dataframes and then various
   operations (e.g. group by, aggregation etc.) can be applied very easily
   to its columns.

   more: [114]10 minutes to pandas


practice data set     loan prediction problem

   you can download the dataset from [115]here. here is the description of
   variables:

   [gears-b.png]
variable descriptions:
variable                  description
loan_id                   unique loan id
gender                    male/ female
married                   applicant married (y/n)
dependents                number of dependents
education                 applicant education (graduate/ under graduate)
self_employed             self employed (y/n)
applicantincome           applicant income
coapplicantincome         coapplicant income
loanamount                loan amount in thousands
loan_amount_term          term of loan in months
credit_history            credit history meets guidelines
property_area             urban/ semi urban/ rural
loan_status               loan approved (y/n)


let   s begin with exploration

   to begin, start ipython interface in inline pylab mode by typing
   following on your terminal / windows command prompt:

   [gears-b.png]
   ipython notebook --pylab=inline

   this opens up ipython notebook in pylab environment, which has a few
   useful libraries already imported. also, you will be able to plot your
   data inline, which makes this a really good environment for interactive
   data analysis. you can check whether the environment has loaded
   correctly, by typing the following command (and getting the output as
   seen in the figure below):

   [gears-b.png]
   plot(arange(5))

   [116]ipython_pylab_check

   i am currently working in linux, and have stored the dataset in the
   following location:

    /home/kunal/downloads/loan_prediction/train.csv


  importing libraries and the data set:

   following are the libraries we will use during this tutorial:
     * numpy
     * matplotlib
     * pandas

   please note that you do not need to import matplotlib and numpy because
   of pylab environment. i have still kept them in the code, in case you
   use the code in a different environment.

   after importing the library, you read the dataset using function
   read_csv(). this is how the code looks like till this stage:

   [gears-b.png]
   import pandas as pd import numpy as np import matplotlib as plt
   %matplotlib inline df =
   pd.read_csv("/home/kunal/downloads/loan_prediction/train.csv") #reading
   the dataset in a dataframe using pandas


  quick data exploration

   once you have read the dataset, you can have a look at few top rows by
   using the function head()

   [gears-b.png]
   df.head(10)

   this should print 10 rows. alternately, you can also look at more rows
   by printing the dataset.

   next, you can look at summary of numerical fields by using describe()
   function

   [gears-b.png]
   df.describe()

   describe() function would provide count, mean, standard deviation
   (std), min, quartiles and max in its output (read [117]this article to
   refresh basic statistics to understand population distribution)

   here are a few id136s, you can draw by looking at the output of
   describe() function:
    1. loanamount has (614     592) 22 missing values.
    2. loan_amount_term has (614     600) 14 missing values.
    3. credit_history has (614     564) 50 missing values.
    4. we can also look that about 84% applicants have a credit_history.
       how? the mean of credit_history field is 0.84 (remember,
       credit_history has value 1 for those who have a credit history and
       0 otherwise)
    5. the applicantincome distribution seems to be in line with
       expectation. same with coapplicantincome

   please note that we can get an idea of a possible skew in the data by
   comparing the mean to the median, i.e. the 50% figure.

   for the non-numerical values (e.g. property_area, credit_history etc.),
   we can look at frequency distribution to understand whether they make
   sense or not. the frequency table can be printed by following command:

   [gears-b.png]
   df['property_area'].value_counts()

   similarly, we can look at unique values of port of credit history. note
   that dfname[   column_name   ] is a basic indexing technique to acess a
   particular column of the dataframe. it can be a list of columns as
   well. for more information, refer to the    10 minutes to pandas   
   resource shared above.


  distribution analysis

   now that we are familiar with basic data characteristics, let us study
   distribution of various variables. let us start with numeric variables
       namely applicantincome and loanamount

   lets start by plotting the histogram of applicantincome using the
   following commands:

   [gears-b.png]
   df['applicantincome'].hist(bins=50)

   here we observe that there are few extreme values. this is also the
   reason why 50 bins are required to depict the distribution clearly.

   next, we look at box plots to understand the distributions. box plot
   for fare can be plotted by:

   [gears-b.png]
   df.boxplot(column='applicantincome')

   this confirms the presence of a lot of outliers/extreme values. this
   can be attributed to the income disparity in the society. part of this
   can be driven by the fact that we are looking at people with different
   education levels. let us segregate them by education:

   [gears-b.png]
   df.boxplot(column='applicantincome', by = 'education')

   we can see that there is no substantial different between the mean
   income of graduate and non-graduates. but there are a higher number of
   graduates with very high incomes, which are appearing to be the
   outliers.

   now, let   s look at the histogram and boxplot of loanamount using the
   following command:

   [gears-b.png]
   df['loanamount'].hist(bins=50)

[118] output_13_1

   [gears-b.png]
   df.boxplot(column='loanamount')

   [119]output_14_1

   again, there are some extreme values. clearly, both applicantincome and
   loanamount require some amount of data munging. loanamount has missing
   and well as extreme values values, while applicantincome has a few
   extreme values, which demand deeper understanding. we will take this up
   in coming sections.


  categorical variable analysis

   now that we understand distributions for applicantincome and
   loanincome, let us understand categorical variables in more details. we
   will use excel style pivot table and cross-tabulation. for instance,
   let us look at the chances of getting a loan based on credit history.
   this can be achieved in ms excel using a pivot table as:

   note: here loan status has been coded as 1 for yes and 0 for no. so the
   mean represents the id203 of getting loan.

   now we will look at the steps required to generate a similar insight
   using python. please refer to [120]this article for getting a hang of
   the different data manipulation techniques in pandas.

   [gears-b.png]
   temp1 = df['credit_history'].value_counts(ascending=true) temp2 =
   df.pivot_table(values='loan_status',index=['credit_history'],aggfunc=la
   mbda x: x.map({'y':1,'n':0}).mean()) print ('frequency table for credit
   history:') print (temp1) print ('\nprobility of getting loan for each
   credit history class:') print (temp2)

   [121]11. pivot_python

   now we can observe that we get a similar pivot_table like the ms excel
   one. this can be plotted as a bar chart using the    matplotlib    library
   with following code:

   [gears-b.png]
   import matplotlib.pyplot as plt fig = plt.figure(figsize=(8,4)) ax1 =
   fig.add_subplot(121) ax1.set_xlabel('credit_history')
   ax1.set_ylabel('count of applicants') ax1.set_title("applicants by
   credit_history") temp1.plot(kind='bar') ax2 = fig.add_subplot(122)
   temp2.plot(kind = 'bar') ax2.set_xlabel('credit_history')
   ax2.set_ylabel('id203 of getting loan')
   ax2.set_title("id203 of getting loan by credit history")

   this shows that the chances of getting a loan are eight-fold if the
   applicant has a valid credit history. you can plot similar graphs by
   married, self-employed, property_area, etc.

   alternately, these two plots can also be visualized by combining them
   in a stacked chart::

   [gears-b.png]
   temp3 = pd.crosstab(df['credit_history'], df['loan_status'])
   temp3.plot(kind='bar', stacked=true, color=['red','blue'], grid=false)

   you can also add gender into the mix (similar to the pivot table in
   excel):

   if you have not realized already, we have just created two basic
   classification algorithms here, one based on credit history, while
   other on 2 categorical variables (including gender). you can quickly
   code this to create your first submission on av datahacks.

   we just saw how we can do exploratory analysis in python using pandas.
   i hope your love for pandas (the animal) would have increased by now    
   given the amount of help, the library can provide you in analyzing
   datasets.

   next let   s explore applicantincome and loanstatus variables further,
   [122]perform data munging and create a dataset for applying various
   modeling techniques. i would strongly urge that you take another
   dataset and problem and go through an independent example
   before reading further.


4. data munging in python : using pandas

   for those, who have been following, here are your must wear shoes to
   start running.

  data munging     recap of the need

   while our exploration of the data, we found a few problems in the data
   set, which needs to be solved before the data is ready for a good
   model. this exercise is typically referred as    data munging   . here are
   the problems, we are already aware of:
    1. there are missing values in some variables. we should estimate
       those values wisely depending on the amount of missing values and
       the expected importance of variables.
    2. while looking at the distributions, we saw that applicantincome and
       loanamount seemed to contain extreme values at either end. though
       they might make intuitive sense, but should be treated
       appropriately.

   in addition to these problems with numerical fields, we should also
   look at the non-numerical fields i.e. gender, property_area, married,
   education and dependents to see, if they contain any useful
   information.

   if you are new to pandas, i would recommend reading [123]this
   article before moving on. it details some useful techniques of data
   manipulation.


  check missing values in the dataset

   let us look at missing values in all the variables because most of the
   models don   t work with missing data and even if they do, imputing them
   helps more often than not. so, let us check the number of nulls / nans
   in the dataset

   [gears-b.png]
   df.apply(lambda x: sum(x.isnull()),axis=0)

   this command should tell us the number of missing values in each column
   as isnull() returns 1, if the value is null.

   [124]4. missing

   though the missing values are not very high in number, but many
   variables have them and each one of these should be estimated and added
   in the data. get a detailed view on different imputation techniques
   through [125]this article.

   note: remember that missing values may not always be nans. for
   instance, if the loan_amount_term is 0, does it makes sense or would
   you consider that missing? i suppose your answer is missing and you   re
   right. so we should check for values which are unpractical.


  how to fill missing values in loanamount?

   there are numerous ways to fill the missing values of loan amount     the
   simplest being replacement by mean, which can be done by following
   code:

   [gears-b.png]
   df['loanamount'].fillna(df['loanamount'].mean(), inplace=true)

   the other extreme could be to build a supervised learning model to
   predict loan amount on the basis of other variables and then use age
   along with other variables to predict survival.

   since, the purpose now is to bring out the steps in data munging, i   ll
   rather take an approach, which lies some where in between these 2
   extremes. a key hypothesis is that the whether a person is educated
   or self-employed can combine to give a good estimate of loan amount.

   first, let   s look at the boxplot to see if a trend exists:

   thus we see some variations in the median of loan amount for each group
   and this can be used to impute the values. but first, we have to ensure
   that each of self_employed and education variables should not have a
   missing values.

   as we say earlier, self_employed has some missing values. let   s look at
   the frequency table:

   [126]6. self emp

   since ~86% values are    no   , it is safe to impute the missing values as
      no    as there is a high id203 of success. this can be done using
   the following code:

   [gears-b.png]
   df['self_employed'].fillna('no',inplace=true)

   now, we will create a pivot table, which provides us median values for
   all the groups of unique values of self_employed and education
   features. next, we define a function, which returns the values of these
   cells and apply it to fill the missing values of loan amount:

   [gears-b.png]
   table = df.pivot_table(values='loanamount', index='self_employed'
   ,columns='education', aggfunc=np.median) # define function to return
   value of this pivot_table def fage(x): return
   table.loc[x['self_employed'],x['education']] # replace missing values
   df['loanamount'].fillna(df[df['loanamount'].isnull()].apply(fage,
   axis=1), inplace=true)

   this should provide you a good way to impute missing values of loan
   amount.

   note : this method will work only if you have not filled the missing
   values in loan_amount variable using the previous approach, i.e. using
   mean.


  how to treat for extreme values in distribution of loanamount and
  applicantincome?

   let   s analyze loanamount first. since the extreme values are
   practically possible, i.e. some people might apply for high value loans
   due to specific needs. so instead of treating them as outliers, let   s
   try a log transformation to nullify their effect:

   [gears-b.png]
   df['loanamount_log'] = np.log(df['loanamount'])
   df['loanamount_log'].hist(bins=20)

   looking at the histogram again:
   [127]7. loan log

   now the distribution looks much closer to normal and effect of extreme
   values has been significantly subsided.

   coming to applicantincome. one intuition can be that some applicants
   have lower income but strong support co-applicants. so it might be a
   good idea to combine both incomes as total income and take a log
   transformation of the same.

   [gears-b.png]
   df['totalincome'] = df['applicantincome'] + df['coapplicantincome']
   df['totalincome_log'] = np.log(df['totalincome'])
   df['loanamount_log'].hist(bins=20)

   [128]8. total income log

   now we see that the distribution is much better than before. i will
   leave it upto you to impute the missing values for gender, married,
   dependents, loan_amount_term, credit_history. also, i encourage you to
   think about possible additional information which can be derived from
   the data. for example, creating a column for loanamount/totalincome
   might make sense as it gives an idea of how well the applicant is
   suited to pay back his loan.

   next, we will look at making predictive models.


5. building a predictive model in python

   after, we have made the data useful for modeling, let   s now look at the
   python code to create a predictive model on our data set. skicit-learn
   (sklearn) is the most commonly used library in python for this purpose
   and we will follow the trail. i encourage you to get a refresher on
   sklearn through [129]this article.

   since, sklearn requires all inputs to be numeric, we should convert all
   our categorical variables into numeric by encoding the categories.
   before that we will fill all the missing values in the dataset. this
   can be done using the following code:
df['gender'].fillna(df['gender'].mode()[0], inplace=true)
df['married'].fillna(df['married'].mode()[0], inplace=true)
df['dependents'].fillna(df['dependents'].mode()[0], inplace=true)
df['loan_amount_term'].fillna(df['loan_amount_term'].mode()[0], inplace=true)
df['credit_history'].fillna(df['credit_history'].mode()[0], inplace=true)

   [gears-b.png]
   from sklearn.preprocessing import labelencoder var_mod =
   ['gender','married','dependents','education','self_employed','property_
   area','loan_status'] le = labelencoder() for i in var_mod: df[i] =
   le.fit_transform(df[i]) df.dtypes

   next, we will import the required modules. then we will define a
   generic classification function, which takes a model as input and
   determines the accuracy and cross-validation scores. since this is an
   introductory article, i will not go into the details of coding. please
   refer to [130]this article for getting details of the algorithms with r
   and python codes. also, it   ll be good to get a refresher
   on cross-validation through [131]this article, as it is a very
   important measure of power performance.

   [gears-b.png]
   #import models from scikit learn module: from sklearn.linear_model
   import logisticregression from sklearn.cross_validation import kfold
   #for k-fold cross validation from sklearn.ensemble import
   randomforestclassifier from sklearn.tree import decisiontreeclassifier,
   export_graphviz from sklearn import metrics #generic function for
   making a classification model and accessing performance: def
   classification_model(model, data, predictors, outcome): #fit the model:
   model.fit(data[predictors],data[outcome]) #make predictions on training
   set: predictions = model.predict(data[predictors]) #print accuracy
   accuracy = metrics.accuracy_score(predictions,data[outcome]) print
   ("accuracy : %s" % "{0:.3%}".format(accuracy)) #perform k-fold
   cross-validation with 5 folds kf = kfold(data.shape[0], n_folds=5)
   error = [] for train, test in kf: # filter training data
   train_predictors = (data[predictors].iloc[train,:]) # the target we're
   using to train the algorithm. train_target = data[outcome].iloc[train]
      # training the algorithm using the predictors and target.
   model.fit(train_predictors, train_target)    #record error from each
   cross-validation run
   error.append(model.score(data[predictors].iloc[test,:],
   data[outcome].iloc[test])) print ("cross-validation score : %s" %
   "{0:.3%}".format(np.mean(error))) #fit the model again so that it can
   be refered outside the function:
   model.fit(data[predictors],data[outcome])


  id28

   let   s make our first id28 model. one way would be to
   take all the variables into the model but this might result in
   overfitting (don   t worry if you   re unaware of this terminology yet). in
   simple words, taking all variables might result in the model
   understanding complex relations specific to the data and will not
   generalize well. read more about [132]id28.

   we can easily make some intuitive hypothesis to set the ball rolling.
   the chances of getting a loan will be higher for:
    1. applicants having a credit history (remember we observed this in
       exploration?)
    2. applicants with higher applicant and co-applicant incomes
    3. applicants with higher education level
    4. properties in urban areas with high growth perspectives

   so let   s make our first model with    credit_history   .

   [gears-b.png]
   outcome_var = 'loan_status' model = logisticregression() predictor_var
   = ['credit_history'] classification_model(model,
   df,predictor_var,outcome_var)

   accuracy : 80.945% cross-validation score : 80.946%

   [gears-b.png]
   #we can try different combination of variables: predictor_var =
   ['credit_history','education','married','self_employed','property_area'
   ] classification_model(model, df,predictor_var,outcome_var)

   accuracy : 80.945% cross-validation score : 80.946%

   generally we expect the accuracy to increase on adding variables. but
   this is a more challenging case. the accuracy and cross-validation
   score are not getting impacted by less important variables.
   credit_history is dominating the mode. we have two options now:
    1. feature engineering: dereive new information and try to predict
       those. i will leave this to your creativity.
    2. better modeling techniques. let   s explore this next.


  decision tree

   decision tree is another method for making a predictive model. it is
   known to provide higher accuracy than id28 model. read
   more about [133]id90.

   [gears-b.png]
   model = decisiontreeclassifier() predictor_var =
   ['credit_history','gender','married','education']
   classification_model(model, df,predictor_var,outcome_var)

   accuracy : 81.930% cross-validation score : 76.656%

   here the model based on categorical variables is unable to have an
   impact because credit history is dominating over them. let   s try a few
   numerical variables:

   [gears-b.png]
   #we can try different combination of variables: predictor_var =
   ['credit_history','loan_amount_term','loanamount_log']
   classification_model(model, df,predictor_var,outcome_var)

   accuracy : 92.345% cross-validation score : 71.009%

   here we observed that although the accuracy went up on adding
   variables, the cross-validation error went down. this is the result of
   model over-fitting the data. let   s try an even more sophisticated
   algorithm and see if it helps:


  id79

   id79 is another algorithm for solving the classification
   problem. read more about [134]id79.

   an advantage with id79 is that we can make it work with all
   the features and it returns a feature importance matrix which can be
   used to select features.

   [gears-b.png]
   model = randomforestclassifier(n_estimators=100) predictor_var =
   ['gender', 'married', 'dependents', 'education', 'self_employed',
   'loan_amount_term', 'credit_history', 'property_area',
   'loanamount_log','totalincome_log'] classification_model(model,
   df,predictor_var,outcome_var)

   accuracy : 100.000% cross-validation score : 78.179%

   here we see that the accuracy is 100% for the training set. this is the
   ultimate case of overfitting and can be resolved in two ways:
    1. reducing the number of predictors
    2. tuning the model parameters

   let   s try both of these. first we see the feature importance matrix
   from which we   ll take the most important features.

   [gears-b.png]
   #create a series with feature importances: featimp =
   pd.series(model.feature_importances_,
   index=predictor_var).sort_values(ascending=false) print (featimp)

   let   s use the top 5 variables for creating a model. also, we will
   modify the parameters of id79 model a little bit:

   [gears-b.png]
   model = randomforestclassifier(n_estimators=25, min_samples_split=25,
   max_depth=7, max_features=1) predictor_var =
   ['totalincome_log','loanamount_log','credit_history','dependents','prop
   erty_area'] classification_model(model, df,predictor_var,outcome_var)

   accuracy : 82.899% cross-validation score : 81.461%

   notice that although accuracy reduced, but the cross-validation score
   is improving showing that the model is generalizing well. remember that
   id79 models are not exactly repeatable. different runs will
   result in slight variations because of randomization. but the output
   should stay in the ballpark.

   you would have noticed that even after some basic parameter tuning on
   id79, we have reached a cross-validation accuracy only
   slightly better than the original id28 model. this
   exercise gives us some very interesting and unique learning:
    1. using a more sophisticated model does not guarantee better results.
    2. avoid using complex modeling techniques as a black box without
       understanding the underlying concepts. doing so would increase the
       tendency of overfitting thus making your models less interpretable
    3. [135]feature engineering is the key to success. everyone can use an
       xgboost models but the real art and creativity lies in enhancing
       your features to better suit the model.

   you can access the dataset and problem statement used in this post at
   this link: [136]loan prediction challenge


projects

   now, its time to take the plunge and actually play with some other real
   datasets. so are you ready to take on the challenge? accelerate your
   data science journey with the following practice problems:

   [137]practice problem: food demand forecasting challenge predict the
   demand of meals for a meal delivery company
   [138]practice problem: hr analytics challenge identify the employees
   most likely to get promoted
   [139]practice problem: predict number of upvotes predict number of
   upvotes on a query asked at an online question & answer platform


end notes

   i hope this tutorial will help you maximize your efficiency when
   starting with data science in python. i am sure this not only gave you
   an idea about basic data analysis methods but it also showed you how to
   implement some of the more sophisticated techniques available today.

   you should also check out our [140]free python course and then jump
   over to learn how to apply it for [141]data science.

   python is really a great tool, and is becoming an increasingly popular
   language among the data scientists. the reason being, it   s easy to
   learn, integrates well with other databases and tools like spark and
   hadoop. majorly, it has great computational intensity and has powerful
   data analytics libraries.

   so, learn python to perform the full life-cycle of any data science
   project. it includes reading, analyzing, visualizing and finally making
   predictions.

   if you come across any difficulty while practicing python, or you have
   any thoughts / suggestions / feedback on the post, please feel free to
   post them through comments below.

  note     the discussions of this article are going on at av   s discuss portal.
  [142]join here!

  if you like what you just read & want to continue your analytics
  learning, [143]subscribe to our emails, [144]follow us on twitter or like
  our [145]facebook page.

   you can also read this article on analytics vidhya's android app
   [146]get it on google play

share this:

     * [147]click to share on linkedin (opens in new window)
     * [148]click to share on facebook (opens in new window)
     * [149]click to share on twitter (opens in new window)
     * [150]click to share on pocket (opens in new window)
     * [151]click to share on reddit (opens in new window)
     *

related articles

   [ins: :ins]

   tags : [152]apply(), [153]box plots, [154]categorical variable,
   [155]data cleaning, [156]data exploration, [157]data frames, [158]data
   mining, [159]data munging, [160]data structure, [161]data wrangling,
   [162]dictionary, [163]distribution analysis, [164]installing python,
   [165]ipython, [166]lists, [167]id28, [168]matplotlib,
   [169]mean, [170]merge, [171]numpy, [172]pandas, [173]python,
   [174]python tutorial, [175]scikit-learn, [176]scipy, [177]sets,
   [178]split, [179]stacked chart, [180]starting python, [181]statistics,
   [182]strings, [183]tuple, [184]tuples, [185]tutorial

   next article

[infographic] 10 popular tv shows on data science and artificial intelligence

   previous article

model monitoring senior business analyst/assistant manager     gurgaon (5-6
years of experience)

[186]kunal jain

   kunal is a post graduate from iit bombay in aerospace engineering. he
   has spent more than 10 years in field of data science. his work
   experience ranges from mature markets like uk to a developing market
   like india. during this period he has lead teams of various sizes and
   has worked on various tools like sas, spss, qlikview, r, python and
   matlab.

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [187]discussion portal to get your queries resolved

54 comments

     * moumita mitra says:
       [188]january 15, 2016 at 4:41 am
       can you please suggest me good data analysis book on python
          + paritosh gupta says:
            [189]january 15, 2016 at 6:15 am
            there is a very good book on python for data analysis, o reily
                python for data analysis
          + [190]kunal jain says:
            [191]january 18, 2016 at 7:06 pm
            moumita,
            the book mentioned by paritosh is a good place to start. you
            can also refer some of the books mentioned here:
            [192]http://www.analyticsvidhya.com/blog/2014/06/books-data-sc
            ientists-or-aspiring-ones/
            hope this helps.
            kunal
               o deepak says:
                 [193]january 31, 2016 at 5:49 pm
                 hey kunal
                 im trying to follow your lesson however i am stuck at
                 reading the csv file. im using ipython and trying to read
                 it. i am following the syntax that you have provided but
                 it still doesnt work.
                 can you please help me if its possible i would really
                 appreciate it
                 thanks
                 deepak
     * pranesh says:
       [194]january 15, 2016 at 5:00 am
       hi kunal,
       when you are planning to schedule next data science meetup in
       bangalore. i have missed the previous session due to conflict
          + [195]kunal jain says:
            [196]january 18, 2016 at 7:08 pm
            pranesh,
            we will have a meetup some time in early march. we will
            announce the dates on datahack platform and our meetup group
            page.
            hope to see you around this time.
            regards,
            kunal
     * gianfranco says:
       [197]january 15, 2016 at 4:16 pm
       little error just matter for newbe as i   m:
       import matplotlib.pyplot as plt
       fig = plt.pyplot.figure(figsize=(8,4)) error
       import matplotlib.pyplot as plt
       fig = plt.figure(figsize=(8,4)) right
          + [198]kunal jain says:
            [199]january 18, 2016 at 7:12 pm
            thanks gianfranco for highlighting it. have corrected the
            same.
            regards,
            kunal
     * dheeraj patta says:
       [200]january 15, 2016 at 5:19 pm
       thank you so much kunal, this is indeed a great start for any
       python beginner.
       really appreciate your team   s effort in bringing data science to a
       wider audience.
       i strongly suggest    a byte of python    by swaroop ch. it may be bit
       old now but helped me in getting a good start in python.
     * highspirits says:
       [201]january 15, 2016 at 5:31 pm
       awesome!!! this is one area where i was looking for help and av has
       provided it!!! thanks a lot for the quick guide kunal   very much
       helpful   
          + [202]kunal jain says:
            [203]january 18, 2016 at 7:13 pm
            glad that you liked it highspirits!
     * kami888 says:
       [204]january 16, 2016 at 3:33 am
       great!!!!!! thank you!. i was just looking around for this.
          + [205]kunal jain says:
            [206]january 18, 2016 at 7:14 pm
            thanks kami888 for your comment.
            do let us know how you progress with this.
            regards,
            kunal
     * dr.d.k.samuel says:
       [207]january 16, 2016 at 3:59 am
       really well written, will be nice if it is made available as a pdf
       for download (with all supporting references). i will print and
       refer till i learn in full. thanks
     * smrutiranjan tripathy says:
       [208]january 17, 2016 at 10:04 am
       hi kunal ji ,
       can you please guide (for a newbie )who dont have any software
       background , how can acquire big data knowledge. whether is it
       necessary to learn sql , java ?before stepping in the big data
       practically, how can i warm up my self without getting in touch
       with the bias. can you please suggest good blog regarding big data
       for newbie.
          + [209]kunal jain says:
            [210]january 18, 2016 at 7:15 pm
            smrutiranjan,
            :
            kindly post this question on our discussion portal    
            [211]http://discuss.analyticsvidhya.com
            this is not relevant to the article above.
            regards,
            kunal
     * falkor says:
       [212]january 18, 2016 at 9:41 pm
       this was good until, the fact hit me. i am using idle and don   t
       have the libraries installed. now, how do i get these pandas, numpy
       etc installed for idle on windows!?
       its been a long complicated browsing session. only solution i seem
       to get is to ditch idle and move to spyder or move to python 3.5
       altogether.
       any solutions will be helpful, thank you.
          + digvijay says:
            [213]march 7, 2016 at 3:02 pm
            i suggest installing anaconda. its better to start with as it
            contains most of the commonly used libraries for data
            analysis. once anaconda is up and working, you can use any ide
            of your choice.
     * falkor says:
       [214]january 19, 2016 at 1:26 am
       got pandas to finally install and work, here is how i did it. just
       in case it helps somebody else.
       download pip-installer from here:
       [215]https://bootstrap.pypa.io/get-pip.py
       put it on to desktop or some known path
       open command prompt and point to the path or open the path
       execute the file in command prompt with: python get-pip.py
       check if you got it right using: python -m pip install -u pip
       this will ensure that you are on the current version
       restart the system, just for the heck of it. to be on safer side.
       in command prompt, set the path using this: c:\users\yourname>set
       path = %path%;c:\python27\scripts
       still in command prompt, install a library like: pip install numpy
       should work (maybe)
       *
       i had a c++ compiler error, installing this resolved it:
       [216]https://www.microsoft.com/en-us/download/details.aspx?id=44266
       try installing libraries again
       *
       **sources from all over the place!
     * [217]erik says:
       [218]january 19, 2016 at 9:20 am
       thank you for a real comprehensive post. personally, i am mainly
       using python for creating psychology experiments but i would like
       to start doing some analysis with python (right now i mainly use
       r). some of the libraries (e.g., seaborn) was new to me.
     * gt_67 says:
       [219]january 19, 2016 at 12:15 pm
       hello !
       i can   t let this piece of code to work:
       table = df.pivot_table(values=   loanamount   , index=   self_employed   
       ,columns=   education   , aggfunc=np.median)
       # define function to return value of this pivot_table
       def fage(x):
       return table.loc[x[   self_employed   ],x[   education   ]]
       # replace missing values
       df[   loanamount   ].fillna(df[df[   loanamount   ].isnull()].apply(fage,
       axis=1), inplace=true)
       i   ve this error:
       valueerror: invalid fill value with a
       i checked the null values of the columns    loanamount   ,
          self_employed    and    education    and nothing wrong shows out. 614
       values as others full columns.
       someone else had the same error ?
          + mohamed says:
            [220]january 20, 2016 at 1:02 am
            mr. gt_67,
            i have same the error do you have any idea what that could be?
            if kunal can help understand and fix this piece of code will
            be great.
          + dignity says:
            [221]february 29, 2016 at 6:14 am
            missing values are already replaced by the mean with this line
            of code (1.st way)
            df[   loanamount   ].fillna(df[   loanamount   ].mean(), inplace=true)
            before.
            this part is the second way of replacing missing values so
            if you skip above line the code should work.
     * mohamed says:
       [222]january 19, 2016 at 12:40 pm
       this is a great, great resource. thanks kunal. but let me ask you
       for curiosity is this how data scientist do at work, i mean it is
       like using a command like to get insight from the data, isn   t there
       gui with python so you can be more productive?
       keep up the good work.
     * kishore says:
       [223]january 19, 2016 at 12:51 pm
       hi kunal,
       thanks for the excellent tutorial using python. it would be great
       if you could do a similar tutorial using r.
       regards,
       kishore
     * [224]erik marsja says:
       [225]january 20, 2016 at 5:33 pm
       thank you kunal for a real comprehensive tutorial on doing data
       science in python! i really appreciated the list of libraires.
       really useful. i have, my self, started to look more and more on
       doing data analysis with python. i have tested pandas some and your
       exploratory analysis with-pandas part was also helpful.
     * venu says:
       [226]january 24, 2016 at 5:39 am
       good one
     * hemanth says:
       [227]january 24, 2016 at 3:00 pm
       is there a python library for performing ocr on pdf files? or for
       converting a raw scanned pdf to a    searchable    pdf? to perform text
       analytics   
     * abhi says:
       [228]january 24, 2016 at 4:38 pm
       hey, great article. i find my self getting hiccups the moment
       id203 and statistics start appearing. can you suggest a book
       that takes me through these easily just like in this tutorial. both
       of these seem to be the lifeline of ml.
     * deepak says:
       [229]january 31, 2016 at 5:51 pm
       hey kunal
       im trying to follow your lesson however i am stuck at reading the
       csv file. im using ipython and trying to read it. i am following
       the syntax that you have provided but it still doesnt work.
       can you please help me if its possible i would really appreciate it
       thanks
       deepak
     * deepak says:
       [230]february 2, 2016 at 2:21 am
       hello kunal i have started your tutorial but i am having difficulty
       at importing pandas an opening the csv file
       do you mind assisting me
       thanks
          + [231]kunal jain says:
            [232]february 3, 2016 at 11:35 am
            deepak,
            what is the problem you are facing? can you attach a
            screenshot?
            also, tell me which os are you working on and which python
            installation are you working on?
            regards,
            kunal
               o deepak says:
                 [233]february 3, 2016 at 4:14 pm
                 hey thanks for replying no i do not think i can attach a
                 screen shot on this blog wall. i would love to email it
                 to you but do not have your email address though.
                 but the problem i am having is trying to open the .csv
                 file (train). i have opened pylab inline
                 my code is like this :
                 line 1: %pylab inline
                 populating the interactive namespace from numpy and
                 matplotlib
                 line 2: import pandas as pd
                 df = pd.read_csv(   /desktop/studying_tools/av/train.csv   )
                 when i click run in ipython notebook. it gives me an
                 error like this:
                 oserror traceback (most recent call last)
                 in ()
                 1 import pandas as pd
                 2
                    -> 3 df =
                 pd.read_csv(   /desktop/studying_tools/av/train.csv   )
                 c:\users\deepak
                 mahtani\anaconda3\lib\site-packages\pandas\io\parsers.py
                 in parser_f(filepath_or_buffer, sep, dialect,
                 compression, doublequote, escapechar, quotechar, quoting,
                 skipinitialspace, lineterminator, header, index_col,
                 names, prefix, skiprows, skipfooter, skip_footer,
                 na_values, true_values, false_values, delimiter,
                 converters, dtype, usecols, engine, delim_whitespace,
                 as_recarray, na_filter, compact_ints, use_unsigned,
                 low_memory, buffer_lines, warn_bad_lines,
                 error_bad_lines, keep_default_na, thousands, comment,
                 decimal, parse_dates, keep_date_col, dayfirst,
                 date_parser, memory_map, float_precision, nrows,
                 iterator, chunksize, verbose, encoding, squeeze,
                 mangle_dupe_cols, tupleize_cols, infer_datetime_format,
                 skip_blank_lines)
                 496 skip_blank_lines=skip_blank_lines)
                 497
                    > 498 return _read(filepath_or_buffer, kwds)
                 499
                 500 parser_f.__name__ = name
                 c:\users\deepak
                 mahtani\anaconda3\lib\site-packages\pandas\io\parsers.py
                 in _read(filepath_or_buffer, kwds)
                 273
                 274 # create the parser.
                    > 275 parser = textfilereader(filepath_or_buffer,
                 **kwds)
                 276
                 277 if (nrows is not none) and (chunksize is not none):
                 c:\users\deepak
                 mahtani\anaconda3\lib\site-packages\pandas\io\parsers.py
                 in __init__(self, f, engine, **kwds)
                 588 self.options[   has_index_names   ] =
                 kwds[   has_index_names   ]
                 589
                    > 590 self._make_engine(self.engine)
                 591
                 592 def _get_options_with_defaults(self, engine):
                 c:\users\deepak
                 mahtani\anaconda3\lib\site-packages\pandas\io\parsers.py
                 in _make_engine(self, engine)
                 729 def _make_engine(self, engine=   c   ):
                 730 if engine ==    c   :
                    > 731 self._engine = cparserwrapper(self.f,
                 **self.options)
                 732 else:
                 733 if engine ==    python   :
                 c:\users\deepak
                 mahtani\anaconda3\lib\site-packages\pandas\io\parsers.py
                 in __init__(self, src, **kwds)
                 1101 kwds[   allow_leading_cols   ] = self.index_col is not
                 false
                 1102
                 -> 1103 self._reader = _parser.textreader(src, **kwds)
                 1104
                 1105 # xxx
                 pandas\parser.pyx in pandas.parser.textreader.__cinit__
                 (pandas\parser.c:3246)()
                 pandas\parser.pyx in
                 pandas.parser.textreader._setup_parser_source
                 (pandas\parser.c:6111)()
                 oserror: file b   /desktop/studying_tools/av/train.csv   
                 does not exist
                 im using anaconda
                 ipython notebook (jupyter)- version 4.0.4
                 im running it on my windows 8 laptop
                 please try help , and thanks again
     * jaini says:
       [234]february 7, 2016 at 12:35 am
       hi kunal
       sincere apologies for a very basic question. i have installed
       python per above instructions. unfortunately i am unable to launch
       ipython notebook. have spent hours but i guess i missing something.
       could you please kindly guide.
       thank you
       jaini
          + [235]kunal jain says:
            [236]february 7, 2016 at 10:46 am
            jaini,
            what is the error you are getting? which os you are on? and
            what happens when you type ipython notebook in shell /
            terminal / cmd ?
            regards,
            kunal
     * emanuel woiski says:
       [237]february 7, 2016 at 8:18 pm
       nice article!
       a few remarks:
       1-       pylab=inline    is not recommended any more. use    %matplotlib
       inline    for each notebook.
       2- you can start a jupyter server using    jupyter notebook    instead
       of    ipython notebook   . for me, notebooks open faster that way.
       3- for plotting, use    import matplotlib.pyplot as plt   .
       regards
       woiski
     * jaini says:
       [238]february 8, 2016 at 1:11 am
       thank you. i sincerely appreciate your instant response. i just
       reinstalled and went through command prompt and it worked.
     * ngnikhilgoyal says:
       [239]february 20, 2016 at 9:45 pm
       it would be good if you explained the code as you went along the
       exercise. for someone unfamiliar with some of the methods and
       functions, it is difficult to understand why you are doing certain
       things. for e.g.: while creating the pivot table, you introduced
       aggfunc=lambda x: x.map({   y   :1,   n   :0}).mean()) without explaining
       it. intuitively i know you are coding y as 1 and n as 0 and taking
       mean of each but you still need to explain what is lambda x: x.map
       . . . .
     * olga says:
       [240]february 26, 2016 at 1:06 pm
       there seems to be a bit of confusion, when you plot histogram.
       histogram, by definition, is a plot of occurrence frequency of some
       variable. so, when you do manipulation with applicantincome,
       transforming to a totalincome by adding coapplicantincome, the
       outcome does not affect the histogram of loanamount, because the
       outcome of this manipulation does not change the occurrence
       frequency or the values of loanamount. if you compare both of your
       plots, they will look exactly the same for mentioned above reason.
       so, it will be, probably, better to correct this part of the
       article.
     * adull kku says:
       [241]february 29, 2016 at 3:40 am
       thanks
     * vlad says:
       [242]february 29, 2016 at 11:13 pm
       hi kunal     first off thanks for this informative tutorial. great
       stuff. unfortunately i   m unable to download the dataset     i need to
       be signed up on av, and i get an invalid request on signup. thank
       you again for this material.
     * vlad says:
       [243]march 1, 2016 at 3:37 am
       worked when i tried again after a few hours. nevermind!
     * dorinel says:
       [244]march 10, 2016 at 1:28 pm
       hi kunal,
       dont you give us access to the data set any more? i am reading your
       tutorial and want to repeat your steps for data analysis!
       thanks,
       dorinel
     * sam says:
       [245]march 11, 2016 at 10:43 am
       when running this code :
       table = df.pivot_table(values=   loanamount   , index=   self_employed   
       ,columns=   education   , aggfunc=np.median)
       # define function to return value of this pivot_table
       def fage(x):
       return table.loc[x[   self_employed   ],x[   education   ]]
       # replace missing values
       df[   loanamount   ].fillna(df[df[   loanamount   ].isnull()].apply(fage,
       axis=1), inplace=true)
       i am getting this error:
       keyerror: (   the label [graduate] is not in the [index]   , u   occurred
       at index 0   )
       any ideas?
       thanks in advance
     * marc says:
       [246]march 12, 2016 at 12:59 am
       thanks for this. is there a way to get access to the dataset that
       was used for this? seems like it became unavailable from march 7!
     * pavan kumar says:
       [247]may 29, 2016 at 6:29 am
       really great and would start following     i am a new entry to the
       data analysis stream
     * harneet says:
       [248]july 28, 2016 at 4:20 am
       hi kunal,
       i have trying to get some validations in python for logistic
       regression as available for sas, like area under curve, concordant,
       discordant and tied pairs, ginni value etc.. but i am unable to
       find it through google, what ever i was able to find was very
       confusing.
       can you please help me with this?
       regards,
       harneet.
     * umaryusuf says:
       [249]august 8, 2016 at 9:54 am
       very well written tutorial to learn data science with python.
     * asif ameer says:
       [250]august 19, 2016 at 5:06 pm
       really awesome kunal jain, i appreciate your work   .
     * peter frech says:
       [251]august 23, 2016 at 9:08 am
       hello,
       very good article. i just stumbled upon one piece of code where i
       am not quite sure if i just don    interpret the arguments well, or
       whether there is truely a mistake in your code. it is the
       following:
       metrics.accuracy_score(predictions,data[outcome])
       isn   t    predictions    the true predictions, which should be placed as
       the argument    y_pred    of the accuracy_score method, and
          data[outcome]    are the real values which should be associated with
       the argument    y_true   ?
       if that is so, then i think the order of passing the arguments is
       wrong, because the method is defined as following (according to
       doc): confusion_matrix(y_true, y_pred[, labels])    > that means
       y_true comes as 1st argument. you have it the other way arround.
       or doesn   t make it a difference at all? anyways.
       best regards,
       peter
     * nicola says:
       [252]september 4, 2016 at 8:36 am
       hi!
       and thank you very much for your tutorial
       unfortunately there is no way to find the .csv file for the loan
       prediction problem in
       [253]https://datahack.analyticsvidhya.com/contest/practice-problem-
       loan-prediction-iii/
     * wayne says:
       [254]september 18, 2016 at 1:32 am
       hello,
       thank you for the tutorial.
       but as already mentioned by nicola, there is no way to download the
       dataset.
       could you please check it?
       thanks
     * gopalankailash says:
       [255]october 28, 2016 at 3:49 pm
       the amount of effort you guys put into these article is a true
       inspiration for folks like me to learn!
       thanks for all this!
     * jack ma says:
       [256]november 8, 2016 at 10:52 pm
       great one. thank you.
       when i type in    df.describe()     , it works, but it gives me a
       warning information :
          user\appdata\local\continuum\anaconda3\lib\site-packages\numpy\lib
       \function_base.py:3834: runtimewarning: invalid value encountered
       in percentile
       runtimewarning)    
       what is it means?
       secondly, when i running    df[   applicantincome   ].hist(bins=50)   
       it tells me       , so i can not see the chart.
       anyone can helps? thank you.

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [257]srk       3924
   2    [2.jpg?date=2019-04-05] [258]mark12    3510
   3    [3.jpg?date=2019-04-05] [259]nilabha   3261
   4    [4.jpg?date=2019-04-05] [260]nitish007 3237
   5    [5.jpg?date=2019-04-05] [261]tezdhar   3082
   [262]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [263]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [264]understanding support vector machine algorithm from examples
       (along with code)
     * [265]essentials of machine learning algorithms (with python and r
       codes)
     * [266]a complete tutorial to learn data science with python from
       scratch
     * [267]7 types of regression techniques you should know!
     * [268]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [269]a simple introduction to anova (with applications in excel)
     * [270]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [271]top 5 machine learning github repositories and reddit discussions
   from march 2019

[272]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [273]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[274]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [275]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[276]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [277]16 opencv functions to start your id161 journey (with
   python code)

[278]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [279][ds-finhack.jpg]

   [280][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [281]about us
     * [282]our team
     * [283]career
     * [284]contact us
     * [285]write for us

   [286]about us
   [287]   
   [288]our team
   [289]   
   [290]careers
   [291]   
   [292]contact us

data scientists

     * [293]blog
     * [294]hackathon
     * [295]discussions
     * [296]apply jobs
     * [297]leaderboard

companies

     * [298]post jobs
     * [299]trainings
     * [300]hiring hackathons
     * [301]advertising
     * [302]reach us

   don't have an account? [303]sign up here.

join our community :

   [304]46336 [305]followers
   [306]20220 [307]followers
   [308]followers
   [309]7513 [310]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [311]privacy policy
     * [312]terms of use
     * [313]refund policy

   don't have an account? [314]sign up here

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [315](button) join now

   subscribe!

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [316](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/machine-learning/
  94. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
  95. https://www.analyticsvidhya.com/blog/category/machine-learning/
  96. https://www.analyticsvidhya.com/blog/category/python-2/pandas/
  97. https://www.analyticsvidhya.com/blog/category/python-2/
  98. https://www.analyticsvidhya.com/blog/author/kunalj/
  99. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=learndswithpythonscratcharticle
 100. https://courses.analyticsvidhya.com/courses/introduction-to-data-science?utm_source=blog&utm_medium=learndswithpythonscratcharticle
 101. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=learndswithpythonscratcharticle
 102. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=learndswithpythonscratcharticle
 103. https://www.analyticsvidhya.com/blog/2014/03/sas-vs-vs-python-tool-learn/utm_source=blog&utm_medium=learndswithpythonscratcharticle
 104. https://www.python.org/download/releases/2.7/
 105. https://www.continuum.io/downloads
 106. https://www.enthought.com/downloads/
 107. https://www.analyticsvidhya.com/blog/wp-content/uploads/2014/07/python_idle.png
 108. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/jupyter1.png
 109. https://www.analyticsvidhya.com/blog/wp-content/uploads/2014/07/python_lists.png
 110. https://www.analyticsvidhya.com/blog/wp-content/uploads/2014/07/python_strings.png
 111. https://www.analyticsvidhya.com/blog/wp-content/uploads/2014/07/python_tuples.png
 112. https://www.analyticsvidhya.com/blog/wp-content/uploads/2014/07/python_dictionary.png
 113. https://www.analyticsvidhya.com/blog/wp-content/uploads/2014/08/pandas.jpg
 114. http://pandas.pydata.org/pandas-docs/stable/10min.html
 115. http://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii
 116. https://www.analyticsvidhya.com/blog/wp-content/uploads/2014/08/ipython_pylab_check.png
 117. https://www.analyticsvidhya.com/blog/2014/07/statistics/
 118. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/output_13_1.png
 119. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/output_14_1.png
 120. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
 121. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/11.-pivot_python.png
 122. https://www.analyticsvidhya.com/blog/2014/09/data-munging-python-using-pandas-baby-steps-python/
 123. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
 124. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/4.-missing.png
 125. https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/
 126. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/6.-self-emp.png
 127. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/7.-loan-log.png
 128. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/8.-total-income-log.png
 129. https://www.analyticsvidhya.com/blog/2015/01/scikit-learn-python-machine-learning-tool/
 130. https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/
 131. https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-python-r/
 132. https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/
 133. https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/
 134. https://www.analyticsvidhya.com/blog/2015/09/random-forest-algorithm-multiple-challenges/
 135. https://www.analyticsvidhya.com/blog/2015/03/feature-engineering-variable-transformation-creation/
 136. https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&utm_medium=blog
 137. https://datahack.analyticsvidhya.com/contest/genpact-machine-learning-hackathon-1/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&utm_medium=blog
 138. https://datahack.analyticsvidhya.com/contest/wns-analytics-hackathon-2018-1/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&utm_medium=blog
 139. https://datahack.analyticsvidhya.com/contest/enigma-codefest-machine-learning-1/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&utm_medium=blog
 140. https://courses.analyticsvidhya.com/courses/introduction-to-data-science?utm_source=blog&utm_medium=learndswithpythonscratcharticle
 141. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=learndswithpythonscratcharticle
 142. https://discuss.analyticsvidhya.com/t/discussions-for-article-a-complete-tutorial-to-learn-data-science-with-python-from-scratch/65186?u=jalfaizy
 143. http://feedburner.google.com/fb/a/mailverify?uri=analyticsvidhya
 144. http://twitter.com/analyticsvidhya
 145. http://facebook.com/analyticsvidhya
 146. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 147. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/?share=linkedin
 148. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/?share=facebook
 149. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/?share=twitter
 150. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/?share=pocket
 151. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/?share=reddit
 152. https://www.analyticsvidhya.com/blog/tag/apply/
 153. https://www.analyticsvidhya.com/blog/tag/box-plots/
 154. https://www.analyticsvidhya.com/blog/tag/categorical-variable/
 155. https://www.analyticsvidhya.com/blog/tag/data-cleaning/
 156. https://www.analyticsvidhya.com/blog/tag/data-exploration/
 157. https://www.analyticsvidhya.com/blog/tag/data-frames/
 158. https://www.analyticsvidhya.com/blog/tag/data-mining/
 159. https://www.analyticsvidhya.com/blog/tag/data-munging/
 160. https://www.analyticsvidhya.com/blog/tag/data-structure/
 161. https://www.analyticsvidhya.com/blog/tag/data-wrangling/
 162. https://www.analyticsvidhya.com/blog/tag/dictionary/
 163. https://www.analyticsvidhya.com/blog/tag/distribution-analysis/
 164. https://www.analyticsvidhya.com/blog/tag/installing-python/
 165. https://www.analyticsvidhya.com/blog/tag/ipython/
 166. https://www.analyticsvidhya.com/blog/tag/lists/
 167. https://www.analyticsvidhya.com/blog/tag/logistic-regression/
 168. https://www.analyticsvidhya.com/blog/tag/matplotlib/
 169. https://www.analyticsvidhya.com/blog/tag/mean/
 170. https://www.analyticsvidhya.com/blog/tag/merge/
 171. https://www.analyticsvidhya.com/blog/tag/numpy/
 172. https://www.analyticsvidhya.com/blog/tag/pandas/
 173. https://www.analyticsvidhya.com/blog/tag/python/
 174. https://www.analyticsvidhya.com/blog/tag/python-tutorial/
 175. https://www.analyticsvidhya.com/blog/tag/scikit-learn/
 176. https://www.analyticsvidhya.com/blog/tag/scipy/
 177. https://www.analyticsvidhya.com/blog/tag/sets/
 178. https://www.analyticsvidhya.com/blog/tag/split/
 179. https://www.analyticsvidhya.com/blog/tag/stacked-chart/
 180. https://www.analyticsvidhya.com/blog/tag/starting-python/
 181. https://www.analyticsvidhya.com/blog/tag/statistics/
 182. https://www.analyticsvidhya.com/blog/tag/strings/
 183. https://www.analyticsvidhya.com/blog/tag/tuple/
 184. https://www.analyticsvidhya.com/blog/tag/tuples/
 185. https://www.analyticsvidhya.com/blog/tag/tutorial/
 186. https://www.analyticsvidhya.com/blog/author/kunalj/
 187. https://discuss.analyticsvidhya.com/
 188. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-103788
 189. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-103796
 190. http://www.analyticsvidhya.com/
 191. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104034
 192. http://www.analyticsvidhya.com/blog/2014/06/books-data-scientists-or-aspiring-ones/
 193. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104988
 194. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-103789
 195. http://www.analyticsvidhya.com/
 196. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104036
 197. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-103830
 198. http://www.analyticsvidhya.com/
 199. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104037
 200. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-103834
 201. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-103835
 202. http://www.analyticsvidhya.com/
 203. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104038
 204. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-103865
 205. http://www.analyticsvidhya.com/
 206. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104039
 207. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-103868
 208. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-103955
 209. http://www.analyticsvidhya.com/
 210. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104040
 211. http://discuss.analyticsvidhya.com/
 212. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104049
 213. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-106809
 214. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104056
 215. https://bootstrap.pypa.io/get-pip.py
 216. https://www.microsoft.com/en-us/download/details.aspx?id=44266
 217. http://www.marsja.se/
 218. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104093
 219. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104100
 220. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104156
 221. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-106345
 222. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104108
 223. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104110
 224. http://www.marsja.se/
 225. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104206
 226. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104498
 227. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104522
 228. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104534
 229. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-104989
 230. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-105054
 231. http://www.analyticsvidhya.com/
 232. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-105143
 233. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-105160
 234. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-105300
 235. http://www.analyticsvidhya.com/
 236. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-105313
 237. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-105342
 238. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-105350
 239. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-105981
 240. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-106263
 241. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-106333
 242. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-106401
 243. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-106408
 244. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-107006
 245. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-107088
 246. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-107119
 247. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-111563
 248. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-114171
 249. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-114561
 250. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-114960
 251. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-115048
 252. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-115563
 253. https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/
 254. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-116168
 255. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-117605
 256. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/#comment-118148
 257. https://datahack.analyticsvidhya.com/user/profile/srk
 258. https://datahack.analyticsvidhya.com/user/profile/mark12
 259. https://datahack.analyticsvidhya.com/user/profile/nilabha
 260. https://datahack.analyticsvidhya.com/user/profile/nitish007
 261. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 262. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 263. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 264. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 265. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 266. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 267. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 268. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 269. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 270. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 271. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 272. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 273. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 274. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 275. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 276. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 277. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 278. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 279. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 280. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 281. http://www.analyticsvidhya.com/about-me/
 282. https://www.analyticsvidhya.com/about-me/team/
 283. https://www.analyticsvidhya.com/career-analytics-vidhya/
 284. https://www.analyticsvidhya.com/contact/
 285. https://www.analyticsvidhya.com/about-me/write/
 286. http://www.analyticsvidhya.com/about-me/
 287. https://www.analyticsvidhya.com/about-me/team/
 288. https://www.analyticsvidhya.com/about-me/team/
 289. https://www.analyticsvidhya.com/about-me/team/
 290. https://www.analyticsvidhya.com/career-analytics-vidhya/
 291. https://www.analyticsvidhya.com/about-me/team/
 292. https://www.analyticsvidhya.com/contact/
 293. https://www.analyticsvidhya.com/blog
 294. https://datahack.analyticsvidhya.com/
 295. https://discuss.analyticsvidhya.com/
 296. https://www.analyticsvidhya.com/jobs/
 297. https://datahack.analyticsvidhya.com/users/
 298. https://www.analyticsvidhya.com/corporate/
 299. https://trainings.analyticsvidhya.com/
 300. https://datahack.analyticsvidhya.com/
 301. https://www.analyticsvidhya.com/contact/
 302. https://www.analyticsvidhya.com/contact/
 303. https://datahack.analyticsvidhya.com/signup/
 304. https://www.facebook.com/analyticsvidhya/
 305. https://www.facebook.com/analyticsvidhya/
 306. https://twitter.com/analyticsvidhya
 307. https://twitter.com/analyticsvidhya
 308. https://plus.google.com/+analyticsvidhya
 309. https://in.linkedin.com/company/analytics-vidhya
 310. https://in.linkedin.com/company/analytics-vidhya
 311. https://www.analyticsvidhya.com/privacy-policy/
 312. https://www.analyticsvidhya.com/terms/
 313. https://www.analyticsvidhya.com/refund-policy/
 314. https://id.analyticsvidhya.com/accounts/signup/
 315. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 316. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 318. https://www.facebook.com/analyticsvidhya
 319. https://twitter.com/analyticsvidhya
 320. https://plus.google.com/+analyticsvidhya/posts
 321. https://in.linkedin.com/company/analytics-vidhya
 322. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2016/01/learn-data-science-with-python-from-scratch.png
 323. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/1.-head.png
 324. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/2.-describe.png
 325. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/output_6_1.png
 326. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/output_7_1.png
 327. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/10.-pivot_table3.png
 328. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/output_16_1.png
 329. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/output_17_1.png
 330. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/output_18_1.png
 331. https://www.analyticsvidhya.com/blog/wp-content/uploads/2014/09/orange-shoes.jpg
 332. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/5.-loan-amount-boxplot.png
 333. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/9.-rf-feat-imp.png
 334. https://datahack.analyticsvidhya.com/contest/genpact-machine-learning-hackathon-1/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&utm_medium=blog
 335. https://datahack.analyticsvidhya.com/contest/wns-analytics-hackathon-2018-1/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&utm_medium=blog
 336. https://datahack.analyticsvidhya.com/contest/enigma-codefest-machine-learning-1/?utm_source=complete-tutorial-learn-data-science-python-scratch-2&utm_medium=blog
 337. https://www.analyticsvidhya.com/blog/2016/01/10-popular-tv-shows-data-science-artificial-intelligence/
 338. https://www.analyticsvidhya.com/blog/2016/01/model-monitoring-senior-business-analystassistant-manager-gurgaon-5-6-years-experience/
 339. https://www.analyticsvidhya.com/blog/author/kunalj/
 340. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 341. https://www.facebook.com/analyticsvidhya/
 342. https://twitter.com/analyticsvidhya
 343. https://plus.google.com/+analyticsvidhya
 344. https://plus.google.com/+analyticsvidhya
 345. https://in.linkedin.com/company/analytics-vidhya
 346. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 347. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 348. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 349. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 350. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 351. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 352. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 353. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 354. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 355. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 356. javascript:void(0);
 357. javascript:void(0);
 358. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 359. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 360. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 361. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 362. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 363. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 364. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 365. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 366. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 367. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2fcomplete-tutorial-learn-data-science-python-scratch-2%2f&linkname=a%20complete%20tutorial%20to%20learn%20data%20science%20with%20python%20from%20scratch
 368. javascript:void(0);
 369. javascript:void(0);
