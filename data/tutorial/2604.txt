   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    12 useful pandas techniques in python for data
   manipulation comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]machine learning [94]12 useful pandas techniques in python
   for data manipulation

   [95]machine learning[96]pandas[97]python

12 useful pandas techniques in python for data manipulation

   [98]aarshay jain, january 3, 2016

introduction

   python is fast becoming the preferred language in [99]data science    
   and for good reason(s). it provides the larger ecosystem of a
   programming language and the depth of good scientific computation
   libraries. if you are starting to learn python, have a look at
   [100]learning path on python.

   among its scientific computation libraries, i found pandas to be the
   most useful for data science operations. pandas, along with
   scikit-learn provides almost the entire stack needed by a data
   scientist. this article focuses on providing 12 ways for data
   manipulation in python. i   ve also shared some tips & tricks which will
   allow you to work faster.

   i would recommend that you look at the codes for [101]data
   exploration before going ahead. to help you understand better, i   ve
   taken a data set to perform these operations and manipulations.

   if you   re just starting out your data science journey, they you   ll love
   the    [102]introduction to data science    course. it covers the basics of
   python, comprehensive introduction to statistics and several machine
   learning algorithms. a must-have course!

   data set: i   ve used the data set of [103]loan prediction problem.
   download the dataset and get started.

   [104]pandas, python techniques


let   s get started

   i   ll start by importing modules and loading the data set into python
   environment:
import pandas as pd
import numpy as np
data = pd.read_csv("train.csv", index_col="loan_id")



#1     boolean indexing

   what do you do, if you want to filter values of a column based on
   conditions from another set of columns? for instance, we want a list of
   all females who are not graduate and got a loan. boolean indexing can
   help here. you can use the following code:
data.loc[(data["gender"]=="female") & (data["education"]=="not graduate") & (dat
a["loan_status"]=="y"), ["gender","education","loan_status"]]

   [105]1. boolean indexing

   read more: [106]pandas selecting and indexing


#2     apply function

   it is one of the commonly used functions for playing with data and
   creating new variables. apply returns some value after passing each
   row/column of a data frame with some function. the function can be both
   default or user-defined. for instance, here it can be used to find the
   #missing values in each row and column.
#create a new function:
def num_missing(x):
  return sum(x.isnull())

#applying per column:
print "missing values per column:"
print data.apply(num_missing, axis=0) #axis=0 defines that function is to be app
lied on each column

#applying per row:
print "\nmissing values per row:"
print data.apply(num_missing, axis=1).head() #axis=1 defines that function is to
 be applied on each row

   [107]3 - apply

   thus we get the desired result.

   note: head() function is used in second output because it contains many
   rows.
   read more: [108]pandas reference (apply)


#3     imputing missing files

      fillna()    does it in one go. it is used for updating missing values
   with the overall mean/mode/median of the column. let   s impute the
      gender   ,    married    and    self_employed    columns with their respective
   modes.
#first we import a function to determine the mode
from scipy.stats import mode
mode(data['gender'])

   output: moderesult(mode=array([   male   ], dtype=object),
   count=array([489]))

   this returns both mode and count. remember that mode can be an array as
   there can be multiple values with high frequency. we will take the
   first one by default always using:
mode(data['gender']).mode[0]

   [109]4.2 - mode2

   now we can fill the missing values and check using technique #2.
#impute the values:
data['gender'].fillna(mode(data['gender']).mode[0], inplace=true)
data['married'].fillna(mode(data['married']).mode[0], inplace=true)
data['self_employed'].fillna(mode(data['self_employed']).mode[0], inplace=true)

#now check the #missing values again to confirm:
print data.apply(num_missing, axis=0)

   [110]4.3 - after impute

   hence, it is confirmed that missing values are imputed. please note
   that this is the most primitive form of imputation. other sophisticated
   techniques include modeling the missing values, using grouped averages
   (mean/mode/median). i   ll cover that part in my next articles.

   read more: [111]pandas reference (fillna)


#4     pivot table

   pandas can be used to create ms excel style pivot tables. for instance,
   in this case, a key column is    loanamount    which has missing values. we
   can impute it using mean amount of each    gender   ,    married    and
      self_employed    group. the mean    loanamount    of each group can be
   determined as:
#determine pivot table
impute_grps = data.pivot_table(values=["loanamount"], index=["gender","married",
"self_employed"], aggfunc=np.mean)
print impute_grps

   [112]5. pivot table

   more: [113]pandas reference (pivot table)


#5     multi-indexing

   if you notice the output of step #3, it has a strange property. each
   index is made up of a combination of 3 values. this is called
   multi-indexing. it helps in performing operations really fast.

   continuing the example from #3, we have the values for each group but
   they have not been imputed.
   this can be done using the various techniques learned till now.
#iterate only through rows with missing loanamount
for i,row in data.loc[data['loanamount'].isnull(),:].iterrows():
  ind = tuple([row['gender'],row['married'],row['self_employed']])
  data.loc[i,'loanamount'] = impute_grps.loc[ind].values[0]

#now check the #missing values again to confirm:
print data.apply(num_missing, axis=0)

   [114]6. multi-indexing

   note:
    1. multi-index requires tuple for defining groups of indices in loc
       statement. this a tuple used in function.
    2. the .values[0] suffix is required because, by default a series
       element is returned which has an index not matching with that of
       the dataframe. in this case, a direct assignment gives an error.


#6. crosstab

   this function is used to get an initial    feel    (view) of the data.
   here, we can validate some basic hypothesis. for instance, in this
   case,    credit_history    is expected to affect the loan status
   significantly. this can be tested using cross-tabulation as shown
   below:
pd.crosstab(data["credit_history"],data["loan_status"],margins=true)

   [115]7.1 - abs crosstab

   these are absolute numbers. but, percentages can be more intuitive in
   making some quick insights. we can do this using the apply function:
def percconvert(ser):
  return ser/float(ser[-1])
  pd.crosstab(data["credit_history"],data["loan_status"],margins=true).apply(per
cconvert, axis=1)

   [116]7.2 - perc crosstab

   now, it is evident that people with a credit history have much higher
   chances of getting a loan as 80% people with credit history got a loan
   as compared to only 9% without credit history.

   but that   s not it. it tells an interesting story. since i know that
   having a credit history is super important, what if i predict loan
   status to be y for ones with credit history and n otherwise.
   surprisingly, we   ll be right 82+378=460 times out of 614 which is a
   whopping 75%!

   i won   t blame you if you   re wondering why the hell do we need
   statistical models. but trust me, increasing the accuracy by even
   0.001% beyond this mark is a challenging task. would you take this
   [117]challenge?

   note: 75% is on train set. the test set will be slightly different but
   close. also, i hope this gives some intuition into why even a 0.05%
   increase in accuracy can result in jump of 500 ranks on the kaggle
   leaderboard.

   read more: [118]pandas reference (crosstab)


#7     merge dataframes

   merging dataframes become essential when we have information coming
   from different sources to be collated. consider a hypothetical case
   where the average property rates (inr per sq meters) is available for
   different property types. let   s define a dataframe as:
prop_rates = pd.dataframe([1000, 5000, 12000], index=['rural','semiurban','urban
'],columns=['rates'])
prop_rates

   [119]8.1 - rates

   now we can merge this information with the original dataframe as:
data_merged = data.merge(right=prop_rates, how='inner',left_on='property_area',r
ight_index=true, sort=false)
data_merged.pivot_table(values='credit_history',index=['property_area','rates'],
 aggfunc=len)

   [120]8.2 - pivot

   the pivot table validates successful merge operation. note that the
      values    argument is irrelevant here because we are simply counting the
   values.

   readmore: [121]pandas reference (merge)


#8     sorting dataframes

   pandas allow easy sorting based on multiple columns. this can be done
   as:
data_sorted = data.sort_values(['applicantincome','coapplicantincome'], ascendin
g=false)
data_sorted[['applicantincome','coapplicantincome']].head(10)

   [122]9. sort

   note: pandas    sort    function is now deprecated. we should use
      sort_values    instead.

   more: [123]pandas reference (sort_values)


#9     plotting (boxplot & histogram)

   many of you might be unaware that boxplots and histograms can be
   directly plotted in pandas and calling matplotlib separately is not
   necessary. it   s just a 1-line command. for instance, if we want to
   compare the distribution of applicantincome by loan_status:
import matplotlib.pyplot as plt
%matplotlib inline
data.boxplot(column="applicantincome",by="loan_status")

   [124]10.1 - boxplot
data.hist(column="applicantincome",by="loan_status",bins=30)

   [125]10.2 - hist

   this shows that income is not a big deciding factor on its own as there
   is no appreciable difference between the people who received and were
   denied the loan.

   read more: [126]pandas reference (hist) | [127]pandas reference
   (boxplot)


#10     cut function for binning

   sometimes numerical values make more sense if clustered together. for
   example, if we   re trying to model traffic (#cars on road) with time of
   the day (minutes). the exact minute of an hour might not be that
   relevant for predicting traffic as compared to actual period of the day
   like    morning   ,    afternoon   ,    evening   ,    night   ,    late night   . modeling
   traffic this way will be more intuitive and will avoid overfitting.

   here we define a simple function which can be re-used for binning any
   variable fairly easily.
#binning:
def binning(col, cut_points, labels=none):
  #define min and max values:
  minval = col.min()
  maxval = col.max()

  #create list by adding min and max to cut_points
  break_points = [minval] + cut_points + [maxval]

  #if no labels provided, use default labels 0 ... (n-1)
  if not labels:
    labels = range(len(cut_points)+1)

  #binning using cut function of pandas
  colbin = pd.cut(col,bins=break_points,labels=labels,include_lowest=true)
  return colbin

#binning age:
cut_points = [90,140,190]
labels = ["low","medium","high","very high"]
data["loanamount_bin"] = binning(data["loanamount"], cut_points, labels)
print pd.value_counts(data["loanamount_bin"], sort=false)

   read more: [128]pandas reference (cut)


#11     coding nominal data

   often, we find a case where we   ve to modify the categories of a nominal
   variable. this can be due to various reasons:
    1. some algorithms (like id28) require all inputs to be
       numeric. so nominal variables are mostly coded as 0, 1   .(n-1)
    2. sometimes a category might be represented in 2 ways. for e.g.
       temperature might be recorded as    high   ,    medium   ,    low   ,    h   ,
          low   . here, both    high    and    h    refer to same category. similarly,
       in    low    and    low    there is only a difference of case. but, python
       would read them as different levels.
    3. some categories might have very low frequencies and its generally a
       good idea to combine them.

   here i   ve defined a generic function which takes in input as a
   dictionary and codes the values using    replace    function in pandas.
#define a generic function using pandas replace function
def coding(col, codedict):
  colcoded = pd.series(col, copy=true)
  for key, value in codedict.items():
    colcoded.replace(key, value, inplace=true)
  return colcoded

#coding loanstatus as y=1, n=0:
print 'before coding:'
print pd.value_counts(data["loan_status"])
data["loan_status_coded"] = coding(data["loan_status"], {'n':0,'y':1})
print '\nafter coding:'
print pd.value_counts(data["loan_status_coded"])

   [129]12. code

   similar counts before and after proves the coding.

   read more: [130]pandas reference (replace)


#12     iterating over rows of a dataframe

   this is not a frequently used operation. still, you don   t want to get
   stuck. right? at times you may need to iterate through all rows using a
   for loop. for instance, one common problem we face is the incorrect
   treatment of variables in python. this generally happens when:
    1. nominal variables with numeric categories are treated as numerical.
    2. numeric variables with characters entered in one of the rows (due
       to a data error) are considered categorical.

   so it   s generally a good idea to manually define the column types. if
   we check the data types of all columns:
#check current type:
data.dtypes

   [131]2.1 - initial type

   here we see that credit_history is a nominal variable but appearing as
   float. a good way to tackle such issues is to create a csv file with
   column names and types. this way, we can make a generic function to
   read the file and assign column data types. for instance, here i have
   created a csv file [132]datatypes.csv.
#load the file:
coltypes = pd.read_csv('datatypes.csv')
print coltypes


   [133]2.2 - file content

   after loading this file, we can iterate through each row and assign the
   datatype using column    type    to the variable name defined in the
      feature    column.
#iterate through each row and assign variable type.
#note: astype is used to assign types

for i, row in coltypes.iterrows():  #i: dataframe index; row: each row in series
 format
    if row['type']=="categorical":
        data[row['feature']]=data[row['feature']].astype(np.object)
    elif row['type']=="continuous":
        data[row['feature']]=data[row['feature']].astype(np.float)
print data.dtypes


   [134]2.3 - after changing

   now the credit history column is modified to    object    type which is
   used for representing nominal variables in pandas.
   read more: [135]pandas reference (iterrows)


projects

   now, its time to take the plunge and actually play with some other real
   datasets. so are you ready to take on the challenge? accelerate your
   data science journey with the following practice problems:
   [136]practice problem: food demand forecasting challenge predict the
   demand of meals for a meal delivery company
   [137]practice problem: hr analytics challenge identify the employees
   most likely to get promoted
   [138]practice problem: predict number of upvotes predict number of
   upvotes on a query asked at an online question & answer platform


end notes

   in this article, we covered various functions of pandas which can make
   our life easy while performing data exploration and feature
   engineering. also, we defined some generic functions which can be
   reused for achieving similar objective on different datasets.

   also see: if you have any doubts pertaining to pandas or python in
   general, feel free to [139]discuss with us.

   did you find the article useful? do you use some better (easier/faster)
   techniques for performing the tasks discussed above? do you think there
   are better alternatives to pandas in python? we   ll be glad if you share
   your thoughts as comments below.

if you like what you just read & want to continue your analytics
learning, [140]subscribe to our emails, [141]follow us on twitter or like
our [142]facebook page.

   you can also read this article on analytics vidhya's android app
   [143]get it on google play

share this:

     * [144]click to share on linkedin (opens in new window)
     * [145]click to share on facebook (opens in new window)
     * [146]click to share on twitter (opens in new window)
     * [147]click to share on pocket (opens in new window)
     * [148]click to share on reddit (opens in new window)
     *

related articles

   [ins: :ins]

   tags : [149]binning, [150]data exploration, [151]data manipulation,
   [152]imputing missing values, [153]pandas, [154]python, [155]sorting
   next article

data software engineer     ahmedabad (5-12 years of experience)

   previous article

semantic web experts     gurgaon (6+ years of experience)

[156]aarshay jain

   aarshay is a ml enthusiast, pursuing ms in data science at columbia
   university, graduating in dec 2017. he is currently exploring the
   various ml techniques and writes articles for av to share his knowledge
   with the community.
     *
     *
     *
     *

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [157]discussion portal to get your queries resolved

55 comments

     * cowboybobjr says:
       [158]january 4, 2016 at 6:41 pm
       for #3     imputing missing values, where do    moderesult    come from?
       using it as it   s given in the example above gives error:
          moderesult is not defined.   
       [159]reply
          + aarshay jain says:
            [160]january 5, 2016 at 4:41 am
            actually the line    moderesult(mode=array([   male   ],
            dtype=object), count=array([489]))    is the output of the above
            code:
               mode(data[   gender   ])   .
            this shows that the output is not a scalar but an array
            containing    mode    and    count    as the 2 parts. to extract the
            mode value as a scalar, we need to write:
            mode(data[   gender   ]).mode[0]
               .mode    would point to the mode element of the array. but this
            results in again an array because mode need not always be a
            unique value. thus, we have to include    [0]    to get the first
            element of the array as a scalar, which can be used for
            imputation.
            hope this makes sense.
            [161]reply
               o cowboybobjr says:
                 [162]january 5, 2016 at 1:23 pm
                 aarshay, thank you for your thoughtful reply. i
                 understand now that moderesult is meant as the output
                 line.
                 however, when executing: mode(data[   gender   ]).mode[0]
                 i receive an error:    tuple    object has no attribute
                    mode   .
                 any help with this is much appreciated! thanks for
                 putting together an excellent tutorial.
                 [163]reply
                    # aarshay jain says:
                      [164]january 5, 2016 at 1:57 pm
                      i just cross-checked and it seems to be working on
                      my system.
                      maybe you can check the version of scipy you are
                      using. i   m using 0.16.0
                      my guess is that if the    tuple    returned by mode has
                      no object    mode    then it would be returning unnamed
                      elements. you can try:
                      mode(data[   gender   ]).[0][0]
                      let me know your scipy version and whether the above
                      code works.
                      cheers!
                      [165]reply
                         @ cowboybobjr says:
                           [166]january 5, 2016 at 5:19 pm
                           my version of of scipy was 0.15.0. trying with
                              mode(data[   gender   ]).[0][0]    did not work.
                           after updating my version to 0.16.0, your
                           original code works. thanks again, aarshay!
                           [167]reply
                              - aarshay jain says:
                                [168]january 6, 2016 at 6:16 am
                                i   m glad upgrading to 0.16.0 worked. not
                                sure why the code in 0.15.0 didn   t work.
                                but i guess the issue is resolved.
                                cheers!
                                [169]reply
     * mudit rastogi says:
       [170]january 5, 2016 at 9:53 am
       while imputing the values in the blank places
       data[   gender   ].fillna(mode(data[   gender   ]).mode[0], inplace=true)
       i am getting error:
       data[   gender   ].fillna(mode(data[   gender   ]).mode[0], inplace=true)
       attributeerror:    tuple    object has no attribute    mode   
       [171]reply
          + aarshay jain says:
            [172]january 6, 2016 at 6:19 am
            hi mudit,
            please refer to the discussion above with    cowboybobjr   . it   s
            probably because you have scipy version 0.15.0
            please upgrade to 0.16.0. let me know if it still gives an
            error.
            cheers!
            [173]reply
          + kabangu says:
            [174]march 13, 2018 at 2:50 am
            hello @mudit rastogi,
            i had the same error as you, but solve it by using astype()
            function.
            try the example below.
            mode(df[   gender   ].astype(str))
            [175]reply
               o sagar says:
                 [176]april 25, 2018 at 9:20 am
                 thanks for suggestion
                 [177]reply
     * p samarkhand says:
       [178]january 5, 2016 at 7:29 pm
       why not use map in #11.
       data[   loan_status_coded   ] = coding(data[   loan_status   ],
       {   n   :0,   y   :1})    >
       data[   loan_status_coded   ] = data[   loan_status   ].map( {   n   :0,   y   :1})
       [179]http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/
       pandas.series.map.html
       [180]reply
          + aarshay jain says:
            [181]january 6, 2016 at 6:30 am
            hello,
            yes using map is another way of doing this. but there is one
            catch which should be kept in mind. map requires all the
            possible values to be entered and would return nan for others.
            for example, suppose:
            x = pd.series([   yes   ,   no   ,   y   ,   no   ,   yes   ])
            now we see that one element    y    has to be re-coded to    yes   
            but i don   t want to change the others.
            case1     using map:
            x.map({   y   :   yes   })
            output: nan nan yes nan nan
            this is because map required all the elements to be passed.
            case2     using replace:
            x.replace(   y   ,   yes   )
            output: yes no yes no yes
            this works with only a single value being passed.
            to summarize, map can be used but we should take special care
            to mention all the unique values even if they are not to be
            re-coded. on the other hand, replace is more generic.
            hope this makes sense.
            cheers!
            [182]reply
     * shravanbm says:
       [183]january 8, 2016 at 4:43 pm
       for 11, coding nominal data
       i found a better way to encode categorical data to numerical using
       from sklearn.preprocessing.labelencoder.
       this encodes the data to numeric and later once can reproduce
       labels back passing the numeric data to decoder.
       eg.
       >>> from sklearn import preprocessing
       >>> le = preprocessing.labelencoder()
       >>> le.fit([1, 2, 2, 6])
       labelencoder()
       >>> le.classes_
       array([1, 2, 6])
       >>> le.transform([1, 1, 2, 6]) # encode
       array([0, 0, 1, 2]   )
       >>> le.inverse_transform([0, 0, 1, 2]) #decode
       array([1, 1, 2, 6])
       [184]http://scikit-learn.org/stable/modules/generated/sklearn.prepr
       ocessing.labelencoder.html
       [185]reply
          + aarshay jain says:
            [186]january 8, 2016 at 6:38 pm
            hi shravan,
            thanks for sharing this information. yes this is definitely
            another way and looks to be shorter.
            please note that this will automatically assign values [0 1 .
            . . . (#classes-1)] and it applies them on sorted categories.
            sometimes, we might want to assign different codes which won   t
            be possible with this. but these cases are rare and your code
            would work in most cases.
            thanks!
            aarshay
            [187]reply
     * sanoj says:
       [188]january 11, 2016 at 9:16 pm
       i have a pandas problem of creating additional columns. can you
       look into it?
       i have a pandas data frame (x11) like this:
       dx1 dx2 dx3 dx4
       0 25041 40391 5856 0
       1 25041 40391 25081 5856
       2 25041 40391 42822 0
       3 25061 40391 0 0
       4 25041 40391 0 5856
       5 40391 25002 5856 3569
       i want to create dummy column(s) for cell values like
       25041,40391,5856 etc. so there will be a column 25041 with value as
       1 or 0 if 25041 occurs in that particular row in any dxs columns. i
       am using this code and it works when number of rows are less. final
       outcome is at bottom.
       mat = x11.as_matrix(columns=none)
       values, counts = np.unique(mat.astype(str), return_counts=true)
       for x in values:
       x11[x] = x11.isin([x]).any(1).astype(int)
       when number of rows are many thousands or in millions, it hangs and
       takes forever and i am not getting any result.
       the output should be like this.
       dx1 dx2 dx3 dx4 0 25002 25041 25061 25081 3569 40391 42822 5856
       25041 40391 5856 0 0 0 1 0 0 0 1 0 1
       25041 40391 25081 5856 0 0 1 0 1 0 1 0 1
       25041 40391 42822 0 0 0 1 0 0 0 1 1 0
       25061 40391 0 0 0 0 0 1 0 0 1 0 0
       25041 40391 0 5856 0 0 1 0 0 0 1 0 1
       40391 25002 5856 3569 0 1 0 0 0 1 1 0 1
       i tried pd.get_dummies(x11[column_name]). but it creates multiple
       dummies for same cell value and last one overwrites the earlier
       occurrence and i loose previous values. any idea?
       [189]reply
          + aarshay jain says:
            [190]january 18, 2016 at 7:53 pm
            hi sanoj,
            you seem to be using a pretty complicated way. i have shared
            my code where i have first replicated your data and then
            performed the necessary steps to get the output:
            import pandas as pd
            import numpy as np
            #preparing data:
            matrix = [[25041, 40391, 5856, 0],
            [25041, 40391, 25081, 5856],
            [25041, 40391, 42822, 0],
            [25061, 40391, 0, 0],
            [25041, 40391, 0, 5856],
            [40391, 25002, 5856, 3569]]
            data = pd.dataframe(matrix, columns = [   dx1   ,    dx2   ,    dx3   ,
               dx4   ])
            #performing action:
            for col in data.columns:
            data[col].astype(np.object,copy=false)
            unq = data[col].unique()
            for val in unq:
            data[val] = data[col].apply(lambda x: 1 if x==val else 0)
            print data
            result:
            dx1 dx2 dx3 dx4 25041 25061 40391 25002 5856 25081 42822 \
            0 25041 40391 5856 0 1 0 1 0 0 0 0
            1 25041 40391 25081 5856 1 0 1 0 1 1 0
            2 25041 40391 42822 0 1 0 1 0 0 0 1
            3 25061 40391 0 0 0 1 1 0 0 0 0
            4 25041 40391 0 5856 1 0 1 0 1 0 0
            5 40391 25002 5856 3569 0 0 0 1 0 0 0
            0 3569
            0 1 0
            1 0 0
            2 1 0
            3 1 0
            4 0 0
            5 0 1
            try running this on your end and let me know if you face
            challenges in understanding the code.
            cheers,
            aarshay
            [191]reply
               o sanoj says:
                 [192]january 25, 2016 at 8:47 pm
                 hello akshay,
                 if you see your provided result carefully then you will
                 find that 40391 does not have a value    1    in 5th row,
                 whereas it is present in 5th row. similarly for 5856, it
                 is missing    1    in 1st row. it seems you are creating
                 unique values per column and if the same value occurs in
                 another column then it over-writes previous values.
                 therefore it does not meet my requirement. i had done
                 similar thing using get_dummies() method.
                 [193]reply
                    # aarshay jain says:
                      [194]january 26, 2016 at 7:32 am
                      hi saroj,
                      yes you   re right. the values are getting overwritten
                      that   s because a unique value is being worked upon
                      more than once. get_dummies() is a cool way of doing
                      it in pandas.
                      just in case you want to do it using a for-loop, i
                      have updated the code:
                      #using same matrix definition as above
                      #first finding unique set of values:
                      unq = set()
                      for col in data.columns:
                      unq.update(data[col])
                      unq.remove(0) #i guess 0 wasn   t required
                      print unq
                      output: set([5856, 25061, 42822, 40391, 25002,
                      25041, 3569, 25081])
                      #looping over unique values:
                      for val in unq:
                      data[val] = data.apply(lambda x: 1 if val in
                      x.iloc[0:4].values else 0,axis=1)
                      print data
                      output:
                      dx1 dx2 dx3 dx4 5856 25061 42822 40391 25002 25041
                      3569 25081
                      0 25041 40391 5856 0 1 0 0 1 0 1 0 0
                      1 25041 40391 25081 5856 1 0 0 1 0 1 0 1
                      2 25041 40391 42822 0 0 0 1 1 0 1 0 0
                      3 25061 40391 0 0 0 1 0 1 0 0 0 0
                      4 25041 40391 0 5856 1 0 0 1 0 1 0 0
                      5 40391 25002 5856 3569 1 0 0 1 1 0 1 0
                      idea is to first get unique set of values and then
                      iterate over them to avoid the issue being faced
                      above.
                      thanks again for reaching out. i could learn a new
                      function    get_dummies()    today, which seems to be
                      really helpful.
                      cheers,
                      aarshay
                      [195]reply
     * harmandeep singh says:
       [196]january 23, 2016 at 2:19 pm
       i have used different approach for binning the loanamount column
       but i am getting few values diffrent can anyone help me why this is
       occuring.
       my approach for binning :
       minimum = data[   loanamount   ].min()
       maximum = data[   loanamount   ].max()
       cut_points = [minimum,90,140,190,maximum]
       labels = [   low   ,   medium   ,   high   ,   veryhigh   ]
       data[   loanamount_bin   ] =
       pd.cut(data[   loanamount   ],cut_points,labels=labels)
       pd.value_counts(data[   loanamount_bin   ],sort=false)
       output:
       low 103
       medium 273
       high 146
       veryhigh 91
       [197]reply
          + aarshay jain says:
            [198]january 23, 2016 at 9:04 pm
            hi harmandeep,
            you should add a parameter:    include_lowest = true    update 1
            line:
            data[   loanamount_bin   ] =
            pd.cut(data[   loanamount   ],cut_points,labels=labels,include_low
            est=true)
            you are just 1 short and this is because the minimum value is
            not being included. hope this answers the query.
            feel free to reach out in case you have more concerns.     
            [199]reply
     * praveen gupta sanka says:
       [200]february 16, 2016 at 5:32 am
       hi aarshay,
       it is really helpful and a great tutorial.
       there is a small correction in #12 point stated.
       if row[   feature   ]==   categorical   :
       data[row[   feature   ]]=data[row[   feature   ]].astype(np.object)
       elif row[   feature   ]==   continuous   :
       data[row[   feature   ]]=data[row[   feature   ]].astype(np.float
       i think the    if    condition should be row[   type   ]. i think the image
       after changing the datatype is also wrong.
       please let me know if my interpretation is wrong.
       [201]reply
          + aarshay jain says:
            [202]february 16, 2016 at 7:55 am
            point well taken.. i   m feeling really happy that people are
            going into such depth.. things like these make av the right
            ecosystem for learning for everyone. please see the updated
            changes.
            thanks!     
            [203]reply
               o praveen gupta sanka says:
                 [204]february 16, 2016 at 2:30 pm
                 thanks a lot. your article was great. i appreciate the
                 quick changes. keep going. i will look forward for some
                 new articles by you.
                 [205]reply
                    # aarshay jain says:
                      [206]february 16, 2016 at 3:07 pm
                      thanks for the appreciation. really means a lot to
                      me.
                      this was my first articles. here is the list of all:
                      [207]http://www.analyticsvidhya.com/blog/author/aars
                      hay/
                      i   ve started writing an article every week so stay
                      tuned!!
                      cheers,
                      aarshay
                      [208]reply
     * [209]freddy says:
       [210]march 1, 2016 at 8:37 am
       thank you for a valuable post. i really learned so much on python
       and pandas by reading it. i needed to learn how to loop through
       pandas dataframe, which i did learn but you put so much more
       valuable stuff. again, thanks,
       regards,
       freddy
       [211]reply
          + aarshay jain says:
            [212]march 1, 2016 at 8:42 am
            i   m glad you liked it. stay tuned for many more of this kind!
            [213]reply
     * t.laurelen says:
       [214]april 4, 2016 at 12:44 pm
       hi aarshay,
       i have practiced your code and data, but i made the mistake! my
       python   s edition is the 3.5.
       #3
       #first we import a function to determine the mode
       from scipy.stats import mode
       mode(data[   gender   ]).mode[0]
       #impute the values:
       data[   gender   ].fillna(mode(data[   gender   ]).mode[0],inplace=true)
       data[   married   ].fillna(mode(data[   married   ]).mode[0],inplace=true)
       data[   self_employed   ].fillna(mode(data[   self_employed   ]).mode[0],in
       place=true)
       #now check the #missing values again to confirm:
       print (data.apply(num_missing,axis=0))
       the output error:
       typeerror: unorderable types: str() > float()
       what should i do?
       thank you very much!
       [215]reply
          + aarshay jain says:
            [216]april 4, 2016 at 3:05 pm
            which line are you getting this error?
            check this post out, you might be having a similar problem    
            [217]http://stackoverflow.com/questions/10779187/what-does-the
            -unorderable-type-error-mean-in-python
            [218]reply
          + emailme says:
            [219]february 16, 2018 at 9:52 am
            mode(x.astype(   str   )).mode[0]
            [220]reply
     * dilip krishna says:
       [221]april 13, 2016 at 10:49 am
       i cannot find the data set in the link that is pointed to. can
       someone please help me?
       [222]reply
          + aarshay jain says:
            [223]april 13, 2016 at 10:54 am
            you need to make an account and register for the practise
            problem. you   ll get the data set then.
            [224]reply
               o rafael del rey says:
                 [225]august 1, 2016 at 2:43 am
                 how can i download the dataset? i have signed up to this
                 site, but it looks they dont keep dataset for closed
                 competitions.
                 [226]reply
     * [227]anant gupta says:
       [228]may 23, 2016 at 12:09 pm
       the boxplot and histogram features were something that i did not
       know. pretty useful for me
       [229]reply
     * charles sutton says:
       [230]june 22, 2016 at 2:43 am
       i figured it out. silly mistake. i logged out of jupyter and then
       when reopened forgot to run code to insert file into    data   
       [231]reply
     * sunkanmi says:
       [232]september 13, 2016 at 2:04 pm
       hi everyone, i am new to python and data science altogether. pardon
       because because my question is not directly related to the post. i
       am writing a program to read and analyze a csv with pandas. the
       problem is that the csv will be supplied by the user and it can
       have variable number of columns depending on the user. i do not
       have a prior knowledge of the column names.
       what i did is to read the csv using pandas and read the colum names
       into a python list. however problem ensued when i attempted to
       access the dataframe column by doing something like this:
       #list of column names, coln
       coln = df.columns
       df.ix[:, df.coln[0]] # to access the first column of the dataframe.
       but this did not work. please help how do i do this?
       [233]reply
     * [234]oli says:
       [235]january 18, 2017 at 8:47 am
       such a helpful tutorial, thank you.
       [236]reply
     * naval katoch says:
       [237]march 28, 2017 at 3:27 pm
       i have read many articles on data science with python (pandas) and
       this is by far the most lucidly explained and to the point article
       i have come across. thanks for the efforts and also the complete
       tutorial to data science from scratch was great!
       [238]reply
          + aishwarya singh says:
            [239]april 5, 2018 at 3:15 pm
            hi naval,
            glad you found it useful!
            [240]reply
     * gupta says:
       [241]april 25, 2017 at 2:15 am
       this article has good info about pandas.
       [242]reply
     * k rupesh rao says:
       [243]december 27, 2017 at 1:51 pm
       it was really helpful. thank you.
       [244]reply
     * gangadhar says:
       [245]january 18, 2018 at 3:54 pm
       hi,
       i am just new to python and very keen on data analytics. could you
       please share the project.
       thanks,
       [246]reply
          + aishwarya singh says:
            [247]april 5, 2018 at 5:05 pm
            hi gangadhar,
            you can download the dataset from this [248]link.
            [249]reply
     * akshay kumar says:
       [250]march 17, 2018 at 12:39 am
       so many functions to keep in mind. how to do that?
       [251]reply
     * pravin mhaske says:
       [252]april 4, 2018 at 6:03 pm
       good article aarshay. few questions    
       1. imputing missing values     you did not show it for dependents,
       term and cr history. i used same pivot table approach for
       dependents based on gender and married. for term, i simply used the
       mode which was 360. but for credit history, not sure what to do.
       tried building a binary logit model to predict the missing credit
       history but there is no variable which could predict the history.
       all p values were pretty high. how do we fill these missing values?
       2. data type for credit history     i believe it should be kept as
       integer as not sure how    n    and    y    for a response variable would
       help. (just thinking in terms of id28 though, not
       sure about other models.)
       [253]reply
          + aishwarya singh says:
            [254]april 5, 2018 at 3:06 pm
            hi pravin,
            like you filled the values with the mode in    term   , you can
            similarly do it for    credit_history   .
            another idea could be, missing values in credit_history mean
            that there is no credit history for the person, and so the
            missing values could be made 0.
            also, you certainly can change the data type for the variable
            to integer for credit_history
            [255]reply
               o pravin mhaske says:
                 [256]april 11, 2018 at 11:47 am
                 thanks aishwarya.
                 for term, mode is fine as the mode (360) comprises
                 whopping 86% of all values and the term doesn   t play any
                 big role in loan status.
                 for term = 360, loan approvals 70% and for term 360, it
                 is 64% which is not significantly different.
                 however for ch, it is a different story as it plays big
                 role in predicting the loan status (80% vs 8% as we
                 know). so not happy to use mode here as it would mean
                 every missing value would be 1 and that would be wrong.
                 making it 0 sounds way better as missing ch is as good as
                 no ch, thinking like a banker     
                 but again, there is a twist here. for blank credit
                 history, the loan approval percentage is pretty high    
                 74% (for ch = 1, it is 80% and for ch = 0, it is 8%) and
                 that   s strange!
                 how do really impute the missing values here?
                 [257]reply
                    # aishwarya singh says:
                      [258]april 13, 2018 at 5:02 pm
                      hi pravin,
                      thank you for the feedback. you   re correct, filling
                      ch values with 1 would not be the best way. maybe
                      you can try finding relation between applicant
                      income and credit history. people with high income
                      usually have credit history as 1 or 0, and then
                      impute the values accordingly.
                      [259]reply
     * phil says:
       [260]april 19, 2018 at 1:46 pm
       this is really helpful. very well laid out and easy to follow.
       thank you very much for posting this.
       [261]reply
          + aishwarya singh says:
            [262]april 20, 2018 at 11:45 am
            hi phil,
            glad you found this useful.
            [263]reply
     * subhendu giri says:
       [264]april 23, 2018 at 2:57 pm
       hi,
       good to have such blog..
       i have some other concern, can anyone guide here.
       having excel sheet, where i need to copy value of one cell to
       another cell alternatively. say, from b3 to c2, b5 to c4 and so on.
       so that, further we can analyze value of b2 and c2, b4 and c4   .
       thanks
       subhendu
       [265]reply
          + aishwarya singh says:
            [266]april 25, 2018 at 4:13 pm
            hi subhendu,
            you can use the shift function in pandas for this task.
            [267]reply
     * amir fuhl says:
       [268]may 2, 2018 at 12:10 am
       i   m new to pandas and data frames, and am facing a task that has me
       stumped.
       my dataframe has 12 columns, but the only one affected here is the
       first column.
       this column contains string values with the following format:
       1.new york
       2.new york
          
       11.new york
       12.new york
       13.california
       14.california
          
       100.california
       101.california
       102.north dakota
       103.north dakota
          
       each value contains a period, and i want to replace the existing
       values with only the respective text components that follow the
       period.
       thus,    1.new york    should become    new york   ,    101.california   
       should become    california   , and so on.
       i thought that i would use the    find    function (ex. find(   .   ) to
       locate the period in each entry, as the placement of the period
       will vary depending on the number that precedes the period.
       i thought that i would incorporate this into a for loop, such as
          for x in df[   location   ]   , and then replace the existing value with
       a    sliced    version of each respective value, beginning at one
       position past the period, and going to the end of the value, but
       i   m having trouble making this work.
       as i   m testing, i can isolate the text that follows the period for
       each value in the column, but i   m unable to set each entry to this
       revised string.
       i feel like i   m very close to what needs to be done, but i can   t
       get there. i   m hoping that someone has an idea that i haven   t
       considered.
       [269]reply
          + aishwarya singh says:
            [270]may 4, 2018 at 1:32 pm
            hi amir,
            you can post your query on the [271]discuss portal so that the
            community can help you.
            [272]reply
     * vitovla says:
       [273]may 2, 2018 at 9:30 pm
       hi there,
       very interesting post, i know that this is a 2016 posts. i had some
       difficulties to find the files. would you please update/send a link
       with the files.
       thanks in advance
       [274]reply
          + aishwarya singh says:
            [275]may 4, 2018 at 12:59 pm
            hi vitovla,
            thank you for pointing it out. [276]here is the link for the
            dataset. i have also updated the same in the article.
            [277]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [278]srk       3924
   2    [2.jpg?date=2019-04-05] [279]mark12    3510
   3    [3.jpg?date=2019-04-05] [280]nilabha   3261
   4    [4.jpg?date=2019-04-05] [281]nitish007 3237
   5    [5.jpg?date=2019-04-05] [282]tezdhar   3082
   [283]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [284]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [285]understanding support vector machine algorithm from examples
       (along with code)
     * [286]essentials of machine learning algorithms (with python and r
       codes)
     * [287]a complete tutorial to learn data science with python from
       scratch
     * [288]7 types of regression techniques you should know!
     * [289]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [290]a simple introduction to anova (with applications in excel)
     * [291]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [292]top 5 machine learning github repositories and reddit discussions
   from march 2019

[293]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [294]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[295]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [296]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[297]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [298]16 opencv functions to start your id161 journey (with
   python code)

[299]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [300][ds-finhack.jpg]

   [301][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [302]about us
     * [303]our team
     * [304]career
     * [305]contact us
     * [306]write for us

   [307]about us
   [308]   
   [309]our team
   [310]   
   [311]careers
   [312]   
   [313]contact us

data scientists

     * [314]blog
     * [315]hackathon
     * [316]discussions
     * [317]apply jobs
     * [318]leaderboard

companies

     * [319]post jobs
     * [320]trainings
     * [321]hiring hackathons
     * [322]advertising
     * [323]reach us

   don't have an account? [324]sign up here.

join our community :

   [325]46336 [326]followers
   [327]20224 [328]followers
   [329]followers
   [330]7513 [331]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [332]privacy policy
     * [333]terms of use
     * [334]refund policy

   don't have an account? [335]sign up here

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [336](button) join now

   subscribe!

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [337](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/machine-learning/
  94. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
  95. https://www.analyticsvidhya.com/blog/category/machine-learning/
  96. https://www.analyticsvidhya.com/blog/category/python-2/pandas/
  97. https://www.analyticsvidhya.com/blog/category/python-2/
  98. https://www.analyticsvidhya.com/blog/author/aarshay/
  99. http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=12pandastechniquesarticle
 100. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
 101. https://www.analyticsvidhya.com/blog/2015/02/data-exploration-preparation-model/?utm_source=blog&utm_medium=12pandastechniquesarticle
 102. http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=12pandastechniquesarticle
 103. https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/?utm_source=blog&utm_medium=12pandastechniquesarticle
 104. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2016/01/12-useful-pandas-techniques-in-python-for-data-manipulation.png
 105. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/1.-boolean-indexing.png
 106. http://pandas.pydata.org/pandas-docs/stable/indexing.html
 107. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/3-apply.png
 108. http://pandas.pydata.org/pandas-docs/stable/generated/pandas.dataframe.apply.html#pandas.dataframe.apply
 109. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/4.2-mode2.png
 110. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/4.3-after-impute.png
 111. http://pandas.pydata.org/pandas-docs/stable/generated/pandas.dataframe.fillna.html#pandas.dataframe.fillna
 112. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/5.-pivot-table.png
 113. http://pandas.pydata.org/pandas-docs/stable/generated/pandas.dataframe.pivot_table.html#pandas.dataframe.pivot_table
 114. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/6.-multi-indexing.png
 115. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/7.1-abs-crosstab.png
 116. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/7.2-perc-crosstab.png
 117. http://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction
 118. http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.crosstab.html
 119. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/8.1-rates.png
 120. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/8.2-pivot.png
 121. http://pandas.pydata.org/pandas-docs/stable/generated/pandas.dataframe.merge.html#pandas.dataframe.merge
 122. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/9.-sort.png
 123. http://pandas.pydata.org/pandas-docs/stable/generated/pandas.dataframe.sort_values.html#pandas.dataframe.sort_values
 124. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/10.1-boxplot.png
 125. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/10.2-hist.png
 126. http://pandas.pydata.org/pandas-docs/stable/generated/pandas.dataframe.hist.html#pandas.dataframe.hist
 127. http://pandas.pydata.org/pandas-docs/stable/generated/pandas.dataframe.boxplot.html#pandas.dataframe.boxplot
 128. http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.cut.html
 129. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/12.-code.png
 130. http://pandas.pydata.org/pandas-docs/stable/generated/pandas.dataframe.replace.html#pandas.dataframe.replace
 131. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/2.1-initial-type.png
 132. https://www.analyticsvidhya.com/wp-content/uploads/2015/12/datatypes.csv
 133. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/2.2-file-content.png
 134. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/2.3-after-changing.png
 135. http://pandas.pydata.org/pandas-docs/stable/generated/pandas.dataframe.iterrows.html#pandas.dataframe.iterrows
 136. https://datahack.analyticsvidhya.com/contest/genpact-machine-learning-hackathon-1/?utm_source=12-pandas-techniques-python-data-manipulation&utm_medium=blog
 137. https://datahack.analyticsvidhya.com/contest/wns-analytics-hackathon-2018-1/?utm_source=12-pandas-techniques-python-data-manipulation&utm_medium=blog
 138. https://datahack.analyticsvidhya.com/contest/12-pandas-techniques-python-data-manipulation&utm_medium=blog
 139. http://discuss.analyticsvidhya.com/
 140. http://feedburner.google.com/fb/a/mailverify?uri=analyticsvidhya
 141. http://twitter.com/analyticsvidhya
 142. http://facebook.com/analyticsvidhya
 143. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 144. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/?share=linkedin
 145. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/?share=facebook
 146. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/?share=twitter
 147. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/?share=pocket
 148. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/?share=reddit
 149. https://www.analyticsvidhya.com/blog/tag/binning/
 150. https://www.analyticsvidhya.com/blog/tag/data-exploration/
 151. https://www.analyticsvidhya.com/blog/tag/data-manipulation/
 152. https://www.analyticsvidhya.com/blog/tag/imputing-missing-values/
 153. https://www.analyticsvidhya.com/blog/tag/pandas/
 154. https://www.analyticsvidhya.com/blog/tag/python/
 155. https://www.analyticsvidhya.com/blog/tag/sorting/
 156. https://www.analyticsvidhya.com/blog/author/aarshay/
 157. https://discuss.analyticsvidhya.com/
 158. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103089
 159. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103089
 160. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103124
 161. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103124
 162. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103149
 163. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103149
 164. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103150
 165. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103150
 166. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103154
 167. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103154
 168. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103190
 169. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103190
 170. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103137
 171. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103137
 172. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103191
 173. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103191
 174. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-151859
 175. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-151859
 176. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152825
 177. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152825
 178. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103157
 179. http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.series.map.html
 180. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103157
 181. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103193
 182. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103193
 183. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103350
 184. http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.labelencoder.html
 185. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103350
 186. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103357
 187. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103357
 188. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103545
 189. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-103545
 190. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-104043
 191. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-104043
 192. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-104600
 193. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-104600
 194. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-104625
 195. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-104625
 196. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-104455
 197. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-104455
 198. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-104477
 199. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-104477
 200. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-105763
 201. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-105763
 202. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-105766
 203. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-105766
 204. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-105785
 205. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-105785
 206. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-105787
 207. http://www.analyticsvidhya.com/blog/author/aarshay/
 208. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-105787
 209. http://programminginpsychology.wordpress.com/
 210. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-106423
 211. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-106423
 212. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-106424
 213. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-106424
 214. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-108927
 215. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-108927
 216. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-108942
 217. http://stackoverflow.com/questions/10779187/what-does-the-unorderable-type-error-mean-in-python
 218. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-108942
 219. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-151406
 220. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-151406
 221. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-109396
 222. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-109396
 223. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-109397
 224. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-109397
 225. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-114315
 226. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-114315
 227. http://simplyanant.blogspot.com/
 228. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-111339
 229. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-111339
 230. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-112515
 231. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-112515
 232. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-116029
 233. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-116029
 234. https://gamesalike.com/
 235. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-121214
 236. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-121214
 237. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-125695
 238. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-125695
 239. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152387
 240. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152387
 241. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-127580
 242. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-127580
 243. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-149665
 244. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-149665
 245. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-150828
 246. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-150828
 247. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152393
 248. https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/
 249. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152393
 250. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-151952
 251. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-151952
 252. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152371
 253. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152371
 254. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152386
 255. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152386
 256. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152505
 257. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152505
 258. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152557
 259. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152557
 260. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152684
 261. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152684
 262. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152709
 263. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152709
 264. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152783
 265. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152783
 266. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152837
 267. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152837
 268. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152972
 269. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152972
 270. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-153032
 271. https://discuss.analyticsvidhya.com/
 272. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-153032
 273. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152997
 274. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-152997
 275. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-153031
 276. https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/
 277. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/#comment-153031
 278. https://datahack.analyticsvidhya.com/user/profile/srk
 279. https://datahack.analyticsvidhya.com/user/profile/mark12
 280. https://datahack.analyticsvidhya.com/user/profile/nilabha
 281. https://datahack.analyticsvidhya.com/user/profile/nitish007
 282. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 283. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 284. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 285. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 286. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 287. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 288. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 289. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 290. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 291. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 292. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 293. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 294. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 295. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 296. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 297. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 298. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 299. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 300. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 301. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 302. http://www.analyticsvidhya.com/about-me/
 303. https://www.analyticsvidhya.com/about-me/team/
 304. https://www.analyticsvidhya.com/career-analytics-vidhya/
 305. https://www.analyticsvidhya.com/contact/
 306. https://www.analyticsvidhya.com/about-me/write/
 307. http://www.analyticsvidhya.com/about-me/
 308. https://www.analyticsvidhya.com/about-me/team/
 309. https://www.analyticsvidhya.com/about-me/team/
 310. https://www.analyticsvidhya.com/about-me/team/
 311. https://www.analyticsvidhya.com/career-analytics-vidhya/
 312. https://www.analyticsvidhya.com/about-me/team/
 313. https://www.analyticsvidhya.com/contact/
 314. https://www.analyticsvidhya.com/blog
 315. https://datahack.analyticsvidhya.com/
 316. https://discuss.analyticsvidhya.com/
 317. https://www.analyticsvidhya.com/jobs/
 318. https://datahack.analyticsvidhya.com/users/
 319. https://www.analyticsvidhya.com/corporate/
 320. https://trainings.analyticsvidhya.com/
 321. https://datahack.analyticsvidhya.com/
 322. https://www.analyticsvidhya.com/contact/
 323. https://www.analyticsvidhya.com/contact/
 324. https://datahack.analyticsvidhya.com/signup/
 325. https://www.facebook.com/analyticsvidhya/
 326. https://www.facebook.com/analyticsvidhya/
 327. https://twitter.com/analyticsvidhya
 328. https://twitter.com/analyticsvidhya
 329. https://plus.google.com/+analyticsvidhya
 330. https://in.linkedin.com/company/analytics-vidhya
 331. https://in.linkedin.com/company/analytics-vidhya
 332. https://www.analyticsvidhya.com/privacy-policy/
 333. https://www.analyticsvidhya.com/terms/
 334. https://www.analyticsvidhya.com/refund-policy/
 335. https://id.analyticsvidhya.com/accounts/signup/
 336. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 337. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 339. https://www.facebook.com/analyticsvidhya
 340. https://twitter.com/analyticsvidhya
 341. https://plus.google.com/+analyticsvidhya/posts
 342. https://in.linkedin.com/company/analytics-vidhya
 343. https://www.analyticsvidhya.com/wp-content/uploads/2016/01/eg12.png
 344. https://datahack.analyticsvidhya.com/contest/genpact-machine-learning-hackathon-1/?utm_source=12-pandas-techniques-python-data-manipulation&utm_medium=blog
 345. https://datahack.analyticsvidhya.com/contest/wns-analytics-hackathon-2018-1/?utm_source=12-pandas-techniques-python-data-manipulation&utm_medium=blog
 346. https://datahack.analyticsvidhya.com/contest/enigma-codefest-machine-learning-1/?utm_source=12-pandas-techniques-python-data-manipulation&utm_medium=blog
 347. https://www.analyticsvidhya.com/blog/2016/01/data-software-engineer-ahmadabad-5-12-years-experience/
 348. https://www.analyticsvidhya.com/blog/2016/01/semantic-web-experts-gurgaon-6-years-experience/
 349. https://www.analyticsvidhya.com/blog/author/aarshay/
 350. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection#43222231302b223a29222a2d03242e222a2f6d202c2e
 351. https://in.linkedin.com/in/aarshayjain
 352. https://github.com/aarshayj
 353. https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/aarshay
 354. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 355. https://www.facebook.com/analyticsvidhya/
 356. https://twitter.com/analyticsvidhya
 357. https://plus.google.com/+analyticsvidhya
 358. https://plus.google.com/+analyticsvidhya
 359. https://in.linkedin.com/company/analytics-vidhya
 360. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 361. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 362. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 363. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 364. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 365. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 366. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 367. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 368. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 369. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 370. javascript:void(0);
 371. javascript:void(0);
 372. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 373. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 374. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 375. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 376. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 377. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 378. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 379. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 380. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 381. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f01%2f12-pandas-techniques-python-data-manipulation%2f&linkname=12%20useful%20pandas%20techniques%20in%20python%20for%20data%20manipulation
 382. javascript:void(0);
 383. javascript:void(0);
