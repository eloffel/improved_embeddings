   #[1]intel ai    feed [2]intel ai    comments feed [3]alternate
   [4]alternate

   search ____________________ search

   [5]intel ai
     * ____________________
     * [6]technologies
          + [7]frameworks
          + [8]hardware
          + [9]software libraries
          + [10]research projects
          + [11]tools
          + [12]featured solutions
     * [13]industries
          + [14]health & life sciences
          + [15]telecommunications
          + [16]retail
          + [17]media & entertainment
          + [18]financial services (fsi)
          + [19]energy
          + [20]government
     * [21]library
          + [22]blog
          + [23]videos
          + [24]white papers
          + [25]podcasts
          + [26]docs
          + [27]news
     * [28]research
          + [29]publications
          + [30]researchers
          + [31]research programs
          + [32]research projects
     * [33]careers
     * programs
          + [34]partners
          + [35]research residency
          + [36]intel ai developer program
          + [37]ai student ambassadors
          + [38]ai4socialgood
     * [39]blog

   ____________________
   show the menu

   [40]blog
   authors
   tambet matiisen

   [41]tambet matiisen

   phd student in university of tartu
   tags

   [42]neon   

   [43]blog
   12.22.15

   12.22.15

guest post (part i): demystifying deep id23

   two years ago, a small company in london called deepmind uploaded their
   pioneering paper    [44]playing atari with deep id23   
   to arxiv. in this paper they demonstrated how a computer learned to
   play atari 2600 video games by observing just the screen pixels and
   receiving a reward when the game score increased. the result was
   remarkable, because the games and the goals in every game were very
   different and designed to be challenging for humans. the same model
   architecture, without any change, was used to learn seven different
   games, and in three of them the algorithm performed even better than a
   human!

   it has been hailed since then as the first step towards [45]general
   artificial intelligence     an ai that can survive in a variety of
   environments, instead of being confined to strict realms such as
   playing chess. no wonder [46]deepmind was immediately bought by google
   and has been on the forefront of deep learning research ever since. in
   february 2015 their paper    [47]human-level control through deep
   id23    was featured on the cover of nature, one of the
   most prestigious journals in science. in this paper they applied the
   same model to 49 different games and achieved superhuman performance in
   half of them.

   still, while deep models for supervised and unsupervised learning have
   seen widespread adoption in the community, deep id23
   has remained a bit of a mystery. in this blog post i will be trying to
   demystify this technique and understand the rationale behind it. the
   intended audience is someone who already has background in machine
   learning and possibly in neural networks, but hasn   t had time to delve
   into id23 yet.

   the roadmap ahead:
    1. what are the main challenges in id23? we will
       cover the credit assignment problem and the
       exploration-exploitation dilemma here.
    2. how to formalize id23 in mathematical terms? we
       will define markov decision process and use it for reasoning about
       id23.
    3. how do we form long-term strategies? we define    discounted future
       reward   , that forms the main basis for the algorithms in the next
       sections.
    4. how can we estimate or approximate the future reward? simple
       table-based id24 algorithm is defined and explained here.
    5. what if our state space is too big? here we see how q-table can be
       replaced with a (deep) neural network.
    6. what do we need to make it actually work? experience replay
       technique will be discussed here, that stabilizes the learning with
       neural networks.
    7. are we done yet? finally we will consider some simple solutions to
       the exploration-exploitation problem.

id23

   consider the game breakout. in this game you control a paddle at the
   bottom of the screen and have to bounce the ball back to clear all the
   bricks in the upper half of the screen. each time you hit a brick, it
   disappears and your score increases     you get a reward.

   id23 simple example ^figure 1: atari breakout game.
   image credit: deepmind.

   suppose you want to teach a neural network to play this game. input to
   your network would be screen images, and output would be three actions:
   left, right or fire (to launch the ball). it would make sense to treat
   it as a classification problem     for each game screen you have to
   decide, whether you should move left, right or press fire. sounds
   straightforward? sure, but then you need training examples, and a lots
   of them. of course you could go and record game sessions using expert
   players, but that   s not really how we learn. we don   t need somebody to
   tell us a million times which move to choose at each screen. we just
   need occasional feedback that we did the right thing and can then
   figure out everything else ourselves.

   this is the task id23 tries to solve. reinforcement
   learning lies somewhere in between supervised and unsupervised
   learning. whereas in supervised learning one has a target label for
   each training example and in unsupervised learning one has no labels at
   all, in id23 one has sparse and time-delayed labels    
   the rewards. based only on those rewards the agent has to learn to
   behave in the environment.

   while the idea is quite intuitive, in practice there are numerous
   challenges. for example when you hit a brick and score a reward in the
   breakout game, it often has nothing to do with the actions (paddle
   movements) you did just before getting the reward. all the hard work
   was already done, when you positioned the paddle correctly and bounced
   the ball back. this is called the credit assignment problem     i.e.,
   which of the preceding actions was responsible for getting the reward
   and to what extent.

   once you have figured out a strategy to collect a certain number of
   rewards, should you stick with it or experiment with something that
   could result in even bigger rewards? in the above breakout game a
   simple strategy is to move to the left edge and wait there. when
   launched, the ball tends to fly left more often than right and you will
   easily score about 10 points before you die. will you be satisfied with
   this or do you want more? this is called the explore-exploit dilemma    
   should you exploit the known working strategy or explore other,
   possibly better strategies.

   id23 is an important model of how we (and all animals
   in general) learn. praise from our parents, grades in school, salary at
   work     these are all examples of rewards. credit assignment problems
   and exploration-exploitation dilemmas come up every day both in
   business and in relationships. that   s why it is important to study this
   problem, and games form a wonderful sandbox for trying out new
   approaches.

markov decision process

   now the question is, how do you formalize a id23
   problem, so that you can reason about it? the most common method is to
   represent it as a markov decision process.

   suppose you are an agent, situated in an environment (e.g. breakout
   game). the environment is in a certain state (e.g. location of the
   paddle, location and direction of the ball, existence of every brick
   and so on). the agent can perform certain actions in the environment
   (e.g. move the paddle to the left or to the right). these actions
   sometimes result in a reward (e.g. increase in score). actions
   transform the environment and lead to a new state, where the agent can
   perform another action, and so on. the rules for how you choose those
   actions are called policy. the environment in general is stochastic,
   which means the next state may be somewhat random (e.g. when you lose a
   ball and launch a new one, it goes towards a random direction).

   id23 an introduction ^figure 2: ^left: reinforcement
   learning problem. right: markov decision process.

   the set of states and actions, together with rules for transitioning
   from one state to another, make up a markov decision process. one
   episode of this process (e.g. one game) forms a finite sequence of
   states, actions and rewards:

   markov decision process example

   here s[i] represents the state, a[i] is the action and r[i+1] is the
   reward after performing the action. the episode ends with terminal
   state s[n] (e.g.    game over    screen). a markov decision process relies
   on the markov assumption, that the id203 of the next state s[i+1]
   depends only on current state s[i] and action a[i], but not on
   preceding states or actions.

discounted future reward

   to perform well in the long-term, we need to take into account not only
   the immediate rewards, but also the future rewards we are going to get.
   how should we go about that?

   given one run of the markov decision process, we can easily calculate
   the total reward for one episode:

   total reward markov decision

   given that, the total future reward from time point t onward can be
   expressed as:

   markov decision model

   but because our environment is stochastic, we can never be sure, if we
   will get the same rewards the next time we perform the same actions.
   the more into the future we go, the more it may diverge. for that
   reason it is common to use discounted future reward instead:

   decision process model

   here    is the discount factor between 0 and 1     the more into the
   future the reward is, the less we take it into consideration. it is
   easy to see, that discounted future reward at time step t can be
   expressed in terms of the same thing at time step t+1:

   process of mdp

   if we set the discount factor   =0, then our strategy will be
   short-sighted and we rely only on the immediate rewards. if we want to
   balance between immediate and future rewards, we should set discount
   factor to something like   =0.9. if our environment is deterministic and
   the same actions always result in same rewards, then we can set
   discount factor   =1.

   a good strategy for an agent would be to always choose an action that
   maximizes the (discounted) future reward.

id24

   in id24 we define a function q(s, a) representing the maximum
   discounted future reward when we perform action a in state s, and
   continue optimally from that point on.

   id24 example

   the way to think about q(s, a) is that it is    the best possible score
   at the end of the game after performing action a in state s   . it is
   called q-function, because it represents the    quality    of a certain
   action in a given state.

   this may sound like quite a puzzling definition. how can we estimate
   the score at the end of game, if we know just the current state and
   action, and not the actions and rewards coming after that? we really
   can   t. but as a theoretical construct we can assume existence of such a
   function. just close your eyes and repeat to yourself five times:    q(s,
   a) exists, q(s, a) exists,       . feel it?

   if you   re still not convinced, then consider what the implications of
   having such a function would be. suppose you are in state and pondering
   whether you should take action a or b. you want to select the action
   that results in the highest score at the end of game. once you have the
   magical q-function, the answer becomes really simple     pick the action
   with the highest q-value!

   id24 algorithm

   here    represents the policy, the rule how we choose an action in each
   state.

   ok, how do we get that q-function then? let   s focus on just one
   transition <s, a, r, s   >. just like with discounted future rewards in
   the previous section, we can express the q-value of state s and action
   a in terms of the q-value of the next state s   .

   bellman equation example this is called the bellman equation. if you
   think about it, it is quite logical     maximum future reward for this
   state and action is the immediate reward plus maximum future reward for
   the next state.

   the main idea in id24 is that we can iteratively approximate the
   q-function using the bellman equation. in the simplest case the
   q-function is implemented as a table, with states as rows and actions
   as columns. the gist of the id24 algorithm is as simple as the
   following[48]^^[1]:

   id24 algorithm example

      in the algorithm is a learning rate that controls how much of the
   difference between previous q-value and newly proposed q-value is taken
   into account. in particular, when   =1, then two q[s,a] cancel and the
   update is exactly the same as the bellman equation.

   the max[a   ] q[s   ,a   ] that we use to update q[s,a] is only an
   approximation and in early stages of learning it may be completely
   wrong. however the approximation get more and more accurate with every
   iteration and [49]it has been shown, that if we perform this update
   enough times, then the q-function will converge and represent the true
   q-value.

deep q network

   the state of the environment in the breakout game can be defined by the
   location of the paddle, location and direction of the ball and the
   presence or absence of each individual brick. this intuitive
   representation however is game specific. could we come up with
   something more universal, that would be suitable for all the games? the
   obvious choice is screen pixels     they implicitly contain all of the
   relevant information about the game situation, except for the speed and
   direction of the ball. two consecutive screens would have these covered
   as well.

   if we apply the same preprocessing to game screens as in the deepmind
   paper     take the four last screen images, resize them to 84  84 and
   convert to grayscale with 256 gray levels     we would have 256^84x84x4    
   10^67970 possible game states. this means 10^67970 rows in our
   imaginary q-table     more than the number of atoms in the known
   universe! one could argue that many pixel combinations (and therefore
   states) never occur     we could possibly represent it as a sparse table
   containing only visited states. even so, most of the states are very
   rarely visited and it would take a lifetime of the universe for the
   q-table to converge. ideally, we would also like to have a good guess
   for q-values for states we have never seen before.

   this is the point where deep learning steps in. neural networks are
   exceptionally good at coming up with good features for highly
   structured data. we could represent our q-function with a neural
   network, that takes the state (four game screens) and action as input
   and outputs the corresponding q-value. alternatively we could take only
   game screens as input and output the q-value for each possible action.
   this approach has the advantage, that if we want to perform a q-value
   update or pick the action with the highest q-value, we only have to do
   one forward pass through the network and have all q-values for all
   actions available immediately.

   deep q network example

   ^figure 3: ^left: naive formulation of deep q-network. right: more
   optimized architecture of deep q-network, used in deepmind paper.

   the network architecture that deepmind used is as follows:

   deep convolutional neural networks

   this is a classical convolutional neural network with three
   convolutional layers, followed by two fully connected layers. people
   familiar with object recognition networks may notice that there are no
   pooling layers. but if you really think about it, pooling layers buy
   you translation invariance     the network becomes insensitive to the
   location of an object in the image. that makes perfectly sense for a
   classification task like id163, but for games the location of the
   ball is crucial in determining the potential reward and we wouldn   t
   want to discard this information!

   input to the network are four 84  84 grayscale game screens. outputs of
   the network are q-values for each possible action (18 in atari).
   q-values can be any real values, which makes it a regression task, that
   can be optimized with simple squared error loss.

   id24 equation

   given a transition < s, a, r, s    >, the q-table update rule in the
   previous algorithm must be replaced with the following:
    1. do a feedforward pass for the current state s to get predicted
       q-values for all actions.
    2. do a feedforward pass for the next state s    and calculate maximum
       overall network outputs max [a    ]q(s   , a   ).
    3. set q-value target for action to r +   max [a    ]q(s   , a   ) (use the
       max calculated in step 2). for all other actions, set the q-value
       target to the same as originally returned from step 1, making the
       error 0 for those outputs.
    4. update the weights using id26.

experience replay

   by now we have an idea how to estimate the future reward in each state
   using id24 and approximate the q-function using a convolutional
   neural network. but it turns out that approximation of q-values using
   non-linear functions is not very stable. there is a whole bag of tricks
   that you have to use to actually make it converge. and it takes a long
   time, almost a week on a single gpu.

   the most important trick is experience replay. during gameplay all the
   experiences < s, a, r, s    > are stored in a replay memory. when
   training the network, random minibatches from the replay memory are
   used instead of the most recent transition. this breaks the similarity
   of subsequent training samples, which otherwise might drive the network
   into a local minimum. also experience replay makes the training task
   more similar to usual supervised learning, which simplifies debugging
   and testing the algorithm. one could actually collect all those
   experiences from human gameplay and then train network on these.

exploration-exploitation

   id24 attempts to solve the credit assignment problem     it
   propagates rewards back in time, until it reaches the crucial decision
   point which was the actual cause for the obtained reward. but we
   haven   t touched the exploration-exploitation dilemma yet   

   firstly observe, that when a q-table or q-network is initialized
   randomly, then its predictions are initially random as well. if we pick
   an action with the highest q-value, the action will be random and the
   agent performs crude    exploration   . as a q-function converges, it
   returns more consistent q-values and the amount of exploration
   decreases. so one could say, that id24 incorporates the
   exploration as part of the algorithm. but this exploration is    greedy   ,
   it settles with the first effective strategy it finds.

   a simple and effective fix for the above problem is   -greedy
   exploration     with id203    choose a random action, otherwise go
   with the    greedy    action with the highest q-value. in their system
   deepmind actually decreases    over time from 1 to 0.1     in the
   beginning the system makes completely random moves to explore the state
   space maximally, and then it settles down to a fixed exploration rate.

deep id24 algorithm

   this gives us the final deep id24 algorithm with experience
   replay:

   deep id24 algorithm

   there are many more tricks that deepmind used to actually make it work
       like target network, error clipping, reward clipping etc, but these
   are out of scope for this introduction.

   the most amazing part of this algorithm is that it learns anything at
   all. just think about it     because our q-function is initialized
   randomly, it initially outputs complete garbage. and we are using this
   garbage (the maximum q-value of the next state) as targets for the
   network, only occasionally folding in a tiny reward. that sounds
   insane, how could it learn anything meaningful at all? the fact is,
   that it does.

final notes

   many improvements to deep id24 have been proposed since its first
   introduction     [50]double id24, [51]prioritized experience
   replay, [52]dueling network architecture and [53]extension to
   continuous action space to name a few. for latest advancements check
   out the [54]nips 2015 deep id23 workshop and [55]iclr
   2016 (search for    reinforcement    in title). but beware, that [56]deep
   id24 has been patented by google.

   it is often said, that artificial intelligence is something we haven   t
   figured out yet. once we know how it works, it doesn   t seem intelligent
   any more. but deep q-networks still continue to amaze me. watching them
   figure out a new game is like observing an animal in the wild     a
   rewarding experience by itself.

credits

   thanks to ardi tampuu, tanel p  rnamaa, jaan aru, ilya kuzovkin, arjun
   bansal and urs k  ster for comments and suggestions on the drafts of
   this post.

links

     * [57]david silver   s lecture about deep id23
     * [58]slightly awkward but accessible illustration of id24
     * [59]uc berkley   s course on deep id23
     * [60]david silver   s id23 course
     * [61]nando de freitas    course on machine learning (two lectures
       about id23 in the end)
     * [62]andrej karpathy   s course on convolutional neural networks

   [63]^^[1] algorithm adapted from
   [64]http://artint.info/html/artint_265.html
   [this blog was first published
   at: [65]http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
   ]

this is the part 1 of my series on deep id23. tune in next
week for [66]   deep id23 with neon    for an actual
implementation with [67]neon deep learning toolkit.

   authors
   tambet matiisen

   [68]tambet matiisen

   phd student in university of tartu
   tags

   [69]neon   

   stay connected
   keep tabs on all the latest news with our monthly newsletter.
   (button) subscribe
     * [70]@intelai
     * [71]@intelaidev
     * [72]@intelai
     * [73]intel-ai
     * [74]nervanasystems

   (button) x

stay connected with intel   ai

   first name * ____________________
   last name * ____________________
   business email address * ____________________
   country/region * [please select...____________________________]
   job title * ____________________
   company * ____________________
   profession * [please select..._____________________________]
   [x] yes, i would like to subscribe to the intel ai newsletter to stay
   connected to the latest intel technologies and industry trends by
   email. i can unsubscribe at any time. *
   (button) submit

   by submitting this form, you are confirming you are an adult 18 years
   or older and you agree to share your personal information with intel to
   use for this business request. you also agree to subscribe to stay
   connected to the latest intel technologies and industry trends by
   email. you may unsubscribe at any time. intel   s web sites and
   communications are subject to our [75]privacy notice and [76]terms of
   use.

   thank you!

   sorry, there was an error in your submission.
     *   intel corporation
     * [77]terms of use
     * [78]*trademarks
     * [79]privacy
     * [80]cookies
     * [81]supply chain transparency
     * [82]site map

references

   visible links
   1. https://www.intel.ai/feed/
   2. https://www.intel.ai/comments/feed/
   3. https://www.intel.ai/wp-json/oembed/1.0/embed?url=https://www.intel.ai/demystifying-deep-reinforcement-learning/
   4. https://www.intel.ai/wp-json/oembed/1.0/embed?url=https://www.intel.ai/demystifying-deep-reinforcement-learning/&format=xml
   5. https://www.intel.ai/
   6. https://www.intel.ai/technology/
   7. https://www.intel.ai/framework-optimizations/
   8. https://www.intel.ai/hardware/
   9. https://www.intel.ai/software-libraries/
  10. https://www.intel.ai/research-projects/
  11. https://www.intel.ai/tools/
  12. https://www.intel.ai/featured-solutions/
  13. https://www.intel.ai/industries/
  14. https://www.intel.ai/health-and-life-sciences/
  15. https://www.intel.ai/telecommunications/
  16. https://www.intel.ai/retail/
  17. https://www.intel.ai/media-and-entertainment/
  18. https://www.intel.ai/financial-services/
  19. https://www.intel.ai/energy/
  20. https://www.intel.ai/government/
  21. https://www.intel.ai/library/
  22. https://www.intel.ai/blog/
  23. https://www.intel.ai/videos/
  24. https://www.intel.ai/white-papers/
  25. https://www.intel.ai/podcasts/
  26. https://www.intel.ai/docs/
  27. https://www.intel.ai/news/
  28. https://www.intel.ai/research/
  29. https://www.intel.ai/publications/
  30. https://www.intel.ai/researchers/
  31. https://www.intel.ai/research-programs/
  32. https://www.intel.ai/research-projects/
  33. https://jobs.intel.com/page/show/artificial-intelligence-careers
  34. https://www.intel.ai/partners/
  35. https://www.intel.ai/research-programs/
  36. https://software.intel.com/en-us/ai-academy
  37. https://software.intel.com/en-us/ai-academy/ambassadors
  38. https://ai.intel.com/ai/ai4socialgood/
  39. https://www.intel.ai/blog/
  40. https://www.intel.ai/blog/
  41. https://www.intel.ai/bio/tambet-matiisen/
  42. https://www.intel.ai/library/?post_tag=neon
  43. https://www.intel.ai/blog/
  44. http://arxiv.org/abs/1312.5602
  45. https://en.wikipedia.org/wiki/artificial_general_intelligence
  46. http://techcrunch.com/2014/01/26/google-deepmind/
  47. http://www.nature.com/articles/nature14236
  48. https://www.intel.ai/demystifying-deep-reinforcement-learning/#_ftn1
  49. http://simplecore-dev.intel.com/nervana/wp-content/uploads/sites/55/2015/12/proofqlearning.pdf
  50. http://arxiv.org/abs/1509.06461
  51. http://arxiv.org/abs/1511.05952
  52. http://arxiv.org/abs/1511.06581
  53. http://arxiv.org/abs/1509.02971
  54. http://rll.berkeley.edu/deeprlworkshop/
  55. https://cmt.research.microsoft.com/iclr2016conference/protected/publiccomment.aspx
  56. http://www.google.com/patents/us20150100530
  57. http://videolectures.net/rldm2015_silver_reinforcement_learning/
  58. https://www.youtube.com/watch?v=b1a53he0yqs
  59. http://rll.berkeley.edu/deeprlcourse/
  60. http://www0.cs.ucl.ac.uk/staff/d.silver/web/teaching.html
  61. https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/
  62. http://cs231n.github.io/
  63. https://www.intel.ai/demystifying-deep-reinforcement-learning/#_ftnref1
  64. http://artint.info/html/artint_265.html
  65. http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
  66. https://www.intel.ai/deep-reinforcement-learning-with-neon/
  67. https://github.com/nervanasystems/neon
  68. https://www.intel.ai/bio/tambet-matiisen/
  69. https://www.intel.ai/library/?post_tag=neon
  70. https://twitter.com/intelai
  71. https://twitter.com/intelaidev
  72. https://www.facebook.com/intelai/
  73. https://www.linkedin.com/company/3628074/
  74. https://github.com/nervanasystems
  75. https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html?elqtrackid=e6a9dfea0f904f66aa7346e8f8e74476&elq=00000000000000000000000000000000&elqaid=21534&elqat=2&elqcampaignid=
  76. https://www.intel.com/content/www/us/en/legal/terms-of-use.html?elqtrackid=8445f4bedfeb45dcb04ff6097e1001e8&elq=00000000000000000000000000000000&elqaid=21534&elqat=2&elqcampaignid=
  77. https://www.intel.com/content/www/us/en/legal/terms-of-use.html
  78. https://www.intel.com/content/www/us/en/legal/trademarks.html
  79. https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html
  80. https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html
  81. https://www.intel.com/content/www/us/en/policy/policy-human-trafficking-and-slavery.html
  82. https://www.intel.com/content/www/us/en/siteindex.html

   hidden links:
  84. https://www.intel.com/content/www/us/en/homepage.html
  85. https://www.intel.ai/demystifying-deep-reinforcement-learning/#menus
  86. https://www.intel.ai/demystifying-deep-reinforcement-learning/#top
