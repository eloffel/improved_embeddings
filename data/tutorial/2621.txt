   #[1]quora

   [2]quora
   ____________________

   sign in
   [3]why does lda work?
   [4]james mcinerney
   [5]james mcinerney, machine learning researcher at spotify
   [6]updated 177w ago    upvoted by
   [7]naran bayanbat, mscs with focus in machine learning
   most descriptions of id44 (lda) deal with the
   "what" question. they appeal to the generative process (see below), the
   id136 procedure, or exchangeability. these considerations are
   important but they fail to explain why lda is so effective across many
   domains. once you're familiar with lda, the question of why it works is
   fascinating.
   to review, lda is a probabilistic model that posits a set of global
   topics (i.e., a set of discrete distributions over words) and a set of
   document topics (i.e., a set of discrete distributions over topics, one
   distribution per document). to generate a document, repeatedly draw
   from the local topics a topic assignment for the n-th word, then draw
   the word by looking up the corresponding global topic.
   these discrete distributions have an important property called sparsity
   due to a mathematical detail. the detail [which you can skip if you
   like] is that lda uses the dirichlet distribution as a prior on its
   discrete distributions, and,
   1) the sufficient statistics of the dirichlet are [math] [\log p_1,
   \dots, \log p_k] [/math];
   2) [math] p_1 + \dots + p_k [/math] must sum to 1 because it is a
   discrete id203 distribution; and
   3) [math] \log [/math] is a concave function.
   these three facts imply that the dirichlet penalizes the
   ''switching-on'' of components (e.g., topics, words) when you use
   symmetric hyperparameters. intuitively, picture what would happen if
   you added some id203 mass to component [math]k[/math], switching
   it on (see diagram below). because [math]p[/math] has to normalize,
   this requires taking id203 mass away from all the other
   components [math] j \neq k [/math]. the sum of [math]p[/math] is still
   1, but [math] \sum_i \log p_i [/math] has gone down because
   [math]\log[/math] is a concave function. this makes the joint log
   likelihood of the model go down, all other things being equal, meaning
   that the model penalizes non-sparse [math]p[/math]'s. (interestingly,
   this line of reasoning does not require hyperparameters < 1 which is
   usually referred to as a sparse dirichlet).
   [main-qimg-971f4a81ccce95b477da846321b85466]
   sparsity is cool because it encourages most of the latent variables of
   a model to be zero (or close to zero) and this is useful because it
   encodes the belief that there are only a few (i.e., not a hundred)
   hidden explanations for the data you are modelling. these explanations
   might be different depending on the document you are looking at because
   lda is a hierarchical model.
   but lda is not just a hierarchical sparse model, it is intriguingly
   doubly sparse. it trades off sparsity in the document topics with the
   global topics. why is there a trade-off? because the document topic
   distribution ''prefers'' fewer topics per document while the global
   topic distributions ''prefers'' fewer words per topic, and these goals
   are at odds.
   the sparsity goals are at odds with each other because if the sparse
   property of the document topics had its way, each document would only
   have one topic, not several. but this means that to explain all the
   words in the document, the topic distribution over words would have to
   mirror that of the whole document (i.e., it would be the empirical
   distribution over words for the document, subject to its hyperparameter
   being 1), which is not a sparse outcome for the global topics.
   similarly, if the sparse property of the global topics had its way,
   each topic would only have one word, meaning that every document would
   have to exhibit a large number of topics to explain the words in the
   document.
   [main-qimg-bca17faa408f934e6d266e41e9f4fb75]
   lda finds a balance between these two extremes, and the reason why it
   is so useful is because this balance is determined by structure in the
   data. furthermore, the reason why it is an advance over other models
   that also use competing sparsity (e.g., plsi) is because the balance is
   resolved at the level of documents while plsi must resolve the balance
   corpus-wide.
   in summary, lda works well because it expresses competing sparsity
   between the topic distribution of documents and the word distribution
   of topics.
   8.6k views    [8]view 40 upvoters    [9]view sharers
   view 1 other answer to this question

about the author

   [10]james mcinerney

[11]james mcinerney

   machine learning researcher at spotify
   research scientist at spotify2016-present
   studied at university of oxford
   133.9k answer views2.1k this month

   [12]about    [13]careers    [14]privacy    [15]terms    [16]contact

references

   visible links
   1. https://www.quora.com/opensearch/description.xml
   2. https://www.quora.com/
   3. https://www.quora.com/why-does-lda-work
   4. https://www.quora.com/profile/james-mcinerney-1
   5. https://www.quora.com/profile/james-mcinerney-1
   6. https://www.quora.com/why-does-lda-work/answer/james-mcinerney-1
   7. https://www.quora.com/profile/naran-bayanbat
   8. https://www.quora.com/why-does-lda-work/answer/james-mcinerney-1
   9. https://www.quora.com/why-does-lda-work/answer/james-mcinerney-1
  10. https://www.quora.com/profile/james-mcinerney-1
  11. https://www.quora.com/profile/james-mcinerney-1
  12. https://www.quora.com/about
  13. https://www.quora.com/careers
  14. https://www.quora.com/about/privacy
  15. https://www.quora.com/about/tos
  16. https://www.quora.com/contact

   hidden links:
  18. https://www.quora.com/why-does-lda-work
