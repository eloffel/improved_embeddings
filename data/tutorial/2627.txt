chat disentanglement

   contents
     * [1]introduction
     * [2]dataset
          + [3]ethics
          + [4]data files
          + [5]format
     * [6]preprocess and annotation
     * [7]scoring and analysis
     * [8]training and testing
     * [9]references

[10]introduction

   this is the dataset and code described in [11][elsnercharniak08] . it
   is designed to analyze irc chat conversations, with the aim of
   disentangling multiple simultaneous conversations.
   contact: micha elsner ([12]melsner@cs.brown.edu)

   some of the code in this package is written by other people.
   other authors:

   grouper.py (python cookbook)

   aima utilities (support code for ai: a modern approach;
   [13]http://aima.cs.berkeley.edu/python/readme.html)

   waterworks utility package (david mcclosky;
   [14]http://pypi.python.org/pypi/waterworks/0.2.5); the parts you need
   are included in the download

   probably/clustermetrics utilities (david mcclosky; the parts you need
   are included in the download)

   pyung.py hungarian algorithm implementation (federico tomassini)
   to make the code work, you will also need to download some programs:


   [15][megam], a maximum-id178 classifier by hal daume iii

   pylab plotting library, which in turn needs [16][matplotlib] (needed
   only for analysis programs which make diagrams)

   add the utils directory, which contains some general-purpose libraries,
   to your pythonpath environment variable.

[17]dataset

   the transcript is a recording from irc, in late may 2007. it comes from
   the ##linux channel at [18]http://freenode.net. you can see a usage
   profile of this channel at
   [19]http://irc.netsplit.de/channels/details.php?room=%23%23linux&net=fr
   eenode.

[20]ethics

   the brown irb has determined that our study "does not involve the use
   of human subjects as defined by the code of federal regulations", since
   it is not "about" the individuals in the chat room (presumably because
   it does not use "private identifiable data").

   however, we did feel it was appropriate to anonymize the chat as much
   as possible, so we assigned each commentator an alias. these aliases
   are randomly selected from a us census name list (included as
   data/names). name substitution is not perfect; the system can't detect
   misspelled or abbreviated names.

[21]data files

   the dataset is contained in irc. there are three annotated sections,
   irc/test, irc/pilot and irc/dev. in addition, there is the whole
   dataset, irc/linux, and all the unannotated data, irc/rest.

   the data is divided as follows:

     1-500: pilot (markable lines 359: 0:58:10)

     501-1501: dev (markable lines 706: 2:6:24)

     100 line gap, since the same annotators may mark both dev and test
     and we don't want them to make test decisions based on remembered
     dev data

     1601-2601: test (markable lines 800: 1:39:37)

     100 line gap

     rest

   currently we have four pilot annotations:

     pilot-0x (experimenter)

     1-2 (subjects)

     3x (subject, unfamiliar with linux os)

   one dev annotation:

     dev-0x (experimenter)

   7 test annotations:

     0-5 (subjects)

     6x (subject, misunderstood instructions)

   for linux-test-5.annot, a bug in the annotation software allowed the
   annotation -1 (sys_thread) for real lines. the lines which were
   mislabeled were replaced with sequential annotations t70 through t85. i
   have listed these here (by line number).
6 t-1 15423 chauncey: the human touch ?
278 t-1 17586 felicia: her*
409 t-1 18196 inez: call later in the afternoon, since i sleep in late~
495 t-1 18646 lai: alisa: heh
646 t-1 19400 lai: thanks
657 t-1 19452 lai: felicia:
665 t-1 19526 nicki: why do you say "alleged" non-payment? did you pay or didn't
 you? :)
666 t-1 19528 felicia: lai: pm me the address .. atleast one of us will have it
687 t-1 19693 felicia: hehe .. figures
721 t-1 20016 arlie: congratulations
791 t-1 20388 madison: tampa
795 t-1 20407 alisa: arlie: there are some nice coal fires in china too....maybe
 there.
857 t-1 20627 mellisa: awww
908 t-1 20905 santo: yay for romance languages
909 t-1 20909 madison: lol
911 t-1 20912 madison: you like ?

[22]format

   for privacy and convenience, the chats have been edited so that each
   line conforms to the following format:
time name (: comment)|(* action)

   times are given in seconds after the start of the transcript.

   the name is the name of the speaker.

   a comment is something someone "says"; an action is either a
   system-generated message ("matilda * entered the room") or a person's
   action, shown in the third-person ("matilda * slaps morris with a
   trout").

   annotated lines have an additional field showing the thread:
t(thread) time name (: comment)|(* action)

   t-1 is the "system" thread, reserved for system messages. we recognize
   four of these:
entered the room
left the room
set mode
is now known as

   the namechange action ("foo is now known as bar") is rendered
   meaningless by the aliasing system, since the old and new nicknames are
   given the same alias ("matilda * is now known as matilda").

[23]preprocess and annotation

   we recorded our data using the gaim chat client. (gaim, now called
   pidgin, is available from [24]http://www.pidgin.im/) you can activate
   logging from the tools/preferences menu item; click the logging tab,
   select "plain text" logs and click "log all chats".

   the raw data can be formatted using the preprocess/stripchat.py script,
   which takes the name of the data file as an argument and writes the
   formatted version to standard out.

   the annotation software is written in java. although we are providing
   the code, it's honestly pretty terrible-- editing it may be more
   trouble than it's worth.

   running the code is easy:
java chatview [filename]

   the intro.txt file in the same directory contains a quick tutorial on
   using the annotator, which we gave to all our experimental subjects.
   load it into the viewer and follow the instructions.

[25]scoring and analysis

   the main routines for analysis of a single annotation are in
   analysis/chatstats.py. you can also run this file as a program, using
   an annotated file as an argument, to get a variety of information about
   the annotation. it will print out some statistics related to the
   annotation, then run some baselines and print statistics about them.

   the output looks like this:
annotation:
the annotated part of this transcript has 500 lines.
non-system lines: 359 .
36 unique threads.
the average conversation length is 9.97222222222 .
the median conversation length is 3 .
the id178 is 4.0024467585 bits.
the median chat has 1.0 interruptions per line.
the average block of 10 contains 2.94555873926 threads.
the line-averaged conversation density is  2.28969359331 .

   at the bottom it prints agreement metrics between the baselines and the
   annotation.
evaluating all1 against human vi: 4.0024467585 mi:
-8.881784197e-16 1-1-g: 0.197771587744 1-1-o: 0.197771587744 m-1:
0.197771587744 loc-1: 0.555865921788 loc-2: 0.551820728291 loc-3:
0.544943820225

   vi is variation of information [26][meila99]. mi is mutual information.
   1-1-g is one-to-one overlap, computed by a greedy algorithm, 1-1-o is
   one-to-one overlap computed optimally (with the hungarian algorithm).
   m-1 is many-to-one (not by id178 as in the paper, just treats the
   human annotation as target), and the loc-n are local error, as in the
   paper.

   there are a few small scripts that get other information: describe.py
   gets min/max/mean for some statistics over a collection of annotations.
   printdeltat.py prints all the time gaps (sorted) and printtimes.py
   prints total duration of a transcript. speakerstats.py prints some
   information on how many conversations speakers participate in, then
   scatter-plots conversations participated in versus utterances spoken.

   it's fairly easy to write scripts like this, and the code shouldn't be
   too awful.

   the distmat.py script is the main tool for comparison of different
   annotations. to use this tool, you must supply the --metric flag,
   followed by "121", "m21", "loc3" or "vi", as the first argument. (m21
   here does determine source/target by id178.) then you must supply a
   ".annot" file as the second argument. after this, you can supply any
   number of ".annot" files, plus "speaker", "alldiff", "all1", "b[#n]"
   (blocks of n) or "s[#n]" (pause of n) for baseline annotations of the
   dataset.
% python analysis/distmat.py --metric loc3 irc/pilot/*.annot speaker

   the output is a symmetric matrix of the metric values. min/max/average
   or other statistics can be extracted with matlab or some other tool,
   and you can also use it for plotting, mds and so forth.

   finally, you can search for optimal values for block and pause
   baselines using bestbaseline.py. here you must supply --metric,
   --score, which must be "max", "min" or "avg", and --annotator, which
   must be "blocks" or "pause", followed by a list of .annot files.

[27]training and testing

   to test the model, you will first need the unigram statistics and
   technical word list. we have included ours (data/linux-unigrams.dump
   and data/techwords.dump) so you don't have to make them. if you would
   like to do so anyway, you can make the unigram stats file by running
   unigramstats.py:
% python model/unigramstats.py [transcript] [output file]

   you can make the techwords file by running "cachelinuxwords.py":
% python model/cachelinuxwords.py [directory with linux text] [directory with pe
nn treebank parses] [output file]

   this program reads its non-linux text from id32 parse files,
   and requires the "inputtree" python interface to the charniak parser's
   tree-reader, which can be obtained from cs.brown.edu/~dmcc. however, it
   should be trivial to change this if you want.

   now you can run the test. this takes place in two stages: to do feature
   extraction, train the classifier and run it, use classifiertest.py:
% python model/classifiertest.py [training file] [test file] [unigram stats] [te
ch words list] [working dir]

   for instance you can use:
% python model/classifiertest.py irc/dev/linux-dev-0x.annot irc/pilot/linux-pilo
t-0x.annot data/linux-unigrams.dump data/techwords.dump scratch

   currently you can only train or test on one annotation at a time.
   (there is no automatic way to find the average classification error
   over multiple annotations either-- sorry!)

   the program will create a directory scratch/129 (129 is the maximum
   number of seconds between utterances which the classifier will try to
   link.). in this directory are files keys, feats, model and predictions.
   keys contains the indices i j of the two utterances corresponding to
   each classification instance. for the other file formats, see the
   documentation for [28][megam].

   to evaluate the classifier, use model/classifierprecrec.py [model dir]
   (model dir is a directory created in the previous step, eg
   scratch/129). this prints precision, recall and balanced f-score of the
   same conversation class.

   finally, a new annotation can be created using the greedy cut procedure
   described in the paper. to do this, use model/greedy.py:
% python model/greedy.py [transcript] [predictions] [keys]

   the new transcript (along with some other information which you will
   probably need to remove by hand) is printed to standard out. to
   evaluate this transcript, see the previous section.

   we have included the system annotation of the test set, as
   model/system.annot. since the algorithm is deterministic, you should be
   able to reproduce this transcript using the instructions here.

[29]references

   [30][elsnercharniak08] micha elsner and eugene charniak. "you talking
   to me? a corpus and algorithm for conversation disentanglement". acl
   '08.
   [31][meila99] marina meila. "comparing id91s". uw statistics
   technical reports, colt '03.
   [32]http://www.stat.washington.edu/mmp/www.stat.washington.edu/mmp/pape
   rs/compare-colt.pdf
   [megam] ([33]1, [34]2) hal daume iii. paper at
   [35]http://pub.hal3.name#daume04id18-bfgs.pdf, program at
   [36]http://hal3.name/megam
   [37][matplotlib] [38]http://matplotlib.sourceforge.net/

references

   1. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#introduction
   2. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#dataset
   3. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#ethics
   4. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#data-files
   5. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#format
   6. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#preprocess-and-annotation
   7. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#scoring-and-analysis
   8. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#training-and-testing
   9. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#references
  10. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  11. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#elsnercharniak08
  12. mailto:melsner@cs.brown.edu
  13. http://aima.cs.berkeley.edu/python/readme.html
  14. http://pypi.python.org/pypi/waterworks/0.2.5
  15. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#megam
  16. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#matplotlib
  17. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  18. http://freenode.net/
  19. http://irc.netsplit.de/channels/details.php?room=##linux&net=freenode
  20. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  21. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  22. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  23. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  24. http://www.pidgin.im/
  25. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  26. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#meila99
  27. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  28. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#megam
  29. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  30. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html#id1
  31. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  32. http://www.stat.washington.edu/mmp/www.stat.washington.edu/mmp/papers/compare-colt.pdf
  33. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  34. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  35. http://pub.hal3.name/#daume04id18-bfgs.pdf
  36. http://hal3.name/megam
  37. https://www.asc.ohio-state.edu/elsner.14/resources/chat-manual.html# 
  38. http://matplotlib.sourceforge.net/
