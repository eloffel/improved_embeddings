    #[1]nvidia developer blog    feed [2]nvidia developer blog    comments
   feed [3]nvidia developer blog    deep learning in a nutshell: core
   concepts comments feed [4]alternate [5]alternate

   [6]nvidia accelerated computing [7]developer
     * ____________________
       [ ] search nvidia developer
     * [8]join
     * [9]login

   ____________________
   [ ] search nvidia developer

[10]nvidia developer blog

main menu

   [11]skip to primary content
   [12]skip to secondary content
   [13]developer news
   [14]subscribe
   [15]follow us
   [16]nvidiaaidev
   [17]nvidiahpcdev
   [18]nvidiagamedev
   [19]nvidiaembedded
   [20]nvidiadrive
   [21]nvidiadesign

   (button) toggle navigation

   topics
     * [22]accelerated computing
     * [23]artificial intelligence
     * [24]autonomous vehicles
     * [25]design & visualization
     * [26]features
     * [27]game development
     * [28]robotics
     * [29]smart cities
     * [30]virtual reality

   [31]accelerated computing
   [32]artificial intelligence
   [33]autonomous vehicles
   [34]design & visualization
   [35]features
   [36]game development
   [37]robotics
   [38]smart cities
   [39]virtual reality

   [40]artificial intelligence


deep learning in a nutshell: core concepts

   by [41]tim dettmers | [42]november 3, 2015
   tags: [43]cudnn, [44]deep learning, [45]deep neural networks,
   [46]machine learning, [47]neural networks

   dl_dog_340x340 this post is the first in a series i   ll be writing for
   parallel forall that aims to provide an intuitive and gentle
   introduction to[48] deep learning. it covers the most important deep
   learning concepts and aims to provide an understanding of each concept
   rather than its mathematical and theoretical details. while the
   mathematical terminology is sometimes necessary and can further
   understanding, these posts use analogies and images whenever possible
   to provide easily digestible bits comprising an intuitive overview of
   the field of deep learning.

   i wrote this series in a glossary style so it can also be used as a
   reference for deep learning concepts.

   part 1 focuses on introducing the main concepts of deep learning.
   [49]part 2 provides historical background and delves into the training
   procedures, algorithms and practical tricks that are used in training
   for deep learning. [50]part 3 covers sequence learning, including
   recurrent neural networks, lstms, and encoder-decoder systems for
   id4. [51]part 4 covers id23.

core concepts

machine learning

   in machine learning we (1) take some data, (2) train a model on that
   data, and (3) use the trained model to make predictions on new data.
   the process of [52]training a model can be seen as a learning process
   where the model is exposed to new, unfamiliar data step by step. at
   each step, the model makes predictions and gets feedback about how
   accurate its generated predictions were. this feedback, which is
   provided in terms of an error according to some measure (for example
   distance from the correct solution), is used to correct the errors made
   in prediction.

   the learning process is often a game of back-and-forth in the parameter
   space: if you tweak a parameter of the model to get a prediction right,
   the model may have in such that it gets a previously correct prediction
   wrong. it may take many iterations to train a model with good
   predictive performance. this iterative predict-and-adjust process
   continues until the predictions of the model no longer improve.

feature engineering

   feature engineering is the art of extracting useful patterns from data
   that will make it easier for [53]machine learning models to distinguish
   between classes. for example, you might take the number of greenish vs.
   bluish pixels as an indicator of whether a land or water animal is in
   some picture. this feature is helpful for a machine learning model
   because it limits the number of classes that need to be considered for
   a good classification.

   feature engineering is the most important skill when you want to
   achieve good results for most predictions tasks. however, it is
   difficult to learn and master since different data sets and different
   kinds of data require different feature engineering approaches. only
   crude guidelines exist, which makes feature engineering more of an art
   than a science. features that are usable for one data set often are not
   usable for other data sets (for example the next image data set only
   contains land animals). the difficulty of feature engineering and the
   effort involved is the main reason to seek algorithms that can learn
   features; that is, algorithms that automatically engineer features.

   while many tasks can be automated by id171 (like object and
   id103), feature engineering remains [54]the single most
   effective technique to do well in difficult tasks (like most tasks in
   kaggle machine learning competitions).

id171

   id171 algorithms find the common patterns that are important
   to distinguish between classes and extract them automatically to be
   used in a classification or regression process. id171 can be
   thought of as [55]feature engineering done automatically by algorithms.
   in deep learning, convolutional layers are exceptionally good at
   finding good features in images to the next layer to form a hierarchy
   of nonlinear features that grow in complexity (e.g. blobs, edges ->
   noses, eyes, cheeks -> faces). the final layer(s) use all these
   generated features for classification or regression (the last layer in
   a convolutional net is, essentially, multinomial [56]logistic
   regression).

   hierarchical features

   figure 1: learned hierarchical features from a deep learning algorithm.
   each feature can be thought of as a filter, which filters the input
   image for that feature (a nose). if the feature is found, the
   responsible unit or units generate large activations, which can be
   picked up by the later classifier stages as a good indicator that the
   class is present. image by honglak lee and colleagues (2011) as
   published in    unsupervised learning of hierarchical representations
   with convolutional id50   .

   figure 1 shows features generated by a deep learning algorithm that
   generates easily interpretable features. this is rather unusual.
   features are normally difficult to interpret, especially in deep
   networks like [57]recurrent neural networks and [58]lstms or very deep
   convolutional networks.

deep learning

   in hierarchical [59]id171, we extract multiple layers of
   non-linear features and pass them to a classifier that combines all the
   features to make predictions. we are interested in stacking such very
   deep hierarchies of non-linear features because we cannot learn complex
   features from a few layers. it can be shown mathematically that for
   images the best features for a single layer are edges and blobs because
   they contain the most information that we can extract from a single
   non-linear transformation. to generate features that contain more
   information we cannot operate on the inputs directly, but we need to
   transform our first features (edges and blobs) again to get more
   complex features that contain more information to distinguish between
   classes.

   it has been shown that the human brain does exactly the same thing: the
   first hierarchy of neurons that receives information in the visual
   cortex are sensitive to specific edges and blobs while brain regions
   further down the visual pipeline are sensitive to more complex
   structures such as faces.

   while hierarchical id171 was used before the field deep
   learning existed, these architectures suffered from major problems such
   as the vanishing [60]gradient problem where the gradients became too
   small to provide a learning signal for very deep layers, thus making
   these architectures perform poorly when compared to shallow learning
   algorithms (such as support vector machines).

   the term deep learning originated from new methods and strategies
   designed to generate these deep hierarchies of non-linear features by
   overcoming the problems with vanishing gradients so that we can train
   architectures with dozens of layers of non-linear hierarchical
   features. in the early 2010s, it was shown that combining gpus with
   [61]id180 that offered better gradient flow was
   sufficient to train deep architectures without major difficulties. from
   here the interest in deep learning grew steadily.

   deep learning is not associated just with learning deep non-linear
   hierarchical features, but also with learning to detect very long
   non-linear time dependencies in sequential data. while most other
   algorithms that work on sequential data only have a memory of the last
   10 time steps, [62]long short-term memory (lstm) [63]recurrent neural
   networks (invented by sepp hochreiter and j  rgen schmidhuber in 1997)
   allow the network to pick up on activity hundreds of time-steps in the
   past to make accurate predictions. while id137 have been mostly
   ignored in the past 10 years, their usage has grown rapidly since 2013
   and together with convolutional nets they form one of two major success
   stories of deep learning.

fundamental concepts

id28

   regression analysis estimates the relationship between statistical
   input variables in order to predict an outcome variable. logistic
   regression is a regression model that uses input variables to predict a
   categorical outcome variable that can take on one of a limited set of
   class values, for example    cancer    /    no cancer   , or an image category
   such as    bird    /    car    /    dog    /    cat    /    horse   .

   id28 applies the logistic sigmoid function (see figure
   2) to weighted input values to generate a prediction of which of two
   classes the input data belongs to (or in case of multinomial logistic
   regression, which of multiple classes).
   figure 2: the logistic sigmoid function $latex f(x) =
   \frac{1}{1+e^{-x}}$. image source figure 2: the logistic sigmoid
   function f(x) = \frac{1}{1+e^{-x}} . [64]image source

   id28 is similar to a non-linear [65]id88 or a
   neural network without hidden layers. the main difference from other
   basic models is that id28 is easy to interpret and
   reliable if some statistical properties for the input variables hold.
   if these statistical properties hold one can produce a very reliable
   model with very little input data. this makes id28
   valuable for areas where data are scarce, like the medical and social
   sciences where id28 is used to analyze and interpret
   results from experiments. because it is simple and fast it is also used
   for very large data sets.

   in deep learning, the final layer of a neural network used for
   classification can often be interpreted as a id28. in
   this context, one can see a deep learning algorithm as multiple feature
   learning stages, which then pass their features into a logistic
   regression that classifies an input.

id158

   an id158 (1) takes some input data, and (2)
   transforms this input data by calculating a weighted sum over the
   inputs and (3) applies a non-linear function to this transformation to
   calculate an intermediate state. the three steps above constitute what
   is known as a [66]layer, and the transformative function is often
   referred to as a [67]unit. the intermediate states   often termed
   features   are used as the input into another layer.

   through repetition of these steps, the id158 learns
   multiple layers of non-linear features, which it then combines in a
   final layer to create a prediction.

   the neural network learns by generating an error signal that measures
   the difference between the predictions of the network and the desired
   values and then using this error signal to change the weights (or
   parameters) so that predictions get more accurate.

unit

   a unit often refers to the [68]activation function in a layer by which
   the inputs are transformed via a nonlinear activation function (for
   example by the [69]logistic sigmoid function). usually, a unit has
   several incoming connections and several outgoing connections. however,
   units can also be more complex, like [70]long short-term memory (lstm)
   units, which have multiple id180 with a distinct layout
   of connections to the nonlinear id180, or maxout units,
   which compute the final output over an array of nonlinearly transformed
   input values. [71]pooling, [72]convolution, and other input
   transforming functions are usually not referred to as units.

artificial neuron

   the term artificial neuron   or most often just neuron   is an equivalent
   term to [73]unit, but implies a close connection to neurobiology and
   the human brain while deep learning has very little to do with the
   brain (for example, it is now thought that biological neurons are more
   similar to entire multilayer id88s rather than a single unit in a
   neural network). the term neuron was encouraged after the last [74]ai
   winter to differentiate the more successful neural network from the
   failing and abandoned id88. however, since the wild successes of
   deep learning after 2012, the media often picked up on the term
      neuron    and sought to explain deep learning as mimicry of the human
   brain, which is very misleading and potentially dangerous for the
   perception of the field of deep learning. now the term neuron is
   discouraged and the more descriptive term unit should be used instead.

activation function

   an activation function takes in weighted data (id127
   between input data and weights) and outputs a non-linear transformation
   of the data. for example, output = max(0,weighted_data) is the
   [75]rectified linear activation function (essentially set all negative
   values to zero). the difference between units and id180
   is that units can be more complex, that is, a unit can have multiple
   id180 (for example [76]lstm units) or a slightly more
   complex structure (for example maxout units).

   the difference between linear and non-linear id180 can
   be shown with the relationship of some weighted values: imagine the
   four points a1, a2, b1 and b2. the pairs a1 / a2, and b1 / b2 lie close
   to each other, but a1 is distant from b1 and b2, and vice versa; the
   same for a2.

   with a linear transformation the relationship between pairs might
   change. for example a1 and a2 might be far apart, but this implies that
   b1 and b2 are also far apart. the distance between the pairs might
   shrink, but if it does, then both b1 and b2 will be close to a1 and a2
   at the same time. we can apply many linear transformations, but the
   relationship between a1 / a2 and b1 / b2 will always be similar.

   in contrast, with a non-linear activation function we can increase the
   distance between a1 and a2 while we decrease the distance between b1
   and b2. we can make b1 close to a1, but b2 distant from a1. by applying
   non-linear functions, we create new relationships between the points.
   with every new non-linear transformation we can increase the complexity
   of the relationships. in deep learning, using non-linear activation
   functions creates increasingly complex features with every layer.

   in contrast, the features of 1000 layers of pure linear transformations
   can be reproduced by a single layer (because a chain of matrix
   multiplication can always be represented by a single matrix
   multiplication). this is why non-linear id180 are so
   important in deep learning.

layer

   a layer is the highest-level building block in deep learning. a layer
   is a container that usually receives weighted input, transforms it with
   a set of mostly non-linear functions and then passes these values as
   output to the next layer. a layer is usually uniform, that is it only
   contains one type of activation function, [77]pooling, [78]convolution
   etc. so that it can be easily compared to other parts of the network.
   the first and last layers in a network are called input and output
   layers, respectively, and all layers in between are called hidden
   layers.

convolutional deep learning

convolution

   convolution is a mathematical operation which describes a rule of how
   to mix two functions or pieces of information: (1) the feature map (or
   input data) and (2) the convolution kernel mix together to form (3) a
   transformed feature map. convolution is often interpreted as a filter,
   where the kernel filters the feature map for information of a certain
   kind (for example one kernel might filter for edges and discard other
   information).
   figure 2: convolution of an image with an edge detector convolution
   kernel. figure 2: convolution of an image with an edge detector
   convolution kernel. sources: [79]1 [80]2.

   convolution is important in physics and mathematics as it defines a
   bridge between the spatial and time domains (pixel with intensity 147
   at position (0,30)) and the frequency domain (amplitude of 0.3, at
   30hz, with 60-degree phase) through the convolution theorem. this
   bridge is defined by the use of fourier transforms: when you use a
   fourier transform on both the kernel and the feature map, then the
   convolution operation is simplified significantly (integration becomes
   mere multiplication). some of the fastest gpu implementations of
   convolutions (for example some implementations in the [81]nvidia cudnn
   library) currently make use of fourier transforms.
   convolution sliding window figure 3: calculating convolution by sliding
   image patches over the entire image. one image patch (yellow) of the
   original image (green) is multiplied by the kernel (red numbers in the
   yellow patch), and its sum is written to one feature map pixel (red
   cell in convolved feature). image source: [82]1.

   convolution can describe the diffusion of information, for example, the
   diffusion that takes place if you put milk into your coffee and do not
   stir can be accurately modeled by a convolution operation (pixels
   diffuse towards contours in an image). in quantum mechanics, it
   describes the id203 of a quantum particle being in a certain
   place when you measure the particle   s position (average id203 for
   a pixel   s position is highest at contours). in id203 theory, it
   describes cross-correlation, which is the degree of similarity for two
   sequences that overlap (similarity high if the pixels of a feature
   (e.g. nose) overlap in an image (e.g. face)). in statistics, it
   describes a weighted moving average over a normalized sequence of input
   (large weights for contours, small weights for everything else). many
   other interpretations exist.

   while it is unknown which interpretation of convolution is correct for
   deep learning, the cross-correlation interpretation is currently the
   most useful: convolutional filters can be interpreted as feature
   detectors, that is, the input (feature map) is filtered for a certain
   feature (the kernel) and the output is large if the feature is detected
   in the image. this is exactly how you interpret cross-correlation for
   an image.
   cross-correlation example figure 4: cross-correlation for an image.
   convolution can be transformed to cross-correlation by reversing the
   kernel (upside-down image). the kernel can then be interpreted as a
   feature detector where a detected feature results in large outputs
   (white) and small outputs if no feature is present (black). images are
   taken from [83]steven smith   s excellent [84]free online book about
   digital signal processing.

   additional material: [85]understanding convolution in deep learning

pooling / subsampling

   pooling is a procedure that takes input over a certain area and reduces
   that to a single value (subsampling). in [86]convolutional neural
   networks, this concentration of information has the useful property
   that outgoing connections usually receive similar information (the
   information is    funneled    into the right place for the input feature
   map of the next convolutional layer). this provides basic invariance to
   rotations and translations. for example, if the face on an image patch
   is not in the center of the image but slightly translated, it should
   still work fine because the information is funneled into the right
   place by the pooling operation so that the convolutional filters can
   detect the face.

   the larger the size of the pooling area, the more information is
   condensed, which leads to slim networks that fit more easily into gpu
   memory. however, if the pooling area is too large, too much information
   is thrown away and predictive performance decreases.

   additional material: [87]neural networks [9.5]: id161    
   pooling and subsampling

convolutional neural network (id98)

   a convolutional neural network, or preferably convolutional network or
   convolutional net (the term neural is misleading; see also
   [88]artificial neuron), uses convolutional [89]layers (see
   [90]convolution) that filter inputs for useful information. these
   convolutional layers have parameters that are learned so that these
   filters are adjusted automatically to extract the most useful
   information for the task at hand (see id171). for example,
   in a general object recognition task it might be most useful to filter
   information about the shape of an object (objects usually have very
   different shapes) while for a bird recognition task it might be more
   suitable to extract information about the color of the bird (most birds
   have a similar shape, but different colors; here color is more useful
   to distinguish between birds). convolutional networks adjust
   automatically to find the best feature for these tasks.

   usually, multiple convolutional layers are used that filter images for
   more and more abstract information after each layer (see hierarchical
   features).

   convolutional networks usually also use pooling layers (see
   [91]pooling) for limited translation and rotation invariance (detect
   the object even if it appears at some unusual place). pooling also
   reduces the memory consumption and thus allows for the usage of more
   convolutional layers.

   more recent convolutional networks use inception modules (see
   [92]inception) which use 1  1 convolutional kernels to reduce the memory
   consumption further while speeding up the computation (and thus
   training).
   convolutional neural network figure 5: an image of a traffic sign is
   filtered by 4 5  5 convolutional kernels which create 4 feature maps,
   these feature maps are subsampled by max pooling. the next layer
   applies 10 5  5 convolutional kernels to these subsampled images and
   again we pool the feature maps. the final layer is a fully connected
   layer where all generated features are combined and used in the
   classifier (essentially id28). image by [93]maurice
   peemen.

   additional material: [94]coursera: neural networks for machine
   learning: object recognition with neural nets.

inception

   inception modules in [95]convolutional networks were designed to allow
   for deeper and larger [96]convolutional [97]layers while at the same
   time allowing for more efficient computation. this is done by using 1  1
   convolutions with small feature map size, for example, 192 28  28 sized
   feature maps can be reduced to 64 28  28 feature maps through 64 1  1
   convolutions. because of the reduced size, these 1  1 convolutions can
   be followed up with larger convolutions of size 3  3 and 5  5. in
   addition to 1  1 convolution, max pooling may also be used to reduce
   dimensionality.

   in the output of an inception module, all the large convolutions are
   concatenated into a big feature map which is then fed into the next
   layer (or inception module).

   additional material: [98]going deeper with convolutions

conclusion to part 1

   this concludes part one of this crash course on deep learning. please
   check back soon for the next two parts of the series. in [99]part 2,
   i   ll provide a brief historical overview followed by an introduction to
   training deep neural networks.

   meanwhile, you might be interested in learning about [100]cudnn,
   [101]digits, [102]id161 with caffe, [103]natural language
   processing with torch, [104]id4, the
   [105]mocha.jl deep learning framework for julia, or other [106]parallel
   forall posts on deep learning.
   [107]15 comments
   about the authors
   about tim dettmers
   tim dettmers is a masters student in informatics at the university of
   lugano where he works on deep learning research. before that he studied
   applied mathematics and worked for three years as a software engineer
   in the automation industry. he runs a [108]blog about deep learning and
   takes part in kaggle data science competitions where he has reached a
   world rank of 63.
   [109]follow @tim_dettmers on twitter
   [110]view all posts by tim dettmers
   related posts
   [111]deep learning in a nutshell: history and training
   by [112]tim dettmers | [113]december 16, 2015
   [114]cuda_cube_1k
   [115]deep learning in a nutshell: sequence learning
   by [116]tim dettmers | [117]march 7, 2016
   [118]cuda ai cube
   [119]deep learning for id161 with caffe and cudnn
   by [120]evan shelhamer | [121]october 15, 2014
   [122]cudnn_logo_black_on_white_179x115
   [123]introduction to id4 with gpus (part 3)
   by [124]kyunghyun cho | [125]july 26, 2015
   [126]figure 8. image id134 with attention mechanism.
   comments

   copyright    2019 nvidia corporation
   [127]legal information [128]privacy policy

   [129]close

parallel forall

     * [130]features

   search for: ____________________ search

accelerated computing

     * [131]accelerated computing
     * [132]downloads
     * [133]training
     * [134]ecosystem
     * [135]forums
     * [136]register now
     * [137]login

references

   visible links
   1. https://devblogs.nvidia.com/feed/
   2. https://devblogs.nvidia.com/comments/feed/
   3. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/feed/
   4. https://devblogs.nvidia.com/wp-json/oembed/1.0/embed?url=https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
   5. https://devblogs.nvidia.com/wp-json/oembed/1.0/embed?url=https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/&format=xml
   6. https://developer.nvidia.com/
   7. https://developer.nvidia.com/
   8. https://developer.nvidia.com/accelerated-computing-developer
   9. https://developer.nvidia.com/user
  10. https://devblogs.nvidia.com/
  11. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#content
  12. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#secondary
  13. https://news.developer.nvidia.com/
  14. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
  15. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
  16. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
  17. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
  18. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
  19. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
  20. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
  21. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
  22. https://devblogs.nvidia.com/category/accelerated-computing/
  23. https://devblogs.nvidia.com/category/artificial-intelligence/
  24. https://devblogs.nvidia.com/category/autonomous-vehicles/
  25. https://devblogs.nvidia.com/category/design-visualization/
  26. https://devblogs.nvidia.com/category/features/
  27. https://devblogs.nvidia.com/category/game-development/
  28. https://devblogs.nvidia.com/category/robotics/
  29. https://devblogs.nvidia.com/category/smart-cities/
  30. https://devblogs.nvidia.com/category/virtual-reality/
  31. https://devblogs.nvidia.com/category/accelerated-computing/
  32. https://devblogs.nvidia.com/category/artificial-intelligence/
  33. https://devblogs.nvidia.com/category/autonomous-vehicles/
  34. https://devblogs.nvidia.com/category/design-visualization/
  35. https://devblogs.nvidia.com/category/features/
  36. https://devblogs.nvidia.com/category/game-development/
  37. https://devblogs.nvidia.com/category/robotics/
  38. https://devblogs.nvidia.com/category/smart-cities/
  39. https://devblogs.nvidia.com/category/virtual-reality/
  40. https://devblogs.nvidia.com/category/artificial-intelligence/
  41. https://devblogs.nvidia.com/author/tdettmers/
  42. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
  43. https://devblogs.nvidia.com/tag/cudnn/
  44. https://devblogs.nvidia.com/tag/deep-learning/
  45. https://devblogs.nvidia.com/tag/deep-neural-networks/
  46. https://devblogs.nvidia.com/tag/machine-learning/
  47. https://devblogs.nvidia.com/tag/neural-networks/
  48. https://developer.nvidia.com/deep-learning
  49. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training/
  50. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-sequence-learning/
  51. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/
  52. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training#training
  53. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#machine-learning
  54. http://blog.kaggle.com/2014/08/01/learning-from-the-best/
  55. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#feature-engineering
  56. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#logistic-regression
  57. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-sequence-learning#recurrent-neural-networks
  58. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-sequence-learning#lstm
  59. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#feature-learning
  60. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training#stochastic-gradient-descent
  61. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#activation-function
  62. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-sequence-learning#lstm
  63. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-sequence-learning#recurrent-neural-networks
  64. https://en.wikipedia.org/wiki/file:logistic-curve.svg
  65. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training#id88
  66. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#layer
  67. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#unit
  68. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#activation-function
  69. https://en.wikipedia.org/wiki/logistic_function
  70. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-sequence-learning#lstm
  71. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#pooling
  72. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#convolution
  73. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#unit
  74. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training#ai-winter
  75. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training#rectified-linear-function
  76. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-sequence-learning#lstm
  77. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#pooling
  78. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#convolution
  79. https://en.wikipedia.org/wiki/file:vd-orig.png
  80. https://en.wikipedia.org/wiki/file:vd-edge3.png
  81. https://devblogs.nvidia.com/parallelforall/accelerate-machine-learning-cudnn-deep-neural-network-library/
  82. http://deeplearning.stanford.edu/wiki/index.php/feature_extraction_using_convolution
  83. http://www.dspguide.com/swsmith.htm
  84. http://www.dspguide.com/pdfbook.htm
  85. http://timdettmers.com/2015/03/26/convolution-deep-learning/
  86. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#convolutional-neural-networks
  87. https://www.youtube.com/watch?v=i-jkxcpbrt4
  88. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#artificial-neuron
  89. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#layer
  90. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#convolution
  91. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#pooling
  92. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#inception
  93. http://parse.ele.tue.nl/mpeemen
  94. https://www.youtube.com/playlist?list=plnnr1o8owc6ylzzlohzx2q5c2wwmoiuzy
  95. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#convolutional-neural-network
  96. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#convolution
  97. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#layer
  98. http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/szegedy_going_deeper_with_2015_cvpr_paper.pdf
  99. https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training/
 100. https://devblogs.nvidia.com/parallelforall/cudnn-v2-higher-performance-deep-learning-gpus/
 101. https://devblogs.nvidia.com/parallelforall/easy-multi-gpu-deep-learning-digits-2/
 102. https://devblogs.nvidia.com/parallelforall/deep-learning-computer-vision-caffe-cudnn/
 103. https://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/
 104. https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/
 105. https://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/
 106. https://devblogs.nvidia.com/parallelforall/tag/deep-learning
 107. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#comments
 108. http://timdettmers.com/
 109. https://twitter.com/intent/user?screen_name=tim_dettmers
 110. https://devblogs.nvidia.com/author/tdettmers/
 111. https://devblogs.nvidia.com/deep-learning-nutshell-history-training/
 112. https://devblogs.nvidia.com/author/tdettmers/
 113. https://devblogs.nvidia.com/deep-learning-nutshell-history-training/
 114. https://devblogs.nvidia.com/deep-learning-nutshell-history-training/
 115. https://devblogs.nvidia.com/deep-learning-nutshell-sequence-learning/
 116. https://devblogs.nvidia.com/author/tdettmers/
 117. https://devblogs.nvidia.com/deep-learning-nutshell-sequence-learning/
 118. https://devblogs.nvidia.com/deep-learning-nutshell-sequence-learning/
 119. https://devblogs.nvidia.com/deep-learning-computer-vision-caffe-cudnn/
 120. https://devblogs.nvidia.com/author/eshelhammer/
 121. https://devblogs.nvidia.com/deep-learning-computer-vision-caffe-cudnn/
 122. https://devblogs.nvidia.com/deep-learning-computer-vision-caffe-cudnn/
 123. https://devblogs.nvidia.com/introduction-neural-machine-translation-gpus-part-3/
 124. https://devblogs.nvidia.com/author/kcho/
 125. https://devblogs.nvidia.com/introduction-neural-machine-translation-gpus-part-3/
 126. https://devblogs.nvidia.com/introduction-neural-machine-translation-gpus-part-3/
 127. http://www.nvidia.com/object/legal_info.html
 128. http://www.nvidia.com/object/privacy_policy.html
 129. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/#sidr-left
 130. https://devblogs.nvidia.com/category/features/
 131. https://developer.nvidia.com/accelerated-computing
 132. https://developer.nvidia.com/accelerated-computing-toolkit
 133. https://developer.nvidia.com/accelerated-computing-training
 134. https://developer.nvidia.com/tools-ecosystem
 135. https://devtalk.nvidia.com/
 136. https://developer.nvidia.com/accelerated-computing-developer
 137. https://developer.nvidia.com/user

   hidden links:
 139. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
 140. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
 141. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
