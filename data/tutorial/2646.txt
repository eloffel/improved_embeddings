   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]chatbots life
     * [9]     start a project
     * [10]     tools
     * [11]    business
     * [12]     voice
     * [13]     tutorials
     * [14]     design
     * [15]            ai & nlp
     * [16]     ai for shopify
     __________________________________________________________________

resnets, highwaynets, and densenets, oh my!

implementing really deep neural networks in tensorflow

   [17]go to the profile of arthur juliani
   [18]arthur juliani (button) blockedunblock (button) followfollowing
   oct 14, 2016
   [1*ytor7idkajxoce6petjghw.jpeg]

   when it comes to neural network design, the trend in the past few years
   has pointed in one direction: deeper. whereas the state of the art only
   a few years ago consisted of networks which were roughly twelve layers
   deep, it is now not surprising to come across networks which are
   hundreds of layers deep. this move hasn   t just consisted of greater
   depth for depths sake. for many applications, the most prominent of
   which being object classification, the deeper the neural network, the
   better the performance. that is, provided they can be properly trained!
   in this post i would like to walk through the logic behind three recent
   deep learning architectures: resnet, highwaynet, and densenet. each
   make it more possible to successfully trainable deep networks by
   overcoming the limitations of traditional network design. i will also
   be providing tensorflow code to easily implement each of these
   networks. if you   d just like the code, you can find that [19]here.
   otherwise, read on!
   [1*uqiewr26h3eamjkwpog-hg.png]
   the trend toward deeper networks is clear.

why naively going deeper doesn   t work

   the first intuition when designing a deep network may be to simply
   stack many of the typical building blocks such as convolutional or
   fully-connected layers together. this works to a point, but performance
   quickly diminishes the deeper a traditional network becomes. the issue
   arises from the way in which neural networks are trained through
   backpropogation. when a network is being trained, a gradient signal
   must be propagated backwards through the network from the top layer all
   the way down to the bottom most layer in order to ensure that the
   network updates itself appropriately. with a traditional network this
   gradient becomes slightly diminished as it passes through each layer of
   the network. for a network with just a few layers, this isn   t an issue.
   for a network with more than a couple dozen layers however, the signal
   essentially disappears by the time it reaches the beginning of the
   network again.

   so the problem is to design a network in which the gradient can more
   easily reach all the layers of a network which might be dozens, or even
   hundreds of layers deep. this is the goal behind the following state of
   the art architectures: resnets, highwaynets, and densenets.

residual network

   a residual network, or resnet is a neural network architecture which
   solves the problem of vanishing gradients in the simplest way possible.
   if there is trouble sending the gradient signal backwards, why not
   provide the network with a shortcut at each layer to make things happen
   more smoothly? in a traditional network the activation at a layer is
   defined as follows:

     y = f(x)

   where f(x) is our convolution, id127, or batch
   id172, etc. when the signal is sent backwards, the gradient
   always must pass through f(x), which can cause trouble due to the
   nonlinearities which are involved. instead, at each layer the resnet
   implements:

     y = f(x) + x

   the    + x    at the end is the shortcut. it allows the gradient to pass
   backwards directly. by stacking these layers, the gradient could
   theoretically    skip    over all the intermediate layers and reach the
   bottom without being diminished.

   while this is the intuition, the actual implementation is a little more
   complex. in the latest [20]incarnation of resnets, f(x) + x takes the
   form:
   [1*hx7aqbqmlspkra7wlinfpg.png]
   resnet unit architecture. bn stands for batch id172. weight can
   refer to fully-connected or convolutional layer.

   with tensorflow we can implement a network composed of these residual
   units as follows:

   iframe: [21]/media/3ba5a0b82d0081403a64218eb0aa8dc2?postid=9bb15918ee32

highway network

   the second architecture i   d like to introduce is the [22]highway
   network. it builds on the resnet in a pretty intuitive way. the highway
   network preserves the shortcuts introduced in the resnet, but augments
   them with a learnable parameter to determine to what extent each layer
   should be a skip connection or a nonlinear connection. layers in a
   highway network are defined as follows:
   [1*eplib5veivv0e-ip3jgm8a.png]

   in this equation we can see an outline of the previous two kinds of
   layers discussed: y = h(x,wh) mirrors our traditional layer, and y =
   h(x,wh) + x mirrors our residual unit. what is new is the t(x,wt)
   function. this serves at the switch to determine to what extent
   information should be sent through the primary pathway or the skip
   pathway. by using t and (1-t) for each of the two pathways, the
   activation must always sum to 1. we can implement this in tensorflow as
   follows:

   iframe: [23]/media/e468bd5c4cce32b4424cf80dadeb4de4?postid=9bb15918ee32

dense networks

   finally i want to introduce the dense network, or [24]densenet. you
   might say that is architecture takes the insights of the skip
   connection to the extreme. the idea here is that if connecting a skip
   connection from the previous layer improves performance, why not
   connect every layer to every other layer? that way there is always a
   direct route for the information backwards through the network.
   [1*kojux1st5rndozwwlwrgkw.png]
   image demonstrating dense connection principal. from:
   [25]https://arxiv.org/abs/1608.06993

   instead of using an addition however, the densenet relies on stacking
   of layers. mathematically this looks like:

     y = f(x,x-1,x-2   x-n)

   this architecture makes intuitive sense in both the feedforward and
   feed backward settings. in the feed-forward setting, a task may benefit
   from being able to get low-level feature activations in addition to
   high level feature activations. in classifying objects for example, a
   lower layer of the network may determine edges in an image, whereas a
   higher layer would determine larger-scale features such as presence of
   faces. there may be cases where being able to use information about
   edges can help in determining the correct object in a complex scene. in
   the backwards case, having all the layers connected allows us to
   quickly send gradients to their respective places in the network
   easily.

   when implementing densenets, we can   t just connected everything though.
   only layers with the same height and width can be stacked. so we
   instead densely stack a set of convolutional layers, then apply a
   striding or pooling layer, then densely stack another set of
   convolutional layers, etc. this can be implemented in tensorflow as
   follows:

   iframe: [26]/media/dbd66afc362550736a51570d522e5cfc?postid=9bb15918ee32

   all of these network can be trained to classify images using the
   [27]cifar10 dataset, and can perform well with dozens of layers where a
   traditional neural network fails. with little parameter tuning i was
   able to get them to perform above 90% accuracy on a test set after only
   an hour or so. the full code for training each of these models, and
   comparing them to a traditional networks is available [28]here. i hope
   this walkthrough has been a helpful introduction to the world of really
   deep neural networks!
     __________________________________________________________________

   if this post has been valuable to you, please consider [29]donating to
   help support future tutorials, articles, and implementations. any
   contribution is greatly appreciated!

   if you   d like to follow my work on deep learning, ai, and cognitive
   science, follow me on medium @[30]arthur juliani, or on twitter
   [31]@awjliani.
   [1*yfanetnnkgbij0fvzixymq.gif]
   [32][1*6jufpm8ptrrv7ododmmrfa.png]
   [33][1*x_shmpnyem_tctqzgmn1lq.png]
   [34][1*h9owa84yus6tyzhlnlj6vq.png]

     * [35]machine learning
     * [36]deep learning
     * [37]artificial intelligence
     * [38]neural networks
     * [39]how to

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 20 (button) (button)

     (button) blockedunblock (button) followfollowing
   [40]go to the profile of arthur juliani

[41]arthur juliani

   deep learning [42]@unity3d

     (button) follow
   [43]chatbots life

[44]chatbots life

   best place to learn about chatbots. we share the latest bot news, info,
   ai & nlp, tools, tutorials & more.

     * (button)
       (button) 1k
     * (button)
     *
     *

   [45]chatbots life
   never miss a story from chatbots life, when you sign up for medium.
   [46]learn more
   never miss a story from chatbots life
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://chatbotslife.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/9bb15918ee32
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32&source=--------------------------nav_reg&operation=register
   8. https://chatbotslife.com/?source=logo-lo_vilgnsrp14tb---a49517e4c30b
   9. https://chatbotslife.com/chatbot-development-hire-top-ai-chatbot-developers-c8b45ef7f207
  10. https://chatbotslife.com/chatbot-tools-62dfc60d2b8a
  11. https://chatbotslife.com/bots-for-business/home
  12. https://chatbotslife.com/tagged/voice-assistant
  13. https://chatbotslife.com/tagged/how-to
  14. https://chatbotslife.com/tagged/user-experience
  15. https://chatbotslife.com/tagged/artificial-intelligence
  16. https://www.gobeyond.ai/
  17. https://chatbotslife.com/@awjuliani?source=post_header_lockup
  18. https://chatbotslife.com/@awjuliani
  19. https://github.com/awjuliani/tf-tutorials/blob/master/deep network comparison.ipynb
  20. https://arxiv.org/abs/1603.05027
  21. https://chatbotslife.com/media/3ba5a0b82d0081403a64218eb0aa8dc2?postid=9bb15918ee32
  22. https://arxiv.org/abs/1505.00387
  23. https://chatbotslife.com/media/e468bd5c4cce32b4424cf80dadeb4de4?postid=9bb15918ee32
  24. https://arxiv.org/abs/1608.06993
  25. https://arxiv.org/abs/1608.06993
  26. https://chatbotslife.com/media/dbd66afc362550736a51570d522e5cfc?postid=9bb15918ee32
  27. https://www.cs.toronto.edu/~kriz/cifar.html
  28. https://github.com/awjuliani/tf-tutorials/blob/master/deep network comparison.ipynb
  29. https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=v2r22dv4xsr5y&lc=us&item_name=arthur juliani's deep learning tutorials&currency_code=usd&bn=pp-donationsbf:btn_donatecc_lg.gif:nonhosted
  30. https://medium.com/@awjuliani
  31. https://twitter.com/awjuliani
  32. https://goo.gl/forms/iluqhlfvrmgfncck2
  33. https://goo.gl/forms/u4airwi7d8om63as2
  34. https://goo.gl/forms/k0te5wizh4r4invj1
  35. https://chatbotslife.com/tagged/machine-learning?source=post
  36. https://chatbotslife.com/tagged/deep-learning?source=post
  37. https://chatbotslife.com/tagged/artificial-intelligence?source=post
  38. https://chatbotslife.com/tagged/neural-networks?source=post
  39. https://chatbotslife.com/tagged/how-to?source=post
  40. https://chatbotslife.com/@awjuliani?source=footer_card
  41. https://chatbotslife.com/@awjuliani
  42. http://twitter.com/unity3d
  43. https://chatbotslife.com/?source=footer_card
  44. https://chatbotslife.com/?source=footer_card
  45. https://chatbotslife.com/
  46. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  48. https://medium.com/p/9bb15918ee32/share/twitter
  49. https://medium.com/p/9bb15918ee32/share/facebook
  50. https://medium.com/p/9bb15918ee32/share/twitter
  51. https://medium.com/p/9bb15918ee32/share/facebook
