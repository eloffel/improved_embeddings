   #[1]github [2]recent commits to
   deep-learning-papers-reading-roadmap:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]2,116
     * [35]star [36]22,316
     * [37]fork [38]5,034

[39]floodsung/[40]deep-learning-papers-reading-roadmap

   [41]code [42]issues 34 [43]pull requests 37 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   deep learning papers reading roadmap for anyone who are eager to learn
   this amazing tech!
   [47]deep-learning
     * [48]100 commits
     * [49]1 branch
     * [50]0 releases
     * [51]fetching contributors

    1. [52]python 100.0%

   (button) python
   branch: master (button) new pull request
   [53]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/f
   [54]download zip

downloading...

   want to be notified of new releases in
   floodsung/deep-learning-papers-reading-roadmap?
   [55]sign in [56]sign up

launching github desktop...

   if nothing happens, [57]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [59]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [60]download the github extension for visual studio
   and try again.

   (button) go back
   [61]@floodsung
   [62]floodsung [63]merge pull request [64]#76 [65]from
   the0demiurge/patch-1 (button)    
update download.py

   latest commit [66]98e3b45 mar 17, 2018
   [67]permalink
   type         name         latest commit message commit time
        failed to load latest commit information.
        [68]readme.md
        [69]download.py
        [70]requirements.txt

readme.md

deep learning papers reading roadmap

     if you are a newcomer to the deep learning area, the first question
     you may have is "which paper should i start reading from?"

     here is a reading roadmap of deep learning papers!

   the roadmap is constructed in accordance with the following four
   guidelines:
     * from outline to detail
     * from old to state-of-the-art
     * from generic to specific areas
     * focus on state-of-the-art

   you will find many papers that are quite new but really worth reading.

   i would continue adding papers to this roadmap.
     __________________________________________________________________

1 deep learning history and basics

1.0 book

   [0] bengio, yoshua, ian j. goodfellow, and aaron courville. "deep
   learning." an mit press book. (2015). [71][html] (deep learning bible,
   you can read this book while reading following papers.)                               

1.1 survey

   [1] lecun, yann, yoshua bengio, and geoffrey hinton. "deep learning."
   nature 521.7553 (2015): 436-444. [72][pdf] (three giants' survey)
                                 

1.2 deep belief network(dbn)(milestone of deep learning eve)

   [2] hinton, geoffrey e., simon osindero, and yee-whye teh. "a fast
   learning algorithm for deep belief nets." neural computation 18.7
   (2006): 1527-1554. [73][pdf](deep learning eve)                   

   [3] hinton, geoffrey e., and ruslan r. salakhutdinov. "reducing the
   dimensionality of data with neural networks." science 313.5786 (2006):
   504-507. [74][pdf] (milestone, show the promise of deep learning)
                     

1.3 id163 evolution   deep learning broke out from here   

   [4] krizhevsky, alex, ilya sutskever, and geoffrey e. hinton. "id163
   classification with deep convolutional neural networks." advances in
   neural information processing systems. 2012. [75][pdf] (alexnet, deep
   learning breakthrough)                               

   [5] simonyan, karen, and andrew zisserman. "very deep convolutional
   networks for large-scale image recognition." arxiv preprint
   arxiv:1409.1556 (2014). [76][pdf] (vggnet,neural networks become very
   deep!)                   

   [6] szegedy, christian, et al. "going deeper with convolutions."
   proceedings of the ieee conference on id161 and pattern
   recognition. 2015. [77][pdf] (googlenet)                   

   [7] he, kaiming, et al. "deep residual learning for image recognition."
   arxiv preprint arxiv:1512.03385 (2015). [78][pdf] (resnet,very very
   deep networks, cvpr best paper)                               

1.4 id103 evolution

   [8] hinton, geoffrey, et al. "deep neural networks for acoustic
   modeling in id103: the shared views of four research
   groups." ieee signal processing magazine 29.6 (2012): 82-97. [79][pdf]
   (breakthrough in id103)                        

   [9] graves, alex, abdel-rahman mohamed, and geoffrey hinton. "speech
   recognition with deep recurrent neural networks." 2013 ieee
   international conference on acoustics, speech and signal processing.
   ieee, 2013. [80][pdf] (id56)                  

   [10] graves, alex, and navdeep jaitly. "towards end-to-end speech
   recognition with recurrent neural networks." icml. vol. 14. 2014.
   [81][pdf]                  

   [11] sak, ha  im, et al. "fast and accurate recurrent neural network
   acoustic models for id103." arxiv preprint
   arxiv:1507.06947 (2015). [82][pdf] (google id103 system)
                     

   [12] amodei, dario, et al. "deep speech 2: end-to-end speech
   recognition in english and mandarin." arxiv preprint arxiv:1512.02595
   (2015). [83][pdf] (baidu id103 system)                         

   [13] w. xiong, j. droppo, x. huang, f. seide, m. seltzer, a. stolcke,
   d. yu, g. zweig "achieving human parity in conversational speech
   recognition." arxiv preprint arxiv:1610.05256 (2016). [84][pdf]
   (state-of-the-art in id103, microsoft)                         

     after reading above papers, you will have a basic understanding of
     the deep learning history, the basic architectures of deep learning
     model(including id98, id56, lstm) and how deep learning can be applied
     to image and id103 issues. the following papers will
     take you in-depth understanding of the deep learning method, deep
     learning in different areas of application and the frontiers. i
     suggest that you can choose the following papers based on your
     interests and research direction.

   #2 deep learning method

2.1 model

   [14] hinton, geoffrey e., et al. "improving neural networks by
   preventing co-adaptation of feature detectors." arxiv preprint
   arxiv:1207.0580 (2012). [85][pdf] (dropout)                   

   [15] srivastava, nitish, et al. "dropout: a simple way to prevent
   neural networks from overfitting." journal of machine learning research
   15.1 (2014): 1929-1958. [86][pdf]                   

   [16] ioffe, sergey, and christian szegedy. "batch id172:
   accelerating deep network training by reducing internal covariate
   shift." arxiv preprint arxiv:1502.03167 (2015). [87][pdf] (an
   outstanding work in 2015)                         

   [17] ba, jimmy lei, jamie ryan kiros, and geoffrey e. hinton. "layer
   id172." arxiv preprint arxiv:1607.06450 (2016). [88][pdf]
   (update of batch id172)                         

   [18] courbariaux, matthieu, et al. "binarized neural networks: training
   neural networks with weights and activations constrained to+ 1 or   1."
   [89][pdf] (new model,fast)                   

   [19] jaderberg, max, et al. "decoupled neural interfaces using
   synthetic gradients." arxiv preprint arxiv:1608.05343 (2016). [90][pdf]
   (innovation of training method,amazing work)                               

   [20] chen, tianqi, ian goodfellow, and jonathon shlens. "net2net:
   accelerating learning via knowledge transfer." arxiv preprint
   arxiv:1511.05641 (2015). [91][pdf] (modify previously trained network
   to reduce training epochs)                   

   [21] wei, tao, et al. "network morphism." arxiv preprint
   arxiv:1603.01670 (2016). [92][pdf] (modify previously trained network
   to reduce training epochs)                   

2.2 optimization

   [22] sutskever, ilya, et al. "on the importance of initialization and
   momentum in deep learning." icml (3) 28 (2013): 1139-1147. [93][pdf]
   (momentum optimizer)             

   [23] kingma, diederik, and jimmy ba. "adam: a method for stochastic
   optimization." arxiv preprint arxiv:1412.6980 (2014). [94][pdf] (maybe
   used most often currently)                   

   [24] andrychowicz, marcin, et al. "learning to learn by gradient
   descent by id119." arxiv preprint arxiv:1606.04474 (2016).
   [95][pdf] (neural optimizer,amazing work)                               

   [25] han, song, huizi mao, and william j. dally. "deep compression:
   compressing deep neural network with pruning, trained quantization and
   huffman coding." corr, abs/1510.00149 2 (2015). [96][pdf] (iclr best
   paper, new direction to make nn running fast,deephi tech startup)
                                 

   [26] iandola, forrest n., et al. "squeezenet: alexnet-level accuracy
   with 50x fewer parameters and< 1mb model size." arxiv preprint
   arxiv:1602.07360 (2016). [97][pdf] (also a new direction to optimize
   nn,deephi tech startup)                         

2.3 unsupervised learning / deep generative model

   [27] le, quoc v. "building high-level features using large scale
   unsupervised learning." 2013 ieee international conference on
   acoustics, speech and signal processing. ieee, 2013. [98][pdf]
   (milestone, andrew ng, google brain project, cat)                         

   [28] kingma, diederik p., and max welling. "auto-encoding variational
   bayes." arxiv preprint arxiv:1312.6114 (2013). [99][pdf] (vae)                         

   [29] goodfellow, ian, et al. "generative adversarial nets." advances in
   neural information processing systems. 2014. [100][pdf] (gan,super cool
   idea)                               

   [30] radford, alec, luke metz, and soumith chintala. "unsupervised
   representation learning with deep convolutional generative adversarial
   networks." arxiv preprint arxiv:1511.06434 (2015). [101][pdf] (dcgan)
                           

   [31] gregor, karol, et al. "draw: a recurrent neural network for image
   generation." arxiv preprint arxiv:1502.04623 (2015). [102][pdf] (vae
   with attention, outstanding work)                               

   [32] oord, aaron van den, nal kalchbrenner, and koray kavukcuoglu.
   "pixel recurrent neural networks." arxiv preprint arxiv:1601.06759
   (2016). [103][pdf] (pixelid56)                         

   [33] oord, aaron van den, et al. "conditional image generation with
   pixelid98 decoders." arxiv preprint arxiv:1606.05328 (2016). [104][pdf]
   (pixelid98)                         

2.4 id56 / sequence-to-sequence model

   [34] graves, alex. "generating sequences with recurrent neural
   networks." arxiv preprint arxiv:1308.0850 (2013). [105][pdf] (lstm,
   very nice generating result, show the power of id56)                         

   [35] cho, kyunghyun, et al. "learning phrase representations using id56
   encoder-decoder for id151." arxiv preprint
   arxiv:1406.1078 (2014). [106][pdf] (first seq-to-seq paper)                         

   [36] sutskever, ilya, oriol vinyals, and quoc v. le. "sequence to
   sequence learning with neural networks." advances in neural information
   processing systems. 2014. [107][pdf] (outstanding work)                               

   [37] bahdanau, dzmitry, kyunghyun cho, and yoshua bengio. "neural
   machine translation by jointly learning to align and translate." arxiv
   preprint arxiv:1409.0473 (2014). [108][pdf]                         

   [38] vinyals, oriol, and quoc le. "a neural conversational model."
   arxiv preprint arxiv:1506.05869 (2015). [109][pdf] (seq-to-seq on
   chatbot)                   

2.5 id63

   [39] graves, alex, greg wayne, and ivo danihelka. "neural turing
   machines." arxiv preprint arxiv:1410.5401 (2014). [110][pdf] (basic
   prototype of future computer)                               

   [40] zaremba, wojciech, and ilya sutskever. "id23
   id63s." arxiv preprint arxiv:1505.00521 362 (2015).
   [111][pdf]                   

   [41] weston, jason, sumit chopra, and antoine bordes. "memory
   networks." arxiv preprint arxiv:1410.3916 (2014). [112][pdf]                   

   [42] sukhbaatar, sainbayar, jason weston, and rob fergus. "end-to-end
   memory networks." advances in neural information processing systems.
   2015. [113][pdf]                         

   [43] vinyals, oriol, meire fortunato, and navdeep jaitly. "pointer
   networks." advances in neural information processing systems. 2015.
   [114][pdf]                         

   [44] graves, alex, et al. "hybrid computing using a neural network with
   dynamic external memory." nature (2016). [115][pdf] (milestone,combine
   above papers' ideas)                               

2.6 deep id23

   [45] mnih, volodymyr, et al. "playing atari with deep reinforcement
   learning." arxiv preprint arxiv:1312.5602 (2013). [116][pdf]) (first
   paper named deep id23)                         

   [46] mnih, volodymyr, et al. "human-level control through deep
   id23." nature 518.7540 (2015): 529-533. [117][pdf]
   (milestone)                               

   [47] wang, ziyu, nando de freitas, and marc lanctot. "dueling network
   architectures for deep id23." arxiv preprint
   arxiv:1511.06581 (2015). [118][pdf] (iclr best paper,great idea)
                           

   [48] mnih, volodymyr, et al. "asynchronous methods for deep
   id23." arxiv preprint arxiv:1602.01783 (2016).
   [119][pdf] (state-of-the-art method)                               

   [49] lillicrap, timothy p., et al. "continuous control with deep
   id23." arxiv preprint arxiv:1509.02971 (2015).
   [120][pdf] (ddpg)                         

   [50] gu, shixiang, et al. "continuous deep id24 with model-based
   acceleration." arxiv preprint arxiv:1603.00748 (2016). [121][pdf] (naf)
                           

   [51] schulman, john, et al. "trust region policy optimization." corr,
   abs/1502.05477 (2015). [122][pdf] (trpo)                         

   [52] silver, david, et al. "mastering the game of go with deep neural
   networks and tree search." nature 529.7587 (2016): 484-489. [123][pdf]
   (alphago)                               

2.7 deep id21 / lifelong learning / especially for rl

   [53] bengio, yoshua. "deep learning of representations for unsupervised
   and id21." icml unsupervised and id21 27
   (2012): 17-36. [124][pdf] (a tutorial)                   

   [54] silver, daniel l., qiang yang, and lianghao li. "lifelong machine
   learning systems: beyond learning algorithms." aaai spring symposium:
   lifelong machine learning. 2013. [125][pdf] (a brief discussion about
   lifelong learning)                   

   [55] hinton, geoffrey, oriol vinyals, and jeff dean. "distilling the
   knowledge in a neural network." arxiv preprint arxiv:1503.02531 (2015).
   [126][pdf] (godfather's work)                         

   [56] rusu, andrei a., et al. "policy distillation." arxiv preprint
   arxiv:1511.06295 (2015). [127][pdf] (rl domain)                   

   [57] parisotto, emilio, jimmy lei ba, and ruslan salakhutdinov.
   "actor-mimic: deep multitask and transfer id23."
   arxiv preprint arxiv:1511.06342 (2015). [128][pdf] (rl domain)                   

   [58] rusu, andrei a., et al. "progressive neural networks." arxiv
   preprint arxiv:1606.04671 (2016). [129][pdf] (outstanding work, a novel
   idea)                               

2.8 one shot deep learning

   [59] lake, brenden m., ruslan salakhutdinov, and joshua b. tenenbaum.
   "human-level concept learning through probabilistic program induction."
   science 350.6266 (2015): 1332-1338. [130][pdf] (no deep learning,but
   worth reading)                               

   [60] koch, gregory, richard zemel, and ruslan salakhutdinov. "siamese
   neural networks for one-shot image recognition."(2015) [131][pdf]
                     

   [61] santoro, adam, et al. "id62 with memory-augmented
   neural networks." arxiv preprint arxiv:1605.06065 (2016). [132][pdf] (a
   basic step to one shot learning)                         

   [62] vinyals, oriol, et al. "matching networks for one shot learning."
   arxiv preprint arxiv:1606.04080 (2016). [133][pdf]                   

   [63] hariharan, bharath, and ross girshick. "low-shot visual object
   recognition." arxiv preprint arxiv:1606.02819 (2016). [134][pdf] (a
   step to large data)                         

3 applications

3.1 nlp(natural language processing)

   [1] antoine bordes, et al. "joint learning of words and meaning
   representations for open-text id29." aistats(2012)
   [135][pdf]                         

   [2] mikolov, et al. "distributed representations of words and phrases
   and their compositionality." anips(2013): 3111-3119 [136][pdf]
   (id97)                   

   [3] sutskever, et al. "   sequence to sequence learning with neural
   networks." anips(2014) [137][pdf]                   

   [4] ankit kumar, et al. "   ask me anything: dynamic memory networks for
   natural language processing." arxiv preprint arxiv:1506.07285(2015)
   [138][pdf]                         

   [5] yoon kim, et al. "character-aware neural language models."
   nips(2015) arxiv preprint arxiv:1508.06615(2015) [139][pdf]                         

   [6] jason weston, et al. "towards ai-complete id53: a set
   of prerequisite toy tasks." arxiv preprint arxiv:1502.05698(2015)
   [140][pdf] (babi tasks)                   

   [7] karl moritz hermann, et al. "teaching machines to read and
   comprehend." arxiv preprint arxiv:1506.03340(2015) [141][pdf]
   (id98/dailymail cloze style questions)             

   [8] alexis conneau, et al. "very deep convolutional networks for
   natural language processing." arxiv preprint arxiv:1606.01781(2016)
   [142][pdf] (state-of-the-art in text classification)                   

   [9] armand joulin, et al. "bag of tricks for efficient text
   classification." arxiv preprint arxiv:1607.01759(2016) [143][pdf]
   (slightly worse than state-of-the-art, but a lot faster)                   

3.2 id164

   [1] szegedy, christian, alexander toshev, and dumitru erhan. "deep
   neural networks for id164." advances in neural information
   processing systems. 2013. [144][pdf]                   

   [2] girshick, ross, et al. "rich feature hierarchies for accurate
   id164 and semantic segmentation." proceedings of the ieee
   conference on id161 and pattern recognition. 2014. [145][pdf]
   (rid98)                               

   [3] he, kaiming, et al. "spatial pyramid pooling in deep convolutional
   networks for visual recognition." european conference on computer
   vision. springer international publishing, 2014. [146][pdf] (sppnet)
                           

   [4] girshick, ross. "fast r-id98." proceedings of the ieee international
   conference on id161. 2015. [147][pdf]                         

   [5] ren, shaoqing, et al. "faster r-id98: towards real-time object
   detection with region proposal networks." advances in neural
   information processing systems. 2015. [148][pdf]                         

   [6] redmon, joseph, et al. "you only look once: unified, real-time
   id164." arxiv preprint arxiv:1506.02640 (2015). [149][pdf]
   (yolo,oustanding work, really practical)                               

   [7] liu, wei, et al. "ssd: single shot multibox detector." arxiv
   preprint arxiv:1512.02325 (2015). [150][pdf]                   

   [8] dai, jifeng, et al. "r-fcn: id164 via region-based fully
   convolutional networks." arxiv preprint arxiv:1605.06409 (2016).
   [151][pdf]                         

   [9] he, gkioxari, et al. "mask r-id98" arxiv preprint arxiv:1703.06870
   (2017). [152][pdf]                         

3.3 visual tracking

   [1] wang, naiyan, and dit-yan yeung. "learning a deep compact image
   representation for visual tracking." advances in neural information
   processing systems. 2013. [153][pdf] (first paper to do visual tracking
   using deep learning,dlt tracker)                   

   [2] wang, naiyan, et al. "transferring rich feature hierarchies for
   robust visual tracking." arxiv preprint arxiv:1501.04587 (2015).
   [154][pdf] (so-dlt)                         

   [3] wang, lijun, et al. "visual tracking with fully convolutional
   networks." proceedings of the ieee international conference on computer
   vision. 2015. [155][pdf] (fcnt)                         

   [4] held, david, sebastian thrun, and silvio savarese. "learning to
   track at 100 fps with deep regression networks." arxiv preprint
   arxiv:1604.01802 (2016). [156][pdf] (goturn,really fast as a deep
   learning method,but still far behind un-deep-learning methods)                         

   [5] bertinetto, luca, et al. "fully-convolutional siamese networks for
   object tracking." arxiv preprint arxiv:1606.09549 (2016). [157][pdf]
   (siamesefc,new state-of-the-art for real-time object tracking)                         

   [6] martin danelljan, andreas robinson, fahad khan, michael felsberg.
   "beyond correlation filters: learning continuous convolution operators
   for visual tracking." eccv (2016) [158][pdf] (c-cot)                         

   [7] nam, hyeonseob, mooyeol baek, and bohyung han. "modeling and
   propagating id98s in a tree structure for visual tracking." arxiv
   preprint arxiv:1608.07242 (2016). [159][pdf] (vot2016 winner,tid98)
                           

3.4 image caption

   [1] farhadi,ali,etal. "every picture tells a story: generating
   sentences from images". in id161eccv 2010. springer berlin
   heidelberg:15-29, 2010. [160][pdf]                   

   [2] kulkarni, girish, et al. "baby talk: understanding and generating
   image descriptions". in proceedings of the 24th cvpr, 2011.
   [161][pdf]                        

   [3] vinyals, oriol, et al. "show and tell: a neural image caption
   generator". in arxiv preprint arxiv:1411.4555, 2014. [162][pdf]                  

   [4] donahue, jeff, et al. "long-term recurrent convolutional networks
   for visual recognition and description". in arxiv preprint
   arxiv:1411.4389 ,2014. [163][pdf]

   [5] karpathy, andrej, and li fei-fei. "deep visual-semantic alignments
   for generating image descriptions". in arxiv preprint arxiv:1412.2306,
   2014. [164][pdf]                              

   [6] karpathy, andrej, armand joulin, and fei fei f. li. "deep fragment
   embeddings for bidirectional image sentence mapping". in advances in
   neural information processing systems, 2014. [165][pdf]                        

   [7] fang, hao, et al. "from captions to visual concepts and back". in
   arxiv preprint arxiv:1411.4952, 2014. [166][pdf]                              

   [8] chen, xinlei, and c. lawrence zitnick. "learning a recurrent visual
   representation for image id134". in arxiv preprint
   arxiv:1411.5654, 2014. [167][pdf]                        

   [9] mao, junhua, et al. "deep captioning with multimodal recurrent
   neural networks (m-id56)". in arxiv preprint arxiv:1412.6632, 2014.
   [168][pdf]                  

   [10] xu, kelvin, et al. "show, attend and tell: neural image caption
   generation with visual attention". in arxiv preprint arxiv:1502.03044,
   2015. [169][pdf]                              

3.5 machine translation

     some milestone papers are listed in id56 / seq-to-seq topic.

   [1] luong, minh-thang, et al. "addressing the rare word problem in
   id4." arxiv preprint arxiv:1410.8206 (2014).
   [170][pdf]                         

   [2] sennrich, et al. "id4 of rare words with
   subword units". in arxiv preprint arxiv:1508.07909, 2015.
   [171][pdf]                  

   [3] luong, minh-thang, hieu pham, and christopher d. manning.
   "effective approaches to attention-based id4."
   arxiv preprint arxiv:1508.04025 (2015). [172][pdf]                         

   [4] chung, et al. "a character-level decoder without explicit
   segmentation for id4". in arxiv preprint
   arxiv:1603.06147, 2016. [173][pdf]            

   [5] lee, et al. "fully character-level id4
   without explicit segmentation". in arxiv preprint arxiv:1610.03017,
   2016. [174][pdf]                              

   [6] wu, schuster, chen, le, et al. "google's id4
   system: bridging the gap between human and machine translation". in
   arxiv preprint arxiv:1609.08144v2, 2016. [175][pdf] (milestone)
                           

3.6 robotics

   [1] koutn  k, jan, et al. "evolving large-scale neural networks for
   vision-based id23." proceedings of the 15th annual
   conference on genetic and evolutionary computation. acm, 2013.
   [176][pdf]                   

   [2] levine, sergey, et al. "end-to-end training of deep visuomotor
   policies." journal of machine learning research 17.39 (2016): 1-40.
   [177][pdf]                               

   [3] pinto, lerrel, and abhinav gupta. "supersizing self-supervision:
   learning to grasp from 50k tries and 700 robot hours." arxiv preprint
   arxiv:1509.06825 (2015). [178][pdf]                   

   [4] levine, sergey, et al. "learning hand-eye coordination for robotic
   grasping with deep learning and large-scale data collection." arxiv
   preprint arxiv:1603.02199 (2016). [179][pdf]                         

   [5] zhu, yuke, et al. "target-driven visual navigation in indoor scenes
   using deep id23." arxiv preprint arxiv:1609.05143
   (2016). [180][pdf]                         

   [6] yahya, ali, et al. "collective robot id23 with
   distributed asynchronous guided policy search." arxiv preprint
   arxiv:1610.00673 (2016). [181][pdf]                         

   [7] gu, shixiang, et al. "deep id23 for robotic
   manipulation." arxiv preprint arxiv:1610.00633 (2016). [182][pdf]
                           

   [8] a rusu, m vecerik, thomas roth  rl, n heess, r pascanu, r
   hadsell."sim-to-real robot learning from pixels with progressive nets."
   arxiv preprint arxiv:1610.04286 (2016). [183][pdf]                         

   [9] mirowski, piotr, et al. "learning to navigate in complex
   environments." arxiv preprint arxiv:1611.03673 (2016). [184][pdf]
                           

3.7 art

   [1] mordvintsev, alexander; olah, christopher; tyka, mike (2015).
   "inceptionism: going deeper into neural networks". google research.
   [185][html] (deep dream)                         

   [2] gatys, leon a., alexander s. ecker, and matthias bethge. "a neural
   algorithm of artistic style." arxiv preprint arxiv:1508.06576 (2015).
   [186][pdf] (outstanding work, most successful method currently)
                                 

   [3] zhu, jun-yan, et al. "generative visual manipulation on the natural
   image manifold." european conference on id161. springer
   international publishing, 2016. [187][pdf] (igan)                         

   [4] champandard, alex j. "semantic style transfer and turning two-bit
   doodles into fine artworks." arxiv preprint arxiv:1603.01768 (2016).
   [188][pdf] (neural doodle)                         

   [5] zhang, richard, phillip isola, and alexei a. efros. "colorful image
   colorization." arxiv preprint arxiv:1603.08511 (2016). [189][pdf]
                           

   [6] johnson, justin, alexandre alahi, and li fei-fei. "perceptual
   losses for real-time style transfer and super-resolution." arxiv
   preprint arxiv:1603.08155 (2016). [190][pdf]                         

   [7] vincent dumoulin, jonathon shlens and manjunath kudlur. "a learned
   representation for artistic style." arxiv preprint arxiv:1610.07629
   (2016). [191][pdf]                         

   [8] gatys, leon and ecker, et al."controlling perceptual factors in
   neural style transfer." arxiv preprint arxiv:1611.07865 (2016).
   [192][pdf] (control style transfer over spatial location,colour
   information and across spatial scale)                        

   [9] ulyanov, dmitry and lebedev, vadim, et al. "texture networks:
   feed-forward synthesis of textures and stylized images." arxiv preprint
   arxiv:1603.03417(2016). [193][pdf] (texture generation and style
   transfer)                         

3.8 object segmentation

   [1] j. long, e. shelhamer, and t. darrell,    fully convolutional
   networks for semantic segmentation.    in cvpr, 2015. [194][pdf]
                                 

   [2] l.-c. chen, g. papandreou, i. kokkinos, k. murphy, and a. l.
   yuille. "semantic image segmentation with deep convolutional nets and
   fully connected crfs." in iclr, 2015. [195][pdf]                               

   [3] pinheiro, p.o., collobert, r., dollar, p. "learning to segment
   object candidates." in: nips. 2015. [196][pdf]                         

   [4] dai, j., he, k., sun, j. "instance-aware semantic segmentation via
   multi-task network cascades." in cvpr. 2016 [197][pdf]                   

   [5] dai, j., he, k., sun, j. "instance-sensitive fully convolutional
   networks." arxiv preprint arxiv:1603.08678 (2016). [198][pdf]                   

     *    2019 github, inc.
     * [199]terms
     * [200]privacy
     * [201]security
     * [202]status
     * [203]help

     * [204]contact github
     * [205]pricing
     * [206]api
     * [207]training
     * [208]blog
     * [209]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [210]reload to refresh your
   session. you signed out in another tab or window. [211]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/floodsung/deep-learning-papers-reading-roadmap/commits/master.atom
   3. https://github.com/floodsung/deep-learning-papers-reading-roadmap#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/floodsung/deep-learning-papers-reading-roadmap
  32. https://github.com/join
  33. https://github.com/login?return_to=/floodsung/deep-learning-papers-reading-roadmap
  34. https://github.com/floodsung/deep-learning-papers-reading-roadmap/watchers
  35. https://github.com/login?return_to=/floodsung/deep-learning-papers-reading-roadmap
  36. https://github.com/floodsung/deep-learning-papers-reading-roadmap/stargazers
  37. https://github.com/login?return_to=/floodsung/deep-learning-papers-reading-roadmap
  38. https://github.com/floodsung/deep-learning-papers-reading-roadmap/network/members
  39. https://github.com/floodsung
  40. https://github.com/floodsung/deep-learning-papers-reading-roadmap
  41. https://github.com/floodsung/deep-learning-papers-reading-roadmap
  42. https://github.com/floodsung/deep-learning-papers-reading-roadmap/issues
  43. https://github.com/floodsung/deep-learning-papers-reading-roadmap/pulls
  44. https://github.com/floodsung/deep-learning-papers-reading-roadmap/projects
  45. https://github.com/floodsung/deep-learning-papers-reading-roadmap/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/deep-learning
  48. https://github.com/floodsung/deep-learning-papers-reading-roadmap/commits/master
  49. https://github.com/floodsung/deep-learning-papers-reading-roadmap/branches
  50. https://github.com/floodsung/deep-learning-papers-reading-roadmap/releases
  51. https://github.com/floodsung/deep-learning-papers-reading-roadmap/graphs/contributors
  52. https://github.com/floodsung/deep-learning-papers-reading-roadmap/search?l=python
  53. https://github.com/floodsung/deep-learning-papers-reading-roadmap/find/master
  54. https://github.com/floodsung/deep-learning-papers-reading-roadmap/archive/master.zip
  55. https://github.com/login?return_to=https://github.com/floodsung/deep-learning-papers-reading-roadmap
  56. https://github.com/join?return_to=/floodsung/deep-learning-papers-reading-roadmap
  57. https://desktop.github.com/
  58. https://desktop.github.com/
  59. https://developer.apple.com/xcode/
  60. https://visualstudio.github.com/
  61. https://github.com/floodsung
  62. https://github.com/floodsung/deep-learning-papers-reading-roadmap/commits?author=floodsung
  63. https://github.com/floodsung/deep-learning-papers-reading-roadmap/commit/98e3b45f2a03eff7c09b6fc603862466e0e2e56b
  64. https://github.com/floodsung/deep-learning-papers-reading-roadmap/pull/76
  65. https://github.com/floodsung/deep-learning-papers-reading-roadmap/commit/98e3b45f2a03eff7c09b6fc603862466e0e2e56b
  66. https://github.com/floodsung/deep-learning-papers-reading-roadmap/commit/98e3b45f2a03eff7c09b6fc603862466e0e2e56b
  67. https://github.com/floodsung/deep-learning-papers-reading-roadmap/tree/98e3b45f2a03eff7c09b6fc603862466e0e2e56b
  68. https://github.com/floodsung/deep-learning-papers-reading-roadmap/blob/master/readme.md
  69. https://github.com/floodsung/deep-learning-papers-reading-roadmap/blob/master/download.py
  70. https://github.com/floodsung/deep-learning-papers-reading-roadmap/blob/master/requirements.txt
  71. http://www.deeplearningbook.org/
  72. http://www.cs.toronto.edu/~hinton/absps/naturedeepreview.pdf
  73. http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf
  74. http://www.cs.toronto.edu/~hinton/science.pdf
  75. http://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks.pdf
  76. https://arxiv.org/pdf/1409.1556.pdf
  77. http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/szegedy_going_deeper_with_2015_cvpr_paper.pdf
  78. https://arxiv.org/pdf/1512.03385.pdf
  79. http://cs224d.stanford.edu/papers/maas_paper.pdf
  80. http://arxiv.org/pdf/1303.5778.pdf
  81. http://www.jmlr.org/proceedings/papers/v32/graves14.pdf
  82. http://arxiv.org/pdf/1507.06947
  83. https://arxiv.org/pdf/1512.02595.pdf
  84. https://arxiv.org/pdf/1610.05256v1
  85. https://arxiv.org/pdf/1207.0580.pdf
  86. https://www.cs.toronto.edu/~hinton/absps/jmlrdropout.pdf
  87. http://arxiv.org/pdf/1502.03167
  88. https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote
  89. https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf
  90. https://arxiv.org/pdf/1608.05343
  91. https://arxiv.org/abs/1511.05641
  92. https://arxiv.org/abs/1603.01670
  93. http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf
  94. http://arxiv.org/pdf/1412.6980
  95. https://arxiv.org/pdf/1606.04474
  96. https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf
  97. http://arxiv.org/pdf/1602.07360
  98. http://arxiv.org/pdf/1112.6209.pdf&embed
  99. http://arxiv.org/pdf/1312.6114
 100. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
 101. http://arxiv.org/pdf/1511.06434
 102. http://jmlr.org/proceedings/papers/v37/gregor15.pdf
 103. http://arxiv.org/pdf/1601.06759
 104. https://arxiv.org/pdf/1606.05328
 105. http://arxiv.org/pdf/1308.0850
 106. http://arxiv.org/pdf/1406.1078
 107. https://arxiv.org/pdf/1409.3215.pdf
 108. https://arxiv.org/pdf/1409.0473v7.pdf
 109. http://arxiv.org/pdf/1506.05869.pdf (http://arxiv.org/pdf/1506.05869.pdf)
 110. http://arxiv.org/pdf/1410.5401.pdf
 111. https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf
 112. http://arxiv.org/pdf/1410.3916
 113. http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf
 114. http://papers.nips.cc/paper/5866-pointer-networks.pdf
 115. https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf
 116. http://arxiv.org/pdf/1312.5602.pdf
 117. https://storage.googleapis.com/deepmind-data/assets/papers/deepmindnature14236paper.pdf
 118. http://arxiv.org/pdf/1511.06581
 119. http://arxiv.org/pdf/1602.01783
 120. http://arxiv.org/pdf/1509.02971
 121. http://arxiv.org/pdf/1603.00748
 122. http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf
 123. http://willamette.edu/~levenick/cs448/gonature.pdf
 124. http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf
 125. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&rep=rep1&type=pdf
 126. http://arxiv.org/pdf/1503.02531
 127. http://arxiv.org/pdf/1511.06295
 128. http://arxiv.org/pdf/1511.06342
 129. https://arxiv.org/pdf/1606.04671
 130. http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf
 131. http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf
 132. http://arxiv.org/pdf/1605.06065
 133. https://arxiv.org/pdf/1606.04080
 134. http://arxiv.org/pdf/1606.02819
 135. https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en:publi&cache=cache&media=en:bordes12aistats.pdf
 136. http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
 137. http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
 138. https://arxiv.org/abs/1506.07285
 139. https://arxiv.org/abs/1508.06615
 140. https://arxiv.org/abs/1502.05698
 141. https://arxiv.org/abs/1506.03340
 142. https://arxiv.org/abs/1606.01781
 143. https://arxiv.org/abs/1607.01759
 144. http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf
 145. http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/girshick_rich_feature_hierarchies_2014_cvpr_paper.pdf
 146. http://arxiv.org/pdf/1406.4729
 147. https://pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf
 148. https://arxiv.org/pdf/1506.01497.pdf
 149. http://homes.cs.washington.edu/~ali/papers/yolo.pdf
 150. http://arxiv.org/pdf/1512.02325
 151. https://arxiv.org/abs/1605.06409
 152. https://arxiv.org/abs/1703.06870
 153. http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf
 154. http://arxiv.org/pdf/1501.04587
 155. http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/wang_visual_tracking_with_iccv_2015_paper.pdf
 156. http://arxiv.org/pdf/1604.01802
 157. https://arxiv.org/pdf/1606.09549
 158. http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/c-cot_eccv16.pdf
 159. https://arxiv.org/pdf/1608.07242
 160. https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf
 161. http://tamaraberg.com/papers/generation_cvpr11.pdf
 162. https://arxiv.org/pdf/1411.4555.pdf
 163. https://arxiv.org/pdf/1411.4389.pdf
 164. https://cs.stanford.edu/people/karpathy/cvpr2015.pdf
 165. https://arxiv.org/pdf/1406.5679v1.pdf
 166. https://arxiv.org/pdf/1411.4952v3.pdf
 167. https://arxiv.org/pdf/1411.5654v1.pdf
 168. https://arxiv.org/pdf/1412.6632v5.pdf
 169. https://arxiv.org/pdf/1502.03044v3.pdf
 170. http://arxiv.org/pdf/1410.8206
 171. https://arxiv.org/pdf/1508.07909.pdf
 172. http://arxiv.org/pdf/1508.04025
 173. https://arxiv.org/pdf/1603.06147.pdf
 174. https://arxiv.org/pdf/1610.03017.pdf
 175. https://arxiv.org/pdf/1609.08144v2.pdf
 176. http://repository.supsi.ch/4550/1/koutnik2013gecco.pdf
 177. http://www.jmlr.org/papers/volume17/15-522/15-522.pdf
 178. http://arxiv.org/pdf/1509.06825
 179. http://arxiv.org/pdf/1603.02199
 180. https://arxiv.org/pdf/1609.05143
 181. https://arxiv.org/pdf/1610.00673
 182. https://arxiv.org/pdf/1610.00633
 183. https://arxiv.org/pdf/1610.04286.pdf
 184. https://arxiv.org/pdf/1611.03673
 185. https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html
 186. http://arxiv.org/pdf/1508.06576
 187. https://arxiv.org/pdf/1609.03552
 188. http://arxiv.org/pdf/1603.01768
 189. http://arxiv.org/pdf/1603.08511
 190. https://arxiv.org/pdf/1603.08155.pdf
 191. https://arxiv.org/pdf/1610.07629v1.pdf
 192. https://arxiv.org/pdf/1611.07865.pdf
 193. http://arxiv.org/abs/1603.03417
 194. https://arxiv.org/pdf/1411.4038v2.pdf
 195. https://arxiv.org/pdf/1606.00915v1.pdf
 196. https://arxiv.org/pdf/1506.06204v2.pdf
 197. https://arxiv.org/pdf/1512.04412v1.pdf
 198. https://arxiv.org/pdf/1603.08678v1.pdf
 199. https://github.com/site/terms
 200. https://github.com/site/privacy
 201. https://github.com/security
 202. https://githubstatus.com/
 203. https://help.github.com/
 204. https://github.com/contact
 205. https://github.com/pricing
 206. https://developer.github.com/
 207. https://training.github.com/
 208. https://github.blog/
 209. https://github.com/about
 210. https://github.com/floodsung/deep-learning-papers-reading-roadmap
 211. https://github.com/floodsung/deep-learning-papers-reading-roadmap

   hidden links:
 213. https://github.com/
 214. https://github.com/floodsung/deep-learning-papers-reading-roadmap
 215. https://github.com/floodsung/deep-learning-papers-reading-roadmap
 216. https://github.com/floodsung/deep-learning-papers-reading-roadmap
 217. https://help.github.com/articles/which-remote-url-should-i-use
 218. https://github.com/floodsung/deep-learning-papers-reading-roadmap#deep-learning-papers-reading-roadmap
 219. https://github.com/floodsung/deep-learning-papers-reading-roadmap#1-deep-learning-history-and-basics
 220. https://github.com/floodsung/deep-learning-papers-reading-roadmap#10-book
 221. https://github.com/floodsung/deep-learning-papers-reading-roadmap#11-survey
 222. https://github.com/floodsung/deep-learning-papers-reading-roadmap#12-deep-belief-networkdbnmilestone-of-deep-learning-eve
 223. https://github.com/floodsung/deep-learning-papers-reading-roadmap#13-id163-evolutiondeep-learning-broke-out-from-here
 224. https://github.com/floodsung/deep-learning-papers-reading-roadmap#14-speech-recognition-evolution
 225. https://github.com/floodsung/deep-learning-papers-reading-roadmap#21-model
 226. https://github.com/floodsung/deep-learning-papers-reading-roadmap#22-optimization
 227. https://github.com/floodsung/deep-learning-papers-reading-roadmap#23-unsupervised-learning--deep-generative-model
 228. https://github.com/floodsung/deep-learning-papers-reading-roadmap#24-id56--sequence-to-sequence-model
 229. https://github.com/floodsung/deep-learning-papers-reading-roadmap#25-neural-turing-machine
 230. https://github.com/floodsung/deep-learning-papers-reading-roadmap#26-deep-reinforcement-learning
 231. https://github.com/floodsung/deep-learning-papers-reading-roadmap#27-deep-transfer-learning--lifelong-learning--especially-for-rl
 232. https://github.com/floodsung/deep-learning-papers-reading-roadmap#28-one-shot-deep-learning
 233. https://github.com/floodsung/deep-learning-papers-reading-roadmap#3-applications
 234. https://github.com/floodsung/deep-learning-papers-reading-roadmap#31-nlpnatural-language-processing
 235. https://github.com/floodsung/deep-learning-papers-reading-roadmap#32-object-detection
 236. https://github.com/floodsung/deep-learning-papers-reading-roadmap#33-visual-tracking
 237. https://github.com/floodsung/deep-learning-papers-reading-roadmap#34-image-caption
 238. https://github.com/floodsung/deep-learning-papers-reading-roadmap#35-machine-translation
 239. https://github.com/floodsung/deep-learning-papers-reading-roadmap#36-robotics
 240. https://github.com/floodsung/deep-learning-papers-reading-roadmap#37-art
 241. https://github.com/floodsung/deep-learning-papers-reading-roadmap#38-object-segmentation
 242. https://github.com/
