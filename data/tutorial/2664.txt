   #[1]adit deshpande - cs undergrad at ucla ('19)

   [2][pic.jpg]

[3]adit deshpande

   cs undergrad at ucla ('19)

   [4]blog [5]about [6]github [7]projects [8]resume

deep learning research review week 2: id23

   [cover6th.png]

   this is the 2^nd installment of a new series called deep learning
   research review. every couple weeks or so, i   ll be summarizing and
   explaining research papers in specific subfields of deep learning. this
   week focuses on id23. [9]last time was generative
   adversarial networks icymi

introduction to id23

   3 categories of machine learning

                   before getting into the papers, let   s first talk about
   what id23 is. the field of machine learning can be
   separated into 3 main categories.
    1. supervised learning
    2. unsupervised learning
    3. id23

   [irl1.png]

   the first category, supervised learning, is the one you may be most
   familiar with. it relies on the idea of creating a function or model
   based on a set of training data, which contains inputs and their
   corresponding labels. convolutional neural networks are a great example
   of this, as the images are the inputs and the outputs are the
   classifications of the images (dog, cat, etc).

   unsupervised learning seeks to find some sort of structure within data
   through methods of cluster analysis. one of the most well-known ml
   id91 algorithms, id116, is an example of unsupervised learning.

   id23 is the task of learning what actions to take,
   given a certain situation/environment, so as to maximize a reward
   signal. the interesting difference between supervised and reinforcement
   learning is that this reward signal simply tells you whether the action
   (or input) that the agent takes is good or bad. it doesn   t tell you
   anything about what the best action is. contrast this to id98s where the
   corresponding label for each image input is a definite instruction of
   what the output should be for each input.  another unique component of
   rl is that an agent   s actions will affect the subsequent data it
   receives. for example, an agent   s action of moving left instead of
   right means that the agent will receive different input from the
   environment at the next time step. let   s look at an example to start
   off.

   the rl problem

                   so, let   s first think about what have in a
   id23 problem. let   s imagine a tiny robot in a small
   room. we haven   t programmed this robot to move or walk or take any
   action. it   s just standing there. this robot is our agent.
   [irl2.png]

   like we mentioned before, id23 is all about trying to
   understand the optimal way of making decisions/actions so that we
   maximize some reward r. this reward is a feedback signal that just
   indicates how well the agent is doing at a given time step. the action
   a that an agent takes at every time step is a function of both the
   reward (signal telling the agent how well it   s currently doing) and the
   state s, which is a description of the environment the agent is in. the
   mapping from environment states to actions is called our policy p. the
   policy basically defines the agent   s way of behaving at a certain time,
   given a certain situation. now, we also have a value function v which
   is a measure of how good each position is. this is different from the
   reward in that the reward signal indicates what is good in the
   immediate sense, while the value function is more indicative of how
   good it is to be in this state/position in the long run. finally, we
   also have a model m which is the agent   s representation of the
   environment. this is the agent   s model of how it thinks that the
   environment is going to behave.
   [irl3.png]

   markov decision process

                   so, let   s now think back to our robot (the agent) in
   the small room. our reward function is dependent on what we want the
   agent to accomplish. let   s say that we want it to move to one of the
   corners of the room where it will receive a reward. the robot will get
   a +25 when it reaches this point, and will get a -1 for every time step
   it takes to get there. we basically want the robot to get the corner as
   fast as possible. the actions the agent can take are moving north,
   south, east, or west. the agent   s policy can be a simple one, where the
   behavior is that the agent will always move to the location with the
   higher value function. makes sense right? a position with a high value
   function = good to be in this position (with regards to long term
   reward).

   now, this whole rl environment can be described with a markov decision
   process. for those that haven   t heard the term before, an mdp is a
   framework for modeling an agent   s decision making. it contains a finite
   set of states (and value functions for those states), a finite set of
   actions, a policy, and a reward function. our value function can be
   split into 2 terms.
    1. state-value function v: the expected return from being in a state s
       and following a policy   . this return is calculated by looking at
       summation of the reward at each future time step (the gamma refers
       to a constant discount factor, which means that the reward at time
       step 10 is weighted a little less than the reward at time step 1).
       [irl4.png]
    2. action-value function q: the expected return from being in a state
       s, following a policy   , and taking an action a (equation will be
       same as above except that we have an additional condition that a[t]
       = a).

   now that we have all the components, what do we do with this mdp? well,
   we want to solve it, of course. by solving an mdp, you   ll be able to
   find the optimal behavior (policy) that maximizes the amount of reward
   the agent can expect to get from any state in the environment.

   solving the mdp

                   we can solve an mdp and get the optimum policy through
   the use of id145 and specifically through the use of
   policy iteration (there is another technique called value iteration,
   but won   t go into that right now). the idea is that we take some
   initial policy   [1] and evaluate the state value function for that
   policy. the way we do this is through the bellman expectation equation.
   [irl5.png]

   this equation basically says that our value function, given that we   re
   following policy   , can be decomposed into the expected return sum of
   the immediate reward r[t+1] and the value function of the successor
   state s[t+1]. if you think about it closely, this is equivalent to the
   value function definition we used in the previous section. using this
   equation is our policy evaluation component. in order to get a better
   policy, we use a policy improvement step where we simply act greedily
   with respect to the value function. in other words, the agent takes the
   action that maximizes value.
   [irl6.png]

   now, in order to get the optimal policy, we repeat these 2 steps, one
   after the other, until we converge to optimal policy   ^*.

   when you   re not given an mdp

                   policy iteration is great and all, but it only works
   when we have a given mdp. the mdp essentially tells you how the
   environment works, which realistically is not going to be given in real
   world scenarios. when not given an mdp, we use model free methods that
   go directly from the experience/interactions of the agents and the
   environment to the value functions and policies. we   re going to be
   doing the same steps of policy evaluation and policy improvement, just
   without the information given by the mdp.

   the way we do this is instead of improving our policy by optimizing
   over the state value function, we   re going to optimize over the action
   value function q. remember how we decomposed the state value function
   into the sum of immediate reward and value function of the successor
   state? well, we can do the same with our q function.
   [irl7.png]

   now, we   re going to go through the same process of policy evaluation
   and policy improvement, except we replace our state value function v
   with our action value function q. now, i   m going to skip over the
   details of what changes with the evaluation/improvement steps. to
   understand mdp free evaluation and improvement methods, topics such as
   monte carlo learning, temporal difference learning, and sarsa would
   require whole blogs just themselves (if you are interested, though,
   please take a listen to david silver   s [10]lecture 4 and [11]lecture
   5). right now, however, i   m going to jump ahead to value function
   approximation and the methods discussed in the alphago and atari
   papers, and hopefully that should give a taste of modern rl techniques.
   the main takeaway is that we want to find the optimal policy   ^* that
   maximizes our action value function q.

   value function approximation

                   so, if you think about everything we   ve learned up
   until this point, we   ve treated our problem in a relatively simplistic
   way. look at the above q equation. we   re taking in a specific state s
   and action a, and then computing a number that basically tells us what
   the expected return is. now let   s imagine that our agent moves 1
   millimeter to the right. this means we have a whole new state s   , and
   now we   re going to have to compute a q value for that. in real world rl
   problems, there are millions and millions of states so it   s important
   that our value functions understand generalization in that we don   t
   have to store a completely separate value function for every possible
   state. the solution is to use a q value function approximation that is
   able to generalize to unknown states.

   so, what we want is some function, let   s call is qhat, that gives a
   rough approximation of the q value given some state s and some action
   a.
   [irl8.png]

   this function is going to take in s, a, and a good old weight vector w
   (once you see that w, you already know we   re bringing in some gradient
   descent ). it is going to compute the dot product between x (which is
   just a feature vector that represents s and a) and w. the way we   re
   going to improve this function is by calculating the loss between the
   true q value (let   s just assume that it   s given to us for now) and the
   output of the approximate function.
   [irl9.png]

   after we compute the loss, we use id119 to find the minimum
   value, at which point we will have our optimal w vector. this idea of
   function approximation is going to be very key when taking a look at
   the papers a little later.

   just one more thing

                   before getting to the papers, just wanted to touch on
   one last thing. an interesting discussion with the topic of
   id23 is that of exploration vs exploitation.
   exploitation is the agent   s process of taking what it already knows,
   and then making the actions that it knows will produce the maximum
   reward. this sounds great, right? the agent will always be making the
   best action based on its current knowledge. however, there is a key
   phrase in that statement. current knowledge. if the agent hasn   t
   explored enough of the state space, it can   t possibly know whether it
   is really taking the best possible action. this idea of taking actions
   with the main purpose of exploring the state space is called
   exploration.

   this idea can be easily related to a real world example. let   s say you
   have a choice of what restaurant to eat at tonight. you (acting as the
   agent) know that you like mexican food, so in rl terms, going to a
   mexican restaurant will be the action that maximizes your reward, or
   happiness/satisfaction in this case. however, there is also a choice of
   italian food, which you   ve never had before. there   s a possibility that
   it could be better than mexican food, or could be a lot worse. this
   tradeoff between whether to exploit an agent   s past knowledge vs trying
   something new in hope of discovering a greater reward is one of the
   major challenges in id23 (and in our daily lives
   tbh).

   other resources for learning rl

                   phew. that was a lot of info. by no means, however, was
   that a comprehensive overview of the field. if you   d like a more
   in-depth overview of rl, i   d strongly recommend these resources.
     * david silver (from deepmind) id23 [12]video
       lectures
          + my [13]personal notes from the rl course
     * sutton and barto   s [14]id23 textbook (this is
       really the holy grail if you are determined to learn the ins and
       outs of this subfield)
     * andrej karpathy   s [15]blog post on rl (start with this one if you
       want to ease into rl and want to see a really well done practical
       example)
     * [16]uc berkeley cs 188 lectures 8-11
     * [17]open ai gym: when you feel comfortable with rl, try creating
       your own agents with this id23 toolkit that open
       ai created

[18]id25 for id23 (rl with atari games)

   [irl10.png]

   introduction

                   this paper was published by google deepmind in february
   of 2015 and graced the cover of nature, a world famous weekly journal
   of science. this was one of the first successful attempts at combining
   deep neural networks with id23 ([19]this was
   deepmind   s original paper). the paper showed that their system was able
   to play atari games at a level comparable to professional game testers
   across a set of 49 games. let   s take a look at how they did it.

   approach

                   okay, so remember where we left off in the intro
   tutorial at the beginning of the post? we had just described the main
   goal of having to optimize our action value function q. the folks at
   deepmind approached this task through a deep q-network, or a id25. this
   network is able to come up with successful policies that optimize q,
   all from inputs in the form of pixels from the game screen and the
   current score.

   network architecture

                   let   s take a closer look at what inputs this id25 will
   have. consider the game of breakout, and take 4 of the most recent
   frames in the current game. each of these frame originally starts as a
   210 x 160 x 3 image (because width and height are 210 and 160 pixels
   and it is a color image). then, some preprocessing takes place where
   the frames are scaled to 84 x 84 (not extremely important to know how
   this is done, but check page 6 for details). so, now we have an 84 x 84
   x 4 input volume.  this volume is going to get plugged into a
   convolutional neural network ([20]tutorial) where it will go through a
   series of conv and relu layers. the output of the network is an 18
   dimensional vector where each number is the q-value for each possible
   action the user can take (move up, down, left, etc).
   [irl11.png]

   okay, so let   s take a step back for a second and figure out how we   re
   going to train this network so that it will predict accurate q-values.
   let   s first remember what we   re trying to optimize.
   [irl12.png]

   this is the same form as the q function we saw earlier, except this one
   represents q* which is the max over all q   s. let   s examine how we   re
   going to get this q*. now, remember we just want an approximation for
   q*, which is where our function approximators are going to come in (our
   qhats). just keep that thought in your head while we switch gears a
   little.

   in order to find the best policy, we want to frame some sort of
   supervised learning problem where the predicted q function is compared
   to some expected one, and then is adjusted in the correct direction. in
   order to do that, we need a set of training examples. in our case, we
   are going to have to create a set of experiences that store the agent   s
   state, action, reward, and next state for every time step. let   s
   formalize that a bit more. we have a replay memory d which contains
   (s[t], a[t, ]r[t, ]s[t+1]) for a bunch of different time steps. this
   dataset gets built over time, as the agent interacts more with the
   environment. now, we   re going a take a random batch of this data (let   s
   say data for 64 time steps), compute the id168 for each of
   them, and then follow the gradient to improve our q function
   approximation.
   [irl13.png]

   so, as you can see, the id168 wants to optimize the mean
   squared error (mse) between the q network function approximation
   (q(s,a,theta)) and the id24 targets. let me quickly explain
   those.  this id24 target is the reward r plus the maximum q value
   (in the next time step) that you can get from some action a   .

   once the id168 is computed, the derivatives are taken w.r.t the
   theta values (or the w vector). these values are then updated so as to
   minimize the id168.

   conclusion

                   one of my favorite parts about the paper is this
   visualization it gives of the value function during certain points of
   the game.
   [irl14.png]

   as you remember, the value function is basically a metric for measuring
      how good it is to be in a particular situation   . if you look at #4,
   you can see, based on the trajectory of the ball and the location of
   the bricks, that we   re in for a lot of points and the high value
   function is quite representative of that.

   all 49 atari games used the same network architecture, algorithm, and
   hyperparameters which is an impressive testament to the robustness of
   such an approach to id23. the combination of deep
   networks and traditional id23 strategies, like q
   learning, proved to be a great breakthrough in setting the stage for      

[21]mastering alphago with rl

   [irl15.png]

   introduction

                   4-1. that   s the record deepmind   s rl agent had against
   one of the best go players in the world, lee sedol. in case you didn   t
   know, go is an abstract strategy game of capturing territory on a game
   board. it is considered to be one of the hardest games in the world for
   ai because of the incredible number of different game scenarios and
   moves. the paper begins with a comparison of go and common board games
   like chess and checkers. while those can be attacked with variations of
   tree search algorithms, go is a totally different animal because there
   are about 250^150 different sequences of moves in a game. it   s clear
   that id23 was needed, so let   s look into how alphago
   managed to beat the odds.

   approach

                   the basis behind alphago are the ideas of evaluation
   and selection. with any id23 problem (especially with
   a board game), you need a way of evaluating the environment, or the
   current board position. this is going to be our value network. you then
   need a way of selecting an action to take through a policy network.
   we   ve definitely had experience with both of these terms, value and
   policy.

   network architecture

                   let   s look at what inputs both of these networks are
   going to take. the board position is passed in as a 19 x 19 image that
   goes through a series of conv layers to construct a good representation
   of the current state. so let   s first look at our sl (supervised
   learning) policy network. this network is going to take in the image as
   input and then output a id203 distribution over all of the legal
   actions the agent can take. this network is pretrained (before the
   actual game) on 30 million different go board positions. each of these
   board positions is labeled with what an expert move would be in that
   situation. the team also trained a smaller, but faster rollout policy
   network.

   now, id98s can only do so much to predict the correct move you should
   take, given a representation of the current board. that   s when
   id23 comes in. we   re going to improve this policy
   network through a process called policy gradients. remember how in the
   last paper, we wanted to optimize our action value function q? well
   now, we   re going straight to optimizing our policy (policy gradients
   take a while to explain but david silver does a good job in [22]lecture
   7). from a high level, the policy is improved by simulating games
   between the current policy network and a previous iteration of the
   network. the reward signal is +1 for winning the game, -1 for losing,
   and so we can improve the network through the normal id119.
   [irl16.png]

   okay, so now we have a pretty good network that tells us the best
   action to play. the next step is having a value network that predicts
   the outcome a game which is at board position s and where both players
   are using policy p.
   [irl17.png]

   in order to get the optimal v*, we   ll use our good old function
   approximators with weights w. the weights are trained by the value
   network which are conditioned on state, outcome pairs (similar to what
   we saw in the last paper).

   now that we have these main two networks, our final step is to use a
   id169 to put everything together. the basic idea
   behind mcts is that it selects the best actions through lookahead
   search where each edge in the tree stores an action value q, a visit
   count, and a prior id203. from that info, the mcts algorithm will
   pick the best action a from the current state. this part of the system
   is a little less rl and more traditional ai so if you   d like more
   details, definitely check out the paper, which will do a much better
   job of summarizing.

   conclusion

   a computer system just beat the world   s best player at one of the
   hardest board games ever. who even needs a conclusion?

   iframe: [23]//giphy.com/embed/3o7qdsovfaco9b3mlo?html5=true

   [24]via giphy

   big thanks to david silver for the equations and the excellent lecture
   course on rl

   dueces.
   [25]sources

   [26]tweet

   written on november 16, 2016

   please enable javascript to view the [27]comments powered by disqus.

references

   visible links
   1. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
   2. https://adeshpande3.github.io/adeshpande3.github.io/
   3. https://adeshpande3.github.io/adeshpande3.github.io/
   4. https://adeshpande3.github.io/adeshpande3.github.io/
   5. https://adeshpande3.github.io/adeshpande3.github.io/about
   6. https://github.com/adeshpande3
   7. https://adeshpande3.github.io/adeshpande3.github.io/projects
   8. https://adeshpande3.github.io/adeshpande3.github.io/resume.pdf
   9. https://adeshpande3.github.io/adeshpande3.github.io/deep-learning-research-review-week-1-generative-adversarial-nets
  10. https://www.youtube.com/watch?v=pnhcvfgc_za
  11. https://www.youtube.com/watch?v=0g4j2k_ggc4
  12. https://www.youtube.com/watch?v=2pwv7govuf0&list=pl7-jpktc4r78-wczcqn5iqyuwhbz8foxt
  13. https://docs.google.com/document/d/1tjmydoxqzoq0jd0luifovq1hnamtwkfsaacck9dr7a8/edit?usp=sharing
  14. https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf
  15. http://karpathy.github.io/2016/05/31/rl/
  16. http://ai.berkeley.edu/lecture_videos.html
  17. https://gym.openai.com/
  18. http://www.nature.com/nature/journal/v518/n7540/pdf/nature14236.pdf
  19. https://www.cs.toronto.edu/~vmnih/docs/id25.pdf
  20. https://adeshpande3.github.io/adeshpande3.github.io/a-beginner's-guide-to-understanding-convolutional-neural-networks/
  21. http://www.nature.com/nature/journal/v529/n7587/pdf/nature16961.pdf
  22. https://www.youtube.com/watch?v=khzvxao4qxs
  23. https://giphy.com/embed/3o7qdsovfaco9b3mlo?html5=true
  24. http://giphy.com/gifs/obama-mic-drop-out-3o7qdsovfaco9b3mlo
  25. https://adeshpande3.github.io/assets/sources6.txt
  26. https://twitter.com/share
  27. http://disqus.com/?ref_noscript

   hidden links:
  29. mailto:adeshpande3@g.ucla.edu
  30. https://www.facebook.com/adit.deshpande.5
  31. https://github.com/adeshpande3
  32. https://instagram.com/thejugglinguy
  33. https://www.linkedin.com/in/aditdeshpande
  34. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
  35. https://www.twitter.com/aditdeshpande3
