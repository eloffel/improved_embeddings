   #[1]adit deshpande - cs undergrad at ucla ('19)

   [2][pic.jpg]

[3]adit deshpande

   cs undergrad at ucla ('19)

   [4]blog [5]about [6]github [7]projects [8]resume

the 9 deep learning papers you need to know about (understanding id98s part 3)

   [cover3rd.png]

introduction

   [9]link to part 1
   [10]link to part 2

                   in this post, we   ll go into summarizing a lot of the
   new and important developments in the field of id161 and
   convolutional neural networks. we   ll look at some of the most important
   papers that have been published over the last 5 years and discuss why
   they   re so important.  the first half of the list (alexnet to resnet)
   deals with advancements in general network architecture, while the
   second half is just a collection of interesting papers in other
   subareas.

[11]alexnet  (2012)

                   the one that started it all (though some may say that
   yann lecun   s [12]paper in 1998 was the real pioneering publication).
   this paper, titled    id163 classification with deep convolutional
   networks   , has been cited a total of 6,184 times and is widely regarded
   as one of the most influential publications in the field. alex
   krizhevsky, ilya sutskever, and geoffrey hinton created a    large, deep
   convolutional neural network    that was used to win the 2012 ilsvrc
   (id163 large-scale visual recognition challenge). for those that
   aren   t familiar, this competition can be thought of as the annual
   olympics of id161, where teams from across the world compete
   to see who has the best id161 model for tasks such as
   classification, localization, detection, and more. 2012 marked the
   first year where a id98 was used to achieve a top 5 test error rate of
   15.4% (top 5 error is the rate at which, given an image, the model does
   not output the correct label with its top 5 predictions). the next best
   entry achieved an error of 26.2%, which was an astounding improvement
   that pretty much shocked the id161 community. safe to say,
   id98s became household names in the competition from then on out.

                   in the paper, the group discussed the architecture of
   the network (which was called alexnet). they used a relatively simple
   layout, compared to modern architectures. the network was made up of 5
   conv layers, max-pooling layers, dropout layers, and 3 fully connected
   layers. the network they designed was used for classification with 1000
   possible categories.
   [alexnet.png]

   main points
     * trained the network on id163 data, which contained over 15
       million annotated images from a total of over 22,000 categories.
     * used relu for the nonlinearity functions (found to decrease
       training time as relus are several times faster than the
       conventional tanh function).
     * used data augmentation techniques that consisted of image
       translations, horizontal reflections, and patch extractions.
     * implemented dropout layers in order to combat the problem of
       overfitting to the training data.
     * trained the model using batch stochastic id119, with
       specific values for momentum and weight decay.
     * trained on two gtx 580 gpus for five to six days.

   why it   s important

                   the neural network developed by krizhevsky, sutskever,
   and hinton in 2012 was the coming out party for id98s in the computer
   vision community. this was the first time a model performed so well on
   a historically difficult id163 dataset. utilizing techniques that
   are still used today, such as data augmentation and dropout, this paper
   really illustrated the benefits of id98s and backed them up with record
   breaking performance in the competition.

[13]zf net (2013)

                   with alexnet stealing the show in 2012, there was a
   large increase in the number of id98 models submitted to ilsvrc 2013.
   the winner of the competition that year was a network built by matthew
   zeiler and rob fergus from nyu. named zf net, this model achieved an
   11.2% error rate. this architecture was more of a fine tuning to the
   previous alexnet structure, but still developed some very keys ideas
   about improving performance. another reason this was such a great paper
   is that the authors spent a good amount of time explaining a lot of the
   intuition behind convnets and showing how to visualize the filters and
   weights correctly.

                   in this paper titled    visualizing and understanding
   convolutional neural networks   , zeiler and fergus begin by discussing
   the idea that this renewed interest in id98s is due to the accessibility
   of large training sets and increased computational power with the usage
   of gpus. they also talk about the limited knowledge that researchers
   had on inner mechanisms of these models, saying that without this
   insight, the    development of better models is reduced to trial and
   error   . while we do currently have a better understanding than 3 years
   ago, this still remains an issue for a lot of researchers! the main
   contributions of this paper are details of a slightly modified alexnet
   model and a very interesting way of visualizing feature maps.
   [zfnet.png]

   main points
     * very similar architecture to alexnet, except for a few minor
       modifications.
     * alexnet trained on 15 million images, while zf net trained on only
       1.3 million images.
     * instead of using 11x11 sized filters in the first layer (which is
       what alexnet implemented), zf net used filters of size 7x7 and a
       decreased stride value. the reasoning behind this modification is
       that a smaller filter size in the first conv layer helps retain a
       lot of original pixel information in the input volume. a filtering
       of size 11x11 proved to be skipping a lot of relevant information,
       especially as this is the first conv layer.
     * as the network grows, we also see a rise in the number of filters
       used.
     * used relus for their id180, cross-id178 loss for
       the error function, and trained using batch stochastic gradient
       descent.
     * trained on a gtx 580 gpu for twelve days.
     * developed a visualization technique named deconvolutional network,
       which helps to examine different feature activations and their
       relation to the input space. called    deconvnet    because it maps
       features to pixels (the opposite of what a convolutional layer
       does).

   deconvnet

                   the basic idea behind how this works is that at every
   layer of the trained id98, you attach a    deconvnet    which has a path
   back to the image pixels. an input image is fed into the id98 and
   activations are computed at each level. this is the forward pass. now,
   let   s say we want to examine the activations of a certain feature in
   the 4^th conv layer. we would store the activations of this one feature
   map, but set all of the other activations in the layer to 0, and then
   pass this feature map as the input into the deconvnet. this deconvnet
   has the same filters as the original id98. this input then goes through
   a series of unpool (reverse maxpooling), rectify, and filter operations
   for each preceding layer until the input space is reached.

                   the reasoning behind this whole process is that we want
   to examine what type of structures excite a given feature map. let   s
   look at the visualizations of the first and second layers.
   [deconvnet.png]

   like we discussed in [14]part 1, the first layer of your convnet is
   always a low level feature detector that will detect simple edges or
   colors in this particular case. we can see that with the second layer,
   we have more circular features that are being detected. let   s look at
   layers 3, 4, and 5.
   [deconvnet2.png]

   these layers show a lot more of the higher level features such as dogs   
   faces or flowers. one thing to note is that as you may remember, after
   the first conv layer, we normally have a pooling layer that downsamples
   the image (for example, turns a 32x32x3 volume into a 16x16x3 volume).
   the effect this has is that the 2^nd layer has a broader scope of what
   it can see in the original image. for more info on deconvnet or the
   paper in general, check out zeiler himself [15]presenting on the topic.

   why it   s important

                   zf net was not only the winner of the competition in
   2013, but also provided great intuition as to the workings on id98s and
   illustrated more ways to improve performance. the visualization
   approach described helps not only to explain the inner workings of
   id98s, but also provides insight for improvements to network
   architectures. the fascinating deconv visualization approach and
   occlusion experiments make this one of my personal favorite papers.

[16]vgg net (2014)

                   simplicity and depth. that   s what a model created in
   2014 (weren   t the winners of ilsvrc 2014) best utilized with its 7.3%
   error rate. karen simonyan and andrew zisserman of the university of
   oxford created a 19 layer id98 that strictly used 3x3 filters with
   stride and pad of 1, along with 2x2 maxpooling layers with stride 2.
   simple enough right?
   [vggnet.png]

   main points
     * the use of only 3x3 sized filters is quite different from alexnet   s
       11x11 filters in the first layer and zf net   s 7x7 filters. the
       authors    reasoning is that the combination of two 3x3 conv layers
       has an effective receptive field of 5x5. this in turn simulates a
       larger filter while keeping the benefits of smaller filter sizes.
       one of the benefits is a decrease in the number of parameters.
       also, with two conv layers, we   re able to use two relu layers
       instead of one.
     * 3 conv layers back to back have an effective receptive field of
       7x7.
     * as the spatial size of the input volumes at each layer decrease
       (result of the conv and pool layers), the depth of the volumes
       increase due to the increased number of filters as you go down the
       network.
     * interesting to notice that the number of filters doubles after each
       maxpool layer. this reinforces the idea of shrinking spatial
       dimensions, but growing depth.
     * worked well on both image classification and localization tasks.
       the authors used a form of localization as regression (see page 10
       of the [17]paper for all details).
     * built model with the caffe toolbox.
     * used scale jittering as one data augmentation technique during
       training.
     * used relu layers after each conv layer and trained with batch
       id119.
     * trained on 4 nvidia titan black gpus for two to three weeks.

   why it   s important

                   vgg net is one of the most influential papers in my
   mind because it reinforced the notion that convolutional neural
   networks have to have a deep network of layers in order for this
   hierarchical representation of visual data to work. keep it deep. keep
   it simple.

[18]googlenet (2015)

                   you know that idea of simplicity in network
   architecture that we just talked about? well, google kind of threw that
   out the window with the introduction of the inception module. googlenet
   is a 22 layer id98 and was the winner of ilsvrc 2014 with a top 5 error
   rate of 6.7%. to my knowledge, this was one of the first id98
   architectures that really strayed from the general approach of simply
   stacking conv and pooling layers on top of each other in a sequential
   structure. the authors of the paper also emphasized that this new model
   places notable consideration on memory and power usage (important note
   that i sometimes forget too: stacking all of these layers and adding
   huge numbers of filters has a computational and memory cost, as well as
   an increased chance of overfitting).
   [googlenet.gif]

   [googlenet.png]

   inception module

                   when we first take a look at the structure of
   googlenet, we notice immediately that not everything is happening
   sequentially, as seen in previous architectures. we have pieces of the
   network that are happening in parallel.
   [googlenet2.png]

   this box is called an inception module. let   s take a closer look at
   what it   s made of.
   [googlenet3.png]

   the bottom green box is our input and the top one is the output of the
   model (turning this picture right 90 degrees would let you visualize
   the model in relation to the last picture which shows the full
   network). basically, at each layer of a traditional convnet, you have
   to make a choice of whether to have a pooling operation or a conv
   operation (there is also the choice of filter size). what an inception
   module allows you to do is perform all of these operations in parallel.
   in fact, this was exactly the    na  ve    idea that the authors came up
   with.
   [googlenet4.png]

   now, why doesn   t this work? it would lead to way too many outputs. we
   would end up with an extremely large depth channel for the output
   volume. the way that the authors address this is by adding 1x1 conv
   operations before the 3x3 and 5x5 layers. the 1x1 convolutions (or
   network in network layer) provide a method of id84.
   for example, let   s say you had an input volume of 100x100x60 (this
   isn   t necessarily the dimensions of the image, just the input to any
   layer of the network). applying 20 filters of 1x1 convolution would
   allow you to reduce the volume to 100x100x20. this means that the 3x3
   and 5x5 convolutions won   t have as large of a volume to deal with. this
   can be thought of as a    pooling of features    because we are reducing
   the depth of the volume, similar to how we reduce the dimensions of
   height and width with normal maxpooling layers.  another note is that
   these 1x1 conv layers are followed by relu units which definitely can   t
   hurt (see aaditya prakash   s [19]great post for more info on the
   effectiveness of 1x1 convolutions). check out this [20]video for a
   great visualization of the filter concatenation at the end.

                   you may be asking yourself    how does this architecture
   help?   . well, you have a module that consists of a network in network
   layer, a medium sized filter convolution, a large sized filter
   convolution, and a pooling operation. the network in network conv is
   able to extract information about the very fine grain details in the
   volume, while the 5x5 filter is able to cover a large receptive field
   of the input, and thus able to extract its information as well. you
   also have a pooling operation that helps to reduce spatial sizes and
   combat overfitting. on top of all of that, you have relus after each
   conv layer, which help improve the nonlinearity of the network.
   basically, the network is able to perform the functions of these
   different operations while still remaining computationally considerate.
   the paper does also give more of a high level reasoning that involves
   topics like sparsity and dense connections (read sections 3 and 4 of
   the [21]paper. still not totally clear to me, but if anybody has any
   insights, i   d love to hear them in the comments!).

   main points
     * used 9 inception modules in the whole architecture, with over 100
       layers in total! now that is deep   
     * no use of fully connected layers! they use an average pool instead,
       to go from a 7x7x1024 volume to a 1x1x1024 volume. this saves a
       huge number of parameters.
     * uses 12x fewer parameters than alexnet.
     * during testing, multiple crops of the same image were created, fed
       into the network, and the softmax probabilities were averaged to
       give us the final solution.
     * utilized concepts from r-id98 (a paper we   ll discuss later) for
       their detection model.
     * there are updated versions to the inception module (versions 6 and
       7).
     * trained on    a few high-end gpus within a week   .

   why it   s important

                   googlenet was one of the first models that introduced
   the idea that id98 layers didn   t always have to be stacked up
   sequentially. coming up with the inception module, the authors showed
   that a creative structuring of layers can lead to improved performance
   and computationally efficiency. this paper has really set the stage for
   some amazing architectures that we could see in the coming years.
   [googlenet5.png]

[22]microsoft resnet (2015)

                   imagine a deep id98 architecture. take that, double the
   number of layers, add a couple more, and it still probably isn   t as
   deep as the resnet architecture that microsoft research asia came up
   with in late 2015. resnet is a new 152 layer network architecture that
   set new records in classification, detection, and localization through
   one incredible architecture. aside from the new record in terms of
   number of layers, resnet won ilsvrc 2015 with an incredible error rate
   of 3.6% (depending on their skill and expertise, humans generally hover
   around a 5-10% error rate. see andrej karpathy   s [23]great post on his
   experiences with competing against convnets on the id163 challenge).
   [resnet.gif]

   residual block

                   the idea behind a residual block is that you have your
   input x go through conv-relu-conv series. this will give you some f(x).
   that result is then added to the original input x. let   s call that h(x)
   = f(x) + x. in traditional id98s, your h(x) would just be equal to f(x)
   right? so, instead of just computing that transformation (straight from
   x to f(x)), we   re computing the term that you have to add, f(x), to
   your input, x. basically, the mini module shown below is computing a
      delta    or a slight change to the original input x to get a slightly
   altered representation (when we think of traditional id98s, we go from x
   to f(x) which is a completely new representation that doesn   t keep any
   information about the original x). the authors believe that    it is
   easier to optimize the residual mapping than to optimize the original,
   unreferenced mapping   .
   [resnet.png]

   another reason for why this residual block might be effective is that
   during the backward pass of id26, the gradient will flow
   easily through the graph because we have addition operations, which
   distributes the gradient.

   main points
     *    ultra-deep        yann lecun.
     * 152 layers   
     * interesting note that after only the first 2 layers, the spatial
       size gets compressed from an input volume of 224x224 to a 56x56
       volume.
     * authors claim that a na  ve increase of layers in plain nets result
       in higher training and test error (figure 1 in the [24]paper).
     * the group tried a 1202-layer network, but got a lower test
       accuracy, presumably due to overfitting.
     * trained on an 8 gpu machine for two to three weeks.

   why it   s important

                   3.6% error rate. that itself should be enough to
   convince you. the resnet model is the best id98 architecture that we
   currently have and is a great innovation for the idea of residual
   learning. with error rates dropping every year since 2012, i   m
   skeptical about whether or not they will go down for ilsvrc 2016. i
   believe we   ve gotten to the point where stacking more layers on top of
   each other isn   t going to result in a substantial performance boost.
   there would definitely have to be creative new architectures like we   ve
   seen the last 2 years. on september 16^th, the results for this year   s
   competition will be released. mark your calendar.

   bonus: [25]resnets inside of resnets. yeah. i went there.

region based id98s ([26]r-id98 - 2013, [27]fast r-id98 - 2015, [28]faster r-id98
- 2015)

                   some may argue that the advent of r-id98s has been more
   impactful that any of the previous papers on new network architectures.
   with the first r-id98 paper being cited over 1600 times, ross girshick
   and his group at uc berkeley created one of the most impactful
   advancements in id161. as evident by their titles, fast r-id98
   and faster r-id98 worked to make the model faster and better suited for
   modern id164 tasks.

                   the purpose of r-id98s is to solve the problem of object
   detection. given a certain image, we want to be able to draw bounding
   boxes over all of the objects. the process can be split into two
   general components, the region proposal step and the classification
   step.

                   the authors note that any class agnostic region
   proposal method should fit. [29]selective search is used in particular
   for rid98. selective search performs the function of generating 2000
   different regions that have the highest id203 of containing an
   object. after we   ve come up with a set of region proposals, these
   proposals are then    warped    into an image size that can be fed into a
   trained id98 (alexnet in this case) that extracts a feature vector for
   each region. this vector is then used as the input to a set of linear
   id166s that are trained for each class and output a classification. the
   vector also gets fed into a bounding box regressor to obtain the most
   accurate coordinates.
   [rid98.png]

   non-maxima suppression is then used to suppress bounding boxes that
   have a significant overlap with each other.

   fast r-id98

                   improvements were made to the original model because of
   3 main problems. training took multiple stages (convnets to id166s to
   bounding box regressors), was computationally expensive, and was
   extremely slow (rid98 took 53 seconds per image). fast r-id98 was able to
   solve the problem of speed by basically sharing computation of the conv
   layers between different proposals and swapping the order of generating
   region proposals and running the id98. in this model, the image is first
   fed through a convnet, features of the region proposals are obtained
   from the last feature map of the convnet (check section 2.1 of the
   [30]paper for more details), and lastly we have our fully connected
   layers as well as our regression and classification heads.
   [fastrid98.png]

   faster r-id98

                   faster r-id98 works to combat the somewhat complex
   training pipeline that both r-id98 and fast r-id98 exhibited. the authors
   insert a region proposal network (rpn) after the last convolutional
   layer. this network is able to just look at the last convolutional
   feature map and produce region proposals from that. from that stage,
   the same pipeline as r-id98 is used (roi pooling, fc, and then
   classification and regression heads).
   [fasterrid98.png]

   why it   s important

                   being able to determine that a specific object is in an
   image is one thing, but being able to determine that object   s exact
   location is a huge jump in knowledge for the computer. faster r-id98 has
   become the standard for id164 programs today.

[31]id3 (2014)

   [32]                according to yann lecun, these networks could be
   the next big development. before talking about this paper, let   s talk a
   little about adversarial examples. for example, let   s consider a
   trained id98 that works well on id163 data. let   s take an example
   image and apply a perturbation, or a slight modification, so that the
   prediction error is maximized. thus, the object category of the
   prediction changes, while the image itself looks the same when compared
   to the image without the perturbation. from the highest level,
   adversarial examples are basically the images that fool convnets.
   [adversarial.png]

   adversarial examples ([33]paper) definitely surprised a lot of
   researchers and quickly became a topic of interest. now let   s talk
   about the id3. let   s think of two models, a
   generative model and a discriminative model. the discriminative model
   has the task of determining whether a given image looks natural (an
   image from the dataset) or looks like it has been artificially created.
   the task of the generator is to create images so that the discriminator
   gets trained to produce the correct outputs. this can be thought of as
   a zero-sum or minimax two player game. the analogy used in the paper is
   that the generative model is like    a team of counterfeiters, trying to
   produce and use fake currency    while the discriminative model is like
      the police, trying to detect the counterfeit currency   . the generator
   is trying to fool the discriminator while the discriminator is trying
   to not get fooled by the generator. as the models train, both methods
   are improved until a point where the    counterfeits are
   indistinguishable from the genuine articles   .

   why it   s important

                   sounds simple enough, but why do we care about these
   networks? as yann lecun stated in his quora [34]post, the discriminator
   now is aware of the    internal representation of the data    because it
   has been trained to understand the differences between real images from
   the dataset and artificially created ones. thus, it can be used as a
   feature extractor that you can use in a id98. plus, you can just create
   really cool artificial images that look pretty natural to me
   ([35]link).

[36]generating image descriptions (2014)

                   what happens when you combine id98s with id56s (no, you
   don   t get r-id98s, sorry )?but you do get one really amazing
   application. written by andrej karpathy (one of my personal favorite
   authors) and fei-fei li, this paper looks into a combination of id98s
   and bidirectional id56s (recurrent neural networks) to generate natural
   language descriptions of different image regions. basically, the model
   is able to take in an image, and output this:
   [caption.png]

   that   s pretty incredible. let   s look at how this compares to normal
   id98s. with traditional id98s, there is a single clear label associated
   with each image in the training data. the model described in the paper
   has training examples that have a sentence (or caption) associated with
   each image. this type of label is called a weak label, where segments
   of the sentence refer to (unknown) parts of the image. using this
   training data, a deep neural network    infers the latent alignment
   between segments of the sentences and the region that they describe   
   (quote from the paper). another neural net takes in the image as input
   and generates a description in text. let   s take a separate look at the
   two components, alignment and generation.

   alignment model

                   the goal of this part of the model is to be able to
   align the visual and textual data (the image and its sentence
   description). the model works by accepting an image and a sentence as
   input, where the output is a score for how well they match (now,
   karpathy refers a different [37]paper which goes into the specifics of
   how this works. this model is trained on compatible and incompatible
   image-sentence pairs).

                   now let   s think about representing the images. the
   first step is feeding the image into an r-id98 in order to detect the
   individual objects. this r-id98 was trained on id163 data. the top 19
   (plus the original image) object regions are embedded to a 500
   dimensional space. now we have 20 different 500 dimensional vectors
   (represented by v in the paper) for each image. we have information
   about the image. now, we want information about the sentence. we   re
   going to embed words into this same multimodal space. this is done by
   using a bidirectional recurrent neural network. from the highest level,
   this serves to illustrate information about the context of words in a
   given sentence. since this information about the picture and the
   sentence are both in the same space, we can compute inner products to
   show a measure of similarity.

   generation model

                   the alignment model has the main purpose of creating a
   dataset where you have a set of image regions (found by the rid98) and
   corresponding text (thanks to the bid56). now, the generation model is
   going to learn from that dataset in order to generate descriptions
   given an image. the model takes in an image and feeds it through a id98.
   the softmax layer is disregarded as the outputs of the fully connected
   layer become the inputs to another id56. for those that aren   t as
   familiar with id56s, their function is to basically form id203
   distributions on the different words in a sentence (id56s also need to
   be trained just like id98s do).

   disclaimer: this was definitely one of the more dense papers in this
   section, so if anyone has any corrections or other explanations, i   d
   love to hear them in the comments!
   [generatingimagedescriptions.png]

   why it   s important

                   the interesting idea for me was that of using these
   seemingly different id56 and id98 models to create a very useful
   application that in a way combines the fields of id161 and
   natural language processing. it opens the door for new ideas in terms
   of how to make computers and models smarter when dealing with tasks
   that cross different fields.

[38]spatial transformer networks (2015)

                   last, but not least, let   s get into one of the more
   recent papers in the field. this paper was written by a group at google
   deepmind a little over a year ago. the main contribution is the
   introduction of a spatial transformer module. the basic idea is that
   this module transforms the input image in a way so that the subsequent
   layers have an easier time making a classification. instead of making
   changes to the main id98 architecture itself, the authors worry about
   making changes to the image before it is fed into the specific conv
   layer. the 2 things that this module hopes to correct are pose
   id172 (scenarios where the object is tilted or scaled) and
   spatial attention (bringing attention to the correct object in a
   crowded image). for traditional id98s, if you wanted to make your model
   invariant to images with different scales and rotations, you   d need a
   lot of training examples for the model to learn properly. let   s get
   into the specifics of how this transformer module helps combat that
   problem.

                   the entity in traditional id98 models that dealt with
   spatial invariance was the maxpooling layer. the intuitive reasoning
   behind this layer was that once we know that a specific feature is in
   the original input volume (wherever there are high activation values),
   it   s exact location is not as important as its relative location to
   other features. this new spatial transformer is dynamic in a way that
   it will produce different behavior (different
   distortions/transformations) for each input image. it   s not just as
   simple and pre-defined as a traditional maxpool. let   s take look at how
   this transformer module works. the module consists of:
     * a localization network which takes in the input volume and outputs
       parameters of the spatial transformation that should be applied.
       the parameters, or theta, can be 6 dimensional for an affine
       transformation.
     * the creation of a sampling grid that is the result of warping the
       regular grid with the affine transformation (theta) created in the
       localization network.
     * a sampler whose purpose is to perform a warping of the input
       feature map.

   [spatialtransformer.png]

   this module can be dropped into a id98 at any point and basically helps
   the network learn how to transform feature maps in a way that minimizes
   the cost function during training.
   [spatialtransformer2.png]

   why it   s important

                   this paper caught my eye for the main reason that
   improvements in id98s don   t necessarily have to come from drastic
   changes in network architecture. we don   t need to create the next
   resnet or inception module. this paper implements the simple idea of
   making affine transformations to the input image in order to help
   models become more invariant to translation, scale, and rotation. for
   those interested, here is a [39]video from deepmind that has a great
   animation of the results of placing a spatial transformer module in a
   id98 and a good quora [40]discussion.


   and that ends our 3 part series on convnets! hope everyone was able to
   follow along, and if you feel that i may have left something important
   out, let me know in the comments! if you want more info on some of
   these concepts, i once again highly recommend stanford cs 231n lecture
   videos which can be found with a simple youtube search.

   dueces.
   [41]sources

   [42]tweet

   written on august 24, 2016

   please enable javascript to view the [43]comments powered by disqus.

references

   visible links
   1. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
   2. https://adeshpande3.github.io/adeshpande3.github.io/
   3. https://adeshpande3.github.io/adeshpande3.github.io/
   4. https://adeshpande3.github.io/adeshpande3.github.io/
   5. https://adeshpande3.github.io/adeshpande3.github.io/about
   6. https://github.com/adeshpande3
   7. https://adeshpande3.github.io/adeshpande3.github.io/projects
   8. https://adeshpande3.github.io/adeshpande3.github.io/resume.pdf
   9. http://bit.ly/29u99ty
  10. http://bit.ly/2aarg7f
  11. https://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks.pdf
  12. http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf
  13. http://arxiv.org/pdf/1311.2901v3.pdf
  14. https://adeshpande3.github.io/adeshpande3.github.io/a-beginner's-guide-to-understanding-convolutional-neural-networks/
  15. https://www.youtube.com/watch?v=ghemqsxt6tw
  16. http://arxiv.org/pdf/1409.1556v6.pdf
  17. http://arxiv.org/pdf/1409.1556v6.pdf
  18. http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/szegedy_going_deeper_with_2015_cvpr_paper.pdf
  19. http://iamaaditya.github.io/2016/03/one-by-one-convolution/
  20. https://www.youtube.com/watch?v=vxhsouuszdy
  21. http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/szegedy_going_deeper_with_2015_cvpr_paper.pdf
  22. https://arxiv.org/pdf/1512.03385v1.pdf
  23. http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-id163/
  24. https://arxiv.org/pdf/1512.03385v1.pdf
  25. http://arxiv.org/pdf/1608.02908.pdf
  26. https://arxiv.org/pdf/1311.2524v5.pdf
  27. https://arxiv.org/pdf/1504.08083.pdf
  28. http://arxiv.org/pdf/1506.01497v3.pdf
  29. https://ivi.fnwi.uva.nl/isis/publications/2013/uijlingsijcv2013/uijlingsijcv2013.pdf
  30. https://arxiv.org/pdf/1504.08083.pdf
  31. https://arxiv.org/pdf/1406.2661v1.pdf
  32. https://www.quora.com/what-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning
  33. http://arxiv.org/pdf/1312.6199v4.pdf
  34. https://www.quora.com/what-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning
  35. http://soumith.ch/eyescream/
  36. https://arxiv.org/pdf/1412.2306v2.pdf
  37. https://arxiv.org/pdf/1406.5679v1.pdf
  38. https://arxiv.org/pdf/1506.02025.pdf
  39. https://drive.google.com/file/d/0b1nqa_sa3w2in3rqlxvfrknxn0k/view
  40. https://www.quora.com/how-do-spatial-transformer-networks-work
  41. https://adeshpande3.github.io/assets/sources3.txt
  42. https://twitter.com/share
  43. http://disqus.com/?ref_noscript

   hidden links:
  45. mailto:adeshpande3@g.ucla.edu
  46. https://www.facebook.com/adit.deshpande.5
  47. https://github.com/adeshpande3
  48. https://instagram.com/thejugglinguy
  49. https://www.linkedin.com/in/aditdeshpande
  50. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
  51. https://www.twitter.com/aditdeshpande3
