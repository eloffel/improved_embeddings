   #[1]github [2]recent commits to spinn:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]19
     * [35]star [36]177
     * [37]fork [38]83

[39]stanfordnlp/[40]spinn

   [41]code [42]issues 1 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   spinn (stack-augmented parser-interpreter neural network): fast,
   batchable, context-aware treeid56s
     * [47]893 commits
     * [48]13 branches
     * [49]4 releases
     * [50]fetching contributors
     * [51]mit

    1. [52]python 57.1%
    2. [53]tex 33.6%
    3. [54]c++ 5.3%
    4. [55]shell 2.5%
    5. [56]cuda 1.2%
    6. [57]makefile 0.3%

   (button) python tex c++ shell cuda makefile
   branch: master (button) new pull request
   [58]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/s
   [59]download zip

downloading...

   want to be notified of new releases in stanfordnlp/spinn?
   [60]sign in [61]sign up

launching github desktop...

   if nothing happens, [62]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [63]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [64]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [65]download the github extension for visual studio
   and try again.

   (button) go back
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [66]permalink
   type         name         latest commit message commit time
        failed to load latest commit information.
        [67]checkpoints
        [68]cpp
        [69]python
        [70]scripts
        [71]writing
        [72].gitignore
        [73].gitmodules
        [74].travis.yml
        [75]license.txt
        [76]readme.md
        [77]evalb_rembed.prm

readme.md

   note: this codebase is under active development. to exactly reproduce
   the experiments published in acl 2016, use [78]this release. for the
   most recent version, see the [79]nyu fork.

stack-augmented parser-interpreter neural network

   this repository contains the source code described in our paper [80]a
   fast unified model for sentence parsing and understanding. for a more
   informal introduction to the ideas behind the model, see this
   [81]stanford nlp blog post.

   there are three separate implementations available:
     * a python/theano implementation of spinn using a na  ve stack
       representation (named fat-stack)
     * a python/theano implementation of spinn using the thin-stack
       representation described in our paper
     * a c++/cuda implementation of the spinn feedforward, used for
       performance testing

python code

   the python code lives, quite intuitively, in the python folder. we used
   this code to train and test the spinn models before publication.

   there is one enormous difference in the fat- and thin-stack
   implementations: fat-stack uses theano's automatically generated
   symbolic id26 graphs, while thin-stack generates its own
   optimal id26 graph. this makes thin-stack oodles faster than
   its brother, but we have not yet implemented all spinn variants to
   support this custom id26.

installation

   requirements:
     * python 2.7
     * cuda >= 7.0
     * cudnn == v4 (v5 is not compatible with our theano fork)

   install all required python dependencies using the command below.
   (warning: this installs our custom theano fork. we recommend installing
   in a virtual environment in order to avoid overwriting any stock theano
   install you already have.)
pip install -r python/requirements.txt

   we use [82]a modified version of theano in order to support fast
   forward- and backward-prop in thin-stack. while it isn't absolutely
   necessary to use this hacked theano, it greatly improves thin-stack
   performance.

   alternatively, you can use a custom docker image that we've prepared,
   as discussed in this [83]codalab worksheet.

running the code

   the easiest way to launch a train/test run is to use one of the
   [84]checkpoints directory. the bash scripts in this directory will
   download the necessary data and launch train/test runs of all models
   reported in our paper. you can run any of the following scripts:
./checkpoints/spinn.sh
./checkpoints/spinn_pi.sh
./checkpoints/spinn_pi_nt.sh
./checkpoints/id56.sh

   all of the above scripts will by default launch a training run
   beginning with the recorded parameters of our best models. you can
   change their behavior using the arguments below:
$ ./checkpoints/spinn.sh -h
spinn.sh [-h] [-e] [-t] [-s] -- run a train or test run of a spinn model

where:
    -h    show this help text
    -e    run in eval-only mode (evaluates on dev set by default)
    -t    evaluate on test set
    -s    skip the checkpoint loading; run with a randomly initialized model

   to evaluate our best spinn-pi-nt model on the test set, for example,
   run
$ ./checkpoints/spinn_pi_nt.sh -e -t
running command:
python -m spinn.models.fat_classifier --data_type snli --embedding_data_path ../
glove/glove.840b.300d.txt --log_path ../logs --training_data_path ../snli_1.0/sn
li_1.0_train.jsonl --experiment_name spinn_pi_nt --expanded_eval_only --eval_dat
a_path ../snli_1.0/snli_1.0_test.jsonl --ckpt_path spinn_pi_nt.ckpt_best   --bat
ch_size 32 --embedding_keep_rate 0.828528124124 --eval_seq_length 50 --init_rang
e 0.005 --l2_lambda 3.45058959758e-06 --learning_rate 0.000297682444894 --model_
dim 600 --model_type model0 --noconnect_tracking_comp  --num_sentence_pair_combi
nation_layers 2 --semantic_classifier_keep_rate 0.9437038157 --seq_length 50 --t
racking_lstm_hidden_dim 57 --use_tracking_lstm  --word_embedding_dim 300
...
[1] checkpointed model was trained for 156500 steps.
[1] building forward pass.
[1] writing eval output for ../snli_1.0/snli_1.0_test.jsonl.
[1] written gold parses in spinn_pi_nt-snli_1.0_test.jsonl-parse.gld
[1] written predicted parses in spinn_pi_nt-snli_1.0_test.jsonl-parse.tst
[1] step: 156500    eval acc: 0.808734   0.000000   ../snli_1.0/snli_1.0_test.js
onl

custom model configurations

   the main executable for the snli experiments in the paper is
   [85]fat_classifier.py, whose flags specify the hyperparameters of the
   model. you may also need to set theano flags through the theano_flags
   environment variable, which specifies compilation mode (set it to
   fast_compile during development, and delete it to use the default state
   for longer runs), device, which can be set to cpu or gpu#, and
   cuda.root, which specifies the location of cuda when running on gpu.
   floatx should always be set to float32.

   here's a sample command that runs a fast, low-dimensional cpu training
   run, training and testing only on the dev set. it assumes that you have
   a copy of [86]snli available locally.
pythonpath=spinn/python \
    theano_flags=optimizer=fast_compile,device=cpu,floatx=float32 \
    python2.7 -m spinn.models.fat_classifier --data_type snli \
    --training_data_path snli_1.0/snli_1.0_dev.jsonl \
    --eval_data_path snli_1.0/snli_1.0_dev.jsonl \
    --embedding_data_path spinn/python/spinn/tests/test_embedding_matrix.5d.txt
\
    --word_embedding_dim 5 --model_dim 10

   for full runs, you'll also need a copy of the 840b word 300d [87]glove
   word vectors.

c++ code

   the c++ code lives in the cpp folder. this code implements a basic
   spinn feedforward. (this implementation corresponds to the bare
   spinn-pi-nt, "parsed input / no tracking" model, described in the
   paper.) it has been verified to produce the exact same output as a
   id56 with the same weights and inputs. (we used a
   simplified version of ozan irsoy's [88]deep-recursive project as a
   comparison.)

   the main binary, stacktest, simply generates random input data and runs
   a feedforward. it outputs the total feedforward time elapsed and the
   numerical result of the feedforward.

dependencies

   the only external dependency of the c++ code is cuda >=7.0. the tests
   depend on the [89]googletest library, included in this repository as a
   git submodule.

installation

   first install cuda >=7.0 and ensure that nvcc is on your path. then:
# from project root
cd cpp

# pull down git submodules (libraries)
git submodule update --init

# compile c++ code
make stacktest
make id56test

   this should generate a binary in cpp/bin/stacktest.

running

   the binary cpp/bin/stacktest runs on random input data. you can time
   the feedforward yourself by running the following commands:
# from project root
cd cpp

batch_size=512 ./bin/stacktest

   you can of course set batch_size to whatever integer you desire. the
   other model architecture parameters are fixed in the code, but you can
   easily change them as well [90]on this line if you desire.

baseline id56

   the binary cpp/bin/id56test runs a vanilla id56 (relu activations) with
   random input data. you can run this performance test script as follows:
# from project root
cd cpp

batch_size=512 ./bin/id56test

license

   copyright 2018, stanford university

   permission is hereby granted, free of charge, to any person obtaining a
   copy of this software and associated documentation files (the
   "software"), to deal in the software without restriction, including
   without limitation the rights to use, copy, modify, merge, publish,
   distribute, sublicense, and/or sell copies of the software, and to
   permit persons to whom the software is furnished to do so, subject to
   the following conditions:

   the above copyright notice and this permission notice shall be included
   in all copies or substantial portions of the software.

   the software is provided "as is", without warranty of any kind, express
   or implied, including but not limited to the warranties of
   merchantability, fitness for a particular purpose and noninfringement.
   in no event shall the authors or copyright holders be liable for any
   claim, damages or other liability, whether in an action of contract,
   tort or otherwise, arising from, out of or in connection with the
   software or the use or other dealings in the software.

     *    2019 github, inc.
     * [91]terms
     * [92]privacy
     * [93]security
     * [94]status
     * [95]help

     * [96]contact github
     * [97]pricing
     * [98]api
     * [99]training
     * [100]blog
     * [101]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [102]reload to refresh your
   session. you signed out in another tab or window. [103]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/stanfordnlp/spinn/commits/master.atom
   3. https://github.com/stanfordnlp/spinn#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/stanfordnlp/spinn
  32. https://github.com/join
  33. https://github.com/login?return_to=/stanfordnlp/spinn
  34. https://github.com/stanfordnlp/spinn/watchers
  35. https://github.com/login?return_to=/stanfordnlp/spinn
  36. https://github.com/stanfordnlp/spinn/stargazers
  37. https://github.com/login?return_to=/stanfordnlp/spinn
  38. https://github.com/stanfordnlp/spinn/network/members
  39. https://github.com/stanfordnlp
  40. https://github.com/stanfordnlp/spinn
  41. https://github.com/stanfordnlp/spinn
  42. https://github.com/stanfordnlp/spinn/issues
  43. https://github.com/stanfordnlp/spinn/pulls
  44. https://github.com/stanfordnlp/spinn/projects
  45. https://github.com/stanfordnlp/spinn/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/stanfordnlp/spinn/commits/master
  48. https://github.com/stanfordnlp/spinn/branches
  49. https://github.com/stanfordnlp/spinn/releases
  50. https://github.com/stanfordnlp/spinn/graphs/contributors
  51. https://github.com/stanfordnlp/spinn/blob/master/license.txt
  52. https://github.com/stanfordnlp/spinn/search?l=python
  53. https://github.com/stanfordnlp/spinn/search?l=tex
  54. https://github.com/stanfordnlp/spinn/search?l=c++
  55. https://github.com/stanfordnlp/spinn/search?l=shell
  56. https://github.com/stanfordnlp/spinn/search?l=cuda
  57. https://github.com/stanfordnlp/spinn/search?l=makefile
  58. https://github.com/stanfordnlp/spinn/find/master
  59. https://github.com/stanfordnlp/spinn/archive/master.zip
  60. https://github.com/login?return_to=https://github.com/stanfordnlp/spinn
  61. https://github.com/join?return_to=/stanfordnlp/spinn
  62. https://desktop.github.com/
  63. https://desktop.github.com/
  64. https://developer.apple.com/xcode/
  65. https://visualstudio.github.com/
  66. https://github.com/stanfordnlp/spinn/tree/f2f95f585328ec5cd7da071753f5e0582e7600a0
  67. https://github.com/stanfordnlp/spinn/tree/master/checkpoints
  68. https://github.com/stanfordnlp/spinn/tree/master/cpp
  69. https://github.com/stanfordnlp/spinn/tree/master/python
  70. https://github.com/stanfordnlp/spinn/tree/master/scripts
  71. https://github.com/stanfordnlp/spinn/tree/master/writing
  72. https://github.com/stanfordnlp/spinn/blob/master/.gitignore
  73. https://github.com/stanfordnlp/spinn/blob/master/.gitmodules
  74. https://github.com/stanfordnlp/spinn/blob/master/.travis.yml
  75. https://github.com/stanfordnlp/spinn/blob/master/license.txt
  76. https://github.com/stanfordnlp/spinn/blob/master/readme.md
  77. https://github.com/stanfordnlp/spinn/blob/master/evalb_rembed.prm
  78. https://github.com/stanfordnlp/spinn/releases/tag/acl2016
  79. https://github.com/nyu-mll/spinn
  80. http://arxiv.org/abs/1603.06021
  81. http://nlp.stanford.edu/blog/hybrid-tree-sequence-neural-networks-with-spinn/
  82. https://github.com/hans/theano-hacked/tree/8964f10e44bcd7f21ae74ea7cdc3682cc7d3258e
  83. https://worksheets.codalab.org/worksheets/0xa85b2da5365f423d952f800370ebb9b5/
  84. https://github.com/stanfordnlp/spinn/tree/master/checkpoints
  85. https://github.com/stanfordnlp/spinn/blob/master/python/spinn/models/fat_classifier.py
  86. http://nlp.stanford.edu/projects/snli/
  87. http://nlp.stanford.edu/projects/glove/
  88. https://github.com/oir/deep-recursive
  89. https://github.com/google/googletest
  90. https://github.com/stanfordnlp/spinn/blob/5d4257f4cd15cf7213d2ff87f6f3d7f6716e2ea1/cpp/bin/stacktest.cc#l33
  91. https://github.com/site/terms
  92. https://github.com/site/privacy
  93. https://github.com/security
  94. https://githubstatus.com/
  95. https://help.github.com/
  96. https://github.com/contact
  97. https://github.com/pricing
  98. https://developer.github.com/
  99. https://training.github.com/
 100. https://github.blog/
 101. https://github.com/about
 102. https://github.com/stanfordnlp/spinn
 103. https://github.com/stanfordnlp/spinn

   hidden links:
 105. https://github.com/
 106. https://github.com/stanfordnlp/spinn
 107. https://github.com/stanfordnlp/spinn
 108. https://github.com/stanfordnlp/spinn
 109. https://help.github.com/articles/which-remote-url-should-i-use
 110. https://github.com/stanfordnlp/spinn#stack-augmented-parser-interpreter-neural-network
 111. https://github.com/stanfordnlp/spinn#python-code
 112. https://github.com/stanfordnlp/spinn#installation
 113. https://github.com/stanfordnlp/spinn#running-the-code
 114. https://github.com/stanfordnlp/spinn#custom-model-configurations
 115. https://github.com/stanfordnlp/spinn#c-code
 116. https://github.com/stanfordnlp/spinn#dependencies
 117. https://github.com/stanfordnlp/spinn#installation-1
 118. https://github.com/stanfordnlp/spinn#running
 119. https://github.com/stanfordnlp/spinn#baseline-id56
 120. https://github.com/stanfordnlp/spinn#license
 121. https://github.com/
