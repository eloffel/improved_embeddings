         contextual id56-gans for abstract reasoning diagram generation

   [1]link to paper

        [2]contextual id56-gans for abstract reasoning diagram generation

   arnab ghosh^*, viveka kulharia^*, amitabha mukerjee, vinay namboodiri,
   mohit bansal

   ^*equal contribution

motivation

     * understanding, predicting, and generating object motions and
       transformations is a core problem in artificial intelligence.
     * modeling sequences of evolving images may provide better
       representations and models of motion and may ultimately be used for
       forecasting, simulation, or video generation.
     * diagrammatic abstract reasoning is an avenue in which diagrams
       evolve in complex patterns and one needs to infer the underlying
       pattern sequence and generate the next image in the sequence.

an example with an explanation

   [id56-gan%20model-6.png]

   an explanation of the ground truth is that the dashed line first goes
   to the left, then to the right, and then on both sides, and also
   changes from single to double, hence the ground truth should have
   double dashed lines on both the sides. on the corners, the number of
   slanted lines increase by one after every two images, hence the ground
   truth should have four slant lines on both the corners.

some more example problems from dat-dar dataset

   [id56-gan%20model-8.png]
     __________________________________________________________________

the model
     __________________________________________________________________

contextual id56-gan

     * gans have been shown to be useful in several image generation and
       manipulation tasks and hence it was a natural choice to prevent the
       model make fuzzy generations.
     * in context-id56-gan, 'context' refers to the adversary receiving
       previous images (modeled as an id56) and the generator is also an
       id56. the name distinguishes it from our simpler id56 gan model where
       the adversary is not contextual (as it only uses a single image)
       and only the generator is an id56.
     * the discriminator is modeled as a gru-id56 which gets all the
       preceding images to decide whether the generation by the generator
       is the correct image for the timestep.
     * the generator is modeled as a gru-id56 which tries to generate an
       image using the preceding images. it is guided by the contextual
       discriminator to produce real looking images.

   [id56-gan%20model.png]

   the above figure corresponds to our context-id56-gan model, where the
   generator g and the discriminator d (where d[i] represents its ith
   timestep snapshot) are both id56s. g generates an image at every
   timestep while d receives all the preceding images as context to decide
   whether the current image output by g is real vs generated for that
   particular timestep. x[i] are the input images.

impact of adversarial loss

     * when using an l-2 id168, some of the generated images were
       superimpositions of the component parts and were too cluttered.
     * when using an l-1 id168, although it was sharper than using
       an l-2 loss, it was missing some components of the actual diagrams.
     * a weighted combination of l-1 and adversarial loss (as defined for
       the context based discriminator model described above) was used for
       the context-id56-gan model to produce the best results based on
       empirical evaluation.

   [id56-gan%20model-3.png]

some generations (contextual-id56-gan)

   [id56-gan%20model-10.png]
     __________________________________________________________________

modeling of consecutive timesteps using siamese networks for better accuracy

   [id56-gan%20model-13.png]

   [id56-gan%20model-14.png]
     __________________________________________________________________

comparison with human performance

             college-grade 10th-grade
   age range 20-22         14-16
   #students 21            48
   mean      44.17%        36.67%
   std       16.67%        17.67%
   max       66.67%        75.00%
   min       8.33%         8.33%

   context-id56-gan with features obtained from siamese id98 is competitive
   with humans in 10th grade in the sense that it is able to achieve
   accuracy of 35.4% when the generated features are compared with the
   features obtained from actual answer images. it needs to be noted that
   humans can see the options to get the best possible overall sequence of
   six images and hence can select the best choice while our model is just
   comparing the generations (obtained using sequence of five images in
   the problem) with options to get the best option. so, we can say that
   our model is very good generator and comparable to even 10th grade
   humans. an interesting aspect is that the model is never trained on the
   correct answers, it is just trained on multiple sequences from the
   problem images and still performs remarkably well.
     __________________________________________________________________

interesting cases

   [id56-gan%20model-12.png]
     * first generation in the first example generation it is interesting
       to note that the model correctly predicted elements in off diagonal
       while faltered in the shape of the elements in leading diagonals.
     * second generation second example shows an interesting case whereby
       the image generated by our model is also plausible (if by symmetry
       it is considered that first and third the semicircular ring is
       solid and hence fourth and sixth should be solid) while the actual
       answer is of course plausible according to the reasoning that the
       (solid vs hollow) flipped in the first two cases then stayed the
       same for the next two timesteps. even more interesting is its
       analysis of the spatial dynamics of the ball and the semicircular
       ring which it almost correctly captured.
     * third generation another very interesting case is the generation
       which it gets correct. however, in this case the answer figure is
       exactly similar to the second figure in the sequence. therefore, it
       is not illustrative of the ability of the model to generate the
       sequence of the pattern.
     __________________________________________________________________

application of the model to moving-mnist

   [id56-gan%20model-11.png]

   the model can be applied to video prediction tasks as illustrated by
   above figure. the [3]moving mnist task consists of videos of two moving
   mnist digits and the next frame has to be predicted from the preceding
   frames.
   [4]contextual-id56-gan is maintained by [5]arnabgho. this page was
   generated by [6]github pages using the [7]cayman theme by [8]jason
   long.

references

   visible links
   1. https://arxiv.org/abs/1609.09444
   2. https://arxiv.org/abs/1609.09444
   3. http://www.cs.toronto.edu/~nitish/unsupervised_video/
   4. https://github.com/arnabgho/contextual-id56-gan
   5. https://github.com/arnabgho
   6. https://pages.github.com/
   7. https://github.com/jasonlong/cayman-theme
   8. https://twitter.com/jasonlong

   hidden links:
  10. https://arnabgho.github.io/contextual-id56-gan/#contextual-id56-gan
  11. https://arnabgho.github.io/contextual-id56-gan/#motivation
  12. https://arnabgho.github.io/contextual-id56-gan/#an-example-with-an-explanation
  13. https://arnabgho.github.io/contextual-id56-gan/#some-more-example-problems-from-dat-dar-dataset
  14. https://arnabgho.github.io/contextual-id56-gan/#the-model
  15. https://arnabgho.github.io/contextual-id56-gan/#contextual-id56-gan-1
  16. https://arnabgho.github.io/contextual-id56-gan/#impact-of-adversarial-loss-
  17. https://arnabgho.github.io/contextual-id56-gan/#some-generations-contextual-id56-gan
  18. https://arnabgho.github.io/contextual-id56-gan/#modeling-of-consecutive-timesteps-using-siamese-networks-for-better-accuracy
  19. https://arnabgho.github.io/contextual-id56-gan/#comparison-with-human-performance
  20. https://arnabgho.github.io/contextual-id56-gan/#interesting-cases
  21. https://arnabgho.github.io/contextual-id56-gan/#application-of-the-model-to-moving-mnist
