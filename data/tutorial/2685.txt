1 billion word language model benchmark

   [1]paper | [2]code | [3]data | [4]output probabilities

   the purpose of the project is to make available a standard training and
   test setup for id38 experiments.

   the training/held-out data was produced from the [5]wmt 2011 news crawl
   data using a combination of bash shell and perl scripts distributed
   [6]here.

   this also means that your results on this data set are reproducible by
   the research community at large.

   besides the scripts needed to rebuild the training/held-out data, it
   also makes available log-id203 values for each word in each of
   ten feld-out data sets, for each of the following baseline models:
     * unpruned katz (1.1b id165s),
     * pruned katz (~15m id165s),
     * unpruned interpolated kneser-ney (1.1b id165s),
     * pruned interpolated kneser-ney (~15m id165s)

   happy benchmarking!

references

   1. http://arxiv.org/abs/1312.3005
   2. https://github.com/ciprian-chelba/1-billion-word-language-modeling-benchmark
   3. http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz
   4. http://www.statmt.org/lm-benchmark/output.tar
   5. http://www.statmt.org/wmt11/translation-task.html#download
   6. https://code.google.com/p/1-billion-word-language-modeling-benchmark/
