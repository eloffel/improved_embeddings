   #[1]subscribe to the jaan feed

[2]jaan altosaar

   [3]   

     * [4]about
     * [5]blog
     * [6]projects
     * [7]papers
     * [8]talks

                [9]tutorial - what is a variational autoencoder?

   understanding id5 (vaes) from two perspectives:
   deep learning and id114.

   why do deep learning researchers and probabilistic machine learning
   folks get confused when discussing id5? what is a
   variational autoencoder? why is there unreasonable confusion
   surrounding this term?

   there is a conceptual and language gap. the sciences of neural networks
   and id203 models do not have a shared language. my goal is to
   bridge this idea gap and allow for more collaboration and discussion
   between these fields, and provide a consistent implementation
   ([10]github link). if many words here are new to you, jump to the
   [11]glossary.

   id5 are cool. they let us design complex
   generative models of data, and fit them to large datasets. they can
   generate images of fictional celebrity faces and high-resolution
   [12]digital artwork.
   [variational-autoencoder-faces.jpg] fictional celebrity faces generated
   by a variational autoencoder ([13]by alec radford).

   these models also yield state-of-the-art machine learning results in
   [14]image generation and [15]id23. variational
   autoencoders (vaes) were defined in 2013 by [16]kingma et al. and
   [17]rezende et al..

   how can we create a language for discussing id5?
   let   s think about them first using neural networks, then using
   variational id136 in id203 models.

  the neural net perspective

   in neural net language, a variational autoencoder consists of an
   encoder, a decoder, and a id168.
   [encoder-decoder.png] the encoder compresses data into a latent space
   (z). the decoder reconstructs the data given the hidden representation.

   the encoder is a neural network. its input is a datapoint , its output
   is a hidden representation , and it has weights and biases . to be
   concrete, let   s say is a 28 by 28-pixel photo of a handwritten number.
   the encoder    encodes    the data which is -dimensional into a latent
   (hidden) representation space , which is much less than dimensions.
   this is typically referred to as a    bottleneck    because the encoder
   must learn an efficient compression of the data into this
   lower-dimensional space. let   s denote the encoder . we note that the
   lower-dimensional space is stochastic: the encoder outputs parameters
   to , which is a gaussian id203 density. we can sample from this
   distribution to get noisy values of the representations .

   the decoder is another neural net. its input is the representation , it
   outputs the parameters to the id203 distribution of the data, and
   has weights and biases . the decoder is denoted by . running with the
   handwritten digit example, let   s say the photos are black and white and
   represent each pixel as or . the id203 distribution of a single
   pixel can be then represented using a bernoulli distribution. the
   decoder gets as input the latent representation of a digit and outputs
   bernoulli parameters, one for each of the pixels in the image. the
   decoder    decodes    the real-valued numbers in into real-valued numbers
   between and . information is lost because it goes from a smaller to a
   larger dimensionality. how much information is lost? we measure this
   using the reconstruction log-likelihood whose units are nats. this
   measure tells us how effectively the decoder has learned to reconstruct
   an input image given its latent representation .

   the id168 of the variational autoencoder is the negative
   log-likelihood with a regularizer. because there are no global
   representations that are shared by all datapoints, we can decompose the
   id168 into only terms that depend on a single datapoint . the
   total loss is then for total datapoints. the id168 for
   datapoint is:

   the first term is the reconstruction loss, or expected negative
   log-likelihood of the -th datapoint. the expectation is taken with
   respect to the encoder   s distribution over the representations. this
   term encourages the decoder to learn to reconstruct the data. if the
   decoder   s output does not reconstruct the data well, statistically we
   say that the decoder parameterizes a likelihood distribution that does
   not place much id203 mass on the true data. for example, if our
   goal is to model black and white images and our model places high
   id203 on there being black spots where there are actually white
   spots, this will yield the worst possible reconstruction. poor
   reconstruction will incur a large cost in this id168.

   the second term is a regularizer that we throw in (we   ll see how it   s
   derived later). this is the id181 between the
   encoder   s distribution and . this divergence measures how much
   information is lost (in units of nats) when using to represent . it is
   one measure of how close is to .

   in the variational autoencoder, is specified as a standard normal
   distribution with mean zero and variance one, or . if the encoder
   outputs representations that are different than those from a standard
   normal distribution, it will receive a penalty in the loss. this
   regularizer term means    keep the representations of each digit
   sufficiently diverse   . if we didn   t include the regularizer, the
   encoder could learn to cheat and give each datapoint a representation
   in a different region of euclidean space. this is bad, because then two
   images of the same number (say a 2 written by different people, and )
   could end up with very different representations . we want the
   representation space of to be meaningful, so we penalize this behavior.
   this has the effect of keeping similar numbers    representations close
   together (e.g. so the representations of the digit two remain
   sufficiently close).

   we train the variational autoencoder using id119 to optimize
   the loss with respect to the parameters of the encoder and decoder and
   . for stochastic id119 with step size , the encoder
   parameters are updated using and the decoder is updated similarly.

  the id203 model perspective

   now let   s think about id5 from a id203 model
   perspective. please forget everything you know about deep learning and
   neural networks for now. thinking about the following concepts in
   isolation from neural networks will clarify things. at the very end,
   we   ll bring back neural nets.

   in the id203 model framework, a variational autoencoder contains
   a specific id203 model of data and latent variables . we can
   write the joint id203 of the model as . the generative process
   can be written as follows.

   for each datapoint :
     * draw latent variables
     * draw datapoint

   we can represent this as a graphical model:
   [graphical-model-variational-autoencoder.png] the graphical model
   representation of the model in the variational autoencoder. the latent
   variable z is a standard normal, and the data are drawn from p(x|z).
   the shaded node for x denotes observed data. for black and white images
   of handwritten digits, this data likelihood is bernoulli distributed.

   this is the central object we think about when discussing variational
   autoencoders from a id203 model perspective. the latent variables
   are drawn from a prior . the data have a likelihood that is conditioned
   on latent variables . the model defines a joint id203
   distribution over data and latent variables: . we can decompose this
   into the likelihood and prior: . for black and white digits, the
   likelihood is bernoulli distributed.

   now we can think about id136 in this model. the goal is to infer
   good values of the latent variables given observed data, or to
   calculate the posterior . bayes says:

   examine the denominator . this is called the evidence, and we can
   calculate it by marginalizing out the latent variables: .
   unfortunately, this integral requires exponential time to compute as it
   needs to be evaluated over all configurations of latent variables. we
   therefore need to approximate this posterior distribution.

   variational id136 approximates the posterior with a family of
   distributions . the variational parameter indexes the family of
   distributions. for example, if were gaussian, it would be the mean and
   variance of the latent variables for each datapoint .

   how can we know how well our variational posterior approximates the
   true posterior ? we can use the id181, which
   measures the information lost when using to approximate (in units of
   nats):

   our goal is to find the variational parameters that minimize this
   divergence. the optimal approximate posterior is thus

   why is this impossible to compute directly? the pesky evidence appears
   in the divergence. this is intractable as discussed above. we need one
   more ingredient for tractable variational id136. consider the
   following function:

   notice that we can combine this with the id181
   and rewrite the evidence as

   by jensen   s inequality, the id181 is always
   greater than or equal to zero. this means that minimizing the
   id181 is equivalent to maximizing the elbo. the
   abbreviation is revealed: the evidence lower bound allows us to do
   approximate posterior id136. we are saved from having to compute
   and minimize the id181 between the approximate
   and exact posteriors. instead, we can maximize the elbo which is
   equivalent (but computationally tractable).

   in the variational autoencoder model, there are only local latent
   variables (no datapoint shares its latent with the latent variable of
   another datapoint). so we can decompose the elbo into a sum where each
   term depends on a single datapoint. this allows us to use stochastic
   id119 with respect to the parameters (important: the
   variational parameters are shared across datapoints - more on this
   [18]here). the elbo for a single datapoint in the variational
   autoencoder is:

   to see that this is equivalent to our previous definition of the elbo,
   expand the log joint into the prior and likelihood terms and use the
   product rule for the logarithm.

   let   s make the connection to neural net language. the final step is to
   parametrize the approximate posterior with an id136 network (or
   encoder) that takes as input data and outputs parameters . we
   parametrize the likelihood with a generative network (or decoder) that
   takes latent variables and outputs parameters to the data distribution
   . the id136 and generative networks have parameters and
   respectively. the parameters are typically the weights and biases of
   the neural nets. we optimize these to maximize the elbo using
   stochastic id119 (there are no global latent variables, so
   it is kosher to minibatch our data). we can write the elbo and include
   the id136 and generative network parameters as:

   this evidence lower bound is the negative of the id168 for
   id5 we discussed from the neural net perspective;
   . however, we arrived at it from principled reasoning about id203
   models and approximate posterior id136. we can still interpret the
   id181 term as a regularizer, and the expected
   likelihood term as a reconstruction    loss   . but the id203 model
   approach makes clear why these terms exist: to minimize the
   id181 between the approximate posterior and model
   posterior .

   what about the model parameters? we glossed over this, but it is an
   important point. the term    variational id136    usually refers to
   maximizing the elbo with respect to the variational parameters . we can
   also maximize the elbo with respect to the model parameters (e.g. the
   weights and biases of the generative neural network parameterizing the
   likelihood). this technique is called variational em (expectation
   maximization), because we are maximizing the expected log-likelihood of
   the data with respect to the model parameters.

   that   s it! we have followed the recipe for variational id136. we   ve
   defined:
     * a id203 model of latent variables and data
     * a variational family for the latent variables to approximate our
       posterior

   then we used the variational id136 algorithm to learn the
   variational parameters (gradient ascent on the elbo to learn ). we used
   variational em for the model parameters (gradient ascent on the elbo to
   learn ).

  experiments

   now we are ready to look at samples from the model. we have two choices
   to measure progress: sampling from the prior or the posterior. to give
   us a better idea of how to interpret the learned latent space, we can
   visualize what the posterior distribution of the latent variables looks
   like.

   computationally, this means feeding an input image through the
   id136 network to get the parameters of the normal distribution,
   then taking a sample of the latent variable . we can plot this during
   training to see how the id136 network learns to better approximate
   the posterior distribution, and place the latent variables for the
   different classes of digits in different parts of the latent space.
   note that at the start of training, the distribution of latent
   variables is close to the prior (a round blob around ).

         iframe: [19]//giphy.com/embed/26ufovqzdjhoprp8k?html5=true

   visualizing the learned approximate posterior during training. as
   training progresses the digit classes become differentiated in the
   two-dimensional latent space.

   we can also visualize the prior predictive distribution. we fix the
   values of the latent variables to be equally spaced between and . then
   we can take samples from the likelihood parametrized by the generative
   network. these    hallucinated    images show us what the model associates
   with each part of the latent space.

         iframe: [20]//giphy.com/embed/26ufgj5lh3yko1zlu?html5=true

   visualizing the prior predictive distribution by looking at samples of
   the likelihood. the x and y-axes represent equally spaced latent
   variable values between -3 and 3 (in two dimensions).

  glossary

   we need to decide on the language used for discussing variational
   autoencoders in a clear and concise way. here is a glossary of terms
   i   ve found confusing:
     * variational autoencoder (vae): in neural net language, a vae
       consists of an encoder, a decoder, and a id168. in
       id203 model terms, the variational autoencoder refers to
       approximate id136 in a latent gaussian model where the
       approximate posterior and model likelihood are parametrized by
       neural nets (the id136 and generative networks).
     * id168: in neural net language, we think of id168s.
       training means minimizing these id168s. but in variational
       id136, we maximize the elbo (which is not a id168).
       this leads to awkwardness like calling optimizer.minimize(-elbo) as
       optimizers in neural net frameworks only support minimization.
     * encoder: in the neural net world, the encoder is a neural network
       that outputs a representation of data . in id203 model terms,
       the id136 network parametrizes the approximate posterior of the
       latent variables . the id136 network outputs parameters to the
       distribution .
     * decoder: in deep learning, the decoder is a neural net that learns
       to reconstruct the data given a representation . in terms of
       id203 models, the likelihood of the data given latent
       variables is parametrized by a generative network. the generative
       network outputs parameters to the likelihood distribution .
     * local latent variables: these are the for each datapoint . there
       are no global latent variables. because there are only local latent
       variables, we can easily decompose the elbo into terms that depend
       only on a single datapoint . this enables stochastic gradient
       descent.
     * id136: in neural nets, id136 usually means prediction of
       latent representations given new, never-before-seen datapoints. in
       id203 models, id136 refers to inferring the values of
       latent variables given observed data.

   one jargon-laden concept deserves its own subsection:

  mean-field versus amortized id136

   this issue was very confusing for me, and i can see how it might be
   even more confusing for someone coming from a deep learning background.
   in deep learning, we think of inputs and outputs, encoders and
   decoders, and id168s. this can lead to fuzzy, imprecise
   concepts when learning about probabilistic modeling.

   let   s discuss how mean-field id136 differs from amortized
   id136. this is a choice we face when doing approximate id136 to
   estimate a posterior distribution of latent variables. we might have
   various constraints: do we have lots of data? do we have big computers
   or gpus? do we have local, per-datapoint latent variables, or global
   latent variables shared across all datapoints?

   mean-field variational id136 refers to a choice of a variational
   distribution that factorizes across the data points, with no shared
   parameters:

   this means there are free parameters for each datapoint (e.g. for
   gaussian latent variables). how do we do    learning    for a new, unseen
   datapoint? we need to maximize the elbo for each new datapoint, with
   respect to its mean-field parameter(s) .

   amortized id136 refers to    amortizing    the cost of id136 across
   datapoints. one way to do this is by sharing (amortizing) the
   variational parameters across datapoints. for example, in the
   variational autoencoder, the parameters of the id136 network. these
   global parameters are shared across all datapoints. if we see a new
   datapoint and want to see what its approximate posterior looks like, we
   can run variational id136 again (maximizing the elbo until
   convergence), or trust that the shared parameters are    good-enough   .
   this can be an advantage over mean-field.

   which one is more flexible? mean-field id136 is strictly more
   expressive, because it has no shared parameters. the per-data
   parameters can ensure our approximate posterior is most faithful to the
   data. another way to think of this is that we are limiting the capacity
   or representational power of our variational family by tying parameters
   across datapoints (e.g. with a neural network that shares weights and
   biases across data).

  sample pytorch/tensorflow implementation

   here is the implementation that was used to generate the figures in
   this post: [21]github link

  footnote: the reparametrization trick

   the final thing we need to implement the variational autoencoder is how
   to take derivatives with respect to the parameters of a stochastic
   variable. if we are given that is drawn from a distribution , and we
   want to take derivatives of a function of with respect to , how do we
   do that? the sample is fixed, but intuitively its derivative should be
   nonzero.

   for some distributions, it is possible to reparametrize samples in a
   clever way, such that the stochasticity is independent of the
   parameters. we want our samples to deterministically depend on the
   parameters of the distribution. for example, in a normally-distributed
   variable with mean and standard devation , we can sample from it like
   this:

   where . going from denoting a draw from the distribution to the equals
   sign is the crucial step. we have defined a function that depends on on
   the parameters deterministically. we can thus take derivatives of
   functions involving , with respect to the parameters of its
   distribution and .
   [reparametrization.png] the reparametrization trick allows us to push
   the randomness of a normally-distributed random variable z into
   epsilon, which is sampled from a standard normal. diamonds indicate
   deterministic dependencies, circles indicate random variables.

   in the variational autoencoder, the mean and variance are output by an
   id136 network with parameters that we optimize. the
   reparametrization trick lets us backpropagate (take derivatives using
   the chain rule) with respect to through the objective (the elbo) which
   is a function of samples of the latent variables .

   is anything in this article confusing or can any explanation be
   improved? please submit a [22]pull request, [23]tweet me, or [24]email
   me :)

  references for ideas and figures

   many ideas and figures are from shakir mohamed   s excellent blog posts
   on the [25]reparametrization trick and [26]autoencoders. durk kingma
   created the great visual of the [27]reparametrization trick. great
   references for variational id136 are this [28]tutorial and david
   blei   s [29]course notes. dustin tran has a helpful blog post on
   [30]id5. the header   s mnist gif is from [31]rui
   shu.

   thanks to rajesh ranganath, andriy mnih, ben poole, jon berliner,
   cassandra xia, and ryan sepassi for discussions and many concepts in
   this article.

   discussion on [32]hacker news and [33]reddit. featured in david
   duvenaud   s course syllabus on [34]   differentiable id136 and
   generative models   .

   [35]back to blog

references

   visible links
   1. https://jaan.io/feed.xml
   2. https://jaan.io/
   3. https://jaan.io/what-is-variational-autoencoder-vae-tutorial/#nav
   4. https://jaan.io/about
   5. https://jaan.io/blog
   6. https://jaan.io/projects
   7. https://jaan.io/publications
   8. https://jaan.io/talks
   9. https://jaan.io/what-is-variational-autoencoder-vae-tutorial/
  10. https://github.com/altosaar/variational-autoencoder
  11. https://jaan.io/what-is-variational-autoencoder-vae-tutorial/#glossary
  12. http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/
  13. https://www.youtube.com/watch?v=xnzin7jh3sg
  14. https://arxiv.org/abs/1502.04623
  15. https://arxiv.org/abs/1509.08731
  16. https://arxiv.org/abs/1312.6114
  17. https://arxiv.org/abs/1401.4082
  18. https://jaan.io/what-is-variational-autoencoder-vae-tutorial/#mean-field
  19. https://giphy.com/embed/26ufovqzdjhoprp8k?html5=true
  20. https://giphy.com/embed/26ufgj5lh3yko1zlu?html5=true
  21. https://github.com/altosaar/variational-autoencoder
  22. https://github.com/altosaar/jaan.io/blob/master/_posts/blog/2016-07-18-what-is-variational-autoencoder-vae-tutorial.md
  23. https://twitter.com/thejaan
  24. https://jaan.io/cdn-cgi/l/email-protection#96f7fae2f9e5f7f7e4d6e6e4fff8f5f3e2f9f8b8f3f2e3
  25. http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/
  26. http://blog.shakirm.com/2015/03/a-statistical-view-of-deep-learning-ii-auto-encoders-and-free-energy/
  27. http://dpkingma.com/?page_id=277
  28. https://arxiv.org/abs/1601.00670
  29. https://www.cs.princeton.edu/courses/archive/fall11/cos597c/lectures/variational-id136-i.pdf
  30. http://dustintran.com/blog/denoising-criterion-for-variational-auto-encoding-framework/
  31. https://github.com/ruishu/variational-autoencoder
  32. https://news.ycombinator.com/edit?id=12292576
  33. https://www.reddit.com/r/machinelearning/comments/4xv5b5/explainer_of_variational_autoencoders_from_a/
  34. http://www.cs.toronto.edu/~duvenaud/courses/csc2541/
  35. https://jaan.io/blog

   hidden links:
  37. https://www.google.com/+jaanaltosaar
  38. https://twitter.com/thejaan
