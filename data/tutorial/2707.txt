   #[1]the asimov institute    feed [2]the asimov institute    comments feed
   [3]the asimov institute    the neural network zoo comments feed
   [4]alternate [5]alternate

   [6]skip to content

[7]the asimov institute

   search for: ____________________ search

     * [8]home
     * [9]team
     * [10]news

[11]the neural network zoo

   posted on [12]september 14, 2016may 9, 2018 by [13]fjodor van veen
     __________________________________________________________________

   with new neural network architectures popping up every now and then,
   it   s hard to keep track of them all. knowing all the abbreviations
   being thrown around (dcign, bilstm, dcgan, anyone?) can be a bit
   overwhelming at first.

   so i decided to compose a cheat sheet containing many of
   those architectures. most of these are neural networks, some are
   completely different beasts. though all of these architectures are
   presented as novel and unique, when i drew the node structures    their
   underlying relations started to make more sense.

   neuralnetworks

   one problem with drawing them as node maps: it doesn   t really show how
   they   re used. for example, id5 (vae) may look just
   like autoencoders (ae), but the training process is actually quite
   different. the use-cases for trained networks differ even more, because
   vaes are generators, where you insert noise to get a new sample. aes,
   simply map whatever they get as input to the closest training sample
   they    remember   . i should add that this overview is in no way
   clarifying how each of the different node types work internally (but
   that   s a topic for another day).

   it should be noted that while most of the abbreviations used are
   generally accepted, not all of them are. id56s sometimes refer to
   id56s, but most of the time they refer to recurrent
   neural networks. that   s not the end of it though, in many places you   ll
   find id56 used as placeholder for any recurrent architecture, including
   lstms, grus and even the bidirectional variants. aes suffer from a
   similar problem from time to time, where vaes and daes and the like are
   called simply aes. many abbreviations also vary in the amount of    n   s
   to add at the end, because you could call it a convolutional neural
   network but also simply a convolutional network (resulting in id98 or
   cn).

   composing a complete list is practically impossible, as new
   architectures are invented all the time. even if published it can still
   be quite challenging to find them even if you   re looking for them, or
   sometimes you just overlook some. so while this list may provide you
   with some insights into the world of ai, please, by no means take this
   list for being comprehensive; especially if you read this post long
   after it was written.

   for each of the architectures depicted in the picture, i wrote a very,
   very brief description. you may find some of these to be useful if
   you   re quite familiar with some architectures, but you aren   t familiar
   with a particular one.
     __________________________________________________________________

   [ff.png]

   feed forward neural networks (ff or ffnn) and id88s (p) are very
   straight forward, they feed information from the front to the back
   (input and output, respectively). neural networks are often described
   as having layers, where each layer consists of either input, hidden or
   output cells in parallel. a layer alone never has connections and in
   general two adjacent layers are fully connected (every neuron form one
   layer to every neuron to another layer). the simplest somewhat
   practical network has two input cells and one output cell, which can be
   used to model logic gates. one usually trains ffnns through
   back-propagation, giving the network paired datasets of    what goes in   
   and    what we want to have coming out   . this is called supervised
   learning, as opposed to unsupervised learning where we only give it
   input and let the network fill in the blanks. the error being
   back-propagated is often some variation of the difference between the
   input and the output (like mse or just the linear difference). given
   that the network has enough hidden neurons, it can theoretically always
   model the relationship between the input and output. practically their
   use is a lot more limited but they are popularly combined with other
   networks to form new networks.

   rosenblatt, frank.    the id88: a probabilistic model for
   information storage and organization in the brain.    psychological
   review 65.6 (1958): 386.
   [14]original paper pdf
     __________________________________________________________________

   [rbf.png]

   radial basis function (rbf) networks are ffnns with radial basis
   functions as id180. there   s nothing more to it. doesn   t
   mean they don   t have their uses, but most ffnns with other activation
   functions don   t get their own name. this mostly has to do with
   inventing them at the right time.

   broomhead, david s., and david lowe. radial basis functions,
   multi-variable functional interpolation and adaptive networks. no.
   rsre-memo-4148. royal signals and radar establishment malvern (united
   kingdom), 1988.
   [15]original paper pdf
     __________________________________________________________________

   [hn.png]

   a hopfield network (hn) is a network where every neuron is connected to
   every other neuron; it is a completely entangled plate of spaghetti as
   even all the nodes function as everything. each node is input before
   training, then hidden during training and output afterwards. the
   networks are trained by setting the value of the neurons to the desired
   pattern after which the weights can be computed. the weights do not
   change after this. once trained for one or more patterns, the network
   will always converge to one of the learned patterns because the network
   is only stable in those states. note that it does not always conform to
   the desired state (it   s not a magic black box sadly). it stabilises in
   part due to the total    energy    or    temperature    of the network being
   reduced incrementally during training. each neuron has an activation
   threshold which scales to this temperature, which if surpassed by
   summing the input causes the neuron to take the form of one of two
   states (usually -1 or 1, sometimes 0 or 1). updating the network can be
   done synchronously or more commonly one by one. if updated one by one,
   a fair random sequence is created to organise which cells update in
   what order (fair random being all options (n) occurring exactly once
   every n items). this is so you can tell when the network is stable
   (done converging), once every cell has been updated and none of them
   changed, the network is stable (annealed). these networks are often
   called associative memory because the converge to the most similar
   state as the input; if humans see half a table we can image the other
   half, this network will converge to a table if presented with half
   noise and half a table.

   hopfield, john j.    neural networks and physical systems with emergent
   collective computational abilities.    proceedings of the national
   academy of sciences 79.8 (1982): 2554-2558.
   [16]original paper pdf
     __________________________________________________________________

   [mc.png]

   markov chains (mc or discrete time markov chain, dtmc) are kind of the
   predecessors to bms and hns. they can be understood as follows: from
   this node where i am now, what are the odds of me going to any of my
   neighbouring nodes? they are memoryless (i.e. markov property) which
   means that every state you end up in depends completely on the previous
   state. while not really a neural network, they do resemble neural
   networks and form the theoretical basis for bms and hns. mc aren   t
   always considered neural networks, as goes for bms, rbms and hns.
   markov chains aren   t always fully connected either.

   hayes, brian.    first links in the markov chain.    american scientist
   101.2 (2013): 252.
   [17]original paper pdf
     __________________________________________________________________

   [bm.png]

   id82s (bm) are a lot like hns, but: some neurons are
   marked as input neurons and others remain    hidden   . the input neurons
   become output neurons at the end of a full network update. it starts
   with random weights and learns through back-propagation, or more
   recently through contrastive divergence (a markov chain is used to
   determine the gradients between two informational gains). compared to a
   hn, the neurons mostly have binary activation patterns. as hinted by
   being trained by mcs, bms are stochastic networks. the training and
   running process of a bm is fairly similar to a hn: one sets the input
   neurons to certain clamped values after which the network is set free
   (it doesn   t get a sock). while free the cells can get any value and we
   repetitively go back and forth between the input and hidden neurons.
   the activation is controlled by a global temperature value, which if
   lowered lowers the energy of the cells. this lower energy causes their
   activation patterns to stabilise. the network reaches an equilibrium
   given the right temperature.

   hinton, geoffrey e., and terrence j. sejnowski.    learning and releaming
   in id82s.    parallel distributed processing: explorations
   in the microstructure of cognition 1 (1986): 282-317.
   [18]original paper pdf
     __________________________________________________________________

   [rbm.png]

   restricted id82s (rbm) are remarkably similar to bms
   (surprise) and therefore also similar to hns. the biggest difference
   between bms and rbms is that rbms are a better usable because they are
   more restricted. they don   t trigger-happily connect every neuron to
   every other neuron but only connect every different group of neurons to
   every other group, so no input neurons are directly connected to other
   input neurons and no hidden to hidden connections are made either. rbms
   can be trained like ffnns with a twist: instead of passing data forward
   and then back-propagating, you forward pass the data and then backward
   pass the data (back to the first layer). after that you train with
   forward-and-back-propagation.

   smolensky, paul. information processing in dynamical systems:
   foundations of harmony theory. no. cu-cs-321-86. colorado univ at
   boulder dept of computer science, 1986.
   [19]original paper pdf
     __________________________________________________________________

   [ae.png]

   autoencoders (ae) are somewhat similar to ffnns as aes are more like a
   different use of ffnns than a fundamentally different architecture. the
   basic idea behind autoencoders is to encode information (as in
   compress, not encrypt) automatically, hence the name. the entire
   network always resembles an hourglass like shape, with smaller hidden
   layers than the input and output layers. aes are also always
   symmetrical around the middle layer(s) (one or two depending on an even
   or odd amount of layers). the smallest layer(s) is|are almost always in
   the middle, the place where the information is most compressed (the
   chokepoint of the network). everything up to the middle is called the
   encoding part, everything after the middle the decoding and the middle
   (surprise) the code. one can train them using id26 by
   feeding input and setting the error to be the difference between the
   input and what came out. aes can be built symmetrically when it comes
   to weights as well, so the encoding weights are the same as the
   decoding weights.

   bourlard, herv  , and yves kamp.    auto-association by multilayer
   id88s and singular value decomposition.    biological cybernetics
   59.4-5 (1988): 291-294.
   [20]original paper pdf
     __________________________________________________________________

   [sae.png]

   sparse autoencoders (sae) are in a way the opposite of aes. instead of
   teaching a network to represent a bunch of information in less    space   
   or nodes, we try to encode information in more space. so instead of the
   network converging in the middle and then expanding back to the input
   size, we blow up the middle. these types of networks can be used to
   extract many small features from a dataset. if one were to train a sae
   the same way as an ae, you would in almost all cases end up with a
   pretty useless identity network (as in what comes in is what comes out,
   without any transformation or decomposition). to prevent this, instead
   of feeding back the input, we feed back the input plus a sparsity
   driver. this sparsity driver can take the form of a threshold filter,
   where only a certain error is passed back and trained, the other error
   will be    irrelevant    for that pass and set to zero. in a way this
   resembles spiking neural networks, where not all neurons fire all the
   time (and points are scored for biological plausibility).

   marc   aurelio ranzato, christopher poultney, sumit chopra, and yann
   lecun.    efficient learning of sparse representations with an
   energy-based model.    proceedings of nips. 2007.
   [21]original paper pdf
     __________________________________________________________________

   [vae.png]

   id5 (vae) have the same architecture as aes but
   are    taught    something else: an approximated id203 distribution
   of the input samples. it   s a bit back to the roots as they are bit more
   closely related to bms and rbms. they do however rely on bayesian
   mathematics regarding probabilistic id136 and independence, as well
   as a re-parametrisation trick to achieve this different representation.
   the id136 and independence parts make sense intuitively, but they
   rely on somewhat complex mathematics. the basics come down to this:
   take influence into account. if one thing happens in one place and
   something else happens somewhere else, they are not necessarily
   related. if they are not related, then the error propagation should
   consider that. this is a useful approach because neural networks are
   large graphs (in a way), so it helps if you can rule out influence from
   some nodes to other nodes as you dive into deeper layers.

   kingma, diederik p., and max welling.    auto-encoding variational
   bayes.    arxiv preprint arxiv:1312.6114 (2013).
   [22]original paper pdf
     __________________________________________________________________

   [dae.png]

   denoising autoencoders (dae) are aes where we don   t feed just the input
   data, but we feed the input data with noise (like making an image more
   grainy). we compute the error the same way though, so the output of the
   network is compared to the original input without noise. this
   encourages the network not to learn details but broader features, as
   learning smaller features often turns out to be    wrong    due to it
   constantly changing with noise.

   vincent, pascal, et al.    extracting and composing robust features with
   denoising autoencoders.    proceedings of the 25th international
   conference on machine learning. acm, 2008.
   [23]original paper pdf
     __________________________________________________________________

   [dbn.png]

   id50 (dbn) is the name given to stacked architectures
   of mostly rbms or vaes. these networks have been shown to be
   effectively trainable stack by stack, where each ae or rbm only has to
   learn to encode the previous network. this technique is also known as
   greedy training, where greedy means making locally optimal solutions to
   get to a decent but possibly not optimal answer. dbns can be trained
   through contrastive divergence or back-propagation and learn to
   represent the data as a probabilistic model, just like regular rbms or
   vaes. once trained or converged to a (more) stable state through
   unsupervised learning, the model can be used to generate new data. if
   trained with contrastive divergence, it can even classify existing data
   because the neurons have been taught to look for different features.

   bengio, yoshua, et al.    greedy layer-wise training of deep networks.   
   advances in neural information processing systems 19 (2007): 153.
   [24]original paper pdf
     __________________________________________________________________

   [id98.png]

   convolutional neural networks (id98 or deep convolutional neural
   networks, did98) are quite different from most other networks. they are
   primarily used for image processing but can also be used for other
   types of input such as as audio. a typical use case for id98s is where
   you feed the network images and the network classifies the data, e.g.
   it outputs    cat    if you give it a cat picture and    dog    when you give
   it a dog picture. id98s tend to start with an input    scanner    which is
   not intended to parse all the training data at once. for example, to
   input an image of 200 x 200 pixels, you wouldn   t want a layer with 40
   000 nodes. rather, you create a scanning input layer of say 20 x 20
   which you feed the first 20 x 20 pixels of the image (usually starting
   in the upper left corner). once you passed that input (and possibly use
   it for training) you feed it the next 20 x 20 pixels: you move the
   scanner one pixel to the right. note that one wouldn   t move the input
   20 pixels (or whatever scanner width) over, you   re not dissecting the
   image into blocks of 20 x 20, but rather you   re crawling over it. this
   input data is then fed through convolutional layers instead of normal
   layers, where not all nodes are connected to all nodes. each node only
   concerns itself with close neighbouring cells (how close depends on the
   implementation, but usually not more than a few). these convolutional
   layers also tend to shrink as they become deeper, mostly by easily
   divisible factors of the input (so 20 would probably go to a layer of
   10 followed by a layer of 5). powers of two are very commonly used
   here, as they can be divided cleanly and completely by definition: 32,
   16, 8, 4, 2, 1. besides these convolutional layers, they also often
   feature pooling layers. pooling is a way to filter out details: a
   commonly found pooling technique is max pooling, where we take say 2 x
   2 pixels and pass on the pixel with the most amount of red. to apply
   id98s for audio, you basically feed the input audio waves and inch over
   the length of the clip, segment by segment. real world implementations
   of id98s often glue an ffnn to the end to further process the data,
   which allows for highly non-linear abstractions. these networks are
   called did98s but the names and abbreviations between these two are
   often used interchangeably.

   lecun, yann, et al.    gradient-based learning applied to document
   recognition.    proceedings of the ieee 86.11 (1998): 2278-2324.
   [25]original paper pdf
     __________________________________________________________________

   [dn.png]

   deconvolutional networks (dn), also called inverse graphics networks
   (igns), are reversed convolutional neural networks. imagine feeding a
   network the word    cat    and training it to produce cat-like pictures, by
   comparing what it generates to real pictures of cats. dnns can be
   combined with ffnns just like regular id98s, but this is about the point
   where the line is drawn with coming up with new abbreviations. they may
   be referenced as deep deconvolutional neural networks, but you could
   argue that when you stick ffnns to the back and the front of dnns that
   you have yet another architecture which deserves a new name. note that
   in most applications one wouldn   t actually feed text-like input to the
   network, more likely a binary classification input vector. think <0, 1>
   being cat, <1, 0> being dog and <1, 1> being cat and dog. the pooling
   layers commonly found in id98s are often replaced with similar inverse
   operations, mainly interpolation and extrapolation with biased
   assumptions (if a pooling layer uses max pooling, you can invent
   exclusively lower new data when reversing it).

   zeiler, matthew d., et al.    deconvolutional networks.    id161
   and pattern recognition (cvpr), 2010 ieee conference on. ieee, 2010.
   [26]original paper pdf
     __________________________________________________________________

   [dcign.png]

   deep convolutional inverse graphics networks (dcign) have a somewhat
   misleading name, as they are actually vaes but with id98s and dnns for
   the respective encoders and decoders. these networks attempt to model
      features    in the encoding as probabilities, so that it can learn to
   produce a picture with a cat and a dog together, having only ever seen
   one of the two in separate pictures. similarly, you could feed it a
   picture of a cat with your neighbours    annoying dog on it, and ask it
   to remove the dog, without ever having done such an operation. demo   s
   have shown that these networks can also learn to model complex
   transformations on images, such as changing the source of light or the
   rotation of a 3d object. these networks tend to be trained with
   back-propagation.

   kulkarni, tejas d., et al.    deep convolutional inverse graphics
   network.    advances in neural information processing systems. 2015.
   [27]original paper pdf
     __________________________________________________________________

   [gan.png]

   id3 (gan) are from a different breed of
   networks, they are twins: two networks working together. gans consist
   of any two networks (although often a combination of ffs and id98s),
   with one tasked to generate content and the other has to judge content.
   the discriminating network receives either training data or generated
   content from the generative network. how well the discriminating
   network was able to correctly predict the data source is then used as
   part of the error for the generating network. this creates a form of
   competition where the discriminator is getting better at distinguishing
   real data from generated data and the generator is learning to become
   less predictable to the discriminator. this works well in part because
   even quite complex noise-like patterns are eventually predictable but
   generated content similar in features to the input data is harder to
   learn to distinguish. gans can be quite difficult to train, as you
   don   t just have to train two networks (either of which can pose it   s
   own problems) but their dynamics need to be balanced as well.
   if prediction or generation becomes to good compared to the other, a
   gan won   t converge as there is intrinsic divergence.

   goodfellow, ian, et al.    generative adversarial nets.    advances in
   neural information processing systems. 2014.
   [28]original paper pdf
     __________________________________________________________________

   [id56.png]

   recurrent neural networks (id56) are ffnns with a time twist: they are
   not stateless; they have connections between passes, connections
   through time. neurons are fed information not just from the previous
   layer but also from themselves from the previous pass. this means that
   the order in which you feed the input and train the network matters:
   feeding it    milk    and then    cookies    may yield different results
   compared to feeding it    cookies    and then    milk   . one big problem with
   id56s is the vanishing (or exploding) gradient problem
   where, depending on the id180 used, information
   rapidly gets lost over time, just like very deep ffnns lose information
   in depth. intuitively this wouldn   t be much of a problem because these
   are just weights and not neuron states, but the weights through time is
   actually where the information from the past is stored; if the weight
   reaches a value of 0 or 1 000 000, the previous state won   t be very
   informative. id56s can in principle be used in many fields as most forms
   of data that don   t actually have a timeline (i.e. unlike sound
   or video) can be represented as a sequence. a picture or a string of
   text can be fed one pixel or character at a time, so the time dependent
   weights are used for what came before in the sequence, not actually
   from what happened x seconds before. in general, recurrent networks are
   a good choice for advancing or completing information, such
   as autocompletion.

   elman, jeffrey l.    finding structure in time.    cognitive science 14.2
   (1990): 179-211.
   [29]original paper pdf
     __________________________________________________________________

   [lstm.png]

   long / short term memory (lstm) networks try to combat the vanishing /
   exploding gradient problem by introducing gates and an explicitly
   defined memory cell. these are inspired mostly by circuitry, not so
   much biology. each neuron has a memory cell and three gates: input,
   output and forget. the function of these gates is to safeguard the
   information by stopping or allowing the flow of it. the input gate
   determines how much of the information from the previous layer gets
   stored in the cell. the output layer takes the job on the other end and
   determines how much of the next layer gets to know about the state of
   this cell. the forget gate seems like an odd inclusion at first but
   sometimes it   s good to forget: if it   s learning a book and a new
   chapter begins, it may be necessary for the network to forget some
   characters from the previous chapter. lstms have been shown to be able
   to learn complex sequences, such as writing like shakespeare or
   composing primitive music. note that each of these gates has a weight
   to a cell in the previous neuron, so they typically require more
   resources to run.

   hochreiter, sepp, and j  rgen schmidhuber.    long short-term memory.   
   neural computation 9.8 (1997): 1735-1780.
   [30]original paper pdf
     __________________________________________________________________

   [gru.png]

   id149 (gru) are a slight variation on lstms. they have
   one less gate and are wired slightly differently: instead of an input,
   output and a forget gate, they have an update gate. this update gate
   determines both how much information to keep from the last state and
   how much information to let in from the previous layer. the reset gate
   functions much like the forget gate of an lstm but it   s located
   slightly differently. they always send out their full state, they don   t
   have an output gate. in most cases, they function very similarly to
   lstms, with the biggest difference being that grus are slightly faster
   and easier to run (but also slightly less expressive). in practice
   these tend to cancel each other out, as you need a bigger network to
   regain some expressiveness which then in turn cancels out the
   performance benefits. in some cases where the extra expressiveness is
   not needed, grus can outperform lstms.

   chung, junyoung, et al.    empirical evaluation of gated recurrent neural
   networks on sequence modeling.    arxiv preprint arxiv:1412.3555 (2014).
   [31]original paper pdf
     __________________________________________________________________


   [ntm.png]

   id63s (ntm) can be understood as an abstraction of
   lstms and an attempt to un-black-box neural networks (and give us some
   insight in what is going on in there). instead of coding a memory cell
   directly into a neuron, the memory is separated. it   s an attempt to
   combine the efficiency and permanency of regular digital storage and
   the efficiency and expressive power of neural networks. the idea is to
   have a content-addressable memory bank and a neural network that can
   read and write from it. the    turing    in id63s comes
   from them being turing complete: the ability to read and write and
   change state based on what it reads means it can represent anything a
   universal turing machine can represent.

   graves, alex, greg wayne, and ivo danihelka.    id63s.   
   arxiv preprint arxiv:1410.5401 (2014).
   [32]original paper pdf
     __________________________________________________________________

   id182, bidirectional long / short
   term memory networks and bidirectional id149 (biid56,
   bilstm and bigru respectively) are not shown on the chart because they
   look exactly the same as their unidirectional counterparts. the
   difference is that these networks are not just connected to the past,
   but also to the future. as an example, unidirectional lstms might be
   trained to predict the word    fish    by being fed the letters one by one,
   where the recurrent connections through time remember the last value. a
   bilstm would also be fed the next letter in the sequence on the
   backward pass, giving it access to future information. this trains the
   network to fill in gaps instead of advancing information, so instead of
   expanding an image on the edge, it could fill a hole in the middle of
   an image.

   schuster, mike, and kuldip k. paliwal.    bidirectional recurrent neural
   networks.    ieee transactions on signal processing 45.11 (1997):
   2673-2681.
   [33]original paper pdf
     __________________________________________________________________

   [drn.png]

   deep residual networks (drn) are very deep ffnns with extra connections
   passing input from one layer to a later layer (often 2 to 5 layers) as
   well as the next layer. instead of trying to find a solution for
   mapping some input to some output across say 5 layers, the network is
   enforced to learn to map some input to some output + some input.
   basically, it adds an identity to the solution, carrying the older
   input over and serving it freshly to a later layer. it has been shown
   that these networks are very effective at learning patterns up to 150
   layers deep, much more than the regular 2 to 5 layers one could expect
   to train. however, it has been proven that these networks are in
   essence just id56s without the explicit time based construction and
   they   re often compared to lstms without gates.

   he, kaiming, et al.    deep residual learning for image recognition.   
   arxiv preprint arxiv:1512.03385 (2015).
   [34]original paper pdf
     __________________________________________________________________

   [esn.png]

   echo state networks (esn) are yet another different type of (recurrent)
   network. this one sets itself apart from others by having random
   connections between the neurons (i.e. not organised into neat sets of
   layers), and they are trained differently. instead of feeding input and
   back-propagating the error, we feed the input, forward it and update
   the neurons for a while, and observe the output over time. the input
   and the output layers have a slightly unconventional role as the input
   layer is used to prime the network and the output layer acts as an
   observer of the activation patterns that unfold over time. during
   training, only the connections between the observer and the (soup of)
   hidden units are changed.

   jaeger, herbert, and harald haas.    harnessing nonlinearity: predicting
   chaotic systems and saving energy in wireless communication.    science
   304.5667 (2004): 78-80.
   [35]original paper pdf
     __________________________________________________________________

   [elm.png]

   extreme learning machines (elm) are basically ffnns but with random
   connections. they look very similar to lsms and esns, but they are not
   recurrent nor spiking. they also do not use id26. instead,
   they start with random weights and train the weights in a single step
   according to the least-squares fit (lowest error across all functions).
   this results in a much less expressive network but it   s also much
   faster than id26.

   cambria, erik, et al.    extreme learning machines [trends &
   controversies].    ieee intelligent systems 28.6 (2013): 30-59.
   [36]original paper pdf
     __________________________________________________________________

   [lsm.png]

   liquid state machines (lsm) are similar soups, looking a lot like esns.
   the real difference is that lsms are a type of spiking neural networks:
   sigmoid activations are replaced with threshold functions and each
   neuron is also an accumulating memory cell. so when updating a neuron,
   the value is not set to the sum of the neighbours, but rather added to
   itself. once the threshold is reached, it releases its    energy to other
   neurons. this creates a spiking like pattern, where nothing happens for
   a while until a threshold is suddenly reached.

   maass, wolfgang, thomas natschl  ger, and henry markram.    real-time
   computing without stable states: a new framework for neural computation
   based on perturbations.    neural computation 14.11 (2002): 2531-2560.
   [37]original paper pdf
     __________________________________________________________________

   [id166.png]

   support vector machines (id166) find optimal solutions for classification
   problems. classically they were only capable of categorising linearly
   separable data; say finding which images are of garfield and which of
   snoopy, with any other outcome not being possible. during training,
   id166s can be thought of as plotting all the data (garfields and snoopys)
   on a graph (2d) and figuring out how to draw a line between the data
   points. this line would separate the data, so that all snoopys are on
   one side and the garfields on the other. this line moves to an optimal
   line in such a way that the margins between the data points and the
   line are maximised on both sides. classifying new data would be done by
   plotting a point on this graph and simply looking on which side of the
   line it is (snoopy side or garfield side). using the kernel trick, they
   can be taught to classify n-dimensional data. this entails plotting
   points in a 3d plot, allowing it to distinguish between snoopy,
   garfield and simon   s cat, or even higher dimensions distinguishing even
   more cartoon characters. id166s are not always considered neural
   networks.

   cortes, corinna, and vladimir vapnik.    support-vector networks.   
   machine learning 20.3 (1995): 273-297.
   [38]original paper pdf
     __________________________________________________________________

   [kn.png]

   and finally, kohonen networks (kn, also self organising (feature) map,
   som, sofm)    complete    our zoo. kns utilise competitive learning to
   classify data without supervision. input is presented to the network,
   after which the network assesses which of its neurons most closely
   match that input. these neurons are then adjusted to match the input
   even better, dragging along their neighbours in the process. how much
   the neighbours are moved depends on the distance of the neighbours to
   the best matching units. kns are sometimes not considered neural
   networks either.

   kohonen, teuvo.    self-organized formation of topologically correct
   feature maps.    biological cybernetics 43.1 (1982): 59-69.
   [39]original paper pdf
     __________________________________________________________________

   any feedback and criticism is welcome. at the asimov institute we do
   deep learning research and development, so be sure to follow us on
   [40]twitter for future updates and posts! thank you for reading!

   [update 15 september 2016] i would like to thank everybody for their
   insights and corrections, all feedback is hugely appreciated. i will
   add links and a couple more suggested networks in a future update, stay
   tuned.

   [update 29 september 2016] added links and citations to all the
   original papers. a follow up post is planned, since i found at least 9
   more architectures. i will not include them in this post for better
   consistency in terms of content.

   [41][update 30 november 2017] looking for a poster of the neural
   network zoo? click here

   [42]deep learning

post navigation

   [43]artistic style transfer blending
   [44]analyzing six deep learning tools for music generation


   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   ______________________
   ______________________

   post comment

    1. volkan cirik
       sep 14, 2016
       how about id56s?
       [45]reply
          + fjodor van veen
            sep 14, 2016
            interesting, another branch of networks. will definitely
            incorporate them in a potential follow up post!
            [46]reply
    2. colin morris
       sep 14, 2016
       awesome. minor correction regarding id82s.    compared
       to a hn, the neurons also sometimes have binary activation patterns
       but at other times they are stochastic   . the units are always
       stochastic, and usually also binary. but there are variations where
       units are instead gaussian, binomial, etc.
       [47]reply
          + fjodor van veen
            sep 14, 2016
            good stuff. thank you for your feedback!
            [48]reply
          + b@rmaley.e>
            sep 14, 2016
            i don   t understand why in markov chains you have a
            fully-connected graph. it should be either a chain, or a
            generalized    chain    where previous m nodes have edges to
            current node (if we   re talking about m-order markov chains)
            [49]reply
               o fjodor van veen
                 sep 14, 2016
                 thank you for your feedback! when you say chain, you mean
                 something like this?
                 [50]http://blogs.canisius.edu/mathblog/wp-content/uploads
                 /sites/31/2013/02/markovchainpic3.png
                 or more like this?
                 [51]http://1.bp.blogspot.com/-ulvbbbntdti/tbwsnea4zpi/aaa
                 aaaaaaby/xyrtcxrhskq/s1600/markovchain.png
                 note that all architectures have a rather finite
                 representation here. as pointed out elsewhere, the daes
                 often have a complete or overcomplete hidden layer, but
                 not always. most examples i can find are either arbitrary
                 chains or fully connected graphs.
                 [52]reply
                    # b@rmaley.exe
                      sep 15, 2016
                      yes, something like this but without loops.
                      basically a bunch of probabilistic cells (if you
                      make them hidden, you get a id48) chained in a linear
                      way.
                      [53]reply
                         @ fjodor van veen
                           sep 15, 2016
                           id48, will take that into consideration. the
                           cells themselves are not probabilistic though,
                           the connections between them are. i   m
                           considering giving a few example networks for
                           some architectures, especially the ones that
                           vary greatly like this one.
    3. daniel fay
       sep 14, 2016
       cool! i think you could give the denoising autoencoder a
       higher-dimensional hidden layer since it doesn   t need a bottleneck.
       [54]reply
          + fjodor van veen
            sep 14, 2016
            from what i understand, daes are simply aes but used
            differently, which themselves are just symmetric ffnns used
            differently. it is true that daes are often either complete or
            overcomplete, since overcomplete aes usually learn identity;
            this is what daes are trying to prevent. the size restrictions
            are rarely explicitly defined though. thank you for reading!
            [55]reply
               o jim
                 sep 15, 2016
                 this reply is a hidden layer
                 [56]reply
                    # stylianos iordanis
                      sep 15, 2016
                      excellent work!
                      could you please enhance the article by adding and
                      presenting the id48 using exactly
                      the same approach ? that would be very enlightening,
                      and i am very curious to read your explanation.
                      thank you
                      [57]reply
                         @ fjodor van veen
                           sep 15, 2016
                           working on it [:
    4. muhong guo
       sep 15, 2016
       awesome! could you also add some references for each network?
       [58]reply
          + fjodor van veen
            sep 15, 2016
            i   d love to, if i can find the time [:
            [59]reply
    5. summit suen
       sep 15, 2016
       awesome work! btw, should the middle neurons in the deep
       convolutional inverse graphics networks (dcign) be probabilistic
       hidden cells?
       [60]reply
          + fjodor van veen
            sep 15, 2016
            yes. good observation, will correct!
            [61]reply
    6. kristofer
       sep 15, 2016
       very nice summary!
       from the pictures of the denoising autoencoder and the variational
       autoencoder, it looks like they also have to compress the data
       (since the hidden layer has fewer elements than the input and
       output layers), just the ordinary autoencoder. is that the case?
       also, is there some specific name for the ordinary autoencoder to
       let people know that you are talking about an autoencoder that
       compresses the data? perhaps    compressive autoencoder   ? to me, the
       term    autoencoder    includes all kinds of autoencoders, i.e. also
       denoising, variational and sparse autoencoders, not just
          compressive    (?) autoencoders. (so to me it feels a bit wrong to
       talk about    autoencoders    like all of them compress the data.)
       [62]reply
          + fjodor van veen
            sep 15, 2016
            a common problem with the networks. id56 stands for either
            recurrent nn or recursive nn. worse yet, people use id56 to
            indicate lstm, gru, bigru and bilstm. and no, there is no name
            for that.
            and yes, a problem with this    one shot    representation is that
            you cannot capture all uses. ae, vae, sae and dae are all
            autoencoders, each of which somehow tries to reconstruct their
            input. only saes almost always have an overcomplete code (more
            hidden neurons than input / output), but the others can have
            compressed, complete or overcomplete codes.
            thank you for your feedback!
            [63]reply
    7. djeb
       sep 15, 2016
       amazing. what software did you used to plot these figures ? cheers
       !
       [64]reply
          + fjodor van veen
            sep 15, 2016
            i drew them in adobe animate, they   re not plots. yes it was a
            lot of work to draw the lines.
            [65]reply
               o vitali
                 sep 28, 2016
                 therefore they look really amazing! would it be possible
                 to publish the images as vector? maybe in svg or some
                 other format? i would like to use them in my master
                 thesis. of course i would cite you     
                 [66]reply
                    # fjodor van veen
                      sep 29, 2016
                      thanks, please do! i have a poster link of the whole
                      thing which is 8k resolution, best i can do for now
                      sorry.
                      [67]reply
                         @ joshua pratt
                           jun 05, 2017
                           could another version of the poster be produced
                           with the descriptions incorporated?
                           i   d love to hang one somewhere to keep them
                           fresh in my memory (and also because the art
                           work is lovely).
                         @ david koubek
                           jan 21, 2018
                           wow great work on the summary and high quality
                           images! may i also cite you in my thesis and
                           use a snippet of the 8k poster for
                           demonstrative purposes? thanks for the article,
                           anns are less confusing after reading it.)
    8. rob
       sep 15, 2016
       never thought of id166s as a network, though they can be used that
       way. this seems to be a different take on id166s than i   ve heard
       before.
       [68]reply
    9. philip regenie
       sep 15, 2016
       absolutely the best article and classification i have read in 10
       years. clearly written, easily understood. points anyone reading to
       research those areas applicable to their problem set.
       [69]reply
   10. jagannath rajagopal
       sep 15, 2016
       one of the most useful and comprehensive posts i   ve seen in a
       while. very nicely done     
       i   m the creator of deeplearning.tv on youtube. if you would like to
       follow this up with a series of videos explaining this, i would be
       honored to collaborate with you     
       [70]reply
   11. york fun
       sep 15, 2016
       very nice work   
       i have a question   why id166 could be seen as as a 3 level network   and
       the input cells are not full connected to the first hiden level,
       what   s that mean?
       [71]reply
          + fjodor van veen
            sep 15, 2016
            the idea behind deeper id166s is that they allow for
            classification tasks more complex than binary. usually it
            would just be the directly one-to-one connected stuff as seen
            in the first layer. it   s set up like that because each neuron
            in the input represents a point in the possibility space, and
            the network tries to separate the inputs with a margin as
            large as possible.
            [72]reply
               o york fun
                 sep 16, 2016
                 thanks for your quick reply   
                 could you please give me some reference papers about
                 id166-network, i    m interested in this research how neural
                 network could union these algorthims.
                 [73]reply
                    # julian liersch
                      jan 28, 2018
                      embedding id166s in the framework of neural networks
                      is an intriguing idea. i   d love to read more about
                      this, too. is there a paper on this or where does
                      this idea come from?
                      [74]reply
   12. conic
       sep 15, 2016
       can you eventually give a link to a high resolution image of these
       networks? i was thinking about getting a poster printed for myself.
       [75]reply
          + fjodor van veen
            sep 15, 2016
            cool! see the bottom of the post.
            [76]reply
   13. abatesins
       sep 15, 2016
       nice job, summarizing and representing all of these! one
       observation regarding gans is that the discriminator, as originally
       conceived, has only two output neurons (whether the sample passed
       to it came from the true distribution or from the generated one).
       the corresponding graphic shows three.
       [77]reply
          + fjodor van veen
            sep 15, 2016
            yes. on it. although you could also make do with one, i think
            i   ll draw it with two. thank you!
            [78]reply
               o abatesins
                 sep 15, 2016
                 you   re right! the discriminator does actually a
                 regression on that one decision so one output is indeed
                 more precise (although having two neurons drawn looks
                 more representative to me)
                 [79]reply
          + fjodor van veen
            sep 15, 2016
            fixed it.
            [80]reply
   14. danijar hafner
       sep 15, 2016
       hi, thanks for the very nice visualization! a common mistake with
       id56s is to not connect neurons within the same layer. while each
       lstm neuron has its own hidden state, its output feeds back to all
       neurons in the current layer. the mistake also appears here.
       [81]reply
          + fjodor van veen
            sep 15, 2016
            keen eye! forgot to draw the lines, but very aware of the fine
            workings. it will make it a spiderweb of lines, but eh [:
            [update] fixed
            [82]reply
   15. garrett smith
       sep 15, 2016
       are you excellent images available for reuse under a particular
       license? do you have an attribution policy?
       [83]reply
          + fjodor van veen
            sep 16, 2016
            as long as you mention the author and link to the asimov
            institute, use them however and wherever you like!
            [84]reply
   16. fjodor van veen
       sep 16, 2016
       if you look closely, you   ll find only completely blind people will
       not be able to read the image. there are slight lines on the circle
       edges with unique patterns for each of the five different colours.
       [85]reply
          + john m danskin
            sep 16, 2016
            if i turn off color blind enhancement, i can technically tell
            input and hidden cells apart, especially if they are adjacent.
            in practice, i was using a program which tells me the color
            under the cursor to be sure. in the normal view, it   s as if
            you used coins differing only in the date minted, instead of
            gold and green. i don   t want to give you a hard time, i just
            noticed that you probably spent a lot of time on the graphics,
            and i thought i   d share what i can actually see. no reply
            necessary.
            [86]reply
   17. rohit ghosh
       sep 16, 2016
       hey , nice coverage ! even wasn   t aware of a couple of
       architectures myself. btw, you can integrate fully convolutional
       nets (like fcn, u-net, v-nets ) etc. also deep recurrent
       convolutional nets (drcns) might be a good idea to give an idea of
       merging of id56s & id98s
       [87]reply
          + fjodor van veen
            sep 16, 2016
            will look into those. may have a to draw a line at some point,
            i cannot add all the possible permutations of all different
            cells. at some point someone may find a use for deep residual
            convolutional long short term memory echo state networks [:
            thank you for pointing them out though!
            [88]reply
   18. vivek menon
       sep 16, 2016
       this is beautiful. thank you so much.
       [89]reply
   19. aditya
       sep 17, 2016
       hi, could you add horizontal line between each explanation please.
       i get confused easily.
       thanks for the wonderful explanation
       [90]reply
          + fjodor van veen
            sep 17, 2016
            good idea, thank you!
            [91]reply
   20. amir
       sep 17, 2016
       good job! thanks a lot.
       [92]reply
   21. soroor
       sep 17, 2016
       hi
       that   s very nice
       thank you so much
       if i want to use some of these information in my paper, how can
       cited?
       [93]reply
          + fjodor van veen
            sep 17, 2016
            i   d strongly recommend citing original papers, but feel free
            to use the images depicted here. conventional citation methods
            should suffice just fine. thank you for your interest!
            [94]reply
   22. aaron c sanchez
       sep 18, 2016
       hi, great post, just a question. doesn   t the number of outputs in a
       kohonen network should be 2 and the input n? because those networks
       help you mapping multidimensional data into (x,y) coordinates for
       visualization, if i   m wrong, please correct me. and kohonen
       networks help in id84, your input data should
       be multidimensional and it   s mapped to one or two dimensions.
       [95]https://www.cs.bham.ac.uk/~jlw/sem2a2/web/kohonen.htm
       [96]reply
          + fjodor van veen
            sep 18, 2016
            i think it   s a matter of choice, i see both representations
            frequently. as far as i understand it, the output you get is
            just one of the inputs.
            [97]https://www.google.nl/search?q=kohonen+network&source=lnms
            &tbm=isch&sa=x&ved=0ahukewjukrn9wjnpahwiqjokhzkwdz4q_auiid35b&b
            iw=1345&bih=1099&dpr=2#imgrc=_
            [98]reply
          + ch
            feb 07, 2017
            traditional kohonen nets has k inputs and j processing nodes
            (outputs). it is a competitive learning type of network with
            one layer (if we ignore the input vector). the output nodes
            (processing nodes) are traditionally mapped in 2d to reflect
            the topology in the input data (say, pixels in an image).
            [99]reply
   23. a
       sep 19, 2016
       id166 is not implemented using neural networks. this is a beast from
       another zoo   
       [100]reply
          + fjodor van veen
            sep 19, 2016
            i know, that   s why i made a note of it. i included them
            because you can represent them as a network and they are used
            in machine learning applications [:
            [101]reply
   24. vivek
       sep 19, 2016
       hey there,
       thanks for the post. i   ve attempted to convert this into
       flashcards. it   s incredibly rough and wordy at the moment, but i
       will refine this over time. i am also still searching for
       definitions for your cell structures (backfed input cell, memory
       cell, etc.)
       it   s available here:
       [102]https://quizlet.com/152146730/neural-network-zoo-flash-cards/
       my objectives are twofold:
       1) i wanted to make sure you were okay with me retrofitting your
       work like this.
       2) i was hoping you   d be able to help me fill in some of the blanks
       (literally and figuratively). i have tried to copy your blog post,
       but as i refine and explore, i   d love to have your input and notes
       to help inform its direction.
       if you   re interested / want me to take it down, please let me know
       on here or via email.
       [103]reply
          + fjodor van veen
            sep 19, 2016
            sure thing, cool project! leave a note somewhere to the asimov
            institute and my name and i   m happy [:
            i may do a follow up post explaining the different cells.
            they   re not official names, i just made them up. if you want
            to move forward quickly i   d recommend looking up the original
            papers; i   m confident you   ll be able to figure out why i gave
            the cells their names if you completely understand the
            networks and their uses. if you think of better names for the
            different cells, let me know!
            if you have very specific questions, feel free to mail them to
            me, i can probably answer them relatively quickly.
            [104]reply
   25. auro tripathy
       sep 22, 2016
       curious why there;   s no mention of network-in-network, the basis
       for googlenet inception
       [105]reply
          + fjodor van veen
            sep 22, 2016
            i would argue it;   s not curious at all, i simply overlooked it
            like some others! thank you for pointing it out though, i will
            include it in the update!
            [106]reply
   26. chris greene
       sep 26, 2016
       this is pretty awesome.      i think that a great educational
       enhancement to this would be cite the original papers that
       introduced the associated network. be a great way to introduce
       people learning to both the higher order concepts and the
       literature.
       [107]reply
          + fjodor van veen
            sep 29, 2016
            coming soon    and thank you very much!
            [108]reply
   27. artem
       sep 29, 2016
       extremely cool review. short, simple, informative, contains
       references to original papers. thank you very much!
       [109]reply
   28. guy fortabat
       sep 29, 2016
       i am stuned by all th   possibilit  s offered by ann. i started to
       learn just few months ago and i am 60 ! so many thanks to you to
       have written superb articles ! guy from paris
       [110]reply
   29. gida
       sep 30, 2016
       hello! would you please clarify, are the coloured discs cells
       within a neuron, and their network is an architecture within a
       neuron or how is it organised exactly? thank you very much.
       [111]reply
          + fjodor van veen
            oct 02, 2016
            each    disc    or circle represents a singular artificial
            neurone, the architecture is mostly the types of neurones used
            and how they   re connected. it   s also a little bit how they   re
            used, as some different neurones are really the same neurones
            but used in a different way (take noisy inputs and backfed
            outputs). each connection represents a connection between two
            cells, simulating a neurite in a way.
            [112]reply
   30. andrej w.
       oct 02, 2016
       hey,
       really nice post ! i   ve yet another suggestion for an additional
       network^^ :
       it   s kind of an improved version of the echo state network where
       the crucial point is that they feed back the erroneous output back
       into the network. the first paper is this here:
       [113]http://neurotheory.columbia.edu/larry/sussilloneuron09.pdf
       they also managed to make the training work by changing the weights
       in the network. maybe you can just add it as more information for
       the echo state networks. there are several follow up papers on
       larry abbotts webpage where the link is from but i don   t know them
       (yet).
       [114]reply
          + fjodor van veen
            oct 02, 2016
            thank you for your comment, cool findings. i will look into
            those; i may add them to the follow up post.
            [115]reply
   31. shuichi shigeno
       oct 07, 2016
       this is very good job. i would suggest where is the origin
       (id88) and make simple phylogenetic tree. because here is
       zoo.
       many thanks for this site.
       [116]reply
   32. mehran
       oct 11, 2016
       excellent work..thank you so much. when we will see the other
       architectures?
       [117]reply
   33. manu
       oct 14, 2016
       extreme learning machines don   t use id26, they are
       random projections + a linear model.
       [118]reply
          + fjodor van veen
            oct 21, 2016
            will look into it. thanks for pointing it out!
            [119]reply
   34. sirius fuenmayor
       oct 15, 2016
       it would be great if you could add the dynamics of each type of
       cell.
       [120]reply
   35. sebastian
       oct 17, 2016
       you should think about selling the summary figure as a poster.
       [121]reply
          + fjodor van veen
            oct 28, 2016
            thanks for the suggestion, but i prefer to keep it as
            informative. if i end the post with a link to a webshop people
            will might see the whole thing as an ad.
            [122]reply
   36. pinouchon
       oct 20, 2016
       would love to see contractive aes in here
       [123]reply
   37. roman
       oct 21, 2016
       i have never seen id166s classified as neural networks. the paper
       does call them support vector _networks_ and does compare/contrast
       them to id88s, but i still think it   s a fundamentally
       different approach to learning. it   s not connectivist.
       [124]reply
   38. machiner_ps
       oct 24, 2016
       where is helmholtz machine ?
       [125]reply
   39. arif jahangir
       nov 10, 2016
       your zoo is very beautiful and it is difficult to make it more
       beautiful. i was wondering whether we can add two more information
       to each network. one is weight constraints on each network and
       other is learning algorithm of each algorithm. i think your zoo
       will become a little more beautiful. for learning algorithm, we may
       add another legend and place bars of different color under each
       network depicting what kind of learning algorithms may be used in
       each network. i could not figure out how the weight constrained
       information can be fitted artistically into this scheme. maybe
       others can pitch in.
       [126]reply
   40. mike
       dec 04, 2016
       just another question out of curiosity: which one of the neural
       networks presented here is nearest to an nmda receptor?
       just asking because of this article:
       [127]http://journal.frontiersin.org/article/10.3389/fnsys.2016.0009
       5/full
       [128]reply
          + fjodor van veen
            dec 06, 2016
            i reckon a combination of ff and possibly elm. sounds like a
            large deep ff network but with permanent learned dropout in
            the deeper layers. interesting!
            [129]reply
   41. ilya
       dec 07, 2016
       great article, i keep sharing it with friends and colleagues,
       looking forward to follow-up post with the new architectures.
       you mention demos of dcigns using more complex transformations, any
       way to get a link to one or at least the name of the researcher who
       did it.
       [130]reply
          + fjodor van veen
            dec 22, 2016
            thank you! the original paper includes examples of rotation i
            believe.
            [131]reply
   42. massimo de gregorio
       dec 12, 2016
       unfortunately, you forgot to mention all the family of weightless
       neural systems.
       have a look at them, they are really interesting and powerful.
       massimo
       [132]reply
          + peter_m
            mar 06, 2017
            amazing, thank you.
            [133]reply
   43. sylvain pronovost
       jan 03, 2017
       wonderful work! i would add    cascade correlation    anns, by fahlman
       and lebiere (1989). the cascade correlation learning architecture
       is:
                
       cascade-correlation begins with a minimal network, then
       automatically trains and adds new hidden units one by one, creating
       a multi-layer structure. once a new hidden unit has been added to
       the network, its input-side weights are frozen. this unit then
       becomes a permanent feature-detector in the network, available for
       producing outputs or for creating other, more complex feature
       detectors.
                
       you could say that is a meta-algorithm, and a generative
       architecture. it adds hidden neurons as required by the task at
       hand. i know of two versions, cascade-correlation and recurrent
       cascade-correlation.
       primary source: fahlman, s. e., & lebiere, c. (1989). the
       cascade-correlation learning architecture.
       [134]reply
   44. neocognitron
       jan 03, 2017
       really great article. i just finished this semester   s course in
       neural nets and feel this nn zoo perhaps lacks a few networks;
       in competitive learning; standard competitive network and lateral
       inhibition net. you included kohonen som, so that   s nice. further
       we have the growing hiearchical som (ghsom) & variations of it.
       further; the counter-propagation network is nice; a combo of
       competitive/kohonen layer & a feed forward layer, iirc, very
       practical (supervised learning on top of self-organized learning).
       in associative memory nets there is also the associatron and
       bi-directional associative memory net (bam), and nummerus
       modifications (ieee has some articles, i.e. for pbham).
       we also have the cognitron and neocognitron which were developed
       for rotation-invariant recognition of visual patterns.
       lastly there are the generative advisarial networks, meant more for
       generating data than classifying, iirc (there is also the patented
          creativity machine   /   imagination engine    by stephen thaler).
       thanks again.     
       [135]reply
          + neocognitron
            jan 03, 2017
            forgot to say i like comprehensive overviews or lists, that is
            mostly why i mentioned these networks here.
            [136]reply
   45. ahmed
       jan 05, 2017
       haven   t read the whole thing, but the graphics used are so
       aesthetically pleasing, that i simply fell in love with it. thanks
       mate
       [137]reply
   46. hrushikesh
       jan 11, 2017
       great post. thanks
       [138]reply
   47. santiago
       jan 20, 2017
       great resource     
       small comment: lstm paper   s link does not work anymore     
       [139]reply
   48. cap
       feb 03, 2017
       are there any semi supervised nn? if it would be nice to add info
       on them.
       [140]reply
   49. js
       feb 08, 2017
       it would be very convenient to make the keys/legend remain close to
       each graphical explanation !
       [141]reply
   50. alexander jarvis
       feb 22, 2017
       wow    you are a boss!
       [142]reply
          + valeriya simeonova
            jul 19, 2017
            can you add please the spike nn, boolean nn anf fuzzy nn
            [143]reply
   51. karan patel
       may 06, 2017
       great article
       [144]reply
   52. subramaniam vaithiyalingam
       may 22, 2017
       awesome work! i have started using it as a handy deep learning
       cookbook!
       [145]reply
   53. joblow
       jul 18, 2017
       hey, if you read this, please consider keeping the legend on the
       side as we scroll. i kept having to go back up to check the purpose
       of the nodes. great article!
       [146]reply
   54. robert lucente
       aug 27, 2017
       minor nit. if you update the article, please consider stating that
       dcgan stands for deep convolutional id3
       [147]reply
   55. sm
       sep 19, 2017
       very nice summary of the various structures. would it be possible
       to reuse some of the pictures for an (academic) presentation,
       giving proper credits?
       [148]reply
   56. arun kumar kalakanti
       oct 06, 2017
       good job. the clean, precise and one of the best description.
       thanks a lot !
       [149]reply
   57. soufiane
       oct 30, 2017
       hello,
       thank you for this great work!
       i think it would be better to use arrows (when necessary) to
       indicate the stream of information between neurons. for instance,
       it could be helpful in order to distinguish between a deep
       feedforward network and and rbm.
       again, great job!
       [150]reply
   58. moht
       nov 29, 2017
       this post is awesome but needs some updates. there are lots of
       networks that need to be included.
       [151]reply
   59. ck
       jan 22, 2018
       this is an awesome initiative, giving an overview of models of
       neural nets out there, referencing original papers. now, a nice
       squel could be a material covering methodology of training and
       approach used in hand-picked applications (e.g. alphago).
       [152]reply
   60. khadidja
       apr 07, 2018
       thank you very much!
       [153]reply
   61. jonas
       may 23, 2018
       would it make sense to put some pseudo-code or tensorflow code
       snippets along with the models to better illustrate how to setup a
       test?
       [154]reply
   62. santosh manicka
       jun 19, 2018
       thank you for the comprehensive survey! this is very useful. i was
       wondering if you could also add a section on    continuous-time
       recurrent neural networks    (ctid56) which are often using the
       cognitive sciences?
       [155]https://en.wikipedia.org/wiki/recurrent_neural_network#continu
       ous-time
       thanks!
       [156]reply
   63. cheat sheets for ai, neural networks, machine learning, deep
       learning & big data     codesign.blog
       aug 07, 2018
       [   ] neural networks cheat
       sheet: http://www.asimovinstitute.org/neural-network-zoo/ [   ]
       [157]reply
   64. pablo w. mart  nez
       sep 02, 2018
       really nice work
       [158]reply
   65. vmworld session vap2340bu     resources | wondernerd.net
       sep 14, 2018
       [   ] the neural network zoo:
       [159]http://www.asimovinstitute.org/neural-network-zoo/ [   ]
       [160]reply
   66. ashraf zia
       sep 25, 2018
       its a good review of the networks. i would suggest extending the
       article with other deep learning frameworks available like resnet,
       densenet, vgg, inception, googlenet etc along with the fine-tuning
       concepts.
       [161]reply
   67. machine learning resources     mlait
       oct 15, 2018
       [   ] the neural network zoo (cheat sheet of nn architectures) [   ]
       [162]reply
   68. #el30 graphing     clyde street
       nov 07, 2018
       [   ] second resource shared by stephen is fjodor van veen   s (2016)
       neural network zoo. in his post fjodor shares a    mostly complete
       chart of neural [   ]
       [163]reply
   69. deep learning for natural language processing     part ii     trifork
       blog
       nov 13, 2018
       [   ] there are many more curiosities and things to learn about the
       neural network zoo. if that   s something that would make you more
       interested in neural networks and their intrinsic details, you can
       find it all in their own blog post
       here: https://www.asimovinstitute.org/neural-network-zoo/. [   ]
       [164]reply
   70. id158s     let's get started with machine
       learning
       jan 05, 2019
       [   ] source: the asimov institute [   ]
       [165]reply
   71. how to visualize a neural network     pythoncharm
       jan 08, 2019
       [   ] the first parameter defines the theme of node. for a neural
       network node (theme start with    nn.   ), its style refers from neural
       network zoo page    [   ]
       [166]reply
   72.                                                         techbird     techbird                                               
       jan 11, 2019
       [   ] o   reilly media, 2017 s. raschka et al. deeper insights into
       machine learning. packt, 2016
       [167]http://www.asimovinstitute.org/neural-network-zoo/
       [168]https://www.microsoft.com/en-us/research/wp-content/uploads/20
       12/01/tricks-2012.pdf [   ]
       [169]reply
   73.                                            [            &                   ]     techbird     techbird    
                                                 
       jan 11, 2019
       [   ]     lstm                                         lstm ~                              the neural network zoo
       ->                   id98, [   ]
       [170]reply
   74. how not to start with machine learning     clickedyclick
       feb 01, 2019
       [   ] the network right is the big part of the whole thing, that   s
       how it look). sources like the neural network zoo by the asimov
       institute, which shows different networks for different kinds of
       [   ]
       [171]reply
   75. making pictures     chris luginbuhl
       feb 19, 2019
       [   ] of neural networks and found some great resources. the asimov
       institute   s neural network zoo (link), and piotr midga     s very
       insightful paper on medium about the value of visualizing in [   ]
       [172]reply
   76. deep learning for natural language processing     part ii     robot and
       machine learning
       mar 04, 2019
       [   ] there are many more curiosities and things to learn about the
       neural network zoo. if that   s something that would make you more
       interested in neural networks and their intrinsic details, you can
       find it all in their own blog post
       here: https://www.asimovinstitute.org/neural-network-zoo/. [   ]
       [173]reply

the asimov institute

for artificial creativity & constraint

blog

     * [174]the neural network zoo (2016)
     * [175]neural network zoo prequel: cells and layers
     * [176]analyzing six deep learning tools for music generation
     * [177]the neural network zoo
     * [178]artistic style transfer blending
     * [179]a quick guide to installing tensorflow on mac os

links

     * [180]github
     * [181]bitbucket
     * [182]linkedin

admin

     * [183]log in
     * [184]entries rss
     * [185]comments rss
     * [186]wordpress.org

search

   search for: ____________________ search
     __________________________________________________________________

      2019, the asimov institute
   [187]proudly powered by wordpress
   theme: yuuta by [188]felix dorner

references

   visible links
   1. http://www.asimovinstitute.org/feed/
   2. http://www.asimovinstitute.org/comments/feed/
   3. http://www.asimovinstitute.org/neural-network-zoo/feed/
   4. http://www.asimovinstitute.org/wp-json/oembed/1.0/embed?url=http://www.asimovinstitute.org/neural-network-zoo/
   5. http://www.asimovinstitute.org/wp-json/oembed/1.0/embed?url=http://www.asimovinstitute.org/neural-network-zoo/&format=xml
   6. http://www.asimovinstitute.org/neural-network-zoo/#content
   7. http://www.asimovinstitute.org/
   8. http://www.asimovinstitute.org/
   9. http://www.asimovinstitute.org/team/
  10. http://www.asimovinstitute.org/news/
  11. http://www.asimovinstitute.org/neural-network-zoo/
  12. http://www.asimovinstitute.org/neural-network-zoo/
  13. http://www.asimovinstitute.org/author/fjodorvanveen/
  14. http://www.ling.upenn.edu/courses/cogs501/rosenblatt1958.pdf
  15. http://www.dtic.mil/cgi-bin/gettrdoc?ad=ada196234
  16. https://bi.snu.ac.kr/courses/g-ai09-2/hopfield82.pdf
  17. http://www.americanscientist.org/libraries/documents/201321152149545-2013-03hayes.pdf
  18. https://www.researchgate.net/profile/terrence_sejnowski/publication/242509302_learning_and_relearning_in_boltzmann_machines/links/54a4b00f0cf256bf8bb327cc.pdf
  19. http://www.dtic.mil/cgi-bin/gettrdoc?location=u2&doc=gettrdoc.pdf&ad=ada620727
  20. https://pdfs.semanticscholar.org/f582/1548720901c89b3b7481f7500d7cd64e99bd.pdf
  21. https://papers.nips.cc/paper/3112-efficient-learning-of-sparse-representations-with-an-energy-based-model.pdf
  22. https://arxiv.org/pdf/1312.6114v10.pdf
  23. http://machinelearning.org/archive/icml2008/papers/592.pdf
  24. https://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf
  25. http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf
  26. http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf
  27. https://arxiv.org/pdf/1503.03167v4.pdf
  28. https://arxiv.org/pdf/1406.2661v1.pdf
  29. https://crl.ucsd.edu/~elman/papers/fsit.pdf
  30. http://deeplearning.cs.cmu.edu/pdfs/hochreiter97_lstm.pdf
  31. https://arxiv.org/pdf/1412.3555v1.pdf
  32. https://arxiv.org/pdf/1410.5401v2.pdf
  33. http://www.di.ufpe.br/~fnj/rna/bibliografia/bid56.pdf
  34. https://arxiv.org/pdf/1512.03385v1.pdf
  35. https://pdfs.semanticscholar.org/8922/17bb82c11e6e2263178ed20ac23db6279c7a.pdf
  36. http://www.ntu.edu.sg/home/egbhuang/pdf/ieee-is-elm.pdf
  37. https://web.archive.org/web/20120222154641/http://ramsesii.upf.es/seminar/maass_et_al_2002.pdf
  38. http://image.diku.dk/imagecanon/material/cortes_vapnik95.pdf
  39. http://cioslab.vcu.edu/alg/visualize/kohonen-82.pdf
  40. http://www.twitter.com/asimovinstitute
  41. https://www.zazzle.com/the_neural_network_zoo_poster-228451996183116574
  42. http://www.asimovinstitute.org/category/deep-learning/
  43. http://www.asimovinstitute.org/artistic-style-transfer-blending/
  44. http://www.asimovinstitute.org/analyzing-deep-learning-tools-music/
  45. http://www.asimovinstitute.org/neural-network-zoo/#comment-2
  46. http://www.asimovinstitute.org/neural-network-zoo/#comment-3
  47. http://www.asimovinstitute.org/neural-network-zoo/#comment-5
  48. http://www.asimovinstitute.org/neural-network-zoo/#comment-7
  49. http://www.asimovinstitute.org/neural-network-zoo/#comment-9
  50. http://blogs.canisius.edu/mathblog/wp-content/uploads/sites/31/2013/02/markovchainpic3.png
  51. http://1.bp.blogspot.com/-ulvbbbntdti/tbwsnea4zpi/aaaaaaaaaby/xyrtcxrhskq/s1600/markovchain.png
  52. http://www.asimovinstitute.org/neural-network-zoo/#comment-11
  53. http://www.asimovinstitute.org/neural-network-zoo/#comment-15
  54. http://www.asimovinstitute.org/neural-network-zoo/#comment-6
  55. http://www.asimovinstitute.org/neural-network-zoo/#comment-8
  56. http://www.asimovinstitute.org/neural-network-zoo/#comment-16
  57. http://www.asimovinstitute.org/neural-network-zoo/#comment-29
  58. http://www.asimovinstitute.org/neural-network-zoo/#comment-13
  59. http://www.asimovinstitute.org/neural-network-zoo/#comment-21
  60. http://www.asimovinstitute.org/neural-network-zoo/#comment-14
  61. http://www.asimovinstitute.org/neural-network-zoo/#comment-22
  62. http://www.asimovinstitute.org/neural-network-zoo/#comment-17
  63. http://www.asimovinstitute.org/neural-network-zoo/#comment-19
  64. http://www.asimovinstitute.org/neural-network-zoo/#comment-18
  65. http://www.asimovinstitute.org/neural-network-zoo/#comment-20
  66. http://www.asimovinstitute.org/neural-network-zoo/#comment-228
  67. http://www.asimovinstitute.org/neural-network-zoo/#comment-232
  68. http://www.asimovinstitute.org/neural-network-zoo/#comment-25
  69. http://www.asimovinstitute.org/neural-network-zoo/#comment-26
  70. http://www.asimovinstitute.org/neural-network-zoo/#comment-27
  71. http://www.asimovinstitute.org/neural-network-zoo/#comment-32
  72. http://www.asimovinstitute.org/neural-network-zoo/#comment-35
  73. http://www.asimovinstitute.org/neural-network-zoo/#comment-45
  74. http://www.asimovinstitute.org/neural-network-zoo/#comment-4094
  75. http://www.asimovinstitute.org/neural-network-zoo/#comment-33
  76. http://www.asimovinstitute.org/neural-network-zoo/#comment-34
  77. http://www.asimovinstitute.org/neural-network-zoo/#comment-36
  78. http://www.asimovinstitute.org/neural-network-zoo/#comment-40
  79. http://www.asimovinstitute.org/neural-network-zoo/#comment-42
  80. http://www.asimovinstitute.org/neural-network-zoo/#comment-41
  81. http://www.asimovinstitute.org/neural-network-zoo/#comment-37
  82. http://www.asimovinstitute.org/neural-network-zoo/#comment-39
  83. http://www.asimovinstitute.org/neural-network-zoo/#comment-43
  84. http://www.asimovinstitute.org/neural-network-zoo/#comment-46
  85. http://www.asimovinstitute.org/neural-network-zoo/#comment-48
  86. http://www.asimovinstitute.org/neural-network-zoo/#comment-49
  87. http://www.asimovinstitute.org/neural-network-zoo/#comment-50
  88. http://www.asimovinstitute.org/neural-network-zoo/#comment-51
  89. http://www.asimovinstitute.org/neural-network-zoo/#comment-52
  90. http://www.asimovinstitute.org/neural-network-zoo/#comment-53
  91. http://www.asimovinstitute.org/neural-network-zoo/#comment-57
  92. http://www.asimovinstitute.org/neural-network-zoo/#comment-54
  93. http://www.asimovinstitute.org/neural-network-zoo/#comment-55
  94. http://www.asimovinstitute.org/neural-network-zoo/#comment-56
  95. https://www.cs.bham.ac.uk/~jlw/sem2a2/web/kohonen.htm
  96. http://www.asimovinstitute.org/neural-network-zoo/#comment-58
  97. https://www.google.nl/search?q=kohonen+network&source=lnms&tbm=isch&sa=x&ved=0ahukewjukrn9wjnpahwiqjokhzkwdz4q_auiid35b&biw=1345&bih=1099&dpr=2#imgrc=_
  98. http://www.asimovinstitute.org/neural-network-zoo/#comment-59
  99. http://www.asimovinstitute.org/neural-network-zoo/#comment-823
 100. http://www.asimovinstitute.org/neural-network-zoo/#comment-61
 101. http://www.asimovinstitute.org/neural-network-zoo/#comment-65
 102. https://quizlet.com/152146730/neural-network-zoo-flash-cards/
 103. http://www.asimovinstitute.org/neural-network-zoo/#comment-64
 104. http://www.asimovinstitute.org/neural-network-zoo/#comment-66
 105. http://www.asimovinstitute.org/neural-network-zoo/#comment-72
 106. http://www.asimovinstitute.org/neural-network-zoo/#comment-75
 107. http://www.asimovinstitute.org/neural-network-zoo/#comment-169
 108. http://www.asimovinstitute.org/neural-network-zoo/#comment-233
 109. http://www.asimovinstitute.org/neural-network-zoo/#comment-234
 110. http://www.asimovinstitute.org/neural-network-zoo/#comment-243
 111. http://www.asimovinstitute.org/neural-network-zoo/#comment-253
 112. http://www.asimovinstitute.org/neural-network-zoo/#comment-259
 113. http://neurotheory.columbia.edu/larry/sussilloneuron09.pdf
 114. http://www.asimovinstitute.org/neural-network-zoo/#comment-258
 115. http://www.asimovinstitute.org/neural-network-zoo/#comment-260
 116. http://www.asimovinstitute.org/neural-network-zoo/#comment-270
 117. http://www.asimovinstitute.org/neural-network-zoo/#comment-284
 118. http://www.asimovinstitute.org/neural-network-zoo/#comment-292
 119. http://www.asimovinstitute.org/neural-network-zoo/#comment-308
 120. http://www.asimovinstitute.org/neural-network-zoo/#comment-297
 121. http://www.asimovinstitute.org/neural-network-zoo/#comment-300
 122. http://www.asimovinstitute.org/neural-network-zoo/#comment-324
 123. http://www.asimovinstitute.org/neural-network-zoo/#comment-306
 124. http://www.asimovinstitute.org/neural-network-zoo/#comment-309
 125. http://www.asimovinstitute.org/neural-network-zoo/#comment-314
 126. http://www.asimovinstitute.org/neural-network-zoo/#comment-374
 127. http://journal.frontiersin.org/article/10.3389/fnsys.2016.00095/full
 128. http://www.asimovinstitute.org/neural-network-zoo/#comment-497
 129. http://www.asimovinstitute.org/neural-network-zoo/#comment-519
 130. http://www.asimovinstitute.org/neural-network-zoo/#comment-531
 131. http://www.asimovinstitute.org/neural-network-zoo/#comment-609
 132. http://www.asimovinstitute.org/neural-network-zoo/#comment-595
 133. http://www.asimovinstitute.org/neural-network-zoo/#comment-967
 134. http://www.asimovinstitute.org/neural-network-zoo/#comment-678
 135. http://www.asimovinstitute.org/neural-network-zoo/#comment-679
 136. http://www.asimovinstitute.org/neural-network-zoo/#comment-680
 137. http://www.asimovinstitute.org/neural-network-zoo/#comment-686
 138. http://www.asimovinstitute.org/neural-network-zoo/#comment-706
 139. http://www.asimovinstitute.org/neural-network-zoo/#comment-728
 140. http://www.asimovinstitute.org/neural-network-zoo/#comment-813
 141. http://www.asimovinstitute.org/neural-network-zoo/#comment-824
 142. http://www.asimovinstitute.org/neural-network-zoo/#comment-924
 143. http://www.asimovinstitute.org/neural-network-zoo/#comment-1545
 144. http://www.asimovinstitute.org/neural-network-zoo/#comment-1258
 145. http://www.asimovinstitute.org/neural-network-zoo/#comment-1341
 146. http://www.asimovinstitute.org/neural-network-zoo/#comment-1539
 147. http://www.asimovinstitute.org/neural-network-zoo/#comment-1731
 148. http://www.asimovinstitute.org/neural-network-zoo/#comment-1928
 149. http://www.asimovinstitute.org/neural-network-zoo/#comment-2114
 150. http://www.asimovinstitute.org/neural-network-zoo/#comment-2521
 151. http://www.asimovinstitute.org/neural-network-zoo/#comment-2900
 152. http://www.asimovinstitute.org/neural-network-zoo/#comment-3962
 153. http://www.asimovinstitute.org/neural-network-zoo/#comment-5300
 154. http://www.asimovinstitute.org/neural-network-zoo/#comment-6266
 155. https://en.wikipedia.org/wiki/recurrent_neural_network#continuous-time
 156. http://www.asimovinstitute.org/neural-network-zoo/#comment-7190
 157. http://www.asimovinstitute.org/neural-network-zoo/#comment-7957
 158. http://www.asimovinstitute.org/neural-network-zoo/#comment-8340
 159. http://www.asimovinstitute.org/neural-network-zoo/
 160. http://www.asimovinstitute.org/neural-network-zoo/#comment-8521
 161. http://www.asimovinstitute.org/neural-network-zoo/#comment-8732
 162. http://www.asimovinstitute.org/neural-network-zoo/#comment-9020
 163. http://www.asimovinstitute.org/neural-network-zoo/#comment-9297
 164. http://www.asimovinstitute.org/neural-network-zoo/#comment-9345
 165. http://www.asimovinstitute.org/neural-network-zoo/#comment-10518
 166. http://www.asimovinstitute.org/neural-network-zoo/#comment-10603
 167. http://www.asimovinstitute.org/neural-network-zoo/
 168. https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf
 169. http://www.asimovinstitute.org/neural-network-zoo/#comment-10665
 170. http://www.asimovinstitute.org/neural-network-zoo/#comment-10666
 171. http://www.asimovinstitute.org/neural-network-zoo/#comment-11793
 172. http://www.asimovinstitute.org/neural-network-zoo/#comment-12381
 173. http://www.asimovinstitute.org/neural-network-zoo/#comment-13053
 174. http://www.asimovinstitute.org/the-neural-network-zoo-2016/
 175. http://www.asimovinstitute.org/neural-network-zoo-prequel-cells-layers/
 176. http://www.asimovinstitute.org/analyzing-deep-learning-tools-music/
 177. http://www.asimovinstitute.org/neural-network-zoo/
 178. http://www.asimovinstitute.org/artistic-style-transfer-blending/
 179. http://www.asimovinstitute.org/a-quick-guide-to-installing-tensorflow-on-mac-os/
 180. https://github.com/asimovinstitute
 181. https://bitbucket.org/asimovinstitute/
 182. https://www.linkedin.com/company/the-asimov-institute
 183. http://www.asimovinstitute.org/wp-login.php
 184. http://www.asimovinstitute.org/feed/
 185. http://www.asimovinstitute.org/comments/feed/
 186. https://wordpress.org/
 187. http://wordpress.org/
 188. http://felixdorner.de/

   hidden links:
 190. javascript:void(0)
 191. javascript:void(0)
 192. https://www.addtoany.com/add_to/facebook?linkurl=http%3a%2f%2fwww.asimovinstitute.org%2fneural-network-zoo%2f&linkname=the%20neural%20network%20zoo
 193. https://www.addtoany.com/add_to/twitter?linkurl=http%3a%2f%2fwww.asimovinstitute.org%2fneural-network-zoo%2f&linkname=the%20neural%20network%20zoo
 194. https://www.addtoany.com/add_to/google_plus?linkurl=http%3a%2f%2fwww.asimovinstitute.org%2fneural-network-zoo%2f&linkname=the%20neural%20network%20zoo
 195. https://www.addtoany.com/add_to/reddit?linkurl=http%3a%2f%2fwww.asimovinstitute.org%2fneural-network-zoo%2f&linkname=the%20neural%20network%20zoo
 196. https://www.addtoany.com/add_to/linkedin?linkurl=http%3a%2f%2fwww.asimovinstitute.org%2fneural-network-zoo%2f&linkname=the%20neural%20network%20zoo
 197. http://www.asimovinstitute.org/neural-network-zoo/#respond
