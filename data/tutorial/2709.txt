   #[1]algobeans    feed [2]algobeans    comments feed [3]algobeans   
   id158s introduction (part ii) comments feed
   [4]k-nearest neighbors & anomaly detection tutorial [5]time series
   analysis with generalized additive models [6]alternate [7]alternate
   [8]algobeans [9]wordpress.com

   [10]skip to content

[11]algobeans

layman tutorials in analytics

   (button) menu
     * [12]home
     * [13]about
     * [14]all posts
     * [15]subscribe

id158s introduction (part ii)

   [16]november 3, 2016november 3, 2016

   we   ve learned [17]how id158s (ann) can be used to
   recognize handwritten digits in a previous post. in the current post,
   we discuss additional techniques to improve the accuracy of neural
   networks. neural networks have been used successfully to solve problems
   such as image/audio recognition and language processing (see figure 1).
   [18]neural networks infographic tutorial.png

   figure 1. uses of neural networks     click to enlarge.

   despite their potential, neural networks were popular only in recent
   years due to 3 reasons:

   advances in storing and sharing data. with more data available to train
   on, the performance of neural networks have improved.

   increased computing power. used mainly to display computer images in
   the past, graphics processing units (gpus) were discovered to be up to
   150 faster than central processing units (cpus). now, gpus enable
   neural network models to train efficiently on large datasets.

   enhanced neural network algorithms. matching the performance of a human
   brain is a difficult feat, but techniques have been developed to
   improve the performance of neural network algorithms, 3 of which are
   discussed in this post:
     * distortion (to increase training data)
     * mini-batch id119 (to shorten training time)
     * dropout (to improve prediction accuracy)

technique 1: distortion
(increase training data)

   an id158 learns to recognize handwritten digits
   when it is given more handwritten images to train on, along with labels
   of those images. hence, providing a sufficiently large training dataset
   of labelled images is critical. however, sometimes the number of
   available labelled images could be limited.

   one way to overcome data shortage is to create more data. by applying
   different distortions to existing images, each distorted image could be
   treated as a new training example, thus vastly expanding the size of
   our training data.
   [19]ann-distortions.png

   figure 2. different distortions applied to the digit    3   . source:
   codeproject

   the most effective distortions are the ones that are represented in the
   existing dataset. for example, we could rotate the images to simulate
   how people write at an angle. another technique is elastic deformation,
   which stretches and squeezes an image at certain points to simulate
   uncontrolled oscillations of hand muscles (see figure 2).

   distortion techniques could also be applied to non-visual data. to
   train a neural network to recognize speech, background noises could be
   added to existing audio clips to generate new audio samples. background
   noises used should already be present in our dataset.

technique 2: mini-batch id119
(shorten training time)

   in our [20]previous tutorial, we learned that an artificial neural
   network comprises neurons, and their activation is governed by a set of
   rules. using a mathematical concept called id119, neuron
   activation rules are tweaked incrementally to improve overall
   prediction accuracy. to do this however, the neural network cycles
   through every single training example before determining how best to
   revise the rules. hence, while a larger training dataset improves
   prediction, it will also increase the time taken to process all the
   training examples. a more efficient solution would be to look at only a
   small batch of examples each time to approximate the best rule change.
   this technique is called mini-batch id119.

   to understand how this works, let   s look at the following analogy
   (corresponding technical terms are in bold):

   imagine you are a ruler of a rich kingdom, and you need to decide how
   much money to allocate (neurons    rules) to each government department
   (neurons). being a rich ruler, you have an unlimited budget, and you
   want to take into account the wishes of everyone (training data) in
   your kingdom. however, everyone has a different idea of how the budget
   should be allocated.
   [21]neural network mini batch id119.png

   figure 3. mini-batch id119 analogy.

   step 1: you consult individuals on your budget plans. during the
   interview, each person will state, vaguely, the degree of change
   (gradient) they want to see in budget allocation. for example, to
   increase education spending by a bit, and to cut welfare spending by a
   lot.

   step 2: you take an average of everyone   s views (id119) and
   revise your budget accordingly (back propagation). because of your
   citizens    ambiguous wordings, you are careful not to change the budget
   too drastically. instead, you stick to small changes (learning rate).

   step 3: you present your budget plan, and your citizens vote on whether
   they approve or disapprove of it (model accuracy).

   if you decide that the approval rate is high enough, the budget is
   passed. otherwise, you repeat steps 1 to 3, gradually improving
   approval rates.

   this process is slow because you take one step only after speaking to
   everyone. one way to speed things up is to approximate the overall
   opinion by consulting a small subset of your citizens (mini-batch
   id119).

   the following graphs compare prediction accuracy between a vanilla
   id119 (left) vs. mini-batch id119 (right):
   [22]neural network mini batch id119 results.png

   figure 4. performance of a vanilla id119 (left) and a
   mini-batch id119 with 10 training examples in a batch
   (right) in a id75 model predicting 100 simulated training
   samples.

   using a vanilla id119, prediction error decreased steadily
   and stabilized after about 250 cycles. on the other hand, a mini-batch
   id119 caused error to decrease with fluctuations and it
   stabilized only after about 400 cycles. while mini-batch gradient
   descent needed more cycles to reach the end, each phase cycle was much
   shorter and thus required less time on the whole.

technique 3: dropout
(improve prediction accuracy)

   sometimes a neural network might attempt to adjust its neuron
   activation rules to fit training examples, to the point where the rules
   do not generalize well to new examples. this phenomenon is called
   overfitting, a common problem in machine learning which could lead to
   poor accuracy when predicting new data. the dropout technique could
   prevent that.

   let   s look at another analogy to describe the intuition behind dropout.

   imagine yourself as the manager of a soccer team. you have two players
   who have developed strong teamwork and chemistry after playing together
   for many months. while this is advantageous when both players in the
   game, their over-reliance on each other might impair performance when
   one player injured. to overcome this problem, you could force these two
   players to train with other team members more often.

   in the same way, neurons in a neural network might grow reliant on each
   other when a few neurons co-adapt to patterns in training examples,
   causing their rules to change in a similar way during id119.
   because they do not work with other neurons, they might overlook
   intrinsic features of training examples, resulting in less robust
   predictions for new data. to solve this, we could force different
   neurons to work together by randomly dropping half the neurons in each
   cycle of a id119.
   [23]neural network dropout tutorial.png

   figure 5. fully-connected neural network (left) and neural network with
   dropped neurons (right). in the dropout, neurons b, d, and f do not
   transmit signals to other neurons.

   neurons which are dropped are completely deactivated and do not send
   any signals. hence, they do not affect the activation of neurons in the
   next layer. furthermore, their rules remain constant during that cycle,
   and the entire neural network trains as if these neurons did not exist.
   the dropout technique thus forces neurons to discover more features in
   training examples as neurons collaborate in different combinations.

conclusion

   this wraps up the 3 techniques that could be used to improve the
   accuracy of id158s. did you learn something useful
   today? we would be glad to inform you when we have new tutorials, so
   that your learning continues!

   sign up below to get bite-sized tutorials delivered to your inbox:

   [24]free data science tutorials

   copyright    2015-present algobeans.com. all rights reserved. be a cool
   bean.

share this:

     * [25]click to share on facebook (opens in new window)
     * [26]click to share on linkedin (opens in new window)
     * [27]click to share on twitter (opens in new window)
     * [28]click to share on reddit (opens in new window)
     * [29]click to email this to a friend (opens in new window)
     *

like this:

   like loading...

related

   posted in: [30]tutorial | tagged: [31]ai, [32]distortion, [33]dropout,
   [34]id119, [35]layman, [36]machine learning, [37]neural
   networks

post navigation

   [38] k-nearest neighbors & anomaly detection tutorial
   [39]time series analysis with generalized additive models

one thought on    id158s introduction (part ii)   

    1. pingback: [40]distilled news | data analytics & r

leave a reply [41]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [42]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [43]log out /
   [44]change )
   google photo

   you are commenting using your google account. ( [45]log out /
   [46]change )
   twitter picture

   you are commenting using your twitter account. ( [47]log out /
   [48]change )
   facebook photo

   you are commenting using your facebook account. ( [49]log out /
   [50]change )
   [51]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

   search for: ____________________ search

wanna learn data science?

   get intuitive explanations focusing on core concepts with no math.
   discover applications and pitfalls in concise 1500-word chapters.

   [52]free data science tutorials

   to see what you've missed so far, check out our tutorial compilation in
   our brand new book:

   [53]numsense algobeans

                  [54]top kdnuggets blogger for april 2017

follow us

     * [55]facebook
     * [56]twitter
     * [57]rss
     * [58]github

about algobeans

   algobeans is the brainchild of two data science enthusiasts, annalyn
   (university of cambridge) and kenneth (stanford university). we noticed
   that while data science is increasingly used to improve workplace
   decisions, many people know little about the field. hence, we created
   algobeans so that everyone and anyone can learn - be it an aspiring
   student or enterprising business professional. each tutorial covers the
   important functions and assumptions of a data science technique,
   without any math or jargon. we also illustrate these techniques with
   real-world data and examples.

wanna learn data science?

   get intuitive explanations focusing on core concepts with no math.
   discover applications and pitfalls in concise 1500-word posts delivered
   to your inbox.

   [59]free data science tutorials

   [tr?id=1258053584302511&amp;ev=pageview&amp;noscript=1]

   copyright    2015-present algobeans.com.
   all rights reserved. be a cool bean.

     * [60]facebook
     * [61]twitter
     * [62]rss
     * [63]github


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [64]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [65]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [66]likes-master

   %d bloggers like this:

references

   visible links
   1. https://algobeans.com/feed/
   2. https://algobeans.com/comments/feed/
   3. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/feed/
   4. https://algobeans.com/2016/09/14/k-nearest-neighbors-anomaly-detection-tutorial/
   5. https://algobeans.com/2017/04/04/laymans-tutorial-time-series-analysis/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/&for=wpcom-auto-discovery
   8. https://algobeans.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/#content
  11. https://algobeans.com/
  12. https://algobeans.com/
  13. https://algobeans.com/about/
  14. https://algobeans.com/all-posts/
  15. https://algobeans.com/subscribe/
  16. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/
  17. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/
  18. https://annalyzin.files.wordpress.com/2016/11/neural-networks-infographic-tutorial.png
  19. https://annalyzin.files.wordpress.com/2016/11/ann-distortions.png
  20. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/
  21. https://annalyzin.files.wordpress.com/2016/11/neural-network-mini-batch-gradient-descent.png
  22. https://annalyzin.files.wordpress.com/2016/11/neural-network-mini-batch-gradient-descent-results.png
  23. https://annalyzin.files.wordpress.com/2016/11/neural-network-dropout-tutorial.png
  24. http://eepurl.com/cbvfy1
  25. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/?share=facebook
  26. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/?share=linkedin
  27. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/?share=twitter
  28. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/?share=reddit
  29. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/?share=email
  30. https://algobeans.com/category/tutorial/
  31. https://algobeans.com/tag/ai/
  32. https://algobeans.com/tag/distortion/
  33. https://algobeans.com/tag/dropout/
  34. https://algobeans.com/tag/gradient-descent/
  35. https://algobeans.com/tag/layman/
  36. https://algobeans.com/tag/machine-learning/
  37. https://algobeans.com/tag/neural-networks/
  38. https://algobeans.com/2016/09/14/k-nearest-neighbors-anomaly-detection-tutorial/
  39. https://algobeans.com/2017/04/04/laymans-tutorial-time-series-analysis/
  40. http://advanceddataanalytics.net/2016/12/10/distilled-news-408/
  41. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/#respond
  42. https://gravatar.com/site/signup/
  43. javascript:highlandercomments.doexternallogout( 'wordpress' );
  44. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/
  45. javascript:highlandercomments.doexternallogout( 'googleplus' );
  46. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/
  47. javascript:highlandercomments.doexternallogout( 'twitter' );
  48. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/
  49. javascript:highlandercomments.doexternallogout( 'facebook' );
  50. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/
  51. javascript:highlandercomments.cancelexternalwindow();
  52. http://eepurl.com/cbvfy1
  53. http://getbook.at/numsense
  54. http://www.kdnuggets.com/2017/04/data-science-layman-no-math-added.html
  55. https://www.facebook.com/algobeans/
  56. https://twitter.com/algobeans
  57. https://algobeans.com/feed
  58. https://github.com/algobeans/numsense
  59. http://eepurl.com/cbvfy1
  60. https://www.facebook.com/algobeans/
  61. https://twitter.com/algobeans
  62. https://algobeans.com/feed
  63. https://github.com/algobeans/numsense
  64. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/
  65. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/#cancel
  66. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  68. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/#comment-form-guest
  69. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/#comment-form-load-service:wordpress.com
  70. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/#comment-form-load-service:twitter
  71. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/#comment-form-load-service:facebook
