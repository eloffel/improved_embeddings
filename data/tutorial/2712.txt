   #[1]algobeans    feed [2]algobeans    comments feed [3]algobeans   
   id158s (ann) introduction comments feed
   [4]regression & correlation tutorial [5]association rules and the
   apriori algorithm [6]alternate [7]alternate [8]algobeans
   [9]wordpress.com

   [10]skip to content

[11]algobeans

layman tutorials in analytics

   (button) menu
     * [12]home
     * [13]about
     * [14]all posts
     * [15]subscribe

id158s (ann) introduction

   [16]march 13, 2016october 20, 2016

training a computer to recognize your handwriting

   take a look at the picture below and try to identify what it is:

   fat_giraffe_ann

   one should be able to tell that it is a giraffe, despite it being
   strangely fat. we recognize images and objects instantly, even if these
   images are presented in a form that is different from what we have seen
   before. we do this with the 80 billion neurons in our brain working
   together to transmit information. this remarkable system of neurons is
   also the inspiration behind a widely-used machine learning technique
   called id158s (ann). some computers using this
   technique have even out-performed humans in recognizing images.

the problem

   image recognition is important for many of the advanced technologies we
   use today. it is used in visual surveillance, guiding autonomous
   vehicles and even identifying ailments from x-ray images. most modern
   smartphones also come with image recognition apps that convert
   handwriting into typed words.

   in this chapter we will look at how we can train an ann algorithm to
   recognize images of handwritten digits. we will be using the images
   from the famous mnist (mixed national institute of standards and
   technology) database.
   mnist_ann.png

   handwritten digits in the mnist database

an illustration

   in this section, we present the performance of an ann model in
   recognizing handwritten digits, before explaining how the model works
   in the next section.

   an ann model is trained by giving it examples of 10,000 handwritten
   digits, together with the correct digits they represent. this allows
   the ann model to understand how the handwriting translates into actual
   digits. after the ann model is trained, we can test how well the model
   performs by giving it 1,000 new handwritten digits without the correct
   answer. the model is then required to recognize the actual digit.

   at the start, the ann translates handwritten images into pixels     a
   language it understands. black pixels are given the value    0    and white
   pixels the value    1   . each pixel in an image is called a variable.

   out of the 1,000 handwritten images that the model was asked to
   recognize, it correctly identified 922 of them, which is a 92.2%
   accuracy. we can use a contingency table to view the results, as shown
   below:
   ann handwriting contingency table

   contingency table showing the performance of the ann model. for
   example, the first row tells us that out of 85 images of digit    0   s
   given to the model, 84 were correctly identified and 1 was wrongly
   identified as    6   . the last column indicates prediction accuracy.

   from the table, we can see that when given a handwritten image of
   either    0    or    1   , the model almost always identifies it correctly. on
   the other hand, the digit    5    is the trickiest to identify. an
   advantage of using a contingency table is that it tells us the
   frequency of mis-identification. image of the digit    2    are
   misidentified as    7    or    8    about 8% of the time. let   s take an
   in-depth look at some of these misidentified digits:

   ann mis-identification errors

   while the images may look obviously like a digit    2    to human eyes, the
   ann is sometimes unable to recognize certain features of images, like
   the tail of the digit    2    (explained in limitations section). another
   interesting observation is how the model confuses digits    3    and    5   
   about 10% of the time:

   ann mis-identification errors

the neurons that inspired the network

   our brain has a large network of interlinked neurons, which act as a
   highway for information to be transmitted from point a to point b. when
   different information is sent from a to b, the brain activates
   different sets of neurons, and so essentially uses a different route to
   get from a to b. this is how a typical neuron looks like:
   neuron illustration

   a brain neuron and its main components

   at each neuron, dendrites receive incoming signals sent by other
   neurons. if the neuron receives a high enough level of signals within a
   certain period of time, the neuron sends an electrical pulse into the
   terminals. these outgoing signals are then received by other neurons.

technical explanation i: how the model works

   ann tutorial overview.png

   a simple id158 map, showing two scenarios with two
   different inputs but with the same output. activated neurons along the
   path are shown in red.

   similarly, in the ann model, we have an input node, which is the image
   we give the model, and an output node, which is the digit that the
   model recognizes. the main characteristics of an ann model is as such:

   step 1. when the input node is given an image, it activates a unique
   set of neurons in the first layer, starting a chain reaction that would
   pave a unique path to the output node. in scenario 1, neurons a, b, and
   d are activated in layer 1.

   step 2. the activated neurons send signals to every connected neuron in
   the next layer. this directly affects which neurons are activated in
   the next layer. in scenario 1, neuron a sends a signal to e and g,
   neuron b sends a signal to e, and neuron d sends a signal to f and g.

   step 3. in the next layer, each neuron is governed by a rule on what
   combinations of received signals would activate the neuron. in scenario
   1, neuron e is activated by the signals from a and b. however, for
   neuron f and g, their neurons    rules tell them that they have not
   received the right signals to be activated, and hence they remain grey.

   step 4. steps 2-3 are repeated for all the remaining layers (it is
   possible for the model to have more than 2 layers), until we are left
   with the output node.

   step 5. the output node deduces the correct digit based on signals
   received from neurons in the layer directly preceding it (layer 2).
   each combination of activated neurons in layer 2 leads to one solution,
   though each solution can be represented by different combinations of
   activated neurons. in scenarios 1 & 2, two images are fed as input.
   because the images are different, the network activates different
   neural paths from input to the output. however, the output is still
   recognizes both images as the digit    6   .

technical explanation ii: training the model

   we need to first decide the number of layers and number of neurons in
   each layer for our ann model. while there is no limit, a good start is
   to use 3 layers, with the number of neurons being proportional to the
   number of variables. for the digit recognizer ann, we used 3 layers
   with 500 neurons each. the two key factors involved in training a model
   are:
     * a metric to evaluate the model   s accuracy
     * rules that govern whether neurons are activated or not

   a common metric to evaluate model accuracy is the sum of the squared
   errors (sse). put simply, a squared error denotes how close a predicted
   digit is to the actual digit. the ann model will try to minimize the
   sse by changing the rules that govern neuron activation, and these
   changes are determined by a mathematical concept known as
   differentiation.

   each neuron   s rule has two components     the weight (i.e. strength) of
   incoming signals [w], and the minimum received signal strength required
   for activation [m]. in the following example, we illustrate the rules
   for neuron g. zero weight is given to the signals from a and b (i.e. no
   connection), and weights of 1, 2, and -1 are given to the signals from
   c, d, and e respectively. the m-value for g is 2, so g is activated if:
     * d is activated and e is not activated, or if,
     * c and d are activated.

   ann animation.gif

   an example of a neuron(g)   s rule. the value below g indicates the
   received signal strength

limitations

   computationally expensive. training an ann model takes more time and
   cpu power  compared to training other types of models (e.g. random
   forests). moreover, the performance of ann models may not necessarily
   be superior. although ann is not a new technique, it has only been used
   more frequently in recent years because of hardware advances that made
   its computing feasible. however, ann is the basis for more advanced
   models, like deep neural networks (dnn) which were used by google in
   oct 2015 and mar 2016 to defeat human champions in the game of go,
   widely viewed as an unsolved    grand challenge    for artificial
   intelligence.

   lack of feature recognition. the ann is unable to recognize images if
   they take on slight variations in shape, or are placed in a different
   location. for example, if we want our ann model to recognize images of
   cats, and suppose our training examples always feature cats at the
   bottom of the image, then the ann model would not recognize the same
   cats if they appear at the top, or the same cats of larger sizes. an
   advanced version of ann called convolutional neural networks (id98)
   solves this problem by looking at various regions of the image. in
   fact, id98s are also more efficient, and are widely used in image and
   video recognition. for more information, check out a previous post on
   [17]introduction to convolutional neural network.

   did you learn something useful today? we would be glad to inform you
   when we have new tutorials, so that your learning continues!

   sign up below to get bite-sized tutorials delivered to your inbox:

   [18]free data science tutorials

   copyright    2015-present algobeans.com. all rights reserved. be a cool
   bean.
     __________________________________________________________________

additional notes for advanced readers

   [this section is intended for readers who have mathematics or computer
   science background and wish to implement their own ann.]

   the neuron   s rule described in the technical explanation is a
   mathematical function called    activation function   . it gives zero
   output when the input is low, and gives positive output when the input
   is sufficiently high. commonly-used id180 include the
   sigmoid function and rectifier function. the output node is also
   governed by a function, and softmax (a generalization of the logistic
   function) is usually used. as such, the ann can be viewed as    a grand
   function of functions   . this is also why we use differentiation to find
   the correct weights through id119. lastly, note that it is
   essential to normalize the input data during implementation.

   try out ann: c++ code by ben graham:
   [19]https://github.com/btgraham/batchwise-dropout , or python code by
   michael nielsen:
   [20]https://github.com/mnielsen/neural-networks-and-deep-learning

   did you learn something useful today? we would be glad to inform you
   when we have new tutorials, so that your learning continues!

   sign up below to get bite-sized tutorials delivered to your inbox:

   [21]free data science tutorials

   copyright    2015-present algobeans.com. all rights reserved. be a cool
   bean.

share this:

     * [22]click to share on facebook (opens in new window)
     * [23]click to share on linkedin (opens in new window)
     * [24]click to share on twitter (opens in new window)
     * [25]click to share on reddit (opens in new window)
     * [26]click to email this to a friend (opens in new window)
     *

like this:

   like loading...

related

   posted in: [27]tutorial | tagged: [28]ai, [29]classification,
   [30]handwriting, [31]image recognition, [32]layman, [33]machine
   learning, [34]neural networks

post navigation

   [35] regression & correlation tutorial
   [36]association rules and the apriori algorithm

21 thoughts on    id158s (ann) introduction   

    1. pingback: [37]ai                                            -          
    2. pingback: [38]time series analysis with generalized additive models
           @noeliagorod
    3. pingback: [39]                  ai                        10                      |             
    4. pingback: [40]   ai                                          -            
    5. pingback: [41]   ai                                           | 36         
    6. pingback: [42]ai                     10                      |             
    7. pingback: [43]time series analysis with generalized additive models
       | open data science
    8. pingback: [44]time series analysis with generalized additive models
       | open data science
    9. pingback: [45]time series analysis with generalized additive models
       - biva
   10. pingback: [46]time series analysis with generalized additive models
           cloud data architect
   11. pingback: [47]time series analysis with generalized additive models
           algobeans
   12. pingback: [48]distilled news | data analytics & r
   13. pingback: [49]id158s (ann) introduction, part 2
       - use-r!use-r!
   14. pingback: [50]id158s (ann) introduction, part 1
       - use-r!use-r!
   15. pingback: [51]id158s introduction (part ii)    
       algobeans
   16. pingback: [52]id158s (ann) introduction | a
       bunch of data
   17. pingback: [53]build your own deep learning box | annalyzin |
       analytics tutorials for laymen
   18. pingback: [54]id158s (ann) introduction | a
       bunch of data
   19. pingback: [55]training a computer to recognize your handwriting -
   20. pingback: [56]training a computer to recognize your handwriting |
       make data work
   21. pingback: [57]introduction to convolutional neural network |
       annalyzin

leave a reply [58]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [59]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [60]log out /
   [61]change )
   google photo

   you are commenting using your google account. ( [62]log out /
   [63]change )
   twitter picture

   you are commenting using your twitter account. ( [64]log out /
   [65]change )
   facebook photo

   you are commenting using your facebook account. ( [66]log out /
   [67]change )
   [68]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

   search for: ____________________ search

wanna learn data science?

   get intuitive explanations focusing on core concepts with no math.
   discover applications and pitfalls in concise 1500-word chapters.

   [69]free data science tutorials

   to see what you've missed so far, check out our tutorial compilation in
   our brand new book:

   [70]numsense algobeans

                  [71]top kdnuggets blogger for april 2017

follow us

     * [72]facebook
     * [73]twitter
     * [74]rss
     * [75]github

about algobeans

   algobeans is the brainchild of two data science enthusiasts, annalyn
   (university of cambridge) and kenneth (stanford university). we noticed
   that while data science is increasingly used to improve workplace
   decisions, many people know little about the field. hence, we created
   algobeans so that everyone and anyone can learn - be it an aspiring
   student or enterprising business professional. each tutorial covers the
   important functions and assumptions of a data science technique,
   without any math or jargon. we also illustrate these techniques with
   real-world data and examples.

wanna learn data science?

   get intuitive explanations focusing on core concepts with no math.
   discover applications and pitfalls in concise 1500-word posts delivered
   to your inbox.

   [76]free data science tutorials

   [tr?id=1258053584302511&amp;ev=pageview&amp;noscript=1]

   copyright    2015-present algobeans.com.
   all rights reserved. be a cool bean.

     * [77]facebook
     * [78]twitter
     * [79]rss
     * [80]github


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [81]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [82]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [83]likes-master

   %d bloggers like this:

references

   visible links
   1. https://algobeans.com/feed/
   2. https://algobeans.com/comments/feed/
   3. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/feed/
   4. https://algobeans.com/2016/01/31/regression-correlation-tutorial/
   5. https://algobeans.com/2016/04/01/association-rules-and-the-apriori-algorithm/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/&for=wpcom-auto-discovery
   8. https://algobeans.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/#content
  11. https://algobeans.com/
  12. https://algobeans.com/
  13. https://algobeans.com/about/
  14. https://algobeans.com/all-posts/
  15. https://algobeans.com/subscribe/
  16. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/
  17. https://annalyzin.wordpress.com/2016/01/26/introduction-to-convolutional-neural-network/
  18. http://eepurl.com/cbvfy1
  19. https://github.com/btgraham/batchwise-dropout
  20. https://github.com/mnielsen/neural-networks-and-deep-learning
  21. http://eepurl.com/cbvfy1
  22. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/?share=facebook
  23. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/?share=linkedin
  24. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/?share=twitter
  25. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/?share=reddit
  26. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/?share=email
  27. https://algobeans.com/category/tutorial/
  28. https://algobeans.com/tag/ai/
  29. https://algobeans.com/tag/classification/
  30. https://algobeans.com/tag/handwriting/
  31. https://algobeans.com/tag/image-recognition/
  32. https://algobeans.com/tag/layman/
  33. https://algobeans.com/tag/machine-learning/
  34. https://algobeans.com/tag/neural-networks/
  35. https://algobeans.com/2016/01/31/regression-correlation-tutorial/
  36. https://algobeans.com/2016/04/01/association-rules-and-the-apriori-algorithm/
  37. http://www.xiandaner.com/ai-                                          /
  38. http://noeliagorod.com/2018/03/12/time-series-analysis-with-generalized-additive-models/
  39. http://www.tudouqiu.com/tech/30270.html
  40. http://www.shixunkuaibao.com/?p=761432
  41. http://www.36dsj.com/archives/104195
  42. https://wissenpress.wordpress.com/2017/11/27/ai                     10                     /
  43. https://opendatascience.com/blog/time-series-analysis-with-generalized-additive-models/
  44. https://opendatascience.com/blog/time-series-analysis-with-generalized-additive-models-2/
  45. http://biva-ags.com/time-series-analysis-with-generalized-additive-models/
  46. http://www.dataarchitect.cloud/time-series-analysis-with-generalized-additive-models/
  47. https://algobeans.com/2017/04/04/laymans-tutorial-time-series-analysis/
  48. http://advanceddataanalytics.net/2016/12/10/distilled-news-408/
  49. http://use-r.com/artificial-neural-networks-ann-introduction-part-2/
  50. https://use-r.com/artificial-neural-networks-ann-introduction-part-1/
  51. https://algobeans.com/2016/11/03/artificial-neural-networks-intro2/
  52. http://abunchofdata.com/artificial-neural-networks-ann-introduction-3/
  53. https://annalyzin.wordpress.com/2016/05/19/build-a-deep-learning-box/
  54. http://abunchofdata.com/artificial-neural-networks-ann-introduction/
  55. http://www.thepointofinterest.com/2016/04/training-a-computer-to-recognize-your-handwriting/
  56. http://makedatawork.com/2016/04/06/training-a-computer-to-recognize-your-handwriting/
  57. https://annalyzin.wordpress.com/2016/01/26/introduction-to-convolutional-neural-network/
  58. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/#respond
  59. https://gravatar.com/site/signup/
  60. javascript:highlandercomments.doexternallogout( 'wordpress' );
  61. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/
  62. javascript:highlandercomments.doexternallogout( 'googleplus' );
  63. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/
  64. javascript:highlandercomments.doexternallogout( 'twitter' );
  65. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/
  66. javascript:highlandercomments.doexternallogout( 'facebook' );
  67. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/
  68. javascript:highlandercomments.cancelexternalwindow();
  69. http://eepurl.com/cbvfy1
  70. http://getbook.at/numsense
  71. http://www.kdnuggets.com/2017/04/data-science-layman-no-math-added.html
  72. https://www.facebook.com/algobeans/
  73. https://twitter.com/algobeans
  74. https://algobeans.com/feed
  75. https://github.com/algobeans/numsense
  76. http://eepurl.com/cbvfy1
  77. https://www.facebook.com/algobeans/
  78. https://twitter.com/algobeans
  79. https://algobeans.com/feed
  80. https://github.com/algobeans/numsense
  81. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/
  82. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/#cancel
  83. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  85. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/#comment-form-guest
  86. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/#comment-form-load-service:wordpress.com
  87. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/#comment-form-load-service:twitter
  88. https://algobeans.com/2016/03/13/how-do-computers-recognise-handwriting-using-artificial-neural-networks/#comment-form-load-service:facebook
