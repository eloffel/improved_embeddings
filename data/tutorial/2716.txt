   #[1]salesforce research

   [2]salesforce logo [3]mission [4]research [5]careers [6]ethics
   [7]products [8]press

   [9]publications [10]blog [11]open source [12]grants

   [13]salesforce logo [14]   
   [15]mission [16]research [17]publications [18]blog [19]open source
   [20]grants [21]careers [22]ethics [23]products [24]press

   [25]salesforce logo [26]mission [27]research [28]careers [29]ethics
   [30]products [31]press

   [32]publications [33]blog [34]open source [35]grants

new neural network building block allows faster and more accurate text
understanding

   by: [36]james bradbury

   [qid56_block.png] block diagram illustrating the qid56 architecture
   (right), compared to traditional architectures for text (left) and
   images (center).

   in deep learning, there are two very different ways to process input
   (like an image or a document). typically, images are processed all at
   once, with the same kind of computation happening for every part of the
   image simultaneously. but researchers have usually assumed that you
   can't do this for text data: that you need to process it the way people
   read text: word by word, from the beginning to the end, taking context
   into account as you go. in [37]this paper, we showed that this doesn't
   have to be true: you can process it all at once like an image, then
   quickly take into account context and fix the parts that you got wrong.
   since the conventional, one-word-at-a-time approach to deep learning
   for text uses what's called a "recurrent neural network," we called our
   system a "quasi-recurrent neural network," or qid56. because most of the
   computation happens all at once, in parallel, it's up to 16 times
   faster than the old approach. amazingly, it also gives better results
   than conventional deep-learning models for all three tasks that we
   tried (id31, next-word prediction, and translation).

   the diagram above shows the basic structure of our model. a continuous
   block of red means that complicated, slow computations can proceed in
   parallel (i.e., much faster). blue signifies fast, simple functions (in
   the case of the qid56, the components that "fix" the parts the
   out-of-context red computations got wrong). depending on the length of
   text inputs we need to process (whether they're sentences, paragraphs,
   etc.) and other properties of the dataset, our architecture, shown on
   the right, runs anywhere between 30% faster and 16 times faster than
   the architecture usually used for text, shown on the left. the
   advantage is highest for long sequences like paragraphs.

accuracy comparison

   we compared our new building block with the traditional lstm
   architecture (a special type of recurrent neural network) by building a
   pair of models for each of three tasks. in each case, the models are
   identical other than the fact that one uses the lstm and one uses the
   qid56. in all three cases, the one with the qid56 performs better:
   model time to run through dataset once (s) test accuracy (%)
   deeply connected 4-layer lstm 480 90.9
   deeply connected 4-layer qid56 150 91.4
   results on imdb movie review id31 dataset.
   model time to run through dataset once (s) test perplexity (lower is
   better)
   ordinary 2-layer lstm 128 82.0
   ordinary 2-layer qid56 66 79.9
   [38]2-layer lstm with state-of-the-art id173     78.9
   2-layer qid56 with zoneout id173 66 78.3
   results on id32 language analysis (next-word prediction) task.
      model     time to run through dataset once (hrs) test id7 score
   4-layer lstm 4.2                                    16.53
   4-layer qid56 1.0                                    19.41
   results on iwslt (ted talk transcript) german to english machine
   translation task.

interpretability

   deep learning models are often said to be "black boxes": you feed data
   in, and results come out without a human-readable explanation of why
   the neural network decided to produce that particular output. in
   principle, it is always possible to attempt to understand the internal
   workings of a neural network by looking at the individual neuron
   activations, but this hasn't been very productive   especially for
   natural language tasks. one reason why it's so difficult to assign a
   meaning to individual neurons is that traditional approaches like the
   lstm allow every neuron's activation for one word in a sentence to
   depend on every other neuron's activation for the previous word. so the
   activations of all the neurons mix together with each other, and it's
   unlikely for any one neuron to have a single well-defined meaning.

   this new qid56 approach may help interpretability of neurons, because
   each neuron's activation doesn't depend at all on the past history of
   any other neurons. this means that neurons are more likely, although
   not guaranteed, to have independent and well-defined meanings, and
   these meanings are more likely to be simpler and more
   human-interpretable. one way to see this is to plot the activations of
   all neurons in one layer as the qid56 reads an input paragraph from the
   id31 dataset. the input is a movie review from imdb.com
   which contains some positive and some negative commentary about the
   movie. individually, each neuron measures some aspect of positive or
   negative sentiment and isn't directly affected by the activations of
   other neurons; together, they make it clear how the network perceives
   the positive and negative swings in the writer's comments.

   visualization of neuron activations for the last qid56 layer of a
   network processing the sentiment of an imdb movie review. time (the
   number of words read) is on the horizontal axis; different neurons are
   along the vertical axis. colors denote neuron activations; hover over
   the visualization to see the context. the word at that timestep is
   bolded. after an initial positive statement this movie is simply
   gorgeous (at timestep 9), timestep 117 triggers a reset of many neurons
   towards negative sentiment due to the phrase not exactly a bad story
   (soon after main weakness is its story). only at timestep 158, after i
   recommend this movie to everyone, even if you've never played the game,
   do the neurons recover. the (correct) positive prediction prevails.

sample code

   we're happy to see community interest in the qid56 architecture. to help
   people develop their own implementations, we've embedded the core of
   ours, written in [39]chainer, below. stid56function is a cuda
   implementation of the forward and backward passes of the recurrent
   pooling function, while qid56layer implements a qid56 layer composed of
   convolutional and pooling subcomponents, with optional attention and
   state-saving features for the three tasks described in the paper.

citation credit

   james bradbury, stephen merity, caiming xiong, and richard socher.
   2016.
   [40]quasi-recurrent neural networks

   [41]salesforce logo [42]content [43]terms [44]privacy
      copyright 2017 [45]salesforce.com, inc. [46]all rights reserved.
   rights of albert einstein are used with permission of the hebrew
   university of jerusalem. represented exclusively by greenlight.

references

   1. https://blog.einstein.ai/rss/
   2. https://www.einstein.ai/
   3. https://einstein.ai/mission
   4. https://einstein.ai/research
   5. https://einstein.ai/careers
   6. https://einstein.ai/ethics
   7. https://einstein.ai/products
   8. https://einstein.ai/press
   9. https://einstein.ai/research/publications
  10. https://einstein.ai/research/blog
  11. https://einstein.ai/research/open-source
  12. https://einstein.ai/research/grants
  13. https://www.einstein.ai/
  14. javascript:void(0);
  15. https://einstein.ai/mission
  16. https://einstein.ai/research
  17. https://einstein.ai/research/publications
  18. https://einstein.ai/research/blog
  19. https://einstein.ai/research/open-source
  20. https://einstein.ai/research/grants
  21. https://einstein.ai/careers
  22. https://einstein.ai/ethics
  23. https://einstein.ai/products
  24. https://einstein.ai/press
  25. https://www.einstein.ai/
  26. https://einstein.ai/mission
  27. https://einstein.ai/research
  28. https://einstein.ai/careers
  29. https://einstein.ai/ethics
  30. https://einstein.ai/products
  31. https://einstein.ai/press
  32. https://einstein.ai/research/publications
  33. https://einstein.ai/research/blog
  34. https://einstein.ai/research/open-source
  35. https://einstein.ai/research/grants
  36. https://blog.einstein.ai/author/james/
  37. https://arxiv.org/pdf/1611.01576v1.pdf
  38. http://arxiv.org/abs/1512.05287
  39. http://chainer.org/
  40. http://arxiv.org/abs/1611.01576
  41. https://www.salesforce.com/
  42. https://einstein.ai/content-takedown
  43. https://einstein.ai/terms-of-service
  44. https://einstein.ai/privacy
  45. https://www.salesforce.com/
  46. https://www.salesforce.com/company/legal/intellectual.jsp
