   #[1]salesforce research

   [2]salesforce logo [3]mission [4]research [5]careers [6]ethics
   [7]products [8]press

   [9]publications [10]blog [11]open source [12]grants

   [13]salesforce logo [14]   
   [15]mission [16]research [17]publications [18]blog [19]open source
   [20]grants [21]careers [22]ethics [23]products [24]press

   [25]salesforce logo [26]mission [27]research [28]careers [29]ethics
   [30]products [31]press

   [32]publications [33]blog [34]open source [35]grants

learning when to skim and when to read

   by: [36]alexander rosenberg johansen

   the rise of machine learning, deep learning, and artificial
   intelligence more generally has been undeniable, and it has already had
   a massive impact on the field of computer science. by now, you might
   have heard how deep learning has surpassed super-human performance in a
   number of tasks ranging from [37]image recognition to the [38]game of
   go.

     the deep learning community is now eyeing natural language
     processing (nlp) as the next frontier of research and application.

   one beauty of deep learning is that advances tend to be very generic.
   for example, techniques that make deep learning work for one domain can
   often be transferred to other domains with little to no modification.
   more specifically, the approach of building massive, computationally
   expensive, deep learning models for [39]image and [40]speech
   recognition has spilled into nlp. one can see this in the case of the
   most recent [41]state-of-the-art translation system, which outperformed
   all previous results, but required an exorbitant amount of computers.
   such demanding systems can capture very complex patterns occasionally
   found in real world data, but this has led many to apply these massive
   models to all tasks. this raises the question:

     do all tasks always have the complexity that requires such models?

   let's look at the innards of a two layered mlp trained on bag-of-words
   embeddings for id31.
   [frontimg.png] the innards of a simple deep learning system, known as
   the bag-of-words, classifying sentences as positive or negative. the
   visualization is a id167 of the last hidden layer from a in a
   two-layered mlp ontop of a bag-of-words. each data point corresponds to
   a sentence and is coloured accordingly to the deep learning systems
   prediction and the true target. the bounding boxes are drawn according
   to the linguistic content in the sentences. later you will get to
   inspect them for yourself with an interactive plot!

   the boundary boxes in the plot above offers some important insights.
   real world data comes in different difficulties, some sentences are
   easily classified while others contain complex semantic structures. in
   the case of easily classified sentences, running a high-capacity system
   might be unnecessary. a much simpler model could potentially do an
   equivalent job. this blog post will explore whether this is the case.
   it will show that we can often do with simple models.

deep learning with text

   most deep learning methods require floating point numbers as input and,
   unless you have been working with text before, you might wonder:

     how do i go from a piece of text to deep learning?

   a core issue with text is how to represent an arbitrarily large amount
   of information, given the length of the material. a popular solution
   has been tokenizing text into either words, [42]sub-words, or even
   [43]characters. each word is transformed into a floating point vector
   using well studied methods such as [44]id97 or [45]glove. this
   provides for meaningful representations of a word through the implicit
   relationships between different words.
   [wordembedd.png] take a word, turn it into a high dimensional embedding
   (e.g. 300 dimensions) and use pca or id167 (popular tools to reduce
   dimensionality, e.g. to two dimensions in this case) and you will find
   interesting relationships between words. as one can see above the
   distance between uncle and aunt is similar to the distance between man
   and woman. (source: [46]mikolov et al., 2013)

   by using id121 and the id97 methods we can turn a piece of
   text into a sequence of floating point representations of each word.

     now, what can we use a sequence of word representations for?

bag-of-words

   now let's talk about the [47]bag-of-words (bow), perhaps one of the
   simplest machine learning algorithms you will ever learn!
   [bow.png] take a number of word representations (the bottom gray boxes)
   and either sum or average them into a common representation (blue box)
   that should then contain some information from each word. in this post,
   the common representation is used to predict whether the sentence is
   positive or negative (red box).

   simply take the mean of the words across each feature dimension. it
   turns out that simply averaging id27s, even though it
   completely ignores the order of the sentence, works well on many simple
   practical examples and will often give a strong baseline when combined
   with deep neural networks (shown later). furthermore, taking the mean
   is a cheap operation and reduces the dimensionality of the sentence to
   a fixed sized vector.

recurrent neural networks

   some sentences require high precision or rely on sentence structure.
   using a bag-of-words for these problems might not cut it. instead, you
   might want to consider the amazing [48]recurrent neural network!
   [id56.png] at each timestep (going from left to right) an input (e.g. a
   word) is fed to the id56 (grey box) together with the previous internal
   memory (blue box). the id56 then perform some computation that results
   in a new internal memory (blue box) that represents all previous units
   seen (e.g. all previous words). the id56 should now contain information
   on a sentence level that allows it to better predict whether the
   sentence is positive or negative (red box).

   each id27 is, in order, fed to a recurrent neural network
   that then manages to store previously seen information and combine it
   with new words. when using an id56 powered by the famous memory cells
   such as the [49]long-short term memory cell (lstm) or the gated
   recurrent unit (gru), the id56 is capable of remembering what has
   happened in sentences with up to many words! (because of the lstm's
   success, the id56 with lstm memory cells is often referred to as the
   lstm). the biggest of these models stack eight of these on top of one
   another.
   [gid4.png] welcome to probably the most advanced deep learning model
   ever created, which uses id56s with lstm cells to translate language
   pairs. the pink, orange and green boxes are recurrent neural networks
   with lstm cells. they also applies tricks of the trade such as skip
   connections between the lstm layers and a method known as
   [50]attention. also notice that the green lstm is heading in the
   opposite direction. when combined with a normal lstm this is called a
   bidirectional lstm, as it gains information from the sequence of data
   in both directions. for more information check out this [51]blog post
   by stephen merity. (source: [52]wu et al., 2016)

   however, the lstm is much, much more expensive than the cheap
   bag-of-words model and will often require an experienced deep learning
   engineer to implement and support efficiently with high-performance
   computing hardware.

example: id31

   id31 is a type of document classification for quantifying
   polarity in subjective passages. given a sentence, the model evaluates
   whether it is positive, negative or neutral.

     want to find livid customers on twitter before they start trending?
     well, id31 might be just what you   re looking for!

   a great public dataset for this purpose (which we will use) is the
   [53]stanford sentiment treebank (sst). we have provided a [54]publicly
   available data loader in [55]pytorch. the sst provides not only the
   class (positive, negative) for a sentence, but also each of its
   grammatical subphrases. in our systems we do not utilize any tree
   information however. the original sst constitutes five classes: very
   positive, positive, neutral, negative and very negative. we consider
   the simpler task of binary classification where very positive is
   combined with positive, very negative is combined with negative and all
   neutrals are removed.

   we have provided a brief and technical description of our model
   architecture. the important point is not exactly how it is structured,
   but the fact that the cheap model gets 82% validation accuracy and
   takes 10 ms for a 64 sized batch, and the expensive lstm achieves a
   significantly higher 88% validation accuracy but costs 87 ms for a 64
   sized batch ([56]top models will be in the 88-90% accuracy ballpark).
   [bow-model.png] [id56-model.png] the bottom green box represents words
   embeddings, initialized with glove, followed by taking the mean of the
   words (bag-of-words) and a two layer mlp with dropout.the bottom
   turquise box represents words embeddings, initialized with glove.
   gradients are not tracked through the words embeddings. we use a
   bi-directional id56 with lstm cells, in similar fashion to the
   bag-of-words, we use the id56 hidden states to extract a mean and a max
   followed by a two layer mlp with dropout.

the cheap skim reader

   on some tasks, algorithms can perform at near human level accuracy, but
   obtaining this performance might burn a hole in the server budget. you
   also know that if it is not always necessary to have an lstm powerhouse
   with real world data, we might be just fine with the cheaper
   bag-of-words. but what happens when you get a sentence such as this:

     "horrible cast, complete lack of reality,    , but i loved it 9/10   

   the order agnostic bag-of-words will surely missclassify with the
   overwhelming amount of negative words. completely switching to a crummy
   bag-of-words would drop our overall performance, which doesn   t sound
   that compelling. so the question becomes:

     can we learn to separate    easy    and    hard    sentences?

   and can we do so with a cheap model to save time?

exploring the innards

   a popular way of exploring deep learning models is by plotting how each
   sentence is represented in the hidden layers. however, as the hidden
   layers are often high dimensional, we can use algorithms such as the
   [57]id167 to reduce dimensionality to 2d, allowing us to plot it for
   human inspection.

   id167 plots are vulnerable to many over-interpretations, but a few
   trends might strike you.

   interpretations of id167
     * the sentences fall into clusters. the clusters consitutes different
       semantic types.
     * some clusters lie along a simple manifold with high confidence and
       accuracy.
     * other clusters are more scattered with low accuracy and low
       confidence.
     * sentences with positive and negative consituents are difficult.

   let's now look at a similar plot for the lstm.

   you can move around, zoom, save and hover over the data points to see
   their informations. notice that the tooltip might work better if it is
   rendered to the right (move the data points to the left side).
   same setup as with the bag-of-words, explore the innards of the lstm!

   we can assess that many of these observations hold true for the lstm as
   well. however, the lstm only has relatively few examples with low
   confidence, and cooccurrence of positive and negative consituents in
   sentences does not look to be as challenging for the lstm as it is for
   the bag-of-words.

   it seems the bag-of-words has been able to cluster sentences and use
   its id203 to identify whether or not it is likely to give a
   correct prediction for the sentences in that cluster. from these
   observations, a reasonable hypothesis could be

     confident answers are more correct.

   to investigate this hypothesis, we can look at id203 thresholds.

id203 thresholding

   the bag-of-words and lstm are trained to give us probabilities for each
   class, which we can use as a measure of certainty. what do we mean by
   this? if the bag-of-words returns a 1, it is very confident in its
   prediction.

   often when predicting we would take the class with the highest
   likelihood provided by our model. in the case of binary classification
   (e.g. positive or negative) the likelihood has to be over 0.5 (or else
   we would be predicting the opposite class!). but a low likelihood for
   the predicted class might indicate that the model was in doubt. say the
   model predicted 0.49 for negative and 0.51 for positive, it might not
   be so convincing that it actually is positive.

   when we say that we threshold, what we mean is that we compare the
   predicted id203 to a value and assess whether or not to use it.
   e.g. we could decide that we use all sentences with a id203 above
   0.7. or we look at the interval 0.5-0.55 to see how accurate
   predictions with this confidence are, which is exactly what we will
   investigate in the next plot.

   on the threshold plot, the height of the bar corresponds to the
   accuracy of data points within two thresholds and the line is analogous
   to the accuracy when using all data points beyond a given threshold.
   in the data amount plot, the height of the bar corresponds to the
   amount of data reciding within two thresholds and the line is the
   accumulated data from each threshold bin.

   from the bag-of-words plots it might occour to you that increasing the
   id203 threshold increases the performance. from the lstm plot it
   is not so obvious, which seems common as the lstm overfits the training
   set and only provides confident answers.

     use the bow for easy examples, and the pristine lstm for difficult
     ones.

   thus, simply using the output id203 could give us an indication
   of when a sentence is easy and when it is in need of guidance from a
   stronger system, like the powerful lstm.

   using the id203 threshold, we create a strategy which we refer to
   as the "id203 strategy", such that we threshold the id203
   of the bag-of-word system, and use the lstm on all data points not
   reaching the threshold. doing so provides us with an amount of data
   used for the bag-of-words (sentences above the threshold) and a set of
   data points where we have either chosen the bow (above the threshold)
   or the lstm (below the threshold), which we can use to find an accuracy
   and cost of computing. we then get a ratio between the bow and the lstm
   increasing from 0.0 (only using lstm) to 1.0 (only using bow), which we
   can use to calculate the accuracy and time to compute.

baseline

   to construct a baseline we consider the ratio between the two models,
   e.g. 0.1 data used for bow would correspond to 0.9 times lstm accuracy
   and 0.1 times bow accuracy. the purpose is to have a baseline with no
   guided strategy where the choice of using bow or lstm on a sentence is
   randomly assigned. however, there is a cost to using the strategies. we
   have to run all of the sentences through the bag-of-words model first,
   to determine if we should use the bag-of-words or the lstm. in case
   that none of the sentences reaches the id203 threshold, we could
   be running an extra model for no good reason. to incorporate this, we
   calculate the cost of our strategies and the ratio in the following
   way.
   [time.png]

   where c is the cost and p the proportion of data used for
   bag-of-wordsresults on the validation set comparing the accuracy and
   speed from different ratio combinations between bow and lstm (red line)
   and the id203 thresholding strategy (blue line) data points to
   the far left corresponds to only using a lstm while the far right
   corresponds to only using the bag-of-words, in between corresponds to
   mixes of the two. the blue line represents using a combination of cbow
   and lstm with no guided strategies, while the red curve depicts using
   the bag-of-word id203 as a strategy to guide which passages to
   use which system for. hover over the lines to see time saved for each
   ratio/id203 threshold. notice that maximum time saves is ~90% as
   this corresponds to only using bag-of-words.

   the interesting discovery is that we find that using the bag-of-words
   thresholds significantly outperforms not having a guided strategy.

   we then measure the average value on the curve, which, we refer to as
   speed under the curve (suc). as shown in table below.
            strategy          validation suc
   ratio between bow and lstm 84.84
   id203                86.03 (std=0.3)
   results on strategy for discretely choosing whether to use the bow or
   lstm on the validation set. each model is computed ten times with
   different seed. the results in the table are from averaging the sucs.
   the id203 strategy is compared to the ratio.

learning when to skim and when to read

   knowing when to switch between two different models is not enough. we
   want to build a more general system that learns when to switch between
   each model. such a system would help us deal with the more complicated
   behaviour of

     can we learn when reading is strictly better than skimming in a
     supervised way?

   where "reading" is using the lstm which goes from left to right and
   stores a memory at each time step and "skimming" is using the bow
   model. when operating on the id203 from the bag-of-words model we
   make our decision based on the invariant that the more powerful lstm
   will do a better job when the bag-of-word system is in doubt, but is
   that always the case?
   [conf.png] a confusion matrix between when the bag-of-words and the
   lstm are correct or incorrect about a sentence. similar to the
   confusion id167 plot between bag-of-words and lstm from earlier.

   in fact, it turns out that it is only the case 12% of the time, whereas
   6% of the sentences neither the bag-of-words or the lstm get correct.
   in such case, we have no reason to run the lstm and we might as well
   just save time by only using the bag-of-words.

learning to skim, the setup

   so we don   t always want to use the lstm when the bow is in doubt. can
   we make our bag-of-word model understand when the lstm also might be
   wrong and when we should spare our precious computational resources?
   let us look at the id167 plot again, but now with the confusion matrix
   between the bow and the lstm plotted. we hope to find a relationship
   between the elements of the confusion matrix, especially when the bow
   is incorrect.

   from the comparison plot, we find that it is easy to assert when the
   bow is correct and when it is in doubt. however, there is no clear
   relationship between when the lstm might be right or wrong.

     can we learn this relationship?

   further, the id203 strategy is quite restrictive as it relies on
   an inheritent binary decision and requires probabilities. instead, we
   propose a trainable decision network that is based on a neural network.
   if we look at the confusion matrix, we can use that information to
   generate labels for a supervised decision network. in this way, we
   would only use the lstm in the cases where the lstm is correct and the
   bow is wrong.

   to generate the dataset, we need a set of sentences having the true,
   underlying, prediction of our bag-of-words and the lstm. however,
   during training the lstm will often achieve upwards 99% training
   accuracy, significantly overfitting the training set. to avoid this, we
   split our training set into a model training set (80% of training data)
   and a decision training set (remaining 20% of training data) that the
   model has not yet seen. afterwards we fine-tune our model with the
   remaining 20%, hoping that the decision network will still generalize
   to this new, unseen, but very related and slightly better system.
   [split.png] both the bag-of-words and the lstm are initially trained on
   the    model train    (80\% of the training data), then these models are
   used to generate labels for the decision network and lastly both models
   are trained on the full dataset. the same validation set is used at all
   times.

   to build our decision network, we tap into the last hidden layer of our
   cheap bag-of-words system (the same layer we used to generate our id167
   plots). we then stack a two layer mlp on top of our bag-of-words
   training on the model training set. we found that if we do not follow
   this recipe, the decision network will not be able to learn the
   tendencies of the bow model and will not generalize well.
   [bow-decision.png] the bottom turquoise boxes represents the layers
   from the bag-of-words system, without dropout. a two layer mlp is then
   attached on top and a class on whether to use the bag-of-words or the
   preeminent lstm.

   the classes chosen on the validation set by the decision network, based
   on the models trained on the model training set, is then applied to the
   full, but very related, models on the full training set. the reason why
   we apply it on the model trained on the full training set, is that the
   models on the model training set will often be inferior and thus result
   in a lower accuracy. the decision network is trained with early
   stopping, based on maximizing the suc on the validation set.

     how does our decision network perform?

   let us start by looking at the predictions of the decision network.
   [dn_plot_predictions.png] the data points are the same from the
   previous id167 plots that we have seen using the bag-of-words model.
   the green dots represent sentences predicted with the bag-of-words
   whereas the yellow represents the lstm.

   notice how closely this resembles the id203 cutoff of the
   bag-of-words. now let us look at the id167 of the last hidden layer of
   the decision network, to see if it is actually able to cluster some
   information of when the lstm is correct or wrong.
   [dn_plot_compare.png] the data points are based on sentence
   representation of the last hidden state of the decision network, from
   the validation sentences. the colours are equal to the compare plot we
   have shown previously.

   it seems the decision network is capable of picking up the id91
   from the hidden states of the bag-of-words. however, it does not seem
   like it is able to understand when the lstm might also be wrong
   (id91 yellows from reds).
   [dn_curve.png] the purple curve represents the new introduced decision
   network on the validation set. notice how the decision network achieves
   a close, but slight different solution than id203 thresholding.

   from the data accuracy over saved time curves, it is not obvious
   whether or not the decision network is better.
             policy           validation suc     test suc
   ratio between bow and lstm 84.84           83.77
   id203                86.03 (std=0.3) 85.49 (std=0.3)
   decision network           86.13 (std=0.3) 85.49 (std=0.3)
   results on strategy for discretely choosing whether to use the bow or
   lstm on the validation and test set. the suc is based on the mean value
   of the curve for our accuracy vs speed-up plot. each model is computed
   ten times with different seed. the results in the table are from
   averaging the sucs. the standard deviation is based on the difference
   from the ratio.

   from prediction plot, data amount vs. accuracy and suc score we can
   infer that the decision network is splendid at understanding when the
   bow might be correct and when it is not. further, it allows us to build
   a more general system that taps into the hidden states of deep learning
   models. however, it also goes to show that it was very difficult to
   make the decision network understand the behaviour of systems that it
   did not have access to, such as the more complex lstm.

discussion

   we now know that large powerful lstms can achieve near human-level
   performance on text, that not all real-world data needs near
   human-level performance, that we can train a bag-of-words model to
   understand when a sentence is easy and that using bag-of-words for easy
   sentences allows us to save a significant amount of computation time
   with only a minor drop in performance (depending on how aggressive we
   threshold the bag-of-words).

   this approach is related to mean averaging usually performed when model
   ensembling as often the model with high confidence will be used.
   however, by having an adjustable confidence from the bag-of-words and
   not needing to run the lstm, we can decide how much computation time
   vs. accuracy savings we are interested in. we believe that this method
   will be useful for deep learning engineers looking to save
   computational resources without having to sacrifice performance.

citation credit

   alexander rosenberg johansen, and richard socher. 2017.
   [58]learning when to skim and when to read
     __________________________________________________________________

   by having a better understanding of when a deep learning system might
   be wrong, we can make informed decisions about when to use which deep
   learning model. this allows us to save computational time by only
   running the bare minimum to complete a task.

   [59]salesforce logo [60]content [61]terms [62]privacy
      copyright 2017 [63]salesforce.com, inc. [64]all rights reserved.
   rights of albert einstein are used with permission of the hebrew
   university of jerusalem. represented exclusively by greenlight.

references

   1. https://blog.einstein.ai/rss/
   2. https://www.einstein.ai/
   3. https://einstein.ai/mission
   4. https://einstein.ai/research
   5. https://einstein.ai/careers
   6. https://einstein.ai/ethics
   7. https://einstein.ai/products
   8. https://einstein.ai/press
   9. https://einstein.ai/research/publications
  10. https://einstein.ai/research/blog
  11. https://einstein.ai/research/open-source
  12. https://einstein.ai/research/grants
  13. https://www.einstein.ai/
  14. javascript:void(0);
  15. https://einstein.ai/mission
  16. https://einstein.ai/research
  17. https://einstein.ai/research/publications
  18. https://einstein.ai/research/blog
  19. https://einstein.ai/research/open-source
  20. https://einstein.ai/research/grants
  21. https://einstein.ai/careers
  22. https://einstein.ai/ethics
  23. https://einstein.ai/products
  24. https://einstein.ai/press
  25. https://www.einstein.ai/
  26. https://einstein.ai/mission
  27. https://einstein.ai/research
  28. https://einstein.ai/careers
  29. https://einstein.ai/ethics
  30. https://einstein.ai/products
  31. https://einstein.ai/press
  32. https://einstein.ai/research/publications
  33. https://einstein.ai/research/blog
  34. https://einstein.ai/research/open-source
  35. https://einstein.ai/research/grants
  36. https://blog.einstein.ai/author/alexander/
  37. https://www.wired.com/2016/01/microsoft-neural-net-shows-deep-learning-can-get-way-deeper/
  38. https://www.sciencenews.org/article/alphago-artificial-intelligence-top-science-stories-2016
  39. http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf
  40. https://svail.github.io/mandarin/
  41. https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
  42. https://arxiv.org/abs/1508.07909
  43. https://arxiv.org/abs/1610.03017
  44. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  45. http://nlp.stanford.edu/projects/glove/
  46. https://docs.google.com/file/d/0b7xkcwpi5kdyrwrnd1rzwxq2twc/edit
  47. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  48. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  49. http://colah.github.io/posts/2015-08-understanding-lstms/
  50. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  51. https://smerity.com/articles/2016/google_id4_arch.html
  52. https://arxiv.org/abs/1609.08144
  53. http://nlp.stanford.edu/sentiment/treebank.html
  54. https://github.com/pytorch/text
  55. https://einstein.ai/research/blog/pytorch.org
  56. https://arxiv.org/abs/1506.07285
  57. https://lvdmaaten.github.io/tsne/
  58. https://arxiv.org/abs/1712.05483
  59. https://www.salesforce.com/
  60. https://einstein.ai/content-takedown
  61. https://einstein.ai/terms-of-service
  62. https://einstein.ai/privacy
  63. https://www.salesforce.com/
  64. https://www.salesforce.com/company/legal/intellectual.jsp
