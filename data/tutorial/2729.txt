   #[1]tensorflow

   (button)
   [2]tensorflow
     *

   [3]install [4]learn
     * [5]introduction
       new to tensorflow?
     * [6]tensorflow
       the core open source ml library
     * [7]for javascript
       tensorflow.js for ml using javascript
     * [8]for mobile & iot
       tensorflow lite for mobile and embedded devices
     * [9]for production
       tensorflow extended for end-to-end ml components
     * [10]swift for tensorflow (in beta)

   [11]api
     * api r1
     * [12]r1.13 (stable)
     * [13]r1.12
     * [14]r1.11
     * [15]r1.10
     * [16]r1.9
     * [17]more   

     * api r2
     * [18]r2.0 (preview)

   [19]resources
     * [20]models & datasets
       pre-trained models and datasets built by google and the community
     * [21]tools
       ecosystem of tools to help you use tensorflow
     * [22]libraries & extensions
       libraries and extensions built on tensorflow

   [23]community [24]why tensorflow
     * [25]about
     * [26]case studies

   ____________________
   (button)
   (button)
   [27]github
     * [28]tensorflow core

   [29]overview [30]tutorials [31]guide [32]tf 2.0 alpha

   (button)
     * [33]install
     * [34]learn
          + more
          + [35]overview
          + [36]tutorials
          + [37]guide
          + [38]tf 2.0 alpha
     * [39]api
          + more
     * [40]resources
          + more
     * [41]community
     * [42]why tensorflow
          + more
     * [43]github

     * [44]get started with tensorflow
     * learn and use ml
          + [45]overview
          + [46]basic classification
          + [47]text classification
          + [48]regression
          + [49]overfitting and underfitting
          + [50]save and restore models
     * research and experimentation
          + [51]overview
          + [52]eager execution
          + [53]automatic differentiation
          + [54]custom training: basics
          + [55]custom layers
          + [56]custom training: walkthrough
     * ml at production scale
          + [57]linear model with estimators
          + [58]wide and deep learning
          + [59]boosted trees
          + [60]boosted trees model understanding
          + [61]build a id98 using estimators
     * generative models
          + [62]translation with attention
          + [63]image captioning
          + [64]dcgan
          + [65]vae
     * images
          + [66]image recognition
          + [67]pix2pix
          + [68]neural style transfer
          + [69]image segmentation
          + [70]advanced id98
     * sequences
          + [71]text generation with an id56
          + [72]recurrent neural network
          + [73]drawing classification
          + [74]simple audio recognition
          + [75]id4
     * load data
          + [76]load images
          + [77]tfrecords and tf.example
     * data representation
          + [78]vector representations of words
          + [79]kernel methods
          + [80]large-scale linear models
          + [81]unicode
     * non-ml
          + [82]mandelbrot set
          + [83]partial differential equations

     * [84]introduction
     * [85]tensorflow
     * [86]for javascript
     * [87]for mobile & iot
     * [88]for production
     * [89]swift for tensorflow (in beta)

     * api r1
     * [90]r1.13 (stable)
     * [91]r1.12
     * [92]r1.11
     * [93]r1.10
     * [94]r1.9
     * [95]more   
     * api r2
     * [96]r2.0 (preview)

     * [97]models & datasets
     * [98]tools
     * [99]libraries & extensions

     * [100]about
     * [101]case studies

   watch talks from the 2019 tensorflow dev summit [102]watch now
     * [103]tensorflow
     * [104]learn
     * [105]tensorflow core
     * [106]tutorials

recurrent neural networks

introduction

   see [107]understanding id137 for an introduction to recurrent
   neural networks and lstms.

id38

   in this tutorial we will show how to train a recurrent neural network
   on a challenging task of id38. the goal of the problem is
   to fit a probabilistic model which assigns probabilities to sentences.
   it does so by predicting next words in a text given a history of
   previous words. for this purpose we will use the [108]penn tree bank
   (ptb) dataset, which is a popular benchmark for measuring the quality
   of these models, whilst being small and relatively fast to train.

   id38 is key to many interesting problems such as speech
   recognition, machine translation, or image captioning. it is also fun
   -- take a look [109]here.

   for the purpose of this tutorial, we will reproduce the results from
   [110]zaremba et al., 2014 ([111]pdf), which achieves very good quality
   on the ptb dataset.

tutorial files

   this tutorial references the following files from
   models/tutorials/id56/ptb in the [112]tensorflow models repo:
        file                             purpose
   ptb_word_lm.py the code to train a language model on the ptb dataset.
   reader.py      the code to read the dataset.

download and prepare the data

   the data required for this tutorial is in the data/ directory of the
   [113]ptb dataset from tomas mikolov's webpage.

   the dataset is already preprocessed and contains overall 10000
   different words, including the end-of-sentence marker and a special
   symbol (<unk>) for rare words. in reader.py, we convert each word to a
   unique integer identifier, in order to make it easy for the neural
   network to process the data.

the model

lstm

   the core of the model consists of an lstm cell that processes one word
   at a time and computes probabilities of the possible values for the
   next word in the sentence. the memory state of the network is
   initialized with a vector of zeros and gets updated after reading each
   word. for computational reasons, we will process data in mini-batches
   of size batch_size. in this example, it is important to note that
   current_batch_of_words does not correspond to a "sentence" of words.
   every word in a batch should correspond to a time t. tensorflow will
   automatically sum the gradients of each batch for you.

   for example:
 t=0  t=1    t=2  t=3     t=4
[the, brown, fox, is,     quick]
[the, red,   fox, jumped, high]

words_in_dataset[0] = [the, the]
words_in_dataset[1] = [brown, red]
words_in_dataset[2] = [fox, fox]
words_in_dataset[3] = [is, jumped]
words_in_dataset[4] = [quick, high]
batch_size = 2, time_steps = 5

   the basic pseudocode is as follows:
words_in_dataset = tf.placeholder(tf.float32, [time_steps, batch_size, num_featu
res])
lstm = tf.contrib.id56.basiclstmcell(lstm_size)
# initial state of the lstm memory.
state = lstm.zero_state(batch_size, dtype=tf.float32)
probabilities = []
loss = 0.0
for current_batch_of_words in words_in_dataset:
    # the value of state is updated after processing each batch of words.
    output, state = lstm(current_batch_of_words, state)

    # the lstm output can be used to make next word predictions
    logits = tf.matmul(output, softmax_w) + softmax_b
    probabilities.append(tf.nn.softmax(logits))
    loss += loss_function(probabilities, target_words)

truncated id26

   by design, the output of a recurrent neural network (id56) depends on
   arbitrarily distant inputs. unfortunately, this makes id26
   computation difficult. in order to make the learning process tractable,
   it is common practice to create an "unrolled" version of the network,
   which contains a fixed number (num_steps) of lstm inputs and outputs.
   the model is then trained on this finite approximation of the id56. this
   can be implemented by feeding inputs of length num_steps at a time and
   performing a backward pass after each such input block.

   here is a simplified block of code for creating a graph which performs
   truncated id26:
# placeholder for the inputs in a given iteration.
words = tf.placeholder(tf.int32, [batch_size, num_steps])

lstm = tf.contrib.id56.basiclstmcell(lstm_size)
# initial state of the lstm memory.
initial_state = state = lstm.zero_state(batch_size, dtype=tf.float32)

for i in range(num_steps):
    # the value of state is updated after processing each batch of words.
    output, state = lstm(words[:, i], state)

    # the rest of the code.
    # ...

final_state = state

   and this is how to implement an iteration over the whole dataset:
# a numpy array holding the state of lstm after each batch of words.
numpy_state = initial_state.eval()
total_loss = 0.0
for current_batch_of_words in words_in_dataset:
    numpy_state, current_loss = session.run([final_state, loss],
        # initialize the lstm state from the previous iteration.
        feed_dict={initial_state: numpy_state, words: current_batch_of_words})
    total_loss += current_loss

inputs

   the word ids will be embedded into a dense representation (see the
   [114]vector representations tutorial) before feeding to the lstm. this
   allows the model to efficiently represent the knowledge about
   particular words. it is also easy to write:
# embedding_matrix is a tensor of shape [vocabulary_size, embedding size]
word_embeddings = tf.nn.embedding_lookup(embedding_matrix, word_ids)

   the embedding matrix will be initialized randomly and the model will
   learn to differentiate the meaning of words just by looking at the
   data.

id168

   we want to minimize the average negative log id203 of the target
   words:
   $$ \text{loss} = -\frac{1}{n}\sum_{i=1}^{n} \ln p_{\text{target}_i} $$

   it is not very difficult to implement but the function
   sequence_loss_by_example is already available, so we can just use it
   here.

   the typical measure reported in the papers is average per-word
   perplexity (often just called perplexity), which is equal to
   $$e^{-\frac{1}{n}\sum_{i=1}^{n} \ln p_{\text{target}_i}} =
   e^{\text{loss}} $$

   and we will monitor its value throughout the training process.

stacking multiple lstms

   to give the model more expressive power, we can add multiple layers of
   lstms to process the data. the output of the first layer will become
   the input of the second and so on.

   we have a class called multiid56cell that makes the implementation
   seaid113ss:
def lstm_cell():
  return tf.contrib.id56.basiclstmcell(lstm_size)
stacked_lstm = tf.contrib.id56.multiid56cell(
    [lstm_cell() for _ in range(number_of_layers)])

initial_state = state = stacked_lstm.zero_state(batch_size, tf.float32)
for i in range(num_steps):
    # the value of state is updated after processing each batch of words.
    output, state = stacked_lstm(words[:, i], state)

    # the rest of the code.
    # ...

final_state = state

run the code

   before running the code, download the ptb dataset, as discussed at the
   beginning of this tutorial. then, extract the ptb dataset underneath
   your home directory as follows:
tar xvfz simple-examples.tgz -c $home

   (note: on windows, you may need to use [115]other tools.)

   now, clone the [116]tensorflow models repo from github. run the
   following commands:
cd models/tutorials/id56/ptb
python ptb_word_lm.py --data_path=$home/simple-examples/data/ --model=small

   there are 3 supported model configurations in the tutorial code:
   "small", "medium" and "large". the difference between them is in size
   of the lstms and the set of hyperparameters used for training.

   the larger the model, the better results it should get. the small model
   should be able to reach perplexity below 120 on the test set and the
   large one below 80, though it might take several hours to train.

what next?

   there are several tricks that we haven't mentioned that make the model
   better, including:
     * decreasing learning rate schedule,
     * dropout between the lstm layers.

   study the code and modify it to improve the model even further.

   except as otherwise noted, the content of this page is licensed under
   the [117]creative commons attribution 3.0 license, and code samples are
   licensed under the [118]apache 2.0 license. for details, see the
   [119]google developers site policies. java is a registered trademark of
   oracle and/or its affiliates.

     * stay connected
          + [120]blog
          + [121]github
          + [122]twitter
          + [123]youtube
     * support
          + [124]issue tracker
          + [125]release notes
          + [126]stack overflow
          + [127]brand guidelines

     * [128]terms
     * [129]privacy

   [english_____]

references

   visible links
   1. https://www.tensorflow.org/s/opensearch.xml
   2. https://www.tensorflow.org/
   3. https://www.tensorflow.org/install
   4. https://www.tensorflow.org/learn
   5. https://www.tensorflow.org/learn
   6. https://www.tensorflow.org/overview
   7. https://www.tensorflow.org/js
   8. https://www.tensorflow.org/lite
   9. https://www.tensorflow.org/tfx
  10. https://www.tensorflow.org/swift
  11. https://www.tensorflow.org/api_docs/python/tf
  12. https://www.tensorflow.org/api_docs/python/tf
  13. https://www.tensorflow.org/versions/r1.12/api_docs/python/tf
  14. https://www.tensorflow.org/versions/r1.11/api_docs/python/tf
  15. https://www.tensorflow.org/versions/r1.10/api_docs/python/tf
  16. https://www.tensorflow.org/versions/r1.9/api_docs/python/tf
  17. https://www.tensorflow.org/versions
  18. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf
  19. https://www.tensorflow.org/resources/models-datasets
  20. https://www.tensorflow.org/resources/models-datasets
  21. https://www.tensorflow.org/resources/tools
  22. https://www.tensorflow.org/resources/libraries-extensions
  23. https://www.tensorflow.org/community
  24. https://www.tensorflow.org/about
  25. https://www.tensorflow.org/about
  26. https://www.tensorflow.org/about/case-studies
  27. https://github.com/tensorflow
  28. https://www.tensorflow.org/overview
  29. https://www.tensorflow.org/overview
  30. https://www.tensorflow.org/tutorials
  31. https://www.tensorflow.org/guide
  32. https://www.tensorflow.org/alpha
  33. https://www.tensorflow.org/install
  34. https://www.tensorflow.org/learn
  35. https://www.tensorflow.org/overview
  36. https://www.tensorflow.org/tutorials
  37. https://www.tensorflow.org/guide
  38. https://www.tensorflow.org/alpha
  39. https://www.tensorflow.org/api_docs/python/tf
  40. https://www.tensorflow.org/resources/models-datasets
  41. https://www.tensorflow.org/community
  42. https://www.tensorflow.org/about
  43. https://github.com/tensorflow
  44. https://www.tensorflow.org/tutorials
  45. https://www.tensorflow.org/tutorials/keras
  46. https://www.tensorflow.org/tutorials/keras/basic_classification
  47. https://www.tensorflow.org/tutorials/keras/basic_text_classification
  48. https://www.tensorflow.org/tutorials/keras/basic_regression
  49. https://www.tensorflow.org/tutorials/keras/overfit_and_underfit
  50. https://www.tensorflow.org/tutorials/keras/save_and_restore_models
  51. https://www.tensorflow.org/tutorials/eager
  52. https://www.tensorflow.org/tutorials/eager/eager_basics
  53. https://www.tensorflow.org/tutorials/eager/automatic_differentiation
  54. https://www.tensorflow.org/tutorials/eager/custom_training
  55. https://www.tensorflow.org/tutorials/eager/custom_layers
  56. https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough
  57. https://www.tensorflow.org/tutorials/estimators/linear
  58. https://github.com/tensorflow/models/tree/master/official/wide_deep
  59. https://www.tensorflow.org/tutorials/estimators/boosted_trees
  60. https://www.tensorflow.org/tutorials/estimators/boosted_trees_model_understanding
  61. https://www.tensorflow.org/tutorials/estimators/id98
  62. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/id4_with_attention/id4_with_attention.ipynb
  63. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb
  64. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb
  65. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/cvae.ipynb
  66. https://www.tensorflow.org/tutorials/images/hub_with_keras
  67. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb
  68. https://github.com/tensorflow/models/blob/master/research/nst_blogpost/4_neural_style_transfer_with_eager_execution.ipynb
  69. https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb
  70. https://www.tensorflow.org/tutorials/images/deep_id98
  71. https://www.tensorflow.org/tutorials/sequences/text_generation
  72. https://www.tensorflow.org/tutorials/sequences/recurrent
  73. https://www.tensorflow.org/tutorials/sequences/recurrent_quickdraw
  74. https://www.tensorflow.org/tutorials/sequences/audio_recognition
  75. https://github.com/tensorflow/id4
  76. https://www.tensorflow.org/tutorials/load_data/images
  77. https://www.tensorflow.org/tutorials/load_data/tf_records
  78. https://www.tensorflow.org/tutorials/representation/id97
  79. https://www.tensorflow.org/tutorials/representation/kernel_methods
  80. https://www.tensorflow.org/tutorials/representation/linear
  81. https://www.tensorflow.org/tutorials/representation/unicode
  82. https://www.tensorflow.org/tutorials/non-ml/mandelbrot
  83. https://www.tensorflow.org/tutorials/non-ml/pdes
  84. https://www.tensorflow.org/learn
  85. https://www.tensorflow.org/overview
  86. https://www.tensorflow.org/js
  87. https://www.tensorflow.org/lite
  88. https://www.tensorflow.org/tfx
  89. https://www.tensorflow.org/swift
  90. https://www.tensorflow.org/api_docs/python/tf
  91. https://www.tensorflow.org/versions/r1.12/api_docs/python/tf
  92. https://www.tensorflow.org/versions/r1.11/api_docs/python/tf
  93. https://www.tensorflow.org/versions/r1.10/api_docs/python/tf
  94. https://www.tensorflow.org/versions/r1.9/api_docs/python/tf
  95. https://www.tensorflow.org/versions
  96. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf
  97. https://www.tensorflow.org/resources/models-datasets
  98. https://www.tensorflow.org/resources/tools
  99. https://www.tensorflow.org/resources/libraries-extensions
 100. https://www.tensorflow.org/about
 101. https://www.tensorflow.org/about/case-studies
 102. https://www.youtube.com/playlist?list=plqy2h8rroyvzouyi26khmksjbedn3squb
 103. https://www.tensorflow.org/
 104. https://www.tensorflow.org/learn
 105. https://www.tensorflow.org/overview
 106. https://www.tensorflow.org/tutorials
 107. https://colah.github.io/posts/2015-08-understanding-lstms/
 108. https://catalog.ldc.upenn.edu/ldc99t42
 109. https://karpathy.github.io/2015/05/21/id56-effectiveness/
 110. https://arxiv.org/abs/1409.2329
 111. https://arxiv.org/pdf/1409.2329.pdf
 112. https://github.com/tensorflow/models
 113. http://www.fit.vutbr.cz/~imikolov/id56lm/simple-examples.tgz
 114. https://www.tensorflow.org/tutorials/representation/id97
 115. https://wiki.haskell.org/how_to_unpack_a_tar_file_in_windows
 116. https://github.com/tensorflow/models
 117. https://creativecommons.org/licenses/by/3.0/
 118. https://www.apache.org/licenses/license-2.0
 119. https://developers.google.com/site-policies
 120. https://medium.com/tensorflow
 121. https://github.com/tensorflow/
 122. https://twitter.com/tensorflow
 123. https://youtube.com/tensorflow
 124. https://github.com/tensorflow/tensorflow/issues
 125. https://github.com/tensorflow/tensorflow/blob/master/release.md
 126. https://stackoverflow.com/questions/tagged/tensorflow
 127. https://www.tensorflow.org/extras/tensorflow_brand_guidelines.pdf
 128. https://policies.google.com/terms
 129. https://policies.google.com/privacy

   hidden links:
 131. https://www.tensorflow.org/tutorials/sequences/recurrent
 132. https://www.tensorflow.org/tutorials/sequences/recurrent
 133. https://www.tensorflow.org/tutorials/sequences/recurrent
 134. https://www.tensorflow.org/tutorials/sequences/recurrent
