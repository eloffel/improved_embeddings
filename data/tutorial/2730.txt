a guide to deep learning by

   deep learning is a fast-changing field at the intersection of computer
   science and mathematics. it is a relatively new branch of a wider field
   called machine learning. the goal of machine learning is to teach
   computers to perform various tasks based on the given data. this guide
   is for those who know some math, know some programming language and now
   want to dive deep into deep learning.
   this is not a guide to:
       general machine learning
       big data processing
       data science
       [1]deep id23

prerequisites

   you must know standard university-level math. you can review those
   concepts in the first chapters of the book [2]deep learning:
     * [3]deep learning, chapter 2: id202
     * [4]deep learning, chapter 3: id203 and id205
     * [5]deep learning, chapter 4: numerical computation

   you must know programming to develop and test deep learning models. we
   suggest using python for machine learning. numpy/scipy libraries for
   scientific computing are required.
     * [6]justin johnson's python / numpy / scipy / matplotlib tutorial
       for stanford's cs231n    
     * [7]scipy lecture notes - cover commonly used libraries in more
       details and introduce more advanced topics       

   when you are comfortable with the prerequisites, we suggest four
   options for studying deep learning. choose any of them or any
   combination of them. the number of stars indicates the difficulty.
     * [8]hugo larochelle's video course on youtube. the videos were
       recorded in 2013 but most of the content is still fresh. the
       mathematics behind neural networks is explained in detail.
       [9]slides and related materials are available.       
     * [10]stanford's cs231n (convolutional neural networks for visual
       recognition) by fei-fei li, andrej karpathy and justin johnson. the
       course is focused on image processing, but covers most of the
       important concepts in deep learning. [11]videos (2016) and
       [12]lecture notes are available.       
     * michael nielsen's online book [13]neural networks and deep learning
       is the easiest way to study neural networks. it doesn't cover all
       important topics, but contains intuitive explanations and code for
       the basic concepts.    
     * [14]deep learning, a book by ian goodfellow, yoshua bengio and
       aaron courville, is the most comprehensive resource for studying
       deep learning. it covers a lot more than all the other courses
       combined.          
       there are many software frameworks that provide necessary
       functions, classes and modules for machine learning and for deep
       learning in particular. we suggest you not use these frameworks at
       the early stages of studying, instead we suggest you implement the
       basic algorithms from scratch. most of the courses describe the
       maths behind the algorithms in enough detail, so they can be easily
       implemented.
          + [15]jupyter notebooks are a convenient way to play with python
            code. they are nicely integrated with matplotlib, a popular
            tool for visualizations. we suggest you implement algorithms
            in such environments.    

machine learning basics
       machine learning is the art and science of teaching computers based
       on data. it is a relatively established field at the intersection
       of computer science and mathematics, while deep learning is just a
       small subfield of it. the concepts and tools of machine learning
       are important for understanding deep learning.
          + [16]visual introduction to machine learning - id90    
          + [17]andrew ng's course on machine learning, the most popular
            course on coursera       
          + larochelle's course doesn't have separate introductory
            lectures for general machine learning, but all required
            concepts are defined and explained whenever needed.
          + [18]1. training and testing the models (knn)       
          + [19]2. linear classification (id166)       
          + [20]3. optimization (stochastic id119)       
          + [21]5. machine learning basics          
          + [22]principal component analysis explained visually    
          + [23]how to use id167 effectively       
       most of the popular machine learning algorithms are implemented in
       the [24]scikit-learn python library. implementing some of them from
       scratch helps with understanding how machine learning works.
          + [25]practical machine learning tutorial with python covers
            id75, k-nearest-neighbors and support vector
            machines. first it shows how to use them from scikit-learn,
            then implements the algorithms from scratch.    
          + andrew ng's course on coursera has many assignments in octave
            language. the same algorithms can be implemented in python.       

neural networks basics
       neural networks are powerful machine learning algorithms. they form
       the basis of deep learning.
          + [26]a visual and interactive guide to the basics of neural
            networks - shows how simple neural networks can do linear
            regression    
          + [27]1. feedforward neural network       
          + [28]2. training neural networks (up to 2.7)       
          + [29]4. id26       
          + [30]5. architecture of neural networks       
          + [31]1. using neural nets to recognize handwritten digits    
          + [32]2. how the id26 algorithm works    
          + [33]4. a visual proof that neural nets can compute any
            function    
          + [34]6. deep feedforward networks          
          + [35]yes you should understand backprop explains why it is
            important to implement id26 once from scratch       
          + [36]calculus on computational graphs: id26       
          + [37]play with neural networks!    
       try to implement a single layer neural network from scratch,
       including the training procedure.
          + [38]implementing softmax classifier and a simple neural
            network in pure python/numpy - jupyter notebook available    
          + andrej karpathy implements id26 in javascript in
            his [39]hacker's guide to neural networks.    
          + [40]implementing a neural network from scratch in python    

improving the way neural networks learn
       it's not very easy to train neural networks. sometimes they don't
       learn at all (underfitting), sometimes they learn exactly what you
       give them and their "knowledge" does not generalize to new, unseen
       data (overfitting). there are many ways to handle these problems.
          + [41]2.8-2.11. id173, parameter initialization etc.       
          + [42]7.5. dropout       
          + [43]6 (first half). setting up the data and loss       
          + [44]3. improving the way neural networks learn    
          + [45]5. why are deep neural networks hard to train?    
          + [46]7. id173 for deep learning          
          + [47]8. optimization for training deep models          
          + [48]11. practical methodology          
          + [49]convnetjs trainer demo on mnist - visualizes the
            performance of different optimization algorithms    
          + [50]an overview of id119 optimization algorithms
                     
          + [51]neural networks, manifolds, and topology          
       there are many frameworks that provide the standard algorithms and
       are optimised for good performance on modern hardware. most of
       these frameworks have interfaces for python with the notable
       exception of torch, which requires lua. once you know how basic
       learning algorithms are implemented under the hood, it's time to
       choose a framework to build on.
          + [52]theano provides low-level primitives for constructing all
            kinds of neural networks. it is maintained by [53]a machine
            learning group at university of montreal. see also:
            [54]speeding up your neural network with theano and the gpu -
            jupyter notebook available    
          + [55]tensorflow is another low-level framework. its
            architecture is similar to theano. it is maintained by the
            google brain team.
          + [56]torch is a popular framework that uses lua language. the
            main disadvantage is that lua's community is not as large as
            python's. torch is mostly maintained by facebook and twitter.
       there are also higher-level frameworks that run on top of these:
          + [57]lasagne is a higher level framework built on top of
            theano. it provides simple functions to create large networks
            with few lines of code.
          + [58]keras is a higher level framework that works on top of
            either theano or tensorflow.
          + if you need more guidance on which framework is right for you,
            see [59]lecture 12 of stanford's cs231n.       

convolutional neural networks
       convolutional networks ("id98s") are a special kind of neural nets
       that use several clever tricks to learn faster and better. convnets
       essentially revolutionized id161 and are heavily used in
       id103 and text classification as well.
          + [60]9. id161 (up to 9.9)       
          + [61]6 (second half). intro to convnets       
          + [62]7. convolutional neural networks       
          + [63]8. localization and detection       
          + [64]9. visualization, deep dream, neural style, adversarial
            examples       
          + [65]13. image segmentation (up to 38:00) includes
            upconvolutions       
          + [66]6. deep learning    
          + [67]9. convolutional networks          
          + [68]image kernels explained visually - shows how convolutional
            filters (also known as image kernels) transform the image    
          + [69]convnetjs mnist demo - live visualization of a
            convolutional network right in the browser    
          + [70]conv nets: a modular perspective       
          + [71]understanding convolutions          
          + [72]understanding convolutional neural networks for nlp       
       convolutional networks are implemented in every major framework. it
       is usually easier to understand the code that is written using
       higher level libraries.
          + [73]theano: convolutional neural networks (lenet)       
          + [74]using lasagne for training deep neural networks    
          + [75]detecting diabetic retinopathy in eye images - a blog post
            by one of the best performers of diabetic retinopathy
            detection contest in kaggle. includes a good example of data
            augmentation.       
          + [76]face recognition for right whales using deep learning -
            the authors used different convnets for localization and
            classification. [77]code and models are available.       
          + [78]tensorflow: convolutional neural networks for image
            classification on cifar-10 dataset       
          + [79]implementing a id98 for text classification in tensorflow
                  
          + [80]deepdream implementation in tensorflow          
          + [81]92.45% on cifar-10 in torch - implements famous vggnet
            network with batch id172 layers in torch    
          + [82]training and investigating residual nets - residual
            networks perform very well on image classification tasks. two
            researchers from facebook and cornelltech implemented these
            networks in torch          
          + [83]convnets in practice - lots of practical tips on using
            convolutional networks including data augmentation, transfer
            learning, fast implementations of convolution operation       

recurrent neural networks
       recurrent networks ("id56s") are designed to work with sequences.
       usually they are used for sentence classification (e.g. sentiment
       analysis) and id103, but also for text generation and
       even image generation.
          + [84]the unreasonable effectiveness of recurrent neural
            networks - describes how id56s can generate text, math papers
            and c++ code    
          + hugo larochelle's course doesn't cover recurrent neural
            networks (although it covers many topics that id56s are used
            for). we suggest watching [85]recurrent neural nets and lstms
            by nando de freitas to fill the gap       
          + [86]10. recurrent neural networks, image captioning, lstm       
          + [87]13. soft attention (starting at 38:00)       
          + michael nielsen's book stops at convolutional networks. in the
            [88]other approaches to deep neural nets section there is just
            a brief review of simple recurrent networks and lstms.    
          + [89]10. sequence modeling: recurrent and recursive nets          
          + [90]recurrent neural networks from stanford's cs224d (2016) by
            richard socher       
          + [91]understanding id137       
       recurrent neural networks are also implemented in every modern
       framework.
          + [92]theano: recurrent neural networks with id27s          
          + [93]theano: id137 for id31          
          + [94]implementing a id56 with python, numpy and theano       
          + [95]lasagne implementation of karpathy's char-id56    
          + [96]combining id98 and id56 for spoken id46
            in lasagne    
          + [97]automatic id68 with lstm using lasagne    
          + [98]tensorflow: recurrent neural networks for language
            modeling       
          + [99]recurrent neural networks in tensorflow       
          + [100]understanding and implementing deepmind's draw model          
          + [101]lstm implementation explained       
          + [102]torch implementation of karpathy's char-id56          

autoencoders
       autoencoders are neural networks designed for unsupervised
       learning, i.e. when the data is not labeled. they can be used for
       dimension reduction, pretraining of other neural networks, for data
       generation etc. here we also include resources about an interesting
       hybrid of autoencoders and id114 called variational
       autoencoders, although their mathematical basis is not introduced
       until the next section.
          + [103]6. autoencoder       
          + [104]7.6. deep autoencoder       
          + [105]14. videos and unsupervised learning (from 32:29) - this
            video also touches an exciting topic of generative adversarial
            networks.       
          + [106]14. autoencoders          
          + [107]convnetjs denoising autoencoder demo    
          + [108]karol gregor on id5 and image
            generation       
       most autoencoders are pretty easy to implement. we suggest you try
       to implement one before looking at complete examples.
          + [109]theano: denoising autoencoders       
          + [110]diving into tensorflow with stacked autoencoders       
          + [111]variational autoencoder in tensorflow       
          + [112]training autoencoders on id163 using torch 7       
          + [113]building autoencoders in keras    

probabilistic id114
       probabilistic id114 (   pgms   ) form a separate subfield at
       the intersection of statistics and machine learning. there are many
       books and courses on pgms in general. here we present how these
       models are applied in the context of deep learning. hugo
       larochelle's course describes a few famous models, while the book
       deep learning devotes four chapters (16-19) to the theory and
       describes more than a dozen models in the last chapter. these
       topics require a lot of mathematics.
          + [114]3. id49          
          + [115]4. training crfs          
          + [116]5. restricted boltzman machine          
          + [117]7.7-7.9. id50          
          + [118]9.10. convolutional rbm          
          + [119]13. linear factor models - first steps towards
            probabilistic models          
          + [120]16. structured probabilistic models for deep learning          
          + [121]17. monte carlo methods          
          + [122]18. confronting the partition function          
          + [123]19. approximate id136          
          + [124]20. deep generative models - includes id82s
            (rbm, dbn, ...), id5, generative
            adversarial networks, autoregressive models etc.          
          + [125]generative models - a blog post on variational
            autoencoders, id3 and their
            improvements by openai.          
          + [126]the neural network zoo attempts to organize lots of
            architectures using a single scheme.       
       higher level frameworks (lasagne, keras) do not implement graphical
       models. but there is a lot of code for theano, tensorflow and
       torch.
          + [127]restricted id82s in theano          
          + [128]id50 in theano          
          + [129]generating large images from latent vectors - uses a
            combination of id5 and generative
            adversarial networks.          
          + [130]image completion with deep learning in tensorflow -
            another application of id3.          
          + [131]generating faces with torch - torch implementation of
            id3       

the state of the art
       deep learning is a very active area of scientific research. to
       follow the state of the art one has to read new papers and follow
       important conferences. usually every new idea is announced in a
       preprint paper on arxiv.org. then some of them are submitted to
       conferences and are peer reviewed. the best of them are presented
       in the conferences and are published in journals. if the authors do
       not release code for their models, many people attempt to implement
       them and put them on github. it takes a year or two before high
       quality blog posts, tutorials and videos appear on the web that
       properly explain the ideas and implementations.
          + [132]deep learning papers reading roadmap contains a long list
            of important papers.
          + [133]arxiv sanity preserver is a nice ui for browsing papers
            from arxiv.
          + [134]videolectures.net contains lots of videos on advanced
            topics.
          + [135]/r/machinelearning is a very active subreddit. all major
            new papers are discussed there.
       we are going to keep this guide up to date.
          + if you find broken links or any other problems, please report
            an [136]issue on github.
          + last updated on december 26, 2016

references

   visible links
   1. https://karpathy.github.io/2016/05/31/rl/
   2. http://www.deeplearningbook.org/
   3. http://www.deeplearningbook.org/contents/linear_algebra.html
   4. http://www.deeplearningbook.org/contents/prob.html
   5. http://www.deeplearningbook.org/contents/numerical.html
   6. https://cs231n.github.io/python-numpy-tutorial/
   7. http://www.scipy-lectures.org/
   8. https://www.youtube.com/playlist?list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh
   9. http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html
  10. http://cs231n.stanford.edu/
  11. https://www.youtube.com/playlist?list=plljy-ebtnft6eumxfyrinrs07mcwn5uia
  12. https://cs231n.github.io/
  13. http://neuralnetworksanddeeplearning.com/
  14. http://www.deeplearningbook.org/
  15. https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/
  16. http://www.r2d3.us/visual-intro-to-machine-learning-part-1/
  17. https://www.coursera.org/learn/machine-learning
  18. https://www.youtube.com/watch?v=g-pvxujd6qg&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia
  19. https://www.youtube.com/watch?v=haeos2tocj8&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&index=2
  20. https://www.youtube.com/watch?v=wjy57k9xx4s&index=3&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia
  21. http://www.deeplearningbook.org/contents/ml.html
  22. http://setosa.io/ev/principal-component-analysis/
  23. http://distill.pub/2016/misread-tsne/
  24. http://scikit-learn.org/stable/tutorial/basic/tutorial.html
  25. https://pythonprogramming.net/machine-learning-tutorial-python-introduction/
  26. https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/
  27. https://www.youtube.com/playlist?list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh
  28. https://www.youtube.com/watch?v=5adnqvslf50&list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh&index=7
  29. https://www.youtube.com/watch?v=gztvxoshzio&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&index=4
  30. https://www.youtube.com/watch?v=gutlrdbhhjm&index=5&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia
  31. http://neuralnetworksanddeeplearning.com/chap1.html
  32. http://neuralnetworksanddeeplearning.com/chap2.html
  33. http://neuralnetworksanddeeplearning.com/chap4.html
  34. http://www.deeplearningbook.org/contents/mlp.html
  35. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b
  36. https://colah.github.io/posts/2015-08-backprop/
  37. http://playground.tensorflow.org/
  38. https://cs231n.github.io/neural-networks-case-study/
  39. https://karpathy.github.io/neuralnets/
  40. http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/
  41. https://www.youtube.com/watch?v=jfkbyodyujw&list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh&index=14
  42. https://www.youtube.com/watch?v=uckpdam8cni&list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh&index=55
  43. https://www.youtube.com/watch?v=kar4lidi1mq&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&index=6
  44. http://neuralnetworksanddeeplearning.com/chap3.html
  45. http://neuralnetworksanddeeplearning.com/chap5.html
  46. http://www.deeplearningbook.org/contents/id173.html
  47. http://www.deeplearningbook.org/contents/optimization.html
  48. http://www.deeplearningbook.org/contents/guidelines.html
  49. http://cs.stanford.edu/people/karpathy/convnetjs/demo/trainers.html
  50. http://sebastianruder.com/optimizing-gradient-descent/
  51. https://colah.github.io/posts/2014-03-nn-manifolds-topology/
  52. http://deeplearning.net/software/theano/
  53. https://mila.umontreal.ca/en/
  54. http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/
  55. https://www.tensorflow.org/
  56. http://torch.ch/
  57. https://lasagne.readthedocs.io/en/latest/user/tutorial.html
  58. https://keras.io/
  59. https://www.youtube.com/watch?v=xgflbsl0lq4&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&index=12
  60. https://www.youtube.com/watch?v=rxkrca4bg1i&list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh&index=69
  61. https://www.youtube.com/watch?v=kar4lidi1mq&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&index=6
  62. https://www.youtube.com/watch?v=v8jdmkardfu&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&index=7
  63. https://www.youtube.com/watch?v=2xtx-gk3pqy&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&index=8
  64. https://www.youtube.com/watch?v=n--ysfuyyne&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&index=9
  65. https://www.youtube.com/watch?v=ufno-adc-k0&index=13&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia
  66. http://neuralnetworksanddeeplearning.com/chap6.html
  67. http://www.deeplearningbook.org/contents/convnets.html
  68. http://setosa.io/ev/image-kernels/
  69. http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html
  70. https://colah.github.io/posts/2014-07-conv-nets-modular/
  71. https://colah.github.io/posts/2014-07-understanding-convolutions/
  72. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
  73. http://deeplearning.net/tutorial/lenet.html
  74. https://luizgh.github.io/libraries/2015/12/08/getting-started-with-lasagne/
  75. https://jeffreydf.github.io/diabetic-retinopathy-detection/
  76. https://deepsense.io/deep-learning-right-whale-recognition-kaggle/
  77. https://www.dropbox.com/s/rohrc1btslxwxzr/deepsense-whales.zip?dl=0
  78. https://www.tensorflow.org/tutorials/deep_id98/
  79. http://www.wildml.com/2015/12/implementing-a-id98-for-text-classification-in-tensorflow/
  80. https://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb
  81. http://torch.ch/blog/2015/07/30/cifar.html
  82. http://torch.ch/blog/2016/02/04/resnets.html
  83. https://www.youtube.com/watch?v=ue4rjdi8yra&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&index=11
  84. https://karpathy.github.io/2015/05/21/id56-effectiveness/
  85. https://www.youtube.com/watch?v=56tylaqn4n8
  86. https://www.youtube.com/watch?v=co0a0qymfm8&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&index=10
  87. https://youtu.be/ufno-adc-k0?list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&t=2280
  88. http://neuralnetworksanddeeplearning.com/chap6.html#other_approaches_to_deep_neural_nets
  89. http://www.deeplearningbook.org/contents/id56.html
  90. https://www.youtube.com/watch?v=nwcjugug-0s&index=8&list=plmimxx8char9ig0zhsytqgsdhb9weegam
  91. https://colah.github.io/posts/2015-08-understanding-lstms/
  92. http://deeplearning.net/tutorial/id56slu.html
  93. http://deeplearning.net/tutorial/lstm.html
  94. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-id56-with-python-numpy-and-theano/
  95. https://github.com/lasagne/recipes/blob/master/examples/lstm_text_generation.py
  96. https://yerevann.github.io/2016/06/26/combining-id98-and-id56-for-spoken-language-identification/
  97. https://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/
  98. https://www.tensorflow.org/tutorials/recurrent/
  99. http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html
 100. http://blog.evjang.com/2016/06/understanding-and-implementing.html
 101. https://apaszke.github.io/lstm-explained.html
 102. https://github.com/jcjohnson/torch-id56
 103. https://www.youtube.com/watch?v=fzs3tml4nsc&t=2s&list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh&index=44
 104. https://www.youtube.com/watch?v=z5zym_wj37c&list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh&index=56
 105. https://youtu.be/i-i1kbushcc?list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&t=1949
 106. http://www.deeplearningbook.org/contents/autoencoders.html
 107. http://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html
 108. https://www.youtube.com/watch?v=p78qyjwh5sm&index=3&list=ple6wd9fr--efw8dtjaupotupcqmov53fu
 109. http://deeplearning.net/tutorial/da.html
 110. http://cmgreen.io/2016/01/04/tensorflow_deep_autoencoder.html
 111. https://jmetzen.github.io/2015-11-27/vae.html
 112. https://siavashk.github.io/2016/02/22/autoencoder-id163/
 113. https://blog.keras.io/building-autoencoders-in-keras.html
 114. https://www.youtube.com/watch?v=gf3isjkgpba&list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh&index=18
 115. https://www.youtube.com/watch?v=6dpgb60q1ts&list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh&index=28
 116. https://www.youtube.com/watch?v=p4vh_zmw-hq&list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh&index=36
 117. https://www.youtube.com/watch?v=vkb6awyxz5i&list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh&index=57
 118. https://www.youtube.com/watch?v=y0sisi_t6s8&list=pl6xpj9i5qxyecohn7tqghaj6naprnmubh&index=78
 119. http://www.deeplearningbook.org/contents/linear_factors.html
 120. http://www.deeplearningbook.org/contents/graphical_models.html
 121. http://www.deeplearningbook.org/contents/monte_carlo.html
 122. http://www.deeplearningbook.org/contents/partition.html
 123. http://www.deeplearningbook.org/contents/id136.html
 124. http://www.deeplearningbook.org/contents/generative_models.html
 125. https://openai.com/blog/generative-models/
 126. http://www.asimovinstitute.org/neural-network-zoo/
 127. http://deeplearning.net/tutorial/rbm.html
 128. http://deeplearning.net/tutorial/dbn.html
 129. http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/
 130. https://bamos.github.io/2016/08/09/deep-completion/
 131. http://torch.ch/blog/2015/11/13/gan.html
 132. https://github.com/songrotek/deep-learning-papers-reading-roadmap
 133. http://www.arxiv-sanity.com/
 134. http://videolectures.net/
 135. https://www.reddit.com/r/machinelearning/
 136. https://github.com/yerevann/a-guide-to-deep-learning/issues

   hidden links:
 138. https://yerevann.com/
