   [1]cs231n convolutional neural networks for visual recognition

   table of contents:
     * [2]generating some data
     * [3]training a softmax linear classifier
          + [4]initialize the parameters
          + [5]compute the class scores
          + [6]compute the loss
          + [7]computing the analytic gradient with id26
          + [8]performing a parameter update
          + [9]putting it all together: training a softmax classifier
     * [10]training a neural network
     * [11]summary

   in this section we   ll walk through a complete implementation of a toy
   neural network in 2 dimensions. we   ll first implement a simple linear
   classifier and then extend the code to a 2-layer neural network. as
   we   ll see, this extension is surprisingly simple and very few changes
   are necessary.

generating some data

   lets generate a classification dataset that is not easily linearly
   separable. our favorite example is the spiral dataset, which can be
   generated as follows:
n = 100 # number of points per class
d = 2 # dimensionality
k = 3 # number of classes
x = np.zeros((n*k,d)) # data matrix (each row = single example)
y = np.zeros(n*k, dtype='uint8') # class labels
for j in xrange(k):
  ix = range(n*j,n*(j+1))
  r = np.linspace(0.0,1,n) # radius
  t = np.linspace(j*4,(j+1)*4,n) + np.random.randn(n)*0.2 # theta
  x[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
  y[ix] = j
# lets visualize the data:
plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap=plt.cm.spectral)
plt.show()

   [spiral_raw.png]
   the toy spiral data consists of three classes (blue, red, yellow) that
   are not linearly separable.

   normally we would want to preprocess the dataset so that each feature
   has zero mean and unit standard deviation, but in this case the
   features are already in a nice range from -1 to 1, so we skip this
   step.

training a softmax linear classifier

initialize the parameters

   lets first train a softmax classifier on this classification dataset.
   as we saw in the previous sections, the softmax classifier has a linear
   score function and uses the cross-id178 loss. the parameters of the
   linear classifier consist of a weight matrix w and a bias vector b for
   each class. lets first initialize these parameters to be random
   numbers:
# initialize parameters randomly
w = 0.01 * np.random.randn(d,k)
b = np.zeros((1,k))

   recall that we d = 2 is the dimensionality and k = 3 is the number of
   classes.

compute the class scores

   since this is a linear classifier, we can compute all class scores very
   simply in parallel with a single id127:
# compute class scores for a linear classifier
scores = np.dot(x, w) + b

   in this example we have 300 2-d points, so after this multiplication
   the array scores will have size [300 x 3], where each row gives the
   class scores corresponding to the 3 classes (blue, red, yellow).

compute the loss

   the second key ingredient we need is a id168, which is a
   differentiable objective that quantifies our unhappiness with the
   computed class scores. intuitively, we want the correct class to have a
   higher score than the other classes. when this is the case, the loss
   should be low and otherwise the loss should be high. there are many
   ways to quantify this intuition, but in this example lets use the
   cross-id178 loss that is associated with the softmax classifier.
   recall that if \(f\) is the array of class scores for a single example
   (e.g. array of 3 numbers here), then the softmax classifier computes
   the loss for that example as:

   we can see that the softmax classifier interprets every element of
   \(f\) as holding the (unnormalized) log probabilities of the three
   classes. we exponentiate these to get (unnormalized) probabilities, and
   then normalize them to get probabilites. therefore, the expression
   inside the log is the normalized id203 of the correct class. note
   how this expression works: this quantity is always between 0 and 1.
   when the id203 of the correct class is very small (near 0), the
   loss will go towards (positive) infinity. conversely, when the correct
   class id203 goes towards 1, the loss will go towards zero because
   \(log(1) = 0\). hence, the expression for \(l_i\) is low when the
   correct class id203 is high, and it   s very high when it is low.

   recall also that the full softmax classifier loss is then defined as
   the average cross-id178 loss over the training examples and the
   id173:

   given the array of scores we   ve computed above, we can compute the
   loss. first, the way to obtain the probabilities is straight forward:
num_examples = x.shape[0]
# get unnormalized probabilities
exp_scores = np.exp(scores)
# normalize them for each example
probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=true)

   we now have an array probs of size [300 x 3], where each row now
   contains the class probabilities. in particular, since we   ve normalized
   them every row now sums to one. we can now query for the log
   probabilities assigned to the correct classes in each example:
correct_logprobs = -np.log(probs[range(num_examples),y])

   the array correct_logprobs is a 1d array of just the probabilities
   assigned to the correct classes for each example. the full loss is then
   the average of these log probabilities and the id173 loss:
# compute the loss: average cross-id178 loss and id173
data_loss = np.sum(correct_logprobs)/num_examples
reg_loss = 0.5*reg*np.sum(w*w)
loss = data_loss + reg_loss

   in this code, the id173 strength \(\lambda\) is stored inside
   the reg. the convenience factor of 0.5 multiplying the id173
   will become clear in a second. evaluating this in the beginning (with
   random parameters) might give us loss = 1.1, which is np.log(1.0/3),
   since with small initial random weights all probabilities assigned to
   all classes are about one third. we now want to make the loss as low as
   possible, with loss = 0 as the absolute lower bound. but the lower the
   loss is, the higher are the probabilities assigned to the correct
   classes for all examples.

computing the analytic gradient with id26

   we have a way of evaluating the loss, and now we have to minimize it.
   we   ll do so with id119. that is, we start with random
   parameters (as shown above), and evaluate the gradient of the loss
   function with respect to the parameters, so that we know how we should
   change the parameters to decrease the loss. lets introduce the
   intermediate variable \(p\), which is a vector of the (normalized)
   probabilities. the loss for one example is:

   we now wish to understand how the computed scores inside \(f\) should
   change to decrease the loss \(l_i\) that this example contributes to
   the full objective. in other words, we want to derive the gradient \(
   \partial l_i / \partial f_k \). the loss \(l_i\) is computed from
   \(p\), which in turn depends on \(f\). it   s a fun exercise to the
   reader to use the chain rule to derive the gradient, but it turns out
   to be extremely simple and interpretible in the end, after a lot of
   things cancel out:

   notice how elegant and simple this expression is. suppose the
   probabilities we computed were p = [0.2, 0.3, 0.5], and that the
   correct class was the middle one (with id203 0.3). according to
   this derivation the gradient on the scores would be df = [0.2, -0.7,
   0.5]. recalling what the interpretation of the gradient, we see that
   this result is highly intuitive: increasing the first or last element
   of the score vector f (the scores of the incorrect classes) leads to an
   increased loss (due to the positive signs +0.2 and +0.5) - and
   increasing the loss is bad, as expected. however, increasing the score
   of the correct class has negative influence on the loss. the gradient
   of -0.7 is telling us that increasing the correct class score would
   lead to a decrease of the loss \(l_i\), which makes sense.

   all of this boils down to the following code. recall that probs stores
   the probabilities of all classes (as rows) for each example. to get the
   gradient on the scores, which we call dscores, we proceed as follows:
dscores = probs
dscores[range(num_examples),y] -= 1
dscores /= num_examples

   lastly, we had that scores = np.dot(x, w) + b, so armed with the
   gradient on scores (stored in dscores), we can now backpropagate into w
   and b:
dw = np.dot(x.t, dscores)
db = np.sum(dscores, axis=0, keepdims=true)
dw += reg*w # don't forget the id173 gradient

   where we see that we have backpropped through the matrix multiply
   operation, and also added the contribution from the id173.
   note that the id173 gradient has the very simple form reg*w
   since we used the constant 0.5 for its loss contribution (i.e.
   \(\frac{d}{dw} ( \frac{1}{2} \lambda w^2) = \lambda w\). this is a
   common convenience trick that simplifies the gradient expression.

performing a parameter update

   now that we   ve evaluated the gradient we know how every parameter
   influences the id168. we will now perform a parameter update in
   the negative gradient direction to decrease the loss:
# perform a parameter update
w += -step_size * dw
b += -step_size * db

putting it all together: training a softmax classifier

   putting all of this together, here is the full code for training a
   softmax classifier with id119:
#train a linear classifier

# initialize parameters randomly
w = 0.01 * np.random.randn(d,k)
b = np.zeros((1,k))

# some hyperparameters
step_size = 1e-0
reg = 1e-3 # id173 strength

# id119 loop
num_examples = x.shape[0]
for i in xrange(200):

  # evaluate class scores, [n x k]
  scores = np.dot(x, w) + b

  # compute the class probabilities
  exp_scores = np.exp(scores)
  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=true) # [n x k]

  # compute the loss: average cross-id178 loss and id173
  correct_logprobs = -np.log(probs[range(num_examples),y])
  data_loss = np.sum(correct_logprobs)/num_examples
  reg_loss = 0.5*reg*np.sum(w*w)
  loss = data_loss + reg_loss
  if i % 10 == 0:
    print "iteration %d: loss %f" % (i, loss)

  # compute the gradient on scores
  dscores = probs
  dscores[range(num_examples),y] -= 1
  dscores /= num_examples

  # backpropate the gradient to the parameters (w,b)
  dw = np.dot(x.t, dscores)
  db = np.sum(dscores, axis=0, keepdims=true)

  dw += reg*w # id173 gradient

  # perform a parameter update
  w += -step_size * dw
  b += -step_size * db

   running this prints the output:
iteration 0: loss 1.096956
iteration 10: loss 0.917265
iteration 20: loss 0.851503
iteration 30: loss 0.822336
iteration 40: loss 0.807586
iteration 50: loss 0.799448
iteration 60: loss 0.794681
iteration 70: loss 0.791764
iteration 80: loss 0.789920
iteration 90: loss 0.788726
iteration 100: loss 0.787938
iteration 110: loss 0.787409
iteration 120: loss 0.787049
iteration 130: loss 0.786803
iteration 140: loss 0.786633
iteration 150: loss 0.786514
iteration 160: loss 0.786431
iteration 170: loss 0.786373
iteration 180: loss 0.786331
iteration 190: loss 0.786302

   we see that we   ve converged to something after about 190 iterations. we
   can evaluate the training set accuracy:
# evaluate training set accuracy
scores = np.dot(x, w) + b
predicted_class = np.argmax(scores, axis=1)
print 'training accuracy: %.2f' % (np.mean(predicted_class == y))

   this prints 49%. not very good at all, but also not surprising given
   that the dataset is constructed so it is not linearly separable. we can
   also plot the learned decision boundaries:
   [spiral_linear.png]
   linear classifier fails to learn the toy spiral dataset.

training a neural network

   clearly, a linear classifier is inadequate for this dataset and we
   would like to use a neural network. one additional hidden layer will
   suffice for this toy data. we will now need two sets of weights and
   biases (for the first and second layers):
# initialize parameters randomly
h = 100 # size of hidden layer
w = 0.01 * np.random.randn(d,h)
b = np.zeros((1,h))
w2 = 0.01 * np.random.randn(h,k)
b2 = np.zeros((1,k))

   the forward pass to compute scores now changes form:
# evaluate class scores with a 2-layer neural network
hidden_layer = np.maximum(0, np.dot(x, w) + b) # note, relu activation
scores = np.dot(hidden_layer, w2) + b2

   notice that the only change from before is one extra line of code,
   where we first compute the hidden layer representation and then the
   scores based on this hidden layer. crucially, we   ve also added a
   non-linearity, which in this case is simple relu that thresholds the
   activations on the hidden layer at zero.

   everything else remains the same. we compute the loss based on the
   scores exactly as before, and get the gradient for the scores dscores
   exactly as before. however, the way we backpropagate that gradient into
   the model parameters now changes form, of course. first lets
   backpropagate the second layer of the neural network. this looks
   identical to the code we had for the softmax classifier, except we   re
   replacing x (the raw data), with the variable hidden_layer):
# backpropate the gradient to the parameters
# first backprop into parameters w2 and b2
dw2 = np.dot(hidden_layer.t, dscores)
db2 = np.sum(dscores, axis=0, keepdims=true)

   however, unlike before we are not yet done, because hidden_layer is
   itself a function of other parameters and the data! we need to continue
   id26 through this variable. its gradient can be computed as:
dhidden = np.dot(dscores, w2.t)

   now we have the gradient on the outputs of the hidden layer. next, we
   have to backpropagate the relu non-linearity. this turns out to be easy
   because relu during the backward pass is effectively a switch. since
   \(r = max(0, x)\), we have that \(\frac{dr}{dx} = 1(x > 0) \). combined
   with the chain rule, we see that the relu unit lets the gradient pass
   through unchanged if its input was greater than 0, but kills it if its
   input was less than zero during the forward pass. hence, we can
   backpropagate the relu in place simply with:
# backprop the relu non-linearity
dhidden[hidden_layer <= 0] = 0

   and now we finally continue to the first layer weights and biases:
# finally into w,b
dw = np.dot(x.t, dhidden)
db = np.sum(dhidden, axis=0, keepdims=true)

   we   re done! we have the gradients dw,db,dw2,db2 and can perform the
   parameter update. everything else remains unchanged. the full code
   looks very similar:
# initialize parameters randomly
h = 100 # size of hidden layer
w = 0.01 * np.random.randn(d,h)
b = np.zeros((1,h))
w2 = 0.01 * np.random.randn(h,k)
b2 = np.zeros((1,k))

# some hyperparameters
step_size = 1e-0
reg = 1e-3 # id173 strength

# id119 loop
num_examples = x.shape[0]
for i in xrange(10000):

  # evaluate class scores, [n x k]
  hidden_layer = np.maximum(0, np.dot(x, w) + b) # note, relu activation
  scores = np.dot(hidden_layer, w2) + b2

  # compute the class probabilities
  exp_scores = np.exp(scores)
  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=true) # [n x k]

  # compute the loss: average cross-id178 loss and id173
  correct_logprobs = -np.log(probs[range(num_examples),y])
  data_loss = np.sum(correct_logprobs)/num_examples
  reg_loss = 0.5*reg*np.sum(w*w) + 0.5*reg*np.sum(w2*w2)
  loss = data_loss + reg_loss
  if i % 1000 == 0:
    print "iteration %d: loss %f" % (i, loss)

  # compute the gradient on scores
  dscores = probs
  dscores[range(num_examples),y] -= 1
  dscores /= num_examples

  # backpropate the gradient to the parameters
  # first backprop into parameters w2 and b2
  dw2 = np.dot(hidden_layer.t, dscores)
  db2 = np.sum(dscores, axis=0, keepdims=true)
  # next backprop into hidden layer
  dhidden = np.dot(dscores, w2.t)
  # backprop the relu non-linearity
  dhidden[hidden_layer <= 0] = 0
  # finally into w,b
  dw = np.dot(x.t, dhidden)
  db = np.sum(dhidden, axis=0, keepdims=true)

  # add id173 gradient contribution
  dw2 += reg * w2
  dw += reg * w

  # perform a parameter update
  w += -step_size * dw
  b += -step_size * db
  w2 += -step_size * dw2
  b2 += -step_size * db2

   this prints:
iteration 0: loss 1.098744
iteration 1000: loss 0.294946
iteration 2000: loss 0.259301
iteration 3000: loss 0.248310
iteration 4000: loss 0.246170
iteration 5000: loss 0.245649
iteration 6000: loss 0.245491
iteration 7000: loss 0.245400
iteration 8000: loss 0.245335
iteration 9000: loss 0.245292

   the training accuracy is now:
# evaluate training set accuracy
hidden_layer = np.maximum(0, np.dot(x, w) + b)
scores = np.dot(hidden_layer, w2) + b2
predicted_class = np.argmax(scores, axis=1)
print 'training accuracy: %.2f' % (np.mean(predicted_class == y))

   which prints 98%!. we can also visualize the decision boundaries:
   [spiral_net.png]
   neural network classifier crushes the spiral dataset.

summary

   we   ve worked with a toy 2d dataset and trained both a linear network
   and a 2-layer neural network. we saw that the change from a linear
   classifier to a neural network involves very few changes in the code.
   the score function changes its form (1 line of code difference), and
   the id26 changes its form (we have to perform one more round
   of backprop through the hidden layer to the first layer of the
   network).
     * you may want to look at this ipython notebook code [12]rendered as
       html.
     * or download the [13]ipynb file

     * [14]cs231n
     * [15]cs231n
     * [16]karpathy@cs.stanford.edu

references

   1. http://cs231n.github.io/
   2. http://cs231n.github.io/neural-networks-case-study/#data
   3. http://cs231n.github.io/neural-networks-case-study/#linear
   4. http://cs231n.github.io/neural-networks-case-study/#init
   5. http://cs231n.github.io/neural-networks-case-study/#scores
   6. http://cs231n.github.io/neural-networks-case-study/#loss
   7. http://cs231n.github.io/neural-networks-case-study/#grad
   8. http://cs231n.github.io/neural-networks-case-study/#update
   9. http://cs231n.github.io/neural-networks-case-study/#together
  10. http://cs231n.github.io/neural-networks-case-study/#net
  11. http://cs231n.github.io/neural-networks-case-study/#summary
  12. http://cs.stanford.edu/people/karpathy/cs231nfiles/minimal_net.html
  13. http://cs.stanford.edu/people/karpathy/cs231nfiles/minimal_net.ipynb
  14. https://github.com/cs231n
  15. https://twitter.com/cs231n
  16. mailto:karpathy@cs.stanford.edu
