                  neural probabilistic language model toolkit

   nplm is a toolkit for training and using feedforward neural language
   models (bengio, 2003). it is fast even for large vocabularies (100k or
   more): a model can be trained on a billion words of data in about a
   week, and can be queried in about 40   s, which is usable inside a
   decoder for machine translation.

   nplm is written by [1]ashish vaswani, with contributions from [2]david
   chiang and [3]victoria fossum. it is distributed under the mit
   open-source license.
     * latest stable version: [4]nplm-0.3.tar.gz
     * paper: decoding with large-scale neural language models improves
       translation. ashish vaswani, yinggong zhao, victoria fossum, and
       david chiang, 2013. in proceedings of emnlp. [5][pdf]

references

   1. http://www.isi.edu/~avaswani
   2. http://www.isi.edu/~chiang
   3. http://www.isi.edu/~vfossum
   4. https://www.isi.edu/natural-language/software/nplm/nplm-0.3.tar.gz
   5. https://www.isi.edu/natural-language/software/nplm/vaswani-emnlp13.pdf
