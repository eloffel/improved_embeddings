   #[1]the data science blog    feed [2]the data science blog    comments
   feed [3]the data science blog    a quick introduction to neural networks
   comments feed [4]introducing xda: r package for exploratory data
   analysis [5]an intuitive explanation of convolutional neural networks
   [6]alternate [7]alternate [8]the data science blog [9]wordpress.com

   [10]skip to content

[11]the data science blog

machine learning, deep learning, nlp, data science

   (button) menu
     * [12]about
     * [13]the data science blog
     * [14]reading material
     * [15]contact me

a quick introduction to neural networks

   posted on [16]august 9, 2016august 10, 2016 by [17]ujjwalkarn

   an id158 (ann) is a computational model that is
   inspired by the way biological neural networks in the human brain
   process information. id158s have generated a lot
   of excitement in machine learning research and industry, thanks to many
   breakthrough results in id103, id161 and text
   processing. in this blog post we will try to develop an understanding
   of a particular type of id158 called the multi
   layer id88.

a single neuron

   the basic unit of computation in a neural network is the neuron, often
   called a node or unit. it receives input from some other nodes, or from
   an external source and computes an output. each input has an associated
   weight (w), which is assigned on the basis of its relative importance
   to other inputs. the node applies a function f (defined below) to the
   weighted sum of its inputs as shown in figure 1 below:

   screen shot 2016-08-09 at 3.42.21 am.png

figure 1: a single neuron

   the above network takes numerical inputs x1 and x2 and has weights w1
   and w2 associated with those inputs. additionally, there is another
   input 1 with weight b (called the bias) associated with it. we will
   learn more details about role of the bias later.

   the output y from the neuron is computed as shown in the figure
   1. the function f is non-linear and is called the activation
   function. the purpose of the activation function is to introduce
   non-linearity into the output of a neuron. this is important because
   most real world data is non linear and we want neurons to learn
   these non linear representations.

   every activation function (or non-linearity) takes a single number and
   performs a certain fixed mathematical operation on it [2]. there are
   several id180 you may encounter in practice:
     * sigmoid: takes a real-valued input and squashes it to range between
       0 and 1

     (x) = 1 / (1 + exp(   x))
     * tanh: takes a real-valued input and squashes it to the range [-1,
       1]

   tanh(x) = 2  (2x)     1
     * relu: relu stands for rectified linear unit. it takes a real-valued
       input and thresholds it at zero (replaces negative values with
       zero)

   f(x) = max(0, x)

   the below figures [2]  show each of the above id180.

screen shot 2016-08-08 at 11.53.41 am figure 2: different activation
functions

   importance of bias: the main function of bias is to provide every node
   with a trainable constant value (in addition to the normal inputs that
   the node receives). see [18]this link to learn more about the role of
   bias in a neuron.

feedforward neural network

   the feedforward neural network was the first and simplest type of
   id158 devised [3]. it contains multiple neurons
   (nodes) arranged in layers. nodes from adjacent layers have connections
   or edges between them. all these connections have weights associated
   with them.

   an example of a feedforward neural network is shown in figure 3.

   screen shot 2016-08-09 at 4.19.50 am.png

figure 3: an example of feedforward neural network

   a feedforward neural network can consist of three types of nodes:
    1. input nodes     the input nodes provide information from the outside
       world to the network and are together referred to as the    input
       layer   . no computation is performed in any of the input nodes    
       they just pass on the information to the hidden nodes.
    2. hidden nodes     the hidden nodes have no direct connection with the
       outside world (hence the name    hidden   ). they perform computations
       and transfer information from the input nodes to the output nodes.
       a collection of hidden nodes forms a    hidden layer   . while a
       feedforward network will only have a single input layer and a
       single output layer, it can have zero or multiple hidden layers.
    3. output nodes     the output nodes are collectively referred to as the
          output layer    and are responsible for computations and
       transferring information from the network to the outside world.

   in a feedforward network, the information moves in only one direction    
   forward     from the input nodes, through the hidden nodes (if any) and
   to the output nodes. there are no cycles or loops in the network
   [3] (this property of feed forward networks is different from recurrent
   neural networks in which the connections between the nodes form a
   cycle).

   two examples of feedforward networks are given below:
    1. single layer id88     this is the simplest feedforward neural
       network [4] and does not contain any hidden layer. you can learn
       more about single layer id88s in [4], [5], [6], [7].
    2. multi layer id88     a multi layer id88 has one or more
       hidden layers. we will only discuss multi layer id88s below
       since they are more useful than single layer perceptons for
       practical applications today.

multi layer id88

   a multi layer id88 (mlp) contains one or more hidden layers
   (apart from one input and one output layer).  while a single layer
   id88 can only learn linear functions, a multi layer id88
   can also learn non     linear functions.

   figure 4 shows a multi layer id88 with a single hidden layer.
   note that all connections have weights associated with them, but only
   three weights (w0, w1, w2) are shown in the figure.

   input layer: the input layer has three nodes. the bias node has a value
   of 1. the other two nodes take x1 and x2 as external inputs (which are
   numerical values depending upon the input dataset). as discussed above,
   no computation is performed in the input layer, so the outputs from
   nodes in the input layer are 1, x1 and x2 respectively, which are fed
   into the hidden layer.

   hidden layer: the hidden layer also has three nodes with the bias node
   having an output of 1. the output of the other two nodes in the hidden
   layer depends on the outputs from the input layer (1, x1, x2) as well
   as the weights associated with the connections (edges). figure 4 shows
   the output calculation for one of the hidden nodes (highlighted).
   similarly, the output from other hidden node can be calculated.
   remember that f refers to the activation function. these outputs are
   then fed to the nodes in the output layer.

   ds.png

figure 4: a multi layer id88 having one hidden layer

   output layer: the output layer has two nodes which take inputs from the
   hidden layer and perform similar computations as shown for the
   highlighted hidden node. the values calculated (y1 and y2) as a result
   of these computations act as outputs of the multi layer id88.

   given a set of features x = (x1, x2,    ) and a target y, a multi layer
   id88 can learn the relationship between the features and the
   target, for either classification or regression.

   lets take an example to understand multi layer id88s better.
   suppose we have the following student-marks dataset:

   train.png

   the two input columns show the number of hours the student has studied
   and the mid term marks obtained by the student. the final result column
   can have two values 1 or 0 indicating whether the student passed in the
   final term. for example, we can see that if the student studied 35
   hours and had obtained 67 marks in the mid term, he / she ended up
   passing the final term.

   now, suppose, we want to predict whether a student studying 25 hours
   and having 70 marks in the mid term will pass the final term.

   test.png

   this is a binary classification problem where a multi layer id88
   can learn from the given examples (training data) and make an informed
   prediction given a new data point. we will see below how a multi layer
   id88 learns such relationships.

training our mlp: the back-propagation algorithm

   the process by which a multi layer id88 learns is called the
   id26 algorithm. i would recommend reading [19]this quora
   answer by hemanth kumar (quoted below) which explains id26
   clearly.

     backward propagation of errors, often abbreviated as backprop is one
     of the several ways in which an id158 (ann) can
     be trained. it is a supervised training scheme, which means, it
     learns from labeled training data (there is a supervisor, to guide
     its learning).

     to put in simple terms, backprop is like    learning from mistakes   .
     the supervisor corrects the ann whenever it makes mistakes.

     an ann consists of nodes in different layers; input layer,
     intermediate hidden layer(s) and the output layer. the connections
     between nodes of adjacent layers have    weights    associated with
     them. the goal of learning is to assign correct weights for these
     edges. given an input vector, these weights determine what the
     output vector is.

     in supervised learning, the training set is labeled. this means, for
     some given inputs, we know the desired/expected output (label).

     backprop algorithm:
     initially all the edge weights are randomly assigned. for every
     input in the training dataset, the ann is activated and its output
     is observed. this output is compared with the desired output that we
     already know, and the error is    propagated    back to the previous
     layer. this error is noted and the weights are    adjusted   
     accordingly. this process is repeated until the output error is
     below a predetermined threshold.

     once the above algorithm terminates, we have a    learned    ann which,
     we consider is ready to work with    new    inputs. this ann is said to
     have learned from several examples (labeled data) and from its
     mistakes (error propagation).

   now that we have an idea of how id26 works, lets come back
   to our student-marks dataset shown above.

   the multi layer id88 shown in figure 5 (adapted from sebastian
   raschka   s [20]excellent visual explanation of the id26
   algorithm) has two nodes in the input layer (apart from the bias node)
   which take the inputs    hours studied    and    mid term marks   . it also has
   a hidden layer with two nodes (apart from the bias node). the output
   layer has two nodes as well     the upper node outputs the id203 of
      pass    while the lower node outputs the id203 of    fail   .

   in classification tasks, we generally use a [21]softmax function as the
   activation function in the output layer of the multi layer id88
   to ensure that the outputs are probabilities and they add up to 1. the
   softmax function takes a vector of arbitrary real-valued scores and
   squashes it to a vector of values between zero and one that sum to one.
   so, in this case,

   id203 (pass) + id203 (fail) = 1

   step 1: forward propagation

   all weights in the network are randomly assigned. lets consider
   the hidden layer node marked v in figure 5 below. assume the weights of
   the connections from the inputs to that node are w1, w2 and w3 (as
   shown).

   the network then takes the first training example as input (we know
   that for inputs 35 and 67, the id203 of pass is 1).
     * input to the network = [35, 67]
     * desired output from the network (target) = [1, 0]

   then output v from the node in consideration can be calculated as below
   (f is an activation function such as sigmoid):

   v = f (1*w1 + 35*w2 + 67*w3)

   similarly, outputs from the other node in the hidden layer is also
   calculated. the outputs of the two nodes in the hidden layer act as
   inputs to the two nodes in the output layer. this enables us to
   calculate output probabilities from the two nodes in output layer.

   suppose the output probabilities from the two nodes in the output layer
   are 0.4 and 0.6 respectively (since the weights are randomly assigned,
   outputs will also be random). we can see that the calculated
   probabilities (0.4 and 0.6) are very far from the desired probabilities
   (1 and 0 respectively), hence the network in figure 5 is said to have
   an    incorrect output   .

   screen shot 2016-08-09 at 11.52.57 pm.png

figure 5: forward propagation step in a multi layer id88

   step 2: back propagation and weight updation

   we calculate the total error at the output nodes and propagate these
   errors back through the network using id26 to calculate the
   gradients. then we use an optimization method such as id119
   to    adjust    all weights in the network with an aim of reducing the
   error at the output layer. this is shown in the figure 6 below (ignore
   the mathematical equations in the figure for now).

   suppose that the new weights associated with the node in consideration
   are w4, w5 and w6 (after id26 and adjusting weights).

   screen shot 2016-08-09 at 11.53.06 pm.png

figure 6: backward propagation and weight updation step in a multi layer
id88

   if we now input the same example to the network again, the network
   should perform better than before since the weights have now been
   adjusted to minimize the error in prediction. as shown in figure 7, the
   errors at the output nodes now reduce to [0.2, -0.2] as compared to
   [0.6, -0.4] earlier. this means that our network has learnt to
   correctly classify our first training example.

   screen shot 2016-08-09 at 11.53.15 pm.png

figure 7: the mlp network now performs better on the same input

   we repeat this process with all other training examples in our
   dataset. then, our network is said to have learnt those examples.

   if we now want to predict whether a student studying 25 hours and
   having 70 marks in the mid term will pass the final term, we go through
   the forward propagation step and find the output probabilities for pass
   and fail.

   i have avoided mathematical equations and explanation of concepts such
   as    id119    here and have rather tried to develop an
   intuition for the algorithm. for a more mathematically involved
   discussion of the id26 algorithm, refer to [22]this link.

3d visualization of a multi layer id88

   adam harley has created a [23]3d visualization of a multi layer
   id88 which has already been trained (using id26) on
   the mnist database of handwritten digits.

   the network takes 784 numeric pixel values as inputs from a 28 x 28
   image of a handwritten digit (it has 784 nodes in the input layer
   corresponding to pixels). the network has 300 nodes in the first hidden
   layer, 100 nodes in the second hidden layer, and 10 nodes in the output
   layer (corresponding to the 10 digits) [15].

   although the network described here is much larger (uses more hidden
   layers and nodes) compared to the one we discussed in the previous
   section, all computations in the forward propagation step and
   id26 step are done in the same way (at each node) as
   discussed before.

   figure 8 shows the network when the input is the digit    5   .

   screen shot 2016-08-09 at 5.45.34 pm.png

figure 8: visualizing the network for an input of    5   

   a node which has a higher output value than others is represented by a
   brighter color. in the input layer, the bright nodes are those which
   receive higher numerical pixel values as input. notice how in the
   output layer, the only bright node corresponds to the digit 5 (it has
   an output id203 of 1, which is higher than the other nine nodes
   which have an output id203 of 0). this indicates that the mlp has
   correctly classified the input digit. i highly recommend playing around
   with this visualization and observing connections between nodes of
   different layers.

deep neural networks

    1. [24]what is the difference between deep learning and usual machine
       learning?
    2. [25]what is the difference between a neural network and a deep
       neural network?
    3. [26]how is deep learning different from multilayer id88?

conclusion

   i have skipped important details of some of the concepts discussed in
   this post to facilitate understanding. i would recommend going through
   [27]part1, [28]part2, [29]part3 and [30]case study from stanford   s
   neural network tutorial for a thorough understanding of multi layer
   id88s.

   let me know in the comments below if you have any questions or
   suggestions!

references

    1. [31]artificial neuron models
    2. [32]neural networks part 1: setting up the architecture (stanford
       id98 tutorial)
    3. [33]wikipedia article on feed forward neural network
    4. [34]wikipedia article on id88
    5. [35]single-layer neural networks (id88s)
    6. [36]single layer id88s
    7. [37]weighted networks     the id88
    8. [38]neural network models (supervised) (scikit learn documentation)
    9. [39]what does the hidden layer in a neural network compute?
   10. [40]how to choose the number of hidden layers and nodes in a
       feedforward neural network?
   11. [41]crash introduction to id158s
   12. [42]why the bias is necessary in ann? should we have separate bias
       for each layer?
   13. [43]basic neural network tutorial     theory
   14. [44]neural networks demystified (video series): part 1, welch labs
       @ mlconf sf
   15. a. w. harley,    an interactive node-link visualization of
       convolutional neural networks,    in isvc, pages 867-877, 2015
       ([45]link)

share this:

     * [46]tweet
     *
     *
     *

       iframe:
       [47]https://www.reddit.com/static/button/button1.html?newwindow=tru
       e&width=120&url=https%3a%2f%2fujjwalkarn.me%2f2016%2f08%2f09%2fquic
       k-intro-neural-networks%2f&title=a%20quick%20introduction%20to%20ne
       ural%20networks

     *

related

   posted in [48]deep learning

post navigation

   [49]introducing xda: r package for exploratory data analysis
   [50]an intuitive explanation of convolutional neural networks
     __________________________________________________________________

33 thoughts on    a quick introduction to neural networks   

    1.
   [51]snendroid says:
       [52]august 13, 2016 at 6:02 am
       that was an amazing article!
       [53]likelike
       [54]reply
    2.
   scott says:
       [55]august 25, 2016 at 5:47 am
       this is a great post. the main question i have is what is gained by
       adding additional hidden layers? does it reduce error? is there
       diminishing returns by adding additional hidden layers into the
       network? how should someone new to neural networks think about the
       benefits of additional hidden layers vs. the additional cpu cycles
       and memory resource costs of a bigger neural network?
       [56]likelike
       [57]reply
    3.
   [58]sivathanu k (@sivathanu_k) says:
       [59]august 31, 2016 at 4:16 pm
       thanks for the article..!!
       [60]likelike
       [61]reply
    4.
   minwei shen says:
       [62]october 26, 2016 at 1:42 am
       this is exactly what    introduction to neural network    should be!
       thanks for this great article!
       [63]likelike
       [64]reply
    5.
   muneera hmdi says:
       [65]november 29, 2016 at 2:30 am
       thanks a lot for this amazing article. honestly, i learned many
       things from it. kindly, can u provide like this artificial about
       the id98? thank again.
       [66]likelike
       [67]reply
         1.
        jxf0623 says:
            [68]june 29, 2018 at 6:07 pm
            [69]https://ujjwalkarn.me/2016/08/11/intuitive-explanation-con
            vnets/
            here is it.
            [70]likelike
            [71]reply
    6.
   ke xu says:
       [72]january 8, 2017 at 9:09 am
       thanks very much ! after reading several posts and watching couple
       of videos, i eventually found the most gentle introduction to id98 !
       [73]likelike
       [74]reply
    7.
   german says:
       [75]february 24, 2017 at 10:43 pm
       thank you, awesome post!
       [76]likelike
       [77]reply
    8.
   barry says:
       [78]march 3, 2017 at 7:37 am
       good job! great explanation for beginners!
       [79]likelike
       [80]reply
    9.
   [81]nnitsew says:
       [82]april 1, 2017 at 11:53 pm
       thank you for this good article.
       [83]likelike
       [84]reply
   10.
   jupi says:
       [85]may 12, 2017 at 9:44 am
       very helpful, finally i understood, thank you!
       [86]likelike
       [87]reply
   11.
   jasaz says:
       [88]may 15, 2017 at 12:09 am
       explained in very simple way. loved it. do keep publishing more
       plz.
       [89]likelike
       [90]reply
   12.
   [91]nobiel ho says:
       [92]july 2, 2017 at 8:51 pm
       great article and very helpful. thank you!
       [93]likelike
       [94]reply
   13.
   ralph m. says:
       [95]july 18, 2017 at 10:19 pm
       great intro on neural networks. i just finished a course on experfy
       on machine learning     definitely recommend it to anyone who wants
       to learn more!
       [96]https://www.experfy.com/training/courses/machine-learning-found
       ations-supervised-learning
       [97]likelike
       [98]reply
   14.
   aa says:
       [99]august 2, 2017 at 6:57 am
       i think there is a mistake in figure 5, the error of the second
       output node should be -0.6, shouldn   t it?
       [100]likelike
       [101]reply
         1.
        [102]ujjwalkarn says:
            [103]august 3, 2017 at 3:10 pm
            yes, will fix it, thanks for flagging!
            [104]likelike
            [105]reply
   15.
   andy says:
       [106]september 14, 2017 at 9:11 am
       fantastic article     this one explained from the ground up. thank
       you!
       [107]likelike
       [108]reply
   16.
   [109]johnburner91 says:
       [110]october 11, 2017 at 7:58 am
       thumbs up man. you   er a blessing
       [111]likelike
       [112]reply
   17.
   sam says:
       [113]october 17, 2017 at 8:28 pm
       great article, very helpful to me. this is a real    introduction   
       compares to other information on internet.
       [114]likelike
       [115]reply
   18.
   [116]mjbolanos9 says:
       [117]november 19, 2017 at 11:40 pm
       easy to follow explanation.. thanks!
       [118]likelike
       [119]reply
   19.
   [120]mj says:
       [121]november 20, 2017 at 12:16 am
       detailed article and easy to follow.. thank you!
       [122]likelike
       [123]reply
   20.
   amy says:
       [124]november 25, 2017 at 12:59 pm
       this is a fantastic article on ann. my one question after reading
       this was    why multiple neurons in the hidden layer    and    why
       multiple hidden layers.    this began to answer my question:
       [125]https://datascience.stackexchange.com/questions/14028/what-is-
       the-purpose-of-multiple-neurons-in-a-hidden-layer/14030 but i have
       a lot more learning to do!
       [126]likelike
       [127]reply
   21.
   [128]abu noman says:
       [129]january 3, 2018 at 5:48 pm
       great article! it   s easy to catch-up. keep writing   
       [130]likelike
       [131]reply
   22.
   kelly says:
       [132]february 17, 2018 at 10:33 pm
          while a single layer id88 can only learn linear functions       
       can   t there be an activation function such as tanh, therefore it   s
       learning a non-linear function? the wikipedia article on
       id88s says,    single layer id88s are only capable of
       learning *linearly separable patterns*    (emphasis added). i   m new
       to this, but    can only learn linear functions    seems inaccurate    
       what do you think?
       [133]likelike
       [134]reply
   23.
   asha says:
       [135]march 7, 2018 at 2:13 am
       well explained. thanks!
       [136]likelike
       [137]reply
   24.
   [138]babita koundal says:
       [139]march 31, 2018 at 3:16 pm
       a nice explained article   
       [140]likelike
       [141]reply
   25.
   charles fu says:
       [142]may 7, 2018 at 2:42 pm
       a nice writing. figure 5 has a typo: error = (0-0.6) = (-0.4)
       should be (-0.6)
       [143]likelike
       [144]reply
   26.
   [145]andrew bannerman says:
       [146]may 16, 2018 at 9:08 am
       great post     
       [147]likelike
       [148]reply
   27.
   cedric says:
       [149]june 20, 2018 at 1:24 am
       this was a great post to explain the very basics to those that are
       new to neural networks. thanks.
       [150]likelike
       [151]reply
   28.
   than k ful says:
       [152]june 20, 2018 at 3:42 am
       thank you! this is a great intro
       [153]likelike
       [154]reply
   29.
   [155]abhishek subramanian says:
       [156]september 7, 2018 at 6:05 pm
       that is a very nice introduction to neural networks. great job
       [157]likelike
       [158]reply
   30.
   [159]ericehlert says:
       [160]january 22, 2019 at 7:12 am
       every neuron takes a single number and performs a fixed activation
       function on it, this i understand. my question is how can this hold
       true when the input to a neuron on the input layer representing a
       nominal or categorical feature is not in the form of a single
       number, but instead a vector?
       [161]likelike
       [162]reply
         1.
        anonymous says:
            [163]march 24, 2019 at 2:45 pm
            convnets deals with vector representation of data like images,
            audio.
            [164]likelike
            [165]reply

leave a reply [166]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [167]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [168]log out /
   [169]change )
   google photo

   you are commenting using your google account. ( [170]log out /
   [171]change )
   twitter picture

   you are commenting using your twitter account. ( [172]log out /
   [173]change )
   facebook photo

   you are commenting using your facebook account. ( [174]log out /
   [175]change )
   [176]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

connect

     * [177]view thedatascienceblog   s profile on facebook
     * [178]view ujwlkarn   s profile on twitter
     * [179]view ujjwalkarn   s profile on linkedin
     * [180]view ujjwalkarn   s profile on github

categories

     * [181]data science
     * [182]deep learning
     * [183]machine learning
     * [184]python
     * [185]r language

recent posts

     * [186]an intuitive explanation of convolutional neural networks
     * [187]a quick introduction to neural networks
     * [188]introducing xda: r package for exploratory data analysis
     * [189]curated list of r tutorials for data science
     * [190]common operations on pandas dataframe

follow me on twitter

   [191]my tweets

[192]like facebook page for updates

     [193]like facebook page for updates

follow blog via email

   join 531 other followers

   ____________________

   (button) subscribe

blog view count

     * 807,375 hits

search this blog

   search for: ____________________ search
   [194]create a website or blog at wordpress.com


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [195]cancel reblog post

references

   visible links
   1. https://ujjwalkarn.me/feed/
   2. https://ujjwalkarn.me/comments/feed/
   3. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/feed/
   4. https://ujjwalkarn.me/2016/06/17/introducing-xda-r-package-for-exploratory-data-analysis/
   5. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/&for=wpcom-auto-discovery
   8. https://ujjwalkarn.me/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#content
  11. https://ujjwalkarn.me/
  12. https://ujjwalkarn.me/
  13. https://ujjwalkarn.me/blog/
  14. https://ujjwalkarn.me/blogroll/
  15. https://ujjwalkarn.me/contact-me/
  16. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
  17. https://ujjwalkarn.me/author/ujwlkarn/
  18. http://stackoverflow.com/q/2480650/3297280
  19. https://www.quora.com/how-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/hemanth-kumar-mantri
  20. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-id26.md
  21. http://cs231n.github.io/linear-classify/#softmax
  22. http://home.agh.edu.pl/~vlsi/ai/backp_t_en/backprop.html
  23. http://scs.ryerson.ca/~aharley/vis/fc/
  24. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md
  25. http://stats.stackexchange.com/questions/182734/what-is-the-difference-between-a-neural-network-and-a-deep-neural-network?rq=1
  26. https://www.quora.com/how-is-deep-learning-different-from-multilayer-id88
  27. http://cs231n.github.io/neural-networks-1/
  28. http://cs231n.github.io/neural-networks-2/
  29. http://cs231n.github.io/neural-networks-3/
  30. http://cs231n.github.io/neural-networks-case-study/
  31. https://www.willamette.edu/~gorr/classes/cs449/ann-overview.html
  32. http://cs231n.github.io/neural-networks-1/
  33. https://en.wikipedia.org/wiki/feedforward_neural_network
  34. https://en.wikipedia.org/wiki/id88
  35. http://computing.dcu.ie/~humphrys/notes/neural/single.neural.html
  36. http://www.cs.stir.ac.uk/courses/itnp4b/lectures/kms/2-id88s.pdf
  37. http://page.mi.fu-berlin.de/rojas/neural/chapter/k3.pdf
  38. http://scikit-learn.org/dev/modules/neural_networks_supervised.html
  39. http://stats.stackexchange.com/a/63163/53914
  40. http://stats.stackexchange.com/a/1097/53914
  41. http://ulcar.uml.edu/~iag/cs/intro-to-ann.html
  42. http://stackoverflow.com/questions/7175099/why-the-bias-is-necessary-in-ann-should-we-have-separate-bias-for-each-layer
  43. https://takinginitiative.wordpress.com/2008/04/03/basic-neural-network-tutorial-theory/
  44. https://www.youtube.com/watch?v=5mxp9uuksmc
  45. http://scs.ryerson.ca/~aharley/vis/harley_vis_isvc15.pdf
  46. https://twitter.com/share
  47. https://www.reddit.com/static/button/button1.html?newwindow=true&width=120&url=https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/&title=a quick introduction to neural networks
  48. https://ujjwalkarn.me/category/deep-learning/
  49. https://ujjwalkarn.me/2016/06/17/introducing-xda-r-package-for-exploratory-data-analysis/
  50. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
  51. http://snendroid.wordpress.com/
  52. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-71
  53. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=71&_wpnonce=5ea3f67b40
  54. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=71#respond
  55. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-90
  56. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=90&_wpnonce=fd5ef7abc8
  57. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=90#respond
  58. http://twitter.com/sivathanu_k
  59. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-103
  60. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=103&_wpnonce=8d5528796f
  61. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=103#respond
  62. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-148
  63. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=148&_wpnonce=8c74bcc0d8
  64. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=148#respond
  65. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-198
  66. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=198&_wpnonce=4b93837a0c
  67. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=198#respond
  68. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-4517
  69. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
  70. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=4517&_wpnonce=614c9cd183
  71. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=4517#respond
  72. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-294
  73. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=294&_wpnonce=c32f22428c
  74. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=294#respond
  75. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-456
  76. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=456&_wpnonce=ab755b1f7b
  77. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=456#respond
  78. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-491
  79. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=491&_wpnonce=8ad424d2ee
  80. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=491#respond
  81. http://nnitsew.wordpress.com/
  82. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-661
  83. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=661&_wpnonce=b254c4158b
  84. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=661#respond
  85. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-939
  86. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=939&_wpnonce=f7f6c4c787
  87. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=939#respond
  88. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-949
  89. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=949&_wpnonce=e76817e904
  90. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=949#respond
  91. http://nobiel.tk/
  92. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-1194
  93. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=1194&_wpnonce=c7c76a4546
  94. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=1194#respond
  95. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-1273
  96. https://www.experfy.com/training/courses/machine-learning-foundations-supervised-learning
  97. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=1273&_wpnonce=0c0fa399f6
  98. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=1273#respond
  99. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-1359
 100. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=1359&_wpnonce=024091eb0d
 101. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=1359#respond
 102. http://ujjwalkarn.me/
 103. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-1373
 104. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=1373&_wpnonce=171bbe3eee
 105. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=1373#respond
 106. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-1707
 107. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=1707&_wpnonce=0042df62a4
 108. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=1707#respond
 109. http://gravatar.com/johnburner91
 110. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-1934
 111. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=1934&_wpnonce=32217912d5
 112. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=1934#respond
 113. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-1997
 114. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=1997&_wpnonce=2a35af40ea
 115. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=1997#respond
 116. http://richfrugal.wordpress.com/
 117. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-2288
 118. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=2288&_wpnonce=ca0f1c7623
 119. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=2288#respond
 120. http://www.insightsbot.com/
 121. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-2290
 122. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=2290&_wpnonce=ff6bd1349e
 123. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=2290#respond
 124. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-2382
 125. https://datascience.stackexchange.com/questions/14028/what-is-the-purpose-of-multiple-neurons-in-a-hidden-layer/14030
 126. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=2382&_wpnonce=3e664f5454
 127. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=2382#respond
 128. http://nomantech.wordpress.com/
 129. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-2828
 130. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=2828&_wpnonce=e99f49bee2
 131. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=2828#respond
 132. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-3171
 133. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=3171&_wpnonce=79f66f6859
 134. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=3171#respond
 135. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-3411
 136. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=3411&_wpnonce=7198f60638
 137. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=3411#respond
 138. https://www.webtunix.com/
 139. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-3651
 140. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=3651&_wpnonce=c1cbcd0c41
 141. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=3651#respond
 142. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-3934
 143. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=3934&_wpnonce=e8a6034189
 144. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=3934#respond
 145. http://flare9xblog.wordpress.com/
 146. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-4021
 147. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=4021&_wpnonce=82b08c49d7
 148. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=4021#respond
 149. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-4406
 150. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=4406&_wpnonce=28dbb1e793
 151. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=4406#respond
 152. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-4407
 153. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=4407&_wpnonce=1c63504206
 154. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=4407#respond
 155. http://www.soleconceptlab.com/
 156. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-5374
 157. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=5374&_wpnonce=f351b6959e
 158. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=5374#respond
 159. http://gravatar.com/tanyamexico
 160. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-7575
 161. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=7575&_wpnonce=714e1a850a
 162. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=7575#respond
 163. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-18249
 164. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?like_comment=18249&_wpnonce=994b19b218
 165. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/?replytocom=18249#respond
 166. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#respond
 167. https://gravatar.com/site/signup/
 168. javascript:highlandercomments.doexternallogout( 'wordpress' );
 169. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
 170. javascript:highlandercomments.doexternallogout( 'googleplus' );
 171. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
 172. javascript:highlandercomments.doexternallogout( 'twitter' );
 173. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
 174. javascript:highlandercomments.doexternallogout( 'facebook' );
 175. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
 176. javascript:highlandercomments.cancelexternalwindow();
 177. https://www.facebook.com/thedatascienceblog/
 178. https://twitter.com/ujwlkarn/
 179. https://www.linkedin.com/in/ujjwalkarn/
 180. https://github.com/ujjwalkarn/
 181. https://ujjwalkarn.me/category/data-science/
 182. https://ujjwalkarn.me/category/deep-learning/
 183. https://ujjwalkarn.me/category/machine-learning/
 184. https://ujjwalkarn.me/category/python/
 185. https://ujjwalkarn.me/category/r-language/
 186. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
 187. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
 188. https://ujjwalkarn.me/2016/06/17/introducing-xda-r-package-for-exploratory-data-analysis/
 189. https://ujjwalkarn.me/2016/06/04/curated-list-of-r-tutorials-for-data-science/
 190. https://ujjwalkarn.me/2016/05/30/common-operations-on-pandas-dataframe/
 191. https://twitter.com/ujwlkarn
 192. https://www.facebook.com/thedatascienceblog/
 193. https://www.facebook.com/thedatascienceblog/
 194. https://wordpress.com/?ref=footer_custom_svg
 195. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/

   hidden links:
 197. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
 198. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-form-guest
 199. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-form-load-service:wordpress.com
 200. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-form-load-service:twitter
 201. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/#comment-form-load-service:facebook
