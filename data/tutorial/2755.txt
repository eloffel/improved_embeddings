   #[1]github [2]recent commits to wvec:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]16
     * [35]star [36]67
     * [37]fork [38]15

[39]ai-ku/[40]wvec

   [41]code [42]issues 1 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   word vectors
     * [47]11 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors
     * [51]mit

    1. [52]perl 78.4%
    2. [53]makefile 16.1%
    3. [54]python 4.1%
    4. [55]c++ 1.4%

   (button) perl makefile python c++
   branch: master (button) new pull request
   [56]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/a
   [57]download zip

downloading...

   want to be notified of new releases in ai-ku/wvec?
   [58]sign in [59]sign up

launching github desktop...

   if nothing happens, [60]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [61]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [62]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [63]download the github extension for visual studio
   and try again.

   (button) go back
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [64]permalink
   type      name      latest commit message commit time
        failed to load latest commit information.
        [65]bin
        [66]data
        [67]run
        [68]src
        [69].gitignore
        [70]license
        [71]readme.md

readme.md

scode id27s using substitute words##

   this repository provides a tool to induce word vectors using substitute
   words. using this repository you can:
     * generate word-type embeddings described in [1,3,4]
     * generate word-token embeddings described in [2,5]

   [1]. learning syntactic categories using paradigmatic representations
   of word context, in proceedings of the 2012 conference on empirical
   methods in natural language processing (emnlp-conll 2012), jeju, korea,
   july. association for computational linguistics, [72]paper,
   presentation & code, [73]bib.

   [2]. word context and token representations from paradigmatic relations
   and their application to part-of-speech induction, [74]paper &
   presentation

   [3]. the ai-ku system at the spmrl 2013 shared task : unsupervised
   features for id33, in proceedings of the fourth workshop
   on statistical parsing of morphologically-rich languages, pp 78--85,
   seattle, washington, usa, october. association for computational
   linguistics, [75]paper, [76]id27s, [77]bib

   [4] substitute based scode id27s in supervised nlp tasks,
   [78]word vectors for 7 languages, [79]bib

other word vectors

   here is a list of other word vectors :
     * turian, [80]word representations: a simple and general method for
       semi-supervised learning
     * dhillon, [81]multi-view learning of id27s via cca
     * mikolov, [82]recurrent neural network based language model
     * mikolov, [83]efficient estimation of word representations in vector
       space
     * huang, [84]improving word representations via global context and
       multiple word prototypes
     * stratos, [85]a spectral algorithm for learning class-based id165
       models of natural language
     * yogatama, [86]learning word representations with hierarchical
       sparse coding
     * faruqui, [87]improving vector space word representations using
       multilingual correlation
     * lebret, [88]id27s through hellinger pca

semi-supervised word vectors

   below is a list of word vectors using dependency pairs for inducing
   representations.
     * murphy, [89]learning effective and interpretable semantic models
       using non-negative sparse embedding
     * levy, [90]dependency-based id27s
     * bansal, [91]tailoring continuous word representations for
       id33

0 - setting-up the environment

   after cloning the repository, the first thing to do is initializing the
   binary directory. simlpy go to /run directory and run this command:
 make bin

   this will clones some repositories needed for a successfull run and
   install them. it will also copy the script files unde /bin directory.

   then, in order to generate word substitutes you need a language
   model(lm) in srilm format. install [92]srilm and set the srilm_path
   variable in the makefile.

   a one-liner procuding word vectors is as follows, just change the ones
   in the capital letters accordingly:
zcat ../data/your_corpus.tok.gz | fastsubs-omp -n 100  -p 1.0 your_lm.lm.gz | gr
ep -v '^</s>' | wordsub-n -s 1 -n 100  | scode-online -v2 -d number_of_dimension
 -s 1  | perl -ne 'print if s/^0://' | cut -f1,3- | tr '\t' ' ' | gzip > your_wv
ec.gz

   this target is at the end of the run/makefile . but, one may need a
   detailed description about what's going on. let's move on.

1 - generate type & token vectors - a shortcut:

   this section here to generate id27s embeddings right away.
   however, if you want to know how they are generated please skip this
   section. we provide a test file named mini-{train|test}.tok.gz so that
   you can test your setup.

   use an lm corpus(e.g. wikipedia) and a target corpus(e.g conll task) to
   generate 25-dimension id27s:
make -n your-target-corpus.unk-type-25.gz lmfile=your-lm-corpus dim=25

   for word-token vectors, you can use 4 different methods,
make your-target-corpus.xsubx.gz lmfile=your-lm-corpus dim=25 x1=your-target-cor
pus.unk-type-25.gz x2=your-target-corpus.unk-type-25.gz

make your-target-corpus.xplusx.gz lmfile=your-lm-corpus dim=25 x1=your-target-co
rpus.unk-type-25.gz x2=your-target-corpus.unk-type-25.gz

make your-target-corpus.xplusy.gz lmfile=your-lm-corpus dim=25 x1=your-target-co
rpus.unk-type-25.gz x2=your-target-corpus.unk-type-25.gz

make your-target-corpus.knn.xy.gz lmfile=your-lm-corpus dim=25 x1=your-target-co
rpus.unk-type-25.gz x2=your-target-corpus.unk-type-25.gz

   or you can combine your favorite id27s:
make your-target-corpus.xmixx.gz lmfile=your-lm-corpus dim=25 x1=your-embeddings
1.gz x2=your-embeddings2.gz

2 - how to generate word-type embeddings?

   first train an lm using a large corpus. tokenize your corpus into
   your-lm-corpus.tok and place it under /data and gzip tokenized file
   into your-lm-corpus.tok.gz.

   then run:
make your-lm-corpus.lm.gz

   you may want to change lm related variables in the makefile such as
   lm_discount, lm_ngram etc.

   then generate substitute words. first put your substitute(target)
   corpus under /data in a similar way you did to lm corpus,
   your-sub-corpus.tok.gz. your substitute corpus and lm corpus may
   differ. but use the same id121. then, you should run:
make your-sub-corpus.sub.gz lm=your-lm-corpus

   again you may want to change word substitute options like fs_nsub,
   fs_psub. note that, fastsubs-omp can be run in parallel, thus , you can
   set number of threads with omp_num_threads variable. we observe no gain
   after 24 core.

   now you can generate <word,substitute word> pairs using substitute
   distributions. you can change the number of substitutes and random seed
   by changing corresponding variables(i.e wordsub,seed). just run:
 make your-sub-corpus.pairs.gz

   if you want to generate an embedding for unknown words (probably you
   do) with an unknown tag *unknown*:
 make your-sub-corpus.unk-pairs.gz

   now you can generate word-type (one embedding per word) embeddings.
   let's say you want 25-dimension id27s with unknown word tag.
   run:
make your-sub-corpus.unk-type-25.gz dim=25

   or you can generate for all words (without unknown tag):
make your-sub-corpus.type-25.gz dim=25

   it runs scode, then extracts word-type embeddings. note that, you can
   change a variety of parameters of scode.

   if you only interested in word-type embeddings you are good to go.

3 - how to generate word-token (context-dependent) embeddings?

   after generating word-type embeddings, you can generate context
   dependent id27s in a couple of different ways.

enis sert's knn based vectors(2):

   first find the k-nearest-neighbors(k=128).
make your-sub-corpus.knn128.gz

   as always there are variables that you can change here such as
   knn_metric -- number of nearest neighbors.

   run following command to generate 50-dimension word-token vectors:
make your-sub-corpus.knn.xy.gz dim=25

substitute pairs based vectors:

using substitute id27s of scode sphere

   you can use <word,substitue word> pairs (<x,y> where x is target word,
   y is substitute word) and substite id27s of scode sphere to
   generate word-token embeddings.

   after generating unk-pairs.gz and 25-dimension word-type embeddings,
   just run:
your-sub-corpus.xplusy.gz dim=25

   or, you can use <word,substitue word> pairs and id27s of
   target words for substitute words:
your-sub-corpus.xplusx.gz dim=25

   these will generate 25+25-dimension word-token vectors.

using any id27s you like

   you can combine two different id27s using <word,substitute
   word> pairs to generate word-token embeddings. first generate
   unk-pair.gz then gzip your favorite id27s [93]1,[94]2 which
   should contain *unknown* tag for unknown words and run:
make your-sub-corpus.xmixx.gz x1=your-embedding1.gz x2=your-embedding2.gz

   of course you can use the same id27 for x1 and x2.

substitute distribution based vectors:

   the generalization of the previous method is using substitute
   distribution instead of <word,substitute word> pairs.
make your-sub-corpus.xsubx.gz x1=your-embedding1.gz x2=your-embedding2.gz

4 - using morphologic and orthographic features

   if you want to generate id27s using orthographic and
   morphologic features using [95]morfessor, we suggest you use a big
   corpus(maybe your lm corpus) and run:
make your-feat-corpus.feat.gz

   it will generate these features. please take a look at morfessor
   parameters in the makefile. than simply append "+f" to your target
   file, set featureflag=+f and featfile=your-feat-corpus:

   make your-sub-corpus.unk-type+f.gz dim=25 featureflag=+f
   featfile=your-feat-corpus

   or

   make your-sub-corpus.xplusy+f.gz dim=25 featureflag=+f
   featfile=your-feat-corpus

   or

   make your-sub-corpus.knn.xy+f.gz dim=25 featureflag=+f
   featfile=your-feat-corpus

todo:

     * write a better readme.
     * word features

     *    2019 github, inc.
     * [96]terms
     * [97]privacy
     * [98]security
     * [99]status
     * [100]help

     * [101]contact github
     * [102]pricing
     * [103]api
     * [104]training
     * [105]blog
     * [106]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [107]reload to refresh your
   session. you signed out in another tab or window. [108]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/ai-ku/wvec/commits/master.atom
   3. https://github.com/ai-ku/wvec#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/ai-ku/wvec
  32. https://github.com/join
  33. https://github.com/login?return_to=/ai-ku/wvec
  34. https://github.com/ai-ku/wvec/watchers
  35. https://github.com/login?return_to=/ai-ku/wvec
  36. https://github.com/ai-ku/wvec/stargazers
  37. https://github.com/login?return_to=/ai-ku/wvec
  38. https://github.com/ai-ku/wvec/network/members
  39. https://github.com/ai-ku
  40. https://github.com/ai-ku/wvec
  41. https://github.com/ai-ku/wvec
  42. https://github.com/ai-ku/wvec/issues
  43. https://github.com/ai-ku/wvec/pulls
  44. https://github.com/ai-ku/wvec/projects
  45. https://github.com/ai-ku/wvec/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/ai-ku/wvec/commits/master
  48. https://github.com/ai-ku/wvec/branches
  49. https://github.com/ai-ku/wvec/releases
  50. https://github.com/ai-ku/wvec/graphs/contributors
  51. https://github.com/ai-ku/wvec/blob/master/license
  52. https://github.com/ai-ku/wvec/search?l=perl
  53. https://github.com/ai-ku/wvec/search?l=makefile
  54. https://github.com/ai-ku/wvec/search?l=python
  55. https://github.com/ai-ku/wvec/search?l=c++
  56. https://github.com/ai-ku/wvec/find/master
  57. https://github.com/ai-ku/wvec/archive/master.zip
  58. https://github.com/login?return_to=https://github.com/ai-ku/wvec
  59. https://github.com/join?return_to=/ai-ku/wvec
  60. https://desktop.github.com/
  61. https://desktop.github.com/
  62. https://developer.apple.com/xcode/
  63. https://visualstudio.github.com/
  64. https://github.com/ai-ku/wvec/tree/179eb5ac1d80bfc54948760489ac31596563d7e1
  65. https://github.com/ai-ku/wvec/tree/master/bin
  66. https://github.com/ai-ku/wvec/tree/master/data
  67. https://github.com/ai-ku/wvec/tree/master/run
  68. https://github.com/ai-ku/wvec/tree/master/src
  69. https://github.com/ai-ku/wvec/blob/master/.gitignore
  70. https://github.com/ai-ku/wvec/blob/master/license
  71. https://github.com/ai-ku/wvec/blob/master/readme.md
  72. http://www.denizyuret.com/2012/05/learning-syntactic-categories-using.html
  73. http://aclweb.org/anthology/d/d12/d12-1086.bib
  74. http://www.denizyuret.com/2013/09/enis-rfat-sert-ms-2013.html
  75. http://aclweb.org/anthology//w/w13/w13-4909.pdf
  76. https://github.com/wolet/sprml13-word-embeddings
  77. http://aclweb.org/anthology//w/w13/w13-4909.bib
  78. https://drive.google.com/folderview?id=0bysegodiumc6expaqwznzwdxt1e&usp=sharing
  79. https://www.cs.cmu.edu//~vcirik/assets/bibs/arxiv2014.bib
  80. http://metaoptimize.com/projects/wordreprs/
  81. http://www.pdhillon.com/publications.html
  82. http://www.fit.vutbr.cz/~imikolov/id56lm/
  83. https://code.google.com/p/id97/
  84. http://www.socher.org/index.php/main/improvingwordrepresentationsviaglobalcontextandmultiplewordprototypes
  85. https://github.com/karlstratos/cca
  86. http://www.ark.cs.cmu.edu/dyogatam/wordvecs/
  87. https://github.com/mfaruqui/eacl14-cca
  88. http://lebret.ch/words/
  89. http://www.cs.cmu.edu/~bmurphy/nnse/
  90. http://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/
  91. https://github.com/ai-ku/wvec/blob/master/ttic.edu/bansal/data/syntacticembeddings.zip
  92. http://www.speech.sri.com/projects/srilm/download.html
  93. https://code.google.com/p/id97/
  94. http://metaoptimize.com/projects/wordreprs/
  95. http://www.cis.hut.fi/projects/morpho/
  96. https://github.com/site/terms
  97. https://github.com/site/privacy
  98. https://github.com/security
  99. https://githubstatus.com/
 100. https://help.github.com/
 101. https://github.com/contact
 102. https://github.com/pricing
 103. https://developer.github.com/
 104. https://training.github.com/
 105. https://github.blog/
 106. https://github.com/about
 107. https://github.com/ai-ku/wvec
 108. https://github.com/ai-ku/wvec

   hidden links:
 110. https://github.com/
 111. https://github.com/ai-ku/wvec
 112. https://github.com/ai-ku/wvec
 113. https://github.com/ai-ku/wvec
 114. https://help.github.com/articles/which-remote-url-should-i-use
 115. https://github.com/ai-ku/wvec#scode-word-embeddings-using-substitute-words
 116. https://github.com/ai-ku/wvec#other-word-vectors
 117. https://github.com/ai-ku/wvec#semi-supervised-word-vectors
 118. https://github.com/ai-ku/wvec#0---setting-up-the-environment
 119. https://github.com/ai-ku/wvec#1---generate-type--token-vectors---a-shortcut
 120. https://github.com/ai-ku/wvec#2---how-to-generate-word-type-embeddings
 121. https://github.com/ai-ku/wvec#3---how-to-generate-word-token-context-dependent-embeddings
 122. https://github.com/ai-ku/wvec#enis-serts-knn-based-vectors2
 123. https://github.com/ai-ku/wvec#substitute-pairs-based-vectors
 124. https://github.com/ai-ku/wvec#using-substitute-word-embeddings-of-scode-sphere
 125. https://github.com/ai-ku/wvec#using-any-word-embeddings-you-like
 126. https://github.com/ai-ku/wvec#substitute-distribution-based-vectors
 127. https://github.com/ai-ku/wvec#4---using-morphologic-and-orthographic-features
 128. https://github.com/ai-ku/wvec#todo
 129. https://github.com/
