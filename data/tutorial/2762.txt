probabilistic language models 1.0

noah a. smith

c(cid:13) 2017

march 2017

abstract

we introduce and explain language models. these notes are inspired in part by notes by collins
[2011a], who provides more detail about some topics. see also jurafsky and martin [2016] for a textbook
treatment.

what makes a good english sentence? a human might tell you that a good sentence is grammatical,
meaningful, and       ts    into the context in which it is uttered or written. in nlp, all of these considerations
eventually come to play a role, but we begin with a much simpler observation: the vast majority of se-
quences of words are not good sentences, but almost anything is possible. probabilistic id38   
assigning probabilities to pieces of language   is a    exible framework for capturing a notion of plausibility
that allows anything to happen but still tries to minimize surprise.

1 the problem

formally, the id38 problem is as follows. let v be the vocabulary: a (for now,    nite) set of

discrete symbols. let (cid:56), called the    stop    symbol, be one element of v. let v denote the size of v (also
written as |v|. let v    be the (in   nite) set of sequences of symbols from v whose    nal symbol is(cid:56).
a language model is a id203 distribution for random variable x, which takes values in v    (i.e.,
sequences in the vocabulary that end in(cid:56)). therefore, a language model de   nes p : v        r such that:

(cid:88)

   x     v   , p(x)     0, and
p(x = x) = 1.

x   v   

(1)
(2)

the steps to building a language model include:

1. selecting the vocabulary v.
2. de   ning the parameters of the id203 distribution p.
3. estimating those parameters from a training dataset of sentences x1:n = (cid:104)x1, x2, . . . , xn(cid:105), where each xi

is a sequence in v   .
we next explain why we want to build language models (section 2), then discuss how we evaluate them
(section 3). we discuss each step in turn (sections 4   5). we then discuss algorithms relating to language
models (section 6). finally, we present log-linear language models, which reparameterize the id203
distribution using features. (section 7).

1

2 why id38?

one motivation for id38 comes from the noisy channel paradigm, a metaphor and modeling
pattern that has inspired considerable research in nlp.

suppose we have two random variables, o (which is the observed input to our system) and d (which is
the desired output of our system). we might view o   s value as a ciphertext and d as a plaintext into which
we would like to decode the value of o. to do this, we must solve:

d    = argmax

d

= argmax

d

= argmax

d

p(d | o)
p(o | d)    p(d)

p(o)
p(o | d)

(cid:124) (cid:123)(cid:122) (cid:125)

channel model

  

(cid:124)(cid:123)(cid:122)(cid:125)

p(d)

source model

(3)

(4)

(5)

the last line of the above formula shows how the noisy channel pattern decomposes the model over o and
d. a good plaintext is likely a priori (i.e., under the source model) and also likely to have generated the
observed ciphertext (i.e., through the channel model). if either of these is too small for a value d, then d is
not a good decoding.

the id87 corresponds to a    story    about how data is generated. the story is visualized

as follows:

source        d        channel        o

first, the source distribution randomly generates a value for the plaintext random variable d. second, this
plaintext passes through a channel that corrupts it (or    adds noise to it   ), resulting in a value for the observed
random variable o. decoding works in the reverse direction, applying bayes rule (equation 4) to solve for
the most likely value of d given the observation that o = o.

classic examples of the id87 include id103 and machine translation; in both
cases, the source model is a language model over word sequences in the output language; d ranges over
v   . in id103, the acoustic model gives a distribution over sounds given words. in machine
translation, the translation model gives a distribution over input-language sentences given output-language
sentences. both of these are channel models; note that they counterintuitively model a process of transform-
ing an output into an input.

the other motivation for studying language models    rst in a natural language processing course is that
the techniques we use in id38 provide a kind of simplest    rst case for methods that are used
repeatedly later on.

3 evaluating language models with perplexity

a high-quality language model will assign high id203 to real sentences it has never observed (i.e., that
were not part of the data used to train the model). the standard way to measure the quality of a language
model is to consider a dataset that is separate from the training data, called a test dataset,   x1:m, and calculate
the model   s perplexity on this dataset:

   
perplexity(p;   x1:m) = 2

where m is the number of words in   x1:m (i.e.,(cid:80)m

log2

m

          1

m(cid:89)

         

          1

m

m(cid:88)

i=1

p(  xi)

i=1

= 2

i=1 |  xi|). lower is better.

         

    log2 p(  xi)

(6)

2 of 15

do not be thrown by the    two to the power . . .     at the beginning of this expression, or by the loga-
rithm! remember that exponentiation and logarithms are inverses; what   s going on in this formula is kind of
intuitive if we take it step by step. we   ll consider the expression on the right of equation 6.
1. first, calculate the id203 of each element of the test data   xi, for i     {1, . . . , m}.
2. taking the (base 2) logarithm of each of those values gives us a nonpositive number that could have
arbitrarily large magnitude. big negative values go with tiny probabilities; values close to zero go with
larger probabilities. recall that the logarithm function is monotonic; as a increases (decreases), log2 a
also increases (decreases).

3. negating those logarithms gives us something known in machine learning as the log loss. now, the values
are nonnegative; large values mean low probabilities (bad), and values close to zero mean higher proba-
bilities. this is sometimes described as a measure of surprise (or, in id205,    information   )
that the model experiences when it observe a test sentence.

4. take a per-word average of those sentence losses, by summing them up and dividing by the number of

words. this converts to a per-word surprise score.

5. raise 2 to this power. again, this is a monotonic transformation (as a increases, so does 2a, only much

faster).
perplexity can therefore be understood as a kind of branching factor:    in general,    how many choices
must the model make among the possible next words from v? consider a simpler case where we have only
one test sentence,   x. perplexity is then 2
    if the model (miraculously) assigns id203 of 1 to   x, then perplexity will be 1, since 2    1

m log2 1 = 1.
here, there is always one choice, and it   s always correct! this never happens, because in reality anyone
could say anything at any time.

. a few boundary cases are interesting to consider:

   (cid:16) 1|x| log2 p(  x)
(cid:17)

    a model that assigns p(  x) =(cid:0) 1

(cid:1)|  x|

   essentially rolling an evenly weighted v -sided die for each word in
  x   will have perplexity v . every word is uniformly possible at every step and therefore pretty surprising.
it doesn   t get more    random    than this, as there are always v choices.

    a model that assigns p(  x) = 0 will have in   nite perplexity, because log2 0 =       .

v

perplexity is not a perfect measure of the quality of a language model. it is sometimes the case that
improvements to perplexity don   t correspond to improvements in the quality of the output of the system that
uses the language model. some researchers see it as a    pure,    system-agnostic way to test the quality of
a model (or model family, or estimation procedure). when you see perplexity scores in the literature, take
them with a grain of salt.

two important caveats to remember about perplexity are (i) that you cannot directly compare two models   
perplexity if they do not use identical vocabularies, and (ii) perplexity is not meaningful if the sum-to-one
constraint (equation 2) is not satis   ed by your language model.

4 vocabulary

our starting point for de   ning v, the vocabulary for our language model, is to include every word that occurs
in the training dataset x1:n. it should be immediately obvious that a    nite training dataset is not going to
include all of the possible words in a natural language, because:
    new words show up all the time, either because they are borrowed from other languages or because they
    many languages form words productively through their rules of morphology, so that the number of possi-

are invented (   neologisms   ).

ble words results from a combinatorial explosion.

3 of 15

    real-world data often shows variation in how words are written or spelled, either because there aren   t

standards or because people sometimes do not adhere to them.
a common solution is to introduce a special symbol into v called the    unknown word    denoted    unk   
for any out-of-vocabulary word. in the training dataset, some words   usually some or all instances of some
or all of the rarest word types   are replaced by unk.

another solution, for morphologically rich languages, is to build a computational model that maps be-
tween the    surface forms    of words (as they are observed in text) and their    underlying    morpheme se-
quences. for example, in english, the word starting might map to (cid:104)start, -ing(cid:105), the word vocabularies might
map to (cid:104)vocabulary, -s(cid:105), and the word morphologically might map to (cid:104)morph, -ology, -ical, -ly(cid:105). if every
word in a text dataset can be decomposed into its morphemes, we could build a language model with v as
the set of morphemes, rather than surface words. morphological ambiguity, where a word can be broken
down in multiple ways,1

a third solution is to build language models at the level of characters, or to combine character- and
word-level language models. this powerful idea has become more popular recently, but in order to read
those papers, you    rst need a basic understanding of how language models work with the simpler    unk   
solution.

5 parameterizing a language model and estimating the parameters

we proceed through a series of language models.

5.1 creepy language model

the    rst language model we consider is quite simple. it assigns id203 to a sequence x proportional to
the number of times that sequence was observed in x1:n. more formally:

(7)

p(x = x) =

|{i | xi = x}|

cx1:n(x)

=

n
where cx1:n(  ) returns the count of its argument in the training dataset.

n

this model does a great job of    tting the training dataset; it won   t waste any id203 mass on se-
quences not seen in training. but that is its downfall; consider what will happen when we evaluate this
model on a test example   x that happens not to have been seen in training data. its id203 will be 0, and
perplexity will be in   nite.

i call this the    creepy    language model because of what will happen if we randomly sample from it.
imagine simulated data, drawn according to the distribution p in equation 7. the only sentences drawn will
be ones seen in training data. this model just memorizes what it has seen before; it does not generalize at
all.
to get away from the problem of creepiness, we will use the chain rule of id203 to decompose the
distribution. here we use xi:j to denote the collection of random variables (cid:104)xi, xi+1, . . . , xj   1, xj(cid:105), all
within the sequence x. we also introduce a special start symbol x0 = (cid:13) that is not in the vocabulary v,
1consider this example: unlockable might refer to something that can be unlocked or something that cannot be locked. both of
these would probably break down into (cid:104)un-, lock, -able(cid:105), but they correspond to attaching the af   xes to lock in different orders. in
more morphologically rich languages, the potential for ambiguity is greater.

4 of 15

assumed to appear just before every sentence; the random variable x0 always takes the value x0.2

                     

(cid:96)(cid:89)

p(x1 = x1 | x0 = x0)
   p(x2 = x2 | x0::1 = x0:1)
   p(x3 = x3 | x0:2 = x0:2)
...
   p(x(cid:96) = (cid:56) | x0:(cid:96)   1 = x0:(cid:96)   1)
p(xj = xj | x0:j   1 = x0:j   1)

                     

p(x = x) =

=

j=1

note that this decomposition does not involve any assumptions. each word is generated conditioned on the
history of words that came before, and the entire history is considered.

how would we estimate the component distributions? the starting point for estimating distributions
like these is the same as in the creepy language model: probabilities should be proportional to counts (or
frequencies). for conditional probabilities like those in equation 9, the relative frequency estimate is:

p(xj = xj | x0:j   1 = x0:j   1) =

cx1:n(x0:j)
cx1:n(x0:j   1)

(10)

that is, the id203 is estimated as the frequency of observing a history/word pair, divided by the fre-
quency of observing just the history (in the training dataset). plugging in to equation 9, we get:

j=1

(cid:96)(cid:89)
(cid:96)(cid:89)
(cid:96)(cid:89)

j=1

j=1

cx1:n(x0:j)
cx1:n(x0:j   1)

cx1:n(x0:j)

cx1:n(x0:j   1)

p(x = x) =

=

=

=

=

cx1:n(x0:1)    cx1:n(x0:2)    cx1:n(x0:3)       cx1:n(x0:(cid:96)   1)    cx1:n(x0:(cid:96))

cx1:n(x0)    cx1:n(x0:1)    cx1:n(x0:2)    cx1:n(x0:3)       cx1:n(x0:(cid:96)   1)
cx1:n(x0:(cid:96))
cx1:n(x0)
cx1:n(x)

n

this is identical to equation 7, so it should be clear that nothing has changed yet. (note that cx1:n(x0)
is exactly n, the number of sentences in the training dataset, since the start symbol (cid:13) occurs before the
beginning of each sentence, and only there.)

5.2 unigram model
the unigram model adds an extremely strong assumption to the above decomposition: every word is dis-
tributed independent of its history. this gives us a very simple model of language, in which a v -sized die

is rolled once for every word, ignoring all previous words. (when the die rolls (cid:56), the sentence ends.) you

should see that this model is very different from the creepy language model.

2so it isn   t really random!

5 of 15

(8)

(9)

(11)

(12)

(13)

(14)

(15)

mathematically:

p(xj = xj | x0:j   1 = x0:j   1)

assumption

= p  (xj = xj)

(cid:96)(cid:89)

p  (x = x) =

p  (x = xj)

(16)

(17)

j=1

we use    to denote the set of parameters of the model.

this model requires that we estimate the id203 for each word v     v. the simplest way to do this
is to store a value for each word. let p(x = v) take a value denoted by   v (   the unigram id203 of v   );
its estimate from data is denoted by     v. the relative frequency estimate for   v is:

    v =

|{i, j | [xi]j = v}|

n

(18)

where n is the total number of words in the training dataset, (cid:80)n
(19)
i=1 |xi|. both when calculating n and
when estimating the distribution   , we include the stop symbol (cid:56); because it occurs once per training data
n . we do not count the start symbol (cid:13), because it is always assumed to be

n

=

cx1:n(v)

instance, its id203 will be n
given (not generated by the model).

the unigram model will have v parameters, one for every word in v.
unigram models are sometimes called    bag of words    models, because they assign the same id203
to a sentence x and any permutation of it; from the model   s perspective, x is a bag (i.e., a collection that
can contain duplicates, unlike a set) of words. this terminology is commonly used when building language
models over documents, where each x is a document, rather than a sentence. in general,    bag of words   
models view each document as a histogram of word frequencies, without regard to the order the words.

unigram models are easy to understand, computationally cheap to estimate and store, and they work
well for some problems (e.g., information retieval). linguistically, they are very simplistic, assigning high
id203 to some absurdly implausible sentences. for example, the most common word in typical english
corpora is the. this means that   the >   v for all v     v \ {the}. this means that the most common sequence
of length (cid:96) = 5 is    the the the the (cid:56)   ; this is more likely than    i want ice cream (cid:56)    and    good morning to
you(cid:56)    and    make america great again(cid:56)   .

5.3 bigram models

the creepy model doesn   t generalize well, but the unigram model   s strong independence assumption means
that it can   t pick up on any patterns beyond the relative frequencies of words. the next model we consider
weakens that independence assumption, ever so slightly.

a bigram model lets each word depend directly on its predecessor, the most recent word:

p(xj = xj | x0:j   1 = x0:j   1)

assumption

(cid:96)(cid:89)

= p  (xj = xj | xj   1 = xj   1)
p  (xj = xj | xj   1 = xj   1)

p  (x = x) =

(20)

(21)

this model also is known as a (   rst-order) markov model, because the assumption in equation 20 is a
(   rst-order) markov assumption.

j=1

6 of 15

the parameters of this model,   , include a value for the id203 of every word v in v, conditioned on

every word v(cid:48) in v, plus the start symbol (cid:13). the relative frequency estimate is:

|{i, j | [xi]j   1 = v(cid:48)     [xi]j = v}|

|{i, j | [xi]j   1 = v(cid:48)}|

    v|v(cid:48) =

=

(cid:80)

cx1:n(v(cid:48)v)
u   v cx1:n(v(cid:48)u)

(22)

(23)

it   s important to notice that the denominator here is the count of the history word, occurring as a history word
with any vocabulary item u     v. that means that when we estimate (for example)   the|(cid:13), the denominator
will count the number of times some word occurs after (cid:13) (which will be the number of sentences n, since
every sentence   s    rst word follows an implied (cid:13)).
histories (v + 1, accounting for (cid:13)) times the vocabulary size (v ): v (v + 1).

the number of parameters that need to be estimated for the bigram model is given by the number of

5.4 id165 models
generalizing from unigram and bigram models, id165 models condition each word   s id203 on the
history of the most recent (n     1) words, also known as a (n     1)-order markov assumption. when n = 3,
we have a trigram model; for larger values of n, we simply use the number (e.g.,    four-gram,          ve-gram,   
etc.).

here is the general form for id165 models:

p(xj = xj | x0:j   1 = x0:j   1)

assumption

(cid:96)(cid:89)

= p  (xj = xj | xj   n+1:j   1 = xj   n+1:j   1)
p  (xj = xj | xj   n+1:j   1 = xj   n+1:j   1)

p  (x = x) =

(24)

(25)

j=1

the parameters of this model,   , include a value for the id203 of every word v in v, conditioned
on every history. histories are now a bit more complicated, because we need a (n     1)-length history for
every word, including the    rst one (which up until now was only preceded by x0 = (cid:13)). to keep notation
simple, we assume that every sentence is preceded by as many copies of (cid:13) as we need to ensure that x1 has
a (n     1)-length history. this means that x   (n   2) = x   (n   3) =        = x   1 = x0 = (cid:13). we use h to denote
a history of length (n     1). the relative frequency estimate is:

|{i, j | [xi]j   n+1:j   1 = h     [xi]j = v}|

|{i, j | [xi]j   n+1:j   1 = h}|

    v|h =

=

(cid:80)

cx1:n(hv)
u   v cx1:n(hu)

(26)

(27)

as before, the denominator here is the count of the history word, occurring as a history word.

notice that, as we increase n, these models approach the creepy language model, remembering more
and more of each word   s history. with relative frequency estimation, they will never allow a word to occur
following a history it wasn   t observed with in the training dataset. as we decrease n, we approach a unigram
model that simply ignores the history. the best n value for your problem will depend on the training dataset,
especially its size (n and n), and on the size of the vocabulary (v ). when making a high-level design choice
like n, it is advisable to reserve a portion of data, separate from your training dataset, and use it to make
the choice. high-level choices like n   s value are sometimes called model selection or hyperparameter
selection. here   s a simple procedure for choosing n:

7 of 15

1. when constructing your training dataset, hold some data aside (i.e., do not include it in x1:n). call this

your development dataset; we   ll refer to it as   x1:d.

2. let n be the set of values for n you are willing to consider.
3. for each g     n:
(a) estimate the parameters     
(b) calculate perplexity(p    
4. choose the g    with the lowest development-dataset perplexity, and let n = g   .

(g) for a g-model from the training dataset x1:n.

(g);   x1:d), the perplexity of the g-gram model on the development dataset.

id165 models are fairly easy to build, requiring only a pass over the data to gather all of the appropriate
counts, then simple operations to obtain relative frequency estimates. when working with very large training
datasets some engineering is required to work with them ef   ciently. many of the key tricks are implemented
in open-source implementations like kenlm3 and srilm.4

5.5 data sparsity in id165 model estimation

n-models suffer from a curse-of-dimensionality problem. as n increases, the number of parameters to be
estimated increases exponentially. this means that getting good estimates for each and every id165   s     v|h
requires much more data. in concrete terms, the vast majority of id165s will never be observed, even if
they are linguistically plausible. this is sometimes referred to as data sparseness, because the count values
will be mostly zeroes. imagine a vector holding cx1:n(hv) for a single history, and all v     v; as n goes up,
vectors will get sparser and sparser, since each n-in the corpus now has a longer and more detailed history to
associate with.

(data sparseness can be mitigated somewhat by mapping more and more words to unk, but this comes
at a cost of expressive power. a model that maps most words to unk will miss a lot of the nuance in natural
language!)
we presented relative frequency estimation (counting id165s and their (n     1)-length histories, then
dividing) uncritically. in fact, relative frequency estimation is motivated not just by intuition, but by the
maximum likelihood principle. this is an idea from statistics, telling us that we should choose parameter
estimates that make the training dataset as likely as possible. in notation:

    id113 = argmax

  

p  (x1:n)

(28)

it is not dif   cult to prove that relative frequency estimation gives the maximum likelihood estimate (id113)
for conditional categorical distributions like those our id165 models are built out of.
one of the reasons statisticians like the id113 in general is theoretical. in the limit as n        , the id113
will converge on the true distribution that generated the data, provided that the model family (here, an n-
model for a particular value of n) includes the true distribution. in nlp, we almost never believe that the
model family includes the true distribution; we accept that our models are imperfect and simply try to make
them as useful as possible.

and id113 n-models are not very useful, because, for history h, the id203 of any word not observed
immediately after that history will be 0. further, for any history that wasn   t observed, the distribution over
words that can immediately follow it is completely unde   ned (since its frequency is 0). this spells disaster
for any perplexity calculations!

if you   ve taken a class in machine learning, you   ve likely learned about the problem of over   tting,
that is, when a model   s performance on training data is very good, but it doesn   t generalize well to test (or

3https://kheafield.com/code/kenlm/
4http://www.speech.sri.com/projects/srilm/

8 of 15

development) data. the inability of id113 id165 models to deal well with data sparsity can be seen as an
example of over   tting.

5.6 smoothing
in order to make id165 models useful, we must apply a smoothing transformation that eliminates zero-
counts, both for histories and for id165s. there are many methods available for smoothing, and some of
them are quite complicated. to understand why, consider the original de   nition of a language model, which
requires that the sum of probabilities over all sequences in v    be one (equation 2. it is straightforward to
show that this constraint is met by id113 with relative frequency estimation; it is harder to guarantee when
we start manipulating the estimation procedure.

instead of explaining speci   c smoothing algorithms in detail, we will consider some    safe    transforma-

tions that provide building blocks for smoothing.

5.6.1 linear interpolation

noting that different values of n lead to models with different strengths and weaknesses, we can build a
language model that interpolates among them. suppose we have two language models, a unigram model
p  (1) and a bigram model p  (2). let the interpolated model be given by:

    (interp)
v|v(cid:48) =       (1)

v + (1       )    (2)
v|v(cid:48)

(29)
where        [0, 1]. this is a bigram model whose estimate is given partly by id113 and partly by a unigram
id113 estimate. unlike the unsmoothed bigram estimate, this inteprolated estimate will never assign zero to
a word that has a nonzero unigram count, as long as    > 0. unlike the unsmoothed unigram estimate, the
interpolated estimate pays attention to the context, as long as    < 1.

the above idea generalizes to any number (k) of models. let        rk

+ be a vector of positive interpola-

tion coef   cients that sums to one. then let

    (interp)
v|h =

k(cid:88)

  k

    (k)
v|h

(30)

k=1

(we abuse notation slightly; some models may not de   ne   (k)
truncate the history as needed to    t the model.)

v|h for the full history h. in such cases, we

this idea leads naturally to the question of how to choose   . for the unigram/bigram example above,
it   s straightforward to show that if we follow the maximum likelihood principle and choose    to make the
training dataset as likely as possible, we will pick    = 0; a bigram model has more    exibility than a unigram
model and can always make the training dataset more likely.

we therefore use development data to choose   , simulating the test-time experiment where the model
is exposed to new data.    is an example of a hyperparameter, so a model selection method like the one
presented in section 5.4 (for choosing n) can be applied.

5.6.2 additive smoothing

another way to avoid zeroes transform the counts directly before estimating   . consider, for example, the
estimation formula for a trigram id203:

    (id113)
v|v(cid:48)(cid:48)v(cid:48) =

(cid:80)

cx1:n(v(cid:48)(cid:48)v(cid:48)v)
u   v cx1:n(v(cid:48)(cid:48)v(cid:48)u)

9 of 15

(31)

if, prior to this calculation, we augmented every count by a    xed quantity   , we   d have:

   + cx1:n(v(cid:48)(cid:48)v(cid:48)v)
u   v(   + cx1:n(v(cid:48)(cid:48)v(cid:48)u))
for    > 0, the resulting estimate is always strictly positive.

    (add-  )
v|v(cid:48)(cid:48)v(cid:48) =

=

(cid:80)

v    +(cid:80)

   + cx1:n(v(cid:48)(cid:48)v(cid:48)v)

u   v cx1:n(v(cid:48)(cid:48)v(cid:48)u)

(32)

how to choose   ? again, we treat    as a hyperparameter and apply model selection. the value    = 1
is often presented    rst, and has a special name (   laplace smoothing   ), but in practice the best values are
usually smaller than one (i.e., fractional). it is also possible to choose different values of    for different
histories.

5.6.3 discounting
a third tool, similar in spirit to adding   , is discounting, where we subtract away from each count before
normalizing into probabilities. let        (0, 1) be the discount value. for a given history h, we divide the
vocabulary v into two disjoint sets:

for v     a(h), de   ne:

a(h) = {v     v : cx1:n(hv) > 0}
b(h) = {v     v : cx1:n(hv) = 0}

(cid:80)
cx1:n(hv)       
u   v cx1:n(hu)

    (disc)
v|h =

(33)
(34)

(35)

the result of this transformation will be that the probabilities sum up to a value less than one. let the
   missing mass    be denoted by

    (disc)
v|h

(36)

q(h) = 1     (cid:88)

v   a(h)

this mass, q(h), will be divided up among all of the words in b(h). one common way to do it is to divide
up proportionally according to a shorter-n id165 model. this is known as backoff. a simpler way is to
simply divide it up uniformly across b(h). the value of    can be chosen using model selection.

note that, in general,    need not be a    xed value for every history length, every history frequency, or even
every history. one famous method for discounting, good-turing discounting takes the total count mass for
words with observed count c and redistributes it among words with observed count c     1. this means that,
for a given history, zero-frequency words are given a id203 similar to what we would normally assign to
words that occurred once, which are given a id203 similar to what we would normally assign to words
that occurred twice, and so on.

6 algorithms

there are three useful language model-related algorithms you should be able to    gure out:
1. given a language model p and a sentence x, calculate p(x), or its logarithm.5 you should be able to do
this in o((cid:96)) runtime, where (cid:96) is the length of the sentence. because language model probabilities are tiny
and can lead to under   ow, the following equation is extremely useful:

(cid:96)(cid:88)

log p(x) =

log p(xj | x0:j   1)

(37)

5the logarithm is sometimes preferred to avoid under   ow, and it   s what you need for perplexity (equation 6).

j=1

10 of 15

that is, summing logarithms of probabilities (log-probabilities) is a smart way to avoid the under   ow that
would likely result if we multiplied together probabilities. it doesn   t really matter which logarithm base
you use, as long as you exponentiate with the same base (the default in most programming languages is
base e).

2. estimate a language model   s parameters from a training dataset. you should be able to do this in o(n )
runtime, where n is the number of words in the training data, and also o(n ) space.6 importantly, you
should not require o(v n) space, which is what you would need if you na    vely stored every id165   s
id203! depending on your smoothing method, many id165s will have the same id203, and
you will want to store that value just once if you can. in particular, an id165 that had a training dataset
frequency of zero should probably take a    default    id203 value, depending on some properties of the
history it contains.

3. randomly sample a sentence x from a language model p. you should be able to sample each word in

o(v ) time.

7 log-linear language models

another solution to the data sparseness problem is to rethink the way we parameterize language models.
instead of associating a parameter   v|h with every id165, the approach we consider next converts the n-
gram into a collection of attributes (known as features) and uses those to assign it a id203.

see collins [2011b] or smith [2004] for general introductions to id148. here we present

them for the id38 case, speci   cally.

to de   ne a log-linear language model, we    rst introduce a function    that maps a history and a word (in
v) to a real-valued vector in rd. associated with the kth dimension of this vector is a coef   cient or weight
wk, and together these are stacked into w     rd, which are the parameters of the model. the form of the
language model is given by:

pw(x = x) =

=

(cid:96)(cid:89)
(cid:96)(cid:89)

j=1

j=1

assumption

=

(cid:96)(cid:89)

=

pw(xj = xj | x0:j   1 = x0:j   1)

exp w      (x0:j   1, xj)

(cid:96)(cid:89)

zw(x0:j   1)
exp w      (xj   n+1:j   1, xj)

zw(xj   n+1:j   1)

j   1
exp w      (hj, xj)

j=1

zw(hj)

(38)

(39)

(40)

(41)

the assumption in equation 40 is the standard (n     1)-order markov assumption, and the result is that the
feature vector    maps id165s (hv) to vectors. the zw function in the denominator is shorthand for a
summation that guarantees that the distribution over words given history h will sum to one. its full form is:

zw(h) =

exp w      (h, v)

(42)

(cid:88)

v   v

the form of the log-linear language model is a bit daunting at    rst. it   s helpful to consider how it   s built

from the inside out, which we   ll do in the next few sections.

6if you use interpolation or backoff, you might need o(n n) runtime and space.

11 of 15

7.1 features

when we consider an id165 hv, we start by mapping it to its feature vector   (hv). some kinds of features
conventionally used include:
    indicators for id165s. for example, a feature for the trigram do not disturb would return 1 if and only if
the last two words in h are do not and v is disturb; otherwise it returns 0. there are v 3 trigram features
(and v n features for general n-length id165s).
    indicators for gappy id165s. this allows, for example, the old man, the healthy man, the bearded man,
and so on, to all have probabilities that rise or fall together, through a feature that returns 1 if and only if
the second-to-last word in h is the and v is man.
    spelling features, such as whether a word begins with a vowel or a capital letter, or whether a word
contains a digit. these can be conjoined with a history word; for example, a feature that returns 1 if and
only if the last word of h is an and v starts with a vowel could let the language model learn to prefer an
(to a) before words that start with vowels.
    class features, that use some external resource to de   ne sets of words that share membership in some
class like    protein names    or    geographic place names    or    transitive verbs.    such a feature returns 1 if v
is in the class, and 0 if it isn   t; as with the spelling features, it can be conjoined with some feature of the
history.

you can de   ne any features as you   d like, as long as they can be calculated by considering only h and v.7
it   s common in nlp for features to be binary indicators, like those above, and to form new features by
conjoining the values of simpler binary indicator features.

the    rst challenge of log-linear language models is in choosing good features. if you choose too many,
your model will be prone to over   tting the training dataset. if you don   t choose enough features that capture
the properties of the language, your model will not learn to generalize.

7.2 coef   cients

once we   ve mapped hv to its feature vector representation   (h, v), we take an inner product with the
coef   cients w:

w      (h, v) =

wk  k(h, v)

(43)

k=1

this linear mapping can be understood as projecting our id165 to a linear score. for a binary feature   k,
the value of the corresponding coef   cient wk tells us immediately how the presence of the feature changes
the score:
    if wk > 0, then the score of any id165 that    has    this feature (i.e., where   k(h, v) = 1) increases by wk.
    if wk < 0, then the score of any id165 that    has    this feature (i.e., where   k(h, v) = 1) decreases by
    if wk = 0, then the score of any id165 that    has    this feature (i.e., where   k(h, v) = 1) is unaffected.

this will make the id165 more likely.
   wk. this will make the id165 less likely.

7it can be shown that a feature that depends only on h and not v will have no effect on the id203 distribution for history h.

this is left as an exercise.

12 of 15

d(cid:88)

7.3 softmax: exponentiate and normalize

once we map id165s to scores, we can transform the scores into id203 distributions for each history
h, by applying a transformation often called the softmax:

(cid:42)

ea1(cid:80)v

ea2(cid:80)v

,

k=1 eak

k=1 eak

, . . . ,

eav(cid:80)v

k=1 eak

(cid:43)

(44)

softmax ((cid:104)a1, a2, . . . , av (cid:105)) =

the softmax takes every value in a vector   here, the collection of scores for words in v paired with a single
history h   and exponentiates them before renormalizing them. this transformation has some desirable
properties:
    it preserves monotonicity (i.e., if ai > aj then the ith value of the new vector will be larger than the jth
value of the new vector). this means that the higher an id165   s score is, the higher its id203 will
be.
    it gives a proper id203 distribution over v, i.e., one comprised of nonnegative values that sum to one.
    if the scores are all    nite, then every id165 will have positive id203; there will be no zeroes.

7.4 parameter estimation

unfortunately, there is no closed-form solution for the maximum likelihood estimate of id148,
or for any principled method that seeks to avoid over   tting. instead, the parameter estimation problem is
typically solved using a id76 algorithm.
we let i index the n words in our training dataset, rather than the n sentences. each word   s history is
de   ned as before (i.e., if it is the    rst word in a sentence, then its history is (n     1) copies of (cid:13)). here is
the form of the id113 problem for log-linear language models, written in terms of
log-likelihood:8

i=1

n(cid:88)
n(cid:88)
n(cid:88)

i=1

i=1

max
w   rd

= max
w   rd

= max
w   rd

log pw(xi | hi)

exp w      (hi, v)

zw(hi)

log

w      (hi, xi)     log zw(hi)

(45)

(46)

(47)

this problem is concave in w and differentiable with respect to w, which means that we can negate it and
apply standard methods for id76 to solve it with high precision. two popular methods are:
    l-bfgs, a    batch    method that iteratively calculates the gradient of the total log-likelihood with respect
to w and updates in the gradient direction, with clever math for deciding how far to go in that direction;
and
    stochastic id119, a method that calculates the gradient of the log-likelihood with respect to one
(or a small number) of instances i, often chosen by random shuf   ing of the data, and updating w after
each such calculation.
there is an intuitive way to understand equation 47. for each word xi in the training set, the goal is to
increase the score that it receives with its observed history h (the    rst term inside the summation over i).

8since the logarithm function is monotonic, the parameters that maximize log-likelihood will be identical to those that maximize

likelihood.

13 of 15

(cid:88)
(cid:123)(cid:122)

v   v

(cid:124)

(cid:125)

we can   t stop there, though; if we did, then the trivial solution would drive every wk to +   . we must give
the optimization method something to decrease, and that   s where the second term, the logarithm of zw(h)
comes in. we sometimes refer to these two components as the    hope    and    fear    parts of the objective.
letting s(v) denote the linear score of a word with history h (suppressed for clarity), we have:

(cid:124)(cid:123)(cid:122)(cid:125)

increase s(xi)
hope

while decreasing log

exp s(v)

(48)

note that the    log(cid:80) exp    transformation is a smooth upper bound on the max function. it arises frequently

fear

in machine learning. here, the intuition comes from imagining that it (approximately) picks out the score
of the currently-most probable v given history h, and tries to push it down. in fact, the scores of all v get
pushed down, in aggregate. the only way to achieve the maximum likelihood solution is to achieve balance
between hope and fear, so that the score of xi is as close as possible to the (approximately) most-probable
next word.

of course, in real data, many words may share this history, so really what we   re trying to achieve for

history h is:

increase (cid:88)

i:hi=h

(cid:124)(cid:123)(cid:122)(cid:125)

s(xi)
hope

while decreasing |{i : hi = h}|    log

(cid:124)

(cid:88)
(cid:123)(cid:122)

v   v

fear

(cid:125)

exp s(v)

(49)

so, in aggregate, we want the scores of words that actually follow h to come close to the (approximately)
most probable word   s scores. words that occur with h more often will be counted more times in the    hope   
term, so we   ll have more to gain by increasing their probabilities   just as in relative frequency estimation
for id165 models.

of course, because the coef   cients w are shared across all histories, we can   t solve the above problem
separately for each history by itself. the larger maximum likelihood problem considers all of the data
together, balancing between hope and fear across all histories.

notice that calculating zw requires summing up scores over the vocabulary size; simply calculating the
objective function will require o(v n d) runtime. this implies that solving equation 47 will be a fairly
expensive operation, especially compared to classical id165 models. this is one reason that log-linear
models never became truly mainstream.

a second challenge with log-linear language models is that they tend to over   t. the over   tting problem
here has less to do with data sparsity and more with the rich expressive power of models that use many
features. it   s straightforward to show that, under certain conditions, a feature   s coef   cient might tend toward
positive or negative in   nity as we approach the maximum likelihood solution. one way to prevent this is to
regularize the log-likelihood objective by adding a penalty that grows as coef   cients take on more extreme
values. the simplest version of this is a quadratic penalty:

(cid:32) n(cid:88)

(cid:33)

d(cid:88)

max
w   rd

w      (hi, xi)     log zw(hi)

      

i=1

k=1

w2
k

(50)

where    is a hyperparameter chosen via model selection. this technique is sometimes called (squared) (cid:96)2
id173, because the penalty is proportional to the squared (cid:96)2-norm of the w vector, written as (cid:107)w(cid:107)2
2.
the (cid:96)1 norm can also be used; it has the interesting and arguably desirable property of driving many of the
wk to 0, so that they can be safely eliminated from the model.

14 of 15

references

michael collins. course notes for coms w4705: id38, 2011a. url http://www.cs.

columbia.edu/  mcollins/courses/nlp2011/notes/lm.pdf.

michael collins. id148, memms, and crfs, 2011b. url http://www.cs.columbia.

edu/  mcollins/crf.pdf.

daniel jurafsky and james h. martin. id165s (draft chapter), 2016. url https://web.stanford.

noah a. smith. id148, 2004. url http://homes.cs.washington.edu/  nasmith/

edu/  jurafsky/slp3/4.pdf.

papers/smith.tut04.pdf.

15 of 15

