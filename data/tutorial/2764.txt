seven lectures on statistical parsing

a bit more on lexicalized parsing

christopher manning

lsa linguistic institute 2007

lsa 354
lecture 5

complexity of lexicalized pid18
parsing

complexity of exhaustive lexicalized
pid18 parsing

a[d2]

b[d1]

c[d2]

i

d1

k

d2

j

time charged :
   

i, k, j 

     n 3

      g 3

    a, b, c
    naively, g becomes huge
    d1, d2

      n 2

running time is o ( g 3    n 5 ) !!

complexity of lexicalized pid18
parsing

    work such as collins (1997) and charniak

(1997) is o(n5)     but uses heuristic search to be
fast in practice

    eisner and satta (2000, etc.) have explored

various ways to parse more restricted classes of
bilexical grammars in o(n4) or o(n3) time
    neat algorithmic stuff!!!
    see example later from id33

100000

10000

1000

e
m

i
t

100

10

1

10

y = c x 5.2019

bu naive

length

100

refining the node expansion
probabilities

    charniak (1997) expands each phrase structure tree

in a single step.

child nodes

    this is good for capturing dependencies between

    but it is bad because of data sparseness
    a pure dependency, one child at a time, model is

worse

    but one can do better by in between models, such as
generating the children as a markov process on both
sides of the head (collins 1997; charniak 2000)
    cf. the accurate unlexicalized parsing discussion

1

collins (1997, 1999); bikel (2004)

overview of collins    model

    collins (1999): also a generative model
    underlying lexicalized pid18 has rules of form

    a more elaborate set of grammar transforms

and factorizations to deal with data sparseness
and interesting linguistic properties

    so, generate each child in turn: given p has been
generated, generate h, then generate modifying
nonterminals from head-adjacent outward with
some limited conditioning

li generated
conditioning on

subcat

p(th,wh)

li

li   1

    l1

{subcatl}

h(th,wh)

  

modifying nonterminals
generated in two steps

smoothing for head words of
modifying nonterminals

s(vbd   sat)

np(nnp

)
   john

vp(vbd   sat)

back-off level
1
0
2
1
3
2

collins model     and linguistics

    collins had 3 generative models: models 1 to 3
    especially as you work up from model 1 to 3,

significant linguistic modeling is present:
    distance measure: favors close attachments

    model is sensitive to punctuation

    distinguish base np from full np with post-modifiers
    coordination feature
    mark gapped subjects
    model of subcategorization; arguments vs. adjuncts
    slash feature/gap threading treatment of displaced

constituents
    didn   t really get clear gains from this.

    other parameter classes have similar or more

elaborate backoff schemes

bilexical statistics: is use of
maximal context of pmw useful?
    collins (1999):    most importantly, the model has

parameters corresponding to dependencies
between pairs of headwords.   

    gildea (2001) reproduced collins    model 1 (like
regular model, but no subcats)
    removing maximal back-off level from pmw resulted in
    gildea   s experiment somewhat unconvincing to the
extent that his model   s performance was lower than
collins    reported results

only 0.5% reduction in f-measure

2

  ! p"ljlj#1ll1hr1lrk#1rk! ph! pmw! pm! pmw(wmi|   )  ! mi,tmi,coord,punc,p,h,wh,th,"m,subcatsidemi,tmi,coord,punc,p,h,th,"m,subcatsidetmichoice of heads

   

if not bilexical statistics, then surely choice of
heads is important to parser performance   
    chiang and bikel (2002): parsers performed

decently even when all head rules were of form
   if parent is x, choose left/rightmost child   
    parsing engine in collins model 2   emulation

mode: lr 88.55% and lp 88.80% on   00
(sent. len.    40 words)
    compared to lr 89.9%, lp 90.1%

use of maximal context of pmw
[bikel 2004]

lr

lp

89.9

90.1

cbs

0.78

0 cbs

   2 cbs

68.8

89.2

89.5

90.0

0.80

68.0

88.8

full
model

no
bigrams

performance on   00 of id32

on sentences of length    40 words

use of maximal context of pmw

back-off level

0

1

2

number of
accesses

3,257,309

24,294,084

191,527,387

percentage

1.49

11.0

87.4

total

219,078,780

100.0

number of times parsing engine was able to deliver a id203

for the various back-off levels of the mod-word generation model, pmw,

when testing on   00 having trained on     02   21

charniak (2000) naacl:
a maximum-id178-inspired parser

    there was nothing maximum id178 about it. it was a
    smoothes estimates by smoothing ratio of conditional

cleverly smoothed generative model

terms (which are a bit like maxent features):

    biggest improvement is actually that generative model

predicts head tag first and then does p(w|t,   )
    like collins (1999)

    markovizes rules similarly to collins (1999)
    gets 90.1% lp/lr f score on sentences     40 wds

bilexical statistics are used often
 [bikel 2004]

play much of a role in parsing

    the 1.49% use of bilexical dependencies suggests they don   t
    but the parser pursues many (very) incorrect theories
   

so, instead of asking how often the decoder can use bigram
id203 on average, ask how often while pursuing its top-
scoring theory

    answering question by having parser constrain-parse its own output

train as normal on     02   21

feed parse trees as constraints

   
    parse   00
   
percentage of time parser made use of bigram statistics shot up to
28.8%
so, used often, but use barely affect overall parsing accuracy
exploratory data analysis suggests explanation
    distributions that include head words are usually sufficiently similar to those

that do not as to make almost no difference in terms of accuracy

   
   
   

petrov and klein (2006):
learning latent annotations
outside

can you automatically find good symbols?

    brackets are known
    base categories are known
    induce subcategoriesinduce subcategories
    clever split/merge category refinement

em algorithm, like forward-backward for
id48s, but constrained by tree.

x1

x4

x5

x6

was
right
inside

x7

.

x2
x3

he

3

),,|(),,,|(ppgpptlltpltlltpnumber of phrasal subcategories

pos tag splits, commonest words:
effectively a class-based model

    proper nouns (nnp):
oct.

nnp-14

nnp-12

nnp-2

nnp-1

nnp-15
nnp-3

john

j.

bush

new
york

nov.

robert

e.

noriega

san

francisco

    personal pronouns (prp):

prp-0
prp-1
prp-2

it
it
it

he
he

them

sept.

james

l.

peters

wall
street

i

they
him

treebanks and linguistic theory

node labels:

phrase categories

oc
vp

tiger treebank
crossing branches for

discontinuous constituency types

edge labels:

syntactic functions

mo
pp
nk
n  chsten
adja
sup.dat.
sg.neut
nahe

nk
jahr
nn
dat.
pl.neut
jahr

apprart

ac
im
dat
in

hd

s
sb

np

nk

regierung

nk
die
art
nom.
sg.fem
die

nk
will
ihre
vmfin
pposat
3.sg.
acc.
pres.ind
pl.masc
wollen
ihr
annotation on word level:
morphology, lemmata

part-of-speech,

nn
nom.
sg.fem
regierung

hd

oa
np

nk

reformpl  ne

nn
acc.
pl.masc
plan

umsetzen
vvinf

inf

umsetzen

.
$.

4

the latest parsing results   

parser

klein & manning unlexicalized
2003

matsuzaki et al. simple em
latent states 2005

charniak generative (   maxent
inspired   ) 2000

petrov and klein naacl 2007

charniak & johnson
discriminative reranker 2005

f1

f1

    40 words

all words

86.3

86.7

90.1

90.6

92.0

85.7

86.1

89.5

90.1

91.4

treebanks

    treebank and parsing experimentation has been
   

dominated by the id32
if you parse other languages, parser
performance generally heads, downhill, even if
you normalize for treebank size:

    wsj small
    chinese tb

82.5%
75.2%

    what do we make of this?  we   re changing

several variables at once.
       is it harder to parse chinese, or the chinese

treebank?    

[levy and manning 2003]

    what is the basis of the structure of the penn

english treebank anyway?

0510152025303540npvpppadvpsadjpsbarqpwhnppid56xsinvprtwhppsqconjpfragnacucpwhadvpintjsbarqrrcwhadjpxrootlst prague dependency bank

treebanks

chce
wants
sb

investovat
to-invest
act.vol.t

obj

kdo
who
sb
act.t

ste
obj

hundred
restr.f

korun
crowns
atr
pat.f

do
to
auxp

automobilu

car
adv
dir.f

annotation on word level:
lemmata, morphology
syntactic functions
dependency structure

semantic information 
on constituent roles, 
theme/rheme, etc.

    a good treebank has an extensive manual for

each stage of annotation
    consistency is often at least as important as being right
    but is what   s in these treebanks right?

    id121:

    has  n   t

i     ll
    hyphenated terms

    cancer-causing/jj asbestos/nn
    the back-on-terra-firma/jj toast/nn
    the/dt nerd-and-geek/jj club/nn

treebank: pos

criteria for part of speech

    some id52 errors reflect not only human
inconsistency but problems in the definition of
pos tags, suggestive of clines/blends

    example: near
   
    today is it an adjective or a preposition?

in middle english, an adjective

    the near side of the moon
    we were near the station

    not just a word with multiple parts of speech!

evidence of blending:
    i was nearer the bus stop than the train

where a rule was followed:
   marginal prepositions   

    fowler (1926):    there is a continual change

going on by which certain participles or
adjectives acquire the character of prepositions
or adverbs, no longer needing the prop of a
noun to cling to   
it is easy to have no tagging ambiguity in such
cases:

   
    id32 (santorini 1991):

       putative prepositions ending in -ed or -ing should be

tagged as past participles (vbn) or gerunds (vbg),
respectively, not as prepositions (in).
    according/vbg to reliable sources
    concerning/vbg your request of last week   

   

in some cases functional/notional tagging
dominates structure in id32, even
against explicit instructions to the contrary:

    worth: 114 instances

    10 tagged in (8 placed in adjp!)
    65 tagged jj (48 in adjp, 13 in pp, 4 nn/np errors)
    39 tagged nn (2 in/jj errors)

    linguist hat on: i tend to agree with in choice

(when not a noun):
    tagging accuracy only 41% for worth!

preposition in     verb (vbg)

    but this makes no real sense
    rather we see    a development caught in the act    (fowler

1926)

wall

trip to church

    they moved slowly, toward the main gate, following the
    repeat the instructions following the asterisk
    this continued most of the week following that ill-starred
    he bled profusely following circumcision
    following a telephone call, a little earlier, winter had said
   
   

   
in: during [cf. endure], pending, notwithstanding
??: concerning, excepting, considering,    

5

