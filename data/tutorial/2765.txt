representa)on	learning	for	

reading	comprehension	

russ	salakhutdinov	
machine learning department
carnegie mellon university

canadian institute for advanced research

joint work with

bhuwan	dhingra,	zhilin	yang,	ye	yuan,	junjie	hu,	

hanxiao	liu,	and	william	cohen

talk	roadmap	

       mul)plica)ve	and	fine-grained	ajen)on		

       incorpora)ng	knowledge	as	explicit	

memory	for	id56s	

       genera)ve	domain-adap)ve	nets	

who-did-what	dataset	

       document:	      arrested	illinois	governor	rod	blagojevich	and	his	

chief	of	sta   	john	harris	on	corrup)on	charges	   	included	
blogojevich	allegedly	conspiring	to	sell	or	trade	the	senate	seat	lez	
vacant	by	president-elect	barack	obama      	

       query:	president-elect	barack	obama	said	tuesday	he	was	not	

aware	of	alleged	corrup)on	by	x	who	was	arrested	on	charges	of	
trying	to	sell	obama   s	senate	seat.	

       answer:	rod	blagojevich	

onishi, wang, bansal, gimpel, mcallester, emnlp, 2016

introduc)on	

      	the	cloze-style	q/a	task	involves	tuples	of	the	form		

     
     
     

				is	a	document	(paragraph	/	context)	
				is	a	ques)on	over	the	contents	of	that	document	
				is	the	answer	to	this	query	

      	the	answer	comes	from	a	   xed	vocabulary				.	
      	task:	given	a	document	query	pair												   nd															which	
answers			.	

prior	work	

      	lstms	with	ajen)on:	hermann	et	al.,2015,	hill	et	al,	2015,	chen	et	al,	
2016,	bahdanau	et	al.,	2014	
      	memory	networks:	weston	et	al.,	2014,	sukhbaatar	et	al.,	2015,	
bajgar	et	al.,	2016		
      		ajen)on	sum	reader:	kadlec	et	al.,	2016,	cui	wt	al,	2016		
      	dynamic	en)ty	representa)ons:	kobayashi	et	al.,	2016	
      	neural	seman)c	encoders,	munkhdalai	&	yu,	2016	
      	itera)ve	ajen)ve	reader,	sordoni	et	al.,	2016		
      	reasonet,	shen	et	al.,	2016	

recurrent	neural	network		

nonlinearity		

hidden	state		at	
previous	)me	step	

input	at	)me	
step	t	

h1	

h2	

h3	

x1	

x2	

x3	

mul)plica)ve	integra)on	

      	replace	

      	with		

      	or	more	generally	

wu et al., nips 2016

mul)plica)ve	integra)on	

      	or	more	generally	

wu et al., nips 2016

represen)ng	document/query	

      	let																																											denote	denote	the	token	embeddings	
of	the	document.	
      	let																																											denote	embeddings	of	the	query.	

      	|d|	and	|q|	denote	the	document	and	query	lengths	respec)vely	

represen)ng	document/query	

      	forward	id56	reads	sentences			
from	lez	to	right:	

      	backward	id56	reads	sentences	
from	right	to	lez:	

      	the	hidden	states	are	then	concatenated:	

represen)ng	document/query	

      		use	grus	to	encode	a	
document	and	a	query:	

      	note	that,	for	example,	q	is	a	
matrix				

      	we	can	then	use	gated	ajen)on	mechanism:			

gated	ajen)on	mechanism		

      	for	each	token	d	in	d,	we	form	a	token-speci   c	representa)on	
of	the	query:	

     

use	the	element-wise	mul)plica)on	
operator	to	model	the	interac)ons	
between								and	

 dhingra, liu, yang, cohen, salakhutdinov, acl 2017

mul)-hop	architecture	

      	many	qa	tasks	require	reasoning	over	mul)ple	sentences.		
      	need	to	performs	several	passes	over	the	context.	

 dhingra, liu, yang, cohen, salakhutdinov, acl 2017

output	model	

      	id203	that	a	par)cular	token	in	the	document	answers	
the	query:	

     

take	an	inner	product	between	the	query	embedding	and	the	output	
of	the	last	layer:	

      	the	id203	of	a	par)cular	candidate														is	then	
aggregated	over	all	document	tokens	which	appear	in	c:			

pointer sum attention of kadlec et al., 2016

set	of	posi)ons	where	a	
token	in	c	appears	in	the	
document	d.	

output	model	

      	the	id203	of	a	par)cular	candidate														is	then	
aggregated	over	all	document	tokens	which	appear	in	c:			

      	the	candidate	with	maximum	id203	is	selected	as	the	
predicted	answer:	

      		use	cross-id178	loss	between	the	predicted	probabili)es	
and	the	true	answers.	

datasets	

      	we	have	looked	at	6	datasets:	

      	the	word	lookup	was	ini)alized	with	glove	vectors	(pennington	et	
al.,	2014)		

a   ect	of	mul)plica)ve	ga)ng		

      	performance	of	di   erent	ga)ng	func)ons	on	   who	did	
what   	(wdw)	dataset.	

analysis	of	ajen)on		

       context:	      arrested	illinois	governor	rod	blagojevich	and	his	chief	of	sta   	john	
harris	on	corrup)on	charges	   	included	blogojevich	allegedly	conspiring	to	sell	
or	trade	the	senate	seat	lez	vacant	by	president-elect	barack	obama      	
       query:	   president-elect	barack	obama	said	tuesday	he	was	not	aware	of	

alleged	corrup)on	by	x	who	was	arrested	on	charges	of	trying	to	sell	obama   s	
senate	seat.   	

       answer:	rod	blagojevich	

layer	1	

layer	2	

analysis	of	ajen)on		

       context:	      arrested	illinois	governor	rod	blagojevich	and	his	chief	of	sta   	john	
harris	on	corrup)on	charges	   	included	blogojevich	allegedly	conspiring	to	sell	
or	trade	the	senate	seat	lez	vacant	by	president-elect	barack	obama      	
       query:	   president-elect	barack	obama	said	tuesday	he	was	not	aware	of	

alleged	corrup)on	by	x	who	was	arrested	on	charges	of	trying	to	sell	obama   s	
senate	seat.   	

       answer:	rod	blagojevich	

layer	1	

layer	2	

code	+	data:	hjps://github.com/bdhingra/ga-reader	

words	vs.	characters		

      	word-level	representa)ons	are	good	at	learning	the	seman)cs	
of	the	tokens	
      	character-level	representa)ons	are	more	suitable	for	modeling	
sub-word	morphologies	(   cat   	vs.	   cats   )	

      word-level	representa)ons	are	obtained	from	a	learned	lookup	table	
     
character-level	representa)ons	are	usually	obtained	by	applying	id56	
or	id98	

      	hybrid	word-character	models	have	been	shown	to	be	successful	
in	various	nlp	tasks	(yang	et	al.,	2016a,	miyamoto	&	cho	(2016),	ling	et	al.,	2015)	
      	commonly	used	method	is	to	concatenate	these	two	
representa)ons		

fine-grained	ga)ng	

      	fine-grained	ga)ng	mechanism:		

character	-	level	
representa)on	

ga)ng	

word-	level	
representa)on	

addi)onal	features:	named	en)ty	tags,	part-	of-
speech	tags,	document	frequency	vectors,	word	
look-up	representa)ons	

yang et al., iclr 2017

children   s	book	test	(cbc)	dataset	

yang et al., iclr 2017

words	vs.	characters	
      	high	gate	values:	character-level	representa)ons	
      	low	gate	values:	word-level	representa)ons.		

yang et al., iclr 2017

talk	roadmap	

       mul)plica)ve	and	fine-grained	ajen)on		

       linguis)c	knowledge	as	explicit	memory	

for	id56s	

       genera)ve	domain-adap)ve	nets	

broad-context	language	modeling 
her plain face broke into a huge smile when she saw terry. 
   terry!    she called out. 
she rushed to meet him and they embraced. 
   hon, i want you to meet an old friend, owen mckenna. 
owen, please meet emily.'' 
she gave me a quick nod and turned back to x

 lambada dataset, paperno et al., 2016

broad-context	language	modeling 
her plain face broke into a huge smile when she saw terry. 
   terry!    she called out. 
she rushed to meet him and they embraced. 
   hon, i want you to meet an old friend, owen mckenna. 
owen, please meet emily.'' 
she gave me a quick nod and turned back to x

 lambada dataset, paperno et al., 2016

broad-context	language	modeling 
her plain face broke into a huge smile when she saw terry. 
   terry!    she called out. 
she rushed to meet him and they embraced. 
   hon, i want you to meet an old friend, owen mckenna. 
owen, please meet emily.'' 
she gave me a quick nod and turned back to x

x = terry

 lambada dataset, paperno et al., 2016

incorpora)ng	prior	knowledge 

her	plain	face	broke	into	
a	huge	smile	when	she	
saw	terry.		   terry!   	she	
called	out.		she	rushed	
to	meet	him	and	they	
embraced.		   hon,	i	want	
you	to	meet	an	old	
friend,	owen	mckenna.	
owen,	please	meet	
emily.'   		she	gave	me	a	
quick	nod	and	turned	
back	to	x	

core	nlp	

coreference	
dependency	parses	

freebase	

en)ty	rela)ons	

id138	

word	rela)ons	

recurrent	neural	network	

text	representa)on	

incorpora)ng	prior	knowledge 

mary 

got 

the 

football 

she 

went 

to 

the 

kitchen 

she 

left 

the 

ball 

there 

id56	
coreference	
hyper/hyponymy	

 dhingra, yang, cohen, salakhutdinov 2017

incorpora)ng	prior	knowledge 

mary 

got 

the 

football 

she 

went 

to 

the 

kitchen 

she 

left 

the 

ball 

there 

id56	
coreference	
hyper/hyponymy	

memory	as	acyclic	graph	
encoding	(mage)	-	id56	

mt
. . .

e|e|

e1

h0
h1
...
ht 1
xt

mt+1

gt

	

n
n
r

ht

 dhingra, yang, cohen, salakhutdinov 2017

forward/backward	dag 

mary 

got 

the 

football 

forward	subgraph	

she 

went 

she 

to 

left 

the 

the 

kitchen 

ball 

there 

mary 

got 

the 

football 

backward	subgraph	

she 

went 

she 

to 

left 

the 

the 

kitchen 

ball 

there 

mul)ple	sequences 

mary 

got 

the 

football 

she 

went 

she 

to 

left 

the 

the 

kitchen 

ball 

there 

where 

is 

the 

football 

? 

results	with	coreference 

model	

previous	best	
ga	
ga	+	mage	

babi	qa		
(1k	training)	

lambada	

90.1	
75.2	
91.3	

49.0	
50.0	
51.6	

id98	

77.9	
77.9	
78.6	

      	plan	to	incorporate	rela)ons	beyond	coreference	
      	ajen)on	over	edge	types	

learned	representa)on 

talk	roadmap	

       mul)plica)ve	and	fine-grained	ajen)on		

       linguis)c	knowledge	as	explicit	memory	

for	id56s	

       genera)ve	domain-adap)ve	nets	

extrac)ve	ques)on	answering 

in	meteorology,	precipita)on	is	any	product	of	the	
condensa)on	of	atmospheric	water	vapor	that	falls	
under	gravity.	the	main	forms	of	precipita)on	include	
drizzle,	rain,	sleet,	snow,	and	hail   	precipita)on	forms	
as	smaller	droplets	coalesce	via	collision	with	other	rain	
drops	or	ice		crystals	within	a	cloud.	short,	intense	
periods	of	rain	in	scajered	loca)ons	are	called	
   showers   		

		what	causes	precipita)on	to	fall?	
		gravity					

      	given	a	paragraph/ques)on,	extract	a	span	of	text	as	the	answer	
      	expensive	to	obtain	large	labeled	datasets	
      	sota	approaches	rely	on	large	labeled	datasets	

 squad dataset, rajpurkar et al., 2016

leverage	unlabeled	text 

      	almost	unlimited	unlabeled	text.	

semi-supervised	qa 

labeled	qa	pairs	

unlabeled	text	

qa	model	

extrac)ve	ques)on	answering 

in	meteorology,	precipita)on	is	any	product	of	the	
condensa)on	of	atmospheric	water	vapor	that	falls	
under	gravity.	the	main	forms	of	precipita)on	include	
drizzle,	rain,	sleet,	snow,	and	hail   	precipita)on	forms	
as	smaller	droplets	coalesce	via	collision	with	other	rain	
drops	or	ice		crystals	within	a	cloud.	short,	intense	
periods	of	rain	in	scajered	loca)ons	are	called	
   showers   		

		what	causes	precipita)on	to	fall?	
		gravity					

      	use	pos/ner/parsing	to	extract	possible	answer	chunks	
      	anything	can	be	the	answers	
      	we	will	assume	that	answers	are	available.	

genera)ng	ques)ons 

labeled	data	

unlabeled	data	

p,q,a	

p,	a	

generator	g:	

q	

from	(p,	a)										q	
id195	with	copy	
mechanism	

discriminator	d:	combine	to	

train	a	qa	model	

from	(p,	q)										a	
ga	reader	

baseline	1:	context	ques)ons 

in	meteorology,	precipita)on	is	any	product	of	the	
condensa)on	of	atmospheric	water	vapor	that	falls	
under	gravity.	the	main	forms	of	precipita)on	include	
drizzle,	rain,	sleet,	snow,	and	hail   	precipita)on	forms	
as	smaller	droplets	coalesce	via	collision	with	other	rain	
drops	or	ice		crystals	within	a	cloud.	short,	intense	
periods	of	rain	in	scajered	loca)ons	are	called	
   showers   		

		what	causes	precipita)on	to	fall?	
		gravity					

generate	a	context	ques)on	for	the	
answer	   gravity   	

water	vapor	that	falls	under		
the	main	forms	of	

baseline	2:	gans 

answer	
(reconstruc)on)	

true	or	fake	
ques)on?	

d   	

d	

paragraph,	ques)on	

g	

paragraph,	answer	

 goodfellow et al., 2014, ganin et al. 2014 , xia et al., 2016

genera)ve	domain-adap)ve	nets	(gdans) 

unlabeled	data	

labeled	data	

train	d	

train	d	

train	g	

	johnson et al., 2016; chu et al., 2017

	yang, hu, salakhutdinov, cohen, acl 2017

genera)ve	domain-adap)ve	nets	(gdans) 

unlabeled	data	

labeled	data	

generator	as	a	data	domain	

condi)on	discriminator	d	on	domains	

adversarial	training	for	g	

train	d	

train	d	

train	g	

	johnson et al., 2016; chu et al., 2017

	yang, hu, salakhutdinov, cohen, acl 2017

examples	

context:	      an	addi)onal	warming	of	the	earth   s	surface.	they	calculate	with	

con   dence	that	c02	has	been	responsible	for	over	half	the	enhanced	greenhouse	
e   ect.	they	predict	that	under	a	   business	as	usual   	scenario,      	

answer:	over	half	
quesjon:	what	the	enhanced	greenhouse	e   ect	that	co2	been	responsible	for?	
ground	true	q:	how	much	of	the	greenhouse	e   ect	is	due	to	carbon	dioxide?	

context:	      	in	0000	,	bankamericard	was	renamed	and	spun	o   	into	a	separate	

company	known	today	as	visa	inc.   	

answer:	visa	inc	.	
quesjon:	what	was	the	separate	company	bankamericard?	
ground	true	q:	what	present-day	company	did	bankamericard	turn	into?	

	yang, hu, salakhutdinov, cohen, acl 2017

squad	dataset 

      	squad	dataset:	87,636	training,	10,600	development	instances	
      	use	50k	unlabelled	examples.		
method	
supervised	
context		
gen	+	gan	

exact	matching		

test	f1	
0.3815	
0.4515	
0.4373		
0.4802	
0.5722	
0.5740		
0.5590	
	0.5831	

0.2492	
0.2966	
0.2885	
0.3218	
0.4187	
0.4195	
0.4044	
0.4267	

gdan	

supervised	
context		
gen	+	gan	

gdan	

labeling	rate	
0.1		
0.1	
0.1	
0.1	
0.5	
0.5	
0.5	
0.5	

varia)onal	autoencoder	(vae)	

      	transform	samples	from	some	simple	distribu)on	(e.g.	normal)	
to	the	data	manifold:	

genera)ve	
process	

determinis)c	
neural	network	

the	movie	was	
awful	and	boring		

knigma and welling, 2014

vae	for	text	genera)on	

      	sample	c,	   x	z.		

hu, yang, liang, salakhutdinov, xing, 2017

vae	for	text	genera)on	

      	sample	z,	   x	c.		

hu, yang, liang, salakhutdinov, xing, 2017

thank	you	

