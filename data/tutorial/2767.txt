   #[1]rss [2]slideshare search [3]alternate [4]alternate [5]alternate
   [6]alternate [7]alternate [8]alternate [9]slideshow json oembed profile
   [10]slideshow xml oembed profile [11]alternate [12]alternate
   [13]alternate

   (button)

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our [14]user
   agreement and [15]privacy policy.

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our
   [16]privacy policy and [17]user agreement for details.

   [18]slideshare [19]explore search [20]you

     * [21]linkedin slideshare

     * [22]upload
     * [23]login
     * [24]signup

     *
     * ____________________ (button) submit search

     * [25]home
     * [26]explore

     * [27]presentation courses
     * [28]powerpoint courses
     *
     * by [29]linkedin learning

   ____________________
   successfully reported this slideshow.

   we use your linkedin profile and activity data to personalize ads and
   to show you more relevant ads. [30]you can change your ad preferences
   anytime.
   neural text embeddings for information retrieval (wsdm 2017)

   wsdm 2017 tutorial neural text embeddings for ir bhaskar mitra and nick
   craswell download slides from: http://bit.ly/neuir...

   check out the full tutorial: https://arxiv.org/abs/1705.01509

   sigir papers with title words: neural, embedding, convolution,
   recurrent, lstm 1% 4% 8% 11% 20% 0% 5% 10% 15% 20% 25% sigi...

   neural methods for information retrieval this tutorial mainly focuses
   on:     ranked retrieval of short and long texts, give...

   today   s agenda 1. ir fundamentals 2. id27s 3. id27s
   for ir 4. deep neural nets 5. deep neural nets for...

   bhaskar mitra @underdoggeek bmitra@microsoft.com nick craswell
   @nick_craswell nickcr@microsoft.com send us your questions ...

   fundamentals of retrieval chapter 1

   information retrieval (ir) terminology this tutorial: using neural
   networks in the retrieval system to improve relevance. ...

   ir applications document ranking factoid id53 query
   keywords natural language question document web page, ne...

   challenges in (neural) ir [slide 1/3]     vocabulary mismatch q: how many
   people live in sydney?     sydney   s population is 4....

   challenges in (neural) ir [slide 2/3]     q and d vary in length     models
   must handle variable length input     relevant docs ...

   challenges in (neural) ir [slide 3/3]     need to learn q-d relationship
   that generalizes to the tail     unseen q     unseen d ...

   representation of words chapter 2

   one-hot representation (local) dim = |v| sim(banana,mango) = 0 0 0 0 0
   0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 banana mang...

   hinton, geoffrey e. distributed representations. technical report
   cmu-cs-84-157, 1984

   context-based distributed representation turney and pantel. from
   frequency to meaning: vector space models of semantics. j...

   banana nanana#ba na# ban banana (grows) (tree)(yellow) (on) (africa)
   banana doc7 doc9doc2 banana (grows, +1) (tree, +3)(ye...

   distributional methods use distributed representations distributed and
   distributional     distributed representation: vector...

   vector space models for a given task: choose matrix, choose             
   weighting. could be binary, could be raw counts. example w...

   distributed word representation overview     next we cover
   lower-dimensional dense representations     including id97. que...

   let   s consider the following example    we have four (tiny) documents,
   document 1 :    seattle seahawks jerseys    document 2 : ...

   using word-document context seattle document 1 document 3 document 2
   document 4 seahawks denver broncos similar similar th...

   using word-worddist context seattle (seattle, -1) (denver, -1)
   (seahawks, +1) (broncos, +1) (jerseys, + 1) (jerseys, + 2) ...

   let   s consider another example       seattle map       seattle weather   
      seahawks jerseys       seahawks highlights       seattle seahawks...

   using word-word context seattle seattle denver seahawks broncos jerseys
   highlights wilson sherman seahawks denver broncos ...

   paradigmatic vs syntagmatic do we choose one axis or the other, or
   both, or something else? can we learn a general-purpose...

   embeddings (distributed) dense. dim = 200 (for example) banana mango
   dog

   latent semantic analysis v: vocabulary, d: set of documents, x: sparse
   matrix |v| x |d| xij = tf-idf singular value decomp...

   latent semantic analysis   1,    ,   l: singular values u1,    , ul: left
   singular vectors v1,    , vl: right singular vectors the...

   id97 goal: simple (shallow) neural model learning from billion
   words scale corpus predict middle word from neighbors w...

   skip-gram predict neighbor         +     given word          maximizes following
   average log prob. win wout wt wt+j                                         = 1          =1...

   continuous bag-of-words predict word given bag-of-neighbors modify the
   skip-gram id168.                                         = 1          =1      lo...

   word analogies with id97 [king]     [man] + [woman]     [queen]
   (mikolov et al., 2013)

   word analogies can work in underlying data too seattle seattle denver
   seahawks broncos jerseys highlights wilson sherman s...

   a matrix interpretation of id97 skip-gram looks like this: if we
   aggregate over all training samples                                            =     ...

   glove variety of windows sizes and weighting adagrad w0 w1 w2     wj    
   w|v| w0 w1 w2     wi xij     w|v| (pennington et al., 201...

   discussion of word representations     representations: weighted counts,
   svd, neural embedding     use similar data (w-d, w-w)...

   id27s for ir chapter 3

   traditional ir feature design     inverse document frequency     term
   frequency     adjust tf model for doc length n r n r rober...

   how to incorporate embeddings a. extend traditional ir models     term
   weighting, language model smoothing, translation of v...

   term weighting using id27s      =           (term recall)     fraction
   of positive docs with t     the      and      were missing i...

   traditional ir model: query likelihood     id38 approach to
   ir is quite extensible p                                            p      can ...

   generalized language model ganguly, roy, mitra, and jones. word
   embedding based generalized language model for information...

   neural language translation model zuccon, koopman, bruza, and
   azzopardi. "integrating and evaluating neural id27...

   b. id183 both passages have same number of gold query
   matches. yet non-query green matches can be a good evidenc...

   id183 using w2v identify expansion terms using w2v cosine
   similarity 3 different strategies: pre-retrieval, post...

   id183 using w2v     related paper. distance      has a sigmoid
   transform. first model:     their second model uses the ...

   optimizing the query vector zamani and croft. estimating embedding
   vectors for queries. international conference on the th...

   global vs. local embedding spaces train w2v on documents from first
   round of retrieval fine-grained word sense disambiguat...

   c. ir models in the embedding space     q: bag of word vectors     d: bag
   of word vectors     considerations:     deal with variab...

   comparing short texts     weighted semantic network     related to word
   alignment     each word in longer text is connected to i...

   word mover   s distance kusner, sun, kolkin and weinberger. from word
   embeddings to document distances. icml 2015 see also: ...

   bounds on word mover   s distance wcd <= rwmd <= wmd wcd: word centroid
   distance rwmd: relaxed wmd prefetch and prune algori...

   centroid and related techniques     goal: bag of vectors     single fixed
   size vector     centroid, weighted centroid (or sum [v...

   what if i told you everyone using w2v is throwing half the model away?
   two sets of embeddings are trained (win and wout) b...

   notions of similarity in-out captures more topical notion of similarity
   than in-in and out-out effect is exaggerated when ...

   dual embedding space model compare query terms with every document term
   equivalent to taking centroid of all term vectors ...

   telescoping     desm also works well when    telescoping    i.e. reranking a
   small set     for example, desm can confuse oxford an...

   because cambridge is not "an african even-toed ungulate mammal"
   https://github.com/bmitra-msft/demos/blob/master/notebooks...

   paragraph2vec pv-dm a mix of lsa and and w2v style training data, less
   sparse than training just on paragraph id pv-dbow e...

   models of similar flavour grbovic, djuric, radosavljevic, silvestri and
   bhamidipati. context-and content-aware embeddings ...

   adapting pv-dbow for ir negative sampling using idf instead of corpus
   frequency l2 id173 constraint on the norm t...

   discussion of embeddings in ir     the right mix of lexical and semantic
   matching is important     in non-telescoping settings...

   deep neural networks chapter 4

   a primer on neural networks chains of parameterized linear transforms
   (e.g., multiply weight, add bias) followed by non-li...

   visual motivation for hidden units consider the following    toy   
   challenge for classifying tech queries: vocab: {surface, k...

   visual motivation for hidden units or more succinctly    input features
   hidden layer label surface kerberos book library h1 ...

   why adding depth helps deeper networks can split the input space in
   many (non-independent) linear regions than shallow net...

   why adding depth helps http://playground.tensorflow.org

   neural models for text but how do you feed text to a neural network?
   dogs have owners, cats have staff.

   local representations of input text char-level models d o g s h a v e o
   w n e r s c a t s h a v e s t a f f one-hotvectors...

   local representations of input text word-level models w/ bag-of-chars
   per word d o g s h a v e o w n e r s c a t s h a v e...

   # d o g s # # h a v e # # o w n e r s # # c a t s # # h a v e # # s t a
   f f # local representations of input text word-lev...

   local representations of input text word-level models w/ pre-trained
   embeddings d o g s h a v e o w n e r s c a t s h a v ...

   shift-invariant neural operations detecting a pattern in one part of
   input space is same as detecting it in another (also ...

   convolution move the window over the input space each time applying the
   same cell over the window a typical cell operation...

   pooling move the window over the input space each time applying an
   aggregate function over each dimension in within the wi...

   convolution w/ global pooling stacking a global pooling layer on top of
   a convolutional layer is a common strategy for gen...

   recurrent neural network similar to a convolution layer but additional
   dependency on previous hidden state a simple cell o...

   recursive nn or tree-id56 shared weights among all a levels of the tree
   cell can be an lstm or as simple as     =               +      fu...

   autoencoders unsupervised models trained to minimize reconstruction
   errors information bottleneck method (tishby et al., 1...

   computation networks the    lego    approach to specifying dnn
   architectures library of computation nodes, each node defines l...

   really deep neural networks (larsson et al., 2016) (he et al., 2015)
   (szegedy et al., 2014)

   dnns for ir chapter 6

   taxonomy of models neural networks can be used to: 1. generate query
   representation, 2. generate id194, ...

   you   ve already seen    query text id183 using embeddings doc
   text generate doc term vector query likelihood query ...

   what does ad-hoc ir data look like? search queries document corpus user
   interaction / click data human annotated labels tr...

   search queries document corpus user interaction / click data human
   annotated labels traditional ir uses human labels as gr...

   search queries document corpus user interaction / click data human
   annotated labels in industry: document corpus: billions...

   in academia: document corpus: few billion query corpus: few million but
   other sources (e.g., wiki titles) can be used labe...

   a pessimistic summary            water, water, everywhere, nor any drop to
   drink.^

   levels of supervision (or the optimistic view) unsupervised     train
   embeddings on unlabeled corpus and use in traditional ...

   semantic hashing autoencoder minimizing document reconstruction error
   vocab: 2k popular words (stopwords removed) in/out =...

   deep semantic similarity model siamese network trained e2e on query and
   document title pairs relevance is estimated by cos...

   convolutional dssm replace bag-of-words assumption by concatenating
   term vectors in a sequence on the input (shen et al., ...

   rich space of neural architectures (palangi et al., 2015) (kalchbrenner
   et al., 2014) (denil et al., 2014) (kim, 2014) (se...

   rich space of neural architectures (kalchbrenner et al., 2014) input
   representation based on pre-trained id27s w...

   rich space of neural architectures uses the did98 model to learn
   document embeddings first did98 on id27s to get s...

   convolutional model for text classification and sentiment prediction
   compares strategies for input word representation usi...

   stacked alternate layers of convolution and max-pooling rich space of
   neural architectures (hu et al., 2014) depth of netw...

   rich space of neural architectures convolutional model for short text
   ranking pre-trained id27s for input, not t...

   rich space of neural architectures replace convolution-pooling w/ lstm
   (palangi et al., 2015) lstm-dssm

   rich space of neural architectures recently various tree-structured
   id56s / lstms have also been explored in the context of...

   interaction matrix alternative to siamese networks interaction matrix
   x, where xi,j is obtained by comparing the the ith w...

   ad-hoc retrieval using local representation deep relevance matching
   model (drmm) for ad-hoc retrieval argues exact matchin...

   (mitra et al., 2017) ad-hoc retrieval using local and distributed
   representation argues both    lexical    and    semantic    matc...

   (mitra et al., 2017) ad-hoc retrieval using local and distributed
   representation argues both    lexical    and    semantic    matc...

   using our earlier taxonomy of models    query text generate query term
   vector doc text generate doc term vector generate int...

   (mitra et al., 2017) ad-hoc retrieval using local and distributed
   representation distributed model not likely to have good...

   the library vs. librarian dilemma is there a hidden assumption in
   distributed models that they know about everything in th...

   big vs. small data regimes big data seems to be more crucial for models
   that focus on good representation learning for tex...

   implementing duet using the cognitive toolkit (cntk)
   https://github.com/bmitra-msft/ndrm/blob/master/notebooks/duet.ipynb

   other applications chapter 7

   query recommendations hierarchical sequence-to-sequence model for term-
   by-term query generation similar to ad-hoc ranking...

   query auto-completion given a (rare) query prefix retrieve relevant
   suffixes from a fixed set cdssm model trained on query...

   analogies for session modelling cdssm vectors when trained on
   prefix-suffix pairs or session query pairs are amenable to a...

   analogies for session modelling this property was shown to be useful
   for modelling session context "london"     "things to d...

   id72 dssm model trained under a multi-task setting e.g.,
   query classification and ranking lower layer weigh...

   neural approach for knowledge-based ir incorporating distributed
   representations of kb entities in deep neural retrieval m...

   conversational response retrieval ranking responses for conversational
   systems interesting challenges in modelling utteran...

   proactive retrieval given current task context generate a proactive
   query and retrieve recommendations in this particular ...

   multimodal retrieval this tutorial is focused on text but neural
   representation learning is also leading towards breakthro...

   useful resources pyndri for python: https://github.com/cvangysel/pyndri
   luandri for lua / torch: https://github.com/bmitra...

   bhaskar mitra @underdoggeek bmitra@microsoft.com nick craswell
   @nick_craswell nickcr@microsoft.com send us your questions ...

   neural text embeddings for information retrieval (wsdm 2017)

   neural text embeddings for information retrieval (wsdm 2017)
   upcoming slideshare
   []
   loading in    5
     
   [] 1
   (button)
   1 of 126 (button)
   (button) (button)
   like this presentation? why not share!
     * share
     * email
     *
     *

     * [31]neural models for information retri... neural models for
       information retri... by bhaskar mitra 1084 views
     * [32]   probabilistic logic programs and t...    probabilistic logic
       programs and t... by diannepatricia 1135 views
     * [33]l06 stemmer and id153 l06 stemmer and id153
       by ananth 1484 views
     * [34]language models for information ret... language models for
       information ret... by nik spirin 765 views
     * [35]matthew marge - 2017 - exploring va... matthew marge - 2017 -
       exploring va... by association for c... 438 views
     * [36]construisons ensemble le chatbot ba... construisons ensemble le
       chatbot ba... by linagora 240 views

   (button)

   share slideshare
     __________________________________________________________________

     * [37]facebook
     * [38]twitter
     * [39]linkedin

   embed
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   size (px)
   start on
   [x] show related slideshares at end
   wordpress shortcode ____________________
   link ____________________

neural text embeddings for information retrieval (wsdm 2017)

   15,149 views

     * (button) share
     * (button) like
     * (button) download
     * ...
          +

   [40]bhaskar mitra

[41]bhaskar mitra

   , principal applied scientist at microsoft
   [42]follow

   (button) (button) (button)

   published on feb 6, 2017

   slides from neural text embeddings for information retrieval tutorial
   at wsdm 2017
   (button) ...

   published in: [43]technology

     * [44]3 comments
     * [45]72 likes
     * [46]statistics
     * [47]notes

     * full name
       full name
       comment goes here.
       12 hours ago   [48]delete [49]reply [50]block
       are you sure you want to [51]yes [52]no
       your message goes here

   no profile picture user
   ____________________
   [53](button) post
     * [54]kendricklamost
       [55]kendricklamost
       sex in your area for one night is there tinyurl.com/hotsexinarea
       copy and paste link in your browser to visit a site)
       3 weeks ago    [56]reply
       are you sure you want to  [57]yes  [58]no
       your message goes here
     * [59]kendricklamost
       [60]kendricklamost
       girls for sex are waiting for you https://bit.ly/2tq8uay
       1 month ago    [61]reply
       are you sure you want to  [62]yes  [63]no
       your message goes here
     * [64]kendricklamost
       [65]kendricklamost
       meetings for sex in your area are there: https://bit.ly/2tq8uay
       1 month ago    [66]reply
       are you sure you want to  [67]yes  [68]no
       your message goes here

     * [69]anuragmanutdreddy
       [70]anurag reddy gaddam venkat , seeking full time data scientist
       position at bowling green state university
       1 month ago
     * [71]anuragmanutdreddy
       [72]anurag reddy gaddam venkat , seeking full time data scientist
       position at bowling green state university
       1 month ago
     * [73]markohelin
       [74]marko helin , software engineering at spotify at software
       engineer
       2 months ago
     * [75]smomtazi
       [76]smomtazi
       5 months ago
     * [77]gmarchetti
       [78]giovanni marchetti , principal program manager at microsoft at
       microsoft
       6 months ago

   [79]show more
   no downloads
   views
   total views
   15,149
   on slideshare
   0
   from embeds
   0
   number of embeds
   763
   actions
   shares
   0
   downloads
   1,549
   comments
   3
   likes
   72
   embeds 0
   no embeds
   no notes for slide





























     local representation
   distributed representation
   one dimension for    banana       banana    is a pattern
   brittle under noise more robust to noise
   precise near    mango   ,    pineapple   . (nuanced)
   add vocab     add dimensions add vocab     generate more vectors
   k dimensions     k items k dimensions     2k    regions   



























































































































































     samuel taylor coleridge

neural text embeddings for information retrieval (wsdm 2017)

    1. 1. wsdm 2017 tutorial neural text embeddings for ir bhaskar mitra
       and nick craswell download slides from:
       http://bit.ly/neuirtutorial-wsdm2017
    2. [80]2. check out the full tutorial:
       https://arxiv.org/abs/1705.01509
    3. [81]3. sigir papers with title words: neural, embedding,
       convolution, recurrent, lstm 1% 4% 8% 11% 20% 0% 5% 10% 15% 20% 25%
       sigir 2014 (accepted) sigir 2015 (accepted) sigir 2016 (accepted)
       sigir 2017 (submitted) sigir 2018 (optimistic?) neural network
       papers @ sigir deep learning amazingly successful on many hard
       applied problems. dominating multiple fields: (but also beware the
       hype.) christopher manning. understanding human language: can nlp
       and deep learning help? keynote sigir 2016 (slides 71,72) 0% 5% 10%
       15% 20% 25% sigir 2014 (accepted) sigir 2015 (accepted) sigir 2016
       (accepted) sigir 2017 (submitted) sigir 2018 (optimistic?) neural
       network papers @ sigir 2011 2013 2015 2017 speech vision nlp ir
    4. [82]4. neural methods for information retrieval this tutorial
       mainly focuses on:     ranked retrieval of short and long texts,
       given a text query     shallow and deep neural networks    
       representation learning for broader topics (multimedia, knowledge)
       see:     craswell, croft, guo, mitra, and de rijke. neu-ir: workshop
       on neural information retrieval. sigir 2016 workshop     hang li and
       zhengdong lu. deep learning for information retrieval. sigir 2016
       tutorial
    5. [83]5. today   s agenda 1. ir fundamentals 2. id27s 3. word
       embeddings for ir 4. deep neural nets 5. deep neural nets for ir 6.
       other applications we cover key ideas, rather than exhaustively
       describing all nn ir ranking papers. for a more complete overview
       see:     zhang et al. neural information retrieval: a literature
       review. 2016     onal et al. getting started with neural models for
       semantic matching in web search. 2016 slides are available here for
       download
    6. [84]6. bhaskar mitra @underdoggeek bmitra@microsoft.com nick
       craswell @nick_craswell nickcr@microsoft.com send us your questions
       and feedback during or after the tutorial
    7. [85]7. fundamentals of retrieval chapter 1
    8. [86]8. information retrieval (ir) terminology this tutorial: using
       neural networks in the retrieval system to improve relevance. for
       text queries and documents. information need query results ranking
       (document list) retrieval system indexes a document corpus
       relevance (documents satisfy information need e.g. useful)
    9. [87]9. ir applications document ranking factoid id53
       query keywords natural language question document web page, news
       article a fact and supporting passage trec experiments trec ad hoc
       trec id53 evaluation metric average precision, ndcg
       mean reciprocal rank research solution modern trec rankers bm25,
       id183, learning to rank, links, clicks+likes (if
       available) ibm@trec-qa answer type detection, passage retrieval,
       relation retrieval, answer processing and ranking in products    
       document rankers at: google, bing, baidu, yandex,        
       watson@jeopardy     voice search this nn tutorial long text ranking
       short text ranking
   10. [88]10. challenges in (neural) ir [slide 1/3]     vocabulary mismatch
       q: how many people live in sydney?     sydney   s population is 4.9
       million [relevant, but missing    people    and    live   ]     hundreds of
       people queueing for live music in sydney [irrelevant, and matching
          people    and    live   ]     need to interpret words based on context
       (e.g., temporal) today recent in older (1990s) trec data query:    uk
       prime minister    vocab mismatch:     worse for short texts     still an
       issue for long texts
   11. [89]11. challenges in (neural) ir [slide 2/3]     q and d vary in
       length     models must handle variable length input     relevant docs
       have irrelevant sections
       https://en.wikipedia.org/wiki/file:size_distribution_among_featured
       _articles.png
   12. [90]12. challenges in (neural) ir [slide 3/3]     need to learn q-d
       relationship that generalizes to the tail     unseen q     unseen d    
       unseen information needs     unseen vocabulary     efficient retrieval
       over many documents     kd-tree, lsh, inverted files,     figure from:
       goel, broder, gabrilovich, and pang. anatomy of the long tail:
       ordinary people with extraordinary tastes. www conference 2010
   13. [91]13. representation of words chapter 2
   14. [92]14. one-hot representation (local) dim = |v| sim(banana,mango)
       = 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 banana mango
       notes: 1) popular sim() is cosine, 2) words/tokens come from some
       id121 and transformation
   15. [93]15. hinton, geoffrey e. distributed representations. technical
       report cmu-cs-84-157, 1984
   16. [94]16. context-based distributed representation turney and pantel.
       from frequency to meaning: vector space models of semantics.
       journal of artificial intelligence research 2010    you shall know a
       word by the company it keeps     firth, j. r. (1957). a synopsis of
       linguistic theory 1930   1955. in studies in linguistic analysis, p.
       11. blackwell, oxford. banana mango sim(banana,mango) > 0 appear in
       same documents appear near same words non-zero zero
   17. [95]17. banana nanana#ba na# ban banana (grows) (tree)(yellow) (on)
       (africa) banana doc7 doc9doc2 banana (grows, +1) (tree, +3)(yellow,
       -1) (on, +2) (africa, +5) word-document word-word word-worddist
       word hash (not context-based)    you shall know a word by the company
       it keeps     distributionalsemantics
   18. [96]18. distributional methods use distributed representations
       distributed and distributional     distributed representation: vector
       represents a concept as a pattern, rather than 1-hot    
       id65: linguistic items with similar
       distributions (e.g. context words) have similar meanings
       http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learni
       ng-summer-school/    you shall know a word by the company it keeps    
   19. [97]19. vector space models for a given task: choose matrix, choose
                    weighting. could be binary, could be raw counts. example
       weighting: positive pointwise mutual information (word-word matrix)
       v: vocabulary, c: set of contexts, s: sparse matrix |v| x |c| c0 c1
       c2     cj     c|c| w0 w1 w2     wi sij     w|v| turney and pantel. from
       frequency to meaning: vector space models of semantics. journal of
       artificial intelligence research 2010 ppmi weighting for word-word
       matrix tf-idf weighting for word-document matrix
   20. [98]20. distributed word representation overview     next we cover
       lower-dimensional dense representations     including id97.
       questions on advantage*     but first we consider the effect of
       context choice, in the data itself data counts matrix (sparse)
       learning from counts matrix learning from individual instances
       word-document vector space models lsa paragraph2vec pv-dbow
       word-word vector space models glove id97 word-worddist vector
       space models context * baroni, dinu and kruszewski. don't count,
       predict! a systematic comparison of context-counting vs.
       context-predicting semantic vectors. acl 2014 levy, goldberg and
       dagan. improving distributional similarity with lessons learned
       from id27s. transactions of the acl 3:211-225
   21. [99]21. let   s consider the following example    we have four (tiny)
       documents, document 1 :    seattle seahawks jerseys    document 2 :
          seattle seahawks highlights    document 3 :    denver broncos jerseys   
       document 4 :    denver broncos highlights   
   22. [100]22. using word-document context seattle document 1 document 3
       document 2 document 4 seahawks denver broncos similar similar this
       is topical or syntagmatic similarity.
   23. [101]23. using word-worddist context seattle (seattle, -1) (denver,
       -1) (seahawks, +1) (broncos, +1) (jerseys, + 1) (jerseys, + 2)
       (highlights, +1) (highlights, +2) seahawks denver broncos similar
       similar this is typical or paradigmatic similarity.
   24. [102]24. let   s consider another example       seattle map       seattle
       weather       seahawks jerseys       seahawks highlights       seattle seahawks
       wilson       seattle seahawks sherman       seattle seahawks browner   
          seattle seahawks lfedi       denver map       denver weather       broncos
       jerseys       broncos highlights       denver broncos lynch       denver
       broncos sanchez       denver broncos miller       denver broncos marshall   
       with more (tiny) documents   
   25. [103]25. using word-word context seattle seattle denver seahawks
       broncos jerseys highlights wilson sherman seahawks denver broncos
       browner lfedi lynch sanchez miller marshall map weather similar
       similar 1. word-word is less sparse than word-document (yan et al.,
       2013) 2. a mix of topical and typical similarity (function of
       window size)
   26. [104]26. paradigmatic vs syntagmatic do we choose one axis or the
       other, or both, or something else? can we learn a general-purpose
       word representation that solves all our ir problems? or do we need
       several? seattle seahawks richard sherman jersey denver broncos
       trevor siemian jersey paradigmatic (or typical) axis syntagmatic
       (or topical) axis
   27. [105]27. embeddings (distributed) dense. dim = 200 (for example)
       banana mango dog
   28. [106]28. latent semantic analysis v: vocabulary, d: set of
       documents, x: sparse matrix |v| x |d| xij = tf-idf singular value
       decomposition of x: x = u  vt d0 d1 d2     dj     d|d| t0 t1 t2     ti xij
           t|v| deerwester, dumais, furnas, landauer and harshman. indexing
       by latent semantic analysis. jasis 41, no. 6, 1990
   29. [107]29. latent semantic analysis   1,    ,   l: singular values u1,    ,
       ul: left singular vectors v1,    , vl: right singular vectors the k
       largest singular values, and corresponding singular vectors from u
       and v, is the rank k approximation of x xk = uk  kvk t embedding of
       ith term =   k ti source:
       en.wikipedia.org/wiki/latent_semantic_analysis ^ dumais. latent
       semantic indexing (lsi): trec-3 report. nist special publication sp
       (1995): 219-219.
   30. [108]30. id97 goal: simple (shallow) neural model learning from
       billion words scale corpus predict middle word from neighbors
       within a fixed size context window two different architectures: 1.
       skip-gram 2. cbow (mikolov et al., 2013)
   31. [109]31. skip-gram predict neighbor         +     given word          maximizes
       following average log prob. win wout wt wt+j                                         = 1          =1
                                 ,       0 log              +    |                      +    |         =                                       +                                   =1     
                                                                        full softmax is computationally impractical.
       id97 uses hierarchical softmax or negative sampling instead.
       (mikolov et al., 2013)
   32. [110]32. continuous bag-of-words predict word given
       bag-of-neighbors modify the skip-gram id168.                                         =
       1          =1      log              |                        =                      ,       0         +     win wout wt+2wt+1 wt
       wt-2wt-1 wt* (mikolov et al., 2013)
   33. [111]33. word analogies with id97 [king]     [man] + [woman]    
       [queen] (mikolov et al., 2013)
   34. [112]34. word analogies can work in underlying data too seattle
       seattle denver seahawks broncos jerseys highlights wilson sherman
       seahawks denver broncos similar browner lfedi lynch sanchez miller
       marshall [seahawks]     [seattle] + [denver] map weather sparse
       vectors can work well for an analogy task: levy, goldberg and
       ramat-gan. linguistic regularities in sparse and explicit word
       representations. conll 2014
   35. [113]35. a matrix interpretation of id97 skip-gram looks like
       this: if we aggregate over all training samples                                            =         
            log          |                                             =         =1          =1              ,     log              |         =         =1     
                    =1              ,              log              |         =     =1                                 |         ,              |        
       (pennington et al., 2014) =         =1                   =1                   |         log              |        
       cross-id178actual co-occurence id203 co-occurence
       id203 predicted by the model w0 w1 w2     wj     w|v| w0 w1 w2    
       wi xij     w|v|
   36. [114]36. glove variety of windows sizes and weighting adagrad w0 w1
       w2     wj     w|v| w0 w1 w2     wi xij     w|v| (pennington et al., 2014)    
                            =         =1          =1                   ,                         ,                               2 squared
       erroractual co-occurence id203` co-occurence id203
       predicted by the model weighting functionweighting function actual
       co-occurence id203` weighting function
   37. [115]37. discussion of word representations     representations:
       weighted counts, svd, neural embedding     use similar data (w-d,
       w-w), capture similar semantics     for example, analogies using
       counts     think sparse, act dense     stochastic id119
       allows us to scale to larger data     on individual instances (w2v)
       or count matrix data (glove)     scalability could be the key
       advantage for neural id27s     choice of data affects
       semantics     paradigmatic vs syntagmatic     which is best for your
       application?
   38. [116]38. id27s for ir chapter 3
   39. [117]39. traditional ir feature design     inverse document frequency
           term frequency     adjust tf model for doc length n r n r robertson
       and zaragoza. the probabilistic relevance framework: bm25 and
       beyond. foundations and trends   in information retrieval 3, no. 4
       (2009) 0 0.1 0.2 0.3 0.4 0.5 0.6 0 2 4 6 8 10 12 14 16 18 20
       id203mass term frequency of t in d d not about t d is about t
       robertson and sparck-jones (1977) harter (1975) 2-poisson model of
       tf rsj na  ve bayes model of idf rank d by:                          ,                      (    )
   40. [118]40. how to incorporate embeddings a. extend traditional ir
       models     term weighting, language model smoothing, translation of
       vocab b. expand query using embeddings (followed by non-neural ir)
           add words similar to the query c. ir models that work in the
       embedding space     centroid distance, word mover   s distance
   41. [119]41. term weighting using id27s      =           (term recall)
           fraction of positive docs with t     the      and      were missing in
       rsj      =                        is embedding of t          is centroid of all query
       terms     weight tf-idf using      zheng and callan. learning to
       reweight terms with distributed representations. sigir 2015          
   42. [120]42. traditional ir model: query likelihood     id38
       approach to ir is quite extensible p                                            p      can be
       assumed uniform across docs     p           =                 (    |    ) depends on
       modeling the document     frequent words in      are more likely (term
       frequency)     smoothing according to the corpus (plays the role of
       idf)     various ways of dealing with document length id172
       ponte and croft. a id38 approach to information
       retrieval. sigir 1998 zhai and lafferty. a study of smoothing
       methods for language models applied to ad hoc information
       retrieval. sigir 2001
   43. [121]43. generalized language model ganguly, roy, mitra, and jones.
       id27 based generalized language model for information
       retrieval. sigir 2015 compares query term with every document term
   44. [122]44. neural language translation model zuccon, koopman, bruza,
       and azzopardi. "integrating and evaluating neural id27s
       in information retrieval." australasian document computing
       symposium 2015 translation id203 from document term u to
       query term w considers all query-document term pairs based on:
       berger and lafferty. "information retrieval as statistical
       translation." sigir 1999
   45. [123]45. b. id183 both passages have same number of gold
       query matches. yet non-query green matches can be a good evidence
       of aboutness. we need methods to consider non-query terms.
       traditionally: automatic id183. example from mitra et
       al., 2016 query: albuquerque
   46. [124]46. id183 using w2v identify expansion terms using
       w2v cosine similarity 3 different strategies: pre-retrieval,
       post-retrieval, and pre-retrieval incremental beats no expansion,
       but does not beat non-neural expansion roy, paul, mitra and garain.
       using id27s for automatic id183. neu-ir
       workshop 2016
   47. [125]47. id183 using w2v     related paper. distance      has
       a sigmoid transform. first model:     their second model uses the
       first round of retrieval     can beat non-neural expansion zamani and
       croft. embedding-based query language models. international
       conference on the theory of information retrieval 2016 lavrenko and
       croft. relevance based language models. sigir 2001
   48. [126]48. optimizing the query vector zamani and croft. estimating
       embedding vectors for queries. international conference on the
       theory of information retrieval 2016
   49. [127]49. global vs. local embedding spaces train w2v on documents
       from first round of retrieval fine-grained word sense
       disambiguation a large number of embedding spaces can be cached in
       practice (diaz et al., 2016)
   50. [128]50. c. ir models in the embedding space     q: bag of word
       vectors     d: bag of word vectors     considerations:     deal with
       variable length of q and d     match all pairs? do some alignment of
       q and d?
   51. [129]51. comparing short texts     weighted semantic network    
       related to word alignment     each word in longer text is connected
       to its most similar     bm25-like edge weighting     generates features
       for supervised learning of short text similarity kenter and de
       rijke. short text similarity with id27s. cikm 2015
   52. [130]52. word mover   s distance kusner, sun, kolkin and weinberger.
       from id27s to document distances. icml 2015 see also:
       huang, guo, kusner, sun, weinberger and sha. supervised word
       mover   s distance. nips 2016 sparse normalized tf vectors:      =         =
       1 sparse flow matrix:                   =                            =             minimize cost:     ,                 
           (    ,     )     id97
   53. [131]53. bounds on word mover   s distance wcd <= rwmd <= wmd wcd:
       word centroid distance rwmd: relaxed wmd prefetch and prune
       algorithm:     sort by wcd     compute wmd on top-k     continue, using
       rwmd to prune 95% of wmd kusner, sun, kolkin and weinberger. from
       id27s to document distances. icml 2015
   54. [132]54. centroid and related techniques     goal: bag of vectors    
       single fixed size vector     centroid, weighted centroid (or sum
       [vulic and moens, 2015])     aggregate using a fisher kernel    
       clinchant and perronnin. aggregating continuous id27s for
       information retrieval. (acl) workshop on continuous vector space
       models and their compositionality, 2013     using fisher kernel from
       jaakkola and haussler (1999)
   55. [133]55. what if i told you everyone using w2v is throwing half the
       model away? two sets of embeddings are trained (win and wout) but
       wout is generally discarded in-out dot product captures log prob.
       of co-occurrence win wout wt+j wt (mitra et al., 2016)
   56. [134]56. notions of similarity in-out captures more topical notion
       of similarity than in-in and out-out effect is exaggerated when
       embeddings are trained on short text (e.g., queries) (mitra et al.,
       2016)
   57. [135]57. dual embedding space model compare query terms with every
       document term equivalent to taking centroid of all term vectors
       combine with traditional retrieval model (mitra et al., 2016) query
       and id194: centroid of all words, including
       repetition
   58. [136]58. telescoping     desm also works well when    telescoping    i.e.
       reranking a small set     for example, desm can confuse oxford and
       cambridge     bing rarely makes the oxford-cambridge mistake     the
       upstream ranker (bing) saves desm from making that mistake     if
       desm sees more documents, it may make the mistake matveeva, burges,
       burkard, laucius, and wong. high accuracy retrieval with multiple
       nested ranker. sigir 2006
   59. [137]59. because cambridge is not "an african even-toed ungulate
       mammal"
       https://github.com/bmitra-msft/demos/blob/master/notebooks/desm.ipy
       nb
   60. [138]60. paragraph2vec pv-dm a mix of lsa and and w2v style
       training data, less sparse than training just on paragraph id
       pv-dbow effectively modelling the same relationship as lsa
       (factorizing word-document matrix) le and mikolov. distributed
       representations of sentences and documents. icml 2014
   61. [139]61. models of similar flavour grbovic, djuric, radosavljevic,
       silvestri and bhamidipati. context-and content-aware embeddings for
       query rewriting in sponsored search. sigir 2015. sun, guo, lan, xu
       and cheng learning word representations by jointly modeling
       syntagmatic and paradigmatic relations acl 2015
   62. [140]62. adapting pv-dbow for ir negative sampling using idf
       instead of corpus frequency l2 id173 constraint on the
       norm to avoid over-fitting on short documents reduce sparseness by
       predicting context words (similar to pv-dm?) ai, yang, guo and
       croft. analysis of the paragraph vector model for information
       retrieval. ictir 2016
   63. [141]63. discussion of embeddings in ir     the right mix of lexical
       and semantic matching is important     in non-telescoping settings,
       need to combine with lexical ir     many ir models do some sort of
       vector comparison of q-d words     open question: which notion of
       similarity is best for each ir model     typical vs topical vs some
       mix     these methods seem promising if:     high-quality embeddings
       domain-appropriate embeddings available     no large-scale supervised
       ir data available     if large-scale supervised ir data is available   
       (after the break)
   64. [142]64. deep neural networks chapter 4
   65. [143]65. a primer on neural networks chains of parameterized linear
       transforms (e.g., multiply weight, add bias) followed by non-linear
       functions (  ) popular choices for   : parameters trained using
       id26 e2e training over millions of samples in batched
       mode many choices of architecture and hyper-parameters
       non-linearity input linear transform non-linearity linear transform
       predicted output forwardpass backward pass expected output loss
       tanh relu
   66. [144]66. visual motivation for hidden units consider the following
          toy    challenge for classifying tech queries: vocab: {surface,
       kerberos, book, library} labels:    surface book   ,    kerberos library   
              kerberos surface   ,    library book        can   t separate using a
       linear model! or more succinctly    input features label surface
       kerberos book library 1 0 1 0     1 1 0 0     0 1 0 1     0 0 1 1    
       library booksurface kerberos +0.5 +0.5 -1 -1 -1 -1 +1 +1 +0.5 +0.5
       h1 h2 but let   s consider a tiny neural network with one hidden
       layer   
   67. [145]67. visual motivation for hidden units or more succinctly   
       input features hidden layer label surface kerberos book library h1
       h2 1 0 1 0 1 0     1 1 0 0 0 0     0 1 0 1 0 1     0 0 1 1 0 0     library
       booksurface kerberos +0.5 +0.5 -1 -1 -1 -1 +1 +1 +0.5 +0.5 h1 h2
       but let   s consider a tiny neural network with one hidden layer    can
       separate using a linear model! consider the following    toy   
       challenge for classifying tech queries: vocab: {surface, kerberos,
       book, library} labels:    surface book   ,    kerberos library       
          kerberos surface   ,    library book       
   68. [146]68. why adding depth helps deeper networks can split the input
       space in many (non-independent) linear regions than shallow
       networks mont  far, pascanu, cho and bengio. on the number of linear
       regions of deep neural networks nips 2014
   69. [147]69. why adding depth helps http://playground.tensorflow.org
   70. [148]70. neural models for text but how do you feed text to a
       neural network? dogs have owners, cats have staff.
   71. [149]71. local representations of input text char-level models d o
       g s h a v e o w n e r s c a t s h a v e s t a f f one-hotvectors
       concatenate channels [chars x channels]
   72. [150]72. local representations of input text word-level models w/
       bag-of-chars per word d o g s h a v e o w n e r s c a t s h a v e s
       t a f f one-hotvectors concatenate sum sum sum sum sum sum channels
       [words x channels]
   73. [151]73. # d o g s # # h a v e # # o w n e r s # # c a t s # # h a
       v e # # s t a f f # local representations of input text word-level
       models w/ bag-of-trigraphs per word one-hotvectors concatenate or
       sum sum sum sum sum sum sum channels [words x channels] or [1 x
       channels]
   74. [152]74. local representations of input text word-level models w/
       pre-trained embeddings d o g s h a v e o w n e r s c a t s h a v e
       s t a f f pre-trained embeddings concatenate or sum channels [words
       x channels] or [1 x channels]
   75. [153]75. shift-invariant neural operations detecting a pattern in
       one part of input space is same as detecting it in another (also
       applies to sequential inputs, and inputs with dims >2) leverage
       redundancy by operating on a moving window over whole input space
       and then aggregate the repeatable operation is called a kernel,
       filter, or cell aggregation strategies leads to different
       architectures
   76. [154]76. convolution move the window over the input space each time
       applying the same cell over the window a typical cell operation can
       be,     =               +      full input [words x in_channels] cell input
       [window x in_channels] cell output [1 x out_channels] full output
       [1 + (words     window) / stride x out_channels]          output
   77. [155]77. pooling move the window over the input space each time
       applying an aggregate function over each dimension in within the
       window         =                                         ,                      =                                         ,     full input [words x
       channels] cell input [window x channels] cell output [1 x channels]
       full output [1 + (words     window) / stride x channels]         
       outputmax -pooling average -pooling
   78. [156]78. convolution w/ global pooling stacking a global pooling
       layer on top of a convolutional layer is a common strategy for
       generating a fixed length embedding for a variable length text full
       input [words x in_channels] full output [1 x out_channels]
       convolutionpooling output
   79. [157]79. recurrent neural network similar to a convolution layer
       but additional dependency on previous hidden state a simple cell
       operation shown below but others like lstm and grus are more
       popular in practice,         =                   +               1 +      full input [words x
       in_channels] cell input [window x in_channels] + [1 x out_channels]
       cell output [1 x out_channels] full output [1 x out_channels]         
                        1 output
   80. [158]80. recursive nn or tree-id56 shared weights among all a levels
       of the tree cell can be an lstm or as simple as     =               +      full
       input [words x channels] cell input [window x channels] cell output
       [1 x channels] full output [1 x channels] output
   81. [159]81. autoencoders unsupervised models trained to minimize
       reconstruction errors information bottleneck method (tishby et al.,
       1999) the bottleneck layer captures    minimal sufficient statistics   
       of x and is a compressed representation of the input image source:
       https://en.wikipedia.org/wiki/autoencoder
   82. [160]82. computation networks the    lego    approach to specifying dnn
       architectures library of computation nodes, each node defines logic
       for: 1. forward pass: compute output given input 2. backward pass:
       compute gradient of loss w.r.t. inputs, given gradient of loss
       w.r.t. outputs 3. parameter gradient: compute gradient of loss
       w.r.t. parameters, given gradient of loss w.r.t. outputs chain
       nodes to create bigger and more complex networks
   83. [161]83. really deep neural networks (larsson et al., 2016) (he et
       al., 2015) (szegedy et al., 2014)
   84. [162]84. dnns for ir chapter 6
   85. [163]85. taxonomy of models neural networks can be used to: 1.
       generate query representation, 2. generate id194,
       and/or 3. estimate relevance. this can involve pre-trained
       representations or end-to-end training or some combination of
       approaches. query text generate query representation doc text
       generate doc representation estimate relevance query vector doc
       vector point of query representation point of match point of doc
       representation
   86. [164]86. you   ve already seen    query text id183 using
       embeddings doc text generate doc term vector query likelihood query
       term vector doc term vector query text generate query embedding doc
       text generate doc embedding cosine similarity query embedding doc
       embedding e.g., desm (mitra et al., 2016) e.g., roy et al., 2016
       and diaz et al, 2016
   87. [165]87. what does ad-hoc ir data look like? search queries
       document corpus user interaction / click data human annotated
       labels traditional ir uses human labels as ground truth for
       evaluation so ideally we want to train our ranking models on human
       labels user interaction data is rich but may contain different
       biases compared to human annotated labels
   88. [166]88. search queries document corpus user interaction / click
       data human annotated labels traditional ir uses human labels as
       ground truth for evaluation so ideally we want to train our ranking
       models on human labels user interaction data is rich but may
       contain different biases compared to human annotated labels what
       does ad-hoc ir data look like?
   89. [167]89. search queries document corpus user interaction / click
       data human annotated labels in industry: document corpus: billions?
       query corpus: many billions? labelled data w/ raw text: hundreds of
       thousands of queries labelled data w/ learning-to-rank style
       features: same as above user interaction data: billions? what does
       ad-hoc ir data look like?
   90. [168]90. in academia: document corpus: few billion query corpus:
       few million but other sources (e.g., wiki titles) can be used
       labelled data w/ raw text: few hundred to few thousand queries
       labelled data w/ learning-to-rank style features: tens of thousands
       of queries search queries document corpus human annotated labels
       what does ad-hoc ir data look like?
   91. [169]91. a pessimistic summary            water, water, everywhere, nor
       any drop to drink.^
   92. [170]92. levels of supervision (or the optimistic view)
       unsupervised     train embeddings on unlabeled corpus and use in
       traditional ir models     e.g., glm, ntlm, desm, semantic hashing
       semi-supervised     dnn models using pre- trained embeddings for
       input text representation     e.g., drmm, matchpyramid fully
       supervised     dnns w/ raw text input (one-hot word vectors or
       n-graph vectors) trained on labels or click     e.g., duet *of
       course, ad-hoc retrieval isn   t everything. a lot of recent dnn for
       ir literature focuses on other tasks such as short-text similarity
       or qna, where enough training data is available. we covered most of
       these
   93. [171]93. semantic hashing autoencoder minimizing document
       reconstruction error vocab: 2k popular words (stopwords removed)
       in/out = word counts and binary hidden units stacked rbms w/
       layer-by-layer pre-training followed by e2e tuning class labels
       based eval, no relevance judgments (salakhutdinov and hinton, 2007)
   94. [172]94. deep semantic similarity model siamese network trained e2e
       on query and document title pairs relevance is estimated by cosine
       similarity between query and document embeddings input: character
       trigraph counts (bag of words assumption) minimizes cross-id178
       loss against randomly sampled negative documents (huang et al.,
       2013)
   95. [173]95. convolutional dssm replace bag-of-words assumption by
       concatenating term vectors in a sequence on the input (shen et al.,
       2014) convolution followed by global max-pooling performance
       improves further by including more negative samples during training
       (50 vs. 4)
   96. [174]96. rich space of neural architectures (palangi et al., 2015)
       (kalchbrenner et al., 2014) (denil et al., 2014) (kim, 2014)
       (severyn and moschitti, 2015) (tai et al., 2015) (zhao et al.,
       2015) (hu et al., 2014)
   97. [175]97. rich space of neural architectures (kalchbrenner et al.,
       2014) input representation based on pre-trained id27s
       wide vs. narrow convolution (presence/absence of zero padding)
       dynamic k max-pooling, where k is a function of the number of
       convolutional layers and sentence length evaluated on non-ir tasks
       (sentiment prediction and question-type classification) dynamic
       convolutional neural network
   98. [176]98. rich space of neural architectures uses the did98 model to
       learn document embeddings first did98 on id27s to get
       sentence embedding then separate did98 on sentences to get document
       embedding evaluated on classification tasks (denil et al., 2014)
   99. [177]99. convolutional model for text classification and sentiment
       prediction compares strategies for input word representation using
       id27s rich space of neural architectures (kim, 2014) 1.
       embeddings are randomly initialized and learnt during e2e training
       2. pre-trained embeddings not updated during e2e training 3.
       pre-trained embeddings further tuned during e2e training 4.
       concatenation of two different vectors, both initialized with pre-
       trained embeddings but only one update during e2e training diff.
       strategy wins on diff. dataset, size of e2e training data must be
       an important factor
   100. [178]100. stacked alternate layers of convolution and max-pooling
       rich space of neural architectures (hu et al., 2014) depth of
       network is fixed unlike recursive models, the weights are not
       shared between convolutions at different depth arc-i
   101. [179]101. rich space of neural architectures convolutional model
       for short text ranking pre-trained id27s for input, not
       tuned during e2e training additional layers for q-d embedding
       comparison, could also include non embedding features pointwise
       id168, model predicts relevance class (severyn and
       moschitti, 2015) evaluated on microblog retrieval, unfortunately no
       comparisons to dssm / cdssm
   102. [180]102. rich space of neural architectures replace
       convolution-pooling w/ lstm (palangi et al., 2015) lstm-dssm
   103. [181]103. rich space of neural architectures recently various
       tree-structured id56s / lstms have also been explored in the context
       of text similarity and classification tasks (tai et al., 2015)
       (zhao et al., 2015)
   104. [182]104. interaction matrix alternative to siamese networks
       interaction matrix x, where xi,j is obtained by comparing the the
       ith word in source sentence with jth word in target sentence
       comparison is generally lexical or based on pre-trained embeddings
       focus on recognizing good matching patterns instead of learning
       good representations (hu et al., 2014) (pang et al., 2014)
       matchpyramid arc-ii
   105. [183]105. ad-hoc retrieval using local representation deep
       relevance matching model (drmm) for ad-hoc retrieval argues exact
       matching more important than representation learning for ad-hoc
       retrieval dnn on top of histogram based features gating network
       based on term idf score id27s are also used but their
       impact not clear (guo et al., 2016)
   106. [184]106. (mitra et al., 2017) ad-hoc retrieval using local and
       distributed representation argues both    lexical    and    semantic   
       matching is important for document ranking duet model is a linear
       combination of two dnns using local and distributed representations
       of query/document as inputs, and jointly trained on labelled data
       local model operates on lexical interaction matrix distributed
       model operates on n-graph representation of query and document text
   107. [185]107. (mitra et al., 2017) ad-hoc retrieval using local and
       distributed representation argues both    lexical    and    semantic   
       matching is important for document ranking duet model is a linear
       combination of two dnns using local and distributed representations
       of query/document as inputs, and jointly trained on labelled data
       local model operates on lexical interaction matrix distributed
       model operates on n-graph representation of query and document text
   108. [186]108. using our earlier taxonomy of models    query text
       generate query term vector doc text generate doc term vector
       generate interaction matrix query term vector doc term vector local
       model dnn for matching query text generate query embedding doc text
       generate doc embedding hadamard product query embedding doc
       embedding distributed model dnn for matching
   109. [187]109. (mitra et al., 2017) ad-hoc retrieval using local and
       distributed representation distributed model not likely to have
       good representation for rare intents (e.g., a television model
       number    sc32mn17   ) for the query    what channel are the seahawks on
       today    documents may contain the term    espn    instead     distributed
       model likely to make the    channel           espn    connection distributed
       model more effective for popular intents, fall back on local model
       for rare ones local and distributed models likely to make different
       mistakes and therefore more effective when combined
   110. [188]110. the library vs. librarian dilemma is there a hidden
       assumption in distributed models that they know about everything in
       the universe? the distributed model knows more about    barack obama   
       than    bhaskar mitra    and will perform better on the former the
       local model doesn   t understand either but therefore might perform
       better for the latter query compared to the distributed model is
       your model the library (knows about the world) or the librarian
       (knows how to find information in a without much prior domain
       knowledge)?
   111. [189]111. big vs. small data regimes big data seems to be more
       crucial for models that focus on good representation learning for
       text partial supervision strategies (e.g., unsupervised
       pre-training of id27s) can be effective but may be
       leaving the bigger gains on the table learning to train on
       unlabeled data may be key to making progress on neural ad-hoc
       retrieval which ir models are similar? id91 based on query
       level retrieval performance.
   112. [190]112. implementing duet using the cognitive toolkit (cntk)
       https://github.com/bmitra-msft/ndrm/blob/master/notebooks/duet.ipyn
       b
   113. [191]113. other applications chapter 7
   114. [192]114. query recommendations hierarchical sequence-to-sequence
       model for term- by-term query generation similar to ad-hoc ranking
       the dnn feature alone performs poorly but shows significant
       improvements over a model with lexical contextual features (sordoni
       et al., 2015)
   115. [193]115. query auto-completion given a (rare) query prefix
       retrieve relevant suffixes from a fixed set cdssm model trained on
       query prefix-suffix pairs can be used for suffix ranking training
       on prefix-suffix produces a leads to a more    typical    embedding
       space (mitra and craswell, 2015)
   116. [194]116. analogies for session modelling cdssm vectors when
       trained on prefix-suffix pairs or session query pairs are amenable
       to analogical tasks (mitra, 2015)
   117. [195]117. analogies for session modelling this property was shown
       to be useful for modelling session context "london"     "things to do
       in london    is similar to "new york"     "new york tourist
       attractions    possible scope for modelling sessions as paths in the
       embedding space? (mitra, 2015)
   118. [196]118. id72 dssm model trained under a
       multi-task setting e.g., query classification and ranking lower
       layer weights are shared, top layer specific to the task leverages
       cross-task data as well as reduces over- fitting to particular task
           learnt embedding may be useful for related tasks not directly
       trained for (liu et al., 2015)
   119. [197]119. neural approach for knowledge-based ir incorporating
       distributed representations of kb entities in deep neural retrieval
       models advocates that kbs can either support learning of latent
       representations of text or serve as a semantic translator between
       their latent representations (nguyen et al., 2016)
   120. [198]120. conversational response retrieval ranking responses for
       conversational systems interesting challenges in modelling
       utterance level discourse structure and context can multi-turn
       conversational tasks become a key playground for neural retrieval
       models? (yan et al., 2016) (zhou et al., 2016)
   121. [199]121. proactive retrieval given current task context generate
       a proactive query and retrieve recommendations in this particular
       paper authors focused on recommendations during a document
       authoring task (luukkonen et al., 2016)
   122. [200]122. multimodal retrieval this tutorial is focused on text
       but neural representation learning is also leading towards
       breakthroughs in multimodal retrieval (ma et al., 2015)
   123. [201]123. useful resources pyndri for python:
       https://github.com/cvangysel/pyndri luandri for lua / torch:
       https://github.com/bmitra-msft/luandri public id27
       datasets model description id97 ~3m words vocabulary trained on
       google news dataset link glove multiple datasets with ~400k-2m word
       vocabulary link desm in + out embeddings for ~3m words trained on
       bing query logs link ntlm multiple datasets with ~50k-3m word
       vocabulary link next gen neural ir models (using reinforcement
       learning or adversarial learning) may need to perform retrieval as
       part of the model training / evaluation step
   124. [202]124. bhaskar mitra @underdoggeek bmitra@microsoft.com nick
       craswell @nick_craswell nickcr@microsoft.com send us your questions
       and feedback during or after the tutorial

          [203]recommended

     * social media in the classroom
       social media in the classroom
       online course - linkedin learning
     * elearning techniques: visual design
       elearning techniques: visual design
       online course - linkedin learning
     * teaching techniques: writing effective learning objectives
       teaching techniques: writing effective learning objectives
       online course - linkedin learning
     * neural models for information retrieval
       neural models for information retrieval
       bhaskar mitra
     *    probabilistic logic programs and their applications   
          probabilistic logic programs and their applications   
       diannepatricia
     * l06 stemmer and id153
       l06 stemmer and id153
       ananth
     * language models for information retrieval
       language models for information retrieval
       nik spirin
     * construisons ensemble le chatbot bancaire dedemain !
       construisons ensemble le chatbot bancaire dedemain !
       linagora
     * hackathon 2014 nlp hack
       hackathon 2014 nlp hack
       roelof pieters
     * roee aharoni - 2017 - towards string-to-tree neural machine
       translation
       roee aharoni - 2017 - towards string-to-tree neural machine
       translation
       association for computational linguistics

     * [204]english
     * [205]espa  ol
     * [206]portugu  s
     * [207]fran  ais
     * [208]deutsch

     * [209]about
     * [210]dev & api
     * [211]blog
     * [212]terms
     * [213]privacy
     * [214]copyright
     * [215]support

     *
     *
     *
     *
     *

   linkedin corporation    2019

     

share clipboard
     __________________________________________________________________

   [216]  
     * facebook
     * twitter
     * linkedin

   link ____________________

public clipboards featuring this slide
     __________________________________________________________________

   (button)   
   no public clipboards found for this slide

select another clipboard
     __________________________________________________________________

   [217]  

   looks like you   ve clipped this slide to already.
   ____________________

   create a clipboard

you just clipped your first slide!

   clipping is a handy way to collect important slides you want to go back
   to later. now customize the name of a clipboard to store your clips.
     __________________________________________________________________

   name* ____________________
   description ____________________
   visibility
   others can see my clipboard [ ]
   (button) cancel (button) save

   bizographics tracking image

references

   visible links
   1. https://www.slideshare.net/rss/latest
   2. https://www.slideshare.net/opensearch.xml
   3. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
   4. https://es.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
   5. https://fr.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
   6. https://de.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
   7. https://pt.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
   8. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
   9. https://www.slideshare.net/api/oembed/2?format=json&url=http://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  10. https://www.slideshare.net/api/oembed/2?format=xml&url=http://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  11. https://www.slideshare.net/mobile/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  12. android-app://net.slideshare.mobile/slideshare-app/ss/71792116
  13. ios-app://917418728/slideshare-app/ss/71792116
  14. http://www.linkedin.com/legal/user-agreement
  15. http://www.linkedin.com/legal/privacy-policy
  16. http://www.linkedin.com/legal/privacy-policy
  17. http://www.linkedin.com/legal/user-agreement
  18. https://www.slideshare.net/
  19. https://www.slideshare.net/explore
  20. https://www.slideshare.net/login
  21. https://www.slideshare.net/
  22. https://www.slideshare.net/upload
  23. https://www.slideshare.net/login
  24. https://www.slideshare.net/w/signup
  25. https://www.slideshare.net/
  26. https://www.slideshare.net/explore
  27. https://www.linkedin.com/learning/topics/presentations?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  28. https://www.linkedin.com/learning/topics/powerpoint?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  29. https://www.linkedin.com/learning?trk=slideshare_subnav_learning
  30. https://www.linkedin.com/psettings/privacy
  31. https://public.slidesharecdn.com/bhaskarmitra3/neural-models-for-information-retrieval-80731267
  32. https://public.slidesharecdn.com/diannepatricia/probabilistic-logic-programs-and-their-applications
  33. https://public.slidesharecdn.com/ananth/l06-stemmer-and-edit-distance
  34. https://public.slidesharecdn.com/nikspirin/language-models-for-information-retrieval-71972326
  35. https://public.slidesharecdn.com/aclanthology/matthew-marge-2017-exploring-variation-of-natural-human-commands-to-a-robot-in-a-collaborative-navigation-task
  36. https://public.slidesharecdn.com/linagora/construisons-ensemble-le-chatbot-bancaire-dedemain
  37. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  38. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  39. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  40. https://www.slideshare.net/bhaskarmitra3?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  41. https://www.slideshare.net/bhaskarmitra3?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  42. https://www.slideshare.net/signup?login_source=slideview.popup.follow&from=addcontact&from_source=https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  43. https://www.slideshare.net/featured/category/technology
  44. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017#comments-panel
  45. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017#likes-panel
  46. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017#stats-panel
  47. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017#notes-panel
  48. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  49. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  50. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  51. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  52. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  53. https://www.slideshare.net/signup?login_source=slideview.popup.comment&from=comments&from_source=https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  54. https://www.slideshare.net/kendricklamost?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  55. https://www.slideshare.net/kendricklamost?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  56. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  57. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  58. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  59. https://www.slideshare.net/kendricklamost?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  60. https://www.slideshare.net/kendricklamost?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  61. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  62. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  63. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  64. https://www.slideshare.net/kendricklamost?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  65. https://www.slideshare.net/kendricklamost?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  66. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  67. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  68. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  69. https://www.slideshare.net/anuragmanutdreddy?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  70. https://www.slideshare.net/anuragmanutdreddy?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  71. https://www.slideshare.net/anuragmanutdreddy?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  72. https://www.slideshare.net/anuragmanutdreddy?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  73. https://www.slideshare.net/markohelin?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  74. https://www.slideshare.net/markohelin?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  75. https://www.slideshare.net/smomtazi?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  76. https://www.slideshare.net/smomtazi?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  77. https://www.slideshare.net/gmarchetti?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  78. https://www.slideshare.net/gmarchetti?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  79. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
  80. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-2-638.jpg?cb=1494521316
  81. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-3-638.jpg?cb=1494521316
  82. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-4-638.jpg?cb=1494521316
  83. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-5-638.jpg?cb=1494521316
  84. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-6-638.jpg?cb=1494521316
  85. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-7-638.jpg?cb=1494521316
  86. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-8-638.jpg?cb=1494521316
  87. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-9-638.jpg?cb=1494521316
  88. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-10-638.jpg?cb=1494521316
  89. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-11-638.jpg?cb=1494521316
  90. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-12-638.jpg?cb=1494521316
  91. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-13-638.jpg?cb=1494521316
  92. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-14-638.jpg?cb=1494521316
  93. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-15-638.jpg?cb=1494521316
  94. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-16-638.jpg?cb=1494521316
  95. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-17-638.jpg?cb=1494521316
  96. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-18-638.jpg?cb=1494521316
  97. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-19-638.jpg?cb=1494521316
  98. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-20-638.jpg?cb=1494521316
  99. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-21-638.jpg?cb=1494521316
 100. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-22-638.jpg?cb=1494521316
 101. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-23-638.jpg?cb=1494521316
 102. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-24-638.jpg?cb=1494521316
 103. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-25-638.jpg?cb=1494521316
 104. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-26-638.jpg?cb=1494521316
 105. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-27-638.jpg?cb=1494521316
 106. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-28-638.jpg?cb=1494521316
 107. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-29-638.jpg?cb=1494521316
 108. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-30-638.jpg?cb=1494521316
 109. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-31-638.jpg?cb=1494521316
 110. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-32-638.jpg?cb=1494521316
 111. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-33-638.jpg?cb=1494521316
 112. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-34-638.jpg?cb=1494521316
 113. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-35-638.jpg?cb=1494521316
 114. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-36-638.jpg?cb=1494521316
 115. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-37-638.jpg?cb=1494521316
 116. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-38-638.jpg?cb=1494521316
 117. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-39-638.jpg?cb=1494521316
 118. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-40-638.jpg?cb=1494521316
 119. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-41-638.jpg?cb=1494521316
 120. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-42-638.jpg?cb=1494521316
 121. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-43-638.jpg?cb=1494521316
 122. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-44-638.jpg?cb=1494521316
 123. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-45-638.jpg?cb=1494521316
 124. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-46-638.jpg?cb=1494521316
 125. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-47-638.jpg?cb=1494521316
 126. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-48-638.jpg?cb=1494521316
 127. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-49-638.jpg?cb=1494521316
 128. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-50-638.jpg?cb=1494521316
 129. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-51-638.jpg?cb=1494521316
 130. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-52-638.jpg?cb=1494521316
 131. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-53-638.jpg?cb=1494521316
 132. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-54-638.jpg?cb=1494521316
 133. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-55-638.jpg?cb=1494521316
 134. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-56-638.jpg?cb=1494521316
 135. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-57-638.jpg?cb=1494521316
 136. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-58-638.jpg?cb=1494521316
 137. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-59-638.jpg?cb=1494521316
 138. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-60-638.jpg?cb=1494521316
 139. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-61-638.jpg?cb=1494521316
 140. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-62-638.jpg?cb=1494521316
 141. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-63-638.jpg?cb=1494521316
 142. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-64-638.jpg?cb=1494521316
 143. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-65-638.jpg?cb=1494521316
 144. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-66-638.jpg?cb=1494521316
 145. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-67-638.jpg?cb=1494521316
 146. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-68-638.jpg?cb=1494521316
 147. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-69-638.jpg?cb=1494521316
 148. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-70-638.jpg?cb=1494521316
 149. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-71-638.jpg?cb=1494521316
 150. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-72-638.jpg?cb=1494521316
 151. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-73-638.jpg?cb=1494521316
 152. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-74-638.jpg?cb=1494521316
 153. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-75-638.jpg?cb=1494521316
 154. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-76-638.jpg?cb=1494521316
 155. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-77-638.jpg?cb=1494521316
 156. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-78-638.jpg?cb=1494521316
 157. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-79-638.jpg?cb=1494521316
 158. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-80-638.jpg?cb=1494521316
 159. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-81-638.jpg?cb=1494521316
 160. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-82-638.jpg?cb=1494521316
 161. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-83-638.jpg?cb=1494521316
 162. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-84-638.jpg?cb=1494521316
 163. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-85-638.jpg?cb=1494521316
 164. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-86-638.jpg?cb=1494521316
 165. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-87-638.jpg?cb=1494521316
 166. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-88-638.jpg?cb=1494521316
 167. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-89-638.jpg?cb=1494521316
 168. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-90-638.jpg?cb=1494521316
 169. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-91-638.jpg?cb=1494521316
 170. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-92-638.jpg?cb=1494521316
 171. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-93-638.jpg?cb=1494521316
 172. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-94-638.jpg?cb=1494521316
 173. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-95-638.jpg?cb=1494521316
 174. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-96-638.jpg?cb=1494521316
 175. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-97-638.jpg?cb=1494521316
 176. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-98-638.jpg?cb=1494521316
 177. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-99-638.jpg?cb=1494521316
 178. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-100-638.jpg?cb=1494521316
 179. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-101-638.jpg?cb=1494521316
 180. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-102-638.jpg?cb=1494521316
 181. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-103-638.jpg?cb=1494521316
 182. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-104-638.jpg?cb=1494521316
 183. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-105-638.jpg?cb=1494521316
 184. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-106-638.jpg?cb=1494521316
 185. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-107-638.jpg?cb=1494521316
 186. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-108-638.jpg?cb=1494521316
 187. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-109-638.jpg?cb=1494521316
 188. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-110-638.jpg?cb=1494521316
 189. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-111-638.jpg?cb=1494521316
 190. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-112-638.jpg?cb=1494521316
 191. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-113-638.jpg?cb=1494521316
 192. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-114-638.jpg?cb=1494521316
 193. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-115-638.jpg?cb=1494521316
 194. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-116-638.jpg?cb=1494521316
 195. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-117-638.jpg?cb=1494521316
 196. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-118-638.jpg?cb=1494521316
 197. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-119-638.jpg?cb=1494521316
 198. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-120-638.jpg?cb=1494521316
 199. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-121-638.jpg?cb=1494521316
 200. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-122-638.jpg?cb=1494521316
 201. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-123-638.jpg?cb=1494521316
 202. https://image.slidesharecdn.com/neuirtutorial-wsdm2017-170206010405/95/neural-text-embeddings-for-information-retrieval-wsdm-2017-124-638.jpg?cb=1494521316
 203. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017#related-tab-content
 204. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
 205. https://es.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
 206. https://pt.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
 207. https://fr.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
 208. https://de.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
 209. https://www.slideshare.net/about
 210. https://www.slideshare.net/developers
 211. http://blog.slideshare.net/
 212. https://www.slideshare.net/terms
 213. https://www.slideshare.net/privacy
 214. http://www.linkedin.com/legal/copyright-policy
 215. https://www.linkedin.com/help/slideshare
 216. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
 217. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017

   hidden links:
 219. https://www.slideshare.net/bhaskarmitra3/neural-text-embeddings-for-information-retrieval-wsdm-2017
 220. https://www.slideshare.net/signup?login_source=slideview.clip.like&from=clip&layout=foundation&from_source=
 221. https://www.slideshare.net/login?from_source=%2fbhaskarmitra3%2fneural-text-embeddings-for-information-retrieval-wsdm-2017%3ffrom_action%3dsave&from=download&layout=foundation
 222. https://www.slideshare.net/signup?login_source=slideview.popup.flags&from=flagss&from_source=https%3a%2f%2fwww.slideshare.net%2fbhaskarmitra3%2fneural-text-embeddings-for-information-retrieval-wsdm-2017
 223. https://www.linkedin.com/learning/social-media-in-the-classroom?trk=slideshare_sv_learning
 224. https://www.linkedin.com/learning/elearning-techniques-visual-design?trk=slideshare_sv_learning
 225. https://www.linkedin.com/learning/teaching-techniques-writing-effective-learning-objectives?trk=slideshare_sv_learning
 226. https://www.slideshare.net/bhaskarmitra3/neural-models-for-information-retrieval-80731267
 227. https://www.slideshare.net/diannepatricia/probabilistic-logic-programs-and-their-applications
 228. https://www.slideshare.net/ananth/l06-stemmer-and-edit-distance
 229. https://www.slideshare.net/nikspirin/language-models-for-information-retrieval-71972326
 230. https://www.slideshare.net/linagora/construisons-ensemble-le-chatbot-bancaire-dedemain
 231. https://www.slideshare.net/roelofp/hackathon-2014-nlp-hack
 232. https://www.slideshare.net/aclanthology/roee-aharoni-2017-towards-stringtotree-neural-machine-translation
 233. http://www.linkedin.com/company/linkedin
 234. http://www.facebook.com/linkedin
 235. http://twitter.com/slideshare
 236. http://www.google.com/+linkedin
 237. https://www.slideshare.net/rss/latest
