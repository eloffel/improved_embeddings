   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 2 kernel
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]doc2vec
    2. [10]doc2vec.ipynb

distributed representations of sentences and documents[11]  

-- quoc le & tomas mikolov (2014)[12]  

review of le & mikolov (2014) and introduction into id97[13]  
     __________________________________________________________________

introduction into id97[14]  

   the last two years, the method and tool developed by mikolov et al.
   (2013) for learning continuous id27s has gained a lot of
   traction. the model forms the basis for the study of le & mikolov
   (2014). in order to better understand the methods in that study, i
   believe it would be good to first present a brief overview of id97.
   for this overview i gratefully make use of the excellent, in depth
   explanation of id97 by [15]xin rong (2014). to have a more complete
   understanding of the model, please have a look at that paper.

   id97 attempts to associate words with points in space. the spatial
   distance between words then describes the relation (similarity) between
   these words. words that are spatially close, are similar. words are
   represented by continuous vectors over $x$ dimensions. this example
   shows the relation between a number of words where each word is
   represented by a vector of two dimensions:
   in [4]:
%matplotlib inline

   in [5]:
import seaborn as sb
import numpy as np

words = ['queen', 'book', 'king', 'magazine', 'car', 'bike']
vectors = np.array([[0.1,   0.3],  # queen
                    [-0.5, -0.1],  # book
                    [0.2,   0.2],  # king
                    [-0.3, -0.2],  # magazine
                    [-0.5,  0.4],  # car
                    [-0.45, 0.3]]) # bike

sb.plt.plot(vectors[:,0], vectors[:,1], 'o')
sb.plt.xlim(-0.6, 0.3)
sb.plt.ylim(-0.3, 0.5)
for word, x, y in zip(words, vectors[:,0], vectors[:,1]):
    sb.plt.annotate(word, (x, y), size=12)

   [w4zijepf8e6 aaaaaelftksuqmcc ]

   the displacement vector (the vector between two vectors) describes the
   relation between two words. this makes it possible to compare
   displacement vectors to find pairs of words that have a similar
   relation to each other. a famous example given in the original paper is
   the following analogy relation: queen : king :: woman : man which
   should be read as queen relates to king in the same way as woman
   relates to man. in algebraic formulation: $v_{queen} - v_{king} =
   v_{woman} - v_{man}$. this technique of analogical reasoning can be
   applied to e.g. id53.

   id97 learns continuous id27s from plain text. but how?
   the model assumes the distributional hypothesis that words are
   characterized by words they hang out with. we can use that idea to
   estimate the id203 of two words occurring near each other, e.g.
   what is the id203 of the following words, given cinderella, i.e
   $p(w|\textrm{cinderella})$?
   in [6]:
import pandas as pd

s = pd.series([0.1, 0.4, 0.01, 0.2, 0.05],
              index=["pumpkin", "shoe", "tree", "prince", "luck"])
s.plot(kind='bar')
sb.plt.ylabel("$p(w|cinderella)$")

   out[6]:
<matplotlib.text.text at 0x1136f8e90>

   [wfvhhtayf8rjgaaaabjru5erkjggg== ]

softmax regression[16]  

   id97 is a very simple neural network with a single hidden layer.
   have a look at the following picture (taken from [17]rong 2014). we'll
   get into more details in a moment.

   the model considers each word $w_o$ in turn along with a given context
   $c$ (e.g. $w_o$=cinderella and $c$=shoe). now given this context, can
   we predict what $w_o$ should be? this is essentially a multiclass
   classification where we have as many labels as our vocabulary size $v$.
   using softmax regression, we can compute a id203 distribution
   $\hat{y}$ over the labels. the model attempts to minimize via
   stochastic id119 the difference between the output
   distribution and the target distribution (which is a one-hot
   distribution which places all id203 mass on the correct word).
   the difference between the two distribution is measured by the
   cross-id178.

   the neural network contains two matrics: $w$ and $w'$ of dimensions $v
   \times n$ and $n \times v$ respectively, where $v$ is the vocabulary
   size and $n$ the number of dimensions. let's make this all a little
   more concrete with a small example. say we have a corpus containing the
   following documents:
   in [7]:
sentences = ['the king loves the queen', 'the queen loves the king',
             'the dwarf hates the king', 'the queen hates the dwarf',
             'the dwarf poisons the king', 'the dwarf poisons the queen']

   we first transform these documents into bag-of-indices to enable easier
   computation:
   in [8]:
from collections import defaultdict

def vocabulary():
    dictionary = defaultdict()
    dictionary.default_factory = lambda: len(dictionary)
    return dictionary

def docs2bow(docs, dictionary):
    """transforms a list of strings into a list of lists where
    each unique item is converted into a unique integer."""
    for doc in docs:
        yield [dictionary[word] for word in doc.split()]

   in [9]:
vocabulary = vocabulary()
sentences_bow = list(docs2bow(sentences, vocabulary))
sentences_bow

   out[9]:
[[0, 1, 2, 0, 3],
 [0, 3, 2, 0, 1],
 [0, 4, 5, 0, 1],
 [0, 3, 5, 0, 4],
 [0, 4, 6, 0, 1],
 [0, 4, 6, 0, 3]]

   we now construct the two matrices $w$ and $w'$:
   in [10]:
import numpy as np

v, n = len(vocabulary), 3
wi = (np.random.random((v, n)) - 0.5) / n
wo = (np.random.random((n, v)) - 0.5) / v

   each row $i$ in $w$ corresponds to word $i$ and each column $j$
   corresponds to the $j$th dimension.
   in [11]:
print wi

[[-0.11953936  0.161062   -0.03209868]
 [ 0.00244538  0.11746195  0.12607279]
 [-0.14451005 -0.14540946  0.12184834]
 [-0.15411106 -0.04843555 -0.06943384]
 [ 0.04241556  0.01833369 -0.01999708]
 [ 0.09488057 -0.06968794  0.02873621]
 [-0.02498429  0.00360255  0.05691627]]

   notice that $w'$ isn't simply the transpose of $w$ but a different
   matrix:
   in [12]:
print wo

[[-0.04019078  0.00870839 -0.00461927 -0.0255281  -0.00184819 -0.017375
   0.04739917]
 [ 0.00962237  0.05808629 -0.02183692  0.07120648  0.0051645   0.03994698
  -0.002765  ]
 [-0.03059303  0.05618066  0.029233    0.04137082  0.03396673 -0.01705916
  -0.01613436]]

   with the two matrices in place we continue with computing the posterior
   id203 of an output word given some input word. given an input
   word $w_i$, e.g. dwarf and its corresponding vector $w_i$, what is the
   id203 that the output word $w_o$ is hates? using the dot product
   $w_i \cdot w'^t_o$ we compute the distance between the input word dwarf
   and the output word hates:
   in [13]:
print np.dot(wi[vocabulary['dwarf']], wo.t[vocabulary['hates']])

0.000336538415207

   now using softmax regression, we can compute the posterior id203
   $p(w_o|w_i)$:
   $$ p(w_o|w_i) = y_i = \frac{exp(w_i \cdot w'^t_o)}{\sum^v_{j=1} exp(w_i
   \cdot w'^t_j)} $$
   in [14]:
p_hates_dwarf = (np.exp(np.dot(wi[vocabulary['dwarf']], wo.t[vocabulary['hates']
])) /
                       sum(np.exp(np.dot(wi[vocabulary['dwarf']], wo.t[vocabular
y[w]]))
                           for w in vocabulary))
print p_hates_dwarf

0.14291402521

updating the hidden-to-output layer weights[18]  

   id97 attempts to associate words with points in space. these points
   in space are represented by the continuous embeddings of the words. all
   vectors are initialized as random points in space, so we need to learn
   better positions. the model does so by maximizing the equation above.
   the corresponding id168 which we try to minimize is $e = -\log
   p(w_o|w_i)$. first, let's focus on how to update the hidden-to-output
   layer weights. say the target output word is cinderella. given the
   aformentioned one-hot target distribution $t$, the error can be
   computed as $t_j - y_j = e_j$, where $t_j$ is 1 iff $w_j$ is the actual
   output word. so, the actual output word is cinderella and we compute
   the posterior id203 of p(pumpkin | tree), the error will be 0 -
   p(pumpkin | tree), because pumpkin isn't the actual ouput word.

   to obtain the gradient on the hidden-to-output weights, we compute $e_j
   \cdot h_i$, where $h_i$ is a copy of the vector corresponding to the
   input word (only holds with a context of a single word). finally, using
   stochastic id119, with a learning rate $\nu$ we obtain the
   weight update equation for the hidden to output layer weights:
   $$w'^{t (t)}_j = w'^{t (t-1)}_j - \nu \cdot e_j \cdot h_j$$

   .

   assume the target word is king and the context or input word $c$ is
   queen. given this input word we compute for each word in the vocabulary
   the posterior id203 p(word | queen). if the word is our target
   word, the error will be 1 - p(word | queen); otherwise 0 - p(word |
   queen). finally, using stocastic id119 we update the
   hidden-to-output layer weights:
   in [15]:
target_word = 'king'
input_word = 'queen'
learning_rate = 1.0

for word in vocabulary:
    p_word_queen = (np.exp(np.dot(wo.t[vocabulary[word]], wi[vocabulary[input_wo
rd]])) /
                    sum(np.exp(np.dot(wo.t[vocabulary[w]], wi[vocabulary[input_w
ord]]))
                        for w in vocabulary))
    t = 1 if word == target_word else 0
    error = t - p_word_queen
    wo.t[vocabulary[word]] = (wo.t[vocabulary[word]] - learning_rate *
                              error * wi[vocabulary[input_word]])
print wo

[[-0.06243581  0.14095147 -0.02669861 -0.04758839 -0.0239239  -0.03947435
   0.02543349]
 [ 0.00263098  0.09964895 -0.02877623  0.06427316 -0.00177368  0.03300138
  -0.00966859]
 [-0.04061539  0.11576201  0.01928528  0.03143168  0.02402065 -0.02701589
  -0.02603086]]

updating the input-to-hidden layer weights[19]  

   now that we have a way to update the hidden-to-output layer weights, we
   concentrate on updating the input-to-hidden layer weights. we need to
   backpropagate the prediction errors to the input-to-hidden weights. we
   first compute $eh$ which is an $n$ dimensional vector representing the
   sum of the hidden-to-output vectors for each word in the vocabulary
   weighted by their prediction error:
   $$\sum^v_{j=1} e_j \cdot w'_{i,j} = {eh}_i$$

   again using the learning rate $\nu$ we update the weights using:
   $$w^{(t)}_{w_i} = w^{(t-1)}_{w_i} - \nu \cdot eh$$

   let's see how that works in python:
   in [16]:
wi[vocabulary[input_word]] = wi[vocabulary[input_word]] - learning_rate * wo.sum
(1)

   if we now would recompute the id203 of each word given the input
   word queen, we see that the id203 of king given queen has gone
   up:
   in [17]:
for word in vocabulary:
    p = (np.exp(np.dot(wo.t[vocabulary[word]], wi[vocabulary[input_word]])) /
         sum(np.exp(np.dot(wo.t[vocabulary[w]], wi[vocabulary[input_word]]))
             for w in vocabulary))
    print word, p

king 0.135793930736
dwarf 0.143640274055
queen 0.141911808054
poisons 0.144219025126
loves 0.14461048255
the 0.1457335424
hates 0.144090937079

multi-word context[20]  

   figure taken from (rong 2014)

   the model described above is the cbow architecture of id97.
   however, we assumed that the context $c$ was only a single input word.
   this allowed us to simply copy the input vector to the hidden layer. if
   the context $c$ comprises multiple words, instead of copying the input
   vector we take the mean of their input vectors as our hidden layer:
   $$h = \frac{1}{c} (w_1 + w_2 + \ldots + w_c)$$

   the update functions remain the same except that for the update of the
   input vectors, we need to apply the update to each word in the contect
   $c$:
   $$w^{(t)}_{w_i} = w^{(t-1)}_{w_i} - \frac{1}{c} \cdot \nu \cdot eh$$

   let's see that in action. again assume the target word is king. the
   context consists of two words: queen and loves.
   in [18]:
target_word = 'king'
context = ['queen', 'loves']

   we first take the average of the two context vectors:
   in [19]:
h = (wi[vocabulary['queen']] + wi[vocabulary['loves']]) / 2

   then we apply the hidden-to-output layer update:
   in [20]:
for word in vocabulary:
    p_word_context = (np.exp(np.dot(wo.t[vocabulary[word]], h)) /
                            sum(np.exp(np.dot(wo.t[vocabulary[w]], h)) for w in
vocabulary))
    t = 1 if word == target_word else 0
    error = t - p_word_context
    wo.t[vocabulary[word]] = wo.t[vocabulary[word]] - learning_rate * error * h
print wo

[[-0.08162148  0.25512914 -0.04589425 -0.06655692 -0.04307783 -0.05847402
   0.0063956 ]
 [-0.02294998  0.25188627 -0.0543705   0.03898171 -0.02731232  0.00766842
  -0.03505252]
 [-0.04383296  0.13491036  0.01606604  0.02825053  0.02080841 -0.03020226
  -0.02922364]]

   finally we update the vector of each input word in the context:
   in [21]:
for input_word in context:
    wi[vocabulary[input_word]] = (wi[vocabulary[input_word]] - (1. / len(context
)) *
                                  learning_rate * wo.sum(1))

   in [22]:
h = (wi[vocabulary['queen']] + wi[vocabulary['loves']]) / 2
for word in vocabulary:
    p = (np.exp(np.dot(wo.t[vocabulary[word]], h)) /
               sum(np.exp(np.dot(wo.t[vocabulary[w]], h)) for w in vocabulary))
    print word, p

king 0.129518690596
dwarf 0.145150475243
queen 0.143019721408
poisons 0.145122173218
loves 0.146255939585
the 0.146300899102
hates 0.144632100847

paragraph vector[21]  

   hopefully we know have a better understanding of how id97 learns
   continuous id27s from texts. note that in order to
   efficiently apply this algorithm to real texts, we need some more
   tricks which i won't cover here. for now i would like to proceed with
   the paragraph vector described in le & mikolov (2014).

   the paragraph vector model attempts to learn fixed-length continuous
   representations from variable-length pieces of text. these
   representations combine bag-of-words features with word semantics and
   can be used in all kinds of nlp applications.

   if you read the paper, i think it will be clear by now that the
   paragraph vector is only a very small extension of the original model.
   similar to the id97 model, the paragraph vector model also attemts
   to predict the next word in a sentence. the only real difference, as
   the paper itself states, is with the computation of $h$. where in the
   original model $h$ is based solely on $w$, in the new model we add
   another matrix called $d$, representing the vectors of paragraphs:

   a paragraph token can be thought of as yet another word token except
   that (at least in the paper) all paragraph vectors are unique, whereas
   word tokens share their vector representations among different
   contexts. at each step we compute $h$ by concatenating or averaging a
   paragraph vector $d$ with a context of word vectors $c$:
   $$ h = \frac{1}{c} \cdot (d_d + w_1 + w_w + \ldots + w_c)$$

   the weight update functions are the same as in id97 except that we
   now also update the paragraph vectors. this first model is called the
   distributed memory model of paragraph vectors. le & mikolov present
   another model called distributed id159 of paragraph
   vector. this model ignores the context $c$ and attempts to predict a
   randomly sampled word from a randomly sampled context window.

   let's have a more detailed look at the dm model. in addition to the
   matrix $w$ we need to randomly initialize a matric $d$ with dimensions
   $p \times n$ where $p$ is the number of paragraphs or whatever textual
   unit we use, and $n$ is the number of dimensions.
   in [27]:
v, n, p = len(vocabulary), 3, 5
wi = (np.random.random((v, n)) - 0.5) / n
wo = (np.random.random((n, v)) - 0.5) / v
d =  (np.random.random((p, n)) - 0.5) / n

   say out corpus consists of the following five sentences (paragraphs):
   in [28]:
sentences = ['snowboarding is dangerous', 'skydiving is dangerous',
             'escargots are tasty to some people', 'everyone loves tasty food',
             'the minister has some dangerous ideas']

   we first convert the sentences into a vectorial bow representation:
   in [29]:
vocabulary = vocabulary()
sentences_bow = list(docs2bow(sentences, vocabulary))
sentences_bow

   out[29]:
[[0, 1, 2],
 [3, 1, 2],
 [4, 5, 6, 7, 8, 9],
 [10, 11, 6, 12],
 [13, 14, 15, 8, 2, 16]]

   next we compute the posterior id203 for each word in the
   vocabulary given the concatenation and averaging of the first paragraph
   and the context word snowboarding. we compute the error and update the
   hidden-to-output layer weights.
   in [30]:
target_word = 'dangerous'
h = (d[0] + wi[vocabulary['snowboarding']]) / 2
learning_rate = 1.0

for word in vocabulary:
    p = (np.exp(np.dot(wo.t[vocabulary[word]], h)) /
                      sum(np.exp(np.dot(wo.t[vocabulary[w]], h)) for w in vocabu
lary))
    t = 1 if word == target_word else 0
    error = t - p
    wo.t[vocabulary[word]] = (wo.t[vocabulary[word]] - learning_rate * error * h
)
print wo

[[-0.01599941  0.02135301  0.0927715  -0.00565041 -0.00361651 -0.01454073
   0.02333261  0.00211833  0.00254255  0.02315315 -0.01917578  0.00724787
  -0.00117272 -0.02043504  0.00593186 -0.0166333  -0.0306218 ]
 [ 0.0089237  -0.00397806 -0.13199195  0.02555059 -0.02095756 -0.00978333
   0.01561624  0.03603476 -0.02114407 -0.01552016  0.01289922  0.00119743
  -0.00112818  0.01708133  0.00765248  0.02442374  0.01109005]
 [-0.01205008 -0.03123478  0.05878695  0.02615259 -0.01025209 -0.00442044
   0.00311309  0.01554668  0.02344194  0.00602561 -0.03117694  0.01368817
   0.00858936 -0.00223242 -0.01141366 -0.01719967 -0.01400046]]

   we backpropagate the error to the input-to-hidden layer as follows:
   in [31]:
eh = wo.sum(1)
wi[vocabulary['snowboarding']] = wi[vocabulary['snowboarding']] - 0.5 * learning
_rate * eh
d[0] = d[0] - 0.5 * learning_rate * eh

experiments[22]  

   le & mikolov evaluate and investigate the performance of the paragraph
   vectors on a number of different tasks. i will briefly discuss them
   here.

id31[23]  

   in the first experiment, the authors address the task of sentiment
   analysis. they make use of the stanford sentiment treebank dataset
   which is a manually annotated data set containing 11855 sentences taken
   from the movie review site rotten tomatoes. each sentence in the data
   set has been assigned a label on a scale of negative to positive. the
   task is to predict these labels.

   le & mikolov train paragraph vectors using both the dm model and the
   dbow model. these two representations are concatenated for each
   training instance and fed to a id28 classifier that
   makes a prediction for each unseen test sentence. they compare the
   performance of the model to a number of different models:

   le & mikolov then move on beyond the sentence level and evaluate their
   model on the imdb data set. training and testings follows the same
   procedure as before. the results presented below suggest a strong
   improvement compared to the other reported models:

information retrieval[24]  

   the authors then turn to an experiment in information retrieval. they
   develop a task in which the goal is to predict which of three
   paragraphs isn't the result of the same query. they construct a data
   set on the basis of the search results of a search engine. for each
   query they create a triplet of paragraphs: two paragraphs are results
   from the same query and one is randomly sampled from the rest of the
   collection (result from a different query). the pairwise distances
   between each member of a triplet, should then reflect which two results
   belong to the same query and which snippet is the outlier. the
   following table shows quite convincingly how well the paragraph vectors
   are able to perform on this task compared to the other methods:

comments / points of critique[25]  

general remarks[26]  

   although the proposed model is only a small step beyond the original
   id97 model (and some of the writings is quite sloppy), i think it
   is a really clever one with much potential for many different
   applications. the ease with which the model can be applied to text
   pieces of variable length is perhaps the strongest advantage of the
   model.

availability of code / experimentation details[27]  

   no implementation was available (even not to the second author, which
   led to some serious doubts about the reproducability of the results:

   after a nice series of discussion, the first author finally made a
   suggestion on how to improve the results and actually come to the same
   performance as reported in the paper:

   it is applaudable for the authors to interact and respond to comments
   by the readers. on the other hand, if they would have developed an
   implementation similar to the original id97, this discussion (and
   the resulting doubts in the method) wouldn't have to take place. the
   authors report a number of hyperparameters in the paper, such as the
   window size and the number of dimensions of the vectors. however, some
   crucial parameters weren't mentioned (such as the use of hierarchical
   softmax verus negative sampling). again, in order to reproduce the
   results, the authors must specify in much greater detail how they
   performed the experiments.
   in [ ]:

     __________________________________________________________________

   in [1]:
from ipython.core.display import html
def css_styling():
    styles = open("custom.css", "r").read()
    return html(styles)
css_styling()

   out[1]:
   /* placeholder for custom user css mainly to be overridden in
   profile/static/custom/custom.css this will always be an empty file in
   ipython */
   in [ ]:


   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [28]fastly, rendered by [29]rackspace

   nbviewer github [30]repository.

   nbviewer version: [31]33c4683

   nbconvert version: [32]5.4.0

   rendered (fri, 05 apr 2019 18:15:47 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb
   5. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb
   6. https://github.com/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb
   7. https://mybinder.org/v2/gh/fbkarsdorp/doc2vec/master?filepath=doc2vec.ipynb
   8. https://raw.githubusercontent.com/fbkarsdorp/doc2vec/master/doc2vec.ipynb
   9. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/tree/master
  10. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/tree/master/doc2vec.ipynb
  11. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#distributed-representations-of-sentences-and-documents
  12. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#---quoc-le-&-tomas-mikolov-(2014)
  13. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#review-of-le-&-mikolov-(2014)-and-introduction-into-id97
  14. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#introduction-into-id97
  15. http://arxiv.org/abs/1411.2738
  16. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#softmax-regression
  17. http://arxiv.org/abs/1411.2738
  18. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#updating-the-hidden-to-output-layer-weights
  19. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#updating-the-input-to-hidden-layer-weights
  20. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#multi-word-context
  21. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#paragraph-vector
  22. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#experiments
  23. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#sentiment-analysis
  24. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#information-retrieval
  25. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#comments-/-points-of-critique
  26. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#general-remarks
  27. https://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb#availability-of-code-/-experimentation-details
  28. http://www.fastly.com/
  29. https://developer.rackspace.com/?nbviewer=awesome
  30. https://github.com/jupyter/nbviewer
  31. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  32. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
