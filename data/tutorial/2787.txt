   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

id193 in tensorflow (with sample code)

   [9]go to the profile of dev nag
   [10]dev nag (button) blockedunblock (button) followfollowing
   dec 8, 2016

   tl; dr: deep learning networks can be applied to variable-length
   targets, meaning you can index into arbitrary text, time series, or any
   sequence of data your selfish little heart desires

   i recently read a [11]paper that described a new state-of-the-art
   result on the [12]stanford id53 dataset (squad). the
   performance (f1 of 70%) was impressive, but especially interesting was
   an architectural capability that i hadn   t seen when it came out a year
   earlier in the literature         the ability to compute variable-length
   id203-distributions with a fixed architecture, over arbitrary
   inputs.

   the squad task is a nice step on the way to linguistic ai; you
   basically get a medium-length text passage, a variety of questions
   about that passage, and then text answers. the catch         and what makes
   this task more    feasible    than full-fledged q&a         is that the answer
   has to be a contiguous sequence of letters or words in the original
   passage. in other words, the answer to any of the questions has to be a
   set of two pointers: one pointer to the start of the    answer range    and
   one pointer to the end (say, from character 47 to character 65, or word
   14 through word 17).
   [1*pfnhbm6yziumdl5_quwm_g.png]
   squad example: note that the answer to question #2 on the right is
      composite number   , which is found (verbatim) in the passage to
   the left

   obviously, not all questions can be answered in such a way, so this is
   something of an    easy subset    of all possible, relevant questions
   (   easy    being relative, of course). but if you actually peruse the
   dataset over a chilled glass of pinot grigio, it   s fascinating just how
   many meaningful questions fit into this merest haiku of a linguistic
   task.

   now, let   s describe the naive way that you might try to attack this
   problem. typically, your output layer will be a vector that   s either a
   distributed representation (maybe a so-called    sentence vector    or
      thought vector   ) or a one-hot representation (representing a
   id203 distribution over the set of vector slots/dimensions).

   but there   s a problem.

   you have to pick an output size. you have to train that final matrix
   with some fixed output dimensions (whether 10 or 10,000) and that   s the
   size of the output vector you   re going to get.

   how do we apply this to squad? what if one passage is 300 words long
   and another is 3,000 words long? maybe you should pick the longest
   length passage you   ll accept (say, 10,000) and just hope that 1.) you
   won   t be wasting too much computation time training shorter passages on
   a giant architecture and that 2.) learning will actually transfer well
   across different passage lengths inside this fixed-length box.
   unfortunately, this approach is just as fragile as it sounds.

   there   s a better way.

   the state-of-the-art squad paper described above ([13]wang & jiang,
   2016) used some common designs (heavy use of bidirectional lstms,
   alignment matrices as used in translation tasks, etc.) but also
   mentions a technique called [14]id193 that is, indeed, that
   better way.

   here   s the basic idea, architecturally: we   ll train a fixed-size
   network but map it over variable-length input to get variable-length
   output (pointers).

   to do this, we start with the [15]sequence-to-sequence design pattern
   explained here, folding the input sequence (whatever the length) into a
   fixed-size hidden state. then, we   ll unfold that hidden state into a
   series of soft    pointers            id203 distributions over the input
   sequence. in the squad example above, as well as our coming example,
   there are two pointers (start and end), so we unfold the hidden state
   twice         the first time, to get the    start pointer    id203
   distribution over all the inputs, and the second time, to get the    end
   pointer    id203 distribution over all the inputs. (if we had a
   different problem that required one or three or fourteen pointers, we
   would have to unfold the hidden state one or three or fourteen times.)

   here   s a schematic of the whole pipeline:
   [1*9lrchllhnq03kej0yyb7ha.png]
   from question to answer, with a bunch of scalding hot gpus in
   the middle

   these id193 are a particular example of what   s known as
   content-based attention         using the values of the incoming data to
   decide dynamically where to    attend    (or    point   , with the
   pointers/indices). this can be contrasted with location-based
   attention, which basically says    keeping looking in position x    for the
   datum of interest.
     __________________________________________________________________

   let   s try out some code on a toy problem. id193 are really
   most relevant for recurrency-sensitive data sequences, so we   ll create
   one. suppose we assume our input data is a sequence of integers between
   0 and 10 (with possible duplicates) of unknown length. each sequence
   always begins with low integers (random values between 1 to 5), has a
   run of high integers (random values between 6 to 10), then turns low
   again to finish (1 to 5).

   for example, a sequence might be    4,1,2,3,1,1,6,9,10,8,6,3,1,1   , with
   the run of high integers in bold, surrounded by runs of low integers.
   we want to train a network that can point to these two change
   points         the beginning and end of the run of highs in the middle,
   regardless of the sequence length.

   for a moment, think about why this problem is difficult:
     * first of all, there   s an invisible dividing line between    high    and
          low    that we   re not explicitly telling the pointer network about.
     * it   s also not about the raw value of any individual element, but
       about the values before and after         about the context around each
       element.
     * and finally, we   re training the network to see the first high
       value, but not to see the first low value after the high
       segment         instead, it has to point to the last high value, before
       the low values start showing up again. in other words, you could
       never solve this problem in a streaming fashion because it would
       require knowledge of the future.

   here are some longer sample sequences:
   [1*g_1d9rdk-qdmyufmm0zysq.png]
   three separate low/high/low sequences, with 0-padding at the end, and
   the start (^1) and end (^2) pointers directly under each sequence

   note that these sequences have different lengths         the second sequence
   is the longest (ending in 4 1 2 2) and the third is the shortest
   (ending in 2 2 5 3). the pointers (^1 and ^2) point to the beginning
   and end of the high sequence in the middle         for the first sequence,
   they point to the    7    and the    6    in the high run of    7 10 6 8 10 7 8
   6   , for example.

   to make this problem even harder (and the solution more impressive,
   natch), we   re going to train the pointer network on sequences that have
   longer segments (longer than ten integers) of highs/lows, but then test
   it on sequences that have shorter segments (shorter than ten integers).
   so not only are we testing on sequences that were never seen in
   training; we   re training on schema that were never seen on
   training         just to see if the network has truly generalized our
   pattern.
   [1*nftmsb23s4lntbdshjgofw.png]
   note that loss trajectories are often non-monotonic. have faith.

   during training on the longer sequences, the pointer network takes a
   while to break through the initial plateau of 0.11, but then quickly
   begins to figure out what   s up with this data, with the loss dropping
   to < 0.01 after about 2000 training steps.

   now, how about id136? can training on these sequences (of 33   60
   integers, with segments of 11   20 integers each) possibly help when
   training sequences that are only 15   30 integers?
   [1*f3f7ea8b1yu2d-rg-dkvdg.png]

   during a typical run, on out-of-sample data, we find the correct
   start/end indices 96% of the time.

   on sequences of completely different lengths than what we trained on.

   not bad.
     __________________________________________________________________

   you   ve gone as far as you can without running the code, young padawan.
   [16]try it out yourself!

     * [17]machine learning
     * [18]deep learning
     * [19]id193
     * [20]artificial intelligence
     * [21]tensorflow

   (button)
   (button)
   (button) 262 claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [22]go to the profile of dev nag

[23]dev nag

   ai/ml & startup strategy. founder/cto @ wavefront (acq. by vmware).
   former google, paypal engineer. stanford math.

     * (button)
       (button) 262
     * (button)
     *
     *

   [24]go to the profile of dev nag
   never miss a story from dev nag, when you sign up for medium. [25]learn
   more
   never miss a story from dev nag
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/14645063f264
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@devnag/pointer-networks-in-tensorflow-with-sample-code-14645063f264&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@devnag/pointer-networks-in-tensorflow-with-sample-code-14645063f264&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@devnag?source=post_header_lockup
  10. https://medium.com/@devnag
  11. https://arxiv.org/abs/1608.07905
  12. https://rajpurkar.github.io/squad-explorer/
  13. https://arxiv.org/abs/1608.07905
  14. https://arxiv.org/abs/1506.03134
  15. https://medium.com/@devnag/id195-the-clown-car-of-deep-learning-f88e1204dac3#.tpycp5mhw
  16. https://github.com/devnag/tensorflow-pointer-networks
  17. https://medium.com/tag/machine-learning?source=post
  18. https://medium.com/tag/deep-learning?source=post
  19. https://medium.com/tag/pointer-networks?source=post
  20. https://medium.com/tag/artificial-intelligence?source=post
  21. https://medium.com/tag/tensorflow?source=post
  22. https://medium.com/@devnag?source=footer_card
  23. https://medium.com/@devnag
  24. https://medium.com/@devnag
  25. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  27. https://medium.com/p/14645063f264/share/twitter
  28. https://medium.com/p/14645063f264/share/facebook
  29. https://medium.com/p/14645063f264/share/twitter
  30. https://medium.com/p/14645063f264/share/facebook
