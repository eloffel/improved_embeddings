     *
     * [1]richard socher
     * [2]publications
     * [3]deep learning tutorial
     * [4]notes & misc.
     * [5]photography
     * contact

improving word representations via global context and multiple word
prototypes

   unsupervised word representations are very useful in nlp tasks both as
   inputs to learning algorithms and as extra word features in nlp
   systems. however, most of these models are built with only local
   context and one representation per word. this is problematic because
   words are often polysemous and global context can also provide useful
   information for learning word meanings. we present a new neural network
   architecture which 1) learns id27s that better capture the
   semantics of words by incorporating both local and global document
   context, and 2) accounts for homonymy and polysemy by learning multiple
   embeddings per word. we introduce a new dataset with human judgments on
   pairs of words in sentential context, and evaluate our model on it,
   showing that our model outperforms competitive baselines and other
   neural language models.

paper download

     * paper: [6]huangsochermanning_acl2012.pdf

word vectors and code

     * our final word vectors and code to disambiguate between multiple
       word vectors in context [7]vectors and testing code
     * if you want to train the model on another language or your own
       corpus: [8]training code
     * our method can also provide a single vector for each word. you can
       download these vectors as a text file
       [9]acl2012_wordvectorstextfile.zip (14mb)
          + this includes one text file with the vectors, one text file
            with the word list.
          + if you want them in one file, just type
            paste vocab.txt wordvectors.txt
     * this file contains the 10 word vectors for the frequent words in
       text format: [10]acl2012_wordvectorstextfile_multiple.zip (30mb)
     * jeremy salwen created a fast c++ version for parts of this project:
       [11]https://github.com/antonyms/cmultivec

dataset: stanford's contextual word similarities (scws)

     * download link: [12]scws.zip (dataset)
     * description:
       each line in ratings.txt consists of a pair of words, their
       respective
       contexts, the 10 individual human ratings, as well as their
       averages. the
       target word is surrounded by <b>...</b> in its context. each line
       is tab-
       delimited with the following format:
       <id> <word1> <pos of word1> <word2> <pos of word2> <word1 in
       context> <word2 in context> <average human rating> <10 individual
       human ratings>

training corpus

     * if you want to train your own word vectors and properly compare to
       the ones we trained, you can use the same corpus:
       [13]http://nlp.stanford.edu/data/westburylab.wikicorp.201004.txt.bz
       2 (1.7gb)

bibtex

     * @inproceedings{huangetal2012,
       author = {eric h. huang and richard socher and christopher d.
       manning and andrew y. ng},
       title = {{improving word representations via global context and
       multiple word prototypes}},
       booktitle = {annual meeting of the association for computational
       linguistics (acl)},
       year = 2012
       }

comments

   add comment
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   sign as author  ________________________________
   enter code:
   [improvingwordrepresentationsviaglobalcontextandmultiplewordprototypes?
   action=captchaimage&amp;captchakey=1] _____ post reset

[14]jiangnan[15]?     07 february 2018, 05:48

 socher and manning will have all these word vectors with really great performan
ce

[16]richardsocher     24 july 2014, 09:27

   @anon, a new paper by pennington, socher and manning will have all
   these word vectors with really great performance :)

[17]anon[18]?     15 april 2014, 23:57

   3. is it also possible for you to release the vectors for a larger
   vocab size of 1m to 2m ?

[19]anon[20]?     15 april 2014, 23:55

   1. may i know how long does your code take to learn these vectors ? 2.
   is it possible for you guys to release the vectors of size - 100, 300 ,
   600 and 1000.

references

   visible links
   1. https://www.socher.org/index.php/main/homepage
   2. https://www.socher.org/index.php/main/homepage#publications
   3. https://www.socher.org/index.php/deeplearningtutorial/deeplearningtutorial
   4. https://www.socher.org/index.php/main/notesandcodes
   5. https://www.socher.org/index.php/main/photographyandtravel
   6. http://www.socher.org/uploads/main/huangsochermanning_acl2012.pdf
   7. http://www-nlp.stanford.edu/~ehhuang/wordrep.zip
   8. http://www-nlp.stanford.edu/~ehhuang/trainemb.zip
   9. http://nlp.stanford.edu/~socherr/acl2012_wordvectorstextfile.zip
  10. http://nlp.stanford.edu/~socherr/acl2012_wordvectorstextfile_multiple.zip
  11. https://github.com/antonyms/cmultivec
  12. http://www-nlp.stanford.edu/~ehhuang/scws.zip
  13. http://nlp.stanford.edu/data/westburylab.wikicorp.201004.txt.bz2
  14. https://www.socher.org/index.php/profiles/jiangnan?action=edit
  15. https://www.socher.org/index.php/profiles/jiangnan?action=edit
  16. https://www.socher.org/index.php/profiles/richardsocher
  17. https://www.socher.org/index.php/profiles/anon?action=edit
  18. https://www.socher.org/index.php/profiles/anon?action=edit
  19. https://www.socher.org/index.php/profiles/anon?action=edit
  20. https://www.socher.org/index.php/profiles/anon?action=edit

   hidden links:
  22. https://www.socher.org/uploads/main/multiplevectorwordembedding.pdf
