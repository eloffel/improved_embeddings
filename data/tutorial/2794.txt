     *
     * [1]richard socher
     * [2]publications
     * [3]deep learning tutorial
     * [4]notes & misc.
     * [5]photography
     * contact

dynamic pooling and unfolding recursive autoencoders for paraphrase detection

   paraphrase detection is the task of examining two sentences and
   determining whether they have the same meaning. in order to obtain high
   accuracy on this task, thorough syntactic and semantic analysis of the
   two statements is needed. we introduce a method for paraphrase
   detection based on recursive autoencoders (rae). our unsupervised raes
   are based on a novel unfolding objective and learn feature vectors for
   phrases in syntactic trees. these features are used to measure the
   word- and phrase-wise similarity between two sentences. since sentences
   may be of arbitrary length, the resulting matrix of similarity measures
   is of variable size. we introduce a novel dynamic pooling layer which
   computes a fixed-sized representation from the variable-sized matrices.
   the pooled representation is then used as input to a classifier. our
   method outperforms other state-of-the-art approaches on the challenging
   msrp paraphrase corpus.

   an overview of our paraphrase model. the recursive autoencoder learns
   phrase features for each node in a parse tree. the distances between
   all nodes then fill a similarity matrix whose size depends on the
   length of the sentences. using a novel dynamic pooling layer we can
   compare the variable-sized sentences and classify pairs as being
   paraphrases or not.

download paper

     * download: [6]socherhuangpenningtonngmanning_nips2011.pdf

download code

full paraphrase system

     * this code classifies two sentences as either being paraphrases or
       not
     * download: [7]classifyparaphrases.zip [160 mb]
     * the code includes the word look-up table from [8]joseph turian,
       pre-processed and in matlab format.
     * the code also includes the [9]stanford parser.

computing compositional vectors

     * this smaller code package computes phrase vector representations
       based on a trained, unfolding id56 as described
       in the above paper. it is designed to be easy to use, all you need
       to do is to put phrases for which you want to compute a
       compositional vector into a text file, one phrase or sentence per
       line. the output will be another textfile with the vectors.
     * download: [10]coderaevectorsnips2011.zip [42 mb]

updated related work

     * this page has a comprehensive list of papers on this task and
       dataset:
       [11]http://aclweb.org/aclwiki/index.php?title=paraphrase_identifica
       tion_%28state_of_the_art%29

bibtex

     * please cite the following paper when you use the data set or code:
       @incollection{socheretal2011:poolrae,
       title = {{dynamic pooling and unfolding recursive autoencoders for
       paraphrase detection}},
       author = {{richard socher and eric h. huang and jeffrey pennington
       and andrew y. ng and christopher d. manning}},
       booktitle = {{advances in neural information processing systems
       24}},
       year = {2011}
       }

comments

   for remarks, criticism or other [12]thoughts on the paper. save what
   you write before you post, then type in the password, post (nothing
   happens), then copy the text and re-post. it's the only way to prevent
   spammers.
   add comment
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   sign as author  ________________________________
   enter code:
   [dynamicpoolingandunfoldingrecursiveautoencodersforparaphrasedetection?
   action=captchaimage&amp;captchakey=1] _____ post reset

[13]griffin adams[14]?     27 september 2017, 20:35

   a few questions and thoughts as i try to implement and extend your
   results...

   1. can you explain exactly what is meant by ensuring each entry in the
   pooled similarity matrix has mean 0 and variance 1. is this per matrix,
   per element per mini-batch?

   2. also, did you experiment with other approaches to classification
   besides simply flattening the similarity matrix and performing binary
   id28?

   3. is there an updated version of this general approach you think seems
   promising? i.e. with attention or more complex compositional functions
   (tree-[15]lst ms[16]?, different weight matrices for different pos
   combinations, etc.)

   thanks so much!

[17]ankit sood[18]?     06 may 2017, 20:33

   hi richard,

   is there a python implementation available?

[19]ag[20]?     23 september 2015, 11:45

   hello richard, i am running the full paraphrase system code, but it is
   throwing error while loading savedparams/params.mat file. the error is
   "no such file,
   '/juice/scr21/scr/ehhuang/projects/deeprae/code/norm1tanh.m" i think
   this variable should be "code/norm1tanh.m". can you please tell me how
   to resolve this error? thanks...!!

[21]willie[22]?     19 april 2015, 03:03

   i would also like to know how to retrain the rae model.

[23]connie[24]?     25 march 2015, 08:28

   hi richard, i would like to know can i retrain the rae model? bests,
   connie

[25]richardsocher     12 february 2015, 22:52

   hi connie, yes, you just need to adjust the training data and train it
   on two languages. best, richard

[26]connie tong[27]?     03 february 2015, 05:20

   hi richard, i would like to know whether this paraphrase system can
   apply on bilingual language or not?

   bests, connie

[28]richardsocher     24 july 2014, 09:25

   @[29]d mr[30]?, there is a flag you can set to binarize trees, you can
   find it in the code of our sentiment model from emnlp 2013 and the
   newest [31]core nlp[32]? package.

[33]richardsocher     24 july 2014, 09:24

   @sriram bhargav yes, if you retrain the model you can use any set of
   word vectors you want, e.g. from our acl 2012 paper, or a new set from
   pennington et al 2014 (see front page for link in a few days) or
   id97. you can probably get an improvement in performance by using a
   larger vocabulary. best, richard

[34]dmr[35]?     04 july 2014, 00:21

   to clarify, in the following tree, there are two np nodes with 3 child
   leaves:

   (root
  (s
    (np (nnp shisheh))
    (vp (vbz is)
      (np
        (np (dt a) (nn village))
        (pp (in in)
          (np (nnp hemmatabad) (nnp rural) (nnp district))))
      (, ,)
      (pp (in in)
        (np
          (np (dt the) (nnp central) (nnp district))
          (pp (in of)
            (np
              (np (nnp borujerd) (nnp county))
              (, ,)
              (np
                (np (nnp lorestan) (nnp province))
                (, ,)
                (np (nnp iran))))))))
    (. .)))

   would you just qualify those children as named entities and average the
   leaves on the same level?

   how would you have determined to concatenate the child vectors?

[36]dmr[37]?     03 july 2014, 23:50

   hi richard,
  thanks for all the research and code, it's a great help to all the lowly datam
iners out there.   i'm wondering how you opted to construct the binary parse tre
es for a given sentence during unsupervised training.  unless i'm missing someth
ing, the stanford parser won't restrict itself to binary (p->c1,c2) structures.


   thanks!

[38]sriram bhargav[39]?     29 may 2014, 08:07

   hi socher, i have a question. are the values in params.mat independent
   of vars.normalized.100.mat? when i tried to use your code(
   coderaevectorsnips2011) for computing compositional vectors i found out
   that many words are detected as *unknown*. is there any way to get 50
   or 200 dimension phrase vector instead 100?

[40]richardsocher     13 april 2014, 23:51

   yes, we're only publishing the code for training the paraphrase
   classifier on top. the other code is trained on a large corpus and hard
   to package up nicely. in our most recent tacl paper we have a new id56
   on dependency trees. i think it is even more powerful but haven't tried
   it on the id141 task yet.

[41]yin hang[42]?     12 march 2014, 03:02

   very elegant work! my confusion on variable matrix is almost tackled
   out by your paper.

[43]mike[44]?     06 february 2014, 06:57

   hi richard, i too am interested in training an unfolding rae, but i am
   not sure what you meant by "you can use the training code from this
   website to see." in your last comment. there is no training code above.
   are you saying you won't release it, and we should come up with our own
   training port to the pre-trained code above? thanks, mike.

[45]mike[46]?     06 february 2014, 06:57

   hi richard, i too am interested in training an unfolding rae, but i am
   not sure what you meant by "you can use the training code from this
   website to see." in your last comment. there is no training code above.
   are you saying you won't release it, and we should come up with our own
   training port to the pre-trained code above? thanks, mike.

[47]richardsocher     29 december 2013, 10:51

   hi, it takes at most 5h to train. you can use the training code from
   this website to see. best, richard

[48]bhanu[49]?     23 december 2013, 11:02

   hi socher,

   i am implementing this model as part of my thesis work. could you
   please tell me how much time did it take to train unfolded rae?

   thanks in advance.

[50]wilsoncao[51]?     06 october 2013, 10:14

   hi richard, recently i have been studying your code above. however, i
   can't find any details about the rae training algorithm in your code.
   can i get your rae training algorithm implementation for academic study
   purpose. thx.

[52]wilsoncao[53]?     06 october 2013, 10:14

   hi richard, recently i have been studying your code above. however, i
   can't find any details about the rae training algorithm in your code.
   can i get your rae training algorithm implementation for academic study
   purpose. thx.

[54]richardsocher     20 august 2013, 05:32

   hi tian, it highly depends on your corpus. are there many different
   words that are unknown in this vocabulary? are the new pairs much
   longer or shorter? for shorter phrases you may want to use a smaller
   similarity matrix. best, richard

[55]tian[56]?     08 august 2013, 12:57

   i follow reame.txt to replace the input.txt with new corpus and the
   performance is much lower than results in your paper, so i guess that
   we need to retrain the model with the new corpus, how to make training,
   could you help me,

[57]tian[58]?     08 august 2013, 12:35

   hi richard i try to use your data and code , it is ok for your
   input.txt, and i get some input sentences , and i want to test the
   performance, i read your paper, and find that you train the model with
   a part of ms corpus and then test the rest part of ms corpus&#65292;is
   it right, as i want to test new input, i need to train my corpus, is it
   right, so how to use your code for training model, could you help me

references

   1. https://www.socher.org/index.php/main/homepage
   2. https://www.socher.org/index.php/main/homepage#publications
   3. https://www.socher.org/index.php/deeplearningtutorial/deeplearningtutorial
   4. https://www.socher.org/index.php/main/notesandcodes
   5. https://www.socher.org/index.php/main/photographyandtravel
   6. https://www.socher.org/uploads/main/socherhuangpenningtonngmanning_nips2011.pdf
   7. http://nlp.stanford.edu/~socherr/classifyparaphrases.zip
   8. http://metaoptimize.com/projects/wordreprs/
   9. http://nlp.stanford.edu/software/lex-parser.shtml
  10. http://nlp.stanford.edu/~socherr/coderaevectorsnips2011.zip
  11. http://aclweb.org/aclwiki/index.php?title=paraphrase_identification_(state_of_the_art)
  12. http://www.phdcomics.com/comics.php?f=1178
  13. https://www.socher.org/index.php/profiles/griffinadams?action=edit
  14. https://www.socher.org/index.php/profiles/griffinadams?action=edit
  15. https://www.socher.org/index.php/main/lstms?action=edit
  16. https://www.socher.org/index.php/main/lstms?action=edit
  17. https://www.socher.org/index.php/profiles/ankitsood?action=edit
  18. https://www.socher.org/index.php/profiles/ankitsood?action=edit
  19. https://www.socher.org/index.php/profiles/ag?action=edit
  20. https://www.socher.org/index.php/profiles/ag?action=edit
  21. https://www.socher.org/index.php/profiles/willie?action=edit
  22. https://www.socher.org/index.php/profiles/willie?action=edit
  23. https://www.socher.org/index.php/profiles/connie?action=edit
  24. https://www.socher.org/index.php/profiles/connie?action=edit
  25. https://www.socher.org/index.php/profiles/richardsocher
  26. https://www.socher.org/index.php/profiles/connietong?action=edit
  27. https://www.socher.org/index.php/profiles/connietong?action=edit
  28. https://www.socher.org/index.php/profiles/richardsocher
  29. https://www.socher.org/index.php/main/dmr?action=edit
  30. https://www.socher.org/index.php/main/dmr?action=edit
  31. https://www.socher.org/index.php/main/corenlp?action=edit
  32. https://www.socher.org/index.php/main/corenlp?action=edit
  33. https://www.socher.org/index.php/profiles/richardsocher
  34. https://www.socher.org/index.php/profiles/dmr?action=edit
  35. https://www.socher.org/index.php/profiles/dmr?action=edit
  36. https://www.socher.org/index.php/profiles/dmr?action=edit
  37. https://www.socher.org/index.php/profiles/dmr?action=edit
  38. https://www.socher.org/index.php/profiles/srirambhargav?action=edit
  39. https://www.socher.org/index.php/profiles/srirambhargav?action=edit
  40. https://www.socher.org/index.php/profiles/richardsocher
  41. https://www.socher.org/index.php/profiles/yinhang?action=edit
  42. https://www.socher.org/index.php/profiles/yinhang?action=edit
  43. https://www.socher.org/index.php/profiles/mike?action=edit
  44. https://www.socher.org/index.php/profiles/mike?action=edit
  45. https://www.socher.org/index.php/profiles/mike?action=edit
  46. https://www.socher.org/index.php/profiles/mike?action=edit
  47. https://www.socher.org/index.php/profiles/richardsocher
  48. https://www.socher.org/index.php/profiles/bhanu?action=edit
  49. https://www.socher.org/index.php/profiles/bhanu?action=edit
  50. https://www.socher.org/index.php/profiles/wilsoncao?action=edit
  51. https://www.socher.org/index.php/profiles/wilsoncao?action=edit
  52. https://www.socher.org/index.php/profiles/wilsoncao?action=edit
  53. https://www.socher.org/index.php/profiles/wilsoncao?action=edit
  54. https://www.socher.org/index.php/profiles/richardsocher
  55. https://www.socher.org/index.php/profiles/tian?action=edit
  56. https://www.socher.org/index.php/profiles/tian?action=edit
  57. https://www.socher.org/index.php/profiles/tian?action=edit
  58. https://www.socher.org/index.php/profiles/tian?action=edit
