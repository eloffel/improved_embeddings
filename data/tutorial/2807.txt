     * [1]ali eslami
     * [2]writings

patterns for research in machine learning

18 jul 2012

   here i list a handful of code patterns that i wish i was more aware of
   when i started my phd. each on its own may seem pointless, but
   collectively they go a long way towards making the typical research
   workflow more efficient. and an efficient workflow makes it just that
   little bit easier to ask the research questions that matter.

   my guess is that these patterns will not only be useful for machine
   learning, but also any other computational work that involves either a)
   processing large amounts of data, or b) algorithms that take a
   significant amount of time to execute.

   disclaimer: the ideas below have resulted from my experiences working
   with bash. other ides, languages or frameworks may have better
   solutions for the kinds of problems that i'm trying to address.

always, always use version control.

   this is useful because:
     * it makes it easier to collaborate with others.
     * it makes it easier to replicate experimental results.
     * it makes it easier to work on your code-base from multiple
       computers.

separate code from data.

   create spatial separation between your source code and your data files:
project_name/code/
project_name/data/

   this is useful because:
     * it makes it easier to share your code with others.
     * it makes it easier to swap between datasets.

   even better would be to always assume that your code and your data are
   located independently:
/path/to/code/
/other/path/to/data/

   this is useful because:
     * it reinforces the separation.
     * it's an easy way of hiding your data from revision control.

   the data folder will often be too large to store on your machine, and
   you will have no choice but to separate the two.

separate input data, working data and output data.

   it can be useful to think of data as belonging to three distinct
   categories:
     * input files come with the problem. they never change.
     * working files are generated by your algorithms as they work. they
       always change.
     * output files are generated by your algorithms when they finish work
       successfully. they rarely change.

   the directory structure will now look something like this:
/path/to/code/
/other/path/to/data/input/
/other/path/to/data/working/
/other/path/to/data/output/

   this is useful because:
     * you know that you can safely delete files from working/ (e.g. to
       save space). these files can, in theory, be regenerated simply by
       running your code again.
     * it makes it easy to share the results in output/ with others (e.g.
       in presentations, or as input to latex documents).

modify input data with care.

     * always keep the raw data as distributed. keep a note of where you
       got it from, together with any read-me or licensing files that came
       with it.
     * write a one-touch script to convert the raw data into whatever
       format you use for your own code.
     * don't ever clean data by hand, and if you do, document it
       thoroughly, change by change.

save everything to disk frequently.

     * save the model parameters to disk at suitable intervals.
     * if a figure is useful for run-time diagnosis, then it should
       probably be saved to disk too.
     * when you run your algorithm on different datasets, store the output
       in separate folders.
     * store the output of each day's work in a separate folder.

   this is what the working folder might look like:
working/18_07_2012/dataset_1/
working/18_07_2012/dataset_2/
working/19_07_2012/dataset_1/
working/19_07_2012/dataset_2/

   inside the 18_07_2012/dataset_1 folder you might find:
dataset_1/likelihood_curve_iteration_100.eps
dataset_1/likelihood_curve_iteration_200.eps
dataset_1/likelihood_curve_iteration_300.eps
dataset_1/model_parameters_iteration_100.dat
dataset_1/model_parameters_iteration_200.dat
dataset_1/model_parameters_iteration_300.dat

separate options from parameters.

   i often see code that stores algorithm parameters and model parameters
   in the same data structure. in my experience things work best when the
   two are separated.
     * options specify how your algorithm should run.
     * parameters specify the model, and are usually an output of your
       algorithm.

% set the options
options.run_name = '18_07_2012/dataset_1/';
options.dataset_path = '/other/path/to/data/input/dataset_1.dat';
options.working_path = ['/other/path/to/data/working/' options.run_name];
options.output_path = ['/other/path/to/data/output/' options.run_name];
options.learning_rate = 0.1;
options.num_iterations = 300;

% load the data
data = deserialise(options.dataset_path);

% learn the parameters
parameters = train_model(options, data);

   some parameters will not be affected by the algorithm's execution, e.g.
   model size parameters or the model's hyper-parameters. i store these as
   parameters, but use values specified in options to initialise them.

do not use global variables.

   whenever possible, communicate through function arguments:
% set the options
options = ...

% load the data
data = ...

% learn the parameters
parameters = train_model(options, data);

   and not through global variables:
global options, data;

% set the options
options = ...

% load the data
data = ...

% learn the parameters
parameters = train_model(); % assumes options and data have been set globally

   this is useful because:
     * it makes it much easier to debug your code.
     * it makes it easier to parallelise your code.

record the options used to generate each run of the algorithm.

% set the options
options = ...

% load the data
data = ...

% learn the parameters
parameters = train_model(options, data);

% store the results
serialise(options, 'options.dat', options.working_path);
serialise(parameters, 'parameters.dat', options.working_path);

   this is useful because it makes it easier to reproduce results. for
   completeness you may also want to:
     * consider setting the random number generator seed to a value
       specified in options.
     * consider saving a copy of the code used to execute each run.

make it easy to sweep options.

% set the options
options.learning_rate = 0.1;
options.latent_dimensions = {10, 20};
options.num_iterations = {300, 600};

% load the data
data = ...

% sweep the options
for options_configuration in get_configurations(options)

    % learn the parameters
    parameters = train_model(options, data);

    % store the results
    serialise(parameters, 'parameters.dat'], ...
              [options.working_path '_' options_configuration.name]);

end

   the function get_configurations() can be written in such a way to make
   the above code segment train 4 different models, one for each valid
   combination of variables that are being swept over, and store the
   results in separate directories:
working/latent_dimensions_10_num_iterations_300/
working/latent_dimensions_20_num_iterations_300/
working/latent_dimensions_10_num_iterations_600/
working/latent_dimensions_20_num_iterations_600/

   this is useful because it makes it easier to try out different
   algorithm options. if you have access to a cluster, you can easily use
   this to distribute each run to a different computer.

make it easy to execute only portions of the code.

   if your code can conceptually be thought of as some sort of pipeline
   where computations are made sequentially:

   write your main script in such a way that you can specify which
   computations you want to execute. store the results of each part of the
   computation to disk. for example, the following command runs the
   preprocess_data, initialise_model and train_model scripts.
>> run_experiment('dataset_1_options', '|preprocess_data|initialise_model|train_
model|');

   and this command runs the only the train_model script but also
   evaluates its performance:
>> run_experiment('dataset_1_options', '|train_model|evaluate_model|');

   loading the preprocessed data and initialised model from disk.

   since the run_experiment() function might potentially be performing
   complex tasks such as: loading options from disk, sweeping parameters,
   communicating with a cluster of computers or managing the storing of
   results, you do not want to have to run the script manually for every
   run.

   in my experience, commenting out segments of code to simulate this
   behaviour is a waste of time in the long-run. for complex projects you
   will constantly be switching between work on different parts of the
   pipeline.

use checkpointing.

   your experiments will occasionally fail during execution. this is
   particularly true when many are run in parallel.
     * store the entire state (counters and so on) to disk at suitable
       intervals.
     * write code that, once activated, continues running the algorithm
       from the latest saved state.
     * make sure it is clearly made visible that the algorithm is starting
       from a saved state.

% set the options
options = ...

% load the data
data = ...

if saved_state_exists(options)

    % load from disk
    [parameters, state] = deserialize_latest_params_state(options.working_path);


    % command line output
    disp(['starting from iteration ' state.iteration]);

else

    % initialize
    parameters = init_parameters();
    state = init_state();

end

% learn the parameters
parameters = train_model(options, data, parameters, state);

write demos and tests.

   your project structure will now look like:
project/
    code/
    data/
    demos/
    tests/

   this is useful for reasons which should hopefully be relatively clear!

other thoughts.

     * read [3]the pragmatic programmer by hunt and thomas.
     * always estimate how long you expect an experiment to take to run.
     * keep a journal that explains why you ran each experiment and what
       your findings were.
     * do most of your coding and debugging with datasets that take ~10
       seconds or less to process.
     * make it easy to swap in and out different models and fitness
       measures.
     * also read [4]charles sutton's follow-up post and the [5]hackernews
       discussion on this same topic.

   thanks to [6]iain murray, [7]nicolas heess, [8]jono millin,
   [9]sebastian bitzer, [10]isomorphismes, [11]john langford, [12]yuxi
   luo, [13]bob carpenter and [14]rob lang for comments and suggestions.

     * [15]about
     * [16]research
     * [17]illustrations
     * [18]lab


   [19]ali@arkitus.com

      2002   2016  [20]ali eslami

references

   visible links
   1. http://arkitus.com/
   2. http://arkitus.com/writings/
   3. http://www.amazon.co.uk/gp/product/020161622x/ref=as_li_ss_tl?ie=utf8&camp=1634&creative=19450&creativeasin=020161622x&linkcode=as2&tag=booka0c-21
   4. http://www.theexclusive.org/2012/08/principles-of-research-code.html
   5. http://news.ycombinator.com/item?id=4384317
   6. http://homepages.inf.ed.ac.uk/imurray2/
   7. http://homepages.inf.ed.ac.uk/s0677090/
   8. http://www.jonomillin.com/
   9. http://www.cbs.mpg.de/staff/bitzer-11348
  10. http://isomorphismes.tumblr.com/
  11. http://hunch.net/~jl/
  12. http://havef.github.com/
  13. http://alias-i.com/
  14. http://robsneuron.blogspot.co.uk/
  15. http://arkitus.com/
  16. http://arkitus.com/research/
  17. http://arkitus.com/illustrations/
  18. http://arkitus.com/lab/
  19. mailto:ali@arkitus.com
  20. mailto:ali@arkitus.com

   hidden links:
  22. https://twitter.com/arkitus
  23. http://www.linkedin.com/in/smalieslami
  24. https://www.facebook.com/alieslami
