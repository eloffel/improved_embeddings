   press j to jump to the feed. press question mark to learn the rest of
   the keyboard shortcuts
   (button) r/machinelearning
   ____________________
   [1]log in[2]sign up
   (button)
   user account menu
   r/

machinelearning

   [3]posts
   (button)
   14
   (button)
   posted by
   [4]u/thvasilo
   [5]2 years ago
   archived

kdd panel: is deep learning the new 42?

   [rendertimingpixel.png]
   [6]youtu.be/furfdq...

   iframe:
   [7]https://www.redditmedia.com/mediaembed/515dus?responsive=true

   16 comments
   (button) share
   (button)
   save (button)
   hide (button)
   report
   (button)
   78% upvoted
   this thread is archived
   new comments cannot be posted and votes cannot be cast
   sort by
   (button) best
   [8](button) best[9] (button) top[10] (button) new[11] (button)
   controversial[12] (button) old[13] (button) q&a
   (button)
   (button)
   level 1
   [14]leonoel
   8 points    [15]2 years ago

   yup, this is a very current issue.

   a couple of job interviews i've had.

   q: would you use deep learning to predict stocks?

   a: ehhh. i would use something simpler

   q: but dl is trending

   a: so?

   q: it should work better than the rest.

   q:would you use id98s to analyze time series?

   a: no

   q: why?, they are very good

   a: yes, but they are not suitable to analyze 1d time series

   q: are you sure you know ml
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 2
   [16]rumblestiltsken
   9 points    [17]2 years ago

   i use id98s to analyse time series
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 3
   [18]leonoel
   1 point    [19]2 years ago

   why? there are way better tools.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 4
   [20]rumblestiltsken
   5 points    [21]2 years ago

   a stack of 1d convolutions can do just as well as an id56 for capturing
   mid range structure, and are fast to train. they also scale really well
   with higher dimensional data. the problem is they require fixed size
   input.

   you can also combine them in mixed id56 /id98 networks, which have some
   of the better properties of both.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 5
   [22]leonoel
   1 point    [23]2 years ago

   but how do you convolve 1d time series?
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 6
   [24]rumblestiltsken
   4 points    [25]2 years ago

   what do you mean? as long as you have the whole time series, you just
   convolve in one dimension, like 3 x 1 filter sizes.

   so the first layer filters see 3 time steps, the higher layer filters
   (depending on your step size and pooling) operate over bigger chunks of
   the data.

   just depends what your data is whether it can work well or not.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 7
   [26]leonoel
   2 points    [27]2 years ago

   yeah, but time series (unlike images) are non stationary, the
   correlation between time steps isn't as strong as between pixels in
   images. thus informative convolutions in your training set is not
   necessarily going to be informative in your test set.

   i can't see very informative features coming out of a linear
   convolution. for example an spectrogram is crazy informative, but a id98
   would never learn a fourier transform.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 8
   [28]sieisteinmodel
   2 points    [29]2 years ago

     yeah, but time series (unlike images) are non stationary, the
     correlation between time steps isn't as strong as between pixels in
     images.

   depends on the data, doesn't it? many time series are stationary. and
   in many time series, the correlation between time steps depends on the
   sampling rate.

   e.g. if you have a sensor and the sampling frequency is a hyper
   parameter of your system, it makes perfect sense to use a id98.

     i can't see very informative features coming out of a linear
     convolution. for example an spectrogram is crazy informative, but a
     id98 would never learn a fourier transform.

   given large enough filter sizes, of course it would. plain mlps also
   do. or a mexican hat filter, or sth.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 9
   [30]leonoel
   1 point    [31]2 years ago

   how can large linear convolutions learn convolutions in the sinusoidal
   space?

   a fourier transform are convolutions against sin and cos
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 8
   [32]rumblestiltsken
   1 point    [33]2 years ago

   like i say it depends on the data, and how you present the data. text
   id98s do fine, for example, despite "less structure than images".

   there have been a number of papers that show stacked id98s and unrolled
   id56s are essentially doing the same thing.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 6
   [34]oriolvinyals
   2 points    [35]2 years ago

   fyi (temporal) convolutions are quite much older than id98s, dating back
   to the 1700s according to
   [36]https://en.wikipedia.org/wiki/convolution#historical_developments
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 1
   [37]thvasilo
   original poster2 points    [38]2 years ago

   i this year's kdd the panelists where asked this simple question: "is
   deep learning the answer to everything?"

   with nando de freitas, pedro domingos, isabel guyon, jitendra malik,
   jennifer neville moderated by andrei broder from google.

   as expected the room was [39]overflowing and the discussion was great,
   highly recommended.
   (button) share
   (button) report (button) save
   (button)
   (button)
   level 1
   [40]phillypoopskins
   2 points    [41]2 years ago

   i've whooped the pants off coworkers obsessed w deep learning, just
   using e.g. naive bayes w well crafted features.

   deep learning is only necessary when you need to learn a representation
   at the same time as the model, or if you'd like to build an interesting
   architecture to leverage something about your data.
   (button) share
   (button) report (button) save
   level 2
   comment deleted by user[42]2 years ago
   (button)
   (button)
   level 3
   [43]phillypoopskins
   4 points    [44]2 years ago

   i mean: it definitely does engineer features for you. i wouldn't say
   that's a misconception.

   in images, id98 learn obvious features like gabor filters, edge
   detectors, etc.

   also, any intermediate activation within any network can also can be
   seen as learned features.

   any time you'd have to apply significant processing to a signal to
   extract "meaningful" information, like decomposing it with e.g. a
   fourier transform, you can expect nn's to learn this sort of thing for
   you end-to-end, in a way that's more optimal and problem-specific.

   of course, that happens in large part due to architectural choices;
   nevertheless, learn features, it does.

   the problem i'm talking about is e.g. using something like a conv net
   or a recurrent network for nlp; it does indeed learn features, like
   common ngrams etc, text embeddings and the like.

   however, learning these features end-to-end often isn't as good as hand
   engineering then; and feeding hand engineered features into a network
   instead of coaxing it out of initial layers through architecture isn't
   as good as feeding them into a "shallow" model like a gbm.

   even with the same hand crafted features, i've seen neural nets have
   problems compared to simple or "shallow" models (in high noise data, or
   data w covariate shift in the test set etc, which is most of what i
   come across in the real world).

   there's almost no advantage to using a ffnn on hand crafted features
   compared to e.g. a id79, depending on the use case; and
   certainly it's almost never better than a gbm.
   (button) share
   (button) report (button) save
   level 4
   comment deleted by user[45]2 years ago
   (button)
   (button)
   level 5
   [46]phillypoopskins
   1 point    [47]2 years ago

   for the higgs dataset, for example: what, precisely, do you mean by a
   "well constructed" nn? what architecture did you settle on? and what
   design space were you considering?

   also - how much time did you spend tweaking your network? and how did
   you validate your results?

   i don't doubt that you can tweak a ffnn until you overfit a little on a
   validation set.

   for tabular data, it's pretty much the consensus that empirically,
   xgboost is going to be the best almost every time.

   xgboost won the kaggle higgs competition, if that's the higgs dataset
   you're referring to. i'd be surprised if you got a nn to beat the top
   score without overfitting to the test set.
   (button) share
   (button) report (button) save
   level 6
   comment deleted by user[48]2 years ago
   (button)
   (button)
   level 7
   [49]phillypoopskins
   1 point    [50]2 years ago

   it doesn't sound like you built any knowledge into the architecture;
   just that you may have made a good, standard ffnn with some modern
   components.

   if you'd told me you'd architected it in a non-standard way to take
   advantage of some structure in the features, i might believe the
   difference.

   or, if the data are very sparse - i might believe it as well.

   are either of these things the case?

   otherwise, i'd expect you may just need to tune xgboost a little
   better.
   (button) share
   (button) report (button) save
   community details
   [51]r/machinelearning

   616k

   subscribers

   1.7k

   online

   (button) subscribe[52]create post
   twitter

[53]@slashml

   r/machinelearning rules
   1.
   be nice: no offensive behavior, insults or attacks
   2.
   make your post clear and comprehensive
   3.
   posts without appropriate tag will be removed
   4.
   beginner or career related questions go elsewhere
   5.
   self posts only*
   moderators
   u/kunjaan
   u/cavedave
   naive
   u/olaf_nij
   u/beatlejuce
   u/mtgtraner
   hd hlynsson
   [54]view all moderators
   [55]about[56]careers[57]press
   [58]advertise[59]blog[60]help
   [61]the reddit app[62]reddit coins[63]reddit premium[64]reddit gifts
   [65]content policy|[66] privacy policy
   [67]user agreement|[68] mod policy
      2019 reddit, inc. all rights reserved
   (button) back to top

references

   visible links
   1. https://www.reddit.com/login?dest=https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/
   2. https://www.reddit.com/register?dest=https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/
   3. https://www.reddit.com/r/machinelearning/
   4. https://www.reddit.com/user/thvasilo
   5. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/
   6. https://youtu.be/furfdqtdavc
   7. https://www.redditmedia.com/mediaembed/515dus?responsive=true
   8. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/?sort=confidence
   9. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/?sort=top
  10. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/?sort=new
  11. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/?sort=controversial
  12. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/?sort=old
  13. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/?sort=qa
  14. https://www.reddit.com/user/leonoel
  15. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79nb8i/
  16. https://www.reddit.com/user/rumblestiltsken
  17. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79t58x/
  18. https://www.reddit.com/user/leonoel
  19. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79xh96/
  20. https://www.reddit.com/user/rumblestiltsken
  21. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79xyg9/
  22. https://www.reddit.com/user/leonoel
  23. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79y3h4/
  24. https://www.reddit.com/user/rumblestiltsken
  25. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79yls9/
  26. https://www.reddit.com/user/leonoel
  27. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79zh6n/
  28. https://www.reddit.com/user/sieisteinmodel
  29. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d7a16uk/
  30. https://www.reddit.com/user/leonoel
  31. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d7ac2by/
  32. https://www.reddit.com/user/rumblestiltsken
  33. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d7a276t/
  34. https://www.reddit.com/user/oriolvinyals
  35. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d7a1yo1/
  36. https://en.wikipedia.org/wiki/convolution#historical_developments
  37. https://www.reddit.com/user/thvasilo
  38. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79e0t2/
  39. https://twitter.com/thvasilo/status/765290703505141760
  40. https://www.reddit.com/user/phillypoopskins
  41. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79omr7/
  42. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79rc0m/
  43. https://www.reddit.com/user/phillypoopskins
  44. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79rxku/
  45. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79voj7/
  46. https://www.reddit.com/user/phillypoopskins
  47. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79xkh2/
  48. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d79zs1y/
  49. https://www.reddit.com/user/phillypoopskins
  50. https://www.reddit.com/r/machinelearning/comments/515dus/kdd_panel_is_deep_learning_the_new_42/d7a0062/
  51. https://www.reddit.com/r/machinelearning/
  52. https://www.reddit.com/r/machinelearning/submit
  53. https://twitter.com/slashml
  54. https://www.reddit.com/r/machinelearning/about/moderators/
  55. https://about.reddit.com/
  56. https://about.reddit.com/careers/
  57. https://about.reddit.com/press/
  58. https://about.reddit.com/advertise/
  59. http://www.redditblog.com/
  60. https://www.reddithelp.com/
  61. https://www.reddit.com/mobile/download
  62. https://www.reddit.com/coins
  63. https://www.reddit.com/premium
  64. http://redditgifts.com/
  65. https://www.reddit.com/help/contentpolicy
  66. https://www.reddit.com/help/privacypolicy
  67. https://www.reddit.com/help/useragreement
  68. https://www.reddit.com/help/healthycommunities/

   hidden links:
  70. https://www.reddit.com/
  71. https://www.reddit.com/
  72. https://www.reddit.com/r/all
  73. https://www.reddit.com/original
  74. https://www.reddit.com/r/machinelearning/
  75. https://reddit.com/message/compose?to=/r/machinelearning
  76. https://www.reddit.com/user/kunjaan
  77. https://www.reddit.com/user/cavedave
  78. https://www.reddit.com/user/olaf_nij
  79. https://www.reddit.com/user/beatlejuce
  80. https://www.reddit.com/user/mtgtraner
