   iframe: [1]https://www.googletagmanager.com/ns.html?id=gtm-mp366cc

gensim logo

   [2]gensim
   gensim tagline

get expert help from the gensim authors

       [3]consulting in machine learning & nlp

       commercial document similarity engine: [4]scaletext.ai

       [5]corporate trainings in python data science and deep learning
     * [6]home
     * [7]tutorials
     * [8]install
     * [9]support
     * [10]api
     * [11]about

   models.phrases     phrase (collocation) detection

models.phrases     phrase (collocation) detection[12]  

   automatically detect common phrases     multi-word expressions / word
   id165s     from a stream of sentences.

   inspired by:
     * [13]mikolov, et. al:    distributed representations of words and
       phrases and their compositionality   
     * [14]   normalized (pointwise) mutual information in colocation
       extraction    by gerlof bouma

   examples
>>> from gensim.test.utils import datapath
>>> from gensim.models.id97 import text8corpus
>>> from gensim.models.phrases import phrases, phraser
>>>
>>> sentences = text8corpus(datapath('testcorpus.txt'))
>>> phrases = phrases(sentences, min_count=1, threshold=1)  # train model
>>> phrases[[u'trees', u'graph', u'minors']]  # apply model to sentence
[u'trees_graph', u'minors']
>>>
>>> phrases.add_vocab([["hello", "world"], ["meow"]])  # update model with new s
entences
>>>
>>> bigram = phraser(phrases)  # construct faster model (this is only an wrapper
)
>>> bigram[[u'trees', u'graph', u'minors']]  # apply model to sentence
[u'trees_graph', u'minors']
>>>
>>> for sent in bigram[sentences]:  # apply model to text corpus
...     pass

   class gensim.models.phrases.phraser(phrases_model)[15]  
          bases: [16]gensim.models.phrases.sentenceanalyzer,
          [17]gensim.models.phrases.phrasestransformation

          minimal state & functionality exported from [18]phrases.

          the goal of this class is to cut down memory consumption of
          phrases, by discarding model state not strictly needed for the
          bigram detection task.

          use this instead of phrases if you do not need to update the
          bigram statistics with new documents any more.

          parameters: phrases_model ([19]phrases)     trained phrases instance.

          notes

          after the one-time initialization, a [20]phraser will be much
          smaller and somewhat faster than using the full [21]phrases
          model.

          examples

>>> from gensim.test.utils import datapath
>>> from gensim.models.id97 import text8corpus
>>> from gensim.models.phrases import phrases, phraser
>>>
>>> sentences = text8corpus(datapath('testcorpus.txt'))
>>> phrases = phrases(sentences, min_count=1, threshold=1)
>>>
>>> bigram = phraser(phrases)
>>> sent = [u'trees', u'graph', u'minors']
>>> print(bigram[sent])
[u'trees_graph', u'minors']

        analyze_sentence(sentence, threshold, common_terms, scorer)[22]  
                analyze a sentence, detecting any bigrams that should be
                concatenated.

   parameters:
               o sentence (iterable of str)     token sequence representing
                 the sentence to be analyzed.
               o threshold (float)     the minimum score for a bigram to be
                 taken into account.
               o common_terms (list of object)     list of common terms,
                 they receive special treatment.
               o scorer (function)     scorer function, as given to
                 [23]phrases. see [24]npmi_scorer() and
                 [25]original_scorer().

   yields:

   (str, score)     if bi-gram detected, a tuple where the first element is
   a detect bigram, second its score. otherwise, the first tuple element
   is a single word and second is none.

        classmethod load(*args, **kwargs)[26]  
                load a previously saved [27]phrases / [28]phraser class.
                handles backwards compatibility from older [29]phrases /
                [30]phraser versions which did not support pluggable
                scoring functions.

   parameters:
               o args (object)     sequence of arguments, see [31]load for
                 more information.
               o kwargs (object)     sequence of arguments, see [32]load for
                 more information.

        pseudocorpus(phrases_model)[33]  
                alias for [34]gensim.models.phrases.pseudocorpus().

                parameters:  phrases_model ([35]phrases)     phrases instance.
                  returns:   generator with phrases.
                return type: generator

        save(fname_or_handle, separately=none, sep_limit=10485760,
                ignore=frozenset([]), pickle_protocol=2)[36]  
                save the object to a file.

   parameters:
               o fname_or_handle (str or file-like)     path to output file
                 or already opened file-like object. if the object is a
                 file handle, no special array handling will be performed,
                 all attributes will be saved to the same file.
               o separately (list of str or none, optional)    
                 if none, automatically detect large numpy/scipy.sparse
                 arrays in the object being stored, and store them into
                 separate files. this prevent memory errors for large
                 objects, and also allows [37]memory-mapping the large
                 arrays for efficient loading and sharing the large arrays
                 in ram between multiple processes.
                 if list of str: store these attributes into separate
                 files. the automated size check is not performed in this
                 case.
               o sep_limit (int, optional)     don   t store arrays smaller
                 than this separately. in bytes.
               o ignore (frozenset of str, optional)     attributes that
                 shouldn   t be stored at all.
               o pickle_protocol (int, optional)     protocol number for
                 pickle.

                see also

              [38]load()
                      load object from file.

        score_item(worda, wordb, components, scorer)[39]  
                score a bigram.

   parameters:
               o worda (str)     first word for comparison.
               o wordb (str)     second word for comparison.
               o components (generator)     contain phrases.
               o scorer ({'default', 'npmi'})     not used.

                  returns:

   score for given bi-gram, if bi-gram not presented in dictionary -
   return -1.
                return type:

   float

   class gensim.models.phrases.phrases(sentences=none, min_count=5,
          threshold=10.0, max_vocab_size=40000000, delimiter='_',
          progress_per=10000, scoring='default',
          common_terms=frozenset([]))[40]  
          bases: [41]gensim.models.phrases.sentenceanalyzer,
          [42]gensim.models.phrases.phrasestransformation

          detect phrases based on collocation counts.

   parameters:
          + sentences (iterable of list of str, optional)     the sentences
            iterable can be simply a list, but for larger corpora,
            consider a generator that streams the sentences directly from
            disk/network, see [43]browncorpus, [44]text8corpus or
            [45]linesentence for such examples.
          + min_count (float, optional)     ignore all words and bigrams
            with total collected count lower than this value.
          + threshold (float, optional)     represent a score threshold for
            forming the phrases (higher means fewer phrases). a phrase of
            words a followed by b is accepted if the score of the phrase
            is greater than threshold. heavily depends on concrete
            scoring-function, see the scoring parameter.
          + max_vocab_size (int, optional)     maximum size (number of
            tokens) of the vocabulary. used to control pruning of less
            common words, to keep memory under control. the default of 40m
            needs about 3.6gb of ram. increase/decrease max_vocab_size
            depending on how much available memory you have.
          + delimiter (str, optional)     glue character used to join
            collocation tokens, should be a byte string (e.g. b   _   ).
          + scoring ({'default', 'npmi', function}, optional)    
            specify how potential phrases are scored. scoring can be set
            with either a string that refers to a built-in scoring
            function, or with a function with the expected parameter
            names. two built-in scoring functions are available by setting
            scoring to a string:
              1.    default    - [46]original_scorer().
              2.    npmi    - [47]npmi_scorer().
          + common_terms (set of str, optional)     list of    stop words   
            that won   t affect frequency count of expressions containing
            them. allow to detect expressions like    bank_of_america    or
               eye_of_the_beholder   .

          notes

             npmi    is more robust when dealing with common words that form
          part of common bigrams, and ranges from -1 to 1, but is slower
          to calculate than the default. the default is the pmi-like
          scoring as described by [48]mikolov, et. al:    distributed
          representations of words and phrases and their
          compositionality   .

          to use a custom scoring function, pass in a function with the
          following signature:

          + worda_count - number of corpus occurrences in sentences of the
            first token in the bigram being scored
          + wordb_count - number of corpus occurrences in sentences of the
            second token in the bigram being scored
          + bigram_count - number of occurrences in sentences of the whole
            bigram
          + len_vocab - the number of unique tokens in sentences
          + min_count - the min_count setting of the phrases class
          + corpus_word_count - the total number of tokens (non-unique) in
            sentences

          the scoring function must accept all these parameters, even if
          it doesn   t use them in its scoring. the scoring function must be
          pickleable.

        add_vocab(sentences)[49]  
                update model with new sentences.

                parameters: sentences (iterable of list of str)     text corpus.

                example

>>> from gensim.test.utils import datapath
>>> from gensim.models.id97 import text8corpus
>>> from gensim.models.phrases import phrases
>>> # create corpus and use it for phrase detector
>>> sentences = text8corpus(datapath('testcorpus.txt'))
>>> phrases = phrases(sentences)  # train model
>>> assert len(phrases.vocab) == 37
>>>
>>> more_sentences = [
...     [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there'],
...     [u'machine', u'learning', u'can', u'be', u'new', u'york', u'sometimes']
... ]
>>>
>>> phrases.add_vocab(more_sentences)  # add new sentences to model
>>> assert len(phrases.vocab) == 60

        analyze_sentence(sentence, threshold, common_terms, scorer)[50]  
                analyze a sentence, detecting any bigrams that should be
                concatenated.

   parameters:
               o sentence (iterable of str)     token sequence representing
                 the sentence to be analyzed.
               o threshold (float)     the minimum score for a bigram to be
                 taken into account.
               o common_terms (list of object)     list of common terms,
                 they receive special treatment.
               o scorer (function)     scorer function, as given to
                 [51]phrases. see [52]npmi_scorer() and
                 [53]original_scorer().

   yields:

   (str, score)     if bi-gram detected, a tuple where the first element is
   a detect bigram, second its score. otherwise, the first tuple element
   is a single word and second is none.

        export_phrases(sentences, out_delimiter=' ', as_tuples=false)[54]  
                get all phrases that appear in    sentences    that pass the
                bigram threshold.

   parameters:
               o sentences (iterable of list of str)     text corpus.
               o out_delimiter (str, optional)     delimiter used to    glue   
                 together words that form a bigram phrase.
               o as_tuples (bool, optional)     yield (tuple(words), score)
                 instead of (out_delimiter.join(words), score)?

                  yields:

   ((str, str), float) **or* (str, float)*     phrases detected in
   sentences. return type depends on the as_tuples parameter.

                example

>>> from gensim.test.utils import datapath
>>> from gensim.models.id97 import text8corpus
>>> from gensim.models.phrases import phrases
>>>
>>> sentences = text8corpus(datapath('testcorpus.txt'))
>>> phrases = phrases(sentences, min_count=1, threshold=0.1)
>>>
>>> for phrase, score in phrases.export_phrases(sentences):
...     pass

        static learn_vocab(sentences, max_vocab_size, delimiter='_',
                progress_per=10000, common_terms=frozenset([]))[55]  
                collect unigram/bigram counts from the sentences iterable.

   parameters:
               o sentences (iterable of list of str)     the sentences
                 iterable can be simply a list, but for larger corpora,
                 consider a generator that streams the sentences directly
                 from disk/network, see [56]browncorpus, [57]text8corpus
                 or [58]linesentence for such examples.
               o max_vocab_size (int)     maximum size (number of tokens) of
                 the vocabulary. used to control pruning of less common
                 words, to keep memory under control. the default of 40m
                 needs about 3.6gb of ram. increase/decrease
                 max_vocab_size depending on how much available memory you
                 have.
               o delimiter (str, optional)     glue character used to join
                 collocation tokens, should be a byte string (e.g. b   _   ).
               o progress_per (int)     write logs every progress_per
                 sentence.
               o common_terms (set of str, optional)     list of    stop
                 words    that won   t affect frequency count of expressions
                 containing them. allow to detect expressions like
                    bank_of_america    or    eye_of_the_beholder   .

                  returns:

   number of pruned words, counters for each word/bi-gram and total number
   of words.
                return type:

   (int, dict of (str, int), int)

                example

>>> from gensim.test.utils import datapath
>>> from gensim.models.id97 import text8corpus
>>> from gensim.models.phrases import phrases
>>>
>>> sentences = text8corpus(datapath('testcorpus.txt'))
>>> pruned_words, counters, total_words = phrases.learn_vocab(sentences, 100)
>>> (pruned_words, total_words)
(1, 29)
>>> counters['computer']
2
>>> counters['response_time']
1

        classmethod load(*args, **kwargs)[59]  
                load a previously saved phrases class. handles backwards
                compatibility from older phrases versions which did not
                support pluggable scoring functions.

   parameters:
               o args (object)     sequence of arguments, see [60]load for
                 more information.
               o kwargs (object)     sequence of arguments, see [61]load for
                 more information.

        save(fname_or_handle, separately=none, sep_limit=10485760,
                ignore=frozenset([]), pickle_protocol=2)[62]  
                save the object to a file.

   parameters:
               o fname_or_handle (str or file-like)     path to output file
                 or already opened file-like object. if the object is a
                 file handle, no special array handling will be performed,
                 all attributes will be saved to the same file.
               o separately (list of str or none, optional)    
                 if none, automatically detect large numpy/scipy.sparse
                 arrays in the object being stored, and store them into
                 separate files. this prevent memory errors for large
                 objects, and also allows [63]memory-mapping the large
                 arrays for efficient loading and sharing the large arrays
                 in ram between multiple processes.
                 if list of str: store these attributes into separate
                 files. the automated size check is not performed in this
                 case.
               o sep_limit (int, optional)     don   t store arrays smaller
                 than this separately. in bytes.
               o ignore (frozenset of str, optional)     attributes that
                 shouldn   t be stored at all.
               o pickle_protocol (int, optional)     protocol number for
                 pickle.

                see also

              [64]load()
                      load object from file.

        score_item(worda, wordb, components, scorer)[65]  
                get bi-gram score statistics.

   parameters:
               o worda (str)     first word of bi-gram.
               o wordb (str)     second word of bi-gram.
               o components (generator)     contain all phrases.
               o scorer (function)     scorer function, as given to
                 [66]phrases. see [67]npmi_scorer() and
                 [68]original_scorer().

                  returns:

   score for given bi-gram. if bi-gram not present in dictionary - return
   -1.
                return type:

   float

   class gensim.models.phrases.phrasestransformation[69]  
          bases: [70]gensim.interfaces.transformationabc

          base util class for [71]phrases and [72]phraser.

        classmethod load(*args, **kwargs)[73]  
                load a previously saved [74]phrases / [75]phraser class.
                handles backwards compatibility from older [76]phrases /
                [77]phraser versions which did not support pluggable
                scoring functions.

   parameters:
               o args (object)     sequence of arguments, see [78]load for
                 more information.
               o kwargs (object)     sequence of arguments, see [79]load for
                 more information.

        save(fname_or_handle, separately=none, sep_limit=10485760,
                ignore=frozenset([]), pickle_protocol=2)[80]  
                save the object to a file.

   parameters:
               o fname_or_handle (str or file-like)     path to output file
                 or already opened file-like object. if the object is a
                 file handle, no special array handling will be performed,
                 all attributes will be saved to the same file.
               o separately (list of str or none, optional)    
                 if none, automatically detect large numpy/scipy.sparse
                 arrays in the object being stored, and store them into
                 separate files. this prevent memory errors for large
                 objects, and also allows [81]memory-mapping the large
                 arrays for efficient loading and sharing the large arrays
                 in ram between multiple processes.
                 if list of str: store these attributes into separate
                 files. the automated size check is not performed in this
                 case.
               o sep_limit (int, optional)     don   t store arrays smaller
                 than this separately. in bytes.
               o ignore (frozenset of str, optional)     attributes that
                 shouldn   t be stored at all.
               o pickle_protocol (int, optional)     protocol number for
                 pickle.

                see also

              [82]load()
                      load object from file.

   class gensim.models.phrases.sentenceanalyzer[83]  
          bases: object

          base util class for [84]phrases and [85]phraser.

        analyze_sentence(sentence, threshold, common_terms, scorer)[86]  
                analyze a sentence, detecting any bigrams that should be
                concatenated.

   parameters:
               o sentence (iterable of str)     token sequence representing
                 the sentence to be analyzed.
               o threshold (float)     the minimum score for a bigram to be
                 taken into account.
               o common_terms (list of object)     list of common terms,
                 they receive special treatment.
               o scorer (function)     scorer function, as given to
                 [87]phrases. see [88]npmi_scorer() and
                 [89]original_scorer().

   yields:

   (str, score)     if bi-gram detected, a tuple where the first element is
   a detect bigram, second its score. otherwise, the first tuple element
   is a single word and second is none.

        score_item(worda, wordb, components, scorer)[90]  
                get bi-gram score statistics.

   parameters:
               o worda (str)     first word of bi-gram.
               o wordb (str)     second word of bi-gram.
               o components (generator)     contain all phrases.
               o scorer (function)     scorer function, as given to
                 [91]phrases. see [92]npmi_scorer() and
                 [93]original_scorer().

                  returns:

   score for given bi-gram. if bi-gram not present in dictionary - return
   -1.
                return type:

   float

   gensim.models.phrases.npmi_scorer(worda_count, wordb_count,
          bigram_count, len_vocab, min_count, corpus_word_count)[94]  
          calculation npmi score based on [95]   normalized (pointwise)
          mutual information in colocation extraction    by gerlof bouma.

   parameters:
          + worda_count (int)     number of occurrences for first word.
          + wordb_count (int)     number of occurrences for second word.
          + bigram_count (int)     number of co-occurrences for phrase
               worda_wordb   .
          + len_vocab (int)     not used.
          + min_count (int)     ignore all bigrams with total collected
            count lower than this value.
          + corpus_word_count (int)     total number of words in the corpus.

            returns:

   score for given bi-gram, in the range -1 to 1.
          return type:

   float

          notes

          formula: \frac{ln(prop(word_a, word_b) /
          (prop(word_a)*prop(word_b)))}{ -ln(prop(word_a, word_b)} , where
          prob(word) = \frac{word\_count}{corpus\_word\_count}

   gensim.models.phrases.original_scorer(worda_count, wordb_count,
          bigram_count, len_vocab, min_count, corpus_word_count)[96]  
          bigram scoring function, based on the original [97]mikolov, et.
          al:    distributed representations of words and phrases and their
          compositionality   .

   parameters:
          + worda_count (int)     number of occurrences for first word.
          + wordb_count (int)     number of occurrences for second word.
          + bigram_count (int)     number of co-occurrences for phrase
               worda_wordb   .
          + len_vocab (int)     size of vocabulary.
          + min_count (int)     minimum collocation count threshold.
          + corpus_word_count (int)     not used in this particular scoring
            technique.

            returns:

   score for given bi-gram, greater than or equal to 0.
          return type:

   float

          notes

          formula: \frac{(bigram\_count - min\_count) * len\_vocab }{
          (worda\_count * wordb\_count)} .

   gensim.models.phrases.pseudocorpus(source_vocab, sep,
          common_terms=frozenset([]))[98]  
          feeds source_vocab   s compound keys back to it, to discover
          phrases.

   parameters:
          + source_vocab (iterable of list of str)     tokens vocabulary.
          + sep (str)     separator element.
          + common_terms (set, optional)     immutable set of stopwords.

            yields:

   list of str     phrase.

   smaller gensim logo [99]gensim footer image
      copyright 2009-now, [100]radim   eh    ek
   last updated on jan 31, 2019.
     * [101]home
     * |
     * [102]tutorials
     * |
     * [103]install
     * |
     * [104]support
     * |
     * [105]api
     * |
     * [106]about

   [107]tweet @gensim_py
   support:
   [108]stay informed via gensim mailing list:
   ____________________________ subscribe

references

   1. https://www.googletagmanager.com/ns.html?id=gtm-mp366cc
   2. https://radimrehurek.com/gensim/index.html
   3. https://rare-technologies.com/
   4. https://scaletext.com/
   5. https://rare-technologies.com/corporate-training/
   6. https://radimrehurek.com/gensim/index.html
   7. https://radimrehurek.com/gensim/tutorial.html
   8. https://radimrehurek.com/gensim/install.html
   9. https://radimrehurek.com/gensim/support.html
  10. https://radimrehurek.com/gensim/apiref.html
  11. https://radimrehurek.com/gensim/about.html
  12. https://radimrehurek.com/gensim/models/phrases.html#module-gensim.models.phrases
  13. https://arxiv.org/abs/1310.4546
  14. https://svn.spraakdata.gu.se/repos/gerlof/pub/www/docs/npmi-pfd.pdf
  15. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser
  16. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.sentenceanalyzer
  17. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrasestransformation
  18. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  19. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  20. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser
  21. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  22. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser.analyze_sentence
  23. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  24. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.npmi_scorer
  25. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.original_scorer
  26. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser.load
  27. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  28. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser
  29. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  30. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser
  31. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
  32. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
  33. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser.pseudocorpus
  34. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.pseudocorpus
  35. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  36. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser.save
  37. https://en.wikipedia.org/wiki/mmap
  38. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
  39. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser.score_item
  40. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  41. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.sentenceanalyzer
  42. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrasestransformation
  43. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.browncorpus
  44. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.text8corpus
  45. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
  46. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.original_scorer
  47. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.npmi_scorer
  48. https://arxiv.org/abs/1310.4546
  49. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases.add_vocab
  50. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases.analyze_sentence
  51. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  52. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.npmi_scorer
  53. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.original_scorer
  54. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases.export_phrases
  55. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases.learn_vocab
  56. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.browncorpus
  57. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.text8corpus
  58. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.linesentence
  59. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases.load
  60. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
  61. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
  62. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases.save
  63. https://en.wikipedia.org/wiki/mmap
  64. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
  65. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases.score_item
  66. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  67. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.npmi_scorer
  68. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.original_scorer
  69. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrasestransformation
  70. https://radimrehurek.com/gensim/interfaces.html#gensim.interfaces.transformationabc
  71. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  72. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser
  73. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrasestransformation.load
  74. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  75. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser
  76. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  77. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser
  78. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
  79. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
  80. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrasestransformation.save
  81. https://en.wikipedia.org/wiki/mmap
  82. https://radimrehurek.com/gensim/utils.html#gensim.utils.saveload.load
  83. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.sentenceanalyzer
  84. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  85. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phraser
  86. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.sentenceanalyzer.analyze_sentence
  87. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  88. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.npmi_scorer
  89. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.original_scorer
  90. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.sentenceanalyzer.score_item
  91. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.phrases
  92. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.npmi_scorer
  93. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.original_scorer
  94. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.npmi_scorer
  95. https://svn.spraakdata.gu.se/repos/gerlof/pub/www/docs/npmi-pfd.pdf
  96. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.original_scorer
  97. https://arxiv.org/abs/1310.4546
  98. https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.pseudocorpus
  99. https://radimrehurek.com/gensim/index.html
 100. https://radimrehurek.com/cdn-cgi/l/email-protection#f58794919c9887909d8087909eb586908f9b9498db968f
 101. https://radimrehurek.com/gensim/index.html
 102. https://radimrehurek.com/gensim/tutorial.html
 103. https://radimrehurek.com/gensim/install.html
 104. https://radimrehurek.com/gensim/support.html
 105. https://radimrehurek.com/gensim/apiref.html
 106. https://radimrehurek.com/gensim/about.html
 107. https://twitter.com/gensim_py
 108. https://groups.google.com/group/gensim
