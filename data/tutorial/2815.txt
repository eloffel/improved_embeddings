   #[1]github [2]recent commits to epic:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]43
     * [35]star [36]451
     * [37]fork [38]82

[39]dlwh/[40]epic

   [41]code [42]issues 27 [43]pull requests 2 [44]projects 0 [45]wiki
   [46]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [47]sign up
   epic is a high performance statistical parser written in scala, along
   with a framework for building complex id170 models.
   [48]http://scalanlp.org/
     * [49]1,790 commits
     * [50]17 branches
     * [51]11 releases
     * [52]fetching contributors
     * [53]apache-2.0

    1. [54]scala 91.3%
    2. [55]tex 6.6%
    3. [56]lex 2.0%
    4. [57]java 0.1%

   (button) scala tex lex java
   branch: master (button) new pull request
   [58]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/d
   [59]download zip

downloading...

   want to be notified of new releases in dlwh/epic?
   [60]sign in [61]sign up

launching github desktop...

   if nothing happens, [62]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [63]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [64]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [65]download the github extension for visual studio
   and try again.

   (button) go back
   [66]@dlwh
   [67]dlwh [68]merge branch 'master' of github.com:dlwh/epic
   latest commit [69]f275baa aug 29, 2017
   [70]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [71]odin [72]toward a writeup. aug 21, 2012
   [73]project
   [74]src
   [75].gitignore [76]updating to latest version of epic; minor
   modifications to trace remo    jun 18, 2015
   [77]license
   [78]notice
   [79]readme-neural.md [80]update readme-neural.md mar 30, 2017
   [81]readme.md
   [82]build.sbt

readme.md

epic

   (c) 2014 david hall.

   epic is a id170 framework for scala. it also includes
   classes for training high-accuracy syntactic parsers, part-of-speech
   taggers, name entity recognizers, and more.

   epic is distributed under the [83]apache license, version 2.0.

   the current version is 0.3.

documentation

   documentation will (eventually) live at the github wiki:
   [84]https://github.com/dlwh/epic/wiki

   see some example usages at [85]https://github.com/dlwh/epic-demo.

using epic

   epic can be used programmatically or from the command line, using
   either pretrained models ([86]see below) or with models you have
   trained yourself.

   currently, epic has support for three kinds of models: parsers,
   sequence labelers, and segmenters. parsers produce syntactic
   representations of sentences. sequence labelers are things like
   part-of-speech taggers. these associate each word in a sentence with a
   label. for instance, a part-of-speech tagger can identify nouns, verbs,
   etc. segmenters break a sentence into a sequence of fields. for
   instance, a id39 system might identify all the
   people, places and things in a sentence.

command-line usage

   epic bundles command line interfaces for using parsers, ner systems,
   and pos taggers (and more generally, segmentation and tagging systems).
   there are three classes, one for each kind of system:
     * epic.parser.parsetext runs a parser.
     * epic.sequences.segmenttext runs an ner system, or any kind of
       segmentation system.
     * epic.sequences.tagtext runs a pos tagger, or any kind of tagging
       system.

   all of these systems expect plain text files as input, along with a
   path to a model file. the syntax is:
java -xmx4g -cp /path/to/epic-assembly-0.3-snapshot.jar epic.parser.parsetext --
model /path/to/model.ser.gz --nthreads <number of threads> [files]

   currently, all text is output to standard out. in the future, we will
   support output in a way that differentiates the files. if no files are
   given, the system will read from standard input. by default, the system
   will use all available cores for execution.

   models can be downloaded from [87]http://www.scalanlp.org/models/ or
   from maven central. ([88]see below.)

programmatic usage

   epic also supports programmatic usage. all of the models assume that
   text has been segmented and tokenized.

preprocessing text

   to preprocess text so that the models can use them, you will need to
   segment out sentences and tokenize the sentences into individual words.
   epic comes with classes to do both.

   once you have a sentence, you can tokenize it using a
   epic.preprocess.treebanktokenizer, which takes a string and returns a
   sequence of tokens. all told, the pipeline looks like this:
val text = getsometext();

val sentencesplitter = mlsentencesegmenter.bundled().get
val tokenizer = new epic.preprocess.treebanktokenizer()

val sentences: indexedseq[indexedseq[string]] = sentencesplitter(text).map(token
izer).toindexedseq

for(sentence <- sentences) {
  // use the sentence tokens
}

parser

   to use the parser programmaticaly, deserialize a parser model--either
   using epic.models.deserialize[parser[annotatedlabel, string]](path) or
   using the parserselector. then, give the parser segmented and tokenized
   text:
val parser = epic.models.deserialize[parser[annotataedlabel, string]](path)

// or:

val parser = epic.models.parserselector.loadparser("en").get // or another 2 let
ter code.

val tree = parser(sentence)

println(tree.render(sentence))

   trees have a number of methods on them. see the class definition or
   [89]api docs.

part-of-speech tagger

   using a part-of-speech tagger is similar to using a parser: load a
   model, tokenize some text, run the tagger. all taggers are (currently)
   [90]linear chain id49, or crfs. (you don't need to
   understand them to use them. they are just a machine learning method
   for assigning a sequence of tags to a sequence of words.)
val tagger = epic.models.deserialize[crf[annotatedlabel, string]](path)

// or:

val tagger = epic.models.postagselector.loadtagger("en").get // or another 2 let
ter code.

val tags = tagger.bestsequence(sentence)

println(tags.render)

id39

   using a named entity recognizer is similar to using a pos tagger: load
   a model, tokenize some text, run the recognizer. all ner systems are
   (currently) [91]linear chain semi-markov id49, or
   semicrfs. (you don't need to understand them to use them. they are just
   a machine learning method for segmenting text into fields.)
val ner = epic.models.deserialize[semicrf[annotatedlabel, string]](path)

// or:

val ner = epic.models.nerselector.loadner("en").get// or another 2 letter code.

val segments = ner.bestsequence(sentence)

println(segments.render)

   the outside label of a semicrf is the label that is consider not part
   of a "real" segment. for instance, in ner, it is the label given to
   words that are not named entities.

pre-trained models

   epic provides a number of pre-trained models. these are available as
   maven artifacts from maven central, and can be loaded at runtime. to
   use a specific model, just depend on it (or alternatively download the
   jar file). you can then load the parser by calling, for example:
epic.parser.models.en.span.englishspanparser.load()

   this will load the model and return a parser object. if you want to not
   hardwire dependencies, either for internationalization or to
   potentially try different models, use
   epic.models.parserselector.loadparser(language), where language is the
   [92]two letter code for the language you want to use.

   to following models are available at this time:

   as of writing only models for english are available! write me if you
   want these other models.
     * parser
          + english:
"org.scalanlp" %% "epic-parser-en-span" % "2015.1.25"

     * pos taggers
          + english:
"org.scalanlp" %% "epic-pos-en" % "2015.1.25"

     * named entity recognizers
          + english:
"org.scalanlp" %% "epic-ner-en-conll" % "2015.1.25"

   there is also a meta-dependency that includes the above three models:
"org.scalanlp" %% "english"  % "2015.1.25"

   i meant to name that "epic-english" but messed up. so it's that for
   now. expect it to change.

   todo:
     * parser
          + english:
"org.scalanlp" %% "epic-parser-en-span" % "2014.9.15-snapshot"

          + basque:
"org.scalanlp" %% "epic-parser-eu-span" % "2014.9.15-snapshot"

          + french:
"org.scalanlp" %% "epic-parser-fr-span" % "2014.9.15-snapshot"

          + german:
"org.scalanlp" %% "epic-parser-de-span" % "2014.9.15-snapshot"

          + hungarian:
"org.scalanlp" %% "epic-parser-hu-span" % "2014.9.15-snapshot"

          + korean:
"org.scalanlp" %% "epic-parser-ko-span" % "2014.9.15-snapshot"

          + polish:
"org.scalanlp" %% "epic-parser-pl-span" % "2014.9.15-snapshot"

          + swedish:
"org.scalanlp" %% "epic-parser-sv-span" % "2014.9.15-snapshot"

     * pos taggers
          + basque:
"org.scalanlp" %% "epic-pos-eu" % "2014.9.15-snapshot"

          + french:
"org.scalanlp" %% "epic-pos-fr" % "2014.9.15-snapshot"

          + german:
"org.scalanlp" %% "epic-pos-de" % "2014.9.15-snapshot"

          + hungarian:
"org.scalanlp" %% "epic-pos-hu" % "2014.9.15-snapshot"

          + polish:
"org.scalanlp" %% "epic-pos-pl" % "2014.9.15-snapshot"

          + swedish:
"org.scalanlp" %% "epic-pos-sv" % "2014.9.15-snapshot"

     * named entity recognizers
          + english:
"org.scalanlp" %% "epic-ner-en-conll" % "2014.9.15-snapshot"

   if you use any of the parser models in research publications, please
   cite:

     david hall, greg durrett, and dan klein. 2014. less grammar, more
     features. in acl.

   if you use the other things, just link to epic.

building epic

   in order to do anything besides use pre-trained models, you will
   probably need to build epic.

   to build, you need a release of [93]sbt 0.13.2

   then run
$ sbt assembly

   which will compile everything, run tests, and build a fatjar that
   includes all dependencies.

training models

training parsers

   there are several different discriminative parsers you can train, and
   the trainer main class has lots of options. to get a sense of them, run
   the following command:
$ java -cp target/scala-2.10/epic-assembly-0.2-snapshot.jar epic.parser.models.p
arsertrainer --help

   you'll get a list of all the available options (so many!) the important
   ones are:
--treebank.path "path/to/treebank"
--cache.path "constraint.cache"
--modelfactory  xxx                              # the kind of parser to train.
see below.
--opt.usestochastic true                         # turn on stochastic gradient
--opt.id173 1.0                         # id173 constant. you
need to regularize, badly.

   there are 4 kinds of base models you can train, and you can tie them
   together with an epparsermodel, if you want. the 4 base models are:
     * epic.parser.models.latentmodelfactory: latent annotation (like the
       berkeley parser)
     * epic.parser.models.lexmodelfactory: lexical annotation (kind of
       like the collins parser)
     * epic.parser.models.structmodelfactory: structural annotation (kind
       of like the stanford parser)
     * epic.parser.models.spanmodelfactory: span features (hall, durrett,
       and klein, 2014)

   these models all have their own options. you can see those by
   specifying the modelfactory and adding --help:
$ java -cp target/scala-2.10/epic-assembly-0.2-snapshot.jar epic.parser.models.p
arsertrainer --modelfactory "model" --help

   if you use the first three in research papers, please cite

     david hall and dan klein. 2012. training factored pid18s with
     expectation propagation. in emnlp.

   if you use the spanmodel, please cite:

     david hall, greg durrett, and dan klein. 2014. less grammar, more
     features. in acl.

   if you use something else, cite one of these, or something.

   for training a spanmodel, the following configurations are known to
   work well in general:
     * english:

epic.parser.models.parsertrainer \
  --modelfactory epic.parser.models.spanmodelfactory \
  --cache.path constraints.cache \
  --opt.usestochastic \
  --opt.id173 5 \
  --opt.batchsize 500 \
  --alpha 0.1 \
  --maxiterations 1000 \
  --trainer.modelfactory.annotator epic.trees.annotations.pipelineannotator \
  --ann.0 epic.trees.annotations.filterannotations \
  --ann.1 epic.trees.annotations.forgetheadtag \
  --ann.2 epic.trees.annotations.markovize \
  --vertical 1 \
  --horizontal 0 \
  --treebank.path /home/dlwh/wsj/

     * other (spmrl languages):

epic.parser.models.parsertrainer \
  --treebanktype spmrl \
  --binarization head \
  --modelfactory epic.parser.models.spanmodelfactory \
  --opt.usestochastic --opt.id173 5.0 \
  --opt.batchsize 400 --maxiterations 502 \
  --iterationspereval 100 \
  --alpha 0.1 \
  --trainer.modelfactory.annotator epic.trees.annotations.pipelineannotator \
  --ann.0 epic.trees.annotations.filterannotations  \
  --ann.1 epic.trees.annotations.forgetheadtag \
  --ann.2 epic.trees.annotations.markovize \
  --ann.2.vertical 1 \
  --ann.2.horizontal 0 \
  --ann.3 epic.trees.annotations.splitpunct \
  --cache.path $languc-constraints.cache \
  --treebank.path ${spmrl}/${languc}_spmrl/gold/ptb/ \
  --supervisedheadfinderptbpath ${spmrl}/${languc}_spmrl/gold/ptb/${train}/${tra
in}.$lang.gold.ptb \
  --supervisedheadfinderconllpath ${spmrl}/${languc}_spmrl/gold/conll/${train}/$
{train}.$lang.gold.conll \
  --threads 8

   training a parser currently needs four files that are cached to the
   pwd:
     * xbar.gr: caches the topology of the grammar
     * constraints.cache, constraints.cache.*: remembers pruning masks
       computed from the base grammar.

   todo: remove this reliance.

treebank types

   there is a treebank.type commandline flag that supports a few different
   formats for treebanks. they are:
     * penn: reads from the wsj/ subdirectory of the id32. this
       expects a set of directories 00-24, each of which contains a number
       of mrg files. standard splits are used.
     * chinese: expects a number of chtbnn.mrg files in a single
       directory.
     * negra: expects a directory with three files, negra_[1-3].mrg
     * conllonto: expects data formatted like the 2011 conll shared task.
       only reads the trees.
     * spmrl: expects a directory layout like that used in the 2012 spmrl
       shared task.
     * simple: expects a directory with 3 files: {train, dev, test}.txt

training a parser programmatically

   you can also train a span model programmatically, by using the
   spanmodelfactory.buildsimple method. for example:
spanmodelfactory.buildsimple(trees, optparams(id173=1.0, usestochastic
= true))

   the build simple model also supports using custom featurizers.

training pos taggers and other sequence models

   the main class epic.sequences.trainpostagger can be used to train a pos
   tagger from a treebank. it expects the same treebank options (namely
   treebank.path and treebank.type) as the parser trainer does, as well as
   the same optimization options.

   the following configuration is known to work well:
     * english:

     epic.sequences.trainpostagger \
     --treebank.path $path_to/wsj \
     --opt.id173 2.0 \
     --usestochastic \
     --maxiterations 1000

     * others (spmrl):

  epic.sequences.trainpostagger --opt.id173 2.0 --usestochastic --maxit
erations 1000 \
  --treebanktype spmrl \
  --binarization left \
  --treebank.path ${spmrl}/${languc}_spmrl/gold/ptb/

   if you want to train other kinds of models, you will probably need to
   build crfs programmatically. for inspiration, you should probably look
   at the source code for trainpostagger. it's wonderfully short:
object trainpostagger extends lazylogging {
  case class params(opt: optparams, treebank: processedtreebank, hashfeaturescal
e: double = 0.00)

  def main(args: array[string]) {
    val params = commandlineparser.readin[params](args)
    logger.info("command line arguments for recovery:\n" + configuration.fromobj
ect(params).tocommandlinestring)
    import params._
    val train = treebank.traintrees.map(_.astaggedsequence)
    val test = treebank.devtrees.map(_.astaggedsequence)

    val crf = crf.buildsimple(train, annotatedlabel("top"), opt = opt, hashfeatu
res = hashfeaturescale)

    val stats = taggedsequenceeval.eval(crf, test)
    println("final stats: " + stats)
    println("confusion matrix:\n" + stats.confusion)

  }

}

   basically, you need to create a collection of taggedsequences, which is
   a pair of sequences, one for tags and one for words. then pass in the
   training data to crf.buildsimple, along with a start symbol (used for
   the "beginning of sentence" tag), an optional [94]gazetteer (not
   shown), and an [95]optparams, which is used to control the
   optimization. there is also an optional hashfeatures argument, which
   isn't used.

   we can also pass in two [wordfeaturizer] instances, one for "label"
   features, and one for "transition" features. most of the featurizers in
   epic have a cross product form (label x surface), where label is a
   feature on the label (e.g. the pos tag) and the surface feature is a
   feature on the surface string.here, the label featurizer features are
   crossed with the tag, and the transition featurizer features are
   crossed with pairs of sucessive labels. see the wiki page on
   [[featurizers]] for more detail.

training ner systems and other segmentation models

   training an ner system or other semicrf is very similar to training a
   crf. the main difference is that the inputs are segmentations, rather
   than taggedsequences. the main class
   epic.sequences.semiconllnerpipeline can be used to train ner models,
   with data in the [96]conll 2003 shared task format. this class
   completely ignores all fields except the first and last. the
   commandline takes two paths, --train and --test, to specify training
   and test set files, respectively.

   if you need to do something more complicated, you will need to write
   your own code. as an example, here is the code for
   epic.sequences.semiconllnerpipeline. this code is somewhat more
   complicated, as the conll sequences need to be turned into
   segmentations.
def main(args: array[string]) {
    val params:params = commandlineparser.readin[params](args)
    logger.info("command line arguments for recovery:\n" + configuration.fromobj
ect(params).tocommandlinestring)
    val (train,test) = {
      val standardtrain = conllsequencereader.readtrain(new fileinputstream(para
ms.path), params.path.getname).toindexedseq
      val standardtest = conllsequencereader.readtrain(new fileinputstream(param
s.test), params.path.getname).toindexedseq

      standardtrain.take(params.nsents).map(makesegmentation) -> standardtest.ma
p(makesegmentation)
    }


    // you can optionally pass in an a gazetteer, though i've not gotten much mi
leage with them.
    val crf = semicrf.buildsimple(train, "--begin--", "o", params.opt)

    val stats = segmentationeval.eval(crf, test)

    println(stats)


  }

   we can also pass in featurizers, like in the crf trainer. in this case,
   we can pass in a [wordfeaturizer] and a [spanfeaturizer]. wordfeatures
   are like before, while spanfeaturizer give features over the entire
   input span. for ner, this can be useful for adding features noting that
   an entity is entirely surrounded by quotation marks, for instance, or
   for matching against entries in a [[gazetteer]].

optparams

   optparams is a configuration class that controls the optimizer. there
   are a bunch of different options:
--opt.batchsize: int = 512


--opt.id173: double = 0.0


--opt.alpha: double = 0.5


--opt.maxiterations: int = 1000


--opt.usel1: boolean = false


--opt.tolerance: double = 1.0e-5


--opt.usestochastic: boolean = false


--opt.randomseed: int = 0

   id173 is generally very important. using a value of 1.0
   usually works pretty well. 5.0 works better on the spanmodel for
   parsing. usestochastic turns on stochastic id119 (rather
   than full batch optimization). it makes training much faster, usually.

     *    2019 github, inc.
     * [97]terms
     * [98]privacy
     * [99]security
     * [100]status
     * [101]help

     * [102]contact github
     * [103]pricing
     * [104]api
     * [105]training
     * [106]blog
     * [107]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [108]reload to refresh your
   session. you signed out in another tab or window. [109]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/dlwh/epic/commits/master.atom
   3. https://github.com/dlwh/epic#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/dlwh/epic
  32. https://github.com/join
  33. https://github.com/login?return_to=/dlwh/epic
  34. https://github.com/dlwh/epic/watchers
  35. https://github.com/login?return_to=/dlwh/epic
  36. https://github.com/dlwh/epic/stargazers
  37. https://github.com/login?return_to=/dlwh/epic
  38. https://github.com/dlwh/epic/network/members
  39. https://github.com/dlwh
  40. https://github.com/dlwh/epic
  41. https://github.com/dlwh/epic
  42. https://github.com/dlwh/epic/issues
  43. https://github.com/dlwh/epic/pulls
  44. https://github.com/dlwh/epic/projects
  45. https://github.com/dlwh/epic/wiki
  46. https://github.com/dlwh/epic/pulse
  47. https://github.com/join?source=prompt-code
  48. http://scalanlp.org/
  49. https://github.com/dlwh/epic/commits/master
  50. https://github.com/dlwh/epic/branches
  51. https://github.com/dlwh/epic/releases
  52. https://github.com/dlwh/epic/graphs/contributors
  53. https://github.com/dlwh/epic/blob/master/license
  54. https://github.com/dlwh/epic/search?l=scala
  55. https://github.com/dlwh/epic/search?l=tex
  56. https://github.com/dlwh/epic/search?l=lex
  57. https://github.com/dlwh/epic/search?l=java
  58. https://github.com/dlwh/epic/find/master
  59. https://github.com/dlwh/epic/archive/master.zip
  60. https://github.com/login?return_to=https://github.com/dlwh/epic
  61. https://github.com/join?return_to=/dlwh/epic
  62. https://desktop.github.com/
  63. https://desktop.github.com/
  64. https://developer.apple.com/xcode/
  65. https://visualstudio.github.com/
  66. https://github.com/dlwh
  67. https://github.com/dlwh/epic/commits?author=dlwh
  68. https://github.com/dlwh/epic/commit/f275baa5f48806dadbbd54ac7d2c60c8556b4b34
  69. https://github.com/dlwh/epic/commit/f275baa5f48806dadbbd54ac7d2c60c8556b4b34
  70. https://github.com/dlwh/epic/tree/f275baa5f48806dadbbd54ac7d2c60c8556b4b34
  71. https://github.com/dlwh/epic/tree/master/odin
  72. https://github.com/dlwh/epic/commit/e2824f10311dc8b5368e37f5c25fa37b18e4128d
  73. https://github.com/dlwh/epic/tree/master/project
  74. https://github.com/dlwh/epic/tree/master/src
  75. https://github.com/dlwh/epic/blob/master/.gitignore
  76. https://github.com/dlwh/epic/commit/7fc3285bd60dd1aec903eaa08bb33cf93db9626e
  77. https://github.com/dlwh/epic/blob/master/license
  78. https://github.com/dlwh/epic/blob/master/notice
  79. https://github.com/dlwh/epic/blob/master/readme-neural.md
  80. https://github.com/dlwh/epic/commit/57b91844976a97e519d362dc4be1b38eed929608
  81. https://github.com/dlwh/epic/blob/master/readme.md
  82. https://github.com/dlwh/epic/blob/master/build.sbt
  83. http://www.apache.org/licenses/license-2.0.html
  84. https://github.com/dlwh/epic/wiki
  85. https://github.com/dlwh/epic-demo
  86. https://github.com/dlwh/epic#pre-trained-models
  87. http://www.scalanlp.org/models/
  88. https://github.com/dlwh/epic#pre-trained-models
  89. http://www.scalanlp.org/api/epic
  90. http://people.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf
  91. http://people.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf
  92. http://www.loc.gov/standards/iso639-2/php/code_list.php
  93. http://www.scala-sbt.org/0.13.2/docs/getting-started/setup.html
  94. https://github.com/dlwh/epic#gazetteer
  95. https://github.com/dlwh/epic#optparams
  96. http://www.cnts.ua.ac.be/conll2003/ner/
  97. https://github.com/site/terms
  98. https://github.com/site/privacy
  99. https://github.com/security
 100. https://githubstatus.com/
 101. https://help.github.com/
 102. https://github.com/contact
 103. https://github.com/pricing
 104. https://developer.github.com/
 105. https://training.github.com/
 106. https://github.blog/
 107. https://github.com/about
 108. https://github.com/dlwh/epic
 109. https://github.com/dlwh/epic

   hidden links:
 111. https://github.com/
 112. https://github.com/dlwh/epic
 113. https://github.com/dlwh/epic
 114. https://github.com/dlwh/epic
 115. https://help.github.com/articles/which-remote-url-should-i-use
 116. https://github.com/dlwh/epic#epic
 117. https://github.com/dlwh/epic#documentation
 118. https://github.com/dlwh/epic#using-epic
 119. https://github.com/dlwh/epic#command-line-usage
 120. https://github.com/dlwh/epic#programmatic-usage
 121. https://github.com/dlwh/epic#preprocessing-text
 122. https://github.com/dlwh/epic#parser
 123. https://github.com/dlwh/epic#part-of-speech-tagger
 124. https://github.com/dlwh/epic#named-entity-recognition
 125. https://github.com/dlwh/epic#pre-trained-models
 126. https://github.com/dlwh/epic#building-epic
 127. https://github.com/dlwh/epic#training-models
 128. https://github.com/dlwh/epic#training-parsers
 129. https://github.com/dlwh/epic#treebank-types
 130. https://github.com/dlwh/epic#training-a-parser-programmatically
 131. https://github.com/dlwh/epic#training-pos-taggers-and-other-sequence-models
 132. https://github.com/dlwh/epic#training-ner-systems-and-other-segmentation-models
 133. https://github.com/dlwh/epic#optparams
 134. https://github.com/
