   #[1]colin_morris

   [2]colin_morris

   [3]blog [4]toys [5]about

dreaming of names with rbms

   a classic problem in natural language processing is [6]named entity
   recognition. given a text, we have to identify the proper nouns. but
   what about the generative mirror image of this problem - i.e. named
   entity generation? what if we ask a model to dream up new names of
   people, places and things?

   i wrote some code to do this using restricted id82s, a
   nifty (if pass  ) variety of generative neural network. it turns out
   they come up with some funny stuff! for example, if we train an rbm on
   github repository names, it can come up with new ones like   
fuzzytools
slick-android-app
sublime-app
backbone-switcher
model1302110000

   if you want to flip through more examples, there   s a (web) app for
   that. check out   
     * github repo generator
     * place name generator
     * actor name generator

   if you want to learn about how i got there, read on. in this post, i   ll
   give a brief overview of restricted id82s and how i
   applied them to this problem, and try to give some intuition about
   what   s going on in the brain of one of these models.

   my code is available [7]here on github. feel free to play with it (with
   the caveat that it   s more of a research notebook than a polished
   library).

restricted id82s

background - generative models

   our goal is to build a model that spits out funny names, but our path
   there will be a bit indirect. the problem that rbms - and generative
   models in general - are trying to solve is learning a id203
   distribution. we want to learn a function p that assigns every string a
   id203 according to its plausibility as a particular kind of name.
   e.g. in the case of human names, we probably want   
p("john smith") > p("dweezil zappa") >> p("mcn xgl  jey")

   if we can sample from this distribution, the effect should be like
   thumbing through a phone book. we   ll see lots of    john smith   s, we
   might eventually see a    dweezil zappa   , but we   ll probably never find a
      mcn xgl jey   .

representing inputs

   we   ve said we want to learn a function over strings, but anything we   re
   going to feed into a neural network needs to be transformed into a
   vector of numbers first. how should we do that in this case?

   most nlp models stop at the word level, representing texts by counts of
   words (or by id27s, such as those produced by id97). but
   breaking up github repository names (like tool_dbg, burgvan.github.io,
   or refcounting) into words isn   t trivial. and more to the point, we
   don   t want to limit ourselves to regurgitating words we   ve seen in the
   training data. we want to generate whole new words (like, say,
   [8]brinesville). for that, we need to go deeper, down to the character
   level.

   we   ll represent names as sequences of [9]one-hot vectors of length n,
   where n is the size of our alphabet.

   because we   re not using a recurrent architecture, we   ll need to fix
   some maximum string length m ahead of time. names shorter than m will
   need to be padded with some special character.

   for example, let   s take our alphabet to be just {a,b,c,d,e,$}, where
      $    is our padding character, and set m to 4. we can encode the name
      deb    using the following 4x6 matrix:
   index a b c d e $
   0     0 0 0 1 0 0
   1     0 0 0 0 1 0
   2     0 1 0 0 0 0
   3     0 0 0 0 0 1

rbms

   a restricted id82 (henceforth rbm) is a neural network
   consisting of two layers of binary units, one visible and one hidden.
   the visible units represent examples of the data distribution we   re
   interested in - in this case, names.
   [rbm.svg] a tiny rbm with 3 hidden units and 24 visible units (not all
   shown) representing the name "deb". two hidden units and 2 visible
   units (that we can see) are turned on - the rest are off.

   again, rbms try to learn a id203 distribution from the data
   they   re given. they do this by learning to assign relatively low energy
   to samples from the data distribution. that energy will be proportional
   to the (negative log) learned id203.

   in the diagram above, the energy of the rbm will be equal to the
   negative sum of:
     * the biases on the two active (red) hidden units
     * the biases on the active (blue) visible units
     * the weights connecting the red and blue units (i.e. the bold lines)

   there are weights connecting every visible unit to every hidden unit,
   but no intra-layer (visible-visible, or hidden-hidden) weights. during
   training, the rbm will adjust these weights, and a vector of biases for
   the visible and hidden units, in such a way as to bring down the energy
   of training examples, without bringing down the energy of everything
   else along with it.
   aside
   we said that energy is defined for a configuration of the visible and
   hidden layer, so what does it mean when we talk about the energy of a
   training example? the energy of a visible configuration v is defined as
   sum(energy(v, hidden) for hidden in all_possible_hidden_vectors). we
   can't feasibly iterate over all 2^n possible hidden layers, but it
   turns out there's an equivalent closed-form that's easy to calculate.

drawing samples

   once we   ve trained a model, how do we get it to talk? starting from any
   random string, we sample the hidden layer. then using that hidden
   layer, we sample the visible layer, getting a new string. if we repeat
   this process (called id150) a whole bunch of times, we should
   get a name out at the end.

   what does it mean to sample the hidden/visible layer? our binary units
   are stochastic, so given a string, each hidden unit will want to turn
   on with some id203, according to its bias and the weights coming
   into it from active visible units. sampling the hidden layer given a
   visible layer means turning each hidden unit on or off according to
   some rolls of the dice. (and vice-versa for sampling the visible
   layer.)

more details (for nerds)

   if you   re interested in reading more about rbms, i highly recommend
   geoff hinton   s [10]a practical guide to training restricted boltzmann
   machines, which was my bible during this project.

   i trained my models using [11]persistent contrastive divergence. i used
   softmax sampling (described in 13.1 of    practical guide   ) for the
   visible layer - without it, results were very poor.

   sampling was not quite as simple as my handwaving in the section above
   would suggest. i used simulated annealing, which turned out to help a
   lot. i wrote a separate little post about sampling [12]here.

   [13]this readme has details on hyperparameters used to train each of my
   models and the annealing schedule used to sample from them.

results

   let   s look at what some rbms dreamed up on a few different name-like
   datasets. [14]this readme describes where each dataset was downloaded
   from and how it was preprocessed.

   any names that existed in the training data were filtered out of the
   lists below (and in the name generator apps). this cut anywhere between
   .01% of samples to 10% depending on the model.

human names

   here are some samples drawn from a model trained on the full names of
   1.5m actors from imdb (more [15]here):
omar vole
r.j. pen
ronald w. males
jean-paul recan
marxel sode
samuel j. varga
lionel cone

   (it turns out they meant    actors    in the gendered sense of the word,
   which is why you won   t see any female names.)

   the dataset includes naming traditions from around the world which
   created several distinct modes that the model captured pretty well. for
   example, it generates names like   
hiroshi tajamara
hing-hying li
vladimir tjomanovic
giuseppe rariali

   but is unlikely to generate a name like    hiroshi tjomanovic   .

   (incidentally, google tells me that none of    tajamara   ,    hing-hying   ,
      tjomanovic    and    rariali    are actual extant names - though based on my
   limited exposure to japanese/chinese/slavic/italian names, i could have
   believed they were all real. we want to generate novel examples not
   copied from the training set, so this is good news.)

   the model   s favourite name (that is, the sample it assigned the lowest
   energy) was christian scheller (who [16]exists in the training set -
   christian schuller, who doesn   t exist, is a close second).

geographic names

   it   s not much of a stretch of the imagination to go from training on
   names of people to names of places (examples from the dataset:    gall
   creek   ,    grovertown   ,    aneta   ,    goodyear heights   ). here are some
   random examples from our rbm   s dreamed atlas (more [17]here):
sama
marchestee hill
wano
fleminger river
arring lake south
jicky park
mount ono
oste
lake day

   not bad! who wouldn   t enjoy a picnic in jicky park?

   the map at the top of this post is a terrifying vision of a whole
   territory dreamed up by an rbm - full version [18]here.

   the model   s favourite place name was indian post office, which exists
   in the training set. it   s second favourite is wester post office, which
   doesn   t.

github repository names

   how about some github repos? more [19]here:
frost2
gruntus.js
simpleshefe
backbook.com
thetesters
mandolind
smart-cheling
shreecheck
redget-2014
tumber_server

   some of the [20]favourites i jotted down as i hacked on different
   models:
supervaluation
justquery
hello-bool
dataserverclient
bachbone.github.io
2048-ing-master
jonky-howler
pingleplungerdemo
faceboogler

   model   s favourite name: unity.github.io, which doesn   t exist in the
   training set.

bonus: board games

   i spent a bit of time trying to learn board game names, but wasn   t
   particularly successful. i suspect my dataset, at about 50k games, was
   just too small. some samples (more [21]here):
stopeest game
chef ths gome
the mitean game
eleppitt care game
chipling gome
the sidal game
elepes on the game
the hing board game

   well, it   s certainly figured out that the word    game    is important to
   unlocking the mystery of this distribution. good job on that, rbm. it
   caught a few other types of game names, but again with a lot of jpeg
   compression:
hocket'& pace
spop the gime
brauk
pocket quizs

   favourite name: the : the card game. the most commonly sampled name was
   the bile game, which appeared 700 times in 35k samples. neither game
   exists in the training set. if you do own a copy of the bile game,
   don   t invite me over for board game night.

   did they really need a neural network for that?   

   this is a question that probably doesn   t get asked often enough. the
   results here are pretty neat, but before we claim another victory for
   trendy neural networks, we should ask whether the problem we solved was
   actually difficult. i   ll follow the example of [22]this blog post by
   yoav goldberg and use unsmoothed maximum-likelihood character level
   language models as a dumb baseline to compare against. in short, we   ll
   generate strings one letter at a time, choosing the next letter by
   looking at the last n, and seeing which letters tended to follow that
   sequence in the training set.

   some examples from an order-4 model trained on the us place names
   dataset:
bonny maringer city of lake
sour motoruk mountain
mount branchorage lakes
duck kill bar rock
goatyard point
noblit hollow
spenceton
jay canal cemetery
oriflamming beach

   huh. those are, uh, actually pretty excellent. and with 4-grams it   s
   not at the point where it   s just copying the training data. around 25%
   of generated names exist in the training set, but i filtered those out
   of the list above. several of the individual tokens above don   t exist
   in the training set either, including the excellent    goatyard   .

   let   s see if we can salvage our dignity by comparing performance on the
   github dataset. the order-4 output is pretty goofy, so let   s give it 5
   characters of context:
littlepython-hall-effectv-frontangles2
media
easycanvas
shutupmrnotific
minkghost-deployshpaste_ember
terrain-exercise3
gaben
bot-repots-interestinggithub.io
credstatus
wall-as
bb-flappybao
py_shopping-sample

   our baseline   s not looking so hot now. it   s interesting to note some
   mistakes here that the rbm model almost never makes. for example, it
   never flubs the formatting of a url. it   s also very good at picking a
   consistent scheme for case and separators for each name, e.g.:
sapapp
rails_work_app
datatownsample
java-mails-rails

   this is where being able to see the whole string at once really comes
   in handy. when our markov model has generated as far as    java-mails   ,
   it doesn   t have enough context to know whether it should make
   java-mails-rails or java-mailsrails or java-mails_rails. we can always
   feed it even more context, but a window of 5 already leads to a lot of
   copy-pasting from the training set. for example, shutupmrnotific is
   funny, but it   s just a truncation of a repo from the training set,
   shutupmrnotification.

more stupid rbm tricks

   the coolest thing we can do with our trained models is ask them to come
   up with new names, but that   s not the only thing we can ask of them. we
   can also give them a name of our own choosing and ask them how good
   they think it is. let   s see if the model we trained on actor names has
   the hoped-for behaviour on the example names we described at the
   beginning:
>>> e('john smith')
-75.10
>>> e('dweezil zappa')
-38.14
>>> e('mcn zgl jey')
-34.25

   remember that lower energy corresponds to higher id203, so this
   is great! energy is proportional to the log of the id203, so the
   model thinks that dweezil is about 4 orders of magnitude more likely
   than mcn, and 37(!) orders of magnitude less likely than john. (that
   sounds like a lot, but dweezil is a pretty extreme example of a rare
   name - [23]it   s globally unique!)

   it can be interesting to walk around the neighbours of a name to get a
   feel for the energy landscape of the model, and its robustness to small
   changes:
   [zohnsmith.png] energy assigned by our model to various
   single-character substitutions on john smith. names are arranged into
   columns according to the affected index in the string. note that the
   y-axis is reversed.

   the chart above is heartening. first of all, it   s great that our model
   assigned lower energy to the    real    name than to any of the corrupted
   versions. but the order assigned to the corrupted names also seems very
   reasonable. the ones with the lowest energy - messrs. smitt, smitz, and
   smich - are the most plausible. the three samples with the highest
   energy are not only weird - they   re not even pronouncable in english.
   aside - score matching

   this is a nice intuitive way of evaluating our model's density function
   - it seems obvious that our model should generally assign more energy
   to a sample from our dataset after we've randomly nudged it. it also
   plays around a key weakness of rbms: we can't calculate the exact
   id203 they assign to any instance, because of an intractable term
   called the [24]partition function. but we can compare probabilities.
   when we take the energy difference between two instances, the partition
   function cancels out, and we get the exact log-ratio of their
   probabilities.

   in fact, this is the basis for a clever training technique called
   [25]score matching, which turns out to have a [26]surprising connection
   with denoising autoencoders, another powerful variety of generative
   neural network.

understanding what   s going on

   a common trick when working with neural nets in the image domain is to
   visualize what a neuron in the first hidden layer is    seeing    by
   treating the weights between that neuron and each input pixel as pixel
   intensities.
   [net_full_layer_0.png]
   visualization of 20 learned image filters from a convolutional neural
   network trained for handwritten digit classification (from michael
   nielsen's [27]neural networks and deep learning).

   we can do something similar here. the tables below each represent the
      receptive fields    of individual hidden units. the columns correspond
   to positions in a 17-character github repo name. a green character
   represents a strongly positive weight (i.e. this hidden unit    wants    to
   see that character at the position). red characters have strongly
   negative weights.

   these are just a couple examples taken from a model with 350 hidden
   units (the same model from which the above samples were taken).
   [28]this page has visualizations of all those units, as well as
   examples of strings having high affinity for each unit.
   v n d u b n r l z m       . 6   v
   a a g r l i o _ k
   r r s m i a e 4
   j r d i a l d 1
   v e b   o o t -
   e y e e n d y n a g h i e v p p i
   o d a o r s - a . . g v j h 2 n s
   i l q a m c s e $ - _ x f w c g a
   e s i g t t g i _ _ - j y p w - y
   i i t t e e n o - $ . . g _ . s 1

   this hidden unit wants to see    vndubnr? actually, this word search is
   hiding several useful words.
     * vagrant
     * android
     * arduino
     * angular
     * ansible

   this kind of multitasking is a common theme. and maybe it shouldn   t be
   surprising. this model only has 350 hidden units, but a good model of
   repository names needs to remember more words than that (not to mention
   formatting and the [29]phonotactic rules for inventing new words).

   of course, this hidden unit alone is perfectly happy to see hybrid
   prefixes like vadroid, or andulant or even aadmaae. it needs to work in
   concert with other hidden units that impose their own regularities,
   like   
   b y w a n e y   y
   a _ v e t o
   3 i k a d a
       c i b i
       r o k u
   s f 8 q i 5   h
   m 2 a t e l   z
   w i q c o x   4
   x m o r u r c $
   w h q l a 0 b k   7

   whereas the last unit was focused on a few domain-specific words, this
   unit is pretty generic. it mostly just wants to see a vowel in the
   fourth position followed immediately by a consonant (note that the
   chars it least wants to see there are [a, u, o, e, i]). with this and
   the previous unit turned on, we   re now happy to see    angular    and
      ansible   , but not    vagrant   ,    android   , or    arduino   .

   of course, our model has no explicit knowledge of what a    vowel    is, so
   it   s neat to see it picked up naturally as a useful feature.

   another emergent behaviour is the strong spatial locality. with few
   exceptions, hidden units have their strong weights tightly clustered on
   a particular neighbourhood of character positions. this is neat
   because, again, we never told our model that certain visible units are
      next to    each other - it knows nothing about the input geometry.

   a few more examples below.
     4 k y i . g i t h u b . i o $
     0   n h     g         - c
         j
   l n c _ l i p t n o . s o l p g
   t e n s e e o a - i o r a b c n
   t u - . t g a e o e e n b o r t e
   l t l t b o t o l n r u t . . i o
   m y . - l n e u c t - a l $ $ o s
   github url recognizer
     j   . q 1 $ $ $ $ $
         0 0 c 2 7
         a q r 1 3
         o w 8 c 9
         3 e 6 8 8
   0 9 8 b y u i 1 a s a
   7 6 6 h k i t o t a s
   9 5 - g b o _ t e e l y
   8 0 7 r l a o e 2 0 t e       e e
   - 4 3 q b u e s s i e s s s s s s
   short names (e.g. cs3001, team11). '$' is the padding character.
     n   8 3 7 0 . p p b $ n n t q m
         9 7 8 7 - w d g g r p . h o
         6 4 6 5 _ h q o y $ t - x t
           5 w 6 a w w p _ o c m i p
           9 1 3 r a l m - 8 j m q y
     3 1 k e c w u y a h k s u a l -
     5 _ _ a k g o u - a u h z z c k
   2 _ 6 b u s r m 1 _ n a k k g y n
   5 x i l - - h i . . y l e s n g d
   0 2 a a _ _ a u i $ e o y . d w g
   word delimeter at position 8
   n s f m s s s p s p p s a a a s s
   a d m d r t c c p s a a t t m p p
   c t g h d m m s t a s c s d p m l
   s m j f t c a m a t m m d s l b c
   d q w c c p b t c c b p m p s o d
   v w q 4 0 6 b v . k w q x v j w v
   y m v 2 2 b j p b q . p q j w m k
   h - . . . . p . v . q . . - _ b m
   b k _ - _ - _ _ _ _ _ _ _ _ . j q
   k l - _ - _ - - - - - - - . q q i
   cruise control for cool.
   u o i . 5 _ b w _ - y - - v y y w
   i n o _ _ -   j - _ - y _ - e w y
   s i s - 1 9   b     _ k k y w n o
   a h u k - 5         v _ y   - g e
   c v l l 4 4             v       g
   c c a u n h c                 a
   b p p o u z m 4               7 f
   l a l a h m p 5   .     w     s b
   g s e r k x s 9 5 5 7 v q     8 a
   d e o z z k . . 9 9 . . . . . 6 8
   three letter acronym prefixes (e.g. php-server)

see all 350 in the [30]hidden unit zoo

   [31]previous [32]next

making it better

going deeper

   rbms can be stacked on one another to form a [33]deep belief network.
   it seems plausible that additional layers would be able to learn higher
   layers of abstraction and generate even better samples.

   remember how we saw above that most units learn patterns local to a
   particular region of the string? this may explain solecisms like lake
   lake, or church swamp. if, as seems to be the case, our initial layer
   of hidden units is mostly learning words, phonotactics, and low-level
   structural patterns (word lengths, spacing), another layer of hidden
   units on top of those could learn more semantic patterns. e.g.    lake
   $foo   ,    $foo lake   ,    $foo swamp   , but never    lake lake   ,    lake swamp   ,
   etc. i would be sad to see days inn oil field go though. that one was
   pretty good.

translation invariance

   under the current architecture, for a model to learn the word    pond    (a
   very useful word to learn if you want to generate place names), it
   needs to memorize a separate version for each position it can appear:
   ___ pond, ____ pond, _____ pond, etc.

   we   d like our model to learn robust, position-invariant patterns and
   understand that    hays pond    and    darby pond    are quite similar (even
   though their vector representations are completely disjoint).

   one solution to this problem is to use a recurrent architecture.
   another, which is more readily applicable to rbms, is to use
   convolutional units. if you   re familiar with the use of [34]id98s for
   vision tasks, this will sound familiar.

   in a convolutional rbm, each hidden unit will only have connections to
   a small substring of the input. weakening our hidden units like this
   doesn   t sound like much of a win, but because each unit has fewer
   weights, we can use a lot more of them without slowing down training.
   not only that, but each hidden unit will work together with many
   siblings that share the exact same weights but look at different
   regions of the string. together these make up what   s called a    filter   .

   as an example, the model might learn a filter that recognizes a
   consonant followed by a vowel anywhere in the string. imagine taking
   the [35]consonant-vowel hidden unit from before, and making 15 copies
   of it, one copy for a vowel at index 0 and consonant at index 1,
   another copy for vowel at index 1 and consonant at index 2, etc.

   again, the promise of this is strongly suggested by the weights we see
   on the hidden units above. our hidden units are already looking at
   local regions of the input, and there   s clear evidence that the model
   is having to learn and store the same pattern multiple times for
   different positions: [36]the hidden unit zoo lists 20 hidden units that
   seem to be primarily responsible for recognizing github.io and
   github.com urls. [37]some are even pseudo-convolutional, trying to
   recognize two shifted versions simultaneously.

   with convolutional units, we could help the network do what it   s
   already doing much more efficiently (in terms of the size of the model,
   and the amount of information learned per training instance).

practical applications

   none whatsoever.

acknowledgements

   [38]the unreasonable effectiveness of recurrent neural networks by
   andrej karpathy inspired me to play with character-level
   representations (and was basically the first thing i read that got me
   excited about deep learning). if you haven   t read it already, go do it
   now! if you   re curious how these results compare to char-id56, [39]here
   are some samples from a char-id56 model trained for around 24 hours on
   github repos with id56_size=256, seq_length=20 and all other options set
   to their defaults.

   thanks to [40]falsifian for reviewing a draft of this post and teaching
   me about simulated annealing. thanks to an anonymous brilliant artist
   for drawing that tolkien-esque map of [41]   kicksville    in the header.

   tagged: [42]machine learning
   please enable javascript to view the [43]comments powered by disqus.

     * colin_morris
     *

     * [44]colinmorris
     * [45]halfeatenscone

   splashing around in the shallow end of the deep learning pool.

references

   visible links
   1. http://colinmorris.github.io/feed.xml
   2. https://colinmorris.github.io/
   3. https://colinmorris.github.io/blog/
   4. https://colinmorris.github.io/toys/
   5. https://colinmorris.github.io/about/
   6. https://en.wikipedia.org/wiki/named-entity_recognition
   7. https://github.com/colinmorris/char-rbm
   8. https://github.com/colinmorris/char-rbm/blob/master/samples/usgeo_unique.txt#l191
   9. https://en.wikipedia.org/wiki/one-hot
  10. http://www.cs.toronto.edu/~hinton/absps/guidetr.pdf
  11. http://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf
  12. https://colinmorris.github.io/blog/rbm-sampling
  13. https://github.com/colinmorris/char-rbm/blob/master/samples/readme.markdown
  14. https://github.com/colinmorris/char-rbm/blob/master/readme-datasets.md
  15. https://github.com/colinmorris/char-rbm/blob/master/samples/actors_unique.txt
  16. http://www.imdb.com/name/nm1013241/
  17. https://github.com/colinmorris/char-rbm/blob/master/samples/usgeo_unique.txt
  18. https://colinmorris.github.io/assets/rbm/kicksville.png
  19. https://github.com/colinmorris/char-rbm/blob/master/samples/repos_unique.txt
  20. https://github.com/colinmorris/char-rbm/blob/workspace/halloffame.txt
  21. https://github.com/colinmorris/char-rbm/blob/master/samples/games_unique.txt
  22. http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139
  23. https://books.google.com/ngrams/graph?content=dweezil&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=&direct_url=t1;,dweezil;,c0
  24. https://en.wikipedia.org/wiki/partition_function_(mathematics)
  25. https://www.cs.helsinki.fi/u/ahyvarin/papers/jmlr05.pdf
  26. http://www.iro.umontreal.ca/~vincentp/publications/smdae_techreport.pdf
  27. http://neuralnetworksanddeeplearning.com/chap6.html
  28. https://colinmorris.github.io/rbm/zoo
  29. https://en.wikipedia.org/wiki/phonotactics
  30. https://colinmorris.github.io/rbm/zoo
  31. https://colinmorris.github.io/blog/dreaming-rbms#hidden-carousel
  32. https://colinmorris.github.io/blog/dreaming-rbms#hidden-carousel
  33. https://en.wikipedia.org/wiki/deep_belief_network
  34. https://en.wikipedia.org/wiki/convolutional_neural_network
  35. http://colinmorris.github.io/rbm/zoo/#unit122
  36. https://colinmorris.github.io/rbm/zoo
  37. https://colinmorris.github.io/rbm/zoo#unit109
  38. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  39. https://github.com/colinmorris/char-rbm/blob/workspace/samples/cleaned/repos_char-id56_unique.txt
  40. http://www.falsifian.org/
  41. https://colinmorris.github.io/assets/rbm/kicksville.png
  42. https://colinmorris.github.io/blog/tagged/machine-learning/
  43. https://disqus.com/?ref_noscript
  44. https://github.com/colinmorris
  45. https://twitter.com/halfeatenscone

   hidden links:
  47. https://colinmorris.github.io/blog/dreaming-rbms
  48. https://colinmorris.github.io/rbm/repos
  49. https://colinmorris.github.io/rbm/geo
  50. https://colinmorris.github.io/rbm/actors
  51. mailto:/
