   #[1]github [2]recent commits to restricted-boltzmann-machines:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]63
     * [35]star [36]770
     * [37]fork [38]334

[39]echen/[40]restricted-boltzmann-machines

   [41]code [42]issues 1 [43]pull requests 1 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   restricted id82s in python.
   [47]http://blog.echen.me/2011/07/18/intro   
     * [48]26 commits
     * [49]2 branches
     * [50]0 releases
     * [51]fetching contributors
     * [52]mit

    1. [53]python 100.0%

   (button) python
   branch: master (button) new pull request
   [54]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/e
   [55]download zip

downloading...

   want to be notified of new releases in
   echen/restricted-boltzmann-machines?
   [56]sign in [57]sign up

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [60]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [61]download the github extension for visual studio
   and try again.

   (button) go back
   [62]@echen
   [63]echen [64]merge pull request [65]#7 [66]from c-connors/master
   (button)    
fixed bias unit.

   latest commit [67]7d69db9 jul 25, 2017
   [68]permalink
   type       name            latest commit message       commit time
        failed to load latest commit information.
        [69]license.txt
        [70]readme.md   [71]document rbmcmd in readme.md may 14, 2014
        [72]rbm.py
        [73]rbmcmd

readme.md

how to use

   first, initialize an rbm with the desired number of visible and hidden
   units.
rbm = rbm(num_visible = 6, num_hidden = 2)

   next, train the machine:
training_data = np.array([[1,1,1,0,0,0],[1,0,1,0,0,0],[1,1,1,0,0,0],[0,0,1,1,1,0
], [0,0,1,1,0,0],[0,0,1,1,1,0]]) # a 6x6 matrix where each row is a training exa
mple and each column is a visible unit.
r.train(training_data, max_epochs = 5000) # don't run the training for more than
 5000 epochs.

   finally, run wild!
# given a new set of visible units, we can see what hidden units are activated.
visible_data = np.array([[0,0,0,1,1,0]]) # a matrix with a single row that conta
ins the states of the visible units. (we can also include more rows.)
r.run_visible(visible_data) # see what hidden units are activated.

# given a set of hidden units, we can see what visible units are activated.
hidden_data = np.array([[1,0]]) # a matrix with a single row that contains the s
tates of the hidden units. (we can also include more rows.)
r.run_hidden(hidden_data) # see what visible units are activated.

# we can let the network run freely (aka, daydream).
r.daydream(100) # daydream for 100 steps on a single initialization.

introduction

   suppose you ask a bunch of users to rate a set of movies on a 0-100
   scale. in classical [74]factor analysis, you could then try to explain
   each movie and user in terms of a set of latent factors. for example,
   movies like star wars and lord of the rings might have strong
   associations with a latent science fiction and fantasy factor, and
   users who like wall-e and toy story might have strong associations with
   a latent pixar factor.

   restricted id82s essentially perform a binary version of
   factor analysis. (this is one way of thinking about rbms; there are, of
   course, others, and lots of different ways to use rbms, but i'll adopt
   this approach for this post.) instead of users rating a set of movies
   on a continuous scale, they simply tell you whether they like a movie
   or not, and the rbm will try to discover latent factors that can
   explain the activation of these movie choices.

   more technically, a restricted id82 is a stochastic neural
   network (neural network meaning we have neuron-like units whose binary
   activations depend on the neighbors they're connected to; stochastic
   meaning these activations have a probabilistic element) consisting of:
     * one layer of visible units (users' movie preferences whose states
       we know and set);
     * one layer of hidden units (the latent factors we try to learn); and
     * a bias unit (whose state is always on, and is a way of adjusting
       for the different inherent popularities of each movie).

   furthermore, each visible unit is connected to all the hidden units
   (this connection is undirected, so each hidden unit is also connected
   to all the visible units), and the bias unit is connected to all the
   visible units and all the hidden units. to make learning easier, we
   restrict the network so that no visible unit is connected to any other
   visible unit and no hidden unit is connected to any other hidden unit.

   for example, suppose we have a set of six movies (harry potter, avatar,
   lotr 3, gladiator, titanic, and glitter) and we ask users to tell us
   which ones they want to watch. if we want to learn two latent units
   underlying movie preferences -- for example, two natural groups in our
   set of six movies appear to be sf/fantasy (containing harry potter,
   avatar, and lotr 3) and oscar winners (containing lotr 3, gladiator,
   and titanic), so we might hope that our latent units will correspond to
   these categories -- then our rbm would look like the following:

   [75]rbm example

   (note the resemblance to a factor analysis graphical model.)

state activation

   restricted id82s, and neural networks in general, work by
   updating the states of some neurons given the states of others, so
   let's talk about how the states of individual units change. assuming we
   know the connection weights in our rbm (we'll explain how to learn
   these below), to update the state of unit $i$:
     * compute the activation energy $a_i = \sum_j w_{ij} x_j$ of unit
       $i$, where the sum runs over all units $j$ that unit $i$ is
       connected to, $w_{ij}$ is the weight of the connection between $i$
       and $j$, and $x_j$ is the 0 or 1 state of unit $j$. in other words,
       all of unit $i$'s neighbors send it a message, and we compute the
       sum of all these messages.
     * let $p_i = \sigma(a_i)$, where $\sigma(x) = 1/(1 + exp(-x))$ is the
       logistic function. note that $p_i$ is close to 1 for large positive
       activation energies, and $p_i$ is close to 0 for negative
       activation energies.
     * we then turn unit $i$ on with id203 $p_i$, and turn it off
       with id203 $1 - p_i$.
     * (in layman's terms, units that are positively connected to each
       other try to get each other to share the same state (i.e., be both
       on or off), while units that are negatively connected to each other
       are enemies that prefer to be in different states.)

   for example, let's suppose our two hidden units really do correspond to
   sf/fantasy and oscar winners.
     * if alice has told us her six binary preferences on our set of
       movies, we could then ask our rbm which of the hidden units her
       preferences activate (i.e., ask the rbm to explain her preferences
       in terms of latent factors). so the six movies send messages to the
       hidden units, telling them to update themselves. (note that even if
       alice has declared she wants to watch harry potter, avatar, and
       lotr 3, this doesn't guarantee that the sf/fantasy hidden unit will
       turn on, but only that it will turn on with high id203. this
       makes a bit of sense: in the real world, alice wanting to watch all
       three of those movies makes us highly suspect she likes sf/fantasy
       in general, but there's a small chance she wants to watch them for
       other reasons. thus, the rbm allows us to generate models of people
       in the messy, real world.)
     * conversely, if we know that one person likes sf/fantasy (so that
       the sf/fantasy unit is on), we can then ask the rbm which of the
       movie units that hidden unit turns on (i.e., ask the rbm to
       generate a set of movie recommendations). so the hidden units send
       messages to the movie units, telling them to update their states.
       (again, note that the sf/fantasy unit being on doesn't guarantee
       that we'll always recommend all three of harry potter, avatar, and
       lotr 3 because, hey, not everyone who likes science fiction liked
       avatar.)

learning weights

   so how do we learn the connection weights in our network? suppose we
   have a bunch of training examples, where each training example is a
   binary vector with six elements corresponding to a user's movie
   preferences. then for each epoch, do the following:
     * take a training example (a set of six movie preferences). set the
       states of the visible units to these preferences.
     * next, update the states of the hidden units using the logistic
       activation rule described above: for the $j$th hidden unit, compute
       its activation energy $a_j = \sum_i w_{ij} x_i$, and set $x_j$ to 1
       with id203 $\sigma(a_j)$ and to 0 with id203 $1 -
       \sigma(a_j)$. then for each edge $e_{ij}$, compute
       $positive(e_{ij}) = x_i * x_j$ (i.e., for each pair of units,
       measure whether they're both on).
     * now reconstruct the visible units in a similar manner: for each
       visible unit, compute its activation energy $a_i$, and update its
       state. (note that this reconstruction may not match the original
       preferences.) then update the hidden units again, and compute
       $negative(e_{ij}) = x_i * x_j$ for each edge.
     * update the weight of each edge $e_{ij}$ by setting $w_{ij} = w_{ij}
       + l * (positive(e_{ij}) - negative(e_{ij}))$, where $l$ is a
       learning rate.
     * repeat over all training examples.

   continue until the network converges (i.e., the error between the
   training examples and their reconstructions falls below some threshold)
   or we reach some maximum number of epochs.

   why does this update rule make sense? note that
     * in the first phase, $positive(e_{ij})$ measures the association
       between the $i$th and $j$th unit that we want the network to learn
       from our training examples;
     * in the "reconstruction" phase, where the rbm generates the states
       of visible units based on its hypotheses about the hidden units
       alone, $negative(e_{ij})$ measures the association that the network
       itself generates (or "daydreams" about) when no units are fixed to
       training data.

   so by adding $positive(e_{ij}) - negative(e_{ij})$ to each edge weight,
   we're helping the network's daydreams better match the reality of our
   training examples.

   (you may hear this update rule called contrastive divergence, which is
   basically a funky term for "approximate id119".)

examples

   i wrote [76]a simple rbm implementation in python (the code is heavily
   commented, so take a look if you're still a little fuzzy on how
   everything works), so let's use it to walk through some examples.

   first, i trained the rbm using some fake data.
     * alice: (harry potter = 1, avatar = 1, lotr 3 = 1, gladiator = 0,
       titanic = 0, glitter = 0). big sf/fantasy fan.
     * bob: (harry potter = 1, avatar = 0, lotr 3 = 1, gladiator = 0,
       titanic = 0, glitter = 0). sf/fantasy fan, but doesn't like avatar.
     * carol: (harry potter = 1, avatar = 1, lotr 3 = 1, gladiator = 0,
       titanic = 0, glitter = 0). big sf/fantasy fan.
     * david: (harry potter = 0, avatar = 0, lotr 3 = 1, gladiator = 1,
       titanic = 1, glitter = 0). big oscar winners fan.
     * eric: (harry potter = 0, avatar = 0, lotr 3 = 1, gladiator = 1,
       titanic = 1, glitter = 0). oscar winners fan, except for titanic.
     * fred: (harry potter = 0, avatar = 0, lotr 3 = 1, gladiator = 1,
       titanic = 1, glitter = 0). big oscar winners fan.

   the network learned the following weights:
                 bias unit       hidden 1        hidden 2
bias unit       -0.08257658     -0.19041546      1.57007782
harry potter    -0.82602559     -7.08986885      4.96606654
avatar          -1.84023877     -5.18354129      2.27197472
lotr 3           3.92321075      2.51720193      4.11061383
gladiator        0.10316995      6.74833901     -4.00505343
titanic         -0.97646029      3.25474524     -5.59606865
glitter         -4.44685751     -2.81563804     -2.91540988

   note that the first hidden unit seems to correspond to the oscar
   winners, and the second hidden unit seems to correspond to the
   sf/fantasy movies, just as we were hoping.

   what happens if we give the rbm a new user, george, who has (harry
   potter = 0, avatar = 0, lotr 3 = 0, gladiator = 1, titanic = 1, glitter
   = 0) as his preferences? it turns the oscar winners unit on (but not
   the sf/fantasy unit), correctly guessing that george probably likes
   movies that are oscar winners.

   what happens if we activate only the sf/fantasy unit, and run the rbm a
   bunch of different times? in my trials, it turned on harry potter,
   avatar, and lotr 3 three times; it turned on avatar and lotr 3, but not
   harry potter, once; and it turned on harry potter and lotr 3, but not
   avatar, twice. note that, based on our training examples, these
   generated preferences do indeed match what we might expect real
   sf/fantasy fans want to watch.

modifications

   i tried to keep the connection-learning algorithm i described above
   pretty simple, so here are some modifications that often appear in
   practice:
     * above, $negative(e_{ij})$ was determined by taking the product of
       the $i$th and $j$th units after reconstructing the visible units
       once and then updating the hidden units again. we could also take
       the product after some larger number of reconstructions (i.e.,
       repeat updating the visible units, then the hidden units, then the
       visible units again, and so on); this is slower, but describes the
       network's daydreams more accurately.
     * instead of using $positive(e_{ij})=x_i * x_j$, where $x_i$ and
       $x_j$ are binary 0 or 1 states, we could also let $x_i$ and/or
       $x_j$ be activation probabilities. similarly for
       $negative(e_{ij})$.
     * we could penalize larger edge weights, in order to get a sparser or
       more regularized model.
     * when updating edge weights, we could use a momentum factor: we
       would add to each edge a weighted sum of the current step as
       described above (i.e., $l * (positive(e_{ij}) - negative(e_{ij})$)
       and the step previously taken.
     * instead of using only one training example in each epoch, we could
       use batches of examples in each epoch, and only update the
       network's weights after passing through all the examples in the
       batch. this can speed up the learning by taking advantage of fast
       matrix-multiplication algorithms.

rbmcmd

   there is command-line tool to train and run rbm.

   here is the code that corresponds to the first example from "how to
   use" section
# first, initialize an rbm with the desired number of visible and hidden units.
./rbmcmd rbmstate.dat init  6  2  0.1

# next, train the machine:
./rbmcmd rbmstate.dat train 5000 << \eof
1 1 1 0 0 0
1 0 1 0 0 0
1 1 1 0 0 0
0 0 1 1 1 0
0 0 1 1 0 0
0 0 1 1 1 0
eof

# finally, run wild
# given a new set of visible units, we can see what hidden units are activated.
echo "0 0 0 1 1 0" | ./rbmcmd rbmstate.dat run_visible
# 1 0

# given a set of hidden units, we can see what visible units are activated.
echo "1 0" | ./rbmcmd rbmstate.dat run_hidden
# 0 1 1 1 0 0

# we can let the network run freely (aka, daydream).
# daydream for 3 steps on a single initialization.
./rbmcmd rbmstate.dat daydream_trace 3
# 0.901633539115 0.718084610948 0.00650400574634 0.853636318291 0.938241835347 0
.0747538486547
# 1.0 1.0 1.0 0.0 0.0 0.0
# 1.0 1.0 1.0 0.0 0.0 0.0


# see 5 dreams, each of 2 step from random data
./rbmcmd rbmstate.dat daydream 3 5
# 1 1 1 0 0 0
# 0 0 1 1 0 0
# 1 0 1 0 0 0
# 1 0 1 0 0 0
# 1 1 1 0 0 0


further

   if you're interested in learning more about restricted boltzmann
   machines, here are some good links.
     * [77]a practical guide to training restricted id82s, by
       geoffrey hinton.
     * a talk by andrew ng on [78]unsupervised id171 and deep
       learning.
     * [79]restricted id82s for id185. i
       found this paper hard to read, but it's an interesting application
       to the netflix prize.
     * [80]geometry of the restricted id82. a very readable
       introduction to rbms, "starting with the observation that its
       zariski closure is a hadamard power of the first secant variety of
       the segre variety of projective lines". (i kid, i kid.)

     *    2019 github, inc.
     * [81]terms
     * [82]privacy
     * [83]security
     * [84]status
     * [85]help

     * [86]contact github
     * [87]pricing
     * [88]api
     * [89]training
     * [90]blog
     * [91]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [92]reload to refresh your
   session. you signed out in another tab or window. [93]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/echen/restricted-boltzmann-machines/commits/master.atom
   3. https://github.com/echen/restricted-boltzmann-machines#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/echen/restricted-boltzmann-machines
  32. https://github.com/join
  33. https://github.com/login?return_to=/echen/restricted-boltzmann-machines
  34. https://github.com/echen/restricted-boltzmann-machines/watchers
  35. https://github.com/login?return_to=/echen/restricted-boltzmann-machines
  36. https://github.com/echen/restricted-boltzmann-machines/stargazers
  37. https://github.com/login?return_to=/echen/restricted-boltzmann-machines
  38. https://github.com/echen/restricted-boltzmann-machines/network/members
  39. https://github.com/echen
  40. https://github.com/echen/restricted-boltzmann-machines
  41. https://github.com/echen/restricted-boltzmann-machines
  42. https://github.com/echen/restricted-boltzmann-machines/issues
  43. https://github.com/echen/restricted-boltzmann-machines/pulls
  44. https://github.com/echen/restricted-boltzmann-machines/projects
  45. https://github.com/echen/restricted-boltzmann-machines/pulse
  46. https://github.com/join?source=prompt-code
  47. http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/
  48. https://github.com/echen/restricted-boltzmann-machines/commits/master
  49. https://github.com/echen/restricted-boltzmann-machines/branches
  50. https://github.com/echen/restricted-boltzmann-machines/releases
  51. https://github.com/echen/restricted-boltzmann-machines/graphs/contributors
  52. https://github.com/echen/restricted-boltzmann-machines/blob/master/license.txt
  53. https://github.com/echen/restricted-boltzmann-machines/search?l=python
  54. https://github.com/echen/restricted-boltzmann-machines/find/master
  55. https://github.com/echen/restricted-boltzmann-machines/archive/master.zip
  56. https://github.com/login?return_to=https://github.com/echen/restricted-boltzmann-machines
  57. https://github.com/join?return_to=/echen/restricted-boltzmann-machines
  58. https://desktop.github.com/
  59. https://desktop.github.com/
  60. https://developer.apple.com/xcode/
  61. https://visualstudio.github.com/
  62. https://github.com/echen
  63. https://github.com/echen/restricted-boltzmann-machines/commits?author=echen
  64. https://github.com/echen/restricted-boltzmann-machines/commit/7d69db921636bbac4086d2a017b34d90db01ff0f
  65. https://github.com/echen/restricted-boltzmann-machines/pull/7
  66. https://github.com/echen/restricted-boltzmann-machines/commit/7d69db921636bbac4086d2a017b34d90db01ff0f
  67. https://github.com/echen/restricted-boltzmann-machines/commit/7d69db921636bbac4086d2a017b34d90db01ff0f
  68. https://github.com/echen/restricted-boltzmann-machines/tree/7d69db921636bbac4086d2a017b34d90db01ff0f
  69. https://github.com/echen/restricted-boltzmann-machines/blob/master/license.txt
  70. https://github.com/echen/restricted-boltzmann-machines/blob/master/readme.md
  71. https://github.com/echen/restricted-boltzmann-machines/commit/dc45e4510dd81dfc50d8d9293a4ddcd4e35edb58
  72. https://github.com/echen/restricted-boltzmann-machines/blob/master/rbm.py
  73. https://github.com/echen/restricted-boltzmann-machines/blob/master/rbmcmd
  74. http://en.wikipedia.org/wiki/factor_analysis
  75. http://dl.dropbox.com/u/10506/blog/rbms/rbm-example.png
  76. https://github.com/echen/restricted-boltzmann-machines
  77. http://www.cs.toronto.edu/~hinton/absps/guidetr.pdf
  78. http://www.youtube.com/watch?v=zmnoatzigik
  79. http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf
  80. http://arxiv.org/abs/0908.4425
  81. https://github.com/site/terms
  82. https://github.com/site/privacy
  83. https://github.com/security
  84. https://githubstatus.com/
  85. https://help.github.com/
  86. https://github.com/contact
  87. https://github.com/pricing
  88. https://developer.github.com/
  89. https://training.github.com/
  90. https://github.blog/
  91. https://github.com/about
  92. https://github.com/echen/restricted-boltzmann-machines
  93. https://github.com/echen/restricted-boltzmann-machines

   hidden links:
  95. https://github.com/
  96. https://github.com/echen/restricted-boltzmann-machines
  97. https://github.com/echen/restricted-boltzmann-machines
  98. https://github.com/echen/restricted-boltzmann-machines
  99. https://help.github.com/articles/which-remote-url-should-i-use
 100. https://github.com/echen/restricted-boltzmann-machines#how-to-use
 101. https://github.com/echen/restricted-boltzmann-machines#introduction
 102. https://github.com/echen/restricted-boltzmann-machines#state-activation
 103. https://github.com/echen/restricted-boltzmann-machines#learning-weights
 104. https://github.com/echen/restricted-boltzmann-machines#examples
 105. https://github.com/echen/restricted-boltzmann-machines#modifications
 106. https://github.com/echen/restricted-boltzmann-machines#rbmcmd
 107. https://github.com/echen/restricted-boltzmann-machines#further
 108. https://github.com/
