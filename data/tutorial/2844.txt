   #[1]github [2]recent commits to yaid48:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]25
     * [35]star [36]232
     * [37]fork [38]33

[39]jmschrei/[40]yaid48

   [41]code [42]issues 7 [43]pull requests 0 [44]projects 0 [45]wiki
   [46]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [47]sign up
   yet another hidden markov model repository.
     * [48]117 commits
     * [49]2 branches
     * [50]2 releases
     * [51]fetching contributors
     * [52]mit

    1. [53]python 100.0%

   (button) python
   branch: master (button) new pull request
   [54]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/j
   [55]download zip

downloading...

   want to be notified of new releases in jmschrei/yaid48?
   [56]sign in [57]sign up

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [60]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [61]download the github extension for visual studio
   and try again.

   (button) go back
   [62]@jmschrei
   [63]jmschrei [64]update readme.md
   latest commit [65]f363f59 dec 2, 2015
   [66]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [67]examples
   [68]tests [69]v1.1.1 dec 10, 2014
   [70]yaid48 [71]fixed error on backwards algorithm jan 29, 2015
   [72].gitignore
   [73].travis.yml [74]tell travis not to helpfully provide us with a
   newer, broken numpy. 1    sep 4, 2014
   [75]changes.txt
   [76]license.txt
   [77]manifest
   [78]manifest.in
   [79]readme.md
   [80]requirements.txt
   [81]setup.py

readme.md

yaid48

   [82]build status

   yet another hidden markov model library

   note: while yaid48 is still fully functional, active development has
   moved over to [83]pomegranate. please switch over at your convenience.

   this module implements id48 (id48s) with a
   compositional, graph- based interface. models can be constructed node
   by node and edge by edge, built up from smaller models, loaded from
   files, baked (into a form that can be used to calculate probabilities
   efficiently), trained on data, and saved.

   implements the forwards, backwards, forward-backward, and viterbi
   algorithms, and training by both baum-welch and viterbi algorithms.

   silent states are accounted for, but loops containing all silent states
   are prohibited. tied states are also implemented, and handled
   appropriately in the training of models.

installation

   since yaid48 is on pypi, installation is as easy as running
pip install yaid48

contributing

   if you would like to contribute a feature then fork the master branch
   (fork the release if you are fixing a bug). be sure to run the tests
   before changing any code. you'll need to have [84]nosetests installed.
   the following command will run all the tests:
nosetests -w tests/

   let us know what you want to do just in case we're already working on
   an implementation of something similar. this way we can avoid any
   needless duplication of effort. also, please don't forget to add tests
   for any new functions.

documentation

   see the [85]wiki for documentation of yaid48's functions and design. for
   real-world usage check out the [86]examples.

tutorial

   for our examples here we're going to make the random number generator
   deterministic:
>>> random.seed(0)

   to use this module, first create a model, which is the main id48 class:
>>> model = model(name="examplemodel")

   you then need to populate the model with state objects. states are
   constructed from emission distributions; right now a few continuous
   distributions over floats are available, but new distribution classes
   are simple to write. for our example, we will use the
   uniformdistribution:
>>> distribution = uniformdistribution(0.0, 1.0)

   and then construct a state that emits from the distribution:
>>> state = state(distribution, name="uniform")

   and another state, emitting from a normal distribution with mean 0 and
   standard deviation 2:
>>> state2 = state(normaldistribution(0, 2), name="normal")

   if none is used as the distribution when creating a state, that state
   is a "silent state". silent states don't emit anything, but are useful
   for wiring together complex id48s. by default, a model has two special
   silent states: a start state model.start, and an end state model.end.

   topologies which include cycles of only silent states are prohibited;
   most id48 algorithms cannot process them.
>>> silent = state(none, name="silent")

   we then add states to the id48 with the model.add_state method:
>>> model.add_state(state)
>>> model.add_state(state2)

   you can then add transitions between states, with associated
   probabilities. out-edge probabilities are normalized to 1. for every
   state when the model is baked, not before.
>>> model.add_transition(state, state, 0.4)
>>> model.add_transition(state, state2, 0.4)
>>> model.add_transition(state2, state2, 0.4)
>>> model.add_transition(state2, state, 0.4)

   don't forget transitions in from the start state and out to the end
   state:
>>> model.add_transition(model.start, state, 0.5)
>>> model.add_transition(model.start, state2, 0.5)
>>> model.add_transition(state, model.end, 0.2)
>>> model.add_transition(state2, model.end, 0.2)

   if you want to look at your model, try model.draw(). note that this
   unfortunately cannot plot self loops. if you want to do a better job of
   drawing the model, the underlying id48 graph is accessible as the graph
   attribute of the model object.

   if you want to compose two models together, use the model.add_model()
   method. note that you should not try to use the added model separately
   once you do this. you can also make use of the
   model.concatenate_model() method, which will assume you simply want to
   connect model_a.end to model_b.start with a 1. id203 edge.

   once we've finished building our model, we have to bake it. internally,
   baking the model generates the transition log id203 matrix, and
   imposes a numerical ordering on the states. if you add more states to
   the model, you will have to bake it again. baking is also where edge
   id172 occurs to ensure that the out-edges for all nodes (except
   model.end) sum to 1. lastly, a simplification of the graph occurs here,
   merging any silent states which are connected simply by a 1.0
   id203 edge, as they cannot add value to the graph. you may toggle
   'verbose=true' in the bake method to get a log of when either change
   occurs to your graph.
>>> model.bake()

   now that our model is complete, we can generate an example sequence
   from it:
>>> sequence = model.sample()
>>> sequence
[0.7579544029403025, 0.25891675029296335, 0.4049341374504143, \
0.30331272607892745, 0.5833820394550312]

   and another:
>>> model.sample()
[0.28183784439970383, 0.6183689966753316, -2.411068768608379]

   and another:
>>> model.sample()
[0.47214271545271336, -0.5804485412450214]

   we can calculate the log id203 of the sequence given the model
   (the log likelihood), summing over all possible paths, using both the
   forward and backward algorithms. log id203 is reported in nats
   (i.e. it is natural log id203). both algorithms return the full
   table of size len( observations ) x len( states ). for the forward
   algorithm, the entry at position i, j represents the log id203 of
   beginning at the start of the sequence, and summing over all paths to
   align observation i to hidden state j. this state can be recovered by
   pulling it from model.states[j].
>>> model.forward(sequence)
[[       -inf        -inf        -inf  0.        ]
 [-2.37704475 -0.69314718 -2.1322948         -inf]
 [-3.05961307 -1.43914762 -2.86809348        -inf]
 [-3.80752847 -2.1749463  -3.60588302        -inf]
 [-4.53632138 -2.91273584 -4.34219628        -inf]
 [-5.30367664 -3.6490491  -5.08355666        -inf]]

   in order to get the log id203 of the full sequence given the
   model, you can write the following:
>>> model.forward(sequence)[ len(sequence), model.end_index ]
-5.0835566645

   or, use a wrapper to get that value by default:
>>> model.log_id203(sequence)
-5.0835566645

   the same paradigm is used for the backward algorithm. indices i, j
   represent the id203 of having aligned observation i to state j
   and continued aligning the remainder of the sequence till the end.
>>> model.backward(sequence)
[[-5.30670022 -5.30670022        -inf -5.08355666]
 [-4.56069977 -4.56069977        -inf -4.33755622]
 [-3.8249011  -3.8249011         -inf -3.60175755]
 [-3.08711156 -3.08711156        -inf -2.863968  ]
 [-2.3507983  -2.3507983         -inf -2.12765475]
 [-1.60943791 -1.60943791  0.                -inf]]

>>> model.backward(sequence)[ 0, model.start_index ]
-5.0835566645

   the forward-backward algorithm is also implemented in a similar manner.
   it will return a tuple of the estimated transition probabilities given
   with that sequence and the table of log probabilities of the sum of all
   paths of the alignment of observation i with state j. indices i, j
   represent having started at the beginning of the sequence, aligned
   observation i to state j, and then continued on to align the remainder
   of the sequence to the model.
>>> model.forward_backward(sequence)
(array([[-2.03205947, -0.39913252, -1.61932212,        -inf],
       [-2.03481952, -0.40209763, -1.60753724,        -inf],
       [       -inf,        -inf,        -inf,        -inf],
       [-1.85418786, -0.17029029,        -inf,        -inf]]),
array([[-1.85418786, -0.17029029,  0.        ,  0.        ],
       [-1.80095751, -0.18049206,  0.        ,  0.        ],
       [-1.81108336, -0.17850119,  0.        ,  0.        ],
       [-1.80356301, -0.17997747,  0.        ,  0.        ],
       [-1.82955788, -0.17493035,  0.        ,  0.        ]]))

   we can also find the most likely path, and the id203 thereof,
   using the viterbi algorithm. this returns a tuple of the likelihood
   under the ml path and the ml path itself. the ml path is in turn a list
   of tuples of state objects and the number of items in the sequence that
   had been generated by that point in the path (to account for the
   presence of silent states).
>>> model.viterbi(sequence)
(-5.9677480204906654, \
[(0, state(examplemodel-start, none)), \
(1, state(uniform, uniformdistribution(0.0, 1.0))), \
(2, state(uniform, uniformdistribution(0.0, 1.0))), \
(3, state(uniform, uniformdistribution(0.0, 1.0))), \
(4, state(uniform, uniformdistribution(0.0, 1.0))), \
(5, state(uniform, uniformdistribution(0.0, 1.0))), \
(5, state(examplemodel-end, none))])

   given a list of sequences, we can train our id48 by calling
   model.train(). this returns the final log score: the log of the sum of
   the probabilities of all training sequences. it also prints the
   improvement in log score on each training iteration, and stops if the
   improvement gets too small or actually goes negative.
>>> sequences = [sequence]
>>> model.log_id203(sequence)
-5.0835566644993735

>>> log_score = model.train(sequences)
training improvement: 5.81315226327
training improvement: 0.156159401683
training improvement: 0.0806734819188
training improvement: 0.0506679952827
training improvement: 0.142593661095
training improvement: 0.305806209012
training improvement: 0.301331333752
training improvement: 0.380117757466
training improvement: 0.773814416569
training improvement: 1.58660096053
training improvement: 0.439182120777
training improvement: 0.0067603436265
training improvement: 5.5971526649e-06
training improvement: 3.75166564481e-12

>>> model.log_id203(sequence)
-4.9533088776424528

   in addition to the baum-welch algorithm, viterbi training is also
   included. this training is quicker, but less exact than the baum-welch
   algorithm. it makes the id203 of a transition equal to the
   frequency of seeing that transition in the viterbi path of all the
   training sequences, and emissions to be the distribution retrained on
   all obervations tagged with that state in the viterbi path.

   model.train is a wrapper for both the viterbi and baum-welch
   algorithms, which can be specified with "algorithm='baum-welch'" or
   "algorithm='viterbi'". the baum-welch algorithm can also take
   min_iterations to do at least that any iterations of baum-welch
   training, and stop_threshold to indicate the log score improvement at
   which to stop at-- currently set at 1e-9. viterbi training takes no
   arguments.

   lastly, tied states are supported in both training algorithms. this is
   useful if many states are supposed to represent the same underlying
   distribution, which should be kept the same even upon being retrained.
   when not tied, these states may diverge slightly from each other. tying
   them both keeps them all the same, and increases the amount of training
   data each distribution gets, to hopefully get a better result.

   in order to use a tied state, simply pass the same distribution object
   into multiple states. see the following example.
# not tied states
>>> a = state( normaldistribution( 5, 2 ), name="a" )
>>> b = state( uniformdistribution( 2, 7 ), name="b" )
>>> c = state( normaldistribution( 5, 2 ), name="c" )

# a and c tied states
>>> d = normaldistribution( 5, 2 )
>>>
>>> a = state( d, name="a" )
>>> b = state( uniformdistribution( 2, 7 ), name="b" )
>>> c = state( d, name="c" )

   once you're done working with your model, you can write it out to a
   stream with model.write(), to be read back in later with model.read().
>>> model.write(sys.stdout)
examplemodel 4
302687936 examplemodel-end 1.0 none
302688008 examplemodel-start 1.0 none
302688080 normal 1.0 normaldistribution(0.281114738186, 0.022197987893)
302688152 uniform 1.0 uniformdistribution(0.258916750293, 0.75795440294)
uniform uniform 6.02182522366e-25 0.4 302688152 302688152
uniform examplemodel-end 0.333333333333 0.2 302688152 302687936
uniform normal 0.666666666667 0.4 302688152 302688080
normal uniform 1.0 0.4 302688080 302688152
normal examplemodel-end 9.71474187173e-184 0.2 302688080 302687936
normal normal 2.59561866186e-45 0.4 302688080 302688080
examplemodel-start uniform 1.0 0.5 302688008 302688152
examplemodel-start normal 0.0 0.5 302688008 302688080

   this file contains states, and then transitions. the first line is the
   name of the model and the number of states present. then, each line
   contains a single state containing a unique id, the name, the state
   weight, and the distribution that the state contains. for the start and
   end state, this value is none, as they are silent states. then, the
   remaining lines contain transitions in the model, formatted by
   from_state_name, to_state_name, id203, pseudocount,
   from_state_id, and to_state_id. the ids are unique tags generated from
   the memory address of the state, and are needed in case the user names
   two states with the same name. as an example, the first transition is
   from the state named uniform to the state named uniform with a very low
   id203, and the ids are the same meaning that it is a self loop.

   lets explore the bake method a little more. in addition to finalizing
   the internal structure of the model, it will normalize out-edge
   weights, and also merge silent states with a id203 1. edge
   between them to simplify the model. lets see this in action.
        model_a = model( "model_a" )
        s1 = state( normaldistribution( 25., 1. ), name="s1" )
        s2 = state( normaldistribution( 13., 1. ), name="s2" )

        model_a.add_state( s1 )
        model_a.add_state( s2 )
        model_a.add_transition( model.start, s1, 0.95 )
        model_a.add_transition( s1, s1, 0.40 )
        model_a.add_transition( s1, s2, 0.50 )
        model_a.add_transition( s2, s1, 0.50 )
        model_a.add_transition( s2, s2, 0.40 )
        model_a.add_transition( s1, model.end, 0.1 )
        model_a.add_transition( s2, model.end, 0.1 )

        model_b = model( "model_b" )
        s3 = state( normaldistribution( 34., 1. ), name="s3" )
        s4 = state( normaldistribution( 45., 1. ), name="s4" )

        model_b.add_state( s3 )
        model_b.add_state( s4 )
        model_b.add_transition( model.start, s3, 1.0 )
        model_b.add_transition( s3, s3, 0.50 )
        model_b.add_transition( s3, s4, 0.30 )
        model_b.add_transition( s4, s4, 0.20 )
        model_b.add_transition( s4, s3, 0.30 )
        model_b.add_transition( s4, model.end, 1.0 )

   if at this point we baked model_a and ran it, we'd get the following:
>>> sequence = [ 24.57, 23.10, 11.56, 14.3, 36.4, 33.2, 44.2, 46.7 ]
>>> model_a.bake( verbose=true )
model_a : model_a-start summed to 0.95, normalized to 1.0
>>>
>>> print model_a.forward( sequence )
[[         -inf          -inf          -inf    0.        ]
 [         -inf   -1.01138853   -3.31397363          -inf]
 [ -53.62847425   -4.6516178    -6.95420289          -inf]
 [  -7.30050351  -96.80364706   -9.60308861          -inf]
 [  -9.98073278  -66.15758923  -12.28331787          -inf]
 [-285.59596204  -76.57281849  -78.87540358          -inf]
 [-282.2049042  -112.02804776 -114.33063285          -inf]
 [-600.36013347 -298.18327702 -300.48586211          -inf]
 [-867.64036273 -535.46350629 -537.76609138          -inf]]
>>>
>>> print model_a.log_id203( sequence )
-537.766091379

   by setting verbose=true, we get a log that the out-edges from
   model.start have been normalized to 1.0. this forward log id203
   matrix is the same as if we had originally set the transition to 1.0

   if instead of the above, we concatenated the models and ran the code,
   we'd get the following:
>>> sequence = [ 24.57, 23.10, 11.56, 14.3, 36.4, 33.2, 44.2, 46.7 ]
>>> model_a.concatenate_model( model_b )
>>> model_a.bake( verbose=true )
model_a : model_a-end (silent) - model_b-start (silent) merged
model_a : model_a-start summed to 0.95, normalized to 1.0
model_a : s3 summed to 0.8, normalized to 1.0
>>>
>>> print model_a.forward( sequence )
[[         -inf          -inf          -inf          -inf          -inf
 -inf            0.]
 [         -inf   -1.01138853          -inf          -inf   -3.31397363
         -inf          -inf]
 [ -63.63791216   -4.6516178           -inf  -53.62847425   -6.95420289
         -inf          -inf]
 [-259.64994142  -96.80364706 -624.65447995   -7.30050351   -9.60308861
 -624.65447995          -inf]
 [-204.56702714  -66.15758923 -732.79470921   -9.98073278  -12.28331787
         -inf          -inf]
 [ -16.0822564   -76.57281849 -243.44679492 -285.59596204  -78.87540358
 -243.44679492          -inf]
 [ -17.79119857 -112.02804776  -87.60202419 -282.2049042  -114.33063285
  -87.60202419          -inf]
 [ -71.20014073 -298.18327702  -20.01096635 -600.36013347 -300.48586211
  -20.01096635          -inf]
 [-102.77887769 -535.46350629  -23.9843428  -867.64036273 -537.76609138
   -23.9843428          -inf]]

>>>
>>> print model_a.log_id203( sequence )
-23.9843427976

   we see both bake processing operations in effect. both model_a.start
   and s3 did not have properly summed out-edges, and needed to have them
   normalized. but now there was a useless edge between model_a.end and
   model_b.start due to the concatenate method. this allowed those two
   states to be merged, speeding up later algorithms. we can also see that
   the addition of model_b made the sequence significantly more likely
   given the model, as a sanity check that concatenate_model really did
   work.

   as said above, this module provides a few distributions over floats by
   default:

   uniformdistribution( start, end )

   normaldistribution( mean, std )

   exponentialdistribution( rate )

   gammadistribution( shape, rate ) (note that this differs from the
   parameterization used in the random module, even though the parameters
   have the same names.

   inversegammadistribution( shape, rate )

   gaussiankerneldensity( points, bandwidth, weights=none )

   uniformkerneldensity( points, bandwidth, weights=none )

   trianglekerneldensity( points, bandwidth, weights=none )

   mixturedistribution( distributions, weights=none )

   the module also provides two other distributions:

   discretedistribution( characters ) ( allows you to pass in a dictionary
   of key: id203 pairs )

   lambdadistribution( lambda_funct ) ( allows you to pass in an arbitrary
   function that returns a log id203 for a given symbol )

   to add a new distribution, with full serialization and deserialization
   support, you have to make a new class that inherits from distribution.
   that class must have:
* a class-level name attribute that is unique amoung all distributions, and
  is used for serialization.
* an __init__ method that stores all constructor arguments into
  self.parameters as a list.
* a log_id203 method that returns the log of the id203 of the
  given value under the distribution, and which reads its parameters from
  the self.parameters list. this module's log() and exp() functions can be
  used instead of the default python ones; they handle numpy arrays and
  "properly" consider the log of 0 to be negative infinity.
* a from_sample method, which takes a numpy array of samples and an optional
  numpy array of weights, and re-estimates self.parameters to maximize the
  likelihood of the samples weighted by the weights. note that weighted
  id113 can be somewhat complicated for some
  distributions (see, for example, the gammadistribution here).
* a sample method, which returns a randomly sampled value from the
  distribution.

   the easiest way to define a new distribution is to just copy-paste the
   uniformdistribution from the module and replace all its method bodies.
   any distribution you define can be easily plugged in with other
   distributions, assuming that it has the correct methods. however, if
   you write the model and give it to someone else, they might not have
   the custom distribution.

   here is an example discrete distribution over {true, false}:
>>> class bernoullidistribution(distribution):
...     name = "bernoullidistribution"
...     def __init__(self, p):
...         self.parameters = [p]
...     def log_id203(self, sample):
...         if sample:
...             return log(self.parameters[0])
...         else:
...             return log(1 - self.parameters[0])
...     def from_sample(self, items, weights=none):
...         if weights is none:
...             weights = numpy.ones_like(items, dtype=float)
...         self.parameters = [float(numpy.dot(items, weights)) / len(items)]
...     def sample(self):
...         return random.random() < self.parameters[0]
>>> bernoulli = bernoullidistribution(0.5)
>>> exp(bernoulli.log_id203(true))
0.5
>>> sample = [bernoulli.sample() for i in xrange(10)]
>>> sample
[false, true, false, true, false, false, true, false, true, false]
>>> bernoulli.from_sample(sample)
>>> bernoulli.write(sys.stdout)
bernoullidistribution(0.4)

        # test id48s

        model_a = model(name="a")
        model_b = model(name="b")

        s1 = state(uniformdistribution(0.0, 1.0), name="s1")
        s2 = state(uniformdistribution(0.5, 1.5), name="s2")
        s3 = state(uniformdistribution(-1.0, 1.0), name="s3")

        # make a simple 2-state model
        model_a.add_state(s1)
        model_a.add_state(s2)
        model_a.add_transition(s1, s1, 0.70)
        model_a.add_transition(s1, s2, 0.25)
        model_a.add_transition(s1, model_a.end, 0.05)
        model_a.add_transition(s2, s2, 0.70)
        model_a.add_transition(s2, s1, 0.25)
        model_a.add_transition(s2, model_a.end, 0.05)
        model_a.add_transition(model_a.start, s1, 0.5)
        model_a.add_transition(model_a.start, s2, 0.5)

        # make another model with that model as a component
        model_b.add_state(s3)
        model_b.add_transition(model_b.start, s3, 1.0)
        model_b.add_model(model_a)
        model_b.add_transition(s3, model_a.start, 1.0)
        model_b.add_transition(model_a.end, model_b.end, 1.0)

        model_b.bake()

        print "id48:"
        print model_b

        print "id48 serialization:"
        model_b.write(sys.stdout)

        print "a sample from the id48:"
        print model_b.sample()

        print "forward algorithm:"
        print model_b.forward([]) # impossible
        print model_b.forward([-0.5, 0.2, 0.2]) # possible
        print model_b.forward([-0.5, 0.2, 0.2 -0.5]) # impossible
        print model_b.forward([-0.5, 0.2, 1.2, 0.8]) # possible

        print "backward algorithm:"
        print model_b.backward([]) # impossible
        print model_b.backward([-0.5, 0.2, 0.2]) # possible
        print model_b.backward([-0.5, 0.2, 0.2 -0.5]) # impossible
        print model_b.backward([-0.5, 0.2, 1.2, 0.8]) # possible

        print "viterbi:"
        print model_b.viterbi([]) # impossible
        print model_b.viterbi([-0.5, 0.2, 0.2]) # possible
        print model_b.viterbi([-0.5, 0.2, 0.2 -0.5]) # impossible
        print model_b.viterbi([-0.5, 0.2, 1.2, 0.8]) # possible

        # train on some of the possible data
        print "training..."
        model_b.train([[-0.5, 0.2, 0.2], [-0.5, 0.2, 1.2, 0.8]],
                transition_pseudocount=1)
        print "id48 after training:"
        print model_b

        print "id48 serialization:"
        model_b.write(sys.stdout)

        print "probabilities after training:"
        print model_b.forward([]) # impossible
        print model_b.forward([-0.5, 0.2, 0.2]) # possible
        print model_b.forward([-0.5, 0.2, 0.2 -0.5]) # impossible
        print model_b.forward([-0.5, 0.2, 1.2, 0.8]) # possible

     *    2019 github, inc.
     * [87]terms
     * [88]privacy
     * [89]security
     * [90]status
     * [91]help

     * [92]contact github
     * [93]pricing
     * [94]api
     * [95]training
     * [96]blog
     * [97]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [98]reload to refresh your
   session. you signed out in another tab or window. [99]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/jmschrei/yaid48/commits/master.atom
   3. https://github.com/jmschrei/yaid48/#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/jmschrei/yaid48
  32. https://github.com/join
  33. https://github.com/login?return_to=/jmschrei/yaid48
  34. https://github.com/jmschrei/yaid48/watchers
  35. https://github.com/login?return_to=/jmschrei/yaid48
  36. https://github.com/jmschrei/yaid48/stargazers
  37. https://github.com/login?return_to=/jmschrei/yaid48
  38. https://github.com/jmschrei/yaid48/network/members
  39. https://github.com/jmschrei
  40. https://github.com/jmschrei/yaid48
  41. https://github.com/jmschrei/yaid48
  42. https://github.com/jmschrei/yaid48/issues
  43. https://github.com/jmschrei/yaid48/pulls
  44. https://github.com/jmschrei/yaid48/projects
  45. https://github.com/jmschrei/yaid48/wiki
  46. https://github.com/jmschrei/yaid48/pulse
  47. https://github.com/join?source=prompt-code
  48. https://github.com/jmschrei/yaid48/commits/master
  49. https://github.com/jmschrei/yaid48/branches
  50. https://github.com/jmschrei/yaid48/releases
  51. https://github.com/jmschrei/yaid48/graphs/contributors
  52. https://github.com/jmschrei/yaid48/blob/master/license.txt
  53. https://github.com/jmschrei/yaid48/search?l=python
  54. https://github.com/jmschrei/yaid48/find/master
  55. https://github.com/jmschrei/yaid48/archive/master.zip
  56. https://github.com/login?return_to=https://github.com/jmschrei/yaid48/
  57. https://github.com/join?return_to=/jmschrei/yaid48
  58. https://desktop.github.com/
  59. https://desktop.github.com/
  60. https://developer.apple.com/xcode/
  61. https://visualstudio.github.com/
  62. https://github.com/jmschrei
  63. https://github.com/jmschrei/yaid48/commits?author=jmschrei
  64. https://github.com/jmschrei/yaid48/commit/f363f5909bc773c6e2c9e22b77b7b7a2b49af23b
  65. https://github.com/jmschrei/yaid48/commit/f363f5909bc773c6e2c9e22b77b7b7a2b49af23b
  66. https://github.com/jmschrei/yaid48/tree/f363f5909bc773c6e2c9e22b77b7b7a2b49af23b
  67. https://github.com/jmschrei/yaid48/tree/master/examples
  68. https://github.com/jmschrei/yaid48/tree/master/tests
  69. https://github.com/jmschrei/yaid48/commit/d98f051360887b3ec917c98e094585b2eb8bacb7
  70. https://github.com/jmschrei/yaid48/tree/master/yaid48
  71. https://github.com/jmschrei/yaid48/commit/4030af930e6e2b57abbf362e05bbccf0c6e11feb
  72. https://github.com/jmschrei/yaid48/blob/master/.gitignore
  73. https://github.com/jmschrei/yaid48/blob/master/.travis.yml
  74. https://github.com/jmschrei/yaid48/commit/34f6de89a0559bcd84b85542cfdb3e84b79bf745
  75. https://github.com/jmschrei/yaid48/blob/master/changes.txt
  76. https://github.com/jmschrei/yaid48/blob/master/license.txt
  77. https://github.com/jmschrei/yaid48/blob/master/manifest
  78. https://github.com/jmschrei/yaid48/blob/master/manifest.in
  79. https://github.com/jmschrei/yaid48/blob/master/readme.md
  80. https://github.com/jmschrei/yaid48/blob/master/requirements.txt
  81. https://github.com/jmschrei/yaid48/blob/master/setup.py
  82. https://travis-ci.org/jmschrei/yaid48
  83. https://github.com/jmschrei/pomegranate
  84. https://github.com/nose-devs/nose
  85. https://github.com/jmschrei/yaid48/wiki
  86. http://nbviewer.ipython.org/github/jmschrei/yaid48/tree/master/examples/
  87. https://github.com/site/terms
  88. https://github.com/site/privacy
  89. https://github.com/security
  90. https://githubstatus.com/
  91. https://help.github.com/
  92. https://github.com/contact
  93. https://github.com/pricing
  94. https://developer.github.com/
  95. https://training.github.com/
  96. https://github.blog/
  97. https://github.com/about
  98. https://github.com/jmschrei/yaid48/
  99. https://github.com/jmschrei/yaid48/

   hidden links:
 101. https://github.com/
 102. https://github.com/jmschrei/yaid48/
 103. https://github.com/jmschrei/yaid48/
 104. https://github.com/jmschrei/yaid48/
 105. https://help.github.com/articles/which-remote-url-should-i-use
 106. https://github.com/jmschrei/yaid48/#yaid48
 107. https://github.com/jmschrei/yaid48/#installation
 108. https://github.com/jmschrei/yaid48/#contributing
 109. https://github.com/jmschrei/yaid48/#documentation
 110. https://github.com/jmschrei/yaid48/#tutorial
 111. https://github.com/
