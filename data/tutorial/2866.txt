   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    deep
   learning research review: id23 comments feed
   [5]neighbors know best: (re) classifying an underappreciated beer
   [6]continuous improvement for iot through ai / continuous learning

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2016    [28]nov    [29]tutorials,
   overviews    deep learning research review: id23
   ( [30]16:n42 )

deep learning research review: id23

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 181
   tags: [33]deep learning, [34]machine learning, [35]reinforcement
   learning

   this edition of deep learning research review explains recent research
   papers in id23 (rl). if you don't have the time to
   read the top papers yourself, or need an overview of rl in general,
   this post has you covered.
     __________________________________________________________________

   by [36]adit deshpande, ucla.

   this is the 2nd installment of a new series called deep learning
   research review. every couple weeks or so, i   ll be summarizing and
   explaining research papers in specific subfields of deep learning. this
   week focuses on id23.

   [37]last time was id3 icymi

introduction to id23


   3 categories of machine learning

   before getting into the papers, let   s first talk about
   what id23 is. the field of machine learning can be
   separated into 3 main categories.
    1. supervised learning
    2. unsupervised learning
    3. id23

   the first category, supervised learning, is the one you may be most
   familiar with. it relies on the idea of creating a function or model
   based on a set of training data, which contains inputs and their
   corresponding labels. convolutional neural networks are a great example
   of this, as the images are the inputs and the outputs are the
   classifications of the images (dog, cat, etc).

   unsupervised learning seeks to find some sort of structure within data
   through methods of cluster analysis. one of the most well-known ml
   id91 algorithms, id116, is an example of unsupervised learning.

   id23 is the task of learning what actions to take,
   given a certain situation/environment, so as to maximize a reward
   signal. the interesting difference between supervised and reinforcement
   learning is that this reward signal simply tells you whether the action
   (or input) that the agent takes is good or bad. it doesn   t tell you
   anything about what the best action is. contrast this to id98s where the
   corresponding label for each image input is a definite instruction of
   what the output should be for each input.  another unique component of
   rl is that an agent   s actions will affect the subsequent data it
   receives. for example, an agent   s action of moving left instead of
   right means that the agent will receive different input from the
   environment at the next time step. let   s look at an example to start
   off.

   the rl problem

   so, let   s first think about what have in a id23
   problem. let   s imagine a tiny robot in a small room. we haven   t
   programmed this robot to move or walk or take any action. it   s just
   standing there. this robot is our agent.

   like we mentioned before, id23 is all about trying to
   understand the optimal way of making decisions/actions so that we
   maximize some reward r. this reward is a feedback signal that just
   indicates how well the agent is doing at a given time step. the action
   a that an agent takes at every time step is a function of both the
   reward (signal telling the agent how well it   s currently doing) and
   the state s, which is a description of the environment the agent is in.
   the mapping from environment states to actions is called our policy p.
   the policy basically defines the agent   s way of behaving at a certain
   time, given a certain situation. now, we also have a value function
   v which is a measure of how good each position is. this is different
   from the reward in that the reward signal indicates what is good in the
   immediate sense, while the value function is more indicative of how
   good it is to be in this state/position in the long run. finally, we
   also have a model m which is the agent   s representation of the
   environment. this is the agent   s model of how it thinks that the
   environment is going to behave.

   markov decision process

   so, let   s now think back to our robot (the agent) in the small room.
   our reward function is dependent on what we want the agent to
   accomplish. let   s say that we want it to move to one of the corners of
   the room where it will receive a reward. the robot will get a +25 when
   it reaches this point, and will get a -1 for every time step it takes
   to get there. we basically want the robot to get the corner as fast as
   possible. the actions the agent can take are moving north, south, east,
   or west. the agent   s policy can be a simple one, where the behavior is
   that the agent will always move to the location with the higher value
   function. makes sense right? a position with a high value function =
   good to be in this position (with regards to long term reward).

   now, this whole rl environment can be described with a markov decision
   process. for those that haven   t heard the term before, an mdp is a
   framework for modeling an agent   s decision making. it contains a finite
   set of states (and value functions for those states), a finite set of
   actions, a policy, and a reward function. our value function can be
   split into 2 terms.
    1. state-value function v: the expected return from being in a state s
       and following a policy   . this return is calculated by looking at
       summation of the reward at each future time step (the gamma refers
       to a constant discount factor, which means that the reward at time
       step 10 is weighted a little less than the reward at time step 1).

    2. action-value function q: the expected return from being in a state
       s, following a policy   , and taking an action a (equation will be
       same as above except that we have an additional condition that at =
       a).

   now that we have all the components, what do we do with this mdp? well,
   we want to solve it, of course. by solving an mdp, you   ll be able to
   find the optimal behavior (policy) that maximizes the amount of reward
   the agent can expect to get from any state in the environment.

   solving the mdp

   we can solve an mdp and get the optimum policy through the use of
   id145 and specifically through the use of policy
   iteration (there is another technique called value iteration, but won   t
   go into that right now). the idea is that we take some initial policy
     1 and evaluate the state value function for that policy. the way we do
   this is through the bellman expectation equation.

   this equation basically says that our value function, given that we   re
   following policy   , can be decomposed into the expected return sum of
   the immediate reward rt+1 and the value function of the successor state
   st+1. if you think about it closely, this is equivalent to the value
   function definition we used in the previous section. using this
   equation is our policy evaluationcomponent. in order to get a better
   policy, we use a policy improvement step where we simply act greedily
   with respect to the value function. in other words, the agent takes the
   action that maximizes value.

   now, in order to get the optimal policy, we repeat these 2 steps, one
   after the other, until we converge to optimal policy   *.

   when you   re not given an mdp

   policy iteration is great and all, but it only works when we have a
   given mdp. the mdp essentially tells you how the environment works,
   which realistically is not going to be given in real world scenarios.
   when not given an mdp, we use model free methods that go directly from
   the experience/interactions of the agents and the environment to the
   value functions and policies. we   re going to be doing the same steps of
   policy evaluation and policy improvement, just without the information
   given by the mdp.

   the way we do this is instead of improving our policy by optimizing
   over the state value function, we   re going to optimize over the action
   value function q. remember how we decomposed the state value function
   into the sum of immediate reward and value function of the successor
   state? well, we can do the same with our q function.

   now, we   re going to go through the same process of policy evaluation
   and policy improvement, except we replace our state value function v
   with our action value function q. now, i   m going to skip over the
   details of what changes with the evaluation/improvement steps. to
   understand mdp free evaluation and improvement methods, topics such as
   monte carlo learning, temporal difference learning, and sarsa would
   require whole blogs just themselves (if you are interested, though,
   please take a listen to david silver   s [38]lecture 4 and [39]lecture
   5). right now, however, i   m going to jump ahead to value function
   approximation and the methods discussed in the alphago and atari
   papers, and hopefully that should give a taste of modern rl
   techniques. the main takeaway is that we want to find the optimal
   policy   * that maximizes our action value function q.

   value function approximation

   so, if you think about everything we   ve learned up until this point,
   we   ve treated our problem in a relatively simplistic way. look at the
   above q equation. we   re taking in a specific state s and action a, and
   then computing a number that basically tells us what the expected
   return is. now let   s imagine that our agent moves 1 millimeter to the
   right. this means we have a whole new state s   , and now we   re going to
   have to compute a q value for that. in real world rl problems, there
   are millions and millions of states so it   s important that our value
   functions understand generalization in that we don   t have to store a
   completely separate value function for every possible state. the
   solution is to use a q value function approximation that is able to
   generalize to unknown states.

   so, what we want is some function, let   s call is qhat, that gives a
   rough approximation of the q value given some state s and some action
   a.

   this function is going to take in s, a, and a good old weight vector w
   (once you see that w, you already know we   re bringing in some gradient
   descent ). it is going to compute the dot product between x (which is
   just a feature vector that represents s and a) and w. the way we   re
   going to improve this function is by calculating the loss between the
   true q value (let   s just assume that it   s given to us for now) and the
   output of the approximate function.

   after we compute the loss, we use id119 to find the minimum
   value, at which point we will have our optimal w vector. this idea of
   function approximation is going to be very key when taking a look at
   the papers a little later.

   just one more thing

   before getting to the papers, just wanted to touch on one last thing.
   an interesting discussion with the topic of id23 is
   that of exploration vs exploitation. exploitation is the agent   s
   process of taking what it already knows, and then making the actions
   that it knows will produce the maximum reward. this sounds great,
   right? the agent will always be making the best action based on its
   current knowledge. however, there is a key phrase in that
   statement. current knowledge. if the agent hasn   t explored enough of
   the state space, it can   t possibly know whether it is really taking the
   best possible action. this idea of taking actions with the main purpose
   of exploring the state space is called exploration.

   this idea can be easily related to a real world example. let   s say you
   have a choice of what restaurant to eat at tonight. you (acting as the
   agent) know that you like mexican food, so in rl terms, going to a
   mexican restaurant will be the action that maximizes your reward, or
   happiness/satisfaction in this case. however, there is also a choice of
   italian food, which you   ve never had before. there   s a possibility that
   it could be better than mexican food, or could be a lot worse. this
   tradeoff between whether to exploit an agent   s past knowledge vs trying
   something new in hope of discovering a greater reward is one of the
   major challenges in id23 (and in our daily lives
   tbh).

   other resources for learning rl

   phew. that was a lot of info. by no means, however, was that a
   comprehensive overview of the field. if you   d like a more in-depth
   overview of rl, i   d strongly recommend these resources.
     * david silver (from deepmind) id23 [40]video
       lectures
          + my [41]personal notes from the rl course
     * sutton and barto   s [42]id23 textbook (this is
       really the holy grail if you are determined to learn the ins and
       outs of this subfield)
     * andrej karpathy   s [43]blog post on rl (start with this one if you
       want to ease into rl and want to see a really well done practical
       example)
     * [44]uc berkeley cs 188 lectures 8-11
     * [45]open ai gym: when you feel comfortable with rl, try creating
       your own agents with this id23 toolkit that open
       ai created

   pages: 1 [46]2
     __________________________________________________________________

   [47][prv.gif] previous post
   [48]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [49]another 10 free must-read books for machine learning and data
       science
    2. [50]9 must-have skills you need to become a data scientist, updated
    3. [51]who is a typical data scientist in 2019?
    4. [52]the pareto principle for data scientists
    5. [53]what no one will tell you about data science job applications
    6. [54]19 inspiring women in ai, big data, data science, machine
       learning
    7. [55]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [56]id158s optimization using genetic algorithm
       with python
    2. [57]who is a typical data scientist in 2019?
    3. [58]8 reasons why you should get a microsoft azure certification
    4. [59]the pareto principle for data scientists
    5. [60]r vs python for data visualization
    6. [61]how to work in data science, ai, big data
    7. [62]the deep learning toolset     an overview

[63]latest news

     * [64]download your datax guide to ai in marketing
     * [65]kdnuggets offer: save 20% on strata in london
     * [66]training a champion: building deep neural nets for big ...
     * [67]building a recommender system
     * [68]predict age and gender using convolutional neural netwo...
     * [69]top tweets, mar 27     apr 02: here is a great ex...

more recent stories

     * [70]top tweets, mar 27     apr 02: here is a great explanat...
     * [71]odsc east is selling out; odsc india announced
     * [72]accelerate ai and data science career expo, may 4, 2019
     * [73]grow your data career at datasciencego, san diego, sep 27-29
     * [74]getting started with nlp using the pytorch framework
     * [75]how to diy your data science education
     * [76]top 8 data science use cases in gaming
     * [77]kdnuggets 19:n13, apr 3: top 10 data scientist coding mista...
     * [78]make better data-driven business decisions
     * [79]top stories, mar 25-31: r vs python for data visualization;
       th...
     * [80]two predictive analytics world events in europe this fall
     * [81]7 qualities your big data visualization tools absolutely must
       ...
     * [82]yeshiva university: tenure-track faculty in ai and machine
       lea...
     * [83]which face is real?
     * [84]yeshiva university: program director / tenure track faculty
       me...
     * [85]top 10 coding mistakes made by data scientists
     * [86]uber   s case study at paw industry 4.0: machine learning ...
     * [87]xai     a data scientist   s mouthpiece
     * [88]what does gpt-2 think about the ai arms race?
     * [89]openclassrooms: data freelance online course creator
       [telecomm...

   [90]kdnuggets home    [91]news    [92]2016    [93]nov    [94]tutorials,
   overviews    deep learning research review: id23
   ( [95]16:n42 )
      2019 kdnuggets. [96]about kdnuggets.  [97]privacy policy. [98]terms
   of service

   [99]subscribe to kdnuggets news
   [100][tw_c48.png] [101]facebook [102]linkedin
   x
   [envelope.png] [103]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2016/11/deep-learning-research-review-reinforcement-learning.html/feed
   5. https://www.kdnuggets.com/2016/11/neighbors-know-best-classifying-underappreciated-beer.html
   6. https://www.kdnuggets.com/2016/11/continuous-improvement-iot-ai-learning.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2016/index.html
  28. https://www.kdnuggets.com/2016/11/index.html
  29. https://www.kdnuggets.com/2016/11/tutorials.html
  30. https://www.kdnuggets.com/2016/n42.html
  31. https://www.kdnuggets.com/2016/11/neighbors-know-best-classifying-underappreciated-beer.html
  32. https://www.kdnuggets.com/2016/11/continuous-improvement-iot-ai-learning.html
  33. https://www.kdnuggets.com/tag/deep-learning
  34. https://www.kdnuggets.com/tag/machine-learning
  35. https://www.kdnuggets.com/tag/reinforcement-learning
  36. https://www.kdnuggets.com/author/adit-deshpande
  37. https://adeshpande3.github.io/adeshpande3.github.io/deep-learning-research-review-week-1-generative-adversarial-nets
  38. https://www.youtube.com/watch?v=pnhcvfgc_za
  39. https://www.youtube.com/watch?v=0g4j2k_ggc4
  40. https://www.youtube.com/watch?v=2pwv7govuf0&list=pl7-jpktc4r78-wczcqn5iqyuwhbz8foxt
  41. https://docs.google.com/document/d/1tjmydoxqzoq0jd0luifovq1hnamtwkfsaacck9dr7a8/edit?usp=sharing
  42. https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf
  43. http://karpathy.github.io/2016/05/31/rl/
  44. http://ai.berkeley.edu/lecture_videos.html
  45. https://gym.openai.com/
  46. https://www.kdnuggets.com/2016/11/deep-learning-research-review-reinforcement-learning.html/2
  47. https://www.kdnuggets.com/2016/11/neighbors-know-best-classifying-underappreciated-beer.html
  48. https://www.kdnuggets.com/2016/11/continuous-improvement-iot-ai-learning.html
  49. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  50. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  51. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  52. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  53. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  54. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  55. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  56. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  57. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  58. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  59. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  60. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  61. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  62. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  63. https://www.kdnuggets.com/news/index.html
  64. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  65. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  66. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  67. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  68. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  69. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  70. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  71. https://www.kdnuggets.com/2019/04/odsc-east-selling-out-india-announced.html
  72. https://www.kdnuggets.com/jobs/19/04-03-accelerate-ai-data-science-career-expo-2019.html
  73. https://www.kdnuggets.com/2019/04/formulated-data-career-datasciencego-san-diego.html
  74. https://www.kdnuggets.com/2019/04/nlp-pytorch.html
  75. https://www.kdnuggets.com/2019/04/diy-your-data-science-education.html
  76. https://www.kdnuggets.com/2019/04/top-8-data-science-use-cases-gaming.html
  77. https://www.kdnuggets.com/2019/n13.html
  78. https://www.kdnuggets.com/2019/04/psu-make-better-data-driven-business-decisions.html
  79. https://www.kdnuggets.com/2019/04/top-news-week-0325-0331.html
  80. https://www.kdnuggets.com/2019/04/paw-two-predictive-analytics-world-events-europe-fall.html
  81. https://www.kdnuggets.com/2019/04/7-qualities-big-data-visualization-tools.html
  82. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-faculty-ai-machine-learning.html
  83. https://www.kdnuggets.com/2019/04/which-face-real-stylegan.html
  84. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-program-director-faculty-artificial-intelligence-machine-learning.html
  85. https://www.kdnuggets.com/2019/04/top-10-coding-mistakes-data-scientists.html
  86. https://www.kdnuggets.com/2019/04/paw-uber-case-study-machine-learning-enforce-mobile-performance.html
  87. https://www.kdnuggets.com/2019/04/xai-data-scientist.html
  88. https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html
  89. https://www.kdnuggets.com/jobs/19/04-01-openclassrooms-data-freelance-online-course-creator-b.html
  90. https://www.kdnuggets.com/
  91. https://www.kdnuggets.com/news/index.html
  92. https://www.kdnuggets.com/2016/index.html
  93. https://www.kdnuggets.com/2016/11/index.html
  94. https://www.kdnuggets.com/2016/11/tutorials.html
  95. https://www.kdnuggets.com/2016/n42.html
  96. https://www.kdnuggets.com/about/index.html
  97. https://www.kdnuggets.com/news/privacy-policy.html
  98. https://www.kdnuggets.com/terms-of-service.html
  99. https://www.kdnuggets.com/news/subscribe.html
 100. https://twitter.com/kdnuggets
 101. https://facebook.com/kdnuggets
 102. https://www.linkedin.com/groups/54257
 103. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
 105. https://www.kdnuggets.com/
 106. https://www.kdnuggets.com/
