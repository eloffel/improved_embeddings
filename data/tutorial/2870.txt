   #[1]the morning paper    feed [2]the morning paper    comments feed
   [3]the morning paper    recurrent neural network models comments feed
   [4]convolution neural networks, part 3 [5]a miscellany of fun deep
   learning papers [6]alternate [7]alternate [8]the morning paper
   [9]wordpress.com

   [10]skip to content

   [11]the morning paper

an interesting/influential/important paper from the world of cs every weekday
morning, as selected by adrian colyer

     * [12]home
     * [13]about
     * [14]infoq qr editions
     * [15]subscribe
     * [16]privacy

recurrent neural network models

   march 23, 2017
   tags: [17]deep learning

   today we   re pressing on with the [18]top 100 awesome deep learning
   papers list, and the section on recurrent neural networks (id56s). this
   contains only four papers (joy!), and even better we   ve covered two of
   them previously (id63s and memory networks, the links
   below are to the write-ups). that leaves up with only two papers to
   cover today, however the first paper does run to 43 pages and it   s a
   lot of fun so i   m glad to be able to devote a little more space to it.
     * [19]generating sequences with recurrent neural networks, graves
       2013
     * [20]id49 as recurrent neural networks, zheng
       and jayasumana, 2015
     * [21]id63s
     * [22]memory networks

   these papers are easier to understand with some background in id56s and
   lstms. christopher olah has a wonderful post on    [23]understanding lstm
   networks    which i highly recommend.

generating sequences with recurrent neural networks

   this paper explores the use of id56s, in particular, lstms, for
   generating sequences. it looks at sequences over discrete domains
   (characters and words), generating synthetic wikipedia entries, and
   sequences over real-valued domains, generating handwriting samples. i
   especially like the moment where graves demonstrates that the trained
   networks can be used to    clean up    your handwriting, showing what a
   slightly neater / easier to read version of your handwriting could look
   like. we   ll get to that shortly   

     id56s can be trained for sequence generation by processing real data
     sequences one step at a time and predicting what comes next.
     assuming the predictions are probabilistic, novel sequences can be
     generated from a trained network by iteratively sampling from the
     network   s output distribution, then feeding in the sample as input
     at the next step. in other words by making the network treat its
     inventions as if they were real, much like a person dreaming.

   using lstms effectively gives the network a longer memory, enabling it
   to look back further in history to formulate its predictions.

   the basic id56 architecture used for all the models in the paper looks
   like this:

   note how each output vector y_t is used to parameterise a predictive
   distribution pr(x_{t+1} | y_t) over the next possible inputs (the
   dashed lines in the above figure). also note the use of    skip
   connections    as we looked at in [24]yesterday   s post.

   the lstm cells used in the network look like this:

   they are trained with the full gradient using id26. to
   prevent the derivatives becoming too large, the derivative of the loss
   with respect to the inputs to the lstm layers are clipped to lie within
   a predefined range.

   onto the experiments   

text prediction

   for text prediction we can either use sequences of words, or sequences
   of characters. with one-hot encodings, the number of different classes
   for words makes for very large input vectors (e.g. a vocabulary with
   10   s of thousands of words of more). in contrast, the number of
   characters is much more limited. also,

         predicting one character at a time is more interesting from the
     perspective of sequence generation, because it allows the network to
     invent novel words and strings. in general, the experiments in this
     paper aim to predict at the finest granularity found in the data, so
     as to maximise the generative flexibility of the network.

   the id32 dataset is a selection of wall street journal
   articles. it   s relatively small at just over a million words in total,
   but widely used as a language modelling benchmark. both word and
   character level networks were trained on this corpus using a single
   hidden layer with 1000 lstm units. both networks are capable of
   overfitting the training data, so regularisation is applied. two forms
   of regularisation were experimented with: weight noise applied at the
   start of each training sequence, and adaptive weight noise, where the
   variance of the noise is learned along with the weights.
   the word-level id56 performed better than the character-based one, but
   the gap closes with regularisation (perplexity of 117 in the best
   word-based configuration, vs 122 for the best character-based
   configuration).

      perplexity can be considered to be a measure of on average how many
   different equally most probable words can follow any given word. lower
   perplexities represent better language models       ([25][][]
   http://www1.icsi.berkeley.edu/speech/docs/htkbook3.2/node188_mn.html )

   much more interesting is a network that graves trains on the first 96m
   bytes of the wikipedia corpus (as of march 3rd 2006, captured for the
   hutter prize competition). this has seven hidden layers of 700 lstm
   cells each. this is an extract of the real wikipedia data:

   and here   s a sample generated by the network (for additional samples,
   see the full paper):

     the sample shows that the network has learned a lot of structure
     from the data, at a wide range of different scales. most obviously,
     it has learned a large vocabulary of dictionary words, along with a
     subword model that enables it to invent feasible-looking words and
     names: for example    lochroom river   ,    mughal ralvaldens   ,
        submandration   ,    swalloped   . it has also learned basic punctuation,
     with commas, full stops and paragraph breaks occurring at roughly
     the right rhythm in the text blocks.

   it can correctly open and close quotation marks and parentheses,
   indicating the models memory and these often span a distance that a
   short-range context cannot handle. likewise, it can generate distinct
   large-scale regions such as xml headers, bullet-point lists, and
   article text.

   of course, the actual generated articles don   t make any sense to a
   human reader, it is just their structure that is mimicked. when we move
   onto handwriting though, the outputs do make a lot of sense to us   

handwriting prediction

     to test whether the prediction network could also be used to
     generate convincing real-valued sequences, we applied it to online
     handwriting data (online in this context means that the writing is
     recorded as a sequence of pen-tip locations, as opposed to offline
     handwriting, where only the page images are available). online
     handwriting is an attractive choice for sequence generation due to
     its low dimensionality (two real numbers per data point) and ease of
     visualisation.

   the dataset consists of handwritten lines on a smart whiteboard, with
   x,y co-ordinates and end-of-stroke markers (yes/no) captured at each
   time point. the main challenge was figuring out how to determine a
   predictive distribution for real-value inputs. the solution is to use
   mixture density neworks. here the outputs of the network are used to
   parameterise a [26]mixture distribution. each output vector consists of
   the end of stroke id203 e, along with a set of means, standard
   deviations, correlations, and mixture weights for the mixture
   components used to predict the x and y positions. see pages 20 and 21
   for the detailed explanation.

   here are the mixture density outputs for predicted locations as the
   word under is written. the small blobs show accurate predictions while
   individual strokes are being written, and the large blobs show greater
   uncertainty at the end of strokes when the pen is lifted from the
   whiteboard.

   the best samples were generated by a network with three hidden layers
   of 400 lstm cells each, and 20 mixture components to model the offsets.
   here are some samples created by the network.

     the network has clearly learned to model strokes, letters and even
     short words (especially common ones such as    of    and    the   ). it also
     appears to have learned a basic id186,
     since the words it invents (   eald   ,    bryoes   ,    lenrest   ) look
     somewhat plausible in english. given that the average character
     occupies more than 25 timesteps, this again demonstrates the
     network   s ability to generate coherent long-range structures

handwriting generation

   those samples do of course look like handwriting, but as with our
   wikipedia example, the actual words are nonsense. can we learn to
   generated handwriting for a given text? to meet this challenge a soft
   window is convolved with the text string and fed as an extra input to
   the prediction network.

     the parameters of the window are output by the network at the same
     time as it makes the predictions, so that it dynamically determines
     an alignment between the text and the pen locations. put simply, it
     learns to decide which character to write next.

   the network learns how far to slide the text window at each step,
   rather than learning an absolute position.    using offsets was essential
   to getting the network to align the text with the pen trace.   

   and here are samples generated by the resulting network:

   pretty good!

biased and primed sampling to control generation

     one problem with unbiased samples is that they tend to be difficult
     to read (partly because real handwriting is difficult to read, and
     partly because the network is an imperfect model). intuitively, we
     would expect the network to give higher id203 to good
     handwriting because it tends to be smoother and more predictable
     than bad handwriting. if this is true, we should aim to output more
     probable elements of pr(x|c) if we want the samples to be easier to
     read. a principled search for high id203 samples could lead to
     a difficult id136 problem, as the id203 of every output
     depends on all previous outputs. however a simple heuristic, where
     the sampler is biased towards more probable predictions at each step
     independently, generally gives good results.

   as we increase the bias towards higher id203 predictions, the
   handwriting gets neater and neater   

   as a final flourish, we can prime the network with a real sequence in
   the handwriting of a particular writer. the network then continues in
   this style, generating handwriting mimicking the author   s style.

   combine this with bias, and you also get neater versions of their
   handwriting!

id49 as recurrent neural networks

   now we turn our attention to a new challenge problem that we haven   t
   looked at yet: semantic segmentation. this requires us to label the
   pixels in an image to indicate what kind of object they represent/are
   part of (land, building, sky, bicycle, chair, person, and so on   ). by
   joining together regions with the same label, we segment the image
   based on the meaning of the pixels. like this:

   (the crf-id56 column in the above figure shows the results from the
   network architecture described in this paper).

   as we   ve seen, id98s have been very successful in image classification
   and detection, but there are challenges applying them to
   pixel-labelling problems. firstly, traditional id98s don   t produce
   fine-grained enough outputs to label every pixel. but perhaps more
   significantly, even if we could overcome that hurdle, they don   t have
   any way of understanding that if pixel a is part of, say, a bicycle,
   then it   s likely that the adjacent pixel b is also part of a bicycle.
   or in more fancy words:

     id98s lack smoothness constraints that encourage label agreement
     between similar pixels, and spatial and appearance consistency of
     the labelling output. lack of such smoothness constraints can result
     in poor object delineation and small spurious regions in the
     segmentation output.

   [27]id49 (a variant of markov random fields) are
   very good at smoothing. they   re basically models that take into account
   surrounding context when making predictions. so maybe we can combine
   id49 (crf) and id98s in some way to get the best of
   both worlds?

     the key idea of crf id136 for semantic labelling is to formulate
     the label assignment problem as a probabilistic id136 problem
     that incorporates assumptions such as the label agreement between
     similar pixels. crf id136 is able to refine weak and coarse
     pixel-level label predictions to produce sharp boundaries and
     fine-grained segmentations. therefore, intuitively, crfs can be used
     to overcome the drawbacks in utilizing id98s for pixel-level
     labelling tasks.

     sounds good in theory, but it   s quite tricky in practice. the
     authors proceed in two stages: firstly showing that one iteration of
     the mean-field algorithm used in crf can be modelled as a stack of
     common id98 layers; and secondly by showing that repeating the
     crf-id98 stack with outputs from the previous iteration fed back into
     the next iteration you can end up with an id56 structure, dubbed
     crf-id56, that implements the full algorithm.

     our approach comprises a fully convolutional network stage, which
     predicts pixel-level labels without considering structure, followed
     by a crf-id56 stage, which performs crf-based probabilistic graphical
     modelling for id170. the complete system, therefore,
     unifies the strengths of both id98s and crfs and is trainable
     end-to-end using the back-propagation algorithm and the stochastic
     id119 (sgd) procedure.

   there   s a lot of detail in the paper, some of which passes straight
   over my head, for example, the following sentence which warranted me
   bringing out the    hot pink    highlighter:

     in terms of permutohedral lattice operations, this can be
     accomplished by only reversing the order of the separable filters in
     the blur stage, while building the permutohedral lattice, splatting,
     and slicing in the same way as in the forward pass.

   (what is a permutohedron you may ask? it   s actually [28]not as scary as
   it sounds   )

   fortunately, we   re just trying to grok the big picture in this
   write-up, and for that the key is to understand how id98s can model one
   mean-field iteration, and then how we stack the resulting structures in
   id56 formation.

mean-field iteration as a stack of id98 layers

   consider a vector x with one element per pixel, representing the label
   assigned to that pixel drawn from some pre-defined set of labels. we
   construct a graph where the vertices are the elements in x, and edges
   between the elements hold pairwise    energy    values. minimising the
   overall energy of the configuration yields the most probable label
   assignments. energy has two components: a unary component which depends
   only on the individual pixel and roughly speaking, predicts labels for
   pixels without considering smoothness and consistency; and pairwise
   energies that provide an image data-dependent smoothing term that
   encourages assigning similar labels to pixels with similar properties.
   the energy calculations are based on feature vectors derived from image
   features. mean-field iteration is used to find an approximate solution
   for the minimal energy configuration.

   the steps involved in a single iteration are:
     * message passing,
     * re-weighting,
     * compatibility transformation,
     * unary addition, and
     * normalisation

   message passing is made tractable by using approximation techniques
   (those permutohedral lattice thingies) and two guassian kernels: a
   spatial kernel and a bilateral kernel. re-weighting can be implemented
   as a 1  1 convolution. each kernel is given independent weights:

     the intuition is that the relative importance of the spatial kernel
     vs the bilateral kernel depends on the visual class. for example,
     bilateral kernels may have on the one hand a high importance in
     bicycle detection, because similarity of colours is determinant; on
     the other hand they may have low importance for tv detection, given
     that whatever is inside the tv screen may have different colours.

   compatibility transformation assigns penalties when different labels
   are assigned to pixels with similar properties. it is implemented with
   a convolutional filter with learned weights (equivalent to learning a
   label compatibility function).

   the addition (copying) and normalisation (softmax) operations are easy.

crf as a stack of crf-id98 layers

     multiple mean-field iterations can be implemented by repeating the
     above stack of layers in such a way that each iteration takes q
     value estimates from the previous iteration and the unary values in
     their original form. this is equivalent to treating the iterative
     mean-field id136 as a recurrent neural network (id56)    we name
     this id56 structure crf-id56.

   recall that the overall network has a fully-convolution network stage
   predicting pixels labels in isolation, followed by a crf-id98 for
   id170. in one forward pass the computation goes through
   the initial id98 stage, and then it takes t iterations for data to leave
   the loop created by the id56. one the data leaves this loop, a softmax
   loss layer directly follows and terminates the network.

   the resulting network achieves the state-of-the-art on the pascal voc
   2010-2012 test datasets.

share this:

     * [29]twitter
     * [30]linkedin
     * [31]email
     * [32]print
     *

like this:

   like loading...

related

   from     [33]machine learning, [34]uncategorized
   [35]    convolution neural networks, part 3
   [36]a miscellany of fun deep learning papers    
   2 comments [37]leave one    
    1. [38]ziaul kamal [39]permalink
       august 21, 2017 4:02 pm
       [40]nice info
       [41]reply

trackbacks

    1. [42]sentiment classification about finance statements | ssix -
       social sentiment indices powered by x-scores

leave a reply [43]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [44]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [45]log out /
   [46]change )
   google photo

   you are commenting using your google account. ( [47]log out /
   [48]change )
   twitter picture

   you are commenting using your twitter account. ( [49]log out /
   [50]change )
   facebook photo

   you are commenting using your facebook account. ( [51]log out /
   [52]change )
   [53]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

   this site uses akismet to reduce spam. [54]learn how your comment data
   is processed.
     * subscribe

                        [55][mail-new-icon.png?w=600]
      never miss an issue! the morning paper delivered straight to your
                                   inbox.
     * search
       ____________________
     * archives archives [select month________]
     * most read in the last few days
          + [56]ginseng: keeping secrets in registers when you distrust
            the operating system
          + [57]amazon aurora: design considerations for high throughput
            cloud-native id208
          + [58]establishing software root of trust unconditionally
          + [59]amazon aurora: on avoiding distributed consensus for i/os,
            commits, and membership changes
          + [60]calvin: fast distributed transactions for partitioned
            database systems
          + [61]the amazing power of word vectors
          + [62]on the criteria to be used in decomposing systems into
            modules
          + [63]neural ordinary differential equations
          + [64]applying the universal scalability law to organisations
          + [65]programming paradigms for dummies: what every programmer
            should know
     * rss
          + [66]rss - posts
          + [67]rss - comments
     * live on twitter[68]my tweets

   [69]blog at wordpress.com.

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [70]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [71]likes-master

   %d bloggers like this:

references

   visible links
   1. https://blog.acolyer.org/feed/
   2. https://blog.acolyer.org/comments/feed/
   3. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/feed/
   4. https://blog.acolyer.org/2017/03/22/convolution-neural-networks-part-3/
   5. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/&for=wpcom-auto-discovery
   8. https://blog.acolyer.org/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/#content
  11. https://blog.acolyer.org/
  12. https://blog.acolyer.org/
  13. https://blog.acolyer.org/about/
  14. https://blog.acolyer.org/infoq-quarterly-review-editions/
  15. https://blog.acolyer.org/email-subs/
  16. https://blog.acolyer.org/privacy/
  17. https://blog.acolyer.org/tag/deep-learning/
  18. https://github.com/terryum/awesome-deep-learning-papers
  19. https://arxiv.org/pdf/1308.0850
  20. http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/zheng_conditional_random_fields_iccv_2015_paper.pdf
  21. https://blog.acolyer.org/2016/03/09/neural-turing-machines/
  22. https://blog.acolyer.org/2016/03/10/memory-networks/
  23. http://colah.github.io/posts/2015-08-understanding-lstms/
  24. https://blog.acolyer.org/2017/03/22/convolution-neural-networks-part-3
  25. http://www1.icsi.berkeley.edu/speech/docs/htkbook3.2/node188_mn.html
  26. https://en.m.wikipedia.org/wiki/mixture_distribution
  27. http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/
  28. https://en.m.wikipedia.org/wiki/permutohedron
  29. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/?share=twitter
  30. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/?share=linkedin
  31. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/?share=email
  32. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/#print
  33. https://blog.acolyer.org/category/machine-learning/
  34. https://blog.acolyer.org/category/uncategorized/
  35. https://blog.acolyer.org/2017/03/22/convolution-neural-networks-part-3/
  36. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/
  37. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/#respond
  38. https://driversrush.blogspot.co.uk/
  39. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/#comment-14252
  40. https://driversrush.blogspot.co.uk/search/label/asus
  41. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/?replytocom=14252#respond
  42. http://ssix.blogactiv.eu/2017/05/30/sentiment-classification-about-finance-statements/
  43. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/#respond
  44. https://gravatar.com/site/signup/
  45. javascript:highlandercomments.doexternallogout( 'wordpress' );
  46. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/
  47. javascript:highlandercomments.doexternallogout( 'googleplus' );
  48. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/
  49. javascript:highlandercomments.doexternallogout( 'twitter' );
  50. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/
  51. javascript:highlandercomments.doexternallogout( 'facebook' );
  52. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/
  53. javascript:highlandercomments.cancelexternalwindow();
  54. https://akismet.com/privacy/
  55. http://eepurl.com/bbzpm9
  56. https://blog.acolyer.org/2019/04/05/ginseng-keeping-secrets-in-registers-when-you-distrust-the-operating-system/
  57. https://blog.acolyer.org/2019/03/25/amazon-aurora-design-considerations-for-high-throughput-cloud-native-relational-databases/
  58. https://blog.acolyer.org/2019/04/03/establishing-software-root-of-trust-unconditionally/
  59. https://blog.acolyer.org/2019/03/27/amazon-aurora-on-avoiding-distributed-consensus-for-i-os-commits-and-membership-changes/
  60. https://blog.acolyer.org/2019/03/29/calvin-fast-distributed-transactions-for-partitioned-database-systems/
  61. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
  62. https://blog.acolyer.org/2016/09/05/on-the-criteria-to-be-used-in-decomposing-systems-into-modules/
  63. https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/
  64. https://blog.acolyer.org/2015/04/29/applying-the-universal-scalability-law-to-organisations/
  65. https://blog.acolyer.org/2019/01/25/programming-paradigms-for-dummies-what-every-programmer-should-know/
  66. https://blog.acolyer.org/feed/
  67. https://blog.acolyer.org/comments/feed/
  68. https://twitter.com/519408925733425153
  69. https://wordpress.com/?ref=footer_blog
  70. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/#cancel
  71. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  73. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/#comment-form-guest
  74. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/#comment-form-load-service:wordpress.com
  75. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/#comment-form-load-service:twitter
  76. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/#comment-form-load-service:facebook
