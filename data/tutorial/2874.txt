     *
     * [1]richard socher
     * [2]publications
     * [3]deep learning tutorial
     * [4]notes & misc.
     * [5]photography
     * contact

parsing with compositional vector grammars

   richard socher, john bauer, christopher d. manning, andrew y. ng

   natural language parsing has typically been done with small sets of
   discrete categories such as np and vp, but this representation does not
   capture the full syntactic nor semantic richness of linguistic phrases,
   and attempts to improve on this by lexicalizing phrases or splitting
   categories only partly address the problem at the cost of huge feature
   spaces and sparseness. instead, we introduce a compositional vector
   grammar (cvg), which combines pid18s with a syntactically untied
   id56 that learns syntactico-semantic, compositional
   vector representations. the cvg improves the pid18 of the stanford
   parser by 3.8% to obtain an f1 score of 90.4%. it is fast to train and
   implemented approximately as an efficient reranker it is about 20%
   faster than the current stanford factored parser. the cvg learns a soft
   notion of head words and improves performance on the types of
   ambiguities that require semantic information such as pp attachments.

download paper

     * download: [6]socherbauermanningng_acl2013.pdf

code

     * the new parser is shipped in version 3.2 of the stanford parser:
     * java code for training and testing the su-id56 and full cvg model:
       [7]http://nlp.stanford.edu/software/stanford-parser-full-2013-06-20
       .zip
     * for instructions how to train the model on your own data or with
       different word vectors or other languages, see
       [8]http://nlp.stanford.edu/software/parser-faq.shtml#id56
     * more information will follow soon.

further analysis and visualization

     * here are three of the learned w matrices showing how the model
       learns a soft version of head words:
     * vp-np:
     * dt-np:
     * adjp-np
     * pp-np
     * qp-sbar: example for matrices that occur very rarely and hence stay
       close to the initialization

bibtex

     * please cite the following paper when you use the code:
       @incollection{socheretal2013:cvg,
       title = {{parsing with compositional vector grammars}},
       author = {richard socher and john bauer and christopher d. manning
       and andrew y. ng},
       booktitle = {{acl}},
       year = {2013}
       }

comments critique, questions

   copy what you write before you post, then type in the password, post
   (nothing happens), then copy the text and re-post.
   add comment
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
           sign as author _________________________________
   enter code:
   [parsingwithcompositionalvectorgrammars?action=captchaimage&amp;captcha
   key=1] _____ post reset

[9]hebby[10]?     22 november 2018, 10:12

   hello! can you do me a favour. i want to know how many layers does the
   neural network have?

[11]hebby[12]?     22 november 2018, 10:10

   hello! can you do me a favour. i want to know how many layers does the
   neural network have?

[13]omid[14]?     29 june 2018, 04:49

   i want to get features vector of nodes and sentences, how can i do it?

[15]yun[16]?     14 april 2017, 17:38

   hello! can you do me a favour. i want to get the ppt of paper "parsing
   with compositional vector grammars" in 2013. can you send it to me
   through email? my email is yunzhao@cs.ucsb.edu. thank you very much!

[17]ugur[18]?     27 september 2016, 21:27

   is there a video for the paper? thanks,

[19]chen along[20]?     28 may 2016, 04:02

   i'am sorry, in last comment my email address have some wrong. follow is
   my email address. c2010120422@gmail.com

   thank you very much!

[21]chen along[22]?     28 may 2016, 02:41

   hello! can you do me a favour. i want to get the ppt of essay "parsing
   with compositional vector grammars" in 2013. can you send it to me
   through email. my email is c2010120422@gmai.com.

   thank you very mush!

[23]shujon naha[24]?     11 may 2015, 04:27

   hi richard can you please give me some idea how to extract the vector
   representation of a sentence using this tool? it would be really
   helpful if you can give some idea about where should i look to get
   them.

   best

[25]youtube spammer[26]?     08 april 2015, 20:43

   does really work? can i just write anything!?!?

[27]richardsocher     14 march 2015, 05:32

   @manan, you have to go into the code to get them.

[28]manan[29]?     29 august 2014, 15:58

   hi,

   the code which you have shared gives the parsed output, but does it
   even return the scores computed/vector representation ?

   manan

[30]richardsocher     13 april 2014, 23:48

   the tensor v is learned via id26 using the equations in the
   paper. u is another set of parameters that we learn, it is an n
   dimensional vector.

[31]pritpal[32]?     20 march 2014, 13:29

   hi sir,
          can you please clear my doubt in recursive neural tensor network. the
equation (bc)^t v[1:2] (bc) +w(bc)in this how we compute v[1:2]. and in previous
 equations while calculating the score score = u^tp what exactly is u^t??

references

   1. https://www.socher.org/index.php/main/homepage
   2. https://www.socher.org/index.php/main/homepage#publications
   3. https://www.socher.org/index.php/deeplearningtutorial/deeplearningtutorial
   4. https://www.socher.org/index.php/main/notesandcodes
   5. https://www.socher.org/index.php/main/photographyandtravel
   6. https://www.socher.org/uploads/main/socherbauermanningng_acl2013.pdf
   7. http://nlp.stanford.edu/software/stanford-parser-full-2013-06-20.zip
   8. http://nlp.stanford.edu/software/parser-faq.shtml#id56
   9. https://www.socher.org/index.php/profiles/hebby?action=edit
  10. https://www.socher.org/index.php/profiles/hebby?action=edit
  11. https://www.socher.org/index.php/profiles/hebby?action=edit
  12. https://www.socher.org/index.php/profiles/hebby?action=edit
  13. https://www.socher.org/index.php/profiles/omid?action=edit
  14. https://www.socher.org/index.php/profiles/omid?action=edit
  15. https://www.socher.org/index.php/profiles/yun?action=edit
  16. https://www.socher.org/index.php/profiles/yun?action=edit
  17. https://www.socher.org/index.php/profiles/ugur?action=edit
  18. https://www.socher.org/index.php/profiles/ugur?action=edit
  19. https://www.socher.org/index.php/profiles/chenalong?action=edit
  20. https://www.socher.org/index.php/profiles/chenalong?action=edit
  21. https://www.socher.org/index.php/profiles/chenalong?action=edit
  22. https://www.socher.org/index.php/profiles/chenalong?action=edit
  23. https://www.socher.org/index.php/profiles/shujonnaha?action=edit
  24. https://www.socher.org/index.php/profiles/shujonnaha?action=edit
  25. https://www.socher.org/index.php/profiles/youtubespammer?action=edit
  26. https://www.socher.org/index.php/profiles/youtubespammer?action=edit
  27. https://www.socher.org/index.php/profiles/richardsocher
  28. https://www.socher.org/index.php/profiles/manan?action=edit
  29. https://www.socher.org/index.php/profiles/manan?action=edit
  30. https://www.socher.org/index.php/profiles/richardsocher
  31. https://www.socher.org/index.php/profiles/pritpal?action=edit
  32. https://www.socher.org/index.php/profiles/pritpal?action=edit
