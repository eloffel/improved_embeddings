   [1]

linkedin

     * [2]sign in
     * [3]join now

   deep learning for stock price prediction explained

               deep learning for stock price prediction explained

   published on february 17, 2017february 17, 2017     56 likes     9 comments

   [4]joe ellsworth

[5]joe ellsworth[6]follow

vp and chief technology officer at delta denta

     * (button) like56
     * (button) comment9
     * [ ] share
          + (button) linkedin
          + (button) facebook
          + (button) twitter
       9

   as part of my work on [7]quantized classifier i have built a set of
   examples showing how to use deep learning to predict future stock
   prices. tensorflow is a popular deep learning library provided by
   google. this article explains my approach, some terminology and results
   from a set of examples used to predict future stock prices. the code is
   free so please[8] download and experiment.

   [:0]

   related articles: [9]stock price prediction tutorial, [10]analyzing
   predictive value of features, [11]faq, [12]how to make money with
   quantized classifier

  general purpose tensorflow id98 classifier

   the first step was building a general purpose utility that could read a
   wide variety of csv input files submit them to tensorflow's id98 for
   classification. the utility needed to work across a wide range of
   inputs from classifying heart disease to predicting stock prices
   without any changes to the code. the utility i built is called
   [13]id98classify.py when used for stock price prediction it reads two
   csv files a training file and a test file. it outputs classification
   results to the console. it builds a deep learning id98 using the
   training data and then uses the id98 to classify rows from the test data
   file. it generates output showing how successful it was in the
   classification process.

   you need to install tensorflow, tlearn and python 3.52 to run these
   samples.

  classes explained

   in supervised learning we look at a given row and assign a class. you
   can think of a class as a grouping of rows. any classification project
   requires at least two classes such happy or sad, alive or dead. for
   these examples i use class=0 to indicate a bar that failed to meet the
   goal. class=1 if the bar did meet the goal.

   we are seeking bars that will rise in price enough to meet a profit
   taker goal before they would encounter a stop loss or that rise at
   least 1/2 of the way to the goal before reaching a maximum hold time.

   to determine the class we look ahead at future bars and determined if
   the price for that symbol has moved in a way that would meet our goal.
   for these stock examples we have three factors for each goal.
    1. a percentage rise in price that would allow exit with a profit
       taker order.
    2. a point where if the price drops by more than a given percentage it
       will exit the trade with stop limit.
    3. a maximum hold time where if the price has not risen to at least
       1/2 of the way to goal before the hold time expires it is
       considered a failure.

   bars that satisfy these rules are assigned a class of #1.  bars with
   prices that rose through hold time but did not reach 1/2 of goal are
   class #2.  classes that dropped but did not hit the stop limit are
   class #3.  bars that fail by hitting stop limit are assigned class #0.
   the classes are used by the learning algorithm to train the machine
   learning model. they are also used to test verify the classification
   accuracy of the model when processing the test file. in a production
   trading system the predicted class is used to generate buy signals that
   could be executed either by a human or an automated trading system.

   in our examples we seek to maximize precision and recall of class 1 to
   find successful bars. the premise is that we will eventually use the
   predicted class for current bars to generate buy signals.

   in a more sophisticated system we could have dozens or hundreds of
   classes but in this instance we are only seeking a signal about whether
   it is a good time to buy a specific symbol using a specific goal set.
   it would be relatively easy to run the engine across hundreds of stocks
   so you always had something available to buy.

   the utility we use to generate the training and test data files with
   the classes assigned is [14]stock-prep-sma.py you need to have
   downloaded the bar data before running it. you can use your own bar
   data or [15]yahoo-stock-download.py will download it from yahoo. these
   utilities are only samples but they could be a good example of where to
   start building a more sophisticated system.

   the class computation used in these tests is intentionally simple. feel
   free to take these samples and extend them with your own creative
   enhancements. unique combinations of different classification logic and
   different feature engineering can allow a single engine like quantized
   classifier to produce millions of different trading signals all
   customized to each specific users trading preferences.

  base id203 and lift

   when evaluating any machine learning system there are several numbers
   we use to measure how effective the system is.
     * base id203 - given a input test data set a certain portion of
       bars will be class 0 and a certain number will be class 1. if 33
       bars out of 100 met the goal then the base id203 that any bar
       will be a member of class 1 is 33 / 100 = 0.33
     * precision - when the system runs across test data it attempt to
       classify bars. how often the actual class of a given bar matches
       the predicted class is called precision. if the system predicted
       that 27 bars would be class 1 and only 22 bars actually were class
       1 then the precision for class 1 is 22 / 27 = 0.8148 the precision
       can also be measured for all records in a system but in this
       context we care most about the precision of class 1 because we plan
       to use it to generate buy signals.
     * recall - when the system evaluates test data it will attempt to
       find all the bars that it should classify as a given class. in
       reality it will only find a fraction of the bars that are
       available. recall is computed as a ratio of those it classified
       correctly and the total number of records of that class. a general
       rule is that you can increase recall at the expense of precision or
       increase precision at the expense of recall. better engines and
       improved feature engineering are used to increase both. recall is
       typically computed on a class by class basis. we care most about
       recall for class 1 because higher recall will generate more buy
       signals. if there were 33 bars available and the system correctly
       found 22 bars that it correctly classified as class 1 then recall
       for class 1 would be 22 / 33 = 0.66.
     * lift - lift is the measure of how much better precision is than
       base id203. lift is important because it allows the relative
       improvement in prediction accuracy to be compared even when base
       id203 changes. if base id203 is 0.33 and precision is
       0.8148 then lift = 0.8148 - 0.33 = 0.4848. i use lift to help guide
       exploration of features. if i can increase lift without a
       significant reduction in recall then it is normally a good change.

  understand probabilities in context:

   a common mistake is to look at a precision such as 55% out of context
   of the goal. it is incorrect to look at 55% precision and say it is
   poor odds unless you understand how much you win versus how much you
   would loose. if the wins are bigger than the losses then you can remain
   profitable with a lower percentage of winning bars.

     if you tell any gambler you will give them 50% odds and winning
     hands will earn 2 times as much as they loose with loosing hands
     they will gamble all week long.

   the law of large numbers indicates that if gambler has a 55% chance of
   winning and they are betting $100 each time after a large number bets
   they they will have won 55 times and lost 45 times on average per 100
   bets. they would have won 55 * $200 = $11,000 from the wining bets.
   they would have lost 45 * 100 = $4,500 from the loosing bets giving
   them a net profit of $6,500 as long as the magnitude of the win versus
   loss stays the same and the id203 of win versus loss stays at 55%
   then they will continue making profit. the law of big numbers also
   indicates they could have a long run of losses in the short term and
   still average 55% wins in the long term so they need to manage the
   amount they bet using something like the k[16]elly criteria to avoid
   gamblers ruin. in this example even if the win % dropped little below
   50% while the magnitude of win versus loss remained the same it would
   still be a winning system.

   i used samples where the amount won is larger than the amount lost
   because i found that forcing a larger win magnitude helps isolate the
   signal from the noise. this helps the prediction system deliver greater
   lift. greater lift normally comes at the cost of recall so we have
   fewer trades but i would rather run dozens of diverse strategies that
   earn more profit per trade than accept more losses with smaller profits
   per trade. we can still find enough trades but it may consume a bit
   more compute power to evaluate dozens or hundreds of strategies
   simultaneously.

  feature engineering

   feature engineering is where some of the most important work is done in
   machine learning. many data sets such as bar data yield relatively low
   predictive value in their native form. it is only after this data has
   been transformed that it produces useful classification.

   in the context of our stock trading samples we used a few basic
   indicators which are applied across variable time horizons to produce
   machine learning features. they are:
    1. slope of the price change compared to a bar in the past
    2. percentage above minimum price within some # of bars in the past
    3. percentage below the maximum price within some # of bars in the
       past
    4. slope of percentage above minimum price some # of bars in the past
    5. slope of percentage below maximum price some # of bars in the past.

   each of these may be applied to any column such as high, low, open,
   close or they can be applied against a derived indicators such as a
   sma.

   the utility that produces test and training files containing both the
   machine learning features and classes is called [17]stock-prep-sma.py.
   it is only intended as an example that you can modify to add your own
   creativity. i do not claim these are great features but they were good
   enough to demonstrate quantized classifier and deep learning id98
   delivering some lift and reasonable recall.

   feature engineering is an area with nearly infinite potential for
   creative thought. by using different combinations of features ml
   classifiers can produce radically different trading signals for
   different users. i encourage you to explore this area there are
   hundreds of indicators explained across thousands of trading books most
   of which can be converted into machine learning friendly features.

  deep learning number of epoch explained

   the tensorflow flow deep learning id98 (convolutional neural network) a
   learning strategy based on how scientists think brains learn. the c
   portion essentially adds multiple layers to the nn which can allow them
   to perform better in ares where id90 have previously
   dominated.

   just as biological systems learn best with repetition the id98 needs to
   see data multiple times while it is building its internal memory model
   it uses to support the classification process. each repetition where
   the training data is re-submitted to the id98 engine is considered one
   epoch.

   i have generally found minimum acceptable results are at 80 repetitions
   while the id98 seems to perform best with at least 800 repetitions when
   working with stock data. each of these repetitions is what the
   tensor-flow libraries call a epoch. for these samples i chose to use
   800 epoch. for those that didn't produce good results i increased the
   number of epoch to 2800.

  have fun and experiment

     before you ask if i have tried it with x data set? or if it has been
     hooked to broker x. the engine is free, the examples are free. you
     are free to take them and test them with any data or configurations
     you desire. have fun and please let me know what you learn. if you
     want help then i sell [18]consulting services.

   sample call
id98classifystockspy1up1dnmh3.bat

or

python id98classify.py ../data/spy-1up1-1dn-mh3-close.train.csv  ../data/spy-1up1
-1dn-mh3-close.test.csv 800

   sample output

   [:0]

  id98 related stock tests

   parm 0 = id98classify.py the script python will run, parm 1 = training
   file to use when building internal memory model. parm 1 - test file to
   use when testing the classification engine. parm 3 - number of epoch to
   use for this test.

   [19]id98classifystock-slv-1p5up0p3dnmh5.bat - slv (silver) goal to rise
   by 1.5% before it drops by 0.3% with max hold of 5 days.
python id98classify.py ../data/slv-1p5up-0p3dn-mh10-close.train.csv  ../data/slv-
1p5up-0p3dn-mh10-close.test.csv 800

    [20]id98classifystock-spy-2p2up1p1dnmh6.bat - spy goal to rise to exit
   with profit taker at 2.2% with a 1% stop limit. max hold of 6 days.
python id98classify.py ../data/slv-1p5up-0p3dn-mh10-close.train.csv  ../data/slv-
1p5up-0p3dn-mh10-close.test.csv 800

   [21]id98classifystock-spy-6p0up1p0dnmh45.bat spy goal to rise to exit
   with profit taker at 6% gain with stop loss at 1% and max hold time of
   45 days.
python id98classify.py ../data/spy-6up-1dn-mh10-smahigh90.train.csv  ../data/spy-
6up-1dn-mh10-smahigh90.test.csv 800

    [22]id98classifystock-spy-8up4dnmh90.bat spy goal to rise to exit with
   profit taker at 8% gain with stop loss at 5% and maximum hold time of
   90 days.
python id98classify.py ../data/spy-8up1-4dn-mh90-close.train.csv  ../data/spy-8up
1-4dn-mh90-close.test.csv 800

   [23]id98classifystock-spy-1p0up0p5dnmh4.bat spy goal to rise to exit
   with profit taker at 1% gain before it encounters a stop loss of 0.5%.
   max hold time 4 days.

   [24]id98classifystock-cat-1p7up1p2dnmh2.bat cat goal to rise to exit
   with profit taker at 1.7% before it encounters a stop loss at 1.2% with
   max hold of 2 days.

   [25]id98classifystock-cat-6p0up1p0dnmh45.bat cat goal to rise to exist
   with profit taker at 6% before it encounters a stop loss at 1% with max
   hold of 45 days.

   [26]id98classifystock-cat-7p8up1p2dnmh5.bat cat goal to rise 7.8% to
   exit with profit taker before it encounters a stop loss at 1.2%. max
   hold of 5 days.

  deep learning tensorflow disclaimer

   deep learning is a broad topic area. tensorflow is a fairly large and
   complex product. i have used one configuration of a tensorflow id98 for
   these examples. tensorflow supports many other models and id98 can be
   configured in many ways including a different initialization and
   different layer configurations. there are most likely ways to configure
   tensorflow to produce better results than i have shown in these
   samples. if you find them please let me know.

  summary

   this work is only intended to provide a starting point from which you
   can easily branch out with your own discovery process. i do sell
   [27]consulting services you can purchase if you would like to use my
   expertise to accelerate your own work.

   i wrote these examples to test the [28]quantized classifier and for
   some of the samples it delivers better results than tensorflow did.
   this may be due to selection bias where i used examples that performed
   well with the quantized classifier. it may be possible to find a
   configuration of tensorflow the would deliver superior results. i
   personally find that [29]quantized classifier and [30]quantized filter
   make it easier to find profitable combinations. the quantized library
   also seems to provide better support to help [31]discover which
   features are adding predictive value. having the engine help guide the
   feature selection is beneficial when there are millions of possible
   feature and indicator combinations available but only a small fraction
   of them will actually help predict future stock prices.

   if you would like to build the [32]quantized classifier into a larger
   trading system then i can help by [33]providing expertise and
   consulting services.

   please let me know if you would enjoy similar articles exploring the
   same examples using the spark ml libraries or other popular ml
   libraries.

   thanks joe ellsworth. [34]contact

   taken at temple ruins near bankok winter-2016

   [:0]

   note: after i wrote this article i added slope below maximum and slope
   above minimum for 90,180,360 day features.   they have the names
   sam90,sam180,sam360, sbm90, sbm180,sbm360 this change caused changes in
   the numbers reported in the table above. you can get an exact match on
   those numbers by removing those columns from the csv files. you can
   modify the stock_prep python script to remove those columns as well.

   also published on the [35]bayes analytic blog

other reading

   thanks for all the feedback. some readers have sent me some great links
   that dive deeper into the therory of using deep learning for stock
   price prediction. keep sending them and i will add those i think add
   value here.

   these links are independent and unrelated to my work on [36]quantized
   classifier but they do investigate different ways to apply deep
   learning and other ml techniques to stock price prediction as such i
   included them here.
     * [37]deep learning explained a linked in article by [38]ramkumar
       balasubramanian covers it from customer and advertising perspective
       written in 2015 easy to follow.
       https://www.linkedin.com/pulse/deep-learning-explained-ramkumar-bal
       asubramanian
     * [39]applying deep learning to enhance momentum trading strategies
       in stocks lawrence takeuchi stanford.edu yu-ying (albert) lee 2013

  yue zhang

     * [40]deep learning for event - driven stock prediction xiaoding      ,
       yuezhang   , tingliu   , junwenduan       research center for social
       computing and information retrieval harbin institute of technology,
       china
     * [41]deep learning for stock price prediction yue zhang, singapoor
       university - 39 page slide set. high level but informative.
     * [42]machine learning in stock price trend forecasting yuqing dai,
       yuning zhang stanford.edu,


   [43]joe ellsworth

[44]joe ellsworth

vp and chief technology officer at delta denta

   [45]follow

   9 comments
   article-comment__guest-image
   [46]sign in to leave your comment
   show more comments.
     __________________________________________________________________

more from joe ellsworth

   [47]19 articles
   analyzing feature value in ml classification

[48]analyzing feature value in ml classification

   february 2, 2017

   parsing speed of a csv file with sma computation c, java, go, python,
   node ...

[49]parsing speed of a csv file with sma   

   january 21, 2017

   sample scala code to render & download nasa srtm digital elevation
   models (dem)

[50]sample scala code to render & download nasa   

   november 12, 2016

     *    2019
     * [51]about
     * [52]user agreement
     * [53]privacy policy
     * [54]cookie policy
     * [55]copyright policy
     * [56]brand policy
     * [57]manage subscription
     * [58]community guidelines
     * [ ]
          + (button) bahasa indonesia
          + (button) bahasa malaysia
          + (button)   e  tina
          + (button) dansk
          + (button) deutsch
          + (button) english
          + (button) espa  ol
          + (button)             
          + (button) fran  ais
          + (button)          
          + (button) italiano
          + (button)             
          + (button) nederlands
          + (button)          
          + (button) norsk
          + (button) polski
          + (button) portugu  s
          + (button) rom  n  
          + (button)               
          + (button) svenska
          + (button) tagalog
          + (button)                      
          + (button) t  rk  e
          + (button)               
       languagelanguage

references

   1. https://www.linkedin.com/?trk=header_logo
   2. https://www.linkedin.com/uas/login?trk=header_signin
   3. https://www.linkedin.com/start/join?trk=header_join
   4. https://www.linkedin.com/in/joe-ellsworth-68222?trk=author_mini-profile_image
   5. https://www.linkedin.com/in/joe-ellsworth-68222?trk=author_mini-profile_title
   6. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/deep-learning-stock-price-prediction-explained-joe-ellsworth&trk=author-info__follow-button
   7. http://(https://bitbucket.org/joexdobs/ml-classifier-gesture-recognition
   8. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition
   9. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/wiki/stock-example/predict-future-stock-price-using-machine-learning.md
  10. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/demo/stock/spy/1-up-1-dn/docs/stock-price-prediction-analyze-feature-value.md
  11. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/docs/faq.md
  12. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/wiki/how-can-i-use-quantized-classifier-to-make-money.md
  13. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/tlearn/id98classify.py
  14. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/8aab8c92d0478037c9dcd5145d62e240aa7c9ebd/stock-prep-sma.py?at=default&fileviewer=file-view-default
  15. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/yahoo-stock-download.py
  16. http://bayesanalytic.com/tag/kelly-criteria-algorithm/
  17. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/8aab8c92d0478037c9dcd5145d62e240aa7c9ebd/stock-prep-sma.py?at=default&fileviewer=file-view-default
  18. http://bayesanalytic.com/contact
  19. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/tlearn/id98classifystock-slv-1p5up0p3dnmh5.bat
  20. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/tlearn/id98classifystock-spy-2p2up1p1dnmh6.bat
  21. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/tlearn/id98classifystock-spy-6p0up1p0dnmh45.bat
  22. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/tlearn/id98classifystock-spy-8up4dnmh90.bat
  23. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/tlearn/id98classifystock-spy-1p0up0p5dnmh4.bat
  24. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/tlearn/id98classifystock-cat-1p7up1p2dnmh2.bat
  25. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/tlearn/id98classifystock-spy-6p0up1p0dnmh45.bat
  26. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/tlearn/id98classifystock-cat-7p8up1p2dnmh5.bat
  27. http://bayesanalytic.com/contact
  28. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition
  29. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/wiki/stock-example/predict-future-stock-price-using-machine-learning.md
  30. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/quant_filt.py
  31. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/src/default/demo/stock/spy/1-up-1-dn/docs/stock-price-prediction-analyze-feature-value.md?at=adding+by+feature+and+by+feature+group+analysis&fileviewer=file-view-default
  32. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition
  33. http://bayesanalytic.com/contact
  34. http://bayesanalytic.com/contact
  35. http://bayesanalytic.com/deep-learning-for-stock-price-prediction-explained/
  36. http://bitbucket.org/joexdobs/ml-classifier-gesture-recognition
  37. http://www.linkedin.com/pulse/deep-learning-explained-ramkumar-balasubramanian
  38. http://www.linkedin.com/in/ramkumar-balasubramanian-93482589
  39. http://cs229.stanford.edu/proj2013/takeuchilee-applyingdeeplearningtoenhancemomentumtradingstrategiesinstocks.pdf
  40. http://www.ijcai.org/proceedings/15/papers/329.pdf
  41. http://www.slideshare.net/limzhiyuanzane/deep-learning-for-stock-prediction
  42. http://cs229.stanford.edu/proj2013/daizhang-machinelearninginstockpricetrendforecasting.pdf
  43. https://www.linkedin.com/in/joe-ellsworth-68222?trk=author_mini-profile_image
  44. https://www.linkedin.com/in/joe-ellsworth-68222?trk=author_mini-profile_title
  45. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/deep-learning-stock-price-prediction-explained-joe-ellsworth&trk=author-info__follow-button-bottom
  46. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/deep-learning-stock-price-prediction-explained-joe-ellsworth&trk=article-reader_leave-comment
  47. https://www.linkedin.com/today/author/joe-ellsworth-68222
  48. https://www.linkedin.com/pulse/next-feature-quantized-ml-classifiermachine-learning-joe-ellsworth?trk=related_artice_analyzing feature value in ml classification _article-card_title
  49. https://www.linkedin.com/pulse/parsing-speed-csv-file-sma-computation-c-java-go-python-joe-ellsworth?trk=related_artice_parsing speed of a csv file with sma computation c, java, go, python, node ..._article-card_title
  50. https://www.linkedin.com/pulse/sample-scala-code-render-download-nasa-srtm-digital-models-ellsworth?trk=related_artice_sample scala code to render &amp;amp;amp; download nasa  srtm digital elevation models (dem) _article-card_title
  51. https://press.linkedin.com/about-linkedin?trk=article_reader_footer_footer-about
  52. https://www.linkedin.com/legal/user-agreement?trk=article_reader_footer_footer-user-agreement
  53. https://www.linkedin.com/legal/privacy-policy?trk=article_reader_footer_footer-privacy-policy
  54. https://www.linkedin.com/legal/cookie-policy?trk=article_reader_footer_footer-cookie-policy
  55. https://www.linkedin.com/legal/copyright-policy?trk=article_reader_footer_footer-copyright-policy
  56. https://brand.linkedin.com/policies?trk=article_reader_footer_footer-brand-policy
  57. https://www.linkedin.com/psettings/guest-controls?trk=article_reader_footer_footer-manage-sub
  58. https://www.linkedin.com/help/linkedin/answer/34593?lang=en&trk=article_reader_footer_footer-community-guide
