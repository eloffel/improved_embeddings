
   [1]

the stanford natural language processing group

the stanford nlp group

     * [2]people
     * [3]publications
     * [4]research blog
     * [5]software
     * [6]teaching
     * [7]local

[8]software > deterministic coreference resolution system

            stanford deterministic coreference resolution system

      [9]news | [10]about | [11]download | [12]usage | [13]questions |
                   [14]mailing lists | [15]release history

news

   december 9, 2015: the deterministic coreference resolution system is
   still supported in stanfordcorenlp by using the annotator dcoref.
   however, we have now added new, better performing statistical and
   neural coreference systems written by kevin clark, which are invoked by
   default or explicitly using the annotator coref. see the
   [16]corefannotator documentation.

   may 7, 2013: recent improvements to the stanford deterministic
   coreference resolution system (recasens et al., below) won [17]the best
   short paper award at naacl 2013.

   june 30, 2011: this system was the top ranked system at the conll-2011
   shared task.

about

   this system implements the multi-pass sieve coreference resolution (or
   id2) system described in [18]lee et al. (conll shared
   task 2011) and [19]raghunathan et al. (emnlp 2010).

   the score obtained is higher than that in emnlp 2010 paper because of
   additional sieves and better rules (see [20]lee et al. 2011 for
   details). mention detection is included in the package (see [21]usage
   for instructions).

   [22]the computational linguistics paper includes more details and
   additional experimental results.

   the papers to cite for this system are as follows:

     marta recasens, marie-catherine de marneffe, and christopher potts.
     [23]the life and death of discourse entities: identifying singleton
     mentions.
     in proceedings of naacl 2013.

     heeyoung lee, angel chang, yves peirsman, nathanael chambers, mihai
     surdeanu and dan jurafsky.
     [24]deterministic coreference resolution based on entity-centric,
     precision-ranked rules.
     computational linguistics 39(4), 2013.

     heeyoung lee, yves peirsman, angel chang, nathanael chambers, mihai
     surdeanu, dan jurafsky.
     [25]stanford's multi-pass sieve coreference resolution system at the
     conll-2011 shared task.
     in proceedings of the conll-2011 shared task, 2011.

     karthik raghunathan, heeyoung lee, sudarshan rangarajan, nathanael
     chambers, mihai surdeanu, dan jurafsky, christopher manning
     [26]a multi-pass sieve for coreference resolution
     emnlp-2010, boston, usa. 2010.

current evaluation results

   the scores of the dcoref code in v3.6.0 (conll 2011 shared task winner
   descendant) on the conll 2011 shared task dev data set, measured on
   2016/02/07 using the v4 scorer (used for the 2011 evaluation).
--------------------------------------------------------------------------------
---------------------------------------------------------
                            muc               b cubed              ceaf (m)
       ceaf (e)            blanc        |
                       p     r     f1      p     r     f1      p     r     f1
   p     r     f1      p     r     f1   | avg f1
--------------------------------------------------------------------------------
---------------------------------------------------------
conllst2011 dev   |   62.1  59.3  60.7  | 74.2  67.7  70.8  | 59.4  59.4  59.4
| 46.1  48.9  47.5  | 79.6  72.4  75.4  |  59.56
--------------------------------------------------------------------------------
---------------------------------------------------------
* automatic mention detection used. avg f1 = (muc + b cubed + ceafe)/3.

   the scores of the dcoref code in v3.6.0 (conll 2011 shared task winner
   descendant) on the conll 2011/2012 shared task dev data sets, measured
   on 2016/02/07 using the v8.01 scorer (current in 2016).
--------------------------------------------------------------------------------
---------------------------------------------------------
                            muc               b cubed              ceaf (m)
       ceaf (e)            blanc        |
                       p     r     f1      p     r     f1      p     r     f1
   p     r     f1      p     r     f1   | avg f1
--------------------------------------------------------------------------------
---------------------------------------------------------
conllst2011 dev   |   62.1  59.3  60.7  | 56.2  48.6  52.1  | 58.0  57.5  57.8
| 48.9  53.5  51.1  | 54.1  47.2  50.1  |  54.62
conllst2012 dev   |   65.9  64.1  65.0  | 58.7  50.9  54.5  | 59.2  59.6  59.4
| 48.6  54.3  51.3  | 59.5  53.7  56.1  |  56.92
--------------------------------------------------------------------------------
---------------------------------------------------------
* automatic mention detection used. avg f1 = (muc + b cubed + ceafe)/3.

download

   the coreference resolution system is integrated in the stanford suite
   of nlp tools, [27]stanfordcorenlp. please download the entire suite
   from [28]this page.

usage

   running coreference resolution on raw text

   this software is now fully incorporated in [29]stanfordcorenlp, so all
   you have to do is add the dcoref annotator to the "annotators" property
   in stanfordcorenlp. for example, add "dcoref" to the end of the list of
   text annotators:
annotators = tokenize, ssplit, pos, lemma, ner, parse, dcoref

   the properties you can set for the dcoref system itself are the
   following:
dcoref.demonym                   // the path for a file that includes a list of
demonyms
dcoref.animate                   // the list of animate/inanimate mentions (ji a
nd lin, 2009)
dcoref.inanimate
dcoref.male                      // the list of male/neutral/female mentions (be
rgsma and lin, 2006)
dcoref.neutral                   // neutral means a mention that is usually refe
rred by 'it'
dcoref.female
dcoref.plural                    // the list of plural/singular mentions (bergsm
a and lin, 2006)
dcoref.singular

// above 8 options do not have to be set; default models in stanfordcorenlp pack
age will be used if unspecified.

dcoref.score = false             // scoring the output of the system
dcoref.postprocessing = false    // do post processing
dcoref.maxdist = -1              // maximum sentence distance between two mentio
ns for resolution (-1: no constraint on the distance)
dcoref.use.big.gender.number = false // load a big list of gender and number inf
ormation
dcoref.replicate.conll = false   // turn on this for replicating conllst result

// if above 5 options are omitted, default values (as shown in above) are used.

sievepasses                      // sieve passes - each class is defined in dcor
ef/sievepasses/
                                 // if omitted, the default sieves will be used
(recommended).

   see [30]stanfordcorenlp for more details.

   how to replicate the results in our conll shared task 2011 paper

   to replicate the results in the paper run:
java -cp <jars_in_corenlp> -xmx8g edu.stanford.nlp.dcoref.sievecoreferencesystem
 -props <properties file>

   a sample properties file (coref.properties) is included in the dcoref
   package. the properties file includes the following:
# annotators needed for coreference resolution
annotators = pos, lemma, ner, parse

# scoring the output of the system.
# scores in log file are different from the output of conll scorer because it is
 before post processing.
dcoref.score = true


# do post processing
dcoref.postprocessing = true
# maximum sentence distance between two mentions for resolution (-1: no constrai
nt on the distance)
dcoref.maxdist = -1
# load a big list of gender and number information
dcoref.use.big.gender.number = true
# older corenlp versions loaded huge text file; newer versions load serialized m
ap
# dcoref.big.gender.number = edu/stanford/nlp/models/dcoref/gender.data.gz
dcoref.big.gender.number = edu/stanford/nlp/models/dcoref/gender.map.ser.gz

# turn on this for replicating conllst result
dcoref.replicate.conll = true
# path for the official conll 2011 scorer script. if omitted, no scoring
dcoref.conll.scorer = /path/for/scorer

# path for log file for coref system evaluation
dcoref.logfile = /path/for/logs

# for scoring on other corpora, one of following options can be set
# dcoref.conll2011: path for the directory containing conllst files
# dcoref.ace2004: path for the directory containing ace2004 files
# dcoref.mucfile: path for the muc file
dcoref.conll2011 = /path/for/corpus

   this system can process ace2004, muc6, and conll shared task 2011
   corpora in their original formats. examples from the corpora are given
   here:

   conllst 2011:
nw/wsj/00/wsj_0020          0          0        the         dt (top_(s_(np_*
      -          -          -          -          *          *     (arg0*
   *          *          *        (11
nw/wsj/00/wsj_0020          0          1       u.s.        nnp         *)
   -          -          -          -      (gpe)          *         *)
*          *          *        11)
nw/wsj/00/wsj_0020          0          2          ,          ,          *
   -          -          -          -          *          *          *
*          *          *          -
nw/wsj/00/wsj_0020          0          3   claiming        vbg   (s_(vp_*      c
laim         01          2          -          *       (v*) (argm-adv*
*          *          *          -


   muc6:
...
<s> by/in proposing/vbg <coref id="13" type="ident" ref="6" min="date"> a/dt mee
ting/nn date/nn</coref> ,/, <coref id="14" type="ident" ref="0">
<organization> eastern/nnp</organization></coref> moved/vbd one/cd step/nn close
r/jjr toward/in reopening/vbg current/jj high-cost/jj contract/nn agreements/nns
 with/in <coref id="15" type="ident" ref="8" min="unions"><coref id="16" type="i
dent" ref="14"> its/prp$</coref> unions/nns</coref> ./. </s>
...

   ace2004:
...
<document docid="20001115_afp_arb.0212.eng">
<entity id="20001115_afp_arb.0212.eng-e1" type="org" subtype="educational" class
="spc">
  <entity_mention id="1-47" type="nam" ldctype="nam">
      <extent>
            <charseq start="475" end="506">the globalization studies center</cha
rseq>
                </extent>
                    <head>
                          <charseq start="479" end="506">globalization studies c
enter</charseq>
                              </head>
                                </entity_mention>
                                ...

   if you have issues getting this to work, you may need to follow a few
   steps:
     * use the latest version of the [31]evaluation software
     * there are some document naming inconsistencies between [32]the test
       set and the [33]test key. the following should help. in the /tc/
       part of the data, run
sed -i s/ch_0001/ch_0009/g res
sed -i s/ch_0002/ch_0019/g res
sed -i s/ch_0003/ch_0029/g res
sed -i s/ch_0004/ch_0039/g res
sed -i s/ch_0005/ch_0049/g res

       e.g. ch_0005 from the test set is named ch_0049 in the test key.
     * we used v4 of the coref scorer to get the numbers cited in the
       paper.

   how to run chinese coreference

   since corenlp version 3.5.2, we added support for chinese coreference.
     * running on raw text:
string text = ...;
string[] args = new string[]{
  "-props", "edu/stanford/nlp/hcoref/properties/zh-dcoref-default.properties"
};

annotation document = new annotation(text);
properties props = stringutils.argstoproperties(args);
stanfordcorenlp corenlp = new stanfordcorenlp(props);
corenlp.annotate(document);
hybridcorefannotator hcoref = new hybridcorefannotator(props);
hcoref.annotate(document);
map corefchain = document.get(corefchainannotation.class);
system.out.println(corefchain);

     * running on conll data:

// note that you have to replace the following properties file with your own.
// to do so, copy the following file, replace the # evaluation section with
// your own paths and refer to it in args.
string[] args = new string[]{
  "-props", "edu/stanford/nlp/hcoref/properties/zh-dcoref-conll.properties"
}
edu.stanford.nlp.hcoref.corefsystem.main(args);

questions

   questions, feedback, and bug reports/fixes can be sent to our
   [34]mailing lists.

mailing lists

   we have 3 mailing lists for the stanford coreference resolution system,
   all of which are shared with other javanlp tools (with the exclusion of
   the parser). each address is at @lists.stanford.edu:
    1. java-nlp-user this is the best list to post to in order to ask
       questions, make announcements, or for discussion among javanlp
       users. you have to subscribe to be able to use it. join the list
       via [35]this webpage or by emailing
       java-nlp-user-join@lists.stanford.edu. (leave the subject and
       message body empty.) you can also [36]look at the list archives.
    2. java-nlp-announce this list will be used only to announce new
       versions of stanford javanlp tools. so it will be very low volume
       (expect 1-3 messages a year). join the list via [37]this webpage or
       by emailing java-nlp-announce-join@lists.stanford.edu. (leave the
       subject and message body empty.)
    3. java-nlp-support this list goes only to the software maintainers.
       it's a good address for licensing questions, etc. for general use
       and support questions, you're better off joining and using
       java-nlp-user. you cannot join java-nlp-support, but you can mail
       questions to java-nlp-support@lists.stanford.edu.

release history

   version 3.6.0 - february 7, 2016

   the scores of the dcoref code in v3.6.0 (conll 2011 shared task winner
   descendant) on the conll 2011 shared task dev data set, measured on
   2016/02/07 using the v4 scorer (used for the 2011 evaluation).
--------------------------------------------------------------------------------
---------------------------------------------------------
                            muc               b cubed              ceaf (m)
       ceaf (e)            blanc        |
                       p     r     f1      p     r     f1      p     r     f1
   p     r     f1      p     r     f1   | avg f1
--------------------------------------------------------------------------------
---------------------------------------------------------
conllst2011 dev   |   62.1  59.3  60.7  | 74.2  67.7  70.8  | 59.4  59.4  59.4
| 46.1  48.9  47.5  | 79.6  72.4  75.4  |  59.56
--------------------------------------------------------------------------------
---------------------------------------------------------
* automatic mention detection used. avg f1 = (muc + b cubed + ceafe)/3.

   the scores of the dcoref code in v3.6.0 (conll 2011 shared task winner
   descendant) on the conll 2011/2012 shared task dev data sets, measured
   on 2016/02/07 using the v8.01 scorer (current in 2016).
--------------------------------------------------------------------------------
---------------------------------------------------------
                            muc               b cubed              ceaf (m)
       ceaf (e)            blanc        |
                       p     r     f1      p     r     f1      p     r     f1
   p     r     f1      p     r     f1   | avg f1
--------------------------------------------------------------------------------
---------------------------------------------------------
conllst2011 dev   |   62.1  59.3  60.7  | 56.2  48.6  52.1  | 58.0  57.5  57.8
| 48.9  53.5  51.1  | 54.1  47.2  50.1  |  54.62
conllst2012 dev   |   65.9  64.1  65.0  | 58.7  50.9  54.5  | 59.2  59.6  59.4
| 48.6  54.3  51.3  | 59.5  53.7  56.1  |  56.92
--------------------------------------------------------------------------------
---------------------------------------------------------
* automatic mention detection used. avg f1 = (muc + b cubed + ceafe)/3.

   july 9, 2013

   single mention detection (recasens et al. 2013) is integrated. the
   score may differ due to the change in parser or ner.
--------------------------------------------------------------------------------
---------------------------------------------------------
                            muc               b cubed              ceaf (m)
       ceaf (e)            blanc        |
                       p     r     f1      p     r     f1      p     r     f1
   p     r     f1      p     r     f1   | avg f1
--------------------------------------------------------------------------------
---------------------------------------------------------
conllst2011 dev   |   62.4  59.3  60.8  | 74.2  67.6  70.8  | 59.3  59.3  59.3
| 45.5  48.6  47.0  | 79.1  72.5  75.3  |  59.5
--------------------------------------------------------------------------------
---------------------------------------------------------
* automatic mention detection used. avg f1 = (muc + b cubed + ceafe)/3.

   june 6, 2011

   this release is the code used for conll shared task 2011. the score may
   differ due to the change in parser or ner.
--------------------------------------------------------------------------------
---------------------------------------------------------
                   conllst         muc               b cubed              ceaf (
m)            ceaf (e)            blanc        |
                    track     p     r     f1      p     r     f1      p     r
  f1      p     r     f1      p     r     f1   | avg f1
--------------------------------------------------------------------------------
---------------------------------------------------------
conllst2011 dev   | close |  59.1  57.5  58.3  | 69.2  71.0  70.1  | 58.6  58.6
 58.6  | 46.5  48.1  47.3  | 72.2  78.1  74.8  |  58.6
conllst2011 dev   | open  |  60.1  59.5  59.8  | 69.5  71.9  70.7  | 59.0  59.0
 59.0  | 46.5  47.1  46.8  | 73.8  78.6  76.0  |  59.1
conllst2011 test  | close |  57.5  61.8  59.6  | 68.2  68.4  68.3  | 56.4  56.4
 56.4  | 47.8  43.4  45.5  | 76.2  70.6  73.0  |  57.8
conllst2011 test  | open  |  59.3  62.8  61.0  | 69.0  68.9  68.9  | 56.7  56.7
 56.7  | 46.8  43.3  45.0  | 76.6  71.9  74.0  |  58.3
--------------------------------------------------------------------------------
---------------------------------------------------------
* automatic mention detection used. avg f1 = (muc + b cubed + ceafe)/3.

----------------------------------------------------------------------------
                      muc               b cubed             pairwise
                 p     r     f1      p     r     f1      p     r     f1
----------------------------------------------------------------------------
ace2004 dev   | 86.0  75.5  80.4  | 89.3  76.5  82.4  | 81.7  55.2  65.9
ace2004 test  | 82.7  70.2  75.9  | 88.7  74.5  81.0  | 77.2  44.6  56.6
ace2004 nwire | 84.6  75.1  79.6  | 87.3  74.1  80.2  | 79.4  50.1  61.4
muc6 test     | 90.6  69.1  78.4  | 90.6  63.1  74.4  | 89.7  57.0  69.7
----------------------------------------------------------------------------
* gold mentions are used.

   august 26, 2010

   this release is generally similar to the code used for emnlp 2010, with
   one additional sieve: relaxed exact string match.
----------------------------------------------------------------------------
                      muc               b cubed             pairwise
                 p     r     f1      p     r     f1      p     r     f1
----------------------------------------------------------------------------
ace2004 dev   | 84.1  73.9  78.7  | 88.3  74.2  80.7  | 80.0  51.0  62.3
ace2004 test  | 80.5  72.4  76.2  | 85.4  75.9  80.4  | 68.7  47.9  56.4
ace2004 nwire | 83.8  72.8  77.9  | 87.5  72.1  79.0  | 79.3  47.6  59.5
muc6 test     | 90.3  68.9  78.2  | 90.5  62.3  73.8  | 89.4  55.5  68.5
----------------------------------------------------------------------------

stanford nlp group

   gates computer science building
   353 serra mall
   stanford, ca 94305-9020
   [38]directions and parking

affiliated groups

     * [39]stanford ai lab
     * [40]stanford infolab
     * [41]csli

connect

     * [42]stack overflow
     * [43]github
     * [44]twitter

local links

   [45]nlp lunch    [46]nlp reading group
   [47]nlp seminar    [48]calendar
   [49]javanlp ([50]javadocs)    [51]machines
   [52]ai speakers    [53]q&a

references

   visible links
   1. https://nlp.stanford.edu/
   2. https://nlp.stanford.edu/people/
   3. https://nlp.stanford.edu/pubs/
   4. https://nlp.stanford.edu/blog/
   5. https://nlp.stanford.edu/software/
   6. https://nlp.stanford.edu/teaching/
   7. https://nlp.stanford.edu/new_local/
   8. https://nlp.stanford.edu/software/
   9. https://nlp.stanford.edu/software/dcoref.shtml#news
  10. https://nlp.stanford.edu/software/dcoref.shtml#about
  11. https://nlp.stanford.edu/software/dcoref.shtml#download
  12. https://nlp.stanford.edu/software/dcoref.shtml#usage
  13. https://nlp.stanford.edu/software/dcoref.shtml#questions
  14. https://nlp.stanford.edu/software/dcoref.shtml#mail
  15. https://nlp.stanford.edu/software/dcoref.shtml#history
  16. https://stanfordnlp.github.io/corenlp/coref.html
  17. http://naacl2013.naacl.org/schedule.aspx
  18. http://nlp.stanford.edu/pubs/conllst2011-coref.pdf
  19. http://nlp.stanford.edu/pubs/coreference-emnlp10.pdf
  20. http://nlp.stanford.edu/pubs/conllst2011-coref.pdf
  21. https://nlp.stanford.edu/software/dcoref.shtml#usage
  22. http://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00152
  23. http://nlp.stanford.edu/pubs/discourse-referent-lifespans.pdf
  24. http://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00152
  25. http://nlp.stanford.edu/pubs/conllst2011-coref.pdf
  26. http://nlp.stanford.edu/pubs/coreference-emnlp10.pdf
  27. https://nlp.stanford.edu/software/corenlp.html
  28. https://nlp.stanford.edu/software/corenlp.html#download
  29. https://nlp.stanford.edu/software/corenlp.html
  30. https://nlp.stanford.edu/software/corenlp.html
  31. http://conll.cemantix.org/2011/download/test/conll-2011-test-official.v5.tar.gz
  32. http://conll.cemantix.org/2011/download/test/conll-2011-test-official.v5.tar.gz
  33. http://conll.cemantix.org/2011/download/test/conll-2011-test-key.tar.gz
  34. https://nlp.stanford.edu/software/dcoref.shtml#mail
  35. https://mailman.stanford.edu/mailman/listinfo/java-nlp-user
  36. https://mailman.stanford.edu/pipermail/java-nlp-user/
  37. https://mailman.stanford.edu/mailman/listinfo/java-nlp-announce
  38. http://forum.stanford.edu/visitors/directions/gates.php
  39. http://ai.stanford.edu/
  40. http://infolab.stanford.edu/
  41. https://www-csli.stanford.edu/
  42. http://stackoverflow.com/tags/stanford-nlp
  43. https://github.com/stanfordnlp/corenlp
  44. https://twitter.com/stanfordnlp
  45. https://nlp.stanford.edu/local/nlp_lunch.shtml
  46. http://nlp.stanford.edu/read/
  47. http://nlp.stanford.edu/seminar/
  48. https://nlp.stanford.edu/local/calendar.shtml
  49. https://nlp.stanford.edu/javanlp/
  50. https://nlp.stanford.edu/nlp/javadoc/javanlp/
  51. https://nlp.stanford.edu/local/machines.shtml
  52. http://ai.stanford.edu/portfolio-view/distinguished-speaker-series
  53. https://nlp.stanford.edu/local/qa/

   hidden links:
  55. https://nlp.stanford.edu/software/dcoref.shtml
