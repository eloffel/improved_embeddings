   [nlp-logo.gif] id31 | information | [1]live demo |
   [2]sentiment treebank | [3]help the model | [4]source code

   deeply moving: deep learning for id31

   this website provides a [5]live demo for predicting the sentiment of
   movie reviews. most sentiment prediction systems work just by looking
   at words in isolation, giving positive points for positive words and
   negative points for negative words and then summing up these points.
   that way, the order of words is ignored and important information is
   lost. in constrast, our new deep learning model actually builds up a
   representation of whole sentences based on the sentence structure. it
   computes the sentiment based on how words compose the meaning of longer
   phrases. this way, the model is not as easily fooled as previous
   models. for example, our model learned that funny and witty are
   positive but the following sentence is still negative overall:

   this movie was actually neither that funny, nor super witty.

   the underlying technology of this demo is based on a new type of
   id56 that builds on top of grammatical structures.
   you can also browse the [6]stanford sentiment treebank, the dataset on
   which this model was trained. the model and dataset are described in an
   upcoming [7]emnlp paper. of course, no model is perfect. you can help
   the model learn even more by [8]labeling sentences we think would help
   the model or those you try in the live demo.

   paper title and abstract

   recursive deep models for semantic compositionality over a sentiment
   treebank semantic word spaces have been very useful but cannot express
   the meaning of longer phrases in a principled way. further progress
   towards understanding compositionality in tasks such as sentiment
   detection requires richer supervised training and evaluation resources
   and more powerful models of composition. to remedy this, we introduce a
   sentiment treebank. it includes fine grained sentiment labels for
   215,154 phrases in the parse trees of 11,855 sentences and presents new
   challenges for sentiment compositionality. to address them, we
   introduce the recursive neural tensor network. when trained on the new
   treebank, this model outperforms all previous methods on several
   metrics. it pushes the state of the art in single sentence
   positive/negative classification from 80% up to 85.4%. the accuracy of
   predicting fine-grained sentiment labels for all phrases reaches 80.7%,
   an improvement of 9.7% over bag of features baselines. lastly, it is
   the only model that can accurately capture the effect of contrastive
   conjunctions as well as negation and its scope at various tree levels
   for both positive and negative phrases.

   [9]test the recursive neural tensor network in a live demo   

   [10]explore the sentiment treebank   

   [11]help the recursive neural tensor network improve by labeling   

   [12]source code page   

   paper: [13]download pdf

   [14]richard socher, [15]alex perelygin, [16]jean wu, [17]jason chuang,
   [18]christopher manning, [19]andrew ng and [20]christopher potts

   recursive deep models for semantic compositionality over a sentiment
   treebank

   conference on empirical methods in natural language processing (emnlp
   2013)

   dataset downloads:

   [21]main zip file with readme (6mb)
   [22]dataset raw counts (5mb) [23]train,dev,test splits in ptb tree
   format

   code: [24]download page

   press: [25]stanford press release

   dataset visualization and web design by jason chuang. live demo by jean
   wu, richard socher, rukmani ravisundaram and tayyab tariq.

   this webpage requires one of the following web browsers:

   [26]chrome [27]safari [28]firefox [29]opera

   [30]tweet
   please enable javascript to view the [31]comments powered by disqus.
   [32]comments powered by disqus
   updated in august 2013    |    [33]comments

references

   1. http://nlp.stanford.edu:8080/sentiment/rntndemo.html
   2. https://nlp.stanford.edu/sentiment/treebank.html
   3. http://nlp.stanford.edu:8080/sentiment/labeling.html
   4. https://nlp.stanford.edu/sentiment/code.html
   5. http://nlp.stanford.edu:8080/sentiment/rntndemo.html
   6. https://nlp.stanford.edu/sentiment/treebank.html
   7. http://nlp.stanford.edu/~socherr/emnlp2013_rntn.pdf
   8. http://nlp.stanford.edu:8080/sentiment/labeling.html
   9. http://nlp.stanford.edu:8080/sentiment/rntndemo.html
  10. https://nlp.stanford.edu/sentiment/treebank.html
  11. http://nlp.stanford.edu:8080/sentiment/labeling.html
  12. https://nlp.stanford.edu/sentiment/code.html
  13. http://nlp.stanford.edu/~socherr/emnlp2013_rntn.pdf
  14. http://www.socher.org/
  15. http://nlp.stanford.edu/~aperelyg/
  16. http://www.stanford.edu/~jeaneis/
  17. http://jason.chuang.info/
  18. http://nlp.stanford.edu/manning/
  19. http://cs.stanford.edu/people/ang/
  20. http://www.stanford.edu/~cgpotts/
  21. http://nlp.stanford.edu/~socherr/stanfordsentimenttreebank.zip
  22. http://nlp.stanford.edu/~socherr/stanfordsentimenttreebankraw.zip
  23. https://nlp.stanford.edu/sentiment/traindevtesttrees_ptb.zip
  24. https://nlp.stanford.edu/sentiment/code.html
  25. http://engineering.stanford.edu/news/stanford-algorithm-analyzes-sentence-sentiment-advances-machine-learning
  26. http://www.google.com/chrome/
  27. http://www.apple.com/safari/
  28. http://www.mozilla.org/firefox/new/
  29. http://www.opera.com/download/
  30. https://twitter.com/share
  31. http://disqus.com/?ref_noscript
  32. http://disqus.com/
  33. mailto:sentiment.trees@gmail.com?subject=comments
