   (button) toggle navigation [1]colah's blog
     * [2]blog
     * [3]about
     * [4]contact

visualizing mnist: an exploration of id84

   posted on october 9, 2014

   [5]mnist, [6]data visualization, [7]machine learning, [8]word
   embeddings, [9]neural networks, [10]deep learning

   at some fundamental level, no one understands machine learning.

   it isn   t a matter of things being too complicated. almost everything we
   do is fundamentally very simple. unfortunately, an innate human
   handicap interferes with us understanding these simple things.

   humans evolved to reason fluidly about two and three dimensions. with
   some effort, we may think in four dimensions. machine learning often
   demands we work with thousands of dimensions     or tens of thousands, or
   millions! even very simple things become hard to understand when you do
   them in very high numbers of dimensions.

   reasoning directly about these high dimensional spaces is just short of
   hopeless.

   as is often the case when humans can   t directly do something, we   ve
   built tools to help us. there is an entire, well-developed field,
   called id84, which explores techniques for
   translating high-dimensional data into lower dimensional data. much
   work has also been done on the closely related subject of visualizing
   high dimensional data.

   these techniques are the basic building blocks we will need if we wish
   to visualize machine learning, and deep learning specifically. my hope
   is that, through visualization and observing more directly what is
   actually happening, we can understand neural networks in a much deeper
   and more direct way.

   and so, the first thing on our agenda is to familiarize ourselves with
   id84. to do that, we   re going to need a dataset to
   test these techniques on.

mnist

   mnist is a simple id161 dataset. it consists of 28x28 pixel
   images of handwritten digits, such as:

   every mnist data point, every image, can be thought of as an array of
   numbers describing how dark each pixel is. for example, we might think
   of \(\mnist[1]{1}\) as something like:

   since each image has 28 by 28 pixels, we get a 28x28 array. we can
   flatten each array into a \(28*28 = 784\) dimensional vector. each
   component of the vector is a value between zero and one describing the
   intensity of the pixel. thus, we generally think of mnist as being a
   collection of 784-dimensional vectors.

   not all vectors in this 784-dimensional space are mnist digits. typical
   points in this space are very different! to get a sense of what a
   typical point looks like, we can randomly pick a few points and examine
   them. in a random point     a random 28x28 image     each pixel is randomly
   black, white or some shade of gray. the result is that random points
   look like noise.

   images like mnist digits are very rare. while the mnist data points are
   embedded in 784-dimensional space, they live in a very small subspace.
   with some slightly harder arguments, we can see that they occupy a
   lower dimensional subspace.

   people have lots of theories about what sort of lower dimensional
   structure mnist, and similar data, have. one popular theory among
   machine learning researchers is the manifold hypothesis: mnist is a low
   dimensional manifold, sweeping and curving through its high-dimensional
   embedding space. another hypothesis, more associated with topological
   data analysis, is that data like mnist consists of blobs with
   tentacle-like protrusions sticking out into the surrounding space.

   but no one really knows, so lets explore!

the mnist cube

   we can think of the mnist data points as points suspended in a
   784-dimensional cube. each dimension of the cube corresponds to a
   particular pixel. the data points range from zero to one according to
   the pixels intensity. on one side of the dimension, there are images
   where that pixel is white. on the other side of the dimension, there
   are images where it is black. in between, there are images where it is
   gray.
   [mnist-p1815-1.png] [mnist-p1815-2.png] [mnist-p1815-3.png]
   [mnist-p1815-4.png]

   if we think of it this way, a natural question occurs. what does the
   cube look like if we look at a particular two-dimensional face? like
   staring into a snow-globe, we see the data points projected into two
   dimensions, with one dimension corresponding to the intensity of a
   particular pixel, and the other corresponding to the intensity of a
   second pixel. examining this allows us to explore mnist in a very raw
   way.

   in this visualization, each dot is an mnist data point. the dots are
   colored based on which class of digit the data point belongs to. when
   your mouse hovers over a dot, the image for that data point is
   displayed on each axis. each axis corresponds to the intensity of a
   particular pixel, as labeled and visualized as a blue dot in the small
   image beside it. by clicking on the image, you can change which pixel
   is displayed on that axis.

   exploring this visualization, we can see some glimpses of the structure
   of mnist. looking at the [11]pixels \(p_{18,16}\) and \(p_{7,12}\), we
   are able to separate a lot of zeros to the bottom right and a lot of
   nines to the top left. looking at [12]pixels \(p_{5,6}\) and
   \(p_{7,9}\) we can see a lot of twos at the top right and threes at the
   bottom right.

   despite minor successes like these, one can   t really can   t understand
   mnist this way. the small insights one gains feel very fragile and feel
   a lot like luck. the truth is, simply, that very little of mnist   s
   structure is visible from these perspectives. you can   t understand
   images by looking at just two pixels at a time.

   but there   s lots of other perspectives we could look at mnist from! in
   these perspectives, instead of looking a face straight on, one looks at
   it from an angle.

   the challenge is that we need to choose what perspective we want to
   use. what angle do we want to look at it from horizontally? what angle
   do we want to look at it from vertically? thankfully, there   s a
   technique called [13]principal components analysis (pca) that will find
   the best possible angle for us. by this, we mean that pca will find the
   angle that spreads out the points the most (captures the most variance
   possible).

   but, what does it even mean to look at a 784-dimensional cube from an
   angle? well, we need to decide which direction every axis of the cube
   should be tilted: to one side, to the other, or somewhere in between?

   to be concrete, the following are pictures of the two angles pca
   chooses. red represents tilting a pixel   s dimension to one side, blue
   to the other.
   [mnist-pca1-1.png] [mnist-pca1.png] [mnist-pca2.png]

   if an mnist digit primarily highlights red, it ends up on one side. if
   it highlights blue, it ends up on a different side. the first angle    
   the    first principal component        will be our horizontal angle, pushing
   ones (which highlight lots of red and little blue) to the left and
   zeros (which highlight lots of blue and little red) to the right.
   [mnist-pca1-1.png] [mnist-pca1-2.png] [mnist-pca1-3.png]
   [mnist-pca1-4.png]

   now that we know what the best horizontal and vertical angle are, we
   can try to look at the cube from that perspective.

   this visualization is much like the one above, but now the axes are
   fixed to displaying the first and second    principal components,   
   basically angles of looking at the data. in the image on each axis,
   blue and red are used to denote what the    tilt    is for that pixel.
   pixel intensity in blue regions pushes a data point to one side, pixel
   intensity in red regions pushes us to the other.
   visualizing mnist with pca

   while much better than before, it   s still not terribly good.
   unfortunately, even looking at the data from the best angle, mnist data
   doesn   t line up nicely for us to look at. it   s a non-trivial
   high-dimensional structure, and these sorts of linear projections just
   aren   t going to cut it.

   thankfully, we have some powerful tools for dealing with datasets which
   are    uncooperative.

optimization-based id84

   what would we consider a success? what would it mean to have the
      perfect    visualization of mnist? what should our goal be?

   one really nice property would be if the distances between points in
   our visualization were the same as the distances between points in the
   original space. if that was true, we   d be capturing the global geometry
   of the data.

   let   s be a bit more precise. for any two mnist data points, \(x_i\) and
   \(x_j\), there are two notions of distance between them. one is the
   distance between them in the original space[14]^1 and one is the
   distance between them in our visualization. we will use \(d^*_{i,j}\)
   to denote the distance between \(x_i\) and \(x_j\) in the original
   space and \(d_{i,j}\) to denote the distance between \(x_i\) and
   \(x_j\) in our visualization. now we can define a cost:

   \[c = \sum_{i\neq j} ~(d^{*}_{i,j} - d_{i,j})^2\]

   this value describes how bad a visualization is. it basically says:
      it   s bad for distances to not be the same. in fact, it   s quadratically
   bad.    if it   s high, it means that distances are dissimilar to the
   original space. if it   s small, it means they are similar. if it is
   zero, we have a    perfect    embedding.

   that sounds like an optimization problem! and deep learning researchers
   know what to do with those! we pick a random starting point and apply
   [15]id119. [16]^2
   visualizing mnist with mds

   this technique is called [17]multidimensional scaling (or mds). if you
   like, there   s a more physical description of what   s going on. first, we
   randomly position each point on a plane. next we connect each pair of
   points with a spring with the length of the original distance,
   \(d^{*}_{i,j}\). then we let the points move freely and allow physics
   to take its course!

   we don   t reach a cost of zero, of course. generally, high-dimensional
   structures can   t be embedded in two dimensions in a way that preserves
   distances perfectly. we   re demanding the impossible! but, even though
   we don   t get a perfect answer, we do improve a lot on the original
   random embedding, and come to a decent visualization. we can see the
   different classes begin to separate, especially the ones.

sammon   s mapping

   still, it seems like we should be able to do much better. perhaps we
   should consider different cost functions? there   s a huge space of
   possibilities. to start, there   s a lot of variations on mds. a common
   theme is cost functions emphasizing local structure as more important
   to maintain than global structure. a very simple example of this is
   [18]sammon   s mapping, defined by the cost function:

   \[c = \sum_{i\neq j} \frac{(d^{*}_{i,j} - d_{i,j})^2}{d^{*}_{i,j}}\]

   in sammon   s mapping, we try harder to preserve the distances between
   nearby points than between those which are far apart. if two points are
   twice as close in the original space as two others, it is twice as
   important to maintain the distance between them.
   visualizing mnist with sammon   s mapping

   for mnist, the result isn   t that different. the reason has to do with a
   rather unintuitive property regarding distances in high-dimensional
   data like mnist. let   s consider the distances between some mnist
   digits. for example, the distance between the similar ones,
   \(\mnist{6}\) and \(\mnist{8}\), is \[d(\mnist{6}, \mnist{8}) = 4.53\]
   on the other hand, the difference between the very different data
   points, \(\mnist{4}\) and \(\mnist{12}\), is \[d(\mnist{4}, \mnist{12})
   = 12.0\] less than three times \(d(\mnist{6}, \mnist{8})\)!

   because there   s so many ways similar points can be slightly different,
   the average distance between similar points is quite high. conversely,
   as you get further away from a point, the amount of volume within that
   distance increases to an extremely high power, and so you are likely to
   run into different kinds of points. the result is that, in pixel space,
   the difference in distances between    similar    and    different    points
   can be much less than we   d like, even in good cases.

graph based visualization

   perhaps, if local behavior is what we want our embedding to preserve,
   we should optimize for that more explicitly.

   consider a [19]nearest neighbor graph of mnist. for example, consider a
   graph \((v,e)\) where the nodes are mnist data points, and each point
   is connected to the three points that are closest to it in the original
   space.[20]^3 this graph is a simple way to encode local structure and
   forget about everything else.

   given such a graph, we can use standard graph layout algorithms to
   visualize mnist. here, we will use [21]force-directed graph drawing: we
   pretend that all points are repelling charged particles, and that the
   edges are springs. this gives us a cost function:

   \[c~ = ~\sum_{i\neq j}\frac{1}{d_{i,j}} ~+~ \frac{1}{2}\sum_{(i,j) \in
   e} (d_{i,j} - d^{*}_{i,j})^2\]

   which we minimize.
   visualizing mnist as a graph

   the graph discovers a lot of structure in mnist. in particular, it
   seems to find the different mnist classes. while they overlap, during
   the graph layout optimization we can see the clusters sliding over each
   other. they are unable to avoid overlapping when embedded on the plane
   due to connections between classes, but the cost function is at least
   trying to separate them.

   one nice property of the graph visualization is that it explicitly
   shows us which points are connected to which other points. in earlier
   visualizations, if we see a point in a strange place, we are uncertain
   as to whether it   s just stuck there, or if it should actually be there.
   the graph structure avoids this. for example, if you look at the red
   cluster of zeros, you will see a single blue point, the six
   \(\mnist{494}\), among them. you can see from its neighbors that it is
   supposed to be there, and from looking at it you can see that it is, in
   fact, a very poorly written six that looks more like a zero.

t-distributed stochastic neighbor embedding

   the final technique i wish to introduce is the [22]t-distributed
   stochastic neighbor embedding (id167). this technique is extremely
   popular in the deep learning community. unfortunately, id167   s cost
   function involves some non-trivial mathematical machinery and requires
   some significant effort to understand.

   but, roughly, what id167 tries to optimize for is preserving the
   topology of the data. for every point, it constructs a notion of which
   other points are its    neighbors,    trying to make all points have the
   same number of neighbors. then it tries to embed them so that those
   points all have the same number of neighbors.

   in some ways, id167 is a lot like the graph based visualization. but
   instead of just having points be neighbors (if there   s an edge) or not
   neighbors (if there isn   t an edge), id167 has a continuous spectrum of
   having points be neighbors to different extents.

   id167 is often very successful at revealing clusters and subclusters in
   data.
   visualizing mnist with id167

   id167 does an impressive job finding clusters and subclusters in the
   data, but is prone to getting stuck in local minima. for example, in
   the following image we can see two clusters of zeros (red) that fail to
   come together because a cluster of sixes (blue) get stuck between them.
   [tsne-localmin-1.png]

   a number of tricks can help us avoid these bad local minima. firstly,
   using more data helps a lot. because these visualizations are embeded
   in a blog post, they only use 1,000 points. using the full 50,000 mnist
   points works a lot better. in addition, it is recommended that one use
   [23]simulated annealing and carefully select a number of
   hyperparamters.

   well done id167 plots reveal many interesting features of mnist.
   a id167 plot of mnist

   an even nicer plot can be found on the page labeled 2590, in the
   original id167 paper, [24]maaten & hinton (2008).

   it   s not just the classes that id167 finds. let   s look more closely at
   the ones.
   a id167 plot of mnist ones

   the ones cluster is stretched horizontally. as we look at digits from
   left to right, we see a consistent pattern.

   \[\mnist[1]{7} \to \mnist[1]{4} \to \mnist[1]{8} \to \mnist[1]{6} \to
   \mnist[1]{2} \to \mnist[1]{1}\]

   they move from forward leaning ones, like \(\mnist[1]{4}\), into
   straighter like \(\mnist[1]{6}\), and finally to slightly backwards
   leaning ones, like \(\mnist[1]{1}\). it seems that in mnist, the
   primary factor of variation in the ones is tilting. this is likely
   because mnist normalizes digits in a number of ways, centering and
   scaling them. after that, the easiest way to be    far apart    is to
   rotate and not overlap very much.

   similar structure can be observed in other classes, if you look at the
   [25]id167 plot again.

visualization in three dimensions

   watching these visualizations, there   s sometimes this sense that
   they   re begging for another dimension. for example, watching the graph
   visualization optimize, one can see clusters slide over top of each
   other.

   really, we   re trying to compress this extremely high-dimensional
   structure into two dimensions. it seems natural to think that there
   would be very big wins from adding an additional dimension. if nothing
   else, at least in three dimensions a line connecting two clusters
   doesn   t divide the plane, precluding other connections between
   clusters.

   in the following visualization, we construct a nearest neighbor graph
   of mnist, as before, and optimize the same cost function. the only
   difference is that there are now three dimensions to lay it out in.
   visualizing mnist as a graph in 3d
   (click and drag to rotate)

   the three dimensional version, unsurprisingly, works much better. the
   clusters are quite separated and, while entangled, no longer overlap.

   in this visualization, we can begin to see why it is easy to achieve
   around 95% accuracy classifying mnist digits, but quickly becomes
   harder after that. you can make a lot of ground classifying digits by
   chopping off the colored protrusions above, the clusters of each class
   sticking out. (this is more or less what a linear support vector
   machine does.[26]^4) but there   s some much harder entangled sections,
   especially in the middle, that are difficult to classify.

   of course, we could do any of the above techniques in 3d! even
   something as simple as mds is able to display quite a bit in 3d.
   visualizing mnist with mds in 3d
   (click and drag to rotate)

   in three dimensions, mds does a much better job separating the classes
   than it did with two dimensions.

   and, of course, we can do id167 in three dimensions.
   visualizing mnist with id167 in 3d
   (click and drag to rotate)

   because id167 puts so much space between clusters, it benefits a lot
   less from the transition to three dimensions. it   s still quite nice,
   though, and becomes much more so with more points.

   if you want to visualize high dimensional data, there are, indeed,
   significant gains to doing it in three dimensions over two.

conclusion

   id84 is a well developed area, and we   re only
   scratching the surface here. there are hundreds of techniques and
   variants that are unmentioned here. i encourage you to explore!

   it   s easy to slip into a mind set of thinking one of these techniques
   is better than the others, but i think they   re all complementary.
   there   s no way to map high-dimensional data into low dimensions and
   preserve all the structure. so, an approach must make trade-offs,
   sacrificing one property to preserve another. pca tries to preserve
   linear structure, mds tries to preserve global geometry, and id167
   tries to preserve topology (neighborhood structure).

   these techniques give us a way to gain traction on understanding
   high-dimensional data. while directly trying to understand
   high-dimensional data with the human mind is all but hopeless, with
   these tools we can begin to make progress.

   in the next post, we will explore applying these techniques to some
   different kinds of data     in particular, to visualizing representations
   of text. then, equipped with these techniques, we will shift our focus
   to understanding neural networks themselves, visualizing how they
   transform high-dimensional data and building techniques to visualize
   the space of neural networks. if you   re interested, you can subscribe
   to my [27]rss feed so that you   ll see these posts when they are
   published.

   (i would be delighted to hear your comments and thoughts: you can
   comment inline or at the end. for typos, technical errors, or
   clarifications you would like to see added, you are encouraged to make
   a pull request on [28]github)

acknowledgements

   i   m grateful for the hospitality of google   s deep learning research
   group, which had me as an intern while i wrote this post and did the
   work it is based on. i   m especially grateful to my internship host,
   jeff dean.

   i was greatly helped by the comments, advice, and encouragement of many
   googlers, both in the deep learning group and outside of it. these
   include: greg corrado, jon shlens, matthieu devin, andrew dai, quoc le,
   anelia angelova, oriol vinyals, ilya sutskever, ian goodfellow, jutta
   degener, and anna goldie.

   i was strongly influenced by the thoughts, comments and notes of
   michael nielsen, especially his notes on bret victor   s work. michael   s
   thoughts persuaded me that i should think seriously about interactive
   visualizations for understanding deep learning.

   i was also helped by the support of a number of non-googler friends,
   including yoshua bengio, dario amodei, eliana lorch, taren
   stinebrickner-kauffman, and laura ball.

   this blog post was made possible by a number of wonderful javascript
   libraries, including [29]d3.js, [30]mathjax, [31]jquery, and
   [32]three.js. a big thank you to everyone who contributed to these
   libraries.
     __________________________________________________________________

    1. we have a number of options for defining distance between these
       high-dimensional vectors. for this post, we will use l2 distance,
       \(d(x_i,x_j) = \sqrt{\sum_n (x_{i,n}-x_{j,n})^2}\)  [33]   
    2. we initialize the points    positions by sampling a gaussian around
       the origin. our optimization process isn   t standard gradient
       descent. instead, we use a variant of momentum id119.
       before adding the gradient to the momentum, we normalize the
       gradient. this reduces the need for hyper-parameter tuning.  [34]   
    3. note that points can end up connected to more, if they are the
       nearest neighbor of many points.  [35]   
    4. this isn   t quite true. a linear id166 operates on the original space.
       this is a non-linear transformation of the original space. that
       said, this strongly suggests something similar in the original
       space, and so we   d expect something similar to be true.  [36]   

   subscribe to the [37]rss feed. built by [38]oinkina with [39]hakyll
   using [40]bootstrap, [41]mathjax, and [42]disqus.

   enable javascript for footnotes, disqus comments, and other cool stuff.

references

   1. http://colah.github.io/
   2. http://colah.github.io/
   3. http://colah.github.io/about.html
   4. http://colah.github.io/contact.html
   5. http://colah.github.io/posts/tags/mnist.html
   6. http://colah.github.io/posts/tags/data_visualization.html
   7. http://colah.github.io/posts/tags/machine_learning.html
   8. http://colah.github.io/posts/tags/word_embeddings.html
   9. http://colah.github.io/posts/tags/neural_networks.html
  10. http://colah.github.io/posts/tags/deep_learning.html
  11. http://colah.github.io/posts/2014-10-visualizing-mnist/#raw_mnist
  12. http://colah.github.io/posts/2014-10-visualizing-mnist/#raw_mnist
  13. http://en.wikipedia.org/wiki/principal_component_analysis
  14. http://colah.github.io/posts/2014-10-visualizing-mnist/#fn1
  15. http://en.wikipedia.org/wiki/gradient_descent
  16. http://colah.github.io/posts/2014-10-visualizing-mnist/#fn2
  17. http://en.wikipedia.org/wiki/multidimensional_scaling
  18. http://en.wikipedia.org/wiki/sammon_mapping
  19. http://en.wikipedia.org/wiki/nearest_neighbor_graph
  20. http://colah.github.io/posts/2014-10-visualizing-mnist/#fn3
  21. http://en.wikipedia.org/wiki/force-directed_graph_drawing
  22. http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  23. http://en.wikipedia.org/wiki/simulated_annealing
  24. http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  25. http://colah.github.io/posts/2014-10-visualizing-mnist/#tsne_mnist_nice
  26. http://colah.github.io/posts/2014-10-visualizing-mnist/#fn4
  27. http://colah.github.io/rss.xml
  28. https://github.com/colah/visualizing-deep-learning/
  29. http://d3js.org/
  30. http://www.mathjax.org/
  31. http://jquery.com/
  32. http://threejs.org/
  33. http://colah.github.io/posts/2014-10-visualizing-mnist/#fnref1
  34. http://colah.github.io/posts/2014-10-visualizing-mnist/#fnref2
  35. http://colah.github.io/posts/2014-10-visualizing-mnist/#fnref3
  36. http://colah.github.io/posts/2014-10-visualizing-mnist/#fnref4
  37. http://colah.github.io/rss.xml
  38. https://github.com/oinkina
  39. http://jaspervdj.be/hakyll
  40. http://getbootstrap.com/
  41. http://www.mathjax.org/
  42. http://disqus.com/
