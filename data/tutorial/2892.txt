   #[1]blog.dbrgn.ch full atom feed

   [2]blog.dbrgn.ch
     * [3]home
     * [4]archive
     * [5]tags
     * [6]about this blog
     * [7]about me

programming a id88 in python

   written on tuesday, march 26, 2013 by [8]danilo bargen

   [9]my profile on google+

   at [10]hsr, i'm currently enrolled in a course about neural networks
   and machine learning. one of the simplest forms of a neural network
   model is the [11]id88.

background information

   a id88 classifier is a simple model of a neuron. it has different
   inputs ($x_1$...$x_n$) with different weights ($w_1$...$w_n$).
   $$s = \sum_{i=0}^n w_i \cdot x_i$$

   the weighted sum $s$ of these inputs is then passed through a step
   function $f$ (usually a [12]heaviside step function).
   $$f(s) = \begin{cases} 1 & \textrm{if } s \ge 0 \\ 0 &
   \textrm{otherwise} \end{cases}$$

   to make things cleaner, here's a little diagram:
   the mathematical model of a id88.

python!

   here's a simple version of such a id88 using python and
   [13]numpy. it will take two inputs and learn to act like the logical or
   function. first, let's import some libraries we need:
from random import choice
from numpy import array, dot, random

   then let's create the step function. [14]in reference to mathematica,
   i'll call this function unit_step.
unit_step = lambda x: 0 if x < 0 else 1

   next we need to map the possible input to the expected output. the
   first two entries of the numpy array in each tuple are the two input
   values. the second element of the tuple is the expected result. and the
   third entry of the array is a "dummy" input (also called the bias)
   which is needed to move the threshold (also known as the decision
   boundary) up or down as needed by the step function. its value is
   always 1, so that its influence on the result can be controlled by its
   weight.
training_data = [
    (array([0,0,1]), 0),
    (array([0,1,1]), 1),
    (array([1,0,1]), 1),
    (array([1,1,1]), 1),
]

   as you can see, this training sequence maps exactly to the definition
   of the or function:
   a b a or b
   0 0 0
   0 1 1
   1 0 1
   1 1 1

   next we'll choose three random numbers between 0 and 1 as the initial
   weights:
w = random.rand(3)

   now on to some variable initializations. the errors list is only used
   to store the error values so that they can be plotted later on. if you
   don't want to do any plotting, you can just leave it away. the eta
   variable controls the learning rate. and n specifies the number of
   learning iterations.
errors = []
eta = 0.2
n = 100

   in order to find the ideal values for the weights w, we try to reduce
   the error magnitude to zero. in this simple case n = 100 iterations are
   enough; for a bigger and possibly "noisier" set of input data much
   larger numbers should be used.

   first we get a random input set from the training data. then we
   calculate the dot product (sometimes also called scalar product or
   inner product) of the input and weight vectors. this is our (scalar)
   result, which we can compare to the expected value. if the expected
   value is bigger, we need to increase the weights, if it's smaller, we
   need to decrease them. this correction factor is calculated in the last
   line, where the error is multiplied with the learning rate (eta) and
   the input vector (x). it is then added to the weights vector, in order
   to improve the results in the next iteration.
for i in xrange(n):
    x, expected = choice(training_data)
    result = dot(w, x)
    error = expected - unit_step(result)
    errors.append(error)
    w += eta * error * x

   and that's already everything we need in order to train the id88!
   it has now "learned" to act like a logical or function:
for x, _ in training_data:
    result = dot(x, w)
    print("{}: {} -> {}".format(x[:2], result, unit_step(result)))

[0 0]: -0.0714566687173 -> 0
[0 1]: 0.829739696273 -> 1
[1 0]: 0.345454042997 -> 1
[1 1]: 1.24665040799 -> 1

   if you're interested, you can also plot the errors, which is a great
   way to visualize the learning process:
from pylab import plot, ylim
ylim([-1,1])
plot(errors)

   it's easy to see that the errors stabilize around the 60th iteration.
   if you doubt that the errors are definitely eliminated, you can re-run
   the training with an iteration count of 500 or more and plot the
   errors:

   you could also try to change the training sequence in order to model an
   and, nor or not function. note that it's not possible to model an xor
   function using a single id88 like this, because the two classes
   (0 and 1) of an xor function are not linearly separable. in that case
   you would have to use multiple layers of id88s (which is
   basically a small neural network).

wrap up

   here's the entire code:
from random import choice
from numpy import array, dot, random

unit_step = lambda x: 0 if x < 0 else 1

training_data = [
    (array([0,0,1]), 0),
    (array([0,1,1]), 1),
    (array([1,0,1]), 1),
    (array([1,1,1]), 1),
]

w = random.rand(3)
errors = []
eta = 0.2
n = 100

for i in xrange(n):
    x, expected = choice(training_data)
    result = dot(w, x)
    error = expected - unit_step(result)
    errors.append(error)
    w += eta * error * x

for x, _ in training_data:
    result = dot(x, w)
    print("{}: {} -> {}".format(x[:2], result, unit_step(result)))

   if you have any questions, or if you've discovered an error (which is
   easily possible as i've just learned about this stuff), feel free to
   leave a comment below.

   this entry was tagged [15]ai, [16]machine_learning and [17]python
   please enable javascript to view the comments.

references

   1. https://blog.dbrgn.ch/feed.atom
   2. https://blog.dbrgn.ch/
   3. https://blog.dbrgn.ch/
   4. https://blog.dbrgn.ch/archive/
   5. https://blog.dbrgn.ch/tags/
   6. https://blog.dbrgn.ch/blog/
   7. https://blog.dbrgn.ch/about/
   8. https://blog.dbrgn.ch/about/
   9. https://plus.google.com/107821756387719288540/?rel=author
  10. http://www.hsr.ch/
  11. http://reference.wolfram.com/applications/neuralnetworks/neuralnetworktheory/2.4.0.html
  12. http://en.wikipedia.org/wiki/heaviside_step_function
  13. http://www.numpy.org/
  14. http://reference.wolfram.com/mathematica/ref/unitstep.html
  15. https://blog.dbrgn.ch/tags/ai/
  16. https://blog.dbrgn.ch/tags/machine-learning/
  17. https://blog.dbrgn.ch/tags/python/
