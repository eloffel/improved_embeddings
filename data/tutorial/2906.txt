   #[1]rare technologies    feed [2]rare technologies    comments feed
   [3]rare technologies    deep learning with id97 and gensim comments
   feed [4]alternate [5]alternate

   [tr?id=1761346240851963&ev=pageview&noscript=1]

   iframe: [6]https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld

   [7]pragmatic machine learning rare technologies [8]navigation

     * [9]services
     * [10]products
          + [11]pii tools
          + [12]scaletext
     * [13]corporate training
          + [14]overview
          + [15]python best practices
          + [16]practical machine learning
          + [17]topic modelling
          + [18]deep learning in practice
     * [19]for students
          + [20]open source
          + [21]incubator
          + [22]competitions
     * [23]company
          + [24]careers
          + [25]our team
     * [26]blog
     * [27]contact
     * [28]search

     * [29]services
     * [30]products
          + [31]pii tools
          + [32]scaletext
     * [33]corporate training
          + [34]overview
          + [35]python best practices
          + [36]practical machine learning
          + [37]topic modelling
          + [38]deep learning in practice
     * [39]for students
          + [40]open source
          + [41]incubator
          + [42]competitions
     * [43]company
          + [44]careers
          + [45]our team
     * [46]blog
     * [47]contact
     * [48]search

deep learning with id97 and gensim

   [49]radim   eh    ek 2013-09-17[50] gensim, [51]programming[52] 33
   comments

   neural networks have been a bit of a punching bag historically: neither
   particularly fast, nor robust or accurate, nor open to introspection by
   humans curious to gain insights from them. but things have been
   changing lately, with [53]deep learning becoming a hot topic in
   academia with spectacular results. i decided to check out one deep
   learning algorithm via [54]gensim.

id97: the good, the bad (and the fast)

   the kind folks at google have recently published several new
   unsupervised, deep learning algorithms in [55]this article.

   selling point:    our model can answer the query    give me a word like
   king, like woman, but unlike man    with    queen   . pretty cool.

   not only do these algorithms boast great performance, accuracy and a
   theoretically-not-so-well-founded-but-pragmatically-superior-model (all
   three solid plusses in my book), but they were also devised by my
   fellow country and county-man, tom     mikolov from brno! the googlers
   have also released an [56]open source implementation of these
   algorithms, which always helps with uptake of fresh academic ideas.
   brilliant.

   although, in words of id97   s authors, the toolkit is meant for
      research purposes   , it   s actually optimized c, down to cache
   alignments, memory look-up tables, static memory allocations and a
   penchant for single letter variable names. somebody obviously spent
   time profiling this, which is good news for people running it, and bad
   news for people wanting to understand it, extend it or integrate it (as
   researchers are wont to do).

   in short, the spirit of id97 fits gensim   s tagline of [57]topic
   modelling for humans, but the actual code doesn   t, tight and beautiful
   as it is. i therefore decided to reimplement id97 in gensim,
   starting with the hierarchical softmax skip-gram model, because that   s
   the one with the best reported accuracy. i reimplemented it from
   scratch, de-obfuscating id97 into a less menial state. no need for
   a custom implementation of hashing, lists, dicts, random number
   generators    all of these come built-in with python.

   free, fast, pretty     pick any two. as the ratio of clever code to
   comments shrank and shrank (down to ~100 python lines, with 40% of them
   comments), so did the performance. about 1000x. yuck. i rewrote the
   explicit python loops in numpy, speeding things up ~50x (yay), but that
   means it   s still ~20x slower than the original (ouch). i could optimize
   it further, using cython and whatnot, but that would lead back to
   obfuscation, beating the purpose of this exercise. i may still do it
   anyway, for selected hotspots. edit: done, see [58]part ii: optimizing
   id97 in python     performance of the python port is now on par with
   the c code, and sometimes even faster.

   for now, the code lives [59]in a git branch, to be merged into gensim
   proper once i   m happy with its functionality and performance. in the
   meanwhile, the gensim version is already good enough to be unleashed on
   reasonably-sized corpora, taking on natural language processing tasks
      the python way   . edit: done, merged into gensim release 0.8.8.
   [60]installation instructions.

so, what can it do?

   id65 goodness; see [61]here and the [62]original
   article for more background. basically, the algorithm takes some
   unstructured text and learns    features    about each word. the neat thing
   is (apart from it learning the features completely automatically,
   without any human input/supervision!) that these features capture
   different relationships     both semantic and syntactic. this allows some
   (very basic) algebraic operations, like the above mentioned
      king   man+woman=queen   . more concretely:

>>> # import modules and set up logging
>>> from gensim.models import id97
>>> import logging
>>> logging.basicconfig(format='%(asctime)s : %(levelname)s : %(message)s', leve
l=logging.info)
>>> # load up unzipped corpus from http://mattmahoney.net/dc/text8.zip
>>> sentences = id97.text8corpus('/tmp/text8')
>>> # train the skip-gram model; default window=5
>>> model = id97.id97(sentences, size=200)
>>> # ... and some hours later... just as advertised...
>>> model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
[('queen', 0.5359965)]

>>> # pickle the entire model to disk, so we can load&resume training later
>>> model.save('/tmp/text8.model')
>>> # store the learned weights, in a format the original c tool understands
>>> model.save_id97_format('/tmp/text8.model.bin', binary=true)
>>> # or, import word weights created by the (faster) c id97
>>> # this way, you can switch between the c/python toolkits easily
>>> model = id97.id97.load_id97_format('/tmp/vectors.bin', binary=tr
ue)

>>> # "boy" is to "father" as "girl" is to ...?
>>> model.most_similar(['girl', 'father'], ['boy'], topn=3)
[('mother', 0.61849487), ('wife', 0.57972813), ('daughter', 0.56296098)]
>>> more_examples = ["he his she", "big bigger bad", "going went being"]
>>> for example in more_examples:
...     a, b, x = example.split()
...     predicted = model.most_similar([x, b], [a])[0][0]
...     print "'%s' is to '%s' as '%s' is to '%s'" % (a, b, x, predicted)
'he' is to 'his' as 'she' is to 'her'
'big' is to 'bigger' as 'bad' is to 'worse'
'going' is to 'went' as 'being' is to 'was'

>>> # which word doesn't go with the others?
>>> model.doesnt_match("breakfast cereal dinner lunch".split())
'cereal'

   this already beats the english of some of my friends     

python, sweet home

   note from radim: get my latest machine learning tips & articles
   delivered straight to your inbox (it's free).
   ____________________ ____________________

    unsubscribe anytime, no spamming. max 2 posts per month, if lucky.
   subscribe now
   ____________________

   having deep learning available in python allows us to plug in the
   multitude of nlp tools available in python. more intelligent
   id121/sentence splitting, id39? just use
   [63]nltk. web crawling, lemmatization? try [64]pattern. removing
   boilerplate html and extracting meaningful, plain text? [65]justext.
   continue the learning pipeline with id116 or other machine learning
   algos? [66]scikit-learn has loads.

   needless to say, better integration with gensim is also under way.

   part ii: [67]optimizing id97 in python

   [68]gensim[69]id97

comments 33

    1.
   [70]rene nederhand
       [71]2013-09-21 at 9:54 am
       hi radim,
       i already played with id97. it   s a wonderful tool, but i miss
       the easiness of integrating it with python. thanks for helping
       solving this issue with gensim and giving us a true pythonic way to
       deal with these deep learning algorithms.
       cheers,
       rene
       [72]reply
    2.
   sujith ps
       [73]2013-09-25 at 7:05 am
       hi ,
       i used your tool, it is very interesting.
       but , while using id97 in c, it took only 15minutes to train ,
       in your application it took almost 9hours.
       [74]reply
         1. radim post
            author
        radim
            [75]2013-09-25 at 12:10 pm
            hi sujith, did you miss the link to    part ii    at the end? 72x
            speed up. if you need absolutely top speed, you may be happy
            to hear there will be a    part iii    too, with further
            optimizations.
            also, the port is still very much in the making, so be sure to
            use the latest, [76]develop branch from github and not the
               stable    release version.
            [77]reply
    3.
   brent payne
       [78]2013-09-26 at 6:09 pm
       enjoying your hard work. thanks
       [79]reply
    4.
   [80]mike kestemont
       [81]2013-10-07 at 8:51 am
       hi radim,
       thank you so much for this: this is really cool!
       thanks,
       mike
       [82]reply
    5.
   mark pinches
       [83]2013-10-11 at 11:37 am
       hi radim,
       thanks so much for this. i   m trying to get started by loading the
       pretrained .bin files from the id97 site (
       freebase-vectors-skipgram1000.bin.gz). it loads fine, but when i
       run the most similar function. it cant find the words in the
       vocabulary. my error code is below.
       any ideas where i   m going wrong?
       model.most_similar([   girl   ,    father   ], [   boy   ], topn=3)
       2013-10-11 10:22:00,562 : warning : word    girl    not in vocabulary;
       ignoring it
       2013-10-11 10:22:00,562 : warning : word    father    not in
       vocabulary; ignoring it
       2013-10-11 10:22:00,563 : warning : word    boy    not in vocabulary;
       ignoring it
       traceback (most recent call last):
       file       , line 1, in
       file
          /users/mark/anaconda/python.app/contents/lib/python2.7/site-packag
       es/gensim-0.8.7-py2.7.egg/gensim/models/id97.py   , line 312, in
       most_similar
       raise valueerror(   cannot compute similarity with no input   )
       valueerror: cannot compute similarity with no input
       [84]reply
         1. radim post
            author
        radim
            [85]2013-10-11 at 11:42 am
            hi mark, i never managed to download the freebase vectors (the
            download always came out corrupted for me). but its
            description on id97 page suggests the words in freebase
            look like    /en/marvin_minsky    or    /en/godel_prize   . that   s why
            they don   t match your input. check the freebase file contents
            for its word syntax.
            [86]reply
    6.
   mark pinches
       [87]2013-10-11 at 6:15 pm
       thanks radim,
       i tried that (using the word structure you suggested) but no joy.
       i   m going to continue looking for a solution and if i come up with
       something, i   ll post it here.
       cheers
       mark
       [88]reply
         1. radim post
            author
        radim
            [89]2013-10-11 at 6:28 pm
            the syntax is no mystery mark, really. just open the file with
            e.g.    less    and see what   s in it.
            alternatively, load it from gensim and do a `print
            my_model.index2word`.
            [90]reply
    7.
   dg2
       [91]2013-10-20 at 8:40 am
       hi radim. thanks for the great work!
       i have been looking through the code and i   m curious about one
       thing with the    train_sentence    function. it seems to me that the
       architecture implied by the code tries to predict    word    using
          word2    (since you are using the latent representation for word2
       and the code for word), while the skip gram architecture actually
       tries to estimate word2 from word.
       i have probably missed something, so sorry if this makes no sense.
       cheers,
       d.
       [92]reply
         1. radim post
            author
        radim
            [93]2013-10-20 at 3:51 pm
            hello, yes, the model tries to estimate context words based on
            the current word. in the code, this corresponds to
            syn0[word2.index] += dot(ga, l2a). the code also has to update
            the hierarchical embeddings (syn1), so maybe that   s what threw
            you off?
            [94]reply
    8.
   break
       [95]2013-11-01 at 8:51 am
       hi radim.thanks for the great work!
       [96]reply
    9.
   mark
       [97]2013-11-01 at 3:13 pm
       hi radim,
       just to follow up on my earlier query above, for noobs and poor
       programmers (like myself)    the code model is essentially a
       dictionary and can be queried using my_model.index2word[0] . this
       should return a string.
       m
       [98]reply
   10.
   mark
       [99]2013-11-01 at 3:37 pm
       hi again,
       i have 57,000 short strings of text, and i want to create vector
       representations of them for id91. so how do i access each
       terms vector? for the calculation. note i am using 0.8.7 gensim and
       when i query my_model[   term   ] i get the error
       traceback (most recent call last):
       file       , line 1, in
       typeerror:    id97    object has no attribute    __getitem__   
       >>>
       any ideas?
       thanks
       m
       [100]reply
   11.
   mark
       [101]2013-11-01 at 3:55 pm
       got it ! use:
       my_model.syn0[my_model.vocab[   term   ].index]
       [102]reply
         1. radim post
            author
        radim
            [103]2013-11-01 at 6:30 pm
            correct      the __getitem__ method that is in the latest dev
            branch (not in 0.8.7 release) does exactly that:
            [104]https://github.com/piskvorky/gensim/blob/170b1b8158b19f33
            ef1375fc7dd4ae390617cb8c/gensim/models/id97.py#l429
            [105]reply
   12.
   [106]janson
       [107]2013-11-09 at 9:30 am
       i love this article.
       it   s a great work!
       [108]reply
   13.
   m
       [109]2013-12-19 at 7:43 pm
       can the gensim id97 api load a pretrained model directly? e.g.
       freebase-vectors-skipgram1000-en.bin at
       [110]https://docs.google.com/file/d/0b7xkcwpi5kdyefdmcvltwkhtbmm/ed
       it?usp=sharing
       [111]reply
   14.
   m
       [112]2013-12-19 at 7:47 pm
       nevermind i just saw this line in your code sample:
       model = id97.id97.load_id97_format(   /tmp/vectors.bin   ,
       binary=true)
       [113]reply
   15.
   irt24
       [114]2014-06-06 at 6:51 pm
       is there a reason why word indices in the vocabulary are sometimes
       not consecutive? for instance [model.vocab[word].index for word in
       model.vocab.keys()[0:9]] is not always the list of numbers from 0
       to 8. in one particular case, i got [0, 2, 3, 4, 5, 6, 8, 10, 11].
       [115]reply
   16.
   simon
       [116]2014-08-20 at 2:23 pm
       the use of    deep learning    in this post and in general to talk
       about skipgram and cbow architectures is misleading. see the open
       reviews for mikolov et al. paper you   re citing:
       [117]http://openreview.net/document/7b076554-87ba-4e1e-b7cc-2ac107c
       e8e4d
       [118]reply
         1.
        [119]tim mcnamara
            [120]2014-11-24 at 7:57 pm
            could you provide a summary for people who are a slightly time
            poor?
            [121]reply
   17.
   [122]adam
       [123]2015-01-22 at 2:15 am
       hi radim,
       thanks for putting this together! i   m wondering how i can generate
       a list of similar words similar to the distance examples on the
       original id97 page.
       for instance, instead of saying
       most_similar(postitive=[   dog   ,    cat   ], negative= [   kitten   ])
       i   d like to just put in the word    dog    and then get the top 10 most
       similar, without having to make analogy type comparisons.
       any ideas? thanks again     
       [124]reply
         1. radim post
            author
        [125]radim
            [126]2015-03-06 at 9:40 pm
            hi adam, sure. try `most_similar(   dog   )`     
            [127]reply
   18.
   aayush
       [128]2015-04-07 at 5:15 am
       hello radim, i wish to know can the model be used to predict the
       next word in a phrase given a phrase. can it be used in word
       prediction?
       [129]reply
   19. pingback: [130]why i cannot reproduce id97 results using gensim
       - blogosfera
   20.
   lachlan
       [131]2015-09-28 at 11:19 am
       hi, i   m pretty new to both machine learning and reddit so my
       apoligies if this topic is out of place, in the wrong subreddit, or
       not appropriate.
       this semester, my professor has asked me to investigate id97,
       by t milokov and his team at google, and particularly with regards
       to machine translation. for this task, i   m using the implementation
       of id97 in the gensim package for python.
       in the paper (link below) milokov describes how after training two
       monolingual models, they generate a translation matrix on the most
       frequently occurring 5000 words, and using this translation matrix,
       evaluate the accuracy of the translations of the following
       1000 words. paper: [132]http://arxiv.org/pdf/1309.4168.pdf
       here are two screencaps, one of the description of the matrix in
       the paper and one of some clarification milokov posted on a board.
       from paper: [133]http://imgur.com/vdyyy2n milokov post:
       [134]http://imgur.com/ummnhwy
       i have been playing around with the models i have generated for
       japanese and english in gensim quite a bit. i downloaded wikipedia
       dumps, processed and tokenised them, and generated the models with
       gensim.
       i would like to emulate what milokov did and see how accurate the
       translation are for japanese/english (my two languages). i am
       unsure how to get the top 6000 words (5000 for make the trans
       vector, 1000 for testing), and especially how to produce the
       vector. i have read the papers and seen the algorithms but can   t
       quite put it into code.
       if anyone has some ideas/suggestions on how to do so, provide some
       pseudocode or has gensim knowledge and can lend a hand it would be
       greatly appreciated. i   m very motivated for this task but having
       difficulty progressing.
       [135]reply
   21.
   parisa
       [136]2015-10-25 at 7:36 pm
       hi,i am new in gensim,i installed canopy and gensim but i don   t
       know how to work it ,i am confused pleas help and guid me
       thanks
       [137]reply
   22. pingback: [138]processing text | pearltrees
   23. pingback: [139]why i cannot reproduce id97 results using gensim
       - html code
   24. pingback: [140]google news id97 | i love you zones
   25. pingback: [141]getting started with id97 | textprocessing | a
       text processing portal for humans
   26. pingback: [142]the id97 algorithm     site title

leave a reply [143]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   submit

   current [144][email protected] * 4.2_________________

   leave this field empty ____________________

author of post

   radim   eh    ek

radim   eh    ek's bio:

   founder at rare technologies, creator of gensim. sw engineer since
   2004, phd in ai in 2011. lover of geology, history and beginnings in
   general. occasional travel blogger.

need expert consulting in ml and nlp?

   ________________________________________

   ________________________________________


   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   please leave this field empty. ________________________________________

   send

categories

   categories[select category___________]

archives

   archives [select month__]

recent posts

     * [145]export pii drill-down reports
     * [146]personal data analytics
     * [147]scanning office 365 for sensitive pii information
     * [148]pivoted document length normalisation
     * [149]sent2vec: an unsupervised approach towards learning sentence
       embeddings

stay ahead of the curve

get our latest tutorials, updates and insights delivered straight to your
inbox.

   ____________________

   ____________________

   subscribe
   ____________________
   1-2 times a month, if lucky. your information will not be shared.

   [150][footer-logo.png]
     * [151]services
     * [152]careers
     * [153]our team
     * [154]corporate training
     * [155]blog
     * [156]incubator
     * [157]contact
     * [158]competitions
     * [159]site map

   rare technologies [160][email protected] sv  tova 5, prague, czech
   republic [161](eu) +420 776 288 853
   type and press    enter    to search ____________________

references

   visible links
   1. https://rare-technologies.com/feed/
   2. https://rare-technologies.com/comments/feed/
   3. https://rare-technologies.com/deep-learning-with-id97-and-gensim/feed/
   4. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/deep-learning-with-id97-and-gensim/
   5. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/deep-learning-with-id97-and-gensim/&format=xml
   6. https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld
   7. https://rare-technologies.com/
   8. https://rare-technologies.com/deep-learning-with-id97-and-gensim/
   9. https://rare-technologies.com/services/
  10. https://rare-technologies.com/deep-learning-with-id97-and-gensim/
  11. https://pii-tools.com/
  12. https://scaletext.com/
  13. https://rare-technologies.com/corporate-training/
  14. https://rare-technologies.com/corporate-training/
  15. https://rare-technologies.com/python-best-practices/
  16. https://rare-technologies.com/practical-machine-learning/
  17. https://rare-technologies.com/topic-modelling-training/
  18. https://rare-technologies.com/deep_learning_training/
  19. https://rare-technologies.com/incubator
  20. https://github.com/rare-technologies/
  21. https://rare-technologies.com/incubator/
  22. https://rare-technologies.com/competitions/
  23. https://rare-technologies.com/#braintrust
  24. https://rare-technologies.com/careers/
  25. https://rare-technologies.com/our-team/
  26. https://rare-technologies.com/blog/
  27. https://rare-technologies.com/contact/
  28. https://rare-technologies.com/deep-learning-with-id97-and-gensim/
  29. https://rare-technologies.com/services/
  30. https://rare-technologies.com/deep-learning-with-id97-and-gensim/
  31. https://pii-tools.com/
  32. https://scaletext.com/
  33. https://rare-technologies.com/corporate-training/
  34. https://rare-technologies.com/corporate-training/
  35. https://rare-technologies.com/python-best-practices/
  36. https://rare-technologies.com/practical-machine-learning/
  37. https://rare-technologies.com/topic-modelling-training/
  38. https://rare-technologies.com/deep_learning_training/
  39. https://rare-technologies.com/incubator
  40. https://github.com/rare-technologies/
  41. https://rare-technologies.com/incubator/
  42. https://rare-technologies.com/competitions/
  43. https://rare-technologies.com/#braintrust
  44. https://rare-technologies.com/careers/
  45. https://rare-technologies.com/our-team/
  46. https://rare-technologies.com/blog/
  47. https://rare-technologies.com/contact/
  48. https://rare-technologies.com/deep-learning-with-id97-and-gensim/
  49. https://rare-technologies.com/author/radim/
  50. https://rare-technologies.com/category/gensim/
  51. https://rare-technologies.com/category/programming/
  52. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comments
  53. https://en.wikipedia.org/wiki/deep_learning
  54. http://radimrehurek.com/gensim/
  55. https://arxiv.org/pdf/1301.3781.pdf
  56. https://code.google.com/p/id97/
  57. http://radimrehurek.com/gensim/
  58. http://radimrehurek.com/id97-in-python-part-two-optimizing/
  59. https://github.com/piskvorky/gensim/blob/develop/gensim/models/id97.py
  60. http://radimrehurek.com/gensim/
  61. https://google-opensource.blogspot.cz/2013/08/learning-meaning-behind-words.html
  62. https://arxiv.org/pdf/1301.3781.pdf
  63. http://nltk.org/
  64. http://www.clips.ua.ac.be/pages/pattern
  65. https://github.com/miso-belica/justext
  66. http://scikit-learn.org/
  67. http://radimrehurek.com/id97-in-python-part-two-optimizing/
  68. https://rare-technologies.com/tag/gensim/
  69. https://rare-technologies.com/tag/id97/
  70. http://cogfor.com/
  71. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2109
  72. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2109#respond
  73. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2110
  74. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2110#respond
  75. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2111
  76. https://github.com/piskvorky/gensim/
  77. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2111#respond
  78. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2112
  79. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2112#respond
  80. http://www.mike-kestemont.org/
  81. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2113
  82. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2113#respond
  83. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2114
  84. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2114#respond
  85. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2115
  86. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2115#respond
  87. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2116
  88. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2116#respond
  89. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2117
  90. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2117#respond
  91. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2118
  92. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2118#respond
  93. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2119
  94. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2119#respond
  95. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2120
  96. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2120#respond
  97. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2121
  98. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2121#respond
  99. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2122
 100. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2122#respond
 101. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2123
 102. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2123#respond
 103. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2124
 104. https://github.com/piskvorky/gensim/blob/170b1b8158b19f33ef1375fc7dd4ae390617cb8c/gensim/models/id97.py#l429
 105. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2124#respond
 106. https://github.com/jannson
 107. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2125
 108. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2125#respond
 109. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2126
 110. https://docs.google.com/file/d/0b7xkcwpi5kdyefdmcvltwkhtbmm/edit?usp=sharing
 111. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2126#respond
 112. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2127
 113. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2127#respond
 114. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2128
 115. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2128#respond
 116. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2129
 117. https://openreview.net/document/7b076554-87ba-4e1e-b7cc-2ac107ce8e4d
 118. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2129#respond
 119. http://timmcnamara.co.nz/
 120. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2130
 121. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2130#respond
 122. http://www.adamferriss.com/
 123. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2131
 124. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2131#respond
 125. http://radimrehurek.com/
 126. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2132
 127. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2132#respond
 128. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2133
 129. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2133#respond
 130. http://www.blogosfera.co.uk/why-i-cannot-reproduce-id97-results-using-gensim/
 131. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2135
 132. https://arxiv.org/pdf/1309.4168.pdf
 133. https://imgur.com/vdyyy2n
 134. https://imgur.com/ummnhwy
 135. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2135#respond
 136. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#comment-2136
 137. https://rare-technologies.com/deep-learning-with-id97-and-gensim/?replytocom=2136#respond
 138. http://www.pearltrees.com/crc/processing-text/ 987137/item159804095
 139. http://htmlcode.space/why-i-cannot-reproduce-id97-results-using-gensim/
 140. http://iloveyouzones.xyz/2016/10/25/google-news-id97/
 141. http://textprocessing.org/getting-started-with-id97
 142. https://ryanmitchum.wordpress.com/2018/11/29/the-id97-algorithm/
 143. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#respond
 144. https://rare-technologies.com/cdn-cgi/l/email-protection
 145. https://rare-technologies.com/personal-data-reports/
 146. https://rare-technologies.com/pii_analytics/
 147. https://rare-technologies.com/pii-scan-o365-connector/
 148. https://rare-technologies.com/pivoted-document-length-normalisation/
 149. https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/
 150. https://rare-technologies.com/deep-learning-with-id97-and-gensim/
 151. https://rare-technologies.com/services/
 152. https://rare-technologies.com/careers/
 153. https://rare-technologies.com/our-team/
 154. https://rare-technologies.com/corporate-training/
 155. https://rare-technologies.com/blog/
 156. https://rare-technologies.com/incubator/
 157. https://rare-technologies.com/contact/
 158. https://rare-technologies.com/competitions/
 159. https://rare-technologies.com/sitemap
 160. https://rare-technologies.com/cdn-cgi/l/email-protection#c1a8afa7ae81b3a0b3a4ecb5a4a2a9afaeadaea6a8a4b2efa2aeac
 161. tel:+420 776 288 853

   hidden links:
 163. https://rare-technologies.com/deep-learning-with-id97-and-gensim/#top
 164. https://www.facebook.com/raretechnologies
 165. https://twitter.com/raretechteam
 166. https://www.linkedin.com/company/6457766
 167. https://github.com/piskvorky/
 168. https://rare-technologies.com/feed/
