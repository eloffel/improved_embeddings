   #[1]rare technologies    feed [2]rare technologies    comments feed
   [3]rare technologies    id97 tutorial comments feed [4]alternate
   [5]alternate

   [tr?id=1761346240851963&ev=pageview&noscript=1]

   iframe: [6]https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld

   [7]pragmatic machine learning rare technologies [8]navigation

     * [9]services
     * [10]products
          + [11]pii tools
          + [12]scaletext
     * [13]corporate training
          + [14]overview
          + [15]python best practices
          + [16]practical machine learning
          + [17]topic modelling
          + [18]deep learning in practice
     * [19]for students
          + [20]open source
          + [21]incubator
          + [22]competitions
     * [23]company
          + [24]careers
          + [25]our team
     * [26]blog
     * [27]contact
     * [28]search

     * [29]services
     * [30]products
          + [31]pii tools
          + [32]scaletext
     * [33]corporate training
          + [34]overview
          + [35]python best practices
          + [36]practical machine learning
          + [37]topic modelling
          + [38]deep learning in practice
     * [39]for students
          + [40]open source
          + [41]incubator
          + [42]competitions
     * [43]company
          + [44]careers
          + [45]our team
     * [46]blog
     * [47]contact
     * [48]search

id97 tutorial

   [49]radim   eh    ek 2014-02-02[50] gensim, [51]programming[52] 157
   comments

   i never got round to writing a tutorial on how to use id97 in
   gensim. it   s simple enough and the [53]api docs are straightforward,
   but i know some people prefer more verbose formats. let this post be a
   tutorial and a reference example.

   update: the complete http server code for the interactive id97 demo
   below is now [54]open sourced on github. for a high-performance
   similarity server for documents, see [55]scaletext.com.

preparing the input

   starting from the beginning, gensim   s id97 expects a sequence of
   sentences as its input. each sentence a list of words (utf8 strings):
# import modules & set up logging
import gensim, logging
logging.basicconfig(format='%(asctime)s : %(levelname)s : %(message)s', level=lo
gging.info)

sentences = [['first', 'sentence'], ['second', 'sentence']]
# train id97 on the two sentences
model = gensim.models.id97(sentences, min_count=1)

   keeping the input as a python built-in list is convenient, but can use
   up a lot of ram when the input is large.

   gensim only requires that the input must provide sentences
   sequentially, when iterated over. no need to keep everything in ram: we
   can provide one sentence, process it, forget it, load another sentence   

   for example, if our input is strewn across several files on disk, with
   one sentence per line, then instead of loading everything into an
   in-memory list, we can process the input file by file, line by line:
class mysentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()

sentences = mysentences('/some/directory') # a memory-friendly iterator
model = gensim.models.id97(sentences)

   say we want to further preprocess the words from the files     convert to
   unicode, lowercase, remove numbers, extract named entities    all of this
   can be done inside the mysentences iterator and id97 doesn   t need
   to know. all that is required is that the input yields one sentence
   (list of utf8 words) after another.

   note to advanced users: calling id97(sentences, iter=1) will run
   two passes over the sentences iterator (or, in general iter+1 passes;
   default iter=5). the first pass collects words and their frequencies to
   build an internal dictionary tree structure. the second and subsequent
   passes train the neural model. these two (or, iter+1) passes can also
   be initiated manually, in case your input stream is non-repeatable (you
   can only afford one pass), and you   re able to initialize the vocabulary
   some other way:
model = gensim.models.id97(iter=1)  # an empty model, no training yet
model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator
model.train(other_sentences)  # can be a non-repeatable, 1-pass generator

   in case you   re confused about iterators, iterables and generators in
   python, check out our tutorial on [56]data streaming in python.

training

   id97 accepts several parameters that affect both training speed and
   quality.

   one of them is for pruning the internal dictionary. words that appear
   only once or twice in a billion-word corpus are probably uninteresting
   typos and garbage. in addition, there   s not enough data to make any
   meaningful training on those words, so it   s best to ignore them:
model = id97(sentences, min_count=10)  # default value is 5

   a reasonable value for min_count is between 0-100, depending on the
   size of your dataset.

   another parameter is the size of the nn layers, which correspond to the
      degrees    of freedom the training algorithm has:
model = id97(sentences, size=200)  # default value is 100

   bigger size values require more training data, but can lead to better
   (more accurate) models. reasonable values are in the tens to hundreds.

   the last of the major parameters (full list [57]here) is for training
   parallelization, to speed up training:
model = id97(sentences, workers=4) # default = 1 worker = no parallelization

   the workers parameter has only effect if you have [58]cython installed.
   without cython, you   ll only be able to use one core because of the
   [59]gil (and id97 training will be [60]miserably slow).

   note from radim: get my latest machine learning tips & articles
   delivered straight to your inbox (it's free).
   ____________________ ____________________

    unsubscribe anytime, no spamming. max 2 posts per month, if lucky.
   subscribe now
   ____________________

memory

   at its core, id97 model parameters are stored as matrices (numpy
   arrays). each array is #vocabulary (controlled by min_count parameter)
   times #size (size parameter) of floats (single precision aka 4 bytes).

   three such matrices are held in ram (work is underway to reduce that
   number to two, or even one). so if your input contains 100,000 unique
   words, and you asked for layer size=200, the model will require approx.
   100,000*200*4*3 bytes = ~229mb.

   there   s a little extra memory needed for storing the vocabulary tree
   (100,000 words would take a few megabytes), but unless your words are
   extremely loooong strings, memory footprint will be dominated by the
   three matrices above.

evaluating

   id97 training is an unsupervised task, there   s no good way to
   objectively evaluate the result. evaluation depends on your end
   application.

   google have released their testing set of about 20,000 syntactic and
   semantic test examples, following the    a is to b as c is to d    task:
   [61]https://raw.githubusercontent.com/rare-technologies/gensim/develop/
   gensim/test/test_data/questions-words.txt.

   gensim support the same evaluation set, in exactly the same format:
model.accuracy('/tmp/questions-words.txt')
2014-02-01 22:14:28,387 : info : family: 88.9% (304/342)
2014-02-01 22:29:24,006 : info : gram1-adjective-to-adverb: 32.4% (263/812)
2014-02-01 22:36:26,528 : info : gram2-opposite: 50.3% (191/380)
2014-02-01 23:00:52,406 : info : gram3-comparative: 91.7% (1222/1332)
2014-02-01 23:13:48,243 : info : gram4-superlative: 87.9% (617/702)
2014-02-01 23:29:52,268 : info : gram5-present-participle: 79.4% (691/870)
2014-02-01 23:57:04,965 : info : gram7-past-tense: 67.1% (995/1482)
2014-02-02 00:15:18,525 : info : gram8-plural: 89.6% (889/992)
2014-02-02 00:28:18,140 : info : gram9-plural-verbs: 68.7% (482/702)
2014-02-02 00:28:18,140 : info : total: 74.3% (5654/7614)

   this accuracy takes an [62]optional parameter restrict_vocab which
   limits which test examples are to be considered.

   once again, good performance on this test set doesn   t mean id97
   will work well in your application, or vice versa. it   s always best to
   evaluate directly on your intended task.

storing and loading models

   you can store/load models using the standard gensim methods:
model.save('/tmp/mymodel')
new_model = gensim.models.id97.load('/tmp/mymodel')

   which uses pickle internally, optionally mmap   ing the model   s internal
   large numpy matrices into virtual memory directly from disk files, for
   inter-process memory sharing.

   in addition, you can load models created by the original c tool, both
   using its text and binary formats:
model = id97.load_id97_format('/tmp/vectors.txt', binary=false)
# using gzipped/bz2 input works too, no need to unzip:
model = id97.load_id97_format('/tmp/vectors.bin.gz', binary=true)

online training / resuming training

   advanced users can load a model and continue training it with more
   sentences:
model = gensim.models.id97.load('/tmp/mymodel')
model.train(more_sentences)

   you may need to tweak the total_words parameter to train(), depending
   on what learning rate decay you want to simulate.

   note that it   s not possible to resume training with models generated by
   the c tool, load_id97_format(). you can still use them for
   querying/similarity, but information vital for training (the vocab
   tree) is missing there.

using the model

   id97 supports several word similarity tasks out of the box:
model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
[('queen', 0.50882536)]
model.doesnt_match("breakfast cereal dinner lunch";.split())
'cereal'
model.similarity('woman', 'man')
0.73723527

   if you need the raw output vectors in your application, you can access
   these either on a word-by-word basis
model['computer']  # raw numpy vector of a word
array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)

      or en-masse as a 2d numpy matrix from model.syn0.

bonus app

   as before with [63]finding similar articles in the english wikipedia
   with latent semantic analysis, here   s a bonus web app for those who
   managed to read this far. it uses the id97 model trained by google
   on the google news dataset, on about 100 billion words:
     __________________________________________________________________

   if you don   t get    queen    back, something went wrong and baby skynet
   cries.
   try more examples too: [64]   he    is to    his    as    she    is to ?,
   [65]   berlin    is to    germany    as    paris    is to ? (click to fill in).
   man_______________ is to king______________ as woman_____________ is to
   (button) ?
     __________________________________________________________________

   try: [66]u.s.a.; [67]monty_python; [68]php; [69]madiba (click to fill
   in).
   iphone__________________________________ (button) get most similar
     __________________________________________________________________

   also try: [70]   monkey ape baboon human chimp gorilla   ; [71]   blue red
   green crimson transparent    (click to fill in).
   dinner cereal breakfast lunch_________________________________________
   (button) which phrase doesn   t fit?
     __________________________________________________________________

   the model contains 3,000,000 unique phrases built with layer size of
   300.

   note that the similarities were trained on a news dataset, and that
   google did very little preprocessing there. so the phrases are case
   sensitive: watch out! especially with proper nouns.

   on a related note, i noticed about half the queries people entered into
   the [72][email protected] demo contained typos/spelling errors, so they
   found nothing. ouch.

   to make it a little less challenging this time, i added phrase
   suggestions to the forms above. start typing to see a list of valid
   phrases from the actual vocabulary of google news    id97 model.

   the    suggested    phrases are simply ten phrases starting from whatever
   bisect_left(all_model_phrases_alphabetically_sorted,
   prefix_you_typed_so_far) from python   s built-in [73]bisect module
   returns.

   see the complete http server code for this    bonus app    [74]on github
   (using cherrypy).

outro

   full id97 api docs [75]here; get gensim [76]here. original c
   toolkit and id97 papers by google [77]here.

   and here   s me talking about the optimizations behind id97 at
   [78]pydata berlin 2014

   iframe: [79]https://www.youtube.com/embed/vu4tlwzztfu?feature=oembed

   [80]gensim[81]optimization[82]id97

comments 157

    1.
   pritpal
       [83]2014-03-12 at 1:44 pm
       hi radim,
       impressive tutorial. i have a query that the output id97 model
       is returning in an array. how can we use that as an input to
       id56??
       thanks
       [84]reply
    2.
   suzana
       [85]2014-03-18 at 8:38 pm
       model = gensim.models.id97(sentences) will not work as shown in
       the tutorial, because you will receive the error message:
          runtimeerror: you must first build vocabulary before training the
       model   . you also have to set down the min_count manually, like
       model = gensim.models.id97(sentences, min_count=1).
       [86]reply
         1. radim post
            author
        radim
            [87]2014-03-18 at 9:47 pm
            default `min_count=5` if you don   t set it explicitly.
            vocabulary is built automatically from the sentences.
            what version of gensim are you using? it should really work
            simply with `id97(sentences)`, there are even unit tests
            for that.
            [88]reply
              1.
             claire
                 [89]2014-05-07 at 3:24 pm
                 if you don   t set    min_count=1   , it will remove all the
                 words in sentences in the example given    
                 logging:
                    info : total 0 word types after removing those with
                 count<5'
                 [90]reply
                   1. radim post
                      author
                  [91]radim
                      [92]2014-05-07 at 3:47 pm
                      ah ok, thanks claire. i   ve add the `min_count=1`
                      parameter.
                      [93]reply
         2.
        rahulvks
            [94]2015-11-18 at 4:37 pm
            it showing    runtimeerror: you must first build vocabulary
            before training the model    even though i changed min_count =1
            .
            please help to correct.
            how can i train vocabulary ?
            [95]reply
         3.
        xueba
            [96]2016-03-26 at 8:57 am
            i suggest that the content of sentences may be error
            format,you may need to delete punctuation,characters separated
            by spaces.
            [97]reply
    3.
   pavel
       [98]2014-03-20 at 3:56 pm
       hi radim,
       is there any way to obtain the similarity of phrases out of the
       id97? i   m trying to get 2-word phrases to compare, but don   t
       know how to do it.
       thanks!
       pavel
       [99]reply
         1. radim post
            author
        radim
            [100]2014-04-06 at 1:17 pm
            hello pavel, yes, there is a way.
            first, you must detect phrases in the text (such as 2-word
            phrases).
            then you build the id97 model like you normally would,
            except some    tokens    will be strings of multiple words instead
            of one (example sentence: [   new york   ,    was   ,    founded   ,    16th
            century   ]).
            then, to get similarity of phrases, you do
            `model.similarity(   new york   ,    16th century   )`.
            it may be a good idea to replace spaces with underscores in
            the phrase-tokens, to avoid potential parsing problems
            (   new_york   ,    16th_century   ).
            as for detecting the phrases, it   s a task unrelated to
            id97. you can use existing nlp tools like the
            nltk/freebase, or help finish a gensim pull request that does
            exactly this:
            [101]https://github.com/piskvorky/gensim/pull/135 .
            [102]reply
    4.
   luopuya
       [103]2014-04-05 at 5:34 pm
       hi radim,
       the id97 function split my words as:
       u   u4e00u822c    ==>> u   u4e00    and u   u822c   
       how could i fix it?
       thanks, luopuya
       [104]reply
         1.
        luopuya
            [105]2014-04-06 at 11:51 am
            sorry, i did not read the blog carefully.
            every time reading a line of file, i should split it like what
               mysentences    do
            [106]reply
    5.
   max
       [107]2014-04-27 at 7:37 pm
       hi radim,
       you are awesome, thank you so much for gensim and this tutorial!!
       i have a question. i read in the docs that by default you utilize
       skip-gram, which can be switched to cbow. from what i gathered in
       the nips slides, cbow is faster, more effective and gets better
       results. so why use skip-gram in the first place? i   m sure i   m
       missing something obvious here     
       thanks,
       max
       [108]reply
         1.
        max
            [109]2014-04-27 at 7:40 pm
            whoops, i just realized the parameter    sg    is not supported
            anymore in the id97 constructor. is that true? so what is
            used by default?
            [110]reply
         2. radim post
            author
        [111]radim
            [112]2014-04-27 at 8:53 pm
            hello max,
            thanks     
            skip-gram is used because it gives better (more accurate)
            results.
            cbow is faster though. if you have lots data, you can be
            advantageous to run the simpler but faster model.
            there   s a [113]pull request under way, to enable all the
            id97 options and parameters. you can try out the various
            models and their performance for yourself     
            [114]reply
              1.
             max
                 [115]2014-04-30 at 8:41 am
                 thanks for you answer radim! i only saw that one
                 experiment in the slides that said that cbow was faster
                 and more accurate, but that might have been an outlier or
                 my misinterpretation. i   m excited for that pull request!
                     
                 anyway, i have another question (or bug report?). i
                 changed a bunch of training parameters and added input
                 data and suddenly got segfaults on python when asking for
                 similarities for certain words    so i tried which of the
                 changes caused this, and it turned out that the cause was
                 that i set the output size to 200! setting it to
                 (apparently) any other number doesn   t cause any trouble,
                 but 200 does    if you hear this from anyone else or are
                 able to reproduce it yourself, consider it a bug     
                 [116]reply
                   1. radim post
                      author
                  [117]radim
                      [118]2014-04-30 at 9:08 am
                      hey max     are you on os x? if so, it may be
                      [119]https://github.com/numpy/numpy/issues/4007
                      if not, please file a bug report at
                      [120]https://github.com/piskvorky/gensim/issues with
                      system/sw info. cheers!
                      [121]reply
    6.
   [122]bogdan
       [123]2014-05-01 at 1:33 am
       hi radim,
       indeed, a great tutorial! thank you for that!
       playied a bit with id97 and it   s quite impressive. couldn   t
       figure out how the first part of the demo app works. can you
       provide some insights please ?
       thanks!     
       [124]reply
         1. radim post
            author
        [125]radim
            [126]2014-05-01 at 9:26 am
            hello bogdan, you   re welcome     
            this demo app just calls the    model.most_similar(positive,
            negative)    method in the background. check out the api docs:
            [127]http://radimrehurek.com/gensim/models/id97.html
            [128]reply
    7.
   indian
       [129]2014-05-19 at 2:14 am
       hello,
       i   d like to ask you, if this all can be done with other languages
       too, like korean, russian, arabic and so, or whether is this
       toolkit fixed to the english only.
       thank you in advance for the answer
       [130]reply
         1. radim post
            author
        [131]radim
            [132]2014-05-21 at 3:28 pm
            hi, it can be done directly for languages where you can split
            text into sentences and tokens.
            the only concern would be that the `window` parameter has to
            be large enough to capture syntactic/semantic relationships.
            for english, the default `window=5`, and i wouldn   t expect it
            to be dramatically different for other languages.
            [133]reply
         2.
        [134]disha punjabi
            [135]2016-04-15 at 1:08 am
            hi i just wanted to ask if anyone tried it on different
            languages yet? i was planning to test it out so i thought i
            should ask if someone already did it!
            [136]reply
              1. radim   eh    ek post
                 author
             [137]radim   eh    ek
                 [138]2016-04-15 at 1:29 am
                 hello disha,
                 the gensim mailing list is a much better place to ask. i
                 doubt anyone regularly scans the blog comments.
                 best,
                 radim
                 [139]reply
    8.
   sebastian
       [140]2014-05-30 at 1:13 am
       hey, i wanted to know if the version you have in gensim is the same
       that you got after    optimizing id97 in python   .. i am using the
       pre-trained model of the google news vector(found in the page of
       id97) and then i run model.accuracy(   file_questions   ) but it
       runs really slow    just wanted to know if this is normal or i have
       to do some things to speed u    the version of gensim.. thanks in
       advance and great work!
       [141]reply
         1. radim post
            author
        [142]radim
            [143]2014-05-30 at 10:54 am
            it is     gensim always contains the latest, most optimized
            version (=newer than this blog post).
            however, the accuracy computations (unlike training) are not
            optimized      i never got to optimizing that part. if you want
            to help, let me know, i don   t think i   ll ever get to it
            myself. (massive optimizations can be done directly in python,
            no need to go c/cython).
            [144]reply
    9.
   zigi
       [145]2014-06-13 at 4:36 am
       hi,
       could you please explain how do cbow and skip-gram models actually
       do the learning. i   ve read    efficient estimation       but it doesn   t
       really explain how does the actual training happen.
       i   ve taken a look at the original source code and your
       implementation, and while i can understand the code i cannot
       understand the logic behind it.
       i don   t understand these lines in your implementation (id97.py,
       gensim 0.9) cbow:
                                           
       l2a = model.syn1[word.point] # 2d matrix, codelen x layer1_size
       fa = 1.0 / (1.0 + exp(-dot(l1, l2a.t))) # propagate hidden ->
       output
       ga = (1     word.code     fa) * alpha # vector of error gradients
       multiplied by the learning rate
       model.syn1[word.point] += outer(ga, l1) # learn hidden -> output
                                              -
       i see that it has something to do with the huffman-tree word
       representation, but despite the comments, i don   t understand what
       is actually happening, what does syn1 represent, why do we multiply
       l1 with l2a    why are we multiplying ga and l1 etc   
       could you please explain in a sentence or two what is actually
       happening in there.
       i would be very grateful.
       [146]reply
   10.
   vimos
       [147]2014-06-27 at 6:03 am
       hi, this is great, but i have a question about unknown words.
       after loading a model, when train more sentences, new words will be
       ignored, not added to the vocab automatically.
       i am not quite sure about this. is that true?
       thank you very much!
       [148]reply
         1. radim post
            author
        [149]radim
            [150]2014-06-27 at 9:20 am
            yes, it is true. the id97 algorithm doesn   t support adding
            new words dynamically.
            [151]reply
   11.
   katja
       [152]2014-07-10 at 7:02 pm
       hi,
       unfortunately i   m not sufficiently versed in programming (yet) to
       solve this by myself. i would like to be able to be able to add a
       new vector to the model after it has been trained.
       i realize i could export the model to a text file, add the vector
       there and load the modified file. is there a way to add the vector
       within python, though? as in: create a new entry for the model such
       that model[   new_entry   ] is assigned the new_vector.
       thanks in advance!
       [153]reply
         1. radim post
            author
        [154]radim
            [155]2014-08-21 at 5:10 pm
            the weights are a numpy matrix     have a look at `model.syn0`
            and `model.syn1` matrices. you can append new vectors easily.
            off the top of my head i   m not sure whether there are any
            other variables in `model` that need to be modified when you
            do this. there probably are. i   d suggest checking the code to
            make sure everything   s consistent.
            [156]reply
   12. pingback: [157]motorblog    [review] pydata berlin 2014    
       satellitenevent zur europython
   13.
   xu
       [158]2014-08-21 at 4:40 pm
       i am new to id97. can i ask you two questions?
       1. when i apply the pre-trained model to my own dataset, do you
       have any suggestion about how to deal with the unknown words?
       2. do you have any suggestion about aggregating the id27s
       of words in a sentence into one vector to represent that sentence?
       thanks a lot!
       [159]reply
         1. radim post
            author
        [160]radim
            [161]2014-08-21 at 5:07 pm
            good questions, but i don   t have any insights beyond the
            standard advice:
            1. unknown words are ignored; or you can build a model with
            one special    word    to represent all oov words.
            2. for short sentences/phrases, you can average the individual
            vectors; for longer texts, look into something like
            paragraph2vec:
            [162]https://github.com/piskvorky/gensim/issues/204#issuecomme
            nt-52093328
            [163]reply
              1.
             xu
                 [164]2014-08-21 at 5:39 pm
                 thanks for your advice     
                 [165]reply
   14.
   xiao zhibo
       [166]2014-08-27 at 9:56 am
       hi,
       could you tell me how to find the most similar word as in web app
       3? calculating the cosine similarity between each word seems like a
       no-brainer way to do it? is there any api in gensim to do that?
       another question, i want to represent sentence using word vector,
       right now i only add up all the words in the sentence to get a new
       vector. i know this method does   t make sense, since each word has a
       coordinate in the semantic space, adding up coordinates is not an
       ideal to represent a sentence. i have read some papers talking
       about this problem? could you tell me what will be an ideal way to
       represent sentence to do sentence id91?
       thank you very much!
       [167]reply
         1. radim post
            author
        [168]radim
            [169]2014-08-27 at 10:14 am
               find most similar word   : look at the api docs.
               ideal way to represent a sentence   : i don   t know about ideal,
            but another way to represent sentences is using
               paragraph2vec   :
            [170]https://github.com/piskvorky/gensim/issues/204
            [171]reply
   15.
   [172]kaihu chen
       [173]2014-09-27 at 5:36 am
       radim, this is great stuff. i have posted a link to your software
       on our ai community blog in the usa at
       [174]http://www.smesh.net/pages/980191274#54262e6cdb3c2facb5a41579
       , so that other people can benefit from it too.
       [175]reply
   16.
   neo
       [176]2014-10-15 at 2:51 pm
       does the workers = x work to multithread the iteration over the
       sentences or just the training of the model ?
       [177]reply
         1. radim post
            author
        [178]radim
            [179]2014-10-15 at 3:20 pm
            it parallelizes training.
            how you iterate over sentences is your business     id97
            only expects an iterator on input. what the iterator does
            internally to iterate over sentences is up to you and not part
            of id97.
            [180]reply
              1.
             neo
                 [181]2014-10-16 at 6:20 pm
                 i have a collection of 1500000 text files (with 10 lines
                 each on average) and a machine with 12 cores/16g of
                 ram(not sure if it is relevant for reading files).
                 how would you suggest me to build the iterator to utilize
                 all the computing resources i have?
                 [182]reply
                   1. radim post
                      author
                  [183]radim
                      [184]2014-10-16 at 9:04 pm
                      no, not relevant.
                      i   d suggest you loop over your files inside
                      __iter__() and yield out your sentences (lines?),
                      one after another.
                      [185]reply
                        1.
                       neo
                           [186]2014-10-17 at 2:32 pm
                           ok thanks!
   17.
   [187]satarupa
       [188]2014-11-08 at 2:22 pm
       if i am using the model pre-trained with google news data set, is
       there any way to control the size of the output vector
       corresponding to a word?
       [189]reply
   18.
   suvir
       [190]2014-11-11 at 3:29 pm
       for    model.build_vocab(sentences)    command to work, we need to add
          import os   . without that, i was getting error for    os    not
       defined.
       [191]reply
         1. radim post
            author
        [192]radim
            [193]2014-11-11 at 3:45 pm
            not sure what you are talking about suvir, you don   t need any
               import os   . if you run into problems, send us the full
            traceback (preferably to the gensim mailing list, not this
            blog). see [194]http://radimrehurek.com/gensim/support.html.
            cheers.
            [195]reply
   19.
   mostafa benhenda
       [196]2014-11-26 at 7:35 pm
       hello,
       where can i find the code (in python) of the bonus app?
       [197]reply
   20. pingback: [198]how to grow a list of related words based on initial
       keywords? | cl-uat
   21.
   t zheng
       [199]2015-02-11 at 9:07 pm
       i am using the train function as described in the api doc. i notice
       that the training might have terminated    prematuredly   , according
       to the logging output below. not sure if i understand the output
       properly. when it said    progress: at 4.10% words   , does it mean
       4.1% of the corpus or 4.1% of the vocabs? i suspect the former, so
       it would suggest it only processed 4.1% of the words. please
       enlighten me. thanks!
       2015-02-11 19:34:40,894 : info : got records: 20143
       2015-02-11 19:34:40,894 : info : training model with 4 workers on
       67186 vocabulary and 200 features, using    skipgram   =1    hierarchical
       softmax   =0    subsample   =0 and    negative sampling   =15
       2015-02-11 19:34:41,903 : info : progress: at 0.45% words, alpha
       0.02491, 93073 words/s
       2015-02-11 19:34:42,925 : info : progress: at 0.96% words, alpha
       0.02477, 97772 words/s
       2015-02-11 19:34:43,930 : info : progress: at 1.48% words, alpha
       0.02465, 100986 words/s
       2015-02-11 19:34:44,941 : info : progress: at 2.00% words, alpha
       0.02452, 102187 words/s
       2015-02-11 19:34:45,960 : info : progress: at 2.51% words, alpha
       0.02439, 102371 words/s
       2015-02-11 19:34:46,966 : info : progress: at 3.05% words, alpha
       0.02426, 104070 words/s
       2015-02-11 19:34:48,006 : info : progress: at 3.55% words, alpha
       0.02413, 103439 words/s
       2015-02-11 19:34:48,625 : info : reached the end of input; waiting
       to finish 8 outstanding jobs
       2015-02-11 19:34:49,026 : info : progress: at 4.10% words, alpha
       0.02400, 104259 words/s
       [200]reply
   22.
   sasha kacanski
       [201]2015-02-25 at 4:43 pm
       hi radim,
       is there a whole example that i can use to understand the whole
       concept and to walk through the code.
       thanks much,
       [202]reply
         1. radim post
            author
        [203]radim
            [204]2015-02-25 at 9:15 pm
            hello sasha,
            not sure what concept / code you need, but there is one
            example right there in the id97.py source file:
            [205]https://github.com/piskvorky/gensim/blob/develop/gensim/m
            odels/id97.py#l997
            (you can download the text8 corpus used there from
            [206]http://mattmahoney.net/dc/text8.zip )
            [207]reply
   23.
   wade
       [208]2015-03-17 at 10:48 am
       hi radim,
       i   m wondering about the difference between model from trained in
       c(original way) and trained in gensim.
       when i trying to use the model.most_similar function,loading the
       model i   ve trained in c, i got a totally different result when i
       trying to do the same stuff with word-analogy.sh? so i just want to
       know if the model.most_similar function use the same way when
       trying to calculate    man   -   king    +    women           queue    like mikolov
       achieved in his c codes (word-analogy) ,thanks!!!
       [209]reply
         1. radim post
            author
        [210]radim
            [211]2015-03-17 at 11:03 am
            yes, exactly the same (cosine similarity).
            the training is almost the same too, up to different
            randomized initialization of weights iirc.
            maybe you   re using different data (preprocessing)?
            [212]reply
              1.
             wade
                 [213]2015-03-18 at 12:53 pm
                 sorry to bother you again,here are two kinds of way when
                 i try to do:
                 the way when i use gensim:
                 model=id97.load_id97_format(   vectors_200.bin   ,bin
                 ary=true)
                 #chinese
                 word1=u               
                 word2=u            
                 word3=u            
                 le=model.most_similar(positive=[word2,word3],negative=[wo
                 rd1])
                 the way use c code:
                 ./word-analogy vectors_200.bin
                 the input :                                   
                 totally different results   
                 the same model loaded, how could that happened?
                 [214]reply
                   1. radim post
                      author
                  [215]radim
                      [216]2015-03-18 at 4:41 pm
                      oh, non-ascii characters.
                      iirc, the c code doesn   t handle unicode in any way,
                      all text is treated as binary. python code (gensim)
                      uses unicode for strings.
                      so, perhaps some encoding mismatch?
                      how was your model trained     with c code? is so,
                      what was the encoding?
                      [217]reply
                        1.
                       wade
                           [218]2015-03-19 at 4:25 am
                           the training corpus encoding in utf-8, that   s
                           the reason?
   24.
   anton
       [219]2015-03-18 at 7:13 pm
       hello radim,
       is there a way to extract the output feature vector (or, sort of,
       predicted probabilities) from the model, just like while it   s
       training?
       thanks
       [220]reply
   25.
   anupama
       [221]2015-03-19 at 7:38 pm
       hey radim
       thanks for the wonderful tutorial.
       i am new to id97 and i am trying generate id165s of words for
       an indian script. i have 2 quesries:
       q1. should the input be in plain text:
                                       or unicodes 2860 2825 2858 2853 2821
       q2. is there any code available to do id91 of the generated
       vectors to form word classes?
       please help
       [222]reply
   26.
   cong
       [223]2015-03-30 at 11:23 am
       hi radim,
       for this example:    woman king man   :
       i run with bonus web app, and got the results:
       521.9ms
       [[   kings   ,0.6490576267242432],[   clown_prince   ,0.5009066462516785],[
          prince   ,0.4854174852371216],[   crown_prince   ,0.48162946105003357],[
          king   ,0.47213971614837646]]
       the above result is the same with id97 by tomas mikolov.
       however, when i run example above in gensim, the output is:
       [(u   queen   , 0.7118195295333862), (u   monarch   , 0.6189675331115723),
       (u   princess   , 0.5902432203292847), (u   crown_prince   ,
       0.5499461889266968), (u   prince   , 0.5377322435379028), (u   kings   ,
       0.523684561252594), (u   queen_consort   , 0.5235946178436279),
       (u   queens   , 0.5181134939193726), (u   sultan   , 0.5098595023155212),
       (u   monarchy   , 0.5087413191795349)]
       so why is this the case?
       your web app   s result is different to gensim ???
       thanks!
       [224]reply
         1. radim post
            author
        [225]radim
            [226]2015-03-30 at 12:15 pm
            hi cong
            no, both are the same.
            in fact, the web app just calls gensim under the hood. there   s
            no extra magic happening regarding id97 queries, it   s just
            gensim wrapped in cherrypy web server.
            [227]reply
              1.
             cong
                 [228]2015-03-30 at 1:06 pm
                 thank you for your reply.
                 i loaded the pre-trained model:
                 googlenews-vectors-negative300.bin by tomas mikolov.
                 then, i used id97 in gensim to find the output.
                 this is my code when using gensim:
                 from gensim.models import id97
                 model_path =       /googlenews-vectors-negative300.bin   
                 model =
                 id97.id97.load_id97_format(model_path,
                 binary=true)
                 stringa =    woman   
                 stringb =    king   
                 stringc =    man   
                 print model.most_similar(positive=[stringa, stringb],
                 negative=[stringc], topn=10)
                    > output is:
                 [(u   queen   , 0.7118195295333862), (u   monarch   ,
                 0.6189675331115723), (u   princess   , 0.5902432203292847),
                 (u   crown_prince   , 0.5499461889266968), (u   prince   ,
                 0.5377322435379028), (u   kings   , 0.523684561252594),
                 (u   queen_consort   , 0.5235946178436279), (u   queens   ,
                 0.5181134939193726), (u   sultan   , 0.5098595023155212),
                 (u   monarchy   , 0.5087413191795349)]
                 the see that the output above is different to the web
                 app?
                 so can you check it for me?
                 thanks so much.
                 [229]reply
                   1.
                  cong
                      [230]2015-03-30 at 2:23 pm
                      i found that in gensim, the order should be:
                         positive=[stringb, stringc], negative=[stringa]..
                      [231]reply
   27.
   boquan tang
       [232]2015-04-01 at 2:06 pm
       hi radim,
       thank you for the great tool and tutorial.
       i have one question regarding learning rate of the online training.
       you mentioned to adjust total_words in train(), but could you give
       a more detailed explanation about how this parameter will affect
       the learning rate?
       thank you in advance.
       [233]reply
   28.
   barry dillon
       [234]2015-05-05 at 5:22 pm
       fantastic tool and tutorial. thanks for sharing.
       i   m wondering about compounding use of lsi. take large corpus and
       perform lsi to map words into some space. now having a document
       when you hit a word look up the point in the space and use that
       rather than just the word. words of similar meaning then start out
       closer together and more sensibly influence the docuement
       classification. would model just reverse out those initial weights
       ? thanks for any ideas.
       [235]reply
   29.
   nima
       [236]2015-05-11 at 1:32 pm
       hi radim,
       first of all, thanks for you great job on developing this tool. i
       am new in id97 and unfortunately literature do not explain the
       details clearly. i would be grateful if you could answer my simple
       questions.
       1- for cbow (sg=0), does the method uses negative sampling as well?
       or this is something just related to skip-gram model.
       2-what about the window size? is the window size also applicable
       when one uses cbow? or all the words in 1 sentences is considered
       as bag-of-words?
       3- what happens if the window size is larger than the size of a
       sentence? is the sentence ignored or simply a smaller window size
       is chosen which fits the size of the sentence?
       4- what happens if the word sits at the end of the sentence? there
       is no word after that for the skip-gram model !
       [237]reply
   30.
   jesse berwald
       [238]2015-05-14 at 5:37 am
       hi radim,
       thanks for such a nice package! it may be bold to suggest, but i
       ran across what i think might be a bug. it   s likely a features :),
       but i thought i   d point it out since i needed to fix it in an
       unintuitive way.
       if i train a id97 model using a list of sentences:
       sentences = mysentences(fname) # generator that yields sentences
       mysentences = list(sentences)
       model = gensim.models.id97(sentences=mysentences **kwargs)
       then the model finishes training. eg., the end of the logging shows
          snip   
       2015-05-13 22:12:07,329 : info : progress: at 97.17% words, alpha
       0.00075, 47620 words/s
       2015-05-13 22:12:08,359 : info : progress: at 98.25% words, alpha
       0.00049, 47605 words/s
       2015-05-13 22:12:09,362 : info : progress: at 99.32% words, alpha
       0.00019, 47603 words/s
       2015-05-13 22:12:09,519 : info : reached the end of input; waiting
       to finish 16 outstanding jobs
       2015-05-13 22:12:09,901 : info : training on 4427131 words took
       92.9s, 47648 words/s
       i   m training on many gb of data, so i need to pass in a generator
       that yields sentences line by line (like your mysentences class
       above). but when i try it as suggested with, say, iter=5:
       sentences = mysentences(fname) # generator that yields sentences
       model = gensim.models.id97(sentences=none, **kwargs) # iter=10
       defined in kwargs
          model.build_vocab(sentences_vocab)   
       model.train(sentences_train)
       the model stops training 1/20 of the way through. if iter=10, it
       stops 1/10 of the way, etc. eg., the end of the logging looks like,
          snip   
       2015-05-13 22:31:37,265 : info : progress: at 18.21% words, alpha
       0.02049, 49695 words/s
       2015-05-13 22:31:38,266 : info : progress: at 19.29% words, alpha
       0.02022, 49585 words/s
       2015-05-13 22:31:38,452 : info : reached the end of input; waiting
       to finish 16 outstanding jobs
       2015-05-13 22:31:38,857 : info : training on 885538 words took
       17.8s, 49703 words/s
       looking in id97.py, around line 316 i noticed
       sentences = gensim.utils.repeatcorpusntimes(sentences, iter)
       so i added
       sentences_train = gensim.utils.repeatcorpusntimes(sentences(fname),
       model.iter)
       before calling model.train() in the above code snippet. does this
       seem like the correct course of action, or am i missing something
       fundamental about the way one should stream sentences to build the
       vocab and train the model?
       thanks for your help,
       jesse
       [239]reply
         1. radim post
            author
        [240]radim
            [241]2015-05-14 at 9:38 am
            hello jesse,
            for your sentences, are you using a generator (=can be
            iterated over only once), or an iterable (can be iterated over
            many times)?
            it is true that for multiple passes, generator is not enough.
            anyway better ask at the gensim mailing list / github, that   s
            a better medium for this:
            [242]http://radimrehurek.com/gensim/support.html
            [243]reply
   31.
   abdullah kiwan
       [244]2015-05-14 at 9:38 pm
       hello,
       it is a great tutorial, thank you very much   .
       but i have a problem,
       i used the function ( accuracy ) to print the evaluation of the
       model , but nothing is printed to me
       how to sove this problem ??
       thanks a lot
       [245]reply
         1. radim post
            author
        radim
            [246]2015-07-09 at 8:42 pm
            try turning on logging     the accuracy may be printed to log.
            see the beginning of this tutorial for how to do that.
            [247]reply
   32.
   shuai wang
       [248]2015-07-09 at 8:08 pm
       great tutorial, radim! is it possible to download your trained
       model of 100 billion google words?
       [249]reply
         1. radim post
            author
        radim
            [250]2015-07-09 at 8:43 pm
            thanks shuai.
            yes, it is possible to download it:
            [251]https://code.google.com/p/id97/#pre-trained_word_and_
            phrase_vectors
            (the model is not mine, it was trained by tomas while at
            google).
            [252]reply
              1.
             shuai wang
                 [253]2015-07-09 at 9:32 pm
                 awesome! i just found it too. cheers.
                 [254]reply
   33.
   [255]swami iyer
       [256]2015-07-13 at 8:05 pm
       hi radim,
       i was wondering if it is possible to train a id97 model, not
       with sentences, but with input and output vectors built from the
       sentences in an application-specific manner?
       thanks.
       swami
       [257]reply
   34.
   burness duan
       [258]2015-07-18 at 1:23 pm
       hi, i   ve got a problem   overflowerror: python int too large to
       convert c long    when i run    model =
       gensim.models.id97(sentences, min_count=1)   . could you help me
       with it ?!
       [259]reply
         1. radim post
            author
        radim
            [260]2015-07-18 at 7:00 pm
            hello!
            it   s best to report such things on the mailing list, or on
            github, not on the blog:
            [261]http://radimrehurek.com/gensim/support.html
            for this particular error, check out [262]this github issue.
            [263]reply
   35.
   [264]paula langmo
       [265]2015-07-18 at 5:54 pm
       thank you very much.
       [266]reply
   36.
   magali demerchant
       [267]2015-07-28 at 7:53 pm
       thanks for writing this, it was quite helpful and told a lot
       [268]reply
   37. pingback: [269]id97 tutorial    rare technologies | d...
   38.
   mike
       [270]2015-08-10 at 6:16 pm
       i ran:
       model = gensim.models.id97(sentences, min_count=1)
       and got the following error:
       model = gensim.models.id97(sentences, min_count=1)
       traceback (most recent call last):
       file       , line 1, in
       model = gensim.models.id97(sentences, min_count=1)
       file    c:anaconda3libsite-packagesgensimmodelsid97.py   , line
       312, in __init__
       self.build_vocab(sentences)
       file    c:anaconda3libsite-packagesgensimmodelsid97.py   , line
       414, in build_vocab
       self.reset_weights()
       file    c:anaconda3libsite-packagesgensimmodelsid97.py   , line
       521, in reset_weights
       random.seed(uint32(self.hashfxn(self.index2word[i] +
       str(self.seed))))
       overflowerror: python int too large to convert to c long
       i am using python 3.4.3 in the anaconda 2.3.0-64bit distribution.
       i   d really like to be able to use this module, but it seems like
       there   s some fundamental issue for my computer.
       thanks!!
       [271]reply
         1. radim post
            author
        radim
            [272]2015-08-11 at 6:53 am
            hello mike, the fix was a part of gensim 0.12.1, released some
            time ago.
            what version of gensim are you using?
            [273]reply
              1.
             mike
                 [274]2015-08-11 at 8:19 pm
                 found the error   i was using    conda update gensim    but it
                 looks like their anaconda repository has not been
                 updated. i   ll let them know, since many people use
                 anaconda distrib.
                 i ran    pip install    upgrade gensim    and it got 0.12.1. i
                 had 10.1!!!
                 [275]reply
              2.
             mike
                 [276]2015-08-11 at 8:37 pm
                 ok, i updated and ran with the following input list of
                 sentences:
                 sentences
                 out[17]:
                 [[   human   ,
                    machine   ,
                    interface   ,
                    for   ,
                    lab   ,
                    abc   ,
                    computer   ,
                    applications   ],
                 [   a   ,
                    survey   ,
                    of   ,
                    user   ,
                    opinion   ,
                    of   ,
                    computer   ,
                    system   ,
                    response   ,
                    time   ],
                 [   the   ,    eps   ,    user   ,    interface   ,    management   ,
                    system   ],
                 [   system   ,    and   ,    human   ,    system   ,    engineering   ,
                    testing   ,    of   ,    eps   ],
                 [   relation   ,
                    of   ,
                    user   ,
                    perceived   ,
                    response   ,
                    time   ,
                    to   ,
                    error   ,
                    measurement   ],
                 [   the   ,    generation   ,    of   ,    random   ,    binary   ,
                    unordered   ,    trees   ],
                 [   the   ,    intersection   ,    graph   ,    of   ,    paths   ,    in   ,
                    trees   ],
                 [   graph   ,
                    minors   ,
                    iv   ,
                    widths   ,
                    of   ,
                    trees   ,
                    and   ,
                    well   ,
                    quasi   ,
                    ordering   ],
                 [   graph   ,    minors   ,    a   ,    survey   ]]
                 still got the same error:
                 in [16]: model =
                 gensim.models.id97.id97(sentences)
                 traceback (most recent call last):
                 file       , line 1, in
                 model = gensim.models.id97.id97(sentences)
                 file
                    c:anaconda3libsite-packagesgensimmodelsid97.py   ,
                 line 417, in __init__
                 self.build_vocab(sentences)
                 file
                    c:anaconda3libsite-packagesgensimmodelsid97.py   ,
                 line 483, in build_vocab
                 self.finalize_vocab() # build tables & arrays
                 file
                    c:anaconda3libsite-packagesgensimmodelsid97.py   ,
                 line 611, in finalize_vocab
                 self.reset_weights()
                 file
                    c:anaconda3libsite-packagesgensimmodelsid97.py   ,
                 line 888, in reset_weights
                 self.syn0[i] = self.seeded_vector(self.index2word[i] +
                 str(self.seed))
                 file
                    c:anaconda3libsite-packagesgensimmodelsid97.py   ,
                 line 900, in seeded_vector
                 once =
                 random.randomstate(uint32(self.hashfxn(seed_string)))
                 overflowerror: python int too large to convert to c long
                 [277]reply
                   1. radim post
                      author
                  radim
                      [278]2015-08-12 at 5:24 am
                      is python picking up the right gensim?
                      afaik anaconda has its own packaging system, i   m not
                      sure how it plays with your `pip install`.
                      what does `import gensim; print gensim.__version__`
                      say?
                      [279]reply
   39.
   mike
       [280]2015-08-10 at 7:07 pm
       ok   i was able to make a couple changes to id97.py to get it to
       run on my computer:
       the current version uses numpy.uint32 on lines 83, 327, 373, and
       522. this was causing an overflow error when converting to c long.
       i changed these to reference numpy.uint64 and it *almost*
       worked   .the use of uint64 on line 522 for setting the seed of the
       random number generator resulted in a seed value being out of
       bounds. i addressed this by truncating the seed to the max
       allowable seed:
          random.seed(min(uint64(self.hashfxn(self.index2word[i] +
       str(self.seed))),4294967295))   
       now everything runs fine (except that my version is not compiled
       under c so i may see some performance issues for large coropra)   
       [281]reply
   40.
   mike
       [282]2015-08-10 at 8:49 pm
       actually, there is a solution on kaggle for 64-bit machines that
       worked really well (do not use my solution   it results in all word
       vectors being collinear).
       def hash32(value):
       return hash(value) & 0xffffffff
       then pass the following arugument to id97: hashfxn=hash32
       this will overwrite the base hashfxn and resolve the issues. also,
       all my cosine similarities were not 1 now!!
       [283]reply
   41.
   alexis c
       [284]2015-08-13 at 3:20 pm
       beware of how you go through your training data :
       when, in your class    mysentences    you use :
          for line in open(os.path.join(self.dirname, fname)):    
       as far as i know, it won   t close your file. you   re letting the
       garbage collector of python deal with the leak in memory.
       use :
          with gzip.open(os.path.join(self.dirname, fname)) as f:   
       instead (ref :
       [285]http://stackoverflow.com/questions/7395542/is-explicitly-closi
       ng-files-important )
       for training on large dataset, it can be a major bottleneck (it was
       for me      ).
       thank you very much for your fast implementation of id97 and
       doc2vec !
       [286]reply
         1. radim post
            author
        radim
            [287]2015-08-13 at 4:10 pm
            no, cpython closes the file immediately after the object goes
            out of scope. there is no leak (though that   s a common
            misconception and a favourite nitpick).
            with gzip it makes more sense, but then you should be using
            [288]smart_open anyway (also to work around missing context
            managers in python 2.6).
            [289]reply
   42.
   rodolpho rosa
       [290]2015-09-09 at 5:43 pm
       hi, radim.
       great tutorial.
       i have a doubt. is there any way i can represent a phrase as a
       vector so that i can calculate similarity between phrases just as
       what we do with words?
       [291]reply
         1. radim post
            author
        [292]radim
            [293]2015-09-10 at 2:01 am
            thanks rodolpho!
            yes there is; check out the doc2vec tutorial.
            [294]reply
   43.
   jin
       [295]2015-09-25 at 6:39 am
       hi radim,
       first of thank you very much for your amazing work and even more
       amazing tutorial.
       i am currently trying to compare two set of phrases.
       i am using googlenews model as my model
       splited all the words into individual words by using .split()
       ie.
       [   golf   ,   field   ] and [   country   ,   club   ]
       [   gas   ,   station   ] and [   fire   ,   station   ]
       as per feature in your app    phrase suggestions    
       i can see that googlenews model have
       county_club
       fire_station
       gas_station
       golf_field
       but it   s difficult to scan for those words because of
       capitalization in gn model.
       i tried.
       model.vocab.keys()
       which would convert all available names into a list.
       but couldn   t get any close to your example above.
       i also looked at gensim.models.phrase.phrases
       hoping that it can help me detect above example with bigram
       or trigram
       for those who are using gn model,
       how could we detect bigram or trigram?
       thank you in advance.
       [296]reply
         1. radim
        radim
            [297]2015-09-25 at 8:24 am
            as far as i know, google didn   t release their
            vocabulary/phrase model, nor their text preprocessing method.
            the only thing you have to go by are the phrases inside the
            model itself (3 million of them), sorted by frequency.
            you can lowercase the model vocabulary and match against that,
            but note that you   ll lose some vectors (no way to tell
            county_club from county_club from county_club).
            you can also try asking at the gensim mailing list, or tomas
            mikolov at his mailing list     better chance someone may have
            an answer or know something.
            [298]reply
              1.
             jin
                 [299]2015-09-25 at 3:18 pm
                 hi radim,
                 thanks for your reply,
                 i went ahead and created a small function which would
                 create bigram and replace the original words if bigram
                 exist in googlenews model.
                 and yes, i will join google mailing group.
                 ####################################################
                 # bigram creator
                 # try to capture fire_station or fire_station rather than
                 using    fire       station    seperately
                 # creating bigram
                 def create_bigram(list):
                 for i in range(0,len(list)-1):
                 #ie. country_club
                 word1 = list[i]+   _   +list[i+1]
                 #ie. country_club
                 word2 = list[i].capitalize()+   _   +list[i+1]
                 #ie. country_clue
                 word3 = list[i].capitalize()+   _   +list[i+1].capitalize()
                 #ie. country_clue
                 word4 = (list[i]+   _   +list[i+1]).upper()
                 word_list = [word1,word2,word3,word4]
                 for item in word_list:
                 print item
                 if item in model.vocab:
                 list.pop(i)
                 list.pop(i)
                 list.append(item)
                 break
                 [300]reply
                   1.
                  jin
                      [301]2015-09-25 at 4:08 pm
                      here   s fixed code,
                      added a line that will not append new word if
                      len(list) gets shorter
                      ####################################################
                      # bigram creator
                      # try to capture fire_station or fire_station rather
                      than using    fire       station    seperately
                      # creating bigram
                      def create_bigram(list):
                      for i in range(0,len(list)-1):
                      if i < len(list)-1:
                      #ie. country_club
                      word1 = list[i]+'_'+list[i+1]
                      #ie. country_club
                      word2 = list[i].capitalize()+'_'+list[i+1]
                      #ie. country_clue
                      word3 =
                      list[i].capitalize()+'_'+list[i+1].capitalize()
                      #ie. country_clue
                      word4 = (list[i]+'_'+list[i+1]).upper()
                      word_list = [word1,word2,word3,word4]
                      for item in word_list:
                      print i
                      if item in model.vocab:
                      list.pop(i)
                      list.pop(i)
                      list.append(item)
                      break
                      [302]reply
   44.
   sangram
       [303]2015-10-09 at 10:45 am
       i am following a tutorial of doc2vec from
       [304]http://districtdatalabs.silvrback.com/modern-methods-for-senti
       ment-analysis
       but while calling
       model_dm.build_vocab(np.concatenate((x_train, x_test,
       unsup_reviews)))
       i am getting an error:
          numpy.ndarray    object has no attribute    words   
       it seems like this error occurs at document.words in doc2vec.py.
       what am i missing here?
       [305]reply
   45.
   lis
       [306]2015-10-26 at 4:07 am
       i trained 2 doc2vec models with the same data, and parameters:
       model = doc2vec(sentences, dm=1, size=300, window=5, negative=10,
       hs=1, sample=1e-4, workers=20, min_count=3)
       but i got 2 different models in each time. is this true?
       can you explain more details for me?
       is that the case for id97 model?
       thanks radim!
       [307]reply
         1. radim post
            author
        [308]radim
            [309]2015-10-27 at 3:21 am
            hello lis,
            it   s best to use the mailing list for gensim support:
            [310]http://radimrehurek.com/gensim/support.html
            you   ll get the quickest and most qualified answers there     
            [311]reply
   46.
   hug
       [312]2015-11-17 at 2:09 pm
       hi radim,
       thanks for this amazing python version of id97!
       i have come to a strange behaviour after training; and i wanted to
       mention it here to you.
       so when i trained id97 model, with default parameters (namely
       the skip-gram model), the results where coherent with what is
       reported (in this blog and in papers..).
       when i used the pre-trained    vectors.bin    model from c version of
       id97 from tomas, loaded in gensim, everything seems fine as
       well (notice that the default model of c version is cbow).
       then i tried to train the gensim id97 with default parameters
       used in c version (which are: size=200, workers=8, window=8, hs=0,
       sampling=1e-4, sg=0 (using cbow), negative=25 and iter=15) and i
       got a strange    squeezed    or shrank vector representation where most
       of computed    most_similar    words shared a value of roughly 0.97!!
       (and from the classical    king   ,    man   ,    woman    the most similar
       will be    and    with 0.98, and in the top 10 i don   t even have the
          queen      ). everything was train on the same text8 dataset.
       so i wondered if you saw such    wrong    training before, with those
       atypical characteristics (all words in roughly one direction in
       vector space) and if you know where might be the issue.
       i am trying different parameters setting to hopefully figure out
       what is wrong (workers>1? iter?).
       thanks for any comment,
       cheers
       [313]reply
         1. radim post
            author
        [314]radim
            [315]2015-11-17 at 2:27 pm
            thanks hug!
            i didn   t see such behaviour. would you mind posting this info
            (plus any other version/data info you might have) on the
            [316]gensim mailing list?
            [317]reply
              1.
             hug
                 [318]2015-11-17 at 2:37 pm
                 i   ll do thanks for the reply!
                 [319]reply
   47.
   rahulvks
       [320]2015-11-18 at 7:03 am
       cant import id97
       runtimerror     kindly any on help
       traceback (most recent call last):
       file    /users/apple/documents/w2c.py   , line 15, in
       model = id97.id97(sentences, size=100, window=4,
       min_count=1, workers=4)
       file
          /library/frameworks/python.framework/versions/2.7/lib/python2.7/si
       te-packages/gensim/models/id97.py   , line 432, in __init__
       self.train(sentences)
       file
          /library/frameworks/python.framework/versions/2.7/lib/python2.7/si
       te-packages/gensim/models/id97.py   , line 690, in train
       raise runtimeerror(   you must first build vocabulary before training
       the model   )
       runtimeerror: you must first build vocabulary before training the
       model
       [321]reply
   48.
   rahulvks
       [322]2015-11-18 at 7:05 am
       cant import id97
       runtimerror     kindly any on help
       traceback (most recent call last):
       file
          /library/frameworks/python.framework/versions/2.7/lib/python2.7/si
       te-packages/gensim/models/id97.py   , line 690, in train
       raise runtimeerror(   you must first build vocabulary before training
       the model   )
       runtimeerror: you must first build vocabulary before training the
       model
       [323]reply
   49.
   siva
       [324]2015-11-21 at 5:23 pm
       hi radim,
       great tutorial.
       i have trained 17 million sentences with my i5core and 4gb ram.
       during the process it hanged a bit, but somehow i managed to save
       the model so that i can load it in future. the saved model is
       pretty huge with three files summing up to 1.2gb. so, whenever i
       load the model the system hangs and gets super slow. is there any
       workaround for this problem or its just about upgrading ram ?.
       is there any command available to determine the vocabulary
       frequencies from the saved model without the help of importing the
       training dataset furthermore ?
       thanks.
       [325]reply
         1. radim post
            author
        [326]radim
            [327]2015-11-22 at 11:28 am
            hello siva,
            yes, you have several options there:
            * [328]model.init_sims(replace=true), to remove unneeded files
            and save memory, if you don   t want to continue training.
            * [329]estimate_memory, to estimate required memory
            * look at the log from training, which contains a lot of
            useful, detailed information and numbers. always a good idea
            to store and inspect the log.
            hope that helps,
            radim
            [330]reply
              1.
             siva
                 [331]2015-11-22 at 12:08 pm
                 i will work on it. thanks for the reply   
                 [332]reply
              2.
             rahul
                 [333]2015-11-23 at 7:55 am
                 hi radim
                 cant import id97 in python showing
                 traceback (most recent call last):
                 file
                    /library/frameworks/python.framework/versions/2.7/lib/py
                 thon2.7/site-packages/gensim/models/id97.py   , line
                 690, in train
                 raise runtimeerror(   you must first build vocabulary
                 before training the model   )
                 runtimeerror: you must first build vocabulary before
                 training the model
                 how to train vocabulary.
                 thank you
                 [334]reply
                   1.
                  kavan
                      [335]2016-03-18 at 6:23 pm
                      hey rahul,
                      you need to build vocab before training.
                      you need to execute command written in 5th line of
                         preparing input    section
                      example if you have input sentences: [i like ice
                      cream],[i enjoy sleeping]
                      you need to split every word of each sentence
                      sentence =
                      [[   i   ,   like   ,   ice   ,   cream   ],[   i   ,   enjoy   ,   sleeping   ]
                      ]
                      you can split sentences into words by nltk library.
                      i just saw your doubt, thought this will be helpful
                          
                      [336]reply
   50.
   lello
       [337]2015-11-25 at 12:48 pm
       baby skynet is in distress, it makes me laugh no more, please fix
       it
       [338]reply
   51.
   vikram
       [339]2015-12-06 at 4:23 pm
       what exactly does the output represent when i do model[   someword   ]?
       [340]reply
   52.
   sahisnu
       [341]2015-12-19 at 10:55 am
       hi radim,
       thanks for your wonderful tutorial and the package!
       i was trying to execute the sample code (given in the tutorial) as
       shown below:
       =====================code====================================
       in[2]: import gensim, logging
       in[3]: logging.basicconfig(format=   %(asctime)s : %(levelname)s :
       %(message)s   , level=logging.info)
       in[4]: sentences = [[   first   ,    sentence   ], [   second   ,    sentence   ]]
       in[5]: model = gensim.models.id97(sentences, min_count=1)
       ==============================================================
       but, it didn   t work properly and the terminated with the following
       error:
       =======================output==============================
       2015-12-19 03:36:41,976 : info : collecting all words and their
       counts
       2015-12-19 03:36:41,976 : info : progress: at sentence #0,
       processed 0 words, keeping 0 word types
       2015-12-19 03:36:41,977 : info : collected 3 word types from a
       corpus of 4 raw words and 2 sentences
       2015-12-19 03:36:41,977 : info : min_count=1 retains 3 unique words
       (drops 0)
       2015-12-19 03:36:41,977 : info : min_count leaves 4 word corpus
       (100% of original 4)
       2015-12-19 03:36:41,977 : info : deleting the raw counts dictionary
       of 3 items
       2015-12-19 03:36:41,977 : info : sample=0 downsamples 0 most-common
       words
       2015-12-19 03:36:41,977 : info : downsampling leaves estimated 4
       word corpus (100.0% of prior 4)
       2015-12-19 03:36:41,977 : info : estimated required memory for 3
       words and 100 dimensions: 4500 bytes
       2015-12-19 03:36:41,978 : info : constructing a huffman tree from 3
       words
       2015-12-19 03:36:41,978 : info : built huffman tree with maximum
       node depth 2
       2015-12-19 03:36:41,978 : info : resetting layer weights
       traceback (most recent call last):
       file
          /usr/lib/python2.7/dist-packages/ipython/core/interactiveshell.py   
       , line 2538, in run_code
       exec code_obj in self.user_global_ns, self.user_ns
       file       , line 1, in
       model = gensim.models.id97(sentences, min_count=1)
       file
          /home/sahisnu/.local/lib/python2.7/site-packages/gensim/models/wor
       d2vec.py   , line 431, in __init__
       self.build_vocab(sentences, trim_rule=trim_rule)
       file
          /home/sahisnu/.local/lib/python2.7/site-packages/gensim/models/wor
       d2vec.py   , line 497, in build_vocab
       self.finalize_vocab() # build tables & arrays
       file
          /home/sahisnu/.local/lib/python2.7/site-packages/gensim/models/wor
       d2vec.py   , line 627, in finalize_vocab
       self.reset_weights()
       file
          /home/sahisnu/.local/lib/python2.7/site-packages/gensim/models/wor
       d2vec.py   , line 958, in reset_weights
       self.syn0[i] = self.seeded_vector(self.index2word[i] +
       str(self.seed))
       file
          /home/sahisnu/.local/lib/python2.7/site-packages/gensim/models/wor
       d2vec.py   , line 970, in seeded_vector
       once = random.randomstate(self.hashfxn(seed_string) & 0xffffffff)
       file    mtrand.pyx   , line 561, in mtrand.randomstate.__init__
       (numpy/random/mtrand/mtrand.c:4716)
       file    mtrand.pyx   , line 597, in mtrand.randomstate.seed
       (numpy/random/mtrand/mtrand.c:4941)
       valueerror: object of too small depth for desired array
       =================================================================
       i searched a lot in the web but couldn   t figure the problem out. i
       also tried to execute it with my own corpus and getting the same
       error. am i missing something required for the successful execution
       of the code? can you please tell me how can i eliminate the error?
       please help!
       with thanks,
       sahisnu
       [342]reply
         1.
        sahisnu
            [343]2015-12-20 at 12:52 am
            solved the problem!
            actually, i was using older version of gensim, numpy and
            scipy   .
            upgraded all and the problem got solved.
            [344]reply
              1.
             khronos
                 [345]2016-05-27 at 9:07 am
                 hi,sahinu
                 recently, i have the same problem with you and i   m using
                 the latest of gensim. i have searched a lot in the web
                 but couldn   t figure it out.can you please help me? and
                 please contact me with my e-mail:[346][email protected]
                 thank you very much!
                 lilingao
                 [347]reply
   53.
   [348]duytinvo
       [349]2016-01-01 at 10:37 am
       hi radim,
       thanks for your great work. i downloaded your newest package and
       read through the id97 code. i saw that in lines 255-260, you
       simultaneously update model.syn1neg[word_indices] or l2b (using l1)
       and l1 (using l2b). i think it is necessary to add deepcopy in line
       255!!! is it correct???
       thank you,
       best regards,
       tin
       [350]reply
   54.
   [351]xiaoshan yang
       [352]2016-01-06 at 6:05 pm
       could you help me with this problem ?
       i use a simple python function load_bin_vec shown as follows to
       load the google pretrained .bin model. but i find that the outputs
       are different from the results using the load_id97_format
       function in gensim.models.id97.
       for example:
       for the word    woman   ,
       the vectors loaded by load_bin_vec function return:
       [ 2.43164062e-01 -7.71484375e-02 -1.03027344e-01 -1.07421875e-01    ]
       while the vectors loaded by load_id97_format function return:
       [ 9.15656984e-02 -2.90509649e-02 -3.87959108e-02    ]
       def load_bin_vec(fname):
                
       loads 300  1 word vecs from google (mikolov) id97
                
       word_vecs = {}
       with open(fname,    rb   ) as f:
       header = f.readline()
       vocab_size, layer1_size = map(int, header.split())
       binary_len = np.dtype(   float32   ).itemsize * layer1_size
       for line in xrange(vocab_size):
       word = []
       while true:
       ch = f.read(1)
       if ch ==        :
       word =    .join(word)
       break
       if ch !=    n   :
       word.append(ch)
       word_vecs[word] = np.fromstring(f.read(binary_len),
       dtype=   float32   )
       return word_vecs
       [353]reply
         1. radim post
            author
        [354]radim
            [355]2016-01-07 at 3:45 am
            hello xiaoshan yang,
            the best medium for gensim support is its mailing list:
            [356]http://radimrehurek.com/gensim/support.html
            [357]reply
              1.
             [358]xiaoshan yang
                 [359]2016-01-07 at 4:47 am
                 thanks for your replay
                 [360]reply
   55. pingback: [361]relatively prime season 2 | god plays dice
   56.
   sam
       [362]2016-01-11 at 10:15 am
       thanks for the tutorial. i   m very new to id97 and so greatly
       appreciate help here. do i have to remove stopwords from my input
       text? because i could see words like    of   ,    when   .. when i do
          model.most_similar(   someword   )..?
       but i haven   t seen that stopword removal is done with id97 when
       i was reading things?
       [363]reply
         1. radim post
            author
        [364]radim
            [365]2016-01-11 at 10:41 am
            hello sam,
            you can remove stopwords (any other other words) either as
            part of your sentences (mysentences in the code above).
            or, keep sentences as-is and add a post-filtering step over
            them:
            >>> model = gensim.models.id97()
            >>> stopword_set = set(   ) # your set of words you don   t want
            >>> discard_stopwords = lambda: ((word for word in sentence if
            word not in stopword_set) for sentence in sentences)
            >>> model.build_vocab(discard_stopwords()))
            >>> model.train(discard_stopwords())
            in general, questions like this are best posted to the
            [366]gensim mailing list, so others can benefit from the
            discussion.
            best,
            radim
            [367]reply
              1.
             sam
                 [368]2016-01-12 at 2:06 am
                 hello radim,
                 thanks for the quick response. i actually have made a
                 post there
                 ([369]https://groups.google.com/forum/#!topic/gensim/njx3
                 pmlzaws) but didn   t get any replies.
                 just one more thing, is it technically wrong to apply
                 model.most_similar(   apple   ,topn=20) on a small text file,
                 say of 2000 tweets?
                 [370]reply
   57.
   rachana
       [371]2016-02-08 at 10:27 am
       i want to know internal working of id97 with example like what
       we have to pass as an input an one hot representation of word or
       frequency of word and as an output what we can get and how?
       [372]reply
         1. radim rehurek post
            author
        radim rehurek
            [373]2016-02-08 at 11:29 am
            hello rachana,
            i didn   t really understand your question, but the best place
            to ask is the gensim mailing list:
            [374]https://groups.google.com/d/forum/gensim
            best,
            radim
            [375]reply
   58.
   rachana
       [376]2016-02-08 at 6:25 pm
       thanks for reply , my question is very simple how does id97
       work internally in neural networid116 as an input what we have to
       pass and as an output what we can get for deep learning further
       processing like this
       [377]https://www.kaggle.com/c/id97-nlp-tutorial/details/part-2-
       word-vectors and what is the good example of id97?
       [378]reply
   59.
   max
       [379]2016-02-29 at 10:33 am
       hi, i am getting the following error when i try to load a saved
       model. could you please let me know how to fix it?
       ioerror: [errno 2] no such file or directory:
          wiki_trained.model.syn1neg.npy   
       [380]reply
         1.
        martin
            [381]2016-03-15 at 1:16 pm
            i am getting the same error on my notebook (ubuntu 14.04).
            have you found a solution?
            [382]reply
   60.
   shreya
       [383]2016-03-03 at 6:38 pm
       hello,
       is there a way to have two different values for min_count in the
       same model?
       for ex: i would like min_count = 3 but i also need some
       representation for words in sentences which occur only once. is
       this possible with in a single model?
       thanks,
       shreya
       [384]reply
         1. radim rehurek post
            author
        [385]radim rehurek
            [386]2016-03-04 at 2:00 am
            hello shreya,
            yes, there is. have a look at the [387]`trim_rule` parameter
            of id97.
            for further questions, please use the [388]gensim mailing
            list.
            [389]reply
   61.
   kavan
       [390]2016-03-18 at 6:07 pm
       hello,
       i have few questions. from where do you find such huge data? is
       there any pre-processing to be done on textual data before training
       id97?
       [391]reply
         1. radim   eh    ek post
            author
        [392]radim   eh    ek
            [393]2016-03-19 at 3:11 am
            the 100-billion-word googlenews corpus was actually prepared
            by google themselves. i don   t think it   s public.
            preprocessing: depends on the app. we usually do careful
            preprocessing; google didn   t do almost any for the googlenews
            model      you   ll see it in the word suggestions in the    bonus
            app    above. there   s lots of words like    ###    and rubbish
            characters, typos, uppercase variants etc.
            [394]reply
              1.
             kavan
                 [395]2016-03-19 at 9:22 am
                 thank you for replying sir. so can you suggest from where
                 can i get huge textual data that is publicly available?
                 [396]reply
                   1. radim   eh    ek post
                      author
                  [397]radim   eh    ek
                      [398]2016-03-19 at 11:52 am
                      wikipedia is one example. you can find a script for
                      automatically preprocessing wikipedia into plain
                      text here:
                      [399]https://github.com/piskvorky/sim-shootout
                      then there   s various web crawls etc. depends what
                         huge    means for you    if 20newsgroups does it for
                      you, that   s great.
                      [400]reply
                        1.
                       kavan
                           [401]2016-04-11 at 7:48 am
                           can you recommend me small data which yields
                           good id97 results?
              2.
             kavan
                 [402]2016-03-19 at 9:40 am
                 to add above comment, i found data here
                 [403]http://qwone.com/~jason/20newsgroups/ . each file
                 contains some information and data. information like from
                 , subject , organization etc. am i supposed to remove
                 such enitites and just keep the raw text data?
                 [404]reply
   62.
   heny chang
       [405]2016-04-01 at 10:03 pm
       hi radim,
       i would like to understand the right way to resume a id97 model
       and continue the training process. there was no issue that i could
       save and load the model, and continue training. but, i couldn   t
       keep the old vocabs built. i used the simple script to test and got
       confused. could you help?
       when i did
       ===
       some_sentences = [[   first   ,    sentence   ], [   second   ,    sentence   ]]
       model = id97(min_count=1)
       model.build_vocab(some_sentences)
       model.train(some_sentences)
       print model.similarity(   first   ,   second   ) # no problem
       other_sentences = [[   third   ,    sentence   ], [   fourth   ,    sentence   ]]
       model.build_vocab(other_sentences)
       model.train(other_sentences)
       print model.similarity(   third   ,    fourth   ) # no problem
       print model.similarity(   first   ,   second   ) # the vocabs in
       some_sentences are no longer available???
       ===
       it complained
       -0.0450417522552
       0.00356975799328
       traceback (most recent call last):
       file    test.py   , line 16, in
       print model.similarity(   first   ,   second   )
       file
          /usr/local/lib/python2.7/dist-packages/gensim/models/id97.py   ,
       line 1233, in similarity
       return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2]))
       file
          /usr/local/lib/python2.7/dist-packages/gensim/models/id97.py   ,
       line 1213, in __getitem__
       return self.syn0[self.vocab[words].index]
       keyerror:    first   
       ===
       furthermore,
       model = id97() # an empty model, no training
       model.build_vocab(some_sentences)
       model.train(other_sentences)
       what is this for? after the execution, i can find the similarity
       between vocabs in some_sentences, but no similarity between vocabs
       in other_sentences. vocabs in some_sentences were built but not
       trained. vocabs in other_sentences were trained but not built. do
       you have a use case to explain me the relationship? i general, we
       should have all vocabs built and trained first so that we are able
       to get the relataion between any of them, right? after loading, the
       old vocabs tree should be there, and allow adding new vocabs to the
       existing tree and training the new vocabs. is my understanding
       correct?
       best,
       henry
       [406]reply
         1. radim   eh    ek post
            author
        [407]radim   eh    ek
            [408]2016-04-02 at 2:10 am
            hello henry,
            the best place for such questions is the mailing list:
            [409]http://radimrehurek.com/gensim/support.html
            in this case, the problem comes from the fact the vocabulary
            scan is only done once. you can continue training on new
            sentences, but cannot add any new vocabulary. there is an
            ongoing work (pull request) to allow dynamic training
            including new vocabulary in gensim, but it   s not finished yet.
            if you have any follow up questions, please use the
            [410]mailing list.
            [411]reply
              1.
             henry chang
                 [412]2016-04-04 at 6:21 pm
                 many thanks for the clarification, radim.
                 i also read the post at
                 [413]http://rutumulkar.com/blog/2015/id97/
                 last friday. i thought what i needed could be done by
                 model.build_vocab(some_sentences, keep_raw_vocab=true)
                 after model loading.
                 best,
                 henry
                 [414]reply
   63.
   paridhi
       [415]2016-04-10 at 7:42 am
       i have made my model and also saved it. i can see the model in my
       folder but the content is gebbrish. also when i am trying to run a
       query like model.similarity(   iphone   ,    battery   ) , i am getting a   
       keyerror: iphone     error.
       how am i supposed to query it???
       [416]reply
         1.
        paridhi
            [417]2016-04-10 at 9:39 am
            give me some way to input my entire txt file instead of
            individual sentences. my file is    ../script/join.txt   
            [418]reply
   64.
   atul
       [419]2016-05-13 at 12:01 pm
       can anyone tell me how to know what are the words int the vocab?
       how the words in vocab are stored? is it based on the description
       which we are giving as input from where it get the vocab?
       kindly reply
       [420]reply
         1. radim   eh    ek post
            author
        [421]radim   eh    ek
            [422]2016-05-17 at 2:31 am
            hello atul,
            such questions are best answered by the gensim commmunity on
            our mailing list:
            [423]http://radimrehurek.com/gensim/support.html
            best,
            radim
            [424]reply
   65. pingback: [425]       gensim     id97        -             
   66. pingback: [426]dupe snoop: identify duplicates on quora | jana
       grcevich
   67. pingback: [427]getting started with id97 | textprocessing | a
       text processing portal for humans
   68. pingback: [428]exploiting wikipedia word similarity by id97    
       id111 online
   69. pingback: [429]google news id97 | cm
   70. pingback: [430]google news id97 | ch
   71. pingback: [431]understanding word vectors and id97     stokastik
   72. pingback: [432]sentence based similarity     research & expreimental
       blog
   73. pingback: [433]id97     idatamining.net
   74. pingback: [434]error code 2538
   75. pingback: [435]semantic analysis for new product development: do
       you care about language? - innoradiant
   76. pingback: [436]how to develop id27s in python with gensim
       | a bunch of data
   77. pingback: [437]id116 id91 example with id97 in data
       mining or machine learning - text analytics techniques
   78. pingback: [438]ai is not just learning our biases; it is amplifying
       them. | copy paste programmers
   79. pingback: [439]curso de procesamiento de textos     gensim & python -
       aprender python desde cero - aprender python online gratis
   80. pingback: [440]id27s in python with spacy and gensim |
       shane lynn
   81. pingback: [441]       gensim     id97        -             
   82.
   alan
       [442]2018-06-15 at 2:45 am
       online training / resuming training
       it can not add new word in model, you can use
       new_model.build_vocab(new_tl, update=true),set    update=true    to
       update the word in your model
       ref:
       [443]http://www.muzhen.tk/2017/06/21/machine%20learning/nlp/gensim%
       20w2v/
       [444]reply
   83. pingback: [445]how to develop id27s in python with gensim
           book of ai
   84. pingback: [446]id97: how to train and update it | machine
       learning and statistics

leave a reply [447]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   submit

   current [448][email protected] * 4.2_________________

   leave this field empty ____________________

author of post

   radim   eh    ek

radim   eh    ek's bio:

   founder at rare technologies, creator of gensim. sw engineer since
   2004, phd in ai in 2011. lover of geology, history and beginnings in
   general. occasional travel blogger.

need expert consulting in ml and nlp?

   ________________________________________

   ________________________________________


   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   please leave this field empty. ________________________________________

   send

categories

   categories[select category___________]

archives

   archives [select month__]

recent posts

     * [449]export pii drill-down reports
     * [450]personal data analytics
     * [451]scanning office 365 for sensitive pii information
     * [452]pivoted document length normalisation
     * [453]sent2vec: an unsupervised approach towards learning sentence
       embeddings

stay ahead of the curve

get our latest tutorials, updates and insights delivered straight to your
inbox.

   ____________________

   ____________________

   subscribe
   ____________________
   1-2 times a month, if lucky. your information will not be shared.

   [454][footer-logo.png]
     * [455]services
     * [456]careers
     * [457]our team
     * [458]corporate training
     * [459]blog
     * [460]incubator
     * [461]contact
     * [462]competitions
     * [463]site map

   rare technologies [464][email protected] sv  tova 5, prague, czech
   republic [465](eu) +420 776 288 853

   type and press    enter    to search ____________________

references

   visible links
   1. https://rare-technologies.com/feed/
   2. https://rare-technologies.com/comments/feed/
   3. https://rare-technologies.com/id97-tutorial/feed/
   4. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/id97-tutorial/
   5. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/id97-tutorial/&format=xml
   6. https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld
   7. https://rare-technologies.com/
   8. https://rare-technologies.com/id97-tutorial/
   9. https://rare-technologies.com/services/
  10. https://rare-technologies.com/id97-tutorial/
  11. https://pii-tools.com/
  12. https://scaletext.com/
  13. https://rare-technologies.com/corporate-training/
  14. https://rare-technologies.com/corporate-training/
  15. https://rare-technologies.com/python-best-practices/
  16. https://rare-technologies.com/practical-machine-learning/
  17. https://rare-technologies.com/topic-modelling-training/
  18. https://rare-technologies.com/deep_learning_training/
  19. https://rare-technologies.com/incubator
  20. https://github.com/rare-technologies/
  21. https://rare-technologies.com/incubator/
  22. https://rare-technologies.com/competitions/
  23. https://rare-technologies.com/#braintrust
  24. https://rare-technologies.com/careers/
  25. https://rare-technologies.com/our-team/
  26. https://rare-technologies.com/blog/
  27. https://rare-technologies.com/contact/
  28. https://rare-technologies.com/id97-tutorial/
  29. https://rare-technologies.com/services/
  30. https://rare-technologies.com/id97-tutorial/
  31. https://pii-tools.com/
  32. https://scaletext.com/
  33. https://rare-technologies.com/corporate-training/
  34. https://rare-technologies.com/corporate-training/
  35. https://rare-technologies.com/python-best-practices/
  36. https://rare-technologies.com/practical-machine-learning/
  37. https://rare-technologies.com/topic-modelling-training/
  38. https://rare-technologies.com/deep_learning_training/
  39. https://rare-technologies.com/incubator
  40. https://github.com/rare-technologies/
  41. https://rare-technologies.com/incubator/
  42. https://rare-technologies.com/competitions/
  43. https://rare-technologies.com/#braintrust
  44. https://rare-technologies.com/careers/
  45. https://rare-technologies.com/our-team/
  46. https://rare-technologies.com/blog/
  47. https://rare-technologies.com/contact/
  48. https://rare-technologies.com/id97-tutorial/
  49. https://rare-technologies.com/author/radim/
  50. https://rare-technologies.com/category/gensim/
  51. https://rare-technologies.com/category/programming/
  52. https://rare-technologies.com/id97-tutorial/#comments
  53. http://radimrehurek.com/gensim/models/id97.html
  54. https://github.com/rare-technologies/w2v_server_googlenews
  55. https://scaletext.com/
  56. https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/
  57. http://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
  58. http://cython.org/
  59. https://wiki.python.org/moin/globalinterpreterlock
  60. http://radimrehurek.com/2013/09/id97-in-python-part-two-optimizing/
  61. https://raw.githubusercontent.com/rare-technologies/gensim/develop/gensim/test/test_data/questions-words.txt
  62. http://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.accuracy
  63. http://radimrehurek.com/2014/01/performance-shootout-of-nearest-neighbours-querying/#wikisim
  64. https://rare-technologies.com/id97-tutorial/
  65. https://rare-technologies.com/id97-tutorial/
  66. https://rare-technologies.com/id97-tutorial/
  67. https://rare-technologies.com/id97-tutorial/
  68. https://rare-technologies.com/id97-tutorial/
  69. https://rare-technologies.com/id97-tutorial/
  70. https://rare-technologies.com/id97-tutorial/
  71. https://rare-technologies.com/id97-tutorial/
  72. http://radimrehurek.com/2014/01/performance-shootout-of-nearest-neighbours-querying/#wikisim
  73. https://docs.python.org/2/library/bisect.html
  74. https://github.com/rare-technologies/w2v_server_googlenews
  75. http://radimrehurek.com/gensim/models/id97.html
  76. http://radimrehurek.com/gensim/
  77. https://code.google.com/p/id97/
  78. https://pydata.org/berlin2014/
  79. https://www.youtube.com/embed/vu4tlwzztfu?feature=oembed
  80. https://rare-technologies.com/tag/gensim/
  81. https://rare-technologies.com/tag/optimization/
  82. https://rare-technologies.com/tag/id97/
  83. https://rare-technologies.com/id97-tutorial/#comment-2259
  84. https://rare-technologies.com/id97-tutorial/?replytocom=2259#respond
  85. https://rare-technologies.com/id97-tutorial/#comment-2260
  86. https://rare-technologies.com/id97-tutorial/?replytocom=2260#respond
  87. https://rare-technologies.com/id97-tutorial/#comment-2261
  88. https://rare-technologies.com/id97-tutorial/?replytocom=2261#respond
  89. https://rare-technologies.com/id97-tutorial/#comment-2273
  90. https://rare-technologies.com/id97-tutorial/?replytocom=2273#respond
  91. http://radimrehurek.com/
  92. https://rare-technologies.com/id97-tutorial/#comment-2274
  93. https://rare-technologies.com/id97-tutorial/?replytocom=2274#respond
  94. https://rare-technologies.com/id97-tutorial/#comment-2354
  95. https://rare-technologies.com/id97-tutorial/?replytocom=2354#respond
  96. https://rare-technologies.com/id97-tutorial/#comment-2384
  97. https://rare-technologies.com/id97-tutorial/?replytocom=2384#respond
  98. https://rare-technologies.com/id97-tutorial/#comment-2262
  99. https://rare-technologies.com/id97-tutorial/?replytocom=2262#respond
 100. https://rare-technologies.com/id97-tutorial/#comment-2265
 101. https://github.com/piskvorky/gensim/pull/135
 102. https://rare-technologies.com/id97-tutorial/?replytocom=2265#respond
 103. https://rare-technologies.com/id97-tutorial/#comment-2263
 104. https://rare-technologies.com/id97-tutorial/?replytocom=2263#respond
 105. https://rare-technologies.com/id97-tutorial/#comment-2264
 106. https://rare-technologies.com/id97-tutorial/?replytocom=2264#respond
 107. https://rare-technologies.com/id97-tutorial/#comment-2266
 108. https://rare-technologies.com/id97-tutorial/?replytocom=2266#respond
 109. https://rare-technologies.com/id97-tutorial/#comment-2267
 110. https://rare-technologies.com/id97-tutorial/?replytocom=2267#respond
 111. http://radimrehurek.com/
 112. https://rare-technologies.com/id97-tutorial/#comment-2268
 113. https://github.com/piskvorky/gensim/pull/162
 114. https://rare-technologies.com/id97-tutorial/?replytocom=2268#respond
 115. https://rare-technologies.com/id97-tutorial/#comment-2269
 116. https://rare-technologies.com/id97-tutorial/?replytocom=2269#respond
 117. http://radimrehurek.com/
 118. https://rare-technologies.com/id97-tutorial/#comment-2270
 119. https://github.com/numpy/numpy/issues/4007
 120. https://github.com/piskvorky/gensim/issues
 121. https://rare-technologies.com/id97-tutorial/?replytocom=2270#respond
 122. http://bogdan-ivanov.com/
 123. https://rare-technologies.com/id97-tutorial/#comment-2271
 124. https://rare-technologies.com/id97-tutorial/?replytocom=2271#respond
 125. http://radimrehurek.com/
 126. https://rare-technologies.com/id97-tutorial/#comment-2272
 127. http://radimrehurek.com/gensim/models/id97.html
 128. https://rare-technologies.com/id97-tutorial/?replytocom=2272#respond
 129. https://rare-technologies.com/id97-tutorial/#comment-2275
 130. https://rare-technologies.com/id97-tutorial/?replytocom=2275#respond
 131. http://radimrehurek.com/
 132. https://rare-technologies.com/id97-tutorial/#comment-2276
 133. https://rare-technologies.com/id97-tutorial/?replytocom=2276#respond
 134. http://www-scf.usc.edu/~dpunjabi/
 135. https://rare-technologies.com/id97-tutorial/#comment-2391
 136. https://rare-technologies.com/id97-tutorial/?replytocom=2391#respond
 137. http://radimrehurek.com/
 138. https://rare-technologies.com/id97-tutorial/#comment-2392
 139. https://rare-technologies.com/id97-tutorial/?replytocom=2392#respond
 140. https://rare-technologies.com/id97-tutorial/#comment-2277
 141. https://rare-technologies.com/id97-tutorial/?replytocom=2277#respond
 142. http://radimrehurek.com/
 143. https://rare-technologies.com/id97-tutorial/#comment-2278
 144. https://rare-technologies.com/id97-tutorial/?replytocom=2278#respond
 145. https://rare-technologies.com/id97-tutorial/#comment-2279
 146. https://rare-technologies.com/id97-tutorial/?replytocom=2279#respond
 147. https://rare-technologies.com/id97-tutorial/#comment-2280
 148. https://rare-technologies.com/id97-tutorial/?replytocom=2280#respond
 149. http://radimrehurek.com/
 150. https://rare-technologies.com/id97-tutorial/#comment-2281
 151. https://rare-technologies.com/id97-tutorial/?replytocom=2281#respond
 152. https://rare-technologies.com/id97-tutorial/#comment-2282
 153. https://rare-technologies.com/id97-tutorial/?replytocom=2282#respond
 154. http://radimrehurek.com/
 155. https://rare-technologies.com/id97-tutorial/#comment-2286
 156. https://rare-technologies.com/id97-tutorial/?replytocom=2286#respond
 157. http://www.cbcity.de/review-pydata-berlin-2014-satellitenevent-zur-europython
 158. https://rare-technologies.com/id97-tutorial/#comment-2284
 159. https://rare-technologies.com/id97-tutorial/?replytocom=2284#respond
 160. http://radimrehurek.com/
 161. https://rare-technologies.com/id97-tutorial/#comment-2285
 162. https://github.com/piskvorky/gensim/issues/204#issuecomment-52093328
 163. https://rare-technologies.com/id97-tutorial/?replytocom=2285#respond
 164. https://rare-technologies.com/id97-tutorial/#comment-2287
 165. https://rare-technologies.com/id97-tutorial/?replytocom=2287#respond
 166. https://rare-technologies.com/id97-tutorial/#comment-2288
 167. https://rare-technologies.com/id97-tutorial/?replytocom=2288#respond
 168. http://radimrehurek.com/
 169. https://rare-technologies.com/id97-tutorial/#comment-2289
 170. https://github.com/piskvorky/gensim/issues/204
 171. https://rare-technologies.com/id97-tutorial/?replytocom=2289#respond
 172. http://www.smesh.net/
 173. https://rare-technologies.com/id97-tutorial/#comment-2290
 174. http://www.smesh.net/pages/980191274#54262e6cdb3c2facb5a41579
 175. https://rare-technologies.com/id97-tutorial/?replytocom=2290#respond
 176. https://rare-technologies.com/id97-tutorial/#comment-2291
 177. https://rare-technologies.com/id97-tutorial/?replytocom=2291#respond
 178. http://radimrehurek.com/
 179. https://rare-technologies.com/id97-tutorial/#comment-2292
 180. https://rare-technologies.com/id97-tutorial/?replytocom=2292#respond
 181. https://rare-technologies.com/id97-tutorial/#comment-2293
 182. https://rare-technologies.com/id97-tutorial/?replytocom=2293#respond
 183. http://radimrehurek.com/
 184. https://rare-technologies.com/id97-tutorial/#comment-2294
 185. https://rare-technologies.com/id97-tutorial/?replytocom=2294#respond
 186. https://rare-technologies.com/id97-tutorial/#comment-2295
 187. http://researchweb.iiit.ac.in/~satarupa.guha/
 188. https://rare-technologies.com/id97-tutorial/#comment-2296
 189. https://rare-technologies.com/id97-tutorial/?replytocom=2296#respond
 190. https://rare-technologies.com/id97-tutorial/#comment-2297
 191. https://rare-technologies.com/id97-tutorial/?replytocom=2297#respond
 192. http://radimrehurek.com/
 193. https://rare-technologies.com/id97-tutorial/#comment-2298
 194. http://radimrehurek.com/gensim/support.html
 195. https://rare-technologies.com/id97-tutorial/?replytocom=2298#respond
 196. https://rare-technologies.com/id97-tutorial/#comment-2299
 197. https://rare-technologies.com/id97-tutorial/?replytocom=2299#respond
 198. http://cluat.com/question/how-to-grow-a-list-of-related-words-based-on-initial-keywords/
 199. https://rare-technologies.com/id97-tutorial/#comment-2301
 200. https://rare-technologies.com/id97-tutorial/?replytocom=2301#respond
 201. https://rare-technologies.com/id97-tutorial/#comment-2302
 202. https://rare-technologies.com/id97-tutorial/?replytocom=2302#respond
 203. http://radimrehurek.com/
 204. https://rare-technologies.com/id97-tutorial/#comment-2303
 205. https://github.com/piskvorky/gensim/blob/develop/gensim/models/id97.py#l997
 206. http://mattmahoney.net/dc/text8.zip
 207. https://rare-technologies.com/id97-tutorial/?replytocom=2303#respond
 208. https://rare-technologies.com/id97-tutorial/#comment-2304
 209. https://rare-technologies.com/id97-tutorial/?replytocom=2304#respond
 210. http://radimrehurek.com/
 211. https://rare-technologies.com/id97-tutorial/#comment-2305
 212. https://rare-technologies.com/id97-tutorial/?replytocom=2305#respond
 213. https://rare-technologies.com/id97-tutorial/#comment-2306
 214. https://rare-technologies.com/id97-tutorial/?replytocom=2306#respond
 215. http://radimrehurek.com/
 216. https://rare-technologies.com/id97-tutorial/#comment-2307
 217. https://rare-technologies.com/id97-tutorial/?replytocom=2307#respond
 218. https://rare-technologies.com/id97-tutorial/#comment-2309
 219. https://rare-technologies.com/id97-tutorial/#comment-2308
 220. https://rare-technologies.com/id97-tutorial/?replytocom=2308#respond
 221. https://rare-technologies.com/id97-tutorial/#comment-2310
 222. https://rare-technologies.com/id97-tutorial/?replytocom=2310#respond
 223. https://rare-technologies.com/id97-tutorial/#comment-2311
 224. https://rare-technologies.com/id97-tutorial/?replytocom=2311#respond
 225. http://radimrehurek.com/
 226. https://rare-technologies.com/id97-tutorial/#comment-2312
 227. https://rare-technologies.com/id97-tutorial/?replytocom=2312#respond
 228. https://rare-technologies.com/id97-tutorial/#comment-2313
 229. https://rare-technologies.com/id97-tutorial/?replytocom=2313#respond
 230. https://rare-technologies.com/id97-tutorial/#comment-2314
 231. https://rare-technologies.com/id97-tutorial/?replytocom=2314#respond
 232. https://rare-technologies.com/id97-tutorial/#comment-2315
 233. https://rare-technologies.com/id97-tutorial/?replytocom=2315#respond
 234. https://rare-technologies.com/id97-tutorial/#comment-2316
 235. https://rare-technologies.com/id97-tutorial/?replytocom=2316#respond
 236. https://rare-technologies.com/id97-tutorial/#comment-2317
 237. https://rare-technologies.com/id97-tutorial/?replytocom=2317#respond
 238. https://rare-technologies.com/id97-tutorial/#comment-2318
 239. https://rare-technologies.com/id97-tutorial/?replytocom=2318#respond
 240. http://radimrehurek.com/
 241. https://rare-technologies.com/id97-tutorial/#comment-2319
 242. http://radimrehurek.com/gensim/support.html
 243. https://rare-technologies.com/id97-tutorial/?replytocom=2319#respond
 244. https://rare-technologies.com/id97-tutorial/#comment-2320
 245. https://rare-technologies.com/id97-tutorial/?replytocom=2320#respond
 246. https://rare-technologies.com/id97-tutorial/#comment-2322
 247. https://rare-technologies.com/id97-tutorial/?replytocom=2322#respond
 248. https://rare-technologies.com/id97-tutorial/#comment-2321
 249. https://rare-technologies.com/id97-tutorial/?replytocom=2321#respond
 250. https://rare-technologies.com/id97-tutorial/#comment-2323
 251. https://code.google.com/p/id97/#pre-trained_word_and_phrase_vectors
 252. https://rare-technologies.com/id97-tutorial/?replytocom=2323#respond
 253. https://rare-technologies.com/id97-tutorial/#comment-2324
 254. https://rare-technologies.com/id97-tutorial/?replytocom=2324#respond
 255. http://www.swamiiyer.net/
 256. https://rare-technologies.com/id97-tutorial/#comment-2325
 257. https://rare-technologies.com/id97-tutorial/?replytocom=2325#respond
 258. https://rare-technologies.com/id97-tutorial/#comment-2326
 259. https://rare-technologies.com/id97-tutorial/?replytocom=2326#respond
 260. https://rare-technologies.com/id97-tutorial/#comment-2328
 261. http://radimrehurek.com/gensim/support.html
 262. https://github.com/piskvorky/gensim/issues/392
 263. https://rare-technologies.com/id97-tutorial/?replytocom=2328#respond
 264. http://www.super-quality.com/pc
 265. https://rare-technologies.com/id97-tutorial/#comment-2327
 266. https://rare-technologies.com/id97-tutorial/?replytocom=2327#respond
 267. https://rare-technologies.com/id97-tutorial/#comment-2329
 268. https://rare-technologies.com/id97-tutorial/?replytocom=2329#respond
 269. http://www.scoop.it/t/data-is-big/p/4049207448/2015/08/10/id97-tutorial-rare-technologies
 270. https://rare-technologies.com/id97-tutorial/#comment-2331
 271. https://rare-technologies.com/id97-tutorial/?replytocom=2331#respond
 272. https://rare-technologies.com/id97-tutorial/#comment-2334
 273. https://rare-technologies.com/id97-tutorial/?replytocom=2334#respond
 274. https://rare-technologies.com/id97-tutorial/#comment-2335
 275. https://rare-technologies.com/id97-tutorial/?replytocom=2335#respond
 276. https://rare-technologies.com/id97-tutorial/#comment-2336
 277. https://rare-technologies.com/id97-tutorial/?replytocom=2336#respond
 278. https://rare-technologies.com/id97-tutorial/#comment-2337
 279. https://rare-technologies.com/id97-tutorial/?replytocom=2337#respond
 280. https://rare-technologies.com/id97-tutorial/#comment-2332
 281. https://rare-technologies.com/id97-tutorial/?replytocom=2332#respond
 282. https://rare-technologies.com/id97-tutorial/#comment-2333
 283. https://rare-technologies.com/id97-tutorial/?replytocom=2333#respond
 284. https://rare-technologies.com/id97-tutorial/#comment-2338
 285. https://stackoverflow.com/questions/7395542/is-explicitly-closing-files-important
 286. https://rare-technologies.com/id97-tutorial/?replytocom=2338#respond
 287. https://rare-technologies.com/id97-tutorial/#comment-2339
 288. https://github.com/piskvorky/smart_open
 289. https://rare-technologies.com/id97-tutorial/?replytocom=2339#respond
 290. https://rare-technologies.com/id97-tutorial/#comment-2340
 291. https://rare-technologies.com/id97-tutorial/?replytocom=2340#respond
 292. http://radimrehurek.com/
 293. https://rare-technologies.com/id97-tutorial/#comment-2341
 294. https://rare-technologies.com/id97-tutorial/?replytocom=2341#respond
 295. https://rare-technologies.com/id97-tutorial/#comment-2342
 296. https://rare-technologies.com/id97-tutorial/?replytocom=2342#respond
 297. https://rare-technologies.com/id97-tutorial/#comment-2343
 298. https://rare-technologies.com/id97-tutorial/?replytocom=2343#respond
 299. https://rare-technologies.com/id97-tutorial/#comment-2344
 300. https://rare-technologies.com/id97-tutorial/?replytocom=2344#respond
 301. https://rare-technologies.com/id97-tutorial/#comment-2345
 302. https://rare-technologies.com/id97-tutorial/?replytocom=2345#respond
 303. https://rare-technologies.com/id97-tutorial/#comment-2346
 304. http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis
 305. https://rare-technologies.com/id97-tutorial/?replytocom=2346#respond
 306. https://rare-technologies.com/id97-tutorial/#comment-2347
 307. https://rare-technologies.com/id97-tutorial/?replytocom=2347#respond
 308. http://radimrehurek.com/
 309. https://rare-technologies.com/id97-tutorial/#comment-2348
 310. http://radimrehurek.com/gensim/support.html
 311. https://rare-technologies.com/id97-tutorial/?replytocom=2348#respond
 312. https://rare-technologies.com/id97-tutorial/#comment-2349
 313. https://rare-technologies.com/id97-tutorial/?replytocom=2349#respond
 314. http://radimrehurek.com/
 315. https://rare-technologies.com/id97-tutorial/#comment-2350
 316. https://groups.google.com/forum/#!forum/gensim
 317. https://rare-technologies.com/id97-tutorial/?replytocom=2350#respond
 318. https://rare-technologies.com/id97-tutorial/#comment-2351
 319. https://rare-technologies.com/id97-tutorial/?replytocom=2351#respond
 320. https://rare-technologies.com/id97-tutorial/#comment-2352
 321. https://rare-technologies.com/id97-tutorial/?replytocom=2352#respond
 322. https://rare-technologies.com/id97-tutorial/#comment-2353
 323. https://rare-technologies.com/id97-tutorial/?replytocom=2353#respond
 324. https://rare-technologies.com/id97-tutorial/#comment-2355
 325. https://rare-technologies.com/id97-tutorial/?replytocom=2355#respond
 326. http://radimrehurek.com/
 327. https://rare-technologies.com/id97-tutorial/#comment-2356
 328. http://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.init_sims
 329. http://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.estimate_memory
 330. https://rare-technologies.com/id97-tutorial/?replytocom=2356#respond
 331. https://rare-technologies.com/id97-tutorial/#comment-2357
 332. https://rare-technologies.com/id97-tutorial/?replytocom=2357#respond
 333. https://rare-technologies.com/id97-tutorial/#comment-2358
 334. https://rare-technologies.com/id97-tutorial/?replytocom=2358#respond
 335. https://rare-technologies.com/id97-tutorial/#comment-2379
 336. https://rare-technologies.com/id97-tutorial/?replytocom=2379#respond
 337. https://rare-technologies.com/id97-tutorial/#comment-2359
 338. https://rare-technologies.com/id97-tutorial/?replytocom=2359#respond
 339. https://rare-technologies.com/id97-tutorial/#comment-2360
 340. https://rare-technologies.com/id97-tutorial/?replytocom=2360#respond
 341. https://rare-technologies.com/id97-tutorial/#comment-2361
 342. https://rare-technologies.com/id97-tutorial/?replytocom=2361#respond
 343. https://rare-technologies.com/id97-tutorial/#comment-2362
 344. https://rare-technologies.com/id97-tutorial/?replytocom=2362#respond
 345. https://rare-technologies.com/id97-tutorial/#comment-2395
 346. https://rare-technologies.com/cdn-cgi/l/email-protection
 347. https://rare-technologies.com/id97-tutorial/?replytocom=2395#respond
 348. https://istd.sutd.edu.sg/phd-students/vo-duy-tin/
 349. https://rare-technologies.com/id97-tutorial/#comment-2363
 350. https://rare-technologies.com/id97-tutorial/?replytocom=2363#respond
 351. http://yangxs.cc/
 352. https://rare-technologies.com/id97-tutorial/#comment-2364
 353. https://rare-technologies.com/id97-tutorial/?replytocom=2364#respond
 354. http://radimrehurek.com/
 355. https://rare-technologies.com/id97-tutorial/#comment-2365
 356. http://radimrehurek.com/gensim/support.html
 357. https://rare-technologies.com/id97-tutorial/?replytocom=2365#respond
 358. http://yangxs.cc/
 359. https://rare-technologies.com/id97-tutorial/#comment-2366
 360. https://rare-technologies.com/id97-tutorial/?replytocom=2366#respond
 361. http://gottwurfelt.com/2016/01/07/relatively-prime-season-2/
 362. https://rare-technologies.com/id97-tutorial/#comment-2368
 363. https://rare-technologies.com/id97-tutorial/?replytocom=2368#respond
 364. http://radimrehurek.com/
 365. https://rare-technologies.com/id97-tutorial/#comment-2369
 366. https://groups.google.com/d/forum/gensim
 367. https://rare-technologies.com/id97-tutorial/?replytocom=2369#respond
 368. https://rare-technologies.com/id97-tutorial/#comment-2370
 369. https://groups.google.com/forum/#!topic/gensim/njx3pmlzaws
 370. https://rare-technologies.com/id97-tutorial/?replytocom=2370#respond
 371. https://rare-technologies.com/id97-tutorial/#comment-2371
 372. https://rare-technologies.com/id97-tutorial/?replytocom=2371#respond
 373. https://rare-technologies.com/id97-tutorial/#comment-2372
 374. https://groups.google.com/d/forum/gensim
 375. https://rare-technologies.com/id97-tutorial/?replytocom=2372#respond
 376. https://rare-technologies.com/id97-tutorial/#comment-2373
 377. https://www.kaggle.com/c/id97-nlp-tutorial/details/part-2-word-vectors
 378. https://rare-technologies.com/id97-tutorial/?replytocom=2373#respond
 379. https://rare-technologies.com/id97-tutorial/#comment-2374
 380. https://rare-technologies.com/id97-tutorial/?replytocom=2374#respond
 381. https://rare-technologies.com/id97-tutorial/#comment-2377
 382. https://rare-technologies.com/id97-tutorial/?replytocom=2377#respond
 383. https://rare-technologies.com/id97-tutorial/#comment-2375
 384. https://rare-technologies.com/id97-tutorial/?replytocom=2375#respond
 385. http://radimrehurek.com/
 386. https://rare-technologies.com/id97-tutorial/#comment-2376
 387. http://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97
 388. https://groups.google.com/d/forum/gensim
 389. https://rare-technologies.com/id97-tutorial/?replytocom=2376#respond
 390. https://rare-technologies.com/id97-tutorial/#comment-2378
 391. https://rare-technologies.com/id97-tutorial/?replytocom=2378#respond
 392. http://radimrehurek.com/
 393. https://rare-technologies.com/id97-tutorial/#comment-2380
 394. https://rare-technologies.com/id97-tutorial/?replytocom=2380#respond
 395. https://rare-technologies.com/id97-tutorial/#comment-2381
 396. https://rare-technologies.com/id97-tutorial/?replytocom=2381#respond
 397. http://radimrehurek.com/
 398. https://rare-technologies.com/id97-tutorial/#comment-2383
 399. https://github.com/piskvorky/sim-shootout
 400. https://rare-technologies.com/id97-tutorial/?replytocom=2383#respond
 401. https://rare-technologies.com/id97-tutorial/#comment-2390
 402. https://rare-technologies.com/id97-tutorial/#comment-2382
 403. http://qwone.com/~jason/20newsgroups/
 404. https://rare-technologies.com/id97-tutorial/?replytocom=2382#respond
 405. https://rare-technologies.com/id97-tutorial/#comment-2385
 406. https://rare-technologies.com/id97-tutorial/?replytocom=2385#respond
 407. http://radimrehurek.com/
 408. https://rare-technologies.com/id97-tutorial/#comment-2386
 409. http://radimrehurek.com/gensim/support.html
 410. http://radimrehurek.com/gensim/support.html
 411. https://rare-technologies.com/id97-tutorial/?replytocom=2386#respond
 412. https://rare-technologies.com/id97-tutorial/#comment-2387
 413. http://rutumulkar.com/blog/2015/id97/
 414. https://rare-technologies.com/id97-tutorial/?replytocom=2387#respond
 415. https://rare-technologies.com/id97-tutorial/#comment-2388
 416. https://rare-technologies.com/id97-tutorial/?replytocom=2388#respond
 417. https://rare-technologies.com/id97-tutorial/#comment-2389
 418. https://rare-technologies.com/id97-tutorial/?replytocom=2389#respond
 419. https://rare-technologies.com/id97-tutorial/#comment-2393
 420. https://rare-technologies.com/id97-tutorial/?replytocom=2393#respond
 421. http://radimrehurek.com/
 422. https://rare-technologies.com/id97-tutorial/#comment-2394
 423. http://radimrehurek.com/gensim/support.html
 424. https://rare-technologies.com/id97-tutorial/?replytocom=2394#respond
 425. http://hack.hk.cn/2017/01/19/      -gensim-   -id97-      /
 426. http://www.janagrc.com/dupe-snoop-identify-duplicates-on-quora/
 427. http://textprocessing.org/getting-started-with-id97
 428. http://textminingonline.com/exploiting-wikipedia-word-similarity-by-id97
 429. http://cm.imart.info/2017/05/07/google-news-id97/
 430. http://ch.imart.info/2017/05/15/google-news-id97/
 431. http://www.stokastik.in/understanding-word-vectors-and-id97/
 432. https://rebcs.wordpress.com/2017/06/05/sentence-based-similarity/
 433. http://idatamining.net/blog/?p=2065
 434. http://thewebarchitect.net/error-code-2538/
 435. https://innoradiant.com/semantic-analysis-new-product-development-care-language/
 436. http://abunchofdata.com/how-to-develop-word-embeddings-in-python-with-gensim/
 437. http://ai.intelligentonlinetools.com/ml/id116-id91-example-id97/
 438. http://copypasteprogrammers.com/ai-is-not-just-learning-our-biases-it-is-amplifying-them-4d0dee75931d/
 439. https://www.aprenderpython.net/curso-de-procesamiento-de-textos-gensim/
 440. https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/
 441. https://www.qqwenda.com/wenda/26335.html
 442. https://rare-technologies.com/id97-tutorial/#comment-2753
 443. http://www.muzhen.tk/2017/06/21/machine learning/nlp/gensim w2v/
 444. https://rare-technologies.com/id97-tutorial/?replytocom=2753#respond
 445. http://198.211.104.183/how-to-develop-word-embeddings-in-python-with-gensim/
 446. https://phdstatsphys.wordpress.com/2018/12/27/id97-how-to-train-and-update-it/
 447. https://rare-technologies.com/id97-tutorial/#respond
 448. https://rare-technologies.com/cdn-cgi/l/email-protection
 449. https://rare-technologies.com/personal-data-reports/
 450. https://rare-technologies.com/pii_analytics/
 451. https://rare-technologies.com/pii-scan-o365-connector/
 452. https://rare-technologies.com/pivoted-document-length-normalisation/
 453. https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/
 454. https://rare-technologies.com/id97-tutorial/
 455. https://rare-technologies.com/services/
 456. https://rare-technologies.com/careers/
 457. https://rare-technologies.com/our-team/
 458. https://rare-technologies.com/corporate-training/
 459. https://rare-technologies.com/blog/
 460. https://rare-technologies.com/incubator/
 461. https://rare-technologies.com/contact/
 462. https://rare-technologies.com/competitions/
 463. https://rare-technologies.com/sitemap
 464. https://rare-technologies.com/cdn-cgi/l/email-protection#c3aaada5ac83b1a2b1a6eeb7a6a0abadacafaca4aaa6b0eda0acae
 465. tel:+420 776 288 853

   hidden links:
 467. https://rare-technologies.com/id97-tutorial/#top
 468. https://www.facebook.com/raretechnologies
 469. https://twitter.com/raretechteam
 470. https://www.linkedin.com/company/6457766
 471. https://github.com/piskvorky/
 472. https://rare-technologies.com/feed/
