   #[1]rare technologies    feed [2]rare technologies    comments feed
   [3]rare technologies    making sense of id97 comments feed
   [4]alternate [5]alternate

   [tr?id=1761346240851963&ev=pageview&noscript=1]

   iframe: [6]https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld

   [7]pragmatic machine learning rare technologies [8]navigation

     * [9]services
     * [10]products
          + [11]pii tools
          + [12]scaletext
     * [13]corporate training
          + [14]overview
          + [15]python best practices
          + [16]practical machine learning
          + [17]topic modelling
          + [18]deep learning in practice
     * [19]for students
          + [20]open source
          + [21]incubator
          + [22]competitions
     * [23]company
          + [24]careers
          + [25]our team
     * [26]blog
     * [27]contact
     * [28]search

     * [29]services
     * [30]products
          + [31]pii tools
          + [32]scaletext
     * [33]corporate training
          + [34]overview
          + [35]python best practices
          + [36]practical machine learning
          + [37]topic modelling
          + [38]deep learning in practice
     * [39]for students
          + [40]open source
          + [41]incubator
          + [42]competitions
     * [43]company
          + [44]careers
          + [45]our team
     * [46]blog
     * [47]contact
     * [48]search

making sense of id97

   [49]radim   eh    ek 2014-12-23[50] gensim, [51]programming[52] 50
   comments

   one year ago, tom     mikolov (together with his colleagues at google)
   made some ripples by [53]releasing id97, an unsupervised algorithm
   for learning the meaning behind words. in this blog post, i   ll evaluate
   some extensions that have appeared over the past year, including glove
   and id105 via svd.

   in case you missed the buzz, id97 was widely featured as a member
   of the    new wave    of machine learning algorithms based on neural
   networks, commonly referred to as deep learning (though id97 itself
   is rather shallow). using large amounts of unannotated plain text,
   id97 learns relationships between words automatically. the output
   are vectors, one vector per word, with remarkable linear relationships
   that allow us to do things like vec(   king   )     vec(   man   ) + vec(   woman   )
   =~ vec(   queen   ), or vec(   montreal canadiens   )     vec(   montreal   ) +
   vec(   toronto   ) resembles the vector for    toronto maple leafs   .

   apparently crows are good at that stuff, too: [54]crows can understand
   analogies.

   check out my [55]online id97 demo and the [56]blog series on
   optimizing id97 in python for more background.

so, what   s changed?

   for one, tom     mikolov no longer works for google :-)

   more relevantly, there was a lovely piece of research done by the good
   people at stanford: jeffrey pennington, richard socher and christopher
   manning. they explicitly identified the objective that id97
   optimizes through its async stochastic gradient id26
   algorithm, and neatly connected it to the well-established field of
   id105s.

   and in case you   ve never heard of that     in short, id97 ultimately
   learns word vectors and word context vectors. these can be viewed as
   two 2d matrices (of floats), of size #words x #dim each. their method
   glove (global vectors) identified a matrix which, when factorized using
   the particular sgd algorithm of id97, yields out exactly these two
   matrices. so where id97 was a bit hazy about what   s going on
   underneath, glove explicitly names the    objective    matrix, identifies
   the factorization, and provides some intuitive justification as to why
   this should give us working similarities.

   very nice and clear paper, [57]go read it if you haven   t!

   for example, if we have the following [58]nine preprocessed sentences,
   and set window=5, the co-occurrence matrix looks like this:
# nine input sentences
texts = [['human', 'interface', 'computer'],
 ['survey', 'user', 'computer', 'system', 'response', 'time'],
 ['eps', 'user', 'interface', 'system'],
 ['system', 'human', 'system', 'eps'],
 ['user', 'response', 'time'],
 ['trees'],
 ['graph', 'trees'],
 ['graph', 'minors', 'trees'],
 ['graph', 'minors', 'survey']]

# word-word co-occurrence matrix, with context window size of 5
[[0 1 1 1 1 1 1 1 0 0 0 0]
 [1 0 1 0 0 2 0 0 1 0 0 0]
 [1 1 0 0 0 1 0 1 1 0 0 0]
 [1 0 0 0 1 1 2 2 0 0 0 0]
 [1 0 0 1 0 1 1 1 0 0 1 1]
 [1 2 1 1 1 2 1 2 3 0 0 0]
 [1 0 0 2 1 1 0 2 0 0 0 0]
 [1 0 1 2 1 2 2 0 1 0 0 0]
 [0 1 1 0 0 3 0 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 2 1]
 [0 0 0 0 1 0 0 0 0 2 0 2]
 [0 0 0 0 1 0 0 0 0 1 2 0]]
# (rows/columns represent words:
# &quot;computer human interface response survey system time user eps trees grap
h minors&quot;,
# in that order)

   note how the matrix is very sparse and symmetrical; the implementation
   we   ll use below takes advantage of both these properties to train glove
   more efficiently.

   the glove algorithm then transforms such raw integer counts into a
   matrix where the co-occurrences are weighted based on their distance
   within the window (word pairs farther apart get less co-occurrence
   weight):
# same row/column names as above
[[ 0.    0.5   1.    0.5   0.5   1.    0.33  1.    0.    0.    0.    0.  ]
 [ 0.    0.    1.    0.    0.    2.    0.    0.    0.5   0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    1.    0.    1.    0.5   0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.25  1.    2.    1.33  0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.33  0.2   1.    0.    0.    0.5   1.  ]
 [ 0.    0.    0.    0.    0.    0.    0.5   1.    1.67  0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.    0.    0.75  0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.    0.    0.    1.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.5   1.  ]
 [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    2.  ]
 [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]

   then takes a log, and factorizes this matrix to produce the final word
   vectors.

   this was really exciting news     it means the plain softmax id97
   essentially reduces to counting how many times words occur together,
   with some scaling thrown in. technically, this is just a glorified
   cooccurrence_counts[word, other_word]++ in a loop, followed by any of
   the standard id105 algorithms, both of which are well
   understood processes with efficient implementations.

glove vs id97

   oddly, the evaluation section of this glove paper didn   t match the
   quality of the rest. it had serious flaws in how the experiments
   compared glove to other methods. several people called the authors out
   on the weirdness, most lucidly the levy & goldberg research duo from
   bar ilan university     check out their [59]   apples to apples    blog post
   for a bit of academic drama. to summarize, when evaluated properly,
   paying attention to parameter settings, glove doesn   t really seem to
   outperform the original id97, let alone by 11% as the glove paper
   claimed.

   luckily, maciej kula implemented glove in python, using cython for
   performance. using his [60]neat implementation, we can try to make
   sense of the performance and accuracy ourselves.

   code to train glove in python:
from gensim import utils, corpora, matutils, models
import glove

# restrict dictionary to the 30k most common words.
wiki = models.id97.linesentence('/data/shootout/title_tokens.txt.gz')
 word = corpora.dictionary(wiki)
 word.filter_extremes(keep_n=30000)
word2id = dict((word, id) for id, word in  word.iteritems())

# filter all wiki documents to contain only those 30k words.
filter_text = lambda text: [word for word in text if word in word2id]
filtered_wiki = lambda: (filter_text(text) for text in wiki)  # generator

# get the word co-occurrence matrix -- needs lots of ram!!
cooccur = glove.corpus()
cooccur.fit(filtered_wiki(), window=10)

# and train glove model itself, using 10 epochs
model_glove = glove.glove(no_components=600, learning_rate=0.05)
model_glove.fit(cooccur.matrix, epochs=10)

   and similarly for training id97:
model_id97 = models.id97(size=600, window=10)
model_id97.build_vocab(filtered_wiki())
model_id97.train(filtered_wiki())

   the reason why we restricted the vocabulary to only 30,000 words is
   that maciej   s implementation of glove requires memory quadratic in the
   number of words: it keeps that sparse matrix of all word x word
   co-occurrences in ram. in contrast, the gensim id97 implementation
   is happy with linear memory, so millions of words are not a problem
   there. this is not an intrinsic limitation of glove though; with a
   different implementation, the co-occurrence matrix could be assembled
   out-of-core (map/reduce seems ideal for the job), and the factorization
   could just stream over it with constant memory too, in a more
   gensim-like fashion.


   results for 600 dims, context window of 10, 1.9b words of en wikipedia.
   algorithm accuracy on the word analogy task wallclock time peak ram
   [mb]
   i/o only = iterating over wiki with
   sum(len(text) for text in filtered_wiki()) n/a 3m 25
   glove, 10 epochs, learning rate 0.05 67.1% 4h12m 9,414
   glove, 100 epochs, learning rate 0.05 67.3% 18h39m 9,452
   id97, hierarchical skipgram, 1 epoch 57.4% 3h10m 266

   id97, negative sampling with 10 samples, 1 epoch 68.3% 8h38m 628
   id97, [61]pre-trained googlenews model released by tom     mikolov,
   300 dims, 3,000,000 vocabulary 55.3% ? ?

   basically, where glove precomputes the large word x word co-occurrence
   matrix in memory and then quickly factorizes it, id97 sweeps
   through the sentences in an online fashion, handling each co-occurrence
   separately. so, there is a tradeoff between taking more memory (glove)
   vs. taking longer to train (id97). also, once computed, glove can
   re-use the co-occurrence matrix to quickly factorize with any
   dimensionality, whereas id97 has to be trained from scratch after
   changing its embedding dimensionality.

   note that both implementations are fairly optimized, running on 8
   threads (on an 8 core machine), using the exact same input corpus, text
   preprocessing, vocabulary and evaluation code, so that the numbers are
   directly comparable. [62]code here.

sppmi and svd

   in a manner analogous to glove, levy and goldberg (the same researchers
   mentioned above) [63]analyzed the objective function of id97 with
   negative sampling. that   s the one that performed best in the table
   above, so i decided to check it out too.

   again, they manage to derive a beautifully simple connection to matrix
   factorization. this time, the word x context objective    source    matrix
   is computed differently to glove. each matrix cell, corresponding to
   word w and context word c, is computed as max(0.0, pmi(w, c) - log(k))
   , where k is the number of negative samples in id97 (for example,
   k=10 ). pmi is the standard [64]pointwise mutual information     if we
   use the notation that word w and context c occurred together #wc times
   in the training corpus, then pmi(w, c) = log frac{#wc *
   sum{w,c}#wc}{sum_c#wc * sum_w#wc} (no smoothing).

   the funky    sppmi    name simply reflects that we   re subtracting log(k)
   from pmi (   shifting   ) and that we   re taking the max(0.0, spmi)
   (   positive   ; should be non-negative, really). so, shifted positive
   pointwise mutual information.

   for example, for the same nine texts we used above and k=1 , the sppmi
   matrix looks like this:
[[ 0.    0.83  0.83  0.49  0.49  0.    0.49  0.13  0.    0.    0.    0.  ]
 [ 0.83  0.    1.16  0.    0.    0.83  0.    0.    0.98  0.    0.    0.  ]
 [ 0.83  1.16  0.    0.    0.    0.13  0.    0.47  0.98  0.    0.    0.  ]
 [ 0.49  0.    0.    0.    0.49  0.    1.18  0.83  0.    0.    0.    0.  ]
 [ 0.49  0.    0.    0.49  0.    0.    0.49  0.13  0.    0.    0.83  1.05]
 [ 0.    0.83  0.13  0.    0.    0.    0.    0.13  1.05  0.    0.    0.  ]
 [ 0.49  0.    0.    1.18  0.49  0.    0.    0.83  0.    0.    0.    0.  ]
 [ 0.13  0.    0.47  0.83  0.13  0.13  0.83  0.    0.29  0.    0.    0.  ]
 [ 0.    0.98  0.98  0.    0.    1.05  0.    0.29  0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    2.37  1.9 ]
 [ 0.    0.    0.    0.    0.83  0.    0.    0.    0.    2.37  0.    2.08]
 [ 0.    0.    0.    0.    1.05  0.    0.    0.    0.    1.9   2.08  0.  ]]

   no neural network training, no parameter tuning, we can directly take
   rows of this sppmi matrix to be the word vectors. very fast and simple.
   how does raw sppmi compare to id97   s and glove   s factorizations
   though?
   comparison on 600 dims, context window 10, 1.9b words of en wikipedia.
   algorithm accuracy on the analogy task wallclock time peak ram [mb]
   id97, negative sampling k=10, 1 epoch 68.3% 8h38m 628
   glove, learning rate 0.05, 10 epochs 67.1% 4h12m 9,414
   sppmi, k=1 48.7% 50m 3,433
   sppmi, k=10 30.3% 50m 3,429
   sppmi-svd, k=1 39.4% 1h23m 3,426
   sppmi-svd, k=10 3.8% 1h23m 3,444

   the sppmi-svd method simply factorizes the sparse sppmi matrix using
   [65]singular value decomposition (svd), rather than the gradient
   descent methods of id97/glove, and uses the (dense) left singular
   vectors as the final id27s. svd is a fast, scalable method
   with straightforward geometric interpretation, and it performed very
   well in [66]the nips experiments of levy & goldberg, who suggested
   sspmi-svd.

   in the table above, the quality of both sppmi and sppmi-svd models is
   atrocious, especially for higher values of k (more    shift   ). i   m not
   sure why this is; i   ll try to get the original implementation of levy &
   goldberg to compare.

   also, i originally tried to get this table on 1,000 dims, rather than
   600. but the glove implementation started failing, producing word
   vectors with nans in them, and weird <1% accuracies when i tried to
   combat that by decreasing its learning rate. maciej is still working on
   that one, so if you   re thinking of using glove in production, beware.
   edit: successfully [67]resolved, maciej   s glove handles that fine now.

   to make experiments easier, i wrote and published a script that takes
   the parsed [68]english wikipedia and computes accuracy on the analogy
   task, using each of these different algorithms in turn. you can [69]get
   it from github and experiment with the various methods yourself, trying
   them on your own data / application.

what does that all mean?

   playing with wikipedia is fun, but usually clients require more
   concrete insights from us.

   how do we tweak id97 to better model what we want?

   how to tune id97 model quality on a specific task (which is, in all
   likelihood, not    word analogies   )?

   i   ll postpone that until the next post. suffice to say that the
   performance depends on tuning the methods    internal parameters, in
   non-obvious ways. the bar ilan powerhouse of levy, goldberg & dagan
   wrote a full paper on that, exploring the various parameter
   combinations (dynamic vs. fixed context window, subsampling, negative
   distribution smoothing, taking context vectors into account as well   .).
   their paper is under review now     i   ll post a link here as soon as it
   becomes public. edit: acl paper [70]here.

   in the meanwhile, there has been some tentative research into the area
   of id97 error analysis and tuning. check out [71]this web demo (by
   levy & goldberg again, from [72]this paper), for investigating which
   contexts get activated for different words. this can lead to visual
   insights into which co-occurrences are responsible for a particular
   class of errors.

   tl;dr: the id97 implementation is still fine and state-of-the-art,
   you can continue using it :-)

   note from radim: get my latest machine learning tips & articles
   delivered straight to your inbox (it's free).
   ____________________ ____________________

    unsubscribe anytime, no spamming. max 2 posts per month, if lucky.
   subscribe now
   ____________________

   if you like such machine learning shenanigans, sign up for my
   newsletter above, get in touch for [73]commercial projects or check out
   my older posts: [74]efficient nearest neighbour similarity search,
   [75]optimizing id97, its [76]doc2vec extension.

   [77]deep learning[78]gensim[79]id97

comments 50

    1.
   mr. powerhouse
       [80]2014-12-24 at 6:56 am
       nice work, radim! few comments:
           in your first table, you report 55.3% accuracy for the pretrained
       vectors from id97 page     i   m getting >70%
           the training time & accuracy you obtained with your gensim
       implementation of id97 seems to be worse than the c version
       (when i used script demo-train-big-model-v1.sh from id97, the
       vectors themselves are trained in about three hours using 8 billion
       words, and the accuracy is 10% higher than yours     big difference!)
           maybe you use wrong hyper-parameters? or the c version is faster
       and more accurate?
           finally, omer and yoav report much better results with sppmi    
       any idea why your results are worse?
       thanks!
       [81]reply
         1. radim post
            author
        [82]radim
            [83]2014-12-25 at 12:37 pm
            hi mr. powerhouse     
            1. all experiments were done using the same 30k vocabulary, so
            that the numbers can be compared directly. the accuracy there
            is ~55% using the googlenews model, not >70%. the model
            performed very well on the syntactic tasks, and poorly on the
            semantic ones.
            2. no, the two are pretty much identical. check out the
            [84]id97 porting series. your results are most likely due
            to different training corpus / hardware / level of
            parallelization.    id97    are actually many algorithms,
            depending on the parameter settings. you   d have to control for
            all of those, to get meaningful apples-to-apples comparison.
            3. i   m in touch with them, looking for an answer. will update
            when we   ve found out the reason. it   s probably not the sppmi
            code itself, which is very straightforward (see the [85]github
            repo).
            [86]reply
              1.
             sebastien-j
                 [87]2014-12-29 at 9:55 pm
                 the accuracy function in gensim is unfair to the
                 googlenews model, which was trained on non-lowercased
                 data. for a better evaluation, the first letter of
                 countries, cities, etc. should not be lowercased.
                 arguably, words that differ from    a   ,    b    and    c    by
                 letter case only should also be rejected, while answers
                 that are identical to the expected word, except for case,
                 should be accepted.
                 using the full vocabulary, the accuracy with these
                 modifications is 73.8% (14,222/19,544), while the current
                 gensim implementation gives 55.3% (7581/13705), with many
                 questions rejected.
                 [88]reply
                   1.
                  sebastien-j
                      [89]2014-12-29 at 11:05 pm
                      typo: 14,222 -> 14,422
                      [90]reply
                   2. radim post
                      author
                  [91]radim
                      [92]2015-01-01 at 10:54 am
                      no, the evaluation procedure is the same for both
                      the c and gensim version. can you post your code?
                      i   m not sure what you   re talking about.
                      both evaluations treat words as case insensitive,
                      meaning if the model suggests    montreal   , and the
                      expected answer is    montreal   , it   s recorded as a
                      success = correct answer.
                      the only difference is the python version can handle
                      unicode, whereas the c version is ascii-only. (so if
                      a word contained          or         , the c version wouldn   t
                      be able to handle the lower case correctly. no words
                      in the eval set do, though, so it makes no
                      difference here.)
                      [93]reply
                        1.
                       sebastien-j
                           [94]2015-01-02 at 9:32 am
                           [95]https://gist.github.com/anonymous/06e060cdf
                           53257fde1ef
                           for the googlenews model, `lowercase=false`
                           should be used in the function above.
                           if a word is represented by many vectors, which
                           one we choose when asking the analogy questions
                           matters. in general, using the vector
                           corresponding to the most common surface form
                           will lead to higher accuracy. the vector for
                              montreal    (common) would be `better` than the
                           one for    montreal    (rare).
                           for example, the googlenews model gives the
                           wrong answer to    athens:greece::beijing:?   .
                           however, if we ask    athens:greece::beijing:?   ,
                           then the prediction is correct.
                        2. radim post
                           author
                       [96]radim
                           [97]2015-01-02 at 5:38 pm
                           i think i see what you mean. you   re proposing a
                           different evaluation technique, one where
                           letter case matters.
                           that makes sense, and you   re right in that such
                           evaluation may give different results for
                           models trained with case sensitivity. however,
                           neither the c tool nor gensim implement such
                           evaluation. so if they   re    unfair    in their
                           evaluation, they are both equally unfair.
                           looking at the c source, it will
                           ascii-uppercase all eval words and all model
                           vocabulary; gensim will unicode-lowercase all
                           eval words and leave vocabulary intact.
                        3. radim post
                           author
                       [98]radim
                           [99]2015-01-03 at 6:27 pm
                           you left me wondering how the c tools handles
                           conflicts when uppercasing its model
                           vocabulary:    montreal    and    montreal    will both
                           map to    montreal   , so which word   s embedding is
                           actually chosen during the accuracy evaluation?
                           it seems that in this case, the c word match
                           during eval is set up (maybe accidentally,
                           maybe by design, there are no comments) so that
                           the most frequent surface form is used. which
                           is probably what we want. or maybe being
                           explicit, raising an error, is preferable when
                           there   s a casing conflict? or your approach,
                           where there   s a bool flag that says    either
                           ignore case, or use my case exactly   ? i   m not
                           sure.
                           i think i   ll change gensim to use one of these
                           options & make the docs clear about what   s
                           going on. this only affects models with
                           case-sensitive vocabulary, of course.
                           thanks for bringing this up sebastien.
              2.
             stefan
                 [100]2016-04-09 at 2:30 pm
                 the link is dead unfortunately:
                 [101]http://web.stanford.edu/~jpennin/papers/glove.pdf
                 [102]reply
              3.
             jack
                 [103]2016-08-30 at 6:03 pm
                 why do we have to remove words that appear only once when
                 building word2voc?
                 [104]reply
    2.
   [105]leonid boytsov
       [106]2014-12-25 at 8:28 pm
       a very interesting post, thank you.
       i haven   t seen the original papers. however, off the top of my
       head, it seems very strange to me that you can use sppmi matrix
       directly without any id84. the number of
       columns (each one corresponds to a context word) should be huge.
       did i miss something?
       [107]reply
         1. radim post
            author
        [108]radim
            [109]2015-01-01 at 10:49 am
            hi leonid     you missed nothing. the sppmi matrix is huge.
            it   s basically #words x #words, like i described in the post.
            in theory, it   s also very sparse.
            in this particular case though, after using only the most
            frequent 30k words, the sparsity is pretty low = the sppmi
            matrix is almost dense.
            [110]reply
    3. pingback: [111]condensed news | data analytics & r
    4. pingback: [112]                           (   ) |             
    5.
   marcel
       [113]2015-02-16 at 7:50 am
       hi radim,
       thank you for your great post!
       do you have an update on this for us?
       eg. you write
          how do we tweak id97 to better model what we want? how to tune
       id97 model quality on a specific task (which is, in all
       likelihood, not    word analogies   )? i   ll postpone that until the
       next post.    
       and
          levy & goldberg wrote a full paper on that, exploring the various
       parameter combinations (dynamic vs. fixed context window,
       subsampling, negative distribution smoothing, taking context
       vectors into account as well   .). their paper is under review now    
       i   ll post a link here as soon as it becomes public.   
       thank you for your great work!
       marcel
       [114]reply
         1. radim post
            author
        [115]radim
            [116]2015-02-18 at 8:54 am
            thanks marcel!
            too much    real    work, i know i   ve been neglecting the blog a
            bit    i   ll try to post again soon.
            levy & goldberg: i don   t know, i   ll check for you.
            [117]reply
              1. radim post
                 author
             [118]radim
                 [119]2015-02-18 at 9:30 am
                 checked: no, not out yet.
                 but try contacting omer levy directly, they say they   ll
                 be happy to share privately!
                 [120]reply
    6.
   giusepe attardi
       [121]2015-02-17 at 5:58 pm
       have you considered the approach of hellinger pca, which applies
       svd to the co-occurrence matrix and is also cited in the glove
       paper?
       lebret, r  mi, and ronan collobert.    id27s through
       hellinger pca.    eacl 2014 (2014): 482.
       [122]reply
         1. radim post
            author
        [123]radim
            [124]2015-02-18 at 8:52 am
            no we haven   t, i haven   t seen that paper yet. do you want to
            add the method & report back?
            our evaluation here is open source, and gensim supports large
            scale svd, so i   m thinking the implementation may be
            straightforward.
            [125]reply
    7.
   anatoly
       [126]2015-04-17 at 8:59 pm
          levy & goldberg wrote a full paper on that, exploring the various
       parameter combinations (dynamic vs. fixed context window,
       subsampling, negative distribution smoothing, taking context
       vectors into account as well   .)   
       is it following paper?
       [127]https://levyomer.files.wordpress.com/2015/03/improving-distrib
       utional-similarity-tacl-2015.pdf
       [128]reply
         1. radim post
            author
        [129]radim
            [130]2015-04-18 at 2:34 pm
            yes. i tweet it out some time ago, but forgot to update this
            article     
            [131]reply
    8. pingback: [132]glove: global vectors for word representations   
       building babylon
    9. pingback: [133]codienerd (2)     toying with id97 | everything
       about data analytics
   10. pingback: [134]   compressed representation: a really good idea jo's
       blog
   11.
   [135]dmitriy selivanov
       [136]2015-11-09 at 10:12 am
       hi, radim. do you have exactly the same data, on which you perform
       benchmarks? i implemented glove in text2vec r package algorithm
       (from scratch) and on latest 2.1b wikipedia dump it produce much
       better results, then you reported here     75% on word analogy task
       (top 30k words, dim = 600, 10 sgd epochs, 12182 questions).
       [137]reply
         1. radim post
            author
        [138]radim
            [139]2015-11-09 at 11:57 am
            hello dmitriy!
            the script used to preprocess the data (xml.bz2 wikipedia
            dump) is linked to from this post. it comes from the    nearest
            neighbour shootout    blog series and is public.
            the raw data itself was taken as the latest wikipedia dump at
            the time of writing that    shootout    series. also public.
            the glove implementation used was from maciej kule, also
            linked to in this post. this implementation is from the time
            of writing this blog post obviously (it may have changed
            since).
            if you get much better results on the same inputs +
            parameters, it may be worth checking back with maciej. i   m
            sure he   d be interested in hearing comparisons + improving /
            fixing his implementation.
            edit: here   s some file stats i found on disk:
            $ ls -l title_tokens.txt
            -rw-r--r-- 1 radim develop 11566440066 apr 15 2015
            title_tokens.txt
            [140]reply
              1.
             [141]dmitriy selivanov
                 [142]2015-11-09 at 12:14 pm
                 yes, i used your scripts for preparation, but i can   t
                 find wiki dump for 2014-12. the oldest one is for
                 2015-02: [143]http://dumps.wikimedia.org/enwiki/
                 [144]reply
              2.
             [145]dmitriy selivanov
                 [146]2015-11-10 at 11:33 am
                 if you are still have interest, the problem is here:
                 [147]https://github.com/maciejkula/glove-python/issues/22
                 he use only upper triangular co-occurence matrix, and
                 build only single word-vector matrix. this reduce memory
                 and training time by factor of 2, but far less accurate.
                 [148]reply
                   1. radim post
                      author
                  [149]radim
                      [150]2015-11-10 at 11:44 am
                      i don   t see how that would affect accuracy. or in
                      your package, in your evaluation, do you combine the
                      word and context vectors together?
                      anyway, i think your analysis and findings are very
                      interesting! please keep me posted if you write it
                      up in a blog post (and i   ll ping maciej as well).
                      there   s such a lack of practical tools in this space
                      (as opposed to media hype).
                      [151]reply
                        1.
                       [152]dmitriy selivanov
                           [153]2015-11-10 at 11:49 am
                           yes, i combine two matrices (just add), as it
                           done in original paper and implementation. i   ll
                           write a post in next few weeks and ping you
                           back.
                        2.
                       [154]dmitriy selivanov
                           [155]2015-12-01 at 11:28 am
                           hi! see my post
                           [156]http://dsnotes.com/blog/text2vec/2015/12/0
                           1/glove-enwiki/.
                           if comparison looks not fair, let me know, i   ll
                           happy to correct. thanks for all your work, it
                           helps me a lot in development of text2vec.
                        3. radim post
                           author
                       [157]radim
                           [158]2015-12-02 at 6:23 am
                           great work dmitriy!
                           it   s good to see people running realistic
                           comparison on real, replicable datasets. i   ll
                           tweet your results.
   12. pingback: [159]                                   id97          |                      
   13. pingback: [160]inter-id97 | rob myers
   14. pingback: [161]deep learning-ml tutorials | tony deep techs
   15. pingback: [162]                                           |
   16.
   sander
       [163]2016-03-02 at 4:55 pm
       sorry, but what is it    accuracy on the word analogy task   ?
       [164]reply
         1. radim rehurek post
            author
        [165]radim rehurek
            [166]2016-03-03 at 3:08 am
            see [167]https://code.google.com/archive/p/id97/ (it   s a
            set of word 4-tuples, loaded from id97   s
            questions-words.txt file).
            [168]reply
   17.
   liling
       [169]2016-04-15 at 5:49 am
       @radim, just wondering what happens when the #dimensions in the
       glove output is more than the vocab size, would glove break? or
       would glove just throw 0s in the additional dimensions?
       [170]reply
   18. pingback: [171]      numberless degrees of similitude   : a response to
       ryan heuser   s    word vectors in the eighteenth century, part 1   
       gabriel recchia
   19.
   jack
       [172]2016-08-30 at 6:04 pm
       why do we have to remove the words that appear only once?
       [173]reply
   20. pingback: [174]short text categorization using deep neural networks
       and word-embedding models     everything about data analytics
   21. pingback: [175]id97 - deep learning on nlp - solutionhacker.com
   22.
   [176]hobson lane
       [177]2017-04-23 at 6:10 am
       the pennington glove paper has moved. it   s now here:
       [178]https://nlp.stanford.edu/pubs/glove.pdf
       [179]reply
         1. radim   eh    ek post
            author
        [180]radim   eh    ek
            [181]2017-04-23 at 6:58 am
            many thanks! link updated.
            [182]reply
   23. pingback: [183]sentence based similarity     research & expreimental
       blog
   24.
   howard
       [184]2017-11-06 at 4:51 am
       hi, radim,
       thanks for this post. just curious any update on svd regarding    i   m
       not sure why this is; i   ll try to get the original implementation
       of levy & goldberg to compare.    thanks,
       [185]reply
   25. pingback: [186]episode 3: id27s     new paradigm
   26.
   sumeet kumar
       [187]2018-09-16 at 9:52 am
       hi,
       i am using id97 from gensim library for one of the project. i
       am successfully able to get the id27 vector of size = 16.
       now i want to evaluate the result of that vector whether the
       received vector is referring to the correct word in the dataset or
       not. how do i do that?
       [188]reply
   27.
   bob
       [189]2018-09-21 at 9:49 pm
       hi radim, thank you for this very clear explanation, i need to
       create a words co-occurrence matrix which need a lot of memory as
       you said, can you please give me some details about the way you   re
       mentioning here to do the task :    this is not an intrinsic
       limitation of glove though; with a different implementation, the
       co-occurrence matrix could be assembled out-of-core (map/reduce
       seems ideal for the job)   
       thank you
       [190]reply

leave a reply [191]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   submit

   current [192][email protected] * 4.2_________________

   leave this field empty ____________________

author of post

   radim   eh    ek

radim   eh    ek's bio:

   founder at rare technologies, creator of gensim. sw engineer since
   2004, phd in ai in 2011. lover of geology, history and beginnings in
   general. occasional travel blogger.

need expert consulting in ml and nlp?

   ________________________________________

   ________________________________________


   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   please leave this field empty. ________________________________________

   send

categories

   categories[select category___________]

archives

   archives [select month__]

recent posts

     * [193]export pii drill-down reports
     * [194]personal data analytics
     * [195]scanning office 365 for sensitive pii information
     * [196]pivoted document length normalisation
     * [197]sent2vec: an unsupervised approach towards learning sentence
       embeddings

stay ahead of the curve

get our latest tutorials, updates and insights delivered straight to your
inbox.

   ____________________

   ____________________

   subscribe
   ____________________
   1-2 times a month, if lucky. your information will not be shared.

   [198][footer-logo.png]
     * [199]services
     * [200]careers
     * [201]our team
     * [202]corporate training
     * [203]blog
     * [204]incubator
     * [205]contact
     * [206]competitions
     * [207]site map

   rare technologies [208][email protected] sv  tova 5, prague, czech
   republic [209](eu) +420 776 288 853
   type and press    enter    to search ____________________

references

   visible links
   1. https://rare-technologies.com/feed/
   2. https://rare-technologies.com/comments/feed/
   3. https://rare-technologies.com/making-sense-of-id97/feed/
   4. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/making-sense-of-id97/
   5. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/making-sense-of-id97/&format=xml
   6. https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld
   7. https://rare-technologies.com/
   8. https://rare-technologies.com/making-sense-of-id97/
   9. https://rare-technologies.com/services/
  10. https://rare-technologies.com/making-sense-of-id97/
  11. https://pii-tools.com/
  12. https://scaletext.com/
  13. https://rare-technologies.com/corporate-training/
  14. https://rare-technologies.com/corporate-training/
  15. https://rare-technologies.com/python-best-practices/
  16. https://rare-technologies.com/practical-machine-learning/
  17. https://rare-technologies.com/topic-modelling-training/
  18. https://rare-technologies.com/deep_learning_training/
  19. https://rare-technologies.com/incubator
  20. https://github.com/rare-technologies/
  21. https://rare-technologies.com/incubator/
  22. https://rare-technologies.com/competitions/
  23. https://rare-technologies.com/#braintrust
  24. https://rare-technologies.com/careers/
  25. https://rare-technologies.com/our-team/
  26. https://rare-technologies.com/blog/
  27. https://rare-technologies.com/contact/
  28. https://rare-technologies.com/making-sense-of-id97/
  29. https://rare-technologies.com/services/
  30. https://rare-technologies.com/making-sense-of-id97/
  31. https://pii-tools.com/
  32. https://scaletext.com/
  33. https://rare-technologies.com/corporate-training/
  34. https://rare-technologies.com/corporate-training/
  35. https://rare-technologies.com/python-best-practices/
  36. https://rare-technologies.com/practical-machine-learning/
  37. https://rare-technologies.com/topic-modelling-training/
  38. https://rare-technologies.com/deep_learning_training/
  39. https://rare-technologies.com/incubator
  40. https://github.com/rare-technologies/
  41. https://rare-technologies.com/incubator/
  42. https://rare-technologies.com/competitions/
  43. https://rare-technologies.com/#braintrust
  44. https://rare-technologies.com/careers/
  45. https://rare-technologies.com/our-team/
  46. https://rare-technologies.com/blog/
  47. https://rare-technologies.com/contact/
  48. https://rare-technologies.com/making-sense-of-id97/
  49. https://rare-technologies.com/author/radim/
  50. https://rare-technologies.com/category/gensim/
  51. https://rare-technologies.com/category/programming/
  52. https://rare-technologies.com/making-sense-of-id97/#comments
  53. https://code.google.com/p/id97/
  54. http://www.iflscience.com/plants-and-animals/crows-understand-analogies
  55. http://radimrehurek.com/2014/02/id97-tutorial/#app
  56. http://radimrehurek.com/2013/09/deep-learning-with-id97-and-gensim/
  57. http://web.stanford.edu/~jpennin/papers/glove.pdf
  58. http://radimrehurek.com/gensim/tut1.html#from-strings-to-vectors
  59. https://docs.google.com/a/radimrehurek.com/document/d/1ydiujj7etsz688rgfu5imjjsbxai-krl8czswpti15s/edit
  60. https://github.com/maciejkula/glove-python/
  61. https://code.google.com/p/id97/#pre-trained_word_and_phrase_vectors
  62. https://github.com/piskvorky/word_embeddings
  63. https://levyomer.wordpress.com/2014/09/10/neural-word-embeddings-as-implicit-matrix-factorization/
  64. https://en.wikipedia.org/wiki/pointwise_mutual_information
  65. http://radimrehurek.com/gensim/models/lsimodel.html
  66. https://levyomer.wordpress.com/2014/09/10/neural-word-embeddings-as-implicit-matrix-factorization/
  67. https://github.com/maciejkula/glove-python/pull/9#issuecomment-68058795
  68. http://radimrehurek.com/2013/12/performance-shootout-of-nearest-neighbours-contestants/
  69. https://github.com/piskvorky/word_embeddings
  70. http://www.aclweb.org/anthology/q15-1016
  71. http://irsrv2.cs.biu.ac.il:9998/?word=machine
  72. http://www.cs.bgu.ac.il/~yoavg/publications/acl2014syntemb.pdf
  73. https://rare-technologies.com/contact
  74. http://radimrehurek.com/2013/11/performance-shootout-of-nearest-neighbours-intro/
  75. http://radimrehurek.com/2013/09/deep-learning-with-id97-and-gensim/
  76. http://radimrehurek.com/2014/12/doc2vec-tutorial/
  77. https://rare-technologies.com/tag/deep-learning/
  78. https://rare-technologies.com/tag/gensim/
  79. https://rare-technologies.com/tag/id97/
  80. https://rare-technologies.com/making-sense-of-id97/#comment-2507
  81. https://rare-technologies.com/making-sense-of-id97/?replytocom=2507#respond
  82. http://radimrehurek.com/
  83. https://rare-technologies.com/making-sense-of-id97/#comment-2508
  84. http://radimrehurek.com/2013/09/deep-learning-with-id97-and-gensim/optimizing id97
  85. https://github.com/piskvorky/word_embeddings/blob/master/run_embed.py#l177
  86. https://rare-technologies.com/making-sense-of-id97/?replytocom=2508#respond
  87. https://rare-technologies.com/making-sense-of-id97/#comment-2510
  88. https://rare-technologies.com/making-sense-of-id97/?replytocom=2510#respond
  89. https://rare-technologies.com/making-sense-of-id97/#comment-2511
  90. https://rare-technologies.com/making-sense-of-id97/?replytocom=2511#respond
  91. http://radimrehurek.com/
  92. https://rare-technologies.com/making-sense-of-id97/#comment-2513
  93. https://rare-technologies.com/making-sense-of-id97/?replytocom=2513#respond
  94. https://rare-technologies.com/making-sense-of-id97/#comment-2514
  95. https://gist.github.com/anonymous/06e060cdf53257fde1ef
  96. http://radimrehurek.com/
  97. https://rare-technologies.com/making-sense-of-id97/#comment-2515
  98. http://radimrehurek.com/
  99. https://rare-technologies.com/making-sense-of-id97/#comment-2516
 100. https://rare-technologies.com/making-sense-of-id97/#comment-2543
 101. http://web.stanford.edu/~jpennin/papers/glove.pdf
 102. https://rare-technologies.com/making-sense-of-id97/?replytocom=2543#respond
 103. https://rare-technologies.com/making-sense-of-id97/#comment-2558
 104. https://rare-technologies.com/making-sense-of-id97/?replytocom=2558#respond
 105. http://searchivarius.org/about
 106. https://rare-technologies.com/making-sense-of-id97/#comment-2509
 107. https://rare-technologies.com/making-sense-of-id97/?replytocom=2509#respond
 108. http://radimrehurek.com/
 109. https://rare-technologies.com/making-sense-of-id97/#comment-2512
 110. https://rare-technologies.com/making-sense-of-id97/?replytocom=2512#respond
 111. http://advanceddataanalytics.net/2015/01/27/condensed-news-3/
 112. http://www.flickering.cn/ads/2015/02/                              /
 113. https://rare-technologies.com/making-sense-of-id97/#comment-2519
 114. https://rare-technologies.com/making-sense-of-id97/?replytocom=2519#respond
 115. http://radimrehurek.com/
 116. https://rare-technologies.com/making-sense-of-id97/#comment-2522
 117. https://rare-technologies.com/making-sense-of-id97/?replytocom=2522#respond
 118. http://radimrehurek.com/
 119. https://rare-technologies.com/making-sense-of-id97/#comment-2523
 120. https://rare-technologies.com/making-sense-of-id97/?replytocom=2523#respond
 121. https://rare-technologies.com/making-sense-of-id97/#comment-2520
 122. https://rare-technologies.com/making-sense-of-id97/?replytocom=2520#respond
 123. http://radimrehurek.com/
 124. https://rare-technologies.com/making-sense-of-id97/#comment-2521
 125. https://rare-technologies.com/making-sense-of-id97/?replytocom=2521#respond
 126. https://rare-technologies.com/making-sense-of-id97/#comment-2524
 127. https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf
 128. https://rare-technologies.com/making-sense-of-id97/?replytocom=2524#respond
 129. http://radimrehurek.com/
 130. https://rare-technologies.com/making-sense-of-id97/#comment-2525
 131. https://rare-technologies.com/making-sense-of-id97/?replytocom=2525#respond
 132. http://building-babylon.net/2015/07/29/glove-global-vectors-for-word-representations/
 133. https://datawarrior.wordpress.com/2015/10/25/codienerd-2-toying-with-id97/
 134. http://www.josephcatrambone.com/?p=806
 135. http://dsnotes.com/
 136. https://rare-technologies.com/making-sense-of-id97/#comment-2529
 137. https://rare-technologies.com/making-sense-of-id97/?replytocom=2529#respond
 138. http://radimrehurek.com/
 139. https://rare-technologies.com/making-sense-of-id97/#comment-2530
 140. https://rare-technologies.com/making-sense-of-id97/?replytocom=2530#respond
 141. http://dsnotes.com/
 142. https://rare-technologies.com/making-sense-of-id97/#comment-2531
 143. https://dumps.wikimedia.org/enwiki/
 144. https://rare-technologies.com/making-sense-of-id97/?replytocom=2531#respond
 145. http://dsnotes.com/
 146. https://rare-technologies.com/making-sense-of-id97/#comment-2532
 147. https://github.com/maciejkula/glove-python/issues/22
 148. https://rare-technologies.com/making-sense-of-id97/?replytocom=2532#respond
 149. http://radimrehurek.com/
 150. https://rare-technologies.com/making-sense-of-id97/#comment-2533
 151. https://rare-technologies.com/making-sense-of-id97/?replytocom=2533#respond
 152. http://dsnotes.com/
 153. https://rare-technologies.com/making-sense-of-id97/#comment-2534
 154. http://dsnotes.com/
 155. https://rare-technologies.com/making-sense-of-id97/#comment-2536
 156. http://dsnotes.com/blog/text2vec/2015/12/01/glove-enwiki/
 157. http://radimrehurek.com/
 158. https://rare-technologies.com/making-sense-of-id97/#comment-2537
 159. http://www.bigdata.ir/2015/10/            -    -                -id97-        /
 160. http://robmyers.org/2015/07/21/inter-id97/
 161. https://tonydeep.wordpress.com/2015/12/14/deep-learning-ml-tutorials/
 162. http://hanshitou.com/archives/674
 163. https://rare-technologies.com/making-sense-of-id97/#comment-2541
 164. https://rare-technologies.com/making-sense-of-id97/?replytocom=2541#respond
 165. http://radimrehurek.com/
 166. https://rare-technologies.com/making-sense-of-id97/#comment-2542
 167. https://code.google.com/archive/p/id97/
 168. https://rare-technologies.com/making-sense-of-id97/?replytocom=2542#respond
 169. https://rare-technologies.com/making-sense-of-id97/#comment-2544
 170. https://rare-technologies.com/making-sense-of-id97/?replytocom=2544#respond
 171. http://www.twonewthings.com/gabrielrecchia/2016/06/11/numberless-degrees-of-similitude-word-vectors/
 172. https://rare-technologies.com/making-sense-of-id97/#comment-2559
 173. https://rare-technologies.com/making-sense-of-id97/?replytocom=2559#respond
 174. https://datawarrior.wordpress.com/2016/10/12/short-text-categorization-using-deep-neural-networks-and-word-embedding-models/
 175. http://solutionhacker.com/id97-deep-learning-nlp/
 176. http://totalgood.com/
 177. https://rare-technologies.com/making-sense-of-id97/#comment-2633
 178. https://nlp.stanford.edu/pubs/glove.pdf
 179. https://rare-technologies.com/making-sense-of-id97/?replytocom=2633#respond
 180. http://radimrehurek.com/
 181. https://rare-technologies.com/making-sense-of-id97/#comment-2634
 182. https://rare-technologies.com/making-sense-of-id97/?replytocom=2634#respond
 183. https://rebcs.wordpress.com/2017/06/05/sentence-based-similarity/
 184. https://rare-technologies.com/making-sense-of-id97/#comment-2668
 185. https://rare-technologies.com/making-sense-of-id97/?replytocom=2668#respond
 186. https://mungingdata.wordpress.com/2018/01/15/episode-3-word-embeddings/
 187. https://rare-technologies.com/making-sense-of-id97/#comment-2770
 188. https://rare-technologies.com/making-sense-of-id97/?replytocom=2770#respond
 189. https://rare-technologies.com/making-sense-of-id97/#comment-2771
 190. https://rare-technologies.com/making-sense-of-id97/?replytocom=2771#respond
 191. https://rare-technologies.com/making-sense-of-id97/#respond
 192. https://rare-technologies.com/cdn-cgi/l/email-protection
 193. https://rare-technologies.com/personal-data-reports/
 194. https://rare-technologies.com/pii_analytics/
 195. https://rare-technologies.com/pii-scan-o365-connector/
 196. https://rare-technologies.com/pivoted-document-length-normalisation/
 197. https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/
 198. https://rare-technologies.com/making-sense-of-id97/
 199. https://rare-technologies.com/services/
 200. https://rare-technologies.com/careers/
 201. https://rare-technologies.com/our-team/
 202. https://rare-technologies.com/corporate-training/
 203. https://rare-technologies.com/blog/
 204. https://rare-technologies.com/incubator/
 205. https://rare-technologies.com/contact/
 206. https://rare-technologies.com/competitions/
 207. https://rare-technologies.com/sitemap
 208. https://rare-technologies.com/cdn-cgi/l/email-protection#771e19111837051605125a0312141f19181b18101e12045914181a
 209. tel:+420 776 288 853

   hidden links:
 211. https://rare-technologies.com/making-sense-of-id97/#top
 212. https://www.facebook.com/raretechnologies
 213. https://twitter.com/raretechteam
 214. https://www.linkedin.com/company/6457766
 215. https://github.com/piskvorky/
 216. https://rare-technologies.com/feed/
