   #[1]rare technologies    feed [2]rare technologies    comments feed
   [3]rare technologies    doc2vec tutorial comments feed [4]alternate
   [5]alternate

   [tr?id=1761346240851963&ev=pageview&noscript=1]

   iframe: [6]https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld

   [7]pragmatic machine learning rare technologies [8]navigation

     * [9]services
     * [10]products
          + [11]pii tools
          + [12]scaletext
     * [13]corporate training
          + [14]overview
          + [15]python best practices
          + [16]practical machine learning
          + [17]topic modelling
          + [18]deep learning in practice
     * [19]for students
          + [20]open source
          + [21]incubator
          + [22]competitions
     * [23]company
          + [24]careers
          + [25]our team
     * [26]blog
     * [27]contact
     * [28]search

     * [29]services
     * [30]products
          + [31]pii tools
          + [32]scaletext
     * [33]corporate training
          + [34]overview
          + [35]python best practices
          + [36]practical machine learning
          + [37]topic modelling
          + [38]deep learning in practice
     * [39]for students
          + [40]open source
          + [41]incubator
          + [42]competitions
     * [43]company
          + [44]careers
          + [45]our team
     * [46]blog
     * [47]contact
     * [48]search

doc2vec tutorial

   [49]radim   eh    ek 2014-12-15[50] gensim, [51]programming[52] 89
   comments

   the latest [53]gensim release of 0.10.3 has a new class named
   [54]doc2vec. all credit for this class, which is an implementation of
   [55]quoc le & tom     mikolov:    distributed representations of sentences
   and documents   , as well as for this tutorial, goes to the illustrious
   [56]tim emerick.

   doc2vec (aka paragraph2vec, aka sentence embeddings) modifies the
   id97 algorithm to unsupervised learning of continuous
   representations for larger blocks of text, such as sentences,
   paragraphs or entire documents.

   important note: the doc2vec functionality received a major facelift in
   gensim 0.12.0. the api is now cleaner, training faster, there are more
   tuning parameters exposed etc. while the basic ideas explained below
   still apply, see [57]this ipython notebook for a more up-to-date
   tutorial on using doc2vec. for a commercial document similarity engine,
   see our [58]scaletext.com.

   continuing in tim   s own words:

input

     since the doc2vec class extends gensim   s [59]original id97
     class, many of the usage patterns are similar. you can easily adjust
     the dimension of the representation, the size of the sliding window,
     the number of workers, or almost any other parameter that you can
     change with the id97 model.

     the one exception to this rule are the parameters relating to the
     training method used by the model. in the id97 architecture, the
     two algorithm names are    continuous bag of words    (cbow) and
        skip-gram    (sg); in the doc2vec architecture, the corresponding
     algorithms are    distributed memory    (dm) and    distributed bag of
     words    (dbow). since the distributed memory model performed
     noticeably better in the paper, that algorithm is the default when
     running doc2vec. you can still force the dbow model if you wish, by
     using the dm=0 flag in constructor.

     the input to doc2vec is an iterator of [60]labeledsentence objects.
     each such object represents a single sentence, and consists of two
     simple lists: a list of words and a list of labels:
sentence = labeledsentence(words=[u'some', u'words', u'here'], labels=[u'sent_1'
])

     the algorithm then runs through the sentences iterator twice: once
     to build the vocab, and once to train the model on the input data,
     learning a vector representation for each word and for each label in
     the dataset.

     although this architecture permits more than one label per sentence
     (and i myself have used it this way), i suspect the most popular use
     case would be to have a single label per sentence which is the
     unique identifier for the sentence. one could implement this kind of
     use case for a file with one sentence per line by using the
     following class as training data:
class labeledlinesentence(object):
    def __init__(self, filename):
        self.filename = filename
    def __iter__(self):
        for uid, line in enumerate(open(filename)):
            yield labeledsentence(words=line.split(), labels=['sent_%s' % uid])

     a more robust version of this labeledlinesentence class above is
     also [61]included in the doc2vec module, so you can use that. read
     the [62]doc2vec api docs for all constructor parameters.

training

     doc2vec learns representations for words and labels simultaneously.
     if you wish to only learn representations for words, you can use the
     flag train_lbls=false in your doc2vec class. similarly, if you only
     wish to learn representations for labels and leave the word
     representations fixed, the model also has the flag
     train_words=false.

     one caveat of the way this algorithm runs is that, since the
     learning rate decrease over the course of iterating over the data,
     labels which are only seen in a single labeledsentence during
     training will only be trained with a fixed learning rate. this
     frequently produces less than optimal results. i have obtained
     better results by iterating over the data several times and either
    1. randomizing the order of input sentences, or
    2. manually controlling the learning rate over the course of several
       iterations.

     for example, if one wanted to manually control the learning rate
     over the course of 10 epochs, one could use the following:
model = doc2vec(alpha=0.025, min_alpha=0.025)  # use fixed learning rate
model.build_vocab(sentences)
for epoch in range(10):
    model.train(sentences)
    model.alpha -= 0.002  # decrease the learning rate
    model.min_alpha = model.alpha  # fix the learning rate, no decay

     the code runs on optimized c (via cython), just like the original
     id97, so it   s fairly fast.

     note from radim: i wanted to include the obligatory
     run-on-english-wikipedia-example at this point, with some timings
     and code. but i couldn   t get reasonable results out of doc2vec, and
     didn   t want to delay publishing tim   s write up any longer while i
     experiment. despair not; the caravan goes on, and we   re working on a
     more scalable version of doc2vec, one which doesn   t require a vector
     in ram for each document, and with a simpler api for id136 on
     new documents. [63]ping me if you want to help.

memory usage

     with the current implementation, all label vectors are stored
     separately in ram. in the case above with a unique label per
     sentence, this causes memory usage to grow linearly with the size of
     the corpus, which may or may not be a problem depending on the size
     of your corpus and the amount of ram available on your box. for
     example, i   ve successfully run this over a collection of over 2
     million sentences with no problems whatsoever; however, when i tried
     to run it on 20x that much data my box ran out of ram since it
     needed to create a new vector for each sentence.

i/o

     the usage for doc2vec is the same as for gensim   s id97. one can
     save and load gensim doc2vec instances in the usual ways: directly
     with python   s pickle, or using the optimized doc2vec.save() and
     doc2vec.load() methods:
model = doc2vec(sentences)
...
# store the model to mmap-able files
model.save('/tmp/my_model.doc2vec')
# load the model back
model_loaded = doc2vec.load('/tmp/my_model.doc2vec')

     helper functions like model.most_similar(), model.doesnt_match() and
     model.similarity() also exist. the raw words and label vectors are
     also accessible either individually via model['word'], or all at
     once via model.syn0.
     [64]see the docs.

     the main point is, labels act in the same way as words in doc2vec.
     so, to get the most similar words/sentences to the first sentence
     (label sent_0, for example), you   d do:
print model.most_similar(&quot;sent_0&quot;)
[('sent_48859', 0.2516525387763977),
 (u'paradox', 0.24025458097457886),
 (u'methodically', 0.2379375547170639),
 (u'tongued', 0.22196565568447113),
 (u'cosmetics', 0.21332012116909027),
 (u'loos', 0.2114654779434204),
 (u'backstory', 0.2113303393125534),
 ('sent_60862', 0.21070502698421478),
 (u'gobble', 0.20925869047641754),
 ('sent_73365', 0.20847654342651367)]

     or to get the raw embedding for that sentence as a numpy vector:
print model[&quot;sent_0&quot;]

     etc. more functionality coming soon!

   note from radim: get my latest machine learning tips & articles
   delivered straight to your inbox (it's free).
   ____________________ ____________________

    unsubscribe anytime, no spamming. max 2 posts per month, if lucky.
   subscribe now
   ____________________

   if you liked this article, you may also enjoy the [65]optimizing
   id97 series and the [66]id97 tutorial.

   [67]deep learning[68]gensim[69]id97

comments 89

    1.
   [70]simon smith
       [71]2014-12-16 at 4:55 pm
       tim, radim,
       thank you for this     it looks great. i   m a big fan of gensim and it
       is now even better.
       simon
       [72]reply
         1. radim post
            author
        [73]radim
            [74]2015-03-06 at 10:28 am
            thanks simon, i appreciate it.
            [75]reply
              1.
             gj
                 [76]2016-08-24 at 7:59 am
                 hi, thanks for your tutorial. i want to get
                 sentences   vector ,my train text is    f:\\jj\\g.txt   ,but i
                 can not write code,could you help me write a code to get
                 sentence vector ?thank you very much
                 [77]reply
              2.
             gj
                 [78]2016-08-25 at 1:46 am
                 hi ,i hava a question need your help. if i train a model
                 named    gj.bin   ,but when i write code   model =
                 doc2vec.load_id97_format(   f:\\jj\\gj.bin   ,
                 binary=true)   ,it can show unicodedecodeerror    utf-8   
                 codec can   t decode byte 0xfa in position 2: invalid start
                 byte    what should i do?
                 [79]reply
    2.
   wen
       [80]2014-12-30 at 8:25 am
       hi, thanks for your awesome tutorial. what i should do if i want to
       input new testing sentences to find similar sentences after
       training?
       thank you
       [81]reply
         1.
        [82]samuel r  nnqvist
            [83]2015-01-28 at 3:44 pm
            you should be able to add new sentences to an existing model
            using train(), and then run most_similar([   sent_xx   ]), where
            sent_xx is the label of one of your new sentences.
            [84]reply
              1.
             yikang
                 [85]2015-03-06 at 9:18 am
                 thanks for your awesome work. but i find that train()
                 doesn   t add new sentences to an existing model. i get an
                    keyerror    error while i was trying. maybe train() just
                 update the weight?
                 [86]reply
                   1.   lavur mortensen
                    lavur mortensen
                      [87]2015-04-02 at 2:24 pm
                      i would very much like to know this too. i can   t
                      figure out how to train on more data in id97
                      either.
                      [88]reply
    3.
   dhruv shah
       [89]2015-01-15 at 9:44 am
       hi radim,
       i just had a quick question.
       the doc2vec is an unsupervised algorithm. so why do we have to
       provide labels when we are training the model?
       [90]reply
         1.
        bach
            [91]2015-01-19 at 8:11 am
            hi, i think that the labels here are not the    y   , they are
            like tags attached to each sentence, so you can access the
            vector that represents the sentence.
            [92]reply
              1.
             dhruv shah
                 [93]2015-01-20 at 10:22 am
                 does this mean that every single document label is
                 unique?
                 [94]reply
                   1.
                  jean
                      [95]2016-03-02 at 4:27 pm
                      yes it is !
                      [96]reply
                   2. radim rehurek post
                      author
                  radim rehurek
                      [97]2016-03-03 at 2:56 am
                      that   s up to you. each label can be unique. or you
                      can use only a few labels across all documents (such
                      as 10 target classes = labels for 1 million
                      documents, reusing a single label for many
                      documents).
                      you would then learn 10 vectors via doc2vec, one
                      vector to represent each class=label.
                      also note you can provide multiple labels per
                      document (each document can be tagged with multiple
                         classes   ).
                      [98]reply
                        1.
                       nataly maslova
                           [99]2016-09-15 at 6:12 am
                           hello radim.
                           i try to attend 2 labels to some sentences in
                           my sample and get the error:
                           file    c:\doc2v\mylabelseparate.py   , line 122,
                           in
                           model_dm.build_vocab(np.concatenate((x_train,
                           x_test, unsupforest))) # build
                           vocab over all reviews
                           file
                              c:\python27\lib\site-packages\gensim\models\wo
                           rd2vec.py   , line 396, in b
                           uild_vocab
                           vocab = self._vocab_from(sentences)
                           file
                              c:\python27\lib\site-packages\gensim\models\do
                           c2vec.py   , line 200, in _v
                           ocab_from
                           sentence_length = len(sentence.words)
                           typeerror: object of type    labeledsentence    has
                           no len()
                           i would be extremely grateful to you if you
                           look at my post:
                           [100]http://stackoverflow.com/questions/3950413
                           0/error-object-of-type-labeledsentence-has-no-l
                           en-while-building-vocabulary-in
    4.
   bach
       [101]2015-01-19 at 8:09 am
       hi,
       how can i get the vector representing a paragraph? my understanding
       is that i can access the vectors representing sentences via their
       labels; so how do i label a paragraph contains a few sentences?
       thanks.
       [102]reply
         1.
        denis
            [103]2015-01-22 at 11:18 pm
            i guess you should use the same label for all sentences in
            your paragraph
            [104]reply
              1.
             bach
                 [105]2015-01-27 at 11:18 am
                 hi,
                 so will each sentence in a paragraph have 1 label for its
                 own and 1 label for the paragraph? e.g., for paragraph 1:
                 sentence.labels = [u   sent_i   , u   para_1   ]
                 for sentence i.
                 [106]reply
                   1.
                  [107]samuel r  nnqvist
                      [108]2015-01-28 at 3:54 pm
                      you could also feed entire paragraphs (or
                      arbitrarily long sequences of tokens) together with
                      a para_x label, to reduce memory usage (i.e., number
                      of unique labels), unless you   re interested in
                      separating the semantics of paragraphs and
                      individual sentences.
                      [109]reply
    5.
   jeff
       [110]2015-01-26 at 9:57 pm
       hi radim,
       first, thank you for this tutorial. i did what you said in this
       tutorial, but it didn   t work. the issue is after my training, i got
       a key error with
       print model[   sent_0   ].
       i have checked all of the keys, and i can   t find any labels, then i
       checked the source code, i find:
       self.train_words = train_words
       self.train_lbls = train_lbls
       if sentences is not none:
       self.build_vocab(sentences)
       self.train(sentences)
       in doc2vec class, it just call the build_vocab() method inherited
       from id97 class, i wonder how this can generate a key list
       include    sent_0   ,   sent_1   ,      
       hoping for your reply.
       thanks
       [111]reply
         1.
        [112]samuel r  nnqvist
            [113]2015-01-28 at 4:12 pm
            if training was successful, you should be able to access the
            sentence vector by label like that. here   s a minimal example:
            sentence = labeledsentence(words=[u   some   , u   words   , u   here   ],
            labels=[u   sent_1   ])
            model = doc2vec([sentence], min_count=0)
            model[   sent_1   ]
            [114]reply
              1.
             mark
                 [115]2015-09-02 at 12:16 pm
                 that example code doesn   t work for me:
                 >>> sentence =
                 gensim.models.doc2vec.labeledsentence(words=[u   some   ,
                 u   words   , u   here   ], labels=[u   sent_1   ])
                 traceback (most recent call last):
                 file       , line 1, in
                 typeerror: __new__() got an unexpected keyword argument
                    labels   
                 step one doesn   t work. but it appears that
                 labeledsentence now wants    tags    instead of labels, so
                 i   ll update it.
                 >>> sentence =
                 gensim.models.doc2vec.labeledsentence(words=[u   some   ,
                 u   words   , u   here   ], tags=[u   sent_1   ])
                 >>> model = gensim.models.doc2vec([sentence],
                 min_count=0)
                 >>> model[   sent_1   ]
                 traceback (most recent call last):
                 file       , line 1, in
                 file    c:usersu772700appdatalocalcontinuumanaconda
                 (x86)libsite-packagesgensimmodelsid97.py   , line 1204,
                 in __getitem__
                 return self.syn0[self.vocab[words].index]
                 keyerror:    sent_1   
                 i can build the model, but the sentence tag doesn   t exist
                 in the vocabulary.
                 [116]reply
                   1. radim post
                      author
                  radim
                      [117]2015-09-02 at 12:26 pm
                      mark, did you read the important note above? doc2vec
                      api has changed considerably since this blog post.
                      [118]reply
                   2.
                  wolfgang
                      [119]2015-09-27 at 7:59 pm
                      i believe you need to use the
                      model.docvecs[   sent_1   ], the api has changed as
                      radim mentioned
                      [120]reply
         2.
        aaron
            [121]2016-08-09 at 7:54 pm
            something like this will work with the updated api:
            document_embedding_matrix =
            np.array([d.infer_vector(sents[i].words) for i in
            range(len(sents))])
            [122]reply
         3.
        aaron
            [123]2016-08-09 at 7:54 pm
            something like this will work with the updated api:
            document_embedding_matrix =
            np.array([doc2vec_model.infer_vector(sents[i].words) for i in
            range(len(sents))])
            [124]reply
    6.
   zach
       [125]2015-01-29 at 8:24 pm
       i have a model trained with doc2vec. (it   s very cool!)
       is there an easy way to extract just the vectors for all of the
       sentences?
       basically, i want to save a matrix of all the original documents
       and their corresponding vectors.
       [126]reply
         1.
        claudio
            [127]2015-02-19 at 6:12 am
            i think you only have to read the labels which start with
               sent_     because that are the labels which contains the vector
            representation. however, i   m experimenting with doc2vec and
            after the a successful training process my model have some
            missing sentences labels. did you solve it?
            [128]reply
              1.
             nate
                 [129]2015-02-23 at 6:28 pm
                 let me start off by saying i really like doc2vec and i
                 appreciate the efforts that have gone into making it!
                 after successfully training a doc2vec model, many of my
                 sentence labels are missing in the trained model and many
                 new labels have appeared representing individual words
                 (presumably extracted from some of the sentences,
                 themselves). maybe i   m not correctly understanding the
                 purpose of the labels, but i thought that perhaps they
                 represented the    hooks    for extracting the vector
                 representations for each sentence i trained on?
                 [130]reply
                   1.
                  claudio
                      [131]2015-02-26 at 11:57 pm
                      nate, how are you training your model?. i fixed the
                      missing sentences labels by setting the parameter
                      min_count=1. according with the documentation
                         min_count = ignore all words with total frequency
                      lower than this.    however, the problem with this
                      solution is we are keeping the noise in the dataset.
                      (very specific words).
                      [132]reply
    7.
   paul f
       [133]2015-02-01 at 11:47 pm
       searching for golden needles in unlabeled email haystacks..i was
       astounded to see in table two of the le and mikolov article that
       lda demonstrates a 32.58% error rate compared to the authors   
       paragraph vector model with only a 7.42%.error rate. i am searching
       an un-labled corpus of 10+ million emails to determine by topic
       what exists. we ran our initial test of lda (single core) a few
       days ago on the enron email test file but have not had time to
       thoroughly analyze the results. this difference in the error rate
       means i likely must add doc2vec to the process. under lda our goal
       was a very homogeneous group of clusters with the goal of quickly
       eliminating the not relevant clusters and the documents they
       include from further processing based upon a manual review of the
       top 5-10 top ranked topics in the top 10-20 top ranked documents in
       each cluster (recognizing that there could be many thousands of
       homogeneous clusters, we are thinking of ways that the machine can
       determine 80%+ of the not relevant clusters based on rules yet to
       be determined.) the not relevant clusters and their documents
       become a training set as do specific paragraphs and sentences of
       the possibly relevant clusters when we move to supervised learning
       using lsa. lda gets me top ranked topics of top ranked documents in
       each cluster but apparently has limitation using distributed
       computing. lsa seems apparently more effective in distributed
       computing and is a comfortable known process for training with
       manually determined lsa training sets.
       doc2vec apparently brings sentence, paragraph and even possibly
       document labeling still with the benefit of id31 at
       an apparent vastly increased processing time. since my 12 core
       machine won   t cut it on doc2vec and i need to acquire a
       considerable number of cores and build a lan. what suggestion can
       anyone supply concerning the least cost per core(i am at about
       us$45 each) and how to determine the amount of ram required per
       worker core for a good trade off between processing speed and core
       cost? will each worker need to have identical processors and ram?
       since they are plentiful, i am contemplating dell precision t7500
       with dual 6 core and ram based upon your comments. please confirm
       that under all three models only actual cores and not hyperthreaded
       fake cores are usable?
       thanks folks, your efforts are appreciated.
       paul f.
       [134]reply
    8.
   alex
       [135]2015-02-04 at 6:35 pm
       i am interested in such an application:
       can i use existing id97 c format binary file (e.g.
       googlenews-vectors-negative300.bin) to help training paragraph
       vector. i assume i should follow the steps
       step1
       model =
       doc2vector.load_id97_format(   ./googlenews-vectors-negative300.b
       in   , binary=true)
       step2
       model.train(sentences)
       but when i did in this way, errors pop up like
       file       , line 1, in
       file
          /homes/xx302/lib/python2.7/site-packages/gensim-0.10.3-py2.7-linux
       -x86_64.egg/gensim/models/id97.py   , line 466, in train
       total_words = total_words or int(sum(v.count * v.sample_id203
       for v in itervalues(self.vocab)) * self.iter)
       file
          /homes/xx302/lib/python2.7/site-packages/gensim-0.10.3-py2.7-linux
       -x86_64.egg/gensim/models/id97.py   , line 466, in
       total_words = total_words or int(sum(v.count * v.sample_id203
       for v in itervalues(self.vocab)) * self.iter)
       attributeerror:    vocab    object has no attribute
          sample_id203   
       so is it caused by using binary id97tor in which some
       information is lost?
       [136]reply
         1.
        manuel reis
            [137]2015-04-25 at 4:46 pm
            hello alex,
            i am having the same problem. how did you managed to solve it?
            [138]reply
    9.
   yuka
       [139]2015-02-28 at 10:19 am
       hello radim, thank you for your tutorials, it is really interesting
       and enlightening. i am a rookie in python and therefore have some
       issues while manipulating doc2vec. here it is, i have a corpora
       (folder) with five classes (folders) inside, and each document is
       simply a txt file. in this case, how should i start training all
       these documents? thanks.
       [140]reply
   10.
   huo
       [141]2015-03-09 at 1:37 pm
       hi,
       in the class labledlinesentence(), every sentence line got a unique
       label, which is    sent_%s    % item_no. but what if two lines have
       duplicated contents ? in your code, they will have two different
       labels. is it reasonable? thanks.
       [142]reply
   11.
   johndannl
       [143]2015-03-18 at 3:01 am
       hi radim,
       i really appreciate your good work   i have successfully completed
       the first step: training a model,but i find the same problem as
       yikang posted that    the train() method doesn   t add new sentences to
       an existing model    when i want to predict some new
       sentences.according to mikolov   s paper   in    the id136 stage       we
       can get paragraph vectors d for new paragraphs.is there something i
       missed? looking forward to your reply.
       [144]reply
   12.
   huanliang wang
       [145]2015-04-08 at 5:43 am
       hi, randim:
       i want to get the vector of input sentence after training was
       successful. but i can   t find any method to get it. could you tell
       me how to operate gensim?
       [146]reply
   13.
   huanliang wang
       [147]2015-04-08 at 7:55 am
       hi, randim:
       after training was successful, i find some sent_?? are not exist.
       such as:
       model([   sent_22   ])
       an error is thrown: keyerror:    sent_22   .
       but running both model([   sent_21   ]) and model([   sent_23   ]) are
       right.
       is it an error? how do i avoid such error?
       [148]reply
         1.
        cathy1272015
            [149]2015-12-02 at 9:34 am
            i came across the same problem as you when i run the program
            as mentioned above ,and now i don   t know why, if you get the
            answer please tell me,thanks
            [150]reply
              1. radim post
                 author
             [151]radim
                 [152]2015-12-02 at 9:36 am
                 such questions are best served on the gensim mailing
                 list. the doc2vec api has changed a bit over the past
                 year.
                 [153]reply
                   1.
                  xinchun
                      [154]2016-01-22 at 3:29 am
                      you can use:model.docvess[   ] to got it . but i also
                      have a question, how can i predict my new data using
                      my model which i have trained before
                      [155]reply
   14.
   andrew beam
       [156]2015-04-08 at 10:41 pm
       thanks for implementing this, i   ve had fun with the new module. is
       there any way to assess convergence? can we access the log-likehood
       value somehow? i have no idea how many epochs i should be running
       this for or if my alpha size is reasonable.
       [157]reply
   15.
   debasis ganguly
       [158]2015-04-09 at 10:13 pm
       i   m using doc2vec on one of the semeval    14 datasets. i   ve got 755
       number of lines (sentences) in a text file.
       however, after executing the following code:
       sentences=gensim.models.doc2vec.labeledlinesentence(   test.txt   )
       model = gensim.models.doc2vec.doc2vec(sentences, size=10, window=5,
       min_count=5, workers=4)
       model.save_id97_format(   svectors.txt   )
       when i wanted to check if all sentences have been stored in the
       vector file, to my surprise i found that some sentences are
       missing! more precisely,
       grep -c sent_ svectors.txt
       gave me an o/p of 735.
       wondering what might be the cause of 20 sentences missing?
       [159]reply
         1.
        debasis ganguly
            [160]2015-04-09 at 11:14 pm
            setting min_count to 2 fixes this problem. it was ignoring
            sentences where one of the constituent words was having freq.
            less than the min_count.
            [161]reply
   16.
   erick
       [162]2015-04-15 at 6:04 am
       i   m not sure i understood the point in having two labels in a
       sentence. does it mean that sentence can be used to train the
       vectors of two paragraphs?
       [163]reply
   17.
   hj
       [164]2015-04-16 at 5:40 am
       hello.
       thank you so much for this helpful tutorial.
       i have a question about the speed.
       i have a 366mb text file and wanted to create the doc2vec model.
       however, it seems it stopped since the log stopped at 30.23% for
       the last ten hours. so i was wondering, what is the max size
       doc2vec code can handle? also, any suggestions on how i can
       increase speed and size limit?
       thanks again, have nice day:)
       [165]reply
   18.
   silvia
       [166]2015-04-17 at 10:23 am
       i am using the doc2vec class with a corpus containing very short
       sentences that can have even 1 word. i observed that for many
       sentences, especially the short ones, doc2vec do not provide any
       representations. could you explain why? and could i solve this?
       thanks in advance!
       [167]reply
   19.
   r
       [168]2015-05-05 at 12:09 am
       hi radim
       great port. have you an example on how to use doc2vec for
       information retrieval or any advice on the subject.
       [169]reply
   20.
   pa
       [170]2015-05-12 at 3:10 am
       hi radim,
       great software. i have a question. suppose i have created a model
       and wish to find similarities with new sentences. i create new
       labels for each new sentence. i can do the model.train, but it does
       not update the vocabulary from the loaded model to include the
       words in the new sentences. if i do
       model.load(   orig.model   ) # this has 100000 labels
       sentences = []
       currentlines = 100000
       for line in lines:
       currentlines += 1
       label =    sent_    + str(currentlines)
       sentence = models.doc2vec.labeledsentence(unicodewords,
       labels=[unicode(label)])
       sentences.append(sentence)
       model.train(sentences)
       print model.most_similar(   sent_109900   )
       since this label is not in the original model, it complains with
       keyerror:    word    sent_109900    not in vocabulary   
       the mechanism updates the model with train but does not allow me to
       update the vocabulary. how is this possible?
       [171]reply
         1. radim
        radim
            [172]2015-06-20 at 10:52 pm
            exactly right     the model doesn   t allow adding new vocabulary
            (only updating existing one).
            but i have good news for you! one of gensim users is creating
            a new pull request on github, which will allow adding new
            words too. this way, the model should become fully online.
            [173]reply
              1.
             shima
                 [174]2016-04-30 at 5:48 pm
                 hi radim,
                 i would like to use doc2vec for classification. however,
                 it seems we need to have the test documents as well as
                 training documents at the time of training to build the
                 vocabulary. is it really the case? if yes, how such model
                 can be used in practice to learn a model that can
                 classify unseen documents? i get keyerror when i want to
                 get id194 of unseen docs.
                 [175]reply
   21.
   sky
       [176]2015-07-16 at 6:34 pm
       thanks! although this tutorial seems to be a bit outdated as a
       significant update has been merged
       ([177]https://github.com/piskvorky/gensim/pull/356) and
       labeldinstances class is fully replaced by taggeddocument. it leads
       to major changes in the way to create input sentences.
       [178]reply
   22.
   rob
       [179]2015-07-20 at 11:27 am
       does pull 356 mean that you no longer need each document vector in
       ram?
       also, which pull request was going to make doc2vec online? is it
       the same one?
       thanks!! these implementations are amazing.
       [180]reply
   23. pingback: [181]how to train p(category|title) model with id97 -
       codeengine
   24.
   sagar arora
       [182]2015-08-12 at 8:02 am
       any updates on scalable version of doc2vec, the one which does not
       require all label vectors to be stored separately in ram?
       looking forward to it
       [183]reply
   25.
   arandomuser
       [184]2015-09-15 at 1:47 am
       sorry but why you have published a          tutorial          (a)that is out of
       date, (b) hard to follow, (c) full of errors and (d) you do not
       help people at all. someone post a link of github with a    new   
       tutorial. looks that your company is funny and cannot be taken
       serious. i am looking for your response     if you have something to
       say.
       [185]reply
         1. radim post
            author
        [186]radim
            [187]2015-09-15 at 3:59 am
            yeah, we should probably update this tutorial with the new
            one, instead of just putting a disclaimer on top.
            what errors did you find here?
            [188]reply
   26. pingback: [189]python:how to calculate the sentence similarity
       using id97 model of gensim with python     it sprite
   27.
   ola gustafsson
       [190]2015-10-31 at 2:37 pm
       i   ve done some experimenting with doc2vec for recommendation
       purposes, looking for documents that share similar meaning, and the
       results seem very promising.
       however, when training on a corpus and extracting vectors, these
       are not the same vectors as i get from the infer method, used on
       the same texts. i was expecting them to be identical, but was i
       right to do so? there are some alpha parameters both in training a
       model and at the infer stage. are these or something else the
       reason behind different vectors?
       [191]reply
   28.
   parisa
       [192]2015-11-11 at 8:29 am
       hello
       i   m new in gensim and i want to extract semantic of document   s
       word,but i don   t know how to do it,i don   t know how to load my
       dataset and other steps,your tutorial is not full for beginners i
       study your tutorials but i don   t understand it,please help me i
       need it.
       thanks
       [193]reply
         1. radim post
            author
        [194]radim
            [195]2015-11-11 at 9:35 am
            hello parisa,
            try the mailing list:
            [196]http://radimrehurek.com/gensim/support.html
            but if you don   t know anything at all, it   s probably best to
            start with the basics (programming, python, numpy   ), rather
            than jumping into advanced research like doc2vec.
            [197]reply
              1.
             parisa
                 [198]2015-11-13 at 10:11 am
                 thanks but i don   t have any time for learning this,can
                 you recommend me a person who can write this project for
                 me???/
                 [199]reply
   29. pingback: [200]python:doc2vec : how to get document vectors     it
       sprite
   30.
   alex
       [201]2015-11-18 at 1:09 pm
       big thanks, radim! is it possible to train paragraph representation
       using already trained word representations on really big corpus?
       when i just load pretrained model and then start to learn sentence
       vectors, i   m getting an error about non-existent keys in
       vocabulary.
       currently, i   ve just made my own version to cope with that, but i
       suppose i   m reinventing a wheel.
       so, if i   m right, i   m ready to share a pr, and if i   m wrong    
       please give me some small ( but working ) example
       thank you!
       best regards,
       alex.
       [202]reply
   31. radim post
       author
   [203]radim
       [204]2015-11-18 at 1:20 pm
       hello alex,
       this question comes up regularly on the mailing list: for example
       [205]here or [206]here. it   s probably easiest to continue the
       discussion there.
       best,
       radim
       [207]reply
         1.
        alex
            [208]2015-11-18 at 1:33 pm
            thank you for prompt reply, but i mean slightly different
            thing. i have embeddings of pictures ( obtained from convnet),
            and wanted to compute paragraph2vec for them just to see
            whether wordsim will cook something usable for me.
            that means, that i even have no    words    in usual sense ( my
               words    are already embeddings ).
            but in theory we have no problem for training par2vec for
            pictures, and so my question is just how to do this easily.
            thank you again in advance!
            [209]reply
   32.
   karan singla
       [210]2016-02-05 at 11:29 am
       i want to cluster the chat messages/sentences to group similar
       messages together. when a new comes, i want to assign a cluster to
       it. how would i get a vector for new message? as per one of the
       above questions, i will need to train the model every time i get a
       new sentence to obtain its vector. is there a better approach? if i
       want to add 50-60k messages everyday, how to proceed? will the
       model size grow linearly with the number of new messages?
       [211]reply
   33.
   ps
       [212]2016-02-09 at 1:48 am
       what happened to your good old doc2vec tutorial? :-o
       it explained so nicely, the gensim api: how to load documents,
       train, test, similarity etc. now, you have made it to look and feel
       so complicated. i guess a small writeup on how to work with your
       own data is required.
       [213]reply
         1. radim rehurek post
            author
        radim rehurek
            [214]2016-02-09 at 2:49 am
            hello ps     do you mean the ipython notebook linked in the
            preamble here? or what part concretely feels complicated?
            i agree the doc2vec docs could (should!) be improved    always a
            struggle with open source projects. everyone just scratching
            their own itch, and it   s rarely the documentation.
            [215]reply
              1.
             ps
                 [216]2016-02-10 at 3:07 am
                 thanks for the quick reply radim. i was referring to the
                 original tutorial that you had (year and half back). i
                 worked with doc2vec extensively when it came out and i
                 remember, your description was good enough for me to
                 start the *first* model. i thought of using doc2vec again
                 for some project yesterday. and now it all looks so super
                 complicated     
                 i suppose, we could do a small write up for the most
                 common use case: load your own data (both streaming and
                 in-memory) , train it, get vectors, and predict new data.
                 [217]reply
                   1. radim rehurek post
                      author
                  radim rehurek
                      [218]2016-02-10 at 3:19 am
                      oh yes, help with the docs would be most welcome!
                      but it   s better to discuss on the [219]gensim
                      mailing list, so others can chime in and comment.
                      [220]reply
                        1.
                       ps
                           [221]2016-02-10 at 3:41 am
                           sure thing. will do
   34.
   ken yeung
       [222]2016-02-22 at 12:30 am
       hi radim
       thanks for the great work on doc2vec in python.
       i get a question of how to get similarity by using both word and
       tag? for example something like this below
       model.most_similar(   movie_123   ,    good   )
       also, how could i get the sentence as string based on the tag? if i
       only know the tag movie_123, is it possible to get the original
       sentence as string?
       many thanks!
       [223]reply
         1. radim rehurek post
            author
        radim rehurek
            [224]2016-02-22 at 7:59 am
            hello ken,
            have a look at the gensim mailing list, that is the standard
            support channel. i believe your question is answered there
            (e.g.
            [225]https://groups.google.com/forum/#!topic/gensim/h5iftgrff1
            8).
            best,
            radim
            [226]reply
              1.
             ken
                 [227]2016-02-22 at 2:29 pm
                 hi radim,
                 seems the question in the mailing list is about multiple
                 tags on a sentence. however, my question is more about
                 how to query similarity with both word and tag together.
                 do you know how it might be done with gensim? thanks.
                 [228]reply
   35.
   koorm
       [229]2016-03-21 at 8:40 am
       hi radim. i am having a hard time getting your tutorial to run. a
       basic case solution will be nice: read 2 documents from a file
       where each document has 1 sentence per line, and build the doc2vec
       model. what you have fails here and there. for example,
       import gensim
       import numpy
       import random
       import os
       sentence = gensim.models.doc2vec.labeledsentence(words=[u   some   ,
       u   words   , u   here   ], tags=[u   sent_1   ])
       model = gensim.models.doc2vec.doc2vec(alpha=0.025, min_alpha=0.025)
       model.build_vocab(sentence)
       fails:
                                                                                  
       attributeerror traceback (most recent call last)
       in ()
       6 sentence = gensim.models.doc2vec.labeledsentence(words=[u   some   ,
       u   words   , u   here   ], tags=[u   sent_1   ])
       7 model = gensim.models.doc2vec.doc2vec(alpha=0.025,
       min_alpha=0.025)
          -> 8 model.build_vocab(sentence)
       /home/ehsan/anaconda2/lib/python2.7/site-packages/gensim-0.12.4-py2
       .7-linux-x86_64.egg/gensim/models/id97.pyc in build_vocab(self,
       sentences, keep_raw_vocab, trim_rule)
       506
       507          
          > 508 self.scan_vocab(sentences, trim_rule=trim_rule) # initial
       survey
       509 self.scale_vocab(keep_raw_vocab=keep_raw_vocab,
       trim_rule=trim_rule) # trim by min_count & precalculate
       downsampling
       510 self.finalize_vocab() # build tables & arrays
       /home/ehsan/anaconda2/lib/python2.7/site-packages/gensim-0.12.4-py2
       .7-linux-x86_64.egg/gensim/models/doc2vec.pyc in scan_vocab(self,
       documents, progress_per, trim_rule)
       637 interval_start = default_timer()
       638 interval_count = total_words
          > 639 document_length = len(document.words)
       640
       641 for tag in document.tags:
       attributeerror:    list    object has no attribute    words   
       [230]reply
   36.
   ying
       [231]2016-04-12 at 7:35 am
       if my document label is not unique, the same document may have
       different labels and a few documents have a same label. the form of
       the answer of model.docvecs[label1] is as follow:
       [[[ 0.1111, 0.1111, 0.0222, 1.555,   .]
       [0.444, 0.555, 1.788, -0.222      ..]
             
       [0.555, -0.7777,       ..]]]
       that is all the vectors of documents haing the label [label1]? and
       how can i get each of it?
       [232]reply
         1. radim   eh    ek post
            author
        [233]radim   eh    ek
            [234]2016-04-12 at 8:22 am
            the best place for such questions is the gensim mailing list.
            [235]reply
              1.
             ying
                 [236]2016-04-18 at 11:35 am
                 what is the gensim mailing list
                 [237]reply
   37.
   tim
       [238]2016-04-18 at 10:12 pm
       hi can you please let me know what exactly is the output of the
       build_vocab method given a corpus of tagged documents?
       [239]reply
         1. radim   eh    ek post
            author
        [240]radim   eh    ek
            [241]2016-04-19 at 1:15 am
            hello tim, the    build_vocab    method has no output (   none    in
            python).
            if you have any additional questions, gensim mailing list is
            the best place for them.
            best,
            radim
            [242]reply
   38.
   wayne
       [243]2016-04-20 at 11:21 am
       hi radim.
       thank you for your great work. i am try the doc2vec for documents   
       classification( use the infer_vector to get the vector for new
       documents
       i want to ask:
       1. whether should i split one document into multiple lines?
       2. if i split each document, should i set the same one tag for
       these lines from one document?
       3. if the multiple lines have the same tag, how does the algorithm
       work for the tag when training the model?
       thank you very much?
       [244]reply
         1. radim   eh    ek post
            author
        [245]radim   eh    ek
            [246]2016-04-21 at 1:30 am
            hello wayne,
            you   ll get most success asking such questions at the
            [247]gensim mailing list (+searching the mailing list archive
            for previous answers first).
            [248]reply
   39.
   ahmed mohammed
       [249]2016-05-15 at 5:18 pm
       hi radim
       nice job i have learnt a lot from your tutorials and posts. i am
       facing a big challenge using gensim to check for semantic
       similarity in 20newsgroups. assuming i have a word or sentence to
       check for its similarity from all categories found in 20newsgroup.
       how am i suppose to do this with gensim. thank u.
       [250]reply
   40.
   haroon
       [251]2016-09-08 at 2:16 pm
       hello just discovered your blog   bookmarked!
       quick thing, i think the label attribute should is called tag in
       doc2vec now.
       thanks much
       [252]reply
   41. pingback: [253]visual question answer     badripatro
   42. pingback: [254]gensim vekt  rel dok  man e  itimi - gurmezin
       sci-tech-art
   43. pingback: [255]googling doc2vec - luminis amsterdam : luminis
       amsterdam
   44. pingback: [256]implementing doc2vec - luminis amsterdam : luminis
       amsterdam
   45. pingback: [257]twitterverse's opinion of nasa research - padmashri
       suresh

leave a reply [258]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   submit

   current [259][email protected] * 4.2_________________

   leave this field empty ____________________

author of post

   radim   eh    ek

radim   eh    ek's bio:

   founder at rare technologies, creator of gensim. sw engineer since
   2004, phd in ai in 2011. lover of geology, history and beginnings in
   general. occasional travel blogger.

need expert consulting in ml and nlp?

   ________________________________________

   ________________________________________


   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   please leave this field empty. ________________________________________

   send

categories

   categories[select category___________]

archives

   archives [select month__]

recent posts

     * [260]export pii drill-down reports
     * [261]personal data analytics
     * [262]scanning office 365 for sensitive pii information
     * [263]pivoted document length normalisation
     * [264]sent2vec: an unsupervised approach towards learning sentence
       embeddings

stay ahead of the curve

get our latest tutorials, updates and insights delivered straight to your
inbox.

   ____________________

   ____________________

   subscribe
   ____________________
   1-2 times a month, if lucky. your information will not be shared.

   [265][footer-logo.png]
     * [266]services
     * [267]careers
     * [268]our team
     * [269]corporate training
     * [270]blog
     * [271]incubator
     * [272]contact
     * [273]competitions
     * [274]site map

   rare technologies [275][email protected] sv  tova 5, prague, czech
   republic [276](eu) +420 776 288 853
   type and press    enter    to search ____________________

references

   visible links
   1. https://rare-technologies.com/feed/
   2. https://rare-technologies.com/comments/feed/
   3. https://rare-technologies.com/doc2vec-tutorial/feed/
   4. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/doc2vec-tutorial/
   5. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/doc2vec-tutorial/&format=xml
   6. https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld
   7. https://rare-technologies.com/
   8. https://rare-technologies.com/doc2vec-tutorial/
   9. https://rare-technologies.com/services/
  10. https://rare-technologies.com/doc2vec-tutorial/
  11. https://pii-tools.com/
  12. https://scaletext.com/
  13. https://rare-technologies.com/corporate-training/
  14. https://rare-technologies.com/corporate-training/
  15. https://rare-technologies.com/python-best-practices/
  16. https://rare-technologies.com/practical-machine-learning/
  17. https://rare-technologies.com/topic-modelling-training/
  18. https://rare-technologies.com/deep_learning_training/
  19. https://rare-technologies.com/incubator
  20. https://github.com/rare-technologies/
  21. https://rare-technologies.com/incubator/
  22. https://rare-technologies.com/competitions/
  23. https://rare-technologies.com/#braintrust
  24. https://rare-technologies.com/careers/
  25. https://rare-technologies.com/our-team/
  26. https://rare-technologies.com/blog/
  27. https://rare-technologies.com/contact/
  28. https://rare-technologies.com/doc2vec-tutorial/
  29. https://rare-technologies.com/services/
  30. https://rare-technologies.com/doc2vec-tutorial/
  31. https://pii-tools.com/
  32. https://scaletext.com/
  33. https://rare-technologies.com/corporate-training/
  34. https://rare-technologies.com/corporate-training/
  35. https://rare-technologies.com/python-best-practices/
  36. https://rare-technologies.com/practical-machine-learning/
  37. https://rare-technologies.com/topic-modelling-training/
  38. https://rare-technologies.com/deep_learning_training/
  39. https://rare-technologies.com/incubator
  40. https://github.com/rare-technologies/
  41. https://rare-technologies.com/incubator/
  42. https://rare-technologies.com/competitions/
  43. https://rare-technologies.com/#braintrust
  44. https://rare-technologies.com/careers/
  45. https://rare-technologies.com/our-team/
  46. https://rare-technologies.com/blog/
  47. https://rare-technologies.com/contact/
  48. https://rare-technologies.com/doc2vec-tutorial/
  49. https://rare-technologies.com/author/radim/
  50. https://rare-technologies.com/category/gensim/
  51. https://rare-technologies.com/category/programming/
  52. https://rare-technologies.com/doc2vec-tutorial/#comments
  53. https://github.com/piskvorky/gensim/
  54. http://radimrehurek.com/gensim/models/doc2vec.html
  55. https://arxiv.org/abs/1405.4053
  56. https://github.com/temerick
  57. https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-imdb.ipynb
  58. https://scaletext.com/
  59. http://radimrehurek.com/gensim/models/id97.html
  60. http://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.labeledsentence
  61. http://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.labeledlinesentence
  62. http://radimrehurek.com/gensim/models/doc2vec.html
  63. https://groups.google.com/forum/#!forum/gensim
  64. http://radimrehurek.com/gensim/models/doc2vec.html
  65. http://radimrehurek.com/2013/09/deep-learning-with-id97-and-gensim/
  66. http://radimrehurek.com/2014/02/id97-tutorial/
  67. https://rare-technologies.com/tag/deep-learning/
  68. https://rare-technologies.com/tag/gensim/
  69. https://rare-technologies.com/tag/id97/
  70. http://www.ripjar.com/
  71. https://rare-technologies.com/doc2vec-tutorial/#comment-2429
  72. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2429#respond
  73. http://radimrehurek.com/
  74. https://rare-technologies.com/doc2vec-tutorial/#comment-2449
  75. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2449#respond
  76. https://rare-technologies.com/doc2vec-tutorial/#comment-2556
  77. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2556#respond
  78. https://rare-technologies.com/doc2vec-tutorial/#comment-2557
  79. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2557#respond
  80. https://rare-technologies.com/doc2vec-tutorial/#comment-2430
  81. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2430#respond
  82. http://samuel.ronnqvist.fi/
  83. https://rare-technologies.com/doc2vec-tutorial/#comment-2438
  84. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2438#respond
  85. https://rare-technologies.com/doc2vec-tutorial/#comment-2448
  86. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2448#respond
  87. https://rare-technologies.com/doc2vec-tutorial/#comment-2452
  88. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2452#respond
  89. https://rare-technologies.com/doc2vec-tutorial/#comment-2431
  90. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2431#respond
  91. https://rare-technologies.com/doc2vec-tutorial/#comment-2433
  92. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2433#respond
  93. https://rare-technologies.com/doc2vec-tutorial/#comment-2434
  94. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2434#respond
  95. https://rare-technologies.com/doc2vec-tutorial/#comment-2495
  96. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2495#respond
  97. https://rare-technologies.com/doc2vec-tutorial/#comment-2496
  98. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2496#respond
  99. https://rare-technologies.com/doc2vec-tutorial/#comment-2563
 100. https://stackoverflow.com/questions/39504130/error-object-of-type-labeledsentence-has-no-len-while-building-vocabulary-in
 101. https://rare-technologies.com/doc2vec-tutorial/#comment-2432
 102. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2432#respond
 103. https://rare-technologies.com/doc2vec-tutorial/#comment-2435
 104. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2435#respond
 105. https://rare-technologies.com/doc2vec-tutorial/#comment-2437
 106. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2437#respond
 107. http://samuel.ronnqvist.fi/
 108. https://rare-technologies.com/doc2vec-tutorial/#comment-2439
 109. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2439#respond
 110. https://rare-technologies.com/doc2vec-tutorial/#comment-2436
 111. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2436#respond
 112. http://samuel.ronnqvist.fi/
 113. https://rare-technologies.com/doc2vec-tutorial/#comment-2440
 114. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2440#respond
 115. https://rare-technologies.com/doc2vec-tutorial/#comment-2469
 116. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2469#respond
 117. https://rare-technologies.com/doc2vec-tutorial/#comment-2470
 118. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2470#respond
 119. https://rare-technologies.com/doc2vec-tutorial/#comment-2473
 120. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2473#respond
 121. https://rare-technologies.com/doc2vec-tutorial/#comment-2554
 122. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2554#respond
 123. https://rare-technologies.com/doc2vec-tutorial/#comment-2555
 124. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2555#respond
 125. https://rare-technologies.com/doc2vec-tutorial/#comment-2441
 126. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2441#respond
 127. https://rare-technologies.com/doc2vec-tutorial/#comment-2444
 128. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2444#respond
 129. https://rare-technologies.com/doc2vec-tutorial/#comment-2445
 130. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2445#respond
 131. https://rare-technologies.com/doc2vec-tutorial/#comment-2446
 132. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2446#respond
 133. https://rare-technologies.com/doc2vec-tutorial/#comment-2442
 134. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2442#respond
 135. https://rare-technologies.com/doc2vec-tutorial/#comment-2443
 136. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2443#respond
 137. https://rare-technologies.com/doc2vec-tutorial/#comment-2461
 138. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2461#respond
 139. https://rare-technologies.com/doc2vec-tutorial/#comment-2447
 140. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2447#respond
 141. https://rare-technologies.com/doc2vec-tutorial/#comment-2450
 142. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2450#respond
 143. https://rare-technologies.com/doc2vec-tutorial/#comment-2451
 144. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2451#respond
 145. https://rare-technologies.com/doc2vec-tutorial/#comment-2453
 146. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2453#respond
 147. https://rare-technologies.com/doc2vec-tutorial/#comment-2454
 148. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2454#respond
 149. https://rare-technologies.com/doc2vec-tutorial/#comment-2483
 150. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2483#respond
 151. http://radimrehurek.com/
 152. https://rare-technologies.com/doc2vec-tutorial/#comment-2484
 153. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2484#respond
 154. https://rare-technologies.com/doc2vec-tutorial/#comment-2485
 155. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2485#respond
 156. https://rare-technologies.com/doc2vec-tutorial/#comment-2455
 157. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2455#respond
 158. https://rare-technologies.com/doc2vec-tutorial/#comment-2456
 159. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2456#respond
 160. https://rare-technologies.com/doc2vec-tutorial/#comment-2457
 161. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2457#respond
 162. https://rare-technologies.com/doc2vec-tutorial/#comment-2458
 163. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2458#respond
 164. https://rare-technologies.com/doc2vec-tutorial/#comment-2459
 165. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2459#respond
 166. https://rare-technologies.com/doc2vec-tutorial/#comment-2460
 167. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2460#respond
 168. https://rare-technologies.com/doc2vec-tutorial/#comment-2462
 169. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2462#respond
 170. https://rare-technologies.com/doc2vec-tutorial/#comment-2463
 171. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2463#respond
 172. https://rare-technologies.com/doc2vec-tutorial/#comment-2464
 173. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2464#respond
 174. https://rare-technologies.com/doc2vec-tutorial/#comment-2505
 175. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2505#respond
 176. https://rare-technologies.com/doc2vec-tutorial/#comment-2465
 177. https://github.com/piskvorky/gensim/pull/356
 178. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2465#respond
 179. https://rare-technologies.com/doc2vec-tutorial/#comment-2466
 180. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2466#respond
 181. http://codeengine.org/how-to-train-pcategorytitle-model-with-id97/
 182. https://rare-technologies.com/doc2vec-tutorial/#comment-2468
 183. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2468#respond
 184. https://rare-technologies.com/doc2vec-tutorial/#comment-2471
 185. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2471#respond
 186. http://radimrehurek.com/
 187. https://rare-technologies.com/doc2vec-tutorial/#comment-2472
 188. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2472#respond
 189. http://www.itsprite.com/pythonhow-to-calculate-the-sentence-similarity-using-id97-model-of-gensim-with-python/
 190. https://rare-technologies.com/doc2vec-tutorial/#comment-2475
 191. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2475#respond
 192. https://rare-technologies.com/doc2vec-tutorial/#comment-2476
 193. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2476#respond
 194. http://radimrehurek.com/
 195. https://rare-technologies.com/doc2vec-tutorial/#comment-2477
 196. http://radimrehurek.com/gensim/support.html
 197. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2477#respond
 198. https://rare-technologies.com/doc2vec-tutorial/#comment-2479
 199. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2479#respond
 200. http://www.itsprite.com/pythondoc2vec-how-to-get-document-vectors/
 201. https://rare-technologies.com/doc2vec-tutorial/#comment-2480
 202. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2480#respond
 203. http://radimrehurek.com/
 204. https://rare-technologies.com/doc2vec-tutorial/#comment-2481
 205. https://groups.google.com/d/msg/gensim/sust_vzhmta/suphbizvegaj
 206. https://groups.google.com/forum/#!topic/gensim/xkfa75kcv6y
 207. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2481#respond
 208. https://rare-technologies.com/doc2vec-tutorial/#comment-2482
 209. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2482#respond
 210. https://rare-technologies.com/doc2vec-tutorial/#comment-2486
 211. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2486#respond
 212. https://rare-technologies.com/doc2vec-tutorial/#comment-2487
 213. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2487#respond
 214. https://rare-technologies.com/doc2vec-tutorial/#comment-2488
 215. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2488#respond
 216. https://rare-technologies.com/doc2vec-tutorial/#comment-2489
 217. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2489#respond
 218. https://rare-technologies.com/doc2vec-tutorial/#comment-2490
 219. https://groups.google.com/d/forum/gensim
 220. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2490#respond
 221. https://rare-technologies.com/doc2vec-tutorial/#comment-2491
 222. https://rare-technologies.com/doc2vec-tutorial/#comment-2492
 223. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2492#respond
 224. https://rare-technologies.com/doc2vec-tutorial/#comment-2493
 225. https://groups.google.com/forum/#!topic/gensim/h5iftgrff18
 226. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2493#respond
 227. https://rare-technologies.com/doc2vec-tutorial/#comment-2494
 228. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2494#respond
 229. https://rare-technologies.com/doc2vec-tutorial/#comment-2497
 230. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2497#respond
 231. https://rare-technologies.com/doc2vec-tutorial/#comment-2498
 232. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2498#respond
 233. http://radimrehurek.com/
 234. https://rare-technologies.com/doc2vec-tutorial/#comment-2499
 235. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2499#respond
 236. https://rare-technologies.com/doc2vec-tutorial/#comment-2500
 237. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2500#respond
 238. https://rare-technologies.com/doc2vec-tutorial/#comment-2501
 239. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2501#respond
 240. http://radimrehurek.com/
 241. https://rare-technologies.com/doc2vec-tutorial/#comment-2502
 242. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2502#respond
 243. https://rare-technologies.com/doc2vec-tutorial/#comment-2503
 244. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2503#respond
 245. http://radimrehurek.com/
 246. https://rare-technologies.com/doc2vec-tutorial/#comment-2504
 247. https://groups.google.com/d/forum/gensim
 248. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2504#respond
 249. https://rare-technologies.com/doc2vec-tutorial/#comment-2506
 250. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2506#respond
 251. https://rare-technologies.com/doc2vec-tutorial/#comment-2561
 252. https://rare-technologies.com/doc2vec-tutorial/?replytocom=2561#respond
 253. https://badripatro.wordpress.com/2016/07/31/visual-question-answer/
 254. http://gurmezin.com/gensim-vektorel-dokuman-egitimi/
 255. https://amsterdam.luminis.eu/2017/01/16/googling-doc2vec/
 256. https://amsterdam.luminis.eu/2017/01/30/implementing-doc2vec/
 257. http://www.padmashrisuresh.com/2017/05/20/twitterverses-opinion-of-nasa-research/
 258. https://rare-technologies.com/doc2vec-tutorial/#respond
 259. https://rare-technologies.com/cdn-cgi/l/email-protection
 260. https://rare-technologies.com/personal-data-reports/
 261. https://rare-technologies.com/pii_analytics/
 262. https://rare-technologies.com/pii-scan-o365-connector/
 263. https://rare-technologies.com/pivoted-document-length-normalisation/
 264. https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/
 265. https://rare-technologies.com/doc2vec-tutorial/
 266. https://rare-technologies.com/services/
 267. https://rare-technologies.com/careers/
 268. https://rare-technologies.com/our-team/
 269. https://rare-technologies.com/corporate-training/
 270. https://rare-technologies.com/blog/
 271. https://rare-technologies.com/incubator/
 272. https://rare-technologies.com/contact/
 273. https://rare-technologies.com/competitions/
 274. https://rare-technologies.com/sitemap
 275. https://rare-technologies.com/cdn-cgi/l/email-protection#d5bcbbb3ba95a7b4a7b0f8a1b0b6bdbbbab9bab2bcb0a6fbb6bab8
 276. tel:+420 776 288 853

   hidden links:
 278. https://rare-technologies.com/doc2vec-tutorial/#top
 279. https://www.facebook.com/raretechnologies
 280. https://twitter.com/raretechteam
 281. https://www.linkedin.com/company/6457766
 282. https://github.com/piskvorky/
 283. https://rare-technologies.com/feed/
