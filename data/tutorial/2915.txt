   #[1]rare technologies    feed [2]rare technologies    comments feed
   [3]rare technologies    wordrank embedding:    crowned    is most similar to
      king   , not id97   s    canute    comments feed [4]alternate [5]alternate

   [tr?id=1761346240851963&ev=pageview&noscript=1]

   iframe: [6]https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld

   [7]pragmatic machine learning rare technologies [8]navigation

     * [9]services
     * [10]products
          + [11]pii tools
          + [12]scaletext
     * [13]corporate training
          + [14]overview
          + [15]python best practices
          + [16]practical machine learning
          + [17]topic modelling
          + [18]deep learning in practice
     * [19]for students
          + [20]open source
          + [21]incubator
          + [22]competitions
     * [23]company
          + [24]careers
          + [25]our team
     * [26]blog
     * [27]contact
     * [28]search

     * [29]services
     * [30]products
          + [31]pii tools
          + [32]scaletext
     * [33]corporate training
          + [34]overview
          + [35]python best practices
          + [36]practical machine learning
          + [37]topic modelling
          + [38]deep learning in practice
     * [39]for students
          + [40]open source
          + [41]incubator
          + [42]competitions
     * [43]company
          + [44]careers
          + [45]our team
     * [46]blog
     * [47]contact
     * [48]search

wordrank embedding:    crowned    is most similar to    king   , not id97   s
   canute   

   [49]parul sethi 2017-01-23[50] gensim, [51]student incubator

   kings_list_5 comparisons to id97 and fasttext with tensorboard
   visualizations.

   with various embedding models coming up recently, it could be a
   difficult task to choose one. should you simply go with the ones widely
   used in nlp community such as id97, or is it possible that some
   other model could be more accurate for your use case? there are some
   id74 which could help you to some extent in deciding a
   most favorable embedding model to use.

   for instance, the three results above shows how differently these
   embeddings think for the words to be most similar to    king   . the
   wordrank results are more inclined towards the attributes for    king    or
   we can say the words which would co-occur the most with it, whereas
   id97 results in words(names of few kings) that could be said to
   share similar contexts with each other.

   here, i   ll go through some experiments for evaluating the three word
   embeddings, id97, fasttext and wordrank and show how these
   different methods specialize on different downstream tasks. it
   primarily tells that unfortunately, there is no single global embedding
   model you could rely on for different types of nlp applications. you
   really need to choose carefully according to your final use-case.

comparisons

   the following table compares id97, fasttext and wordrank embeddings
   in terms of their computational efficiency and accuracies on some
   popular nlp benchmarks. you can find the corresponding training
   information and hyperparameters settings used for the models, with the
   code to perform all the experiments below in this [52]jupyter notebook.

   for training id97, gensim-v0.13.3 cython implementation was used.
   for training the other two, original implementations of [53]wordrank
   and [54]fasttext was used. id97 and fasttext was trained using the
   skip-gram with negative sampling(=5) algorithm. 300 dimensions with a
   frequency threshold of 5, and window size 15 was used.
   algorithm corpus size (no. of tokens) train time (sec) no. of passes
   for corpus no. of cores (8gb ram) semantic accuracy syntactic accuracy
   total accuracy word similarity (siid113x-999) word similarity (ws-353)
   id97 1 million 18 6 4 4.69 2.77 3.0 pearson: 0.17
   spearman: 0.15 0.37

   0.37
   fasttext 1 million 50 6 4 6.57 36.95 32.9 pearson: 0.13
   spearman: 0.11 0.36

   0.36
   wordrank 1 million ~4 hours 91 4 15.26 4.23 5.7 pearson: 0.09
   spearman: 0.10 0.39

   0.38
   id97 17 million 402 6 4 40.34 41.48 41.1 pearson: 0.31
   spearman: 0.29 0.69

   0.71
   fasttext 17 million 942 6 4 25.75 57.33 45.2 pearson: 0.29
   spearman: 0.27 0.67

   0.66
   wordrank 17 million ~42 hours 91 4 54.52 39.83 44.7 pearson: 0.29
   spearman: 0.28 0.71

   0.72

   you can also now use the python wrapper around wordrank which i   ve have
   added to gensim as part of my incubator project. it has functions for
   training, sorting and ensemble the wordrank embeddings. [55]here   s an
   introductory notebook on using the wrapper.

   the two evaluations performed here used the following benchmarks:
    1. google analogy dataset for word analogies, is used that contains
       around 19k word analogy questions, partitioned into semantic and
       syntactic questions. the semantic questions contain five types of
       semantic analogies, such as capital cities(paris:france;tokyo:?) or
       family (brother:sister;dad:?). the syntactic questions contain nine
       types of analogies, such as plural nouns (dog:dogs;cat:?) or
       comparatives (bad:worse;good:?).
    2. and for word similarity, two datasets siid113x-999 and ws-353
       containing word pairs together with the human-assigned similarity
       judgments are used. these two datasets differ in their sense of
       similarities as siid113x-999 provides a measure of how well the two
       words are interchangeable in similar contexts, and ws-353 tries to
       estimate the relatedness or co-occurrence of two words. for example
       these two word pairs illustrate the difference^[1]:

   pair             siid113x-999 rating wordsim-353 rating
   coast     shore    9.00              9.10
   clothes     closet 1.96                            8.00

   here clothes are not similar to closets (different materials, function
   etc.) in siid113x-999, even though they are very much related.

   now coming to the table, the main observation that can be drawn is the
   specializing nature of embeddings towards particular tasks, as you can
   see the significant difference fasttext makes on syntactic analogies,
   and wordrank on semantic ones. this is due to the different underlying
   approaches they use. fasttext incorporates the morphological
   information about words. on the other hand, wordrank formulates it as a
   ranking problem. that is, given a word w, it aims to output an ordered
   list (c1, c2,         ) of context words such that the words that co-occur
   most with w appear at the top of the list. it does so by applying the
   ranking loss to be most sensitive at the top of context words list
   (where the rank is small) and less sensitive at the lower end of the
   list (where the rank is high).^[2]

   and for the word similarity, id97 seems to perform better on
   siid113x-999 test data, whereas, wordrank performs better on ws-353. this
   is probably due to the different types of similarities these datasets
   address. the list outputs above of most similar words to    king    also
   showed a notable difference of the way these embeddings think of
   similarity.

visualizing the embeddings

   now we   ll try to visualize the three embeddings using tensorboard. but
   as we go through the visualizations of different embeddings below, you
   would observe that they mostly try to replicate the list outputs of
   most_similar for    king    which we saw in the beginning. we don   t really
   get any new insights into the differences between these embeddings from
   visualization, that we couldn   t get from the most_similar lists above.

   the multi-dimensional embeddings are downsized to 2 or 3 dimensions for
   effective visualization. we basically end up with a new 2d or 3d
   embedding which tries to preserve information from the original
   multi-dimensional one. as the word vectors are reduced to a much
   smaller dimension, the exact cosine/euclidean distances between them
   are not preserved, but rather relative, and hence as you   ll see below
   the nearest similarity results may change.

   tensorboard provides two popular id84 methods for
   visualizing the dataset.
    1. principal component analysis (pca) which tries to preserve the
       global structure in the data, and due to this local similarities of
       data points may suffer in few cases. you can see an intuition
       behind it in this nicely explained [56]answer on stackexchange.
    2. the next one, id167, which tries to reduce the pairwise similarity
       error of data points in the new space. that is, instead of focusing
       only on the global error as in pca, id167 try to minimize the
       similarity error for every single pair of data points. you can
       refer to this [57]blog, which explain about its error minimization
       and how similarities are constructed.

   see this [58]doc for instructions on how to use tensorboard. you can
   also use this [59]already-running tensorboard, and upload the tsv files
   containing id27s. i   ve provided the link to my uploaded
   model   s projector config and also the tsv files used for it.

   id97 visualization                 2d pca results for id97
                                      2d id167 results for id97

   id97 model link: [60]w2v_projector_config
   tsv files:
   [61]https://github.com/parulsethi/embeddingvisdata/tree/master/id97

   the figures above show the results for both pca and id167. in id97
   case, both methods perform fine in placing results relative to    king   
   and pca ends up covering 84% variance in data. now if you go to the
   model link and try using id167, you may get some different results as
   id167 is a stochastic method, and might have multiple minima that lead
   to different solutions. so the results may differ even if you use the
   same parameter settings as mine (mentioned below). you can see this
   [62]thread for some differences between both methods.

   fasttext visualization                 2d pca results for fasttext
                                     2d id167 results for fasttext

   fasttext model link: [63]ft_projector_config
   tsv files:
   [64]https://github.com/parulsethi/embeddingvisdata/tree/master/fasttext

   for fasttext, both the methods performed similar, but the results were
   little inferior than that of id97 in placing the words according to
   their similarity order.

   wordrank visualization                2d pca results for wordrank
                               2d id167 results for wordrank

   wordrank model link: [65]wr_projector_config
   tsv files:
   [66]https://github.com/parulsethi/embeddingvisdata/tree/master/wordrank

   for wordrank, id167 results seems to be slightly better than pca   s, as
   in pca, some top similar results got placed comparatively far.
   probably, because pca covers 58% variance in this case which is also
   the lowest amongst all models.

   now id167 can be a bit tricky to use, and you should really read this
   [67]article before using it for any interpretation of your data. as for
   generating the above visualizations from id167, i used the perplexity
   value of 10, learning rate 10, iteration count of 500 till which all
   three models stabilized to a particular configuration, and isolated my
   data to 20 most similar points around the word    king   . also, there   s an
   option to normalize the data by using    sphereize data    but that could
   change few results in the original space, so it wasn   t used for the
   above visualizations.

   note: you can also use gensim to [68]convert your word vector files to
   tsv format and then simply upload them
   to [69]http://projector.tensorflow.org/ to generate this visualization.

wordrank hyperparameter tuning

   now, before going to the next part of evaluation, i   d like to put some
   light on an important aspect of wordrank that when it comes to tuning
   the hyperparameters, wordrank generally performs better with its
   default values only that are provided in its [70]demo and is not
   extremely sensitive to that as compared to the other embedding models.
   these are some of the explored values for wordrank and their optimal
   cases.
   hyperparameter explored values optimal performing cases
   window size 5, 10, 15 small window sizes decrease the accuracy for both
   word similarity and analogy tasks, and word rank performed optimum with
   a value of 15
   w+c ensemble -w, -w+c while ensemble of word and context embeddings
   improve accuracy in small corpus, it doesn   t help in large corpus and
   effects accuracy negatively. (this is a post-training hyperparameter
   explained below in detailed word analogy experiment)
   epochs 50, 100, 500 as word rank is very time consuming, it may not be
   feasible to do 500 epochs which is default, so you can also go with 100
   epochs which give somewhat comparable results.
   learning rate 0.001, 0.025 large learning rate results in diverged
   embeddings and too small value could take very long for convergence
   given wordrank is already very time consuming

word frequency and model performance

   in this section, we   ll see how embedding model   s performance in analogy
   task varies across the range of word frequency. accuracy vs. frequency
   graph is used to analyze this effect.

   the mean frequency of four words involved in each analogy is computed,
   and then bucketed with other analogies having similar mean frequencies.
   each bucket has six percent of the total analogies involved in the
   particular task. you can go to this [71]repo if you want to inspect
   about what analogies(with their sorted frequencies) were used for each
   of the plot.

   for 1 million tokens (brown corpus):
   accuracy vs frequency 1m

   the main observations that can be drawn here are-
    1. in semantic analogies, all the models perform poorly for rare words
       as compared to their performance at more frequent words.
    2. in syntactic analogies, fasttext performance is way better than
       id97 and wordrank.
    3. if we go through the frequency range in syntactic analogies plot,
       fasttext performance drops significantly at highly frequent words,
       whereas, for id97 and wordrank there is no significant
       difference over the whole frequency range.
    4. end plot shows the results of combined semantic and syntactic
       analogies. it has more resemblance to the syntactic analogy   s plot
       because the total no. of syntactic analogies(=5461) is much greater
       than the total no. of semantic ones(=852). so it   s bound to trace
       the syntactic   s results as they have more weightage in the total
       analogies considered.

   now, let   s see if a larger corpus creates any difference in this
   pattern of model   s performance over different frequencies.

   for 17 million tokens (text8):

   accuracy vs frequency 17m

   following points can be observed in this case-
    1. for semantic analogies, all the models perform comparitively poor
       on rare words and also when the word frequency is high towards the
       end.
    2. for syntactic analogies, fasttext performance is fairly well on
       rare words but then falls steeply at highly frequent words.
    3. wordrank and id97 perform very similar with low accuracy for
       rare and highly frequent words in syntactic analogies.
    4. fasttext is again better in total analogies case due to the same
       reason described previously. here the total no. of semantic
       analogies are 7416 and syntactic analogies are 10411.

   these graphs also conclude that wordrank is the best suited method for
   semantic analogies, and fasttext for syntactic analogies for all the
   frequency ranges and over different corpus sizes, though all the
   embedding methods could become very competitive as the corpus size
   largerly increases^[3].

word analogies

   this section provides the detailed logs of embedding models on analogy
   task that shows the accuracies for different analogy categories. this
   will also show how these embeddings perform differently on semantic and
   syntactic analogies.

   the first comparison is on 1 million tokens (brown corpus).
   analogy id97 1m

   analogy fasttext 1m

   analogy wordrank 1m

   now, wordrank generates two sets of embeddings, word and context
   embeddings. former one is for vocabulary words and latter for the
   context words. wordrank models the relation between them using their
   vector   s inner product which should be directly proportional to their
   mutual relevance i.e. the more relevant the context is, the larger
   should be its inner product with the vocab word. this formulation helps
   in the case where you need to model the semantic relatedness in your
   data, i.e., how often the two words co-occur.

   so, let   s try the ensemble, which is basically a post-training
   hyperparameter and adds the two sets of vectors generated. for example,
   word    cat    can be represented as:
   ensemble eq where ~w and ~c are the word and context embeddings,
   respectively.

   analogy wordrank ensemble 1m

   this vector combination, called ensemble method, generally helps reduce
   overfitting and noise and gives a small performance boost (ciresan et
   al., 2012) and the same is apparent from the results of ensemble
   embedding here.

   now, let   s evaluate these algorithms on a large corpus, text8, which is
   a preprocessed sample of wikipedia data.
   analogy id97 ensemble 17m

   analogy fasttext ensemble 17m

   analogy wordrank ensemble 17m

   with the text8 corpus, we observe a similar pattern. wordrank again
   performs better than both the other embeddings on semantic analogies,
   whereas, fasttext performs significantly better on syntactic analogies.

   note: wordrank can sometimes produce nan values during model
   evaluation, when the embedding vector values get too diverged at some
   iterations, but it dumps embedding vectors after every few iterations,
   so we can just load embeddings from a different iteration   s text file.

conclusion

   the experiments here conclude two main points about comparing word
   embeddings. firstly, there is no single global embedding model we could
   rely on for different types of nlp applications. for example, in word
   similarity, wordrank performed better than the other two algorithms for
   ws-353 test data whereas, id97 performed better on siid113x-999. this
   is probably due to the different type of similarities these datasets
   address as explained in task intro above. and in word analogy task,
   wordrank performed better for semantic analogies and fasttext for
   syntactic analogies. this basically tells that we need to choose the
   embedding method carefully according to our final use-case.

   secondly, frequency of our query words do matter apart from the
   generalized model performance. as we observed in accuracy vs. frequency
   graphs that models perform differently depending on the frequency of
   question analogy words in training corpus. for example, we are likely
   to get poor results if our query words are all highly frequent.

thanks

   i am thankful to lev for guiding me throughout this project and
   teaching me the know-hows of id27s. i also appreciate the
   valuable feedbacks from jayant and radim for this blog.

references

    1. [72]similarity dataset
    2. [73]wordrank: learning id27s via robust ranking
    3. [74]id97 and fasttext comparison notebook
    4. [75]wordrank wrapper quickstart tutorial

   [76]fasttext[77]gensim[78]student incubator[79]word
   embeddings[80]id97[81]wordrank

author of post

parul sethi's bio:

   undergrad student of maths and it at cic, university of delhi. rare
   incubator student. gsoc'17 with gensim

need expert consulting in ml and nlp?

   ________________________________________

   ________________________________________


   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   please leave this field empty. ________________________________________

   send

categories

   categories[select category___________]

archives

   archives [select month__]

recent posts

     * [82]export pii drill-down reports
     * [83]personal data analytics
     * [84]scanning office 365 for sensitive pii information
     * [85]pivoted document length normalisation
     * [86]sent2vec: an unsupervised approach towards learning sentence
       embeddings

stay ahead of the curve

get our latest tutorials, updates and insights delivered straight to your
inbox.

   ____________________

   ____________________

   subscribe
   ____________________
   1-2 times a month, if lucky. your information will not be shared.

   [87][footer-logo.png]
     * [88]services
     * [89]careers
     * [90]our team
     * [91]corporate training
     * [92]blog
     * [93]incubator
     * [94]contact
     * [95]competitions
     * [96]site map

   rare technologies [97][email protected] sv  tova 5, prague, czech
   republic [98](eu) +420 776 288 853
   type and press    enter    to search ____________________

references

   visible links
   1. https://rare-technologies.com/feed/
   2. https://rare-technologies.com/comments/feed/
   3. https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-id97s-canute/feed/
   4. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-id97s-canute/
   5. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-id97s-canute/&format=xml
   6. https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld
   7. https://rare-technologies.com/
   8. https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-id97s-canute/
   9. https://rare-technologies.com/services/
  10. https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-id97s-canute/
  11. https://pii-tools.com/
  12. https://scaletext.com/
  13. https://rare-technologies.com/corporate-training/
  14. https://rare-technologies.com/corporate-training/
  15. https://rare-technologies.com/python-best-practices/
  16. https://rare-technologies.com/practical-machine-learning/
  17. https://rare-technologies.com/topic-modelling-training/
  18. https://rare-technologies.com/deep_learning_training/
  19. https://rare-technologies.com/incubator
  20. https://github.com/rare-technologies/
  21. https://rare-technologies.com/incubator/
  22. https://rare-technologies.com/competitions/
  23. https://rare-technologies.com/#braintrust
  24. https://rare-technologies.com/careers/
  25. https://rare-technologies.com/our-team/
  26. https://rare-technologies.com/blog/
  27. https://rare-technologies.com/contact/
  28. https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-id97s-canute/
  29. https://rare-technologies.com/services/
  30. https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-id97s-canute/
  31. https://pii-tools.com/
  32. https://scaletext.com/
  33. https://rare-technologies.com/corporate-training/
  34. https://rare-technologies.com/corporate-training/
  35. https://rare-technologies.com/python-best-practices/
  36. https://rare-technologies.com/practical-machine-learning/
  37. https://rare-technologies.com/topic-modelling-training/
  38. https://rare-technologies.com/deep_learning_training/
  39. https://rare-technologies.com/incubator
  40. https://github.com/rare-technologies/
  41. https://rare-technologies.com/incubator/
  42. https://rare-technologies.com/competitions/
  43. https://rare-technologies.com/#braintrust
  44. https://rare-technologies.com/careers/
  45. https://rare-technologies.com/our-team/
  46. https://rare-technologies.com/blog/
  47. https://rare-technologies.com/contact/
  48. https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-id97s-canute/
  49. https://rare-technologies.com/author/parul/
  50. https://rare-technologies.com/category/gensim/
  51. https://rare-technologies.com/category/student-incubator/
  52. https://github.com/parulsethi/gensim/blob/wordrank_wrapper/docs/notebooks/wordrank_comparisons.ipynb
  53. https://bitbucket.org/shihaoji/wordrank
  54. https://github.com/facebookresearch/fasttext
  55. https://github.com/rare-technologies/gensim/blob/develop/docs/notebooks/wordrank_wrapper_quickstart.ipynb
  56. https://stats.stackexchange.com/questions/176672/what-is-meant-by-pca-preserving-only-large-pairwise-distances
  57. https://www.oreilly.com/learning/an-illustrated-introduction-to-the-id167-algorithm
  58. https://www.tensorflow.org/how_tos/embedding_viz/
  59. http://projector.tensorflow.org/
  60. http://projector.tensorflow.org/?config=https://raw.githubusercontent.com/parulsethi/embeddingvisdata/master/id97/w2v_config.json
  61. https://github.com/parulsethi/embeddingvisdata/tree/master/id97
  62. https://stats.stackexchange.com/questions/238538/are-there-cases-where-pca-is-more-suitable-than-id167?rq=1
  63. http://projector.tensorflow.org/?config=https://raw.githubusercontent.com/parulsethi/embeddingvisdata/master/fasttext/ft_config.json
  64. https://github.com/parulsethi/embeddingvisdata/tree/master/fasttext
  65. http://projector.tensorflow.org/?config=https://raw.githubusercontent.com/parulsethi/embeddingvisdata/master/wordrank/wr_config.json
  66. https://github.com/parulsethi/embeddingvisdata/tree/master/wordrank
  67. http://distill.pub/2016/misread-tsne/
  68. https://github.com/rare-technologies/gensim/pull/1051
  69. http://projector.tensorflow.org/
  70. https://bitbucket.org/shihaoji/wordrank/src/7f87a8cd7146898368d25615c4a8f02a88ab48ef/scripts/demo.sh?at=master&fileviewer=file-view-default
  71. https://github.com/parulsethi/embeddingvisdata/tree/master/wordanalogyfreq
  72. https://www.cl.cam.ac.uk/~fh295/siid113x.html
  73. https://arxiv.org/pdf/1506.02761v3.pdf
  74. https://github.com/rare-technologies/gensim/blob/develop/docs/notebooks/id97_fasttext_comparison.ipynb
  75. https://github.com/rare-technologies/gensim/blob/develop/docs/notebooks/wordrank_wrapper_quickstart.ipynb
  76. https://rare-technologies.com/tag/fasttext/
  77. https://rare-technologies.com/tag/gensim/
  78. https://rare-technologies.com/tag/student-incubator/
  79. https://rare-technologies.com/tag/word-embeddings/
  80. https://rare-technologies.com/tag/id97/
  81. https://rare-technologies.com/tag/wordrank/
  82. https://rare-technologies.com/personal-data-reports/
  83. https://rare-technologies.com/pii_analytics/
  84. https://rare-technologies.com/pii-scan-o365-connector/
  85. https://rare-technologies.com/pivoted-document-length-normalisation/
  86. https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/
  87. https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-id97s-canute/
  88. https://rare-technologies.com/services/
  89. https://rare-technologies.com/careers/
  90. https://rare-technologies.com/our-team/
  91. https://rare-technologies.com/corporate-training/
  92. https://rare-technologies.com/blog/
  93. https://rare-technologies.com/incubator/
  94. https://rare-technologies.com/contact/
  95. https://rare-technologies.com/competitions/
  96. https://rare-technologies.com/sitemap
  97. https://rare-technologies.com/cdn-cgi/l/email-protection#1871767e77586a796a7d356c7d7b70767774777f717d6b367b7775
  98. tel:+420 776 288 853

   hidden links:
 100. https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-id97s-canute/#top
 101. https://www.facebook.com/raretechnologies
 102. https://twitter.com/raretechteam
 103. https://www.linkedin.com/company/6457766
 104. https://github.com/piskvorky/
 105. https://rare-technologies.com/feed/
