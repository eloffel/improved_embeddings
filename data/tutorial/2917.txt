   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

[ml] what is the difference between id119 and stochastic
id119

   [9]go to the profile of nicola bernini
   [10]nicola bernini (button) blockedunblock (button) followfollowing
   apr 10, 2017

short version

   while in id119 (gd) the whole training set is considered
   before taking one model parameters update step, in stochastic gradient
   descent (sgd) only one data point is considered for each model
   parameters update step, cycling over the training set.

long version

   let   s consider an objective function like
   [1*qllbml0-fhmqagyvz91s5q.gif]
   objective function

   with
     *    w    the model parameters set
     *    q_{i}    the function associating to each data pair the
       corresponding error (see note1)
     *    n    the number of data points

   the new    w    is computed as a local improvement (i.e. local minimization
   of the cost function) of the current    w    hence the objective function
   gradient needs to be computed
   [1*-tfxetxopsmfcapkegccmq.gif]
   model paramters update step

   with
     *    k    the number of data point to be considered for the update step

   in gd we have
   [1*pjzutsgjudlydqdfjesgra.gif]

   while in sgd we have
   [1*__zp7bvflpcwfvv8ahyjyq.gif]

notes

   note1

   formal error function definition based on residual
   [1*yyi-ntuluawkbinibcneqq.gif]

   with
     *    m_{w}(x)    the id136 performed by the model parametrized with
       the    w    parameters set on the    x_{i}    input
     *    y_{i}    observation associated to    x_{i}   

   typically
   [1*xnbd8yxr9oerhjnzjv7j4q.gif]

   which leads to least square minimization

     * [11]machine learning
     * [12]data science

   (button)
   (button)
   (button) 45 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [13]go to the profile of nicola bernini

[14]nicola bernini

   algo engineer, phd, physicist. deep learning, programming, quantum
   computing. [15]https://www.quora.com/profile/nicola-bernini

     * (button)
       (button) 45
     * (button)
     *
     *

   [16]go to the profile of nicola bernini
   never miss a story from nicola bernini, when you sign up for medium.
   [17]learn more
   never miss a story from nicola bernini
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/be79ab450ef0
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@nicolabernini_63880/ml-what-is-the-difference-between-gradient-descent-and-stochastic-gradient-descent-be79ab450ef0&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@nicolabernini_63880/ml-what-is-the-difference-between-gradient-descent-and-stochastic-gradient-descent-be79ab450ef0&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@nicolabernini_63880?source=post_header_lockup
  10. https://medium.com/@nicolabernini_63880
  11. https://medium.com/tag/machine-learning?source=post
  12. https://medium.com/tag/data-science?source=post
  13. https://medium.com/@nicolabernini_63880?source=footer_card
  14. https://medium.com/@nicolabernini_63880
  15. https://www.quora.com/profile/nicola-bernini
  16. https://medium.com/@nicolabernini_63880
  17. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  19. https://medium.com/p/be79ab450ef0/share/twitter
  20. https://medium.com/p/be79ab450ef0/share/facebook
  21. https://medium.com/p/be79ab450ef0/share/twitter
  22. https://medium.com/p/be79ab450ef0/share/facebook
