   #[1]rss [2]slideshare search [3]alternate [4]alternate [5]alternate
   [6]alternate [7]alternate [8]alternate [9]slideshow json oembed profile
   [10]slideshow xml oembed profile [11]alternate [12]alternate
   [13]alternate

   (button)

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our [14]user
   agreement and [15]privacy policy.

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our
   [16]privacy policy and [17]user agreement for details.

   [18]slideshare [19]explore search [20]you

     * [21]linkedin slideshare

     * [22]upload
     * [23]login
     * [24]signup

     *
     * ____________________ (button) submit search

     * [25]home
     * [26]explore

     * [27]presentation courses
     * [28]powerpoint courses
     *
     * by [29]linkedin learning

   ____________________
   successfully reported this slideshow.

   we use your linkedin profile and activity data to personalize ads and
   to show you more relevant ads. [30]you can change your ad preferences
   anytime.
   building a performing machine learning model from a to z

   building a performing machine learning model from a to z a deep dive
   into fundamental concepts and practices in machine le...

      a computer program is said to learn from experience e with respect to
   some class of tasks t and performance measure p, if...

   what is machine learning (ml)? machine learning is everywhere   . uses it
   to recognize the music you are listening to. uses ...

   a machine learning model intends to determine the optimal structure in
   a dataset to achieve an assigned task. + = data alg...

   what is a machine learning model? + = data algorithms model there are 4
   steps to build a machine learning model    2 iii 4iv...

   what is a machine learning model? 2 iii 4ivii data preparation feature
   engineering data modeling performance measure i thi...

   what tools we will use you can code your ml model using programming
   language you can write your code in , a popular interf...

   what tools we will use uses the notebook format, with input cells
   containing code and output cells containing the result o...

   what tools we will use we will also use python modules. modules are
   everywhere in python, they enable to implement an infi...

   what you will learn in this presentation iii data preparation feature
   engineering data modeling performance improvement i ...

   2 iii 4ivii data preparation feature engineering data modeling
   performance measure i v performance improvement

   prepare datai preparing your data can be done in 3 steps query your
   data clean your data format your data 1 2 3 deal with ...

   prepare dataiquery your data1 you can query your data using if you have
   a database to connect to: if you have a csv (e.g. ...

   prepare dataiquery your data1 this will give you a dataframe with your
   raw data a dataframe is a common data format that w...

   prepare dataiclean data deal with missing valuesa 2 compute ratio rm =
   number of missing values total number of values if ...

   prepare dataiclean data remove outliersb 2 some of your columns will
   certainly contain outliers, i.e. a value that lies at...

   prepare dataiformat data the most common transformation is the encoding
   of categorical variables. 3 your will have to modi...

   prepare datai now that i have a clean dataset, i can start feature
   engineering.

   2 iii 4ivii data preparation feature engineering data modeling
   performance measure i v performance improvement

   feature engineeringiiwhat is a feature? example: predict the price of
   an apartment    a feature is an individual measurable ...

   feature engineeringiiwhat is feature engineering? feature engineering
   is the process of transforming raw data into relevan...

   feature engineeringiiwhat is feature engineering? example: predict the
   price of an apartment in paris informative? discrim...

   feature engineeringii x = x1,1 x2,1 x3,1 xm-1,1 xm,1     x1,2 x2,2 x3,2
   xm-1,2 xm,2     x1,n x2,n x3,n xm-1,n xm,n         y = y1 ...

   feature engineeringii feature engineering usually includes,
   successively: feature construction what is feature engineering...

   feature engineeringiifeature construction example: decompose a
   date-time same raw data different problems different featur...

   feature engineeringiifeature construction feature construction is where
   you will need all the domain expertise and is key ...

   feature engineeringiifeature transformation examples of
   transformations: name scaling transformation objectives 2 log redu...

   feature engineeringiidimension reduction3 dimension reduction is the
   process of reducing the number of features used to bu...

   feature engineeringiidimension reduction feature selectiona feature
   selection is the process of selecting the most relevan...

   feature engineeringiidimension reduction feature selectiona remove non
   informative features what is the impact on model pe...

   feature engineeringiidimension reduction feature selectiona remove non
   discriminative features ii principle: we remove any...

   feature engineeringiidimension reduction feature selectiona remove
   redundant features iii principle: we remove features th...

   feature engineeringiidimension reduction feature selectiona identifying
   the most relevant features will help you get a bet...

   feature engineeringiidimension reduction feature extractionb feature
   extraction starts from an initial set of measured dat...

   feature engineeringiidimension reduction the most common algorithm for
   feature extraction is principal component analysis ...

   feature engineeringii pca new feature old feature x1 old feature x2 x   
   = ax1 + bx2 dimension reduction maximized variance ...

   feature engineeringiidimension reduction feature extractionb the
   principal components (pc) are built along axes so that th...

   feature engineeringiidimension reduction feature extractionb in a
   nutshell: principal component analysis (pca) given a des...

   feature engineeringiidimension reduction feature extractionb a famous
   implementation of pca is in face recognition. 3 pca

   feature engineeringiiin a nutshell feature construction1 2 3 turn raw
   data into relevant features that best represent the ...

   your machine can now start learning. let   s see how.

   2 iii 4ivii data preparation feature engineering data modeling
   performance measure i v performance improvement

      the goal is to turn data into information, and information into
   insight.    carly fiorina, former ceo of hewlett-packard

   data modelingiii you are going to train a model on your data using a
   learning algorithm. remember: + = data algorithms mod...

   data modelingiii supervised vs. unsupervised learning 1 regression with
   id75a classification with random fore...

   data modelingiii supervised unsupervisedvs.

   data modelingiiisupervised vs. unsupervised learning supervised
   learning unsupervised learning when the training set conta...

   data modelingiii regression classification when the label to predict is
   a continuous value example: predict the price of a...

   data modelingiiisupervised vs. unsupervised learning1 regression with
   id75a the output of a id75...

   data modelingiiisupervised vs. unsupervised learning1 feature x: # of
   rooms continuous label y: price (m   ) trained model y...

   data modelingiiisupervised vs. unsupervised learning1 regression with
   id75a using a id75 assumes...

   data modelingiiisupervised vs. unsupervised learning1 the basic linear
   regression method is called ordinary least squares ...

   data modelingiiisupervised vs. unsupervised learning1 cost functions
   are often minimized using an algorithm called gradien...

   data modelingiiisupervised vs. unsupervised learning1 now, let   s
   discover classification with id79s! you

   data modelingiiisupervised vs. unsupervised learning1 classification
   with id79sc what is this fruit? ? ?? ? ? ?    ...

   data modelingiiisupervised vs. unsupervised learning1 classification
   with id79sc what are the problems with decis...

   data modelingiiisupervised vs. unsupervised learning1 classification
   with id79sc general idea id79s algo...

   data modelingiiisupervised vs. unsupervised learning1 classification
   with id79sc take different random subsets of...

   data modelingiiisupervised vs. unsupervised learning classification
   with id79sc 1 our id79s model is 99....

   data modelingiiisupervised vs. unsupervised learning classification
   with id79sc 1 note that there id90 ...

   data modelingiii id91 (= unlabelled classification) when we
   cluster examples with no label one cluster name gender a...

   data modelingiiisupervised vs. unsupervised learning1 id91 with
   id116d step all data points are unlabeled. we rand...

   data modelingiiisupervised vs. unsupervised learning1 id91 with
   id116d steps and are repeated   .2 3    until converge...

   data modelingiiisupervised vs. unsupervised learning1 id91 with
   id116d examples of applications of id91 mark...

   data modelingiii these algorithms can easily be implemented in
   supervised vs. unsupervised learning1 id75 ran...

   data modelingiii parametric nonparametricvs.

   data modelingiii because it assumes a pre-defined form for the function
   modelling the data, with a set of parameters of fi...

   data modelingiii algorithms that do not make strong assumptions about
   the form of the mapping function are nonparametric a...

   data modelingiiiparametric vs. nonparametric algorithms2 parametric
   algorithms nonparametric algorithms pros cons simpler ...

   data modelingiii what are the best algorithms?

   data modelingiii we   ll let the cat out of the bag now. there is no such
   thing as    best algorithms   . which is why choosing ...

   data modelingiiiwhat are the best algorithms?3 input data nearest
   neighbors linear id166 rbf id166 gaussian process decision t...

   data modelingiii moreover, every algorithm has parameters called
   hyperparameters. they have default values in , but how yo...

   data modelingiiiwhat are the best algorithms? examples of
   hyperparameters, as implemented in 3 hyperparameters are paramet...

   data modelingiii they will only perform differently because of the
   specificities of your data. these algorithms and the po...

   data modelingiii    when averaged across all possible situations, every
   algorithm performs equally well.    this is what the    ...

   data modelingiii building a performing ml model is all about: - making
   the right assumptions about your data - choosing th...

   data modelingiii but how do i know if my model is performing?

   2 iii 4ivii data preparation feature engineering data modeling
   performance measure i v performance improvement

   performance measureiv use your model to predict the labels in your own
   dataset assessing your model performance is a 2-ste...

   performance measureiv predicting your dataset labels 1 a
   cross-validationb choosing the right performance indicator traini...

   performance measureiv you never train your model and test its
   performance on the same dataset. it   s a bit like sitting for...

   performance measureivpredicting your dataset labels dataset training
   set test set commonly c. 80% of dataset a test set en...

   performance measureivpredicting your dataset labels dataset training
   set test set commonly c. 80% of dataset commonly c. 2...

   performance measureivpredicting your dataset labels dataset training
   set test set commonly c. 60% of dataset commonly c. 2...

   performance measureivpredicting your dataset labels cross-validationb 1
   kfold cross-validation consists in repeating the t...

   performance measureivpredicting your dataset labels cross-validationb 1
   there are several ways to use kfold cross-validati...

   performance measureiv now that i know the method to rigorously measure
   the performance, which indicator will i use?

   performance measureivchoosing the right performance indicator2
   regressiona examples of two commonly used indicators mean s...

   performance measureivchoosing the right performance indicator2
   classificationb number of correctly predicted labels total ...

   performance measureivchoosing the right performance indicator2
   classificationb roc curve recall= 100% precision = % of pos...

   create a dirty but complete model as quick as possible to iterate on it
   afterwards. this is the right way to go!

   2 iii 4ivii data preparation feature engineering data modeling
   performance measure i v performance improvement

   performance improvementv reasons for underperformance 1 a overfittingb
   solutions to increase performance underfitting 2

   performance improvementv what are the reasons why your model is not
   performing well?

   performance improvementv it should reproduce the underlying data
   structure but leave aside random noise in the data. a per...

   performance improvementv there are two reasons why a model would not
   generalize and thus not perform correctly: - underfit...

   performance improvementvreasons for underperformance underfitting
   happens when your model is too simple to reproduce the u...

   performance improvementvreasons for underperformance overfitting
   happens when your model is too complex to reproduce the u...

   performance improvementvin a nutshell performance on    training set test
   set bad bad very good bad good good overfitting ca...

   performance improvementv ok, i got the point    so, how do i address
   these overfitting and underfitting issues?

   performance improvementv    ensemble methods    a c d e issue of the
   potential solutions on     more features more complex algor...

   performance improvementvsolutions to increase performance let   s discuss
   the mentioned solutions one by one. (final stretch...

   performance improvementv the study below shows how different algorithms
   perform similarly for a given problem as the amoun...

   performance improvementvsolutions to increase performance the more
   training examples there are, the more complex it is for...

   performance improvementv some features might contain more noise than
   informative data for your model. this is especially t...

   performance improvementvsolutions to increase performance2 b more
   featuresb we are talking about science, not divination! ...

   performance improvementvsolutions to increase performance2 b in a
   nutshell: more data! however, you must know how to make ...

   performance improvementv "we don   t have better algorithms. we just have
   more data." 's research director peter norvig, 2009

   performance improvementv if you   re convinced you need more data, turk
   might help you. check it out ! solutions to increase...

   performance improvementvsolutions to increase performance error below
   is how a model error will theoretically evolve as it...

   performance improvementvsolutions to increase performance2 d
   id173 id173 aims at reducing overfitting by...

   performance improvementvsolutions to increase performance2 d
   id173 this regularized id75 is called a...

   performance improvementvsolutions to increase performance2 e id112
   id112 = bootstrap + aggregating take n different ra...

   performance improvementvsolutions to increase performance2 f boosting
   id112 boosting aggregating equally the results of ...

   performance improvementvsolutions to increase performance2 f boosting
   illustration of gradient boosted trees (gbt) algorit...

   performance improvementvsolutions to increase performance2 f boosting
   id119     find optimal regression tree dj b...

   performance improvementvsolutions to increase performance2 f boosting
   regression tree     id119 will find the opt...

   performance improvementvsolutions to increase performance2 f boosting
   you can visualize the output of gradient boosting tr...

   performance improvementvsolutions to increase performance2 f boosting
   note that even for classification tasks, gradient bo...

   performance improvementvsolutions to increase performance2 f boosting
   tree ensemble methods (i.e. id112/boosting used wi...

   you now have all you need to build a performing machine learning model!

   you must assess when the effort is not worth it anymore! model
   performance this is how the performance of your model will ...

   conclusion in 2006, netflix offered a $1m prize for anyone who would
   improve the accuracy of their id126 b...

   building a machine learning model: summary iii data preparation feature
   engineering data modeling performance improvement ...

   thank you! good ressources if you wanna talk about machine learning:
   charles.vestur@gmail.com

   the very famous mooc from andrew ng. an excellent introduction to
   machine learning, in which you will learn different algo...
   upcoming slideshare
   []
   loading in    5
     
   [] 1
   (button)
   1 of 127 (button)
   (button) (button)
   like this presentation? why not share!
     * share
     * email
     *
     *

     * [31]machine learning by analogy machine learning by analogy
       by colleen farrelly 47414 views
     * [32]maintenance and management best pra... maintenance and
       management best pra... by ca | automic soft... 1018 views
     * [33]predictive maintenance predictive maintenance by amey kulkarni
       554 views
     * [34]ml workshop 1: a new architecture f... ml workshop 1: a new
       architecture f... by mapr technologies 1332 views
     * [35]using hadoop for big data using hadoop for big data by data
       science thai... 3038 views
     * [36]myths of data science myths of data science by data science
       thai... 630 views

   (button)

   share slideshare
     __________________________________________________________________

     * [37]facebook
     * [38]twitter
     * [39]linkedin

   embed
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   size (px)
   start on
   [x] show related slideshares at end
   wordpress shortcode ____________________
   link ____________________

building a performing machine learning model from a to z

   15,578 views

     * (button) share
     * (button) like
     * ...
          +

   [40]charles vestur

[41]charles vestur

   , consultant at eleven | strategy & management
   [42]follow

   (button) (button) (button)

   published on jan 29, 2017

   a 1-hour read to become highly knowledgeable about machine learning and
   the machinery underneath, from scratch!
   a presentation introducing to all fundamental concepts of machine
   learning step by step, following a classical approach to build a
   performing model. simple examples and illustrations are used all along
   the presentation to make the concepts easier to grasp.
   (button) ...

   published in: [43]data & analytics

     * [44]8 comments
     * [45]120 likes
     * [46]statistics
     * [47]notes

     * full name
       full name
       comment goes here.
       12 hours ago   [48]delete [49]reply [50]block
       are you sure you want to [51]yes [52]no
       your message goes here

   no profile picture user
   ____________________
   [53](button) post
     * [54]kendricklamost
       [55]kendricklamost
       sex in your area for one night is there tinyurl.com/hotsexinarea
       copy and paste link in your browser to visit a site)
       3 weeks ago    [56]reply
       are you sure you want to  [57]yes  [58]no
       your message goes here
     * [59]kushkul
       [60]kush kulshrestha , data analyst at happay
       good presentation
       1 month ago    [61]reply
       are you sure you want to  [62]yes  [63]no
       your message goes here
     * [64]kendricklamost
       [65]kendricklamost
       girls for sex are waiting for you https://bit.ly/2tq8uay
       1 month ago    [66]reply
       are you sure you want to  [67]yes  [68]no
       your message goes here
     * [69]kendricklamost
       [70]kendricklamost
       meetings for sex in your area are there: https://bit.ly/2tq8uay
       1 month ago    [71]reply
       are you sure you want to  [72]yes  [73]no
       your message goes here
     * [74]charlesvestur
       [75]charles vestur , consultant at eleven | strategy & management
       at eleven | strategy & management
       [76]@guillaume lebault merci beaucoup pour ce retour tr  s
       encourageant ! je suis ravi que le document respecte ses promesses.
       a bient  t
       1 year ago    [77]reply
       are you sure you want to  [78]yes  [79]no
       your message goes here

   [80]show more
     * [81]tomdhaene1
       [82]tom dhaene , professor at ghent university
       2 hours ago
     * [83]junyu2
       [84]jun yu , machine learning researcher and engineer at amazon
       2 weeks ago
     * [85]kushkul
       [86]kush kulshrestha , data analyst at happay
       1 month ago
     * [87]yaronsegev4
       [88]yaron segev , xtremio founder; iguazio founder, investor, board
       member at co-founder, vp product, board member
       1 month ago
     * [89]andresrivera177
       [90]andresrivera177
       3 months ago

   [91]show more
   no downloads
   views
   total views
   15,578
   on slideshare
   0
   from embeds
   0
   number of embeds
   120
   actions
   shares
   0
   downloads
   4
   comments
   8
   likes
   120
   embeds 0
   no embeds
   no notes for slide

building a performing machine learning model from a to z

    1. 1. building a performing machine learning model from a to z a deep
       dive into fundamental concepts and practices in machine learning
    2. [92]2.    a computer program is said to learn from experience e with
       respect to some class of tasks t and performance measure p, if its
       performance at tasks in t, as measured by p, improves with
       experience e." what is machine learning (ml)? tom m. mitchel,
       computer scientist, 1997
    3. [93]3. what is machine learning (ml)? machine learning is
       everywhere   . uses it to recognize the music you are listening to.
       uses it to always recommend you more products. some japanese
       farmers use it to classify the different types of cucumbers they
       harvest. (read the story here)
    4. [94]4. a machine learning model intends to determine the optimal
       structure in a dataset to achieve an assigned task. + = data
       algorithms model what is a machine learning model? it results from
       learning algorithms applied on a training dataset.
    5. [95]5. what is a machine learning model? + = data algorithms model
       there are 4 steps to build a machine learning model    2 iii 4ivii
       data preparation feature engineering data modeling performance
       measure i
    6. [96]6. what is a machine learning model? 2 iii 4ivii data
       preparation feature engineering data modeling performance measure i
       this is a highly iterative process, to repeat       until your model
       reaches a satisfying performance! v performance improvement
    7. [97]7. what tools we will use you can code your ml model using
       programming language you can write your code in , a popular
       interface for machine learning.
    8. [98]8. what tools we will use uses the notebook format, with input
       cells containing code and output cells containing the result of
       your code. you can iterate on your code very quickly, instantly
       visualizing the result of your modifications.
    9. [99]9. what tools we will use we will also use python modules.
       modules are everywhere in python, they enable to implement an
       infinity of actions with just a few lines of code. pandas is the
       reference module to efficiently manipulate millions rows of data in
       python. scikit learn is one of the reference module for machine
       learning in python. more convenient modules for data computations
       and data visualization.
   10. [100]10. what you will learn in this presentation iii data
       preparation feature engineering data modeling performance
       improvement i ii v performance measure iv how do you turn raw data
       into relevant data, i.e. meaningful for a learning algorithm? how
       can you make the difference between useful and useless data in a
       huge dataset? what are the different types of machine learning
       algorithms? (with examples) which one should you choose to build
       your model? what is the right method to assess the performance of
       your ml model? which indicator should you use? what are the reasons
       why your ml model is not performing well? what are the most common
       techniques to improve its performance? how can you import your raw
       data? what are the most common data cleaning methods? 11 19 93 42
       79 start from page   
   11. [101]11. 2 iii 4ivii data preparation feature engineering data
       modeling performance measure i v performance improvement
   12. [102]12. prepare datai preparing your data can be done in 3 steps
       query your data clean your data format your data 1 2 3 deal with
       missing valuesa remove outliersb
   13. [103]13. prepare dataiquery your data1 you can query your data
       using if you have a database to connect to: if you have a csv (e.g.
       downloaded from the internet): if you have lots of data, you will
       find useful to start working on a subset of your dataset. you will
       be able to iterate quickly since computations will be fast if there
       is not too much data. importing the whole file slicing to work on a
       subset
   14. [104]14. prepare dataiquery your data1 this will give you a
       dataframe with your raw data a dataframe is a common data format
       that will enable you to efficiently work on large volumes of data.
       x =
   15. [105]15. prepare dataiclean data deal with missing valuesa 2
       compute ratio rm = number of missing values total number of values
       if rm is high, you might want to remove the whole column if rm is
       reasonably low, to avoid losing data, you can impute the mean, the
       median or the most frequent value in place of the missing values.
       some of your columns will certainly contain missing values, often
       as    nan   . you will not be able to use algorithms with nan values.
   16. [106]16. prepare dataiclean data remove outliersb 2 some of your
       columns will certainly contain outliers, i.e. a value that lies at
       an abnormal distance from other values in your sample. outliers are
       likely to mislead your future model, so you will have to remove
       them. 1 remove them arbitrarily 2 use robust methods several
       methods, such as robust regressions, rely on robust estimators
       (e.g. the median) to remove outliers from an analysis. for each of
       your column, you might guess arbitrarily thresholds above which
       your data don   t make sense. you have to be careful that you are not
       removing any insight ! examples of robust methods in sklearn
   17. [107]17. prepare dataiformat data the most common transformation is
       the encoding of categorical variables. 3 your will have to modify
       your data so that they fit constraints of algorithms. we replace
       strings by numbers. because there is no hierarchical relationship
       between the 0 and 1, sex is a    dummy variable   . we create a new
       column for each of the values in the document. you can use the
       patsy module for this:
   18. [108]18. prepare datai now that i have a clean dataset, i can start
       feature engineering.
   19. [109]19. 2 iii 4ivii data preparation feature engineering data
       modeling performance measure i v performance improvement
   20. [110]20. feature engineeringiiwhat is a feature? example: predict
       the price of an apartment    a feature is an individual measurable
       property of a phenomenon being observed.    the number of features
       you will be using is called the dimension. features (individual
       measurable properties) label (phenomenon observed) location:paris
       6th size:33 sqm floor:5th elevator:no # rooms: 2 400k       
   21. [111]21. feature engineeringiiwhat is feature engineering? feature
       engineering is the process of transforming raw data into relevant
       features, i.e. that are: - informative (it provides useful data for
       your model to correctly predict the label) - discriminative (it
       will help your model make differences among your training examples)
       - non-redundant (it does not say the same thing than another
       feature), resulting in improved model performance on unseen data.
   22. [112]22. feature engineeringiiwhat is feature engineering? example:
       predict the price of an apartment in paris informative?
       discriminative? non-redundant? noyes is the feature     size in
       square meters size in square meters size in square meters the name
       of your neighbor (unless it   s brad pitt) simple or double glazed
       window? (99% is double glazing in paris) size in square inches
       obviously a good feature to predict the price of an apartment !
   23. [113]23. feature engineeringii x = x1,1 x2,1 x3,1 xm-1,1 xm,1    
       x1,2 x2,2 x3,2 xm-1,2 xm,2     x1,n x2,n x3,n xm-1,n xm,n         y = y1
       ym y2 y3     ym-1 m training examples n features feature #1 feature
       #2 feature #n value of feature #n for training example #1 labels
       what is feature engineering? after feature engineering, your
       dataset will be a big matrix of numerical values. remember that
       behind    data    there are two very different notions, training
       examples and features. 1 1 1 1 1    
   24. [114]24. feature engineeringii feature engineering usually
       includes, successively: feature construction what is feature
       engineering ? 1 dimension reduction 3 feature transformation 2
       feature selection feature extraction a b
   25. [115]25. feature engineeringiifeature construction example:
       decompose a date-time same raw data different problems different
       features 2017-01-03 15:00:00 2017-01-03 15:00:00 predict how much
       hungry someone is predict the likelihood of a burglary    night   : 0
          hours elapsed since last meal   : 2 (numerical value for    false   ) 1
       feature construction means turning raw data into informative
       features that best represent the underlying problem and that the
       algorithm can understand.
   26. [116]26. feature engineeringiifeature construction feature
       construction is where you will need all the domain expertise and is
       key to the performance of your model! 1
   27. [117]27. feature engineeringiifeature transformation examples of
       transformations: name scaling transformation objectives 2 log
       reduce heteroscedasticity (learn more), which can be an issue for
       some algorithms feature transformation is the process of
       transforming a feature into a new one with a specific function. the
       most important. many algorithms need feature scaling for faster
       computations and relevant results, e.g. in dimension reductionxnew
       = xold -       xnew = log(xold)
   28. [118]28. feature engineeringiidimension reduction3 dimension
       reduction is the process of reducing the number of features used to
       build the model, with the goal of keeping only informative,
       discriminative non-redundant features. the main benefits are:    
       faster computations     less storage space required     increased model
       performance     data visualization (when reduced to 2d or 3d)
   29. [119]29. feature engineeringiidimension reduction feature
       selectiona feature selection is the process of selecting the most
       relevant features among your existing features. to keep    relevant   
       features only, we will remove features that are: - non informative
       - non discriminative - redundant i ii iii 3
   30. [120]30. feature engineeringiidimension reduction feature
       selectiona remove non informative features what is the impact on
       model performance? test model example: predict the price of an
       apartment location: paris 6th size: 33 sqm floor: 5th elevator: no
           principle: we eliminate a single feature in turn, run the model
       each time and note impact on the performance of the model. the
       lower the impact, the less informative the feature is, and
       vice-versa. method: recursive feature elimination (rfe) (among
       others) i 3
   31. [121]31. feature engineeringiidimension reduction feature
       selectiona remove non discriminative features ii principle: we
       remove any feature whose values are close across all the different
       training examples (i.e. that have low variance) method: variance
       threshold filter filter ex: predict the price of houses that are
       all white a feature that always says the same thing won   t help your
       model! 3
   32. [122]32. feature engineeringiidimension reduction feature
       selectiona remove redundant features iii principle: we remove
       features that are similar or highly correlated with other
       feature(s). method: high correlation filter ex: same size in square
       meters and square inches your model doesn   t need the same
       information twice! 3 you can detect correlated features computing
       the pearson product-moment correlation coefficients matrix.
   33. [123]33. feature engineeringiidimension reduction feature
       selectiona identifying the most relevant features will help you get
       a better general understanding of the drivers of the phenomenon you
       are trying to predict. learn more on feature selection 3
   34. [124]34. feature engineeringiidimension reduction feature
       extractionb feature extraction starts from an initial set of
       measured data and automatically builds derived features that are
       more relevant. automated dimension reduction that is efficient and
       easy to implement. the new features given by the algorithms are
       difficult to interpret, unlike feature selection. 3
   35. [125]35. feature engineeringiidimension reduction the most common
       algorithm for feature extraction is principal component analysis
       (pca). feature extractionb pca makes an orthogonal projection on a
       linear space to determine new features, called principal
       components, that are a linear combination of the old ones. example
       of reduction of 2 features into a single one 3 new feature old
       feature x1 old feature x2 x    = ax1 + bx2 min. value of x    max.
       value of x    linear space
   36. [126]36. feature engineeringii pca new feature old feature x1 old
       feature x2 x    = ax1 + bx2 dimension reduction maximized variance
       feature extractionb the principal component (pc) is built along an
       axis so that it is, as much as possible: - discriminative (its
       variance is maximized) - informative (the error to original values
       is minimized) minimum error error is not minimized variance is not
       maximized example of projection that is not a pca 3 old feature x1
       old feature x2
   37. [127]37. feature engineeringiidimension reduction feature
       extractionb the principal components (pc) are built along axes so
       that they are, as much as possible: - independent (i.e.
       non-redundant) from other pcs 3 example of reduction of 3 features
       into 2 pcs correlated features x1x2 x3 x   1 x   2 x      1 x      2
       independent features (principal components)
   38. [128]38. feature engineeringiidimension reduction feature
       extractionb in a nutshell: principal component analysis (pca) given
       a desired number of final features, pca will create these features
       called principal components minimizing the loss of information from
       initial data and thus maximizing their relevance (i.e. informative,
       discriminative, non-redundant). 3 learn more about pca 2d 1d3d or
       pca desired number of final features
   39. [129]39. feature engineeringiidimension reduction feature
       extractionb a famous implementation of pca is in face recognition.
       3 pca
   40. [130]40. feature engineeringiiin a nutshell feature construction1 2
       3 turn raw data into relevant features that best represent the
       underlying problem. dimension reduction eliminate less relevant
       features either by selecting them or by extracting new ones
       automatically. feature transformation transform features so that
       they fit some algorithms constraints. methods benefits feature
       engineering is the process of transforming raw data into i)
       informative ii) discriminative iii) non-redundant features.    
       faster computations     less storage space required     increased model
       performance     data visualization feature engineering is a very
       important part (if not the most) to build a performing machine
       learning model.
   41. [131]41. your machine can now start learning. let   s see how.
   42. [132]42. 2 iii 4ivii data preparation feature engineering data
       modeling performance measure i v performance improvement
   43. [133]43.    the goal is to turn data into information, and
       information into insight.    carly fiorina, former ceo of
       hewlett-packard
   44. [134]44. data modelingiii you are going to train a model on your
       data using a learning algorithm. remember: + = data algorithms
       model
   45. [135]45. data modelingiii supervised vs. unsupervised learning 1
       regression with id75a classification with random
       forestsc id91 with id116d parametric vs. nonparametric
       algorithms 2 what are the best algorithms? 3 cost function and
       id119b
   46. [136]46. data modelingiii supervised unsupervisedvs.
   47. [137]47. data modelingiiisupervised vs. unsupervised learning
       supervised learning unsupervised learning when the training set
       contains labels (i.e. outputs/target) size (m  ) # rooms location
       floor elevator price (k   ) 62 3 paris 3 yes 500 92 4 lyon 4 no 400
       43 2 lille 5 yes 200 features label example: predict the price of
       an apartment when the training set contains no label, only features
       example: define client segments within a customer base name gender
       age location married john m 46 new-york yes sarah f 42 san
       francisco no michael m 18 los angeles yes danielle f 54 atlanta yes
       features label 1
   48. [138]48. data modelingiii regression classification when the label
       to predict is a continuous value example: predict the price of an
       apartment when the label to predict is a discrete value example:
       predict how many stars i am going to rate a movie on netflix
       (0,1,2,3,4,5) supervised learning algorithms are used to build two
       different kind of models. supervised vs. unsupervised learning1 a c
   49. [139]49. data modelingiiisupervised vs. unsupervised learning1
       regression with id75a the output of a linear
       regression on some data will look like this: how is this a machine
       learning model?
   50. [140]50. data modelingiiisupervised vs. unsupervised learning1
       feature x: # of rooms continuous label y: price (m   ) trained model
       y = 0.2x + 0.4 training examples (training dataset) input: unknown
       apartment with 9 rooms x output: predicted price = 2.4 m    example:
       predict the price of an apartment with 1 feature regression with
       id75a
   51. [141]51. data modelingiiisupervised vs. unsupervised learning1
       regression with id75a using a id75
       assumes there is a linear relationship between your features x and
       the labels y. =yi for all i = 1,    , m:   0 +   1xi,1 +     +   nxi,n +
         i which gives, in matrix form: =y x   +       =    0   m    1     where   
       =    0    m    1     and x, y as defined in slide 23 variable
       representing random error (noise) in the data, assumed to follow a
       standard normal distribution. 0
   52. [142]52. data modelingiiisupervised vs. unsupervised learning1 the
       basic id75 method is called ordinary least squares and
       will try to minimize the following function, called    cost function   
       or    id168   , representing the difference between your
       predictions and the true labels. ||y x  ||2 2- =   (yi - xi  )2j(  ) =
       i = 0 m all learning algorithms are about minimizing a certain cost
       function. (where xi is the feature vector of the i-th training
       example, and yi the corresponding label) cost function and gradient
       descentb
   53. [143]53. data modelingiiisupervised vs. unsupervised learning1 cost
       functions are often minimized using an algorithm called gradient
       descent. id119 is an iterative optimization algorithm
       that will look step by step for    values where the gradient (i.e.
       the derivative) equals to 0, thus finding a local minimum of the
       cost function.   0   1 j(  0,  1) 75 50 5 0 -5 -15 -15 -10 0 5   0   1 x
       x x x x x minimum the length of the steps at which the gradient is
       descending is a parameter called    learning rate    each point on the
       green line is a unique value j(  0,  1) for multiple couples of
       (  0,  1) values cost function of a id75 with 2 params
       (  0 ,   1) only 3d representation 2d representation cost function
       and id119b
   54. [144]54. data modelingiiisupervised vs. unsupervised learning1 now,
       let   s discover classification with id79s! you
   55. [145]55. data modelingiiisupervised vs. unsupervised learning1
       classification with id79sc what is this fruit? ? ?? ? ? ?
          root node       child nodes    the depth of a decision tree is the
       length of the longest path from a root to a leaf (here, depth = 3)
          leaf node    (or    leaves   ) how is the splitting attribute is
       selected at each node? taste the selected attribute is the one
       maximizing the purity (i.e. the % of a unique class) in each output
       sub- groups after the split. for example,    shape    as root node
       would be bad as most fruits are rather round. making the decision
       can rely on two different indicators: -    gini impurity   , to
       minimize or -    information gain   , to maximize answer    splitting
       attribute    let   s talk first about decision tree algorithms nb:
          attribute    =    feature   
   56. [146]56. data modelingiiisupervised vs. unsupervised learning1
       classification with id79sc what are the problems with
       decision tree algorithms? imagine you add in your dataset some
       green lemons and not yet ripe bananas and tomatoes. now we have
       many green fruits to classify!    color    is not the attribute with
       the most information gain anymore, so it will not be the splitting
       attribute in the root node, the structure of the tree is going to
       change drastically. id90 are very sensitive to changes in
       training examples. 1 if there is a non-informative features that
       happens to provide good information gain, coincidentally or because
       it is correlated with an informative feature, id90 will
       wrongly use it as a splitting attribute. id90 are very
       sensitive to changes in the features. 2 in a nutshell, decision
       trees are very sensitive to changes in the data and won   t
       generalize well. they are considered as weak learners.
   57. [147]57. data modelingiiisupervised vs. unsupervised learning1
       classification with id79sc general idea id79s
       algorithm is using randomness at 2 levels, that is i) in data
       selection and ii) in attribute selection, and relies on the law of
       large numbers to discard error in the data. let   s see how it works!
   58. [148]58. data modelingiiisupervised vs. unsupervised learning1
       classification with id79sc take different random subsets
       of your data (method known as id64) 1 random subset #1
       random subset #2 random subset #3 random subset #n    2 what is this
       fruit? tree #1 tree #2 tree #3 tree #5trees #... tree #1 votes tree
       #n votes trees #... vote tree #3 votes tree #2 votes errors due to
       a relatively high % of misleading selections in the random data and
       attribute subsets used to build each decision tree we aggregate the
       votes answer: train the model run the model build different
       id90 with each of them when building the trees, splitting
       attributes are chosen among a random subset of features, just like
       for the data
   59. [149]59. data modelingiiisupervised vs. unsupervised learning
       classification with id79sc 1 our id79s model is
       99.33214% sure magritte was wrong!
   60. [150]60. data modelingiiisupervised vs. unsupervised learning
       classification with id79sc 1 note that there decision
       trees and id79s can also solve regression problems.
       example of the evolution of users    interest in an app over time
       time > t1? interest = 7 interest = 3 x x x x x x x x x x users   
       interest timet1 big marketing campaign regression tree model 0 10 7
       3 yes no
   61. [151]61. data modelingiii id91 (= unlabelled classification)
       when we cluster examples with no label one cluster name gender age
       location married john m 46 new-york yes sarah f 42 san francisco no
       michael m 18 los angeles yes danielle f 54 atlanta yes supervised
       vs. unsupervised learning1 unsupervised learning algorithms are
       used for id91 methods.
   62. [152]62. data modelingiiisupervised vs. unsupervised learning1
       id91 with id116d step all data points are unlabeled. we
       randomly initiate two points called    cluster centroids   . 1 step
       data points are labeled according to which centroid they are the
       closest from. 2 step each centroid is moved to the center of the
       data points that were labeled in step 2. 3
   63. [153]63. data modelingiiisupervised vs. unsupervised learning1
       id91 with id116d steps and are repeated   .2 3    until
       convergence. (i.e. no data is relabeled after centroids have been
       recentered) step 2 step 3 end cluster #2 cluster #1
   64. [154]64. data modelingiiisupervised vs. unsupervised learning1
       id91 with id116d examples of applications of id91
       market segmentation social network analysis astronomical data
       analysis
   65. [155]65. data modelingiii these algorithms can easily be
       implemented in supervised vs. unsupervised learning1 linear
       regression id79s id116 desired number of clusters
   66. [156]66. data modelingiii parametric nonparametricvs.
   67. [157]67. data modelingiii because it assumes a pre-defined form for
       the function modelling the data, with a set of parameters of fixed
       size, the id75 is said to be a parametric algorithm.
       in a id75, the vector contains the   =    0   m    1    
       parametric vs. nonparametric algorithms2 parameters of the model
       that are fitted to your data by the id75 algorithm.
   68. [158]68. data modelingiii algorithms that do not make strong
       assumptions about the form of the mapping function are
       nonparametric algorithms. by not making assumptions, they are free
       to learn any functional form (with an unknown number of parameters)
       from the training data. a decision tree is, for instance, a
       nonparametric algorithm. parametric vs. nonparametric algorithms2
   69. [159]69. data modelingiiiparametric vs. nonparametric algorithms2
       parametric algorithms nonparametric algorithms pros cons simpler
       easier to understand and to interpret faster very fast to fit your
       data less data require    few    data to yield good perf. limited
       complexity because of the specified form, parametric algorithms are
       more suited for    simple    problems where you can guess the structure
       in the data slower computations will be significantly longer more
       data require large amount of data to learn overfitting we   ll see in
       a bit what this is, but it affects model performance flexibility
       can fit a large number of functional forms, which doesn   t need to
       be assumed performance performance will likely be higher than
       parametric algorithms as soon as data structures get complex
   70. [160]70. data modelingiii what are the best algorithms?
   71. [161]71. data modelingiii we   ll let the cat out of the bag now.
       there is no such thing as    best algorithms   . which is why choosing
       the right algorithm is one tricky part in machine learning. what
       are the best algorithms?3
   72. [162]72. data modelingiiiwhat are the best algorithms?3 input data
       nearest neighbors linear id166 rbf id166 gaussian process decision tree
       id79 neural network adaboost na  ve bayes qda look at the
       different models obtained from classification algorithms trained on
       the same data. (color shades represent the    decision function   
       guessed by the algorithm for each class) source
   73. [163]73. data modelingiii moreover, every algorithm has parameters
       called hyperparameters. they have default values in , but how your
       model performs will also depend on your ability to fine-tune them.
       what are the best algorithms?3
   74. [164]74. data modelingiiiwhat are the best algorithms? examples of
       hyperparameters, as implemented in 3 hyperparameters are parameters
       of the algorithm, they are not to be confused with the parameters
       of the model. id75    fit_intercept    whether to include
       or not the term    0 in the functional form to fit hyperparameters
       model parameters id79s       n_estimators    number of trees in
       the forest    criterion    indicator to use to determine the splitting
       attribute at each node when building the trees in the forest
       (   gini    or    information gain   ) undefined (nonparametric model)
       undefined (nonparametric model) id116    init    initialization
       method for the centroids
   75. [165]75. data modelingiii they will only perform differently
       because of the specificities of your data. these algorithms and the
       possible values for their hyperparameters are all equivalent in
       absolute. what are the best algorithms?3
   76. [166]76. data modelingiii    when averaged across all possible
       situations, every algorithm performs equally well.    this is what
       the    no free lunch    theorem states: wolpert and macready, 1997 what
       are the best algorithms?3 learn more with an illustration of no
       free lunch theorem on id116 algorithm
   77. [167]77. data modelingiii building a performing ml model is all
       about: - making the right assumptions about your data - choosing
       the right learning algorithm for these assumptions in a nutshell
   78. [168]78. data modelingiii but how do i know if my model is
       performing?
   79. [169]79. 2 iii 4ivii data preparation feature engineering data
       modeling performance measure i v performance improvement
   80. [170]80. performance measureiv use your model to predict the labels
       in your own dataset assessing your model performance is a 2-step
       process use some indicator to compare the predicted values with the
       real values 1 2
   81. [171]81. performance measureiv predicting your dataset labels 1 a
       cross-validationb choosing the right performance indicator training
       set and test set a classificationb regression 2
   82. [172]82. performance measureiv you never train your model and test
       its performance on the same dataset. it   s a bit like sitting for an
       exam where you already know the answer. that would deeply bias the
       performance measure. predicting your dataset labels1 training set
       and test seta
   83. [173]83. performance measureivpredicting your dataset labels
       dataset training set test set commonly c. 80% of dataset a test set
       enables to test our model on unseen data. commonly c. 20% of
       dataset 1 2 model train model run model performance measure 1
       training set and test seta therefore, we split our dataset in 2
       parts:
   84. [174]84. performance measureivpredicting your dataset labels
       dataset training set test set commonly c. 80% of dataset commonly
       c. 20% of dataset 1 2 model train model run model performance
       measure what if i test different algorithms and hyperparameter
       values here       until the performance is good? such a method is often
       used to quickly test different algorithms at the beginning or to
       fine-tune hyperparameters. however, the performance measure will be
       biased again, because it highly depends on the data in the test
       set, which is why you will have to use cross-validation. 1 training
       set and test seta
   85. [175]85. performance measureivpredicting your dataset labels
       dataset training set test set commonly c. 60% of dataset commonly
       c. 20% of dataset 1 3 model train model run final model unbiased
       performance measure 1 cross-validationb 2 cross-validation set
       commonly c. 20% of dataset biased performance measure run tested
       model optimize hyperparameters problem: you will lose c. 20% of the
       data to train your algorithm. if you don   t have lots of data, you
       might prefer kfold cross-validation
   86. [176]86. performance measureivpredicting your dataset labels
       cross-validationb 1 kfold cross-validation consists in repeating
       the training / cv random splitting process k times to come up with
       an average performance measure. split #1 split #2 split #ksplits
       #...     training dataset cv dataset training dataset cv dataset
       training dataset cv dataset training dataset cv dataset training
       dataset cv dataset training dataset cv dataset size: m training
       examples size (usually): m/k training examples biased performance
       measure biased performance measure biased performance measures
       biased performance measure average unbiased performance measure
   87. [177]87. performance measureivpredicting your dataset labels
       cross-validationb 1 there are several ways to use kfold
       cross-validation in 1 simple performance measure with k=10 2
       fine-tuning hyperparameters with gridsearchcv 3 cv is internally
       implemented in some algorithms and computations are optimized, e.g.
       learn more kfold cross- validation is quickly very computationally
       expensive.
   88. [178]88. performance measureiv now that i know the method to
       rigorously measure the performance, which indicator will i use?
   89. [179]89. performance measureivchoosing the right performance
       indicator2 regressiona examples of two commonly used indicators
       mean squared error coefficient of determination (r2) formula pros
       cons easy to understand relative value you need the scale of your
       labels to interpret mse absolute value very roughly, a model with
       r2 > 0.6 is getting good (1 being the best), r2 < 0.6 is not so
       good is the true label for the i-th example in the test set is the
       predicted label for the i-th example in the test set is the average
       of the label values in the test set difficult to explain
   90. [180]90. performance measureivchoosing the right performance
       indicator2 classificationb number of correctly predicted labels
       total number of labels in test set accuracy = confusion matrix
       positive negative negativepositive true positive (tp) false
       negative (fn) true negative (tn) false positive (fp) predicted
       labels actuallabels recall= tp tp + fn recall is a % expressing the
       capacity of your model to recall positive values. precision= tp tp
       + fp precision is a % expressing the precision with which the
       positive values where recalled by your model. accuracy is very easy
       to understand but often too simple to correctly interpret the
       performance of your model
   91. [181]91. performance measureivchoosing the right performance
       indicator2 classificationb roc curve recall= 100% precision = % of
       positive examples in your datasetprecision > recall recall >
       precision if you are running a marketing campaign but don   t have
       too much money, you might want to focus on a smaller target (low
       recall) where your id203 to convert is high (high precision)
       if you are running a marketing campaign and have lots of budget,
       you will rather focus on a large target where your id203 to
       convert is lower (low precision), but on a greater number of people
       (high recall) false positive rate truepositiverate roc curve there
       is always a trade-off in function of whether you want to prefer
       precision or recall. you can visualize how the tp and fp rates
       evolve according to different discrimination thresholds of your
       model with the roc curve.
   92. [182]92. create a dirty but complete model as quick as possible to
       iterate on it afterwards. this is the right way to go!
   93. [183]93. 2 iii 4ivii data preparation feature engineering data
       modeling performance measure i v performance improvement
   94. [184]94. performance improvementv reasons for underperformance 1 a
       overfittingb solutions to increase performance underfitting 2
   95. [185]95. performance improvementv what are the reasons why your
       model is not performing well?
   96. [186]96. performance improvementv it should reproduce the
       underlying data structure but leave aside random noise in the data.
       a performing model will fit the data in a way that it generalizes
       well to new inputs. reasons for underperformance1
   97. [187]97. performance improvementv there are two reasons why a model
       would not generalize and thus not perform correctly: - underfitting
       - overfitting reasons for underperformance1 a b
   98. [188]98. performance improvementvreasons for underperformance
       underfitting happens when your model is too simple to reproduce the
       underlying data structure. 1 underfittinga when underfitting, a
       model is said to have high bias.
   99. [189]99. performance improvementvreasons for underperformance
       overfitting happens when your model is too complex to reproduce the
       underlying data structure. it captures the random noise in the
       data, whereas it shouldn   t. 1 overfittingb when overfitting, a
       model is said to have high variance.
   100. [190]100. performance improvementvin a nutshell performance on   
       training set test set bad bad very good bad good good overfitting
       can easily be spotted with this performance difference on training
       and test sets ! good 1 you want to select a model at the sweet spot
       between underfitting and overfitting. this is not easy!
   101. [191]101. performance improvementv ok, i got the point    so, how do
       i address these overfitting and underfitting issues?
   102. [192]102. performance improvementv    ensemble methods    a c d e
       issue of the potential solutions on     more features more complex
       algorithms boosting less features more training examples simpler
       algorithms id173 id112 bdata algorithms model f
       solutions to increase performance2 a b
   103. [193]103. performance improvementvsolutions to increase
       performance let   s discuss the mentioned solutions one by one.
       (final stretch, i promise !) 2
   104. [194]104. performance improvementv the study below shows how
       different algorithms perform similarly for a given problem as the
       amount of training examples increases. from    scaling to very very
       large corpora for natural language disambiguation    by microsoft
       researchers banko and brill, 2001 solutions to increase
       performance2 a more training examples
   105. [195]105. performance improvementvsolutions to increase
       performance the more training examples there are, the more complex
       it is for an algorithm to fit the noise in the data. therefore, the
       fitted model will be less sensitive to noise and will better
       generalize. 2 a more training examples a few samples a bit more
       samples a lot of samples
   106. [196]106. performance improvementv some features might contain
       more noise than informative data for your model. this is especially
       the case when the features are non-informative or correlated with
       other features. remove them and your model will not take this noise
       into account anymore. solutions to increase performance2 b less
       featuresa
   107. [197]107. performance improvementvsolutions to increase
       performance2 b more featuresb we are talking about science, not
       divination! if your model is underfitting, it might be because you
       did not give it enough informative features.
   108. [198]108. performance improvementvsolutions to increase
       performance2 b in a nutshell: more data! however, you must know how
       to make good use of these data with good feature engineering. data
       is key because it can help you both: - reduce variance
       (overfitting) with more training examples - reduce bias
       (underfitting) with more features more details
   109. [199]109. performance improvementv "we don   t have better
       algorithms. we just have more data." 's research director peter
       norvig, 2009
   110. [200]110. performance improvementv if you   re convinced you need
       more data, turk might help you. check it out ! solutions to
       increase performance2 b in a nutshell: more data!
   111. [201]111. performance improvementvsolutions to increase
       performance error below is how a model error will theoretically
       evolve as its complexity increases (i.e. more complex algorithms,
       more features). 2 c algorithms complexity underfitting area
       overfitting area in addition/place of more features, you might need
       more complex algorithms (nonlinear, nonparametric) to model your
       data structure. in addition/place of less features, you might need
       simpler algorithms. certain algorithms, such as deep neural
       networks, are very powerful and easily tend to overfit if you don   t
       have millions of rows of data. right spot! test error is at its
       minimum
   112. [202]112. performance improvementvsolutions to increase
       performance2 d id173 id173 aims at reducing
       overfitting by adding a complexity term to the cost function. let   s
       see how it works for id75. the id168 will be:
         (yi - xi  )2 +    i = 0 m   j = 0 n   j      complexity    /    penalty    /
          id173    parameter because of the id173 term, the
       algorithm will find smaller    values when minimizing the cost
       function, resulting in lower variance.    = id173 term j(  )
       = minimum of cost function when    = 0 (no id173) minimum
       of cost function when    > 0. this is the closest point from the
       minimum, but within the id173 limits. limits set by the
       id173 term. the greater   , the smaller more restricted
       these limits will be. explanation illustration
   113. [203]113. performance improvementvsolutions to increase
       performance2 d id173 this regularized id75 is
       called a ridge regression, and can be found in there even is an
       implementation with a fast built-in cross-validation that enables
       you to quickly optimize the    parameter. if    is too large, your
       model will underfit. it   s a constant trade-off!
   114. [204]114. performance improvementvsolutions to increase
       performance2 e id112 id112 = bootstrap + aggregating take n
       different random subsets of the training dataset 1 random subset #1
       random subset #2 random subset #3 random subset #n    2 training
       dataset train your model (weak learner) on each of these subsets
       predict a label with each of the obtained model aggregate the votes
       to decide the winning prediction (strong learner) id79 is
       simply id112 applied on decision tree classifiers (week
       learners). did you notice ? you can also apply id112 on your set
       of features. note
   115. [205]115. performance improvementvsolutions to increase
       performance2 f boosting id112 boosting aggregating equally the
       results of weak learners built independently on random samples to
       create a strong learner. combining differently the results of weak
       learners built sequentially on the whole dataset to create a strong
       learner. strong learner weak learner weight, = 1 for all weak
       learners (usually id90) built independently strong
       learner different weight for each weak learner dj weak learners
       (usually id90) built sequentially
   116. [206]116. performance improvementvsolutions to increase
       performance2 f boosting illustration of gradient boosted trees
       (gbt) algorithm. gbt = boosting + id119 + trees boosting
           we start with constant regression tree d0 and model error term at
       each iterationa initialization: ytrue = d0(x) +   0 with d0(x) = 0
       and   0 the error term (   residual   )   0 =   1d1(x) +   1   1 =   2d2(x) +
         2       t-1 =   tdt(x) +   t ypred =   jdj(x)   boosting: we model the
       residual with a regression tree dj (weak learner), slowly
       decreasing total error amount iteration after iteration j = 1 t but
       how do we know which regression tree dj to add at each iteration?
       and   t is the remaining error between our prediction ypred and the
       true label ytruewe stop when   t ~   t-1
   117. [207]117. performance improvementvsolutions to increase
       performance2 f boosting id119     find optimal regression
       tree dj b at the j-th iteration, the steps are as follow: because
       boosted trees do id119 in a space of functions, they are
       very good when the structure of the data is unknown.    (dj(xi)   
         j,i)2 + i = 0 m id173 term for i = 0,    , m, compute
       residual   j,i = ytrue,i -1.      kdk(xi) k = 0 j-1 2. 3. with
       id119, find dj minimizing cost function find optimal   j
       to minimize total residual   j+1 = ytrue -      kdk(xi) k = 0 j nb: dj
       belongs to the space of functions containing all regression trees.
       model obtained from previous iteration j-1
   118. [208]118. performance improvementvsolutions to increase
       performance2 f boosting regression tree     id119 will
       find the optimal regression tree dj x x x x x x x x x x user   s
       interest t t1 c two kinds of parameters to determine: splitting
       positions change at each split 2 1 underfit overfit good
   119. [209]119. performance improvementvsolutions to increase
       performance2 f boosting you can visualize the output of gradient
       boosting trees below: 3d function to modelize gbt output combining
       100 id90 with depth = 3 more visualization
   120. [210]120. performance improvementvsolutions to increase
       performance2 f boosting note that even for classification tasks,
       gradient boosted trees will be using regression trees. the
       algorithm will compute continuous values between 0 and 1 that are
       probabilities of belonging to a certain class.
   121. [211]121. performance improvementvsolutions to increase
       performance2 f boosting tree ensemble methods (i.e.
       id112/boosting used with id90) are very popular
       algorithms in machine learning. they often enable a good
       performance with little effort. learn more on xgboost gradient
       boosted trees the most popular implementation of gradient boosted
       trees is the module it includes id173 that helps the
       algorithm not to overfit, which is the big risk with boosting!
   122. [212]122. you now have all you need to build a performing machine
       learning model!
   123. [213]123. you must assess when the effort is not worth it anymore!
       model performance this is how the performance of your model will
       most likely evolve 20% 80% time 100% conclusion
   124. [214]124. conclusion in 2006, netflix offered a $1m prize for
       anyone who would improve the accuracy of their recommendation
       system by 10%. the 2nd team, which achieved a 8.43% improvement,
       reported more than 2000 hours of work to come up with a combination
       of 107 algorithms! the 10% improvement was only achieved in 2009,
       and the algorithm never went into production    learn more
   125. [215]125. building a machine learning model: summary iii data
       preparation feature engineering data modeling performance
       improvement i ii v performance measure iv you then turn raw data
       into individual measurable properties (features) that will help
       your model complete its task. they must be as informative,
       discriminative and non-redundant as possible. this step is commonly
       acknowledged as the most important part in building a ml model. you
       can now apply either supervised or unsupervised machine learning
       algorithms. their complexity vary but how they correctly model your
       data will solely depend on your assumptions. to assess the
       performance of your model, you will pick a relevant indicator that
       you understand and measure it on unseen test data that you will
       have set aside before training your model. your model can
       underperform for only two reasons: underfitting or overfitting.
       many solutions exist, including two popular techniques:
       id173 (overfitting) and boosting (underfitting.) first,
       you will apply some of the most common data cleaning actions on
       your raw data, including removing outliers and dealing with missing
       values and categorical variables.
   126. [216]126. thank you! good ressources if you wanna talk about
       machine learning: charles.vestur@gmail.com
   127. [217]127. the very famous mooc from andrew ng. an excellent
       introduction to machine learning, in which you will learn different
       algorithms and a bit of the maths behind it. good ressources
       kaggle, a reference for data science, which provides many public
       datasets, organizes competitions in which you can take part or get
       inspired by the code published by the winners! quora: a lot of
       people put in the effort to clearly explain a lot of concepts in
       machine learning. you can quickly spot the best answers with the
       upvotes. crossvalidated: the equivalent of stackoverflow for data
       science. just like in quora, you will find very good quality
       answers on numerous topics. main ressources: blogs: newsletter:
       analytics vidhya: many articles on a ml topic with explanations and
       code samples machine learning mastery: similar to analytics vidhya,
       you will find nice articles on this blog. data elixir: you don   t
       need a thousand newsletter, this one is a very good one with both
       technical and high-level articles.

          [218]recommended

     * time management tips weekly
       time management tips weekly
       online course - linkedin learning
     * grant writing for education
       grant writing for education
       online course - linkedin learning
     * powerpoint: designing better slides
       powerpoint: designing better slides
       online course - linkedin learning
     * machine learning by analogy
       machine learning by analogy
       colleen farrelly
     * maintenance and management best practices from support
       maintenance and management best practices from support
       ca | automic software
     * predictive maintenance
       predictive maintenance
       amey kulkarni
     * ml workshop 1: a new architecture for machine learning logistics
       ml workshop 1: a new architecture for machine learning logistics
       mapr technologies
     * using hadoop for big data
       using hadoop for big data
       data science thailand
     * myths of data science
       myths of data science
       data science thailand
     * database maintenance optimization brad mc gehee
       database maintenance optimization brad mc gehee
       pratik joshi

     * [219]english
     * [220]espa  ol
     * [221]portugu  s
     * [222]fran  ais
     * [223]deutsch

     * [224]about
     * [225]dev & api
     * [226]blog
     * [227]terms
     * [228]privacy
     * [229]copyright
     * [230]support

     *
     *
     *
     *
     *

   linkedin corporation    2019

     

share clipboard
     __________________________________________________________________

   [231]  
     * facebook
     * twitter
     * linkedin

   link ____________________

public clipboards featuring this slide
     __________________________________________________________________

   (button)   
   no public clipboards found for this slide

select another clipboard
     __________________________________________________________________

   [232]  

   looks like you   ve clipped this slide to already.
   ____________________

   create a clipboard

you just clipped your first slide!

   clipping is a handy way to collect important slides you want to go back
   to later. now customize the name of a clipboard to store your clips.
     __________________________________________________________________

   name* ____________________
   description ____________________
   visibility
   others can see my clipboard [ ]
   (button) cancel (button) save

   bizographics tracking image

references

   visible links
   1. https://www.slideshare.net/rss/latest
   2. https://www.slideshare.net/opensearch.xml
   3. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
   4. https://es.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
   5. https://fr.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
   6. https://de.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
   7. https://pt.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
   8. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
   9. https://www.slideshare.net/api/oembed/2?format=json&url=http://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  10. https://www.slideshare.net/api/oembed/2?format=xml&url=http://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  11. https://www.slideshare.net/mobile/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  12. android-app://net.slideshare.mobile/slideshare-app/ss/71514689
  13. ios-app://917418728/slideshare-app/ss/71514689
  14. http://www.linkedin.com/legal/user-agreement
  15. http://www.linkedin.com/legal/privacy-policy
  16. http://www.linkedin.com/legal/privacy-policy
  17. http://www.linkedin.com/legal/user-agreement
  18. https://www.slideshare.net/
  19. https://www.slideshare.net/explore
  20. https://www.slideshare.net/login
  21. https://www.slideshare.net/
  22. https://www.slideshare.net/upload
  23. https://www.slideshare.net/login
  24. https://www.slideshare.net/w/signup
  25. https://www.slideshare.net/
  26. https://www.slideshare.net/explore
  27. https://www.linkedin.com/learning/topics/presentations?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  28. https://www.linkedin.com/learning/topics/powerpoint?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  29. https://www.linkedin.com/learning?trk=slideshare_subnav_learning
  30. https://www.linkedin.com/psettings/privacy
  31. https://public.slidesharecdn.com/colleenfarrelly/machine-learning-by-analogy-59094152
  32. https://public.slidesharecdn.com/automic/maintenance-and-management-best-practices-from-support
  33. https://public.slidesharecdn.com/ameykulkarni32/predictive-maintenance-65977381
  34. https://public.slidesharecdn.com/maprtechnologies/ml-workshop-1-a-new-architecture-for-machine-learning-logistics
  35. https://public.slidesharecdn.com/datascienceth/using-hadoop-for-big-data
  36. https://public.slidesharecdn.com/datascienceth/myths-of-data-science
  37. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  38. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  39. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  40. https://www.slideshare.net/charlesvestur?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  41. https://www.slideshare.net/charlesvestur?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  42. https://www.slideshare.net/signup?login_source=slideview.popup.follow&from=addcontact&from_source=https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  43. https://www.slideshare.net/featured/category/data-analytics
  44. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z#comments-panel
  45. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z#likes-panel
  46. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z#stats-panel
  47. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z#notes-panel
  48. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  49. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  50. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  51. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  52. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  53. https://www.slideshare.net/signup?login_source=slideview.popup.comment&from=comments&from_source=https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  54. https://www.slideshare.net/kendricklamost?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  55. https://www.slideshare.net/kendricklamost?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  56. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  57. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  58. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  59. https://www.slideshare.net/kushkul?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  60. https://www.slideshare.net/kushkul?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  61. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  62. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  63. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  64. https://www.slideshare.net/kendricklamost?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  65. https://www.slideshare.net/kendricklamost?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  66. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  67. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  68. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  69. https://www.slideshare.net/kendricklamost?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  70. https://www.slideshare.net/kendricklamost?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  71. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  72. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  73. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  74. https://www.slideshare.net/charlesvestur?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  75. https://www.slideshare.net/charlesvestur?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  76. https://www.slideshare.net/guillaume
  77. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  78. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  79. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  80. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  81. https://www.slideshare.net/tomdhaene1?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  82. https://www.slideshare.net/tomdhaene1?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  83. https://www.slideshare.net/junyu2?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  84. https://www.slideshare.net/junyu2?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  85. https://www.slideshare.net/kushkul?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  86. https://www.slideshare.net/kushkul?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  87. https://www.slideshare.net/yaronsegev4?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  88. https://www.slideshare.net/yaronsegev4?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  89. https://www.slideshare.net/andresrivera177?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  90. https://www.slideshare.net/andresrivera177?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  91. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
  92. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-2-638.jpg?cb=1486044818
  93. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-3-638.jpg?cb=1486044818
  94. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-4-638.jpg?cb=1486044818
  95. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-5-638.jpg?cb=1486044818
  96. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-6-638.jpg?cb=1486044818
  97. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-7-638.jpg?cb=1486044818
  98. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-8-638.jpg?cb=1486044818
  99. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-9-638.jpg?cb=1486044818
 100. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-10-638.jpg?cb=1486044818
 101. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-11-638.jpg?cb=1486044818
 102. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-12-638.jpg?cb=1486044818
 103. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-13-638.jpg?cb=1486044818
 104. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-14-638.jpg?cb=1486044818
 105. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-15-638.jpg?cb=1486044818
 106. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-16-638.jpg?cb=1486044818
 107. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-17-638.jpg?cb=1486044818
 108. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-18-638.jpg?cb=1486044818
 109. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-19-638.jpg?cb=1486044818
 110. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-20-638.jpg?cb=1486044818
 111. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-21-638.jpg?cb=1486044818
 112. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-22-638.jpg?cb=1486044818
 113. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-23-638.jpg?cb=1486044818
 114. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-24-638.jpg?cb=1486044818
 115. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-25-638.jpg?cb=1486044818
 116. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-26-638.jpg?cb=1486044818
 117. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-27-638.jpg?cb=1486044818
 118. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-28-638.jpg?cb=1486044818
 119. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-29-638.jpg?cb=1486044818
 120. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-30-638.jpg?cb=1486044818
 121. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-31-638.jpg?cb=1486044818
 122. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-32-638.jpg?cb=1486044818
 123. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-33-638.jpg?cb=1486044818
 124. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-34-638.jpg?cb=1486044818
 125. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-35-638.jpg?cb=1486044818
 126. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-36-638.jpg?cb=1486044818
 127. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-37-638.jpg?cb=1486044818
 128. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-38-638.jpg?cb=1486044818
 129. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-39-638.jpg?cb=1486044818
 130. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-40-638.jpg?cb=1486044818
 131. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-41-638.jpg?cb=1486044818
 132. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-42-638.jpg?cb=1486044818
 133. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-43-638.jpg?cb=1486044818
 134. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-44-638.jpg?cb=1486044818
 135. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-45-638.jpg?cb=1486044818
 136. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-46-638.jpg?cb=1486044818
 137. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-47-638.jpg?cb=1486044818
 138. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-48-638.jpg?cb=1486044818
 139. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-49-638.jpg?cb=1486044818
 140. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-50-638.jpg?cb=1486044818
 141. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-51-638.jpg?cb=1486044818
 142. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-52-638.jpg?cb=1486044818
 143. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-53-638.jpg?cb=1486044818
 144. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-54-638.jpg?cb=1486044818
 145. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-55-638.jpg?cb=1486044818
 146. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-56-638.jpg?cb=1486044818
 147. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-57-638.jpg?cb=1486044818
 148. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-58-638.jpg?cb=1486044818
 149. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-59-638.jpg?cb=1486044818
 150. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-60-638.jpg?cb=1486044818
 151. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-61-638.jpg?cb=1486044818
 152. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-62-638.jpg?cb=1486044818
 153. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-63-638.jpg?cb=1486044818
 154. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-64-638.jpg?cb=1486044818
 155. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-65-638.jpg?cb=1486044818
 156. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-66-638.jpg?cb=1486044818
 157. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-67-638.jpg?cb=1486044818
 158. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-68-638.jpg?cb=1486044818
 159. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-69-638.jpg?cb=1486044818
 160. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-70-638.jpg?cb=1486044818
 161. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-71-638.jpg?cb=1486044818
 162. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-72-638.jpg?cb=1486044818
 163. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-73-638.jpg?cb=1486044818
 164. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-74-638.jpg?cb=1486044818
 165. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-75-638.jpg?cb=1486044818
 166. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-76-638.jpg?cb=1486044818
 167. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-77-638.jpg?cb=1486044818
 168. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-78-638.jpg?cb=1486044818
 169. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-79-638.jpg?cb=1486044818
 170. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-80-638.jpg?cb=1486044818
 171. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-81-638.jpg?cb=1486044818
 172. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-82-638.jpg?cb=1486044818
 173. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-83-638.jpg?cb=1486044818
 174. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-84-638.jpg?cb=1486044818
 175. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-85-638.jpg?cb=1486044818
 176. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-86-638.jpg?cb=1486044818
 177. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-87-638.jpg?cb=1486044818
 178. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-88-638.jpg?cb=1486044818
 179. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-89-638.jpg?cb=1486044818
 180. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-90-638.jpg?cb=1486044818
 181. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-91-638.jpg?cb=1486044818
 182. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-92-638.jpg?cb=1486044818
 183. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-93-638.jpg?cb=1486044818
 184. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-94-638.jpg?cb=1486044818
 185. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-95-638.jpg?cb=1486044818
 186. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-96-638.jpg?cb=1486044818
 187. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-97-638.jpg?cb=1486044818
 188. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-98-638.jpg?cb=1486044818
 189. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-99-638.jpg?cb=1486044818
 190. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-100-638.jpg?cb=1486044818
 191. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-101-638.jpg?cb=1486044818
 192. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-102-638.jpg?cb=1486044818
 193. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-103-638.jpg?cb=1486044818
 194. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-104-638.jpg?cb=1486044818
 195. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-105-638.jpg?cb=1486044818
 196. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-106-638.jpg?cb=1486044818
 197. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-107-638.jpg?cb=1486044818
 198. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-108-638.jpg?cb=1486044818
 199. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-109-638.jpg?cb=1486044818
 200. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-110-638.jpg?cb=1486044818
 201. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-111-638.jpg?cb=1486044818
 202. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-112-638.jpg?cb=1486044818
 203. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-113-638.jpg?cb=1486044818
 204. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-114-638.jpg?cb=1486044818
 205. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-115-638.jpg?cb=1486044818
 206. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-116-638.jpg?cb=1486044818
 207. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-117-638.jpg?cb=1486044818
 208. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-118-638.jpg?cb=1486044818
 209. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-119-638.jpg?cb=1486044818
 210. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-120-638.jpg?cb=1486044818
 211. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-121-638.jpg?cb=1486044818
 212. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-122-638.jpg?cb=1486044818
 213. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-123-638.jpg?cb=1486044818
 214. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-124-638.jpg?cb=1486044818
 215. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-125-638.jpg?cb=1486044818
 216. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-126-638.jpg?cb=1486044818
 217. https://image.slidesharecdn.com/buildingaperformingmlmodelfromatoz-170129174308/95/building-a-performing-machine-learning-model-from-a-to-z-127-638.jpg?cb=1486044818
 218. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z#related-tab-content
 219. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
 220. https://es.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
 221. https://pt.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
 222. https://fr.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
 223. https://de.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
 224. https://www.slideshare.net/about
 225. https://www.slideshare.net/developers
 226. http://blog.slideshare.net/
 227. https://www.slideshare.net/terms
 228. https://www.slideshare.net/privacy
 229. http://www.linkedin.com/legal/copyright-policy
 230. https://www.linkedin.com/help/slideshare
 231. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
 232. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z

   hidden links:
 234. https://www.slideshare.net/charlesvestur/building-a-performing-machine-learning-model-from-a-to-z
 235. https://www.slideshare.net/signup?login_source=slideview.clip.like&from=clip&layout=foundation&from_source=
 236. https://www.slideshare.net/login?from_source=%2fcharlesvestur%2fbuilding-a-performing-machine-learning-model-from-a-to-z%3ffrom_action%3dsave&from=download&layout=foundation
 237. https://www.slideshare.net/signup?login_source=slideview.popup.flags&from=flagss&from_source=https%3a%2f%2fwww.slideshare.net%2fcharlesvestur%2fbuilding-a-performing-machine-learning-model-from-a-to-z
 238. https://www.linkedin.com/learning/time-management-tips-weekly?trk=slideshare_sv_learning
 239. https://www.linkedin.com/learning/grant-writing-for-education?trk=slideshare_sv_learning
 240. https://www.linkedin.com/learning/powerpoint-designing-better-slides?trk=slideshare_sv_learning
 241. https://www.slideshare.net/colleenfarrelly/machine-learning-by-analogy-59094152
 242. https://www.slideshare.net/automic/maintenance-and-management-best-practices-from-support
 243. https://www.slideshare.net/ameykulkarni32/predictive-maintenance-65977381
 244. https://www.slideshare.net/maprtechnologies/ml-workshop-1-a-new-architecture-for-machine-learning-logistics
 245. https://www.slideshare.net/datascienceth/using-hadoop-for-big-data
 246. https://www.slideshare.net/datascienceth/myths-of-data-science
 247. https://www.slideshare.net/creatorip/database-maintenance-optimization-brad-mc-gehee
 248. http://www.linkedin.com/company/linkedin
 249. http://www.facebook.com/linkedin
 250. http://twitter.com/slideshare
 251. http://www.google.com/+linkedin
 252. https://www.slideshare.net/rss/latest
