   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

a peek at trends in machine learning

   [9]go to the profile of andrej karpathy
   [10]andrej karpathy (button) blockedunblock (button) followfollowing
   apr 7, 2017

   have you looked at [11]google trends? it   s pretty cool         you enter some
   keywords and see how google searches of that term vary through time. i
   thought         hey, i happen to have this [12]arxiv-sanity database of
   28,303 (arxiv) machine learning papers over the last 5 years, so why
   not do something similar and take a look at how machine learning
   research has evolved over the last 5 years? the results are fairly fun,
   so i thought i   d post.

   (edit: machine learning is a large area. a good chunk of this post is
   about deep learning specifically, which is the subarea i am most
   familiar with.)

the arxiv singularity

   let   s first look at the total number of submitted papers across the
   arxiv-sanity categories (cs.ai,cs.lg,cs.cv,cs.cl,cs.ne,stat.ml), over
   time. we get the following:
   [1*mrs_3i9tcit8ehqydcgb2q.png]

   yes, march of 2017 saw almost 2,000 submissions in these areas. the
   peaks are likely due to conference deadlines (e.g. nips/icml). note
   that this is not directly a statement about the size of the area
   itself, since not everyone submits their paper to arxiv, and the
   fraction of people who do likely changes over time. but the point
   remains         that   s a lot of papers to be aware of, skim, or (gasp) read.

   this total number of papers will serve as the denominator. we can now
   look at what fraction of papers contain certain keywords of interest.

deep learning frameworks

   to warm up let   s look at the deep learning frameworks that are in use.
   to compute this, we record the fraction of papers that mention the
   framework somewhere in the full text (anywhere         including bibliography
   etc). for papers uploaded on march 2017, we get the following:
% of papers      framework       has been around for (months)
------------------------------------------------------------
    9.1          tensorflow       16
    7.1               caffe       37
    4.6              theano       54
    3.3               torch       37
    2.5               keras       19
    1.7          matconvnet       26
    1.2             lasagne       23
    0.5             chainer       16
    0.3               mxnet       17
    0.3                cntk       13
    0.2             pytorch       1
    0.1      deeplearning4j       14

   that is, 10% of all papers submitted in march 2017 mention tensorflow.
   of course, not every paper declares the framework used, but if we
   assume that papers declare the framework with some fixed random
   id203 independent of the framework, then it looks like about 40%
   of the community is currently using tensorflow (or a bit more, if you
   count keras with the tf backend). and here is the plot of how some of
   the more popular frameworks evolved over time:
   [1*8a2nz2sncgt9ufl7rsaywg.png]

   we can see that theano has been around for a while but its growth has
   somewhat stalled. caffe shot up quickly in 2014, but was overtaken by
   the tensorflow singularity in the last few months. torch (and the very
   recent pytorch) are also climbing up, slow and steady. it will be fun
   to watch this develop in the next few months         my own guess is that
   caffe/theano will go on a slow decline and tf growth will become a bit
   slower due to pytorch.

convnet models

   for fun, how about if we look at common convnet models? here, we can
   clearly see a huge spike up for resnets, to the point that they occur
   in 9% of all papers last march:
   [1*dxcjmp5bb8bc353ejvwyxq.png]

   also, who was talking about    inception    before the inceptionnet?
   curious.

optimization algorithms

   in terms of optimization algorithms, it looks like [13]adam is on a
   roll, found in about 23% of papers! the actual fraction of use is hard
   to estimate; it   s likely higher than 23% because some papers don   t
   declare the optimization algorithm, and a good chunk of papers might
   not even be optimizing any neural network at all. it   s then likely
   lower by about 5%, which is the    background activity    of    adam   , likely
   a collision with author names, as the adam optimization algorithm was
   only released on dec 2014.
   [1*jyg34qjcsveweh3wrmon4w.png]

researchers

   i was also curious to plot the mentions of some of the most senior pis
   in deep learning (this gives something similar to citation count, but
   1) it is more robust across population of papers with a    0/1    count,
   and 2) it is normalized by the total size of the pie):
   [1*sgbdg_itok0i0hqwg2p3eq.png]

   a few things to note:    bengio    is mentioned in 35% of all submissions,
   but there are two bengios: samy and yoshua, who add up on this plot. in
   particular, geoff hinton is mentioned in more than 30% of all new
   papers! that seems like a lot.

hot or not keywords

   finally, instead of manually going by categories of keywords, let   s
   actively look at the keywords that are    hot    (or not).

top hot keywords

   there are many ways to define this, but for this experiment i look at
   each unigram or bigram in all the papers and record the ratio of its
   max use last year compared to its max use up to last year. the papers
   that excel at this metric are those that one year ago were niche, but
   this year appear with a much higher relative frequency. the top list
   (slightly edited out some duplicates) comes out as follows:
8.17394726486 resnet
6.76767676768 tensorflow
5.21818181818 gans
5.0098386462 residual networks
4.34787878788 adam
2.95181818182 batch id172
2.61663993305 fcn
2.47812783318 vgg16
2.03636363636 style transfer
1.99958217686 gated
1.99057177616 deep reinforcement
1.98428686543 lstm
1.93700787402 id4
1.90606060606 inception
1.8962962963 siamese
1.88976377953 character level
1.87533998187 region proposal
1.81670721817 distillation
1.81400378481 tree search
1.78578069795 torch
1.77685950413 policy gradient
1.77370153867 encoder decoder
1.74685427385 gru
1.72430399325 id97
1.71884293052 relu activation
1.71459655485 visual question
1.70471560525 image generation

   for example, resnet   s ratio of 8.17 is because until 1 year ago it
   appeared in up to only 1.044% of all submissions (in mar 2016), but
   last last month (mar 2017) it appeared in 8.53% of submissions, so 8.53
   / 1.044 ~= 8.17. so there you have it         the core innovations that
   became all the rage over the last year are 1) resnets, 2) gans, 3)
   adam, 4) batchnorm. use more of these to fit in with your friends. in
   terms of research interests, we see 1) style transfer, 2) deep rl, 3)
   id4 (   id4   ), and perhaps 4) image generation.
   and architecturally, it is hot to use 1) fully convolutional nets
   (fcn), 2) lstms/grus, 3) siamese nets, and 4) encoder decoder nets.
   [1*7cld_ao8jwvlqmervpf04w.png]

top not hot

   how about the reverse? what has seen many fewer submissions over the
   last year than has historically had a higher    mind share   ? here are a
   few:
0.0462375339982 fractal
0.112222705524 learning bayesian
0.123531424661 ibp
0.138351983723 texture analysis
0.152810895084 id110
0.170535340862 differential evolution
0.227932960894 wavelet transform
0.24482875551 dirichlet process

   i   m not sure what    fractal    is referring to, but more generally it
   looks like bayesian nonparametrics are under attack.

conclusion

   now is the time to submit paper on fully convolutional encoder decoder
   batchnorm resnet gan applied to style transfer, optimized with adam.
   hey, that doesn   t even sound too far-fetched.

   :)

     * [14]machine learning
     * [15]artificial intelligence
     * [16]deep learning

   (button)
   (button)
   (button) 2.6k claps
   (button) (button) (button) 13 (button) (button)

     (button) blockedunblock (button) followfollowing
   [17]go to the profile of andrej karpathy

[18]andrej karpathy

   director of ai at tesla. previously research scientist at openai and
   phd student at stanford. i like to train deep neural nets on large
   datasets.

     * (button)
       (button) 2.6k
     * (button)
     *
     *

   [19]go to the profile of andrej karpathy
   never miss a story from andrej karpathy, when you sign up for medium.
   [20]learn more
   never miss a story from andrej karpathy
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ab8a1085a106
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@karpathy/a-peek-at-trends-in-machine-learning-ab8a1085a106&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@karpathy/a-peek-at-trends-in-machine-learning-ab8a1085a106&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@karpathy?source=post_header_lockup
  10. https://medium.com/@karpathy
  11. https://trends.google.com/trends/?cat=
  12. http://arxiv-sanity.com/
  13. https://arxiv.org/abs/1412.6980
  14. https://medium.com/tag/machine-learning?source=post
  15. https://medium.com/tag/artificial-intelligence?source=post
  16. https://medium.com/tag/deep-learning?source=post
  17. https://medium.com/@karpathy?source=footer_card
  18. https://medium.com/@karpathy
  19. https://medium.com/@karpathy
  20. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  22. https://medium.com/p/ab8a1085a106/share/twitter
  23. https://medium.com/p/ab8a1085a106/share/facebook
  24. https://medium.com/p/ab8a1085a106/share/twitter
  25. https://medium.com/p/ab8a1085a106/share/facebook
