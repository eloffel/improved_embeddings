   #[1]source{d} blog

   [2]source{d} source{d} (button)
     * [3]product
       [4]source{d} engine [5]source{d} lookout
     * [6]docs
       [7]source{d} engine
     * [8]open source
     * [9]community
       [10]resources [11]talk to us [12]events
     * [13]company
       [14]about [15]careers [16]our offices [17]team [18]contact us
     * [19]request a demo
     * [20]install engine

[21]source{d} blog

     * [22]source{d} engine (16)
     * [23]open source (13)
     * [24]events (11)
     * [25]community (8)
     * [26]company (7)
     * [27]research papers (7)
     * [28]source{d} lookout (3)
     * [29]use cases (1)

jonker-volgenant algorithm + id167 = super powers

   by [30]devrel team / 14 march 2017

before

   mnist id167 before

after

   mnist id167 after

   intrigued? then    first things first!

id167

   [31]id167 is the very popular algorithm to extremely reduce the
   dimensionality of your data in order to visually present it. it is
   capable of mapping hundreds of dimensions to just 2 while preserving
   important data relationships, that is, when closer samples in the
   original space are closer in the reduced space. id167 works quite well
   for small and moderately sized real-world datasets and does not require
   much tuning of its hyperparameters. in other words, if you   ve got less
   than 100,000 points, you will apply that magic black box thing and get
   a beautiful scatter plot in return.

   here is a classic example from id161. there is a well known
   dataset named [32]   mnist    by yann lecun (one of the inventors of
   [33]id26 method of training neural networks - the core of
   modern deep learning) et. al. it is often used as the default dataset
   for evaluating machine learning ideas and is widely employed in
   academia. mnist is 70,000 greyscale images of size 28x28. each is the
   scan of a handwritten digit    [0,9] there is a way to obtain an
   [34]   infinite    mnist dataset but i shouldn   t diverge.

   thus each mnist sample contains 28   28=784 features and can be
   represented by a 784-dimensional vector. vectors are linear and we lose
   the locality relationships between individual pixels in this
   interpretation but it is still helpful. if you try to imagine how our
   dataset looks like in 784d, you will go nuts unless you are a trained
   mathematician. ordinary humans can consume visual information only in
   3d, 2d or 1d. we may implicitly add another dimension, time, but
   usually nobody says that a computer display is 3d just because it
   changes the picture with 100hz frequency. thus it would be nice to have
   a way to map samples in 784 dimensions to 2. sounds impossible? it is,
   in the general case. this is where [35]dirichlet   s box principle works:
   you are doomed to have collisions, whatever mapping algorithm you
   choose.
   shadowmatic

   3d -> 2d projection illusion in [36]shadowmatic

   luckily, the following two assumptions stand:
    1. the original high-dimensional space is sparse, that is, samples are
       most likely not distributed uniformly in it.
    2. we do not have to find the exact mapping, especially given the fact
       that it does not exist. we can rather solve a different problem
       which has a guaranteed precise solution which approximates what we
       would like to see. this resembles how jpeg compression works: we
       never get the pixel-to-pixel identical result, but the image looks
       very similar to its origin.

   the question is, what is the best approximate problem in (2).
   unfortunately, there is no    best   . the quality of dimensionality
   reduction is subjective and depends on your ultimate goal. the root of
   the confusion is the same as in determining the perfect id91: it
   depends.
   id91 algorithms

   different id91 algorithms from [37]sklearn

   id167 is one of a series of possible id84
   algorithms which are called embedding algorithms. the core idea is to
   preserve the similarity relations as much as possible. play with it
   yourself:

   adapted from [38]how to use id167 effectively

   those are artificial examples - cool but not enough. the majority of
   real-world datasets resemble a cloud with local clusters. for example,
   mnist looks like this:
   mnist_tsne

   mnist after applying id167

   we can clearly see how similar digits tend to attract each other.
   id135

   now let us make a steep turn and review what is [39]id135
   (lp). sorry but it   s not a new design pattern, a javascript framework
   or a startup. it is math:
   [screen-shot-2019-03-06-at-3.19.13-pm.png]

   we minimize the scalar product of c and x given the set of linear
   inequations depending of x and the requirement that all its coordinates
   are not negative. lp is a well-studied topic in id76
   theory, it is known to have [40]weakly-polynomial solutions which
   typically run in o(n3) time where n is the number of variables
   (problem   s dimensionality). there often are approximate algorithms
   which run in linear time. those algorithms deal with matrix
   multiplications and can be parallelized efficiently. a programmer   s
   heaven!

   amazingly many problems can be tracked down to lp. for example, let   s
   take the [41]transportation problem.
   transportation problem

   transportation problem: supplies and demands.

   there is a number of different supplies and demands, which may be not
   equal. every demand needs a fixed amount of supplies. every supply is
   limited and is connected with some of the demands. the core of the
   problem is that every edge sidj has it   s own    cost    cij so we need to
   find the supply scheme which minimizes the sum of those costs.
   formally,
   [screen-shot-2019-03-06-at-3.22.13-pm.png]

   the last condition means that either we run out of supplies or there is
   no more demand. if
   [screen-shot-2019-03-06-at-3.22.57-pm.png]

   can be normalized and simplified as
   [screen-shot-2019-03-06-at-3.23.34-pm.png]

   now if we replace    supplies    and    demands    with    dirt   ,
   [screen-shot-2019-03-06-at-3.24.12-pm.png]

   gives us [42]earth mover   s distance: the minimal volume of work
   required to carry dirt from one pile distribution to another. next time
   you dig holes in the ground, you know what to do   
   earth mover's distance

   earth mover   s distance

   if we replace    supplies    and    demands    with    histograms   , we get the
   most popular way to compare images in pre-deep learning era
   ([43]example paper). it is better than naive l2 because it captures the
   spatial difference additionally to the magnitudal one.
   emd

   earth mover   s distance is better than euclidean distance for histogram
   comparison.

   if we replace    supplies    and    demands    with    words   , we get [44]word
   mover   s distance, a good way of comparing meanings of two sentences
   given id27s from [45]id97.
   word mover's distance

   word mover   s distance

   if we relax the conditions 5-8 by throwing away 8, set wsi=wdi=1 and
   turn inequalities 6 and 7 into equations by adding the symmetric
   negated inequalities, we get the [46]linear assignment problem (lap):
   [screen-shot-2019-03-06-at-3.25.54-pm.png]

   unlike in transportation problem, it can be proved that xij   {0,1} - the
   solution is always binary. in other words, either the whole supply goes
   to some demand, or nothing.

id167 lap

   as we saw in the first section, id167 (or any other embedding) produces
   a scatter plot. while it is perfectly suitable for dataset exploration
   tasks, sometimes we need to map every sample in the original scatter
   plot to a node in the regular grid. e.g. source{d} needs this mapping
   to    you will see why soon.
   regular grid

   the regular grid

   we can draw mnist digits instead of dots after id167, this is how it
   looks like:
   mnist id167 before

   mnist digits after id167.

   not very clear. this where lap arises: we could define the cost matrix
   as the pairwise euclidean distances between id167 samples and grid
   nodes, set the grid square equal to the dataset size ||s||=||d|| and
   eventually solve our problem. but how? no algorithms were presented so
   far.

jonker-volgenant algorithm

   it turns out that there are tons of general-purpose linear optimization
   algorithms, starting from the [47]simplex method and ending with very
   sophisticated solvers. algorithms which are specialized for the
   specific conditions usually converge remarkably faster, though they may
   have some limitations.

   [48]hungarian algorithm is one of those specialized solvers invented in
   1950-s. it   s complexity is o(n3). it is rather simple to understand and
   to implement, thus the popular choice in a lot of projects. for
   example, it has recently become the part of [49]scipy. unfortunately,
   it performs slow on bigger problem sizes; scipy   s variant is
   particularly very slow. i waited an hour for it to finish on 2500 mnist
   samples and yet python was still digesting the victim.

   [50]jonker-volgenant algorithm is an improved approach developed in
   1987. it   s core is still the shortest augmenting path traversal and
   it   s complexity is still cubic, but it uses some smart heuristics and
   tricks to dramatically reduce the computational load. the performance
   of many other lap algorithms including jv was extensively studied in
   [51]2000   s discrete applied mathematics paper. the conclusion was that:

   jv has a good and stable average performance for all the (problem -
   vadim) classes, and it is the best algorithm for the uniform random    
   and for the single-depot class.

   there is a caveat with the jv algorithm though. it is loosely tolerant
   to the difference between any pair of cost elements in the cost matrix.
   for example, if there are two very close costs appearing in the same
   graph where we search for the shortest path using dijkstra   s algorithm,
   it can potentially loop forever. if you take a closer look at
   dijkstra   s algorithm, you will eventually discover that when it reaches
   the floating point precision limit, terrible things may happen. the
   common workaround is to multiply the cost matrix by some big number.
   hold yourselves

   anyway, the most exciting thing about jv for a lazy engineer like me is
   that there is an existing python 2 package which wraps the [52]ancient
   jv c implementation: [53]pylapjv. that c code was written by [54]roy
   jonker in 1996 for magiclogic optimization inc. - he is the company   s
   president. if you read this, roy, please share your paper under
   cc-by-something, everybody wants to read it! besides from being
   abandonware, pylapjv has a minor problem with the output which i
   resolved in [55]pr #2. the c code is reliable, but it does not leverage
   any threads or simd instructions. of course, we saw that jv is
   sequential in it   s nature and cannot be easily parallelized, however, i
   managed to speed it up 2x after optimizing the hottest block -
   augmenting row reduction - with [56]avx2. the result is the new python
   3 package [57]src-d/lapjv which we open sourced under mit license.

   augmenting row reduction phase at its core is finding the minimum and
   the second minimum array elements. sounds easy as it is, the
   unoptimized c version takes about 20 lines of code. avx version is 4
   times bigger: we record minimums in each lane of the simd vector,
   perform [58]blending and cast other dark simd magic spells i learned
   while i was writing samsung   s [59]libsoundfeatureextraction.
template <typename idx, typename cost>
__attribute__((always_inline)) inline
std::tuple<cost, cost, idx, idx> find_umins(
    idx dim, idx i, const cost *assigncost, const cost *v) {
  cost umin = assigncost[i * dim] - v[0];
  idx j1 = 0;
  idx j2 = -1;
  cost usubmin = std::numeric_limits<cost>::max();
  for (idx j = 1; j < dim; j++) {
    cost h = assigncost[i * dim + j] - v[j];
    if (h < usubmin) {
      if (h >= umin) {
        usubmin = h;
        j2 = j;
      } else {
        usubmin = umin;
        umin = h;
        j2 = j1;
        j1 = j;
      }
    }
  }
  return std::make_tuple(umin, usubmin, j1, j2);
}

   finding two consecutive minimums, plain c++.
template <typename idx>
__attribute__((always_inline)) inline
std::tuple<float, float, idx, idx> find_umins(
    idx dim, idx i, const float *assigncost, const float *v) {
  __m256i idxvec = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);
  __m256i j1vec = _mm256_set1_epi32(-1), j2vec = _mm256_set1_epi32(-1);
  __m256 uminvec = _mm256_set1_ps(std::numeric_limits<float>::max()),
         usubminvec = _mm256_set1_ps(std::numeric_limits<float>::max());
  for (idx j = 0; j < dim - 7; j += 8) {
    __m256 acvec = _mm256_loadu_ps(assigncost + i * dim + j);
    __m256 vvec = _mm256_loadu_ps(v + j);
    __m256 h = _mm256_sub_ps(acvec, vvec);
    __m256 cmp = _mm256_cmp_ps(h, uminvec, _cmp_le_oq);
    usubminvec = _mm256_blendv_ps(usubminvec, uminvec, cmp);
    j2vec = _mm256_blendv_epi8(
        j2vec, j1vec, reinterpret_cast<__m256i>(cmp));
    uminvec = _mm256_blendv_ps(uminvec, h, cmp);
    j1vec = _mm256_blendv_epi8(
        j1vec, idxvec, reinterpret_cast<__m256i>(cmp));
    cmp = _mm256_andnot_ps(cmp, _mm256_cmp_ps(h, usubminvec, _cmp_lt_oq));
    usubminvec = _mm256_blendv_ps(usubminvec, h, cmp);
    j2vec = _mm256_blendv_epi8(
        j2vec, idxvec, reinterpret_cast<__m256i>(cmp));
    idxvec = _mm256_add_epi32(idxvec, _mm256_set1_epi32(8));
  }
  float uminmem[8], usubminmem[8];
  int32_t j1mem[8], j2mem[8];
  _mm256_storeu_ps(uminmem, uminvec);
  _mm256_storeu_ps(usubminmem, usubminvec);
  _mm256_storeu_si256(reinterpret_cast<__m256i*>(j1mem), j1vec);
  _mm256_storeu_si256(reinterpret_cast<__m256i*>(j2mem), j2vec);

  idx j1 = -1, j2 = -1;
  float umin = std::numeric_limits<float>::max(),
        usubmin = std::numeric_limits<float>::max();
  for (int vi = 0; vi < 8; vi++) {
    float h = uminmem[vi];
    if (h < usubmin) {
      idx jnew = j1mem[vi];
      if (h >= umin) {
        usubmin = h;
        j2 = jnew;
      } else {
        usubmin = umin;
        umin = h;
        j2 = j1;
        j1 = jnew;
      }
    }
  }
  for (int vi = 0; vi < 8; vi++) {
    float h = usubminmem[vi];
    if (h < usubmin) {
      usubmin = h;
      j2 = j2mem[vi];
    }
  }
  for (idx j = dim & 0xfffffff8u; j < dim; j++) {
    float h = assigncost[i * dim + j] - v[j];
    if (h < usubmin) {
      if (h >= umin) {
        usubmin = h;
        j2 = j;
      } else {
        usubmin = umin;
        umin = h;
        j2 = j1;
        j1 = j;
      }
    }
  }
  return std::make_tuple(umin, usubmin, j1, j2);
}

   finding two consecutive minimums, optimized code with avx2 intrinsics.

   lapjv maps 2500 mnist samples in 5 seconds on my laptop and finally we
   see the precious result:
   mapping

   linear assignment problem solution for mnist after id167.

notebook

   i used the following [60]jupyter notebook ([61]link) to prepare this
   post:

conclusion

   there is an efficient way to map id167-embedded samples to the regular
   grid. it is based on solving the linear assignment problem using
   jonker-volgenant algorithm implemented in [62]src-d/lapjv. this
   algorithm scales up to 10,000 samples.

   devrel team

   [63]devrel team

   devrel team at source{d}

try source{d} engine today

   discover how source{d} can help your business
   [64]request a demo [65]install engine

sign up to our newsletter

   sign up for our machine learning on code newsletter to receive the
   latest blog posts, videos and events from source{d} and others in the
   community.

   (button) sign up

   we   re happy to answer any questions you might have about community,
   help you find the right resources, and meet other machine learning on
   code enthusiasts from around the world!

     * [66]product
     * [67]source{d} engine
     * [68]source{d} lookout

     * [69]docs
     * [70]source{d} engine

     * [71]open source
     * [72]philosophy
     * [73]licensing
     * [74]projects

     * [75]community
     * [76]resources
     * [77]talk to us
     * [78]events

     * [79]company
     * [80]about
     * [81]careers
     * [82]offices
     * [83]team
     * [84]contact us

   ```

references

   visible links
   1. https://blog.sourced.tech/rss/
   2. https://sourced.tech/
   3. https://sourced.tech/#products
   4. https://sourced.tech/engine
   5. https://sourced.tech/lookout
   6. https://docs.sourced.tech/engine/
   7. https://docs.sourced.tech/engine
   8. https://sourced.tech/open-source
   9. https://sourced.tech/community/
  10. https://sourced.tech/community#resources
  11. https://sourced.tech/community#talk
  12. https://sourced.tech/community#events
  13. https://sourced.tech/company/
  14. https://sourced.tech/company#roadmap
  15. https://sourced.tech/company#careers
  16. https://sourced.tech/company#offices
  17. https://sourced.tech/company#team
  18. http://go.sourced.tech/contact
  19. http://go.sourced.tech/engine
  20. https://go.sourced.tech/engine-download
  21. https://blog.sourced.tech/
  22. https://blog.sourced.tech/tag/sourced-engine/
  23. https://blog.sourced.tech/tag/open-source/
  24. https://blog.sourced.tech/tag/events/
  25. https://blog.sourced.tech/tag/community/
  26. https://blog.sourced.tech/tag/company/
  27. https://blog.sourced.tech/tag/research-papers/
  28. https://blog.sourced.tech/tag/source-d-lookout/
  29. https://blog.sourced.tech/tag/use-cases/
  30. https://blog.sourced.tech/author/devrel/
  31. https://lvdmaaten.github.io/tsne/
  32. http://yann.lecun.com/exdb/mnist/
  33. https://en.wikipedia.org/wiki/id26
  34. http://leon.bottou.org/projects/infimnist
  35. https://en.wikipedia.org/wiki/pigeonhole_principle
  36. http://www.shadowmatic.com/
  37. http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html
  38. http://distill.pub/2016/misread-tsne
  39. https://en.wikipedia.org/wiki/linear_programming
  40. https://en.wikipedia.org/wiki/time_complexity#strongly_and_weakly_polynomial_time
  41. http://www.me.utexas.edu/~jensen/models/network/net8.html
  42. https://en.wikipedia.org/wiki/earth_mover's_distance
  43. https://www.cs.cmu.edu/~efros/courses/lbmv07/papers/rubner-jcviu-00.pdf
  44. http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf
  45. https://en.wikipedia.org/wiki/id97
  46. https://en.wikipedia.org/wiki/assignment_problem
  47. https://en.wikipedia.org/wiki/simplex_algorithm
  48. https://en.wikipedia.org/wiki/hungarian_algorithm
  49. https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.linear_sum_assignment.html
  50. https://link.springer.com/article/10.1007/bf02278710
  51. http://www.sciencedirect.com/science/article/pii/s0166218x99001729
  52. http://www.magiclogic.com/assignment.html
  53. https://github.com/hrldcpr/pylapjv
  54. https://www.linkedin.com/in/roy-jonker-9a183310/
  55. https://github.com/hrldcpr/pylapjv/pull/2
  56. https://en.wikipedia.org/wiki/advanced_vector_extensions
  57. https://github.com/src-d/lapjv
  58. https://software.intel.com/en-us/node/524074
  59. https://github.com/samsung/veles.sound_feature_extraction/blob/master/transforms.md
  60. http://jupyter.org/
  61. https://gist.github.com/vmarkovtsev/74e3a973b19113047fdb6b252d741b42
  62. https://github.com/src-d/lapjv
  63. https://blog.sourced.tech/author/devrel/
  64. http://go.sourced.tech/engine
  65. https://go.sourced.tech/engine-download
  66. https://sourced.tech/index.html#products
  67. https://sourced.tech/engine
  68. https://sourced.tech/lookout
  69. https://docs.sourced.tech/engine/
  70. https://docs.sourced.tech/engine/
  71. https://sourced.tech/open-source
  72. https://sourced.tech/open-source#philosophy
  73. https://sourced.tech/open-source#licensing
  74. https://sourced.tech/open-source#projects
  75. https://sourced.tech/community
  76. https://sourced.tech/community#resources
  77. https://sourced.tech/community#talk
  78. https://sourced.tech/community#events
  79. https://sourced.tech/company
  80. https://sourced.tech/company#roadmap
  81. https://sourced.tech/company#careers
  82. https://sourced.tech/company#offices
  83. https://sourced.tech/company#team
  84. http://go.sourced.tech/contact

   hidden links:
  86. https://sourced.tech/
  87. https://www.github.com/src-d/
  88. https://www.twitter.com/sourcedtech/
  89. https://www.youtube.com/sourced/
  90. https://www.linkedin.com/company/source-d/
  91. https://www.slideshare.net/sourcedtech
  92. https://plus.google.com/+sourced
