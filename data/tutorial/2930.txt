   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

tensorflow serving by creating and using docker images

   go to the profile of prathamesh sarang
   [14]prathamesh sarang (button) blockedunblock (button) followfollowing
   apr 6, 2017
   [1*er9liml1g3vrgh8rpbeyga.png]
   tensorflow docker images

   deep learning (dl) and for a good amount, machine learning (ml) suffers
   from the lack of a proper workflow that makes things simple for the
   research to directly translate into production. there   s a dearth of
   easy ways to do it, many cloud providers (microsoft azure ml studio,
   google cloud platform) and independent servcice providers (bigml) have
   made doing ml production a bit easy.

   still there   s a lot of stress to get an ml solution to production. like
   how software engineering has matured with an eco-system of easing the
   developer   s load to get things into production, ml has still a way to
   go. about dl, it still more nascent that traditional ml.

   tensorflow serving debuted in 2015 and has been garnering more
   attention within the deep learning community. google has been subtly
   aggressive about tensorflow, and that the adoption rate is pretty
   strong. statistics says it all:
   [0*t4okg-hcvq5k3wao.png]
   [0*c0rtnuvqyxq3aete.png]

   the sheer monstrosity in the volume is evident in the above images.
   tensorflow isn   t just the media hype, it gives in so many tools that
   makes life easy for a deep learning researcher. when doing
   experiments         [15]tensorboard, it   s integration with other open source
   projects (namely docker, hadoop, kubernetes) and other new things (like
   xla compiler) that makes working on tensorflow exciting. another
   important framework that tensorflow provides us is [16]serving.

   in this part we   ll try to get tensorflow serving model server up and
   running.

what is tensorflow serving?

     tensorflow serving is a flexible, high-performance serving system
     for machine learning models, designed for production environments.
     tensorflow serving makes it easy to deploy new algorithms and
     experiments, while keeping the same server architecture and apis.
     tensorflow serving provides out-of-the-box integration with
     tensorflow models, but can be easily extended to serve other types
     of models and data.

   at the start of this post, we lamented the dearth of machine learning
   workflows that would make developer and researcher   s lives easier.
   tensorflow serving (tfserving) is a good step forward to do that. it
   sets up a continuous delivery pipeline, with lesser headaches that
   include serving a new model instantly, trying to get the requests to
   old model completed and then discard it. below is how the tfserving
   home page describes it:

   tfserving makes it easy for retrained model to deployed to a production
   environment. it also goes a bit further to make it available to the
   outside world via a [17]grpc server making it discoverable. we   ll be
   making use of tfserving to run inceptionv3 model.

   this post would deal with creating a custom docker image (if you aren   t
   familiar with docker do read up on that, plus i   ll be writing a few
   beginner posts on docker) and pushing it to dockerhub or other managed
   container services. why making it as a docker image and pushing it to
   some repo? you   ll get to know.

experience with getting tfserving working

   in the wintery october 2016, i was tasked with getting tfserving
   running for our internal research project. being the naive person i
   was, tried a lot of things but couldn   t get it working. with a sleuth
   of projects coming in, tfserving went to the backburner until a few
   weeks back when shit got real.

   to give you a rough estimate of how much time i took to get tfserving
   working : 4 months (neglect) + 4 weeks (iterative failures). i   m an
   arsenal fan so, 4 is in my horoscope.

   after a lot of reading (not much out there on the web), trail&error
   success did come in the form of a docker image.

pre-requisites

   you could do it on your laptop, provided you have an excellent
   threshold for patience and absolutely smashing internet connection. i
   would advise you to use cloud vms to execute the instructions in this
   blog post.

   i used google cloud   s compute engine to create a 30gb (standard disk),
   debian gnu/linux8 os vm with 2cpus. this costs me around $49.75. we
   need a 30gb disk to commit the images to the disk, had to struggle with
   this since i was using 10gb disk. also, another reason was free $300
   credits to use and no extra to pay.

   you can use digitalocean/aws/ms azure, your choice or even a local
   system. we   ll start with the actual execution, ssh in your vm and
   follow along.

1. downloading the serving repository from tensorflow:

     * git clone --recursive [18]https://github.com/tensorflow/serving
     * cd serving

2. installing docker in debian:

     * sudo apt-get install \ apt-transport-https \ ca-certificates \ curl
       \ software-properties-common
     * curl -fssl "https://download.docker.com/linux/debian/gpg"
     * sudo apt-key add -
     * sudo apt-key fingerprint 0ebfcd88
     * sudo add-apt-repository "deb [arch=amd64]
       https://download.docker.com/linux/debian $(lsb_release -cs) stable"
     * sudo apt-get update
     * sudo apt-get install docker-ce
     * apt-cache madison docker-ce
     * sudo apt-get install docker-ce=17.03.1~ce-0~debian-jessie
     * cd serving

   try out a dry run by checking:

   (note: add docker user to sudo group follow this [19]link)

3. tweaking the docker file in serving

   tensorflow using docker file is an easy way to get started without much
   hassle, but there   s still some things that you need to change. firstly,
   there   s no way the port 9000 (used for tensorflow model server where it
   could be accessed) and with the very recent build the bazel version
   needs to be upgraded from 0.4.2 to 0.4.5 so that things don   t break.

   here are two things that you need to do after doing $ cd
   tensorflow_serving/tools/docker/:
     * change the dockerfile.devel by adding export 9000
     * change the env bazel_version 0.4.2 to env bazel_version 0.4.5
       ([20]tensorflow: error #8738)

4. building the docker file

     * docker build --pull -t $user/tfserving
       -f ./tensorflow_serving/tools/docker/dockerfile.devel .

5. running the docker image, do the following:

     * docker run --name=tfserving -p 9000:9000 -d -it $user/tfserving (-d
       for detached mode)

6. saving the docker image

   this new image would be ~1.09gb and serves as a base docker image for
   tfserving. we   ll be using [21]quay.io to save our images. it has a
   pretty decent plan, with 30 days free trial. register and go to the
   tutorials page to know how to login through the cli.

   follow along after logging in:
     * docker ps

   container id image command created status ports names d3f896a1dde6
   pratos/tfserving "/bin/bash" 4 seconds ago up 3 seconds
   0.0.0.0:9000->9000/tcp tfserving
     * docker commit d3f896a1dde6 quay.io/pratos/tfserving (commit the
       images)
     * docker push quay.io/pratos/tfserving (push the repository to
       quay.io)

   you   ll find your image in the repository.
   [0*sm0yg6uwig3tpy7a.png]

7. compiling serving repo in the docker image

   this is the most important sections of the process and the most time
   consuming. the original image would swell from ~1.09gb to ~5.8gb,
   patience and a stable internet connection is needed (advisable to keep
   this process running in screen or tmux so that it is persisted even
   when your local internet connection drops off).

   delete the container you ran previously and create a new one without
   the -d option or simple attach it (you   ll be able to ssh through docker
   attach <container>)
     * docker run --name=tfserving -p 9000:9000 -it
       quay.io/pratos/tfserving

   you   ll be in the docker image now, run the commands below carefully to
   make sure there   s no problem later:
     * mkdir -p /work/
     * cd /work/
     * sudo apt-get install git
     * git clone --recurse-submodules
       [22]https://github.com/tensorflow/serving
     * cd /work/serving/tensorflow && ./configure

   the last command would ask you a few popup questions (most of which
   would be no). configure that and run the next set of commands below:
     * cd /work/serving
     * bazel build -c opt tensorflow_serving/... --jobs 1 --curses no
       --discard_analysis_cache

   one thing to note here is number of jobs that bazel spawns is 200,
   which would lead to virtual memory error. since, we are on a budget vm
   (not exactly high end) the number of jobs set is --jobs 1. you could
   set it higher/tune it accordingly if you want to process to finish
   faster.

   i experienced a lot of errors when i started with tfserving, after
   reading up documentation and github issues, was able to crack this
   thing. go out for a coffee or do something else once you start the
   compilation, otherwise waiting for bazel to compile would feel like the
   following:
   [0*pdgohffqgwayfxch.jpg]

   if you don   t get error, compile is successful and you can run the below
   command to check:
     * /work/serving/bazel-bin/tensorflow_serving/model_servers/tensorflow
       _model_server

usage: /work/serving/bazel-bin/tensorflow_serving/model_servers/tensorflow_model
_server
flags:
--port=8500 int32       port to listen on
--enable_batching=false bool    enable batching
--model_name="default" string   name of model
--model_version_policy="latest_version" string
the version policy which determines the number of model versions to be served at
 the same time. the default value is latest_version, which will serve only the l
atest version.
see file_system_storage_path_source.proto for the list of possible versionpolicy
.
--file_system_poll_wait_seconds=1       int32
interval in seconds between each poll of the file system for new model version
--model_base_path="" string     path to export (required)
--use_saved_model=false bool    if true, use savedmodel in the server; otherwise
, use sessionbundle.
it is used by tensorflow serving team to control the rollout of savedmodel and i
s not expected to be set by users directly.

8. testing for inceptionv3 model

   our next aim is to make sure that inceptionv3 works and we are able to
   get id136s once we provide test images through the server.

   downloading the inceptionnet models, to test id136s:
     * curl -o
       [23]http://download.tensorflow.org/models/image/id163/inception-
       v3-2016-03-01.tar.gz
     * tar xzf inception-v3-2016-03-01.tar.gz
     * bazel-bin/tensorflow_serving/example/inception_saved_model
       --checkpoint_dir=inception-v3 --output_dir=inception-export

   (a hefty output, with warnings peppered handful.)
warning:tensorflow:from /work/serving/bazel-bin/tensorflow_serving/example/incep
tion_saved_model.runfiles/inception_model/inception/inception_model.py:150: hist
ogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be
 removed after 2016-11-30.
instructions for updating: please switch to tf.summary.histogram. note that tf.s
ummary.histogram uses the node name instead of the tag.
this means that tensorflow will automatically de-duplicate summary names based o
n the scope they are created in. . . . <warnings continued>
2017-03-29 19:55:53.938435: w external/org_tensorflow/tensorflow/core/platform/c
pu_feature_guard.cc:45] the tensorflow library wasn't compiled to use fma instru
ctions, but these are available on your machine and could speed up cpu computati
ons.
successfully loaded model from inception-v3/model.ckpt-157585 at step=157585.
exporting trained model to inception-export/1
warning:tensorflow:from /work/serving/bazel- bin/tensorflow_serving/example/ince
ption_saved_model.runfiles/tf_serving/tensorflow_serving/example/inception_saved
_model.py:155: initialize_all_tables (from tensorflow.python.ops.data_flow_ops)
is deprecated and will be removed after 2017-03-02.
instructions for updating: use `tf.tables_initializer` instead.
successfully exported model to inception-export

   starting the grpc server:
     * bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server
       --port=9000 --model_name=inception
       --model_base_path=inception-export &> inception_log &

   save a few images (any random would do, except for humans i guess)
   using wget/curl

   let   s get the id136s:
     * bazel-bin/tensorflow_serving/example/inception_client
       --server=localhost:9000 --image=./images/dog1.jpg

d0329 20:43:26.487714047 26684 ev_posix.c:101]
using polling engine: poll outputs
{ key: "classes" value
   { dtype: dt_string tensor_shape
        { dim { size: 1 } dim { size: 5 }
    }
string_val: "beagle"
string_val: "english foxhound"
string_val: "walker hound, walker foxhound"
string_val: "bluetick"
string_val: "tennis ball"
     }
}
outputs
{
  key: "scores"
  value { dtype: dt_float tensor_shape {
             dim { size: 1 } dim { size: 5 }
        }
  float_val: 8.8880033493
  float_val: 5.99537038803
  float_val: 5.76968288422
  float_val: 3.91565990448
  float_val: 3.65689516068
  }
}
e0329 20:43:29.299981796 26684 chttp2_transport.c:1810] close_transport: {"creat
ed":"@1490820209.299938346","description":"fd shutdown","file":"src/core/lib/iom
gr/ev_poll_posix.c","file_line":427}

9. saving the final docker container

   now that we have the server running on port 9000, ctrl+p and ctrl+q to
   quit the docker container.

   run the commands below to make sure you commit the containers to images
     * docker commit d3f896a1dde6 quay.io/pratos/baseinception (commit the
       container to images)
     * docker push quay.io/pratos/baseinception (push the repository to
       quay.io)

   so in this blog we looked at how to create a new docker image that can
   be now used just by downloading it from anywhere. dockerhub can also be
   used for the same, but quay.io felt better (mostly coz i was lazy to
   search and read up on dockerhub).

   that   s all for now. this blog wouldn   t be possible without the awesome
   resource on medium (listed below). i   ve just tried to add a few things
   that i felt were much doable on budget resources and small tweaking of
   the base docker.devel file.

   feel free to use the docker image: docker pull
   quay.io/pratos/baseinception

   sources:
    1. [24]tensorflow using docker
    2. [25]tensorflow serving basics
    3. [26]osl engineering blog         tensorflow serving : practical
       introduction
    4. [27]zen desk engineering         tensorflow serving blog
    5. [28]tensorflow serving in 10 minutes

   written on march 31st , 2017 by prathamesh

   feel free to share and give feedback!
     __________________________________________________________________

   originally published at [29]pratos.github.io.

   iframe: [30]/media/a6edaa982bd7d35e5cbcc4b8ed2f9c2b?postid=336ca4de8671

     * [31]docker
     * [32]tensorflow
     * [33]tfserving
     * [34]deep learning
     * [35]artificial intelligence

   (button)
   (button)
   (button) 62 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of prathamesh sarang

[36]prathamesh sarang

   medium member since oct 2018

     (button) follow
   [37]becoming human: artificial intelligence magazine

[38]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 62
     * (button)
     *
     *

   [39]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [40]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/336ca4de8671
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/tensorflow-serving-by-creating-and-using-docker-images-336ca4de8671&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/tensorflow-serving-by-creating-and-using-docker-images-336ca4de8671&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_p7hmhrl78eav---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@prato_s
  15. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  16. https://tensorflow.github.io/serving/
  17. http://www.grpc.io/
  18. https://github.com/tensorflow/serving
  19. http://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo
  20. https://github.com/tensorflow/tensorflow/issues/8738
  21. https://quay.io/
  22. https://github.com/tensorflow/serving
  23. http://download.tensorflow.org/models/image/id163/inception-v3-2016-03-01.tar.gz
  24. https://tensorflow.github.io/serving/docker
  25. https://tensorflow.github.io/serving/serving_basic
  26. https://medium.com/osldev-blog/tensorflow-serving-practical-introduction-9ce29ccd63f
  27. https://medium.com/zendesk-engineering/how-zendesk-serves-tensorflow-models-in-production-751ee22f0f4b
  28. https://medium.com/@avloss/tensorflow-serving-in-10-minutes-a0d03f00ebeb
  29. https://pratos.github.io/tensorflow-serving-using-docker.html
  30. https://becominghuman.ai/media/a6edaa982bd7d35e5cbcc4b8ed2f9c2b?postid=336ca4de8671
  31. https://becominghuman.ai/tagged/docker?source=post
  32. https://becominghuman.ai/tagged/tensorflow?source=post
  33. https://becominghuman.ai/tagged/tfserving?source=post
  34. https://becominghuman.ai/tagged/deep-learning?source=post
  35. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  36. https://becominghuman.ai/@prato_s
  37. https://becominghuman.ai/?source=footer_card
  38. https://becominghuman.ai/?source=footer_card
  39. https://becominghuman.ai/
  40. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  42. https://becominghuman.ai/@prato_s?source=post_header_lockup
  43. https://medium.com/p/336ca4de8671/share/twitter
  44. https://medium.com/p/336ca4de8671/share/facebook
  45. https://becominghuman.ai/@prato_s?source=footer_card
  46. https://medium.com/p/336ca4de8671/share/twitter
  47. https://medium.com/p/336ca4de8671/share/facebook
