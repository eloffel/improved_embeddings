   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]chatbot news daily
     * [9]intro to bots
     * [10]write for us
     * [11]join the community
     * [12]subscribe
     __________________________________________________________________

   [0*pho01zwxnkf0lfhv.jpg]
   my subjective ml timeline

brief history of machine learning

   [13]go to the profile of eren golge
   [14]eren golge (button) blockedunblock (button) followfollowing
   mar 31, 2017

   since the initial standpoint of science, technology and ai, scientists
   following blaise pascal and von leibniz ponder about a machine that is
   intellectually capable as much as humans. famous writers like jules

   verne , frank baum (wizard of oz), marry shelly (frankenstein), george
   lucas (star wars) dreamed artificial beings resembling human behaviors
   or even more, swamp humanized skills in different contexts.
   [0*tflsigpx7wwzcy6f.jpg]
   pascal   s machine performing subtraction and summation         1642

   machine learning is one of the important lanes of ai which is very
   spicy hot subject in the research or industry. companies, universities
   devote many resources to advance their knowledge. recent advances in
   the field propel very solid results for different tasks, comparable to
   human performance (98.98% at [15]traffic signs         higher than human-).

   here i would like to share a crude timeline of machine learning and
   sign some of the milestones by no means complete. in addition, you
   should add    up to my knowledge    to beginning of any argument in the
   text.

   first step toward prevalent ml was proposed by hebb, in 1949, based on
   a neuropsychological learning formulation. it is called hebbian
   learning theory. with a simple explanation, it pursues correlations
   between nodes of a recurrent neural network (id56). it memorizes any
   commonalities on the network and serves like a memory later. formally,
   the argument states that;

     let us assume that the persistence or repetition of a reverberatory
     activity (or    trace   ) tends to induce lasting cellular changes that
     add to its stability.    when an [16]axon of cell a is near enough to
     excite a cell b and repeatedly or persistently takes part in firing
     it, some growth process or metabolic change takes place in one or
     both cells such that a   s efficiency, as one of the cells firing b,
     is increased.[1]

     arthur samuel

   in 1952, arthur samuel at ibm, developed a program playing checkers.
   the program was able to observe positions and learn a implicit model
   that gives better moves for the latter cases. samuel played so many
   games with the program and observed that the program was able to play
   better in the course of time.

   with that program samuel confuted the general providence dictating
   machines cannot go beyond the written codes and learn patterns like
   human-beings. he coined    machine learning,     which he defines as;

     a field of study that gives computer the ability without being
     explicitly programmed.

   [0*ivu6qh9k5npjttw3.jpg]
   f. rosenblatt

   in 1957, rosenblatt   s id88 was the second model proposed again
   with neuroscientific background and it is more similar to today   s ml
   models. it was a very exciting discovery at the time and it was
   practically more applicable than hebbian   s idea. rosenblatt introduced
   the id88 with the following lines;

     the id88 is designed to illustrate some of the fundamental
     properties of intelligent systems in general, without becoming too
     deeply enmeshed in the special, and frequently unknown, conditions
     which hold for particular biological organisms.[2]

   after 3 years later, widrow [4] engraved delta learning rule that is
   then used as practical procedure for id88 training. it is also
   known as least square problem. combination of those two ideas creates a
   good linear classifier. however, id88   s excitement was hinged by
   minsky[3] in 1969 . he proposed the famous xor problem and the
   inability of id88s in such linearly inseparable data
   distributions. it was the minsky   s tackle to nn community. thereafter,
   nn researches would be dormant up until 1980s
   [0*aozngugjnlblze5w.gif]

   xor problem which is nor linearly seperable data orientation

   there had been not to much effort until the intuition of multi-layer
   id88 (mlp) was suggested by werbos[6] in 1981 with nn specific
   id26(bp) algorithm, albeit bp idea had been proposed before
   by linnainmaa [5] in 1970 in the name    reverse mode of automatic
   differentiation   . still bp is the key ingredient of today   s nn
   architectures. with those new ideas, nn researches accelerated again.
   in 1985   1986 nn researchers successively presented the idea of mlp with
   practical bp training (rumelhart, hinton, williams [7]         hetch,
   nielsen[8])
   [0*csfrlimnhbemxjbl.png]
   from hetch and nielsen [8]

   at the another spectrum, a very-well known ml algorithm was proposed by
   j. r. quinlan [9] in 1986 that we call id90, more
   specifically   algorithm. this was the spark point of the another
   mainstream ml. moreover,   was also released as a software able to
   find more real-life use case with its simplistic rules and its clear
   id136, contrary to still black-box nn models.

   after  , many different alternatives or improvements have been
   explored by the community (e.g.  , regression trees, cart    ) and
   still it is one of the active topic in ml.
   [0*blmlrsug-wsv7z3s.png]
   from quinlan [9]

   one of the most important ml breakthrough was support vector machines
   (networks) (id166), proposed by vapnik and cortes[10] in 1995 with very
   strong theoretical standing and empirical results. that was the time
   separating the ml community into two crowds as nn or id166 advocates.
   however the competition between two community was not very easy for the
   nn side after kernelized version of id166 by near 2000s .(i was not able
   to find the first paper about the topic), id166 got the best of many
   tasks that were occupied by nn models before. in addition, id166 was able
   to exploit all the profound knowledge of id76,
   generalization margin theory and kernels against nn models. therefore,
   it could find large push from different disciplines causing very rapid
   theoretical and practical improvements.
   [0*07opiy4zge6dsmkz.png]
   from vapnik and cortes [10]

   nn took another damage by the work of hochreiter   s thesis [40] in 1991
   and hochreiter et. al.[11] in 2001, showing the gradient loss after the
   saturation of nn units as we apply bp learning. simply means, it is
   redundant to train nn units after a certain number of epochs owing to
   saturated units hence nns are very inclined to over-fit in a short
   number of epochs.

   little before, another solid ml model was proposed by freund and
   schapire in 1997 prescribed with boosted ensemble of weak classifiers
   called adaboost. this work also gave the godel prize to the authors at
   the time. adaboost trains weak set of classifiers that are easy to
   train, by giving more importance to hard instances. this model still
   the basis of many different tasks like face recognition and detection.
   it is also a realization of pac (probably approximately correct)
   learning theory. in general, so called weak classifiers are chosen as
   simple decision stumps (single decision tree nodes). they introduced
   adaboost as ;

     the model we study can be interpreted as a broad, abstract extension
     of the well-studied on-line prediction model to a general
     decision-theoretic setting   [11]

   another ensemble model explored by breiman [12] in 2001 that ensembles
   multiple id90 where each of them is curated by a random
   subset of instances and each node is selected from a random subset of
   features. owing to its nature, it is called id79s(rf). rf has
   also theoretical and empirical proofs of endurance against
   over-fitting. even adaboost shows weakness to over-fitting and outlier
   instances in the data, rf is more robust model against these
   caveats.(for more detail about rf, refer to [17]my old post.). rf shows
   its success in many different tasks like kaggle competitions as well.

     id79s are a combination of tree predictors such that each
     tree depends on the values of a
      random vector sampled independently and with the same distribution
     for all trees in the forest. the generalization error for forests
     converges a.s. to a limit as the number of trees in the forest
     becomes large[12]

   as we come closer today, a new era of nn called deep learning has been
   commerced. this phrase simply refers nn models with many wide
   successive layers. the 3rd rise of nn has begun roughly in 2005 with
   the conjunction of many different discoveries from past and present by
   recent mavens hinton, lecun, bengio, andrew ng and other valuable older
   researchers. i enlisted some of the important headings (i guess, i will
   dedicate complete post for deep learning specifically) ;
     * gpu programming
     * convolutional nns [18][20][40]
     * deconvolutional networks [21]
     * stochastic id119 [19][22]
     * bfgs and l-bfgs [23]
     * conjugate id119 [24]
     * id26 [40][19]
     * rectifier units
     * sparsity [15][16]
     * dropout nets [26]
     * maxout nets [25]
     * unsupervised nn models [14]
     * id50 [13]
     * stacked auto-encoders [16][39]
     * denoising nn models [17]
     * (gan) id3 [41]
     * variational auto-encoders [42]

   [1*5y2juz1nu0vhw-k3wbbgnq.png]
   alexnet wins id163 and dl takes off

   with the combination of all those ideas and non-listed ones, nn models
   are able to beat off state of art at very different tasks such as
   object recognition, id103, nlp etc. however, it should be
   noted that this absolutely does not mean, it is the end of other ml
   streams. even deep learning success stories grow rapidly , there are
   many critics directed to training cost and tuning exogenous parameters
   of these models. moreover, still id166 is being used more commonly owing
   to its simplicity. (said but may cause a huge debate :) )

   in the trend of deep learning, we   ve seen image recognition as the
   first threshold and then nlp . those problems seem to solved to quite a
   good extend and now we see many ai products all over the tech space.
   now the next trend seems to directed toward video and generative
   models. video is a larger input and it is much harder problem in terms
   of both computation and algorithm but it servers more to general sense
   ai mission, if we consider that the human visual perception is
   stimulated by temporal inputs. generative learning takes the problem in
   the reverse order. it aims to generate a realistic image given certain
   clues to the model. such model needs to learn how to represent data
   instead of a simpler problem of discerning categories. and to the same
   end, both of these two trends server smarter algorithms that mighr
   reduce the need of abundant data requirement of the present deep
   learning solutions. for instance, with a video input, an algorithm is
   able to learn many different aspects of an object and it can generalize
   this information without seeing millions of id163 dataset. or,
   generative model might generalize its representational knowledge to
   classification tasks in scarce data regimes.

   before finish, i need to touch on one another hot ml topic. after the
   growth of www and social media, a new term, bigdata emerged and
   affected ml research wildly. because of the large problems arising from
   bigdata , many strong ml algorithms are useless for reasonable systems
   (not for giant tech companies of course). hence, research people come
   up with a new set of simple models that are dubbed bandit algorithms
   [27   38] (formally predicated with online learning) that makes learning
   easier and adaptable for large scale problems.

   i would like to conclude this infant sheet of ml history. if you found
   something wrong (you should :) ), insufficient or non-referenced,
   please don   t hesitate to warn me in all manner.

   (this is taken from my [18]blog with couple of updates )

references

   [1] hebb d. o., the organization of behaviour.new york: wiley & sons.

   [2]rosenblatt, frank.    the id88: a probabilistic model for
   information storage and organization in the brain.    psychological
   review 65.6 (1958): 386.

   [3]minsky, marvin, and papert seymour.    id88s.    (1969).

   [4]widrow, hoff    adaptive switching circuits.    (1960): 96   104.

   [5]s. linnainmaa. the representation of the cumulative rounding error
   of an algorithm as a taylor
    expansion of the local rounding errors. master   s thesis, univ.
   helsinki, 1970.

   [6] p. j. werbos. applications of advances in nonlinear sensitivity
   analysis. in proceedings of the 10th
    ifip conference, 31.8   4.9, nyc, pages 762   770, 1981.

   [7] rumelhart, david e., geoffrey e. hinton, and ronald j. williams.
   learning internal representations by error propagation. no. ics-8506.
   california univ san diego la jolla inst for cognitive science, 1985.

   [8] hecht-nielsen, robert.    theory of the id26 neural
   network.    neural networks, 1989. ijid98., international joint conference
   on. ieee, 1989.

   [9] quinlan, j. ross.    induction of id90.    machine learning
   1.1 (1986): 81   106.

   [10] cortes, corinna, and vladimir vapnik.    support-vector networks.   
   machine learning 20.3 (1995): 273   297.

   [11] freund, yoav, robert schapire, and n. abe.    a short introduction
   to boosting.   journal-japanese society for artificial intelligence
   14.771   780 (1999): 1612.

   [12] breiman, leo.    id79s.    machine learning 45.1 (2001):
   5   32.

   [13] hinton, geoffrey e., simon osindero, and yee-whye teh.    a fast
   learning algorithm for deep belief nets.    neural computation 18.7
   (2006): 1527   1554.

   [14] bengio, lamblin, popovici, larochelle,    greedy layer-wise
    training of deep networks   , nips   2006

   [15] ranzato, poultney, chopra, lecun     efficient learning of sparse
   representations with an energy-based model    , nips   2006

   [16] olshausen b a, field dj. sparse coding with an overcomplete basis
   set: a strategy employed by v1? vision res. 1997;37(23):3311   25.
   available at: [19]http://www.ncbi.nlm.nih.gov/pubmed/9425546.

   [17] vincent, h. larochelle y. bengio and p.a. manzagol, [20]extracting
   and composing robust features with denoising autoencoders, proceedings
   of the twenty-fifth international conference on machine learning
   (icml   08), pages 1096   1103, acm, 2008.

   [18] fukushima, k. (1980). neocognitron: a self-organizing neural
   network model for a mechanism of pattern recognition unaffected by
   shift in position. biological cybernetics, 36, 193   202.

   [19] lecun, yann, et al.    gradient-based learning applied to document
   recognition.   proceedings of the ieee 86.11 (1998): 2278   2324.

   [20] lecun, yann, and yoshua bengio.    convolutional networks for
   images, speech, and time series.    the handbook of brain theory and
   neural networks3361 (1995).

   [21] zeiler, matthew d., et al.    deconvolutional networks.    computer
   vision and pattern recognition (cvpr), 2010 ieee conference on. ieee,
   2010.

   [22] s. vishwanathan, n. schraudolph, m. schmidt, and k. mur- phy.
   accelerated training of id49 with stochastic
   meta-descent. in international conference on ma- chine learning (icml
      06), 2006.

   [23] nocedal, j. (1980).    updating quasi-newton matrices with limited
   storage.    mathematics of computation 35 (151): 773782.
   doi:10.1090/s0025   5718   1980   0572855-

   [24] s. yun and k.-c. toh,    a coordinate id119 method for
   l1- regularized convex minimization,    computational optimizations and
   applications, vol. 48, no. 2, pp. 273   307, 2011.

   [25] goodfellow i, warde-farley d. maxout networks. arxiv prepr
   arxiv    . 2013. available at: [21]http://arxiv.org/abs/1302.4389.
   accessed march 20, 2014.

   [26] wan l, zeiler m. id173 of neural networks using
   dropconnect. proc    . 2013;(1). available at:
   [22]http://machinelearning.wustl.edu/mlpapers/papers/icml2013_wan13.
   accessed march 13, 2014.

   [27] [23]alekh agarwal, [24]olivier chapelle, [25]miroslav dudik,
   [26]john langford, [27]a reliable effective terascale linear learning
   system, 2011

   [28] [28]m. hoffman, [29]d. blei, [30]f. bach, [31]online learning for
   id44, in neural information processing systems
   (nips) 2010.

   [29] [32]alina beygelzimer, [33]daniel hsu, [34]john langford, and
   [35]tong zhang [36]agnostic active learning without constraints nips
   2010.

   [30] [37]john duchi, [38]elad hazan, and [39]yoram singer, [40]adaptive
   subgradient methods for online learning and stochastic optimization,
   jmlr 2011 & colt 2010.

   [31] [41]h. brendan mcmahan, [42]matthew streeter, [43]adaptive bound
   optimization for online id76, colt 2010.

   [32] [44]nikos karampatziakis and [45]john langford, [46]importance
   weight aware gradient updates uai 2010.

   [33] [47]kilian weinberger, [48]anirban dasgupta, [49]john langford,
   [50]alex smola, [51]josh attenberg, [52]feature hashing for large scale
   multitask learning, icml 2009.

   [34] [53]qinfeng shi, [54]james petterson, [55]gideon dror, [56]john
   langford, [57]alex smola, and [58]svn vishwanathan, [59]hash kernels
   for structured data, aistat 2009.

   [35] [60]john langford, [61]lihong li, and [62]tong zhang, [63]sparse
   online learning via truncated gradient, nips 2008.

   [36] [64]leon bottou, [65]stochastic id119, 2007.

   [37] [66]avrim blum, [67]adam kalai, and [68]john langford [69]beating
   the holdout: bounds for kfold and progressive cross-validation. colt99
   pages 203   208.

   [38] [70]nocedal, j. (1980).    updating quasi-newton matrices with
   limited storage   . mathematics of computation 35: 773   782.

   [39] d. h. ballard. modular learning in neural networks. in aaai, pages
   279   284, 1987.

   [40] s. hochreiter. untersuchungen zu dynamischen neuronalen netzen.
   diploma thesis, institut f   ur in-
    formatik, lehrstuhl prof. brauer, technische universit   at m   unchen,
   1991. advisor: j. schmidhuber.

   [41] goodfellow, ian, et al.    generative adversarial nets.    advances in
   neural information processing systems. 2014.

   [42] diederik p kingma. auto-encoding id58
   [71]https://arxiv.org/abs/1312.6114

     * [72]machine learning
     * [73]artificial intelligence
     * [74]deep learning
     * [75]history of technology

   (button)
   (button)
   (button) 77 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [76]go to the profile of eren golge

[77]eren golge

   ai researcher

     (button) follow
   [78]chatbot news daily

[79]chatbot news daily

   your #1 source about chatbots in europe

     * (button)
       (button) 77
     * (button)
     *
     *

   [80]chatbot news daily
   never miss a story from chatbot news daily, when you sign up for
   medium. [81]learn more
   never miss a story from chatbot news daily
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://chatbotnewsdaily.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/804ac13d8151
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://chatbotnewsdaily.com/since-the-initial-standpoint-of-science-technology-and-ai-scientists-following-blaise-pascal-and-804ac13d8151&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://chatbotnewsdaily.com/since-the-initial-standpoint-of-science-technology-and-ai-scientists-following-blaise-pascal-and-804ac13d8151&source=--------------------------nav_reg&operation=register
   8. https://chatbotnewsdaily.com/?source=logo-lo_ssmlki5jubp8---a1685d067c12
   9. https://chatbotnewsdaily.com/chatbots-the-entry-into-ai-589505d92e44
  10. https://chatbotnewsdaily.com/be-featured-in-front-of-thousands-of-people-interested-in-bots-a75f04c8fdb2
  11. https://chatbotnewsdaily.com/meet-thousands-of-bot-developers-designers-inventors-and-enthusiasts-28da79764c42
  12. https://chatbotnewsdaily.com/want-to-receive-the-best-chatbot-related-content-in-your-medium-feed-78fa7882051c
  13. https://chatbotnewsdaily.com/@erogol?source=post_header_lockup
  14. https://chatbotnewsdaily.com/@erogol
  15. http://benchmark.ini.rub.de/?section=gtsrb&subsection=results&subsubsection=ijid98
  16. http://en.wikipedia.org/wiki/axon
  17. http://www.erogol.com/randomness-randomforests/
  18. http://www.erogol.com/brief-history-machine-learning/
  19. http://www.ncbi.nlm.nih.gov/pubmed/9425546.
  20. http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/217
  21. http://arxiv.org/abs/1302.4389.
  22. http://machinelearning.wustl.edu/mlpapers/papers/icml2013_wan13.
  23. http://www.cs.berkeley.edu/~alekh/
  24. http://olivier.chapelle.cc/
  25. http://www.cs.cmu.edu/~mdudik/
  26. http://hunch.net/~jl
  27. http://arxiv.org/abs/1110.4198
  28. http://www.cs.princeton.edu/~mdhoffma/
  29. http://www.cs.princeton.edu/~blei/
  30. http://www.di.ens.fr/~fbach/
  31. http://www.cs.princeton.edu/~blei/papers/hoffmanbleibach2010b.pdf
  32. http://hunch.net/~beygel
  33. http://cseweb.ucsd.edu/~djhsu/
  34. http://hunch.net/~jl
  35. http://stat.rutgers.edu/home/tzhang/
  36. http://arxiv.org/abs/1006.2588
  37. http://www.cs.berkeley.edu/~jduchi/
  38. http://ie.technion.ac.il/~ehazan/
  39. http://www.magicbroom.info/about.html
  40. http://www.cs.berkeley.edu/~jduchi/projects/duchihasi10.html
  41. http://www.cs.cmu.edu/~mcmahan/
  42. http://www.cs.cmu.edu/~matts/
  43. http://arxiv.org/abs/1002.4908
  44. http://www.cs.cornell.edu/~nk/
  45. http://hunch.net/~jl
  46. http://arxiv.org/abs/1011.1576
  47. http://www.cse.wustl.edu/~kilian/
  48. http://research.yahoo.com/anirban_dasgupta/
  49. http://hunch.net/~jl
  50. http://alex.smola.org/
  51. http://www.linkedin.com/in/joshattenberg
  52. http://arxiv.org/pdf/0902.2206
  53. http://users.cecs.anu.edu.au/~qshi/
  54. http://users.cecs.anu.edu.au/~jpetterson/
  55. http://www2.mta.ac.il/~gideon/
  56. http://hunch.net/~jl
  57. http://alex.smola.org/
  58. http://www.stat.purdue.edu/~vishy/
  59. http://hunch.net/~jl/projects/hash_reps/hash_kernels/hashkernel.pdf
  60. http://hunch.net/~jl
  61. http://www.research.rutgers.edu/~lihong/
  62. http://stat.rutgers.edu/home/tzhang/
  63. http://hunch.net/~jl/projects/interactive/sparse_online/paper_sparseonline.pdf
  64. http://leon.bottou.org/
  65. http://leon.bottou.org/projects/sgd
  66. http://www.cs.cmu.edu/~avrim/
  67. http://www.cs.cmu.edu/~akalai/
  68. http://hunch.net/~jl
  69. http://hunch.net/~jl/projects/prediction_bounds/progressive_validation/coltfinal.pdf
  70. http://www.ece.northwestern.edu/faculty/nocedal_jorge.html
  71. https://arxiv.org/abs/1312.6114
  72. https://chatbotnewsdaily.com/tagged/machine-learning?source=post
  73. https://chatbotnewsdaily.com/tagged/artificial-intelligence?source=post
  74. https://chatbotnewsdaily.com/tagged/deep-learning?source=post
  75. https://chatbotnewsdaily.com/tagged/history-of-technology?source=post
  76. https://chatbotnewsdaily.com/@erogol?source=footer_card
  77. https://chatbotnewsdaily.com/@erogol
  78. https://chatbotnewsdaily.com/?source=footer_card
  79. https://chatbotnewsdaily.com/?source=footer_card
  80. https://chatbotnewsdaily.com/
  81. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  83. https://medium.com/p/804ac13d8151/share/twitter
  84. https://medium.com/p/804ac13d8151/share/facebook
  85. https://medium.com/p/804ac13d8151/share/twitter
  86. https://medium.com/p/804ac13d8151/share/facebook
