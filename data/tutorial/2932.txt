   #[1]tech blog    flux [2]tech blog    flux des commentaires [3]tech blog
      began: state of the art generation of faces with generative
   adversarial networks flux des commentaires [4]heuritech deep learning
   meetup #8, book your seats! [5]heuritech party : can   t wait for the 2nd
   edition! [6]alternate [7]alternate [8]tech blog [9]wordpress.com

[10]tech blog

menu principal

   [11]acc  der au contenu principal
     * [12]heuritech.com
     * [13]la cha  ne heuritech
     * [14]encore + d   actu

began: state of the art generation of faces with generative adversarial
networks

   [15]11 avril 2017    par [16]anonymous    dans [17]machine learning,
   [18]r&d.   

tl;dr

   this post describes the theory behind the newly introduced [19]began.
     * as of early april 2017, the began is the state of the art when it
       comes to generating realistic faces.
     * it is inspired from the [20]ebgan in that its discriminator is an
       auto-encoder.
     * it shows fast and stable convergence even in the absence of batch
       norm.
     * it automatically balances the trade-off between image diversity and
       quality of generation.
     * and it offers an approximate measure of convergence.

   as a bonus, i   ve added a comparison between the began and the improved
   wgan at the end.

   you can comment on the [21]reddit page associated this post, or on the
   [22]reddit page associated to the paper.

   i hope you   ll enjoy this post!     

the reason it is interesting

   id3 (gans) have achieved impressive (and
   often state-of-the-art) results in various domains such as:
     * image generation with [23]improved wgans (gulrajani et al. 2017)
     * image editing with [24]neural photo editors (brock et al. 2016)
     * super-resolution with [25]srgans (ledig et al. 2016)
     * semi-supervised learning with [26]bigans (donahue et al. 2016)
     * id20 with [27]cyclegans (zhu et al. 2017)
     * etc.

   here at heuritech (a french start-up) we aim to use these models (and
   many others) to help consumers, searchers, corporations and kids with
   their day-to-day deep learning needs. as things turned out, a big part
   of our focus currently lies within the world of fashion (because
   heuritech is also a stylish, well-dressed company), but who knows where
   we   ll be just a few years from now!

   heuritech has been founded by four doctors in artificial intelligence
   and is very research oriented. if you   re interested to be part of the
   adventure you can contact us [28]here.

began

   recently, a new gan architecture (the [29]boundary equilibrium gan)
   caught my interest by successfully generating anatomically coherent
   faces at a resolution of 128  128 pixels. as of early april 2017, this
   is the current state of the art. have a look:

   [1_face_teaser.png?w=440&#038;h=264]

main goal

   it   s all about providing a better loss to the networks.

   it has been previously shown that the [30]first gan architecture
   minimizes the id181 between the real data
   distribution p_x and the generated data distribution p_{g(z)} . an
   unfortunate consequence of trying to minimize this distance is that the
   discriminator d gives meaningless gradients to the generator g if d
   gets too good too quickly.

   since then, a few publications focused their effort on trying to find
   better id168s:
     * the [31]improved wassertein gan (since its [32]first version)
       minimizes the wasserstein distance (also called the earth-mover
       distance) by giving very simple gradients to the networks (+1 if
       the output should be considered real and -1 if the output should be
       considered fake).
     * the [33]least squares gan uses a least squares id168 to
       minimize the pearson \chi^2 divergence between d   s output and its
       target.
     * the [34]generalized loss sensitive gan uses a discriminator that
       quantifies the quality of images. the loss is then computed as a
       function of the distance between the quality of real and generated
       images (which allows the model to focus more on improving poor
       samples than good samples).

   the main goal behind the began is also to change the id168.
   this time, it is achieved by making d an autoencoder. the loss is a
   function of the quality of reconstruction achieved by d on real and
   generated images. this idea (of making d an autoencoder) is inspired by
   the [35]energy based gan (ebgan). below is the architecture of the
   ebgan shamelessly copy-pasted from the paper:

   [2_ebgan_schema.png?w=480&#038;h=150]

the idea

   let   s start by clarifying something important. the reconstruction loss
   is not the same thing as the real loss that the nets are trying to
   minimize. the reconstruction loss is the error associated to
   reconstructing images through the autoencoder/discriminator. in the
   ebgan schema the reconstruction-loss is referred to as    dist    and the
   real loss is referred to as    l   .

   the main idea behind the began is that matching the distributions of
   the reconstruction losses can be a suitable proxy for matching the data
   distributions. the real loss is then derived from the wasserstein
   distance between the reconstruction losses of real and generated data.
   later, the networks are trained by using this real loss in conjunction
   with an equilibrium term to balance d and g.

the training

   i   d like to spoil you right away with the solution to give you an
   understanding of where we   re going.

   the training goes like this:
    1. d (the autoencoder) reconstructs real images better and better.
       said differently, the weights of d are updated so that the
       reconstruction loss of real images is minimized.
    2. d simultaneously increases the reconstruction loss of generated
       images.
    3. and g works adversarially to that by minimizing the reconstruction
       loss of generated images.

   points 1 and 2 can be rephrased as    d tries to discriminate real and
   generated distributions   . so g can only succeed with point 3 by
   generating more realistic images.

deriving the loss

   let   s first derive the real losses and we   ll focus on the equilibrium
   term later. what we want at this point is to use the wasserstein
   distance between the reconstruction losses to derive the real losses.

   if we use an l1 norm between the input image of d and its reconstructed
   version, then the loss distribution (refered to as \mathcal{l}(x) ), is
   approximately normal (at least it looks like it experimentally). we   re
   about to use this fact to simplify the wasserstein distance between the
   reconstruction losses (of real and generated images).

   here is the wassertein distance mathematically:
   w(\mu_{real}, \mu_{gen}) = |m_{real} - m_{gen}| + another\_term
   with:
     * \mu_{real} and \mu_{gen} : the distributions of the reconstruction
       losses of real and generated images ( \mathcal{l}(x) and
       \mathcal{l}(g(z)) ).
     * m_{real} and m_{gen} : the averages associated to the loss
       distributions.
     * another\_term : a term that is the unfortunate victim of 2
       assumptions. the first assumption is that the loss distributions
       are assumed to be normal (as said before) and the second assumption
       is that it stays roughtly constant. if (like me) you   re not so sure
       about these assumptions, you should know that the experimental
       results show that it   s fine.

   in the end, this is the simplified distance that we want to minimize:
   w(\mu_{real}, \mu_{gen})\propto {|m_{real} - m_{gen}|}
   and from this formulation we can derive the gan objective.

the complete began objective

   the reconstruction losses \mathcal{l}(x) and \mathcal{l}(g(z)) can only
   be positive, so as d wants to maximize the distance w(\mu_{real},
   \mu_{gen}) between the losses, it has only two choices:
     * either (case 1) it needs: m_{real} \to \infty and m_{gen} \to 0
     * or (case 2) it needs: m_{real} \to 0 and m_{gen} \to \infty

   case 1 doesn   t make sense of course because we want m_{real} to go 0
   (real images should be reconstructed perfectly). we can enforce case 2
   by applying the following id168 to d:
   \mathcal{l}_d = \mathcal{l}(x) - \mathcal{l}(g(z))

   g can then work adversarially to that by trying to minimize:
   \mathcal{l}_g = \mathcal{l}(g(z))

   in the end these losses (which are not the final ones) look very
   similar to those of the wgan except:
     * that we are matching distributions between losses,
     * and there is no need for the discriminative function to be
       k-lipschitz.

   but this is not the end of the story.

   in the next section i describe the second main contribution of the
   began paper: the diversity ratio \gamma . in short, the role of \gamma
      s is to balance the losses \mathcal{l}(x) and \mathcal{l}(g(z)) to
   stabilize the training. this is done in an adaptive fashion, during
   training, with the help of its surrogate k_t .

   let me first spoil you with the final solution and i   ll explain how
   \gamma and k_t work in the next section.

   here is the complete began objective:
   \mathcal{l}_d = \mathcal{l}(x) - k_t . \mathcal{l}(g(z))

   \mathcal{l}_g = \mathcal{l}(g(z))

   k_{t+1} = k_t + \lambda * (\gamma.\mathcal{l}(x) - \mathcal{l}(g(z))

   in this formulation:
     * \mathcal{l}_d and \mathcal{l}_g are the respective losses for d and
       g (what they try to minimize).
     * \mathcal{l}_d is only used to optimize \theta_d and \mathcal{l}_g
       is only used to optimize \theta_g .
     * \mathcal{l}(x) and \mathcal{l}(g(z)) are the losses of
       reconstruction of real and generated images.
     * \gamma is the diversity ratio (in [0,1] ) defined before as: \gamma
       = {\mathbb{e}[\mathcal{l}(g(z))]} / {\mathbb{e}[\mathcal{l}(x)]} .
     * k_t is the adaptive term that will allow us to balance the losses
       automagically.
     * \lambda is the proportional gain for k_t (aka the learning rate for
       k_t ).

the equilibrium term

   what we aim to do now is to balance the losses however we want. this
   will help us control the equilibrium between g and d in real time,
   stabilize the training, get an approximate measure of convergence and
   adjust the trade-off between image diversity and realism with just one
   hyperparameter. magic!

   the reconstruction losses are considered to be at equilibrium when:
   \mathbb{e}[\mathcal{l}(x)] = \mathbb{e}[\mathcal{l}(g(z))]

   and the thing is we don   t want this equilibrium to ever be reached
   because this would mean that d become incapable to distinguish
   generated samples from real ones. said differently this would mean g
   wins. for the training to go smoothly, neither network should win over
   the other.

   the strategy is to define a ratio \gamma between the 2 losses.
   \gamma =
   \frac{\mathbb{e}[\mathcal{l}(g(z))]}{\mathbb{e}[\mathcal{l}(x)]}

   \gamma is called the diversity ratio and it is should be in [0, 1] . it
   is always positive because the reconstruction losses are always
   positive. and it is below 1 in practice because we are going to make it
   so (because we want to have
   [png.latex?%5cmathcal%7bl%7d%28x%29%20%3e%20%5cmathcal%7bl%7d%28g%28z%2
   9%29] \mathcal{l}(g(z))    title=   \mathcal{l}(x) > \mathcal{l}(g(z))   
   />)

   we will make sure this ratio is maintained during the training in the
   next section, but first i would like to give you an intuition of the
   importance of \gamma .

   d has 2 competing goals: auto-encode real images and discriminate real
   from generated images. \gamma helps us balance these goals:
     * lower values of \gamma lead to \mathbb{e}[\mathcal{l}(x)] \gg
       \mathbb{e}[\mathcal{l}(g(z))] , which means d focuses more on its
       auto-encoding task, which means g has an incentive to produce more
       realistic images, which may be at the cost of image diversity.
     * higher values of \gamma lead to \mathbb{e}[\mathcal{l}(x)] =
       \mathbb{e}[\mathcal{l}(g(z))] , which means d focuses more on its
       discrimination task, which means g has an incentive to produce more
       diverse images (as diverse as the dataset), which may be at the
       cost of image quality.

   take a look at what happens with different values of the hyperparameter
   \gamma :

   [3_gamma_variation.png?w=452&#038;h=174]

balancing the losses

   the strategy, now, is to maintain the ratio \gamma between the 2
   reconstruction losses over time. in practice we can control \gamma by
   adding an adaptive term k_t .

   what makes the k_t adaptive is something called    proportional control
   theory   . it is a fancy name to describe what you   re doing when you   re
   driving at constant speed. if you   re going too fast you slow down
   proportionally to how much faster (than the cruise speed) you   re going.
   if you   re going too slow you accelerate proportionally to how much
   slower (than the cruise speed) you   re going.

   here, the ratio we want to maintain (the speed of the car) is the ratio
   \gamma defined as:
   \gamma =
   \frac{\mathbb{e}[\mathcal{l}(g(z))]}{\mathbb{e}[\mathcal{l}(x)]}
   so in a perfect world we should have exactly this during training:
   \gamma = \frac{\mathcal{l}(g(z))}{\mathcal{l}(x)}
   which is equivalent to:
   \gamma.\mathcal{l}(x) - {\mathcal{l}(g(z))} = 0

   in practice \gamma.\mathcal{l}(x) - {\mathcal{l}(g(z))} is never equal
   to 0 during training. this value represents how off we are from the
   stable point (this is how much faster or slower your car in really
   going).

   now that we have this information we need to control this ratio
   dynamically during the training (we need to maintain the correct speed
   while driving). and the factor that controls this ratio is k_t \in [0,
   1] through the formula: \mathcal{l}_d = \mathcal{l}(x) - k_t .
   \mathcal{l}(g(z)) .

   k_t is adapted in the right direction like this: k_{t+1} = k_t +
   \lambda * (\gamma.\mathcal{l}(x) - \mathcal{l}(g(z)) with \lambda being
   the learning rate that adapts k_t over time. \lambda is also called the
   proportional gain for k and in practice (in the experiments) it is set
   to 0.001.

   it is this formulation of k_t that justifies the complete began
   objective presented above.

consequences of balancing d and g during training

   training procedure: in most gans, d and g are trained alternatively.
   this is not the case with here. they can be trained simultaneously at
   each time step, because d and g are automatically balanced. the
   training is still adversarial, it is just simultaneous.

   there is no need to have k_t show up in \mathcal{l}_g : this is because
   the ratio \gamma (that k_t controls) is there to balance the goals of
   d. it has nothing to do with the generator which has only one
   objective. so, in short, g does its own thing and d adapts. d adapts
   how much it wants to focus on its discrimination task (in which case d
   gets better faster than g) relative to its auto-encoding task (in which
   case g gets better faster than d).

convergence measure

   a convenient consequence of this began formulation is that it makes the
   derivation of an approximate convergence measure possible.

   let   s look at the complete began objective from another point of view:
    1. \mathcal{l}(x) should go to 0 as images are reconstructed better
       and better after each time step.
    2. \gamma.\mathcal{l}(x) - {\mathcal{l}(g(z))} should stay close to 0
       (so that the losses are balanced).

   the combination of both points implies that the reconstruction losses
   get closer to 0 (and therefore to one another). this means that d has
   more and more trouble maximizing the distance between \mathcal{l}(x)
   and \mathcal{l}(g(z)) (by minimizing \mathcal{l}_d = \mathcal{l}(x) -
   k_t . \mathcal{l}(g(z)) ). said differently the task of discriminating
   real from generated images becomes more difficult as time goes on. and
   there is only one way that this happens: g is getting better!

   in short, the combination of (1) and (2) implies that \mathcal{l}(g(z))
   goes to 0 and that p_{g(z)} gets closer to p_x .

   now here is the awesome part: (1) and (2) have such simple forms that
   we can simply add them up to get the following convergence measure:
   m_{global} = \mathcal{l}(x) + |\gamma.\mathcal{l}(x) -
   {\mathcal{l}(g(z))}|

   why is this awesome? because global measures of gan convergence are
   hard to come by. there isn   t any in the typical gan setup and to my
   knowledge only the wgan provides one.

   but now there is a way to know if the network converges or collapses:

   [4_gamma_convergence.png?w=428&#038;h=141]

critical implementation details

   model architecture:
     * both the generator and the decoder are deep deconvolutionals with
       identical architectures but different weights. the encoder and the
       decoder have    opposite    architectures.
     * downsampling is done with strided convolutions, just like in the
       dcgan (cf [36]this awesome blog post for an explanation of what
       strided convolutions are).
     * upsampling is done with nearest-neighbors.
     * the non-linearities are exponential linear units (elus) (by
       opposition to a combination of relus and leakyrelus in the dcgan).
     * the embedding state h (aka the bottleneck of the autoencoder) is
       not connected to non-linearities.
     * z (the random input of g) is sampled uniformly in [-1, 1]^n .

   [5_net_architecture.png?w=460&#038;h=254]

   interpolation: to display the interpolations of real images in latent
   space (figure 4) the authors needed to obtain the embedding z of real
   images. this is done by training directly on the z alone by providing
   the real image as a target of the generator. it is an l1 loss that is
   minimized in this case (between the generated image and the target
   image): e_r = |x - g(z)| . h cannot be used in place of z because there
   is nothing that constrains h to converge toward z .

   [6_interpolations.png?w=464&#038;h=156]

   overview: in the end, the architecture is pretty simple. as they say in
   the paper:    no batch id172, no dropout, no transpose
   convolutions and no exponential growth for convolution filters   . any
   of those techniques, though, may improve the results even more.

   the secret trick: in figure 2 of the paper the authors put side to side
   the results from the ebgan and those from the began. the began results
   look much much better than the ebgan results, but you should keep in
   mind that the models were not trained on the same datasets.

beauty

   as a side note: it is common knowledge that    averageness    is a strong
   indicator of beauty (there are fourteen sources to corroborate this
   statement on the wikipedia page for [37]beauty). the idea is that the
   more you blend faces together the more beautiful they look.

   the faces generated by began are not just visually realistic they also
   seem to be very attractive (the authors even claim that they saw few
   older people and more women than men). my understanding is that g could
   have developed a good representation of the    average    face to be more
   efficient, and the beauty of the generated faces could be a consequence
   of this.

points to take home

   what began can do:
     * it makes possible the generation of the    first anatomically
       coherent    face images at a resolution of 128  128.
     * its convergence is fast and stable even in the absence of batch
       norm.
     * it also offers an approximate measure of convergence.

   how began works:
     * began uses an autoencoder as the discriminator (just like in the
       ebgan).
     * the losses used to update \theta_g and \theta_d are a function of
       the quality of reconstruction achieved by d.
     * matching the distributions of the reconstruction losses is assumed
       to be a suitable proxy for matching the data distributions.
     * the trade-off between image diversity and quality of generation is
       automatically balanced by maintaining the ratio of the
       reconstruction losses close to an hyperparameter \gamma .
     * when \gamma is low, there is less variety and more realism. when
       \gamma is high, there is more variety and less realism.

bonus: began vs improved wgan

   began or improved wgan, which one is better?

   first, they have a lot in common:
     * both models have a training procedure that leads to a fast and
       stable convergence.
     * both models provide an approximate measure of how efficient g
       becomes over time.
     * and both models successfully make the use of batch id172 in
       d unnecessary.

   the [38]improved wgan paper is impressive mostly because:
     * the authors managed to avoid the vanishing/exploding gradient
       problem by adding a constraint on the norm of the gradients.
     * and they trained many different gan architectures including
       101-layer resnets as well as a language model over discrete data.

   the [39]began paper is impressive mostly because:
     * the authors try to make loss distributions match (by opposition to
       making data distributions match).
     * this leads to a way to automatically balance g against d during
       training as well image diversity versus visual quality.

   so which one is better? it doesn   t matter. everybody is a winner!

footnote

   the author (that   s me) would like to thank charles ollion for his very
   insightful comments and advice.

   here is [40]heuritech   s website

   here is the [41]reddit page associated to the paper.

partager :

     * [42]partager sur twitter(ouvre dans une nouvelle fen  tre)
     * [43]cliquer pour partager sur facebook(ouvre dans une nouvelle
       fen  tre)
     * [44]cliquez pour partager sur linkedin(ouvre dans une nouvelle
       fen  tre)
     * [45]partager sur reddit(ouvre dans une nouvelle fen  tre)
     * [46]cliquer pour partager sur tumblr(ouvre dans une nouvelle
       fen  tre)
     * [47]cliquez pour partager sur pinterest(ouvre dans une nouvelle
       fen  tre)
     * [48]cliquez pour envoyer par email    un ami(ouvre dans une nouvelle
       fen  tre)
     * [49]cliquer pour imprimer(ouvre dans une nouvelle fen  tre)
     *

wordpress:

   j'aime chargement   

sur le m  me th  me

     tiquettes : [50]began, [51]face generation, [52]gans, [53]generative
   adversarial networks, [54]state of the art

navigation des articles

   [55]    heuritech deep learning meetup #8, book your seats!
   [56]heuritech party : can   t wait for the 2nd edition!    

une r  ponse       began: state of the art generation of faces with generative
adversarial networks   

    1. pingback: [57]d423: began: id3 faces
       generation | ai:mechanic  

r  pondre [58]annuler la r  ponse.

   entrez votre commentaire...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   entrez vos coordonn  es ci-dessous ou cliquez sur une ic  ne pour vous
   connecter:
     *
     *
     *
     *
     *

   [59]gravatar
   e-mail (obligatoire) (adresse strictement confidentielle)
   ____________________
   nom (obligatoire)
   ____________________
   site web
   ____________________
   logo wordpress.com

   vous commentez    l'aide de votre compte wordpress.com.
   ( [60]d  connexion /  [61]changer )
   photo google

   vous commentez    l'aide de votre compte google. ( [62]d  connexion /
   [63]changer )
   image twitter

   vous commentez    l'aide de votre compte twitter. ( [64]d  connexion /
   [65]changer )
   photo facebook

   vous commentez    l'aide de votre compte facebook. ( [66]d  connexion /
   [67]changer )
   [68]annuler

   connexion    %s

   [ ] avertissez-moi par e-mail des nouveaux commentaires.

   [ ] avertissez-moi par e-mail des nouveaux articles.

   laisser un commentaire

   recherche ____________________ recherche

s'abonner au blog

   entrez votre adresse mail pour suivre ce blog et   tre notifi   par email
   des nouvelles publications.

   rejoignez 217 autres abonn  s

   ____________________

   (button) suivre

liens

     * [69]notre cha  ne youtube
     * [70]notre page facebook
     * [71]notre page twitter
     * [72]notre site corporate

cat  gories

     * [73]big data
     * [74]fashiontech
     * [75]heuritech   v  nements
     * [76]heuritech inside
     * [77]heuritech news
     * [78]heuritech t  moignages
     * [79]machine learning
     * [80]non class  
     * [81]open data
     * [82]r&d
     * [83]techno
     * [84]tests et benchmark
     * [85]tuto et rex

mots-cl  s

   [86]accelerator [87]algorithmes g  n  tiques [88]amazon [89]analyse
   pr  dictive [90]analytics [91]apache [92]artificial intelligence [93]aws
   [94]beauty [95]benchmark [96]big data [97]bnp [98]c++ [99]cassandra
   [100]centos [101]certification [102]challenge [103]cloud [104]cluster
   [105]common crawl [106]convolutional nets [107]convolutional neural
   network [108]crawling [109]data intelligence platform [110]data
   management platform [111]datamining [112]data science [113]dataset
   [114]deep learning [115]dip [116]dmp [117]ebg [118]eclipse [119]epitech
   [120]facebook [121]fashion [122]films [123]gpu [124]grand compte
   [125]grand groupe [126]groupe la poste [127]hackathon [128]hadoop
   [129]heuritech [130]heuritechapi [131]heuritechdip [132]icml [133]image
   classification [134]innovation [135]inria [136]intelligence
   artificielle [137]ipython notebook [138]lecun [139]lip6
   [140]louisvuitton [141]machine learning [142]map reduce [143]marketing
   [144]matthieu cord [145]meetup [146]mongodb [147]multimodal embadding
   [148]multimodal embedding [149]mysql [150]neural network [151]nlp
   [152]nutch [153]plotting [154]predictive model [155]ramp
   [156]representation learning [157]research [158]retail [159]rex
   [160]running [161]salt [162]sbt [163]scala [164]scrapy [165]semantic
   [166]spark [167]sport [168]start'inpost [169]startup [170]stn
   [171]s  mantique [172]team [173]technology [174]techrun [175]theano
   [176]top100 [177]torch [178]training sprint [179]tutorial [180]web
   crawling [181]id97 [182]id27s [183]workshop [184]worshop
   [185]yann lecun

articles phares

     * [186]attention mechanism
     * [187]knowledge extraction from unstructured texts
     * [188]a brief report of the heuritech deep learning meetup #5
     * [189]began: state of the art generation of faces with generative
       adversarial networks
     * [190]choosing a web crawler
     * [191]mutan: multimodal tucker fusion for visual id53
     * [192]heuritech at insect image classification at paris-saclay
       data-science challenge!
     * [193]practical analytics with spark and cassandra
     * [194]how to install nutch on an aws ec2 cluster
     * [195]building a centos 6 ami with cloud-init and automatic resizing
       of the root partition

suivez-nous sur twitter

   [196]mes tweets

   75 rue de la roquette
   75011 paris
   01 83 56 01 15
   info@heuritech.com
   heuritech.com
   rcs paris b 794 196 055

auteurs

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

s'abonner au blog

   entrez votre adresse mail pour suivre ce blog et   tre notifi   par email
   des nouvelles publications.

   rejoignez 217 autres abonn  s

   ____________________

   (button) suivre

credits

   photos :   ursine schmitt   jaimy corcos   123rf

articles r  cents

     * [197]why id161 apis won   t do the trick for verticalized
       applications. heuritech   s take in fashion 12 mars 2018
     * [198]mutan: multimodal tucker fusion for visual id53
       13 d  cembre 2017
     * [199]deep learning lectures at paris-saclay datascience master 29
       mai 2017
     * [200]heuritech party : can   t wait for the 2nd edition! 4 mai 2017
     * [201]began: state of the art generation of faces with generative
       adversarial networks 11 avril 2017
     * [202]heuritech deep learning meetup #8, book your seats! 8 f  vrier
       2017
     * [203]heuritech, deep learning expert, raises 1,1m    from serena
       capital & ba 17 janvier 2017
     * [204]heuritech launches an ai solution for fashion & beauty! 16
       janvier 2017
     * [205]l   ia pour d  tecter les tendances de demain: l   interview de
       tony pinville sur milkshakevalley 5 janvier 2017
     * [206]heuritech deep learning meetup #7: more than 100 attendees for
       convolutionnal neural networks 3 novembre 2016

flus rss

     * [207]rss - articles
     * [208]rss - commentaires

archives

     * [209]mars 2018
     * [210]d  cembre 2017
     * [211]mai 2017
     * [212]avril 2017
     * [213]f  vrier 2017
     * [214]janvier 2017
     * [215]novembre 2016
     * [216]avril 2016
     * [217]mars 2016
     * [218]f  vrier 2016
     * [219]janvier 2016
     * [220]d  cembre 2015
     * [221]novembre 2015
     * [222]octobre 2015
     * [223]septembre 2015
     * [224]ao  t 2015
     * [225]juillet 2015
     * [226]juin 2015
     * [227]mai 2015
     * [228]avril 2015
     * [229]mars 2015
     * [230]f  vrier 2015
     * [231]janvier 2015
     * [232]d  cembre 2014
     * [233]novembre 2014
     * [234]octobre 2014
     * [235]septembre 2014
     * [236]juillet 2014
     * [237]novembre 2013
     * [238]septembre 2013

   [239]cr  ez un site ou un blog sur wordpress.com

   envoyer    l'adresse email ____________________ votre nom
   ____________________ votre adresse e-mail ____________________
   _________________________
   loading envoyer un e-mail [240]annuler
   l'article n'a pas   t   envoy   - v  rifiez vos adresses email !
   la v  rification e-mail a   chou  , veuillez r  essayer
   impossible de partager les articles de votre blog par email.

   iframe: [241]likes-master

   %d blogueurs aiment cette page :

references

   visible links
   1. https://blog.heuritech.com/feed/
   2. https://blog.heuritech.com/comments/feed/
   3. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/feed/
   4. https://blog.heuritech.com/2017/02/08/heuritech-deep-learning-meetup-8-book-your-seats/
   5. https://blog.heuritech.com/2017/05/04/heuritech-party-cant-wait-for-the-2nd-edition/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/&for=wpcom-auto-discovery
   8. https://blog.heuritech.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://blog.heuritech.com/
  11. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/#content
  12. http://www.heuritech.com/
  13. https://www.youtube.com/channel/ucf65w-sgtjfdi3warfwtpwg
  14. https://twitter.com/heuritechdata
  15. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/
  16. https://blog.heuritech.com/author/namelessusers/
  17. https://blog.heuritech.com/category/machine-learning/
  18. https://blog.heuritech.com/category/rd/
  19. https://arxiv.org/abs/1703.10717
  20. https://arxiv.org/abs/1609.03126
  21. https://www.reddit.com/r/machinelearning/comments/64rayf/
  22. https://www.reddit.com/r/machinelearning/comments/633jal/
  23. https://arxiv.org/abs/1704.00028
  24. https://arxiv.org/abs/1609.07093
  25. https://arxiv.org/abs/1609.04802
  26. https://arxiv.org/abs/1605.09782
  27. https://arxiv.org/abs/1703.10593
  28. http://www2.heuritech.com/
  29. https://arxiv.org/abs/1703.10717
  30. https://arxiv.org/abs/1406.2661
  31. https://arxiv.org/abs/1704.00028
  32. https://arxiv.org/abs/1701.07875
  33. https://arxiv.org/abs/1611.04076
  34. https://arxiv.org/abs/1701.06264
  35. https://arxiv.org/abs/1609.03126
  36. https://adeshpande3.github.io/a-beginner's-guide-to-understanding-convolutional-neural-networks-part-2/
  37. https://en.wikipedia.org/wiki/beauty
  38. https://arxiv.org/abs/1704.00028
  39. https://arxiv.org/abs/1703.10717
  40. http://www2.heuritech.com/
  41. https://www.reddit.com/r/machinelearning/comments/633jal/
  42. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/?share=twitter
  43. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/?share=facebook
  44. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/?share=linkedin
  45. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/?share=reddit
  46. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/?share=tumblr
  47. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/?share=pinterest
  48. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/?share=email
  49. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/#print
  50. https://blog.heuritech.com/tag/began/
  51. https://blog.heuritech.com/tag/face-generation/
  52. https://blog.heuritech.com/tag/gans/
  53. https://blog.heuritech.com/tag/generative-adversarial-networks/
  54. https://blog.heuritech.com/tag/state-of-the-art/
  55. https://blog.heuritech.com/2017/02/08/heuritech-deep-learning-meetup-8-book-your-seats/
  56. https://blog.heuritech.com/2017/05/04/heuritech-party-cant-wait-for-the-2nd-edition/
  57. http://www.aimechanic.com/2017/04/11/d423-began-generative-adversarial-networks-faces-generation/
  58. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/#respond
  59. https://gravatar.com/site/signup/
  60. javascript:highlandercomments.doexternallogout( 'wordpress' );
  61. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/
  62. javascript:highlandercomments.doexternallogout( 'googleplus' );
  63. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/
  64. javascript:highlandercomments.doexternallogout( 'twitter' );
  65. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/
  66. javascript:highlandercomments.doexternallogout( 'facebook' );
  67. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/
  68. javascript:highlandercomments.cancelexternalwindow();
  69. https://www.youtube.com/channel/ucf65w-sgtjfdi3warfwtpwg
  70. https://www.facebook.com/heuritech
  71. https://twitter.com/heuritechdata
  72. http://www.heuritech.com/
  73. https://blog.heuritech.com/category/big-data/
  74. https://blog.heuritech.com/category/fashiontech/
  75. https://blog.heuritech.com/category/heuritech-evenements/
  76. https://blog.heuritech.com/category/heuritech-inside/
  77. https://blog.heuritech.com/category/heuritech-news/
  78. https://blog.heuritech.com/category/heuritech-temoignages/
  79. https://blog.heuritech.com/category/machine-learning/
  80. https://blog.heuritech.com/category/non-classe/
  81. https://blog.heuritech.com/category/open-data/
  82. https://blog.heuritech.com/category/rd/
  83. https://blog.heuritech.com/category/techno/
  84. https://blog.heuritech.com/category/tests-et-benchmark/
  85. https://blog.heuritech.com/category/tuto-et-rex/
  86. https://blog.heuritech.com/tag/accelerator/
  87. https://blog.heuritech.com/tag/algorithmes-genetiques/
  88. https://blog.heuritech.com/tag/amazon/
  89. https://blog.heuritech.com/tag/analyse-predictive/
  90. https://blog.heuritech.com/tag/analytics/
  91. https://blog.heuritech.com/tag/apache/
  92. https://blog.heuritech.com/tag/artificial-intelligence/
  93. https://blog.heuritech.com/tag/aws/
  94. https://blog.heuritech.com/tag/beauty/
  95. https://blog.heuritech.com/tag/benchmark/
  96. https://blog.heuritech.com/tag/big-data-2/
  97. https://blog.heuritech.com/tag/bnp/
  98. https://blog.heuritech.com/tag/c/
  99. https://blog.heuritech.com/tag/cassandra/
 100. https://blog.heuritech.com/tag/centos/
 101. https://blog.heuritech.com/tag/certification/
 102. https://blog.heuritech.com/tag/challenge/
 103. https://blog.heuritech.com/tag/cloud/
 104. https://blog.heuritech.com/tag/cluster/
 105. https://blog.heuritech.com/tag/common-crawl/
 106. https://blog.heuritech.com/tag/convolutional-nets/
 107. https://blog.heuritech.com/tag/convolutional-neural-network/
 108. https://blog.heuritech.com/tag/crawling/
 109. https://blog.heuritech.com/tag/data-intelligence-platform/
 110. https://blog.heuritech.com/tag/data-management-platform/
 111. https://blog.heuritech.com/tag/datamining/
 112. https://blog.heuritech.com/tag/data-science/
 113. https://blog.heuritech.com/tag/dataset/
 114. https://blog.heuritech.com/tag/deep-learning/
 115. https://blog.heuritech.com/tag/dip/
 116. https://blog.heuritech.com/tag/dmp/
 117. https://blog.heuritech.com/tag/ebg/
 118. https://blog.heuritech.com/tag/eclipse/
 119. https://blog.heuritech.com/tag/epitech/
 120. https://blog.heuritech.com/tag/facebook/
 121. https://blog.heuritech.com/tag/fashion/
 122. https://blog.heuritech.com/tag/films/
 123. https://blog.heuritech.com/tag/gpu/
 124. https://blog.heuritech.com/tag/grand-compte/
 125. https://blog.heuritech.com/tag/grand-groupe/
 126. https://blog.heuritech.com/tag/groupe-la-poste/
 127. https://blog.heuritech.com/tag/hackathon/
 128. https://blog.heuritech.com/tag/hadoop/
 129. https://blog.heuritech.com/tag/heuritech/
 130. https://blog.heuritech.com/tag/heuritechapi/
 131. https://blog.heuritech.com/tag/heuritechdip/
 132. https://blog.heuritech.com/tag/icml/
 133. https://blog.heuritech.com/tag/image-classification/
 134. https://blog.heuritech.com/tag/innovation/
 135. https://blog.heuritech.com/tag/inria/
 136. https://blog.heuritech.com/tag/intelligence-artificielle/
 137. https://blog.heuritech.com/tag/ipython-notebook/
 138. https://blog.heuritech.com/tag/lecun/
 139. https://blog.heuritech.com/tag/lip6/
 140. https://blog.heuritech.com/tag/louisvuitton/
 141. https://blog.heuritech.com/tag/machine-learning-2/
 142. https://blog.heuritech.com/tag/map-reduce/
 143. https://blog.heuritech.com/tag/marketing/
 144. https://blog.heuritech.com/tag/matthieu-cord/
 145. https://blog.heuritech.com/tag/meetup/
 146. https://blog.heuritech.com/tag/mongodb/
 147. https://blog.heuritech.com/tag/multimodal-embadding/
 148. https://blog.heuritech.com/tag/multimodal-embedding/
 149. https://blog.heuritech.com/tag/mysql/
 150. https://blog.heuritech.com/tag/neural-network/
 151. https://blog.heuritech.com/tag/nlp/
 152. https://blog.heuritech.com/tag/nutch/
 153. https://blog.heuritech.com/tag/plotting/
 154. https://blog.heuritech.com/tag/predictive-model/
 155. https://blog.heuritech.com/tag/ramp/
 156. https://blog.heuritech.com/tag/representation-learning/
 157. https://blog.heuritech.com/tag/research/
 158. https://blog.heuritech.com/tag/retail/
 159. https://blog.heuritech.com/tag/rex/
 160. https://blog.heuritech.com/tag/running/
 161. https://blog.heuritech.com/tag/salt/
 162. https://blog.heuritech.com/tag/sbt/
 163. https://blog.heuritech.com/tag/scala/
 164. https://blog.heuritech.com/tag/scrapy/
 165. https://blog.heuritech.com/tag/semantic/
 166. https://blog.heuritech.com/tag/spark/
 167. https://blog.heuritech.com/tag/sport/
 168. https://blog.heuritech.com/tag/startinpost/
 169. https://blog.heuritech.com/tag/startup/
 170. https://blog.heuritech.com/tag/stn/
 171. https://blog.heuritech.com/tag/semantique/
 172. https://blog.heuritech.com/tag/team/
 173. https://blog.heuritech.com/tag/technology/
 174. https://blog.heuritech.com/tag/techrun/
 175. https://blog.heuritech.com/tag/theano/
 176. https://blog.heuritech.com/tag/top100/
 177. https://blog.heuritech.com/tag/torch/
 178. https://blog.heuritech.com/tag/training-sprint/
 179. https://blog.heuritech.com/tag/tutorial/
 180. https://blog.heuritech.com/tag/web-crawling/
 181. https://blog.heuritech.com/tag/id97/
 182. https://blog.heuritech.com/tag/word-embeddings/
 183. https://blog.heuritech.com/tag/workshop/
 184. https://blog.heuritech.com/tag/worshop/
 185. https://blog.heuritech.com/tag/yann-lecun/
 186. https://blog.heuritech.com/2016/01/20/attention-mechanism/
 187. https://blog.heuritech.com/2016/04/15/knowledge-extraction-from-unstructured-texts/
 188. https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/
 189. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/
 190. https://blog.heuritech.com/2015/05/22/choosing-a-web-crawler/
 191. https://blog.heuritech.com/2017/12/13/mutan-multimodal-tucker-fusion-for-visual-question-answering/
 192. https://blog.heuritech.com/2015/10/27/first-on-insect-image-classification-at-paris-saclay-data-science-challenge/
 193. https://blog.heuritech.com/2015/03/31/practical-analytics-with-spark-and-cassandra/
 194. https://blog.heuritech.com/2015/06/25/how-to-install-nutch-on-an-aws-ec2-cluster/
 195. https://blog.heuritech.com/2015/08/06/creating-a-centos-6-ami-with-cloud-init-and-auto-resize-of-the-root-partition-at-boot/
 196. https://twitter.com/571449260990685184
 197. https://blog.heuritech.com/2018/03/12/why-computer-vision-apis-wont-do-the-trick-for-verticalized-applications-heuritechs-take-in-fashion/
 198. https://blog.heuritech.com/2017/12/13/mutan-multimodal-tucker-fusion-for-visual-question-answering/
 199. https://blog.heuritech.com/2017/05/29/deep-learning-lectures-at-paris-saclay-datascience-master/
 200. https://blog.heuritech.com/2017/05/04/heuritech-party-cant-wait-for-the-2nd-edition/
 201. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/
 202. https://blog.heuritech.com/2017/02/08/heuritech-deep-learning-meetup-8-book-your-seats/
 203. https://blog.heuritech.com/2017/01/17/heuritech-deep-learning-expert-rises-11me-from-serena-capital-ba/
 204. https://blog.heuritech.com/2017/01/16/heuritech-launches-an-ai-solution-for-fashion-beauty/
 205. https://blog.heuritech.com/2017/01/05/lia-pour-detecter-les-tendances-de-demain-linterview-de-tony-pinville-sur-milkshakevalley/
 206. https://blog.heuritech.com/2016/11/03/heuritech-deep-learning-meetup-7-more-than-100-attendees-for-convolutionnal-neural-networks/
 207. https://blog.heuritech.com/feed/
 208. https://blog.heuritech.com/comments/feed/
 209. https://blog.heuritech.com/2018/03/
 210. https://blog.heuritech.com/2017/12/
 211. https://blog.heuritech.com/2017/05/
 212. https://blog.heuritech.com/2017/04/
 213. https://blog.heuritech.com/2017/02/
 214. https://blog.heuritech.com/2017/01/
 215. https://blog.heuritech.com/2016/11/
 216. https://blog.heuritech.com/2016/04/
 217. https://blog.heuritech.com/2016/03/
 218. https://blog.heuritech.com/2016/02/
 219. https://blog.heuritech.com/2016/01/
 220. https://blog.heuritech.com/2015/12/
 221. https://blog.heuritech.com/2015/11/
 222. https://blog.heuritech.com/2015/10/
 223. https://blog.heuritech.com/2015/09/
 224. https://blog.heuritech.com/2015/08/
 225. https://blog.heuritech.com/2015/07/
 226. https://blog.heuritech.com/2015/06/
 227. https://blog.heuritech.com/2015/05/
 228. https://blog.heuritech.com/2015/04/
 229. https://blog.heuritech.com/2015/03/
 230. https://blog.heuritech.com/2015/02/
 231. https://blog.heuritech.com/2015/01/
 232. https://blog.heuritech.com/2014/12/
 233. https://blog.heuritech.com/2014/11/
 234. https://blog.heuritech.com/2014/10/
 235. https://blog.heuritech.com/2014/09/
 236. https://blog.heuritech.com/2014/07/
 237. https://blog.heuritech.com/2013/11/
 238. https://blog.heuritech.com/2013/09/
 239. https://wordpress.com/?ref=footer_custom_svg
 240. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/#cancel
 241. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321&lang=fr

   hidden links:
 243. https://blog.heuritech.com/
 244. https://blog.heuritech.com/
 245. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/#comment-form-guest
 246. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/#comment-form-load-service:wordpress.com
 247. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/#comment-form-load-service:twitter
 248. https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/#comment-form-load-service:facebook
 249. https://blog.heuritech.com/author/apriami/
 250. https://blog.heuritech.com/author/ardparant/
 251. https://blog.heuritech.com/author/celiaponcelinheuritech/
 252. https://blog.heuritech.com/author/charlottefann/
 253. https://blog.heuritech.com/author/collion/
 254. https://blog.heuritech.com/author/dmarin86/
 255. https://blog.heuritech.com/author/hediby/
 256. https://blog.heuritech.com/author/heuritech/
 257. https://blog.heuritech.com/author/namelessusers/
 258. https://blog.heuritech.com/author/ramealex/
 259. https://blog.heuritech.com/author/tonelli2015/
 260. https://blog.heuritech.com/author/tpinville/
 261. https://blog.heuritech.com/author/uccianir/
