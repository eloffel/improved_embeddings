   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as slides
     * [5]view as code
     * [6]python 3 kernel
     * [7]view on github
     * [8]execute on binder
     * [9]download notebook

    1. [10]evolutionary-computation-course
    2. [11]aec.04 - evolutionary strategies and covariance matrix
       adaptation.ipynb

   [12]uff logo [13]ic logo

understanding evolutionary strategies and covariance matrix adaptation[14]  

luis mart  , [15]ic/[16]uff[17]  

   [18]http://lmarti.com; [19][email protected]

   [20]advanced evolutionary computation: theory and practice

   the notebook is better viewed rendered as slides. you can convert it to
   slides and view them by:
     * using [21]nbconvert with a command like:
$ ipython nbconvert --to slides --post serve <this-notebook-name.ipynb>

     * installing [22]reveal.js - jupyter/ipython slideshow extension
     * using the online [23]ipython notebook slide viewer (some slides of
       the notebook might not be properly rendered).

   this and other related ipython notebooks can be found at the course
   github repository:
     * [24]https://github.com/lmarti/evolutionary-computation-course

   in [61]:
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
from matplotlib import cm
from mpl_toolkits.mplot3d import axes3d
from scipy.stats import norm, multivariate_normal
import math

%matplotlib inline
%config inlinebackend.figure_format = 'retina'
plt.rc('text', usetex=true)
plt.rc('font', family='serif')
plt.rcparams['text.latex.preamble'] ='\\usepackage{libertine}\n\\usepackage[utf8
]{inputenc}'

import seaborn
seaborn.set(style='whitegrid')
seaborn.set_context('notebook')

statistics recap[25]  

     * [26]random variable: a variable whose value is subject to
       variations due to chance. a random variable can take on a set of
       possible different values, each with an associated id203, in
       contrast to other mathematical variables.
     * [27]id203 distribution: mathematical function describing the
       possible values of a random variable and their associated
       probabilities.
     * [28]id203 density function (pdf) of a continuous random
       variable is a function that describes the relative likelihood for
       this random variable to take on a given value.
          + the id203 of the random variable falling within a
            particular range of values is given by the integral of this
            variable   s density over that range.
          + the id203 density function is nonnegative everywhere,
            and its integral over the entire space is equal to one.

   [the_normal_distribution.svg]

[29]moments[30]  

   the id203 distribution of a random variable is often
   characterised by a small number of parameters, which also have a
   practical interpretation.
     * [31]mean (a.k.a expected value) refers to one measure of the
       central tendency either of a id203 distribution or of the
       random variable characterized by that distribution.
          + population mean: $\mu = \operatorname{e}[x]$.
          + estimation of sample mean: $\bar{x}$.
     * [32]standard deviation measures the amount of variation or
       dispersion from the mean.
          + population deviation: $$ \sigma = \sqrt{\operatorname
            e[x^2]-(\operatorname e[x])^2} = \sqrt{\frac{1}{n}
            \sum_{i=1}^n (x_i - \mu)^2}. $$
          + unbiased estimator: $$ s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i -
            \overline{x})^2. $$

two samples[33]  

   in [62]:
sample1 = np.random.normal(0, 0.5, 1000)
sample2 = np.random.normal(1,1,500)

   in [63]:
def plot_normal_sample(sample, mu, sigma):
    'plots an histogram and the normal distribution corresponding to the paramet
ers.'
    x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)
    plt.plot(x, norm.pdf(x, mu, sigma), 'b', lw=2)
    plt.hist(sample, 30, normed=true, alpha=0.2)
    plt.annotate('3$\sigma$',
                     xy=(mu + 3*sigma, 0),  xycoords='data',
                     xytext=(0, 100), textcoords='offset points',
                     fontsize=15,
                     arrowprops=dict(arrowstyle="->",
                                    connectionstyle="arc,anglea=180,arma=20,angl
eb=90,armb=15,rad=7"))
    plt.annotate('-3$\sigma$',
                     xy=(mu -3*sigma, 0), xycoords='data',
                     xytext=(0, 100), textcoords='offset points',
                     fontsize=15,
                     arrowprops=dict(arrowstyle="->",
                                     connectionstyle="arc,anglea=180,arma=20,ang
leb=90,armb=15,rad=7"))

   in [64]:
plt.figure(figsize=(11,4))
plt.subplot(121)
plot_normal_sample(sample1, 0, 0.5)
plt.title('sample 1: $\mu=0$, $\sigma=0.5$')
plt.subplot(122)
plot_normal_sample(sample2, 1, 1)
plt.title('sample 2: $\mu=1$, $\sigma=1$')
plt.tight_layout();

   [wfbj636jvcuiqaaaabjru5erkjggg== ]
   in [65]:
print('sample 1; estimated mean:', sample1.mean(), ' and std. dev.: ', sample1.s
td())
print('sample 2; estimated mean:', sample2.mean(), ' and std. dev.: ', sample2.s
td())

sample 1; estimated mean: -0.000176330861078  and std. dev.:  0.472785600332
sample 2; estimated mean: 1.00435499156  and std. dev.:  0.969981727482

   [34]covariance is a measure of how much two random variables change
   together. $$ \operatorname{cov}(x,y) = \operatorname{e}{\big[(x -
   \operatorname{e}[x])(y - \operatorname{e}[y])\big]}, $$ $$
   \operatorname{cov}(x,x) = s(x), $$
     * the sign of the covariance therefore shows the tendency in the
       linear relationship between the variables.
     * the magnitude of the covariance is not easy to interpret.
     * the normalized version of the covariance, the correlation
       coefficient, however, shows by its magnitude the strength of the
       linear relation.

understanding covariance[35]  

   in [66]:
sample_2d = np.array(list(zip(sample1, np.ones(len(sample1))))).t

   in [67]:
plt.scatter(sample_2d[0,:], sample_2d[1,:], marker='x');

   [e6gbaaagwx9wpqaaaadyrai1aaaajbcoaqaaiifadqaaaakeagaaaegguamaaeac
   groaaaascnqaaacqqkagaacabai1aaaajbcoaqaaiifadqaaaakeagaaaegguamaaeacgro
   aaaas cnqaaacq4p8amrn3cjwntfwaaaaasuvork5cyii= ]
   in [68]:
np.cov(sample_2d) # computes covariance between the two components of the sample

   out[68]:
array([[ 0.22374997,  0.        ],
       [ 0.        ,  0.        ]])

   as the sample is only distributed along one axis, the covariance does
   not detects any relationship between them.

   what happens when we rotate the sample?
   in [69]:
def rotate_sample(sample, angle=-45):
    'rotates a sample by `angle` degrees.'
    theta = (angle/180.) * np.pi
    rot_matrix = np.array([[np.cos(theta), -np.sin(theta)],
                           [np.sin(theta), np.cos(theta)]])
    return sample.t.dot(rot_matrix).t

   in [70]:
rot_sample_2d = rotate_sample(sample_2d)

   in [71]:
plt.scatter(rot_sample_2d[0,:], rot_sample_2d[1,:], marker='x');

   [a0rvtaaaaaelftksuqmcc ]
   in [72]:
np.cov(rot_sample_2d)

   out[72]:
array([[ 0.11187499,  0.11187499],
       [ 0.11187499,  0.11187499]])

a two-dimensional normally-distributed variable[36]  

   in [74]:
mu = [0,1]
cov = [[1,0],[0,0.2]] # diagonal covariance, points lie on x or y-axis
sample = np.random.multivariate_normal(mu,cov,1000).t
plt.scatter(sample[0], sample[1], marker='x', alpha=0.29)

estimated_mean = sample.mean(axis=1)
estimated_cov = np.cov(sample)
e_x,e_y = np.random.multivariate_normal(estimated_mean,estimated_cov,500).t

plt.plot(e_x,e_y,'rx', alpha=0.47)
x, y = np.mgrid[-4:4:.01, -1:3:.01]
pos = np.empty(x.shape + (2,))
pos[:, :, 0] = x; pos[:, :, 1] = y
rv = multivariate_normal(estimated_mean, estimated_cov)
plt.contour(x, y, rv.pdf(pos), cmap=cm.viridis_r, lw=4)
plt.axis('equal');

   [ hxiofkvxe4l4aaaaaelftksuqmcc ]

this is better understood in 3d[37]  

   in [76]:
fig = plt.figure(figsize=(11,5))
ax = fig.gca(projection='3d')
ax.plot_surface(x, y, rv.pdf(pos), cmap=cm.viridis_r, rstride=30, cstride=10, li
newidth=1, alpha=0.47)
ax.plot_wireframe(x, y, rv.pdf(pos), linewidth=0.47, alpha=0.47)
ax.scatter(e_x, e_y, 0.4, marker='.', alpha=0.47)
ax.axis('tight');

   [ax2c6ygxvhkfaaaaaelf tksuqmcc ]

   again, what happens if we rotate the sample?
   in [77]:
rot_sample = rotate_sample(sample)
estimated_mean = rot_sample.mean(axis=1)
estimated_cov = np.cov(rot_sample)
e_x,e_y = np.random.multivariate_normal(estimated_mean,estimated_cov,500).t

   in [78]:
fig = plt.figure(figsize=(11,4))
plt.subplot(121)
plt.scatter(rot_sample[0,:], rot_sample[1,:], marker='x', alpha=0.7)
plt.title('"original" data')
plt.axis('equal')
plt.subplot(122)
plt.scatter(e_x, e_y, marker='o', color='g', alpha=0.7)
plt.title('sampled data')
plt.axis('equal');

   [2qpl9lgg3nmjup92zaunvrsw0i
   dqcamy6odh08efdsyhyi1joslkpsnecylwsa4kd4caaaaaaaamcund8daaaaaaaaafcyca8
   baaaa
   aaaamci8baaaaaaaagck8baaaaaaaacakcjdaaaaaaaaakyidweaaaaaaacyijweaaaaaaa
   ayirw
   eaaaaaaaaiapwkmaaaaaaaaapggpaqaaaaaaajgipaqaaaaaaabgivaqaaaaaaaaid197cqwa
   aaaaa
   aacmca8baaaaaaaamci8baaaaaaaagck8baaaaaaaacaqf8f2m7lejv6sbuaaaaasuvork5
   cyii= ]

   covariance captures the dependency and can model disposition of the
   "original" sample.
   in [79]:
x, y = np.mgrid[-4:4:.01, -3:3:.01]
pos = np.empty(x.shape + (2,))
pos[:, :, 0] = x; pos[:, :, 1] = y
rv = multivariate_normal(estimated_mean, estimated_cov)

   in [80]:
fig = plt.figure(figsize=(11,5))
ax = fig.gca(projection='3d')
ax.plot_surface(x, y, rv.pdf(pos), cmap=cm.viridis_r, rstride=30, cstride=10, li
newidth=1, alpha=0.47)
ax.plot_wireframe(x, y, rv.pdf(pos), linewidth=0.47, alpha=0.47)
ax.scatter(e_x, e_y, 0.4, marker='.', alpha=0.47)
ax.axis('tight');

   [wm4uyal1dtngqaaaabj ru5erkjggg== ]

evolutionary strategies[38]  

   we will be using deap again to present some of the es main concepts.
   in [19]:
import array, random, time, copy

from deap import base, creator, benchmarks, tools, algorithms

random.seed(42) # fixing a random seed: you should not do this in practice.

   before we dive into the discussion lets code some support functions.
   in [20]:
def plot_problem_3d(problem, bounds, resolution=100.,
                    cmap=cm.viridis_r, rstride=10, cstride=10,
                    linewidth=0.15, alpha=0.65, ax=none):
    'plots a given deap benchmark problem in 3d mesh.'
    (minx,miny),(maxx,maxy) = bounds
    x_range = np.arange(minx, maxx, (maxx-minx)/resolution)
    y_range = np.arange(miny, maxy, (maxy-miny)/resolution)

    x, y = np.meshgrid(x_range, y_range)
    z = np.zeros((len(x_range), len(y_range)))

    for i in range(len(x_range)):
        for j in range(len(y_range)):
            z[i,j] = problem((x_range[i], y_range[j]))[0]

    if not ax:
        fig = plt.figure(figsize=(11,6))
        ax = fig.gca(projection='3d')

    cset = ax.plot_surface(x, y, z, cmap=cmap, rstride=rstride, cstride=cstride,
 linewidth=linewidth, alpha=alpha)

   in [21]:
def plot_problem_controur(problem, bounds, optimum=none,
                          resolution=100., cmap=cm.viridis_r,
                          rstride=1, cstride=10, linewidth=0.15,
                          alpha=0.65, ax=none):
    'plots a given deap benchmark problem as a countour plot'
    (minx,miny),(maxx,maxy) = bounds
    x_range = np.arange(minx, maxx, (maxx-minx)/resolution)
    y_range = np.arange(miny, maxy, (maxy-miny)/resolution)

    x, y = np.meshgrid(x_range, y_range)
    z = np.zeros((len(x_range), len(y_range)))

    for i in range(len(x_range)):
        for j in range(len(y_range)):
            z[i,j] = problem((x_range[i], y_range[j]))[0]

    if not ax:
        fig = plt.figure(figsize=(6,6))
        ax = fig.gca()
        ax.set_aspect('equal')
        ax.autoscale(tight=true)

    cset = ax.contourf(x, y, z, cmap=cmap, rstride=rstride, cstride=cstride, lin
ewidth=linewidth, alpha=alpha)

    if optimum:
        ax.plot(optimum[0], optimum[1], 'bx', linewidth=4, markersize=15)

   in [22]:
def plot_cov_ellipse(pos, cov, volume=.99, ax=none, fc='lightblue', ec='darkblue
', alpha=1, lw=1):
    ''' plots an ellipse that corresponds to a bivariate normal distribution.
    adapted from http://www.nhsilbert.net/source/2014/06/bivariate-normal-ellips
e-plotting-in-python/'''
    from scipy.stats import chi2
    from matplotlib.patches import ellipse

    def eigsorted(cov):
        vals, vecs = np.linalg.eigh(cov)
        order = vals.argsort()[::-1]
        return vals[order], vecs[:,order]

    if ax is none:
        ax = plt.gca()

    vals, vecs = eigsorted(cov)
    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))

    kwrg = {'facecolor':fc, 'edgecolor':ec, 'alpha':alpha, 'linewidth':lw}

    # width and height are "full" widths, not radius
    width, height = 2 * np.sqrt(chi2.ppf(volume,2)) * np.sqrt(vals)
    ellip = ellipse(xy=pos, width=width, height=height, angle=theta, **kwrg)
    ax.add_artist(ellip)

why benchmarks (test) functions?[39]  

   in applied mathematics, [40]test functions, also known as artificial
   landscapes, are useful to evaluate characteristics of optimization
   algorithms, such as:
     * velocity of convergence.
     * precision.
     * robustness.
     * general performance.

   deap has a number of test problems already implemented. see
   [41]http://deap.readthedocs.org/en/latest/api/benchmarks.html

[42]bohachevsky benchmark problem[43]  

   $$\text{minimize } f(\mathbf{x}) = \sum_{i=1}^{n-1}(x_i^2 + 2x_{i+1}^2
   - 0.3\cos(3\pi x_i) - 0.4\cos(4\pi x_{i+1}) + 0.7), \mathbf{x}\in
   \left[-100,100\right]^n,$$

     optimum in $\mathbf{x}=\mathbf{0}$, $f(\mathbf{x})=0$.

   in [23]:
current_problem = benchmarks.bohachevsky

   in [24]:
plot_problem_3d(current_problem, ((-10,-10), (10,10)))

   [q+xlfvsbiwfcwaaaabjru5erkjggg== ]

   the bohachevsky problem has many local optima.
   in [25]:
plot_problem_3d(current_problem, ((-2.5,-2.5), (2.5,2.5)))

   [pnzonv27agvcaaaol8q
   0aeaaaaaaaa+ogcdaaaaaaaa4cmcogaaaaaaambhbhqaaaaaaacajwjoaaaaaaaaab8r0ae
   aaaaa
   aaa+iqadaaaaaaaaferabwaaaaaaapiiga4aaaaaaadweqedaaaaaaaa4cmcogaaaaaaamb
   hbhqa
   aaaaaacajwjoaaaaaaaaab8r0aeaaaaaaaa+iqadaaaaaaaaferabwaaaaaaapiiga4aaaa
   aaadw
   eqedaaaaaaaa4cmcogaaaaaaambhbhqaaaaaaacajwjoaaaaaaaaab8r0aeaaaaaaaa+iqa
   daaaa aaaaferabwaaaaaaapiiga4aaaaaaadw0f8hocmr+ttm50iaaaaasuvork5cyii=
   ]
   in [26]:
ax = plt.figure().gca()
plot_problem_controur(current_problem, ((-2.5,-2.5), (2.5,2.5)), optimum=(0,0),
ax=ax)
ax.set_aspect('equal')

   [x8fvmnodjvbngaaaabjru5erkjggg== ]

($\mu$,$\lambda$) evolutionary strategy[44]  

   some basic initialization parameters.
   in [27]:
search_space_dims = 2 # we want to plot the individuals so this must be 2

min_value, max_value = -10., 10.
min_strat, max_strat = 0.0000001, 1.

   in [28]:
# we are facing a minimization problem
creator.create("fitnessmin", base.fitness, weights=(-1.0,))

# evolutionary strategies need a location (mean)
creator.create("individual", array.array, typecode='d',
               fitness=creator.fitnessmin, strategy=none)
# ...and a value of the strategy parameter.
creator.create("strategy", array.array, typecode="d")

   evolutionary strategy individuals are more complex than those we have
   seen so far.

   they need a custom creation/initialization function.
   in [29]:
def init_univariate_es_ind(individual_class, strategy_class,
                           size, min_value, max_value,
                           min_strat, max_strat):
    ind = individual_class(random.uniform(min_value, max_value)
                           for _ in range(size))
    # we modify the instance to include the strategy in run-time.
    ind.strategy = strategy_class(random.uniform(min_strat, max_strat) for _ in
range(size))
    return ind

   in [30]:
toolbox = base.toolbox()
toolbox.register("individual", init_univariate_es_ind,
                 creator.individual,
                 creator.strategy,
                 search_space_dims,
                 min_value, max_value,
                 min_strat, max_strat)
toolbox.register("population", tools.initrepeat, list,
                 toolbox.individual)

   how does an individual and a population looks like?
   in [31]:
ind = toolbox.individual()
pop = toolbox.population(n=3)

   in [32]:
def plot_individual(individual, ax=none):
    'plots an es indiviual as center and 3*sigma ellipsis.'
    cov = np.eye(len(individual)) * individual.strategy
    plot_cov_ellipse(individual, cov, volume=0.99, alpha=0.56, ax=ax)
    if ax:
        ax.scatter(individual[0], individual[1],
                    marker='+', color='k', zorder=100)
    else:
        plt.scatter(individual[0], individual[1],
                    marker='+', color='k', zorder=100)


def plot_population(pop, gen=none, max_gen=none, ax=none):
    if gen:
        plt.subplot(max_gen, 1, gen)

    for ind in pop:
        plot_individual(ind, ax)

   in [33]:
plot_problem_controur(current_problem, ((-10,-10), (10,10)), optimum=(0,0))
plot_individual(ind)

   [nixy5nzytljreslxqborerercqg9qgterer
   eqmiqz2iiiiiseam6krerereamjqjyiiiiiseim6ererezgagnsjiiiiiateoe5ererejca
   gdsii
   iiiiatgoexererejiegdiiiiiehadoperererajiucciiiiiehcdoherergrgbjuiyiiiig
   exkbo rerercqgbnuiiiiiigh9pxzd9adrxlclaaaaaelftksuqmcc ]
   in [34]:
plot_problem_controur(current_problem, ((-10,-10), (10,10)), optimum=(0,0))
plot_population(pop)

   [w+j3jmqg2cuswaaaabjru5erkjg gg== ]

mutation of an evolution strategy individual according to its strategy
attribute.[45]  

   first the strategy is mutated according to an extended log normal rule,
   $$ \boldsymbol{\sigma}_t = \exp(\tau_0 \mathcal{n}_0(0, 1)) \left[
   \sigma_{t-1, 1}\exp(\tau \mathcal{n}_1(0, 1)), \ldots, \sigma_{t-1, n}
   \exp(\tau \mathcal{n}_n(0, 1))\right], $$ with $$\tau_0 =
   \frac{c}{\sqrt{2n}}\text{ and }\tau = \frac{c}{\sqrt{2\sqrt{n}}}, $$

   the individual is mutated by a normal distribution of mean 0 and
   standard deviation of $\boldsymbol{\sigma}_{t}$ (its current strategy).

   a recommended choice is $c=1$ when using a $(10,100)$ evolution
   strategy.
   in [35]:
toolbox.register("mutate", tools.muteslognormal, c=1, indpb=0.1)

   blend crossover on both, the individual and the strategy.
   in [36]:
toolbox.register("mate", tools.cxesblend, alpha=0.1)
toolbox.register("evaluate", current_problem)
toolbox.register("select", tools.selbest)

   in [37]:
mu_es, lambda_es = 3,21

pop = toolbox.population(n=mu_es)
hof = tools.halloffame(1)

pop_stats = tools.statistics(key=copy.deepcopy)
pop_stats.register('pop', copy.deepcopy) # -- copies the populations themselves

pop, logbook = algorithms.eamucommalambda(pop, toolbox, mu=mu_es, lambda_=lambda
_es,
        cxpb=0.6, mutpb=0.3, ngen=40, stats=pop_stats, halloffame=hof, verbose=f
alse)

the final population[46]  

   in [38]:
plot_problem_controur(current_problem, ((-10,-10), (10,10)), optimum=(0,0))
plot_population(pop)

   [n+zjzulc8v0lqaaaabjru5erkjggg== ]

   the plot (most probably) shows a "dark blue" ellipse as all individuals
   are overlapping.

   let's see how the evolutionary process took place in animated form.
   in [39]:
from matplotlib import animation
from ipython.display import html

   in [40]:
def animate(i):
    'updates all plots to match frame _i_ of the animation.'
    ax.clear()
    plot_problem_controur(current_problem, ((-10.1,-10.1), (10.1,10.1)), optimum
=(0,0), ax=ax)
    plot_population(logbook[i]['pop'], ax=ax)
    ax.set_title('$t=$' +str(i))
    return []

   in [41]:
fig = plt.figure(figsize=(5,5))
ax = fig.gca()
anim = animation.funcanimation(fig, animate, frames=len(logbook), interval=300,
blit=true)
plt.close()

   in [42]:
html(anim.to_html5_video())

   out[42]:
   your browser does not support the video tag.

   how the population progressed as the evolution proceeded?
   in [43]:
pop = toolbox.population(n=mu_es)

stats = tools.statistics(lambda ind: ind.fitness.values)
stats.register("avg", np.mean)
stats.register("std", np.std)
stats.register("min", np.min)
stats.register("max", np.max)

pop, logbook = algorithms.eamucommalambda(pop, toolbox,
                                          mu=mu_es, lambda_=lambda_es,
                                          cxpb=0.6, mutpb=0.3,
                                          ngen=40, stats=stats,
                                          verbose=false)

   in [44]:
plt.figure(1, figsize=(7, 4))
plt.plot(logbook.select('avg'), 'b-', label='avg. fitness')
plt.fill_between(range(len(logbook)), logbook.select('max'), logbook.select('min
'), facecolor='blue', alpha=0.47)
plt.plot(logbook.select('std'), 'm--', label='std. deviation')
plt.legend(frameon=true)
plt.ylabel('fitness'); plt.xlabel('iterations');

   [axg8+42ado7laaaaaelftksuqmcc ]

   what happens if we increase $\mu$ and $\lambda$?
   in [45]:
mu_es, lambda_es = 10,100
pop, logbook = algorithms.eamucommalambda(toolbox.population(n=mu_es), toolbox,
mu=mu_es, lambda_=lambda_es,
        cxpb=0.6, mutpb=0.3, ngen=40, stats=stats, halloffame=hof, verbose=false
)
plt.figure(1, figsize=(7, 4))
plt.plot(logbook.select('avg'), 'b-', label='avg. fitness')
plt.fill_between(range(len(logbook)), logbook.select('max'), logbook.select('min
'), facecolor='blue', alpha=0.47)
plt.plot(logbook.select('std'), 'm--', label='std. deviation')
plt.legend(frameon=true)
plt.ylabel('fitness'); plt.xlabel('iterations');

   [r0l96l5yhz4aaaaasuvork5cyii= ]

covariance matrix adaptation evolutionary strategy[47]  

     * in an evolution strategy, new candidate solutions are sampled
       according to a multivariate normal distribution in the
       $\mathbb{r}^n$.
     * recombination amounts to selecting a new mean value for the
       distribution.
     * mutation amounts to adding a random vector, a perturbation with
       zero mean.
     * pairwise dependencies between the variables in the distribution are
       represented by a covariance matrix.

the covariance matrix adaptation (cma) is a method to update the covariance
matrix of this distribution.[48]  

     this is particularly useful, if the objective function $f()$ is
     ill-conditioned.

cma-es features[49]  

     * adaptation of the covariance matrix amounts to learning a second
       order model of the underlying objective function.
     * this is similar to the approximation of the inverse hessian matrix
       in the quasi-id77 in classical optimization.
     * in contrast to most classical methods, fewer assumptions on the
       nature of the underlying objective function are made.
     * only the ranking between candidate solutions is exploited for
       learning the sample distribution and neither derivatives nor even
       the function values themselves are required by the method.

   in [46]:
from deap import cma

   a similar setup to the previous one.
   in [47]:
creator.create("individual", list, fitness=creator.fitnessmin)
toolbox = base.toolbox()
toolbox.register("evaluate", current_problem)

/users/lm/anaconda/lib/python3.6/site-packages/deap-1.1.0-py3.6-macosx-10.7-x86_
64.egg/deap/creator.py:141: runtimewarning: a class named 'individual' has alrea
dy been created and it will be overwritten. consider deleting previous creation
of that class or rename it.
  runtimewarning)

   we will place our start point by hand at $(5,5)$.
   in [48]:
cma_es = cma.strategy(centroid=[5.0]*search_space_dims, sigma=5.0, lambda_=5*sea
rch_space_dims)
toolbox.register("generate", cma_es.generate, creator.individual)
toolbox.register("update", cma_es.update)

hof = tools.halloffame(1)
stats = tools.statistics(lambda ind: ind.fitness.values)
stats.register("avg", np.mean)
stats.register("std", np.std)
stats.register("min", np.min)
stats.register("max", np.max)

# the cma-es algorithm converge with good id203 with those settings
pop, logbook = algorithms.eagenerateupdate(toolbox, ngen=60, stats=stats,
                                           halloffame=hof, verbose=false)

print("best individual is %s, fitness: %s" % (hof[0], hof[0].fitness.values))

best individual is [-4.13583834502141e-07, -1.0707551743796275e-07], fitness: (2
.8350655156827997e-12,)

   in [49]:
plt.figure(1, figsize=(7, 4))
plt.plot(logbook.select('avg'), 'b-', label='avg. fitness')
plt.fill_between(range(len(logbook)), logbook.select('max'), logbook.select('min
'), facecolor='blue', alpha=0.47)
plt.plot(logbook.select('std'), 'm--', label='std. deviation')
plt.legend(frameon=true)
plt.ylabel('fitness'); plt.xlabel('iterations');

   [wop1oki3jhhxwaaaabjru5erkjggg== ]

 ok, but wouldn't it be nice to have an animated plot of how cma-es
progressed?[50]  

     * we need to do some coding to make this animation work.
     * we are going to create a class named plotablestrategy that inherits
       from deap.cma.strategy. this class logs the features we need to
       make the plots as evolution takes place. that is, for every
       iteration we store:
          + current centroid and covariance ellipsoid.
          + updated centroid and covariance.
          + sampled individuals.
          + evolution path.

   note: i think that deap's implementation of cma-es has the drawback of
   storing information that should be stored as part of "individuals". i
   leave this for an afternoon hack.
   in [50]:
from math import sqrt, log, exp
class plotablestrategy(cma.strategy):
    """this is a modification of deap.cma.strategy class.
    we store the execution data in order to plot it.
    **note:** this class should not be used for other uses than
    the one it is meant for."""

    def __init__(self, centroid, sigma, **kargs):
        """does the original initialization and then reserves
        the space for the statistics."""
        super(plotablestrategy, self).__init__(centroid, sigma, **kargs)

        self.stats_centroids = []
        self.stats_new_centroids = []
        self.stats_covs = []
        self.stats_new_covs = []
        self.stats_offspring = []
        self.stats_offspring_weights = []
        self.stats_ps = []

    def update(self, population):
        """update the current covariance matrix strategy from the
        *population*.

        :param population: a list of individuals from which to update the
                           parameters.
        """
        # -- store current state of the algorithm
        self.stats_centroids.append(copy.deepcopy(self.centroid))
        self.stats_covs.append(copy.deepcopy(self.c))


        population.sort(key=lambda ind: ind.fitness, reverse=true)

        # -- store sorted offspring
        self.stats_offspring.append(copy.deepcopy(population))

        old_centroid = self.centroid
        self.centroid = np.dot(self.weights, population[0:self.mu])

        # -- store new centroid
        self.stats_new_centroids.append(copy.deepcopy(self.centroid))

        c_diff = self.centroid - old_centroid


        # cumulation : update evolution path
        self.ps = (1 - self.cs) * self.ps \
             + sqrt(self.cs * (2 - self.cs) * self.mueff) / self.sigma \
             * np.dot(self.b, (1. / self.diagd) \
                          * np.dot(self.b.t, c_diff))

        # -- store new evol path
        self.stats_ps.append(copy.deepcopy(self.ps))

        hsig = float((np.linalg.norm(self.ps) /
                sqrt(1. - (1. - self.cs)**(2. * (self.update_count + 1.))) / sel
f.chin
                < (1.4 + 2. / (self.dim + 1.))))

        self.update_count += 1

        self.pc = (1 - self.cc) * self.pc + hsig \
                  * sqrt(self.cc * (2 - self.cc) * self.mueff) / self.sigma \
                  * c_diff

        # update covariance matrix
        artmp = population[0:self.mu] - old_centroid
        self.c = (1 - self.ccov1 - self.ccovmu + (1 - hsig) \
                   * self.ccov1 * self.cc * (2 - self.cc)) * self.c \
                + self.ccov1 * np.outer(self.pc, self.pc) \
                + self.ccovmu * np.dot((self.weights * artmp.t), artmp) \
                / self.sigma**2

        # -- store new covs
        self.stats_new_covs.append(copy.deepcopy(self.c))

        self.sigma *= np.exp((np.linalg.norm(self.ps) / self.chin - 1.) \
                                * self.cs / self.damps)

        self.diagd, self.b = np.linalg.eigh(self.c)
        indx = np.argsort(self.diagd)

        self.cond = self.diagd[indx[-1]]/self.diagd[indx[0]]

        self.diagd = self.diagd[indx]**0.5
        self.b = self.b[:, indx]
        self.bd = self.b * self.diagd

   it is now possible to use/test our new class.
   in [51]:
toolbox = base.toolbox()
toolbox.register("evaluate", current_problem)

   in [52]:
max_gens = 40
cma_es = plotablestrategy(centroid=[5.0]*search_space_dims, sigma=1.0, lambda_=5
*search_space_dims)
toolbox.register("generate", cma_es.generate, creator.individual)
toolbox.register("update", cma_es.update)

# the cma-es algorithm converge with good id203 with those settings
a = algorithms.eagenerateupdate(toolbox, ngen=max_gens, verbose=false)

   me can now code the animate_cma_es() function.
   in [53]:
norm=colors.normalize(vmin=np.min(cma_es.weights), vmax=np.max(cma_es.weights))
sm = cm.scalarmappable(norm=norm, cmap=plt.get_cmap('gray'))

   in [54]:
def animate_cma_es(gen):
    ax.cla()
    plot_problem_controur(current_problem, ((-11,-11), (11,11)), optimum=(0,0),
ax=ax)

    plot_cov_ellipse(cma_es.stats_centroids[gen], cma_es.stats_covs[gen], volume
=0.99, alpha=0.29, ax=ax)
    ax.plot(cma_es.stats_centroids[gen][0], cma_es.stats_centroids[gen][1], 'ro'
, markeredgecolor = 'none', ms=10)

    plot_cov_ellipse(cma_es.stats_new_centroids[gen], cma_es.stats_new_covs[gen]
, volume=0.99,
                     alpha=0.29, fc='green', ec='darkgreen', ax=ax)
    ax.plot(cma_es.stats_new_centroids[gen][0], cma_es.stats_new_centroids[gen][
1], 'go', markeredgecolor = 'none', ms=10)

    for i in range(gen+1):
        if i == 0:
            ax.plot((0,cma_es.stats_ps[i][0]),
                     (0,cma_es.stats_ps[i][1]), 'b--')
        else:
            ax.plot((cma_es.stats_ps[i-1][0],cma_es.stats_ps[i][0]),
                     (cma_es.stats_ps[i-1][1],cma_es.stats_ps[i][1]),'b--')

    for i,ind in enumerate(cma_es.stats_offspring[gen]):
        if i < len(cma_es.weights):
            color = sm.to_rgba(cma_es.weights[i])
        else:
            color= sm.to_rgba(norm.vmin)
        ax.plot(ind[0], ind[1], 'o', color = color, ms=5, markeredgecolor = 'non
e')

    ax.set_ylim((-10,10))
    ax.set_xlim((-10,10))
    ax.set_title('$t=$' +str(gen))
    return []

cma-es progress[51]  

   in [55]:
fig = plt.figure(figsize=(6,6))
ax = fig.gca()
anim = animation.funcanimation(fig, animate_cma_es, frames=max_gens, interval=30
0, blit=true)
plt.close()

   in [56]:
html(anim.to_html5_video())

   out[56]:
   your browser does not support the video tag.
     * current centroid and covariance: red.
     * updated centroid and covariance: green.
     * sampled individuals: shades of gray representing their
       corresponding weight.
     * evolution path: blue line starting in (0,0).

homework[52]  

    1. make an animated plot with the covariance update process. you can
       rely on the notebook of the previous demonstration class.
    2. compare es, cma-es and a genetic algortihm.
    3. how do you think that evolutionary strategies and cma-es should be
       modified in order to cope with combinatorial problems?
    4. how can evolution strategies be improved?
     __________________________________________________________________

                          creative commons license

   this work is licensed under a [creative commons
   attribution-noncommercial-sharealike 4.0 international
   license](http://creativecommons.org/licenses/by-nc-sa/4.0/).
   in [57]:
# to install run: pip install version_information
%load_ext version_information
%version_information scipy, numpy, matplotlib, seaborn, deap

   out[57]:
   software version
   python 3.6.0 64bit [gcc 4.2.1 compatible apple llvm 6.0
   (clang-600.0.57)]
   ipython 5.2.2
   os darwin 16.4.0 x86_64 i386 64bit
   scipy 0.18.1
   numpy 1.11.3
   matplotlib 2.0.0
   seaborn 0.7.1
   deap 1.1
   sat mar 04 02:57:13 2017 brt
   in [58]:
# this code is here for cosmetic reasons
from ipython.core.display import html
from urllib.request import urlopen
html(urlopen('https://raw.githubusercontent.com/lmarti/jupyter_custom/master/cus
tom.include').read().decode('utf-8'))

   out[58]:

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [53]fastly, rendered by [54]rackspace

   nbviewer github [55]repository.

   nbviewer version: [56]33c4683

   nbconvert version: [57]5.4.0

   rendered (fri, 05 apr 2019 18:19:59 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/slides/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb
   5. https://nbviewer.jupyter.org/format/script/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb
   6. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb
   7. https://github.com/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb
   8. https://mybinder.org/v2/gh/lmarti/evolutionary-computation-course/master?filepath=aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb
   9. https://raw.githubusercontent.com/lmarti/evolutionary-computation-course/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb
  10. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/tree/master
  11. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/tree/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb
  12. http://www.uff.br/
  13. http://www.ic.uff.br/
  14. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#understanding-evolutionary-strategies-and-covariance-matrix-adaptation
  15. http://www.ic.uff.br/
  16. http://www.uff.br/
  17. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#luis-mart  ,-ic/uff
  18. http://lmarti.com/
  19. https://nbviewer.jupyter.org/cdn-cgi/l/email-protection#98f4f5f9eaecf1d8f1fbb6edfefeb6faea
  20. http://lmarti.com/aec-2014
  21. http://ipython.org/ipython-doc/1/interactive/nbconvert.html
  22. https://github.com/damianavila/live_reveal
  23. https://slideviewer.herokuapp.com/
  24. https://github.com/lmarti/evolutionary-computation-course
  25. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#statistics-recap
  26. http://en.wikipedia.org/wiki/random_variable
  27. http://en.wikipedia.org/wiki/id203_distribution
  28. http://en.wikipedia.org/wiki/id203_density_function
  29. http://en.wikipedia.org/wiki/moment_(mathematics
  30. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#moments
  31. http://en.wikipedia.org/wiki/mean
  32. http://en.wikipedia.org/wiki/standard_deviation
  33. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#two-samples
  34. http://en.wikipedia.org/wiki/covariance
  35. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#understanding-covariance
  36. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#a-two-dimensional-normally-distributed-variable
  37. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#this-is-better-understood-in-3d
  38. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#evolutionary-strategies
  39. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#why-benchmarks-(test)-functions?
  40. http://en.wikipedia.org/wiki/test_functions_for_optimization
  41. http://deap.readthedocs.org/en/latest/api/benchmarks.html
  42. http://deap.readthedocs.org/en/latest/api/benchmarks.html#deap.benchmarks.bohachevsky
  43. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#bohachevsky-benchmark-problem
  44. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#($\mu$,$\lambda$)-evolutionary-strategy
  45. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#mutation-of-an-evolution-strategy-individual-according-to-its-strategy-attribute.
  46. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#the-final-population
  47. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#covariance-matrix-adaptation-evolutionary-strategy
  48. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#the-covariance-matrix-adaptation-(cma)-is-a-method-to-update-the-covariance-matrix-of-this-distribution.
  49. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#cma-es-features
  50. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#  ok,-but-wouldn't-it-be-nice-to-have-an-animated-plot-of-how-cma-es-progressed?
  51. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#cma-es-progress
  52. https://nbviewer.jupyter.org/github/lmarti/evolutionary-computation-course/blob/master/aec.04 - evolutionary strategies and covariance matrix adaptation.ipynb#homework
  53. http://www.fastly.com/
  54. https://developer.rackspace.com/?nbviewer=awesome
  55. https://github.com/jupyter/nbviewer
  56. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  57. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
