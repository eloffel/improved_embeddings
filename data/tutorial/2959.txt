   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]view on github
     * [6]execute on binder
     * [7]download notebook

    1. [8]tutorial_ml_gkbionics
    2. [9]2 - kmeans.ipynb

the $k$-means algorithm[10]  

   again, we start by generating some artificial data:
   in [31]:
plt.jet() # set the color map. when your colors are lost, re-run this.
import sklearn.datasets as datasets
x, y = datasets.make_blobs(centers=4, cluster_std=0.5, random_state=0)

   as always, we first plot the data to get a feeling of what we're
   dealing with:
   in [26]:
plt.scatter(x[:,0], x[:,1]);

   [okxegykntwca aaaasuvork5cyii= ]

   the data looks like it may contain four different "types" of data
   point.

   in fact, this is how it was created above.

   we can plot this information as well, using color:
   in [27]:
plt.scatter(x[:,0], x[:,1], c=y);

   [whpadejviyqkwaaaabjru5erkjggg== ]

   normally, you do not know the information in y, however.

   you could try to recover it from the data alone.

   this is what the kmeans algorithm does.
   in [28]:
from sklearn.cluster import kmeans
kmeans = kmeans(4, random_state=8)
y_hat = kmeans.fit(x).labels_

   now the label assignments should be quite similar to y, up to a
   different ordering of the colors:
   in [29]:
plt.scatter(x[:,0], x[:,1], c=y_hat);

   [v7++pv7s2hdbnohzbjr164rfbsen58faqebdo7cmdatw5s7rfwxztgxxzaz
   ekiiufjkrb4hhlbqkscfemjcsqixqggljqlccceslcrwiyswujlahrdcqv0fks0qpj0lepc
   aaaaa suvork5cyii= ]

   often, you're not so much interested in the assignments to the means.

   you'll want to have a closer look at the means $\mu$.

   the means in $\mu$ can be seen as representatives of their respective
   cluster.
   in [30]:
plt.scatter(x[:,0], x[:,1], c=y_hat, alpha=0.4)
mu = kmeans.cluster_centers_
plt.scatter(mu[:,0], mu[:,1], s=100, c=np.unique(y_hat))
print mu

[[-1.47935679  3.11716896]
 [-1.26811733  7.76378266]
 [ 1.99186903  0.96561071]
 [ 0.92578447  4.32475792]]

   [pckfnvtmaaaaaelftksuqmcc ]

$k$-means on images[11]  

   in this final example, we use the $k$-means algorithm on the classical
   mnist dataset.

   the mnist dataset contains images of hand-written digits.

   let's first fetch the dataset from the internet (which may take a
   while, note the asterisk [*]):
   in [7]:
from sklearn.datasets import fetch_mldata
from sklearn.cluster import kmeans
from sklearn.utils import shuffle
x_digits, _,_, y_digits = fetch_mldata("mnist original").values() # fetch datase
t from internet
x_digits, y_digits = shuffle(x_digits,y_digits) # shuffle dataset (which is orde
red!)
x_digits = x_digits[-5000:]       # take only the last instances, to shorten run
time of kmeans

   let's have a look at some of the instances in the dataset we just
   loaded:
   in [12]:
plt.rc("image", cmap="binary") # use black/white palette for plotting
for i in xrange(10):
    plt.subplot(2,5,i+1)
    plt.imshow(x_digits[i].reshape(28,28))
    plt.xticks(())
    plt.yticks(())
plt.tight_layout()

   [wbre6u6a6bsuaaaaabjru5erkjggg== ]

   warning: this takes quite a few seconds, so be patient until the
   asterisk [*] disappears!
   in [9]:
kmeans = kmeans(20)
mu_digits = kmeans.fit(x_digits).cluster_centers_

   let's have a closer look at the means. even though there are 10 digits,
   some of them are over/under-represented. do you understand why?
   in [13]:
plt.figure(figsize=(16,6))
for i in xrange(2*(mu_digits.shape[0]/2)): # loop over all means
    plt.subplot(2,mu_digits.shape[0]/2,i+1)
    plt.imshow(mu_digits[i].reshape(28,28))
    plt.xticks(())
    plt.yticks(())
plt.tight_layout()

   [uigucovc
   ovaofaqfqqfqmdok4ckucovcovaofaqfqqfqwdik4ckucovcovaofaqfqqfqwdik4ckucov
   covao
   faqfqqfqwdik4ckucovcovaofaqfqqfqwdik4ckucovcovaofaqfqqfqwdj+d9uznb0hpu7
   9aaaa aelftksuqmcc ]

playing with $k$-means[12]  

   this website allows you to play with the $k$-means algorithm on another
   artificial dataset, the "mickey mouse" dataset.

   you "step" through the algorithm by pressing the buttons in the correct
   order.

   try to get a feeling of how the algorithm proceeds.
   in [11]:
from ipython.display import html
html('<iframe src=http://icperformance.com/wp-content/demos/kmeansmouse.html wid
th=500 height=450></iframe>')

   out[11]:

   iframe: [13]http://icperformance.com/wp-content/demos/kmeansmouse.html

playing with this notebook[14]  

   try to see what happens when you
     * increase the standard deviation of the clusters in this notebook
     * choose a "wrong" number of clusters by:
         1. changing the number of clusters generated
         2. changing the number of clusters used by kmeans
     * what happens to result of the $k$-means algorithm when you have
       multiplied one axis of the matrix $x$ with a large value?
       for example, the 0-th axis with 100:
       x[:,0] *= 100
       why does the result change?
     * combine the $k$-means algorithm with the pca algorithm

   in [11]:


   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [15]fastly, rendered by [16]rackspace

   nbviewer github [17]repository.

   nbviewer version: [18]33c4683

   nbconvert version: [19]5.4.0

   rendered (fri, 05 apr 2019 18:20:04 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/temporaer/tutorial_ml_gkbionics/blob/master/2 - kmeans.ipynb
   5. https://github.com/temporaer/tutorial_ml_gkbionics/blob/master/2 - kmeans.ipynb
   6. https://mybinder.org/v2/gh/temporaer/tutorial_ml_gkbionics/master?filepath=2 - kmeans.ipynb
   7. https://raw.githubusercontent.com/temporaer/tutorial_ml_gkbionics/master/2 - kmeans.ipynb
   8. https://nbviewer.jupyter.org/github/temporaer/tutorial_ml_gkbionics/tree/master
   9. https://nbviewer.jupyter.org/github/temporaer/tutorial_ml_gkbionics/tree/master/2 - kmeans.ipynb
  10. https://nbviewer.jupyter.org/github/temporaer/tutorial_ml_gkbionics/blob/master/2 - kmeans.ipynb#the-$k$-means-algorithm
  11. https://nbviewer.jupyter.org/github/temporaer/tutorial_ml_gkbionics/blob/master/2 - kmeans.ipynb#$k$-means-on-images
  12. https://nbviewer.jupyter.org/github/temporaer/tutorial_ml_gkbionics/blob/master/2 - kmeans.ipynb#playing-with-$k$-means
  13. http://icperformance.com/wp-content/demos/kmeansmouse.html
  14. https://nbviewer.jupyter.org/github/temporaer/tutorial_ml_gkbionics/blob/master/2 - kmeans.ipynb#playing-with-this-notebook
  15. http://www.fastly.com/
  16. https://developer.rackspace.com/?nbviewer=awesome
  17. https://github.com/jupyter/nbviewer
  18. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  19. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
