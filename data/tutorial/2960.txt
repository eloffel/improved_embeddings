   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]view on github
     * [6]execute on binder
     * [7]download notebook

    1. [8]datascience-sp14
    2. [9]hw2

   cs194-16 introduction to data science

   name: please put your name

   student id: please put your student id

assignment 2: introduction to machine learning: id91 and
regression[10]  

overview[11]  

   in this assignment, we will use machine learning techniques to perform
   data analysis and learn models about our data. we will use a real world
   music dataset from [12]last.fm for this assignment. there are two parts
   to this assignment: in the first part we will look at unsupervised
   learning with id91 and in the second part, we will study
   supervised learning. the play data (and user/artist matrix) comes from
   the [13]last.fm 1k users dataset, while the tags come from [14]the
   last.fm music tags dataset. you won't have to interact with these
   datasets directly, because we've already preprocessed them for you.

     note: before you begin, you should install pydot by running sudo
     apt-get install python-pydot. additionally, you should be running on
     a vm with at least 1gb of ram allotted to it - less than that and
     you may run into issues with scikit-learn.

introduction to machine learning[15]  

   machine learning is a branch of artifical intelligence where we try to
   find hidden structure within data. for example, lets say you are hired
   as a data scientist at a cool new music playing startup. you are given
   access to logs from the product and are asked find out what kinds of
   music are played on your website and how you can promote songs that
   will be popular. in this case we wish to extract some structure from
   the raw data we have using machine learning.

   there are two main kinds of machine learning algorithms:
    1. unsupervised learning - is the branch where we don't have any
       ground truth (or labeled data) that can help our training process.
       there are many approaches to unsupervised learning which includes
       topics like id91, mixture models, id48 etc.
       in this assignment we will predominantly look at id91.
    2. supervised learning - we have training data which is labeled
       (either manually or from historical data) and we try to make
       predictions about those labels on new, unlabeled, data. there are
       similarly several approaches to supervised learning - various
       classification and regression techniques all the way up to support
       vector machines and convolutional neural networks. in this
       assignment we'll explore two regression algorithms - least squares
       id75 and regression trees.

   many of the techniques you'll be using (like testing on a validation
   set) are of critical importance to the modeling process, regardless of
   the technique you're using, so keep these in mind in your future
   modeling efforts.

application[16]  

   your assignment is to use machine learning algorithms for two tasks on
   a real world music dataset from last.fm. the goal in the first part is
   to cluster artists and try to discover all artists that belong a
   certain genre. in the second part, we'll use the same dataset and
   attempt to predict how popular a song will be based on a number of
   features of that song. one component will involve incorporating cluster
   information into the models.

files[17]  

   data files for this assignment can be found at:

   https://github.com/amplab/datascience-sp14/raw/master/hw2/hw2data.tar.g
   z

   the zip file includes the following files:
     * artists-tags.txt, user-defined tags for top artists
     * userart-mat-training.csv, training data containing a matrix mapping
       artist-id to users who have played songs by the artists
     * userart-mat-test.csv, test data containing a matrix mapping
       artist-id to users who have played songs by the artists
     * train_model_data.csv, aggregate statsitstics and features about
       songs we'll use to train regression models.
     * validation_model_data.csv, similar statistics computed on a
       hold-out set of users and songs that we'll use to validate our
       regression models.

   we will explain the datasets and how they need to used in the
   assignment sections.

deliverables[18]  

   complete the all the exercises below and turn in a write up in the form
   of an ipython notebook, that is, an .ipynb file. the write up should
   include your code, answers to exercise questions, and plots of results.
   complete submission instructions will be posted on piazza.

   we recommend that you do your work in a copy of this notebook, in case
   there are changes that need to be made that are pushed out via github.
   in this notebook, we provide code templates for many of the exercises.
   they are intended to help with code re-use, since the exercises build
   on each other, and are highly recommended. don't forget to include
   answers to questions that ask for natural language responses, i.e., in
   english, not code!

guidelines[19]  

code[20]  

   this assignment can be done with basic python, matplotlib and
   scikit-learn. feel free to use pandas, too, which you may find well
   suited to several exercises. as for other libraries, please check with
   course staff whether they're allowed.

   you're not required to do your coding in ipython, so feel free to use
   your favorite editor or ide. but when you're done, remember to put your
   code into a notebook for your write up.

collaboration[21]  

   this assignment is to be done individually. everyone should be getting
   a hands on experience in this course. you are free to discuss course
   material with fellow students, and we encourage you to use internet
   resources to aid your understanding, but the work you turn in,
   including all code and answers, must be your own work.

part 0: preliminaries[22]  

exercise 0[23]  

   download the data and unzip it.

   read in the file artists-tags.txt and store the contents in a
   dataframe. the file format for this file is
   artist-id|artist-name|tag|count. the fields mean the following:
    1. artist-id : a unique id for an artist (formatted as a
       [24]musicbrainz identifier)
    2. artist-name: name of the artist
    3. tag: user-defined tag for the artist
    4. count: number of times the tag was applied

   similarly, read in the file userart-mat-training.csv . the file format
   for this file is artist-id, user1, user2, .... user1000. i.e. there are
   846 such columns in this file and each column has a value 1 if the
   particular user played a song from this artist.
   in [ ]:
import pandas as pd

data_path = "/home/saasbook/datascience-sp14/hw2" # make this the /path/to/the/d
ata

def parse_artists_tags(filename):
    df = pd.read_csv(filename, sep="|", names=["artistid", "artistname", "tag",
"count"])
    return df

def parse_user_artists_matrix(filename):
    df = pd.read_csv(filename)
    return df

artists_tags = parse_artists_tags(data_path + "/artists-tags.txt")
user_art_mat = parse_user_artists_matrix(data_path + "/userart-mat-training.csv"
)

print "number of tags %d" % 0 # change this line. should be 952803
print "number of artists %d" % 0 # change this line. should be 17119

part 1: finding genres by id91[25]  

   the first task we will look at is how to discover artist genres by only
   looking at data from plays on last.fm. one of the ways to do this is to
   use id91. to evaluate how well our id91 algorithm performs
   we will use the user-generated tags and compare those to our id91
   results.

1.1 data pre-processing[26]  

   last.fm allows users to associate tags with every artist (see the
   [27]top tags for a live example). however as there are a number of tags
   associated with every artists, in the first step we will pre-process
   the data and get the most popular tag for an artist.

exercise 1[28]  

   a. for every artist in artists_tags calculate the most frequently used
   tag.
   in [ ]:
# todo implement this. you can change the function arguments if necessary
# return a data structure that contains (artist id, artist name, top tag) for ev
ery artist
def calculate_top_tag(all_tags):
    pass

top_tags = calculate_top_tag(artists_tags)

# print the top tag for nirvana
# artist id for nirvana is 5b11f4ce-a62d-471e-81fc-a69a8278c7da
# should be 'grunge'
print "top tag for nirvana is %s" % "todo: tag goes here" # complete this line

   b. to do id91 we will be using numpy matrices. create a matrix
   from user_art_mat with every row in the matrix representing a single
   artist. the matrix will have 846 columns, one for whether each user
   listened to the artist.
   in [ ]:
def create_user_matrix(input_data):
    pass

user_np_matrix = create_user_matrix(user_art_mat)

print user_np_matrix.shape # should be (17119, 846)

1.2 id116 id91[29]  

   having pre-processed the data we can now perform id91 on the
   dataset. in this assignment we will be using the python library
   [30]scikit-learn for our machine learning algorithms. scikit-learn
   provides an extensive library of machine learning algorithms that can
   be used for analysis. here is a [31]nice flow chart that shows various
   algorithms implemented and when to use any of them. in this part of the
   assignment we will look at id116 id91

     note on terminology: "samples" and "features" are two words you will
     come across frequently when you look at machine learning papers or
     documentation. "samples" refer to data points that are used as
     inputs to the machine learning algorithm. for example in our dataset
     each artist is a "sample". "features" refers to some representation
     we have for every sample. for example the list of 1s and 0s we have
     for each artist are "features". similarly the bag-of-words approach
     from the previous homework produced "features" for each document.

id116 algorithm[32]  

   id91 is the process of automatically grouping data points that
   are similar to each other. in the [33]id116 algorithm we start with k
   initially chosen cluster centers (or centroids). we then compute the
   distance of every point from the centroids and assign each point to the
   centroid. next we update the centroids by averaging all the points in
   the cluster. finally, we repeat the algorithm until the cluster centers
   are stable.

running id116[34]  

id116 interface[35]  

   take a minute to look at the scikit-learn interface for calling
   [36]kmeans. the constructor of the kmeans class returns a estimator on
   which you can call [37]fit to perform id91.

id116 parameters[38]  

   from the above description we can see that there are a few parameters
   which control the id116 algorithm. we will look at one parameter
   specifically, the number of clusters used in the algorithm. the number
   of clusters needs to be chosen based on domain knowledge of the data.
   as we do not know how many genres exist we will try different values
   and compare the results.

timing your code[39]  

   we will also measure the performance of id91 algorithms in this
   section. you can time the code in a cell using the %%time [40]ipython
   magic as the first line in the cell.

     note: by default, the scikit-learn kmeans implementation runs the
     algorithm 10 times with different center initializations. for this
     assignment you can run it just once by passing the n_init argument
     as 1.

exercise 2[41]  

   a. run id116 using 5 cluster centers on the user_np_matrix.
   in [ ]:
%%time
from sklearn.cluster import kmeans

# run id116 using 5 cluster centers on user_np_matrix
kmeans_5 = none

   b. run id116 using 25 and 50 cluster centers on the user_np_matrix.
   also measure the time taken for both cases.
   in [ ]:
%%time
kmeans_25 = none

   in [ ]:
%%time
kmeans_50 = none

   d. of the three algorithms, which setting took the longest to run ? why
   do you think this is the case ?

     todo - answer question here.

1.3 evaluating id116[42]  

   in addition to the performance comparisons we also wish to compare how
   good our clusters are. to do this we are first going to look at
   internal id74. for internal evaluation we only use the
   input data and the clusters created and try to measure the quality of
   clusters created. we are going to use two metrics for this:

inertia[43]  

   inertia is a metric that is used to estimate how close the data points
   in a cluster are. this is calculated as the sum of squared distance for
   each point to it's closest centroid, i.e., its assigned cluster center.
   the intution behind inertia is that clusters with lower inertia are
   better as it means closely related points form a cluster.inertia is
   calculated by scikit-learn by default.

   exercise 3

   a. print inertia for all the kmeans model computed above.
   in [ ]:
print "inertia for kmeans with 5 clusters = %lf " % 0.0
print "inertia for kmeans with 25 clusters =  %lf " % 0.0
print "inertia for kmeans with 50 clusters = %lf " % 0.0

   b. does kmeans run with 25 clusters have lower or greater inertia than
   the ones with 5 clusters ? which algorithm is better and why ?

     todo: answer question

silhouette score:[44]  

   the silhouette score measures how close various clusters created are. a
   higher silhouette score is better as it means that we dont have too
   many overlapping clusters. the silhouette score can be computed using
   [45]sklearn.metrics.silhouette_score from scikit learn.

   c. calculate the silhouette score using 500 sample points for all the
   kmeans models.
   in [ ]:
from sklearn.metrics import silhouette_score

# note: use 500 sample points to calculate the silhouette score
def get_silhouette_score(data, model):
    pass

print "silhouette score for kmeans with 5 clusters = %lf" % 0.0
print "silhouette score for kmeans with 25 clusters = %lf " % 0.0
print "silhouette score for kmeans with 50 clusters = %lf " % 0.0

   d. how does increasing the number of clusters affect the silhouette
   score ?

     todo: answer question

1.4 external evaluation[46]  

   while internal evaluation is useful, a better method for measuring
   id91 quality is to do external evaluation. this might not be
   possible always as we may not have ground truth data available. in our
   application we will use top_tags from before as our ground truth data
   for external evaluation. we will first compute purity and accuracy and
   finally we will predict tags for our test dataset.

exercise 4[47]  

   a. as a first step we will need to join the artist_tags data with the
   set of labels generated by id116 model. that is, for every artist we
   will now have the top tag, cluster id and artist name in a data
   structure.
   in [ ]:
# return a data structure that contains artist_id, artist_name, top tag, cluster
_label for every artist
def join_tags_labels(artists_data, user_data, kmeans_model):
    pass

# run the function for all the models
kmeans_5_joined = none
kmeans_25_joined = none
kmeans_50_joined = none

   b. next we need to generate a genre for every cluster id we have (the
   cluster ids are from 0 to n-1). you can do this by grouping the data
   from the previous exercise on cluster id.

   one thing you might notice is that we typically get a bunch of
   different tags associated with every cluster. how do we pick one genre
   or tag from this ? to cover various tags that are part of the cluster,
   we will pick the top 5 tags in each cluster and save the list of top-5
   tags as the genre for the cluster.
   in [ ]:
# return a data structure that contains cluster_id, list of top 5 tags for every
 cluster
def assign_cluster_tags(joined_data):
    pass

kmeans_5_genres = none
kmeans_25_genres = none
kmeans_50_genres = none

purity and accuracy[48]  

   two commonly used metrics used for evaluating id91 using external
   labels are purity and accuracy. purity measures the frequency of data
   belonging to the same cluster sharing the same class label i.e. if we
   have a number of items in a cluster how many of those items have the
   same label ? meanwhile, accuracy measures the frequency of data from
   the same class appearing in a single cluster i.e. of all the items
   which have a particular label what fraction appear in the same cluster
   ?

     note: this is similar to precision and recall that we looked at in
     the previous assignment. purity here makes sure that our clusters
     have mostly homogeneous labels while accuracy make sure that our
     labels are not spread out over too many clusters.

   d. compute the purity for each of our id116 models. to do this find
   the top tags of all artists that belong to a cluster. check what
   fraction of these tags are covered by the top 5 tags of the cluster.
   average this value across all clusters. hint: we used similar ideas to
   get the top 5 tags in a cluster.
   in [ ]:
def get_cluster_purity(joined_data):
    pass

print "purity for kmeans with 5 centers %lf " % 0.0
print "purity for kmeans with 25 centers %lf " % 0.0
print "purity for kmeans with 50 centers %lf " % 0.0

   e. to compute the accuracy first get all the unique tags from top_tags.
   then for each tag, compute how many artists are found in the largest
   cluster. we denote these as correct cluster assignments. for example,
   lets take a tag 'rock'. if there are 100 artists with tag 'rock' and
   say 90 of them are in one cluster while 10 of them are in another. then
   we have 90 correct cluster assignments

   add the number of correct cluster assignments for all tags and divide
   this by the total size of the training data to get the accuracy for a
   model.
   in [ ]:
def get_accuracy(joined_data):
    pass

print "accuracy of kmeans with 5 centers %lf " % 0.0
print "accuracy of kmeans with 25 centers %lf " % 0.0
print "accuracy of kmeans with 50 centers %lf " % 0.0

   f. what do the numbers tell you about the models? do you have a
   favorite?

     todo: your answer here.

1.5 evaluating test data[49]  

   finally we can treat the id91 model as a multi-class classifier
   and make predictions on external test data. to do this we load the test
   data file userart-mat-test.csv and for every artist in the file we use
   the id116 model to predict a cluster. we mark our prediction as
   successful if the artist's top tag belongs to one of the five tags for
   the cluster.

exercise 5[50]  

   a load the testdata file and create a numpy matrix named
   user_np_matrix_test.
   in [ ]:
user_art_mat_test = parse_user_artists_matrix(data_path + "/userart-mat-test.csv
")
# note: the astype(float) converts integer to floats here
user_np_matrix_test = create_user_matrix(user_art_mat_test).astype(float)

user_np_matrix_test.shape # should be (1902, 846)

   b. for each artist in the test set, call [51]predict to get the
   predicted cluster. join the predicted labels with test artist ids.
   return 'artist_id', 'predicted_label' for every artist in the test
   dataset.
   in [ ]:
# for every artist return a list of labels
def predict_cluster(test_data, test_np_matrix, kmeans_model):
    pass

# call the function for every model from before
kmeans_5_predicted = none
kmeans_25_predicted = none
kmeans_50_predicted = none

   c. get the tags for the predicted genre and the tag for the artist from
   top_tags. output the percentage of artists for whom the top tag is one
   of the five that describe its cluster. this is the recall of our model.

     note: since the tag data is not from the same source as user plays,
     there are artists in the test set for whom we do not have top tags.
     you should exclude these artists while making predictions and while
     computing the recall.

   in [ ]:
# calculate recall for our predictions
def verify_predictions(predicted_artist_labels, cluster_genres, top_tag_data):
    pass

   d. print the recall for each kmeans model. we define recall as
   num_correct_predictions / num_artists_in_test_data
   in [ ]:
# use verify_predictions for every model
print "recall of kmeans with 5 centers %lf " % 0.0
print "recall of kmeans with 25 centers %lf " % 0.0
print "recall of kmeans with 50 centers %lf " % 0.0

1.5 visualizing clusters using pca[52]  

   another way to evaluate id91 is to visualize the output of
   id91. however the data we are working with is in 846 dimensions
   !, so it is hard to visualize or plot this. thus the first step for
   visualization is to reduce the dimensionality of the data. to do this
   we can use [53]prinicipal component analysis (pca). pca reduces the
   dimension of data and keeps only the most significant components of it.
   this is a commonly used technique to visualize data from high
   dimensional spaces.

     note: we use [54]randomizedpca, an approximate version of the
     algorithm as this has lower memory requirements. the approximate
     version is good enough when we are reducing to a few dimensions (2
     in this case). we also sample the input data before pca to further
     reduce memory requirements.

exercise 6[55]  

   a. calcluate the randomizedpca of the sampled training data set
   sampled_data and reduce it to 2 components. use the [56]fit_transform
   method to do this.
   in [ ]:
from sklearn.decomposition import randomizedpca
import numpy as np

sample_percent = 0.20
rows_to_sample = int(np.ceil(sample_percent * user_np_matrix.shape[0]))
sampled_data = input_data[np.random.choice(input_data.shape[0], rows_to_sample,
replace=false),:]

# return the data reduced to 2 principal components
def get_reduced_data(input_data):
    pass

user_np_2d = get_reduced_data(sampled_data)

   b. fit the reduced data with the kmeans model with 5 cluster centers.
   plot the cluster centers and all the points. make sure to color points
   in every cluster differently to see a visual separation. you may find
   [57]scatter and [58]plot functions from matplotlib to be useful.
   in [ ]:
# todo: write code to fit and plot reduced_data.
import pylab as pl
%pylab inline

part 2 - regression models - predicting song popularity[59]  

   in this section of the assignment you'll be building a model to predict
   the number of plays a song will get. again, we're going to be using
   scikit-learn to train and evaluate regression models, and pandas to
   pre-process the data.

   in the process, you'll encounter some modeling challenges and we'll
   look at how to deal with them.

   we've started with the same data as above, but this time we've
   pre-computed a number of song statistics for you.

   these are:
    1. plays - the number of times a song has been played.
    2. pctmale - percentage of the plays that came from users who
       self-identified as "male".
    3. age - average age of the listener.
    4. country1 - the country of the users that listened to this song
       most.
    5. country2 - the country of the users that listened to this song
       second most.
    6. country3 - the country of the users that listened to this song
       third most.
    7. pctgt1 - percentage of plays that come from a user who's played the
       song more than once.
    8. pctgt2 - percentage of plays that come from a user who's played the
       song more than twice.
    9. pctgt5 - percentage of plays that come from a user who's played the
       song more than five times.
   10. cluster - the "cluster number" of the artist associated with this
       song - similar to what you came up with above. we chose 25 clusters
       fairly arbitrarily.

2.1 data exploration[60]  

exercise 7[61]  

   a. let's start by loading up the data - we've provided a "training set"
   and a "validation set" for you to test your models on. the training set
   are the examples that we use to create our models, while the validation
   set is a dataset we "hold out" from the model fitting process, we use
   these examples to test whether our models accurately predict new data.
   in [ ]:
%pylab inline
import pandas as pd

train = pd.read_csv(data_path + "/train_model_data.csv")
validation = pd.read_csv(data_path + "/validation_model_data.csv")

   now that you've got the data loaded, play around with it, generate some
   descriptive statistics, and get a feel for what's in the data set. for
   the categorical variables try pandas ".count_values()" on them to get a
   sense of the most likely distributions (countries, etc.).

   b. in the next cell put some commands you ran to get a feel for the
   data.
   in [ ]:
# todo: your commands for data exploration here.

   c. next, create a pairwise scatter plot of the columns: plays, pctmale,
   age, pctgt1, pctgt2, pctgt5. (hint: we did this in lab!)

   do you notice anything about the data in this view? what about the
   relationship between plays and other columns?
   in [ ]:
# todo: your commands to generate a scatter plot here.

     todo: what do you notice about the data in this view? write your
     answer here.

2.2 data prep and intro to id75[62]  

   scikit-learn does a number of things very well, but one of the things
   it doesn't handle easily is categorical or missing data. categorical
   data is data that can take on a finite set of values, e.g. a
   categorical variable might be the color of a stop light (red, yellow,
   green), this is in contrast with continuous variables like real numbers
   in the range -infinity to +infinity. there is another common type of
   data called "ordinal" that can be thought of as categorical data that
   has a natural ordering, like: cold, warm, hot. we won't be dealing with
   this kind of data here, but having that kind of ranking opens up the
   use of certain other statistical methods.

exercise 8[63]  

   a. for the first part of the exercise, let's eliminate categorical
   variables, and impute missing values with pandas. write a function to
   drop all categorical variables from the data set, and return two pandas
   data frames:
    1. a data frame with all categorical items and a user-specified
       response column removed.
    2. a data frame that contains only the response column.

   in [ ]:
def basic_prep(data, col):
    #todo - make a copy of the original dataset but with the categorical variabl
es removed! *cluster* should be thought of as a
    #categorical variable and should be removed! make use of pandas ".drop" func
tion.

    #todo - impute missing values with the mean of those columns, use pandas ".f
illna" function to accomplish this.

    pass

#this will create two new data frames, one that contains training data - in this
 case all the numeric columns,
#and one that contains response data - in this case, the "plays" column.
train_basic_features, train_basic_response = basic_prep(train, 'plays')
validation_basic_features, validation_basic_response = basic_prep(validation, 'p
lays')

   now, we're going to train a id75 model. this is likely the
   most widely used model for fitting data out there today - you've
   probably seen it before, maybe even used it in excel. the goal of
   linear modeling, is to fit a linear equation that maps a set of input
   features to a numerical response. this equation is called a model, and
   can be used to make predictions about the response of similar input
   features. for example, imagine we have a dataset of electricity prices
   ($p$) and outdoor temperature ($t$), and we want to predict, given
   temperature, what electricity price will be. a simple way to model this
   is with an equation that looks something like $p = baseprice +
   factor*t$. when we fit a model, we are estimating the parameters
   ($baseprice$ and $factor$) that best fit our data. this is a very
   simple linear model, but you can easily imagine extending this to
   situations where you need to estimate several parameters.

     note: it is possible to fill a semester with linear models (and
     classes in other departments do!), and there are innumerable issues
     to be aware of when you fit linear models, so this is just the tip
     of the iceberg - don't dismiss linear models outright based on your
     experiences here!

   a linear model models the data as a linear combination of the model and
   its weights. typically, the model is written with something like the
   following form: $y = x\theta + \epsilon$, and when we fit the model, we
   are trying to find the value of $\theta$ that minimizes the loss of the
   model. in the case of regression models, the loss is often represented
   as $\sum (y - x\theta)^2$ - or the squared distance between the
   prediction and the actual value.

   in the code below, x refers to the the training features, y refers to
   the training response, xv refers to the validation features and yv
   refers to the validation response. note that x is a matrix (or a
   dataframe) with the shape $n \times d$ where $n$ is the number of
   examples and $d$ is the number of features in each example, while y is
   a vector of length $n$ (one response per example).

   our goal with this assignment is to accurately estimate the number of
   plays a song will get based on the features we know about it.

   the score we'll be judging the models on is called $r^2$, which is a
   measure of how well the model fits the data. it can be thought of
   roughly as the percentage of the variance that the model explains.

exercise 9[64]  

   a. fit a linearregression model with scikit-learn and return the model
   score on both the training data and the validation data.
   in [ ]:
from sklearn import linear_model

def fit_model(x, y):
    #todo - write a function that fits a linear model to a dataset given a colum
n of values to predict.
    pass

def score_model(model, x, y, xv, yv):
    #todo - write a function that returns scores of a model given its training
    #features and response and validation features and response.
    #the output should be a tuple of two model scores.
    pass

def fit_model_and_score(data, response, validation, val_response):
    #todo - given a training dataset, a validation dataset, and the name of a co
lumn to predict,
    #using the model's ".score()" method, return the model score on the training
 data *and* the validation data
    #as a tuple of two doubles.
    pass
    #end todo

print fit_model_and_score(train_basic_features, train_basic_response, validation
_basic_features, validation_basic_response)

model = fit_model(train_basic_features, train_basic_response)

   we realize that this may be your first experience with linear models -
   but that's a pretty low $r^2$ - we're looking for scores significantly
   higher than 0, and the maximum is a 1.

   so what happened? well, we've modeled a linear response to our input
   features, but the variable we're modeling (plays) clearly has a
   non-linear relationship with respect to the input features. it roughly
   follows a power-law distribution, and so modeling it in linear space
   yields a model with estimates that are way off.

   we can verify this by looking at a plot of the model's residuals - that
   is, the difference between the training responses and the predictions.
   a good model would have residuals with two properties:
    1. small in absolute value.
    2. evenly distributed about the true values.

   b. write a function to calculate the residuals of the model, and plot
   those with a histogram.
   in [ ]:
def residuals(features, y, model):
    #todo - write a function that calculates model residuals given input feature
s, ground truth, and the model.
    pass

#todo - plot the histogram of the residuals of your current model.

   see the structure in the plot? this means we've got more modeling to do
   before we can call it a day! it satisfies neither of our properties -
   we're often way wrong with our predictions, and seem to systematically
   under predict the number of plays a song will get.

   what happens if we try and predict the $log$ of number of plays? this
   controls the exponential behaviour of plays, and gives less weight to
   the case where our prediction was off by 100 when the true answer was
   1000.

exercise 10[65]  

   a. adapt your model fitting from above to fit the log of the nubmer of
   plays as your response variable. print the scores.
   in [ ]:
from sklearn import linear_model

#todo - using what you built above, build a model using the log of the number of
 plays as the response variable.

   b. you should see a significantly better $r^2$ and validation $r^2$,
   though still pretty low. take a look at the model residuals again, do
   they look any better?
   in [ ]:
#todo plot residuals of your log model. note - we want to see these on a "plays"
 scale, not a "log(plays)" scale!

   there must be something we can do here to build a better model. let's
   try incorporating country and cluster information.

2.3 linear modeling with categorical variables: one-hot encoding[66]  

   linear models expect numbers for input features. but we have some
   features that we think could be useful that are discrete or
   categorical. how do we represent these as numbers?

   one solution is something called one-hot encoding. basically, we map a
   discrete space to a vector of binary indicators, then use these
   indicators as numbers.

   for example, if i had an input column that could take on the values
   {$red$, $green$, $blue$}, and i wanted to model this with
   one-hot-encoding, i could use a map:
     * $red = 001$
     * $green = 010$
     * $blue = 100$

   we use this representation instead of traditional binary numbers to
   keep these features independent of one another.

   once we've established this representation, we replace the columns in
   our dataset with their one-hot-encoded values. then, we can fit a
   linear model on the data once it's encoded this way!

   statisticians and econometricians call these types of binary variables
   dummy variables, but we're going to call it one-hot encoding, because
   that sounds cooler.

   scikit-learn has functionality to transform values to this encoding
   built in, so we'll leverage that. the functionality is called
   dictvectorizer in scikit-learn. the idea is that you feed a
   dictvectorizer a bunch of examples of your data (that's the vec.fit
   line), and it builds a map from a categorical variable to a one-hot
   encoded vector like we have in the color example above. then, you can
   use this object to translate from categorical values to sequences of
   numeric ones as we do with vec.transform. in the example below, we fit
   a vectorizer on the training data and use the same vectorizer on the
   validation data so that the mapping is consistent and we don't run into
   issues if the categories don't match perfectly between the two data
   sets.

exercise 11[67]  

   a. use the code below to generate new training and validation datasets
   for the datasets with the categorical features in them.
   in [ ]:
from sklearn import feature_extraction

def one_hot_dataframe(data, cols, vec=none):
    """ takes a dataframe and a list of columns that need to be encoded.
        returns a tuple comprising the data, and the fitted vectorizor.

        based on https://gist.github.com/kljensen/5452382
    """
    if vec is none:
        vec = feature_extraction.dictvectorizer()
        vec.fit(data[cols].to_dict(outtype='records'))

    vecdata = pd.dataframe(vec.transform(data[cols].to_dict(outtype='records')).
toarray())
    vecdata.columns = vec.get_feature_names()
    vecdata.index = data.index

    data = data.drop(cols, axis=1)
    data = data.join(vecdata)
    return (data, vec)

def prep_dset(data, col, vec=none):
    #convert the clusters to strings.
    new_data = data
    new_data['cluster'] = new_data['cluster'].apply(str)

    #encode the data with onehot encoding.
    new_data, vec = one_hot_dataframe(new_data, ['country1','cluster'], vec)

    #eliminate features we don't want to use in the model.
    badcols = ['country2','country3','artid','key','age']
    new_data = new_data.drop(badcols, axis=1)

    new_data = new_data.fillna(new_data.mean())

    return (new_data.drop([col], axis=1), pd.dataframe(new_data[col]), vec)


   in [ ]:
train_cats_features, train_cats_response, vec = prep_dset(train, 'plays')
validation_cats_features, validation_cats_response, _ = prep_dset(validation, 'p
lays', vec)

   b. now that you've added the categorical data, let's see how it works
   with a linear model!
   in [ ]:
print fit_model_and_score(train_cats_features, train_cats_response, validation_c
ats_features, validation_cats_response)

   you should see a much better $r^2$ for the training data, but a much
   worse one for the validation data. what happened?

   this is a phenomenon called overfitting - our model has too many
   degrees of freedom (one parameter for each of the 100+ features of this
   dataset. this means that while our model fits the training data
   reasonably well, but at the expense of being too specific to that data.

   john von neumann famously said [68]"with four parameters i can fit an
   elephant, and with five i can make him wiggle his trunk!".

2.4 non-linear modeling and regression trees[69]  

   so, we're at an impasse. we didn't have enough features and our model
   performed poorly, we added too many features and our model looked good
   on training data, but not so good on test data.

   what's a modeler to do?

   there are a couple of ways of dealing with this situation - one of them
   is called id173, which you might try on your own (see
   ridgeregression or lassoregression in scikit-learn), another is to use
   a model which captures non-linear relationships between the features
   and the response variable.

   one such type of model was pioneered here at berkeley, by the late,
   great leo breiman. these models are called regression trees.

   the basic idea behind regression treees is to recursively partition the
   dataset into subsets that are similar with respect to the response
   variable.

   if we take our temperature example, we might observe a non-linear
   relationship - electricity gets expensive when it's cold outside
   because we use the heater, but it also gets expensive when it's too hot
   outside because we run the air conditioning.

   a decision tree model might dynamically elect to split the data on the
   temperature feature, and estimate high prices both for hot and cold,
   with lower prices for more berkeley-like temperatures. go read the
   [70]scikit-learn id90 documentation for more background.

exercise 12[71]  

   a. using the scikit learn decsiontreeregressor api, write a function
   that fits trees with the parameter 'max_depth' exposed to the user, and
   set to 10 by default.
   in [ ]:
from sklearn import tree

def fit_tree(x, y, depth=10):
    ##todo: using the decisiontreeregressor, train a model to depth 10.
    pass

   b. you should be able to use your same scoring function as above to
   compute your model scores. write a function that fits a tree model to
   your training set and returns the model's score for both the training
   set and the validation set.
   in [ ]:
def fit_model_and_score_tree(train_features, train_response, val_features, val_r
esponse):
    ##todo: fit a tree model and report the score on both the training set and t
est set.
    pass

   c. report the scores on the training and test data for both the basic
   features and the categorical features.
   in [ ]:
print fit_model_and_score_tree(train_basic_features, train_basic_response, valid
ation_basic_features, validation_basic_response)
print fit_model_and_score_tree(train_cats_features, train_cats_response, validat
ion_cats_features, validation_cats_response)

   hooray - we've got a model that performs well on the training data set
   and the validation dataset. which one is better? why do you think that
   is. try varying the depth of the decision tree (from, say, 2 to 20) and
   see how either data set does with respect to training and validation
   error.

   d. now, let's build a tree to depth 3 and take a look at it.
   in [ ]:
import stringio
import pydot
from ipython.display import image

tmodel = fit_tree(train_basic_features, train_basic_response, 3)

def display_tree(tmodel):
    dot_data = stringio.stringio()
    tree.export_graphviz(tmodel, out_file=dot_data)
    graph = pydot.graph_from_dot_data(dot_data.getvalue())
    return image(graph.create_png())

display_tree(tmodel)

   e. what is the tree doing? it looks like it's making a decision on
   variables x[4] and x[2] - can you briefly describe, in words, what the
   tree is doing?

     todo: your answer goes here.

   f. finally, let's take a look at variable importance for a tree trained
   to 10 levels - this is a more formal way of deciding which features are
   important to the tree. the metric that scikit-learn calculates for
   feature importance is called gini importance, and measures how much
   total 'impurity' is removed by splits from a given node. variables that
   are highly discriminitive (e.g. ones that occur frequently throughout
   the tree) have higher gini scores. you can read more about these scores
   [72]here.
   in [ ]:
tmodel = fit_tree(train_basic_features, train_basic_response, 10)

pd.dataframe(tmodel.feature_importances_, train_basic_features.columns)

   g. what do you notice? is the output interpretable? how would you
   explain this to someone?

     todo: your answer goes here.

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [73]fastly, rendered by [74]rackspace

   nbviewer github [75]repository.

   nbviewer version: [76]33c4683

   nbconvert version: [77]5.4.0

   rendered (fri, 05 apr 2019 18:20:06 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb
   5. https://github.com/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb
   6. https://mybinder.org/v2/gh/amplab/datascience-sp14/master?filepath=hw2/hw2.ipynb
   7. https://raw.githubusercontent.com/amplab/datascience-sp14/master/hw2/hw2.ipynb
   8. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/tree/master
   9. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/tree/master/hw2
  10. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#assignment-2:-introduction-to-machine-learning:-id91-and-regression
  11. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#overview
  12. http://last.fm/
  13. http://www.dtic.upf.edu/~ocelma/musicrecommendationdataset/lastfm-1k.html
  14. http://musicmachinery.com/2010/11/10/lastfm-artisttags2007/
  15. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#introduction-to-machine-learning
  16. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#application
  17. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#files
  18. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#deliverables
  19. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#guidelines
  20. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#code
  21. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#collaboration
  22. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#part-0:-preliminaries
  23. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#exercise-0
  24. https://musicbrainz.org/doc/musicbrainz_identifier
  25. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#part-1:-finding-genres-by-id91
  26. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#1.1-data-pre-processing
  27. http://www.last.fm/charts/toptags
  28. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#exercise-1
  29. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#1.2-id116-id91
  30. http://scikit-learn.org/stable/index.html
  31. http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
  32. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#id116-algorithm
  33. http://en.wikipedia.org/wiki/id116_id91
  34. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#running-id116
  35. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#id116-interface
  36. http://scikit-learn.org/stable/modules/generated/sklearn.cluster.kmeans.html
  37. http://scikit-learn.org/stable/modules/generated/sklearn.cluster.kmeans.html#sklearn.cluster.kmeans.fit
  38. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#id116-parameters
  39. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#timing-your-code
  40. http://nbviewer.ipython.org/github/ipython/ipython/blob/1.x/examples/notebooks/cell magics.ipynb
  41. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#exercise-2
  42. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#1.3-evaluating-id116
  43. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#inertia
  44. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#silhouette-score:
  45. http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score
  46. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#1.4-external-evaluation
  47. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#exercise-4
  48. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#purity-and-accuracy
  49. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#1.5-evaluating-test-data
  50. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#exercise-5
  51. http://scikit-learn.org/stable/modules/generated/sklearn.cluster.kmeans.html#sklearn.cluster.kmeans.predict
  52. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#1.5-visualizing-clusters-using-pca
  53. http://en.wikipedia.org/wiki/principal_component_analysis
  54. http://scikit-learn.org/stable/modules/decomposition.html#approximate-pca
  55. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#exercise-6
  56. http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.randomizedpca.html#sklearn.decomposition.randomizedpca.fit_transform
  57. http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter
  58. http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot
  59. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#part-2---regression-models---predicting-song-popularity
  60. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#2.1-data-exploration
  61. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#exercise-7
  62. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#2.2-data-prep-and-intro-to-linear-regression
  63. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#exercise-8
  64. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#exercise-9
  65. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#exercise-10
  66. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#2.3-linear-modeling-with-categorical-variables:-one-hot-encoding
  67. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#exercise-11
  68. http://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/
  69. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#2.4-non-linear-modeling-and-regression-trees
  70. http://scikit-learn.org/stable/modules/tree.html
  71. https://nbviewer.jupyter.org/github/amplab/datascience-sp14/blob/master/hw2/hw2.ipynb#exercise-12
  72. http://www.stat.berkeley.edu/~breiman/randomforests/cc_home.htm#giniimp
  73. http://www.fastly.com/
  74. https://developer.rackspace.com/?nbviewer=awesome
  75. https://github.com/jupyter/nbviewer
  76. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  77. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
