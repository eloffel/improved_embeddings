   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]view on github
     * [6]execute on binder
     * [7]download notebook

    1. [8]machine_learning
    2. [9]04_neural_networks.ipynb

neural networks

   neural networks are one approach to machine learning that attempts to
   deal with the problem of large data dimensionality. the neural network
   approach uses a fixed number of basis functions - in contrast to
   methods such as support vector machines that attempt to adapt the
   number of basis functions - that are themselves parameterized by the
   model parameters. this is a significant departure from linear
   regression and id28 methods where the models consisted
   of linear combinations of fixed basis functions, $\phi(\mathbf{x})$,
   that dependend only on the input vector, $\mathbf{x}$. in neural
   networks, the basis functions can now depend on both the model
   parameters and the input vector and thus take the form $\phi(\mathbf{x}
   | \mathbf{w})$.

   here we will cover only feed-forward neural networks. one can envision
   a neural network as a series of layers where each layer has some number
   of nodes and each node is a function. each layer represents a single
   id75 model. the nodes are connected to each other through
   inputs accepted from and outputs passed to other nodes. a feed-forward
   network is one in which these connections do not form any directed
   cycles. see [10]here for more detail.

   alt text

   as a matter of convention, we will refer the model as a $n$ layer model
   where $n$ is the number of layers for which adaptive parameters,
   $\mathbf{w}$, must be determined. thus for a model consisting of an
   input layer, one hidden layer, and an output layer, we consider $n$ to
   be 2 since parameters are determined only for the hidden and output
   layers.

feed-forward network functions

   we will consider a basic two layer nueral network model, i.e a model
   that maps inputs to a hidden layer and then to an output layer. we will
   make the following assumptions
    1. the final output will be a vector $y$ with $k$ elements, $y_k$,
       where $y_k(\mathbf{x},\mathbf{w}) = p(c_1|\mathbf{x})$ is the
       id203 that node $k$ is in class $c_1$ and $p(c_2|\mathbf{x})
       = 1-p(c_1|\mathbf{x})$
    2. the activation function at a given layer is an arbitrary nonlinear
       function of a linear combination of the inputs and parameters for
       that layer
    3. the network is fully connected, i.e. every node at the input layer
       is connected to every node in the hidden layer and every node in
       the hidden layer is connected to every node in the output layer
    4. a bias parameter is included at the hidden and output layers

   working from the input layer toward the output layer, we can build this
   model as follows:

input layer

   assume we have an input vector $\mathbf{x} \in \re^d$. then the input
   layer consists of $d+1$ nodes where the value of the $i^{th}$ node for
   $i=0\ldots d$, is 0 if $i=0$ and $x_i$, i.e. the $i^{th}$ value of
   $\mathbf{x}$, otherwise.

hidden layer

   at the hidden layer we construct $m$ nodes where the value of $m$
   depends on the specifics of the particular modelling problem. for each
   node, we define a unit activation, $a_m$, for $m=1\ldots m$ as
   $a_m = \sum_{i=0}^d w_{ji}^{(1)}x_i$
   where the $(1)$ superscript indicates this weight is for the hidden
   layer. the output from each node, $z_m$, is then given by the value of
   a fixed nonlinear function, $h$, known as the activation function,
   acting on the unit activation
   $z_m = h(a_m) = h \left( \sum_{i=0}^d w_{mi}^{(1)}x_i \right)$
   notice that $h$ is the same function for all nodes.

output layer

   the process at the output layer is essentially the same as at the
   hidden layer. we construct $k$ nodes, where again the value of $k$
   depends on the specific modeling problem. for each node, we again
   define a unit activation, $a_k$, for $k=1 \ldots k$ by
   $a_k = \sum_{m=0}^m w_{km}^{(2)} z_m$
   we again apply a nonlinear activation function, say $y$, to produce the
   output
   $y_k = y(a_k)$

   thus, the entire model can be summarized as a $k$ dimensional output
   vector $y \in \re^k$ where each element $y_k$ by
   $y_k(\mathbf{x},\mathbf{w}) = y \left( \sum_{m=0}^m w_{km}^{(2)} h
   \left( \sum_{i=0}^d w_{mi}^{(1)}x_i \right) \right)$

generalizations

   there are a wide variety of generalizations possible for this model.
   some of the more important ones for practical applications include
     * addition of hidden layers
     * inclusion of skip-layer connections, e.g. a connection from an
       input node directly to an output node
     * sparse network, i.e. not a fully connected network

network training

   here we will consider how to determine the network model parameters. we
   will use a maximum likelihood approach. the likelihood function for the
   network is dependent on the type of problem the network models. of
   particular importance is the number and type of values generated at the
   output layer. we consider several cases below.

regression single gaussian target

   assume that our target variable, t, is gaussian distributed with an
   $\mathbf{x}$ dependent mean, $\mu(\mathbf{x})$ and constant variance
   $\sigma^2 = 1/\beta$. our network model is designed to estimate the
   unknown function for the mean, i.e. $y(\mathbf{x},\mathbf{w}) \approx
   \mu(\mathbf{x})$ so that the distribution for the target value, t, is
   modeled by
   $p(t|\mathbf{x},\mathbf{w}) = nd(t|y(\mathbf{x},\mathbf{w}),
   \beta^{-1})$
   where $nd(\mu,\sigma^2)$ is the normal distribution with mean $\mu$ and
   variance $\sigma^2$. assume the output layer activation function is the
   identity, i.e. the output is simply the the unit activations, and that
   we have $n$ i.i.d. observations $\mathbf{x}=\\{\mathbf{x}_1, \ldots,
   \mathbf{x}_2 \\}$ and target values $\mathbf{t} = \\{t_1, \ldots,
   t_2\\}$, the likelihood function is
   $p(\mathbf{t}|\mathbf{x},\mathbf{w}, \beta) = \prod_{n=1}^n
   p(t_n|\mathbf{x}_n, \mathbf{w}, \beta)$
   the total error function is defined as the negative logarithm of the
   likelihood function given by

   $\frac {\beta}{2} \sum_{n=1}^n \\{y(\mathbf{x}_n,\mathbf{w}) - t_n
   \\}^2 - \frac{n}{2} \ln(\beta) + \frac{n}{2} \ln (2\pi)$

   the parameter estimates, $\mathbf{w}^{(ml)}$ (ml indicates maximum
   likelihood) are found by maximizing the equivalent sum-of-squares error
   function

   $e(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^n \\{y(\mathbf{x}_n,\mathbf{w})
   - t_n \\}^2$

   note, due to the nonlinearity of the network model, $e(\mathbf{w})$ is
   not necessisarily convex, and thus may have local minima and hence it
   is not possible to know that the global minimum has been obtained. the
   model parameter, $\beta$ is found by first finding $\mathbf{w}_{ml}$
   and then solving

   $\frac{1}{\beta_{ml}} = \frac{1}{n}\sum_{n=1}^n \\{
   y(\mathbf{x}_n,\mathbf{w}^{(ml)}) - t_n \\}^2$

regression multiple gaussian targets

   now assume that we have $k$ gaussian distributed target variables,
   $\[t_1, \ldots, t_k\]$, each with a mean that is independently
   conditional on $\mathbf{x}$, i.e. the mean of $t_k$ is defined by some
   function $\mu_k(\mathbf{x})$. also assume that all $k$ variables share
   the same variance, $\sigma^2=1/\beta$. assuming the network output
   layer has $k$ nodes where $y_k(\mathbf{x},\mathbf{w}) \approx
   \mu_k(\mathbf{x})$ and letting $\mathbf{y}(\mathbf{x},\mathbf{w}) =
   \[y_1(\mathbf{x},\mathbf{w}), \ldots, y_k(\mathbf{x},\mathbf{w})\]$,
   and that we again have $n$ training target values $\mathbf{t}$
   ($\mathbf{t}$ is a $k \times n$ matrix of the training values), the
   conditional distribution of the target training values is given by

   $p(\mathbf{t}|\mathbf{x},\mathbf{w}) = nd(\mathbf{t} |
   \mathbf{y}(\mathbf{x},\mathbf{w}), \beta^{-1} \mathbf{i})$

   the parameter estimates, $\mathbf{w}^{(ml)}$ are again found by
   minimizing the sum-of-squares error function, $e(\mathbf{w})$, and the
   estimate for $\beta$ is found from

   $\frac{1}{\beta_{ml}} = \frac{1}{nk}\sum_{n=1}^n
   ||y(\mathbf{x}_n,\mathbf{w}^{(ml)}) - t_n ||^2$

binary classification single target

   now assume we have a single target variable, $t\in \\{0,1\\}$, such
   that $t=1$ denotes class $c_1$ and $t=0$ denotes class $c_2$. assume
   that the network has a single output node whos activation function is
   the logistic sigmoid

   $y=1/(1+\exp(-a))$

   where $a$ is the output of the hidden layer. we can interpret the
   network output, $y(\mathbf{x},\mathbf{w})$ as the conditional
   id203 $p(c_1|\mathbf{x})$ with
   $p(c_2|\mathbf{x})=1-y(\mathbf{x},\mathbf{w})$ so that the coniditional
   id203 takes a bernoulli distribution

   $p(t|\mathbf{x},\mathbf{w}) = y(\mathbf{x},\mathbf{w})^t\[1 -
   y(\mathbf{x},\mathbf{w}) \]^{1-t}$

   given a training set with $n$ observations the error function is given
   by

   $e(\mathbf{w}) = - \sum_{n=1}^n \\{t_n \ln (y_n) + (1 - t_n) \ln
   (1-y_n) \\}$

   where $y_n = y(\mathbf{x_n},\mathbf{w})$. this model assumes all
   training inputs are correctly labeled.

binary classification $k$ seperate targets

   assume we have $k$ seperate binary classification target variables
   (i.e. there are k classification sets each with two classes. the k sets
   are independent and the input will be assigned to one class from each
   set). this can be modeled with network having $k$ nodes in the output
   layer where the activation function of each node is the logistic
   sigmoid. assume the class labels are independent, i.e. $p(x_i \in c_1|
   x_j \in c_1) = p(x_i \in c_1)$ $\forall \\{i,j\\}$ (this is often an
   invalid assumption in practice), and that we have some training set,
   $\mathbf{t}$, then the coniditional id203 of a single output
   vector $\mathbf{t}$ is

   $p(\mathbf{t}|\mathbf{x},\mathbf{w}) = \prod_{k=1}^k
   y_k(\mathbf{x},\mathbf{w})^{t_k} \[ 1 -
   y_k(\mathbf{x},\mathbf{w})\]^{1-t_k}$

   given a training input with $n$ values (note that the training set is
   an $k \times n$ matrix) the error function is

   $e(\mathbf{w} = - \sum_{n=1}^n \sum_{k=1}^k \[ t_{nk} \ln (y_{nk}) +
   (1-t_{nk}) \ln (1-y_{nk}) \]$

   where $y_{nk} = y_k(\mathbf{x}_n, \mathbf{w}).

multi-class classification

   assume we have a $k$ mutually exclusive classes (i.e. the input can
   only be in one of the $k$ classes). this network is also modelled with
   $k$ output nodes where each output node represents the id203 that
   the input is in class $k$, $y_k(\mathbf{x},\mathbf{w}) = p(t_k = 1 |
   \mathbf{x})$. the error function is given by

   $e(\mathbf{w}) = - \sum_{n=1}^n \sum_{k=1}^k t_{nk} \ln
   y_k(\mathbf{x},\mathbf{w})$

   where the output activation function $y_k$ is the softmax function

   $y_k(\mathbf{x},\mathbf{w}) =
   \frac{\exp(a_k(\mathbf{x},\mathbf{w})}{\sum_j
   \exp(a_j(\mathbf{x},\mathbf{w}))}$

error id26

   most training algorithms involve a two step procedure for minimizing
   the model parameter dependent error function
    1. evaluate the derivatives of the error function with respect to the
       parameters
    2. use the derivatives to iterate the values of the parameters

   in this section, we consider error id26 which provides an
   efficient way to evaluate a feed-forward neural network's error
   function. note that this only satisfies step 1 above. it is still
   necessary to choose an optimization technique in order to use the
   derivative information provided by error id26 to update the
   parameter estimates as indicated by step 2.

   the results presented here are applicable to
    1. an arbitary feed-forward network toplogy
    2. arbitrary differentiable nonlinear id180

   the one assumption that is made is that the error function can be
   expressed as a sum of terms, one for each training data point, $t_n$
   for $n \in \\{1\ldots n\\}$, so that

   $e(\mathbf{w}) = \sum_{n=1}^n e_n(\mathbf{w},t_n)$

   for such error functions, the derivative of the error function with
   respect to $\mathbf{w}) takes the form

   $\bigtriangledown e(\mathbf{w}) = \sum_{n=1}^n \bigtriangledown
   e_n(\mathbf{w},t_n)$

   thus we need only consider the evaluation of $\bigtriangledown
   e_n(\mathbf{w},t_n)$ which may be used directly for sequential
   optimization techniques or summed over the training set for batch
   techniques (see next section). the error id26 method can be
   summarized as follows
    1. given an input vector, $\mathbf{x}_n$, forward propagate through
       the network using
         1. $a_j = \sum_i w_{ji}z_i$
         2. $z_j = h(a_j)$
    2. evaluate $\delta_k \equiv \frac{\partial e_n} {\partial a_k}$ for
       the ouptput layer as
         1. $\delta_k = y_k - t_k$
    3. backpropagate the $\delta$'s to obtain $\delta_j$ for each hidden
       unit in the network using
         1. $\delta_j = h'(a_j) \sum_k w_{kj}\delta_k$
    4. evaluate the derivatives using
         1. $\frac{\partial e_n} {\partial w_{ji}} = \delta_j z_i$

parameter optimization

   given the neural network error function derivative estimate provided by
   error id26, one can use a variety of numerical optimization
   techniques to find appropriate parameter estimates. the simplest
   technique is the method of steepest descent where the new parameter
   estimate $\mathbf{w}^{(\tau+1)}$ is given by

   $\mathbf{w}^{(\tau+1)} = \mathbf{w}^{(\tau)} - \eta \bigtriangledown
   e(\mathbf{w}^{(\tau)})$

   where $\eta>0$ is the learning rate. other more advanced optimization
   algorithms inlcude conjugate gradients and quasi-id77s.

example: exlusive or

   as a first example, let's consider the classical [11]exlusive or (xor)
   problem. we will consider the xor problem involving two inputs $x_1$
   and $x_2$ where $x_i \in \\{0,1\\}$. this problem states that the
   output should be 1 if exactly one of the inputs is 1 and 0 otherwise.
   thus this problem has a very simple known input output relationship
   $x_1$ $x_2$ output
   0     0     0
   1     0     1
   0     1     1
   1     1     0

   while this may be a trivial example, it illustrates all of the features
   of an arbitary feed-forward network while remaining simple enough to
   understand everything that is happening in the network. the xor network
   is typically presented as having 2 input nodes, 2 hidden layer nodes,
   and one output nodes, for example see [12]here. such an arrangement
   however requires that the nodes in the hidden layer use a thresholding
   scheme whereby the output of the node is 0 or 1 depending on the sum of
   the input being greater than some fixed threshold value. however, such
   a network would violoate our model assumptions for training the
   network. specifically, we could not use error id26 because
   the node id180 would be step functions which are not
   differentiable. to overcome this, we will the hyperbolic tangent, tanh,
   as the hidden layer activation function and add a third node to the
   hidden layer representing a bias term. our output layer will have a
   single node. we will interpret the out values as being 0 (or false) for
   output values less than 0 and 1 (or true) for output values greater
   than 0. the figure below provides a graphical representation of the
   network. note, the parameters $w$ and $a$ are distinct for each layer,
   e.g. $w_{11}$ on the edge between $x_1$ and $h(a_1)$ is not the same
   $w_{11}$ on the edge from $h(a_1)$ to $\sigma(a_1)$.

   [xor2.png]

   "the feedforward neural network for the 2 input exclusive or problem"
   in [5]:
import networkx as nx
import sys
from math import tanh
sys.path.append('pysrc')
import neural_networks as nn
from functools import partial

dg = nx.digraph()
dg.add_nodes_from(['x1','x2','h0','h1','h2','o1'])

#set the id180
dg.node['h0']['af']=1.0
for node in ['h1','h2','o1']:
    dg.node[node]['af']=tanh

#set the derivatives of the id180 for all nodes except the output
, this is done below
def zero(x):
    return 0
for node in ['x1','x2','h0']: #the inputs and bias terms have zero derivatives
    dg.node[node]['daf'] = zero
def dtanh(x):
    return 1.0 - tanh(x) * tanh(x)
for node in ['h1','h2']:
    dg.node[node]['daf']=dtanh

#create the edges
for source in ['x1','x2']:
    for target in ['h1','h2']:
        dg.add_weighted_edges_from([(source,target,1.0)])
for source in ['h0','h1','h2']:
    dg.add_weighted_edges_from([(source, 'o1', 1.0)])

#set the input values
dg.node['x1']['af']=0
dg.node['x2']['af']=1

#given these inputs, the correct output should be 1
#we'll use a partial function so we can assign the correct 'daf' value
#dynamically when we iteratively train the network
def dout(x, t):
    if x<0:
        xx = 0
    else:
        xx = 1
    return xx - t
dg.node['o1']['daf']=partial(dout, t=1)

nn.forward_prop(dg)

nn.error_back_prop(dg)

for node in ['x1','x2','h0','h1','h2','o1']:
    print "node {0} has output {1} and error {2}".format(node, dg.node[node]['o'
], dg.node[node]['e'])

node x1 has output 0 and error 0.0
node x2 has output 1 and error 0.0
node h0 has output 1.0 and error 0.0
node h1 has output 0.761594155956 and error 0.0
node h2 has output 0.761594155956 and error 0.0
node o1 has output 0.987217029728 and error 0.0

   in [ ]:


   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [13]fastly, rendered by [14]rackspace

   nbviewer github [15]repository.

   nbviewer version: [16]33c4683

   nbconvert version: [17]5.4.0

   rendered (fri, 05 apr 2019 18:20:06 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/masinoa/machine_learning/blob/master/04_neural_networks.ipynb
   5. https://github.com/masinoa/machine_learning/blob/master/04_neural_networks.ipynb
   6. https://mybinder.org/v2/gh/masinoa/machine_learning/master?filepath=04_neural_networks.ipynb
   7. https://raw.githubusercontent.com/masinoa/machine_learning/master/04_neural_networks.ipynb
   8. https://nbviewer.jupyter.org/github/masinoa/machine_learning/tree/master
   9. https://nbviewer.jupyter.org/github/masinoa/machine_learning/tree/master/04_neural_networks.ipynb
  10. http://en.wikipedia.org/wiki/feedforward_neural_network
  11. http://en.wikipedia.org/wiki/exclusive_or
  12. http://www.heatonresearch.com/online/introduction-neural-networks-java-edition-2/chapter-1/page4.html
  13. http://www.fastly.com/
  14. https://developer.rackspace.com/?nbviewer=awesome
  15. https://github.com/jupyter/nbviewer
  16. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  17. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
