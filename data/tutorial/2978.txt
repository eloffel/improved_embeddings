   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]ai

                   not another mnist tutorial with tensorflow

   an informative, visual, and interactive mnist tutorial.

   by [27]justin francis

   december 6, 2016

   numbers 0-9. numbers 0-9. (source: [28]denise krebs on flickr)

   bear with me: [29]mnist is where everyone in machine learning starts,
   but i hope this tutorial is different from the others out there.

   back when tensorflow was released to the public in november 2015, i
   remember following tensorflow   s [30]beginner mnist tutorial. i blindly
   copied and pasted all this code into my terminal and some numbers
   popped out as they should have. i thought, ok, i know there is
   something amazing happening here, why can i not see it? my goal was to
   make a mnist tutorial that was both interactive and visual, and
   hopefully will teach you a thing or two that others just assume you
   know.

   in this tutorial, i will be using the machine learning library
   tensorflow with python3 on ubuntu 14.04. if you need help installing
   tensorflow on your own system check out my tutorial [31]here.

   if you don't have numpy and matplotlib installed, you   ll need them.
   open a terminal and type in:
$ sudo apt-get install python-numpy python3-numpy python-matplotlib python3-matp
lotlib

   to begin, we will open up python in our terminal and import the mnist
   data set:
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('mnist_data', one_hot=true)
import matplotlib.pyplot as plt
import numpy as np
import random as ran

   first, let   s define a couple of functions that will assign the amount
   of training and test data we will load from the data set. it   s not
   vital to look very deeply at these unless you want to figure out what   s
   going on behind the scenes.

   you will need to copy and paste each function and hit enter twice in
   your terminal:
def train_size(num):
    print ('total training images in dataset = ' + str(mnist.train.images.shape)
)
    print ('--------------------------------------------------')
    x_train = mnist.train.images[:num,:]
    print ('x_train examples loaded = ' + str(x_train.shape))
    y_train = mnist.train.labels[:num,:]
    print ('y_train examples loaded = ' + str(y_train.shape))
    print('')
    return x_train, y_train

def test_size(num):
    print ('total test examples in dataset = ' + str(mnist.test.images.shape))
    print ('--------------------------------------------------')
    x_test = mnist.test.images[:num,:]
    print ('x_test examples loaded = ' + str(x_test.shape))
    y_test = mnist.test.labels[:num,:]
    print ('y_test examples loaded = ' + str(y_test.shape))
    return x_test, y_test

   and we   ll define some simple functions for resizing and displaying the
   data:
def display_digit(num):
    print(y_train[num])
    label = y_train[num].argmax(axis=0)
    image = x_train[num].reshape([28,28])
    plt.title('example: %d  label: %d' % (num, label))
    plt.imshow(image, cmap=plt.get_cmap('gray_r'))
    plt.show()

def display_mult_flat(start, stop):
    images = x_train[start].reshape([1,784])
    for i in range(start+1,stop):
        images = np.concatenate((images, x_train[i].reshape([1,784])))
    plt.imshow(images, cmap=plt.get_cmap('gray_r'))
    plt.show()

   now, we   ll get down to the business of building and training our model.
   first, we define variables with how many training and test examples we
   would like to load. for now, we will load all the data but we will
   change this value later on to save resources:
x_train, y_train = train_size(55000)

total training images in dataset = (55000, 784)
--------------------------------------------------
x_train examples loaded = (55000, 784)
y_train examples loaded = (55000, 10)

   so, what does this mean? in our data set, there are 55,000 examples of
   handwritten digits from zero to nine. each example is a 28x28 pixel
   image flattened in an array with 784 values representing each pixel   s
   intensity. the examples need to be flattened for tensorflow to make
   sense of the digits linearly. this shows that in x_train we have loaded
   55,000 examples each with 784 pixels. our x_train variable is a 55,000
   row and 784 column matrix.

   the y_train data is the associated labels for all the x_train examples.
   rather than storing the label as an integer, it is stored as a 1x10
   binary array with the one representing the digit. this is also known as
   one-hot encoding. in the example below, the array represents a 7:
   array representing a 7 figure 1. array representing a 7. source: justin
   francis.

   so, let   s pull up a random image using one of our custom functions that
   takes the flattened data, reshapes it, displays the example, and prints
   the associated label (note: you have to close the window matplot opens
   to continue using python):
display_digit(ran.randint(0, x_train.shape[0]))

   random image figure 2. [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]
   source: justin francis.

   here is what multiple training examples look like to the classifier in
   their flattened form. of course, instead of pixels, our classifier sees
   values from zero to one representing pixel intensity:
display_mult_flat(0,400)

   mnist tensorflow first 400 training examples figure 3. first 400
   training examples. source: justin francis.

   until this point, we actually have not been using tensorflow at all.
   the next step is importing tensorflow and defining our session.
   tensorflow, in a sense, creates a directed acyclic graph (flow chart)
   which you later feed with data and run in a session:
import tensorflow as tf
sess = tf.session()

   next, we can define a placeholder. a placeholder, as the name suggests,
   is a variable used to feed data into. the only requirement is that in
   order to feed data into this variable, we need to match its shape and
   type exactly. the tensorflow website explains that    a placeholder
   exists solely to serve as the target of feeds. it is not initialized
   and contains no data.    here, we define our x placeholder as the
   variable to feed our x_train data into:
x = tf.placeholder(tf.float32, shape=[none, 784])

   when we assign none to our placeholder, it means the placeholder can be
   fed as many examples as you want to give it. in this case, our
   placeholder can be fed any multitude of 784-sized values.

   we then define y_, which will be used to feed y_train into. this will
   be used later so we can compare the ground truths to our predictions.
   we can also think of our labels as classes:
y_ = tf.placeholder(tf.float32, shape=[none, 10])

   next, we will define the weights w and bias b. these two values are the
   grunt workers of the classifier   they will be the only values we will
   need to calculate our prediction after the classifier is trained.

   we will first set our weight and bias values to zeros because
   tensorflow will optimize these values later. notice how our w is a
   collection of 784 values for each of the 10 classes:
w = tf.variable(tf.zeros([784,10]))
b = tf.variable(tf.zeros([10]))

   i like to think of these weights as 10 cheat sheets for each number.
   this is similar to how a teacher uses a cheat sheet transparency to
   grade a multiple choice exam. the bias, unfortunately, is a little
   beyond the scope of this tutorial, but i like to think of it as a
   special relationship with the weights that influences our final answer.

   we will now define y, which is our classifier function. this particular
   classifier is also known as multinomial id28. we make
   our prediction by multiplying each flattened digit by our weight and
   then adding our bias:
y = tf.nn.softmax(tf.matmul(x,w) + b)

   first, let   s ignore the softmax and look what's inside the softmax
   function. matmul is the function for multiplying matrices. if you know
   your [32]id127, you would understand that this computes
   properly and that x * w + b results in a number of training examples
   fed (m) x number of classes (n) matrix.
   simple id127 figure 4. simple id127.
   source: [33]quartl on wikimedia commons.

   if you don   t believe me, you can confirm it by evaluating y:
print(y)

tensor("softmax:0", shape=(?, 10), dtype=float32)

   that tells us what y is in our session, but what if we want the values
   of y? you cannot just print a tensorflow graph object to get its
   values; you must run an appropriate session in which you feed it data.
   so, let   s feed our classifier three examples and see what it predicts.
   in order to run a function in our session, we first must initialize the
   variables in our session. notice if you just run sess.run(y) tensorflow
   will complain that you need to feed it data:
x_train, y_train = train_size(3)
sess.run(tf.global_variables_initializer())
#if using tensorflow prior to 0.12 use:
#sess.run(tf.initialize_all_variables())
print(sess.run(y, feed_dict={x: x_train}))

[[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]
 [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]
 [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]]

   so, here we can see our prediction for our first three training
   examples. of course, our classifier knows nothing at this point, so it
   outputs an equal 10% id203 of our training examples for each
   possible class.

   but how did tensorflow know the probabilities, you might ask; it
   learned the probabilities by calculating the softmax of our results.
   the [34]softmax function takes a set of values and forces their sum to
   equal one, which will give probabilities for each value. any softmax
   value will always be greater than zero and less than one. still
   confused? try running this or read up on what softmax is doing
   [35]mathematically:
sess.run(tf.nn.softmax(tf.zeros([4])))
sess.run(tf.nn.softmax(tf.constant([0.1, 0.005, 2])))

   next, we will create our cross_id178 function, also known as a loss
   or cost function. it measures how good (or bad) of a job we are doing
   at classifying. the higher the cost, the higher the level of
   inaccuracy. it calculates accuracy by comparing the true values from
   y_train to the results of our prediction y for each example. the goal
   is to minimize your loss:
cross_id178 = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=
[1]))

   this function is taking the log of all our predictions y (whose values
   range from 0 to 1) and [36]element wise multiplying by the example   s
   true value y_. if the log function for each value is close to zero, it
   will make the value a large negative number (i.e., -np.log(0.01) =
   4.6), and if it is close to 1, it will make the value a small negative
   number (i.e., -np.log(0.99) = 0.1).
   y = log (x) figure 5. y = log (x). source: [37]lfahlberg on wikimedia
   commons.

   we are essentially penalizing the classifier with a very large number
   if the prediction is confidently incorrect and a very small number if
   the prediction is confidendently correct.

   here is a simple made up python example of a softmax prediction that is
   very confident that the digit is a 3:
j = [0.03, 0.03, 0.01, 0.9, 0.01, 0.01, 0.0025,0.0025, 0.0025, 0.0025]

   let   s create an array label of "3" as a ground truth to compare to our
   softmax function:
k = [0,0,0,1,0,0,0,0,0,0]

   can you guess what value our id168 gives us? can you see how
   the log of    j    would penalize a wrong answer with a large negative
   number? try this to understand:
-np.log(j)
-np.multiply(np.log(j),k)

   this will return nine zeros and a value of 0.1053, which when all
   summed up, we would consider a good prediction. notice what happens
   when we make the same prediction for what is actually a 2:
k = [0,0,1,0,0,0,0,0,0,0]
np.sum(-np.multiply(np.log(j),k))

   now, our cross_id178 function gives us 4.6051, which shows a heavily
   penalized, poorly made prediction. it was heavily penalized due to the
   fact the classifier was very confident that it was a 3 when it actually
   was a 2.

   next we begin to train our classifier. in order to train, we have to
   develop appropriate values for w and b that will give us the lowest
   possible loss.

   below is where we can now assign custom variables for training if we
   wish. any value that is in all caps below is designed to be changed and
   messed with. in fact, i encourage it! first, use these values, then
   later notice what happens when you use too few training examples or too
   high or low of a learning rate.

   if you set train_size to a large number, be prepared to wait for a
   while. at any point, you can re run all the code starting from here and
   try different values:
x_train, y_train = train_size(5500)
x_test, y_test = test_size(10000)
learning_rate = 0.1
train_steps = 2500

   we can now initialize all variables so that they can be used by our
   tensorflow graph:
init = tf.global_variables_initializer()
#if using tensorflow prior to 0.12 use:
#init = tf.initialize_all_variables()
sess.run(init)

   now, we need to train our classifier using id119. we first
   define our training method and some variables for measuring our
   accuracy. the variable training will perform the id119
   optimizer with a chosen learning_rate in order to try to minimize our
   id168 cross_id178:
training = tf.train.gradientdescentoptimizer(learning_rate).minimize(cross_entro
py)
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

   now, we   ll define a loop that repeats train_steps times; for each loop,
   it runs training, feeding in values from x_train and y_train using
   feed_dict. in order to calculate accuracy, it will run accuracy to
   classify the unseen data in x_test by comparing its y and y_test. it is
   vitally important that our test data was unseen and not used for
   training data. if a teacher were to give students a practice exam and
   use that same exam for the final exam, you would have a very biased
   measure of students    knowledge:
for i in range(train_steps+1):
    sess.run(training, feed_dict={x: x_train, y_: y_train})
    if i%100 == 0:
        print('training step:' + str(i) + '  accuracy =  ' + str(sess.run(accura
cy, feed_dict={x: x_test, y_: y_test})) + '  loss = ' + str(sess.run(cross_entro
py, {x: x_train, y_: y_train})))

   in order to visualize what id119 is doing, you have to
   imagine the loss as being a 784 dimensional graph based on y_ and y,
   which contains different values of x, w, and b. if you can   t visualise
   784 dimensions, that   s to be expected. i highly recommend chris olah   s
   [38]blog to learn more amount the dimensions involved with mnist. to
   explain things more simply in two-dimensions, we will use y = x^2:
   parabola y = x^2 figure 6. parabola y = x^2. source: [39]adrignola on
   wikimedia commons.

   for each step in the loop, depending on how large the cross_id178 is
   the classifier will move a learning_rate step toward where it thinks
   cross_id178   s value will be smaller. this lower point is calculated
   by tensorflow using the derivative of the cross_id178, which gives
   the slope of the tangent line at a given point. as it moves toward this
   new point, the values w and b change and the slope decreases. as in the
   case of y = x^2, you can think of this as moving toward x = 0, which is
   also called the minimum. if the learning rate is too small, the
   classifier will take very small steps when learning; if it's too high,
   the steps it takes will be too large, and it may figuratively
      overshoot    the true minimum.
   parabola-antipodera figure 7. solid black line shows tangent line at
   given points. source: [40]tosha on wikimedia commons.

   notice how near the end, the loss was still decreasing but our accuracy
   slightly went down? this shows that we could still minimize our loss on
   our training data, but this may not help us predict the unseen testing
   data used for measuring accuracy. this is also known as overfitting
   (not generalizing). with the default settings, i got an accuracy of
   about 91%. if i wanted to cheat to get 94% accuracy, i could set the
   test examples to 100. this shows how not having enough test examples
   can give you a biased sense of accuracy.

   keep in mind this a very ineffective way to calculate our classifier.
   but i did this on purpose for the sake of learning and experimentation.
   ideally, when training with large data sets, you train using small
   batches of training data at a time, not all at once. if you would like
   to learn how to do this, follow the tutorial on tensorflow   s
   [41]website.

   this is my favorite part. now that we have calculated our weight
   cheatsheet, we can create a graph with the following code:
for i in range(10):
    plt.subplot(2, 5, i+1)
    weight = sess.run(w)[:,i]
    plt.title(i)
    plt.imshow(weight.reshape([28,28]), cmap=plt.get_cmap('seismic'))
    frame1 = plt.gca()
    frame1.axes.get_xaxis().set_visible(false)
    frame1.axes.get_yaxis().set_visible(false)

   now let   s visualize it:
plt.show()

   visualization of our weights from 0-9 figure 8. source: justin francis.

   this is a visualization of our weights from 0-9. this is the most
   important aspect of our classifier. the bulk of the work of machine
   learning is figuring out what the optimal weights are; once they are
   calculated, you have the    cheat sheet    and can easily find answers.
   (this is part of why neural networks can be readily ported to mobile
   devices; the model, once trained, doesn   t take up that much room to
   store or computing power to calculate.) our classifier makes its
   prediction by comparing how similar or different the digit is to the
   red and blue. i like to think the darker the red, the better of a hit;
   white as neutral; and blue as misses.

   so, now that we have our cheat sheet, let   s load one example and apply
   our classifier to that one example:
x_train, y_train = train_size(1)
display_digit(0)

   let   s look at our predictor y:
answer = sess.run(y, feed_dict={x: x_train})
print(answer)

   this gives us a (1x10) matrix with each column containing one
   id203:
[[  2.12480136e-05   1.16469264e-05   8.96317810e-02   1.92015115e-02
    8.20863759e-04   1.25168199e-05   3.85381973e-05   8.53746116e-01
    6.91888575e-03   2.95970142e-02]]

   but this is not very useful for us. so, we use the argmax function to
   return the position of the highest value and that gives us our
   prediction.
answer.argmax()

   so, let us now take our knowledge to create a function to make
   predictions on a random digit in this data set:
def display_compare(num):
    # this will load one training example
    x_train = mnist.train.images[num,:].reshape(1,784)
    y_train = mnist.train.labels[num,:]
    # this gets our label as a integer
    label = y_train.argmax()
    # this gets our prediction as a integer
    prediction = sess.run(y, feed_dict={x: x_train}).argmax()
    plt.title('prediction: %d label: %d' % (prediction, label))
    plt.imshow(x_train.reshape([28,28]), cmap=plt.get_cmap('gray_r'))
    plt.show()

   and now try the function out:
display_compare(ran.randint(0, 55000))

   can you find any that guessed incorrectly? if you enter
   display_compare(2), you will find one digit the classifier got wrong.
   why do you think it got it wrong?

   this is where this tutorial gets fun: notice what happens to the
   visualizations of the weights when you use 1-10 training examples. it
   becomes clear that using too little data makes it very hard to
   generalize. here is a animation showing how the weights change as you
   increase your training size. can you see what is happening?
   training size animation figure 9. source: justin francis.

   you can also see the limitations of a linear classifier; at a certain
   point, feeding it more data doesn   t help increase your accuracy
   drastically. what do you think would happen if we tried to classify a
      1    that was drawn on the very left side of the square? it would have a
   very hard time classifying it because in all of its training examples,
   the 1 was very close to the center.

   i hope this helps make you more appreciative to know just how much is
   going on behind the scenes in mnist. keep in mind that this a neural
   network with two layers; it   s not deep learning. in order to get close
   to near-perfect accuracy, we have to start thinking convolutionally
   deep.

   if you would prefer to run a more interactive session, here is my
   [42]github repo with the jupyter notebook version of this. i had a lot
   of fun writing this and learning along the way. thanks for reading, and
   most of all, i really hope something new clicked in your brain today.
   article image: numbers 0-9. (source: [43]denise krebs on flickr).

   share
    1. [44]tweet
    2.
    3.
     __________________________________________________________________

   [45]justin francis

[46]justin francis

   justin francis is currently an undergraduate student at the university
   of alberta in canada. justin is also on the software team for the
   university's engineering club 'autonomous robotic vehicle project'
   (arvp.org) helping implement and experiment with deep learning and
   id23 algorithms.  in the past, he was the founder and
   educator at a non-profit community cooperative bicycle shop and spent
   two years exploring and experiencing the georgia strait on his
   sailboat.
   [47]more
     __________________________________________________________________

   [48]bots landscape.

   [49]ai

[50]infographic: the bot platform ecosystem

   by [51]jon bruner

   a look at the artificial intelligence and messaging platforms behind
   the fast-growing chatbot community

   video
   [52]vertical forest, milan.

   [53]ai

[54]evolve ai

   by [55]naveen rao

   naveen rao explains how intel nervana is evolving the ai stack from
   silicon to the cloud.

   [56]welcome sign at o'reilly ai conference 2016

   [57]ai

[58]highlights from the o'reilly ai conference in new york 2016

   by [59]mac slocum

   watch highlights covering artificial intelligence, machine learning,
   intelligence engineering, and more. from the o'reilly ai conference in
   new york 2016.

   video
   [60]close up of uber's self driving car in pittsburgh.

   [61]ai

[62]how ai is propelling driverless cars, the future of surface transport

   by [63]shahin farshchi

   shahin farshchi examines role artificial intelligence will play in
   driverless cars.

about us

     * [64]our company
     * [65]teach/speak/write
     * [66]careers
     * [67]customer service
     * [68]contact us

site map

     * [69]ideas
     * [70]learning
     * [71]topics
     * [72]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [73]terms of service     [74]privacy policy     [75]editorial independence

   animal

   iframe: [76]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/ai
  27. https://www.oreilly.com/people/justin-francis
  28. https://www.flickr.com/photos/mrsdkrebs/9728631593
  29. https://en.wikipedia.org/wiki/mnist_database
  30. https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/index.html
  31. https://alliseesolutions.wordpress.com/2016/07/05/how-to-install-gpu-tensorflow-0-9-from-sources-ubuntu-14-04/
  32. https://en.wikipedia.org/wiki/matrix_multiplication
  33. https://commons.wikimedia.org/wiki/file:matrix_multiplication_qtl3.svg
  34. https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html#softmax-regressions
  35. https://en.wikipedia.org/wiki/softmax_function
  36. https://en.wikipedia.org/wiki/hadamard_product_(matrices)
  37. https://commons.wikimedia.org/wiki/file:logx.svg
  38. http://colah.github.io/posts/2014-10-visualizing-mnist/
  39. https://commons.wikimedia.org/wiki/file:y=x^2.svg
  40. https://commons.wikimedia.org/wiki/file:parabola-antipodera.gif
  41. https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html
  42. https://github.com/wagonhelm/nanmnist
  43. https://www.flickr.com/photos/mrsdkrebs/9728631593
  44. https://twitter.com/share
  45. https://www.oreilly.com/people/justin-francis
  46. https://www.oreilly.com/people/justin-francis
  47. https://www.oreilly.com/people/justin-francis
  48. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  49. https://www.oreilly.com/topics/ai
  50. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  51. https://www.oreilly.com/people/b1d73-jon-bruner
  52. https://www.oreilly.com/ideas/evolve-ai
  53. https://www.oreilly.com/topics/ai
  54. https://www.oreilly.com/ideas/evolve-ai
  55. https://www.oreilly.com/people/14d38-naveen-rao
  56. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  57. https://www.oreilly.com/topics/ai
  58. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  59. https://www.oreilly.com/people/0d2c1-mac-slocum
  60. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  61. https://www.oreilly.com/topics/ai
  62. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  63. https://www.oreilly.com/people/c7521-shahin-farshchi
  64. http://oreilly.com/about/
  65. http://oreilly.com/work-with-us.html
  66. http://oreilly.com/careers/
  67. http://shop.oreilly.com/category/customer-service.do
  68. http://shop.oreilly.com/category/customer-service.do
  69. https://www.oreilly.com/ideas
  70. https://www.oreilly.com/topics/oreilly-learning
  71. https://www.oreilly.com/topics
  72. https://www.oreilly.com/all
  73. http://oreilly.com/terms/
  74. http://oreilly.com/privacy.html
  75. http://www.oreilly.com/about/editorial_independence.html
  76. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  78. https://www.oreilly.com/
  79. https://www.facebook.com/oreilly/
  80. https://twitter.com/oreillymedia
  81. https://www.youtube.com/user/oreillymedia
  82. https://plus.google.com/+oreillymedia
  83. https://www.linkedin.com/company/oreilly-media
  84. https://www.oreilly.com/
