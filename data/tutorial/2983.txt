   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 2 kernel
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]ipython-notebooks
    2. [10]notebooks
    3. [11]ml

machine learning exercise 1 - id75[12]  

   this notebook covers a python-based solution for the first programming
   exercise of the machine learning class on coursera. please refer to the
   [13]exercise text for detailed descriptions and equations.

   in this exercise we'll implement simple id75 using
   id119 and apply it to an example problem. we'll also extend
   our implementation to handle multiple variables and apply it to a
   slightly more difficult example.

id75 with one variable[14]  

   in the first part of the exercise, we're tasked with implementing
   id75 with one variable to predict profits for a food
   truck. suppose you are the ceo of a restaurant franchise and are
   considering different cities for opening a new outlet. the chain
   already has trucks in various cities and you have data for profits and
   populations from the cities.

   let's start by importing some libraries and examining the data.
   in [1]:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

   in [2]:
import os
path = os.getcwd() + '\data\ex1data1.txt'
data = pd.read_csv(path, header=none, names=['population', 'profit'])
data.head()

   out[2]:
     population profit
   0 6.1101     17.5920
   1 5.5277     9.1302
   2 8.5186     13.6620
   3 7.0032     11.8540
   4 5.8598     6.8233
   in [3]:
data.describe()

   out[3]:
         population  profit
   count 97.000000  97.000000
   mean  8.159800   5.839135
    std  3.869884   5.510262
    min  5.026900   -2.680700
    25%  5.707700   1.986900
    50%  6.589400   4.562300
    75%  8.578100   7.046700
    max  22.203000  24.147000

   let's plot it to get a better idea of what the data looks like.
   in [4]:
data.plot(kind='scatter', x='population', y='profit', figsize=(12,8))

   out[4]:
<matplotlib.axes._subplots.axessubplot at 0xd140198>

   [x+iwfvswyr8daaaaabjru5erkjggg== ]

   now let's implement id75 using id119 to
   minimize the cost function. the equations implemented in the following
   code samples are detailed in "ex1.pdf" in the "exercises" folder.

   first we'll create a function to compute the cost of a given solution
   (characterized by the parameters theta).
   in [5]:
def computecost(x, y, theta):
    inner = np.power(((x * theta.t) - y), 2)
    return np.sum(inner) / (2 * len(x))

   let's add a column of ones to the training set so we can use a
   vectorized solution to computing the cost and gradients.
   in [6]:
data.insert(0, 'ones', 1)

   now let's do some variable initialization.
   in [7]:
# set x (training data) and y (target variable)
cols = data.shape[1]
x = data.iloc[:,0:cols-1]
y = data.iloc[:,cols-1:cols]

   let's take a look to make sure x (training set) and y (target variable)
   look correct.
   in [8]:
x.head()

   out[8]:
     ones population
   0 1    6.1101
   1 1    5.5277
   2 1    8.5186
   3 1    7.0032
   4 1    5.8598
   in [9]:
y.head()

   out[9]:
     profit
   0 17.5920
   1 9.1302
   2 13.6620
   3 11.8540
   4 6.8233

   the cost function is expecting numpy matrices so we need to convert x
   and y before we can use them. we also need to initialize theta.
   in [10]:
x = np.matrix(x.values)
y = np.matrix(y.values)
theta = np.matrix(np.array([0,0]))

   here's what theta looks like.
   in [11]:
theta

   out[11]:
matrix([[0, 0]])

   let's take a quick look at the shape of our matrices.
   in [12]:
x.shape, theta.shape, y.shape

   out[12]:
((97l, 2l), (1l, 2l), (97l, 1l))

   now let's compute the cost for our initial solution (0 values for
   theta).
   in [13]:
computecost(x, y, theta)

   out[13]:
32.072733877455676

   so far so good. now we need to define a function to perform gradient
   descent on the parameters theta using the update rules defined in the
   text.
   in [14]:
def gradientdescent(x, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape))
    parameters = int(theta.ravel().shape[1])
    cost = np.zeros(iters)

    for i in range(iters):
        error = (x * theta.t) - y

        for j in range(parameters):
            term = np.multiply(error, x[:,j])
            temp[0,j] = theta[0,j] - ((alpha / len(x)) * np.sum(term))

        theta = temp
        cost[i] = computecost(x, y, theta)

    return theta, cost

   initialize some additional variables - the learning rate alpha, and the
   number of iterations to perform.
   in [15]:
alpha = 0.01
iters = 1000

   now let's run the id119 algorithm to fit our parameters
   theta to the training set.
   in [16]:
g, cost = gradientdescent(x, y, theta, alpha, iters)
g

   out[16]:
matrix([[-3.24140214,  1.1272942 ]])

   finally we can compute the cost (error) of the trained model using our
   fitted parameters.
   in [17]:
computecost(x, y, g)

   out[17]:
4.5159555030789118

   now let's plot the linear model along with the data to visually see how
   well it fits.
   in [18]:
x = np.linspace(data.population.min(), data.population.max(), 100)
f = g[0, 0] + (g[0, 1] * x)

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(x, f, 'r', label='prediction')
ax.scatter(data.population, data.profit, label='traning data')
ax.legend(loc=2)
ax.set_xlabel('population')
ax.set_ylabel('profit')
ax.set_title('predicted profit vs. population size')

   out[18]:
<matplotlib.text.text at 0xd35a518>

   [v1iqj+odedeaea8e3lk+nt6al0lnw5loszpe1uoxlgg+
   fhhbyijvz8vr7ycr4zgmucyl9drarhwhebdyqvaumfrdutpwlhvzduwkw8q4ac2wiskeqki
   ljqw0
   nsjubdaxd32lfj6jqox7vculktsqiixvg5juiyjidfbcsqm76lfikoayneosapszhzjug5y
   jlirj
   knjyjlqsjenkyratszik5wsilirjkniyreusjek5gailszkknp4fs+xbbeut4mqaaaaasuv
   ork5c yii= ]

   looks pretty good! since the gradient decent function also outputs a
   vector with the cost at each training iteration, we can plot that as
   well. notice that the cost always decreases - this is an example of a
   id76 problem.
   in [19]:
fig, ax = plt.subplots(figsize=(12,8))
ax.plot(np.arange(iters), cost, 'r')
ax.set_xlabel('iterations')
ax.set_ylabel('cost')
ax.set_title('error vs. training epoch')

   out[19]:
<matplotlib.text.text at 0xd5bccc0>

   [h6qam3jsvgugaaaaaelf tksuqmcc ]

id75 with multiple variables[15]  

   exercise 1 also included a housing price data set with 2 variables
   (size of the house in square feet and number of bedrooms) and a target
   (price of the house). let's use the techniques we already applied to
   analyze that data set as well.
   in [20]:
path = os.getcwd() + '\data\ex1data2.txt'
data2 = pd.read_csv(path, header=none, names=['size', 'bedrooms', 'price'])
data2.head()

   out[20]:
     size bedrooms price
   0 2104 3        399900
   1 1600 3        329900
   2 2400 3        369000
   3 1416 2        232000
   4 3000 4        539900

   for this task we add another pre-processing step - normalizing the
   features. this is very easy with pandas.
   in [21]:
data2 = (data2 - data2.mean()) / data2.std()
data2.head()

   out[21]:
       size    bedrooms    price
   0 0.130010  -0.223675 0.475747
   1 -0.504190 -0.223675 -0.084074
   2 0.502476  -0.223675 0.228626
   3 -0.735723 -1.537767 -0.867025
   4 1.257476  1.090417  1.595389

   now let's repeat our pre-processing steps from part 1 and run the
   id75 procedure on the new data set.
   in [22]:
# add ones column
data2.insert(0, 'ones', 1)

# set x (training data) and y (target variable)
cols = data2.shape[1]
x2 = data2.iloc[:,0:cols-1]
y2 = data2.iloc[:,cols-1:cols]

# convert to matrices and initialize theta
x2 = np.matrix(x2.values)
y2 = np.matrix(y2.values)
theta2 = np.matrix(np.array([0,0,0]))

# perform id75 on the data set
g2, cost2 = gradientdescent(x2, y2, theta2, alpha, iters)

# get the cost (error) of the model
computecost(x2, y2, g2)

   out[22]:
0.13070336960771897

   we can take a quick look at the training progess for this one as well.
   in [23]:
fig, ax = plt.subplots(figsize=(12,8))
ax.plot(np.arange(iters), cost2, 'r')
ax.set_xlabel('iterations')
ax.set_ylabel('cost')
ax.set_title('error vs. training epoch')

   out[23]:
<matplotlib.text.text at 0xd7bb240>

   [d78fehqxipj4aaaaaelftksu qmcc ]

   instead of implementing these algorithms from scratch, we could also
   use scikit-learn's id75 function. let's apply
   scikit-learn's linear regressio algorithm to the data from part 1 and
   see what it comes up with.
   in [24]:
from sklearn import linear_model
model = linear_model.linearregression()
model.fit(x, y)

   out[24]:
linearregression(copy_x=true, fit_intercept=true, normalize=false)

   here's what the scikit-learn model's predictions look like.
   in [25]:
x = np.array(x[:, 1].a1)
f = model.predict(x).flatten()

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(x, f, 'r', label='prediction')
ax.scatter(data.population, data.profit, label='traning data')
ax.legend(loc=2)
ax.set_xlabel('population')
ax.set_ylabel('profit')
ax.set_title('predicted profit vs. population size')

   out[25]:
<matplotlib.text.text at 0xf258860>

   [nt6dhfv3agaaaaasuvork5cyii= ]

   that's it! thanks for reading. in exercise 2 we'll take a look at
   id28 for classification problems.

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [16]fastly, rendered by [17]rackspace

   nbviewer github [18]repository.

   nbviewer version: [19]33c4683

   nbconvert version: [20]5.4.0

   rendered (fri, 05 apr 2019 18:20:22 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/jdwittenauer/ipython-notebooks/blob/master/notebooks/ml/ml-exercise1.ipynb
   5. https://nbviewer.jupyter.org/github/jdwittenauer/ipython-notebooks/blob/master/notebooks/ml/ml-exercise1.ipynb
   6. https://github.com/jdwittenauer/ipython-notebooks/blob/master/notebooks/ml/ml-exercise1.ipynb
   7. https://mybinder.org/v2/gh/jdwittenauer/ipython-notebooks/master?filepath=notebooks/ml/ml-exercise1.ipynb
   8. https://raw.githubusercontent.com/jdwittenauer/ipython-notebooks/master/notebooks/ml/ml-exercise1.ipynb
   9. https://nbviewer.jupyter.org/github/jdwittenauer/ipython-notebooks/tree/master
  10. https://nbviewer.jupyter.org/github/jdwittenauer/ipython-notebooks/tree/master/notebooks
  11. https://nbviewer.jupyter.org/github/jdwittenauer/ipython-notebooks/tree/master/notebooks/ml
  12. https://nbviewer.jupyter.org/github/jdwittenauer/ipython-notebooks/blob/master/notebooks/ml/ml-exercise1.ipynb#machine-learning-exercise-1---linear-regression
  13. https://github.com/jdwittenauer/ipython-notebooks/blob/master/exercises/ml/ex1.pdf
  14. https://nbviewer.jupyter.org/github/jdwittenauer/ipython-notebooks/blob/master/notebooks/ml/ml-exercise1.ipynb#linear-regression-with-one-variable
  15. https://nbviewer.jupyter.org/github/jdwittenauer/ipython-notebooks/blob/master/notebooks/ml/ml-exercise1.ipynb#linear-regression-with-multiple-variables
  16. http://www.fastly.com/
  17. https://developer.rackspace.com/?nbviewer=awesome
  18. https://github.com/jupyter/nbviewer
  19. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  20. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
