   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 2 kernel
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]python_for_data_science
    2. [10]python_for_data_science_all.ipynb

python for data science[11]  

   [12]joe mccarthy, data scientist, [13]indeed
   in [1]:
from ipython.display import display, image, html

1. introduction[14]  

   [15]python-logo-master-v3-tm.png this short primer on [16]python is
   designed to provide a rapid "on-ramp" to enable computer programmers
   who are already familiar with concepts and constructs in other
   programming languages learn enough about python to facilitate the
   effective use of open-source and proprietary python-based machine
   learning and data science tools.

   [17]nltk_book_cover.gif the primer is motivated, in part, by the
   approach taken in the [18]natural language toolkit (nltk) book, which
   provides a rapid on-ramp for using python and the open-source [19]nltk
   library to develop programs using natural language processing
   techniques (many of which involve [20]machine learning).

   the [21]python tutorial offers a more comprehensive primer, and opens
   with an excellent - if biased - overview of some of the general
   strengths of the python programming language:

     python is an easy to learn, powerful programming language. it has
     efficient high-level data structures and a simple but effective
     approach to object-oriented programming. python   s elegant syntax and
     dynamic typing, together with its interpreted nature, make it an
     ideal language for scripting and rapid application development in
     many areas on most platforms.

   [22]python scripting for computational science cover [23]hans petter
   langtangen, author of [24]python scripting for computational science,
   emphasizes the utility of python for many of the common tasks in all
   areas of computational science:

     very often programming is about shuffling data in and out of
     different tools, converting one data format to another, extracting
     numerical data from a text, and administering numerical experiments
     involving a large number of data files and directories. such tasks
     are much faster to accomplish in a language like python than in
     fortran, c, c++, c#, or java

   [25]foster provost, co-author of [26]data science for business,
   describes why python is such a useful programming language for
   practical data science in [27]python: a practical tool for data
   science, :

     the practice of data science involves many interrelated but
     different activities, including accessing data, manipulating data,
     computing statistics about data, plotting/graphing/visualizing data,
     building predictive and explanatory models from data, evaluating
     those models on yet more data, integrating models into production
     systems, etc. one option for the data scientist is to learn several
     different software packages that each specialize in one or two of
     these things, but don   t do them all well, plus learn a programming
     language to tie them together. (or do a lot of manual work.)

     an alternative is to use a general-purpose, high-level programming
     language that provides libraries to do all these things. python is
     an excellent choice for this. it has a diverse range of open source
     libraries for just about everything the data scientist will do. it
     is available everywhere; high performance python interpreters exist
     for running your code on almost any operating system or
     architecture. python and most of its libraries are both open source
     and free. contrast this with common software packages that are
     available in a course via an academic license, yet are extremely
     expensive to license and use in industry.

   [28]scikit-learn-logo-small.png the goal of this primer is to provide
   efficient and sufficient scaffolding for software engineers with no
   prior knowledge of python to be able to effectively use python-based
   tools for data science research and development, such as the
   open-source library [29]scikit-learn. there is another, more
   comprehensive tutorial for scikit-learn, [30]python scientific lecture
   notes, that includes coverage of a number of other useful python
   open-source libraries used by scikit-learn ([31]numpy, [32]scipy and
   [33]matplotlib) - all highly recommended ... and, to keep things
   simple, all beyond the scope of this primer.

   using an ipython notebook as a delivery vehicle for this primer was
   motivated by brian granger's inspiring tutorial, [34]the ipython
   notebook: get close to your data with python and javascript, one of the
   [35]highlights from my strata 2014 conference experience. you can run
   this notebook locally in a browser once you [36]install ipython
   notebook.

   one final note on external resources: the [37]python style guide
   (pep-0008) offers helpful tips on how best to format python code.
   [38]code like a pythonista offers a number of additional tips on python
   programming style and philosophy, several of which are incorporated
   into this primer.

   we will focus entirely on using python within the interpreter
   environment (as supported within an ipython notebook). python scripts -
   files containing definitions of functions and variables, and typically
   including code invoking some of those functions - can also be run from
   a command line. using python scripts from the command line may be the
   subject of a future primer.

   to help motivate the data science-oriented python programming examples
   provided in this primer, we will start off with a brief overview of
   basic concepts and terminology in data science.

2. data science: basic concepts[39]  

data science and data mining[40]  

   [41]datascienceforbusiness_cover.jpg foster provost and [42]tom fawcett
   offer succinct descriptions of data science and data mining in [43]data
   science for business:

     data science involves principles, processes and techniques for
     understanding phenomena via the (automated) analysis of data.

     data mining is the extraction of knowledge from data, via
     technologies that incorporate these principles.

knowledge discovery, data mining and machine learning[44]  

   provost & fawcett also offer some history and insights into the
   relationship between data mining and machine learning, terms which are
   often used somewhat interchangeably:

     the field of data mining (or kdd: knowledge discovery and data
     mining) started as an offshoot of machine learning, and they remain
     closely linked. both fields are concerned with the analysis of data
     to find useful or informative patterns. techniques and algorithms
     are shared between the two; indeed, the areas are so closely related
     that researchers commonly participate in both communities and
     transition between them seaid113ssly. nevertheless, it is worth
     pointing out some of the differences to give perspective.

     speaking generally, because machine learning is concerned with many
     types of performance improvement, it includes subfields such as
     robotics and id161 that are not part of kdd. it also is
     concerned with issues of agency and cognition     how will an
     intelligent agent use learned knowledge to reason and act in its
     environment     which are not concerns of data mining.

     historically, kdd spun off from machine learning as a research field
     focused on concerns raised by examining real-world applications, and
     a decade and a half later the kdd community remains more concerned
     with applications than machine learning is. as such, research
     focused on commercial applications and business issues of data
     analysis tends to gravitate toward the kdd community rather than to
     machine learning. kdd also tends to be more concerned with the
     entire process of data analytics: data preparation, model learning,
     evaluation, and so on.

cross industry standard process for data mining (crisp-dm)[45]  

   the [46]cross industry standard process for data mining introduced a
   process model for data mining in 2000 that has become widely adopted.

   [47]crisp-dm_process_diagram

   the model emphasizes the iterative nature of the data mining process,
   distinguishing several different stages that are regularly revisited in
   the course of developing and deploying data-driven solutions to
   business problems:
     * business understanding
     * data understanding
     * data preparation
     * modeling
     * deployment

   we will be focusing primarily on using python for data preparation and
   modeling.

data science workflow[48]  

   [49]philip guo presents a [50]data science workflow offering a slightly
   different process model emhasizing the importance of reflection and
   some of the meta-data, data management and bookkeeping challenges that
   typically arise in the data science process. his 2012 phd thesis,
   [51]software tools to facilitate research programming, offers an
   insightful and more comprehensive description of many of these
   challenges.

   [52]pguo-data-science-overview.jpg

   provost & fawcett list a number of different tasks in which data
   science techniques are employed:
     * classification and class id203 estimation
     * regression (aka value estimation)
     * similarity matching
     * id91
     * co-occurrence grouping (aka frequent itemset mining, association
       rule discovery, market-basket analysis)
     * profiling (aka behavior description, fraud / anomaly detection)
     * link prediction
     * data reduction
     * causal modeling

   we will be focusing primarily on classification and class id203
   estimation tasks, which are defined by provost & fawcett as follows:

     classification and class id203 estimation attempt to predict,
     for each individual in a population, which of a (small) set of
     classes this individual belongs to. usually the classes are mutually
     exclusive. an example classification question would be:    among all
     the customers of megatelco, which are likely to respond to a given
     offer?    in this example the two classes could be called will respond
     and will not respond.

   to further simplify this primer, we will focus exclusively on
   supervised methods, in which the data is explicitly labeled with
   classes. there are also unsupervised methods that involve working with
   data in which there are no pre-specified class labels.

supervised classification[53]  

   the [54]natural language toolkit (nltk) book provides a diagram and
   succinct description (below, with italics and bold added for emphasis)
   of supervised classification:

   [55]nltk_ch06_supervised-classification.png

     supervised classification. (a) during training, a feature extractor
     is used to convert each input value to a feature set. these feature
     sets, which capture the basic information about each input that
     should be used to classify it, are discussed in the next section.
     pairs of feature sets and labels are fed into the machine learning
     algorithm to generate a model. (b) during prediction, the same
     feature extractor is used to convert unseen inputs to feature sets.
     these feature sets are then fed into the model, which generates
     predicted labels.

data mining terminology[56]  

     * structured data has simple, well-defined patterns (e.g., a table or
       graph)
     * unstructured data has less well-defined patterns (e.g., text,
       images)
     * model: a pattern that captures / generalizes regularities in data
       (e.g., an equation, set of rules, decision tree)
     * attribute (aka variable, feature, signal, column): an element used
       in a model
     * instance (aka example, feature vector, row): a representation of a
       single entity being modeled
     * target attribute (aka dependent variable, class label): the class /
       type / category of an entity being modeled

data mining example: uci mushroom dataset[57]  

   the [58]center for machine learning and intelligent systems at the
   university of california, irvine (uci), hosts a [59]machine learning
   repository containing over 200 publicly available data sets.

   [60]mushroom we will use the [61]mushroom data set, which forms the
   basis of several examples in chapter 3 of the provost & fawcett data
   science book.

   the following description of the dataset is provided at the uci
   repository:

     this data set includes descriptions of hypothetical samples
     corresponding to 23 species of gilled mushrooms in the agaricus and
     lepiota family (pp. 500-525 [the audubon society field guide to
     north american mushrooms, 1981]). each species is identified as
     definitely edible, definitely poisonous, or of unknown edibility and
     not recommended. this latter class was combined with the poisonous
     one. the guide clearly states that there is no simple rule for
     determining the edibility of a mushroom; no rule like leaflets
     three, let it be'' for poisonous oak and ivy.

     number of instances: 8124

     number of attributes: 22 (all nominally valued)

     attribute information: (classes: edible=e, poisonous=p)
    1. cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s
    2. cap-surface: fibrous=f, grooves=g, scaly=y, smooth=s
    3. cap-color: brown=n ,buff=b, cinnamon=c, gray=g, green=r, pink=p,
       purple=u, red=e, white=w, yellow=y
    4. bruises?: bruises=t, no=f
    5. odor: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m,
       none=n, pungent=p, spicy=s
    6. gill-attachment: attached=a, descending=d, free=f, notched=n
    7. gill-spacing: close=c, crowded=w, distant=d
    8. gill-size: broad=b, narrow=n
    9. gill-color: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r,
       orange=o, pink=p, purple=u, red=e, white=w, yellow=y
   10. stalk-shape: enlarging=e, tapering=t
   11. stalk-root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z,
       rooted=r, missing=?
   12. stalk-surface-above-ring: fibrous=f, scaly=y, silky=k, smooth=s
   13. stalk-surface-below-ring: fibrous=f, scaly=y, silky=k, smooth=s
   14. stalk-color-above-ring: brown=n, buff=b, cinnamon=c, gray=g,
       orange=o, pink=p, red=e, white=w, yellow=y
   15. stalk-color-below-ring: brown=n, buff=b, cinnamon=c, gray=g,
       orange=o, pink=p, red=e, white=w, yellow=y
   16. veil-type: partial=p, universal=u
   17. veil-color: brown=n, orange=o, white=w, yellow=y
   18. ring-number: none=n, one=o, two=t
   19. ring-type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n,
       pendant=p, sheathing=s, zone=z
   20. spore-print-color: black=k, brown=n, buff=b, chocolate=h, green=r,
       orange=o, purple=u, white=w, yellow=y
   21. population: abundant=a, clustered=c, numerous=n, scattered=s,
       several=v, solitary=y
   22. habitat: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w,
       woods=d

     missing attribute values: 2480 of them (denoted by "?"), all for
     attribute #11.

     class distribution: -- edible: 4208 (51.8%) -- poisonous: 3916
     (48.2%) -- total: 8124 instances

   the [62]data file associated with this dataset has one instance of a
   hypothetical mushroom per line, with abbreviations for the values of
   the class and each of the other 22 attributes separated by commas.

   here is a sample line from the data file:

   p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d

   this instance represents a mushroom with the following attribute values
   (highlighted in bold):

   class: edible=e, poisonous=p
    1. cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s
    2. cap-surface: fibrous=f, grooves=g, scaly=y, smooth=s
    3. cap-color: brown=n ,buff=b, cinnamon=c, gray=g, green=r, pink=p,
       purple=u, red=e, white=w, yellow=y
    4. bruises?: bruises=t, no=f
    5. odor: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m,
       none=n, pungent=p, spicy=s
    6. gill-attachment: attached=a, descending=d, free=f, notched=n
    7. gill-spacing: close=c, crowded=w, distant=d
    8. gill-size: broad=b, narrow=n
    9. gill-color: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r,
       orange=o, pink=p, purple=u, red=e, white=w, yellow=y
   10. stalk-shape: enlarging=e, tapering=t
   11. stalk-root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z,
       rooted=r, missing=?
   12. stalk-surface-above-ring: fibrous=f, scaly=y, silky=k, smooth=s
   13. stalk-surface-below-ring: fibrous=f, scaly=y, silky=k, smooth=s
   14. stalk-color-above-ring: brown=n, buff=b, cinnamon=c, gray=g,
       orange=o, pink=p, red=e, white=w, yellow=y
   15. stalk-color-below-ring: brown=n, buff=b, cinnamon=c, gray=g,
       orange=o, pink=p, red=e, white=w, yellow=y
   16. veil-type: partial=p, universal=u
   17. veil-color: brown=n, orange=o, white=w, yellow=y
   18. ring-number: none=n, one=o, two=t
   19. ring-type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n,
       pendant=p, sheathing=s, zone=z
   20. spore-print-color: black=k, brown=n, buff=b, chocolate=h, green=r,
       orange=o, purple=u, white=w, yellow=y
   21. population: abundant=a, clustered=c, numerous=n, scattered=s,
       several=v, solitary=y
   22. habitat: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w,
       woods=d

   building a model with this data set will serve as a motivating example
   throughout much of this primer.

3. python: basic concepts[63]  

a note on python 2 vs. python 3[64]  

   there are 2 major versions of python in widespread use: [65]python 2
   and [66]python 3. python 3 has some features that are not backward
   compatible with python 2, and some python 2 libraries have not been
   updated to work with python 3. i have been using python 2, primarily
   because i use some of those python 2[-only] libraries, but an
   increasing proportion of them are migrating to python 3, and i
   anticipate shifting to python 3 in the near future.

   for more on the topic, i recommend a very well documented ipython
   notebook, which includes numerous helpful examples and links, by
   [67]sebastian raschka, [68]key differences between python 2.7.x and
   python 3.x, the [69]cheat sheet: writing python 2-3 compatible code by
   ed schofield ... or [70]googling python 2 vs 3.

   [71]nick coghlan, a cpython core developer, sent me an email suggesting
   that relatively minor changes in this notebook would enable it to run
   with python 2 or python 3: importing the print_function from the
   [72]__future__ module, and changing my [73]print statements (python 2)
   to [74]print function calls (python 3). although a relatively minor
   conceptual change, it necessitated the changing of many individual
   cells to reflect the python 3 print syntax.

   i decided to import the division module from the future, as i find
   [75]the use of / for "true division" - and the use of // for "floor
   division" - to be more aligned with my intuition. i also needed to
   replace a few functions that are no longer available in python 3 with
   related functions that are available in both versions; i've added notes
   in nearby cells where the incompatible functions were removed
   explaining why they are related ... and no longer available.

   the differences are briefly illustrated below, with print statements or
   function calls before and after the importing of the python 3 versions
   of the print function and division operator.
   in [2]:
print 1, "/", 2, "=", 1 / 2

1 / 2 = 0

   in [3]:
print(1, "/", 2, "=", 1 / 2)

(1, '/', 2, '=', 0)

   in [4]:
from __future__ import print_function, division

   in [5]:
print 1, "/", 2, "=", 1 / 2

  file "<ipython-input-5-168a26d8ec56>", line 1
    print 1, "/", 2, "=", 1 / 2
          ^
syntaxerror: invalid syntax

   in [6]:
print(1, "/", 2, "=", 1 / 2)

1 / 2 = 0.5

names (identifiers), strings & binding values to names (assignment)[76]  

   the sample instance of a mushroom shown above can be represented as a
   string.

   a python string ([77]str) is a sequence of 0 or more characters
   enclosed within a pair of single quotes (') or a pair of double quotes
   (").
   in [7]:
'p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d'

   out[7]:
'p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d'

   python [78]identifiers (or [79]names) are composed of letters, numbers
   and/or underscores ('_'), starting with a letter or underscore. python
   identifiers are case sensitive. although camelcase identifiers can be
   used, it is generally considered more [80]pythonic to use underscores.
   python variables and functions typically start with lowercase letters;
   python classes start with uppercase letters.

   the following [81]assignment statement binds the value of the string
   shown above to the name single_instance_str. typing the name on the
   subsequent line will cause the intepreter to print the value bound to
   that name.
   in [8]:
single_instance_str = 'p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d'
single_instance_str

   out[8]:
'p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d'

printing[82]  

   the [83]print function writes the value of its comma-delimited
   arguments to [84]sys.stdout (typically the console). each value in the
   output is separated by a single blank space.
   in [9]:
print('a', 'b', 'c', 1, 2, 3)
print('instance 1:', single_instance_str)

a b c 1 2 3
instance 1: p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d

   the print function has an optional keyword argument, end. when this
   argument is used and its value does not include '\n' (newline
   character), the output cursor will not advance to the next line.
   in [10]:
print('a', 'b')  # no end argument
print('c')
print ('a', 'b', end='...\n')  # end includes '\n' --> output cursor advancees t
o next line
print ('c')
print('a', 'b', end=' ')  # end=' ' --> use a space rather than newline at the e
nd of the line
print('c')  # so that subsequent printed output will appear on same line

a b
c
a b...
c
a b c

comments[85]  

   the python comment character is '#': anything after '#' on the line is
   ignored by the python interpreter. pep8 style guidelines recommend
   using at least 2 blank spaces before an inline comment that appears on
   the same line as any code.

   multi-line strings can be used within code blocks to provide multi-line
   comments.

   multi-line strings are delimited by pairs of triple quotes (''' or
   """). any newlines in the string will be represented as '\n' characters
   in the string.
   in [11]:
'''
this is
a mult-line
string'''

   out[11]:
'\nthis is\na mult-line\nstring'

   in [12]:
print('before comment')  # this is an inline comment
'''
this is
a multi-line
comment
'''
print('after comment')

before comment
after comment

   multi-line strings can be printed, in which case the embedded newline
   ('\n') characters will be converted to newlines in the output.
   in [13]:
print('''
this is
a mult-line
string''')

this is
a mult-line
string

lists[86]  

   a [87]list is an ordered sequence of 0 or more comma-delimited elements
   enclosed within square brackets ('[', ']'). the python
   [88]str.split(sep) method can be used to split a sep-delimited string
   into a corresponding list of elements.

   in the following example, a comma-delimited string is split using
   sep=','.
   in [14]:
single_instance_list = single_instance_str.split(',')
print(single_instance_list)

['p', 'k', 'f', 'n', 'f', 'n', 'f', 'c', 'n', 'w', 'e', '?', 'k', 'y', 'w', 'n',
 'p', 'w', 'o', 'e', 'w', 'v', 'd']

   python lists are heterogeneous, i.e., they can contain elements of
   different types.
   in [15]:
mixed_list = ['a', 1, 2.3, true, [1, 'b']]
print(mixed_list)

['a', 1, 2.3, true, [1, 'b']]

   the python + operator can be used for addition, and also to concatenate
   strings and lists.
   in [16]:
print(1 + 2 + 3)
print('a' + 'b' + 'c')
print(['a', 1] + [2.3, true] + [[1, 'b']])

6
abc
['a', 1, 2.3, true, [1, 'b']]

accessing sequence elements & subsequences[89]  

   individual elements of [90]sequences (e.g., lists and strings) can be
   accessed by specifying their zero-based index position within square
   brackets ('[', ']').

   the following statements print out the 3rd element - at zero-based
   index position 2 - of single_instance_str and single_instance_list.

   note that the 3rd elements are not the same, as commas count as
   elements in the string, but not in the list created by splitting a
   comma-delimited string.
   in [17]:
print(single_instance_str)
print(single_instance_str[2])
print(single_instance_list)
print(single_instance_list[2])

p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d
k
['p', 'k', 'f', 'n', 'f', 'n', 'f', 'c', 'n', 'w', 'e', '?', 'k', 'y', 'w', 'n',
 'p', 'w', 'o', 'e', 'w', 'v', 'd']
f

   negative index values can be used to specify a position offset from the
   end of the sequence.

   it is often useful to use a -1 index value to access the last element
   of a sequence.
   in [18]:
print(single_instance_str)
print(single_instance_str[-1])
print(single_instance_str[-2])

p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d
d
,

   in [19]:
print(single_instance_list)
print(single_instance_list[-1])
print(single_instance_list[-2])

['p', 'k', 'f', 'n', 'f', 'n', 'f', 'c', 'n', 'w', 'e', '?', 'k', 'y', 'w', 'n',
 'p', 'w', 'o', 'e', 'w', 'v', 'd']
d
v

   the python slice notation can be used to access subsequences by
   specifying two index positions separated by a colon (':');
   seq[start:stop] returns all the elements in seq between start and stop
   - 1 (inclusive).
   in [20]:
print(single_instance_str[2:4])
print(single_instance_list[2:4])

k,
['f', 'n']

   slices index values can be negative.
   in [21]:
print(single_instance_str[-4:-2])
print(single_instance_list[-4:-2])

,v
['e', 'w']

   the start and/or stop index can be omitted. a common use of slices with
   a single index value is to access all but the first element or all but
   the last element of a sequence.
   in [22]:
print(single_instance_str)
print(single_instance_str[:-1])  # all but the last
print(single_instance_str[:-2])  # all but the last 2
print(single_instance_str[1:])  # all but the first
print(single_instance_str[2:])  # all but the first 2

p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d
p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,
p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v
,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d
k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d

   in [23]:
print(single_instance_list)
print(single_instance_list[:-1])
print(single_instance_list[1:])

['p', 'k', 'f', 'n', 'f', 'n', 'f', 'c', 'n', 'w', 'e', '?', 'k', 'y', 'w', 'n',
 'p', 'w', 'o', 'e', 'w', 'v', 'd']
['p', 'k', 'f', 'n', 'f', 'n', 'f', 'c', 'n', 'w', 'e', '?', 'k', 'y', 'w', 'n',
 'p', 'w', 'o', 'e', 'w', 'v']
['k', 'f', 'n', 'f', 'n', 'f', 'c', 'n', 'w', 'e', '?', 'k', 'y', 'w', 'n', 'p',
 'w', 'o', 'e', 'w', 'v', 'd']

   slice notation includes an optional third element, step, as in
   seq[start:stop:step], that specifies the steps or increments by which
   elements are retrieved from seq between start and step - 1:
   in [24]:
print(single_instance_str)
print(single_instance_str[::2])  # print elements in even-numbered positions
print(single_instance_str[1::2])  # print elements in odd-numbered positions
print(single_instance_str[::-1])  # print elements in reverse order

p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d
pkfnfnfcnwe?kywnpwoewvd
,,,,,,,,,,,,,,,,,,,,,,
d,v,w,e,o,w,p,n,w,y,k,?,e,w,n,c,f,n,f,n,f,k,p

   the [91]python tutorial offers a helpful ascii art representation to
   show how positive and negative indexes are interpreted:
 +---+---+---+---+---+
 | h | e | l | p | a |
 +---+---+---+---+---+
 0   1   2   3   4   5
-5  -4  -3  -2  -1

splitting / separating statements[92]  

   python statements are typically separated by newlines (rather than,
   say, the semi-colon in java). statements can extend over more than one
   line; it is generally best to break the lines after commas,
   parentheses, braces or brackets. inserting a backslash character ('\')
   at the end of a line will also enable continuation of the statement on
   the next line, but it is generally best to look for other alternatives.
   in [25]:
attribute_names = ['class',
                   'cap-shape', 'cap-surface', 'cap-color',
                   'bruises?',
                   'odor',
                   'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',

                   'stalk-shape', 'stalk-root',
                   'stalk-surface-above-ring', 'stalk-surface-below-ring',
                   'stalk-color-above-ring', 'stalk-color-below-ring',
                   'veil-type', 'veil-color',
                   'ring-number', 'ring-type',
                   'spore-print-color',
                   'population',
                   'habitat']
print(attribute_names)

['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises?', 'odor', 'gill-att
achment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root'
, 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-rin
g', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-ty
pe', 'spore-print-color', 'population', 'habitat']

   in [26]:
print('a', 'b', 'c',  # no '\' needed when breaking after comma
      1, 2, 3)

a b c 1 2 3

   in [27]:
print(  # no '\' needed when breaking after parenthesis, brace or bracket
    'a', 'b', 'c',
    1, 2, 3)

a b c 1 2 3

   in [28]:
print(1 + 2 \
      + 3)

6

processing strings & other sequences[93]  

   the [94]str.strip([chars]) method returns a copy of str in which any
   leading or trailing chars are removed. if no chars are specified, it
   removes all leading and trailing whitespace. [whitespace is any
   sequence of spaces, tabs ('\t') and/or newline ('\n') characters.]

   note that since a blank space is inserted in the output after every
   item in a comma-delimited list, the second asterisk below is printed
   after a leading blank space is inserted on the new line.
   in [29]:
print('*', '\tp,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d\n', '*')

*       p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d
 *

   in [30]:
print('*', '\tp,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d\n'.strip(), '*')

* p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d *

   in [31]:
print('*', '\tp,k,f,n,f,n,f,c,n,w,e,       ?,k,y\t,w,n,p,w\n,o,e,w,v,d\n'.strip(
), '*')

* p,k,f,n,f,n,f,c,n,w,e,       ?,k,y    ,w,n,p,w
,o,e,w,v,d *

   a common programming pattern when dealing with csv (comma-separated
   value) files, such as the mushroom dataset file mentioned above, is to
   repeatedly:
    1. read a line from a file
    2. strip off any leading and trailing whitespace
    3. split the values separated by commas into a list

   we will get to repetition control structures (loops) and file input and
   output shortly, but here is an example of how str.strip() and
   str.split() be chained together in a single instruction for processing
   a line representing a single instance from the mushroom dataset file.
   note that chained methods are executed in left-to-right order.

   [python providees a [95]csv module to facilitate the processing of csv
   files, but we will not use that module here]
   in [32]:
single_instance_str = 'p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d\n'
print(single_instance_str)
# first strip leading & trailing whitespace, then split on commas
single_instance_list = single_instance_str.strip().split(',')
print(single_instance_list)

p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d

['p', 'k', 'f', 'n', 'f', 'n', 'f', 'c', 'n', 'w', 'e', '?', 'k', 'y', 'w', 'n',
 'p', 'w', 'o', 'e', 'w', 'v', 'd']

   the [96]str.join(words) method is the inverse of str.split(), returning
   a single string in which each string in the sequence of words is
   separated by str.
   in [33]:
print(single_instance_list)
print(','.join(single_instance_list))

['p', 'k', 'f', 'n', 'f', 'n', 'f', 'c', 'n', 'w', 'e', '?', 'k', 'y', 'w', 'n',
 'p', 'w', 'o', 'e', 'w', 'v', 'd']
p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d

   a number of python methods can be used on strings, lists and other
   sequences.

   the [97]len(s) function can be used to find the length of (number of
   items in) a sequence s. it will also return the number of items in a
   dictionary, a data structure we will cover further below.
   in [34]:
print(len(single_instance_str))
print(len(single_instance_list))

46
23

   the in operator can be used to determine whether a sequence contains a
   value.

   boolean values in python are true and false (note the capitalization).
   in [35]:
print(',' in single_instance_str)
print(',' in single_instance_list)

true
false

   the [98]s.count(x) ormethod can be used to count the number of
   occurrences of item x in sequence s.
   in [36]:
print(single_instance_str.count(','))
print(single_instance_list.count('f'))

22
3

   the [99]s.index(x) method can be used to find the first zero-based
   index of item x in sequence s.
   in [37]:
print(single_instance_str.index(','))
print(single_instance_list.index('f'))

1
2

   note that an [100]valueerror exception will be raised if item x is not
   found in sequence s.
   in [38]:
print(single_instance_list.index(','))

---------------------------------------------------------------------------
valueerror                                traceback (most recent call last)
<ipython-input-38-062ca5cc211d> in <module>()
----> 1 print(single_instance_list.index(','))

valueerror: ',' is not in list

mutability[101]  

   one important distinction between strings and lists has to do with
   their [102]mutability.

   python strings are immutable, i.e., they cannot be modified. most
   string methods (like str.strip()) return modified copies of the strings
   on which they are used.

   python lists are mutable, i.e., they can be modified.

   the examples below illustrate a number of [103]list methods that modify
   lists.
   in [39]:
list_1 = [1, 2, 3, 5, 1]
list_2 = list_1  # list_2 now references the same object as list_1

print('list_1:             ', list_1)
print('list_2:             ', list_2)
print()

list_1.remove(1)  # remove [only] the first occurrence of 1 in list_1
print('list_1.remove(1):   ', list_1)
print()

list_1.pop(2)  # remove the element in position 2
print('list_1.pop(2):      ', list_1)
print()

list_1.append(6)  # add 6 to the end of list_1
print('list_1.append(6):   ', list_1)
print()

list_1.insert(0, 7)  # add 7 to the beinning of list_1 (before the element in po
sition 0)
print('list_1.insert(0, 7):', list_1)
print()

list_1.sort()
print('list_1.sort():      ', list_1)
print()

list_1.reverse()
print('list_1.reverse():   ', list_1)

list_1:              [1, 2, 3, 5, 1]
list_2:              [1, 2, 3, 5, 1]

list_1.remove(1):    [2, 3, 5, 1]

list_1.pop(2):       [2, 3, 1]

list_1.append(6):    [2, 3, 1, 6]

list_1.insert(0, 7): [7, 2, 3, 1, 6]

list_1.sort():       [1, 2, 3, 6, 7]

list_1.reverse():    [7, 6, 3, 2, 1]

   when more than one name (e.g., a variable) is bound to the same mutable
   object, changes made to that object are reflected in all names bound to
   that object. for example, in the second statement above, list_2 is
   bound to the same object that is bound to list_1. all changes made to
   the object bound to list_1 will thus be reflected in list_2 (since they
   both reference the same object).
   in [40]:
print('list_1:          ', list_1)
print('list_2:          ', list_2)

list_1:           [7, 6, 3, 2, 1]
list_2:           [7, 6, 3, 2, 1]

   we can create a copy of a list by using slice notation and not
   specifying a start or end parameter, i.e., [:], and if we assign that
   copy to another variable, the variables will be bound to different
   objects, so changes to one do not affect the other.
   in [41]:
list_1 = [1, 2, 3, 5, 1]
list_2 = list_1[:]  # list_1[:] returns a copy of the entire contents of list_1

print('list_1:             ', list_1)
print('list_2:             ', list_2)
print()

list_1.remove(1)  # remove [only] the first occurrence of 1 in list_1
print('list_1.remove(1):   ', list_1)
print()

print('list_1:          ', list_1)
print('list_2:          ', list_2)

list_1:              [1, 2, 3, 5, 1]
list_2:              [1, 2, 3, 5, 1]

list_1.remove(1):    [2, 3, 5, 1]

list_1:           [2, 3, 5, 1]
list_2:           [1, 2, 3, 5, 1]

   the [104]dir() function returns all the attributes associated with a
   python name (e.g., a variable) in alphabetical order.

   when invoked with a name bound to a list object, it will return the
   methods that can be invoked on a list. the attributes with leading and
   trailing underscores should be treated as protected (i.e., they should
   not be used); we'll discuss this further below.
   in [42]:
dir(list_1)

   out[42]:
['__add__',
 '__class__',
 '__contains__',
 '__delattr__',
 '__delitem__',
 '__delslice__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__getitem__',
 '__getslice__',
 '__gt__',
 '__hash__',
 '__iadd__',
 '__imul__',
 '__init__',
 '__iter__',
 '__le__',
 '__len__',
 '__lt__',
 '__mul__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__reversed__',
 '__rmul__',
 '__setattr__',
 '__setitem__',
 '__setslice__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 'append',
 'count',
 'extend',
 'index',
 'insert',
 'pop',
 'remove',
 'reverse',
 'sort']

   there are sorting and reversing functions, [105]sorted() and
   [106]reversed(), that do not modify their arguments, and can thus be
   used on mutable or immutable objects.

   note that sorted() always returns a sorted list of each element in its
   argument, regardless of which type of sequence it is passed. thus,
   invoking sorted() on a string returns a list of sorted characters from
   the string, rather than a sorted string.
   in [43]:
print('sorted(list_1):', sorted(list_1))
print('list_1:        ', list_1)
print()
print('sorted(single_instance_str):', sorted(single_instance_str))
print('single_instance_str:        ', single_instance_str)

sorted(list_1): [1, 2, 3, 5]
list_1:         [2, 3, 5, 1]

sorted(single_instance_str): ['\n', ',', ',', ',', ',', ',', ',', ',', ',', ',',
 ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', '?', 'c', 'd',
 'e', 'e', 'f', 'f', 'f', 'k', 'k', 'n', 'n', 'n', 'n', 'o', 'p', 'p', 'v', 'w',
 'w', 'w', 'w', 'y']
single_instance_str:         p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d


   the sorted() function sorts its argument in ascending order by default.

   an optional [107]keyword argument, reverse, can be used to sort in
   descending order. the default value of this optional parameter is
   false; to get non-default behavior of an optional argument, we must
   specify the name and value of the argument, in this case, reverse=true.
   in [44]:
print(sorted(single_instance_str))
print(sorted(single_instance_str, reverse=true))

['\n', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ','
, ',', ',', ',', ',', ',', ',', ',', '?', 'c', 'd', 'e', 'e', 'f', 'f', 'f', 'k'
, 'k', 'n', 'n', 'n', 'n', 'o', 'p', 'p', 'v', 'w', 'w', 'w', 'w', 'y']
['y', 'w', 'w', 'w', 'w', 'v', 'p', 'p', 'o', 'n', 'n', 'n', 'n', 'k', 'k', 'f',
 'f', 'f', 'e', 'e', 'd', 'c', '?', ',', ',', ',', ',', ',', ',', ',', ',', ',',
 ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', '\n']

tuples (immutable list-like sequences)[108]  

   a [109]tuple is an ordered, immutable sequence of 0 or more
   comma-delimited values enclosed in parentheses ('(', ')'). many of the
   functions and methods that operate on strings and lists also operate on
   tuples.
   in [45]:
x = (5, 4, 3, 2, 1)  # a tuple
print('x =', x)
print('len(x) =', len(x))
print('x.index(3) =', x.index(3))
print('x[2:4] = ', x[2:4])
print('x[4:2:-1] = ', x[4:2:-1])
print('sorted(x):', sorted(x))  # note: sorted() always returns a list

x = (5, 4, 3, 2, 1)
len(x) = 5
x.index(3) = 2
x[2:4] =  (3, 2)
x[4:2:-1] =  (1, 2)
sorted(x): [1, 2, 3, 4, 5]

   note that the methods that modify lists (e.g., append(), remove(),
   reverse(), sort()) are not defined for immutable sequences such as
   tuples (or strings). invoking one of these sequence modification
   methods on an immutable sequence will raise an [110]attributeerror
   exception.
   in [46]:
x.append(6)

---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-46-12939164f245> in <module>()
----> 1 x.append(6)

attributeerror: 'tuple' object has no attribute 'append'

   however, one can approximate these modifications by creating modified
   copies of an immutable sequence and then re-assigning it to a name.
   in [47]:
x = x + (6,)  # need to include a comma to differentiate tuple from numeric expr
ession
x

   out[47]:
(5, 4, 3, 2, 1, 6)

   note that python has a += operator which is a shortcut for the name =
   name + new_value pattern. this can be used for addition (e.g., x += 1
   is shorthand for x = x + 1) or concatenation (e.g., x += (7,) is
   shorthand for x = x + (7,)).
   in [48]:
x += (7,)
x

   out[48]:
(5, 4, 3, 2, 1, 6, 7)

   a tuple of one element must include a trailing comma to differentiate
   it from a parenthesized expression.
   in [49]:
('a')

   out[49]:
'a'

   in [50]:
('a',)

   out[50]:
('a',)

conditionals[111]  

   one common approach to handling errors is to look before you leap
   (lbyl), i.e., test for potential [112]exceptions before executing
   instructions that might raise those exceptions.

   this approach can be implemented using the [113]if statement (which may
   optionally include an else and any number of elif clauses).

   the following is a simple example of an if statement:
   in [51]:
class_value = 'x'  # try changing this to 'p' or 'x'

if class_value == 'e':
    print('edible')
elif class_value == 'p':
    print('poisonous')
else:
    print('unknown')

unknown

   note that
     * a colon (':') is used at the end of the lines with if, else or elif
     * no parentheses are required to enclose the boolean condition (it is
       presumed to include everything between if or elif and the colon)
     * the statements below each if, elif and else line are all indented

   python does not have special characters to delimit statement blocks
   (like the '{' and '}' delimiters in java); instead, sequences of
   statements with the same indentation level are treated as a statement
   block. the [114]python style guide recommends using 4 spaces for each
   indentation level.

   an if statement can be used to follow the lbyl paradigm in preventing
   the valueerror that occured in an earlier example:
   in [52]:
attribute = 'bruises?'  # try substituting 'bruises?' for 'bruises' and re-runni
ng this code

if attribute in attribute_names:
    i = attribute_names.index(attribute)
    print(attribute, 'is in position', i)
else:
    print(attribute, 'is not in', attribute_names)

bruises? is in position 4

seeking forgiveness vs. asking for permission (eafp vs. lbyl)[115]  

   another perspective on handling errors championed by some pythonistas
   is that it is [116]easier to ask forgiveness than permission (eafp).

   as in many practical applications of philosophy, religion or dogma, it
   is helpful to think before you choose (tbyc). there are a number of
   factors to consider in deciding whether to follow the eafp or lbyl
   paradigm, including code readability and the anticipated likelihood and
   relative severity of encountering an exception. for those who are
   interested, oran looney wrote a blog post providing a nice overview of
   the debate over [117]lbyl vs. eafp.

   in keeping with practices most commonly used with other languages, we
   will follow the lbyl paradigm throughout most of this primer.

   however, as a brief illustration of the eafp paradigm in python, here
   is an alternate implementation of the functionality of the code above,
   using a [118]try/except statement.
   in [53]:
attribute = 'bruises?'  # try substituting 'bruises' for 'bruises' and re-runnin
g this code

i = -1
try:
    i = attribute_names.index(attribute)
    print(attribute, 'is in position', i)
except valueerror:
    print(attribute, 'is not found')

bruises? is in position 4

   there is no local scoping inside a try, so the value of i persists
   after the try/except statement.
   in [54]:
i

   out[54]:
4

   the python null object is none (note the capitalization).
   in [55]:
attribute = 'bruises'  # try substituting 'bruises?' for 'bruises' and re-runnin
g this code

if attribute not in attribute_names: # equivalent to 'not attribute in attribute
_names'
    value = none
else:
    i = attribute_names.index(attribute)
    value = single_instance_list[i]

print(attribute, '=', value)

bruises = none

defining and calling functions[119]  

   python [120]function definitions start with the def keyword followed by
   a function name, a list of 0 or more comma-delimited parameters (aka
   'formal parameters') enclosed within parentheses, and then a colon
   (':').

   a function definition may include one or more [121]return statements to
   indicate the value(s) returned to where the function is called. it is
   good practice to include a short [122]docstring to briefly describe the
   behavior of the function and the value(s) it returns.
   in [56]:
def attribute_value(instance, attribute, attribute_names):
    '''returns the value of attribute in instance, based on its position in attr
ibute_names'''
    if attribute not in attribute_names:
        return none
    else:
        i = attribute_names.index(attribute)
        return instance[i]  # using the parameter name here

   a function call starts with the function name, followed by a list of 0
   or more comma-delimited arguments (aka 'actual parameters') enclosed
   within parentheses. a function call can be used as a statement or
   within an expression.
   in [57]:
attribute = 'cap-shape'  # try substituting any of the other attribute names sho
wn above
print(attribute, '=', attribute_value(single_instance_list, 'cap-shape', attribu
te_names))

cap-shape = k

   note that python does not distinguish between names used for variables
   and names used for functions. an assignment statement binds a value to
   a name; a function definition also binds a value to a name. at any
   given time, the value most recently bound to a name is the one that is
   used.

   this can be demonstrated using the [123]type(object) function, which
   returns the type of object.
   in [58]:
x = 0
print('x used as a variable:', x, type(x))

def x():
    print('x')

print('x used as a function:', x, type(x))

x used as a variable: 0 <type 'int'>
x used as a function: <function x at 0x1049ae488> <type 'function'>

   another way to determine the type of an object is to use
   [124]isinstance(object, class). this is generally [125]preferable, as
   it takes into account [126]class inheritance. there is a larger issue
   of [127]duck typing, and whether code should ever explicitly check for
   the type of an object, but we will omit further discussion of the topic
   in this primer.

call by sharing[128]  

   an important feature of python functions is that arguments are passed
   using [129]call by sharing.

   if a mutable object is passed as an argument to a function parameter,
   assignment statements using that parameter do not affect the passed
   argument, however other modifications to the parameter (e.g.,
   modifications to a list using methods such as append(), remove(),
   reverse() or sort()) do affect the passed argument.

   not being aware of - or forgetting - this important distinction can
   lead to challenging debugging sessions.

   the example below demonstrates this difference and introduces another
   [130]list method, list.insert(i, x), which inserts x into list at
   position i.
   in [59]:
def modify_parameters(parameter1, parameter2):
    '''inserts "x" at the head of parameter1, assigns [7, 8, 9] to parameter2'''
    parameter1.insert(0, 'x')  # insert() will affect argument passed as paramet
er1
    print('parameter1, after inserting "x":', parameter1)
    parameter2 = [7, 8, 9]  # assignment will not affect argument passed as para
meter2
    print('parameter2, after assigning "x"', parameter2)
    return

argument1 = [1, 2, 3]
argument2 = [4, 5, 6]
print('argument1, before calling modify_parameters:', argument1)
print('argument2, before calling modify_parameters:', argument2)
print()
modify_parameters(argument1, argument2)
print()
print('argument1, after calling modify_parameters:', argument1)
print('argument2, after calling modify_parameters:', argument2)

argument1, before calling modify_parameters: [1, 2, 3]
argument2, before calling modify_parameters: [4, 5, 6]

parameter1, after inserting "x": ['x', 1, 2, 3]
parameter2, after assigning "x" [7, 8, 9]

argument1, after calling modify_parameters: ['x', 1, 2, 3]
argument2, after calling modify_parameters: [4, 5, 6]

   one way of preventing functions from modifying mutable objects passed
   as parameters is to make a copy of those objects inside the function.
   here is another version of the function above that makes a shallow copy
   of the list_parameter using the slice operator.

   [note: the python [131]copy module provides both [shallow] [132]copy()
   and [133]deepcopy() methods; we will cover modules further below.]
   in [60]:
def modify_parameter_copy(parameter_1):
    '''inserts "x" at the head of parameter_1, without modifying the list argume
nt'''
    parameter_1_copy = parameter_1[:]  # list[:] returns a copy of list
    parameter_1_copy.insert(0, 'x')
    print('inserted "x":', parameter_1_copy)
    return

argument_1 = [1, 2, 3]  # passing a named object will not affect the object boun
d to that name
print('before:', argument_1)
modify_parameter_copy(argument_1)
print('after:', argument_1)

before: [1, 2, 3]
inserted "x": ['x', 1, 2, 3]
after: [1, 2, 3]

   another way to avoid modifying parameters is to use assignment
   statements which do not modify the parameter objects but return a new
   object that is bound to the name (locally).
   in [61]:
def modify_parameter_assignment(parameter_1):
    '''inserts "x" at the head of parameter_1, without modifying the list argume
nt'''
    parameter_1 = ['x'] + parameter_1  # using assignment rather than list.inser
t()
    print('inserted "x":', parameter_1)
    return

argument_1 = [1, 2, 3]  # passing a named object will not affect the object boun
d to that name
print('before:', argument_1)
modify_parameter_assignment(argument_1)
print('after:', argument_1)

before: [1, 2, 3]
inserted "x": ['x', 1, 2, 3]
after: [1, 2, 3]

multiple return values[134]  

   python functions can return more than one value by separating those
   return values with commas in the return statement. multiple values are
   returned as a tuple.

   if the function-invoking expression is an assignment statement,
   multiple variables can be assigned the multiple values returned by the
   function in a single statement. this combining of values and subsequent
   separation is known as tuple packing and unpacking.
   in [62]:
def min_and_max(list_of_values):
    '''returns a tuple containing the min and max values in the list_of_values''
'
    return min(list_of_values), max(list_of_values)

list_1 = [3, 1, 4, 2, 5]
print('min and max of', list_1, ':', min_and_max(list_1))

# a single variable is assigned the two-element tuple
min_and_max_list_1 = min_and_max(list_1)
print('min and max of', list_1, ':', min_and_max_list_1)

# the 1st variable is assigned the 1st value, the 2nd variable is assigned the 2
nd value
min_list_1, max_list_1 = min_and_max(list_1)
print('min and max of', list_1, ':', min_list_1, ',', max_list_1)

min and max of [3, 1, 4, 2, 5] : (1, 5)
min and max of [3, 1, 4, 2, 5] : (1, 5)
min and max of [3, 1, 4, 2, 5] : 1 , 5

iteration: for, range[135]  

   the [136]for statement iterates over the elements of a sequence or
   other [137]iterable object.
   in [63]:
for i in [0, 1, 2]:
    print(i)

0
1
2

   in [64]:
for c in 'abc':
    print(c)

a
b
c

   the value of the variable used to iterate in a for statement persists
   after the for statement
   in [65]:
i, c

   out[65]:
(2, 'c')

   in python 2, the [138]range(stop) function returns a list of values
   from 0 up to stop - 1 (inclusive). it is often used in the context of a
   for loop that iterates over the list of values.
   in [66]:
print('values for the', len(attribute_names), 'attributes:', end='\n\n')  # adds
 a blank line
for i in range(len(attribute_names)):
    print(attribute_names[i], '=',
          attribute_value(single_instance_list, attribute_names[i], attribute_na
mes))

values for the 23 attributes:

class = p
cap-shape = k
cap-surface = f
cap-color = n
bruises? = f
odor = n
gill-attachment = f
gill-spacing = c
gill-size = n
gill-color = w
stalk-shape = e
stalk-root = ?
stalk-surface-above-ring = k
stalk-surface-below-ring = y
stalk-color-above-ring = w
stalk-color-below-ring = n
veil-type = p
veil-color = w
ring-number = o
ring-type = e
spore-print-color = w
population = v
habitat = d

   the more general form of the function, [139]range(start, stop[, step]),
   returns a list of values from start to stop - 1 (inclusive) increasing
   by step (which defaults to 1), or from start down to stop + 1
   (inclusive) decreasing by step if step is negative.
   in [67]:
for i in range(3, 0, -1):
    print(i)

3
2
1

   in python 2, the [140]xrange(stop[, stop[, step]]) function is an
   [141]iterable version of the range() function. in the context of a for
   loop, it returns the next item of the sequence for each iteration of
   the loop rather than creating all the elements of the sequence before
   the first iteration. this can reduce memory consumption in cases where
   iteration over all the items is not required.

   in python 3, the range() function behaves the same way as the xrange()
   function does in python 2, and so the xrange() function is not defined
   in python 3.

   to maximize compatibility, we will use range() throughout this
   notebook; however, note that it is generally more efficient to use
   xrange() rather than range() in python 2.

modules, namespaces and dotted notation[142]  

   a python [143]module is a file containing related definitions (e.g., of
   functions and variables). modules are used to help organize a python
   [144]namespace, the set of identifiers accessible in a particular
   context. all of the functions and variables we define in this ipython
   notebook are in the __main__ namespace, so accessing them does not
   require any specification of a module.

   a python module named simple_ml (in the file simple_ml.py), contains a
   set of solutions to the exercises in this ipython notebook. [the
   learning opportunity provided by this primer will be maximized by not
   looking at that file, or waiting as long as possible to do so.]

   accessing functions in an external module requires that we first
   [145]import the module, and then prefix the function names with the
   module name followed by a dot (this is known as dotted notation).

   for example, the following function call in exercise 1 below:

   simple_ml.print_attribute_names_and_values(single_instance_list,
   attribute_names)

   uses dotted notation to reference the
   print_attribute_names_and_values() function in the simple_ml module.

   after you have defined your own function for exercise 1, you can test
   your function by deleting the simple_ml module specification, so that
   the statement becomes

   print_attribute_names_and_values(single_instance_list, attribute_names)

   this will reference the print_attribute_names_and_values() function in
   the current namespace (__main__), i.e., the top-level interpreter
   environment. the simple_ml.print_attribute_names_and_values() function
   will still be accessible in the simple_ml namespace by using the
   "simple_ml." prefix (so you can easily toggle back and forth between
   your own definition and that provided in the solutions file).

exercise 1: define print_attribute_names_and_values()[146]  

   complete the following function definition,
   print_attribute_names_and_values(instance, attribute_names), so that it
   generates exactly the same output as the code above.
   in [68]:
def print_attribute_names_and_values(instance, attribute_names):
    '''prints the attribute names and values for an instance'''
    # your code here
    return

import simple_ml  # this module contains my solutions to exercises

# delete 'simple_ml.' in the function call below to test your function
simple_ml.print_attribute_names_and_values(single_instance_list, attribute_names
)
print_attribute_names_and_values(single_instance_list, attribute_names)

values for the 23 attributes:

class = p
cap-shape = k
cap-surface = f
cap-color = n
bruises? = f
odor = n
gill-attachment = f
gill-spacing = c
gill-size = n
gill-color = w
stalk-shape = e
stalk-root = ?
stalk-surface-above-ring = k
stalk-surface-below-ring = y
stalk-color-above-ring = w
stalk-color-below-ring = n
veil-type = p
veil-color = w
ring-number = o
ring-type = e
spore-print-color = w
population = v
habitat = d

file i/o[147]  

   python [148]file input and output is done through [149]file objects. a
   file object is created with the [150]open(name[, mode]) statement,
   where name is a string representing the name of the file, and mode is
   'r' (read), 'w' (write) or 'a' (append); if no second argument is
   provided, the mode defaults to 'r'.

   a common python programming pattern for processing an input text file
   is to
     * [151]open the file using a [152]with statement (which will
       automatically [153]close the file after the statements inside the
       with block have been executed)
     * iterate over each line in the file using a for statement

   the following code creates a list of instances, where each instance is
   a list of attribute values (like instance_1_str above).
   in [69]:
all_instances = []  # initialize instances to an empty list
data_filename = 'agaricus-lepiota.data'

with open(data_filename, 'r') as f:
    for line in f:  # 'line' will be bound to the next line in f in each for loo
p iteration
        all_instances.append(line.strip().split(','))

print('read', len(all_instances), 'instances from', data_filename)
# we don't want to print all the instances, so we'll just print the first one to
 verify
print('first instance:', all_instances[0])

read 8124 instances from agaricus-lepiota.data
first instance: ['p', 'x', 's', 'n', 't', 'p', 'f', 'c', 'n', 'k', 'e', 'e', 's'
, 's', 'w', 'w', 'p', 'w', 'o', 'p', 'k', 's', 'u']

exercise 2: define load_instances()[154]  

   define a function, load_instances(filename), that returns a list of
   instances in a text file. the function definition is started for you
   below. the function should exhibit the same behavior as the code above.
   in [70]:
def load_instances(filename):
    '''returns a list of instances stored in a file.

    filename is expected to have a series of comma-separated attribute values pe
r line, e.g.,
        p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d
    '''
    instances = []
    # your code goes here
    return instances

data_filename = 'agaricus-lepiota.data'
# delete 'simple_ml.' in the function call below to test your function
all_instances_2 = simple_ml.load_instances(data_filename)
print('read', len(all_instances_2), 'instances from', data_filename)
print('first instance:', all_instances_2[0])

read 8124 instances from agaricus-lepiota.data
first instance: ['p', 'x', 's', 'n', 't', 'p', 'f', 'c', 'n', 'k', 'e', 'e', 's'
, 's', 'w', 'w', 'p', 'w', 'o', 'p', 'k', 's', 'u']

   output can be written to a text file via the [155]file.write(str)
   method.

   as we saw earlier, the [156]str.join(words) method returns a single
   str-delimited string containing each of the strings in the words list.

   sql and hive database tables sometimes use a pipe ('|') delimiter to
   separate column values for each row when they are stored as flat files.
   the following code creates a new data file using pipes rather than
   commas to separate the attribute values.

   to help maintain internal consistency, it is generally a good practice
   to define a variable such as delimiter or separator, bind it to the
   intended delimiter string, and then use it as a named constant. the
   python language does not support named constants, so the use of
   variables as named constants depends on conventions (e.g., using
   all-caps).
   in [71]:
delimiter = '|'

print('converting to {}-delimited strings, e.g.,'.format(delimiter),
      delimiter.join(all_instances[0]))

datafile2 = 'agaricus-lepiota-2.data'
with open(datafile2, 'w') as f:  # 'w' = open file for writing (output)
    for instance in all_instances:
        f.write(delimiter.join(instance) + '\n')  # write each instance on a sep
arate line

all_instances_3 = []
with open(datafile2, 'r') as f:
    for line in f:
        all_instances_3.append(line.strip().split(delimiter))  # note: changed '
,' to '|'

print('read', len(all_instances_3), 'instances from', datafile2)
print('first instance:', all_instances_3[0])

converting to |-delimited strings, e.g., p|x|s|n|t|p|f|c|n|k|e|e|s|s|w|w|p|w|o|p
|k|s|u
read 8124 instances from agaricus-lepiota-2.data
first instance: ['p', 'x', 's', 'n', 't', 'p', 'f', 'c', 'n', 'k', 'e', 'e', 's'
, 's', 'w', 'w', 'p', 'w', 'o', 'p', 'k', 's', 'u']

list comprehensions[157]  

   python provides a powerful [158]list comprehension construct to
   simplify the creation of a list by specifying a formula in a single
   expression.

   some programmers find list comprehensions confusing, and avoid their
   use. we won't rely on list comprehensions here, but we will offer
   several examples with and without list comprehensions to highlight the
   power of the construct.

   one common use of list comprehensions is in the context of the
   [159]str.join(words) method we saw earlier.

   if we wanted to construct a pipe-delimited string containing elements
   of the list, we could use a for loop to iteratively add list elements
   and pipe delimiters to a string for all but the last element, and then
   manually add the last element.
   in [72]:
# create pipe-delimited string without using list comprehension
delimiter = '|'
delimited_string = ''
token_list = ['a', 'b', 'c']

for token in token_list[:-1]:  # add all but the last token + delimiter
    delimited_string += token + delimiter
delimited_string += token_list[-1]  # add the last token (with no trailing delim
iter)
delimited_string

   out[72]:
'a|b|c'

   this process is much simpler using a list comprehension.
   in [73]:
delimited_string = delimiter.join([token for token in token_list])
delimited_string

   out[73]:
'a|b|c'

missing values & "clean" instances[160]  

   as noted in the initial description of the uci mushroom set above, 2480
   of the 8124 instances have missing attribute values (denoted by '?').

   there are several techniques for dealing with instances that include
   missing attribute values, but to simplify things in the context of this
   primer - and following the example in the [161]data science for
   business book - we will simply ignore any such instances and restrict
   our focus to only the clean instances (with no missing values).

   we could use several lines of code - with an if statement inside a for
   loop - to create a clean_instances list from the all_instances list. or
   we could use a list comprehension that includes an if statement.

   we will show both approaches to creating clean_instances below.
   in [74]:
# version 1: using an if statement nested within a for statement
unknown_value = '?'

clean_instances = []
for instance in all_instances:
    if unknown_value not in instance:
        clean_instances.append(instance)

print(len(clean_instances), 'clean instances')

5644 clean instances

   in [75]:
# version 2: using an equivalent list comprehension
clean_instances = [instance
                   for instance in all_instances
                   if unknown_value not in instance]

print(len(clean_instances), 'clean instances')

5644 clean instances

   note that line breaks can be used before a for or if keyword in a list
   comprehension.

dictionaries (dicts)[162]  

   although single character abbreviations of attribute values (e.g., 'x')
   allow for more compact data files, they are not as easy to understand
   by human readers as the longer attribute value descriptions (e.g.,
   'convex').

   a python [163]dictionary (or dict) is an unordered, comma-delimited
   collection of key: value pairs, serving a siimilar function as a hash
   table or hashmap in other programming languages.

   we could create a dictionary for the cap-type attribute values shown
   above:

     bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s

   since we will want to look up the value using the abbreviation (which
   is the representation of the value stored in the file), we will use the
   abbreviations as keys and the descriptions as values.

   a python dictionary can be created by specifying all key: value pairs
   (with colons separating each key and value), or by adding them
   iteratively. we will show the first method in the cell below, and use
   the second method in a subsequent cell.

   note that a value in a python dictionary (dict) can be accessed by
   specifying its key using the general form dict[key] (or dict.get(key,
   [default]), which allows the specification of a default value to use if
   key is not in dict).
   in [76]:
attribute_values_cap_type = {'b': 'bell',
                             'c': 'conical',
                             'x': 'convex',
                             'f': 'flat',
                             'k': 'knobbed',
                             's': 'sunken'}

attribute_value_abbrev = 'x'
print(attribute_value_abbrev, '=', attribute_values_cap_type[attribute_value_abb
rev])

x = convex

   a python dictionary is an iterable container, so we can iterate over
   the keys in a dictionary using a for loop.

   note that since a dictionary is an unordered collection, the sequence
   of abbreviations and associated values is not guaranteed to appear in
   any particular order.
   in [77]:
for attribute_value_abbrev in attribute_values_cap_type:
    print(attribute_value_abbrev, '=', attribute_values_cap_type[attribute_value
_abbrev])

c = conical
b = bell
f = flat
k = knobbed
s = sunken
x = convex

   python supports dictionary comprehensions, which have a similar form as
   the list comprehensions described above, except that both a key and a
   value have to be specified for each iteration.

   for example, if we provisionally omit the 'convex' cap-type (whose
   abbreviation is the last letter rather than first letter in the
   attribute name), we could construct a dictionary of abbreviations and
   descriptions using the following expression.
   in [78]:
attribute_values_cap_type_2 = {x[0]: x
                               for x in ['bell', 'conical', 'flat', 'knobbed', '
sunken']}
print(attribute_values_cap_type_2)

{'s': 'sunken', 'c': 'conical', 'b': 'bell', 'k': 'knobbed', 'f': 'flat'}

   in [79]:
attribute_values_cap_type_2 = [[x[0], x ]
                               for x in ['bell', 'conical', 'flat', 'knobbed', '
sunken']]
print(attribute_values_cap_type_2)

[['b', 'bell'], ['c', 'conical'], ['f', 'flat'], ['k', 'knobbed'], ['s', 'sunken
']]

   while it's useful to have a dictionary of values for the cap-type
   attribute, it would be even more useful to have a dictionary of values
   for every attribute. earlier, we created a list of attribute_names; we
   will now expand this to create a list of attribute_values wherein each
   list element is a dictionary.

   rather than explicitly type in each dictionary entry in the python
   interpreter, we'll define a function to read a file containing the list
   of attribute names, values and value abbreviations in the format shown
   above:
     * class: edible=e, poisonous=p
     * cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s
     * cap-surface: fibrous=f, grooves=g, scaly=y, smooth=s
     * ...

   we can make calls to [164]shell commands from a python cell by using
   the bang (exclamation point). [there are a large number of [165]cell
   magics that extend the capability of ipython notebooks (which we will
   not explore further in this notebook.]

   for example, the following cell will show the contents of the
   agaricus-lepiota.attributes file on osx or linux (for windows,
   substitute type for cat).
   in [80]:
! cat agaricus-lepiota.attributes

class: edible=e, poisonous=p
cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s
cap-surface: fibrous=f, grooves=g, scaly=y, smooth=s
cap-color: brown=n ,buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e
, white=w, yellow=y
bruises?: bruises=t, no=f
odor: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p
, spicy=s
gill-attachment: attached=a, descending=d, free=f, notched=n
gill-spacing: close=c, crowded=w, distant=d
gill-size: broad=b, narrow=n
gill-color: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pi
nk=p, purple=u, red=e, white=w, yellow=y
stalk-shape: enlarging=e, tapering=t
stalk-root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=
?
stalk-surface-above-ring: fibrous=f, scaly=y, silky=k, smooth=s
stalk-surface-below-ring: fibrous=f, scaly=y, silky=k, smooth=s
stalk-color-above-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, r
ed=e, white=w, yellow=y
stalk-color-below-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, r
ed=e, white=w, yellow=y
veil-type: partial=p, universal=u
veil-color: brown=n, orange=o, white=w, yellow=y
ring-number: none=n, one=o, two=t
ring-type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, shea
thing=s, zone=z
spore-print-color: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, pur
ple=u, white=w, yellow=y
population: abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitar
y=y
habitat: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d

exercise 3: define load_attribute_values()[166]  

   we earlier created the attribute_names list manually. the
   load_attribute_values() function above creates the attribute_values
   list from the contents of a file, each line of which starts with the
   name of an attribute. unfortunately, the function discards the name of
   each attribute.

   it would be nice to retain the name as well as the value abbreviations
   and descriptions. one way to do this would be to create a list of
   dictionaries, in which each dictionary has 2 keys, a name, the value of
   which is the attribute name (a string), and values, the value of which
   is yet another dictionary (with abbreviation keys and description
   values, as in load_attribute_values()).

   complete the following function definition so that the code implements
   this functionality.
   in [81]:
def load_attribute_names_and_values(filename):
    '''returns a list of attribute names and values in a file.

    this list contains dictionaries wherein the keys are names
    and the values are value description dictionariess.

    each value description sub-dictionary will use
    the attribute value abbreviations as its keys
    and the attribute descriptions as the values.

    filename is expected to have one attribute name and set of values per line,
    with the following format:
        name: value_description=value_abbreviation[,value_description=value_abbr
eviation]*
    for example
        cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s
    the attribute name and values dictionary created from this line would be the
 following:
        {'name': 'cap-shape',
         'values': {'c': 'conical',
                    'b': 'bell',
                    'f': 'flat',
                    'k': 'knobbed',
                    's': 'sunken',
                    'x': 'convex'}}
    '''
    attribute_names_and_values = []  # this will be a list of dicts
    # your code goes here
    return attribute_names_and_values

attribute_filename = 'agaricus-lepiota.attributes'
# delete 'simple_ml.' in the function call below to test your function
attribute_names_and_values = simple_ml.load_attribute_names_and_values(attribute
_filename)
print('read', len(attribute_names_and_values), 'attribute values from', attribut
e_filename)
print('first attribute name:', attribute_names_and_values[0]['name'],
      '; values:', attribute_names_and_values[0]['values'])

read 23 attribute values from agaricus-lepiota.attributes
first attribute name: class ; values: {'p': 'poisonous', 'e': 'edible'}

counters[167]  

   data scientists often need to count things. for example, we might want
   to count the numbers of edible and poisonous mushrooms in the
   clean_instances list we created earlier.
   in [82]:
edible_count = 0
for instance in clean_instances:
    if instance[0] == 'e':
        edible_count += 1  # this is shorthand for edible_count = edible_count +
 1

print('there are', edible_count, 'edible mushrooms among the',
      len(clean_instances), 'clean instances')

there are 3488 edible mushrooms among the 5644 clean instances

   more generally, we often want to count the number of occurrences
   (frequencies) of each possible value for an attribute. one way to do so
   is to create a dictionary where each dictionary key is an attribute
   value and each dictionary value is the count of instances with that
   attribute value.

   using an ordinary dictionary, we must be careful to create a new
   dictionary entry the first time we see a new attribute value (that is
   not already contained in the dictionary).
   in [83]:
cap_state_value_counts = {}
for instance in clean_instances:
    cap_state_value = instance[1]  # cap-state is the 2nd attribute
    if cap_state_value not in cap_state_value_counts:
        # first occurrence, must explicitly initialize counter for this cap_stat
e_value
        cap_state_value_counts[cap_state_value] = 0
    cap_state_value_counts[cap_state_value] += 1

print('counts for each value of cap-state:')
for value in cap_state_value_counts:
    print(value, ':', cap_state_value_counts[value])

counts for each value of cap-state:
c : 4
b : 300
f : 2432
k : 36
s : 32
x : 2840

   the python [168]collections module provides a number of high
   performance container datatypes. a frequently useful datatype is a
   [169]counter, a specialized dictionary in which each key is a unique
   element found in a list or some other container, and each value is the
   number of occurrences of that element in the source container. the
   default value for each newly created key is zero.

   a counter includes a method, [170]most_common([n]), that returns a list
   of 2-element tuples representing the values and their associated counts
   for the most common n values in descending order of the counts; if n is
   omitted, the method returns all tuples.

   note that we can either use

   import collections

   and then use collections.counter() in our code, or use

   from collections import counter

   and then use counter() (with no module specification) in our code.
   in [84]:
from collections import counter

cap_state_value_counts = counter()
for instance in clean_instances:
    cap_state_value = instance[1]
    # no need to explicitly initialize counters for cap_state_value; all start a
t zero
    cap_state_value_counts[cap_state_value] += 1

print('counts for each value of cap-state:')
for value in cap_state_value_counts:
    print(value, ':', cap_state_value_counts[value])

counts for each value of cap-state:
c : 4
b : 300
f : 2432
k : 36
s : 32
x : 2840

   when a counter object is instantiated with a list of items, it returns
   a dictionary-like container in which the keys are the unique items in
   the list, and the values are the counts of each unique item in that
   list.
   in [85]:
counts = counter(['a', 'b', 'c', 'a', 'b', 'a'])
print(counts)
print(counts.most_common())

counter({'a': 3, 'b': 2, 'c': 1})
[('a', 3), ('b', 2), ('c', 1)]

   this allows us to count the number of values for cap-state in a very
   compact way.

   we can use a counter initialized with a list comprehension to collect
   all the values of the 2nd attribute, cap-state.

   the following shows the first 10 instances; the second element in each
   sublist is the value of cap-state or that instance.
   in [86]:
print(clean_instances[:10])

[['p', 'x', 's', 'n', 't', 'p', 'f', 'c', 'n', 'k', 'e', 'e', 's', 's', 'w', 'w'
, 'p', 'w', 'o', 'p', 'k', 's', 'u'], ['e', 'x', 's', 'y', 't', 'a', 'f', 'c', '
b', 'k', 'e', 'c', 's', 's', 'w', 'w', 'p', 'w', 'o', 'p', 'n', 'n', 'g'], ['e',
 'b', 's', 'w', 't', 'l', 'f', 'c', 'b', 'n', 'e', 'c', 's', 's', 'w', 'w', 'p',
 'w', 'o', 'p', 'n', 'n', 'm'], ['p', 'x', 'y', 'w', 't', 'p', 'f', 'c', 'n', 'n
', 'e', 'e', 's', 's', 'w', 'w', 'p', 'w', 'o', 'p', 'k', 's', 'u'], ['e', 'x',
's', 'g', 'f', 'n', 'f', 'w', 'b', 'k', 't', 'e', 's', 's', 'w', 'w', 'p', 'w',
'o', 'e', 'n', 'a', 'g'], ['e', 'x', 'y', 'y', 't', 'a', 'f', 'c', 'b', 'n', 'e'
, 'c', 's', 's', 'w', 'w', 'p', 'w', 'o', 'p', 'k', 'n', 'g'], ['e', 'b', 's', '
w', 't', 'a', 'f', 'c', 'b', 'g', 'e', 'c', 's', 's', 'w', 'w', 'p', 'w', 'o', '
p', 'k', 'n', 'm'], ['e', 'b', 'y', 'w', 't', 'l', 'f', 'c', 'b', 'n', 'e', 'c',
 's', 's', 'w', 'w', 'p', 'w', 'o', 'p', 'n', 's', 'm'], ['p', 'x', 'y', 'w', 't
', 'p', 'f', 'c', 'n', 'p', 'e', 'e', 's', 's', 'w', 'w', 'p', 'w', 'o', 'p', 'k
', 'v', 'g'], ['e', 'b', 's', 'y', 't', 'a', 'f', 'c', 'b', 'g', 'e', 'c', 's',
's', 'w', 'w', 'p', 'w', 'o', 'p', 'k', 's', 'm']]

   the following list comprehension gathers the 2nd attribute of each of
   the first 10 sublists (note the slice notation).
   in [87]:
[instance[1] for instance in clean_instances][:10]

   out[87]:
['x', 'x', 'b', 'x', 'x', 'x', 'b', 'b', 'x', 'b']

   now we will gather all of the values for the 2nd attribute into a list
   and create a counter for that list.
   in [88]:
cap_state_value_counts = counter([instance[1] for instance in clean_instances])

print('counts for each value of cap-state:')
for value in cap_state_value_counts:
    print(value, ':', cap_state_value_counts[value])

counts for each value of cap-state:
c : 4
b : 300
f : 2432
k : 36
s : 32
x : 2840

exercise 4: define attribute_value_counts()[171]  

   define a function, attribute_value_counts(instances, attribute,
   attribute_names), that returns a counter containing the counts of
   occurrences of each value of attribute in the list of instances.
   attribute_names is the list we created above, where each element is the
   name of an attribute.

   this exercise is designed to generalize the solution shown in the code
   directly above (which handles only the cap-state attribute).
   in [89]:
# your definition goes here

attribute = 'cap-shape'
# delete 'simple_ml.' in the function call below to test your function
attribute_value_counts = simple_ml.attribute_value_counts(clean_instances,
                                                          attribute,
                                                          attribute_names)

print('counts for each value of', attribute, ':')
for value in attribute_value_counts:
    print(value, ':', attribute_value_counts[value])

counts for each value of cap-shape :
c : 4
b : 300
f : 2432
k : 36
s : 32
x : 2840

more on sorting[172]  

   earlier, we saw that there is a list.sort() method that will sort a
   list in-place, i.e., by replacing the original value of list with a
   sorted version of the elements in list.

   we also saw that the [173]sorted(iterable[, cmp[, key[, reverse]]])
   function can be used to return a copy of a list, dictionary or any
   other [174]iterable container it is passed, in ascending order.
   in [90]:
original_list = [3, 1, 4, 2, 5]
sorted_list = sorted(original_list)

print(original_list)
print(sorted_list)

[3, 1, 4, 2, 5]
[1, 2, 3, 4, 5]

   sorted() can also be used with dictionaries (it returns a sorted list
   of the dictionary keys).
   in [91]:
print(sorted(attribute_values_cap_type))

['b', 'c', 'f', 'k', 's', 'x']

   we can use the sorted keys to access the values of a dictionary in
   ascending order of the keys.
   in [92]:
for attribute_value_abbrev in sorted(attribute_values_cap_type):
    print(attribute_value_abbrev, '=', attribute_values_cap_type[attribute_value
_abbrev])

b = bell
c = conical
f = flat
k = knobbed
s = sunken
x = convex

   in [93]:
attribute = 'cap-shape'
attribute_value_counts = simple_ml.attribute_value_counts(clean_instances,
                                                          attribute,
                                                          attribute_names)

print('counts for each value of', attribute, ':')
for value in sorted(attribute_value_counts):
    print(value, ':', attribute_value_counts[value])

counts for each value of cap-shape :
b : 300
c : 4
f : 2432
k : 36
s : 32
x : 2840

sorting a dictionary by values[175]  

   it is often useful to sort a dictionary by its values rather than its
   keys.

   for example, when we printed out the counts of the attribute values for
   cap-shape above, the counts appeared in an ascending alphabetic order
   of their attribute names. it is often more helpful to show the
   attribute value counts in descending order of the counts (which are the
   values in that dictionary).

   there are a [176]variety of ways to sort a dictionary by values, but
   the approach described in [177]pep-256 is generally considered the most
   efficient.

   in order to understand the components used in this approach, we will
   revisit and elaborate on a few concepts involving dictionaries,
   iterators and modules.

   the [178]dict.items() method returns an unordered list of (key, value)
   tuples in dict.
   in [94]:
attribute_values_cap_type.items()

   out[94]:
[('c', 'conical'),
 ('b', 'bell'),
 ('f', 'flat'),
 ('k', 'knobbed'),
 ('s', 'sunken'),
 ('x', 'convex')]

   in python 2, a related method, [179]dict.iteritems(), returns an
   [180]iterator: a callable object that returns the next item in a
   sequence each time it is referenced (e.g., during each iteration of a
   for loop), which can be more efficient than generating all the items in
   the sequence before any are used ... and so should be used rather than
   items() wherever possible

   this is similar to the distinction between xrange() and range()
   described above ... and, also similarly, dict.items() is an iterator in
   python 3 and so dict.iteritems() is no longer needed (nor defined) ...
   and further similarly, we will use only dict.items() in this notebook,
   but it is generally more efficient to use dict.iteritems() in python 2.
   in [95]:
for key, value in attribute_values_cap_type.items():
    print(key, ':', value)

c : conical
b : bell
f : flat
k : knobbed
s : sunken
x : convex

   the python [181]operator module contains a number of functions that
   perform object comparisons, logical operations, mathematical
   operations, sequence operations, and abstract type tests.

   to facilitate sorting a dictionary by values, we will use the
   [182]operator.itemgetter(i) function that can be used to retrieve the
   ith value in a tuple (such as a (key, value) pair returned by
   [iter]items()).

   we can use operator.itemgetter(1)) to reference the value - the 2nd
   item in each (key, value) tuple, (at zero-based index position 1) -
   rather than the key - the first item in each (key, value) tuple (at
   index position 0).

   we will use the optional keyword argument key in [183]sorted(iterable[,
   cmp[, key[, reverse]]]) to specify a sorting key that is not the same
   as the dict key (recall that the dict key is the default sorting key
   for sorted()).
   in [96]:
import operator

sorted(attribute_values_cap_type.items(),
       key=operator.itemgetter(1))

   out[96]:
[('b', 'bell'),
 ('c', 'conical'),
 ('x', 'convex'),
 ('f', 'flat'),
 ('k', 'knobbed'),
 ('s', 'sunken')]

   we can now sort the counts of attribute values in descending frequency
   of occurrence, and print them out using tuple unpacking.
   in [97]:
attribute = 'cap-shape'
value_counts = simple_ml.attribute_value_counts(clean_instances,
                                                attribute,
                                                attribute_names)

print('counts for each value of', attribute, '(sorted by count):')
for value, count in sorted(value_counts.items(),
                           key=operator.itemgetter(1),
                           reverse=true):
    print(value, ':', count)

counts for each value of cap-shape (sorted by count):
x : 2840
f : 2432
b : 300
k : 36
s : 32
c : 4

   note that this example is rather contrived, as it is generally easiest
   to use a counter and its associated most_common() method when sorting a
   dictionary wherein the values are all counts. the need to sort other
   kinds of dictionaries by their values is rather common.

string formatting[184]  

   it is often helpful to use [185]fancier output formatting than simply
   printing comma-delimited lists of items.

   examples of the [186]str.format() function used in conjunction with
   print statements is shown below.

   more details can be found in the python documentation on [187]format
   string syntax.
   in [98]:
print('{:5.3f}'.format(0.1))  # fieldwidth = 5; precision = 3; f = float
print('{:7.3f}'.format(0.1))  # if fieldwidth is larger than needed, left pad wi
th spaces
print('{:07.3f}'.format(0.1))  # use leading zero to left pad with leading zeros
print('{:3d}'.format(1))  # d = int
print('{:03d}'.format(1))
print('{:10s}'.format('hello'))  # s = string, left-justified
print('{:>10s}'.format('hello'))  # use '>' to right-justify within fieldwidth

0.100
  0.100
000.100
  1
001
hello
     hello

   the following example illustrates the use of str.format() on data
   associated with the mushroom dataset.
   in [99]:
print('class: {} = {} ({:5.3f}), {} = {} ({:5.3f})'.format(
        'e', 3488, 3488 / 5644,
        'p', 2156, 2156 / 5644), end=' ')

class: e = 3488 (0.618), p = 2156 (0.382)

   the following variation - splitting off the printing of the attribute
   name from the printing of the values and counts of values for that
   attrbiute - may be more useful in developing a solution to the
   following exercise.
   in [100]:
print('class:', end=' ')  # keeps cursor on the same line for subsequent print s
tatements
print('{} = {} ({:5.3f}),'.format('e', 3488, 3488 / 5644), end=' ')
print('{} = {} ({:5.3f})'.format('p', 2156, 2156 / 5644), end=' ')
print()  # advance the cursor to the beginning of the next line

class: e = 3488 (0.618), p = 2156 (0.382)

exercise 5: define print_all_attribute_value_counts()[188]  

   define a function, print_all_attribute_value_counts(instances,
   attribute_names), that prints each attribute name in attribute_names,
   and then for each attribute value, prints the value abbreviation, the
   count of occurrences of that value and the proportion of instances that
   have that attribute value.
   in [101]:
# your function definition goes here

print('\ncounts for all attributes and values:\n')
# delete 'simple_ml.' in the function call below to test your function
simple_ml.print_all_attribute_value_counts(clean_instances, attribute_names)

counts for all attributes and values:

class: e = 3488 (0.618), p = 2156 (0.382),
cap-shape: x = 2840 (0.503), f = 2432 (0.431), b = 300 (0.053), k = 36 (0.006),
s = 32 (0.006), c = 4 (0.001),
cap-surface: y = 2220 (0.393), f = 2160 (0.383), s = 1260 (0.223), g = 4 (0.001)
,
cap-color: g = 1696 (0.300), n = 1164 (0.206), y = 1056 (0.187), w = 880 (0.156)
, e = 588 (0.104), b = 120 (0.021), p = 96 (0.017), c = 44 (0.008),
bruises?: t = 3184 (0.564), f = 2460 (0.436),
odor: n = 2776 (0.492), f = 1584 (0.281), a = 400 (0.071), l = 400 (0.071), p =
256 (0.045), c = 192 (0.034), m = 36 (0.006),
gill-attachment: f = 5626 (0.997), a = 18 (0.003),
gill-spacing: c = 4620 (0.819), w = 1024 (0.181),
gill-size: b = 4940 (0.875), n = 704 (0.125),
gill-color: p = 1384 (0.245), n = 984 (0.174), w = 966 (0.171), h = 720 (0.128),
 g = 656 (0.116), u = 480 (0.085), k = 408 (0.072), r = 24 (0.004), y = 22 (0.00
4),
stalk-shape: t = 2880 (0.510), e = 2764 (0.490),
stalk-root: b = 3776 (0.669), e = 1120 (0.198), c = 556 (0.099), r = 192 (0.034)
,
stalk-surface-above-ring: s = 3736 (0.662), k = 1332 (0.236), f = 552 (0.098), y
 = 24 (0.004),
stalk-surface-below-ring: s = 3544 (0.628), k = 1296 (0.230), f = 552 (0.098), y
 = 252 (0.045),
stalk-color-above-ring: w = 3136 (0.556), p = 1008 (0.179), g = 576 (0.102), n =
 448 (0.079), b = 432 (0.077), c = 36 (0.006), y = 8 (0.001),
stalk-color-below-ring: w = 3088 (0.547), p = 1008 (0.179), g = 576 (0.102), n =
 496 (0.088), b = 432 (0.077), c = 36 (0.006), y = 8 (0.001),
veil-type: p = 5644 (1.000),
veil-color: w = 5636 (0.999), y = 8 (0.001),
ring-number: o = 5488 (0.972), t = 120 (0.021), n = 36 (0.006),
ring-type: p = 3488 (0.618), l = 1296 (0.230), e = 824 (0.146), n = 36 (0.006),
spore-print-color: n = 1920 (0.340), k = 1872 (0.332), h = 1584 (0.281), w = 148
 (0.026), r = 72 (0.013), u = 48 (0.009),
population: v = 2160 (0.383), y = 1688 (0.299), s = 1104 (0.196), a = 384 (0.068
), n = 256 (0.045), c = 52 (0.009),
habitat: d = 2492 (0.442), g = 1860 (0.330), p = 568 (0.101), u = 368 (0.065), m
 = 292 (0.052), l = 64 (0.011),

4. using python to build and use a simple decision tree classifier[189]  

id90[190]  

   wikipedia offers the following description of a [191]decision tree
   (with italics added to emphasize terms that will be elaborated below):

     a decision tree is a flowchart-like structure in which each internal
     node represents a test of an attribute, each branch represents an
     outcome of that test and each leaf node represents class label (a
     decision taken after testing all attributes in the path from the
     root to the leaf). each path from the root to a leaf can also be
     represented as a classification rule.

   [id90 can also be used for regression, wherein the goal is to
   predict a continuous value rather than a class label, but we will focus
   here solely on their use for classification.]

   the image below depicts a decision tree created from the uci mushroom
   dataset that appears on [192]andy g's blog post about decision tree
   learning, where
     * a white box represents an internal node (and the label represents
       the attribute being tested)
     * a blue box represents an attribute value (an outcome of the test of
       that attribute)
     * a green box represents a leaf node with a class label of edible
     * a red box represents a leaf node with a class label of poisonous

   [mushroomdecisiontree.png]

   it is important to note that the uci mushroom dataset consists entirely
   of [193]categorical variables, i.e., every variable (or attribute) has
   an enumerated set of possible values. many datasets include numeric
   variables that can take on int or float values. tests for such
   variables typically use comparison operators, e.g., $age < 65$ or
   $36,250 < adjusted\_gross\_income <= 87,850$. [aside: python supports
   boolean expressions containing multiple comparison operators, such as
   the expression comparing adjusted_gross_income in the preceding
   example.]

   our simple decision tree will only accommodate categorical variables.
   we will closely follow a version of the [194]decision tree learning
   algorithm implementation offered by chris roach.

   our goal in the following sections is to use python to
     * create a simple decision tree using a set of training instances
     * classify (predict class labels) for a set of test instances using a
       simple decision tree
     * evaluate the performance of a simple decision tree on classifying a
       set of test instances

   first, we will explore some concepts and algorithms used in building
   and using id90.

id178[195]  

   when building a supervised classification model, the frequency
   distribution of attribute values is a potentially important factor in
   determining the relative importance of each attribute at various stages
   in the model building process.

   in data modeling, we can use frequency distributions to compute
   id178, a measure of disorder (impurity) in a set.

   we compute the id178 of multiplying the proportion of instances with
   each class label by the log of that proportion, and then taking the
   negative sum of those terms.

   more precisely, for a 2-class (binary) classification task:

   $id178(s) = - p_1 log_2 (p_1) - p_2 log_2 (p_2)$

   where $p_i$ is proportion (relative frequency) of class i within the
   set s.

   from the output above, we know that the proportion of clean_instances
   that are labeled 'e' (class edible) in the uci dataset is $3488 \div
   5644 = 0.618$, and the proportion labeled 'p' (class poisonous) is
   $2156 \div 5644 = 0.382$.

   after importing the python [196]math module, we can use the
   [197]math.log(x[, base]) function in computing the id178 of the
   clean_instances of the uci mushroom data set as follows.
   in [102]:
import math

id178 = \
    - (3488 / 5644) * math.log(3488 / 5644, 2) \
    - (2156 / 5644) * math.log(2156 / 5644, 2)
print(id178)

0.959441337353

exercise 6: define id178()[198]  

   define a function, id178(instances), that computes the id178 of
   instances. you may assume the class label is in position 0; we will
   later see how to specify default parameter values in function
   definitions.

   [note: the class label in many data files is the last rather than the
   first item on each line.]
   in [103]:
# your function definition here

# delete 'simple_ml.' in the function call below to test your function
print(simple_ml.id178(clean_instances))

0.959441337353

information gain[199]  

   informally, a decision tree is constructed from a set of instances
   using a recursive algorithm that
     * selects the best attribute
     * splits the set into subsets based on the values of that attribute
       (each subset is composed of instances from the original set that
       have the same value for that attribute)
     * repeats the process on each of these subsets until a stopping
       condition is met (e.g., a subset has no instances or has instances
       which all have the same class label)

   id178 is a metric that can be used in selecting the best attribute
   for each split: the best attribute is the one resulting in the largest
   decrease in id178 for a set of instances. [note: other metrics can be
   used for determining the best attribute]

   information gain measures the decrease in id178 that results from
   splitting a set of instances based on an attribute.

   $ig(s, a) = id178(s) - [p(s_1)    id178(s_1) + p(s_2)    id178(s_2)
   ... + p(s_n)    id178(s_n)]$

   where
     * $n$ is the number of distinct values of attribute $a$
     * $s_i$ is the subset of $s$ where all instances have the $i$th value
       of $a$
     * $p(s_i)$ is the proportion of instances in $s$ that have the $i$th
       value of $a$

   we'll use the definition of information_gain() in simple_ml to print
   the information gain for each of the attributes in the mushroom dataset
   ... before asking you to write your own definition of the function.
   in [104]:
print('information gain for different attributes:', end='\n\n')
for i in range(1, len(attribute_names)):
    print('{:5.3f}  {:2} {}'.format(
        simple_ml.information_gain(clean_instances, i), i, attribute_names[i]))

information gain for different attributes:

0.017   1 cap-shape
0.005   2 cap-surface
0.195   3 cap-color
0.140   4 bruises?
0.860   5 odor
0.004   6 gill-attachment
0.058   7 gill-spacing
0.032   8 gill-size
0.213   9 gill-color
0.275  10 stalk-shape
0.097  11 stalk-root
0.425  12 stalk-surface-above-ring
0.409  13 stalk-surface-below-ring
0.306  14 stalk-color-above-ring
0.279  15 stalk-color-below-ring
0.000  16 veil-type
0.002  17 veil-color
0.012  18 ring-number
0.463  19 ring-type
0.583  20 spore-print-color
0.110  21 population
0.101  22 habitat

   we can sort the attributes based in decreasing order of information
   gain, which shows that odor is the best attribute for the first split
   in a decision tree that models the instances in this dataset.
   in [105]:
print('information gain for different attributes:', end='\n\n')
sorted_information_gain_indexes = sorted([(simple_ml.information_gain(clean_inst
ances, i), i)
                                          for i in range(1, len(attribute_names)
)],
                                         reverse=true)
for gain, i in sorted_information_gain_indexes:
    print('{:5.3f}  {:2} {}'.format(gain, i, attribute_names[i]))

information gain for different attributes:

0.860   5 odor
0.583  20 spore-print-color
0.463  19 ring-type
0.425  12 stalk-surface-above-ring
0.409  13 stalk-surface-below-ring
0.306  14 stalk-color-above-ring
0.279  15 stalk-color-below-ring
0.275  10 stalk-shape
0.213   9 gill-color
0.195   3 cap-color
0.140   4 bruises?
0.110  21 population
0.101  22 habitat
0.097  11 stalk-root
0.058   7 gill-spacing
0.032   8 gill-size
0.017   1 cap-shape
0.012  18 ring-number
0.005   2 cap-surface
0.004   6 gill-attachment
0.002  17 veil-color
0.000  16 veil-type

   [the following variation does not use a list comprehension]
   in [106]:
print('information gain for different attributes:', end='\n\n')

information_gain_values = []
for i in range(1, len(attribute_names)):
    information_gain_values.append((simple_ml.information_gain(clean_instances,
i), i))

sorted_information_gain_indexes = sorted(information_gain_values,
                                         reverse=true)
for gain, i in sorted_information_gain_indexes:
    print('{:5.3f}  {:2} {}'.format(gain, i, attribute_names[i]))

information gain for different attributes:

0.860   5 odor
0.583  20 spore-print-color
0.463  19 ring-type
0.425  12 stalk-surface-above-ring
0.409  13 stalk-surface-below-ring
0.306  14 stalk-color-above-ring
0.279  15 stalk-color-below-ring
0.275  10 stalk-shape
0.213   9 gill-color
0.195   3 cap-color
0.140   4 bruises?
0.110  21 population
0.101  22 habitat
0.097  11 stalk-root
0.058   7 gill-spacing
0.032   8 gill-size
0.017   1 cap-shape
0.012  18 ring-number
0.005   2 cap-surface
0.004   6 gill-attachment
0.002  17 veil-color
0.000  16 veil-type

exercise 7: define information_gain()[200]  

   define a function, information_gain(instances, i), that returns the
   information gain achieved by selecting the ith attribute to split
   instances. it should exhibit the same behavior as the simple_ml version
   of the function.
   in [107]:
# your definition of information_gain(instances, i) here

# delete 'simple_ml.' in the function call below to test your function
sorted_information_gain_indexes = sorted([(simple_ml.information_gain(clean_inst
ances, i), i)
                                          for i in range(1, len(attribute_names)
)],
                                         reverse=true)

print('information gain for different attributes:', end='\n\n')
for gain, i in sorted_information_gain_indexes:
    print('{:5.3f}  {:2} {}'.format(gain, i, attribute_names[i]))

information gain for different attributes:

0.860   5 odor
0.583  20 spore-print-color
0.463  19 ring-type
0.425  12 stalk-surface-above-ring
0.409  13 stalk-surface-below-ring
0.306  14 stalk-color-above-ring
0.279  15 stalk-color-below-ring
0.275  10 stalk-shape
0.213   9 gill-color
0.195   3 cap-color
0.140   4 bruises?
0.110  21 population
0.101  22 habitat
0.097  11 stalk-root
0.058   7 gill-spacing
0.032   8 gill-size
0.017   1 cap-shape
0.012  18 ring-number
0.005   2 cap-surface
0.004   6 gill-attachment
0.002  17 veil-color
0.000  16 veil-type

building a simple decision tree[201]  

   we will implement a modified version of the [202]  algorithm for
   building a simple decision tree.
  (examples, target_attribute, candidate_attributes)
    create a root node for the tree
    if all examples have the same value of the target_attribute,
        return the single-node tree root with label = that value
    if the list of candidate_attributes is empty,
        return the single node tree root,
            with label = most common value of target_attribute in the examples.
    otherwise begin
        a     the attribute that best classifies examples (most information gain)
        decision tree attribute for root = a.
        for each possible value, v_i, of a,
            add a new tree branch below root, corresponding to the test a = v_i.
            let examples(v_i) be the subset of examples that have the value v_i
for a
            if examples(v_i) is empty,
                below this new branch add a leaf node
                    with label = most common target value in the examples
            else
                below this new branch add the subtree
                      (examples(v_i), target_attribute, attributes     {a})
    end
    return root

   [note: the algorithm above is recursive, i.e., the there is a recursive
   call to   within the definition of  . covering recursion is beyond
   the scope of this primer, but there are a number of other resources on
   [203]using recursion in python. familiarity with recursion will be
   important for understanding both the tree construction and
   classification functions below.]

   in building a decision tree, we will need to split the instances based
   on the index of the best attribute, i.e., the attribute that offers the
   highest information gain. we will use separate utility functions to
   handle these subtasks. to simplify the functions, we will rely
   exclusively on attribute indexes rather than attribute names.

   first, we will define a function, split_instances(instances,
   attribute_index), to split a set of instances based on any attribute.
   this function will return a dictionary where each key is a distinct
   value of the specified attribute_index, and the value of each key is a
   list representing the subset of instances that have that
   attribute_index value.

   we will use a [204]defaultdict, a specialized dictionary class in the
   [205]collections module, which automatically creates an appropriate
   default value for a new key. for example, a defaultdict(int)
   automatically initializes a new dictionary entry to 0 (zero); a
   defaultdict(list) automatically initializes a new dictionary entry to
   the empty list ([]).
   in [108]:
from collections import defaultdict

def split_instances(instances, attribute_index):
    '''returns a list of dictionaries, splitting a list of instances
        according to their values of a specified attribute index

    the key of each dictionary is a distinct value of attribute_index,
    and the value of each dictionary is a list representing
       the subset of instances that have that value for the attribute
    '''
    partitions = defaultdict(list)
    for instance in instances:
        partitions[instance[attribute_index]].append(instance)
    return partitions

   to test the function, we will partition the clean_instances based on
   the odor attribute (index position 5) and print out the size (number of
   instances) in each partition rather than the lists of instances in each
   partition.
   in [109]:
partitions = split_instances(clean_instances, 5)
print([(partition, len(partitions[partition])) for partition in partitions])

[('a', 400), ('c', 192), ('f', 1584), ('m', 36), ('l', 400), ('n', 2776), ('p',
256)]

   now that we can split instances based on a particular attribute, we
   would like to be able to choose the best attribute with which to split
   the instances, where best is defined as the attribute that provides the
   greatest information gain if instances were split based on that
   attribute. we will want to restrict the candidate attributes so that we
   don't bother trying to split on an attribute that was used higher up in
   the decision tree (or use the target attribute as a candidate).

exercise 8: define choose_best_attribute_index()[206]  

   define a function, choose_best_attribute_index(instances,
   candidate_attribute_indexes), that returns the index in the list of
   candidate_attribute_indexes that provides the highest information gain
   if instances are split based on that attribute index.
   in [110]:
# your function here

# delete 'simple_ml.' in the function call below to test your function
print('best attribute index:',
      simple_ml.choose_best_attribute_index(clean_instances, range(1, len(attrib
ute_names))))

best attribute index: 5

   a leaf node in a decision tree represents the most frequently occurring
   - or majority - class value for that path through the tree. we will
   need a function that determines the majority value for the class index
   among a set of instances. one way to do this is to use the [207]counter
   class introduced above.
   in [111]:
class_counts = counter([instance[0] for instance in clean_instances])
print('class_counts: {}\n  most_common(1): {}\n  most_common(1)[0][0]: {}'.forma
t(
    class_counts,  # the counter object
    class_counts.most_common(1),  # returns a list in which the 1st element is a
 tuple with the most common value and its count
    class_counts.most_common(1)[0][0]))  # the most common value (1st element in
 that tuple)

class_counts: counter({'e': 3488, 'p': 2156})
  most_common(1): [('e', 3488)]
  most_common(1)[0][0]: e

   [the following variation does not use a list comprehension]
   in [112]:
class_counts = counter()  # create an empty counter
for instance in clean_instances:
    class_counts[instance[0]] += 1

print ('class_counts: {}\n  most_common(1): {}\n  most_common(1)[0][0]: {}'.form
at(
    class_counts,
    class_counts.most_common(1),
    class_counts.most_common(1)[0][0]))

class_counts: counter({'e': 3488, 'p': 2156})
  most_common(1): [('e', 3488)]
  most_common(1)[0][0]: e

   it is often useful to compute the number of unique values and/or the
   total number of values in a counter.

   the number of unique values is simply the number of dictionary entries.

   the total number of values can be computed by taking the [208]sum() of
   all the counts (the value of each key: value pair ... or key, value
   tuple, if we use counter().most_common()).
   in [113]:
print('number of unique values: {}'.format(len(class_counts)))
print('total number of values:  {}'.format(sum([v
                                                for k, v in class_counts.most_co
mmon()])))

number of unique values: 2
total number of values:  5644

   before putting all this together to define a decision tree construction
   function, we will cover a few additional aspects of python used in that
   function.

truth values in python[209]  

   python offers a very flexible mechanism for the [210]testing of truth
   values: in an if condition, any null object, zero-valued numerical
   expression or empty container (string, list, dictionary or tuple) is
   interpreted as false (i.e., not true):
   in [114]:
for x in [false, none, 0, 0.0, "", [], {}, ()]:
    print('"{}" is'.format(x), end=' ')
    if x:
        print(true)
    else:
        print(false)

"false" is false
"none" is false
"0" is false
"0.0" is false
"" is false
"[]" is false
"{}" is false
"()" is false

   sometimes, particularly with function parameters, it is helpful to
   differentiate none from empty lists and other data structures with a
   false truth value (one common use case is illustrated in
   create_decision_tree() below).
   in [115]:
for x in [false, none, 0, 0.0, "", [], {}, ()]:
    print('"{} is none" is'.format(x), end=' ')
    if x is none:
        print(true)
    else:
        print(false)

"false is none" is false
"none is none" is true
"0 is none" is false
"0.0 is none" is false
" is none" is false
"[] is none" is false
"{} is none" is false
"() is none" is false

conditional expressions (ternary operators)[211]  

   python also offers a [212]conditional expression (ternary operator)
   that allows the functionality of an if/else statement that returns a
   value to be implemented as an expression. for example, the if/else
   statement in the code above could be implemented as a conditional
   expression as follows:
   in [116]:
for x in [false, none, 0, 0.0, "", [], {}, ()]:
    print('"{}" is {}'.format(x, true if x else false))

"false" is false
"none" is false
"0" is false
"0.0" is false
"" is false
"[]" is false
"{}" is false
"()" is false

more on optional parameters in python functions[213]  

   python function definitions can specify [214]default parameter values
   indicating the value those parameters will have if no argument is
   explicitly provided when the function is called. arguments can also be
   passed using [215]keyword parameters indicting which parameter will be
   assigned a specific argument value (which may or may not correspond to
   the order in which the parameters are defined).

   the [216]python tutorial page on default parameters includes the
   following warning:

     important warning: the default value is evaluated only once. this
     makes a difference when the default is a mutable object such as a
     list, dictionary, or instances of most classes.

   thus it is generally better to use the python null object, none, rather
   than an empty list ([]), dict ({}) or other mutable data structure when
   specifying default parameter values for any of those data types.
   in [117]:
def parameter_test(parameter1=none, parameter2=none):
    '''prints the values of parameter1 and parameter2'''
    print('parameter1: {}; parameter2: {}'.format(parameter1, parameter2))

parameter_test()  # no args are required
parameter_test(1)  # if any args are provided, 1st arg gets assigned to paramete
r1
parameter_test(1, 2)  # 2nd arg gets assigned to parameter2
parameter_test(2)  # remember: if only 1 arg, 1st arg gets assigned to arg1
parameter_test(parameter2=2)  # can use keyword to provide a value only for para
meter2
parameter_test(parameter2=2, parameter1=1) #  can use keywords for either arg, i
n either order

parameter1: none; parameter2: none
parameter1: 1; parameter2: none
parameter1: 1; parameter2: 2
parameter1: 2; parameter2: none
parameter1: none; parameter2: 2
parameter1: 1; parameter2: 2

exercise 9: define majority_value()[217]  

   define a function, majority_value(instances, class_index), that returns
   the most frequently occurring value of class_index in instances. the
   class_index parameter should be optional, and have a default value of 0
   (zero).

   your function definition should support the use of optional arguments
   as used in the function calls below.
   in [118]:
# your definition of majority_value(instances) here

# delete 'simple_ml.' in the function calls below to test your function

print('majority value of index {}: {}'.format(
    0, simple_ml.majority_value(clean_instances)))

# although there is only one class_index for the dataset,
# we'll test the function by specifying other indexes using optional / keyword a
rguments
print('majority value of index {}: {}'.format(
    1, simple_ml.majority_value(clean_instances, 1)))  # using argument order
print('majority value of index {}: {}'.format(
    2, simple_ml.majority_value(clean_instances, class_index=2)))  # using keywo
rd argument

majority value of index 0: e
majority value of index 1: x
majority value of index 2: y

building a simple decision tree[218]  

   the recursive create_decision_tree() function below uses an optional
   parameter, class_index, which defaults to 0. this is to accommodate
   other datasets in which the class label is the last element on each
   line (which would be most easily specified by using a -1 value). most
   data files in the [219]uci machine learning repository have the class
   labels as either the first element or the last element.

   to show how the decision tree is being built, an optional trace
   parameter, when non-zero, will generate some trace information as the
   tree is constructed. the indentation level is incremented with each
   recursive call via the use of the conditional expression (ternary
   operator), trace + 1 if trace else 0.
   in [119]:
def create_decision_tree(instances,
                         candidate_attribute_indexes=none,
                         class_index=0,
                         default_class=none,
                         trace=0):
    '''returns a new decision tree trained on a list of instances.

    the tree is constructed by recursively selecting and splitting instances bas
ed on
    the highest information_gain of the candidate_attribute_indexes.

    the class label is found in position class_index.

    the default_class is the majority value for the current node's parent in the
 tree.
    a positive (int) trace value will generate trace information
        with increasing levels of indentation.

    derived from the simplified   algorithm presented in building decision tre
es in python
        by christopher roach,
    http://www.onlamp.com/pub/a/python/2006/02/09/ai_decision_trees.html?page=3
    '''

    # if no candidate_attribute_indexes are provided,
    # assume that we will use all but the target_attribute_index
    # note that none != [],
    # as an empty candidate_attribute_indexes list is a recursion stopping condi
tion
    if candidate_attribute_indexes is none:
        candidate_attribute_indexes = [i
                                       for i in range(len(instances[0]))
                                       if i != class_index]
        # note: do not use candidate_attribute_indexes.remove(class_index)
        # as this would destructively modify the argument,
        # causing problems during recursive calls

    class_labels_and_counts = counter([instance[class_index] for instance in ins
tances])

    # if the dataset is empty or the candidate attributes list is empty,
    # return the default value
    if not instances or not candidate_attribute_indexes:
        if trace:
            print('{}using default class {}'.format('< ' * trace, default_class)
)
        return default_class

    # if all the instances have the same class label, return that class label
    elif len(class_labels_and_counts) == 1:
        class_label = class_labels_and_counts.most_common(1)[0][0]
        if trace:
            print('{}all {} instances have label {}'.format(
                '< ' * trace, len(instances), class_label))
        return class_label
    else:
        default_class = simple_ml.majority_value(instances, class_index)

        # choose the next best attribute index to best classify the instances
        best_index = simple_ml.choose_best_attribute_index(
            instances, candidate_attribute_indexes, class_index)
        if trace:
            print('{}creating tree node for attribute index {}'.format(
                    '> ' * trace, best_index))

        # create a new decision tree node with the best attribute index
        # and an empty dictionary object (for now)
        tree = {best_index:{}}

        # create a new decision tree sub-node (branch) for each of the values
        # in the best attribute field
        partitions = simple_ml.split_instances(instances, best_index)

        # remove that attribute from the set of candidates for further splits
        remaining_candidate_attribute_indexes = [i
                                                 for i in candidate_attribute_in
dexes
                                                 if i != best_index]
        for attribute_value in partitions:
            if trace:
                print('{}creating subtree for value {} ({}, {}, {}, {})'.format(
                    '> ' * trace,
                    attribute_value,
                    len(partitions[attribute_value]),
                    len(remaining_candidate_attribute_indexes),
                    class_index,
                    default_class))

            # create a subtree for each value of the the best attribute
            subtree = create_decision_tree(
                partitions[attribute_value],
                remaining_candidate_attribute_indexes,
                class_index,
                default_class,
                trace + 1 if trace else 0)

            # add the new subtree to the empty dictionary object
            # in the new tree/node we just created
            tree[best_index][attribute_value] = subtree

    return tree

# split instances into separate training and testing sets
training_instances = clean_instances[:-20]
test_instances = clean_instances[-20:]
tree = create_decision_tree(training_instances, trace=1)  # remove trace=1 to tu
rn off tracing
print(tree)

> creating tree node for attribute index 5
> creating subtree for value a (400, 21, 0, e)
< < all 400 instances have label e
> creating subtree for value c (192, 21, 0, e)
< < all 192 instances have label p
> creating subtree for value f (1584, 21, 0, e)
< < all 1584 instances have label p
> creating subtree for value m (28, 21, 0, e)
< < all 28 instances have label p
> creating subtree for value l (400, 21, 0, e)
< < all 400 instances have label e
> creating subtree for value n (2764, 21, 0, e)
> > creating tree node for attribute index 20
> > creating subtree for value k (1296, 20, 0, e)
< < < all 1296 instances have label e
> > creating subtree for value r (72, 20, 0, e)
< < < all 72 instances have label p
> > creating subtree for value w (100, 20, 0, e)
> > > creating tree node for attribute index 21
> > > creating subtree for value y (24, 19, 0, e)
< < < < all 24 instances have label e
> > > creating subtree for value c (16, 19, 0, e)
< < < < all 16 instances have label p
> > > creating subtree for value v (60, 19, 0, e)
< < < < all 60 instances have label e
> > creating subtree for value n (1296, 20, 0, e)
< < < all 1296 instances have label e
> creating subtree for value p (256, 21, 0, e)
< < all 256 instances have label p
{5: {'a': 'e', 'c': 'p', 'f': 'p', 'm': 'p', 'l': 'e', 'n': {20: {'k': 'e', 'r':
 'p', 'w': {21: {'y': 'e', 'c': 'p', 'v': 'e'}}, 'n': 'e'}}, 'p': 'p'}}

   the structure of the tree shown above is rather difficult to discern
   from the normal printed representation of a dictionary.

   the python [220]pprint module has a number of useful methods for
   pretty-printing or formatting objects in a more human readable way.

   the [221]pprint.pprint(object, stream=none, indent=1, width=80,
   depth=none) method will print object to a stream (a default value of
   none will dictate the use of [222]sys.stdout, the same destination as
   print function output), using indent spaces to differentiate nesting
   levels, using up to a maximum width columns and up to to a maximum
   nesting level depth (none indicating no maximum).
   in [120]:
from pprint import pprint

pprint(tree)

{5: {'a': 'e',
     'c': 'p',
     'f': 'p',
     'l': 'e',
     'm': 'p',
     'n': {20: {'k': 'e',
                'n': 'e',
                'r': 'p',
                'w': {21: {'c': 'p', 'v': 'e', 'y': 'e'}}}},
     'p': 'p'}}

classifying instances with a simple decision tree[223]  

   usually, when we construct a decision tree based on a set of training
   instances, we do so with the intent of using that tree to classify a
   set of one or more test instances.

   we will define a function, classify(tree, instance,
   default_class=none), to use a decision tree to classify a single
   instance, where an optional default_class can be specified as the
   return value if the instance represents a set of attribute values that
   don't have a representation in the decision tree.

   we will use a design pattern in which we will use a series of if
   statements, each of which returns a value if the condition is true,
   rather than a nested series of if, elif and/or else clauses, as it
   helps constrain the levels of indentation in the function.
   in [121]:
def classify(tree, instance, default_class=none):
    '''returns a classification label for instance, given a decision tree'''
    if not tree:  # if the node is empty, return the default class
        return default_class
    if not isinstance(tree, dict):  # if the node is a leaf, return its class la
bel
        return tree
    attribute_index = list(tree.keys())[0]  # using list(dict.keys()) for python
 3 compatibility
    attribute_values = list(tree.values())[0]
    instance_attribute_value = instance[attribute_index]
    if instance_attribute_value not in attribute_values:  # this value was not i
n training data
        return default_class
    # recursively traverse the subtree (branch) associated with instance_attribu
te_value
    return classify(attribute_values[instance_attribute_value], instance, defaul
t_class)

for instance in test_instances:
    predicted_label = classify(tree, instance)
    actual_label = instance[0]
    print('predicted: {}; actual: {}'.format(predicted_label, actual_label))

predicted: p; actual: p
predicted: p; actual: p
predicted: p; actual: p
predicted: e; actual: e
predicted: e; actual: e
predicted: p; actual: p
predicted: e; actual: e
predicted: e; actual: e
predicted: e; actual: e
predicted: p; actual: p
predicted: e; actual: e
predicted: e; actual: e
predicted: e; actual: e
predicted: p; actual: p
predicted: e; actual: e
predicted: e; actual: e
predicted: e; actual: e
predicted: e; actual: e
predicted: p; actual: p
predicted: p; actual: p

evaluating the accuracy of a simple decision tree[224]  

   it is often helpful to evaluate the performance of a model using a
   dataset not used in the training of that model. in the simple example
   shown above, we used all but the last 20 instances to train a simple
   decision tree, then classified those last 20 instances using the tree.

   the advantage of this training/test split is that visual inspection of
   the classifications (sometimes called predictions) is relatively
   straightforward, revealing that all 20 instances were correctly
   classified.

   there are a variety of metrics that can be used to evaluate the
   performance of a model. [225]scikit learn's model evaluation library
   provides an overview and implementation of several possible metrics.
   for now, we'll simply measure the accuracy of a model, i.e., the
   percentage of test instances that are correctly classified (true
   positives and true negatives).

   the accuracy of the model above, given the set of 20 test instances, is
   100% (20/20).

   the function below calculates the classification accuracy of a tree
   over a set of test_instances (with an optional class_index parameter
   indicating the position of the class label in each instance).
   in [122]:
def classification_accuracy(tree, test_instances, class_index=0, default_class=n
one):
    '''returns the accuracy of classifying test_instances with tree,
    where the class label is in position class_index'''
    num_correct = 0
    for i in range(len(test_instances)):
        prediction = classify(tree, test_instances[i], default_class)
        actual_value = test_instances[i][class_index]
        if prediction == actual_value:
            num_correct += 1
    return num_correct / len(test_instances)

print(classification_accuracy(tree, test_instances))

1.0

   in addition to showing the percentage of correctly classified
   instances, it may be helpful to return the actual counts of correctly
   and incorrectly classified instances, e.g., if we want to compile a
   total count of correctly and incorrectly classified instances over a
   collection of test instances.

   in order to do so, we'll use the [226]zip([iterable, ...]) function,
   which combines 2 or more sequences or iterables; the function returns a
   list of tuples, where the ith tuple contains the ith element from each
   of the argument sequences or iterables.
   in [123]:
zip([0, 1, 2], ['a', 'b', 'c'])

   out[123]:
[(0, 'a'), (1, 'b'), (2, 'c')]

   we can use [227]list comprehensions, the counter class and the zip()
   function to modify classification_accuracy() so that it returns a
   packed tuple with
     * the percentage of instances correctly classified
     * the number of correctly classified instances
     * the number of incorrectly classified instances

   we'll also modify the function to use instances rather than
   test_instances, as we sometimes want to be able to valuate the accuracy
   of a model when tested on the training instances used to create it.
   in [124]:
def classification_accuracy(tree, instances, class_index=0, default_class=none):
    '''returns the accuracy of classifying test_instances with tree,
    where the class label is in position class_index'''
    predicted_labels = [classify(tree, instance, default_class)
                        for instance in instances]
    actual_labels = [x[class_index]
                     for x in instances]
    counts = counter([x == y
                      for x, y in zip(predicted_labels, actual_labels)])
    return counts[true] / len(instances), counts[true], counts[false]

print(classification_accuracy(tree, test_instances))

(1.0, 20, 0)

   we sometimes want to partition the instances into subsets of equal
   sizes to measure performance. one metric this partitioning allows us to
   compute is a [228]learning curve, i.e., assess how well the model
   performs based on the size of its training set. another use of these
   partitions (aka folds) would be to conduct an [229]n-fold cross
   validation evaluation.

   the following function, partition_instances(instances, num_partitions),
   partitions a set of instances into num_partitions relatively equally
   sized subsets.

   we'll use this as yet another opportunity to demonstrate the power of
   using list comprehensions, this time, to condense the use of nested for
   loops.
   in [125]:
def partition_instances(instances, num_partitions):
    '''returns a list of relatively equally sized disjoint sublists (partitions)

    of the list of instances'''
    return [[instances[j]
             for j in range(i, len(instances), num_partitions)]
            for i in range(num_partitions)]

   before testing this function on the 5644 clean_instances from the uci
   mushroom dataset, we'll create a small number of simplified instances
   to verify that the function has the desired behavior.
   in [126]:
instance_length = 3
num_instances = 5

simplified_instances = [[j
                         for j in range(i, instance_length + i)]
                        for i in range(num_instances)]

print('instances:', simplified_instances)
partitions = partition_instances(simplified_instances, 2)
print('partitions:', partitions)

instances: [[0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]]
partitions: [[[0, 1, 2], [2, 3, 4], [4, 5, 6]], [[1, 2, 3], [3, 4, 5]]]

   [the following variations do not use list comprehensions]
   in [127]:
def partition_instances(instances, num_partitions):
    '''returns a list of relatively equally sized disjoint sublists (partitions)

    of the list of instances'''
    partitions = []
    for i in range(num_partitions):
        partition = []
        # iterate over instances starting at position i in increments of num_par
itions
        for j in range(i, len(instances), num_partitions):
            partition.append(instances[j])
        partitions.append(partition)
    return partitions

simplified_instances = []
for i in range(num_instances):
    new_instance = []
    for j in range(i, instance_length + i):
        new_instance.append(j)
    simplified_instances.append(new_instance)

print('instances:', simplified_instances)
partitions = partition_instances(simplified_instances, 2)
print('partitions:', partitions)

instances: [[0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]]
partitions: [[[0, 1, 2], [2, 3, 4], [4, 5, 6]], [[1, 2, 3], [3, 4, 5]]]

   the [230]enumerate(sequence, start=0) function creates an iterator that
   successively returns the index and value of each element in a sequence,
   beginning at the start index.
   in [128]:
for i, x in enumerate(['a', 'b', 'c']):
    print(i, x)

0 a
1 b
2 c

   we can use enumerate() to facilitate slightly more rigorous testing of
   our partition_instances function on our simplified_instances.

   note that since we are printing values rather than accumulating values,
   we will not use nested list comprehensions for this task.
   in [129]:
for i in range(num_instances):
    print('\n# partitions:', i)
    for j, partition in enumerate(partition_instances(simplified_instances, i)):
        print('partition {}: {}'.format(j, partition))

# partitions: 0

# partitions: 1
partition 0: [[0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]]

# partitions: 2
partition 0: [[0, 1, 2], [2, 3, 4], [4, 5, 6]]
partition 1: [[1, 2, 3], [3, 4, 5]]

# partitions: 3
partition 0: [[0, 1, 2], [3, 4, 5]]
partition 1: [[1, 2, 3], [4, 5, 6]]
partition 2: [[2, 3, 4]]

# partitions: 4
partition 0: [[0, 1, 2], [4, 5, 6]]
partition 1: [[1, 2, 3]]
partition 2: [[2, 3, 4]]
partition 3: [[3, 4, 5]]

   returning our attention to the uci mushroom dataset, the following will
   partition our clean_instances into 10 relatively equally sized disjoint
   subsets. we will use a list comprehension to print out the length of
   each partition
   in [130]:
partitions = partition_instances(clean_instances, 10)
print([len(partition) for partition in partitions])

[565, 565, 565, 565, 564, 564, 564, 564, 564, 564]

   [the following variation does not use a list comprehension]
   in [131]:
for partition in partitions:
    print(len(partition), end=' ')
print()

565 565 565 565 564 564 564 564 564 564

   the following shows the different trees that are constructed based on
   partition 0 (first 10th) of clean_instances, partitions 0 and 1 (first
   2/10ths) of clean_instances and all clean_instances.
   in [132]:
tree0 = create_decision_tree(partitions[0])
print('tree trained with {} instances:'.format(len(partitions[0])))
pprint(tree0)
print()

tree1 = create_decision_tree(partitions[0] + partitions[1])
print('tree trained with {} instances:'.format(len(partitions[0] + partitions[1]
)))
pprint(tree1)
print()

tree = create_decision_tree(clean_instances)
print('tree trained with {} instances:'.format(len(clean_instances)))
pprint(tree)

tree trained with 565 instances:
{5: {'a': 'e',
     'c': 'p',
     'f': 'p',
     'l': 'e',
     'm': 'p',
     'n': {20: {'k': 'e', 'n': 'e', 'r': 'p', 'w': 'e'}},
     'p': 'p'}}

tree trained with 1130 instances:
{5: {'a': 'e',
     'c': 'p',
     'f': 'p',
     'l': 'e',
     'm': 'p',
     'n': {20: {'k': 'e',
                'n': 'e',
                'r': 'p',
                'w': {21: {'c': 'p', 'v': 'e', 'y': 'e'}}}},
     'p': 'p'}}

tree trained with 5644 instances:
{5: {'a': 'e',
     'c': 'p',
     'f': 'p',
     'l': 'e',
     'm': 'p',
     'n': {20: {'k': 'e',
                'n': 'e',
                'r': 'p',
                'w': {21: {'c': 'p', 'v': 'e', 'y': 'e'}}}},
     'p': 'p'}}

   the only difference between the first two trees - tree0 and tree1 - is
   that in the first tree, instances with no odor (attribute index 5 is
   'n') and a spore-print-color of white (attribute 20 = 'w') are
   classified as edible ('e'). with additional training data in the 2nd
   partition, an additional distinction is made such that instances with
   no odor, a white spore-print-color and a clustered population
   (attribute 21 = 'c') are classified as poisonous ('p'), while all other
   instances with no odor and a white spore-print-color (and any other
   value for the population attribute) are classified as edible ('e').

   note that there is no difference between tree1 and tree (the tree
   trained with all instances). this early convergence on an optimal model
   is uncommon on most datasets (outside the uci repository).

learning curves[231]  

   now that we can partition our instances into subsets, we can use these
   subsets to construct different-sized training sets in the process of
   computing a learning curve.

   we will start off with an initial training set consisting only of the
   first partition, and then progressively extend that training set by
   adding a new partition during each iteration of computing the learning
   curve.

   the [232]list.extend(l) method enables us to extend list by appending
   all the items in another list, l, to the end of list.
   in [133]:
x = [1, 2, 3]
x.extend([4, 5])
print(x)

[1, 2, 3, 4, 5]

   we can now define the function, compute_learning_curve(instances,
   num_partitions=10), which will take a list of instances, partition it
   into num_partitions relatively equally sized disjoint partitions, and
   then iteratively evaluate the accuracy of models trained with an
   incrementally increasing combination of instances in the first
   num_partitions - 1 partitions then tested with instances in the last
   partition, a variant of . that is, a model trained with the first
   partition will be constructed (and tested), then a model trained with
   the first 2 partitions will be constructed (and tested), and so on.

   the function will return a list of num_partitions - 1 tuples
   representing the size of the training set and the accuracy of a tree
   trained with that set (and tested on the num_partitions - 1 set). this
   will provide some indication of the relative impact of the size of the
   training set on model performance.
   in [134]:
def compute_learning_curve(instances, num_partitions=10):
    '''returns a list of training sizes and scores for incrementally increasing
partitions.

    the list contains 2-element tuples, each representing a training size and sc
ore.
    the i-th training size is the number of instances in partitions 0 through nu
m_partitions - 2.
    the i-th score is the accuracy of a tree trained with instances
    from partitions 0 through num_partitions - 2
    and tested on instances from num_partitions - 1 (the last partition).'''

    partitions = partition_instances(instances, num_partitions)
    test_instances = partitions[-1][:]
    training_instances = []
    accuracy_list = []
    for i in range(0, num_partitions - 1):
        # for each iteration, the training set is composed of partitions 0 throu
gh i - 1
        training_instances.extend(partitions[i][:])
        tree = create_decision_tree(training_instances)
        partition_accuracy = classification_accuracy(tree, test_instances)
        accuracy_list.append((len(training_instances), partition_accuracy))
    return accuracy_list

accuracy_list = compute_learning_curve(clean_instances)
print(accuracy_list)

[(565, (0.9964539007092199, 562, 2)), (1130, (1.0, 564, 0)), (1695, (1.0, 564, 0
)), (2260, (1.0, 564, 0)), (2824, (1.0, 564, 0)), (3388, (1.0, 564, 0)), (3952,
(1.0, 564, 0)), (4516, (1.0, 564, 0)), (5080, (1.0, 564, 0))]

   the uci mushroom dataset is a particularly clean and simple data set,
   enabling quick convergence on an optimal decision tree for classifying
   new instances using relatively few training instances.

   we can use a larger number of smaller partitions to see a little more
   variation in accuracy performance.
   in [135]:
accuracy_list = compute_learning_curve(clean_instances, 100)
print(accuracy_list[:10])

[(57, (0.9821428571428571, 55, 1)), (114, (1.0, 56, 0)), (171, (0.98214285714285
71, 55, 1)), (228, (1.0, 56, 0)), (285, (1.0, 56, 0)), (342, (1.0, 56, 0)), (399
, (1.0, 56, 0)), (456, (1.0, 56, 0)), (513, (1.0, 56, 0)), (570, (1.0, 56, 0))]

object-oriented programming: defining a python class to encapsulate a simple
decision tree[233]  

   the simple decision tree defined above uses a python dictionary for its
   representation. one can imagine using other data structures, and/or
   extending the decision tree to support confidence estimates, numeric
   features and other capabilities that are often included in more fully
   functional implementations. to support future extensibility, and hide
   the details of the representation from the user, it would be helpful to
   have a user-defined class for simple id90.

   python is an [234]object-oriented programming language, offering simple
   syntax and semantics for defining classes and instantiating objects of
   those classes. [it is assumed that the reader is already familiar with
   the concepts of object-oriented programming]

   a python [235]class starts with the keyword class followed by a class
   name (identifier), a colon (':'), and then any number of statements,
   which typically take the form of assignment statements for class or
   instance variables and/or function definitions for class methods. all
   statements are indented to reflect their inclusion in the class
   definition.

   the members - methods, class variables and instance variables - of a
   class are accessed by prepending self. to each reference. class methods
   always include self as the first parameter.

   all class members in python are public (accessible outside the class).
   there is no mechanism for private class members, but identifiers with
   leading double underscores (__member_identifier) are 'mangled'
   (translated into _class_name__member_identifier), and thus not directly
   accessible outside their class, and can be used to approximate private
   members by python programmers.

   there is also no mechanism for protected identifiers - accessible only
   within a defining class and its subclasses - in the python language,
   and so python programmers have adopted the convention of using a single
   underscore (_identifier) at the start of any identifier that is
   intended to be protected (i.e., not to be accessed outside the class or
   its subclasses).

   some python programmers only use the single underscore prefixes and
   avoid double underscore prefixes due to unintended consequences that
   can arise when names are mangled. the following warning about single
   and double underscore prefixes is issued in [236]code like a
   pythonista:

     try to avoid the __private form. i never use it. trust me. if you
     use it, you will regret it later

   we will follow this advice and avoid using the double underscore prefix
   in user-defined member variables and methods.

   python has a number of pre-defined [237]special method names, all of
   which are denoted by leading and trailing double underscores. for
   example, the [238]object.__init__(self[, ...]) method is used to
   specify instructions that should be executed whenever a new object of a
   class is instantiated.

   note that other machine learning libraries may use different
   terminology for some of the functions we defined above. for example, in
   the [239]sklearn.tree.decisiontreeclassifier class (and in most sklearn
   classifier classes), the method for constructing a classifier is named
   [240]fit() - since it "fits" the data to a model - and the method for
   classifying instances is named [241]predict() - since it is predicting
   the class label for an instance.

   in keeping with this common terminology, the code below defines a
   class, simpledecisiontree, with a single pseudo-protected member
   variable _tree, three public methods - fit(), predict() and pprint() -
   and two pseudo-protected auxilary methods - _create_tree() and
   _predict() - to augment the fit() and predict() methods, respectively.

   the fit() method is identical to the create_decision_tree() function
   above, with the inclusion of the self parameter (as it is now a class
   method rather than a function). the predict() method is a similarly
   modified version of the classify() function, with the added capability
   to predict the label of either a single instance or a list of
   instances. the classification_accuracy() method is similar to the
   function of the same name (with the addition of the self parameter).
   the pprint() method prints the tree in a human-readable format.

   most comments and the use of the trace parameter have been removed to
   make the code more compact, but are included in the version found in
   simple_decision_tree.py.
   in [136]:
class simpledecisiontree:

    _tree = {}  # this instance variable becomes accessible to class methods via
 self._tree

    def __init__(self):
        # this is where we would initialize any parameters to the simpledecision
tree
        pass

    def fit(self,
            instances,
            candidate_attribute_indexes=none,
            target_attribute_index=0,
            default_class=none):
        if not candidate_attribute_indexes:
            candidate_attribute_indexes = [i
                                           for i in range(len(instances[0]))
                                           if i != target_attribute_index]
        self._tree = self._create_tree(instances,
                                       candidate_attribute_indexes,
                                       target_attribute_index,
                                       default_class)

    def _create_tree(self,
                     instances,
                     candidate_attribute_indexes,
                     target_attribute_index=0,
                     default_class=none):
        class_labels_and_counts = counter([instance[target_attribute_index]
                                           for instance in instances])
        if not instances or not candidate_attribute_indexes:
            return default_class
        elif len(class_labels_and_counts) == 1:
            class_label = class_labels_and_counts.most_common(1)[0][0]
            return class_label
        else:
            default_class = simple_ml.majority_value(instances, target_attribute
_index)
            best_index = simple_ml.choose_best_attribute_index(instances,
                                                               candidate_attribu
te_indexes,
                                                               target_attribute_
index)
            tree = {best_index:{}}
            partitions = simple_ml.split_instances(instances, best_index)
            remaining_candidate_attribute_indexes = [i
                                                     for i in candidate_attribut
e_indexes
                                                     if i != best_index]
            for attribute_value in partitions:
                subtree = self._create_tree(
                    partitions[attribute_value],
                    remaining_candidate_attribute_indexes,
                    target_attribute_index,
                    default_class)
                tree[best_index][attribute_value] = subtree
            return tree

    def predict(self, instances, default_class=none):
        if not isinstance(instances, list):
            return self._predict(self._tree, instance, default_class)
        else:
            return [self._predict(self._tree, instance, default_class)
                    for instance in instances]

    def _predict(self, tree, instance, default_class=none):
        if not tree:
            return default_class
        if not isinstance(tree, dict):
            return tree
        attribute_index = list(tree.keys())[0]  # using list(dict.keys()) for py
3 compatibiity
        attribute_values = list(tree.values())[0]
        instance_attribute_value = instance[attribute_index]
        if instance_attribute_value not in attribute_values:
            return default_class
        return self._predict(attribute_values[instance_attribute_value],
                             instance,
                             default_class)

    def classification_accuracy(self, instances, default_class=none):
        predicted_labels = self.predict(instances, default_class)
        actual_labels = [x[0] for x in instances]
        counts = counter([x == y for x, y in zip(predicted_labels, actual_labels
)])
        return counts[true] / len(instances), counts[true], counts[false]

    def pprint(self):
        pprint(self._tree)

   the following statements instantiate a simpledecisiontree, using all
   but the last 20 clean_instances, prints out the tree using its pprint()
   method, and then uses the classify() method to print the classification
   of the last 20 clean_instances.
   in [137]:
simple_decision_tree = simpledecisiontree()
simple_decision_tree.fit(training_instances)
simple_decision_tree.pprint()
print()

predicted_labels = simple_decision_tree.predict(test_instances)
actual_labels = [instance[0] for instance in test_instances]
for predicted_label, actual_label in zip(predicted_labels, actual_labels):
    print('model: {}; truth: {}'.format(predicted_label, actual_label))
print()
print('classification accuracy:', simple_decision_tree.classification_accuracy(t
est_instances))

{5: {'a': 'e',
     'c': 'p',
     'f': 'p',
     'l': 'e',
     'm': 'p',
     'n': {20: {'k': 'e',
                'n': 'e',
                'r': 'p',
                'w': {21: {'c': 'p', 'v': 'e', 'y': 'e'}}}},
     'p': 'p'}}

model: p; truth: p
model: p; truth: p
model: p; truth: p
model: e; truth: e
model: e; truth: e
model: p; truth: p
model: e; truth: e
model: e; truth: e
model: e; truth: e
model: p; truth: p
model: e; truth: e
model: e; truth: e
model: e; truth: e
model: p; truth: p
model: e; truth: e
model: e; truth: e
model: e; truth: e
model: e; truth: e
model: p; truth: p
model: p; truth: p

classification accuracy: (1.0, 20, 0)

5. next steps[242]  

   there are a variety of python libraries - e.g., [243]scikit-learn - for
   building more full-featured id90 and other types of models
   based on a variety of machine learning algorithms. hopefully, this
   primer will have prepared you for learning how to use those libraries
   effectively.

   many python-based machine learning libraries use other external python
   libraries such as [244]numpy, [245]scipy, [246]matplotlib and
   [247]pandas. there are tutorials available for each of these libraries,
   including the following:
     * [248]tentative numpy tutorial
     * [249]scipy tutorial
     * [250]matplotlib pyplot tutorial
     * [251]pandas tutorials (especially [252]10 minutes to pandas)

   there are many machine learning or data science resources that may be
   useful to help you continue the journey. here is a sampling:
     * scikit-learn's tutorial, [253]an introduction to machine learning
       with scikit-learn
     * kevin markham's video series (on the kaggle blog), [254]an
       introduction to machine learning with scikit-learn
     * kaggle's [255]getting started with python for data science
     * coursera's [256]introduction to data science
     * olivier grisel's strata 2014 tutorial, [257]parallel machine
       learning with scikit-learn and ipython

   please feel free to contact the author ([258]joe mccarthy) to suggest
   additional resources.

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [259]fastly, rendered by [260]rackspace

   nbviewer github [261]repository.

   nbviewer version: [262]33c4683

   nbconvert version: [263]5.4.0

   rendered (fri, 05 apr 2019 18:20:44 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb
   5. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb
   6. https://github.com/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb
   7. https://mybinder.org/v2/gh/gumption/python_for_data_science/master?filepath=python_for_data_science_all.ipynb
   8. https://raw.githubusercontent.com/gumption/python_for_data_science/master/python_for_data_science_all.ipynb
   9. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/tree/master
  10. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/tree/master/python_for_data_science_all.ipynb
  11. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#python-for-data-science
  12. http://interrelativity.com/joe
  13. http://www.indeed.com/
  14. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#1.-introduction
  15. http://www.python.org/
  16. http://www.python.org/
  17. http://www.nltk.org/book/
  18. http://www.nltk.org/book/
  19. http://www.nltk.org/
  20. http://www.nltk.org/book/ch06.html
  21. http://docs.python.org/2/tutorial/
  22. http://www.amazon.com/python-scripting-computational-science-engineering/dp/3642093159
  23. http://folk.uio.no/hpl/
  24. http://www.amazon.com/python-scripting-computational-science-engineering/dp/3642093159
  25. http://people.stern.nyu.edu/fprovost/
  26. http://data-science-for-biz.com/
  27. https://docs.google.com/document/pub?id=1p6vowseuiezlbwnfkgse70a8lxfsrrixqpf5nbg8f3a
  28. http://scikit-learn.org/
  29. http://scikit-learn.org/
  30. http://scipy-lectures.github.io/index.html
  31. http://www.numpy.org/
  32. http://www.scipy.org/
  33. http://matplotlib.org/
  34. http://strataconf.com/strata2014/public/schedule/detail/32033
  35. http://gumption.typepad.com/blog/2014/02/ipython-deep-learning-doing-good-some-highlights-from-strata-2014.html
  36. http://ipython.org/install.html
  37. http://legacy.python.org/dev/peps/pep-0008/
  38. http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html
  39. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#2.-data-science:-basic-concepts
  40. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#data-science-and-data-mining
  41. http://data-science-for-biz.com/
  42. http://home.comcast.net/~tom.fawcett/public_html/index.html
  43. http://data-science-for-biz.com/
  44. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#knowledge-discovery,-data-mining-and-machine-learning
  45. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#cross-industry-standard-process-for-data-mining-(crisp-dm)
  46. https://en.wikipedia.org/wiki/cross_industry_standard_process_for_data_mining
  47. https://en.wikipedia.org/wiki/cross_industry_standard_process_for_data_mining
  48. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#data-science-workflow
  49. http://www.pgbovine.net/
  50. http://cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext
  51. http://pgbovine.net/projects/pubs/guo_phd_dissertation.pdf
  52. http://cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext
  53. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#supervised-classification
  54. http://www.nltk.org/book
  55. http://www.nltk.org/book/ch06.html
  56. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#data-mining-terminology
  57. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#data-mining-example:-uci-mushroom-dataset
  58. http://cml.ics.uci.edu/
  59. https://archive.ics.uci.edu/ml/datasets.html
  60. https://archive.ics.uci.edu/ml/datasets/mushroom
  61. https://archive.ics.uci.edu/ml/datasets/mushroom
  62. https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data
  63. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#3.-python:-basic-concepts
  64. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#a-note-on-python-2-vs.-python-3
  65. https://docs.python.org/2/
  66. https://docs.python.org/3/
  67. http://sebastianraschka.com/
  68. http://nbviewer.ipython.org/github/rasbt/python_reference/blob/master/tutorials/key_differences_between_python_2_and_3.ipynb
  69. http://python-future.org/compatible_idioms.html
  70. https://www.google.com/q=python 2 vs 3
  71. https://twitter.com/ncoghlan_dev
  72. https://docs.python.org/2/library/__future__.html
  73. https://docs.python.org/2/reference/simple_stmts.html#print
  74. https://docs.python.org/3/library/functions.html#print
  75. https://www.python.org/dev/peps/pep-0238/
  76. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#names-(identifiers),-strings-&-binding-values-to-names-(assignment)
  77. http://docs.python.org/2/tutorial/introduction.html#strings
  78. http://docs.python.org/2/reference/lexical_analysis.html#identifiers
  79. https://docs.python.org/2/reference/executionmodel.html#naming-and-binding
  80. http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html
  81. http://docs.python.org/2/reference/simple_stmts.html#assignment-statements
  82. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#printing
  83. https://docs.python.org/3/library/functions.html#print
  84. http://docs.python.org/2/library/sys.html#sys.stdout
  85. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#comments
  86. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#lists
  87. http://docs.python.org/2/tutorial/introduction.html#lists
  88. http://docs.python.org/2/library/stdtypes.html#str.split
  89. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#accessing-sequence-elements-&-subsequences
  90. http://docs.python.org/2/library/stdtypes.html#typesseq
  91. http://docs.python.org/2/tutorial/introduction.html
  92. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#splitting-/-separating-statements
  93. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#processing-strings-&-other-sequences
  94. http://docs.python.org/2/library/stdtypes.html#str.strip
  95. https://docs.python.org/2/library/csv.html
  96. http://docs.python.org/2/library/string.html#string.join
  97. http://docs.python.org/2/library/functions.html#len
  98. http://docs.python.org/2/library/stdtypes.html#str.count
  99. http://docs.python.org/2/library/stdtypes.html#str.index
 100. https://docs.python.org/2/library/exceptions.html#exceptions.valueerror
 101. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#mutability
 102. http://docs.python.org/2/reference/datamodel.html
 103. http://docs.python.org/2/tutorial/datastructures.html#more-on-lists
 104. https://docs.python.org/2/library/functions.html#dir
 105. https://docs.python.org/2.7/library/functions.html#sorted
 106. https://docs.python.org/2.7/library/functions.html#reversed
 107. http://docs.python.org/2/tutorial/controlflow.html#keyword-arguments
 108. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#tuples-(immutable-list-like-sequences)
 109. http://docs.python.org/2/tutorial/datastructures.html#tuples-and-sequences
 110. https://docs.python.org/2/library/exceptions.html#exceptions.attributeerror
 111. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#conditionals
 112. http://docs.python.org/2/tutorial/errors.html
 113. http://docs.python.org/2/tutorial/controlflow.html#if-statements
 114. http://legacy.python.org/dev/peps/pep-0008/
 115. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#seeking-forgiveness-vs.-asking-for-permission-(eafp-vs.-lbyl)
 116. http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html#eafp-vs-lbyl
 117. http://oranlooney.com/lbyl-vs-eafp/
 118. http://docs.python.org/2/tutorial/errors.html#handling-exceptions
 119. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#defining-and-calling-functions
 120. http://docs.python.org/2/tutorial/controlflow.html#defining-functions
 121. http://docs.python.org/2/reference/simple_stmts.html#the-return-statement
 122. http://docs.python.org/2/tutorial/controlflow.html#tut-docstrings
 123. http://docs.python.org/2.7/library/functions.html#type
 124. https://docs.python.org/2/library/functions.html#isinstance
 125. http://stackoverflow.com/questions/1549801/differences-between-isinstance-and-type-in-python
 126. https://docs.python.org/2/tutorial/classes.html#inheritance
 127. https://en.wikipedia.org/wiki/duck_typing
 128. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#call-by-sharing
 129. https://en.wikipedia.org/wiki/evaluation_strategy#call_by_sharing
 130. https://docs.python.org/2/tutorial/datastructures.html#more-on-lists
 131. http://docs.python.org/2/library/copy.html
 132. http://docs.python.org/2/library/copy.html#copy.copy
 133. http://docs.python.org/2/library/copy.html#copy.deepcopy
 134. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#multiple-return-values
 135. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#iteration:-for,-range
 136. http://docs.python.org/2/tutorial/controlflow.html#for-statements
 137. http://docs.python.org/2/glossary.html#term-iterable
 138. http://docs.python.org/2/tutorial/controlflow.html#the-range-function
 139. http://docs.python.org/2/library/functions.html#range
 140. http://docs.python.org/2/library/functions.html#xrange
 141. http://docs.python.org/2/glossary.html#term-iterable
 142. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#modules,-namespaces-and-dotted-notation
 143. http://docs.python.org/2/tutorial/modules.html
 144. http://docs.python.org/2/tutorial/classes.html#python-scopes-and-namespaces
 145. http://docs.python.org/2/reference/simple_stmts.html#the-import-statement
 146. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#exercise-1:-define-print_attribute_names_and_values()
 147. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#file-i/o
 148. http://docs.python.org/2/tutorial/inputoutput.html#reading-and-writing-files
 149. http://docs.python.org/2/library/stdtypes.html#file-objects
 150. http://docs.python.org/2/library/functions.html#open
 151. http://docs.python.org/2/library/functions.html#open
 152. http://docs.python.org/2/reference/compound_stmts.html#the-with-statement
 153. http://docs.python.org/2/library/stdtypes.html#file.close
 154. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#exercise-2:-define-load_instances()
 155. http://docs.python.org/2/library/stdtypes.html#file.write
 156. http://docs.python.org/2/library/stdtypes.html#str.join
 157. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#list-comprehensions
 158. http://docs.python.org/2/tutorial/datastructures.html#list-comprehensions
 159. http://docs.python.org/2/library/string.html#string.join
 160. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#missing-values-&-"clean"-instances
 161. http://www.data-science-for-biz.com/
 162. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#dictionaries-(dicts)
 163. http://docs.python.org/2/tutorial/datastructures.html#dictionaries
 164. https://ipython.org/ipython-doc/dev/interactive/tutorial.html#system-shell-commands
 165. https://ipython.org/ipython-doc/dev/interactive/magics.html
 166. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#exercise-3:-define-load_attribute_values()
 167. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#counters
 168. http://docs.python.org/2/library/collections.html
 169. http://docs.python.org/2/library/collections.html#collections.counter
 170. http://docs.python.org/2/library/collections.html#collections.counter.most_common
 171. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#exercise-4:-define-attribute_value_counts()
 172. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#more-on-sorting
 173. http://docs.python.org/2/library/functions.html#sorted
 174. http://docs.python.org/2/glossary.html#term-iterable
 175. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#sorting-a-dictionary-by-values
 176. http://writeonly.wordpress.com/2008/08/30/sorting-dictionaries-by-value-in-python-improved/
 177. http://legacy.python.org/dev/peps/pep-0265/
 178. http://docs.python.org/2/library/stdtypes.html#dict.items
 179. http://docs.python.org/2/library/stdtypes.html#dict.iteritems
 180. http://docs.python.org/2/library/stdtypes.html#iterator-types
 181. http://docs.python.org/2/library/operator.html
 182. http://docs.python.org/2/library/operator.html#operator.itemgetter
 183. http://docs.python.org/2/library/functions.html#sorted
 184. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#string-formatting
 185. http://docs.python.org/2/tutorial/inputoutput.html#fancier-output-formatting
 186. https://docs.python.org/2/library/stdtypes.html#str.format
 187. http://docs.python.org/2/library/string.html#format-string-syntax
 188. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#exercise-5:-define-print_all_attribute_value_counts()
 189. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#4.-using-python-to-build-and-use-a-simple-decision-tree-classifier
 190. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#decision-trees
 191. https://en.wikipedia.org/wiki/decision_tree
 192. http://gieseanw.wordpress.com/2012/03/03/decision-tree-learning/
 193. https://en.wikipedia.org/wiki/categorical_variable
 194. http://www.onlamp.com/pub/a/python/2006/02/09/ai_decision_trees.html?page=3
 195. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#id178
 196. http://docs.python.org/2/library/math.html
 197. http://docs.python.org/2/library/math.html#math.log
 198. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#exercise-6:-define-id178()
 199. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#information-gain
 200. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#exercise-7:-define-information_gain()
 201. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#building-a-simple-decision-tree
 202. https://en.wikipedia.org/wiki/ _algorithm
 203. https://www.google.com/search?q=python+recursion
 204. http://docs.python.org/2/library/collections.html#defaultdict-objects
 205. http://docs.python.org/2/library/collections.html
 206. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#exercise-8:-define-choose_best_attribute_index()
 207. https://docs.python.org/2/library/collections.html#counter-objects
 208. https://docs.python.org/2/library/functions.html#sum
 209. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#truth-values-in-python
 210. http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html#testing-for-truth-values
 211. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#conditional-expressions-(ternary-operators)
 212. http://docs.python.org/2/reference/expressions.html#conditional-expressions
 213. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#more-on-optional-parameters-in-python-functions
 214. http://docs.python.org/2/tutorial/controlflow.html#default-argument-values
 215. http://docs.python.org/2/tutorial/controlflow.html#keyword-arguments
 216. http://docs.python.org/2/tutorial/controlflow.html#default-argument-values
 217. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#exercise-9:-define-majority_value()
 218. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#building-a-simple-decision-tree
 219. https://archive.ics.uci.edu/ml/datasets.html
 220. http://docs.python.org/2/library/pprint.html
 221. http://docs.python.org/2/library/pprint.html#pprint.pprint
 222. http://docs.python.org/2/library/sys.html#sys.stdout
 223. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#classifying-instances-with-a-simple-decision-tree
 224. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#evaluating-the-accuracy-of-a-simple-decision-tree
 225. http://scikit-learn.org/stable/modules/model_evaluation.html
 226. http://docs.python.org/2.7/library/functions.html#zip
 227. http://docs.python.org/2/tutorial/datastructures.html#list-comprehensions
 228. https://en.wikipedia.org/wiki/learning_curve
 229. https://en.wikipedia.org/wiki/cross-validation_(statistics
 230. http://docs.python.org/2.7/library/functions.html#enumerate
 231. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#learning-curves
 232. http://docs.python.org/2/tutorial/datastructures.html#more-on-lists
 233. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#object-oriented-programming:-defining-a-python-class-to-encapsulate-a-simple-decision-tree
 234. https://en.wikipedia.org/wiki/object-oriented_programming
 235. http://docs.python.org/2/tutorial/classes.html
 236. http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html#naming
 237. http://docs.python.org/2/reference/datamodel.html#special-method-names
 238. http://docs.python.org/2/reference/datamodel.html#object.__init__
 239. http://scikit-learn.org/stable/modules/generated/sklearn.tree.decisiontreeclassifier.html
 240. http://scikit-learn.org/stable/modules/generated/sklearn.tree.decisiontreeclassifier.html#sklearn.tree.decisiontreeclassifier.fit
 241. http://scikit-learn.org/stable/modules/generated/sklearn.tree.decisiontreeclassifier.html#sklearn.tree.decisiontreeclassifier.predict
 242. https://nbviewer.jupyter.org/github/gumption/python_for_data_science/blob/master/python_for_data_science_all.ipynb#5.-next-steps
 243. http://scikit-learn.org/
 244. http://www.numpy.org/
 245. http://www.scipy.org/scipylib/
 246. http://matplotlib.org/
 247. http://pandas.pydata.org/
 248. http://wiki.scipy.org/tentative_numpy_tutorial
 249. http://docs.scipy.org/doc/scipy/reference/tutorial/
 250. http://matplotlib.org/1.3.1/users/pyplot_tutorial.html
 251. http://pandas.pydata.org/pandas-docs/stable/tutorials.html
 252. http://pandas.pydata.org/pandas-docs/stable/10min.html
 253. http://scikit-learn.org/stable/tutorial/basic/tutorial.html
 254. http://blog.kaggle.com/2015/04/08/new-video-series-introduction-to-machine-learning-with-scikit-learn/
 255. http://www.kaggle.com/wiki/gettingstartedwithpythonfordatascience
 256. https://www.coursera.org/course/datasci
 257. https://github.com/ogrisel/parallel_ml_tutorial
 258. https://nbviewer.jupyter.org/cdn-cgi/l/email-protection#ef85808aaf86819b8a9d9d8a838e9b8699869b96c18c8082d09c9a8d858a8c9bd2bf969b878081cf89809dcfab8e9b8ecfbc8c868a818c8a
 259. http://www.fastly.com/
 260. https://developer.rackspace.com/?nbviewer=awesome
 261. https://github.com/jupyter/nbviewer
 262. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
 263. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
