   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as slides
     * [5]view as code
     * [6]python 2 kernel
     * [7]view on github
     * [8]execute on binder
     * [9]download notebook

    1. [10]python-bigdata
    2. [11]src
    3. [12]main
    4. [13]ipynb

numerical computing in python[14]  

   (c) 2016 [15]steve phelps

overview[16]  

     * floating-point representation
     * arrays and matrices with numpy
     * basic plotting with matplotlib
     * pseudo-random variates with numpy.random

representing continuous values[17]  

     * digital computers are inherently discrete.
     * real numbers $x \in r$ cannot always be represented exactly in a
       digital computer.
     * they are stored in a format called floating-point.
     * [18]ieee standard 754 specifies a universal format across different
       implementations.
          + as always there are deviations from the standard.
     * there are two standard sizes of floating-point numbers: 32-bit and
       64-bit.
     * 64-bit numbers are called double precision, are sometimes called
       double values.
     * ieee floating-point calculations are performed in hardware on
       modern computers.
     * how can we represent aribitrary real values using only 32 bits?

fixed-point verses floating-point[19]  

     * one way we could discretise continuous values is to represent them
       as two integers $x$ and $y$.
     * the final value is obtained by e.g. $r = x + y \times 10^{-5}$.
     * so the number $500.4421$ would be represented as the tuple $x =
       500$, $y = 44210$.
     * the exponent $5$ is fixed for all computations.
     * this number represents the precision with which we can represent
       real values.
     * it corresponds to the where we place we place the decimal point.
     * this scheme is called fixed precision.
     * it is useful in certain circumstances, but suffers from many
       problems.
     * in practice, we use variable precision, also known as floating
       point.

scientific notation[20]  

     * humans also use a form of floating-point representation.
     * in [21]scientific notation, all numbers are written in the form $m
       \times 10^n$.
     * when represented in ascii, we abbreviate this <m>e<n>, for example
       6.72e+11 = $6.72 \times 10^{11}$.
     * the integer $m$ is called the significand or mantissa.
     * the integer $n$ is called the exponent.
     * the integer $10$ is the base.

scientific notation in python[22]  

     * python uses scientific notation when it displays floating-point
       numbers:

   in [1]:
print 672000000000.0

6.72e+11

     * note that internally, the value is not represented exactly like
       this.
     * scientific notation is a convention for writing or rendering
       numbers, not representing them digitally.

floating-point representation[23]  

     * floating point numbers use a base of $2$ instead of $10$.
     * additionally, the mantissa are exponent are stored in binary.
     * the mantissa uses [24]two's complement to represent positive and
       negative numbers.
          + one bit is reserved as the sign-bit: 1 for negative, 0 for
            positive values.
     * the mantissa is normalised, so we assume that it starts with the
       digit $1$ (which is not stored).

bias[25]  

     * we also need to represent signed exponents.
     * the exponent does not use two's complement.
     * instead a bias value is subtracted from the stored value to obtain
       the final stored exponent.
     * double-precision values use a bias of $1023$, and single-precision
       uses a bias value of $127$.
     * for example, in double-precision the exponent $1022$ would be
       encoded as $1$.
     * the stored exponent values $0$ and $1024$ are reserved for special
       values- discussed later.

double and single precision formats[26]  

   the number of bits allocated to represent each integer component of a
   float is given below:
   format sign exponent mantissa total
   single 1    8        23       32
   double 1    11       52       64
     * python normally works 64-bit precision.
     * numpy allows us to [27]specify the type when storing data in
       arrays.
     * this is particularly useful for big data where we may need to be
       careful about the storage requirements of our data-set.

loss of precision[28]  

     * we cannot represent every value in floating-point.
     * consider single-precision (32-bit).
     * let's try to represent $4,039,944,879$.
     * as a binary integer we write this:

   11110000 11001100 10101010 10101111
     * this already takes up 32-bits.
     * the mantissa only allows us to store 24-bit integers.
     * so we have to round. we store it as:

   +1.1110000 11001100 10101011e+31
     * which gives us

   +11110000 11001100 10101011 0000000

   $= 4,039,944,960$

ranges of floating-point values[29]  

   in single precision arithmetic, we cannot represent the following
   values:
     * negative numbers less than $-(2-2^{-23}) \times 2^{127}$
     * negative numbers greater than $-2^{-149}$
     * positive numbers less than $2^{-149}$
     * positive numbers greater than $(2-2^{-23}) \times 2^{127}$

   attempting to represent these numbers results in overflow or underflow.

effective floating-point range[30]  

   format              binary                        decimal
   single $\pm (2-2^{-23}) \times 2^{127}$  $\approx \pm 10^{38.53}$
   double $\pm (2-2^{-52}) \times 2^{1023}$ $\approx \pm 10^{308.25}$

[31]  

representing zero[32]  

     * zero cannot be represented straightforwardly because we assume that
       all mantissa values start with the digit 1.
     * zero is stored as a special-case, by setting mantissa and exponent
       both to zero.
     * the sign-bit can either be set or unset, so there are distinct
       positive and negative representations of zero.

zero in python[33]  

   in [2]:
x = +0.0
x

   out[2]:
0.0

   in [3]:
y = -0.0
y

   out[3]:
-0.0

     * however, these are considered equal:

   in [4]:
x == y

   out[4]:
true

infinity[34]  

     * positive overflow results in a special value of infinity (in python
       inf).
     * this is stored with an exponent consiting of all 1s, and a mantissa
       of all 0s.
     * the sign-bit allows us to differentiate between negative and
       positive overflow: $-\infty$ and $+\infty$.
     * this allows us to carry on calculating past an overflow event.

infinity in python[35]  

   in [5]:
x = 1e300 * 1e100
x

   out[5]:
inf

   in [6]:
x = x + 1
x

   out[6]:
inf

negative infinity in python[36]  

   in [7]:
x > 0

   out[7]:
true

   in [8]:
y = -x
y

   out[8]:
-inf

   in [9]:
y < x

   out[9]:
true

not a number (nan)[37]  

     * some mathematical operations on real numbers do not map onto real
       numbers.
     * these results are represented using the special value to nan which
       represents "not a (real) number".
     * nan is represented by an exponent of all 1s, and a non-zero
       mantissa.

nan in python[38]  

   in [10]:
from numpy import sqrt, inf, isnan, nan
x = sqrt(-1)
x

-c:2: runtimewarning: invalid value encountered in sqrt

   out[10]:
nan

   in [11]:
y = inf - inf
y

   out[11]:
nan

comparing nan values in python[39]  

     * beware of comparing nan values

   in [12]:
x == y

   out[12]:
false

     * to test whether a value is nan use the isnan function:

   in [13]:
isnan(x)

   out[13]:
true

nan is not the same as none[40]  

     * none represents a missing value.
     * nan represents an invalid floating-point value.
     * these are fundamentally different entities:

   in [14]:
nan is none

   out[14]:
false

   in [15]:
isnan(none)

---------------------------------------------------------------------------
typeerror                                 traceback (most recent call last)
<ipython-input-15-7d785ead34f4> in <module>()
----> 1 isnan(none)

typeerror: ufunc 'isnan' not supported for the input types, and the inputs could
 not be safely coerced to any supported types according to the casting rule ''sa
fe''

numerical methods[41]  

     * in e.g. simulation models or quantiative analysis we typically
       repeatedly update numerical values inside long loops.
     * programs such as these implement numerical algorithms.
     * it is very easy to introduce bugs into code like this.

numerical stability[42]  

     * the round-off error associated with a result can be compounded in a
       loop.
     * if the error increases as we go round the loop, we say the
       algorithm is numerically unstable.
     * mathematicians design numerically stable algorithms using
       [43]numerical analysis.

catastrophic cancellation[44]  

     * suppose we have two real values $x$ and $y = x + \epsilon$.
     * $\epsilon$ is very small and $x$ is very large.
     * $x$ has an exact floating point representation
     * however, because of lack of precision $x$ and $y$ have the same
       floating point representation.
          + i.e. they are represented as the same sequence of 64-bits

relative and absolute error[45]  

     * the absolute error $r = x - y = (x + \epsilon) - x$.
     * the relative error is $r = r/x = (x + \epsilon)/x - 1 =
       \epsilon/x$.
     * so the relative error is tiny because $x$ is large relative to
       $\epsilon$.
     * ok, but how about the error in computing the difference between
       these values.
     * if we perform this in floating-point we will get 0.
     * the correct value is $(x + \epsilon) - x = \epsilon$.
     * the relative error is infinite!

catastrophic cancellation in python[46]  

   in [16]:
x = 3.141592653589793
x

   out[16]:
3.141592653589793

   in [17]:
y = 6.022e23
x = (x + y) - y

   in [18]:
x

   out[18]:
0.0

     * avoid subtracting two nearly-equal numbers.
     * especially in a loop!
     * better-yet use a well-validated existing implementation in the form
       of a numerical library.

importing numpy[47]  

     * functions for numerical computiing are provided by a separate
       module called [48]numpy.
     * before we use the numpy module we must import it.
     * by convention, we import numpy using the alias np.
     * once we have done this we can prefix the functions in the numpy
       library using the prefix np.

   in [19]:
import numpy as np

     * we can now use the functions defined in this package by prefixing
       them with np.

arrays[49]  

     * arrays represent a collection of values.
     * in contrast to lists:
          + arrays typically have a fixed length
               o they can be resized, but this involves an expensive
                 copying process.
          + and all values in the array are of the same type.
               o typically we store floating-point values.
     * like lists:
          + arrays are mutable;
          + we can change the elements of an existing array.

arrays in numpy[50]  

     * arrays are provided by the numpy module.
     * the function array() creates an array given a list.

   in [20]:
import numpy as np
x = np.array([0, 1, 2, 3, 4])
x

   out[20]:
array([0, 1, 2, 3, 4])

array indexing[51]  

     * we can index an array just like a list

   in [21]:
x[4]

   out[21]:
4

   in [22]:
x[4] = 2
x

   out[22]:
array([0, 1, 2, 3, 2])

arrays are not lists[52]  

     * although this looks a bit like a list of numbers, it is a
       fundamentally different type of value:

   in [23]:
type(x)

   out[23]:
numpy.ndarray

     * for example, we cannot append to the array:

   in [24]:
x.append(5)

---------------------------------------------------------------------------
attributeerror                            traceback (most recent call last)
<ipython-input-24-2f1e4fff5cec> in <module>()
----> 1 x.append(5)

attributeerror: 'numpy.ndarray' object has no attribute 'append'

functions over arrays[53]  

     * when we use arithmetic operators on arrays, we create a new array
       with the result of applying the operator to each element.

   in [25]:
y = x * 2
y

   out[25]:
array([0, 2, 4, 6, 4])

     * the same goes for numerical functions:

   in [26]:
x = np.array([-1, 2, 3, -4])
y = abs(x)
y

   out[26]:
array([1, 2, 3, 4])

vectorized functions[54]  

     * note that not every function automatically works with arrays.
     * functions that have been written to work with arrays of numbers are
       called vectorized functions.
     * most of the functions in numpy are already vectorized.
     * you can create a vectorized version of any other function using the
       higher-order function numpy.vectorize().

vectorize example[55]  

   in [27]:
def myfunc(x):
    if x >= 0.5:
        return x
    else:
        return 0.0

fv = np.vectorize(myfunc)

   in [28]:
x = np.arange(0, 1, 0.1)
x

   out[28]:
array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])

   in [29]:
fv(x)

   out[29]:
array([ 0. ,  0. ,  0. ,  0. ,  0. ,  0.5,  0.6,  0.7,  0.8,  0.9])

populating arrays[56]  

     * to populate an array with a range of values we use the np.arange()
       function:

   in [30]:
x = np.arange(0, 10)
print x

[0 1 2 3 4 5 6 7 8 9]

     * we can also use floating point increments.

   in [31]:
x = np.arange(0, 1, 0.1)
print x

[ 0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]

basic plotting[57]  

     * we will use a module called matplotlib to plot some simple graphs.
     * this module has a nested module called pyplot.
     * by convention we import this with the alias plt.
     * this module provides functions which are very similar to [58]matlab
       plotting commands.

   in [32]:
import matplotlib.pyplot as plt
%matplotlib inline

y = x*2 + 5
plt.plot(x, y)

   out[32]:
[<matplotlib.lines.line2d at 0x7f33229deb90>]

   [i4k5e5ie4ubmr+seu
   7krefoiloxgrh+litktkh7i4exh5of8hfiljwgntr2maaaaasuvork5cyii= ]

plotting a sine curve[59]  

   in [33]:
from numpy import pi, sin

x = np.arange(0, 2*pi, 0.01)
y = sin(x)
plt.plot(x, y)

   out[33]:
[<matplotlib.lines.line2d at 0x7f33228e8d50>]

   [wddg7ezv2zpxwaaaabjru5erkjggg== ]

multi-dimensional data[60]  

     * numpy arrays can hold multi-dimensional data.
     * to create a multi-dimensional array, we can pass a list of lists to
       the array() function:

   in [34]:
import numpy as np

x = np.array([[1,2], [3,4]])
x

   out[34]:
array([[1, 2],
       [3, 4]])

arrays containing arrays[61]  

     * a multi-dimensional array is an array of an arrays.
     * the outer array holds the rows.
     * each row is itself an array:

   in [35]:
x[0]

   out[35]:
array([1, 2])

   in [36]:
x[1]

   out[36]:
array([3, 4])

     * so the element in the second row, and first column is:

   in [37]:
x[1][0]

   out[37]:
3

matrices[62]  

     * we can create a matrix from a multi-dimensional array.

   in [38]:
m = np.matrix(x)
m

   out[38]:
matrix([[1, 2],
        [3, 4]])

plotting multi-dimensional with matrices[63]  

     * if we supply a matrix to plot() then it will plot the y-values
       taken from the columns of the matrix (notice the transpose in the
       example below).

   in [39]:
from numpy import pi, sin, cos

x = np.arange(0, 2*pi, 0.01)
y = sin(x)
ax = plt.plot(x, np.matrix([sin(x), cos(x)]).t)

   [+kryrsyavlcaaaaasuvork5cyii= ]

performance[64]  

     * when we use numpy matrices in python the corresponding functions
       are linked with libraries written in c and fortran.
     * for example, see the [65]blas (basic id202 subprograms)
       library.
     * these libraries are very fast, and can be configured so that
       operations are performed in parallel on multiple cpu cores, or gpu
       hardware.

matrix operators[66]  

     * once we have a matrix, we can perform matrix computations.
     * to compute the [67]transpose and [68]inverse use the t and i
       attributes:

   to compute the transpose $m^{t}$
   in [40]:
m.t

   out[40]:
matrix([[1, 3],
        [2, 4]])

   to compute the inverse $m^{-1}$
   in [41]:
m.i

   out[41]:
matrix([[-2. ,  1. ],
        [ 1.5, -0.5]])

matrix dimensions[69]  

     * the total number of elements, and the dimensions of the array:

   in [42]:
m.size

   out[42]:
4

   in [43]:
m.shape

   out[43]:
(2, 2)

   in [44]:
len(m.shape)

   out[44]:
2

creating matrices from strings[70]  

     * we can also create arrays directly from strings, which saves some
       typing:

   in [45]:
i2 = np.matrix('2 0; 0 2')
i2

   out[45]:
matrix([[2, 0],
        [0, 2]])

     * the semicolon starts a new row.

id127[71]  

   now that we have two matrices, we can perform [72]matrix
   multiplication:
   in [46]:
m * i2

   out[46]:
matrix([[2, 4],
        [6, 8]])

matrix indexing[73]  

     * we can [74]index and slice matrices using the same syntax as lists.

   in [47]:
m[:,1]

   out[47]:
matrix([[2],
        [4]])

slices are references[75]  

     * if we use this is an assignment, we create a reference to the
       sliced elements, not a copy.

   in [48]:
v = m[:,1]  # this does not make a copy of the elements!
v

   out[48]:
matrix([[2],
        [4]])

   in [49]:
m[0,1] = -2
v

   out[49]:
matrix([[-2],
        [ 4]])

copying matrices and vectors[76]  

     * to copy a matrix, or a slice of its elements, use the function
       np.copy():

   in [50]:
m = np.matrix('1 2; 3 4')
v = np.copy(m[:,1])  # this does copy the elements.
v

   out[50]:
array([[2],
       [4]])

   in [51]:
m[0,1] = -2
v

   out[51]:
array([[2],
       [4]])

sums[77]  

   one way we could sum a vector or matrix is to use a for loop.
   in [52]:
vector = np.arange(0.0, 100.0, 10.0)
vector

   out[52]:
array([  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90.])

   in [53]:
result = 0.0
for x in vector:
    result = result + x
result

   out[53]:
450.0

     * this is not the most efficient way to compute a sum.

efficient sums[78]  

     * instead of using a for loop, we can use a numpy function sum().
     * this function is written in the c language, and is very fast.

   in [54]:
vector = np.array([0, 1, 2, 3, 4])
print np.sum(vector)

10

summing rows and columns[79]  

     * when dealing with multi-dimensional data, the 'sum()' function has
       a named-argument axis which allows us to specify whether to sum
       along, each rows or columns.

   in [55]:
matrix = np.matrix('1 2 3; 4 5 6; 7 8 9')
print matrix

[[1 2 3]
 [4 5 6]
 [7 8 9]]

     * to sum along rows:

   in [56]:
np.sum(matrix, axis=0)

   out[56]:
matrix([[12, 15, 18]])

     * to sum along columns:

   in [57]:
np.sum(matrix, axis=1)

   out[57]:
matrix([[ 6],
        [15],
        [24]])

cumulative sums[80]  

     * suppose we want to compute $y_n = \sum_{i=1}^{n} x_i$ where
       $\mathbf{x}$ is a vector.

   in [58]:
import numpy as np
x = np.array([0, 1, 2, 3, 4])
y = np.cumsum(x)
print y

[ 0  1  3  6 10]

cumulative sums along rows and columns[81]  

   in [59]:
x = np.matrix('1 2 3; 4 5 6; 7 8 9')
print x

[[1 2 3]
 [4 5 6]
 [7 8 9]]

   in [60]:
y = np.cumsum(x)
np.cumsum(x, axis=0)

   out[60]:
matrix([[ 1,  2,  3],
        [ 5,  7,  9],
        [12, 15, 18]])

   in [61]:
np.cumsum(x, axis=1)

   out[61]:
matrix([[ 1,  3,  6],
        [ 4,  9, 15],
        [ 7, 15, 24]])

cumulative products[82]  

     * similarly we can compute $y_n = \pi_{i=1}^{n} x_i$ using cumprod():

   in [62]:
import numpy as np
x = np.array([1, 2, 3, 4, 5])
np.cumprod(x)

   out[62]:
array([  1,   2,   6,  24, 120])

     * we can compute cummulative products along rows and columns using
       the axis parameter, just as with the cumsum() example.

generating (pseudo) random numbers[83]  

     * the nested module numpy.random contains functions for generating
       random numbers from different id203 distributions.

   in [63]:
from numpy.random import normal, uniform, exponential, randint

     * suppose that we have a random variable $\epsilon \sim n(0, 1)$.
     * in python we can draw from this distribution like so:

   in [64]:
epsilon = normal()
print epsilon

-0.906592753436

     * if we execute another call to the function, we will make a new draw
       from the distribution:

   in [65]:
epsilon = normal()
print epsilon

-0.0841335075421

pseudo-random numbers[84]  

     * strictly speaking, these are not random numbers.
     * they rely on an initial state value called the seed.
     * if we know the seed, then we can predict with total accuracy the
       rest of the sequence, given any "random" number.
     * nevertheless, statistically they behave like independently and
       identically-distributed values.
          + statistical tests for correlation and auto-correlation give
            insignificant results.
     * for this reason they called pseudo-random numbers.
     * the algorithms for generating them are called pseudo-random number
       generators (prngs).
     * some applications, such as cryptography, require genuinely
       unpredictable sequences.
          + never use a standard prng for these applications!

managing seed values[85]  

     * in some applications we need to reliably reproduce the same
       sequence of pseudo-random numbers that were used.
     * we can specify the seed value at the beginning of execution to
       achieve this.
     * use the function seed() in the numpy.random module.

setting the seed[86]  

   in [66]:
from numpy.random import seed

seed(5)

   in [67]:
normal()

   out[67]:
0.44122748688504143

   in [68]:
normal()

   out[68]:
-0.33087015189408764

   in [69]:
seed(5)

   in [70]:
normal()

   out[70]:
0.44122748688504143

   in [71]:
normal()

   out[71]:
-0.33087015189408764

drawing multiple variates[87]  

     * to generate more than number, we can specify the size parameter:

   in [72]:
normal(size=10)

   out[72]:
array([ 2.43077119, -0.25209213,  0.10960984,  1.58248112, -0.9092324 ,
       -0.59163666,  0.18760323, -0.32986996, -1.19276461, -0.20487651])

     * if you are generating very many variates, this will be much faster
       than using a for loop

     * we can also specify more than one dimension:

   in [73]:
normal(size=(5,5))

   out[73]:
array([[-0.35882895,  0.6034716 , -1.66478853, -0.70017904,  1.15139101],
       [ 1.85733101, -1.51117956,  0.64484751, -0.98060789, -0.85685315],
       [-0.87187918, -0.42250793,  0.99643983,  0.71242127,  0.05914424],
       [-0.36331088,  0.00328884, -0.10593044,  0.79305332, -0.63157163],
       [-0.00619491, -0.10106761, -0.05230815,  0.24921766,  0.19766009]])

histograms[88]  

     * we can plot a histograms of randomly-distributed data using the
       hist() function from matplotlib:

   in [74]:
import matplotlib.pyplot as plt
%matplotlib inline

data = normal(size=10000)
ax = plt.hist(data)

   [lz89xs0uj0
   s5nqa2vtksnh9plll8dek7t27xk6runt27dpgrkzysnj0cqvkxf0v0js2542l1lubqagwnb
   n9gca
   2uhya4abchsamabhdwagiowbwacepqayglahaamq9gbggp8hxohmodl9iksaaaaasuvork5
   cyii= ]

computing histograms as matrices[89]  

     * the function histogram() in the numpy module will count frequencies
       into bins and return the result as a 2-dimensional array.

   in [75]:
import numpy as np
np.histogram(data)

   out[75]:
(array([  23,  136,  618, 1597, 2626, 2635, 1620,  599,  130,   16]),
 array([-3.59780883, -2.87679609, -2.15578336, -1.43477063, -0.71375789,
        0.00725484,  0.72826758,  1.44928031,  2.17029304,  2.89130578,
        3.61231851]))

summary statistics[90]  

     * we can compute the summary statistics of a sample of values using
       the numpy functions mean() and var() to compute the sample mean
       $\bar{x}$ and sample [91]variance $\sigma_{x}^2$ .

   in [76]:
np.mean(data)

   out[76]:
-0.00045461080333495795

   in [77]:
np.var(data)

   out[77]:
1.0016048722546327

     * these functions also have an axis parameter to compute mean and
       variances of columns or rows of a multi-dimensional data-set.

summary statistics with nan values[92]  

     * if the data contains nan values, then the summary statistics will
       also be nan.

   in [78]:
from numpy import nan
import numpy as np
data = np.array([1, 2, 3, 4, nan])
np.mean(data)

   out[78]:
nan

     * to omit nan values from the calculation, use the functions
       nanmean() and nanvar():

   in [79]:
np.nanmean(data)

   out[79]:
2.5

discrete random numbers[93]  

     * the randint() function in numpy.random can be used to draw from a
       uniform discrete id203 distribution.
     * it takes two parameters: the low value (inclusive), and the high
       value (exclusive).
     * so to simulate one roll of a die, we would use the following python
       code.

   in [80]:
die_roll = randint(0, 6) + 1
die_roll

   out[80]:
4

     * just as with the normal() function, we can generate an entire
       sequence of values.
     * to simulate a [94]bernoulli process with $n=20$ trials:

   in [81]:
bernoulli_trials = randint(0, 2, size = 20)
bernoulli_trials

   out[81]:
array([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0])

acknowledgements[95]  

   the earlier sections of this notebook were adapted from [96]an article
   on floating-point numbers written by [97]steve hollasch.

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [98]fastly, rendered by [99]rackspace

   nbviewer github [100]repository.

   nbviewer version: [101]33c4683

   nbconvert version: [102]5.4.0

   rendered (fri, 05 apr 2019 17:43:30 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/slides/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb
   5. https://nbviewer.jupyter.org/format/script/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb
   6. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb
   7. https://github.com/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb
   8. https://mybinder.org/v2/gh/phelps-sg/python-bigdata/master?filepath=src/main/ipynb/numerical-slides.ipynb
   9. https://raw.githubusercontent.com/phelps-sg/python-bigdata/master/src/main/ipynb/numerical-slides.ipynb
  10. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/tree/master
  11. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/tree/master/src
  12. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/tree/master/src/main
  13. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/tree/master/src/main/ipynb
  14. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#numerical-computing-in-python
  15. http://sphelps.net/
  16. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#overview
  17. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#representing-continuous-values
  18. http://steve.hollasch.net/cgindex/coding/ieeefloat.html
  19. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#fixed-point-verses-floating-point
  20. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#scientific-notation
  21. https://en.wikipedia.org/wiki/scientific_notation
  22. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#scientific-notation-in-python
  23. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#floating-point-representation
  24. https://en.wikipedia.org/wiki/two's_complement
  25. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#bias
  26. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#double-and-single-precision-formats
  27. http://docs.scipy.org/doc/numpy/user/basics.types.html
  28. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#loss-of-precision
  29. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#ranges-of-floating-point-values
  30. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#effective-floating-point-range
  31. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#-
  32. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#representing-zero
  33. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#zero-in-python
  34. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#infinity
  35. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#infinity-in-python
  36. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#negative-infinity-in-python
  37. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#not-a-number-(nan)
  38. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#nan-in-python
  39. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#comparing-nan-values-in-python
  40. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#nan-is-not-the-same-as-none
  41. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#numerical-methods
  42. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#numerical-stability
  43. https://en.wikipedia.org/wiki/numerical_analysis
  44. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#catastrophic-cancellation
  45. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#relative-and-absolute-error
  46. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#catastrophic-cancellation-in-python
  47. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#importing-numpy
  48. http://www.numpy.org/
  49. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#arrays
  50. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#arrays-in-numpy
  51. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#array-indexing
  52. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#arrays-are-not-lists
  53. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#functions-over-arrays
  54. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#vectorized-functions
  55. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#vectorize-example
  56. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#populating-arrays
  57. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#basic-plotting
  58. http://uk.mathworks.com/help/matlab/ref/plot.html
  59. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#plotting-a-sine-curve
  60. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#multi-dimensional-data
  61. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#arrays-containing-arrays
  62. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#matrices
  63. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#plotting-multi-dimensional-with-matrices
  64. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#performance
  65. http://www.netlib.org/blas/
  66. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#matrix-operators
  67. http://mathworld.wolfram.com/matrixtranspose.html
  68. http://mathworld.wolfram.com/matrixinverse.html
  69. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#matrix-dimensions
  70. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#creating-matrices-from-strings
  71. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#matrix-multiplication
  72. https://en.wikipedia.org/wiki/matrix_multiplication
  73. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#matrix-indexing
  74. http://docs.scipy.org/doc/numpy/user/basics.indexing.html
  75. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#slices-are-references
  76. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#copying-matrices-and-vectors
  77. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#sums
  78. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#efficient-sums
  79. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#summing-rows-and-columns
  80. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#cumulative-sums
  81. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#cumulative-sums-along-rows-and-columns
  82. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#cumulative-products
  83. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#generating-(pseudo)-random-numbers
  84. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#pseudo-random-numbers
  85. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#managing-seed-values
  86. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#setting-the-seed
  87. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#drawing-multiple-variates
  88. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#histograms
  89. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#computing-histograms-as-matrices
  90. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#summary-statistics
  91. https://en.wikipedia.org/wiki/variance
  92. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#summary-statistics-with-nan-values
  93. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#discrete-random-numbers
  94. https://en.wikipedia.org/wiki/bernoulli_process
  95. https://nbviewer.jupyter.org/github/phelps-sg/python-bigdata/blob/master/src/main/ipynb/numerical-slides.ipynb#acknowledgements
  96. http://steve.hollasch.net/cgindex/coding/ieeefloat.html
  97. http://steve.hollasch.net/
  98. http://www.fastly.com/
  99. https://developer.rackspace.com/?nbviewer=awesome
 100. https://github.com/jupyter/nbviewer
 101. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
 102. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
