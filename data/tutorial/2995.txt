   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as slides
     * [5]view as code
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]pymc3_talk
    2. [10]bayesian_pymc3.ipynb

                      bayesian data analysis with pymc3

                                thomas wiecki

                               quantopian inc.

about me[11]  

     * phd candidate at brown studying decision making using bayesian
       modeling.
     * quantitative researcher at [12]quantopian inc: building the world's
       first algorithmic trading platform in the web browser.

why should you care about bayesian data analysis?[13]  

   in [1]:
from ipython.display import image
import prettyplotlib as ppl
from prettyplotlib import plt
import numpy as np
from matplotlib import rc
rc('font',**{'family':'sans-serif','sans-serif':['helvetica'], 'size': 22})
rc('xtick', labelsize=14)
rc('ytick', labelsize=14)
## for palatino and other serif fonts use:
#rc('font',**{'family':'serif','serif':['palatino']})
rc('text', usetex=true)
%matplotlib inline

   in [2]:
image('backbox_ml.png')

   out[2]:
   [png;base64,ymfja2jvef9tbc5wbmc= ]
     * blackbox models not good at conveying what they have learned.

   in [4]:
image('openbox_pp.png')

   out[4]:
   [png;base64,b3blbmjvef9wcc5wbmc= ]

probabilistic programming[14]  

     * model unknown causes of a phenomenon as random variables.
     * write a programmatic story of how unknown causes result in
       observable data.
     * use bayes formula to invert generative model to infer unknown
       causes.

random variables as id203 distributions[15]  

     * represents our beliefs about an unknown state.
     * id203 distribution assigns a id203 to each possible
       state.
     * not a single number (e.g. most likely state).

coin-flipping experiment.[16]  

     * given multiple flips, what is id203 of getting heads?
     * maximum likelihood answer:

   $$\frac{\# \text{heads}}{\text{total throws}}$$
     * however:

   $$\frac{50}{100} = \frac{1}{2}$$
     * clearly something is missing!
     * quantification of uncertainty.

   moreover...
     * consider a single flip which comes up heads:

   $$ p(\text{heads}) = \frac{1}{1} = 1 $$
     * again, this doesn't seem right.
     * incorporate prior knowledge.

   in [2]:
from scipy import stats
# set every possibility to be equally possible
x_coin = np.linspace(0, 1, 101)

$$ \text{express id203 of heads as random variable } \theta$$[17]  

   in [4]:
import prettyplotlib as ppl
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, xlabel=r'hypothesis for chance of heads',
            ylabel=r'id203 of hypothesis',
            title=r'prior id203 distribution after no coin tosses')
ppl.plot(ax, x_coin, stats.beta(2, 2).pdf(x_coin), linewidth=3.)
ax.set_xticklabels([r'0\%', r'20\%', r'40\%', r'60\%', r'80\%', r'100\%']);
# fig.savefig('coin1.png')

   out[4]:
[<matplotlib.text.text at 0x48432d0>,
 <matplotlib.text.text at 0x4841210>,
 <matplotlib.text.text at 0x486e590>,
 <matplotlib.text.text at 0x486ec50>,
 <matplotlib.text.text at 0x4872350>,
 <matplotlib.text.text at 0x4872a10>]

/home/wiecki/envs/pymc3/local/lib/python2.7/site-packages/ipython/core/formatter
s.py:239: formatterwarning: exception in image/png formatter: latex was not able
 to process the following string:
'0\\%'
here is the full report generated by latex:

this is pdftex, version 3.1415926-2.5-1.40.14 (tex live 2013/debian)
 restricted \write18 enabled.
entering extended mode
(./73e233a9842fdd7f7961a32eb5d9efbb.tex
latex2e <2011/06/27>
babel <3.9h> and hyphenation patterns for 2 languages loaded.
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
document class: article 2007/10/19 v1.4h standard latex document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))
(/usr/share/texlive/texmf-dist/tex/latex/type1cm/type1cm.sty)
(/usr/share/texlive/texmf-dist/tex/latex/psnfss/helvet.sty
(/usr/share/texlive/texmf-dist/tex/latex/graphics/keyval.sty))
(/usr/share/texlive/texmf-dist/tex/latex/psnfss/courier.sty)
(/usr/share/texlive/texmf-dist/tex/latex/base/textcomp.sty
(/usr/share/texlive/texmf-dist/tex/latex/base/ts1enc.def))
(/usr/share/texlive/texmf-dist/tex/latex/geometry/geometry.sty
(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifpdf.sty)
(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifvtex.sty)
(/usr/share/texlive/texmf-dist/tex/generic/ifxetex/ifxetex.sty)

package geometry warning: over-specification in `h'-direction.
    `width' (5058.9pt) is ignored.


package geometry warning: over-specification in `v'-direction.
    `height' (5058.9pt) is ignored.

)
no file 73e233a9842fdd7f7961a32eb5d9efbb.aux.
(/usr/share/texlive/texmf-dist/tex/latex/base/ts1cmr.fd)
(/usr/share/texlive/texmf-dist/tex/latex/psnfss/ot1pnc.fd)
! font ot1/pnc/m/n/10=pncr7t at 10.0pt not loadable: metric (tfm) file not foun
d.
<to be read again>
                   relax
l.11 \begin{document}

*geometry* driver: auto-detecting
*geometry* detected driver: dvips
(/usr/share/texlive/texmf-dist/tex/latex/psnfss/ot1phv.fd)
! font ot1/phv/m/n/14=phvr7t at 14.0pt not loadable: metric (tfm) file not foun
d.
<to be read again>
                   relax
l.12 \fontsize{14.000000}{17.500000}{\sffamily
                                               0\%}
! font ot1/pnc/m/n/14=pncr7t at 14.0pt not loadable: metric (tfm) file not foun
d.
<to be read again>
                   relax
l.13 \end{document}

[1] (./73e233a9842fdd7f7961a32eb5d9efbb.aux) )
(see the transcript file for additional information)
output written on 73e233a9842fdd7f7961a32eb5d9efbb.dvi (1 page, 188 bytes).
transcript written on 73e233a9842fdd7f7961a32eb5d9efbb.log.

  formatterwarning,

<matplotlib.figure.figure at 0x41dd610>

   in [98]:
stats.beta(2,2).rvs(1)

   out[98]:
array([ 0.37833632])

   in [21]:
import random
import numpy as np

def choose_coin():
    return stats.beta(2, 2).rvs(1) # random.uniform(0,1) # np.random.normal(0,1)
 #
# pylab.hist([choose_coin() for dummy in range(100000)], normed=true, bins=100)
successes = []
returns = 0.
for i in range(10000):
    prob_heads = choose_coin()
    results = [random.uniform(0,1) < prob_heads for dummy in range(10)]
    if results.count(true) == 9:
        successes.append(prob_heads)
        if random.uniform(0,1) < prob_heads:
            returns += -1
        else:
            returns += 1
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111)
r = ax.hist(np.array(successes), normed=true, bins=20)
print len(successes)
print "average return", returns / len(successes)

719
average return -0.613351877608

   [wh3kuejfaid48gaaaabjru5erkjggg== ]
   $$ \theta \sim \text{beta}(2, 2) $$$$ p(\theta) = \text{beta}(2, 2) $$
   in [108]:
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, xlabel='hypothesis for chance of heads',
            ylabel='id203 of hypothesis',
            title='posterior id203 distribution after first heads')
ppl.plot(ax, x_coin, stats.beta(1, 1).pdf(x_coin)*stats.beta(90, 10).pdf(x_coin)
, linewidth=3.)
ax.set_xticklabels([r'0\%', r'20\%', r'40\%', r'60\%', r'80\%', r'100\%']);
plt.savefig('coin2.png')

   [ aykb3fgir6gmaaaaaelftksuqmcc ]
   in [ ]:


   $$ p(\theta | h=1) = \text{beta}(3, 2) $$
   in [10]:
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, xlabel='hypothesis for chance of heads',
            ylabel='id203 of hypothesis',
            title='posterior id203 distribution after 1 head, 1 tail')
ppl.plot(ax, x_coin, stats.beta(3, 3).pdf(x_coin), linewidth=3.)
ax.set_xticklabels(['0\%', '20\%', '40\%', '60\%', '80\%', '100\%']);
fig.savefig('coin3.png')

   [wge7im8tw
   p23bylmwmkypiilgcyyu0rt1dnanoglwh5xzdmmqoeep766waxeliwaabxnyeaakjggaqme
   eaqao mcaaaautbacgyiiaabtsczmbppxiidptaaaaaelftksuqmcc ]
   in [11]:
image('coin3.png')

   out[11]:
   [a8ozqjm9zpusqaaaabj ru5erkjggg== ]
   $$ p(\theta | [h=1, t=1]) = \text{beta}(3, 3) $$
   in [12]:
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, xlabel='hypothesis for chance of heads',
            ylabel='id203 of hypothesis',
            title='posterior id203 distribution after 20 heads and 20 tail
s')
ppl.plot(ax, x_coin, stats.beta(22, 22).pdf(x_coin), linewidth=3.)
ax.set_xticklabels(['0\%', '20\%', '40\%', '60\%', '80\%', '100\%']);
fig.savefig('coin4.png')

   [wnjzwk5n7hdsaaaaabjru5erkjggg== ]
   in [13]:
image('coin4.png')

   out[13]:
   [bxkfbffp4+5i aaaaaelftksuqmcc ]
   $$ p(\theta | [h=20, t=20]) = \text{beta}(22, 22) $$

bayes formula[18]  

   $$p(\theta| \text{data}) \propto p(\theta) \ast p(\text{data}
   |\theta)$$$$\text{posterior} \propto \text{prior} \ast
   \text{likelihood}$$

   $\theta$: parameters of model (chance of getting heads)).
     * except in simple cases, posterior impossible to compute
       analytically.
     * blackbox approximation algorithm: markov-chain monte carlo (mcmc)
       instead draws samples from the posterior.

   in [14]:
from scipy import stats
fig = ppl.plt.figure(figsize=(14, 6))
ax1 = fig.add_subplot(121, title='what we want', ylim=(0, .5), xlabel=r'\theta',
 ylabel=r'p(\theta)')
ppl.plot(ax1, np.linspace(-4, 4, 100), stats.norm.pdf(np.linspace(-3, 3, 100)),
linewidth=4.)
ax2 = fig.add_subplot(122, title='what we get', xlim=(-4, 4), ylim=(0, 1800), xl
abel=r'\theta', ylabel='\# of samples')
ax2.hist(np.random.randn(10000), bins=20);

   [bjwlpddrtdbcahyqscuuiequcaw3evnnp
   caaayecegh9rdaeaaadwjabjaqaaapaliieaaaaavkqxbaaaamcxkiyaaaaa+blfeaaaaab
   fohgc aaaa4esuqwaaaab86f8ahnmhk0jb0jgaaaaasuvork5cyii= ]
   in [14]:


approximating the posterior with mcmc sampling[19]  

   in [15]:
image('wantget.png')

   out[15]:
   [w8hyq62iz13mgaaaabjru5e rkjggg== ]

pymc3[20]  

     * probabilistic programming framework written in python.
     * allows for construction of probabilistic models using intuitive
       syntax.
     * features advanced mcmc samplers.
     * fast: just-in-time compiled by theano.
     * extensible: easily incorporates custom mcmc algorithms and unusual
       id203 distributions.

linear models[21]  

     * assumes a linear relationship between two variables.
     * e.g. stock price between gold and gold miners.

   in [16]:
size = 200
true_intercept = 1
true_slope = 2

x = np.linspace(0, 1, size)
# y = a + b*x
true_regression_line = true_intercept + true_slope * x
# add noise
y = true_regression_line + np.random.normal(scale=.5, size=size)

data = dict(x=x, y=y)

   in [17]:
fig = plt.figure(figsize=(7, 7))
ax = fig.add_subplot(111, xlabel='value of gold', ylabel='value of gold miners',
 title='synthetic data and underlying model')
ppl.scatter(ax, x, y, label='sampled data')
ppl.plot(ax, x, true_regression_line, label='true regression line', linewidth=4.
)
ax.legend(loc=2);
fig.savefig('synth_data.png')

   [pckiekg4sgejtcibcys+jlzjkzbv8fxoj1iieohbciamncgcgeeevlqlai
   iutrkhauqghrtcqehrbcfc0jqsgeeevlqlaiiutr+v8bswb0oygnj64aaaaasuvork5cyii
   = ]
   in [18]:
image('synth_data.png')

   out[18]:
   [p6hcr942k10zaaaaaelftksuqmcc ]

id75[22]  

   $$ y_i = \alpha + \beta \ast x_i + \epsilon $$

   with $$ \epsilon \sim \mathcal{n}(0, \sigma^2) $$

probabilistic reformulation[23]  

   $$ y_i \sim \mathcal{n}(\alpha + \beta \ast x_i, \sigma^2) $$

priors[24]  

   $$ \alpha \sim \mathcal{n}(0, 20^2) $$$$ \beta \sim \mathcal{n}(0,
   20^2) $$$$ \sigma \sim \mathcal{u}(0, 20) $$

constructing model in pymc3[25]  

   in [19]:
import pymc as pm

with pm.model() as model: # model specifications in pymc3 are wrapped in a with-
statement
    # define priors
    alpha = pm.normal('alpha', mu=0, sd=20)
    beta = pm.normal('beta', mu=0, sd=20)
    sigma = pm.uniform('sigma', lower=0, upper=20)

    # define id75
    y_est = alpha + beta * x

    # define likelihood
    likelihood = pm.normal('y', mu=y_est, sd=sigma, observed=y)

    # id136!
    start = pm.find_map() # find starting value by optimization
    step = pm.nuts(state=start) # instantiate mcmc sampling algorithm
    trace = pm.sample(2000, step, start=start, progressbar=false) # draw 2000 po
sterior samples using nuts sampling

/users/alexcoventry/library/enthought/canopy_64bit/user/lib/python2.7/site-packa
ges/theano/scan_module/scan_perform_ext.py:85: runtimewarning: numpy.ndarray siz
e changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *

covenience function glm()[26]  

   in [20]:
with pm.model() as model:
    # specify glm and pass in data. the resulting linear model, its likelihood a
nd
    # and all its parameters are automatically added to our model.
    pm.glm.glm('y ~ x', data)
    step = pm.nuts() # instantiate mcmc sampling algorithm
    trace = pm.sample(2000, step, progressbar=false) # draw 2000 posterior sampl
es using nuts sampling

posterior[27]  

   in [21]:
fig = pm.traceplot(trace, lines={'alpha': 1, 'beta': 2, 'sigma': .5});

   [wmelrx0a6olugaaaabjru5erkjggg== ]
   in [22]:
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, xlabel='value of gold', ylabel='value of gold miners',
 title='posterior predictive regression lines')
ppl.scatter(ax, x, y, label='data')
from pymc import glm
glm.plot_posterior_predictive(trace, samples=100,
                              label='posterior predictive regression lines')
ppl.plot(ax, x, true_regression_line, label='true regression line', linewidth=5.
)
ax.legend(loc=0);
fig.savefig('ppc1.png')

   [cdp5qblt9ioaaaaasuvork5c yii= ]
   in [22]:


   in [23]:
image('ppc1.png')

   out[23]:
   [xjgqgbgdcjaa8ay4pf4bg0q3lhxozizmwnxeacmygroaaaawaigeqiaaaawekabaaaacwj
   q
   aaaagaueaaaaamacajqaaabgaqeaaaaasiaadqaaafhagayaaaasieadaaaafhcgaqaaaas
   i0aaa
   aiafbggaaadaagi0aaaayaebggaaalcaaa0aaabyqiagaaaalcbaawaaabyqoaeaaaalcna
   aaaca
   bqroaaaawaicnaaaagabaroaaacwgaanaaaawecabgaaacwgqamaaaawekabaaaacwjqaaa
   agaue
   aaaaamacajqaaabgaqeaaaaasiaadqaaafhagayaaaasieadaaaafhcgaqaaaasi0aaaaia
   fbgga
   aadaagi0aaaayaebggaaalcaaa0aaabyqiagaaaalcbaawaaabyqoaeaaaalcnaaaacabqr
   oaaaa
   waicnaaaagabaroaaacwgaanaaaawecabgaaacwgqamaaaawekabaaaacwjqaaaagaueaaa
   aamac
   ajqaaabgaqeaaaaasiaadqaaafhagayaaaasieadaaaafhcgaqaaaav+f0xhnsawxixqaaa
   aaelf tksuqmcc ]

robust regression[28]  

   in [24]:
# add outliers
x_out = np.append(x, [.1, .15, .2, .25, .25])
y_out = np.append(y, [8, 6, 9, 7, 9])

data_out = dict(x=x_out, y=y_out)
fig = plt.figure(figsize=(7, 7))
ax = fig.add_subplot(111,  xlabel='value of gold', ylabel='value of gold miners'
, title='posterior predictive regression lines')
ppl.scatter(ax, x_out, y_out, label='data')

   out[24]:
<matplotlib.axes.axessubplot at 0x118ba0150>

   [aaaaaelftksuqmcc ]
   in [25]:
with pm.model() as model:
    glm.glm('y ~ x', data_out)
    trace = pm.sample(2000, pm.nuts(), progressbar=false)

   in [26]:
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111,  xlabel='value of gold', ylabel='value of gold miners'
, title='posterior predictive regression lines')
ppl.scatter(ax, x_out, y_out, label='data')
glm.plot_posterior_predictive(trace, samples=100,
                              label='posterior predictive regression lines')
ppl.plot(ax, x, true_regression_line,
         label='true regression line', linewidth=5.)

plt.legend(loc=0);
fig.savefig('ppc2.png')

   [+eeswwvgnykkauacaofa
   zkkaa9euqbkymnknarbrediaaaasiiudaaaafkriawaascbcggaagaur0gaaacyikayaagb
   bhdqa aaal+v9qvdnrekjqraaaaabjru5erkjggg== ]
   in [27]:
image('ppc2.png')

   out[27]:
   [nwxcpjkaaaaasuvork5c yii= ]
   in [28]:
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111)
normal_dist = pm.normal.dist(mu=0, sd=1)
t_dist = pm.t.dist(mu=0, lam=1, nu=1)
x_eval = np.linspace(-8, 8, 300)
ppl.plot(ax, x_eval, pm.theano.tensor.exp(normal_dist.logp(x_eval)).eval(), labe
l='normal', linewidth=2.)
ppl.plot(ax, x_eval, pm.theano.tensor.exp(t_dist.logp(x_eval)).eval(), label='st
udent t', linewidth=2.)
plt.xlabel('x')
plt.ylabel('id203 density')
plt.legend();
fig.savefig('t-dist.png')

   [fi
   2lfjeaqbfotf7iorqmzapxpcid35ey9sjj4qqqjhggz4qqgjhgav6qgghhgmu6akhhbcouaa
   nhbbc oeabnhbccoeybxpcid35eyxtoid19eei79d2xe5jisttbyaaaaaelftksuqmcc ]

fit strongly biased by outliers[29]  

     * normal distribution has very light tails.
     * -> sensitive to outliers.
     * instead, use student t distribution with heavier tails.

   in [29]:
image('t-dist.png')

   out[29]:
   [wep2r2xavb7mgaaaabj ru5erkjggg== ]
   in [30]:
with pm.model() as model_robust:
    family = pm.glm.families.t()
    pm.glm.glm('y ~ x', data_out, family=family)

    trace_robust = pm.sample(2000, pm.nuts(), progressbar=false)

   in [31]:
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, xlabel='value of gold', ylabel='value of gold miners',
 title='posterior predictive regression lines')
ppl.scatter(ax, x_out, y_out)
glm.plot_posterior_predictive(trace_robust, samples=100,
                              label='posterior predictive regression lines')
ppl.plot(ax, x, true_regression_line,
         label='true regression line', linewidth=5.)
plt.legend();
fig.savefig('ppc3.png')

   [8v8kguqwqkgaaaaasuvork5cyii= ]
   in [32]:
image('ppc3.png')

   out[32]:
   [wc9yktfhagr5qaaaabjru5e rkjggg== ]

real-world example: algorithmic trading[30]  

     * pairtrading is a famous technique that plays two stocks against
       each other.
     * for this to work, stocks must be correlated (cointegrated).
     * one common example is the price of gold (gld) and the price of gold
       mining operations (gdx).

   in [33]:
import zipline
import pytz
from datetime import datetime
fig = plt.figure(figsize=(8, 4))

prices = zipline.data.load_from_yahoo(stocks=['gld', 'gdx'],
                                 end=datetime(2013, 8, 1, 0, 0, 0, 0, pytz.utc))
.dropna()[:1000]
prices.plot();

gld
gdx

<matplotlib.figure.figure at 0x123e0d210>

   [bzgs2+jyrif+aaaaaelftksuqmcc ]
   in [34]:
fig = plt.figure(figsize=(9, 6))
ax = fig.add_subplot(111, xlabel='price gdx in \$', ylabel='price gld in \$')
colors = np.linspace(0.1, 1, len(prices))
mymap = plt.get_cmap("winter")
sc = ax.scatter(prices.gdx, prices.gld, c=colors, cmap=mymap, lw=0)
cb = plt.colorbar(sc)
cb.ax.set_yticklabels([str(p.date()) for p in prices[::len(prices)//10].index]);
fig.savefig('price_corr.png')

   [8av9x+ h49vh0yaaaaasuvork5cyii= ]
   in [35]:
image('price_corr.png')

   out[35]:
   [p+axer+vspuvr8vh5fcowi0omk6rx6woeaorfyorh4841a7u6s9ms+vr6lmvlfagedpbs
   wunnlvbgmkhylka+vr5t3vs6nvy8hpdlwroehnbgvqiavgmwm4bd6fz587py5cpefwmadiq
   craaa
   algwbheaaaaubigaaabwiuaeaacacweiaaaaxagqaqaa4ekacaaaabccraaaalgqiaiaamc
   fabea
   aaaubigaaabwiuaeaacacweiaaaaxagqaqaa4ekacaaaabccraaaalgqiaiaamcfabeaaaa
   ubiga
   aabwiuaeaacacweiaaaaxagqaqaa4ekacaaaabccraaaalj8p0x7mbupdq4yaaaaaelftks
   uqmcc ]

naive model assumes constant id75.[31]  

   in [36]:
with pm.model() as model_reg:
    family = pm.glm.families.normal()
    pm.glm.glm('gld ~ gdx', prices, family=family)
    trace_reg = pm.sample(2000, pm.nuts(), progressbar=false)

hm... kinda unsatisfying...[32]  

   in [37]:
fig = plt.figure(figsize=(9, 6))
ax = fig.add_subplot(111, xlabel='price gdx in \$', ylabel='price gld in \$',
            title='posterior predictive regression lines')
sc = ax.scatter(prices.gdx, prices.gld, c=colors, cmap=mymap, lw=0)
glm.plot_posterior_predictive(trace_reg, samples=100,
                              label='posterior predictive regression lines',
                              lm=lambda x, sample: sample['intercept'] + sample[
'gdx'] * x,
                              eval=np.linspace(prices.gdx.min(), prices.gdx.max(
), 100))
cb = plt.colorbar(sc)
cb.ax.set_yticklabels([str(p.date()) for p in prices[::len(prices)//10].index]);
ax.legend(loc=0);
fig.savefig('ppc4.png')

   [wfmttedydy7fwaaaabjru5erkjggg== ]
   in [38]:
image('ppc4.png')

   out[38]:
   [whtr52dizmx caaaaabjru5erkjggg== ]
     * clearly the regression between gdx and gld changes over time.
     * but it does so gradually.
     * can we build a model that allows for gradual changes in the
       coefficients?
     * yes!

improved model[33]  

     * assumes that intercept and slope follow a random walk.
     * at each time-point, the coefficients can move a step from their
       previous values.
     * this allows the coefficients to track the regression as it changes
       over time.

   $$ \alpha_t \sim \mathcal{n}(\alpha_{t-1}, \sigma_\alpha^2) $$$$
   \beta_t \sim \mathcal{n}(\beta_{t-1}, \sigma_\beta^2) $$
   in [39]:
from pymc.distributions.timeseries import *
from theano.tensor import repeat

$$\text{priors for }\sigma_{\alpha}\text{ and }\sigma_{\beta}$$[34]  

   in [40]:
model_randomwalk = pm.model()
with model_randomwalk:
    # std of random walk, best sampled in log space.
    sigma_alpha, log_sigma_alpha = model_randomwalk.transformedvar(
                            'sigma_alpha',
                            pm.exponential.dist(1./.02, testval = .1),
                            pm.logtransform
    )
    sigma_beta, log_sigma_beta = model_randomwalk.transformedvar(
                            'sigma_beta',
                            pm.exponential.dist(1./.02, testval = .1),
                            pm.logtransform
    )

define regression coefficients to follow a random walk.[35]  

   in [41]:
# to make the model simpler, we will apply the same coefficient for 50 data poin
ts at a time
subsample_alpha = 50
subsample_beta = 50

with model_randomwalk:
    alpha = gaussianrandomwalk('alpha', sigma_alpha**-2,
                               shape=len(prices) / subsample_alpha)
    beta = gaussianrandomwalk('beta', sigma_beta**-2,
                              shape=len(prices) / subsample_beta)

    # make coefficients have the same length as prices
    alpha_r = repeat(alpha, subsample_alpha)
    beta_r = repeat(beta, subsample_beta)

define regression and likelihood[36]  

   in [42]:
with model_randomwalk:
    # define regression
    regression = alpha_r + beta_r * prices.gdx.values

    # assume prices are normally distributed, the mean comes from the regression
.
    sd = pm.uniform('sd', 0, 20)
    likelihood = pm.normal('y',
                           mu=regression,
                           sd=sd,
                           observed=prices.gld.values)

id136![37]  

   in [43]:
from scipy import optimize
with model_randomwalk:
    # first optimize random walk
    start = pm.find_map(vars=[alpha, beta], fmin=optimize.fmin_l_bfgs_b)

    # sample
    step = pm.nuts(scaling=start)
    trace_rw = pm.sample(2000, step, start=start, progressbar=false)

intercept changes over time.[38]  

   in [44]:
fig = plt.figure(figsize=(8, 6))
ax = plt.subplot(111, xlabel='time', ylabel='alpha', title='change of alpha over
 time.')
ppl.plot(ax, trace_rw[-1000:][alpha].t, 'r', alpha=.05);
ax.set_xticklabels([str(p.date()) for p in prices[::len(prices)//5].index]);
fig.savefig('rwalk_alpha.png')

   [wo1wqregl4of8thljy99ragum6tisrk1qul2lxcxeidqca
   pad4eqaaaaagmpihaaaaaebkicgaaaaaebkicgaaaabeboicaaaaajgboaaaaabazcaoaaa
   aabaz caoaaaaaraacagaaaacr+t8opzahqamkzaaaaabjru5erkjggg== ]
   in [45]:
image('rwalk_alpha.png')

   out[45]:
   [fu3zpv1vvzxfwcs98jitcdroerqgghphtq
   akqiiysq0kebraghhjdsqqfeid35eknjbauqiiysq0kebraghhjdsqqfeid35eknjbauqiiys
   q0keb
   raghhjdsqqfeid35eknjbauqiiysq0kebraghhjdsqqfeid35eknjbauqiiysq0kebraghhjd
   sqqfe
   id35eknjbauqiiysq0kebraghhjdsqqfeid35eknjbauqiiysq0kebraghhjdsqqfeid35eknj
   bauqi iysq0kebraghhjdsqqfeid35eknlxf3nasx2ptmjraaaaaelftksuqmcc ]

slope changes over time.[39]  

   in [46]:
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, xlabel='time', ylabel='beta', title='change of beta ov
er time')
ppl.plot(ax, trace_rw[-1000:][beta].t, 'b', alpha=.05);
ax.set_xticklabels([str(p.date()) for p in prices[::len(prices)//5].index]);
fig.savefig('rwalk_beta.png')

   [0kkqy+3quaaaa aelftksuqmcc ]
   in [47]:
image('rwalk_beta.png')

   out[47]:
   [wd7lkdmygd7yqaaaabjru5erkjggg== ]

regression slowly adapts to best fit current data[40]  

   in [48]:
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, xlabel='price gdx in \$', ylabel='price gld in \$',
            title='posterior predictive regression lines')

colors = np.linspace(0.1, 1, len(prices))
colors_sc = np.linspace(0.1, 1, len(trace_rw[-500::10]['alpha'].t))
mymap = plt.get_cmap('winter')
mymap_sc = plt.get_cmap('winter')

xi = np.linspace(prices.gdx.min(), prices.gdx.max(), 50)
for i, (alpha, beta) in enumerate(zip(trace_rw[-500::10]['alpha'].t, trace_rw[-5
00::10]['beta'].t)):
    for a, b in zip(alpha, beta):
        ax.plot(xi, a + b*xi, alpha=.05, lw=1, c=mymap_sc(colors_sc[i]))

sc = ax.scatter(prices.gdx, prices.gld, label='data', cmap=mymap, c=colors)
cb = plt.colorbar(sc)
cb.ax.set_yticklabels([str(p.date()) for p in prices[::len(prices)//10].index]);
fig.savefig('ppc5.png')

   [wpsvslv9ljiwaaaaabjru5erkjggg== ]
   in [49]:
image('ppc5.png')

   out[49]:
   [b9asco73ptnxaaaaabjru5erkjggg== ]

conclusions[41]  

     * probabilistic programming allows you to tell a genarative story.
     * blackbox id136 algorithms allow estimation of complex models.
     * pymc3 puts advanced samplers at your fingertips.

outstanding issues[42]  

   scalability
     * variational id136
     * see also max welling's work for scaling mcmc

   usability
     * still too difficult to use
     * wanted: library on top of pymc3 with common models

further reading[43]  

     * [44]quantopian -- develop trading algorithms like this in your
       browser.
     * [45]probilistic programming for hackers -- ipython notebook book on
       bayesian stats using pymc2.
     * [46]doing bayesian data analysis -- great book by kruschke.
     * [47]get pymc3 alpha
     * [48]my blog on all things bayesian
     * [49]ipython notebook underlying this talk
     * twitter: [50]@twiecki

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [51]fastly, rendered by [52]rackspace

   nbviewer github [53]repository.

   nbviewer version: [54]33c4683

   nbconvert version: [55]5.4.0

   rendered (fri, 05 apr 2019 18:20:47 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/slides/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb
   5. https://nbviewer.jupyter.org/format/script/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb
   6. https://github.com/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb
   7. https://mybinder.org/v2/gh/twiecki/pymc3_talk/master?filepath=bayesian_pymc3.ipynb
   8. https://raw.githubusercontent.com/twiecki/pymc3_talk/master/bayesian_pymc3.ipynb
   9. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/tree/master
  10. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/tree/master/bayesian_pymc3.ipynb
  11. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#about-me
  12. https://www.quantopian.com/
  13. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#why-should-you-care-about-bayesian-data-analysis?
  14. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#probabilistic-programming
  15. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#random-variables-as-id203-distributions
  16. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#coin-flipping-experiment.
  17. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#$$-\text{express-id203-of-heads-as-random-variable-}-\theta$$
  18. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#bayes-formula
  19. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#approximating-the-posterior-with-mcmc-sampling
  20. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#pymc3
  21. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#linear-models
  22. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#linear-regression
  23. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#probabilistic-reformulation
  24. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#priors
  25. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#constructing-model-in-pymc3
  26. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#covenience-function-glm()
  27. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#posterior
  28. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#robust-regression
  29. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#fit-strongly-biased-by-outliers
  30. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#real-world-example:-algorithmic-trading
  31. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#naive-model-assumes-constant-linear-regression.
  32. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#hm...-kinda-unsatisfying...
  33. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#improved-model
  34. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#$$\text{priors-for-}\sigma_{\alpha}\text{-and-}\sigma_{\beta}$$
  35. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#define-regression-coefficients-to-follow-a-random-walk.
  36. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#define-regression-and-likelihood
  37. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#id136!
  38. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#intercept-changes-over-time.
  39. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#slope-changes-over-time.
  40. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#regression-slowly-adapts-to-best-fit-current-data
  41. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#conclusions
  42. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#outstanding-issues
  43. https://nbviewer.jupyter.org/github/twiecki/pymc3_talk/blob/master/bayesian_pymc3.ipynb#further-reading
  44. https://www.quantopian.com/
  45. http://camdavidsonpilon.github.io/probabilistic-programming-and-bayesian-methods-for-hackers/
  46. http://www.indiana.edu/~kruschke/doingbayesiandataanalysis/
  47. https://github.com/pymc-devs/pymc/tree/pymc3
  48. https://twiecki.github.io/
  49. https://rawgithub.com/twiecki/pymc3_talk/master/bayesian_pymc3.ipynb
  50. https://twitter.com/twiecki
  51. http://www.fastly.com/
  52. https://developer.rackspace.com/?nbviewer=awesome
  53. https://github.com/jupyter/nbviewer
  54. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  55. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
