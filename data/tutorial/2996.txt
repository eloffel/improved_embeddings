   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 3 kernel
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]news-categorization-mnb
    2. [10]news categorization mnb.ipynb

news categorization using multinomial naive bayes[11]  

   the objective of this site is to show how to use multinomial naive
   bayes method to classify news according to some predefined classes.

   the news aggregator data set comes from the uci machine learning
   repository.
     * lichman, m. (2013). uci machine learning repository
       [[12]http://archive.ics.uci.edu/ml]. irvine, ca: university of
       california, school of information and computer science.

   this specific dataset can be found in the uci ml repository at this
   url: [13]http://archive.ics.uci.edu/ml/datasets/news+aggregator

   this dataset contains headlines, urls, and categories for 422,937 news
   stories collected by a web aggregator between march 10th, 2014 and
   august 10th, 2014. news categories in this dataset are labelled:
     * b: business;
     * t: science and technology;
     * e: entertainment; and
     * m: health.

   using multinomial naive bayes method, we will try to predict the
   category (business, entertainment, etc.) of a news article given only
   its headline.

   let's begin importing the pandas (python data analysis library) module.
   the import statement is the most common way to gain access to the code
   in another module.
   in [3]:
import pandas as pd

   this way we can refer to pandas by its alias 'pd'. let's import news
   aggregator data via pandas
   in [4]:
news = pd.read_csv("uci-news-aggregator.csv")

   function head gives us the first 5 items in a column (or the first 5
   rows in the dataframe)
   in [5]:
print(news.head())

   id                                              title  \
0   1  fed official says weak data caused by weather,...
1   2  fed's charles plosser sees high bar for change...
2   3  us open: stocks fall after fed official hints ...
3   4  fed risks falling 'behind the curve', charles ...
4   5  fed's plosser: nasty weather has curbed job gr...

                                                 url          publisher  \
0  http://www.latimes.com/business/money/la-fi-mo...  los angeles times
1  http://www.livemint.com/politics/h2evwjsk2ve6o...           livemint
2  http://www.ifamagazine.com/news/us-open-stocks...       ifa magazine
3  http://www.ifamagazine.com/news/fed-risks-fall...       ifa magazine
4  http://www.moneynews.com/economy/federal-reser...          moneynews

  category                          story             hostname      timestamp
0        b  dduyu0vzz0brnemioxupqvp6sixvm      www.latimes.com  1394470370698
1        b  dduyu0vzz0brnemioxupqvp6sixvm     www.livemint.com  1394470371207
2        b  dduyu0vzz0brnemioxupqvp6sixvm  www.ifamagazine.com  1394470371550
3        b  dduyu0vzz0brnemioxupqvp6sixvm  www.ifamagazine.com  1394470371793
4        b  dduyu0vzz0brnemioxupqvp6sixvm    www.moneynews.com  1394470372027

   we want to predict the category of a news article based only on its
   title. class labelencoder allows to encode labels with values between 0
   and n_classes-1.
   in [6]:
from sklearn.preprocessing import labelencoder

encoder = labelencoder()
y = encoder.fit_transform(news['category'])
print(y[:5])

[0 0 0 0 0]

   in [7]:
categories = news['category']
titles = news['title']
n = len(titles)
print('number of news',n)

number of news 422419

   in [8]:
labels = list(set(categories))
print('possible categories',labels)

possible categories ['t', 'm', 'e', 'b']

   in [9]:
for l in labels:
    print('number of ',l,' news',len(news.loc[news['category'] == l]))

number of  t  news 108344
number of  m  news 45639
number of  e  news 152469
number of  b  news 115967

   categories are literal labels, but it is better for machine learning
   algorithms just to work with numbers, so we will encode them using
   labelencoder, which encode labels with value between 0 and n_classes-1.
   in [10]:
from sklearn.preprocessing import labelencoder
encoder = labelencoder()
ncategories = encoder.fit_transform(categories)

   now we should split our data into two sets:
    1. a training set (70%) used to discover potentially predictive
       relationships, and
    2. a test set (30%) used to evaluate whether the discovered
       relationships hold and to assess the strength and utility of a
       predictive relationship.

   samples should be first shuffled and then split into a pair of train
   and test sets. make sure you permute (shuffle) your training data
   before fitting the model.
   in [11]:
ntrain = int(n * 0.7)
from sklearn.utils import shuffle
titles, ncategories = shuffle(titles, ncategories, random_state=0)

   in [12]:
x_train = titles[:ntrain]
print('x_train.shape',x_train.shape)
y_train = ncategories[:ntrain]
print('y_train.shape',y_train.shape)
x_test = titles[ntrain:]
print('x_test.shape',x_test.shape)
y_test = ncategories[ntrain:]
print('y_test.shape',y_test.shape)

x_train.shape (295693,)
y_train.shape (295693,)
x_test.shape (126726,)
y_test.shape (126726,)

   in order to make the training process easier, scikit-learn provides a
   pipeline class that behaves like a compound classifier. the first step
   should be to tokenize and count the number of occurrence of each word
   that appears into the news'titles. for that, we will use the
   countvectorizer class. then we will transform the counters to a tf-idf
   representation using tfidftransformer class. the last step creates the
   naive bayes classifier
   in [13]:
from sklearn.feature_extraction.text import countvectorizer
from sklearn.feature_extraction.text import tfidftransformer
from sklearn.naive_bayes import multinomialnb
from sklearn.pipeline import pipeline

   in [14]:
print('training...')

text_clf = pipeline([('vect', countvectorizer()),
                     ('tfidf', tfidftransformer()),
                     ('clf', multinomialnb()),
                     ])

training...

   now we procede to fit the naive bayes classifier to the train set
   in [15]:
text_clf = text_clf.fit(x_train, y_train)

   now we can procede to apply the classifier to the test set and
   calculate the predicted values
   in [16]:
print('predicting...')
predicted = text_clf.predict(x_test)

predicting...

   sklearn.metrics module includes score functions, performance metrics,
   and pairwise metrics and distance computations. accuracy_score:
   computes subset accuracy; used to compare set of predicted labels for a
   sample to the corresponding set of true labels
   in [17]:
from sklearn import metrics

print('accuracy_score',metrics.accuracy_score(y_test,predicted))
print('reporting...')

accuracy_score 0.92380411281
reporting...

   let's build a text report showing the main classification metrics with
   the precision/recall/f1-score measures for each element in the test
   data.
   in [18]:
print(metrics.classification_report(y_test, predicted, target_names=labels))

             precision    recall  f1-score   support

          t       0.90      0.91      0.90     34729
          m       0.95      0.97      0.96     45625
          e       0.97      0.85      0.90     13709
          b       0.90      0.90      0.90     32663

avg / total       0.92      0.92      0.92    126726


   have you heard about [14]cross-validation? what about k-fold
   cross-validation? you can try it now just by repeating the previous
   steps (don't forget the shuffle part) and averaging the results. let's
   try it! have a nice day!

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [15]fastly, rendered by [16]rackspace

   nbviewer github [17]repository.

   nbviewer version: [18]33c4683

   nbconvert version: [19]5.4.0

   rendered (fri, 05 apr 2019 18:20:50 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/andressotov/news-categorization-mnb/blob/master/news categorization mnb.ipynb
   5. https://nbviewer.jupyter.org/github/andressotov/news-categorization-mnb/blob/master/news categorization mnb.ipynb
   6. https://github.com/andressotov/news-categorization-mnb/blob/master/news categorization mnb.ipynb
   7. https://mybinder.org/v2/gh/andressotov/news-categorization-mnb/master?filepath=news categorization mnb.ipynb
   8. https://raw.githubusercontent.com/andressotov/news-categorization-mnb/master/news categorization mnb.ipynb
   9. https://nbviewer.jupyter.org/github/andressotov/news-categorization-mnb/tree/master
  10. https://nbviewer.jupyter.org/github/andressotov/news-categorization-mnb/tree/master/news categorization mnb.ipynb
  11. https://nbviewer.jupyter.org/github/andressotov/news-categorization-mnb/blob/master/news categorization mnb.ipynb#news-categorization-using-multinomial-naive-bayes
  12. http://archive.ics.uci.edu/ml
  13. http://archive.ics.uci.edu/ml/datasets/news+aggregator
  14. https://en.wikipedia.org/wiki/cross-validation_(statistics)
  15. http://www.fastly.com/
  16. https://developer.rackspace.com/?nbviewer=awesome
  17. https://github.com/jupyter/nbviewer
  18. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  19. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
