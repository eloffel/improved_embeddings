   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

machine learning is fun! part 2

using machine learning to generate super mario maker levels

   [9]go to the profile of adam geitgey
   [10]adam geitgey (button) blockedunblock (button) followfollowing
   jan 2, 2016

   update: this article is part of a series. check out the full series:
   [11]part 1, [12]part 2, [13]part 3, [14]part 4, [15]part 5, [16]part 6,
   [17]part 7 and [18]part 8! you can also read this article in
   [19]italiano, [20]espa  ol, [21]fran  ais, [22]t  rk  e, [23]              ,
   [24]          [25]portugu  s, [26]          , [27]ti   ng vi   t or [28]         .

   giant update: [29]i   ve written a new book based on these articles! it
   not only expands and updates all my articles, but it has tons of brand
   new content and lots of hands-on coding projects. [30]check it out now!

   in [31]part 1, we said that machine learning is using generic
   algorithms to tell you something interesting about your data without
   writing any code specific to the problem you are solving. (if you
   haven   t already read [32]part 1, read it now!).

   this time, we are going to see one of these generic algorithms do
   something really cool         create video game levels that look like they
   were made by humans. we   ll build a neural network, feed it existing
   super mario levels and watch new ones pop out!
   [1*pj2mc_79m544t9mg19xiga.gif]
   [33]one of the levels our algorithm will generate

   just like [34]part 1, this guide is for anyone who is curious about
   machine learning but has no idea where to start. the goal is be
   accessible to anyone         which means that there   s a lot of
   generalizations and we skip lots of details. but who cares? if this
   gets anyone more interested in ml, then mission accomplished.
     __________________________________________________________________

making smarter guesses

   back in [35]part 1, we created a simple algorithm that estimated the
   value of a house based on its attributes. given data about a house like
   this:
   [1*tzn3mzmngmjax59ggnspmq.png]

   we ended up with this simple estimation function:
def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):
 price = 0
# a little pinch of this
 price += num_of_bedrooms * 0.123
# and a big pinch of that
 price += sqft * 0.41
# maybe a handful of this
 price += neighborhood * 0.57
return price

   in other words, we estimated the value of the house by multiplying each
   of its attributes by a weight. then we just added those numbers up to
   get the house   s value.

   instead of using code, let   s represent that same function as a simple
   diagram:
   [1*lltiwe6h0l1aoln2glagcw.png]
   the arrows represent the weights in our function.

   however this algorithm only works for simple problems where the result
   has a linear relationship with the input. what if the truth behind
   house prices isn   t so simple? for example, maybe the neighborhood
   matters a lot for big houses and small houses but doesn   t matter at all
   for medium-sized houses. how could we capture that kind of complicated
   detail in our model?

   to be more clever, we could run this algorithm multiple times with
   different of weights that each capture different edge cases:
   [1*hoemqf_v42khlmyiqcqnyq.png]
   let   s try solving the problem four different ways

   now we have four different price estimates. let   s combine those four
   price estimates into one final estimate. we   ll run them through the
   same algorithm again (but using another set of weights)!
   [1*ves0zisjogcqthpyzh0tiq.png]

   our new super answer combines the estimates from our four different
   attempts to solve the problem. because of this, it can model more cases
   than we could capture in one simple model.

what is a neural network?

   let   s combine our four attempts to guess into one big diagram:
   [1*lt8rzaeq6f6b_ea1od32jq.png]

   this is a neural network! each node knows how to take in a set of
   inputs, apply weights to them, and calculate an output value. by
   chaining together lots of these nodes, we can model complex functions.

   there   s a lot that i   m skipping over to keep this brief (including
   [36]feature scaling and the [37]activation function), but the most
   important part is that these basic ideas click:
     * we made a simple estimation function that takes in a set of inputs
       and multiplies them by weights to get an output. call this simple
       function a neuron.
     * by chaining lots of simple neurons together, we can model functions
       that are too complicated to be modeled by one single neuron.

   it   s just like lego! we can   t model much with one single lego block,
   but we can model anything if we have enough basic lego blocks to stick
   together:
   [1*acwzhbgnmimyyv1_ixcy0w.png]
   a grim preview of our plastic animal future? only time can tell   

giving our neural network a memory

   the neural network we   ve seen always returns the same answer when you
   give it the same inputs. it has no memory. in programming terms, it   s a
   [38]stateless algorithm.

   in many cases (like estimating the price of house), that   s exactly what
   you want. but the one thing this kind of model can   t do is respond to
   patterns in data over time.

   imagine i handed you a keyboard and asked you to write a story. but
   before you start, my job is to guess the very first letter that you
   will type. what letter should i guess?

   i can use my knowledge of english to increase my odds of guessing the
   right letter. for example, you will probably type a letter that is
   common at the beginning of words. if i looked at stories you wrote in
   the past, i could narrow it down further based on the words you usually
   use at the beginning of your stories. once i had all that data, i could
   use it to build a neural network to model how likely it is that you
   would start with any given letter.

   our model might look like this:
   [1*e2ypupjvoizj4b_ngzi5pq.png]

   but let   s make the problem harder. let   s say i need to guess the next
   letter you are going to type at any point in your story. this is a much
   more interesting problem.

   let   s use the first few words of ernest hemingway   s [39]the sun also
   rises as an example:

     robert cohn was once middleweight boxi

   what letter is going to come next?

   you probably guessed    n            the word is probably going to be boxing. we
   know this based on the letters we   ve already seen in the sentence and
   our knowledge of common words in english. also, the word    middleweight   
   gives us an extra clue that we are talking about boxing.

   in other words, it   s easy to guess the next letter if we take into
   account the sequence of letters that came right before it and combine
   that with our knowledge of the rules of english.

   to solve this problem with a neural network, we need to add state to
   our model. each time we ask our neural network for an answer, we also
   save a set of our intermediate calculations and re-use them the next
   time as part of our input. that way, our model will adjust its
   predictions based on the input that it has seen recently.
   [1*dcdvatcm0yjqxr4jgg5bfa.png]

   keeping track of state in our model makes it possible to not just
   predict the most likely first letter in the story, but to predict the
   most likely next letter given all previous letters.

   this is the basic idea of a recurrent neural network. we are updating
   the network each time we use it. this allows it to update its
   predictions based on what it saw most recently. it can even model
   patterns over time as long as we give it enough of a memory.

what   s a single letter good for?

   predicting the next letter in a story might seem pretty useless. what   s
   the point?

   one cool use might be auto-predict for a mobile phone keyboard:
   [1*pmdvdfrt9au3ywniio0fwq.png]
   the next most likely letter is    t   .

   but what if we took this idea to the extreme? what if we asked the
   model to predict the next most likely character over and
   over         forever? we   d be asking it to write a complete story for us!

generating a story

   we saw how we could guess the next letter in hemingway   s sentence.
   let   s try generating a whole story in the style of hemingway.

   to do this, we are going to use the [40]recurrent neural network
   implementation that [41]andrej karpathy wrote. andrej is a
   deep-learning researcher at stanford and he wrote [42]an excellent
   introduction to generating text with id56s, you can view all the
   [43]code for the model on github.

   we   ll create our model from the complete text of the sun also
   rises         362,239 characters using 84 unique letters (including
   punctuation, uppercase/lowercase, etc). this data set is actually
   really small compared to typical real-world applications. to generate a
   really good model of hemingway   s style, it would be much better to have
   at several times as much sample text. but this is good enough to play
   around with as an example.

   as we just start to train the id56, it   s not very good at predicting
   letters. here   s what it generates after a 100 loops of training:

     hjctcnhoofeoxelif edelobe negnk e iohehasenoldndamdai ayio pe e h   e
     btentmuhgehi bcgdltt. gey heho grpiahe.
     ddelnss.eelaishaner    cot aafhb ht ltny
     ehbih a   on bhnte ectrsnae abeahngy
     amo k ns aeo?cdse nh a taei.rairrhelardr er deffijha

   you can see that it has figured out that sometimes words have spaces
   between them, but that   s about it.

   after about 1000 iterations, things are looking more promising:

     hing soor ither. and the caraos, and the crowebel for figttier and
     ale the room of me? streat was not to him bill-stook of the
     momansbed mig out ust on the bull, out here. i been soms
     inick stalling that aid.
         hon   t me and acrained on .hw   s don   t you for the roed,    in   s
     pair.   
         alough marith him.   

   the model has started to identify the patterns in basic sentence
   structure. it   s adding periods at the ends of sentences and even
   quoting dialog. a few words are recognizable, but there   s also still a
   lot of nonsense.

   but after several thousand more training iterations, it looks pretty
   good:

     he went over to the gate of the caf  . it was like a country bed.
         do you know it   s been me.   
         damned us,    bill said.
         i was dangerous,    i said.    you were she did it and think i would a
     fine cape you,    i said.
         i can   t look strange in the cab.   
         you know i was this is though,    brett said.
         it   s a fights no matter?   
         it makes to do it.   
         you make it?   
         sit down,    i said.    i wish i wasn   t do a little with the man.   
         you found it.   
         i don   t know.   
         you see, i   m sorry of chatches,    bill said.    you think it   s a
     friend off back and make you really drunk.   

   at this point, the algorithm has captured the basic pattern of
   hemingway   s short, direct dialog. a few sentences even sort of make
   sense.

   compare that with some real text from the book:

     there were a few people inside at the bar, and outside, alone, sat
     harvey stone. he had a pile of saucers in front of him, and he
     needed a shave.
         sit down,    said harvey,    i   ve been looking for you.   
         what   s the matter?   
         nothing. just looking for you.   
         been out to the races?   
         no. not since sunday.   
         what do you hear from the states?   
         nothing. absolutely nothing.   
         what   s the matter?   

   even by only looking for patterns one character at a time, our
   algorithm has reproduced plausible-looking prose with proper
   formatting. that is kind of amazing!

   we don   t have to generate text completely from scratch, either. we can
   seed the algorithm by supplying the first few letters and just let it
   find the next few letters.

   for fun, let   s make a fake book cover for our imaginary book by
   generating a new author name and a new title using the seed text of
      er   ,    he   , and    the s   :
   [1*asfo32mwrrw7lydvj0tghq.png]
   the real book is on the left and our silly computer-generated nonsense
   book is on the right.

   not bad!

   but the really mind-blowing part is that this algorithm can figure out
   patterns in any sequence of data. it can easily generate real-looking
   [44]recipes or [45]fake obama speeches. but why limit ourselves human
   language? we can apply this same idea to any kind of sequential data
   that has a pattern.

making mario without actually making mario

   in 2015, nintendo released [46]super mario maker    for the wii u gaming
   system.
   [1*f5j2m7usf53vf4stto0tsw.png]
   every kid   s dream!

   this game lets you draw out your own super mario brothers levels on the
   gamepad and then upload them to the internet so you friends can play
   through them. you can include all the classic power-ups and enemies
   from the original mario games in your levels. it   s like a virtual lego
   set for people who grew up playing super mario brothers.

   can we use the same model that generated fake hemingway text to
   generate fake super mario brothers levels?

   first, we need a data set for training our model. let   s take all the
   outdoor levels from the original super mario brothers game released in
   1985:
   [1*hurfjf6p7dzylpazsckgda.png]
   best christmas ever. thanks mom and dad!

   this game has 32 levels and about 70% of them have the same outdoor
   style. so we   ll stick to those.

   to get the designs for each level, i took an original copy of the game
   and wrote a program to pull the level designs out of the game   s memory.
   super mario bros. is a 30-year-old game and there are lots of resources
   online that help you figure out how the levels were stored in the
   game   s memory. extracting level data from an old video game is a fun
   programming exercise that you should try sometime.

   here   s the first level from the game (which you probably remember if
   you ever played it):
   [1*czddr0sxpvnr3ru9kx2fqq.gif]
   super mario bros. level 1   1

   if we look closely, we can see the level is made of a simple grid of
   objects:
   [1*c8wiw6uscluj2kje2rtxlq.png]

   we could just as easily represent this grid as a sequence of characters
   with one character representing each object:
--------------------------
--------------------------
--------------------------
#??#----------------------
--------------------------
--------------------------
--------------------------
-##------=--=----------==-
--------==--==--------===-
-------===--===------====-
------====--====----=====-
=========================-

   we   ve replaced each object in the level with a letter:
     *    -    is a blank space
     *    =    is a solid block
     *    #    is a breakable brick
     *    ?    is a coin block

      and so on, using a different letter for each different kind of object
   in the level.

   i ended up with text files that looked like this:
   [1*d9hjauyoby9xgpfoikczua.png]

   looking at the text file, you can see that mario levels don   t really
   have much of a pattern if you read them line-by-line:
   [1*3anjg6hi_igkk1gar8tqug.png]
   reading line-by-line, there   s not really a pattern to capture. lots of
   lines are completely blank.

   the patterns in a level really emerge when you think of the level as a
   series of columns:
   [1*y-awqi55np9ynxz-da73dq.png]
   looking column-by-column, there   s a real pattern. each column ends in a
      =    for example.

   so in order for the algorithm to find the patterns in our data, we need
   to feed the data in column-by-column. figuring out the most effective
   representation of your input data (called [47]feature selection) is one
   of the keys of using machine learning algorithms well.

   to train the model, i needed to rotate my text files by 90 degrees.
   this made sure the characters were fed into the model in an order where
   a pattern would more easily show up:
-----------=
-------#---=
-------#---=
-------?---=
-------#---=
-----------=
-----------=
----------@=
----------@=
-----------=
-----------=
-----------=
---------pp=
---------pp=
----------==
---------===
--------====
-------=====
------======
-----=======
---=========
---=========

training our model

   just like we saw when creating the model of hemingway   s prose, a model
   improves as we train it.

   after a little training, our model is generating junk:
--------------------------
ll+<&=------p-------------
--------
---------------------t--#--
-----
-=--=-=------------=-&--t--------------
--------------------
--=------$-=#-=-_
--------------=----=<----
-------b
-

   it sort of has an idea that    -   s and    =   s should show up a lot, but
   that   s about it. it hasn   t figured out the pattern yet.

   after several thousand iterations, it   s starting to look like
   something:
--
-----------=
----------=
--------pp=
--------pp=
-----------=
-----------=
-----------=
-------?---=
-----------=
-----------=

   the model has almost figured out that each line should be the same
   length. it has even started to figure out some of the logic of mario:
   the pipes in mario are always two blocks wide and at least two blocks
   high, so the    p   s in the data should appear in 2x2 clusters. that   s
   pretty cool!

   with a lot more training, the model gets to the point where it
   generates perfectly valid data:
--------pp=
--------pp=
----------=
----------=
----------=
---ppp=---=
---ppp=---=
----------=

   let   s sample an entire level   s worth of data from our model and rotate
   it back horizontal:
   [1*msfyg2wgn_tdfpuqrieofa.png]
   a whole level, generated from our model!

   this data looks great! there are several awesome things to notice:
     * it put a [48]lakitu (the monster that floats on a cloud) up in the
       sky at the beginning of the level         just like in a real mario
       level.
     * it knows that pipes floating in the air should be resting on top of
       solid blocks and not just hanging in the air.
     * it places enemies in logical places.
     * it doesn   t create anything that would block a player from moving
       forward.
     * it feels like a real level from super mario bros. 1 because it   s
       based off the style of the original levels that existed in that
       game.

   finally, let   s take this level and recreate it in super mario maker:
   [1*jrm6no8bbknujofwsitcxg.jpeg]
   our level data after being entered into super mario maker

   iframe: [49]/media/2ee2bff6356c8bd6a1e67c26e4c54819?postid=a26a10b68df3

   play it yourself!

   if you have super mario maker, you can play this level by
   [50]bookmarking it online or by looking it up using level code
   [51]4ac9   0000   0157-f3c3.

toys vs. real world applications

   the recurrent neural network algorithm we used to train our model is
   the same kind of algorithm used by real-world companies to solve hard
   problems like speech detection and language translation. what makes our
   model a    toy    instead of cutting-edge is that our model is generated
   from very little data. there just aren   t enough levels in the original
   super mario brothers game to provide enough data for a really good
   model.

   if we could get access to the hundreds of thousands of user-created
   super mario maker levels that nintendo has, we could make an amazing
   model. but we can   t         because nintendo won   t let us have them. big
   companies don   t give away their data for free.

   as machine learning becomes more important in more industries, the
   difference between a good program and a bad program will be how much
   data you have to train your models. that   s why companies like google
   and facebook need your data so badly!

   for example, google recently open sourced [52]tensorflow, its software
   toolkit for building large-scale machine learning applications. it was
   a pretty big deal that google gave away such important, capable
   technology for free. this is the same stuff that powers google
   translate.

   but without google   s massive trove of data in every language, you can   t
   create a competitor to google translate. data is what gives google its
   edge. think about that the next time you open up your [53]google maps
   location history or [54]facebook location history and notice that it
   stores every place you   ve ever been.

further reading

   in machine learning, there   s never a single way to solve a problem. you
   have limitless options when deciding how to pre-process your data and
   which algorithms to use. often [55]combining multiple approaches will
   give you better results than any single approach.

   readers have sent me links to other interesting approaches to
   generating super mario levels:
     * [56]justin michaud expanded on the approach i used here to generate
       levels and [57]figured out how to hack his generated levels back
       into the original nes rom file (code written over 30 years ago)!
       you can even play his [58]hacked rom online.
     * [59]amy k. hoover   s team used an approach that [60]represents each
       type of level object (pipes, ground, platforms, etc) as if it were
       single voice in an overall symphony. using a process called
       functional scaffolding, the system can augment levels with blocks
       of any given object type. for example, you could sketch out the
       basic shape of a level and it could add in pipes and question
       blocks to complete your design.
     * [61]steve dahlskog   s team showed that modeling each column of level
       data as a series of id165    words    [62]makes it possible to
       generate levels with a much simpler algorithm than a large id56.
     __________________________________________________________________

   if you liked this article, please consider [63]signing up for my
   machine learning is fun! email list. i   ll only email you when i have
   something new and awesome to share. it   s the best way to find out when
   i write more articles like this.

   you can also follow me on twitter at [64]@ageitgey, [65]email me
   directly or [66]find me on linkedin. i   d love to hear from you if i can
   help you or your team with machine learning.

   now continue on to [67]machine learning is fun part 3!

     * [68]artificial intelligence
     * [69]machine learning
     * [70]nintendo

   (button)
   (button)
   (button) 16.6k claps
   (button) (button) (button) 71 (button) (button)

     (button) blockedunblock (button) followfollowing
   [71]go to the profile of adam geitgey

[72]adam geitgey

   interested in computers and machine learning. likes to write about it.

     * (button)
       (button) 16.6k
     * (button)
     *
     *

   [73]go to the profile of adam geitgey
   never miss a story from adam geitgey, when you sign up for medium.
   [74]learn more
   never miss a story from adam geitgey
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a26a10b68df3
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@ageitgey?source=post_header_lockup
  10. https://medium.com/@ageitgey
  11. https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471
  12. https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3
  13. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721
  14. https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78
  15. https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa
  16. https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a
  17. https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7
  18. https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196
  19. https://medium.com/botsupply/il-machine-learning-  -divertente-parte-2-dec556e4855d
  20. https://medium.com/@lfcj/machine-learning-es-divertido-parte-2-b12f488675c5
  21. https://medium.com/@alexis.anzieu/samuser-avec-le-machine-learning-part2-41974ee1f586
  22. https://medium.com/@atakanyenel/makine-    renimi-e  lencelidir-2-k  s  m-6b464cbdf40c
  23. http://algotravelling.com/ru/                -                -      -            -2/
  24. https://medium.com/@jongdae.lim/      -      -machine-learning-   -         -part-2-b35f3d327761
  25. https://medium.com/machina-sapiens/aprendizagem-de-m  quina-  -divertido-parte-2-7c00d034e1d5
  26. https://zerotohero.ir/article/machine-learning/              -          -    -        -        -      -      
  27. https://viblo.asia/p/machine-learning-that-thu-vi-2-tao-sach-van-hoc-va-game-mario-wayk81o9zxx
  28. https://zhuanlan.zhihu.com/p/24344720
  29. https://www.machinelearningisfun.com/get-the-book/
  30. https://www.machinelearningisfun.com/get-the-book/
  31. https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471
  32. https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471
  33. https://supermariomakerbookmark.nintendo.net/courses/4ac9-0000-0157-f3c3
  34. https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471
  35. https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471
  36. https://en.wikipedia.org/wiki/feature_scaling
  37. https://en.wikipedia.org/wiki/activation_function
  38. https://en.wikipedia.org/wiki/state_(computer_science)
  39. https://en.wikipedia.org/wiki/the_sun_also_rises
  40. https://github.com/karpathy/char-id56
  41. http://karpathy.github.io/about/
  42. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  43. https://github.com/karpathy/char-id56
  44. https://gist.github.com/nylki/1efbaa36635956d35bcc
  45. https://medium.com/@samim/obama-id56-machine-generated-political-speeches-c8abd18a2ea0#.b4clxzrgf
  46. http://supermariomaker.nintendo.com/
  47. https://en.wikipedia.org/wiki/feature_selection
  48. http://www.mariowiki.com/lakitu
  49. https://medium.com/media/2ee2bff6356c8bd6a1e67c26e4c54819?postid=a26a10b68df3
  50. https://supermariomakerbookmark.nintendo.net/courses/4ac9-0000-0157-f3c3
  51. https://supermariomakerbookmark.nintendo.net/courses/4ac9-0000-0157-f3c3
  52. https://www.tensorflow.org/
  53. https://maps.google.com/locationhistory/b/0
  54. https://www.facebook.com/help/1026190460827516
  55. https://en.wikipedia.org/wiki/ensemble_learning
  56. http://justinmichaud.com/
  57. https://medium.com/@justin_michaud/super-mario-bros-level-generation-using-torch-id56-726ddea7e9b7
  58. http://justinmichaud.com/ml_level/index.html
  59. http://amykhoover.com/
  60. http://julian.togelius.com/hoover2015composing.pdf
  61. http://forskning.mah.se/en/id/tsstda
  62. http://julian.togelius.com/dahlskog2014linear.pdf
  63. http://eepurl.com/b9fg2t
  64. https://twitter.com/ageitgey
  65. mailto:ageitgey@gmail.com
  66. https://www.linkedin.com/in/ageitgey
  67. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.o6srqap2e
  68. https://medium.com/tag/artificial-intelligence?source=post
  69. https://medium.com/tag/machine-learning?source=post
  70. https://medium.com/tag/nintendo?source=post
  71. https://medium.com/@ageitgey?source=footer_card
  72. https://medium.com/@ageitgey
  73. https://medium.com/@ageitgey
  74. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  76. https://medium.com/p/a26a10b68df3/share/twitter
  77. https://medium.com/p/a26a10b68df3/share/facebook
  78. https://medium.com/p/a26a10b68df3/share/twitter
  79. https://medium.com/p/a26a10b68df3/share/facebook
