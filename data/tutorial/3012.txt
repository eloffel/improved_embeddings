   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

machine learning is fun part 6: how to do id103 with
deep learning

   [9]go to the profile of adam geitgey
   [10]adam geitgey (button) blockedunblock (button) followfollowing
   dec 23, 2016

   update: this article is part of a series. check out the full series:
   [11]part 1, [12]part 2, [13]part 3, [14]part 4, [15]part 5, [16]part 6,
   [17]part 7 and [18]part 8! you can also read this article in [19]          ,
   [20]         , [21]ti   ng vi   t or [22]              .

   giant update: [23]i   ve written a new book based on these articles! it
   not only expands and updates all my articles, but it has tons of brand
   new content and lots of hands-on coding projects. [24]check it out now!

   id103 is invading our lives. it   s built into our phones,
   our game consoles and our smart watches. it   s even automating our
   homes. for just $50, you can get an amazon echo dot         a magic box that
   allows you to order pizza, get a weather report or even buy trash
   bags         just by speaking out loud:
   [1*z2bkmxtfghsw6_nffguecg.jpeg]
   alexa, order a large pizza!

   the echo dot has been so popular this holiday season that amazon
   [25]can   t seem to keep them in stock!

   but id103 has been around for decades, so why is it just
   now hitting the mainstream? the reason is that deep learning finally
   made id103 accurate enough to be useful outside of
   carefully controlled environments.

   [26]andrew ng has long predicted that as id103 goes from
   95% accurate to 99% accurate, it will become a primary way that we
   interact with computers. the idea is that this 4% accuracy gap is the
   difference between annoyingly unreliable and incredibly useful. thanks
   to deep learning, we   re finally cresting that peak.

   let   s learn how to do id103 with deep learning!

machine learning isn   t always a black box

   if you know [27]how id4 works, you might guess
   that we could simply feed sound recordings into a neural network and
   train it to produce text:
   [1*njnxfmjahxyjttvfkhgtlg.png]

   that   s the holy grail of id103 with deep learning, but we
   aren   t quite there yet (at least at the time that i wrote this         i bet
   that we will be in a couple of years).

   the big problem is that speech varies in speed. one person might say
      hello!    very quickly and another person might say
      heeeelllllllllllllooooo!    very slowly, producing a much longer sound
   file with much more data. both both sound files should be recognized as
   exactly the same text            hello!    automatically aligning audio files of
   various lengths to a fixed-length piece of text turns out to be pretty
   hard.

   to work around this, we have to use some special tricks and extra
   precessing in addition to a deep neural network. let   s see how it
   works!

turning sounds into bits

   the first step in id103 is obvious         we need to feed sound
   waves into a computer.

   in [28]part 3, we learned how to take an image and treat it as an array
   of numbers so that we can feed directly into a neural network for image
   recognition:
   [1*zy1qfb9affzz66yxxoi2aw.gif]
   images are just arrays of numbers that encode the intensity of
   each pixel

   but sound is transmitted as waves. how do we turn sound waves into
   numbers? let   s use this sound clip of me saying    hello   :
   [1*6_q1vivjuavya-9uby_l-a.png]
   a waveform of me saying    hello   

   sound waves are one-dimensional. at every moment in time, they have a
   single value based on the height of the wave. let   s zoom in on one tiny
   part of the sound wave and take a look:
   [1*dqwhwuiziyoliqvretbaia.png]

   to turn this sound wave into numbers, we just record of the height of
   the wave at equally-spaced points:
   [1*diczccmem_ewwx0ya6b3cw.gif]
   sampling a sound wave

   this is called sampling. we are taking a reading thousands of times a
   second and recording a number representing the height of the sound wave
   at that point in time. that   s basically all an uncompressed .wav audio
   file is.

      cd quality    audio is sampled at 44.1khz (44,100 readings per second).
   but for id103, a sampling rate of 16khz (16,000 samples
   per second) is enough to cover the frequency range of human speech.

   lets sample our    hello    sound wave 16,000 times per second. here   s the
   first 100 samples:
   [1*bg4ifbx7qhb5v_jtr958pq.png]
   each number represents the amplitude of the sound wave at 1/16000th of
   a second intervals

a quick sidebar on digital sampling

   you might be thinking that sampling is only creating a rough
   approximation of the original sound wave because it   s only taking
   occasional readings. there   s gaps in between our readings so we must be
   losing data, right?
   [1*kkwfr3a6htrsz8-4luw0kg.png]
   can digital samples perfectly recreate the original analog sound wave?
   what about those gaps?

   but thanks to the [29]nyquist theorem, we know that we can use math to
   perfectly reconstruct the original sound wave from the spaced-out
   samples         as long as we sample at least twice as fast as the highest
   frequency we want to record.

   i mention this only because [30]nearly everyone gets this wrong and
   assumes that using higher sampling rates always leads to better audio
   quality. it doesn   t.

   </end rant>

pre-processing our sampled sound data

   we now have an array of numbers with each number representing the sound
   wave   s amplitude at 1/16,000th of a second intervals.

   we could feed these numbers right into a neural network. but trying to
   recognize speech patterns by processing these samples directly is
   difficult. instead, we can make the problem easier by doing some
   pre-processing on the audio data.

   let   s start by grouping our sampled audio into 20-millisecond-long
   chunks. here   s our first 20 milliseconds of audio (i.e., our first 320
   samples):
   [1*_quexevlltkfhsritxsa-a.png]

   plotting those numbers as a simple line graph gives us a rough
   approximation of the original sound wave for that 20 millisecond period
   of time:
   [1*zmxcyjnfqiovzjrm9bcmww.png]

   this recording is only 1/50th of a second long. but even this short
   recording is a complex mish-mash of different frequencies of sound.
   there   s some low sounds, some mid-range sounds, and even some
   high-pitched sounds sprinkled in. but taken all together, these
   different frequencies mix together to make up the complex sound of
   human speech.

   to make this data easier for a neural network to process, we are going
   to break apart this complex sound wave into it   s component parts. we   ll
   break out the low-pitched parts, the next-lowest-pitched-parts, and so
   on. then by adding up how much energy is in each of those frequency
   bands (from low to high), we create a fingerprint of sorts for this
   audio snippet.

   imagine you had a recording of someone playing a c major chord on a
   piano. that sound is the combination of three musical notes    c, e and
   g         all mixed together into one complex sound. we want to break apart
   that complex sound into the individual notes to discover that they were
   c, e and g. this is the exact same idea.

   we do this using a mathematic operation called a [31]fourier transform.
   it breaks apart the complex sound wave into the simple sound waves that
   make it up. once we have those individual sound waves, we add up how
   much energy is contained in each one.

   the end result is a score of how important each frequency range is,
   from low pitch (i.e. bass notes) to high pitch. each number below
   represents how much energy was in each 50hz band of our 20 millisecond
   audio clip:
   [1*2vg8z3--moe-e7kybjlupg.png]
   each number in the list represents how much energy was in that 50hz
   frequency band

   but this is a lot easier to see when you draw this as a chart:
   [1*a4cxgdyqyd_nrf3e-7etwa.png]
   you can see that our 20 millisecond sound snippet has a lot of
   low-frequency energy and not much energy in the higher frequencies.
   that   s typical of    male    voices.

   if we repeat this process on every 20 millisecond chunk of audio, we
   end up with a spectrogram (each column from left-to-right is one 20ms
   chunk):
   [1*bhd7b-s-qnds3hgv6loo8a.png]
   the full spectrogram of the    hello    sound clip

   a spectrogram is cool because you can actually see musical notes and
   other pitch patterns in audio data. a neural network can find patterns
   in this kind of data more easily than raw sound waves. so this is the
   data representation we   ll actually feed into our neural network.

recognizing characters from short sounds

   now that we have our audio in a format that   s easy to process, we will
   feed it into a deep neural network. the input to the neural network
   will be 20 millisecond audio chunks. for each little audio slice, it
   will try to figure out the letter that corresponds the sound currently
   being spoken.
   [1*z1nf0es1yvufdzzgw0psdq.png]

   we   ll use a [32]recurrent neural network         that is, a neural network
   that has a memory that influences future predictions. that   s because
   each letter it predicts should affect the likelihood of the next letter
   it will predict too. for example, if we have said    hel    so far, it   s
   very likely we will say    lo    next to finish out the word    hello   . it   s
   much less likely that we will say something unpronounceable next like
      xyz   . so having that memory of previous predictions helps the neural
   network make more accurate predictions going forward.

   after we run our entire audio clip through the neural network (one
   chunk at a time), we   ll end up with a mapping of each audio chunk to
   the letters most likely spoken during that chunk. here   s what that
   mapping looks like for me saying    hello   :
   [1*d1ktmdonfojrkkyjfp6sqq.png]

   our neural net is predicting that one likely thing i said was
      hhhee_ll_lllooo   . but it also thinks that it was possible that i said
      hhhuu_ll_lllooo    or even    aaauu_ll_lllooo   .

   we have some steps we follow to clean up this output. first, we   ll
   replace any repeated characters a single character:
     * hhhee_ll_lllooo becomes he_l_lo
     * hhhuu_ll_lllooo becomes hu_l_lo
     * aaauu_ll_lllooo becomes au_l_lo

   then we   ll remove any blanks:
     * he_l_lo becomes hello
     * hu_l_lo becomes hullo
     * au_l_lo becomes aullo

   that leaves us with three possible transcriptions            hello   ,    hullo   
   and    aullo   . if you say them out loud, all of these sound similar to
      hello   . because it   s predicting one character at a time, the neural
   network will come up with these very sounded-out transcriptions. for
   example if you say    he would not go   , it might give one possible
   transcription as    he wud net go   .

   the trick is to combine these pronunciation-based predictions with
   likelihood scores based on large database of written text (books, news
   articles, etc). you throw out transcriptions that seem the least likely
   to be real and keep the transcription that seems the most realistic.

   of our possible transcriptions    hello   ,    hullo    and    aullo   , obviously
      hello    will appear more frequently in a database of text (not to
   mention in our original audio-based training data) and thus is probably
   correct. so we   ll pick    hello    as our final transcription instead of
   the others. done!

wait a second!

   you might be thinking    but what if someone says    [33]hullo   ? it   s a
   valid word. maybe    hello    is the wrong transcription!   
   [1*yzvdel59vokfmzdkhpe-cq.jpeg]
      hullo! who dis?   

   of course it is possible that someone actually said    hullo    instead of
      hello   . but a id103 system like this (trained on american
   english) will basically never produce    hullo    as the transcription.
   it   s just such an unlikely thing for a user to say compared to    hello   
   that it will always think you are saying    hello    no matter how much you
   emphasize the    u    sound.

   try it out! if your phone is set to american english, try to get your
   phone   s digital assistant to recognize the world    hullo.    you can   t! it
   refuses! it will always understand it as    hello.   

   not recognizing    hullo    is a reasonable behavior, but sometimes you   ll
   find annoying cases where your phone just refuses to understand
   something valid you are saying. that   s why these id103
   models are always being retrained with more data to fix these edge
   cases.

can i build my own id103 system?

   one of the coolest things about machine learning is how simple it
   sometimes seems. you get a bunch of data, feed it into a machine
   learning algorithm, and then magically you have a world-class ai system
   running on your gaming laptop   s video card    right?

   that sort of true in some cases, but not for speech. recognizing speech
   is a hard problem. you have to overcome almost limitless challenges:
   bad quality microphones, background noise, reverb and echo, accent
   variations, and on and on. all of these issues need to be present in
   your training data to make sure the neural network can deal with them.

   here   s another example: did you know that when you speak in a loud room
   you unconsciously raise the pitch of your voice to be able to talk over
   the noise? humans have no problem understanding you either way, but
   neural networks need to be trained to handle this special case. so you
   need training data with people yelling over noise!

   to build a voice recognition system that performs on the level of siri,
   google now!, or alexa, you will need a lot of training data         far more
   data than you can likely get without hiring hundreds of people to
   record it for you. and since users have low tolerance for poor quality
   voice recognition systems, you can   t skimp on this. no one wants a
   voice recognition system that works 80% of the time.

   for a company like google or amazon, hundreds of thousands of hours of
   spoken audio recorded in real-life situations is gold. that   s the
   single biggest thing that separates their world-class speech
   recognition system from your hobby system. the whole point of putting
   google now! and siri on every cell phone for free or selling $50 alexa
   units that have no subscription fee is to get you to use them as much
   as possible. every single thing you say into one of these systems is
   recorded forever and used as training data for future versions of
   id103 algorithms. that   s the whole game!

   don   t believe me? if you have an android phone with google now!,
   [34]click here to listen to actual recordings of yourself saying every
   dumb thing you   ve ever said into it:
   [1*lwflusofh032tbttd52evw.png]
   you can access the same thing for amazon via your alexa app. apple
   unfortunately doesn   t let you access your siri voice data.

   so if you are looking for a start-up idea, i wouldn   t recommend trying
   to build your own id103 system to compete with google.
   instead, figure out a way to get people to give you recordings of
   themselves talking for hours. the data can be your product instead.

where to learn more

     * the algorithm (roughly) described here to deal with variable-length
       audio is called connectionist temporal classification or ctc. you
       can [35]read the original paper from 2006.
     * [36]adam coates of baidu gave a great presentation on deep learning
       for id103 at the bay area deep learning school. you
       can [37]watch the video on youtube (his talk starts at 3:51:00).
       highly recommended.
     __________________________________________________________________

   if you liked this article, please consider [38]signing up for my
   machine learning is fun! email list. i   ll only email you when i have
   something new and awesome to share. it   s the best way to find out when
   i write more articles like this.

   you can also follow me on twitter at [39]@ageitgey, [40]email me
   directly or [41]find me on linkedin. i   d love to hear from you if i can
   help you or your team with machine learning.

   [42]now continue on to machine learning is fun! part 7!

     * [43]machine learning
     * [44]artificial intelligence
     * [45]id103

   (button)
   (button)
   (button) 10.6k claps
   (button) (button) (button) 67 (button) (button)

     (button) blockedunblock (button) followfollowing
   [46]go to the profile of adam geitgey

[47]adam geitgey

   interested in computers and machine learning. likes to write about it.

     * (button)
       (button) 10.6k
     * (button)
     *
     *

   [48]go to the profile of adam geitgey
   never miss a story from adam geitgey, when you sign up for medium.
   [49]learn more
   never miss a story from adam geitgey
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/28293c162f7a
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@ageitgey?source=post_header_lockup
  10. https://medium.com/@ageitgey
  11. https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471
  12. https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3
  13. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721
  14. https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78
  15. https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa
  16. https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a
  17. https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7
  18. https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196
  19. https://zhuanlan.zhihu.com/p/24703268
  20. https://medium.com/@jongdae.lim/      -      -machine-learning-   -         -part-6-eb0ed6b0ed1d
  21. https://viblo.asia/p/machine-learning-that-thu-vi-6-nhan-dien-giong-noi-1je5e8dylnl
  22. http://algotravelling.com/ru/                -                -      -            -6/
  23. https://www.machinelearningisfun.com/get-the-book/
  24. https://www.machinelearningisfun.com/get-the-book/
  25. https://www.bloomberg.com/news/articles/2016-12-21/amazon-sells-out-of-echo-speakers-in-midst-of-holiday-rush
  26. https://en.wikipedia.org/wiki/andrew_ng
  27. https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa
  28. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.wukv4xnp4
  29. https://en.wikipedia.org/wiki/nyquist   shannon_sampling_theorem
  30. http://gizmodo.com/dont-buy-what-neil-young-is-selling-1678446860
  31. https://en.wikipedia.org/wiki/fourier_transform
  32. https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3
  33. https://www.merriam-webster.com/dictionary/hullo
  34. https://myactivity.google.com/udc/vaa
  35. http://www.cs.toronto.edu/~graves/icml_2006.pdf
  36. https://cs.stanford.edu/~acoates/
  37. https://youtu.be/9dxiaecyjry?t=13874
  38. http://eepurl.com/b9fg2t
  39. https://twitter.com/ageitgey
  40. mailto:ageitgey@gmail.com
  41. https://www.linkedin.com/in/ageitgey
  42. https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7
  43. https://medium.com/tag/machine-learning?source=post
  44. https://medium.com/tag/artificial-intelligence?source=post
  45. https://medium.com/tag/speech-recognition?source=post
  46. https://medium.com/@ageitgey?source=footer_card
  47. https://medium.com/@ageitgey
  48. https://medium.com/@ageitgey
  49. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  51. https://medium.com/p/28293c162f7a/share/twitter
  52. https://medium.com/p/28293c162f7a/share/facebook
  53. https://medium.com/p/28293c162f7a/share/twitter
  54. https://medium.com/p/28293c162f7a/share/facebook
