   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

machine learning is fun part 7: abusing id3 to
make 8-bit pixel art

   [9]go to the profile of adam geitgey
   [10]adam geitgey (button) blockedunblock (button) followfollowing
   feb 12, 2017

   update: this article is part of a series. check out the full series:
   [11]part 1, [12]part 2, [13]part 3, [14]part 4, [15]part 5, [16]part 6,
   [17]part 7 and [18]part 8! you can also read this article in
   [19]              , [20]ti   ng vi   t or [21]         .

   giant update: [22]i   ve written a new book based on these articles! it
   not only expands and updates all my articles, but it has tons of brand
   new content and lots of hands-on coding projects. [23]check it out now!

   generative models allow a computer to create data         like photos, movies
   or music         by itself.

   a little over a year ago, [24]alec radford (building on the work of
   [25]ian goodfellow) published a paper that changed how everyone thought
   about building generative models with machine learning. the new system
   is called deep convolutional id3 (or dcgans
   for short).

   dcgans are able to hallucinate original photo-realistic pictures by
   using a clever combination of two deep neural networks that compete
   with each other. all of these pictures of bedrooms were dreamt up by a
   dcgan:
   [1*sha8nd6bpgn6r0gbcasb2a.png]
   picture from alec radford   s original [26]dcgan paper

   ai researchers care about generative models because they seem to be a
   stepping stone towards building ai systems that can consume raw data
   from the world and automatically build understanding from it.

   but let   s use generative models to do something a bit more silly         make
   artwork for 8-bit video games!
   [1*xl2g8ikxisnseyb8zwh9dg.png]
   all the art in this game level is machine-generated.
     __________________________________________________________________

the goal of generative models

   so why exactly are ai researchers building complex systems to generate
   slightly wonky-looking pictures of bedrooms?

   the idea is that if you can generate pictures of something, you must
   have an understanding of it.

   look at this picture:
   [1*wh7fty5yugm0cprk66fg9a.jpeg]
   a dog. more specifically, my dog.

   you instantly know this is a picture of a dog         a furry thing with four
   legs and a tail. but to a computer, the picture is just a grid of
   numbers representing the color of each pixel. the computer has no
   understanding that the picture represents a concept.

   but now imagine that we showed a computer thousands of pictures of dogs
   and after seeing those pictures, the computer was able to generate new
   pictures of dogs on its own         including different dog breeds and
   pictures from different angles. maybe we could even ask it for certain
   types of pictures, like    a side view of a beagle   .

   if the computer was able to do this and the pictures it produced had
   the right number of legs, tails, and ears, it would prove that the
   computer knows what parts go into making up a    dog    even though no one
   told it explicitly. so in a sense, a good generative model is proof of
   basic understanding         at least on a toddler-level.

   that   s why researchers are so excited about building generative models.
   they seem to be a way to train computers to understand concepts without
   being explicitly taught the meaning of those concepts. that   s a big
   step over current systems that can only learn from training data that
   has been painstakingly pre-labeled by humans.

   but if all this research results in programs that generate pictures of
   dogs, how many years until we get the first computer-generated
   dog-a-day calendar as a side effect?
   [1*6lc3ezgurb18rfguupgndw.jpeg]
   yes, the robots are coming for everyone   s jobs. eventually.

   and if you can build a program that understands dogs, why not a program
   that understands anything else? what about a program that could
   generate an unlimited number of stock photos of people shaking hands?
   i   m sure someone would pay for that.
   [1*prl9-ijjdkkefoummrsrvq.jpeg]
   i mean.. sure, that   s a terrible idea for an ai start-up. but i   ve
   definitely heard worse start-up ideas, so   . maybe?

   ok, maybe a program that generates bad stock photos wouldn   t be that
   interesting. but given the rate of progress in generative models over
   just the past year, who knows where we   ll be in 5 or 10 years. what
   happens if someone invents a system to generate entire movies? or
   music? or video games?

   if you look forward 20   30 years and squint, you can already imagine a
   world where entertainment could be 100% machine generated:

   iframe: [27]/media/58f649062a7391345e45fe911f1e11e1?postid=e45d9b96cee7

   the video game industry is the first area of entertainment to start
   [28]seriously experimenting with using ai to generate raw content.
   aside from the obvious venn diagram overlap between computer gaming and
   machine learning engineers, there   s a huge cost incentive to invest in
   video game development automation given the $300+ million budgets of
   modern [29]aaa video games.

   we are still in the earliest days of machine-learning-based generative
   models and their practical uses are currently pretty narrow, but they
   are a lot of fun to play around with. let   s see what we can do with
   one.

how dcgans work

   to build a dcgan, we create two deep neural networks. then we make them
   fight against each other, endlessly attempting to out-do one another.
   in the process, they both become stronger.

   let   s pretend that the first deep neural network is a brand new police
   officer who is being trained to spot counterfeit money. it   s job is to
   look at a picture and tell us if the picture contains real money.

   since we are looking for objects in pictures, we can use a standard
   [30]convolutional neural network for this job. if you aren   t familiar
   with convnets, you can [31]read my earlier post. but the basic idea is
   that the neural network that takes in an image, processes it through
   several layers that recognize increasingly complex features in the
   image and then it outputs a single value   in this case, whether or not
   the image contains a picture of real money.

   this first neural network is called the discriminator:
   [1*dzgbacohvg7ea-ni3bqljg.png]
   the discriminator network

   now let   s pretend the second neural network is a brand new
   counterfeiter who is just learning how to create fake money. for this
   second neural network, we   ll reverse the layers in a normal convnet so
   that everything runs backwards. so instead of taking in a picture and
   outputting a value, it takes in a list of values and outputs a picture.

   this second neural network is called the generator:
   [1*tns5-ymkehhomq34qfpmgg.png]
   the generator network

   so now we have a police officer (the discriminator) looking for fake
   money and a counterfeiter (the generator) that   s printing fake money.
   let   s make them battle!

   in the first round, the generator will create pathetic forgeries that
   barely resemble money at all because it knows absolutely nothing about
   what money is supposed to look like:
   [1*z5kz1ojyt9ld5rgypajx5a.png]
   the generator makes the first (terrible) fake dollar

   but right now the discriminator is equally terrible at it   s job of
   recognizing money, so it won   t know the difference:
   [1*j2hxb7rdc9tc5uhw0fvqiq.png]
   the discriminator thinks the dollar is real!

   at this point, we step in and tell the discriminator that this dollar
   bill is actually fake. then we show it a real dollar bill and ask it
   how it looks different from the fake one. the discriminator looks for a
   new detail to help it separate the real one from the fake one.

   for example, the discriminator might notice that real money has a
   picture of a person on it and the fake money doesn   t. using this
   knowledge, the discriminator learns how to tell the fake from the real
   one. it gets a tiny bit better at its job:
   [1*z-do7lirqpc55gsm1tco7w.png]
   the discriminator levels up! it now can spot very bad fake dollars.

   now we start round 2. we tell the generator that it   s money images are
   suddenly getting rejected as fake so it needs to step up it   s game. we
   also tell it that the discriminator is now looking for faces, so the
   best way to confuse the discriminator is to put a face on the bill:
   [1*oceeurbs5-dncg60l52m8q.png]
   the generator makes a very slightly better counterfeit dollar

   and the fake bills are being accepted as valid again! so now the
   discriminator has to look again at the real dollar and find a new way
   to tell it apart from the fake one.

   this back-and-forth game between the generator and the discriminator
   continues thousands of times until both networks are experts.
   eventually the generator is producing near-perfect counterfeits and the
   discriminator has turned into a master detective looking for the
   slightest mistakes.

   at the point when both networks are sufficiently trained so that humans
   are impressed by the fake images, we can use the fake images for
   whatever purpose we want.

applying this to video games

   so now that we know how dcgans work, let   s see if we can use one to
   generate new artwork for 1980s-style video games.

   let   s build a dcgan that tries to produce screenshots of imaginary
   video games for the nintendo entertainment system (or nes) based on
   screenshots of real games:
   [1*_nrxwwugsz0eggwzlyaeiq.png]

   the idea is that if we can generate convincing screenshots of imaginary
   video games, we could copy and paste bits of art from those screenshots
   and use it in our own retro-style video game. since the generated video
   games never existed, it wouldn   t even be stealing (maybe... more on
   this later).

   video game art in those days was very simple. since the nes had such a
   small amount of memory (the games used way less memory than this
   article takes up!), programmers had to use lots of tricks to fit the
   game art into memory. to maximize the limited space, games used
   tile-based graphics where each screen in the game is made up of just a
   few (usually 16x16 pixel) repeated graphical tiles.

   for example, the starting screen of    [32]the legend of zelda    is made
   up of only 8 unique tiles:
   [1*ldjrkq1yz6xqy1atqnuekq.png]

   here are the tiles for entire    the legend of zelda    game map:
   [1*v16yj0xpguveqkfufnw2pq.png]
   sometimes they swap the colors around to make the different areas look
   different, but that   s it.

   our goal is to create a similar tile sheet for our game. because of
   that, we don   t really care if the game screenshots we generate look
   completely realistic. instead, we   re just looking for the shapes and
   patterns that we can use as 16 x 16 tiles in our game         things like
   stones, water, bridges, etc. then we can use those tiles to build our
   own 8-bit-style video game levels.

getting data

   to train our system, we need lots of data. luckily there are [33]over
   700 games for the nes that we can pull from.

   i used [34]wget to download all the nes game screenshots on [35]the
   video game museum website (sorry for scraping your site!). after a few
   minutes of downloading, i had a little over 10,000 screenshots of
   hundreds of nes games:
   [1*jqqmavocqlqbpdx8aykygw.png]
   just a few of the 10,000 screenshots that make up the data set

   right now, dcgans only work on pretty small images         256 pixels square
   or so. but the entire screen resolution of the nes was only 256 pixels
   by 224 pixels, so that   s not a problem. to make things simple, i
   cropped each nes screenshot to 224 pixels square.

setting up the dcgan

   there are several open-source implementations of dcgans on github that
   you can try out. i used taehoon kim   s [36]tensorflow implementation.
   since dcgans are unsupervised, all you have to do is put the data in a
   folder, tweak the basic parameters, start it training and then wait to
   see what results you get.

   here   s what a sample of the original training data looks like:
   [1*maisubyz03fq-h1jnut7ta.png]

   now training begins. at first, the output from the generator is pure
   noise. but it slowly start to take shape as the generator learns to do
   a better job:
   [1*zbfbvjbphbqnwzfucnyclg.gif]

   after several more training rounds, the images start to resemble
   nightmare-ish versions of classic nintendo games:
   [1*mcvglv4ijsvrt33ek5lesq.gif]

   as training continues further, we start to see the bricks and blocks we
   are hoping to find. you can also see screen elements like life bars and
   even some text:
   [1*eds03c9e_te5k3yyo91hnw.png]

   this is where things get complicated. how do we know the computer is
   creating brand new art and not just regurgitating art directly from the
   training images? in two of these images, you can clearly see the menu
   bar from super mario bros. 3 and the header bar and bricks from the
   original super mario bros.

   regurgitating training data is definitely something that can happen. by
   using a large training data set and not training too long, we can try
   to reduce the chance that this happens. but it   s a thorny issue and
   research on it continues.

   since i   m just going for aesthetics, i tweaked the model until it
   produced art that looked original to me. but i can   t prove that the new
   art is totally original except by searching the training data for
   similar art and verifying that there isn   t any.

   with a few hours of training, the generated images contained 16 x 16
   tiles that looked nice to me. i was looking for some variations on a
   basic stone block, brick patterns, water patterns, bushes, and some
   general    spooky-looking    background atmosphere tiles.

   next i need to pre-process the generated images to the make sure they
   only used the 64 colors that are available on the nes:
   [1*yqniyouk267gm6fd8rlrra.png]
   the original nintendo could only display these 64 colors. technically
   there   s only 54 unique colors because some of them are duplicates.

   then i   ll open up the 64-color images in the [37]tiled map editor. from
   there, i can easily grab the 16 x 16 tiles that match the aesthetic i
   want:
   [1*ln1ppvzqcfbl6-xj-knhbg.png]
   the tiles i grabbed out of the generated screenshots

   then inside of tiled map editor, i   ll arrange those 16 x 16 tiles into
   a simple level layout reminiscent of the nes game    [38]castlevania   :
   [1*mcxnx-hybtzyb_xqyp6nvq.png]

   i think that looks pretty good! keep in mind i didn   t touch a single
   pixel with an image editor. every tile came straight out of the dcgan
   model.

   next, let   s throw in the main character and some enemies from
      castlevania    so we can see what this level would look like in action:
   [1*xzk6gtama3cy-wrnzs04tq.png]

   to get the full effect, let   s see what the level would look like inside
   the game with the menu elements added:
   [1*xl2g8ikxisnseyb8zwh9dg.png]
   so spooooky

   i think that looks like the nes games that i remember! i   m not claiming
   it   s the best nes art ever created, but it   s certainly not the worst:
   [1*wl-cod1eomcjh84ivxgkca.png]
   [39]the cheetahmen is not a good game.

is that it?

   i get really excited about generative models like this. the idea of one
   day cranking out endless artwork with computers is fascinating to me.
   but when i talk to other people about this stuff, sometimes the
   response is    is that it? that   s so basic.   

   there   s certainly a lot of hype around generative models right now.
   gans are already being called the future of ai despite being
   notoriously hard to train and limited to generating tiny images. in
   fact, the very best models can currently only generate
   postage-stamp-sized pictures of mutant dogs:
   [1*wprcbe66_sj_appb4tq3lw.png]
   a nightmare animal! photo from [40]ian goodfellow   s gan tutorial paper

   but a couple of years ago, we couldn   t do anything close to that. we
   were pretty excited by generated pictures that looked like this:
   [1*ruqy1lbbgnvtftnf_lcpig.png]
   it   s a bicycle! i swear!

   and the technology is improving every single day. [41]here   s a random
   paper that came out this week that uses gans to age the faces of
   people:
   [1*ue7pp_dw7xdcacd2dpz1ow.png]
   image from    [42]face aging with conditional generative adversarial
   networks   

   if things keep improving at this pace, it won   t be too long before
   generative models are a mainstream tool helping us create. it   s a great
   time to start experimenting!

keep learning

   if you want to learn more in depth about generative models and dcgans,
   here are some recommended resources:
     * [43]conditional id3 for face generation
       by [44]jon gauthier
     * [45]generative models overview from [46]openai
     * [47]image completion with deep learning in tensorflow by
       [48]brandon amos
     * see how [49]tom white uses generative models to make art in his
       [50]neural facegrid project
     * [51]ian goodfellow   s original paper on gans and [52]his recent
       tutorial on them
     __________________________________________________________________

   this article is part of my machine learning is fun series. you can
   check out the earlier parts here: [53]part 1, [54]part 2, [55]part 3,
   [56]part 4, [57]part 5 and [58]part 6

   if you liked this article, please consider [59]signing up for my
   machine learning is fun! email list. i   ll only email you when i have
   something new and awesome to share. it   s the best way to find out when
   i write more articles like this.

   you can also follow me on twitter at [60]@ageitgey, [61]email me
   directly or [62]find me on linkedin. i   d love to hear from you if i can
   help you or your team with machine learning.

     * [63]artificial intelligence
     * [64]machine learning
     * [65]gaming

   (button)
   (button)
   (button) 6.1k claps
   (button) (button) (button) 25 (button) (button)

     (button) blockedunblock (button) followfollowing
   [66]go to the profile of adam geitgey

[67]adam geitgey

   interested in computers and machine learning. likes to write about it.

     * (button)
       (button) 6.1k
     * (button)
     *
     *

   [68]go to the profile of adam geitgey
   never miss a story from adam geitgey, when you sign up for medium.
   [69]learn more
   never miss a story from adam geitgey
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e45d9b96cee7
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@ageitgey?source=post_header_lockup
  10. https://medium.com/@ageitgey
  11. https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471
  12. https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3
  13. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721
  14. https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78
  15. https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa
  16. https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a
  17. https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7
  18. https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196
  19. http://algotravelling.com/ru/                -                -      -            -7/
  20. https://viblo.asia/p/machine-learning-that-thu-vi-7-tao-kho-anh-va-thiet-ke-tro-choi-co-dien-4dbzngj8lym
  21. https://medium.com/@jongdae.lim/      -      -machine-learning-   -         -part-7-2435b4a55ccd
  22. https://www.machinelearningisfun.com/get-the-book/
  23. https://www.machinelearningisfun.com/get-the-book/
  24. https://twitter.com/alecrad
  25. https://twitter.com/goodfellow_ian
  26. https://arxiv.org/abs/1511.06434
  27. https://medium.com/media/58f649062a7391345e45fe911f1e11e1?postid=e45d9b96cee7
  28. https://www.technologyreview.com/s/601258/artificial-intelligence-can-now-design-realistic-video-and-game-imagery/
  29. https://en.wikipedia.org/wiki/aaa_(video_game_industry)
  30. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.ib3p63rx3
  31. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.ib3p63rx3
  32. https://en.wikipedia.org/wiki/the_legend_of_zelda_(video_game)
  33. https://en.wikipedia.org/wiki/list_of_nintendo_entertainment_system_games
  34. https://www.gnu.org/software/wget/
  35. http://www.vgmuseum.com/nes.htm
  36. https://github.com/carpedm20/dcgan-tensorflow
  37. http://www.mapeditor.org/
  38. https://en.wikipedia.org/wiki/castlevania
  39. https://en.wikipedia.org/wiki/the_cheetahmen
  40. https://arxiv.org/pdf/1701.00160.pdf
  41. https://arxiv.org/pdf/1702.01983v1.pdf
  42. https://arxiv.org/abs/1702.01983v1
  43. http://www.foldl.me/2015/conditional-gans-face-generation/
  44. http://www.foldl.me/
  45. https://openai.com/blog/generative-models/
  46. https://openai.com/about/
  47. https://bamos.github.io/2016/08/09/deep-completion/
  48. https://bamos.github.io/
  49. https://twitter.com/dribnet
  50. http://cargocollective.com/dribnet/facegrid
  51. http://arxiv.org/abs/1406.2661
  52. https://arxiv.org/pdf/1701.00160
  53. https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471
  54. https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3
  55. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721
  56. https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78
  57. https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa
  58. https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a
  59. http://eepurl.com/b9fg2t
  60. https://twitter.com/ageitgey
  61. mailto:ageitgey@gmail.com
  62. https://www.linkedin.com/in/ageitgey
  63. https://medium.com/tag/artificial-intelligence?source=post
  64. https://medium.com/tag/machine-learning?source=post
  65. https://medium.com/tag/gaming?source=post
  66. https://medium.com/@ageitgey?source=footer_card
  67. https://medium.com/@ageitgey
  68. https://medium.com/@ageitgey
  69. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  71. https://medium.com/p/e45d9b96cee7/share/twitter
  72. https://medium.com/p/e45d9b96cee7/share/facebook
  73. https://medium.com/p/e45d9b96cee7/share/twitter
  74. https://medium.com/p/e45d9b96cee7/share/facebook
