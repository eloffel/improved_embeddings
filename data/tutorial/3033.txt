   [1]sebastianraschka

   [2]about [3]blog [4]books [5]elsewhere [6]resources [7]publications
   [8]teaching [9]software

     [10]
   [rss]

id156

    bit by bit

   aug 3, 2014
   by sebastian raschka

sections

     * [11]sections
     * [12]introduction
          + [13]principal component analysis vs. linear discriminant
            analysis
          + [14]what is a    good    feature subspace?
          + [15]summarizing the lda approach in 5 steps
     * [16]preparing the sample data set
          + [17]about the iris dataset
          + [18]reading in the dataset
          + [19]histograms and feature selection
          + [20]normality assumptions
     * [21]lda in 5 steps
          + [22]step 1: computing the d-dimensional mean vectors
          + [23]step 2: computing the scatter matrices
               o [24]2.1 within-class scatter matrix
               o [25]2.1 b
               o [26]2.2 between-class scatter matrix
          + [27]step 3: solving the generalized eigenvalue problem for the
            matrix
               o [28]checking the eigenvector-eigenvalue calculation
          + [29]step 4: selecting linear discriminants for the new feature
            subspace
               o [30]4.1. sorting the eigenvectors by decreasing
                 eigenvalues
               o [31]4.2. choosing k eigenvectors with the largest
                 eigenvalues
     * [32]step 5: transforming the samples onto the new subspace
     * [33]a comparison of pca and lda
     * [34]lda via scikit-learn
     * [35]a note about standardization

introduction

   id156 (lda) is most commonly used as
   id84 technique in the pre-processing step for
   pattern-classification and machine learning applications. the goal is
   to project a dataset onto a lower-dimensional space with good
   class-separability in order avoid overfitting (   curse of
   dimensionality   ) and also reduce computational costs.

   ronald a. fisher formulated the linear discriminant in 1936 ([36]the
   use of multiple measurements in taxonomic problems), and it also has
   some practical uses as classifier. the original linear discriminant was
   described for a 2-class problem, and it was then later generalized as
      multi-class id156    or    multiple discriminant
   analysis    by c. r. rao in 1948 ([37]the utilization of multiple
   measurements in problems of biological classification)

   the general lda approach is very similar to a principal component
   analysis (for more information about the pca, see the previous article
   [38]implementing a principal component analysis (pca) in python step by
   step), but in addition to finding the component axes that maximize the
   variance of our data (pca), we are additionally interested in the axes
   that maximize the separation between multiple classes (lda).

   so, in a nutshell, often the goal of an lda is to project a feature
   space (a dataset n-dimensional samples) onto a smaller subspace (where
   ) while maintaining the class-discriminatory information.
   in general, id84 does not only help reducing
   computational costs for a given classification task, but it can also be
   helpful to avoid overfitting by minimizing the error in parameter
   estimation (   curse of dimensionality   ).

principal component analysis vs. id156

   both id156 (lda) and principal component
   analysis (pca) are linear transformation techniques that are commonly
   used for id84. pca can be described as an
      unsupervised    algorithm, since it    ignores    class labels and its goal
   is to find the directions (the so-called principal components) that
   maximize the variance in a dataset. in contrast to pca, lda is
      supervised    and computes the directions (   linear discriminants   ) that
   will represent the axes that that maximize the separation between
   multiple classes.

   although it might sound intuitive that lda is superior to pca for a
   multi-class classification task where the class labels are known, this
   might not always the case.
   for example, comparisons between classification accuracies for image
   recognition after using pca or lda show that pca tends to outperform
   lda if the number of samples per class is relatively small ([39]pca vs.
   lda, a.m. martinez et al., 2001). in practice, it is also not uncommon
   to use both lda and pca in combination: e.g., pca for dimensionality
   reduction followed by an lda.

what is a    good    feature subspace?

   let   s assume that our goal is to reduce the dimensions of a
   -dimensional dataset by projecting it onto a -dimensional subspace
   (where ). so, how do we know what size we should choose for ( = the
   number of dimensions of the new feature subspace), and how do we know
   if we have a feature space that represents our data    well   ?

   later, we will compute eigenvectors (the components) from our data set
   and collect them in a so-called scatter-matrices (i.e., the
   in-between-class scatter matrix and within-class scatter matrix).
   each of these eigenvectors is associated with an eigenvalue, which
   tells us about the    length    or    magnitude    of the eigenvectors.

   if we would observe that all eigenvalues have a similar magnitude, then
   this may be a good indicator that our data is already projected on a
      good    feature space.

   and in the other scenario, if some of the eigenvalues are much much
   larger than others, we might be interested in keeping only those
   eigenvectors with the highest eigenvalues, since they contain more
   information about our data distribution. vice versa, eigenvalues that
   are close to 0 are less informative and we might consider dropping
   those for constructing the new feature subspace.

summarizing the lda approach in 5 steps

   listed below are the 5 general steps for performing a linear
   discriminant analysis; we will explore them in more detail in the
   following sections.
    1. compute the -dimensional mean vectors for the different classes
       from the dataset.
    2. compute the scatter matrices (in-between-class and within-class
       scatter matrix).
    3. compute the eigenvectors () and corresponding eigenvalues () for
       the scatter matrices.
    4. sort the eigenvectors by decreasing eigenvalues and choose
       eigenvectors with the largest eigenvalues to form a dimensional
       matrix (where every column represents an eigenvector).
    5. use this eigenvector matrix to transform the samples onto the new
       subspace. this can be summarized by the id127:
       (where is a -dimensional matrix representing the samples, and are
       the transformed -dimensional samples in the new subspace).

preparing the sample data set

about the iris dataset

   for the following tutorial, we will be working with the famous    iris   
   dataset that has been deposited on the uci machine learning repository
   (https://archive.ics.uci.edu/ml/datasets/iris).
   **reference:** bache, k. & lichman, m. (2013). uci machine learning
   repository. irvine, ca: university of california, school of information
   and computer science.

   the iris dataset contains measurements for 150 iris flowers from three
   different species.

   the three classes in the iris dataset:
    1. iris-setosa (n=50)
    2. iris-versicolor (n=50)
    3. iris-virginica (n=50)

   the four features of the iris dataset:
    1. sepal length in cm
    2. sepal width in cm
    3. petal length in cm
    4. petal width in cm

feature_dict = {i:label for i,label in zip(
                range(4),
                  ('sepal length in cm',
                  'sepal width in cm',
                  'petal length in cm',
                  'petal width in cm', ))}

reading in the dataset

import pandas as pd

df = pd.io.parsers.read_csv(
    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-database
s/iris/iris.data',
    header=none,
    sep=',',
    )
df.columns = [l for i,l in sorted(feature_dict.items())] + ['class label']
df.dropna(how="all", inplace=true) # to drop the empty line at file-end

df.tail()

   sepal length in cm sepal width in cm petal length in cm petal width in
   cm class label
   145 6.7 3.0 5.2 2.3 iris-virginica
   146 6.3 2.5 5.0 1.9 iris-virginica
   147 6.5 3.0 5.2 2.0 iris-virginica
   148 6.2 3.4 5.4 2.3 iris-virginica
   149 5.9 3.0 5.1 1.8 iris-virginica

   since it is more convenient to work with numerical values, we will use
   the labelencode from the scikit-learn library to convert the class
   labels into numbers: 1, 2, and 3.
from sklearn.preprocessing import labelencoder

x = df[[0,1,2,3]].values
y = df['class label'].values

enc = labelencoder()
label_encoder = enc.fit(y)
y = label_encoder.transform(y) + 1

label_dict = {1: 'setosa', 2: 'versicolor', 3:'virginica'}

histograms and feature selection

   just to get a rough idea how the samples of our three classes , and are
   distributed, let us visualize the distributions of the four different
   features in 1-dimensional histograms.
%matplotlib inline

from matplotlib import pyplot as plt
import numpy as np
import math

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,6))

for ax,cnt in zip(axes.ravel(), range(4)):

    # set bin sizes
    min_b = math.floor(np.min(x[:,cnt]))
    max_b = math.ceil(np.max(x[:,cnt]))
    bins = np.linspace(min_b, max_b, 25)

    # plottling the histograms
    for lab,col in zip(range(1,4), ('blue', 'red', 'green')):
        ax.hist(x[y==lab, cnt],
                   color=col,
                   label='class %s' %label_dict[lab],
                   bins=bins,
                   alpha=0.5,)
    ylims = ax.get_ylim()

    # plot annotation
    leg = ax.legend(loc='upper right', fancybox=true, fontsize=8)
    leg.get_frame().set_alpha(0.5)
    ax.set_ylim([0, max(ylims)+2])
    ax.set_xlabel(feature_dict[cnt])
    ax.set_title('iris histogram #%s' %str(cnt+1))

    # hide axis ticks
    ax.tick_params(axis="both", which="both", bottom="off", top="off",
            labelbottom="on", left="off", right="off", labelleft="on")

    # remove axis spines
    ax.spines["top"].set_visible(false)
    ax.spines["right"].set_visible(false)
    ax.spines["bottom"].set_visible(false)
    ax.spines["left"].set_visible(false)

axes[0][0].set_ylabel('count')
axes[1][0].set_ylabel('count')

fig.tight_layout()

plt.show()

   png

   from just looking at these simple graphical representations of the
   features, we can already tell that the petal lengths and widths are
   likely better suited as potential features two separate between the
   three flower classes. in practice, instead of reducing the
   dimensionality via a projection (here: lda), a good alternative would
   be a feature selection technique. for low-dimensional datasets like
   iris, a glance at those histograms would already be very informative.
   another simple, but very useful technique would be to use feature
   selection algorithms; in case you are interested, i have a more
   detailed description on sequential feature selection algorithms
   [40]here, and scikit-learn also implements a nice selection of
   alternative [41]approaches. for a high-level summary of the different
   approaches, i   ve written a short post on [42]   what is the difference
   between filter, wrapper, and embedded methods for feature selection?   .

normality assumptions

   it should be mentioned that lda assumes normal distributed data,
   features that are statistically independent, and identical covariance
   matrices for every class. however, this only applies for lda as
   classifier and lda for id84 can also work
   reasonably well if those assumptions are violated. and even for
   classification tasks lda seems can be quite robust to the distribution
   of the data:

        id156 frequently achieves good performances
     in the tasks of face and object recognition, even though the
     assumptions of common covariance matrix among groups and normality
     are often violated (duda, et al., 2001)    (tao li, et al., 2006).

   tao li, shenghuo zhu, and mitsunori ogihara.    [43]using discriminant
   analysis for multi-class classification: an experimental
   investigation.    knowledge and information systems 10, no. 4 (2006):
   453   72.)

   duda, richard o, peter e hart, and david g stork. 2001. pattern
   classification. new york: wiley.

lda in 5 steps

   after we went through several preparation steps, our data is finally
   ready for the actual lda. in practice, lda for id84
   would be just another preprocessing step for a typical machine learning
   or pattern classification task.

step 1: computing the d-dimensional mean vectors

   in this first step, we will start off with a simple computation of the
   mean vectors , of the 3 different flower classes:
np.set_printoptions(precision=4)

mean_vectors = []
for cl in range(1,4):
    mean_vectors.append(np.mean(x[y==cl], axis=0))
    print('mean vector class %s: %s\n' %(cl, mean_vectors[cl-1]))

mean vector class 1: [ 5.006  3.418  1.464  0.244]

mean vector class 2: [ 5.936  2.77   4.26   1.326]

mean vector class 3: [ 6.588  2.974  5.552  2.026]

step 2: computing the scatter matrices

   now, we will compute the two 4x4-dimensional matrices: the within-class
   and the between-class scatter matrix.

2.1 within-class scatter matrix

   the within-class scatter matrix is computed by the following equation:

   where
   (scatter matrix for every class)

   and is the mean vector
s_w = np.zeros((4,4))
for cl,mv in zip(range(1,4), mean_vectors):
    class_sc_mat = np.zeros((4,4))                  # scatter matrix for every c
lass
    for row in x[y == cl]:
        row, mv = row.reshape(4,1), mv.reshape(4,1) # make column vectors
        class_sc_mat += (row-mv).dot((row-mv).t)
    s_w += class_sc_mat                             # sum class scatter matrices
print('within-class scatter matrix:\n', s_w)

within-class scatter matrix:
 [[ 38.9562  13.683   24.614    5.6556]
 [ 13.683   17.035    8.12     4.9132]
 [ 24.614    8.12    27.22     6.2536]
 [  5.6556   4.9132   6.2536   6.1756]]

2.1 b

   alternatively, we could also compute the class-covariance matrices by
   adding the scaling factor to the within-class scatter matrix, so that
   our equation becomes

   .

   and

   where is the sample size of the respective class (here: 50), and in
   this particular case, we can drop the term ( since all classes have the
   same sample size.

   however, the resulting eigenspaces will be identical (identical
   eigenvectors, only the eigenvalues are scaled differently by a constant
   factor).

2.2 between-class scatter matrix

   the between-class scatter matrix is computed by the following equation:

   where
   is the overall mean, and and are the sample mean and sizes of the
   respective classes.
overall_mean = np.mean(x, axis=0)

s_b = np.zeros((4,4))
for i,mean_vec in enumerate(mean_vectors):
    n = x[y==i+1,:].shape[0]
    mean_vec = mean_vec.reshape(4,1) # make column vector
    overall_mean = overall_mean.reshape(4,1) # make column vector
    s_b += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).t)

print('between-class scatter matrix:\n', s_b)

between-class scatter matrix:
 [[  63.2121  -19.534   165.1647   71.3631]
 [ -19.534    10.9776  -56.0552  -22.4924]
 [ 165.1647  -56.0552  436.6437  186.9081]
 [  71.3631  -22.4924  186.9081   80.6041]]

step 3: solving the generalized eigenvalue problem for the matrix

   next, we will solve the generalized eigenvalue problem for the matrix
   to obtain the linear discriminants.
eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(s_w).dot(s_b))

for i in range(len(eig_vals)):
    eigvec_sc = eig_vecs[:,i].reshape(4,1)
    print('\neigenvector {}: \n{}'.format(i+1, eigvec_sc.real))
    print('eigenvalue {:}: {:.2e}'.format(i+1, eig_vals[i].real))

eigenvector 1:
[[-0.2049]
 [-0.3871]
 [ 0.5465]
 [ 0.7138]]
eigenvalue 1: 3.23e+01

eigenvector 2:
[[-0.009 ]
 [-0.589 ]
 [ 0.2543]
 [-0.767 ]]
eigenvalue 2: 2.78e-01

eigenvector 3:
[[ 0.179 ]
 [-0.3178]
 [-0.3658]
 [ 0.6011]]
eigenvalue 3: -4.02e-17

eigenvector 4:
[[ 0.179 ]
 [-0.3178]
 [-0.3658]
 [ 0.6011]]
eigenvalue 4: -4.02e-17

   note

   depending on which version of numpy and lapack we are using, we may
   obtain the matrix with its signs flipped. please note that this is not
   an issue; if is an eigenvector of a matrix , we have

   .

   here, is the eigenvalue, and is also an eigenvector that thas the same
   eigenvalue, since

   .

   after this decomposition of our square matrix into eigenvectors and
   eigenvalues, let us briefly recapitulate how we can interpret those
   results. as we remember from our first id202 class in high
   school or college, both eigenvectors and eigenvalues are providing us
   with information about the distortion of a linear transformation: the
   eigenvectors are basically the direction of this distortion, and the
   eigenvalues are the scaling factor for the eigenvectors that describing
   the magnitude of the distortion.

   if we are performing the lda for id84, the
   eigenvectors are important since they will form the new axes of our new
   feature subspace; the associated eigenvalues are of particular interest
   since they will tell us how    informative    the new    axes    are.

   let us briefly double-check our calculation and talk more about the
   eigenvalues in the next section.

checking the eigenvector-eigenvalue calculation

   a quick check that the eigenvector-eigenvalue calculation is correct
   and satisfy the equation:

   where
for i in range(len(eig_vals)):
    eigv = eig_vecs[:,i].reshape(4,1)
    np.testing.assert_array_almost_equal(np.linalg.inv(s_w).dot(s_b).dot(eigv),
                                         eig_vals[i] * eigv,
                                         decimal=6, err_msg='', verbose=true)
print('ok')

ok

step 4: selecting linear discriminants for the new feature subspace

4.1. sorting the eigenvectors by decreasing eigenvalues

   remember from the introduction that we are not only interested in
   merely projecting the data into a subspace that improves the class
   separability, but also reduces the dimensionality of our feature space,
   (where the eigenvectors will form the axes of this new feature
   subspace).

   however, the eigenvectors only define the directions of the new axis,
   since they have all the same unit length 1.

   so, in order to decide which eigenvector(s) we want to drop for our
   lower-dimensional subspace, we have to take a look at the corresponding
   eigenvalues of the eigenvectors. roughly speaking, the eigenvectors
   with the lowest eigenvalues bear the least information about the
   distribution of the data, and those are the ones we want to drop.
   the common approach is to rank the eigenvectors from highest to lowest
   corresponding eigenvalue and choose the top eigenvectors.
# make a list of (eigenvalue, eigenvector) tuples
eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]

# sort the (eigenvalue, eigenvector) tuples from high to low
eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=true)

# visually confirm that the list is correctly sorted by decreasing eigenvalues

print('eigenvalues in decreasing order:\n')
for i in eig_pairs:
    print(i[0])

eigenvalues in decreasing order:

32.2719577997
0.27756686384
5.71450476746e-15
5.71450476746e-15

   note

   if we take a look at the eigenvalues, we can already see that 2
   eigenvalues are close to 0. the reason why these are close to 0 is not
   that they are not informative but it   s due to floating-point
   imprecision. in fact, these two last eigenvalues should be exactly
   zero: in lda, the number of linear discriminants is at most where is
   the number of class labels, since the in-between scatter matrix is the
   sum of matrices with rank 1 or less. note that in the rare case of
   perfect collinearity (all aligned sample points fall on a straight
   line), the covariance matrix would have rank one, which would result in
   only one eigenvector with a nonzero eigenvalue.

   now, let   s express the    explained variance    as percentage:
print('variance explained:\n')
eigv_sum = sum(eig_vals)
for i,j in enumerate(eig_pairs):
    print('eigenvalue {0:}: {1:.2%}'.format(i+1, (j[0]/eigv_sum).real))

variance explained:

eigenvalue 1: 99.15%
eigenvalue 2: 0.85%
eigenvalue 3: 0.00%
eigenvalue 4: 0.00%

   the first eigenpair is by far the most informative one, and we won   t
   loose much information if we would form a 1d-feature spaced based on
   this eigenpair.

4.2. choosing k eigenvectors with the largest eigenvalues

   after sorting the eigenpairs by decreasing eigenvalues, it is now time
   to construct our -dimensional eigenvector matrix (here : based on the 2
   most informative eigenpairs) and thereby reducing the initial
   4-dimensional feature space into a 2-dimensional feature subspace.
w = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))
print('matrix w:\n', w.real)

matrix w:
 [[-0.2049 -0.009 ]
 [-0.3871 -0.589 ]
 [ 0.5465  0.2543]
 [ 0.7138 -0.767 ]]

step 5: transforming the samples onto the new subspace

   in the last step, we use the -dimensional matrix that we just computed
   to transform our samples onto the new subspace via the equation

   .

   (where is a -dimensional matrix representing the samples, and are the
   transformed -dimensional samples in the new subspace).
x_lda = x.dot(w)
assert x_lda.shape == (150,2), "the matrix is not 150x2 dimensional."

from matplotlib import pyplot as plt

def plot_step_lda():

    ax = plt.subplot(111)
    for label,marker,color in zip(
        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):

        plt.scatter(x=x_lda[:,0].real[y == label],
                y=x_lda[:,1].real[y == label],
                marker=marker,
                color=color,
                alpha=0.5,
                label=label_dict[label]
                )

    plt.xlabel('ld1')
    plt.ylabel('ld2')

    leg = plt.legend(loc='upper right', fancybox=true)
    leg.get_frame().set_alpha(0.5)
    plt.title('lda: iris projection onto the first 2 linear discriminants')

    # hide axis ticks
    plt.tick_params(axis="both", which="both", bottom="off", top="off",
            labelbottom="on", left="off", right="off", labelleft="on")

    # remove axis spines
    ax.spines["top"].set_visible(false)
    ax.spines["right"].set_visible(false)
    ax.spines["bottom"].set_visible(false)
    ax.spines["left"].set_visible(false)

    plt.grid()
    plt.tight_layout
    plt.show()

plot_step_lda()

   png

   the scatter plot above represents our new feature subspace that we
   constructed via lda. we can see that the first linear discriminant
      ld1    separates the classes quite nicely. however, the second
   discriminant,    ld2   , does not add much valuable information, which
   we   ve already concluded when we looked at the ranked eigenvalues is
   step 4.

a comparison of pca and lda

   in order to compare the feature subspace that we obtained via the
   id156, we will use the pca class from the
   scikit-learn machine-learning library. the documentation can be found
   here:
   [44]http://scikit-learn.org/stable/modules/generated/sklearn.decomposit
   ion.pca.html.

   for our convenience, we can directly specify to how many components we
   want to retain in our input dataset via the n_components parameter.
n_components : int, none or string

number of components to keep. if n_components is not set all components are kept
:
    n_components == min(n_samples, n_features)
    if n_components ==    id113   , minka   s id113 is used to guess the dimension if 0 <
n_components < 1,
    select the number of components such that the amount of variance that needs
to be explained
    is greater than the percentage specified by n_components

   but before we skip to the results of the respective linear
   transformations, let us quickly recapitulate the purposes of pca and
   lda: pca finds the axes with maximum variance for the whole data set
   where lda tries to find the axes for best class seperability. in
   practice, often a lda is done followed by a pca for dimensionality
   reduction.

from sklearn.decomposition import pca as sklearnpca

sklearn_pca = sklearnpca(n_components=2)
x_pca = sklearn_pca.fit_transform(x)

def plot_pca():

    ax = plt.subplot(111)

    for label,marker,color in zip(
        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):

        plt.scatter(x=x_pca[:,0][y == label],
                y=x_pca[:,1][y == label],
                marker=marker,
                color=color,
                alpha=0.5,
                label=label_dict[label]
                )

    plt.xlabel('pc1')
    plt.ylabel('pc2')

    leg = plt.legend(loc='upper right', fancybox=true)
    leg.get_frame().set_alpha(0.5)
    plt.title('pca: iris projection onto the first 2 principal components')

    # hide axis ticks
    plt.tick_params(axis="both", which="both", bottom="off", top="off",
            labelbottom="on", left="off", right="off", labelleft="on")

    # remove axis spines
    ax.spines["top"].set_visible(false)
    ax.spines["right"].set_visible(false)
    ax.spines["bottom"].set_visible(false)
    ax.spines["left"].set_visible(false)

    plt.tight_layout
    plt.grid()

    plt.show()

plot_pca()
plot_step_lda()

   png

   png

   the two plots above nicely confirm what we have discussed before: where
   the pca accounts for the most variance in the whole dataset, the lda
   gives us the axes that account for the most variance between the
   individual classes.

lda via scikit-learn

   now, after we have seen how an id156 works using
   a step-by-step approach, there is also a more convenient way to achive
   the same via the lda class implemented in the [45]scikit-learn machine
   learning library.
from sklearn.discriminant_analysis import lineardiscriminantanalysis as lda

# lda
sklearn_lda = lda(n_components=2)
x_lda_sklearn = sklearn_lda.fit_transform(x, y)

def plot_scikit_lda(x, title):

    ax = plt.subplot(111)
    for label,marker,color in zip(
        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):

        plt.scatter(x=x[:,0][y == label],
                    y=x[:,1][y == label] * -1, # flip the figure
                    marker=marker,
                    color=color,
                    alpha=0.5,
                    label=label_dict[label])

    plt.xlabel('ld1')
    plt.ylabel('ld2')

    leg = plt.legend(loc='upper right', fancybox=true)
    leg.get_frame().set_alpha(0.5)
    plt.title(title)

    # hide axis ticks
    plt.tick_params(axis="both", which="both", bottom="off", top="off",
            labelbottom="on", left="off", right="off", labelleft="on")

    # remove axis spines
    ax.spines["top"].set_visible(false)
    ax.spines["right"].set_visible(false)
    ax.spines["bottom"].set_visible(false)
    ax.spines["left"].set_visible(false)

    plt.grid()
    plt.tight_layout
    plt.show()

plot_step_lda()
plot_scikit_lda(x_lda_sklearn, title='default lda via scikit-learn')

   png

   png

a note about standardization

   to follow up on a question that i received recently, i wanted to
   clarify that feature scaling such as [standardization] does not change
   the overall results of an lda and thus may be optional. yes, the
   scatter matrices will be different depending on whether the features
   were scaled or not. in addition, the eigenvectors will be different as
   well. however, the important part is that the eigenvalues will be
   exactly the same as well as the final projects     the only difference
   you   ll notice is the scaling of the component axes. this can be shown
   mathematically (i will insert the formulaes some time in future), and
   below is a practical, visual example for demonstration.
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt

import pandas as pd

df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris
/iris.data', header=none)
df[4] = df[4].map({'iris-setosa':0, 'iris-versicolor':1, 'iris-virginica':2})
df.tail()

        0   1   2   3  4
   145 6.7 3.0 5.2 2.3 2
   146 6.3 2.5 5.0 1.9 2
   147 6.5 3.0 5.2 2.0 2
   148 6.2 3.4 5.4 2.3 2
   149 5.9 3.0 5.1 1.8 2

   after loading the dataset, we are going to standardize the columns in
   x. standardization implies mean centering and scaling to unit variance:

   after standardization, the columns will have zero mean ( ) and a
   standard deviation of 1 ().
y, x = df.iloc[:, 4].values, df.iloc[:, 0:4].values
x_cent = x - x.mean(axis=0)
x_std = x_cent / x.std(axis=0)

   below, i simply copied the individual steps of an lda, which we
   discussed previously, into python functions for convenience.
import numpy as np

def comp_mean_vectors(x, y):
    class_labels = np.unique(y)
    n_classes = class_labels.shape[0]
    mean_vectors = []
    for cl in class_labels:
        mean_vectors.append(np.mean(x[y==cl], axis=0))
    return mean_vectors

def scatter_within(x, y):
    class_labels = np.unique(y)
    n_classes = class_labels.shape[0]
    n_features = x.shape[1]
    mean_vectors = comp_mean_vectors(x, y)
    s_w = np.zeros((n_features, n_features))
    for cl, mv in zip(class_labels, mean_vectors):
        class_sc_mat = np.zeros((n_features, n_features))
        for row in x[y == cl]:
            row, mv = row.reshape(n_features, 1), mv.reshape(n_features, 1)
            class_sc_mat += (row-mv).dot((row-mv).t)
        s_w += class_sc_mat
    return s_w

def scatter_between(x, y):
    overall_mean = np.mean(x, axis=0)
    n_features = x.shape[1]
    mean_vectors = comp_mean_vectors(x, y)
    s_b = np.zeros((n_features, n_features))
    for i, mean_vec in enumerate(mean_vectors):
        n = x[y==i+1,:].shape[0]
        mean_vec = mean_vec.reshape(n_features, 1)
        overall_mean = overall_mean.reshape(n_features, 1)
        s_b += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).t)
    return s_b

def get_components(eig_vals, eig_vecs, n_comp=2):
    n_features = x.shape[1]
    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_val
s))]
    eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=true)
    w = np.hstack([eig_pairs[i][1].reshape(4, 1) for i in range(0, n_comp)])
    return w

   first, we are going to print the eigenvalues, eigenvectors,
   transformation matrix of the un-scaled data:
s_w, s_b = scatter_within(x, y), scatter_between(x, y)
eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(s_w).dot(s_b))
w = get_components(eig_vals, eig_vecs, n_comp=2)
print('eigvals: %s\n\neigvecs: %s' % (eig_vals, eig_vecs))
print('\nw: %s' % w)

eigvals: [  2.0905e+01 +0.0000e+00j   1.4283e-01 +0.0000e+00j
  -2.8680e-16 +1.9364e-15j  -2.8680e-16 -1.9364e-15j]

eigvecs: [[ 0.2067+0.j      0.0018+0.j      0.4846-0.4436j  0.4846+0.4436j]
 [ 0.4159+0.j     -0.5626+0.j      0.0599+0.1958j  0.0599-0.1958j]
 [-0.5616+0.j      0.2232+0.j      0.1194+0.1929j  0.1194-0.1929j]
 [-0.6848+0.j     -0.7960+0.j     -0.6892+0.j     -0.6892-0.j    ]]

w: [[ 0.2067+0.j  0.0018+0.j]
 [ 0.4159+0.j -0.5626+0.j]
 [-0.5616+0.j  0.2232+0.j]
 [-0.6848+0.j -0.7960+0.j]]

x_lda = x.dot(w)
for label,marker,color in zip(
        np.unique(y),('^', 's', 'o'),('blue', 'red', 'green')):
    plt.scatter(x_lda[y==label, 0], x_lda[y==label, 1],
                color=color, marker=marker)

/users/sebastian/miniconda3/lib/python3.5/site-packages/numpy/core/numeric.py:52
5: complexwarning: casting complex values to real discards the imaginary part
  return array(a, dtype, copy=false, order=order, subok=true)

   png

   next, we are repeating this process for the standarized flower dataset:
s_w, s_b = scatter_within(x_std, y), scatter_between(x_std, y)
eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(s_w).dot(s_b))
w_std = get_components(eig_vals, eig_vecs, n_comp=2)
print('eigvals: %s\n\neigvecs: %s' % (eig_vals, eig_vecs))
print('\nw: %s' % w_std)

eigvals: [  2.0905e+01   1.4283e-01  -6.7207e-16   1.1082e-15]

eigvecs: [[ 0.1492 -0.0019  0.8194 -0.3704]
 [ 0.1572  0.3193 -0.1382 -0.0884]
 [-0.8635 -0.5155 -0.5078 -0.5106]
 [-0.4554  0.7952 -0.2271  0.7709]]

w: [[ 0.1492 -0.0019]
 [ 0.1572  0.3193]
 [-0.8635 -0.5155]
 [-0.4554  0.7952]]

x_std_lda = x_std.dot(w_std)
x_std_lda[:, 1] = x_std_lda[:, 1]
for label,marker,color in zip(
        np.unique(y),('^', 's', 'o'),('blue', 'red', 'green')):
    plt.scatter(x_std_lda[y==label, 0], x_std_lda[y==label, 1],
                color=color, marker=marker)

   png

   as we can see, the eigenvalues are excactly the same whether we scaled
   our data or not (note that since has a rank of 2, the two lowest
   eigenvalues in this 4-dimensional dataset should effectively be 0).
   furthermore, we see that the projections look identical except for the
   different scaling of the component axes and that it is mirrored in this
   case.

   [46]q

      2013-2019 sebastian raschka

references

   visible links
   1. http://sebastianraschka.com/
   2. https://sebastianraschka.com/about.html
   3. http://sebastianraschka.com/blog/index.html
   4. https://sebastianraschka.com/books.html
   5. https://sebastianraschka.com/elsewhere.html
   6. https://sebastianraschka.com/resources.html
   7. http://stat.wisc.edu/~sraschka/publications/
   8. http://stat.wisc.edu/~sraschka/teaching
   9. http://stat.wisc.edu/~sraschka/software
  10. http://sebastianraschka.com/rss_feed.xml
  11. http://sebastianraschka.com/articles/2014_python_lda.html#sections
  12. http://sebastianraschka.com/articles/2014_python_lda.html#introduction
  13. http://sebastianraschka.com/articles/2014_python_lda.html#principal-component-analysis-vs-linear-discriminant-analysis
  14. http://sebastianraschka.com/articles/2014_python_lda.html#what-is-a-good-feature-subspace
  15. http://sebastianraschka.com/articles/2014_python_lda.html#summarizing-the-lda-approach-in-5-steps
  16. http://sebastianraschka.com/articles/2014_python_lda.html#preparing-the-sample-data-set
  17. http://sebastianraschka.com/articles/2014_python_lda.html#about-the-iris-dataset
  18. http://sebastianraschka.com/articles/2014_python_lda.html#reading-in-the-dataset
  19. http://sebastianraschka.com/articles/2014_python_lda.html#histograms-and-feature-selection
  20. http://sebastianraschka.com/articles/2014_python_lda.html#normality-assumptions
  21. http://sebastianraschka.com/articles/2014_python_lda.html#lda-in-5-steps
  22. http://sebastianraschka.com/articles/2014_python_lda.html#step-1-computing-the-d-dimensional-mean-vectors
  23. http://sebastianraschka.com/articles/2014_python_lda.html#step-2-computing-the-scatter-matrices
  24. http://sebastianraschka.com/articles/2014_python_lda.html#21-within-class-scatter-matrix-s_w
  25. http://sebastianraschka.com/articles/2014_python_lda.html#21-b
  26. http://sebastianraschka.com/articles/2014_python_lda.html#22-between-class-scatter-matrix-s_b
  27. http://sebastianraschka.com/articles/2014_python_lda.html#step-3-solving-the-generalized-eigenvalue-problem-for-the-matrix-s_w-1s_b
  28. http://sebastianraschka.com/articles/2014_python_lda.html#checking-the-eigenvector-eigenvalue-calculation
  29. http://sebastianraschka.com/articles/2014_python_lda.html#step-4-selecting-linear-discriminants-for-the-new-feature-subspace
  30. http://sebastianraschka.com/articles/2014_python_lda.html#41-sorting-the-eigenvectors-by-decreasing-eigenvalues
  31. http://sebastianraschka.com/articles/2014_python_lda.html#42-choosing-k-eigenvectors-with-the-largest-eigenvalues
  32. http://sebastianraschka.com/articles/2014_python_lda.html#step-5-transforming-the-samples-onto-the-new-subspace
  33. http://sebastianraschka.com/articles/2014_python_lda.html#a-comparison-of-pca-and-lda
  34. http://sebastianraschka.com/articles/2014_python_lda.html#lda-via-scikit-learn
  35. http://sebastianraschka.com/articles/2014_python_lda.html#a-note-about-standardization
  36. http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/abstract
  37. http://www.jstor.org/stable/2983775
  38. http://sebastianraschka.com/articles/2014_pca_step_by_step.html
  39. http://ieeexplore.ieee.org/xpl/articledetails.jsp?arnumber=908974
  40. http://rasbt.github.io/mlxtend/user_guide/feature_selection/sequentialfeatureselector/
  41. http://scikit-learn.org/stable/modules/feature_selection.html
  42. https://sebastianraschka.com/faq/docs/feature_sele_categories.html
  43. http://link.springer.com/article/10.1007/s10115-006-0013-y
  44. http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.pca.html
  45. http://scikit-learn.org/stable/
  46. https://www.quora.com/profile/sebastian-raschka-1

   hidden links:
  48. http://sebastianraschka.com/articles/2014_python_lda.html
  49. http://sebastianraschka.com/email.html
  50. https://twitter.com/rasbt
  51. https://github.com/rasbt
  52. https://scholar.google.com/citations?user=x4rcc0iaaaaj&hl=enrasbt
  53. https://linkedin.com/in/sebastianraschka
