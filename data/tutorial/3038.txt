   #[1]rss

   [ ]

   my journey learning data science.

   [2]home [3]about [4]archives [5]running [6]talks

      2019. all rights reserved.

[7]jean-nicholas hould on data science

tidy data in python

   06 dec 2016

   i recently came across a paper named [8]tidy data by hadley wickham.
   published back in 2014, the paper focuses on one aspect of cleaning up
   data, tidying data: structuring datasets to facilitate analysis.
   through the paper, wickham demonstrates how any dataset can be
   structured in a standardized way prior to analysis. he presents in
   detail the different types of data sets and how to wrangle them into a
   standard format.

   as a data scientist, i think you should get very familiar with this
   standardized structure of a dataset. data cleaning is one the most
   frequent task in data science. no matter what kind of data you are
   dealing with or what kind of analysis you are performing, you will have
   to clean the data at some point. tidying your data in a standard format
   makes things easier down the road. you can reuse a standard set of
   tools across your different analysis.

   in this post, i will summarize some tidying examples wickham uses in
   his paper and i will demonstrate how to do so using the python
   [9]pandas library.

defining tidy data

   the structure wickham defines as tidy has the following attributes:
     * each variable forms a column and contains values
     * each observation forms a row
     * each type of observational unit forms a table

   a few definitions:
     * variable: a measurement or an attribute. height, weight, sex, etc.
     * value: the actual measurement or attribute. 152 cm, 80 kg, female,
       etc.
     * observation: all values measure on the same unit. each person.

   an example of a messy dataset:
                treatment a treatment b
    john smith  -           2
     jane doe   16          11
   mary johnson 3           1

   an example of a tidy dataset:
       name     treatment result
    john smith  a         -
     jane doe   a         16
   mary johnson a         3
    john smith  b         2
     jane doe   b         11
   mary johnson b         1

tidying messy datasets

   through the following examples extracted from wickham   s paper, we   ll
   wrangle messy datasets into the tidy format. the goal here is not to
   analyze the datasets but rather prepare them in a standardized way
   prior to the analysis. these are the five types of messy datasets we   ll
   tackle:
     * column headers are values, not variable names.
     * multiple variables are stored in one column.
     * variables are stored in both rows and columns.
     * multiple types of observational units are stored in the same table.
     * a single observational unit is stored in multiple tables.

   note: all of the code presented in this post is available on
   [10]github.

column headers are values, not variable names

pew research center dataset

   this dataset explores the relationship between income and religion.

   problem: the columns headers are composed of the possible income
   values.
import pandas as pd
import datetime
from os import listdir
from os.path import isfile, join
import glob
import re

df = pd.read_csv("./data/pew-raw.csv")
df

          religion         <$10k $10-20k $20-30k $30-40k $40-50k $50-75k
   agnostic                27    34      60      81      76      137
   atheist                 12    27      37      52      35      70
   buddhist                27    21      30      34      33      58
   catholic                418   617     732     670     638     1116
   dont know/refused       15    14      15      11      10      35
   evangelical prot        575   869     1064    982     881     1486
   hindu                   1     9       7       9       11      34
   historically black prot 228   244     236     238     197     223
   jehovahs witness        20    27      24      24      21      30
   jewish                  19    19      25      25      30      95

   a tidy version of this dataset is one in which the income values would
   not be columns headers but rather values in an income column. in order
   to tidy this dataset, we need to [11]melt it. the pandas library has a
   built-in function that allows to do just that. it    unpivots    a
   dataframe from a wide format to a long format. we   ll reuse this
   function a few times through the post.
formatted_df = pd.melt(df,
                       ["religion"],
                       var_name="income",
                       value_name="freq")
formatted_df = formatted_df.sort_values(by=["religion"])
formatted_df.head(10)

   this outputs a tidy version of the dataset:
   religion income  freq
   agnostic <$10k   27
   agnostic $30-40k 81
   agnostic $40-50k 76
   agnostic $50-75k 137
   agnostic $10-20k 34
   agnostic $20-30k 60
   atheist  $40-50k 35
   atheist  $20-30k 37
   atheist  $10-20k 27
   atheist  $30-40k 52

billboard top 100 dataset

   this dataset represents the weekly rank of songs from the moment they
   enter the billboard top 100 to the subsequent 75 weeks.

   problems:
     * the columns headers are composed of values: the week number
       (x1st.week,    )
     * if a song is in the top 100 for less than 75 weeks, the remaining
       columns are filled with missing values.

df = pd.read_csv("./data/billboard.csv", encoding="mac_latin2")
df.head(10)

   year artist.inverted track time genre date.entered date.peaked
   x1st.week x2nd.week ...
   2000 destiny's child independent women part i 3:38 rock 2000-09-23
   2000-11-18 78 63.0 ...
   2000 santana maria, maria 4:18 rock 2000-02-12 2000-04-08 15 8.0 ...
   2000 savage garden i knew i loved you 4:07 rock 1999-10-23 2000-01-29
   71 48.0 ...
   2000 madonna music 3:45 rock 2000-08-12 2000-09-16 41 23.0 ...
   2000 aguilera, christina come on over baby (all i want is you) 3:38
   rock 2000-08-05 2000-10-14 57 47.0 ...
   2000 janet doesn't really matter 4:17 rock 2000-06-17 2000-08-26 59
   52.0 ...
   2000 destiny's child say my name 4:31 rock 1999-12-25 2000-03-18 83
   83.0 ...
   2000 iglesias, enrique be with you 3:36 latin 2000-04-01 2000-06-24 63
   45.0 ...
   2000 sisqo incomplete 3:52 rock 2000-06-24 2000-08-12 77 66.0 ...
   2000 lonestar amazed 4:25 country 1999-06-05 2000-03-04 81 54.0 ...

   a tidy version of this dataset is one without the week   s numbers as
   columns but rather as values of a single column. in order to do so,
   we   ll melt the weeks columns into a single date column. we will create
   one row per week for each record. if there is no data for the given
   week, we will not create a row.
# melting
id_vars = ["year",
           "artist.inverted",
           "track",
           "time",
           "genre",
           "date.entered",
           "date.peaked"]

df = pd.melt(frame=df,id_vars=id_vars, var_name="week", value_name="rank")

# formatting
df["week"] = df['week'].str.extract('(\d+)', expand=false).astype(int)
df["rank"] = df["rank"].astype(int)

# cleaning out unnecessary rows
df = df.dropna()

# create "date" columns
df['date'] = pd.to_datetime(df['date.entered']) + pd.to_timedelta(df['week'], un
it='w') - pd.dateoffset(weeks=1)

df = df[["year",
         "artist.inverted",
         "track",
         "time",
         "genre",
         "week",
         "rank",
         "date"]]
df = df.sort_values(ascending=true, by=["year","artist.inverted","track","week",
"rank"])

# assigning the tidy dataset to a variable for future usage
billboard = df

df.head(10)

   a tidier version of the dataset is shown below. there is still a lot of
   repetition of the song details: the track name, time and genre. for
   this reason, this dataset is still not completely tidy as per wickham   s
   definition. we will address this in the next example.
   year artist.inverted track time genre week rank date
   2000 2 pac baby don't cry (keep ya head up ii) 4:22 rap 1 87 2000-02-26
   2000 2 pac baby don't cry (keep ya head up ii) 4:22 rap 2 82 2000-03-04
   2000 2 pac baby don't cry (keep ya head up ii) 4:22 rap 3 72 2000-03-11
   2000 2 pac baby don't cry (keep ya head up ii) 4:22 rap 4 77 2000-03-18
   2000 2 pac baby don't cry (keep ya head up ii) 4:22 rap 5 87 2000-03-25
   2000 2 pac baby don't cry (keep ya head up ii) 4:22 rap 6 94 2000-04-01
   2000 2 pac baby don't cry (keep ya head up ii) 4:22 rap 7 99 2000-04-08
   2000 2ge+her the hardest part of breaking up (is getting ba... 3:15 r&b
   1 91 2000-09-02
   2000 2ge+her the hardest part of breaking up (is getting ba... 3:15 r&b
   2 87 2000-09-09
   2000 2ge+her the hardest part of breaking up (is getting ba... 3:15 r&b
   3 92 2000-09-16

multiple types in one table

   following up on the billboard dataset, we   ll now address the repetition
   problem of the previous table.

   problems:
     * multiple observational units (the song and its rank) in a single
       table.

   we   ll first create a songs table which contains the details of each
   song:
songs_cols = ["year", "artist.inverted", "track", "time", "genre"]
songs = billboard[songs_cols].drop_duplicates()
songs = songs.reset_index(drop=true)
songs["song_id"] = songs.index
songs.head(10)

   year artist.inverted track time genre song_id
   2000 2 pac baby don't cry (keep ya head up ii) 4:22 rap 0
   2000 2ge+her the hardest part of breaking up (is getting ba... 3:15 r&b
   1
   2000 3 doors down kryptonite 3:53 rock 2
   2000 3 doors down loser 4:24 rock 3
   2000 504 boyz wobble wobble 3:35 rap 4
   2000 98    give me just one night (una noche) 3:24 rock 5
   2000 a*teens dancing queen 3:44 pop 6
   2000 aaliyah i don't wanna 4:15 rock 7
   2000 aaliyah try again 4:03 rock 8
   2000 adams, yolanda open my heart 5:30 gospel 9

   we   ll then create a ranks table which only contains the song_id, date
   and the rank.
ranks = pd.merge(billboard, songs, on=["year","artist.inverted", "track", "time"
, "genre"])
ranks = ranks[["song_id", "date","rank"]]
ranks.head(10)

   song_id    date    rank
   0       2000-02-26 87
   0       2000-03-04 82
   0       2000-03-11 72
   0       2000-03-18 77
   0       2000-03-25 87
   0       2000-04-01 94
   0       2000-04-08 99
   1       2000-09-02 91
   1       2000-09-09 87
   1       2000-09-16 92

multiple variables stored in one column

tubercolosis records from world health organization

   this dataset documents the count of confirmed tuberculosis cases by
   country, year, age and sex.

   problems:
     * some columns contain multiple values: sex and age.
     * mixture of zeros and missing values nan. this is due to the data
       collection process and the distinction is important for this
       dataset.

df = pd.read_csv("./data/tb-raw.csv")
df

   country year m014 m1524 m2534 m3544 m4554 m5564 m65 mu  f014
   ad      2000 0    0     1     0     0     0     0   nan nan
   ae      2000 2    4     4     6     5     12    10  nan 3
   af      2000 52   228   183   149   129   94    80  nan 93
   ag      2000 0    0     0     0     0     0     1   nan 1
   al      2000 2    19    21    14    24    19    16  nan 3
   am      2000 2    152   130   131   63    26    21  nan 1
   an      2000 0    0     1     2     0     0     0   nan 0
   ao      2000 186  999   1003  912   482   312   194 nan 247
   ar      2000 97   278   594   402   419   368   330 nan 121
   as      2000 nan  nan   nan   nan   1     1     nan nan nan

   in order to tidy this dataset, we need to remove the different values
   from the header and unpivot them into rows. we   ll first need to melt
   the sex + age group columns into a single one. once we have that single
   column, we   ll derive three columns from it: sex, age_lower and
   age_upper. with those, we   ll be able to properly build a tidy dataset.
df = pd.melt(df, id_vars=["country","year"], value_name="cases", var_name="sex_a
nd_age")

# extract sex, age lower bound and age upper bound group
tmp_df = df["sex_and_age"].str.extract("(\d)(\d+)(\d{2})")

# name columns
tmp_df.columns = ["sex", "age_lower", "age_upper"]

# create `age`column based on `age_lower` and `age_upper`
tmp_df["age"] = tmp_df["age_lower"] + "-" + tmp_df["age_upper"]

# merge
df = pd.concat([df, tmp_df], axis=1)

# drop unnecessary columns and rows
df = df.drop(['sex_and_age',"age_lower","age_upper"], axis=1)
df = df.dropna()
df = df.sort(ascending=true,columns=["country", "year", "sex", "age"])
df.head(10)

   this results in a tidy dataset.
   country year cases sex  age
   ad      2000 0     m   0-14
   ad      2000 0     m   15-24
   ad      2000 1     m   25-34
   ad      2000 0     m   35-44
   ad      2000 0     m   45-54
   ad      2000 0     m   55-64
   ae      2000 3     f   0-14
   ae      2000 2     m   0-14
   ae      2000 4     m   15-24
   ae      2000 4     m   25-34

variables are stored in both rows and columns

global historical climatology network dataset

   this dataset represents the daily weather records for a weather station
   (mx17004) in mexico for five months in 2010.

   problems:
     * variables are stored in both rows (tmin, tmax) and columns (days).

df = pd.read_csv("./data/weather-raw.csv")
df

     id    year month element d1   d2   d3  d4   d5  d6  d7  d8
   mx17004 2010 1     tmax    nan nan  nan  nan nan  nan nan nan
   mx17004 2010 1     tmin    nan nan  nan  nan nan  nan nan nan
   mx17004 2010 2     tmax    nan 27.3 24.1 nan nan  nan nan nan
   mx17004 2010 2     tmin    nan 14.4 14.4 nan nan  nan nan nan
   mx17004 2010 3     tmax    nan nan  nan  nan 32.1 nan nan nan
   mx17004 2010 3     tmin    nan nan  nan  nan 14.2 nan nan nan
   mx17004 2010 4     tmax    nan nan  nan  nan nan  nan nan nan
   mx17004 2010 4     tmin    nan nan  nan  nan nan  nan nan nan
   mx17004 2010 5     tmax    nan nan  nan  nan nan  nan nan nan
   mx17004 2010 5     tmin    nan nan  nan  nan nan  nan nan nan

   in order to make this dataset tidy, we want to move the three misplaced
   variables (tmin, tmax and days) as three individual columns: tmin. tmax
   and date.
# extracting day
df["day"] = df["day_raw"].str.extract("d(\d+)", expand=false)
df["id"] = "mx17004"

# to numeric values
df[["year","month","day"]] = df[["year","month","day"]].apply(lambda x: pd.to_nu
meric(x, errors='ignore'))

# creating a date from the different columns
def create_date_from_year_month_day(row):
    return datetime.datetime(year=row["year"], month=int(row["month"]), day=row[
"day"])

df["date"] = df.apply(lambda row: create_date_from_year_month_day(row), axis=1)
df = df.drop(['year',"month","day", "day_raw"], axis=1)
df = df.dropna()

# unmelting column "element"
df = df.pivot_table(index=["id","date"], columns="element", values="value")
df.reset_index(drop=false, inplace=true)
df

     id       date    tmax tmin
   mx17004 2010-02-02 27.3 14.4
   mx17004 2010-02-03 24.1 14.4
   mx17004 2010-03-05 32.1 14.2

one type in multiple tables

   dataset: illinois male baby names for the year 2014/2015.

   problems:
     * the data is spread across multiple tables/files.
     * the    year    variable is present in the file name.

   in order to load those different files into a single dataframe, we can
   run a custom script that will append the files together. furthermore,
   we   ll need to extract the    year    variable from the file name.
def extract_year(string):
    match = re.match(".+(\d{4})", string)
    if match != none: return match.group(1)

path = './data'
allfiles = glob.glob(path + "/201*-baby-names-illinois.csv")
frame = pd.dataframe()
df_list= []
for file_ in allfiles:
    df = pd.read_csv(file_,index_col=none, header=0)
    df.columns = map(str.lower, df.columns)
    df["year"] = extract_year(file_)
    df_list.append(df)

df = pd.concat(df_list)
df.head(5)

   rank   name    frequency sex  year
   1    noah      837       male 2014
   2    alexander 747       male 2014
   3    william   687       male 2014
   4    michael   680       male 2014
   5    liam      670       male 2014

final thoughts

   in this post, i focused on one aspect of wickham   s paper, the data
   manipulation part. my main goal was to demonstrate the data
   manipulations in python. it   s important to mention that there is a
   significant section of his paper that covers the tools and
   visualizations from which you can benefit by tidying your dataset. i
   did not cover those in this post.

   overall, i enjoyed preparing this post and wrangling the datasets into
   a streamlined format. the defined format makes it easier to query and
   filter the data. this approach makes it easier to reuse libraries and
   code across analysis. it also makes it easier to share a dataset with
   other data analysts.

related posts

     * [12]profiling a dataset of craft beers 23 apr 2017
     * [13]what is a data scientist? 06 feb 2017
     * [14]scraping for craft beers 17 jan 2017

references

   1. http://www.jeannicholashould.com/atom.xml
   2. http://www.jeannicholashould.com/
   3. http://www.jeannicholashould.com/about.html
   4. http://www.jeannicholashould.com/archive.html
   5. http://www.jeannicholashould.com/marathons.html
   6. http://www.jeannicholashould.com/talks.html
   7. http://www.jeannicholashould.com/
   8. http://vita.had.co.nz/papers/tidy-data.pdf
   9. http://pandas.pydata.org/
  10. https://github.com/nickhould/tidy-data-python
  11. http://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html
  12. http://www.jeannicholashould.com/profiling-a-dataset-of-craft-beers.html
  13. http://www.jeannicholashould.com/what-is-a-data-scientist.html
  14. http://www.jeannicholashould.com/python-web-scraping-tutorial-for-craft-beers.html
