   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]chatbots life
     * [9]     start a project
     * [10]     tools
     * [11]    business
     * [12]     voice
     * [13]     tutorials
     * [14]     design
     * [15]            ai & nlp
     * [16]     ai for shopify
     __________________________________________________________________

training mxnet         part 1: mnist

   [17]go to the profile of julien simon
   [18]julien simon (button) blockedunblock (button) followfollowing
   apr 18, 2017

   in a [19]previous series, we discovered how we could use the [20]mxnet
   library and [21]pre-trained models for id164. in this
   series, we   re going to focus on training models with a number of
   different data sets.

   let   s start with the famous mnist data set.

   please note that is an updated and expanded version of this
   [22]tutorial: i   m using the module api (instead of the deprecated model
   api) as well as the mnist data iterator.

the mnist data set

   this [23]data is a set of 28x28 greyscale images representing
   handwritten digits (0 to 9).
   [1*ybdjcrwijgom7pwu-lnw6q.png]
   samples from the mnist data set

   the training set has 60,000 samples and the test set has 10,000
   examples. let   s download them right away.
# training set: images and labels
$ wget [24]http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
$ wget [25]http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
# validation set: images and labels
$ wget [26]http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
$ wget [27]http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
$ gzip -d *

   how about we take a look inside these files? we   ll start with the
   labels. they are stored as a serialized [28]numpy array holding 60,000
   unsigned bytes.

   the file starts with a big-endian packed structure, holding 2 integers:
   magic number, number of labels.
>>> import struct
>>> import numpy as np
>>> import cv2
>>> labelfile = open("train-labels-idx1-ubyte")
>>> magic, num = struct.unpack(">ii", labelfile.read(8))
>>> labelarray = np.fromstring(labelfile.read(), dtype=np.int8)
>>> print labelarray.shape
>>> print labelarray[0:10]
(60000,)
[5 0 4 1 9 2 1 3 1 4]

   let   s now extract some images. again, they are stored as a serialized
   [29]numpy array, which we will reshape to build 28x28 images. each
   pixel is stored as an unsigned byte (0 for black, 255 for white).

   the file starts with a big-endian packed structure, holding 4 integers:
   magic number, number of images, number of rows and number of columns.
>>> imagefile = open("train-images-idx3-ubyte")
>>> magic, num, rows, cols = struct.unpack(">iiii", imagefile.read(16))
>>> imagearray = np.fromstring(imagefile.read(), dtype=np.uint8)
>>> print imagearray.shape
(47040000,)
>>> imagearray = imagearray.reshape(num, rows, cols)
>>> print imagearray.shape
(60000, 28, 28)

   let   s save the first 10 images to disk.
>>> for i in range(0,10):
        img = imagearray[i]
        imgname = "img"+(str)(i)+".png"
        cv2.imwrite(imgname, img)
$ ls img?.png
img0.png img1.png img2.png img3.png img4.png img5.png img6.png img7.png img8.png
 img9.png

   this is how they look.
   [1*3q5hphgjv17tevewnlt6qw.png]

   ok, now that we understand the data, let   s build a model.

building a model

   we   re going to use a simple multi-layer id88 (similar to what we
   built [30]here) : 784     128     64     10
     * input layer: an array of 784 pixel values (28x28).
     * first layer: 128 neurons activated by the [31]rectifier linear unit
       function.
     * second layer: 64 neurons activated by the same function.
     * output layer: 10 neurons (for our 10 categories), activated by the
       [32]softmax function in order to transform the 10 outputs into 10
       values between 0 and 1 that add up to 1. each value represents the
       predicted id203 for each category, the largest one pointing
       at the most likely category.

data = mx.sym.variable('data')
data = mx.sym.flatten(data=data)
fc1  = mx.sym.fullyconnected(data=data, name='fc1', num_hidden=128)
act1 = mx.sym.activation(data=fc1, name='relu1', act_type="relu")
fc2  = mx.sym.fullyconnected(data=act1, name='fc2', num_hidden = 64)
act2 = mx.sym.activation(data=fc2, name='relu2', act_type="relu")
fc3  = mx.sym.fullyconnected(data=act2, name='fc3', num_hidden=10)
mlp  = mx.sym.softmaxoutput(data=fc3, name='softmax')mod = mx.mod.module(mlp)

building a data iterator

   mxnet conveniently provides a [33]data iterator for the mnist data set.
   thanks to this, we don   t have to open the files, build ndarrays, etc.
   it also has default parameters for filenames and so on. very cool!
train_iter = mx.io.mnistiter(shuffle=true)
val_iter = mx.io.mnistiter(image="./t10k-images-idx3-ubyte", label="./t10k-label
s-idx1-ubyte")

   we can now bind the data to our model. default batch size is 128.
mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_la
bel)

   we   re now ready for training.

training the model

   let   s start with default settings for weight initialization and
   optimization (aka hyperparameters) and hope for the best. here we go!
>>> import logging
>>> logging.basicconfig(level=logging.info)
>>> mod.fit(train_iter, num_epoch=10)
info:root:epoch[0] train-accuracy=0.111662
info:root:epoch[0] time cost=1.244
info:root:epoch[1] train-accuracy=0.112346
info:root:epoch[1] time cost=1.376
info:root:epoch[2] train-accuracy=0.112346
info:root:epoch[2] time cost=1.254
info:root:epoch[3] train-accuracy=0.112346
info:root:epoch[3] time cost=1.296
info:root:epoch[4] train-accuracy=0.112346
info:root:epoch[4] time cost=1.234
info:root:epoch[5] train-accuracy=0.112346
info:root:epoch[5] time cost=1.283
info:root:epoch[6] train-accuracy=0.112346
info:root:epoch[6] time cost=1.440
info:root:epoch[7] train-accuracy=0.112346
info:root:epoch[7] time cost=1.237
info:root:epoch[8] train-accuracy=0.112346
info:root:epoch[8] time cost=1.235
info:root:epoch[9] train-accuracy=0.112346
info:root:epoch[9] time cost=1.307

   id48, things are not going well. it looks like the network is not
   learning. actually, it is learning, but real slow: the default learning
   rate is 0.01, which is too low. let   s use a more reasonable value such
   as 0.1.
>>> mod.init_params()
>>> mod.init_optimizer(optimizer_params=(('learning_rate', 0.1), ))
>>> mod.fit(train_iter, num_epoch=10)
info:root:epoch[0] train-accuracy=0.111846
info:root:epoch[0] time cost=1.288
info:root:epoch[1] train-accuracy=0.427150
info:root:epoch[1] time cost=1.308
info:root:epoch[2] train-accuracy=0.842682
info:root:epoch[2] time cost=1.271
info:root:epoch[3] train-accuracy=0.900875
info:root:epoch[3] time cost=1.282
info:root:epoch[4] train-accuracy=0.928736
info:root:epoch[4] time cost=1.288
info:root:epoch[5] train-accuracy=0.944478
info:root:epoch[5] time cost=1.296
info:root:epoch[6] train-accuracy=0.953993
info:root:epoch[6] time cost=1.287
info:root:epoch[7] train-accuracy=0.960453
info:root:epoch[7] time cost=1.294
info:root:epoch[8] train-accuracy=0.965478
info:root:epoch[8] time cost=1.297
info:root:epoch[9] train-accuracy=0.969267
info:root:epoch[9] time cost=1.291

   that   s more like it. we get to 96.93% accuracy after 10 epochs. what
   about validation accuracy? let   s create a metric and score our
   validation data set.
>> metric = mx.metric.accuracy()
>> mod.score(val_iter, metric)
>> print metric.get()
('accuracy', 0.9654447115384616)

   pretty good accuracy at 96.5%.

   still, the first few training epochs are not great: this is caused by
   default weight initialization. let   s use something [34]smarter, like
   the xavier technique.
>>> mod.init_params(initializer=mx.init.xavier(magnitude=2.))
>>> mod.init_optimizer(optimizer_params=(('learning_rate', 0.1), ))
>>> mod.fit(train_iter, num_epoch=10)
info:root:epoch[0] train-accuracy=0.860994
info:root:epoch[0] time cost=1.338
info:root:epoch[1] train-accuracy=0.935797
info:root:epoch[1] time cost=1.325
info:root:epoch[2] train-accuracy=0.951840
info:root:epoch[2] time cost=1.273
info:root:epoch[3] train-accuracy=0.961438
info:root:epoch[3] time cost=1.264
info:root:epoch[4] train-accuracy=0.968066
info:root:epoch[4] time cost=1.250
info:root:epoch[5] train-accuracy=0.973174
info:root:epoch[5] time cost=1.299
info:root:epoch[6] train-accuracy=0.976846
info:root:epoch[6] time cost=1.374
info:root:epoch[7] train-accuracy=0.979601
info:root:epoch[7] time cost=1.407
info:root:epoch[8] train-accuracy=0.982121
info:root:epoch[8] time cost=1.336
info:root:epoch[9] train-accuracy=0.983958
info:root:epoch[9] time cost=1.343
>> metric = mx.metric.accuracy()
>> mod.score(val_iter, metric)
>> print metric.get()
('accuracy', 0.9744591346153846)

   that   s much better: we get to 86% accuracy after only one epoch. we
   gain almost 1.5% training accuracy and 1% validation accuracy.

   can we get better results? well, we could always try to train the model
   longer. let   s try 50 epochs.
...
info:root:epoch[39] train-accuracy=0.999950
info:root:epoch[39] time cost=1.284
info:root:epoch[40] train-accuracy=0.999967
info:root:epoch[40] time cost=1.301
info:root:epoch[41] train-accuracy=0.999967
info:root:epoch[41] time cost=1.811
info:root:epoch[42] train-accuracy=1.000000
info:root:epoch[42] time cost=1.412
info:root:epoch[43] train-accuracy=1.000000
info:root:epoch[43] time cost=1.275
info:root:epoch[44] train-accuracy=1.000000
info:root:epoch[44] time cost=1.200
...
('accuracy', 0.9785657051282052)

   as you can see, we hit 100% training accuracy after 42 epochs and
   there   s no point in going any further. in the process, we only manage
   to improve validation accuracy by 0.4%.

   is this the best we can do? we could try [35]other optimizers, but
   unless you really know what you   re doing, it   s probably safer to stick
   to sgd.

   maybe we simply need a bigger boat?

using a deeper network

   let   s try this network and see what happens :

   784     256     128     64     10
data = mx.sym.variable('data')
data = mx.sym.flatten(data=data)
fc1  = mx.sym.fullyconnected(data=data, name='fc1', num_hidden=256)
act1 = mx.sym.activation(data=fc1, name='relu1', act_type="relu")
fc2  = mx.sym.fullyconnected(data=act1, name='fc2', num_hidden = 128)
act2 = mx.sym.activation(data=fc2, name='relu2', act_type="relu")
fc3  = mx.sym.fullyconnected(data=act2, name='fc3', num_hidden = 64)
act3 = mx.sym.activation(data=fc3, name='relu3', act_type="relu")
fc4  = mx.sym.fullyconnected(data=act3, name='fc4', num_hidden=10)
mlp  = mx.sym.softmaxoutput(data=fc4, name='softmax')
mod = mx.mod.module(mlp)
mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_la
bel)
mod.init_params(initializer=mx.init.xavier(magnitude=2.))
mod.init_optimizer(optimizer_params=(('learning_rate', 0.1), ))
mod.fit(train_iter, num_epoch=50)

   we hit 100% training accuracy after 25 epochs and get to 97.99%
   validation accuracy, a modest 0.14% increase compared to the previous
   model. clearly, a deeper multi-layer id88 is not the way to go.

   we need a better boat, then.

using a convolutional neural network

   we   ve seen that these networks work very well for image processing.
   let   s try a well-known id98         called [36]lenet         on this data set.

   here is the model definition, everything else is identical.
data = mx.symbol.variable('data')
conv1 = mx.sym.convolution(data=data, kernel=(5,5), num_filter=20)
tanh1 = mx.sym.activation(data=conv1, act_type="tanh")
pool1 = mx.sym.pooling(data=tanh1, pool_type="max", kernel=(2,2), stride=(2,2))
conv2 = mx.sym.convolution(data=pool1, kernel=(5,5), num_filter=50)
tanh2 = mx.sym.activation(data=conv2, act_type="tanh")
pool2 = mx.sym.pooling(data=tanh2, pool_type="max", kernel=(2,2), stride=(2,2))
flatten = mx.sym.flatten(data=pool2)
fc1 = mx.symbol.fullyconnected(data=flatten, num_hidden=500)
tanh3 = mx.sym.activation(data=fc1, act_type="tanh")
fc2 = mx.sym.fullyconnected(data=tanh3, num_hidden=10)
lenet = mx.sym.softmaxoutput(data=fc2, name='softmax')
mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_la
bel)
mod.init_params(initializer=mx.init.xavier(magnitude=2.))
mod.init_optimizer(optimizer_params=(('learning_rate', 0.1), ))
mod.fit(train_iter, num_epoch=10)

   let   s train again.
info:root:epoch[0] train-accuracy=0.906634
info:root:epoch[0] time cost=46.034
info:root:epoch[1] train-accuracy=0.971989
info:root:epoch[1] time cost=47.089

   this is painfully slow. about 45 seconds per epoch, about 30 times
   slower than the multilayer id88. now would be a good time to try
   these fancy gpus, don   t you think?

training on a single gpu

   by chance, i   ve running this on a [37]g2.8xlarge instance. it has 4
   nvidia gpus ready to crunch data :)
[ec2-user]$ nvidia-smi -l
gpu 0: grid k520 (uuid: gpu-5134e206-9b30-e1a8-a949-3d9e637a6465)
gpu 1: grid k520 (uuid: gpu-221cb85e-2d26-b615-b674-ad596a8c12ee)
gpu 2: grid k520 (uuid: gpu-ec4584ae-08e9-036f-d94a-ab60c52cf6fd)
gpu 3: grid k520 (uuid: gpu-9bd3fe35-ac18-5d1a-4fb1-d819c9265ec2)

   all it takes to switch from cpu to gpu is this. amazing!
#mod = mx.mod.module(lenet)
mod = mx.mod.module(lenet, context=mx.gpu(0))

   here we go again.
info:root:epoch[0] train-accuracy=0.906651
info:root:epoch[0] time cost=3.452
info:root:epoch[1] train-accuracy=0.972022
info:root:epoch[1] time cost=3.455
info:root:epoch[2] train-accuracy=0.980786
info:root:epoch[2] time cost=3.450
info:root:epoch[3] train-accuracy=0.985210
info:root:epoch[3] time cost=3.454
info:root:epoch[4] train-accuracy=0.987931
info:root:epoch[4] time cost=3.454
info:root:epoch[5] train-accuracy=0.989633
info:root:epoch[5] time cost=3.453
info:root:epoch[6] train-accuracy=0.991036
info:root:epoch[6] time cost=3.449
info:root:epoch[7] train-accuracy=0.992238
info:root:epoch[7] time cost=3.451
info:root:epoch[8] train-accuracy=0.993323
info:root:epoch[8] time cost=3.453
info:root:epoch[9] train-accuracy=0.994191
info:root:epoch[9] time cost=3.452
('accuracy', 0.9903846153846154)

   nice! training time has been massively reduced. accuracy is now 99+%
   thanks to the more sophisticated model.

   did i mention that there are four gpus in this box? how about using
   more than one?

training on multiple gpus

   once again, this is pretty simple to set up.
#mod = mx.mod.module(lenet, context=mx.gpu(0))
mod = mx.mod.module(lenet, context=(mx.gpu(0), mx.gpu(1)))
info:root:epoch[0] train-accuracy=0.906701
info:root:epoch[0] time cost=2.592
info:root:epoch[1] train-accuracy=0.972055
info:root:epoch[1] time cost=2.329
info:root:epoch[2] train-accuracy=0.980819
info:root:epoch[2] time cost=2.302
info:root:epoch[3] train-accuracy=0.985193
info:root:epoch[3] time cost=2.302
info:root:epoch[4] train-accuracy=0.987981
info:root:epoch[4] time cost=2.297
info:root:epoch[5] train-accuracy=0.989583
info:root:epoch[5] time cost=2.302
info:root:epoch[6] train-accuracy=0.991119
info:root:epoch[6] time cost=2.305
info:root:epoch[7] train-accuracy=0.992238
info:root:epoch[7] time cost=2.303
info:root:epoch[8] train-accuracy=0.993273
info:root:epoch[8] time cost=2.297
info:root:epoch[9] train-accuracy=0.994174
info:root:epoch[9] time cost=2.307

   we saved 50% of training time. let   s go for three gpus.
#mod = mx.mod.module(lenet, context=(mx.gpu(0), mx.gpu(1)))
mod = mx.mod.module(lenet, context=(mx.gpu(0), mx.gpu(1), mx.gpu(2)))
info:root:epoch[0] train-accuracy=0.906667
info:root:epoch[0] time cost=1.938
info:root:epoch[1] train-accuracy=0.972055
info:root:epoch[1] time cost=1.924
info:root:epoch[2] train-accuracy=0.980836
info:root:epoch[2] time cost=1.916
info:root:epoch[3] train-accuracy=0.985193
info:root:epoch[3] time cost=1.903
info:root:epoch[4] train-accuracy=0.987997
info:root:epoch[4] time cost=1.910
info:root:epoch[5] train-accuracy=0.989600
info:root:epoch[5] time cost=1.910
info:root:epoch[6] train-accuracy=0.991052
info:root:epoch[6] time cost=1.912
info:root:epoch[7] train-accuracy=0.992288
info:root:epoch[7] time cost=1.921
info:root:epoch[8] train-accuracy=0.993339
info:root:epoch[8] time cost=1.934
info:root:epoch[9] train-accuracy=0.994157
info:root:epoch[9] time cost=1.937
('accuracy', 0.9904847756410257)

   another 20% saved. training time is now only 50% more than what it was
   for the cpu-version of the multi-layer id88.

   adding a fourth gpu won   t help. yes, i tried :) anyway, we   re pretty
   happy with our model, so let   s save it for future use.

saving our model

   saving a model just requires a file name and an epoch number.
mod.save_checkpoint("lenet", 10)

   this creates two files (which you should now be familiar with):
     * the symbol file, containing the model definition (3.5kb)
     * the parameter file, containing all our trained parameters (1.7mb)

$ ls lenet*
lenet-0010.params  lenet-symbol.json

reusing our model

   just like we did in previous articles, we   re now able to load this
   pre-trained model.
lenet, arg_params, aux_params = mx.model.load_checkpoint("lenet", 10)
mod = mx.mod.module(lenet)
mod.bind(for_training=false, data_shapes=[('data', (1,1,28,28))])
mod.set_params(arg_params, aux_params)

   here are the ugly digits i created with paintbrush :)
   [1*5tcknkh8wbdnxydujofgag.png]
   my home-made digits

   i saved them as a 28x28 images, which i can now load as numpy arrays. i
   need to normalize pixels values and add two dimensions to reshape the
   array from (28, 28) to (1, 1, 28, 28) : batch size of one, one channel
   (greyscale), 28 x 28 pixels.
def loadimage(filename):
        img = cv2.imread(filename, cv2.imread_grayscale)
        img = img / 255
        print img
        img = np.expand_dims(img, axis=0)
        img = np.expand_dims(img, axis=0)
        return mx.nd.array(img)

   we   ll predict image by image. to avoid building a data iterator, i   ll
   use the same trick we   ve seen before (using a namedtuple to provide a
   data attribute).
def predict(model, filename):
        array = loadimage(filename)
        batch = namedtuple('batch', ['data'])
        mod.forward(batch([array]))
        pred = mod.get_outputs()[0].asnumpy()
        return pred

   now we   re ready. let check these digits!
np.set_printoptions(precision=3, suppress=true)
mod = loadmodel()
print predict(mod, "./0.png")
print predict(mod, "./1.png")
print predict(mod, "./2.png")
print predict(mod, "./3.png")
print predict(mod, "./4.png")
print predict(mod, "./5.png")
print predict(mod, "./6.png")
print predict(mod, "./7.png")
print predict(mod, "./8.png")
print predict(mod, "./9.png")

   and here are the results.
[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]
[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]
[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]
[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]
[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]
[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]]
[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]
[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]
[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]
[[ 0.  0.  0.  0.001  0.009  0.  0. 0.002  0. 0.988]]

   wow. hardly any doubt on the first 9 digits (probabilities are
   99.99+%). only my ugly 9 scores lower :)

   well, who thought that we   d have so much fun and that we   d cover so
   much ground using the mnist dataset? code and images are available on
   [38]github. hopefully, this will get you started on building and
   training networks on your own data.

   that   s it for today. stay tuned for part 2 where we   ll look at another
   data set!
     __________________________________________________________________

   next:
     * [39]part 2 : training on cifar-10
     * [40]part 3 : cifar-10 redux
     * [41]part 4: distributed training
     * [42]part 5: distributed training, efs edition

   iframe: [43]/media/7078d8ad19192c4c53d3bf199468e4ab?postid=6f0dc4210c62

   [1*bqlrszfhjemf4q7pyrlgng.gif]
   [44][1*6xuspt9josq0w0fi35hiaa.png]
   [45][1*c1ldmh5vbniz9rmaka8hwg.png]
   [46][1*d0jf3di6zthtqcfwdyy7mg.png]

     * [47]machine learning
     * [48]artificial intelligence
     * [49]deep learning
     * [50]neural networks
     * [51]computer science

   (button)
   (button)
   (button) 54 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [52]go to the profile of julien simon

[53]julien simon

   hacker. headbanger. harley rider. hunter.
   [54]https://aws.amazon.com/evangelists/julien-simon/

     (button) follow
   [55]chatbots life

[56]chatbots life

   best place to learn about chatbots. we share the latest bot news, info,
   ai & nlp, tools, tutorials & more.

     * (button)
       (button) 54
     * (button)
     *
     *

   [57]chatbots life
   never miss a story from chatbots life, when you sign up for medium.
   [58]learn more
   never miss a story from chatbots life
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://chatbotslife.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/6f0dc4210c62
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://chatbotslife.com/training-mxnet-part-1-mnist-6f0dc4210c62&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://chatbotslife.com/training-mxnet-part-1-mnist-6f0dc4210c62&source=--------------------------nav_reg&operation=register
   8. https://chatbotslife.com/?source=logo-lo_elvamsk5dwqp---a49517e4c30b
   9. https://chatbotslife.com/chatbot-development-hire-top-ai-chatbot-developers-c8b45ef7f207
  10. https://chatbotslife.com/chatbot-tools-62dfc60d2b8a
  11. https://chatbotslife.com/bots-for-business/home
  12. https://chatbotslife.com/tagged/voice-assistant
  13. https://chatbotslife.com/tagged/how-to
  14. https://chatbotslife.com/tagged/user-experience
  15. https://chatbotslife.com/tagged/artificial-intelligence
  16. https://www.gobeyond.ai/
  17. https://chatbotslife.com/@julsimon?source=post_header_lockup
  18. https://chatbotslife.com/@julsimon
  19. https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab
  20. http://mxnet.io/
  21. http://mxnet.io/model_zoo/
  22. https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/mnist.ipynb
  23. http://yann.lecun.com/exdb/mnist/
  24. http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
  25. http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
  26. http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
  27. http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
  28. http://www.numpy.org/
  29. http://www.numpy.org/
  30. https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8
  31. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  32. https://en.wikipedia.org/wiki/softmax_function
  33. http://mxnet.io/api/python/io.html#mxnet.io.mnistiter
  34. http://mxnet.io/api/python/optimization.html#the-mxnet-initializer-package
  35. http://mxnet.io/api/python/model.html#optimizer-api-reference
  36. http://yann.lecun.com/exdb/lenet/
  37. https://aws.amazon.com/fr/blogs/aws/new-g2-instance-type-with-4x-more-gpu-power/
  38. https://github.com/juliensimon/aws/tree/master/mxnet/mnist
  39. https://medium.com/@julsimon/training-mxnet-part-2-cifar-10-c7b0b729c33c
  40. https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0
  41. https://medium.com/@julsimon/training-mxnet-part-4-distributed-training-91def5ea3bb7
  42. https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460
  43. https://chatbotslife.com/media/7078d8ad19192c4c53d3bf199468e4ab?postid=6f0dc4210c62
  44. https://chatbotslife.com/bot-communities-mastermind-group-d2dae9876709#.53x0py6ou
  45. https://m.me/chatbotslife
  46. https://chatbotslife.com/how-to-get-a-free-chatbot-b1fb9dfe109#.z9dtp2sy0
  47. https://chatbotslife.com/tagged/machine-learning?source=post
  48. https://chatbotslife.com/tagged/artificial-intelligence?source=post
  49. https://chatbotslife.com/tagged/deep-learning?source=post
  50. https://chatbotslife.com/tagged/neural-networks?source=post
  51. https://chatbotslife.com/tagged/computer-science?source=post
  52. https://chatbotslife.com/@julsimon?source=footer_card
  53. https://chatbotslife.com/@julsimon
  54. https://aws.amazon.com/evangelists/julien-simon/
  55. https://chatbotslife.com/?source=footer_card
  56. https://chatbotslife.com/?source=footer_card
  57. https://chatbotslife.com/
  58. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  60. https://medium.com/p/6f0dc4210c62/share/twitter
  61. https://medium.com/p/6f0dc4210c62/share/facebook
  62. https://medium.com/p/6f0dc4210c62/share/twitter
  63. https://medium.com/p/6f0dc4210c62/share/facebook
