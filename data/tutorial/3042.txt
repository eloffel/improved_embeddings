   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

training mxnet         part 3: cifar-10 redux

   [9]go to the profile of julien simon
   [10]julien simon (button) blockedunblock (button) followfollowing
   apr 28, 2017

   in [11]part 2, we learned about the [12]cifar-10 data set and we saw
   how to easily load it using a [13]recordio object. using this data set,
   we both trained a network from scratch and fine-tuned a network trained
   on id163. in both cases, we used a fixed learning rate and we got to
   a point where validation accuracy plateaued at about 85%.

   in this article, we   re going to focus on improving validation accuracy.

using a variable learning rate

   let   s fine-tune our pre-trained network again. this time, we   re going
   to reduce the learning rate gradually, which should help the model
   converge to a    lower    minimum: we   ll start at 0.05 and multiply by 0.9
   each time 25 epochs have gone by, until we reach 300 epochs.

     gradually reducing the learning rate is a key technique in improving
     validation accuracy.

   we need 2 extra parameters to do this:
     * lr-step-epochs: the epoch steps when the learning rate will be
       updated.
     * lr-factor: the factor by which the learning rate will be updated.

   the new fine-tuning command becomes:
$ python fine-tune.py
--pretrained-model resnext-101 --load-epoch 0000
--gpus 0,1,2,3     batch-size 128
--data-train cifar10_train.rec --data-val cifar10_val.rec
--num-examples 50000 --num-classes 10 --image-shape 3,28,28
--num-epoch 300 --lr 0.05 --lr-factor 0.9
--lr-step-epochs 25,50,75,100,125,150,175,200,225,250,275,300

   a while later, here   s the result.
   [1*d5hose4dujfvkuhu4bsx_q.png]
   fine-tuning a pre-trained resnext-101 model on cifar-10, variable
   learning rate

   as we can see, reducing the learning rate definitely helps reaching
   higher validation accuracy. previously, it plateaued at 85% after 25
   epochs or so. here, it   s climbing steadily up to 89% and would probably
   keep increasing beyond 300 epochs. 4% is a huge difference: on a 10,000
   image validation set, it means that an extra 400 images are correctly
   labelled.

adadelta

   surely, we could keep tweaking and find an even better combination for
   the initial learning rate, the factor and the steps. but we could also
   do without all these parameters, thanks to the adadelta optimizer.

   adadelta is a evolution of the sgd algorithm we   ve been using so far
   for optimization ([14]research paper). it doesn   t need to be given a
   learning rate. in fact, it will automatically select and adapt a
   learning rate for each dimension.

training resnext-101 from scratch with adadelta

   let   s try to train resnext-101 from scratch, using adadelta. instead of
   loading the network, we   re going to build one using
   resnext.get_symbol(), a [15]python function available in mxnet. there   s
   a whole set of functions to build different networks: i suggest that
   you take some time to look at them, as they will help you understand
   how these networks are structured.

   you should now be familiar with the rest of the code :)

   iframe: [16]/media/2eaf3d5005adc401103e28e046ed96ee?postid=ecab17346aa0

   these are the results after 300 epochs.
   [1*t4mbflioqavz4dckc79w0g.png]
   training a resnext-101 model from scratch on cifar-10, adadelta
   optimizer

   in our previous article, training the same model from scratch only
   yielded 80% validation accuracy. here, not only does training accuracy
   increase extremely fast, we also reach 86% validation accuracy without
   having to guess about learning rate or when to decrease it.

   it   s very likely that an expert would achieve better results by
   tweaking optimization parameters, but for the rest of us, adadelta is
   an interesting option.

   mxnet supports a whole set of [17]optimization algorithms. if you   d
   like to learn more on how they work and how they differ, here   s an
   [18]excellent article by sebastian ruder.

fast and furious

   one last thing. i mentioned earlier that training took    a while   . more
   precisely, it took 12+ hours using all 4 gpus of a [19]g2.8xlarge
   instance.

   could we go faster? sure, i could use a [20]p2.16xlarge instance.
   that   s as large as gpu servers get.

   even faster? we need distributed training, which we   ll cover in part 4.

   thanks for reading :)
     __________________________________________________________________

   next:

   [21]part 4         distributed training

   [22]part 5         distributed training, efs edition

     * [23]machine learning
     * [24]data science
     * [25]deep learning
     * [26]neural networks
     * [27]artificial intelligence

   (button)
   (button)
   (button) 18 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [28]go to the profile of julien simon

[29]julien simon

   hacker. headbanger. harley rider. hunter.
   [30]https://aws.amazon.com/evangelists/julien-simon/

     * (button)
       (button) 18
     * (button)
     *
     *

   [31]go to the profile of julien simon
   never miss a story from julien simon, when you sign up for medium.
   [32]learn more
   never miss a story from julien simon
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ecab17346aa0
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@julsimon?source=post_header_lockup
  10. https://medium.com/@julsimon
  11. https://medium.com/@julsimon/training-mxnet-part-2-cifar-10-c7b0b729c33c
  12. https://www.cs.toronto.edu/~kriz/cifar.html
  13. http://mxnet.io/api/python/io.html#module-mxnet.recordio
  14. about:invalid#zsoyz
  15. https://github.com/dmlc/mxnet/tree/master/example/image-classification/symbols
  16. https://medium.com/media/2eaf3d5005adc401103e28e046ed96ee?postid=ecab17346aa0
  17. http://mxnet.io/api/python/model.html#optimizer-api-reference
  18. http://sebastianruder.com/optimizing-gradient-descent/
  19. https://aws.amazon.com/blogs/aws/new-g2-instance-type-with-4x-more-gpu-power/
  20. https://aws.amazon.com/blogs/aws/new-p2-instance-type-for-amazon-ec2-up-to-16-gpus/
  21. https://medium.com/@julsimon/training-mxnet-part-4-distributed-training-91def5ea3bb7
  22. https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460
  23. https://medium.com/tag/machine-learning?source=post
  24. https://medium.com/tag/data-science?source=post
  25. https://medium.com/tag/deep-learning?source=post
  26. https://medium.com/tag/neural-networks?source=post
  27. https://medium.com/tag/artificial-intelligence?source=post
  28. https://medium.com/@julsimon?source=footer_card
  29. https://medium.com/@julsimon
  30. https://aws.amazon.com/evangelists/julien-simon/
  31. https://medium.com/@julsimon
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/ecab17346aa0/share/twitter
  35. https://medium.com/p/ecab17346aa0/share/facebook
  36. https://medium.com/p/ecab17346aa0/share/twitter
  37. https://medium.com/p/ecab17346aa0/share/facebook
