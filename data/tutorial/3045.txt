   #[1]github [2]recent commits to pointer-generator:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]47
     * [35]star [36]1,140
     * [37]fork [38]488

[39]abisee/[40]pointer-generator

   [41]code [42]issues 73 [43]pull requests 7 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   code for the acl 2017 paper "get to the point: summarization with
   pointer-generator networks"
     * [47]29 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors
     * [51]apache-2.0

    1. [52]python 100.0%

   (button) python
   branch: master (button) new pull request
   [53]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/a
   [54]download zip

downloading...

   want to be notified of new releases in abisee/pointer-generator?
   [55]sign in [56]sign up

launching github desktop...

   if nothing happens, [57]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [59]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [60]download the github extension for visual studio
   and try again.

   (button) go back
   [61]@abisee
   [62]abisee [63]update readme.md
   latest commit [64]a7317f5 jul 10, 2018
   [65]permalink
   type            name           latest commit message  commit time
        failed to load latest commit information.
        [66]license.txt
        [67]readme.md
        [68]__init__.py
        [69]attention_decoder.py
        [70]batcher.py
        [71]beam_search.py        [72]add license       may 18, 2017
        [73]data.py               [74]add license       may 18, 2017
        [75]decode.py             [76]add license       may 18, 2017
        [77]inspect_checkpoint.py
        [78]model.py
        [79]run_summarization.py
        [80]util.py

readme.md

   note: this code is no longer actively maintained. however, feel free to
   use the issues section to discuss the code with other users. some users
   have updated this code for newer versions of tensorflow and python -
   see information below and issues section.
     __________________________________________________________________

   this repository contains code for the acl 2017 paper [81]get to the
   point: summarization with pointer-generator networks. for an intuitive
   overview of the paper, read the [82]blog post.

looking for test set output?

   the test set output of the models described in the paper can be found
   [83]here.

looking for pretrained model?

   a pretrained model is available here:
     * [84]version for tensorflow 1.0
     * [85]version for tensorflow 1.2.1

   (the only difference between these two is the naming of some of the
   variables in the checkpoint. tensorflow 1.0 uses lstm_cell/biases and
   lstm_cell/weights whereas tensorflow 1.2.1 uses lstm_cell/bias and
   lstm_cell/kernel).

   note: this pretrained model is not the exact same model that is
   reported in the paper. that is, it is the same architecture, trained
   with the same settings, but resulting from a different training run.
   consequently this pretrained model has slightly lower id8 scores than
   those reported in the paper. this is probably due to us slightly
   overfitting to the randomness in our original experiments (in the
   original experiments we tried various hyperparameter settings and
   selected the model that performed best). repeating the experiment once
   with the same settings did not perform quite as well. better results
   might be obtained from further hyperparameter tuning.

   why can't you release the trained model reported in the paper? due to
   changes to the code between the original experiments and the time of
   releasing the code (e.g. tensorflow version changes, lots of code
   cleanup), it is not possible to release the original trained model
   files.

looking for id98 / daily mail data?

   instructions are [86]here.

about this code

   this code is based on the [87]textsum code from google brain.

   this code was developed for tensorflow 0.12, but has been updated to
   run with tensorflow 1.0. in particular, the code in
   attention_decoder.py is based on
   [88]tf.contrib.legacy_id195_attention_decoder, which is now outdated.
   tensorflow 1.0's [89]new id195 library probably provides a way to do
   this (as well as id125) more elegantly and efficiently in the
   future.

   python 3 version: this code is in python 2. if you want a python 3
   version, see [90]@becxer's fork.

how to run

get the dataset

   to obtain the id98 / daily mail dataset, follow the instructions
   [91]here. once finished, you should have [92]chunked datafiles
   train_000.bin, ..., train_287.bin, val_000.bin, ..., val_013.bin,
   test_000.bin, ..., test_011.bin (each contains 1000 examples) and a
   vocabulary file vocab.

   note: if you did this before 7th may 2017, follow the instructions
   [93]here to correct a bug in the process.

run training

   to train your model, run:
python run_summarization.py --mode=train --data_path=/path/to/chunked/train_* --
vocab_path=/path/to/vocab --log_root=/path/to/a/log/directory --exp_name=myexper
iment

   this will create a subdirectory of your specified log_root called
   myexperiment where all checkpoints and other data will be saved. then
   the model will start training using the train_*.bin files as training
   data.

   warning: using default settings as in the above command, both
   initializing the model and running training iterations will probably be
   quite slow. to make things faster, try setting the following flags
   (especially max_enc_steps and max_dec_steps) to something smaller than
   the defaults specified in run_summarization.py: hidden_dim, emb_dim,
   batch_size, max_enc_steps, max_dec_steps, vocab_size.

   increasing sequence length during training: note that to obtain the
   results described in the paper, we increase the values of max_enc_steps
   and max_dec_steps in stages throughout training (mostly so we can
   perform quicker iterations during early stages of training). if you
   wish to do the same, start with small values of max_enc_steps and
   max_dec_steps, then interrupt and restart the job with larger values
   when you want to increase them.

run (concurrent) eval

   you may want to run a concurrent evaluation job, that runs your model
   on the validation set and logs the loss. to do this, run:
python run_summarization.py --mode=eval --data_path=/path/to/chunked/val_* --voc
ab_path=/path/to/vocab --log_root=/path/to/a/log/directory --exp_name=myexperime
nt

   note: you want to run the above command using the same settings you
   entered for your training job.

   restoring snapshots: the eval job saves a snapshot of the model that
   scored the lowest loss on the validation data so far. you may want to
   restore one of these "best models", e.g. if your training job has
   overfit, or if the training checkpoint has become corrupted by nan
   values. to do this, run your train command plus the
   --restore_best_model=1 flag. this will copy the best model in the eval
   directory to the train directory. then run the usual train command
   again.

run id125 decoding

   to run id125 decoding:
python run_summarization.py --mode=decode --data_path=/path/to/chunked/val_* --v
ocab_path=/path/to/vocab --log_root=/path/to/a/log/directory --exp_name=myexperi
ment

   note: you want to run the above command using the same settings you
   entered for your training job (plus any decode mode specific flags like
   beam_size).

   this will repeatedly load random examples from your specified datafile
   and generate a summary using id125. the results will be printed
   to screen.

   visualize your output: additionally, the decode job produces a file
   called attn_vis_data.json. this file provides the data necessary for an
   in-browser visualization tool that allows you to view the attention
   distributions projected onto the text. to use the visualizer, follow
   the instructions [94]here.

   if you want to run evaluation on the entire validation or test set and
   get id8 scores, set the flag single_pass=1. this will go through the
   entire dataset in order, writing the generated summaries to file, and
   then run evaluation using [95]pyid8. (note this will not produce the
   attn_vis_data.json files for the attention visualizer).

evaluate with id8

   decode.py uses the python package [96]pyid8 to run id8 evaluation.
   pyid8 provides an easier-to-use interface for the official perl id8
   package, which you must install for pyid8 to work. here are some
   useful instructions on how to do this:
     * [97]how to setup perl id8
     * [98]more details about plugins for perl id8

   note: as of 18th may 2017 the [99]website for the official perl package
   appears to be down. unfortunately you need to download a directory
   called id8-1.5.5 from there. as an alternative, it seems that you can
   get that directory from [100]here (however, the version of pyid8 in
   that repo appears to be outdated, so best to install pyid8 from the
   [101]official source).

tensorboard

   run tensorboard from the experiment directory (in the example above,
   myexperiment). you should be able to see data from the train and eval
   runs. if you select "embeddings", you should also see your word
   embeddings visualized.

help, i've got nans!

   for reasons that are [102]difficult to diagnose, nans sometimes occur
   during training, making the loss=nan and sometimes also corrupting the
   model checkpoint with nan values, making it unusable. here are some
   suggestions:
     * if training stopped with the loss is not finite. stopping.
       exception, you can just try restarting. it may be that the
       checkpoint is not corrupted.
     * you can check if your checkpoint is corrupted by using the
       inspect_checkpoint.py script. if it says that all values are
       finite, then your checkpoint is ok and you can try resuming
       training with it.
     * the training job is set to keep 3 checkpoints at any one time (see
       the max_to_keep variable in run_summarization.py). if your newer
       checkpoint is corrupted, it may be that one of the older ones is
       not. you can switch to that checkpoint by editing the checkpoint
       file inside the train directory.
     * alternatively, you can restore a "best model" from the eval
       directory. see the note restoring snapshots above.
     * if you want to try to diagnose the cause of the nans, you can run
       with the --debug=1 flag turned on. this will run [103]tensorflow
       debugger, which checks for nans and diagnoses their causes during
       training.

     *    2019 github, inc.
     * [104]terms
     * [105]privacy
     * [106]security
     * [107]status
     * [108]help

     * [109]contact github
     * [110]pricing
     * [111]api
     * [112]training
     * [113]blog
     * [114]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [115]reload to refresh your
   session. you signed out in another tab or window. [116]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/abisee/pointer-generator/commits/master.atom
   3. https://github.com/abisee/pointer-generator#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/abisee/pointer-generator
  32. https://github.com/join
  33. https://github.com/login?return_to=/abisee/pointer-generator
  34. https://github.com/abisee/pointer-generator/watchers
  35. https://github.com/login?return_to=/abisee/pointer-generator
  36. https://github.com/abisee/pointer-generator/stargazers
  37. https://github.com/login?return_to=/abisee/pointer-generator
  38. https://github.com/abisee/pointer-generator/network/members
  39. https://github.com/abisee
  40. https://github.com/abisee/pointer-generator
  41. https://github.com/abisee/pointer-generator
  42. https://github.com/abisee/pointer-generator/issues
  43. https://github.com/abisee/pointer-generator/pulls
  44. https://github.com/abisee/pointer-generator/projects
  45. https://github.com/abisee/pointer-generator/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/abisee/pointer-generator/commits/master
  48. https://github.com/abisee/pointer-generator/branches
  49. https://github.com/abisee/pointer-generator/releases
  50. https://github.com/abisee/pointer-generator/graphs/contributors
  51. https://github.com/abisee/pointer-generator/blob/master/license.txt
  52. https://github.com/abisee/pointer-generator/search?l=python
  53. https://github.com/abisee/pointer-generator/find/master
  54. https://github.com/abisee/pointer-generator/archive/master.zip
  55. https://github.com/login?return_to=https://github.com/abisee/pointer-generator
  56. https://github.com/join?return_to=/abisee/pointer-generator
  57. https://desktop.github.com/
  58. https://desktop.github.com/
  59. https://developer.apple.com/xcode/
  60. https://visualstudio.github.com/
  61. https://github.com/abisee
  62. https://github.com/abisee/pointer-generator/commits?author=abisee
  63. https://github.com/abisee/pointer-generator/commit/a7317f573d01b944c31a76bde7218bcfc890ef6a
  64. https://github.com/abisee/pointer-generator/commit/a7317f573d01b944c31a76bde7218bcfc890ef6a
  65. https://github.com/abisee/pointer-generator/tree/a7317f573d01b944c31a76bde7218bcfc890ef6a
  66. https://github.com/abisee/pointer-generator/blob/master/license.txt
  67. https://github.com/abisee/pointer-generator/blob/master/readme.md
  68. https://github.com/abisee/pointer-generator/blob/master/__init__.py
  69. https://github.com/abisee/pointer-generator/blob/master/attention_decoder.py
  70. https://github.com/abisee/pointer-generator/blob/master/batcher.py
  71. https://github.com/abisee/pointer-generator/blob/master/beam_search.py
  72. https://github.com/abisee/pointer-generator/commit/4954f405fc4faab5003ae384ad181eb883e453ea
  73. https://github.com/abisee/pointer-generator/blob/master/data.py
  74. https://github.com/abisee/pointer-generator/commit/4954f405fc4faab5003ae384ad181eb883e453ea
  75. https://github.com/abisee/pointer-generator/blob/master/decode.py
  76. https://github.com/abisee/pointer-generator/commit/4954f405fc4faab5003ae384ad181eb883e453ea
  77. https://github.com/abisee/pointer-generator/blob/master/inspect_checkpoint.py
  78. https://github.com/abisee/pointer-generator/blob/master/model.py
  79. https://github.com/abisee/pointer-generator/blob/master/run_summarization.py
  80. https://github.com/abisee/pointer-generator/blob/master/util.py
  81. https://arxiv.org/abs/1704.04368
  82. http://www.abigailsee.com/2017/04/16/taming-id56s-for-better-summarization.html
  83. https://drive.google.com/file/d/0b7pqmm-ofdv7metmvu5sohc5ltg/view?usp=sharing
  84. https://drive.google.com/file/d/0b7pqmm-ofdv7shfadhr4rllfr1e/view?usp=sharing
  85. https://drive.google.com/file/d/0b7pqmm-ofdv7zuhhzm9zweziddg/view?usp=sharing
  86. https://github.com/abisee/id98-dailymail
  87. https://github.com/tensorflow/models/tree/master/textsum
  88. https://www.tensorflow.org/api_docs/python/tf/contrib/legacy_id195/attention_decoder
  89. https://www.tensorflow.org/api_guides/python/contrib.id195#attention
  90. https://github.com/becxer/pointer-generator/
  91. https://github.com/abisee/id98-dailymail
  92. https://github.com/abisee/id98-dailymail/issues/3
  93. https://github.com/abisee/id98-dailymail/issues/2
  94. https://github.com/abisee/attn_vis
  95. https://pypi.python.org/pypi/pyid8
  96. https://pypi.python.org/pypi/pyid8
  97. http://kavita-ganesan.com/id8-howto
  98. http://www.summarizerman.com/post/42675198985/figuring-out-id8
  99. http://beid8.com/
 100. https://github.com/andersjo/pyid8
 101. https://pypi.python.org/pypi/pyid8
 102. https://github.com/abisee/pointer-generator/issues/4
 103. https://www.tensorflow.org/versions/master/programmers_guide/debugger
 104. https://github.com/site/terms
 105. https://github.com/site/privacy
 106. https://github.com/security
 107. https://githubstatus.com/
 108. https://help.github.com/
 109. https://github.com/contact
 110. https://github.com/pricing
 111. https://developer.github.com/
 112. https://training.github.com/
 113. https://github.blog/
 114. https://github.com/about
 115. https://github.com/abisee/pointer-generator
 116. https://github.com/abisee/pointer-generator

   hidden links:
 118. https://github.com/
 119. https://github.com/abisee/pointer-generator
 120. https://github.com/abisee/pointer-generator
 121. https://github.com/abisee/pointer-generator
 122. https://help.github.com/articles/which-remote-url-should-i-use
 123. https://github.com/abisee/pointer-generator#looking-for-test-set-output
 124. https://github.com/abisee/pointer-generator#looking-for-pretrained-model
 125. https://github.com/abisee/pointer-generator#looking-for-id98--daily-mail-data
 126. https://github.com/abisee/pointer-generator#about-this-code
 127. https://github.com/abisee/pointer-generator#how-to-run
 128. https://github.com/abisee/pointer-generator#get-the-dataset
 129. https://github.com/abisee/pointer-generator#run-training
 130. https://github.com/abisee/pointer-generator#run-concurrent-eval
 131. https://github.com/abisee/pointer-generator#run-beam-search-decoding
 132. https://github.com/abisee/pointer-generator#evaluate-with-id8
 133. https://github.com/abisee/pointer-generator#tensorboard
 134. https://github.com/abisee/pointer-generator#help-ive-got-nans
 135. https://github.com/
