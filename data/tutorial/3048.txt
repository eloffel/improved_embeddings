   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    how to
   build a recurrent neural network in tensorflow comments feed
   [5]kdnuggets    news 17:n16, apr 26: awesome deep learning: most cited
   deep learning papers; data science for the layman [6]the analytics of
   emotion and depression

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2017    [28]apr    [29]tutorials,
   overviews    how to build a recurrent neural network in tensorflow
   ( [30]17:n17 )

how to build a recurrent neural network in tensorflow

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 113
   tags: [33]deep learning, [34]neural networks, [35]recurrent neural
   networks, [36]tensorflow

   this is a no-nonsense overview of implementing a recurrent neural
   network (id56) in tensorflow. both theory and practice are covered
   concisely, and the end result is running tensorflow id56 code.
     __________________________________________________________________

   by erik hallstr  m, deep learning research engineer.

   in this tutorial i   ll explain how to build a simple working recurrent
   neural network in tensorflow. this is the first in a series of seven
   parts where various aspects and techniques of building recurrent neural
   networks in tensorflow are covered. a short introduction to tensorflow
   is [37]available here. for now, let   s get started with the id56!

what is a id56?


   it is short for    recurrent neural network   , and is basically a neural
   network that can be used when your data is treated as a sequence, where
   the particular order of the data-points matter. more importantly, this
   sequence can be of arbitrary length.

   the most straight-forward example is perhaps a time-series of numbers,
   where the task is to predict the next value given previous values. the
   input to the id56 at every time-step is the current value as well as a
   state vector which represent what the network has    seen    at time-steps
   before. this state-vector is the encoded memory of the id56, initially
   set to zero.

                       [1*uki9za9ztr-hl8um15wmzw.png]
          schematic of a id56 processing sequential data over time.

   the best and most comprehensive article explaining id56's i   ve found so
   far is [38]this article by researchers at ucsd, highly recommended. for
   now you only need to understand the basics, read it until the    modern
   id56 architectures   -section. that will be covered later.

   although this article contains some explanations, it is mostly focused
   on the practical part, how to build it. you are encouraged to look up
   more theory on the internet, there are plenty of good explanations.

setup


   we will build a simple echo-id56 that remembers the input data and then
   echoes it after a few time-steps. first let   s set some constants we   ll
   need, what they mean will become clear in a moment.

generate data


   now generate the training data, the input is basically a random binary
   vector. the output will be the    echo    of the input, shifted echo_step
   steps to the right.

   notice the reshaping of the data into a matrix with batch_size rows.
   neural networks are trained by approximating the gradient of loss
   function with respect to the neuron-weights, by looking at only a small
   subset of the data, also known as a mini-batch. the theoretical reason
   for doing this is further elaborated in [39]this question. the
   reshaping takes the whole dataset and puts it into a matrix, that later
   will be sliced up into these mini-batches.

                       [1*aftwufsbolv8z5pkeznlxa.png]
     schematic of the reshaped data-matrix, arrow curves shows adjacent
      time-steps that ended up on different rows. light-gray rectangle
              represent a    zero    and dark-gray a    one   .

building the computational graph


   tensorflow works by first building up a computational graph, that
   specifies what operations will be done. the input and output of this
   graph is typically multidimensional arrays, also known as tensors. the
   graph, or parts of it can then be executed iteratively in a session,
   this can either be done on the cpu, gpu or even a resource on a remote
   server.

   variables and placeholders

   the two basic tensorflow data-structures that will be used in this
   example are placeholders and variables. on each run the batch data is
   fed to the placeholders, which are    starting nodes    of the
   computational graph. also the id56-state is supplied in a placeholder,
   which is saved from the output of the previous run.

   the weights and biases of the network are declared as tensorflow
   variables, which makes them persistent across runs and enables them to
   be updated incrementally for each batch.

   the figure below shows the input data-matrix, and the current batch
   batchx_placeholder is in the dashed rectangle. as we will see later,
   this    batch window    is slided truncated_backprop_length steps to the
   right at each run, hence the arrow. in our example below batch_size =
   3, truncated_backprop_length = 3, and total_series_length = 36. note
   that these numbers are just for visualization purposes, the values are
   different in the code. the series order index is shown as numbers in a
   few of the data-points.

                       [1*n45uynaftdrbvg87j-poca.jpeg]
   schematic of the training data, the current batch is sliced out in the
    dashed rectangle. the time-step index of the datapoint is displayed.

   unpacking

   now it   s time to build the part of the graph that resembles the actual
   id56 computation, first we want to split the batch data into adjacent
   time-steps.

   as you can see in the picture below that is done by unpacking the
   columns (axis = 1) of the batch into a python list. the id56 will
   simultaneously be training on different parts in the time-series; steps
   4 to 6, 16 to 18 and 28 to 30 in the current batch-example. the reason
   for using the variable names    plural   _   series    is to emphasize that the
   variable is a list that represent a time-series with multiple entries
   at each step.

                       [1*f2il4zokbubgopve7kyajg.png]
    schematic of the current batch split into columns, the order index is
        shown on each data-point and arrows show adjacent time-steps.

   the fact that the training is done on three places simultaneously in
   our time-series, requires us to save three instances of states when
   propagating forward. that has already been accounted for, as you see
   that the init_state placeholder has batch_size rows.

   forward pass

   next let   s build the part of the graph that does the actual id56
   computation.

   notice the concatenation on line 6, what we actually want to do is
   calculate the sum of two affine transforms current_input * wa +
   current_state * wbin the figure below. by concatenating those two
   tensors you will only use one id127. the addition of
   the bias b is [40]broadcasted on all samples in the batch.

                       [1*fdwnnj5uoe3sx0r_cyfmyg.png]
     schematic of the computations of the matrices on line 8 in the code
         example above, the non-linear transform arctan is omitted.

   you may wonder the variable name truncated_backprop_length is supposed
   to mean. when a id56 is trained, it is actually treated as a deep neural
   network with reoccurring weights in every layer. these layers will not
   be unrolled to the beginning of time, that would be too computationally
   expensive, and are therefore truncated at a limited number of
   time-steps. in our sample schematics above, the error is backpropagated
   three steps in our batch.

   calculating loss

   this is the final part of the graph, a fully connected softmax layer
   from the state to the output that will make the classes one-hot
   encoded, and then calculating the loss of the batch.

   the last line is adding the training functionality, tensorflow will
   perform back-propagation for us automatically         the computation graph
   is executed once for each mini-batch and the network-weights are
   updated incrementally.

   notice the api call to sparse_softmax_cross_id178_with_logits, it
   automatically calculates the softmax internally and then computes the
   cross-id178. in our example the classes are mutually exclusive (they
   are either zero or one), which is the reason for using the
      sparse-softmax   , you can read more about it in [41]the api. the usage
   is to havelogits is of shape [batch_size, num_classes] and labels of
   shape [batch_size].

visualizing the training


   there is a visualization function so we can se what   s going on in the
   network as we train. it will plot the loss over the time, show training
   input, training output and the current predictions by the network on
   different sample series in a training batch.

running a training session


   it   s time to wrap up and train the network, in tensorflow the graph is
   executed in a session. new data is generated on each epoch (not the
   usual way to do it, but it works in this case since everything is
   predictable).

   you can see that we are moving truncated_backprop_length steps forward
   on each iteration (line 15   19), but it is possible have different
   strides. this subject is further elaborated in [42]this article. the
   downside with doing this is that truncated_backprop_length need to be
   significantly larger than the time dependencies (three steps in our
   case) in order to encapsulate the relevant training data. otherwise
   there might a lot of    misses   , as you can see on the figure below.

   [1*ukuukp_m55zapczaiemuca.png]
   time series of squares, the elevated black square symbolizes an
   echo-output, which is activated three steps from the echo input (black
   square). the sliding batch window is also striding three steps at each
   run, which in our sample case means that no batch will encapsulate the
   dependency, so it can not train.

   also realize that this is just simple example to explain how a id56
   works, this functionality could easily be programmed in just a few
   lines of code. the network will be able to exactly learn the echo
   behavior so there is no need for testing data.

   the program will update the plot as training progresses, shown in the
   picture below. blue bars denote a training input signal (binary one),
   red bars show echos in the training output and green bars are the echos
   the net is generating. the different bar plots show different sample
   series in the current batch.

   our algorithm will fairly quickly learn the task. the graph in the
   top-left corner shows the output of the id168, but why are
   there spikes in the curve? think of it for a moment, answer is below.

                       [1*ytqumdmgmjo0-3kxmci1gg.png]
    visualization of the loss, input and output training data (blue, red)
                     as well as the prediction (green).

   the reason for the spikes is that we are starting on a new epoch, and
   generating new data. since the matrix is reshaped, the first element on
   each row is adjacent to the last element in the previous row. the first
   few elements on all rows (except the first) have dependencies that will
   not be included in the state, so the net will always perform badly on
   the first batch.

whole program


   this is the whole runnable program, just copy-paste and run. after each
   part in the article series the whole runnable program will be
   presented. if a line is referenced by number, these are the line
   numbers that we mean.

   bio: [43]erik hallstr  m is a deep learning research engineer at sana.
   he studied engineering physics and machine learning at royal institute
   of technology in stockholm. also been living in taiwan             . interested
   in deep learning.

   [44]original. reposted with permission.

   related:
     * [45]getting started with deep learning
     * [46]deep learning key terms, explained
     * [47]5 machine learning projects you can no longer overlook, april
     __________________________________________________________________

   [48][prv.gif] previous post
   [49]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [50]another 10 free must-read books for machine learning and data
       science
    2. [51]9 must-have skills you need to become a data scientist, updated
    3. [52]who is a typical data scientist in 2019?
    4. [53]the pareto principle for data scientists
    5. [54]what no one will tell you about data science job applications
    6. [55]19 inspiring women in ai, big data, data science, machine
       learning
    7. [56]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [57]id158s optimization using genetic algorithm
       with python
    2. [58]who is a typical data scientist in 2019?
    3. [59]8 reasons why you should get a microsoft azure certification
    4. [60]the pareto principle for data scientists
    5. [61]r vs python for data visualization
    6. [62]how to work in data science, ai, big data
    7. [63]the deep learning toolset     an overview

[64]latest news

     * [65]download your datax guide to ai in marketing
     * [66]kdnuggets offer: save 20% on strata in london
     * [67]training a champion: building deep neural nets for big ...
     * [68]building a recommender system
     * [69]predict age and gender using convolutional neural netwo...
     * [70]top tweets, mar 27     apr 02: here is a great ex...

more recent stories

     * [71]top tweets, mar 27     apr 02: here is a great explanat...
     * [72]odsc east is selling out; odsc india announced
     * [73]accelerate ai and data science career expo, may 4, 2019
     * [74]grow your data career at datasciencego, san diego, sep 27-29
     * [75]getting started with nlp using the pytorch framework
     * [76]how to diy your data science education
     * [77]top 8 data science use cases in gaming
     * [78]kdnuggets 19:n13, apr 3: top 10 data scientist coding mista...
     * [79]make better data-driven business decisions
     * [80]top stories, mar 25-31: r vs python for data visualization;
       th...
     * [81]two predictive analytics world events in europe this fall
     * [82]7 qualities your big data visualization tools absolutely must
       ...
     * [83]yeshiva university: tenure-track faculty in ai and machine
       lea...
     * [84]which face is real?
     * [85]yeshiva university: program director / tenure track faculty
       me...
     * [86]top 10 coding mistakes made by data scientists
     * [87]uber   s case study at paw industry 4.0: machine learning ...
     * [88]xai     a data scientist   s mouthpiece
     * [89]what does gpt-2 think about the ai arms race?
     * [90]openclassrooms: data freelance online course creator
       [telecomm...

   [91]kdnuggets home    [92]news    [93]2017    [94]apr    [95]tutorials,
   overviews    how to build a recurrent neural network in tensorflow
   ( [96]17:n17 )
      2019 kdnuggets. [97]about kdnuggets.  [98]privacy policy. [99]terms
   of service

   [100]subscribe to kdnuggets news
   [101][tw_c48.png] [102]facebook [103]linkedin
   x
   [envelope.png] [104]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2017/04/build-recurrent-neural-network-tensorflow.html/feed
   5. https://www.kdnuggets.com/2017/n16.html
   6. https://www.kdnuggets.com/2017/04/analytics-emotion-depression.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2017/index.html
  28. https://www.kdnuggets.com/2017/04/index.html
  29. https://www.kdnuggets.com/2017/04/tutorials.html
  30. https://www.kdnuggets.com/2017/n17.html
  31. https://www.kdnuggets.com/2017/n16.html
  32. https://www.kdnuggets.com/2017/04/analytics-emotion-depression.html
  33. https://www.kdnuggets.com/tag/deep-learning
  34. https://www.kdnuggets.com/tag/neural-networks
  35. https://www.kdnuggets.com/tag/recurrent-neural-networks
  36. https://www.kdnuggets.com/tag/tensorflow
  37. https://medium.com/@erikhallstrm/hello-world-tensorflow-649b15aed18c#.kogbhxazp
  38. https://arxiv.org/pdf/1506.00019.pdf
  39. https://www.quora.com/in-deep-learning-why-dont-we-use-the-whole-training-set-to-compute-the-gradient/answer/ian-goodfellow?srid=9kxj
  40. https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html
  41. https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sparse_softmax_cross_id178_with_logits
  42. http://r2rt.com/styles-of-truncated-id26.html
  43. https://www.linkedin.com/in/hallstroem/
  44. https://medium.com/@erikhallstrm/hello-world-id56-83cd7105b767
  45. https://www.kdnuggets.com/2017/03/getting-started-deep-learning.html
  46. https://www.kdnuggets.com/2016/10/deep-learning-key-terms-explained.html
  47. https://www.kdnuggets.com/2017/04/five-machine-learning-projects-cant-overlook-april.html
  48. https://www.kdnuggets.com/2017/n16.html
  49. https://www.kdnuggets.com/2017/04/analytics-emotion-depression.html
  50. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  51. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  52. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  53. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  54. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  55. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  56. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  57. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  58. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  59. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  60. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  61. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  62. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  63. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  64. https://www.kdnuggets.com/news/index.html
  65. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  66. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  67. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  68. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  69. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  70. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  71. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  72. https://www.kdnuggets.com/2019/04/odsc-east-selling-out-india-announced.html
  73. https://www.kdnuggets.com/jobs/19/04-03-accelerate-ai-data-science-career-expo-2019.html
  74. https://www.kdnuggets.com/2019/04/formulated-data-career-datasciencego-san-diego.html
  75. https://www.kdnuggets.com/2019/04/nlp-pytorch.html
  76. https://www.kdnuggets.com/2019/04/diy-your-data-science-education.html
  77. https://www.kdnuggets.com/2019/04/top-8-data-science-use-cases-gaming.html
  78. https://www.kdnuggets.com/2019/n13.html
  79. https://www.kdnuggets.com/2019/04/psu-make-better-data-driven-business-decisions.html
  80. https://www.kdnuggets.com/2019/04/top-news-week-0325-0331.html
  81. https://www.kdnuggets.com/2019/04/paw-two-predictive-analytics-world-events-europe-fall.html
  82. https://www.kdnuggets.com/2019/04/7-qualities-big-data-visualization-tools.html
  83. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-faculty-ai-machine-learning.html
  84. https://www.kdnuggets.com/2019/04/which-face-real-stylegan.html
  85. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-program-director-faculty-artificial-intelligence-machine-learning.html
  86. https://www.kdnuggets.com/2019/04/top-10-coding-mistakes-data-scientists.html
  87. https://www.kdnuggets.com/2019/04/paw-uber-case-study-machine-learning-enforce-mobile-performance.html
  88. https://www.kdnuggets.com/2019/04/xai-data-scientist.html
  89. https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html
  90. https://www.kdnuggets.com/jobs/19/04-01-openclassrooms-data-freelance-online-course-creator-b.html
  91. https://www.kdnuggets.com/
  92. https://www.kdnuggets.com/news/index.html
  93. https://www.kdnuggets.com/2017/index.html
  94. https://www.kdnuggets.com/2017/04/index.html
  95. https://www.kdnuggets.com/2017/04/tutorials.html
  96. https://www.kdnuggets.com/2017/n17.html
  97. https://www.kdnuggets.com/about/index.html
  98. https://www.kdnuggets.com/news/privacy-policy.html
  99. https://www.kdnuggets.com/terms-of-service.html
 100. https://www.kdnuggets.com/news/subscribe.html
 101. https://twitter.com/kdnuggets
 102. https://facebook.com/kdnuggets
 103. https://www.linkedin.com/groups/54257
 104. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
 106. https://www.kdnuggets.com/
 107. https://www.kdnuggets.com/
