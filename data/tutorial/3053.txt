   #[1]sebastian ruder

   [2]sebastian ruder
     * [3]about
     * [4]tags
     * [5]papers
     * [6]talks
     * [7]news
     * [8]faq
     * [9]nlp news
     * [10]nlp progress
     * [11]contact

   24 september 2016 / [12]id27s

on id27s - part 3: the secret ingredients of id97

   on id27s - part 3: the secret ingredients of id97

   this post will discuss the factors that account for the success of
   id97 and its connection to more traditional models.

   table of contents:
     * [13]glove
     * [14]id27s vs. distributional semantic models
     * [15]models
     * [16]hyperparameters
     * [17]results

   excuse the rather clickbait-y title. this is a blog post that i meant
   to write for a while. in this post, i want to highlight the factors,
   i.e. the secret ingredients that account for the success of id97.
   in particular, i want to focus on the connection between word
   embeddings trained via neural models and those produced by traditional
   id65 models (dsms). by showing how these
   ingredients can be transferred to dsms, i will demonstrate that
   distributional methods are in no way inferior to the popular word
   embedding methods.
   even though this is no new insight, i feel that traditional methods are
   frequently overshadowed amid the deep learning craze and their
   relevancy consequently deserves to be mentioned more often.

   to this effect, the paper on which this blog post is based is improving
   distributional similarity with lessons learned from id27s
   ^[18][1] by levy et al. (2015). if you haven't read it, i recommend you
   to check it out.

   over the course of this blog post, i will first introduce glove, a
   popular id27 model. i will then highlight the connection
   between id27 models and distributional semantic methods.
   subsequently, i will introduce the four models that will be used to
   measure the impact of the different factors. i will then give an
   overview of all additional factors that play a role in learning word
   representations, besides the choice of the algorithm. i will finally
   present the results by levy et al., their takeaways and
   recommendations.

glove

   in a [19]previous blog post, we have given an overview of popular word
   embedding models. one model that we have omitted so far is glove
   ^[20][2].

   briefly, glove seeks to make explicit what sgns does implicitly:
   encoding meaning as vector offsets in an embedding space -- seemingly
   only a serendipitous by-product of id97 -- is the specified goal of
   glove.
   specifically, the authors of glove show that the ratio of the
   co-occurrence probabilities of two words (rather than their
   co-occurrence probabilities themselves) is what contains information
   and aim to encode this information as vector differences.
   to achieve this, they propose a weighted least squares objective \(j\)
   that directly aims to minimise the difference between the dot product
   of the vectors of two words and the logarithm of their number of
   co-occurrences:

   \(j = \sum\limits_{i, j=1}^v f(x_{ij}) (w_i^t \tilde{w}_j + b_i +
   \tilde{b}_j - \text{log} x_{ij})^2 \)

   where \(w_i\) and \(b_i\) are the word vector and bias respectively of
   word \(i\), \(\tilde{w}_j\) and \(b_j\) are the context word vector and
   bias respectively of word \(j\), \(x_{ij}\) is the number of times word
   \(i\) occurs in the context of word \(j\), and \(f\) is a weighting
   function that assigns relatively lower weight to rare and frequent
   co-occurrences.

   as co-occurrence counts can be directly encoded in a word-context
   co-occurrence matrix, glove takes such a matrix rather than the entire
   corpus as input.

   if you want to know more about glove, the best reference is likely
   [21]the paper and the [22]accompanying website. besides that, you can
   find some additional intuitions on glove and its difference to id97
   by the author of gensim [23]here, in [24]this quora thread, and in
   [25]this blog post.

id27s vs. id65 models

   the reason why id27 models, particularly id97 and glove,
   became so popular is that they seemed to continuously and significantly
   outperform dsms. many attributed this to the neural architecture of
   id97 or the fact that it predicts words, which seemed to have a
   natural edge over solely relying on co-occurrence counts.

   we can view dsms as count models as they "count" co-occurrences among
   words by operating on co-occurrence matrices. in contrast, neural word
   embedding models can be seen as predict models, as they try to predict
   surrounding words.

   in 2014, baroni et al. ^[26][3] showed that predict models consistently
   outperform count models in almost all tasks, thus providing a clear
   verification for the apparent superiority of id27 models. is
   this the end? no.

   already with glove we've seen that the differences are not as
   clear-cut: while glove is considered a predict model by levy et al.
   (2015), it is clearly factorizing a word-context co-occurrence matrix,
   which brings it close to traditional methods such as pca and lsa. even
   more, levy et al. ^[27][4] demonstrate that id97 implicitly
   factorizes a word-context pmi matrix.

   consequently, while on the surface dsms and id27 models use
   different algorithms to learn word representations -- the first count,
   the latter predict -- fundamentally, both types of models act on the
   same underlying statistics of the data, i.e. the co-occurrence counts
   between words.

   thus, the question that still remains and which we will dedicate the
   rest of this blog post to answering is the following:
   why do id27 models still perform better than dsm with almost
   the same information?

models

   following levy et al. (2015), we will isolate and identify the factors
   that account for the success of neural id27 models and show
   how these can be transferred to traditional methods by comparing the
   following four models:
     * positive pointwise mutual information (ppmi): pmi is a common
       measure for the strength of association between two words. it is
       defined as the log ratio between the joint id203 of two words
       \(w\) and \(c\) and the product of their marginal probabilities:
       \(pmi(w,c) = \text{log} \dfrac{p(w,c)}{p(w) p(c)} \). as \(
       pmi(w,c) = \text{log} 0 = - \infty \) for pairs \( (w,c) \) that
       were never observed, pmi is in practice often replaced with
       positive pmi (ppmi), which replaces negative values with \(0\),
       yielding \(ppmi(w,c) = \text{max}(pmi(w,c),0)\).
     * singular value decomposition (svd): svd is one of the most popular
       methods for id84 and found its into nlp
       originally via latent semantic analysis (lsa). svd factories the
       word-context co-occurrence matrix into the product of three
       matrices \(u \cdot \sigma \times v^t \) where \(u\) and \(v\) are
       orthonormal matrices (i.e. square matrices whose rows and columns
       are orthogonal unit vectors) and \(\sigma\) is a diagonal matrix of
       eigenvalues in
       decreasing order. in practice, svd is often used to factorize the
       matrix produced by ppmi. generally, only the top \(d\) elements of
       \(\sigma\) are kept, yielding \(w^{svd} = u_d \cdot \sigma_d\) and
       \(c^{svd} = v_d\), which are typically used as the word and context
       representations respectively.
     * skip-gram with negative sampling (sgns) aka id97: to learn more
       about the skip-gram architecture and the negative sampling refer to
       my previous blog posts [28]here and [29]here respectively.
     * global vectors (glove) as presented in the previous section.

hyperparameters

   we will look at the following hyper-parameters:
     * [30]pre-processing
          + [31]dynamic context window
          + [32]subsampling frequent words
          + [33]deleting rare words
     * [34]association metric
          + [35]shifted pmi
          + [36]context distribution smoothing
     * [37]post-processing
          + [38]adding context vectors
          + [39]eigenvalue weighting
          + [40]vector normalisation

pre-processing

   id97 introduces three ways of pre-processing a corpus, which can be
   easily applied to dsms.

dynamic context window

   in dsms traditionally, the context window is unweighted and of a
   constant size. both sgns and glove, however, use a scheme that assigns
   more weight to closer words, as closer words are generally considered
   to be more important to a word's meaning. in sgns, this weighting is
   implemented by having a dynamic window size that is sampled uniformly
   between \(1\) and the maximum window size during training.

subsampling frequent words

   sgns dilutes very frequent words by randomly removing words whose
   frequency \(f\) is higher than some threshold \(t\) with a id203
   \(p = 1 - \sqrt{\dfrac{t}{f}}\). as this subsampling is done before
   actually creating the windows, the context windows used by sgns in
   practice are larger than indicated by the context window size.

deleting rare words

   in the pre-processing of sgns, rare words are also deleted before
   creating the context windows, which increases the actual size of the
   context windows further. levy et al. (2015) find this not to have a
   significant performance impact, though.

association metric

   pmi has been shown to be an effective metric for measuring the
   association between words. since levy and goldberg (2014) have shown
   sgns to implicitly factorize a pmi matrix, two variations id30 from
   this formulation can be introduced to regular pmi.

shifted pmi

   in sgns, the higher the number of negative samples \(k\), the more data
   is being used and the better should be the estimation of the
   parameters. \(k\) affects the shift of the pmi matrix that is
   implicitly factorized by id97, i.e. \(k\) k shifts the pmi values
   by log \(k\).
   if we transfer this to regular pmi, we obtain shifted ppmi (sppmi):
   \(sppmi(w,c) = \text{max}(pmi(w,c) - \text{log} k,0)\).

context distribution smoothing

   in sgns, the negative samples are sampled according to a smoothed
   unigram distribution, i.e. an unigram distribution raised to the power
   of \(\alpha\), which is empirically set to \(\dfrac{3}{4}\). this leads
   to frequent words being sampled relatively less often than their
   frequency would indicate.
   we can transfer this to pmi by equally raising the frequency of the
   context words \(f(c)\) to the power of \(\alpha\):
   \(pmi(w, c) = \text{log} \dfrac{p(w,c)}{p(w)p_\alpha(c)}\) where
   \(p_\alpha(c) = \dfrac{f(c)^\alpha}{\sum_c f(c)^\alpha}\) and \(f(x)\)
   is the frequency of word \(x\).

post-processing

   similar as in pre-processing, three methods can be used to modify the
   word vectors produced by an algorithm.

adding context vectors

   the authors of glove propose to add word vectors and context vectors to
   create the final output vectors, e.g. \(\vec{v}_{\text{cat}} =
   \vec{w}_{\text{cat}} + \vec{c}_{\text{cat}}\). this adds first-order
   similarity terms, i.e \(w \cdot v\). however, this method cannot be
   applied to pmi, as the vectors produced by pmi are sparse.

eigenvalue weighting

   svd produces the following matrices: \(w^{svd} = u_d \cdot \sigma_d \)
   and \(c^{svd} = v_d\). these matrices, however, have different
   properties: \(c^{svd}\) is orthonormal, while \(w^{svd}\) is not.
   sgns, in contrast, is more symmetric. we can thus weight the eigenvalue
   matrix \(\sigma_d\) with an additional parameter \(p\), which can be
   tuned, to yield the following:
   \(w^{svd} = u_d \cdot \sigma_d^p\).

vector normalisation

   finally, we can also normalise all vectors to unit length.

results

   levy et al. (2015) train all models on a dump of the english wikipedia
   and evaluate them on the commonly used word similarity and analogy
   datasets. you can read more about the experimental setup and training
   details in their paper. we summarise the most important results and
   takeaways below.

takeaways

   levy et al. find that svd -- and not one of the id27
   algorithms -- performs best on similarity tasks, while sgns performs
   best on analogy datasets. they furthermore shed light on the importance
   of hyperparameters compared to other choices:
    1. hyperparameters vs. algorithms:
       hyperparameter settings are often more important than algorithm
       choice.
       no single algorithm consistently outperforms the other methods.
    2. hyperparameters vs. more data:
       training on a larger corpus helps for some tasks.
       in 3 out of 6 cases, tuning hyperparameters is more beneficial.

debunking prior claims

   equipped with these insights, we can now debunk some generally held
   claims:
    1. are embeddings superior to distributional methods?
       with the right hyperparameters, no approach has a consistent
       advantage over another.
    2. is glove superior to sgns?
       sgns outperforms glove on all tasks.
    3. is cbow a good id97 configuration?
       cbow does not outperform sgns on any task.

recommendations

   finally -- and one of the things i like most about the paper -- we can
   give concrete practical recommendations:
     * don't use shifted ppmi with svd.
     * don't use svd "correctly", i.e. without eigenvector weighting
       (performance drops 15 points compared to with eigenvalue weighting
       with \(p = 0.5\)).
     * do use ppmi and svd with short contexts (window size of \(2\)).
     * do use many negative samples with sgns.
     * do always use context distribution smoothing (raise unigram
       distribution to the power of \(\alpha = 0.75\)) for all methods.
     * do use sgns as a baseline (robust, fast and cheap to train).
     * do try adding context vectors in sgns and glove.

conclusions

   these results run counter to what is generally assumed, namely that
   id27s are superior to traditional methods and indicate that
   it generally makes no difference whatsoever whether you use word
   embeddings or distributional methods -- what matters is that you tune
   your hyperparameters and employ the appropriate pre-processing and
   post-processing steps.

   recent papers from jurafsky's group ^[41][5] ^[42][6] echo these
   findings and show that svd -- not sgns -- is often the preferred choice
   when you care about accurate word representations.

   i hope this blog post was useful in highlighting cool research that
   sheds light on the link between traditional distributional semantic and
   in-vogue embedding models. as we've seen, knowledge of distributional
   semantics allows us to improve upon our current methods and develop
   entirely new variations of existing ones. for this reason, i hope that
   the next time you train id27s, you will consider adding
   distributional methods to your toolbox or lean on them for inspiration.

   as always, feel free to ask questions and point out the mistakes i made
   in this blog post in the comments below.

other blog posts on id27s

   if you want to learn more about id27s, these other blog posts
   on id27s are also available:
     * [43]on id27s - part 1
     * [44]on id27s - part 2: approximating the softmax
     * [45]unofficial part 4: a survey of cross-lingual embedding models
     * [46]unofficial part 5: id27s in 2017 - trends and future
       directions

   cover images are courtesy of [47]stanford.
     __________________________________________________________________

    1. levy, o., goldberg, y., & dagan, i. (2015). improving
       distributional similarity with lessons learned from word
       embeddings. transactions of the association for computational
       linguistics, 3, 211   225. retrieved from
       [48]https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/vie
       w/570 [49]      
    2. pennington, j., socher, r., & manning, c. d. (2014). glove: global
       vectors for word representation. proceedings of the 2014 conference
       on empirical methods in natural language processing, 1532   1543.
       [50]http://doi.org/10.3115/v1/d14-1162 [51]      
    3. baroni, m., dinu, g., & kruszewski, g. (2014). don   t count,
       predict! a systematic comparison of context-counting vs.
       context-predicting semantic vectors. acl, 238   247.
       [52]http://doi.org/10.3115/v1/p14-1023 [53]      
    4. levy, o., & goldberg, y. (2014). neural id27 as implicit
       id105. advances in neural information processing
       systems (nips), 2177   2185. retrieved from
       [54]http://papers.nips.cc/paper/5477-neural-word-embedding-as-impli
       cit-matrix-factorization [55]      
    5. hamilton, w. l., clark, k., leskovec, j., & jurafsky, d. (2016).
       inducing domain-specific sentiment lexicons from unlabeled corpora.
       proceedings of the 54th annual meeting of the association for
       computational linguistics. retrieved from
       [56]http://arxiv.org/abs/1606.02820 [57]      
    6. hamilton, w. l., leskovec, j., & jurafsky, d. (2016). diachronic
       id27s reveal statistical laws of semantic change. arxiv
       preprint arxiv:1605.09096. [58]      

   sebastian ruder

[59]sebastian ruder

   read [60]more posts by this author.
   [61]read more

       sebastian ruder    

[62]id27s

     * [63]aaai 2019 highlights: dialogue, reproducibility, and more
     * [64]emnlp 2018 highlights: inductive bias, cross-lingual learning,
       and more
     * [65]a review of the neural history of natural language processing

   [66]see all 9 posts    

   [67]highlights of emnlp 2016: dialogue, deep learning, and more

   natural language processing

highlights of emnlp 2016: dialogue, deep learning, and more

   this post discusses highlights of the 2016 conference on empirical
   methods in natural language processing (emnlp 2016). these include work
   on id23, dialogue, sequence-to-sequence models,
   id29, id86, and many more.

     * sebastian ruder
       [68]sebastian ruder

   [69]lxmls 2016 highlights

   events

lxmls 2016 highlights

   the lisbon machine learning school (lxmls) is an annual event that
   brings together researchers and graduate students in ml, nlp, and
   computational linguistics. this post discusses highlights, key
   insights, and takeaways from the 6th edition of the summer school.

     * sebastian ruder
       [70]sebastian ruder

   [71]sebastian ruder
      
   on id27s - part 3: the secret ingredients of id97
   share this
   please enable javascript to view the [72]comments powered by disqus.

   [73]sebastian ruder    2019

   [74]latest posts [75]twitter [76]ghost

references

   visible links
   1. http://ruder.io/rss/
   2. http://ruder.io/
   3. http://ruder.io/about/
   4. http://ruder.io/tags/
   5. http://ruder.io/publications/
   6. http://ruder.io/talks/
   7. http://ruder.io/news/
   8. http://ruder.io/faq/
   9. http://ruder.io/nlp-news/
  10. https://nlpprogress.com/
  11. http://ruder.io/contact/
  12. http://ruder.io/tag/word-embeddings/index.html
  13. http://ruder.io/secret-id97/index.html#glove
  14. http://ruder.io/secret-id97/index.html#wordembeddingsvsdistributionalsemanticsmodels
  15. http://ruder.io/secret-id97/index.html#models
  16. http://ruder.io/secret-id97/index.html#hyperparameters
  17. http://ruder.io/secret-id97/index.html#results
  18. http://ruder.io/secret-id97/index.html#fn1
  19. http://ruder.io/word-embeddings-1/index.html
  20. http://ruder.io/secret-id97/index.html#fn2
  21. http://www.aclweb.org/anthology/d14-1162
  22. http://nlp.stanford.edu/projects/glove/
  23. http://rare-technologies.com/making-sense-of-id97/
  24. https://www.quora.com/how-is-glove-different-from-id97
  25. https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html
  26. http://ruder.io/secret-id97/index.html#fn3
  27. http://ruder.io/secret-id97/index.html#fn4
  28. http://ruder.io/word-embeddings-1/index.html
  29. http://ruder.io/word-embeddings-softmax/index.html
  30. http://ruder.io/secret-id97/index.html#preprocessing
  31. http://ruder.io/secret-id97/index.html#dynamiccontextwindow
  32. http://ruder.io/secret-id97/index.html#subsamplingfrequentwords
  33. http://ruder.io/secret-id97/index.html#deletingrarewords
  34. http://ruder.io/secret-id97/index.html#associationmetric
  35. http://ruder.io/secret-id97/index.html#shiftedpmi
  36. http://ruder.io/secret-id97/index.html#contextdistributionsmoothing
  37. http://ruder.io/secret-id97/index.html#postprocessing
  38. http://ruder.io/secret-id97/index.html#addingcontextvectors
  39. http://ruder.io/secret-id97/index.html#eigenvalueweighting
  40. http://ruder.io/secret-id97/index.html#vectornormalisation
  41. http://ruder.io/secret-id97/index.html#fn5
  42. http://ruder.io/secret-id97/index.html#fn6
  43. http://ruder.io/word-embeddings-1/index.html
  44. http://ruder.io/word-embeddings-softmax/index.html
  45. http://ruder.io/cross-lingual-embeddings/index.html
  46. http://ruder.io/word-embeddings-2017/index.html
  47. http://nlp.stanford.edu/projects/glove/
  48. https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570
  49. http://ruder.io/secret-id97/index.html#fnref1
  50. http://doi.org/10.3115/v1/d14-1162
  51. http://ruder.io/secret-id97/index.html#fnref2
  52. http://doi.org/10.3115/v1/p14-1023
  53. http://ruder.io/secret-id97/index.html#fnref3
  54. http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization
  55. http://ruder.io/secret-id97/index.html#fnref4
  56. http://arxiv.org/abs/1606.02820
  57. http://ruder.io/secret-id97/index.html#fnref5
  58. http://ruder.io/secret-id97/index.html#fnref6
  59. http://ruder.io/author/sebastian/index.html
  60. http://ruder.io/author/sebastian/index.html
  61. http://ruder.io/author/sebastian/index.html
  62. http://ruder.io/tag/word-embeddings/index.html
  63. http://ruder.io/aaai-2019-highlights/index.html
  64. http://ruder.io/emnlp-2018-highlights/index.html
  65. http://ruder.io/a-review-of-the-recent-history-of-nlp/index.html
  66. http://ruder.io/tag/word-embeddings/index.html
  67. http://ruder.io/index.html
  68. http://ruder.io/author/sebastian/index.html
  69. http://ruder.io/index.html
  70. http://ruder.io/author/sebastian/index.html
  71. http://ruder.io/
  72. https://disqus.com/?ref_noscript
  73. http://ruder.io/
  74. http://ruder.io/
  75. https://twitter.com/seb_ruder
  76. https://ghost.org/

   hidden links:
  78. https://twitter.com/seb_ruder
  79. http://ruder.io/rss/index.rss
  80. http://ruder.io/index.html
  81. http://ruder.io/index.html
  82. https://twitter.com/share?text=on%20word%20embeddings%20-%20part%203%3a%20the%20secret%20ingredients%20of%20id97&url=http://ruder.io/secret-id97/
  83. https://www.facebook.com/sharer/sharer.php?u=http://ruder.io/secret-id97/
