   #[1]github [2]recent commits to speech-to-text-wavenet:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]192
     * [35]star [36]2,974
     * [37]fork [38]624

[39]buriburisuri/[40]speech-to-text-wavenet

   [41]code [42]issues 70 [43]pull requests 3 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   speech-to-text-wavenet : end-to-end sentence level english speech
   recognition based on deepmind's wavenet and tensorflow
     * [47]27 commits
     * [48]2 branches
     * [49]0 releases
     * [50]fetching contributors
     * [51]apache-2.0

    1. [52]python 100.0%

   (button) python
   branch: master (button) new pull request
   [53]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/b
   [54]download zip

downloading...

   want to be notified of new releases in
   buriburisuri/speech-to-text-wavenet?
   [55]sign in [56]sign up

launching github desktop...

   if nothing happens, [57]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [59]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [60]download the github extension for visual studio
   and try again.

   (button) go back
   [61]@buriburisuri
   [62]buriburisuri [63]merge pull request [64]#62 [65]from
   mcfletch/master (button)    
make the preprocessing step check for end-products before processing

   latest commit [66]fe66a5f apr 9, 2017
   [67]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [68]docker [69]add auto directory creation in the preprocess.py apr 3,
   2017
   [70]png
   [71].gitignore
   [72]changelog.md [73]add auto directory creation in the preprocess.py
   apr 3, 2017
   [74]license
   [75]readme.md
   [76]data.py [77]add auto directory creation in the preprocess.py apr 3,
   2017
   [78]model.py
   [79]preprocess.py
   [80]recognize.py [81]add auto directory creation in the preprocess.py
   apr 3, 2017
   [82]requirements.txt
   [83]test.py
   [84]train.py

readme.md

speech-to-text-wavenet : end-to-end sentence level english id103
using deepmind's wavenet

   a tensorflow implementation of id103 based on deepmind's
   [85]wavenet: a generative model for raw audio. (hereafter the paper)

   although [86]ibab and [87]toid113paine have already implemented wavenet
   with tensorflow, they did not implement id103. that's why
   we decided to implement it ourselves.

   some of deepmind's recent papers are tricky to reproduce. the paper
   also omitted specific details about the implementation, and we had to
   fill the gaps in our own way.

   here are a few important notes.

   first, while the paper used the timit dataset for the speech
   recognition experiment, we used the free vtck dataset.

   second, the paper added a mean-pooling layer after the dilated
   convolution layer for down-sampling. we extracted [88]mfcc from wav
   files and removed the final mean-pooling layer because the original
   setting was impossible to run on our titanx gpu.

   third, since the timit dataset has phoneme labels, the paper trained
   the model with two loss terms, phoneme classification and next phoneme
   prediction. we, instead, used a single ctc loss because vctk provides
   sentence-level labels. as a result, we used only dilated conv1d layers
   without any dilated conv1d layers.

   finally, we didn't do quantitative analyses such as id7 score and
   post-processing by combining a language model due to the time
   constraints.

   the final architecture is shown in the following figure.

                           [89][architecture.png]
   (some images are cropped from [wavenet: a generative model for raw
   audio]([90]https://arxiv.org/abs/1609.03499) and [neural machine
   translation in linear time]([91]https://arxiv.org/abs/1610.10099))

version

   current version : 0.0.0.2

dependencies ( version must be matched exactly! )

    1. [92]tensorflow == 1.0.0
    2. [93]sugartensor == 1.0.0.2
    3. [94]pandas >= 0.19.2
    4. [95]librosa == 0.5.0
    5. [96]scikits.audiolab==0.11.0

   if you have problems with the librosa library, try to install ffmpeg by
   the following command. ( ubuntu 14.04 )
sudo add-apt-repository ppa:mc3man/trusty-media
sudo apt-get update
sudo apt-get dist-upgrade -y
sudo apt-get -y install ffmpeg

dataset

   we used [97]vctk, [98]librispeech and [99]tedlium release 2 corpus.
   total number of sentences in the training set composed of the above
   three corpus is 240,612. valid and test set is built using only
   librispeech and tedlium corpuse, because vctk corpus does not have
   valid and test set. after downloading the each corpus, extract them in
   the 'asset/data/vctk-corpus', 'asset/data/librispeech' and
   'asset/data/tedlium_release2' directories.

   audio was augmented by the scheme in the [100]tom ko et al's paper.
   (thanks @migvel for your kind information)

pre-processing dataset

   the tedlium release 2 dataset provides audio data in the sph format, so
   we should convert them to some format librosa library can handle. run
   the following command in the 'asset/data' directory convert sph to wave
   format.
find -type f -name '*.sph' | awk '{printf "sox -t sph %s -b 16 -t wav %s\n", $0,
 $0".wav" }' | bash

   if you don't have installed sox, please installed it first.
sudo apt-get install sox

   we found the main bottle neck is disk read time when training, so we
   decide to pre-process the whole audio data into the mfcc feature files
   which is much smaller. and we highly recommend using ssd instead of
   hard drive.
   run the following command in the console to pre-process whole dataset.
python preprocess.py

training the network

   execute
python train.py ( <== use all available gpus )
or
cuda_visible_devices=0,1 python train.py ( <== use only gpu 0, 1 )

   to train the network. you can see the result ckpt files and log files
   in the 'asset/train' directory. launch tensorboard --logdir
   asset/train/log to monitor training process.

   we've trained this model on a 3 nvidia 1080 pascal gpus during 40 hours
   until 50 epochs and we picked the epoch when the validatation loss is
   minimum. in our case, it is epoch 40. if you face the out of memory
   error, reduce batch_size in the train.py file from 16 to 4.

   the ctc losses at each epoch are as following table:
   epoch train set valid set  test set
    20   79.541500 73.645237 83.607269
    30   72.884180 69.738348 80.145867
    40   69.948266 66.834316 77.316114
    50   69.127240 67.639895 77.866674

testing the network

   after training finished, you can check valid or test set ctc loss by
   the following command.
python test.py --set train|valid|test --frac 1.0(0.01~1.0)

   the frac option will be useful if you want to test only the fraction of
   dataset for fast evaluation.

transforming speech wave file to english text

   execute
python recognize.py --file

   to transform a speech wave file to the english sentence. the result
   will be printed on the console.

   for example, try the following command.
python recognize.py --file asset/data/librispeech/test-clean/1089/134686/1089-13
4686-0000.flac
python recognize.py --file asset/data/librispeech/test-clean/1089/134686/1089-13
4686-0001.flac
python recognize.py --file asset/data/librispeech/test-clean/1089/134686/1089-13
4686-0002.flac
python recognize.py --file asset/data/librispeech/test-clean/1089/134686/1089-13
4686-0003.flac
python recognize.py --file asset/data/librispeech/test-clean/1089/134686/1089-13
4686-0004.flac

   the result will be as follows:
he hoped there would be stoo for dinner turnips and charrats and bruzed patatos
and fat mutton pieces to be ladled out in th thick peppered flower fatan sauce
stuffid into you his belly counsiled him
after early night fall the yetl lampse woich light hop here and there on the squ
alled quarter of the browfles
o berty and he god in your mind
numbrt tan fresh nalli is waiting on nou cold nit husband

   the ground truth is as follows:
he hoped there would be stew for dinner turnips and carrots and bruised potatoes
 and fat mutton pieces to be ladled out in thick peppered flour fattened sauce
stuff it into you his belly counselled him
after early nightfall the yellow lamps would light up here and there the squalid
 quarter of the brothels
hello bertie any good in your mind
number ten fresh nelly is waiting on you good night husband

   as mentioned earlier, there is no language model, so there are some
   cases where capital letters, punctuations, and words are misspelled.

pre-trained models

   you can transform a speech wave file to english text with the
   pre-trained model on the vctk corpus. extract [101]the following zip
   file to the 'asset/train/' directory.

docker support

   see docker [102]readme.md.

future works

    1. language model
    2. polyglot(multi-lingual) model

   we think that we should replace ctc beam decoder with a practical
   language model
   and the polyglot id103 model will be a good candidate to
   future works.

other resources

    1. [103]ibab's wavenet(id133) tensorflow implementation
    2. [104]toid113paine's fast wavenet(id133) tensorflow
       implementation

namju's other repositories

    1. [105]sugartensor
    2. [106]ebgan tensorflow implementation
    3. [107]timeseries gan tensorflow implementation
    4. [108]supervised infogan tensorflow implementation
    5. [109]ac-gan tensorflow implementation
    6. [110]srgan tensorflow implementation
    7. [111]bytenet-fast id4

citation

   if you find this code useful please cite us in your work:
kim and park. speech-to-text-wavenet. 2016. github repository. https://github.co
m/buriburisuri/.

authors

   namju kim ([112]namju.kim@kakaocorp.com) at kakaobrain corp.

   kyubyong park ([113]kbpark@jamonglab.com) at kakaobrain corp.

     *    2019 github, inc.
     * [114]terms
     * [115]privacy
     * [116]security
     * [117]status
     * [118]help

     * [119]contact github
     * [120]pricing
     * [121]api
     * [122]training
     * [123]blog
     * [124]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [125]reload to refresh your
   session. you signed out in another tab or window. [126]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/buriburisuri/speech-to-text-wavenet/commits/master.atom
   3. https://github.com/buriburisuri/speech-to-text-wavenet#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/buriburisuri/speech-to-text-wavenet
  32. https://github.com/join
  33. https://github.com/login?return_to=/buriburisuri/speech-to-text-wavenet
  34. https://github.com/buriburisuri/speech-to-text-wavenet/watchers
  35. https://github.com/login?return_to=/buriburisuri/speech-to-text-wavenet
  36. https://github.com/buriburisuri/speech-to-text-wavenet/stargazers
  37. https://github.com/login?return_to=/buriburisuri/speech-to-text-wavenet
  38. https://github.com/buriburisuri/speech-to-text-wavenet/network/members
  39. https://github.com/buriburisuri
  40. https://github.com/buriburisuri/speech-to-text-wavenet
  41. https://github.com/buriburisuri/speech-to-text-wavenet
  42. https://github.com/buriburisuri/speech-to-text-wavenet/issues
  43. https://github.com/buriburisuri/speech-to-text-wavenet/pulls
  44. https://github.com/buriburisuri/speech-to-text-wavenet/projects
  45. https://github.com/buriburisuri/speech-to-text-wavenet/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/buriburisuri/speech-to-text-wavenet/commits/master
  48. https://github.com/buriburisuri/speech-to-text-wavenet/branches
  49. https://github.com/buriburisuri/speech-to-text-wavenet/releases
  50. https://github.com/buriburisuri/speech-to-text-wavenet/graphs/contributors
  51. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/license
  52. https://github.com/buriburisuri/speech-to-text-wavenet/search?l=python
  53. https://github.com/buriburisuri/speech-to-text-wavenet/find/master
  54. https://github.com/buriburisuri/speech-to-text-wavenet/archive/master.zip
  55. https://github.com/login?return_to=https://github.com/buriburisuri/speech-to-text-wavenet
  56. https://github.com/join?return_to=/buriburisuri/speech-to-text-wavenet
  57. https://desktop.github.com/
  58. https://desktop.github.com/
  59. https://developer.apple.com/xcode/
  60. https://visualstudio.github.com/
  61. https://github.com/buriburisuri
  62. https://github.com/buriburisuri/speech-to-text-wavenet/commits?author=buriburisuri
  63. https://github.com/buriburisuri/speech-to-text-wavenet/commit/fe66a5fca6ae8f7ebb7a58e91fff97dc82e891b6
  64. https://github.com/buriburisuri/speech-to-text-wavenet/pull/62
  65. https://github.com/buriburisuri/speech-to-text-wavenet/commit/fe66a5fca6ae8f7ebb7a58e91fff97dc82e891b6
  66. https://github.com/buriburisuri/speech-to-text-wavenet/commit/fe66a5fca6ae8f7ebb7a58e91fff97dc82e891b6
  67. https://github.com/buriburisuri/speech-to-text-wavenet/tree/fe66a5fca6ae8f7ebb7a58e91fff97dc82e891b6
  68. https://github.com/buriburisuri/speech-to-text-wavenet/tree/master/docker
  69. https://github.com/buriburisuri/speech-to-text-wavenet/commit/a5673c139a13748ba1fdb5b871451f11939db02f
  70. https://github.com/buriburisuri/speech-to-text-wavenet/tree/master/png
  71. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/.gitignore
  72. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/changelog.md
  73. https://github.com/buriburisuri/speech-to-text-wavenet/commit/a5673c139a13748ba1fdb5b871451f11939db02f
  74. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/license
  75. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/readme.md
  76. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/data.py
  77. https://github.com/buriburisuri/speech-to-text-wavenet/commit/a5673c139a13748ba1fdb5b871451f11939db02f
  78. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/model.py
  79. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/preprocess.py
  80. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/recognize.py
  81. https://github.com/buriburisuri/speech-to-text-wavenet/commit/a5673c139a13748ba1fdb5b871451f11939db02f
  82. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/requirements.txt
  83. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/test.py
  84. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/train.py
  85. https://arxiv.org/abs/1609.03499
  86. https://github.com/ibab/tensorflow-wavenet
  87. https://github.com/toid113paine/fast-wavenet
  88. https://en.wikipedia.org/wiki/mel-frequency_cepstrum
  89. https://raw.githubusercontent.com/buriburisuri/speech-to-text-wavenet/master/png/architecture.png
  90. https://arxiv.org/abs/1609.03499
  91. https://arxiv.org/abs/1610.10099
  92. https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation
  93. https://github.com/buriburisuri/sugartensor
  94. http://pandas.pydata.org/pandas-docs/stable/install.html
  95. https://github.com/librosa/librosa
  96. https://pypi.python.org/pypi/scikits.audiolab
  97. http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html
  98. http://www.openslr.org/12/
  99. http://www-lium.univ-lemans.fr/en/content/ted-lium-corpus
 100. http://speak.clsp.jhu.edu/uploads/publications/papers/1050_pdf.pdf
 101. https://drive.google.com/open?id=0b3ilzkxzcruyvwwtt25femzez1k
 102. https://github.com/buriburisuri/speech-to-text-wavenet/blob/master/docker/readme.md
 103. https://github.com/ibab/tensorflow-wavenet
 104. https://github.com/ibab/tensorflow-wavenet
 105. https://github.com/buriburisuri/sugartensor
 106. https://github.com/buriburisuri/ebgan
 107. https://github.com/buriburisuri/timeseries_gan
 108. https://github.com/buriburisuri/supervised_infogan
 109. https://github.com/buriburisuri/ac-gan
 110. https://github.com/buriburisuri/srgan
 111. https://github.com/buriburisuri/bytenet
 112. mailto:namju.kim@kakaocorp.com
 113. mailto:kbpark@jamonglab.com
 114. https://github.com/site/terms
 115. https://github.com/site/privacy
 116. https://github.com/security
 117. https://githubstatus.com/
 118. https://help.github.com/
 119. https://github.com/contact
 120. https://github.com/pricing
 121. https://developer.github.com/
 122. https://training.github.com/
 123. https://github.blog/
 124. https://github.com/about
 125. https://github.com/buriburisuri/speech-to-text-wavenet
 126. https://github.com/buriburisuri/speech-to-text-wavenet

   hidden links:
 128. https://github.com/
 129. https://github.com/buriburisuri/speech-to-text-wavenet
 130. https://github.com/buriburisuri/speech-to-text-wavenet
 131. https://github.com/buriburisuri/speech-to-text-wavenet
 132. https://help.github.com/articles/which-remote-url-should-i-use
 133. https://github.com/buriburisuri/speech-to-text-wavenet#speech-to-text-wavenet--end-to-end-sentence-level-english-speech-recognition-using-deepminds-wavenet
 134. https://github.com/buriburisuri/speech-to-text-wavenet#version
 135. https://github.com/buriburisuri/speech-to-text-wavenet#dependencies--version-must-be-matched-exactly-
 136. https://github.com/buriburisuri/speech-to-text-wavenet#dataset
 137. https://github.com/buriburisuri/speech-to-text-wavenet#pre-processing-dataset
 138. https://github.com/buriburisuri/speech-to-text-wavenet#training-the-network
 139. https://github.com/buriburisuri/speech-to-text-wavenet#testing-the-network
 140. https://github.com/buriburisuri/speech-to-text-wavenet#transforming-speech-wave-file-to-english-text
 141. https://github.com/buriburisuri/speech-to-text-wavenet#pre-trained-models
 142. https://github.com/buriburisuri/speech-to-text-wavenet#docker-support
 143. https://github.com/buriburisuri/speech-to-text-wavenet#future-works
 144. https://github.com/buriburisuri/speech-to-text-wavenet#other-resources
 145. https://github.com/buriburisuri/speech-to-text-wavenet#namjus-other-repositories
 146. https://github.com/buriburisuri/speech-to-text-wavenet#citation
 147. https://github.com/buriburisuri/speech-to-text-wavenet#authors
 148. https://github.com/
