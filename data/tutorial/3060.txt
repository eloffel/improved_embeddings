   #[1]salesforce research

   [2]salesforce logo [3]mission [4]research [5]careers [6]ethics
   [7]products [8]press

   [9]publications [10]blog [11]open source [12]grants

   [13]salesforce logo [14]   
   [15]mission [16]research [17]publications [18]blog [19]open source
   [20]grants [21]careers [22]ethics [23]products [24]press

   [25]salesforce logo [26]mission [27]research [28]careers [29]ethics
   [30]products [31]press

   [32]publications [33]blog [34]open source [35]grants

state of the art deep learning model for id53

   by: [36]victor zhong

   we introduce the dynamic coattention network, a state of the art neural
   network designed to automatically answer questions about documents.
   instead of producing a single, static representation of the document
   without context, our system is able to interpret the document
   differently depending on the question. that is, given the same
   document, the system constructs a different understanding depending on
   the question (e.g. which team represented the nfc in superbowl 50, who
   scored the touchdown in the fourth quarter). based on this conditional
   interpretation, our system iteratively hypothesizes multiple answers,
   allowing it to adjust initially misguided predictions. the dynamic
   coattention network achieves a test f1 score, a measure of similarity
   between the predicted answer and the human annotated answer, of 80.4%
   on the stanford id53 dataset, and significantly
   outperforms other systems developed by the allen institute for
   artificial intelligence, microsoft research asia, google, and ibm.
   details about the dynamic coattention network can be found in [37]our
   paper. the leaderboard for the stanford id53 task can be
   [38]found here.

open-domain id53

   id53 remains one of the most difficult challenges we face
   in natural language processing. the idea of creating an agent capable
   of open-domain id53 - answering arbitrary questions with
   respect to arbitrary documents - has long captured our imagination. an
   agent that responds in natural language rather than by lists of query
   results (as in search) takes on almost anthropomorphic qualities, and
   spurs the imagination to think about the future of artificial
   intelligence.

   the path to open-domain id53 has been long and
   challenging. one crucial problem that has dogged researchers on this
   path has been the lack of large-scale datasets. traditional question
   answering datasets, such as [39]mctest, have been high in quality.
   however, the cost of annotating such datasets with human experts have
   been prohibitively expensive, thus keeping them small. recently,
   researchers have devised techniques to create arbitrarily large
   cloze-form id53 datasets. these cloze-form datasets, such
   as the [40]id98/dailymail corpus are created by replacing an entity with
   a placeholder, thereby creating a problem similar to fill-in-the-blank.
   namely, the task is to infer the missing entity by choosing amongst all
   the entities that appear in the document. the cloze-form question
   answering task is not as natural as open-domain id53, but
   the ease with which cloze-form datasets can be created has led to
   dramatic progress in the development of expressive models such as deep
   neural networks for id53.

   a cloze-form id53 example:

     in 2004, ___ received national attention during his campaign to
     represent illinois in the united states senate with his victory in
     the march democratic party primary, his keynote address at the
     democratic national convention in july, and his election to the
     senate in november. he began his presidential campaign in 2007 and,
     after a close primary campaign against hillary clinton in 2008, he
     won sufficient delegates in the democratic party primaries to
     receive the presidential nomination. he then defeated republican
     nominee john mccain in the general election, and was inaugurated as
     president on january 20, 2009. nine months after his inauguration,
     obama was controversially named the 2009 nobel peace prize laureate.

   just this year, the [41]stanford natural language processing group has
   released yet another large-scale id53 dataset, [42]squad.
   however, unlike the existing cloze-form datasets, the answers to
   questions are spans within the document. this dataset is not only large
   enough to allow the development of expressive models, but natural in
   its task formulation. the squad dataset is comprised of articles from
   english wikepedia and annotated solely by workers on amazon mechanical
   turk.

   a squad like id53 example:

   in 2004, obama received national attention during his campaign to
   represent illinois in the united states senate with his victory in the
   march democratic party primary, his keynote address at the democratic
   national convention in july, and his election to the senate in
   november. he began his presidential campaign in 2007 and, after a close
   primary campaign against hillary clinton in 2008, he won sufficient
   delegates in the democratic party primaries to receive the presidential
   nomination. he then defeated republican nominee john mccain in the
   general election, and was inaugurated as president on january 20, 2009.
   nine months after his inauguration, obama was controversially named the
   2009 nobel peace prize laureate.

   when was obama's keynote address?

     july

   who did obama campaign against in 2008?

     hillary clinton

   where was the keynote address?

     democratic national convention

teaching machines to read

   some of the earliest id53 systems date back to
   [43]baseball and [44]studentin the 1960's. these systems tended to be
   limited in domain, but they are nevertheless telling of our fascination
   with autonomous agents that can understand and communicate in natural
   language to answer questions.

   in recent years, the exponential increase in data and in computational
   power has enabled the development of ever more powerful machine
   learning systems. in particular, the resurgence of neural networks has
   led to the wide-spread adoption of deep learning models in domains
   ranging from machine translation to object recognition to speech
   recognition. today, we announce the dynamic coattention network (dcn),
   an end-to-end deep learning system for id53. the dcn
   combines an coattentive encoder with a dynamic decoder. the combination
   of these two techniques allows the dcn to significantly outperform
   other systems on the stanford id53 dataset.

conditional understanding of text

   in most neural network approaches for natural language processing, the
   system builds a static representation of the input document upon which
   to perform id136.

   although this approach has produced remarkable systems for tasks such
   as machine translation, we feel that it is insufficient for question
   answering. the reason behind this intuition is that it is incredibly
   difficult to build a static representation over a document to answer
   arbitrary questions. it is much easier to build a representation over
   the document to answer a single question that is known in advance.

   to make this idea more concrete, let's consider an example. suppose i
   gave you the following document. you can only read this document once
   (don't cheat!)

     in the meantime, on august 1, 1774, an experiment conducted by the
     british clergyman joseph priestley focused sunlight on mercuric
     oxide (hgo) inside a glass tube, which liberated a gas he named
     "dephlogisticated air". he noted that candles burned brighter in the
     gas and that a mouse was more active and lived longer while
     breathing it. after breathing the gas himself, he wrote: "the
     feeling of it to my lungs was not sensibly different from that of
     common air, but i fancied that my breast felt peculiarly light and
     easy for some time afterwards." priestley published his findings in
     1775 in a paper titled "an account of further discoveries in air"
     which was included in the second volume of his book titled
     experiments and observations on different kinds of air. because he
     published his findings first, priestley is usually given priority in
     the discovery.

   do you remember who published the paper? what was his occupation? how
   about the chemical used in his experiments on oxygen? how about when he
   published his findings? what was the name of the paper he published?
   hopefully, you would agree that it is hard to answer these questions
   based on a single reading.

   now, let's try something else. i am going to give you a document and i
   would like you to answer the question "what is needed to make
   combustion happen".

     highly concentrated sources of oxygen promote rapid combustion. fire
     and explosion hazards exist when concentrated oxidants and fuels are
     brought into close proximity; an ignition event, such as heat or a
     spark, is needed to trigger combustion. oxygen is the oxidant, not
     the fuel, but nevertheless the source of most of the chemical energy
     released in combustion. combustion hazards also apply to compounds
     of oxygen with a high oxidative potential, such as peroxides,
     chlorates, nitrates, perchlorates, and dichromates because they can
     donate oxygen to a fire.

   now, read the document one more time to answer the question "what role
   does oxygen play in combustion?".

   the first approach, in which you were forced to cram as much
   information about the document as possible, not knowing what the
   questions will be, is analogous to the traditional approach of building
   a static representation. the second approach, in which you were able to
   read the document again for each question, is analogous to building a
   conditional representation of the document, based on the question.
   hopefully, you'll agree with me that the latter is much easier than the
   former, since you can selectively read the document and discard
   information irrelevant to the question. this is exactly the idea behind
   our coattention encoder, the first of two parts of the dcn.

   for each document and question pair, the coattention encoder builds a
   conditional representation of the document given the question, as well
   as a conditional representation of the question given the document. the
   encoder then builds a final representation of the document, taking into
   account the two previous conditional representations. a subsequent
   decoder module then produces an answer from this final representation.

scanning for an answer

   given a compact representation of the document and the question, how do
   we come up with an answer?

   suppose we have the following document:

     on carolina's next possession fullback mike tolbert lost a fumble
     while being tackled by safety darian stewart, which linebacker danny
     trevathan recovered on the broncos 40-yard line. however, the
     panthers soon took the ball back when defensive end kony ealy tipped
     a manning pass to himself and then intercepted it, returning the
     ball 19 yards to the panthers 39-yard line with 1:55 left on the
     clock. the panthers could not gain any yards with their possession
     and had to punt. after a denver punt, carolina drove to the broncos
     45-yard line. but with 11 seconds left, newton was sacked by
     demarcus ware as time expired in the half.

   and the question:

     who recovered tolbert's fumble?

   even if we do not follow american football, we have some intuition that
   the answer will be a person. now, if we skim the document, we'd focus
   on the people that are mentioned in the document. for example, upon
   immediate inspection, we may hypothesize that mike tolbert recovered
   his own fumble. upon a closer reading, we may think that darian
   stewart, who forced the fumble, also recovered the fumble. the correct
   answer, ultimately, is danny trevathan.

   the dynamic decoder proceeds in a similar fashion, iteratively
   hypothesizing answers until the hypothesis no longer changes.

   the following figure (click to expand) illustrates the model's
   hypothesis during 3 iterations, after which it converges on the correct
   answer "danny trevathan". each row of the figure denotes a hypothesis
   for a position of the span. the first row (in blue) denotes the
   position for the start word during the first iteration, the second row
   (in red) denotes the position for the end word during the first
   iteration and so forth. darker color indicates higher confidence. for
   example, in the first iteration, the model is more confident that the
   answer starts with "fullback", as opposed to "danny", and ends with
   "trevathan", as opposed to "tolbert".
   [squad-qa-attn.svg]

   in the first iteration, the model hypothesizes the answer "fullback
   mike tolbert lost a fumble while being tackled by safety darian
   stewart, which linebacker danny trevathan". this initial hypothesis is
   clearly incorrect. upon examination of the confidence of the hypothesis
   (darker means stronger confidence), we see that there is clearly three
   candidates under consideration; they are "fullback mike tolbert",
   "safety darian stewart", and "danny trevathan". in iterations 2 and 3,
   the model gradually shifts confidence to the answer "danny trevathan",
   ultimately stopping in iteration 3 after which its hypothesis no longer
   changes.

a new state of the art for id53

   by combining the coattention encoder and the dynamic decoder, the
   dynamic coattention network achieves state of the art performance on
   the stanford id53 dataset. compared to systems developed
   by the allen institute for artificial intelligence, microsoft research
   asia, google, and ibm, our model makes significant improvements and
   outperforms all other approaches. the dynamic coattention network is
   the first model to break the 80% f1 mark, taking machines one step
   closer to the human-level performance of 91.2% f1 on the stanford
   id53 dataset. the full leaderboard for the stanford
   id53 dataset is [45]available here.

citation credit

   caiming xiong, victor zhong, richard socher. 2016.
   [46]dynamic coattention networks for id53

acknowledgement

   we thank kazuma hashimoto, bryan mccann, and richard socher for their
   help and comments.

   [47]salesforce logo [48]content [49]terms [50]privacy
      copyright 2017 [51]salesforce.com, inc. [52]all rights reserved.
   rights of albert einstein are used with permission of the hebrew
   university of jerusalem. represented exclusively by greenlight.

references

   1. https://blog.einstein.ai/rss/
   2. https://www.einstein.ai/
   3. https://einstein.ai/mission
   4. https://einstein.ai/research
   5. https://einstein.ai/careers
   6. https://einstein.ai/ethics
   7. https://einstein.ai/products
   8. https://einstein.ai/press
   9. https://einstein.ai/research/publications
  10. https://einstein.ai/research/blog
  11. https://einstein.ai/research/open-source
  12. https://einstein.ai/research/grants
  13. https://www.einstein.ai/
  14. javascript:void(0);
  15. https://einstein.ai/mission
  16. https://einstein.ai/research
  17. https://einstein.ai/research/publications
  18. https://einstein.ai/research/blog
  19. https://einstein.ai/research/open-source
  20. https://einstein.ai/research/grants
  21. https://einstein.ai/careers
  22. https://einstein.ai/ethics
  23. https://einstein.ai/products
  24. https://einstein.ai/press
  25. https://www.einstein.ai/
  26. https://einstein.ai/mission
  27. https://einstein.ai/research
  28. https://einstein.ai/careers
  29. https://einstein.ai/ethics
  30. https://einstein.ai/products
  31. https://einstein.ai/press
  32. https://einstein.ai/research/publications
  33. https://einstein.ai/research/blog
  34. https://einstein.ai/research/open-source
  35. https://einstein.ai/research/grants
  36. https://blog.einstein.ai/author/victor/
  37. https://arxiv.org/abs/1611.01604
  38. https://rajpurkar.github.io/squad-explorer/
  39. http://research.microsoft.com/en-us/um/redmond/projects/mctest/
  40. https://arxiv.org/abs/1506.03340
  41. http://nlp.stanford.edu/
  42. https://rajpurkar.github.io/squad-explorer/
  43. http://dl.acm.org/citation.cfm?id=1460714
  44. https://dspace.mit.edu/bitstream/handle/1721.1/5922/aim-066.pdf?sequence=2
  45. https://rajpurkar.github.io/squad-explorer/
  46. https://arxiv.org/abs/1611.01604
  47. https://www.salesforce.com/
  48. https://einstein.ai/content-takedown
  49. https://einstein.ai/terms-of-service
  50. https://einstein.ai/privacy
  51. https://www.salesforce.com/
  52. https://www.salesforce.com/company/legal/intellectual.jsp
