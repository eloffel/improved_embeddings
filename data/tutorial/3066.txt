community

   news
   beta
   tutorials
   cheat sheets
   open courses
   podcast - dataframed
   chat
   new

datacamp

   official blog
   tech thoughts
   (button)
   search
   [1](button)
   log in
   (button)
   create account
   (button)
   share an article
   (button)
   back to tutorials
   tutorials
   [2]0
   129
   129
   karlijn willems
   february 25th, 2019
   must read
   python
   +1

python machine learning: scikit-learn tutorial

   an easy-to-follow scikit-learn tutorial that will help you get started
   with python machine learning.

machine learning with python

   machine learning is a branch in computer science that studies the
   design of algorithms that can learn.

   typical tasks are concept learning, function learning or    predictive
   modeling   , id91 and finding predictive patterns. these tasks are
   learned through available data that were observed through experiences
   or instructions, for example.

   the hope that comes with this discipline is that including the
   experience into its tasks will eventually improve the learning. but
   this improvement needs to happen in such a way that the learning itself
   becomes automatic so that humans like ourselves don   t need to interfere
   anymore is the ultimate goal.

   today   s scikit-learn tutorial will introduce you to the basics of
   python machine learning:
     * you'll learn how to use python and its libraries to [3]explore your
       data with the help of matplotlib and principal component analysis
       (pca),
     * and you'll [4]preprocess your data with id172, and you'll
       split your data into training and test sets.
     * next, you'll work with the well-known [5]kmeans algorithm to
       construct an unsupervised model, fit this model to your data,
       predict values, and validate the model that you have built.
     * as an extra, you'll also see how you can also use [6]support vector
       machines (id166) to construct another model to classify your data.

   if you   re more interested in an r tutorial, take a look at our
   [7]machine learning with r for beginners tutorial.

   alternatively, check out datacamp's [8]supervised learning with
   scikit-learn and [9]unsupervised learning in python courses!

loading your data set

   the first step to about anything in data science is loading your data.
   this is also the starting point of this scikit-learn tutorial.

   this discipline typically works with observed data. this data might be
   collected by yourself, or you can browse through other sources to find
   data sets. but if you   re not a researcher or otherwise involved in
   experiments, you   ll probably do the latter.

   if you   re new to this and you want to start problems on your own,
   finding these data sets might prove to be a challenge. however, you can
   typically find good data sets at the [10]uci machine learning
   repository or on the [11]kaggle website. also, check out [12]this kd
   nuggets list with resources.

   for now, you should warm up, not worry about finding any data by
   yourself and just load in the digits data set that comes with a python
   library, called scikit-learn.

   fun fact: did you know the name originates from the fact that this
   library is a scientific toolbox built around scipy? by the way, there
   is [13]more than just one scikit out there. this scikit contains
   modules specifically for machine learning and data mining, which
   explains the second component of the library name. :)

   to load in the data, you import the module datasets from sklearn. then,
   you can use the load_digits() method from datasets to load in the data:
   eyjsyw5ndwfnzsi6inb5dghvbiisinnhbxbszsi6iimgsw1wb3j0igbkyxrhc2v0c2agznj
   vbsbgc2tszwfybmbcbmzyb20gc2tszwfybibpbxbvcnqgx19fx19fx19cblxuiybmb2fkig
   luihrozsbgzglnaxrzycbkyxrhxg5kawdpdhmgpsbkyxrhc2v0cy5sb2fkx2rpz2l0cygpx
   g5cbimguhjpbnqgdghligbkawdpdhngigrhdgegxg5wcmludchfx19fx18piiwic29sdxrp
   b24ioiijieltcg9ydcbgzgf0yxnldhngigzyb20gyhnrbgvhcm5gxg5mcm9tihnrbgvhcm4
   gaw1wb3j0igrhdgfzzxrzxg5cbimgtg9hzcbpbib0agugygrpz2l0c2agzgf0yvxuzglnax
   rzid0gzgf0yxnldhmubg9hzf9kawdpdhmokvxuxg4jifbyaw50ihrozsbgzglnaxrzycbky
   xrhifxuchjpbnqozglnaxrzksisinnjdci6imltcg9ydf9tc2c9xcjeawqgew91igltcg9y
   dcbgzgf0yxnldhngigzyb20gyhnrbgvhcm5gp1wixg5pbmnvcnjly3rfaw1wb3j0x21zzz1
   cikrvbid0igzvcmdldcb0bybpbxbvcnqgdghligbkyxrhc2v0c2agbw9kdwxligzyb20gyh
   nrbgvhcm5givwixg5ub3rfy2fsbgvkx21zzz1cikrpzcb5b3ugdxnligbkyxrhc2v0cy5sb
   2fkx2rpz2l0cygpycb0bybsb2fkigluihrozsbgzglnaxrzycbkyxrhp1wixg5pbmnvcnjl
   y3rfbxnnpvwivxnligbkyxrhc2v0cy5sb2fkx2rpz2l0cygpycb0bybsb2fkigluihrozsb
   gzglnaxrzycbkyxrhivwixg5wcmvkzwzfbxnnpvwirglkihlvdsbjywxsihrozsbgchjpbn
   qokwagznvuy3rpb24/xcjcbnrlc3rfaw1wb3j0kfwic2tszwfybi5kyxrhc2v0c1wilcbzy
   w1lx2fzid0gvhj1zswgbm90x2ltcg9ydgvkx21zzya9igltcg9ydf9tc2csigluy29ycmvj
   df9hc19tc2cgpsbpbmnvcnjly3rfaw1wb3j0x21zzylcbnrlc3rfznvuy3rpb24oxcjza2x
   lyxjulmrhdgfzzxrzlmxvywrfzglnaxrzxcisig5vdf9jywxszwrfbxnnid0gbm90x2nhbg
   xlzf9tc2csigluy29ycmvjdf9tc2cgpsbpbmnvcnjly3rfbxnnkvxuiybuzxn0igbwcmlud
   cgpycbmdw5jdglvblxudgvzdf9mdw5jdglvbihcbiagicbcinbyaw50xcisxg4gicagbm90
   x2nhbgxlzf9tc2c9chjlzgvmx21zzyxcbiagicbpbmnvcnjly3rfbxnnpxbyzwrlzl9tc2c
   sxg4gicagzg9fzxzhbd1gywxzzvxukvxuc3vjy2vzc19tc2c9xcjqzxjmzwn0isbzb3uncm
   ugcmvhzhkgdg8gz28hxciifq==

   note that the datasets module contains other methods to load and fetch
   popular reference datasets, and you can also count on this module in
   case you need artificial data generators. also, this data set is also
   available through the uci repository that was mentioned above: you can
   find the data [14]here.

   if you had decided to pull the data from the latter page, your data
   import would   ve looked like this:
   eyjsyw5ndwfnzsi6inb5dghvbiisinnhbxbszsi6iimgsw1wb3j0ihrozsbgcgfuzgfzycb
   sawjyyxj5igfzigbwzgbcbmltcg9ydcbfx19fx18gyxmgx19cblxuiybmb2fkigluihrozs
   bkyxrhihdpdgggyhjlywrfy3n2kclgxg5kawdpdhmgpsbwzc5yzwfkx2nzdihcimh0dha6l
   y9hcmnoaxzllmljcy51y2kuzwr1l21sl21hy2hpbmutbgvhcm5pbmctzgf0ywjhc2vzl29w
   dgrpz2l0cy9vchrkawdpdhmudhjhxcisighlywrlcj1ob25lkvxuxg4jifbyaw50ig91dcb
   gzglnaxrzyfxuchjpbnqox19fx19fksisinnvbhv0aw9uijoiiybjbxbvcnqgdghligbwyw
   5kyxngigxpynjhcnkgyxmgyhbkyfxuaw1wb3j0ihbhbmrhcybhcybwzfxuxg4jiexvywqga
   w4gdghligrhdgegd2l0acbgcmvhzf9jc3yokwbcbmrpz2l0cya9ihbklnjlywrfy3n2kfwi
   ahr0cdovl2fyy2hpdmuuawnzlnvjas5lzhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymf
   zzxmvb3b0zglnaxrzl29wdgrpz2l0cy50cmfciiwgagvhzgvypu5vbmupxg5cbimguhjpbn
   qgb3v0igbkawdpdhngxg5wcmludchkawdpdhmpiiwic2n0ijoiaw1wb3j0x21zzz1cikrpz
   cb5b3ugywrkihnvbwugy29kzsb0bybpbxbvcnqgyhbhbmrhc2agyxmgyhbkyd9cilxuaw5j
   b3jyzwn0x2ltcg9ydf9tc2c9xcjeb24ndcbmb3jnzxqgdg8gaw1wb3j0ihrozsancgfuzgf
   zjybsawjyyxj5igfzigbwzgahxcjcbmnzdl9tc2c9xcjeawqgew91ihvzzsb0agugyhjlyw
   rfy3n2kclgig1ldghvzcbmcm9tihbhbmrhcyb0bybsb2fkigluihrozsbkyxrhp1wixg5jc
   3zfaw5jb3jyzwn0x21zzz1cilvzzsbgcmvhzf9jc3yokwagznjvbsb0agugcgfuzgfzigxp
   ynjhcnkgdg8gbg9hzcbpbib0agugzgf0ysbcilxuchjlzgvmx21zzz1cikrpzcb5b3ugy2f
   sbcb0agugyhbyaw50kclgigz1bmn0aw9up1wixg4jifrlc3qgaw1wb3j0igbwyw5kyxngxg
   50zxn0x2ltcg9ydchcinbhbmrhc1wilcbzyw1lx2fzid0gvhj1zswgbm90x2ltcg9ydgvkx
   21zzya9igltcg9ydf9tc2csigluy29ycmvjdf9hc19tc2cgpsbpbmnvcnjly3rfaw1wb3j0
   x21zzylcbimgvgvzdcbgcmvhzf9jc3yokwbcbnrlc3rfznvuy3rpb24oxcjwyw5kyxmucmv
   hzf9jc3zciiwgbm90x2nhbgxlzf9tc2cgpsbjc3zfbxnnlcbpbmnvcnjly3rfbxnnid0gy3
   n2x2luy29ycmvjdf9tc2cpxg4jifrlc3qgyhbyaw50kclgigz1bmn0aw9uxg50zxn0x2z1b
   mn0aw9ukfxuicagifwichjpbnrciixcbiagicbub3rfy2fsbgvkx21zzz1wcmvkzwzfbxnn
   lfxuicagigluy29ycmvjdf9tc2c9chjlzgvmx21zzyxcbiagicbkb19ldmfspuzhbhnlxg4
   pxg5zdwnjzxnzx21zzyhcikf3zxnvbwugam9iivwiksj9

   note that if you download the data like this, the data is already split
   up in a training and a test set, indicated by the extensions .tra and
   .tes. you   ll need to load in both files to elaborate your project. with
   the command above, you only load in the training set.

   tip: if you want to know more about importing data with the python data
   manipulation library pandas, consider taking datacamp   s [15]importing
   data in python course.

   [16]learn python for data science with datacamp

explore your data

   when first starting out with a data set, it   s always a good idea to go
   through the data description and see what you can already learn. when
   it comes to scikit-learn, you don   t immediately have this information
   readily available, but in the case where you import data from another
   source, there's usually a data description present, which will already
   be a sufficient amount of information to gather some insights into your
   data.

   however, these insights are not merely deep enough for the analysis
   that you are going to perform. you really need to have a good working
   knowledge about the data set.

   performing an exploratory data analysis (eda) on a data set like the
   one that this tutorial now has might seem difficult.

   where do you start exploring these handwritten digits?

gathering basic information on your data

   let   s say that you haven   t checked any data description folder (or
   maybe you want to double-check the information that has been given to
   you).

   then you should start by gathering the necessary information.

   when you printed out the digits data after having loaded it with the
   help of the scikit-learn datasets module, you will have noticed that
   there is already a lot of information available. you already know
   things such as the target values and the description of your data. you
   can access the digits data through the attribute data. similarly, you
   can also access the target values or labels through the target
   attribute and the description through the descr attribute.

   to see which keys you have available to already get to know your data,
   you can just run digits.keys().

   try this all out in the following datacamp light blocks:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuzglnaxrzid0gzgf0yxnldhmubg9hzf9kawdpdhmoksisin
   nhbxbszsi6iimgr2v0ihrozsbrzxlzig9mihrozsbgzglnaxrzycbkyxrhxg5wcmludchka
   wdpdhmux19fx19fkvxuxg4jifbyaw50ig91dcb0agugzgf0yvxuchjpbnqozglnaxrzll9f
   x18pxg5cbimguhjpbnqgb3v0ihrozsb0yxjnzxqgdmfsdwvzxg5wcmludchkawdpdhmux19
   fx19fkvxuxg4jifbyaw50ig91dcb0agugzgvzy3jpchrpb24gb2ygdghligbkawdpdhngig
   rhdgfcbnbyaw50kgrpz2l0cy5ervnduikilcjzb2x1dglvbii6iimgr2v0ihrozsbrzxlzi
   g9mihrozsbgzglnaxrzycbkyxrhxg5wcmludchkawdpdhmua2v5cygpkvxuxg4jifbyaw50
   ig91dcb0agugzgf0yvxuchjpbnqozglnaxrzlmrhdgepxg5cbimguhjpbnqgb3v0ihrozsb
   0yxjnzxqgdmfsdwvzxg5wcmludchkawdpdhmudgfyz2v0kvxuxg4jifbyaw50ig91dcb0ag
   ugzgvzy3jpchrpb24gb2ygdghligbkawdpdhngigrhdgfcbnbyaw50kgrpz2l0cy5ervndu
   ikilcjzy3qioiijifrlc3qgyhbyaw50ycbcbnrlc3rfznvuy3rpb24oxg4gicagxcjwcmlu
   dfwilfxuicagidesxg4gicagbm90x2nhbgxlzf9tc2c9xcjeawqgew91ihbyaw50ig91dcb
   0aguga2v5cybvzibgzglnaxrzyd9ciixcbiagicbpbmnvcnjly3rfbxnnpvwirg9uj3qgzm
   9yz2v0ihrvihbyaw50ig91dcb0aguga2v5cybvzibgzglnaxrzycfciixcbiagicbkb19ld
   mfspuzhbhnlxg4pxg4jifrlc3qgyhbyaw50yfxudgvzdf9mdw5jdglvbihcbiagicbcinby
   aw50xcisxg4gicagmixcbiagicbub3rfy2fsbgvkx21zzz1cikrpzcb5b3ugchjpbnqgb3v
   0ihrozsbkyxrhp1wilfxuicagigluy29ycmvjdf9tc2c9xcjeb24ndcbmb3jnzxqgdg8gch
   jpbnqgb3v0ihrozsbkyxrhivwilfxuicagigrvx2v2yww9rmfsc2vcbilcbimgvgvzdcbgc
   hjpbnrgxg50zxn0x2z1bmn0aw9ukfxuicagifwichjpbnrciixcbiagicazlfxuicagig5v
   df9jywxszwrfbxnnpvwirglkihlvdsbwcmludcbvdxqgdghlihrhcmdldcb2ywx1zxmgb2y
   gdghligrhdge/xcisxg4gicagaw5jb3jyzwn0x21zzz1cikrvbid0igzvcmdldcb0bybwcm
   ludcbvdxqgdghlihrhcmdldcb2ywx1zxmgb2ygdghligrhdgehxcisxg4gicagzg9fzxzhb
   d1gywxzzvxukvxuiybuzxn0igbwcmludgagxg50zxn0x2z1bmn0aw9ukfxuicagifwichjp
   bnrciixcbiagica0lfxuicagig5vdf9jywxszwrfbxnnpvwirglkihlvdsbwcmludcbvdxq
   gdghligrlc2nyaxb0aw9uig9migbkawdpdhngp1wilfxuicagigluy29ycmvjdf9tc2c9xc
   jeb24ndcbmb3jnzxqgdg8gchjpbnqgb3v0ihrozsbkzxnjcmlwdglvbibvzibgzglnaxrzy
   cfciixcbiagicbkb19ldmfspuzhbhnlxg4pxg5zdwnjzxnzx21zzyhcikf3zxnvbwuhxcip
   in0=

   the next thing that you can (double)check is the type of your data.

   if you used read_csv() to import the data, you would have had a data
   frame that contains just the data. there wouldn   t be any description
   component, but you would be able to resort to, for example, head() or
   tail() to inspect your data. in these cases, it   s always wise to read
   up on the data description folder!

   however, this tutorial assumes that you make use of the library's data
   and the type of the digits variable is not that straightforward if
   you   re not familiar with the library. look at the print out in the
   first code chunk. you   ll see that digits actually contains numpy
   arrays!

   this is already quite vital information. but how do you access these
   arrays?

   it   s straightforward, actually: you use attributes to access the
   relevant arrays.

   remember that you have already seen which attributes are available when
   you printed digits.keys(). for instance, you have the data attribute to
   isolate the data, target to see the target values and the descr for the
   description,    

   but what then?

   the first thing that you should know of an array is its shape. that is
   the number of dimensions and items that are contained within an array.
   the array   s shape is a tuple of integers that specify the sizes of each
   dimension. in other words, if you have a 3d array like this y =
   np.zeros((2, 3, 4)), the shape of your array will be (2,3,4).

   now let   s try to see what the shape is of these three arrays that you
   have distinguished (the data, target and descr arrays).

   use first the data attribute to isolate the numpy array from the digits
   data and then use the shape attribute to find out more. you can do the
   same for the target and descr. there   s also the images attribute, which
   is basically the data in images. you   re also going to test this out.

   check up on this statement by using the shape attribute on the array:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuaw1wb3j0ig51bxb5igfzig5wxg5kawdpdhmgpsbkyxrhc2
   v0cy5sb2fkx2rpz2l0cygpiiwic2ftcgxlijoiiybjc29syxrlihrozsbgzglnaxrzycbky
   xrhxg5kawdpdhnfzgf0ysa9igrpz2l0cy5kyxrhxg5cbimgsw5zcgvjdcb0agugc2hhcgvc
   bnbyaw50kgrpz2l0c19kyxrhlnnoyxblkvxuxg4jielzb2xhdgugdghlihrhcmdldcb2ywx
   1zxmgd2l0acbgdgfyz2v0yfxuzglnaxrzx3rhcmdldca9igrpz2l0cy5fx19fx19cblxuiy
   bjbnnwzwn0ihrozsbzagfwzvxuchjpbnqozglnaxrzx3rhcmdldc5fx19fxylcblxuiybqc
   mludcb0agugbnvtymvyig9mihvuaxf1zsbsywjlbhncbm51bwjlcl9kawdpdhmgpsbszw4o
   bnaudw5pcxvlkgrpz2l0cy50yxjnzxqpkvxuxg4jielzb2xhdgugdghligbpbwfnzxngxg5
   kawdpdhnfaw1hz2vzid0gzglnaxrzlmltywdlc1xuxg4jieluc3bly3qgdghlihnoyxblxg
   5wcmludchkawdpdhnfaw1hz2vzlnnoyxblksisinnvbhv0aw9uijoiiybjc29syxrlihroz
   sbgzglnaxrzycbkyxrhxg5kawdpdhnfzgf0ysa9igrpz2l0cy5kyxrhxg5cbimgsw5zcgvj
   dcb0agugc2hhcgvcbnbyaw50kgrpz2l0c19kyxrhlnnoyxblkvxuxg4jielzb2xhdgugdgh
   lihrhcmdldcb2ywx1zxmgd2l0acbgdgfyz2v0yfxuzglnaxrzx3rhcmdldca9igrpz2l0cy
   50yxjnzxrcblxuiybjbnnwzwn0ihrozsbzagfwzvxuchjpbnqozglnaxrzx3rhcmdldc5za
   gfwzslcblxuiybqcmludcb0agugbnvtymvyig9mihvuaxf1zsbsywjlbhncbm51bwjlcl9k
   awdpdhmgpsbszw4obnaudw5pcxvlkgrpz2l0cy50yxjnzxqpkvxuxg4jielzb2xhdgugdgh
   ligbpbwfnzxngxg5kawdpdhnfaw1hz2vzid0gzglnaxrzlmltywdlc1xuxg4jieluc3bly3
   qgdghlihnoyxblxg5wcmludchkawdpdhnfaw1hz2vzlnnoyxblksisinnjdci6im1zz19ky
   xrhpvwirglkihlvdsbhzgqgyhnoyxblycb0bybnzxqgdghlig51bwjlcibvzibkaw1lbnnp
   b25zigfuzcbpdgvtcybvzib0agugygrpz2l0c19kyxrhycbhcnjhet9cilxubxnnx3rhcmd
   ldd1cikrpzcb5b3ugywrkigbzagfwzwagdg8gz2v0ihrozsbudw1izxigb2ygzgltzw5zaw
   9ucybhbmqgaxrlbxmgb2ygdghligbkawdpdhnfdgfyz2v0ycbhcnjhet9cilxubxnnx2lty
   wdlpvwirglkihlvdsbhzgqgyhnoyxblycb0bybnzxqgdghlig51bwjlcibvzibkaw1lbnnp
   b25zigfuzcbpdgvtcybvzib0agugygrpz2l0c19pbwfnzxngigfycmf5p1wixg4jifrlc3q
   gb2jqzwn0igbkawdpdhnfzgf0ywbcbnrlc3rfb2jqzwn0kfwizglnaxrzx2rhdgfciiwgdw
   5kzwzpbmvkx21zzz1cikrpzcb5b3ugzgvmaw5lihrozsbgzglnaxrzx2rhdgfgig9iamvjd
   d9ciiwgaw5jb3jyzwn0x21zzz1cikrpzcb5b3ugdxnlihrozsbgzgf0ywagyxr0cmlidxrl
   ihrviglzb2xhdgugdghligrhdgegb2ygygrpz2l0c2a/xcipxg4jifrlc3qgb2jqzwn0igb
   kawdpdhnfdgfyz2v0yfxudgvzdf9vymply3qoxcjkawdpdhnfdgfyz2v0xcisihvuzgvmaw
   5lzf9tc2c9xcjeawqgew91igrlzmluzsb0agugygrpz2l0c190yxjnzxrgig9iamvjdd9ci
   iwgaw5jb3jyzwn0x21zzz1cikrpzcb5b3ugdxnlihrozsbgdgfyz2v0ycbhdhryawj1dgug
   dg8gaxnvbgf0zsb0agugdgfyz2v0ihzhbhvlcybvzib0agugygrpz2l0c2agzgf0yt9ciil
   cbimgvgvzdcbgc2hhcgvgig9migbkawdpdhnfzgf0ywbcbin0zxn0igz1bmn0aw9uihbyaw
   50xg50zxn0x2z1bmn0aw9ukfxuicagifwichjpbnrciixcbiagicaxlfxuicagig5vdf9jy
   wxszwrfbxnnpvwirglkihlvdsbwcmludcbvdxqgdghlihnoyxblig9mihrozwrhdge/xcis
   xg4gicagaw5jb3jyzwn0x21zzz1cikrvbid0igzvcmdldcb0bybwcmludcbvdxqgdghlihn
   oyxblig9mihrozsbkyxrhivwilfxuicagigrvx2v2yww9rmfsc2vcbilcbnrlc3rfb2jqzw
   n0x2fjy2vzc2vkkfwizglnaxrzx2rhdgeuc2hhcgvciiwgbm90x2fjy2vzc2vkx21zzz1tc
   2dfzgf0yslcbimgvgvzdcbgchjpbnrgxg50zxn0x2z1bmn0aw9ukfxuicagifwichjpbnrc
   iixcbiagicaylfxuicagig5vdf9jywxszwrfbxnnpvwirglkihlvdsbwcmludcbvdxqgdgh
   lihnoyxblig9mihrozsb0yxjnzxqgdmfsdwvzig9mihrozsbkyxrhp1wilfxuicagigluy2
   9ycmvjdf9tc2c9xcjeb24ndcbmb3jnzxqgdg8gchjpbnqgb3v0ihrozsbzagfwzsbvzib0a
   gugdgfyz2v0ihzhbhvlcybvzib0agugzgf0ysfciixcbiagicbkb19ldmfspuzhbhnlxg4p
   xg4jifrlc3qgywnjzxnzigbzagfwzwagb2ygygrpz2l0c190yxjnzxrgxg50zxn0x29iamv
   jdf9hy2nlc3nlzchcimrpz2l0c190yxjnzxquc2hhcgvciiwgbm90x2fjy2vzc2vkx21zzz
   1tc2dfdgfyz2v0kvxuiybuzxn0ig9iamvjdcbgbnvtymvyx2rpz2l0c2bcbnrlc3rfb2jqz
   wn0kfwibnvtymvyx2rpz2l0c1wilcb1bmrlzmluzwrfbxnnpvwirglkihlvdsbkzwzpbmug
   dghligbudw1izxjfzglnaxrzycbvymply3q/xcisigluy29ycmvjdf9tc2c9xcjeawqgew9
   1ihvzzsbgbnaudw5pcxvlkclgihrvigdpdmugymfjayb0agugdw5pcxvlihrhcmdldcb2yw
   x1zxm/iervbid0igzvcmdldcb0bybnaxzligjhy2sgdghligxlbmd0acbvzib0aglzigfyc
   mf5ihdpdgggygxlbigpycfciilcbimgvgvzdcbvymply3qgygrpz2l0c19pbwfnzxngxg50
   zxn0x29iamvjdchcimrpz2l0c19pbwfnzxnciiwgdw5kzwzpbmvkx21zzz1cikrpzcb5b3u
   gzgvmaw5lihrozsbgzglnaxrzx2ltywdlc2agb2jqzwn0p1wilcbpbmnvcnjly3rfbxnnpv
   wirglkihlvdsb1c2ugdghligbpbwfnzxngigf0dhjpynv0zsb0bybpc29syxrlihrozsbpb
   wfnzxmgb2ygdghligbkawdpdhngigrhdge/xcipxg4jifrlc3qgyhnoyxblycbvzibgzgln
   axrzx2ltywdlc2bcbnrlc3rfb2jqzwn0x2fjy2vzc2vkkfwizglnaxrzx2ltywdlcy5zagf
   wzvwilcbub3rfywnjzxnzzwrfbxnnpw1zz19pbwfnzslcbimgvgvzdcbgchjpbnrgifxudg
   vzdf9mdw5jdglvbihcbiagicbcinbyaw50xcisxg4gicagmyxcbiagicbub3rfy2fsbgvkx
   21zzz1cikrpzcb5b3ugchjpbnqgb3v0ihrozsbzagfwzsbvzib0agugaw1hz2vzig9migbk
   awdpdhngp1wilfxuicagigluy29ycmvjdf9tc2c9xcjeb24ndcbmb3jnzxqgdg8gchjpbnq
   gb3v0ihrozsbzagfwzsbvzib0agugaw1hz2vzig9migbkawdpdhngivwilfxuicagigrvx2
   v2yww9rmfsc2vcbilcbnn1y2nlc3nfbxnnkfwiv2vsbcbkb25livwiksj9

   to recap: by inspecting digits.data, you see that there are 1797
   samples and that there are 64 features. because you have 1797 samples,
   you also have 1797 target values.

   but all those target values contain 10 unique values, namely, from 0 to
   9. in other words, all 1797 target values are made up of numbers that
   lie between 0 and 9. this means that the digits that your model will
   need to recognize are numbers from 0 to 9.

   lastly, you see that the images data contains three dimensions: there
   are 1797 instances that are 8 by 8 pixels big. you can visually check
   that the images and the data are related by reshaping the images array
   to two dimensions: digits.images.reshape((1797, 64)).

   but if you want to be entirely sure, better to check with
print(np.all(digits.images.reshape((1797,64)) == digits.data))

   with the numpy method all(), you test whether all array elements along
   a given axis evaluate to true. in this case, you evaluate if it   s true
   that the reshaped images array equals digits.data. you   ll see that the
   result will be true in this case.

visualize your data images with matplotlib

   then, you can take your exploration up a notch by visualizing the
   images that you   ll be working with. you can use one of python   s data
   visualization libraries, such as [17]matplotlib, for this purpose:
# import matplotlib
import matplotlib.pyplot as plt

# figure size (width, height) in inches
fig = plt.figure(figsize=(6, 6))

# adjust the subplots
fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)

# for each of the 64 images
for i in range(64):
    # initialize the subplots: add a subplot in the grid of 8 by 8, at the i+1-t
h position
    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])
    # display an image at the i-th position
    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')
    # label the image with the target value
    ax.text(0, 7, str(digits.target[i]))

# show the plot
plt.show()

   the code chunk seems quite lengthy at first sight, and this might be
   overwhelming. but, what happens in the code chunk above is actually
   pretty easy once you break it down into parts:
     * you import matplotlib.pyplot.
     * next, you set up a figure with a figure size of 6 inches wide and 6
       inches long. this is your blank canvas where all the subplots with
       the images will appear.
     * then you go to the level of the subplots to adjust some parameters:
       you set the left side of the suplots of the figure to 0, the right
       side of the suplots of the figure to 1, the bottom to 0 and the top
       to 1. the height of the blank space between the suplots is set at
       0.005 and the width is set at 0.05. these are merely layout
       adjustments.
     * after that, you start filling up the figure that you have made with
       the help of a for loop.
     * you initialize the suplots one by one, adding one at each position
       in the grid that is 8 by 8 images big.
     * you display each time one of the images at each position in the
       grid. as a color map, you take binary colors, which in this case
       will result in black, gray values and white colors. the
       interpolation method that you use is 'nearest', which means that
       your data is interpolated in such a way that it isn   t smooth. you
       can see the effect of the different interpolation methods [18]here.
     * the cherry on the pie is the addition of text to your subplots. the
       target labels are printed at coordinates (0,7) of each subplot,
       which in practice means that they will appear in the bottom-left of
       each of the subplots.
     * don   t forget to show the plot with plt.show()!

   in the end, you   ll get to see the following:

   python machine learning visualization of images

   on a more simple note, you can also visualize the target labels with an
   image, just like this:
# import matplotlib
import matplotlib.pyplot as plt

# join the images and target labels in a list
images_and_labels = list(zip(digits.images, digits.target))

# for every element in the list
for index, (image, label) in enumerate(images_and_labels[:8]):
    # initialize a subplot of 2x4 at the i+1-th position
    plt.subplot(2, 4, index + 1)
    # don't plot any axes
    plt.axis('off')
    # display images in all subplots
    plt.imshow(image, cmap=plt.cm.gray_r,interpolation='nearest')
    # add a title to each subplot
    plt.title('training: ' + str(label))

# show the plot
plt.show()

   which will render the following visualization:

   python scikit-learn visualization of images

   note that in this case, after you have imported matplotlib.pyplot, you
   zip the two numpy arrays together and save it into a variable called
   images_and_labels. you   ll see now that this list contains suples of
   each time an instance of digits.images and a corresponding
   digits.target value.

   then, you say that for the first eight elements of images_and_labels
   -note that the index starts at 0!-, you initialize subplots in a grid
   of 2 by 4 at each position. you turn of the plotting of the axes and
   you display images in all the subplots with a color map plt.cm.gray_r
   (which returns all grey colors) and the interpolation method used is
   nearest. you give a title to each subplot, and you show it.

   not too hard, huh?

   and now you have an excellent idea of the data that you   ll be working
   with!

visualizing your data: principal component analysis (pca)

   but is there no other way to visualize the data?

   as the digits data set contains 64 features, this might prove to be a
   challenging task. you can imagine that it   s tough to understand the
   structure and keep the overview of the digits data. in such cases, it
   is said that you   re working with a high dimensional data set.

   high dimensionality of data is a direct result of trying to describe
   the objects via a collection of features. other examples of high
   dimensional data are, for example, financial data, climate data,
   neuroimaging,    

   but, as you might have gathered already, this is not always easy. in
   some cases, high dimensionality can be problematic, as your algorithms
   will need to take into account too many features. in such cases, you
   speak of the curse of dimensionality. because having a lot of
   dimensions can also mean that your data points are far away from
   virtually every other point, which makes the distances between the data
   points uninformative.

   don   t worry, though, because the curse of dimensionality is not merely
   a matter of counting the number of features. there are also cases in
   which the effective dimensionality might be much smaller than the
   number of the features, such as in data sets where some features are
   irrelevant.

   in addition, you can also understand that data with only two or three
   dimensions are easier to grasp and can also be visualized easily.

   that all explains why you   re going to visualize the data with the help
   of one of the id84 techniques, namely principal
   component analysis (pca). the idea in pca is to find a linear
   combination of the two variables that contains most of the information.
   this new variable or    principal component    can replace the two original
   variables.

   in short, it   s a linear transformation method that yields the
   directions (principal components) that maximize the variance of the
   data. remember that the variance indicates how far a set of data points
   lie apart. if you want to know more, go to [19]this page.

   you can easily apply pca do your data with the help of scikit-learn:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuzglnaxrzid0gzgf0yxnldhmubg9hzf9kawdpdhmokvxuzn
   jvbsbza2xlyxjulmrly29tcg9zaxrpb24gaw1wb3j0ifjhbmrvbwl6zwrqq0fcbmzyb20gc
   2tszwfybi5kzwnvbxbvc2l0aw9uigltcg9ydcbqq0fcbmltcg9ydcbudw1wesbhcybuccis
   innhbxbszsi6iimgq3jlyxrligegumfuzg9taxplzcbqq0egbw9kzwwgdghhdcb0ywtlcyb
   0d28gy29tcg9uzw50c1xucmfuzg9taxplzf9wy2egpsbsyw5kb21pemvkuenbkg5fy29tcg
   9uzw50cz0ykvxuxg4jiezpdcbhbmqgdhjhbnnmb3jtihrozsbkyxrhihrvihrozsbtb2rlb
   fxucmvkdwnlzf9kyxrhx3jwy2egpsbyyw5kb21pemvkx3bjys5maxrfdhjhbnnmb3jtkgrp
   z2l0cy5kyxrhkvxuxg4jienyzwf0zsbhihjlz3vsyxiguenbig1vzgvsifxucgnhid0guen
   bkg5fy29tcg9uzw50cz0ykvxuxg4jiezpdcbhbmqgdhjhbnnmb3jtihrozsbkyxrhihrvih
   rozsbtb2rlbfxucmvkdwnlzf9kyxrhx3bjysa9ihbjys5maxrfdhjhbnnmb3jtkgrpz2l0c
   y5kyxrhkvxuxg4jieluc3bly3qgdghlihnoyxblxg5yzwr1y2vkx2rhdgffcgnhlnnoyxbl
   xg5cbimguhjpbnqgb3v0ihrozsbkyxrhxg5wcmludchyzwr1y2vkx2rhdgffcnbjyslcbnb
   yaw50khjlzhvjzwrfzgf0yv9wy2epiiwic29sdxrpb24ioiijienyzwf0zsbhifjhbmrvbw
   l6zwqguenbig1vzgvsihroyxqgdgfrzxmgdhdvignvbxbvbmvudhncbnjhbmrvbwl6zwrfc
   gnhid0gumfuzg9taxplzfbdqshux2nvbxbvbmvudhm9milcblxuiybgaxqgyw5kihryyw5z
   zm9ybsb0agugzgf0ysb0byb0agugbw9kzwxcbnjlzhvjzwrfzgf0yv9ycgnhid0gcmfuzg9
   taxplzf9wy2euzml0x3ryyw5zzm9ybshkawdpdhmuzgf0yslcblxuiybdcmvhdgugysbyzw
   d1bgfyifbdqsbtb2rlbcbcbnbjysa9ifbdqshux2nvbxbvbmvudhm9milcblxuiybgaxqgy
   w5kihryyw5zzm9ybsb0agugzgf0ysb0byb0agugbw9kzwxcbnjlzhvjzwrfzgf0yv9wy2eg
   psbwy2euzml0x3ryyw5zzm9ybshkawdpdhmuzgf0yslcblxuiybjbnnwzwn0ihrozsbzagf
   wzvxucmvkdwnlzf9kyxrhx3bjys5zagfwzvxuxg4jifbyaw50ig91dcb0agugzgf0yvxuch
   jpbnqocmvkdwnlzf9kyxrhx3jwy2epxg5wcmludchyzwr1y2vkx2rhdgffcgnhksisinnjd
   ci6inrlc3rfb2jqzwn0kfwicmfuzg9taxplzf9wy2fciiwgzg9fzxzhbd1gywxzzslcbnrl
   c3rfb2jqzwn0kfwicmvkdwnlzf9kyxrhx3jwy2fciiwgzg9fzxzhbd1gywxzzslcbnrlc3r
   fb2jqzwn0kfwicgnhxcisigrvx2v2yww9rmfsc2upxg50zxn0x29iamvjdchcinjlzhvjzw
   rfzgf0yv9wy2fciiwgzg9fzxzhbd1gywxzzslcbnbyzwrlzl9tc2c9xcjeawqgew91igluc
   3bly3qgdghlihnoyxblig9migbyzwr1y2vkx2rhdgffcgnhyd9cilxudgvzdf9vymply3rf
   ywnjzxnzzwqoxcjyzwr1y2vkx2rhdgffcgnhlnnoyxblxcisig5vdf9hy2nlc3nlzf9tc2c
   9chjlzgvmx21zzylcbimgvgvzdcbgchjpbnrgifxudgvzdf9mdw5jdglvbihcbiagicbcin
   byaw50xcisxg4gicagmsxcbiagicbub3rfy2fsbgvkx21zzz1cikrpzcb5b3ugchjpbnqgb
   3v0ihrozsbgcmvkdwnlzf9kyxrhx3jwy2fgigrhdge/xcisxg4gicagaw5jb3jyzwn0x21z
   zz1cikrvbid0igzvcmdldcb0bybwcmludcbvdxqgdghligbyzwr1y2vkx2rhdgffcnbjywa
   gzgf0ysfciixcbiagicbkb19ldmfspuzhbhnlxg4pxg50zxn0x2z1bmn0aw9ukfxuicagif
   wichjpbnrciixcbiagicaylfxuicagig5vdf9jywxszwrfbxnnpvwirglkihlvdsbwcmlud
   cbvdxqgdghligbyzwr1y2vkx2rhdgffcgnhycbkyxrhp1wilfxuicagigluy29ycmvjdf9t
   c2c9xcjeb24ndcbmb3jnzxqgdg8gchjpbnqgb3v0ihrozsbgcmvkdwnlzf9kyxrhx3bjywa
   gzgf0ysfciixcbiagicbkb19ldmfspuzhbhnlxg4pxg5zdwnjzxnzx21zzyhcikftyxppbm
   chxcipin0=

   tip: you have used the randomizedpca() here because it performs better
   when there   s a high number of dimensions. try replacing the randomized
   pca model or estimator object with a regular pca model and see what the
   difference is.

   note how you explicitly tell the model only to keep two components.
   this is to make sure that you have two-dimensional data to plot. also,
   note that you don   t pass the target class with the labels to the pca
   transformation because you want to investigate if the pca reveals the
   distribution of the different labels and if you can clearly separate
   the instances from each other.

   you can now build a scatterplot to visualize the data:
colors = ['black', 'blue', 'purple', 'yellow', 'white', 'red', 'lime', 'cyan', '
orange', 'gray']
for i in range(len(colors)):
    x = reduced_data_rpca[:, 0][digits.target == i]
    y = reduced_data_rpca[:, 1][digits.target == i]
    plt.scatter(x, y, c=colors[i])
plt.legend(digits.target_names, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0
.)
plt.xlabel('first principal component')
plt.ylabel('second principal component')
plt.title("pca scatter plot")
plt.show()

   which looks like this:

   scikit-learn tutorial - principal component analysis (pca)

   again you use matplotlib to visualize the data. it   s useful for a quick
   visualization of what you   re working with, but you might have to
   consider something a little bit fancier if you   re working on making
   this part of your data science portfolio.

   also note that the last call to show the plot (plt.show()) is not
   necessary if you   re working in jupyter notebook, as you   ll want to put
   the images inline. when in doubt, you can always check out our
   [20]definitive guide to jupyter notebook.

   what happens in the code chunk above is the following:
    1. you put your colors together in a list. note that you list ten
       colors, which is equal to the number of labels that you have. this
       way, you make sure that your data points can be colored in
       according to the labels. then, you set up a range that goes from 0
       to 10. mind you that this range is not inclusive! remember that
       this is the same for indices of a list, for example.
    2. you set up your x and y coordinates. you take the first or the
       second column of reduced_data_rpca, and you select only those data
       points for which the label equals the index that you   re
       considering. that means that in the first run, you   ll consider the
       data points with label 0, then label 1,     and so on.
    3. you construct the scatter plot. fill in the x and y coordinates and
       assign a color to the batch that you   re processing. the first run,
       you   ll give the color black to all data points, the next run blue,
           and so on.
    4. you add a legend to your scatter plot. use the target_names key to
       get the right labels for your data points.
    5. add labels to your x and y axes that are meaningful.
    6. reveal the resulting plot.

where to go now?

   now that you have even more information about your data and you have a
   visualization ready, it does seem a bit like the data points sort of
   group together, but you also see there is quite some overlap.

   this might be interesting to investigate further.

   do you think that, in a case where you knew that there are 10 possible
   digits labels to assign to the data points, but you have no access to
   the labels, the observations would group or    cluster    together by some
   criterion in such a way that you could infer the labels?

   now, this is a research question!

   in general, when you have acquired a good understanding of your data,
   you have to decide on the use cases that would be relevant to your data
   set. in other words, you think about what your data set might teach you
   or what you think you can learn from your data.

   from there on, you can think about what kind of algorithms you would be
   able to apply to your data set in order to get the results that you
   think you can obtain.

   tip: the more familiar you are with your data, the easier it will be to
   assess the use cases for your specific data set. the same also holds
   for finding the appropriate machine algorithm.

   however, when you   re first getting started with scikit-learn, you   ll
   see that the amount of algorithms that the library contains is pretty
   vast and that you might still want additional help when you   re
   assessing your data set. that   s why [21]this scikit-learn machine
   learning map will come in handy.

   note that this map does require you to have some knowledge about the
   algorithms that are included in the scikit-learn library. this, by the
   way, also holds some truth for taking this next step in your project:
   if you have no idea what is possible, it will be tough to decide on
   what your use case will be for the data.

   as your use case was one for id91, you can follow the path on the
   map towards    kmeans   . you   ll see the use case that you have just
   thought about requires you to have more than 50 samples (   check!   ), to
   have labeled data (   check!   ), to know the number of categories that you
   want to predict (   check!   ) and to have less than 10k samples
   (   check!   ).

   but what exactly is the id116 algorithm?

   it is one of the simplest and widely used unsupervised learning
   algorithms to solve id91 problems. the procedure follows a simple
   and easy way to classify a given data set through a certain number of
   clusters that you have configured before you run the algorithm. this
   number of clusters is called k, and you select this number at random.

   then, the id116 algorithm will find the nearest cluster center for
   each data point and assign the data point closest to that cluster.

   once all data points have been assigned to clusters, the cluster
   centers will be recomputed. in other words, new cluster centers will
   emerge from the average of the values of the cluster data points. this
   process is repeated until most data points stick to the same cluster.
   the cluster membership should stabilize.

   you can already see that, because the id116 algorithm works the way
   it does, the initial set of cluster centers that you give up can have a
   significant effect on the clusters that are eventually found. you can,
   of course, deal with this effect, as you will see further on.

   however, before you can go into making a model for your data, you
   should definitely take a look into preparing your data for this
   purpose.

   [22]learn python for data science with datacamp

preprocessing your data

   as you have read in the previous section, before modeling your data,
   you   ll do well by preparing it first. this preparation step is called
      preprocessing   .

data id172

   the first thing that we   re going to do is preprocessing the data. you
   can standardize the digits data by, for example, making use of the
   scale() method:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuzglnaxrzid0gzgf0yxnldhmubg9hzf9kawdpdhmoksisin
   nhbxbszsi6iimgsw1wb3j0xg5mcm9tihnrbgvhcm4uchjlchjvy2vzc2luzybpbxbvcnqgc
   2nhbgvcblxuiybbchbsesbgc2nhbguokwagdg8gdghligbkawdpdhngigrhdgfcbmrhdgeg
   psbfx19fxyhkawdpdhmuzgf0yskilcjzb2x1dglvbii6iimgsw1wb3j0xg5mcm9tihnrbgv
   hcm4uchjlchjvy2vzc2luzybpbxbvcnqgc2nhbgvcblxuiybbchbsesbgc2nhbguokwagdg
   8gdghligbkawdpdhngigrhdgfcbmrhdgegpsbzy2fszshkawdpdhmuzgf0yskilcjzy3qio
   ij0zxn0x2z1bmn0aw9ukfxuicagifwic2tszwfybi5wcmvwcm9jzxnzaw5nlnnjywxlxcis
   xg4gicagbm90x2nhbgxlzf9tc2c9xcjeawqgew91ihn0yw5kyxjkaxplihrozsbgzglnaxr
   zycbkyxrhp1wilfxuicagigluy29ycmvjdf9tc2c9xcjeb24ndcbmb3jnzxqgdg8gc3rhbm
   rhcmrpemugdghligbkawdpdhngigrhdgegd2l0acbgc2nhbguokwahxcisxg4gicagzg9fz
   xzhbd1gywxzzvxukvxuc3vjy2vzc19tc2coxcjbd2vzb21livwiksj9

   by scaling the data, you shift the distribution of each attribute to
   have a mean of zero and a standard deviation of one (unit variance).

splitting your data into training and test sets

   to assess your model   s performance later, you will also need to divide
   the data set into two parts: a training set and a test set. the first
   is used to train the system, while the second is used to evaluate the
   learned or trained system.

   in practice, the division of your data set into a test and a training
   sets are disjoint: the most common splitting choice is to take 2/3 of
   your original data set as the training set, while the 1/3 that remains
   will compose the test set.

   you will try to do this also here. you see in the code chunk below that
   this    traditional    splitting choice is respected: in the arguments of
   the train_test_split() method, you clearly see that the test_size is
   set to 0.25.

   you   ll also note that the argument random_state has the value 42
   assigned to it. with this argument, you can guarantee that your split
   will always be the same. that is particularly handy if you want
   reproducible results.
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuzglnaxrzid0gzgf0yxnldhmubg9hzf9kawdpdhmokvxuzn
   jvbsbza2xlyxjulnbyzxbyb2nlc3npbmcgaw1wb3j0ihnjywxlxg5kyxrhid0gc2nhbguoz
   glnaxrzlmrhdgepiiwic2ftcgxlijoiiybjbxbvcnqgyhryywlux3rlc3rfc3bsaxrgxg5m
   cm9tihnrbgvhcm4uy3jvc3nfdmfsawrhdglvbibpbxbvcnqgx19fx19fx19fx19fx19fx1x
   uxg4jifnwbgl0ihrozsbgzglnaxrzycbkyxrhigludg8gdhjhaw5pbmcgyw5kihrlc3qgc2
   v0c1xuwf90cmfpbiwgwf90zxn0lcb5x3ryywlulcb5x3rlc3qsigltywdlc190cmfpbiwga
   w1hz2vzx3rlc3qgpsb0cmfpbl90zxn0x3nwbgl0kgrhdgesigrpz2l0cy50yxjnzxqsigrp
   z2l0cy5pbwfnzxmsihrlc3rfc2l6zt0wlji1lcbyyw5kb21fc3rhdgu9ndipiiwic29sdxr
   pb24ioiijieltcg9ydcbgdhjhaw5fdgvzdf9zcgxpdgbcbmzyb20gc2tszwfybi5jcm9zc1
   92ywxpzgf0aw9uigltcg9ydcb0cmfpbl90zxn0x3nwbgl0xg5cbimgu3bsaxqgdghligbka
   wdpdhngigrhdgegaw50byb0cmfpbmluzybhbmqgdgvzdcbzzxrzxg5yx3ryywlulcbyx3rl
   c3qsihlfdhjhaw4sihlfdgvzdcwgaw1hz2vzx3ryywlulcbpbwfnzxnfdgvzdca9ihryywl
   ux3rlc3rfc3bsaxqozgf0yswgzglnaxrzlnrhcmdldcwgzglnaxrzlmltywdlcywgdgvzdf
   9zaxplptaumjusihjhbmrvbv9zdgf0zt00mikilcjzy3qioijpbxbvcnrfbxnnpvwirglki
   hlvdsbpbxbvcnqgyhryywlux3rlc3rfc3bsaxrgigzyb20gyhnrbgvhcm4uy3jvc3nfdmfs
   awrhdglvbma/xcjcbnbyzwrlzl9tc2c9xcjeb24ndcbmb3jnzxqgdg8gzmlsbcbpbibgdhj
   haw5fdgvzdf9zcgxpdgahxcjcbnrlc3rfaw1wb3j0kfwic2tszwfybi5jcm9zc192ywxpzg
   f0aw9ulnryywlux3rlc3rfc3bsaxrciiwgc2ftzv9hcya9ifrydwusig5vdf9pbxbvcnrlz
   f9tc2cgpsbpbxbvcnrfbxnnlcbpbmnvcnjly3rfyxnfbxnnid0gchjlzgvmx21zzylcbnrl
   c3rfb2jqzwn0kfwiwf90cmfpblwilcbkb19ldmfspuzhbhnllcagdw5kzwzpbmvkx21zzz1
   cikrpzcb5b3ugbgvhdmugb3v0igbyx3ryywluycbvcibhbnkgb2ygdghlig90agvyihzhcm
   lhymxlcz9ciilcbnrlc3rfb2jqzwn0kfwiwf90zxn0xcisigrvx2v2yww9rmfsc2usihvuz
   gvmaw5lzf9tc2c9xcjeawqgew91igrlzmluzsbgwf90zxn0yd9ciilcbnrlc3rfb2jqzwn0
   kfwiev90cmfpblwilcbkb19ldmfspuzhbhnllcb1bmrlzmluzwrfbxnnpvwirglkihlvdsb
   kzwzpbmugyhlfdhjhaw5gp1wikvxudgvzdf9vymply3qoxcj5x3rlc3rciiwgzg9fzxzhbd
   1gywxzzswgdw5kzwzpbmvkx21zzz1cikrpzcb5b3ugzgvmaw5ligb5x3rlc3rgp1wikvxud
   gvzdf9vymply3qoxcjpbwfnzxnfdhjhaw5ciiwgzg9fzxzhbd1gywxzzswgdw5kzwzpbmvk
   x21zzz1cikrpzcb5b3ugzgvmaw5ligbpbwfnzxnfdhjhaw5gp1wikvxudgvzdf9vymply3q
   oxcjpbwfnzxnfdgvzdfwilcbkb19ldmfspuzhbhnllcb1bmrlzmluzwrfbxnnpvwirglkih
   lvdsbkzwzpbmugygltywdlc190zxn0yd9ciilcbnn1y2nlc3nfbxnnkfwir3jlyxqgam9ii
   vwiksj9

   after you have split up your data set into train and test sets, you can
   quickly inspect the numbers before you go and model the data:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuznjvbsbza2xlyxjulmnyb3nzx3zhbglkyxrpb24gaw1wb3
   j0ihryywlux3rlc3rfc3bsaxrcbmzyb20gc2tszwfybi5wcmvwcm9jzxnzaw5nigltcg9yd
   cbzy2fszvxuaw1wb3j0ig51bxb5igfzig5wxg5kawdpdhmgpsbkyxrhc2v0cy5sb2fkx2rp
   z2l0cygpxg5kyxrhid0gc2nhbguozglnaxrzlmrhdgepxg5yx3ryywlulcbyx3rlc3qsihl
   fdhjhaw4sihlfdgvzdcwgaw1hz2vzx3ryywlulcbpbwfnzxnfdgvzdca9ihryywlux3rlc3
   rfc3bsaxqozgf0yswgzglnaxrzlnrhcmdldcwgzglnaxrzlmltywdlcywgdgvzdf9zaxplp
   taumjusihjhbmrvbv9zdgf0zt00mikilcjzyw1wbguioiijie51bwjlcibvzib0cmfpbmlu
   zybmzwf0dxjlc1xubl9zyw1wbgvzlcbux2zlyxr1cmvzid0gwf90cmfpbi5zagfwzvxuxg4
   jifbyaw50ig91dcbgbl9zyw1wbgvzyfxuchjpbnqox19fx19fx19fkvxuxg4jifbyaw50ig
   91dcbgbl9mzwf0dxjlc2bcbnbyaw50kf9fx19fx19fx18pxg5cbimgtnvtymvyig9mifryy
   wluaw5nigxhymvsc1xubl9kawdpdhmgpsbszw4obnaudw5pcxvlkhlfdhjhaw4pkvxuxg4j
   ieluc3bly3qgyhlfdhjhaw5gxg5wcmludchszw4ox19fx19fxykpiiwic29sdxrpb24ioii
   jie51bwjlcibvzib0cmfpbmluzybmzwf0dxjlc1xubl9zyw1wbgvzlcbux2zlyxr1cmvzid
   0gwf90cmfpbi5zagfwzvxuxg4jifbyaw50ig91dcbgbl9zyw1wbgvzyfxuchjpbnqobl9zy
   w1wbgvzkvxuxg4jifbyaw50ig91dcbgbl9mzwf0dxjlc2bcbnbyaw50kg5fzmvhdhvyzxmp
   xg5cbimgtnvtymvyig9mifryywluaw5nigxhymvsc1xubl9kawdpdhmgpsbszw4obnaudw5
   pcxvlkhlfdhjhaw4pkvxuxg4jieluc3bly3qgyhlfdhjhaw5gxg5wcmludchszw4oev90cm
   fpbikpiiwic2n0ijoidgvzdf9vymply3qoxcjux3nhbxbszxnciiwgdw5kzwzpbmvkx21zz
   z1cimrpzcb5b3ugbgvhdmugb3v0igbux3nhbxbszxngig9yigbux2zlyxr1cmvzyd9ciilc
   bnrlc3rfb2jqzwn0kfwibl9mzwf0dxjlc1wikvxudgvzdf9mdw5jdglvbihcbiagicbcinb
   yaw50xcisxg4gicagmsxcbiagicbub3rfy2fsbgvkx21zzz1cikrpzcb5b3ugchjpbnqgb3
   v0ihrozsbudw1izxigb2ygc2ftcgxlcybvzib0agugygrpz2l0c2agdhjhaw5pbmcgzgf0y
   t9ciixcbiagicbpbmnvcnjly3rfbxnnpvwirg9uj3qgzm9yz2v0ihrvihbyaw50ig91dcb0
   agugbnvtymvyig9mihnhbxbszxmhxcisxg4gicagzg9fzxzhbd1gywxzzvxukvxudgvzdf9
   mdw5jdglvbihcbiagicbcinbyaw50xcisxg4gicagmixcbiagicbub3rfy2fsbgvkx21zzz
   1cikrpzcb5b3ugchjpbnqgb3v0ihrozsbudw1izxigb2ygzmvhdhvyzxmgb2ygdghligbka
   wdpdhngihryywluaw5nigrhdge/xcisxg4gicagaw5jb3jyzwn0x21zzz1cikrvbid0igzv
   cmdldcb0bybwcmludcbvdxqgdghlig51bwjlcibvzibmzwf0dxjlcyfciixcbiagicbkb19
   ldmfspuzhbhnlxg4pxg50zxn0x29iamvjdchcim5fzglnaxrzxcisigluy29ycmvjdf9tc2
   c9xcjkawqgew91igrlzmluzsbgbl9kawdpdhngignvcnjly3rset9ciilcbnrlc3rfznvuy
   3rpb24oxg4gicagxcjwcmludfwilfxuicagidmsxg4gicagbm90x2nhbgxlzf9tc2c9xcje
   awqgew91ihbyaw50ig91dcb0agugbnvtymvyig9mihryywluaw5nigxhymvscybmb3igdgh
   ligbkawdpdhngigrhdge/xcisxg4gicagaw5jb3jyzwn0x21zzz1cikrvbid0igzvcmdldc
   b0bybwcmludcbvdxqgdghlig51bwjlcibvzib0cmfpbmluzybsywjlbhmgd2l0acbgbgvuk
   hlfdhjhaw4pycfciixcbiagicbkb19ldmfspuzhbhnlxg4pxg5zdwnjzxnzx21zzyhcildl
   bgwgzg9uzsfciikifq==

   you   ll see that the training set x_train now contains 1347 samples,
   which is precisely 2/3d of the samples that the original data set
   contained, and 64 features, which hasn   t changed. the y_train training
   set also contains 2/3d of the labels of the original data set. this
   means that the test sets x_test and y_test contains 450 samples.

id91 the digits data

   after all these preparation steps, you have made sure that all your
   known (training) data is stored. no actual model or learning was
   performed up until this moment.

   now, it   s finally time to find those clusters of your training set. use
   kmeans() from the cluster module to set up your model. you   ll see that
   there are three arguments that are passed to this method: init,
   n_clusters and the random_state.

   you might still remember this last argument from before when you split
   the data into training and test sets. this argument basically
   guaranteed that you got reproducible results.
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuznjvbsbza2xlyxjulmnyb3nzx3zhbglkyxrpb24gaw1wb3
   j0ihryywlux3rlc3rfc3bsaxrcbmzyb20gc2tszwfybi5wcmvwcm9jzxnzaw5nigltcg9yd
   cbzy2fszvxuaw1wb3j0ig51bxb5igfzig5wxg5kawdpdhmgpsbkyxrhc2v0cy5sb2fkx2rp
   z2l0cygpxg5kyxrhid0gc2nhbguozglnaxrzlmrhdgepxg5yx3ryywlulcbyx3rlc3qsihl
   fdhjhaw4sihlfdgvzdcwgaw1hz2vzx3ryywlulcbpbwfnzxnfdgvzdca9ihryywlux3rlc3
   rfc3bsaxqozgf0yswgzglnaxrzlnrhcmdldcwgzglnaxrzlmltywdlcywgdgvzdf9zaxplp
   taumjusihjhbmrvbv9zdgf0zt00mikilcjzyw1wbguioiijieltcg9ydcb0agugygnsdxn0
   zxjgig1vzhvszvxuznjvbsbza2xlyxjuigltcg9ydcbfx19fx19fx1xuxg4jienyzwf0zsb
   0agugs01lyw5zig1vzgvsxg5jbgygpsbjbhvzdgvylktnzwfucyhpbml0psdrlw1lyw5zky
   snlcbux2nsdxn0zxjzptewlcbyyw5kb21fc3rhdgu9ndipxg5cbimgrml0ihrozsb0cmfpb
   mluzybkyxrhigbyx3ryywluyhrvihrozsbtb2rlbfxuy2xmlmzpdchfx19fx19fxykilcjz
   b2x1dglvbii6iimgsw1wb3j0ihrozsbgy2x1c3rlcmagbw9kdwxlxg5mcm9tihnrbgvhcm4
   gaw1wb3j0ignsdxn0zxjcblxuiybdcmvhdgugdghlietnzwfucybtb2rlbfxuy2xmid0gy2
   x1c3rlci5ltwvhbnmoaw5pdd0nay1tzwfucysrjywgbl9jbhvzdgvycz0xmcwgcmfuzg9tx
   3n0yxrlptqykvxuxg4jiezpdcb0agugdhjhaw5pbmcgzgf0ysb0byb0agugbw9kzwxcbmns
   zi5maxqowf90cmfpbikilcjzy3qioijpbxbvcnrfbxnnpvwirglkihlvdsbpbxbvcnqgygn
   sdxn0zxjgigzyb20gyhnrbgvhcm5gp1wixg5wcmvkzwzfbxnnpvwirg9uj3qgzm9yz2v0ih
   rvigltcg9ydcbgy2x1c3rlcibmcm9tigbza2xlyxjuycfcilxudgvzdf9pbxbvcnqoxcjza
   2xlyxjulmnsdxn0zxjciiwgc2ftzv9hcya9ifrydwusig5vdf9pbxbvcnrlzf9tc2cgpsbp
   bxbvcnrfbxnnlcbpbmnvcnjly3rfyxnfbxnnid0gchjlzgvmx21zzylcbnrlc3rfb2jqzwn
   0kfwiy2xmxcisigrvx2v2yww9rmfsc2usigluy29ycmvjdf9tc2c9xcjkawqgy3jlyxrlih
   rozsbltwvhbnmgbw9kzwwgy29ycmvjdgx5p1wikvxudgvzdf9mdw5jdglvbihcimnszi5ma
   xrciiwgzg9fzxzhbd1gywxzzslcbnn1y2nlc3nfbxnnkfwiv29vag9vivwiksj9

   the init indicates the method for initialization and even though it
   defaults to    id116++   , you see it explicitly coming back in the code.
   that means that you can leave it out if you want. try it out in the
   datacamp light chunk above!

   next, you also see that the n_clusters argument is set to 10. this
   number not only indicates the number of clusters or groups you want
   your data to form, but also the number of centroids to generate.
   remember that a cluster centroid is the middle of a cluster.

   do you also still remember how the previous section described this as
   one of the possible disadvantages of the id116 algorithm?

   that is that the initial set of cluster centers that you give up can
   have a significant effect on the clusters that are eventually found?

   usually, you try to deal with this effect by trying several initial
   sets in multiple runs and by selecting the set of clusters with the
   minimum sum of the squared errors (sse). in other words, you want to
   minimize the distance of each point in the cluster to the mean or
   centroid of that cluster.

   by adding the n-init argument to kmeans(), you can determine how many
   different centroid configurations the algorithm will try.

   note again that you don   t want to insert the test labels when you fit
   the model to your data: these will be used to see if your model is good
   at predicting the actual classes of your instances!

   you can also visualize the images that make up the cluster centers as
   follows:
# import matplotlib
import matplotlib.pyplot as plt

# figure size in inches
fig = plt.figure(figsize=(8, 3))

# add title
fig.suptitle('cluster center images', fontsize=14, fontweight='bold')

# for all labels (0-9)
for i in range(10):
    # initialize subplots in a grid of 2x5, at i+1th position
    ax = fig.add_subplot(2, 5, 1 + i)
    # display images
    ax.imshow(clf.cluster_centers_[i].reshape((8, 8)), cmap=plt.cm.binary)
    # don't show the axes
    plt.axis('off')

# show the plot
plt.show()

   kmeans cluster visualization with scikit-learn

   if you want to see another example that visualizes the data clusters
   and their centers, go [23]here.

   the next step is to predict the labels of the test set:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuznjvbsbza2xlyxjulmnyb3nzx3zhbglkyxrpb24gaw1wb3
   j0ihryywlux3rlc3rfc3bsaxrcbmzyb20gc2tszwfybi5wcmvwcm9jzxnzaw5nigltcg9yd
   cbzy2fszvxuznjvbsbza2xlyxjuigltcg9ydcbjbhvzdgvyxg5kawdpdhmgpsbkyxrhc2v0
   cy5sb2fkx2rpz2l0cygpxg5kyxrhid0gc2nhbguozglnaxrzlmrhdgepxg5yx3ryywlulcb
   yx3rlc3qsihlfdhjhaw4sihlfdgvzdcwgaw1hz2vzx3ryywlulcbpbwfnzxnfdgvzdca9ih
   ryywlux3rlc3rfc3bsaxqozgf0yswgzglnaxrzlnrhcmdldcwgzglnaxrzlmltywdlcywgd
   gvzdf9zaxplptaumjusihjhbmrvbv9zdgf0zt00milcbmnszia9ignsdxn0zxius01lyw5z
   kgluaxq9j2stbwvhbnmrkycsig5fy2x1c3rlcnm9mtasihjhbmrvbv9zdgf0zt00milcbmn
   szi5maxqowf90cmfpbikilcjzyw1wbguioiijifbyzwrpy3qgdghligxhymvscybmb3igyf
   hfdgvzdgbcbnlfchjlzd1jbgyuchjlzgljdchyx3rlc3qpxg5cbimguhjpbnqgb3v0ihroz
   sbmaxjzdcaxmdagaw5zdgfuy2vzig9migb5x3byzwrgxg5wcmludch5x3byzwrbojewmf0p
   xg5cbimguhjpbnqgb3v0ihrozsbmaxjzdcaxmdagaw5zdgfuy2vzig9migb5x3rlc3rgxg5
   wcmludch5x3rlc3rbojewmf0pxg5cbimgu3r1zhkgdghlihnoyxblig9mihrozsbjbhvzdg
   vyignlbnrlid98cbmnszi5jbhvzdgvyx2nlbnrlid98fll9fx19fiiwic29sdxrpb24ioiiji
   fbyzwrpy3qgdghligxhymvscybmb3igyfhfdgvzdgbcbnlfchjlzd1jbgyuchjlzgljdchy
   x3rlc3qpxg5cbimguhjpbnqgb3v0ihrozsbmaxjzdcaxmdagaw5zdgfuy2vzig9migb5x3b
   yzwrgxg5wcmludch5x3byzwrbojewmf0pxg5cbimguhjpbnqgb3v0ihrozsbmaxjzdcaxmd
   agaw5zdgfuy2vzig9migb5x3rlc3rgxg5wcmludch5x3rlc3rbojewmf0pxg5cbimgu3r1z
   hkgdghlihnoyxblig9mihrozsbjbhvzdgvyignlbnrlid98cbmnszi5jbhvzdgvyx2nlbnrl
   id98flnnoyxbliiwic2n0ijoidgvzdf9vymply3qoxcj5x3byzwrciilcbnrlc3rfznvuy3r
   pb24oxg4gicagxcjwcmludfwilfxuicagidesxg4gicagbm90x2nhbgxlzf9tc2c9xcjeaw
   qgew91ihbyaw50ig91dcb0agugzmlyc3qgmtawigluc3rhbmnlcybvzibgev9wcmvkyd9ci
   ixcbiagicbpbmnvcnjly3rfbxnnpvwirg9uj3qgzm9yz2v0ihrvihbyaw50ig91dcb0agug
   zmlyc3qgmtawigluc3rhbmnlcybvzibgev9wcmvkycfciixcbiagicbkb19ldmfspuzhbhn
   lxg4pxg50zxn0x2z1bmn0aw9ukfxuicagifwichjpbnrciixcbiagicaylfxuicagig5vdf
   9jywxszwrfbxnnpvwirglkihlvdsbwcmludcbvdxqgdghligzpid980idewmcbpbnn0yw5jz
   xmgb2ygyhlfdgvzdga/xcisxg4gicagaw5jb3jyzwn0x21zzz1cikrvbid0igzvcmdldcb0
   bybwcmludcbvdxqgdghligzpid980idewmcbpbnn0yw5jzxmgb2ygyhlfdgvzdgahxcisxg4
   gicagzg9fzxzhbd1gywxzzvxukvxubxnnx2rhdge9xcjeawqgew91igzpbgwgaw4gyhnoyx
   blycb0bybwcmludcbvdxqgdghlihnoyxblig9mihrozsbjbhvzdgvyignlbnrlcnm/xcjcb
   nrlc3rfb2jqzwn0x2fjy2vzc2vkkfwiy2xmlmnsdxn0zxjfy2vudgvyc18uc2hhcgvciiwg
   bm90x2fjy2vzc2vkx21zzz1tc2dfzgf0yslcbnn1y2nlc3nfbxnnpvwiqxdlc29tzsfciij
   9

   in the code chunk above, you predict the values for the test set, which
   contains 450 samples. you store the result in y_pred. you also print
   out the first 100 instances of y_pred and y_test, and you immediately
   see some results.

   in addition, you can study the shape of the cluster centers: you
   immediately see that there are 10 clusters with each 64 features.

   but this doesn   t tell you much because we set the number of clusters to
   10 and you already knew that there were 64 features.

   maybe a visualization would be more helpful.

   let   s visualize the predicted labels:
# import `isomap()`
from sklearn.manifold import isomap

# create an isomap and fit the `digits` data to it
x_iso = isomap(n_neighbors=10).fit_transform(x_train)

# compute cluster centers and predict cluster index for each sample
clusters = clf.fit_predict(x_train)

# create a plot with subplots in a grid of 1x2
fig, ax = plt.subplots(1, 2, figsize=(8, 4))

# adjust layout
fig.suptitle('predicted versus training labels', fontsize=14, fontweight='bold')
fig.subplots_adjust(top=0.85)

# add scatterplots to the subplots
ax[0].scatter(x_iso[:, 0], x_iso[:, 1], c=clusters)
ax[0].set_title('predicted training labels')
ax[1].scatter(x_iso[:, 0], x_iso[:, 1], c=y_train)
ax[1].set_title('actual training labels')

# show the plots
plt.show()

   you use isomap() as a way to reduce the dimensions of your
   high-dimensional data set digits. the difference with the pca method is
   that the isomap is a non-linear reduction method.

   isomap visualization

   tip: run the code from above again, but use the pca reduction method
   instead of the isomap to study the effect of reduction methods
   yourself.

   you will find the solution here:
# import `pca()`
from sklearn.decomposition import pca

# model and fit the `digits` data to the pca model
x_pca = pca(n_components=2).fit_transform(x_train)

# compute cluster centers and predict cluster index for each sample
clusters = clf.fit_predict(x_train)

# create a plot with subplots in a grid of 1x2
fig, ax = plt.subplots(1, 2, figsize=(8, 4))

# adjust layout
fig.suptitle('predicted versus training labels', fontsize=14, fontweight='bold')
fig.subplots_adjust(top=0.85)

# add scatterplots to the subplots
ax[0].scatter(x_pca[:, 0], x_pca[:, 1], c=clusters)
ax[0].set_title('predicted training labels')
ax[1].scatter(x_pca[:, 0], x_pca[:, 1], c=y_train)
ax[1].set_title('actual training labels')

# show the plots
plt.show()

   pca visualization with matplotlib

   at first sight, the visualization doesn   t seem to indicate that the
   model works well.

   but this needs some further investigation.

evaluation of your id91 model

   and this need for further investigation brings you to the next
   essential step, which is the evaluation of your model   s performance. in
   other words, you want to analyze the degree of correctness of the
   model   s predictions.

   let   s print out a confusion matrix:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuznjvbsbza2xlyxjulmnyb3nzx3zhbglkyxrpb24gaw1wb3
   j0ihryywlux3rlc3rfc3bsaxrcbmzyb20gc2tszwfybi5wcmvwcm9jzxnzaw5nigltcg9yd
   cbzy2fszvxuznjvbsbza2xlyxjuigltcg9ydcbjbhvzdgvyxg5kawdpdhmgpsbkyxrhc2v0
   cy5sb2fkx2rpz2l0cygpxg5kyxrhid0gc2nhbguozglnaxrzlmrhdgepxg5yx3ryywlulcb
   yx3rlc3qsihlfdhjhaw4sihlfdgvzdcwgaw1hz2vzx3ryywlulcbpbwfnzxnfdgvzdca9ih
   ryywlux3rlc3rfc3bsaxqozgf0yswgzglnaxrzlnrhcmdldcwgzglnaxrzlmltywdlcywgd
   gvzdf9zaxplptaumjusihjhbmrvbv9zdgf0zt00milcbmnszia9ignsdxn0zxius01lyw5z
   kgluaxq9j2stbwvhbnmrkycsig5fy2x1c3rlcnm9mtasihjhbmrvbv9zdgf0zt00milcbmn
   szi5maxqowf90cmfpbilcbnlfchjlzd1jbgyuchjlzgljdchyx3rlc3qpiiwic2ftcgxlij
   oiiybjbxbvcnqgyg1ldhjpy3ngigzyb20gyhnrbgvhcm5gxg5mcm9tihnrbgvhcm4gaw1wb
   3j0if9fx19fx19cblxuiybqcmludcbvdxqgdghlignvbmz1c2lvbibtyxryaxggd2l0acbg
   y29uznvzaw9ux21hdhjpecgpyfxuchjpbnqobwv0cmljcy5jb25mdxnpb25fbwf0cml4khl
   fdgvzdcwgev9wcmvkkskilcjzb2x1dglvbii6iimgsw1wb3j0igbtzxryawnzycbmcm9tig
   bza2xlyxjuyfxuznjvbsbza2xlyxjuigltcg9ydcbtzxryawnzxg5cbimguhjpbnqgb3v0i
   hrozsbjb25mdxnpb24gbwf0cml4ihdpdgggygnvbmz1c2lvbl9tyxryaxgokwbcbnbyaw50
   kg1ldhjpy3muy29uznvzaw9ux21hdhjpech5x3rlc3qsihlfchjlzckpiiwic2n0ijoidgv
   zdf9pbxbvcnqoxcjza2xlyxjulm1ldhjpy3nciiwgc2ftzv9hcya9ifrydwusig5vdf9pbx
   bvcnrlzf9tc2cgpsbcikrpzcb5b3ugaw1wb3j0igbtzxryawnzycbmcm9tigbza2xlyxjuy
   d9ciiwgaw5jb3jyzwn0x2fzx21zzya9ifwirg9uj3qgzm9yz2v0ihrvigltcg9ydcbgbwv0
   cmljc2agznjvbsbgc2tszwfybmahxcipxg50zxn0x2z1bmn0aw9ukfxuicagifwichjpbnr
   ciixcbiagicbub3rfy2fsbgvkx21zzz1cikrpzcb5b3ugchjpbnqgb3v0ihrozsbjb25mdx
   npb24gbwf0cml4p1wilfxuicagigluy29ycmvjdf9tc2c9xcjeb24ndcbmb3jnzxqgdg8gc
   hjpbnqgb3v0ihrozsbjb25mdxnpb24gbwf0cml4ivwilfxuicagigrvx2v2yww9rmfsc2vc
   bilcbnn1y2nlc3nfbxnnpvwiv2vsbcbkb25lisbob3csihdoyxqgzg8gdghlihjlc3vsdhm
   gdgvsbcb5b3u/xciifq==

   at first sight, the results seem to confirm our first thoughts that you
   gathered from the visualizations. only the digit 5 was classified
   correctly in 41 cases. also, the digit 8 was classified correctly in 11
   instances. but this is not really a success.

   you might need to know a bit more about the results than just the
   confusion matrix.

   let   s try to figure out something more about the quality of the
   clusters by applying different cluster quality metrics. that way, you
   can judge the goodness of fit of the cluster labels to the correct
   labels.
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuznjvbsbza2xlyxjulmnyb3nzx3zhbglkyxrpb24gaw1wb3
   j0ihryywlux3rlc3rfc3bsaxrcbmzyb20gc2tszwfybi5wcmvwcm9jzxnzaw5nigltcg9yd
   cbzy2fszvxuznjvbsbza2xlyxjuigltcg9ydcbjbhvzdgvyxg5mcm9tihnrbgvhcm4ubwv0
   cmljcybpbxbvcnqgag9tb2dlbmvpdhlfc2nvcmusignvbxbszxrlbmvzc19zy29yzswgdl9
   tzwfzdxjlx3njb3jllcbhzgp1c3rlzf9yyw5kx3njb3jllcbhzgp1c3rlzf9tdxr1ywxfaw
   5mb19zy29yzswgc2lsag91zxr0zv9zy29yzvxuzglnaxrzid0gzgf0yxnldhmubg9hzf9ka
   wdpdhmokvxuzgf0ysa9ihnjywxlkgrpz2l0cy5kyxrhkvxuwf90cmfpbiwgwf90zxn0lcb5
   x3ryywlulcb5x3rlc3qsigltywdlc190cmfpbiwgaw1hz2vzx3rlc3qgpsb0cmfpbl90zxn
   0x3nwbgl0kgrhdgesigrpz2l0cy50yxjnzxqsigrpz2l0cy5pbwfnzxmsihrlc3rfc2l6zt
   0wlji1lcbyyw5kb21fc3rhdgu9ndipxg5jbgygpsbjbhvzdgvylktnzwfucyhpbml0psdrl
   w1lyw5zkysnlcbux2nsdxn0zxjzptewlcbyyw5kb21fc3rhdgu9ndipxg5jbgyuzml0kfhf
   dhjhaw4pxg55x3byzwq9y2xmlnbyzwrpy3qowf90zxn0ksisinnhbxbszsi6imzyb20gc2t
   szwfybi5tzxryawnzigltcg9ydcbob21vz2vuzwl0ev9zy29yzswgy29tcgxldgvuzxnzx3
   njb3jllcb2x21lyxn1cmvfc2nvcmusigfkanvzdgvkx3jhbmrfc2nvcmusigfkanvzdgvkx
   211dhvhbf9pbmzvx3njb3jllcbzawxob3vldhrlx3njb3jlxg5wcmludcgnjsa5cycgjsan
   aw5lcnrpysagicbob21vicagy29tcgwgihytbwvhcyagicagqvjjiefnssagc2lsag91zxr
   0zscpxg5wcmludcgnjwkgicalljnmicagjs4zziagicuum2ygicalljnmicagjs4zziagic
   alljnmj1xuicagicagicagicuoy2xmlmluzxj0awfflfxuicagicagag9tb2dlbmvpdhlfc
   2nvcmuoev90zxn0lcb5x3byzwqplfxuicagicagy29tcgxldgvuzxnzx3njb3jlkhlfdgvz
   dcwgev9wcmvkksxcbiagicagihzfbwvhc3vyzv9zy29yzsh5x3rlc3qsihlfchjlzcksxg4
   gicagicbhzgp1c3rlzf9yyw5kx3njb3jlkhlfdgvzdcwgev9wcmvkksxcbiagicagigfkan
   vzdgvkx211dhvhbf9pbmzvx3njb3jlkhlfdgvzdcwgev9wcmvkksxcbiagicagihnpbghvd
   wv0dgvfc2nvcmuowf90zxn0lcb5x3byzwqsig1ldhjpyz0nzxvjbglkzwfujykpksj9

   you   ll see that there are quite some metrics to consider:
     * the homogeneity score tells you to what extent all of the clusters
       contain only data points which are members of a single class.
     * the completeness score measures the extent to which all of the data
       points that are members of a given class are also elements of the
       same cluster.
     * the v-measure score is the harmonic mean between homogeneity and
       completeness.
     * the adjusted rand score measures the similarity between two
       id91s and considers all pairs of samples and counting pairs
       that are assigned in the same or different clusters in the
       predicted and true id91s.
     * the adjusted mutual info (ami) score is used to compare clusters.
       it measures the similarity between the data points that are in the
       id91s, accounting for chance groupings and takes a maximum
       value of 1 when id91s are equivalent.
     * the silhouette score measures how similar an object is to its own
       cluster compared to other clusters. the silhouette scores range
       from -1 to 1, where a higher value indicates that the object is
       better matched to its own cluster and worse matched to neighboring
       clusters. if many points have a high value, the id91
       configuration is good.

   you clearly see that these scores aren   t fantastic: for example, you
   see that the value for the silhouette score is close to 0, which
   indicates that the sample is on or very close to the decision boundary
   between two neighboring clusters. this could indicate that the samples
   could have been assigned to the wrong cluster.

   also, the ari measure seems to indicate that not all data points in a
   given cluster are similar and the completeness score tells you that
   there are definitely data points that weren   t put in the right cluster.

   clearly, you should consider another estimator to predict the labels
   for the digits data.

trying out another model: support vector machines

   when you recapped all of the information that you gathered out of the
   data exploration, you saw that you could build a model to predict which
   group a digit belongs to without you knowing the labels. and indeed,
   you just used the training data and not the target values to build your
   kmeans model.

   let   s assume that you depart from the case where you use both the
   digits training data and the corresponding target values to build your
   model.

   if you follow the algorithm map, you   ll see that the first model that
   you meet is the linear svc. let   s apply this now to the digits data:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuznjvbsbza2xlyxjulnbyzxbyb2nlc3npbmcgaw1wb3j0ih
   njywxlxg5mcm9tihnrbgvhcm4gaw1wb3j0ignsdxn0zxjcbmrpz2l0cya9igrhdgfzzxrzl
   mxvywrfzglnaxrzkclcbmrhdgegpsbzy2fszshkawdpdhmuzgf0yskilcjzyw1wbguioiij
   ieltcg9ydcbgdhjhaw5fdgvzdf9zcgxpdgbcbmzyb20gc2tszwfybi5jcm9zc192ywxpzgf
   0aw9uigltcg9ydcb0cmfpbl90zxn0x3nwbgl0xg5cbimgu3bsaxqgdghligrhdgegaw50by
   b0cmfpbmluzybhbmqgdgvzdcbzzxrzifxuwf90cmfpbiwgwf90zxn0lcb5x3ryywlulcb5x
   3rlc3qsigltywdlc190cmfpbiwgaw1hz2vzx3rlc3qgpsb0cmfpbl90zxn0x3nwbgl0kgrp
   z2l0cy5kyxrhlcbkawdpdhmudgfyz2v0lcbkawdpdhmuaw1hz2vzlcb0zxn0x3npemu9mc4
   ynswgcmfuzg9tx3n0yxrlptqykvxuxg4jieltcg9ydcb0agugyhn2bwagbw9kzwxcbmzyb2
   0gc2tszwfybibpbxbvcnqgc3ztxg5cbimgq3jlyxrlihrozsbtvkmgbw9kzwwgxg5zdmnfb
   w9kzwwgpsbzdm0uu1zdkgdhbw1hptaumdaxlcbdptewmc4sigtlcm5lbd0nbgluzwfyjylc
   blxuiybgaxqgdghligrhdgegdg8gdghlifnwqybtb2rlbfxuc3zjx21vzgvslmzpdchyx3r
   yywlulcb5x3ryywluksisinnvbhv0aw9uijoiiybjbxbvcnqgyhryywlux3rlc3rfc3bsax
   rgxg5mcm9tihnrbgvhcm4uy3jvc3nfdmfsawrhdglvbibpbxbvcnqgdhjhaw5fdgvzdf9zc
   gxpdfxuxg4jifnwbgl0ihrozsbkyxrhigludg8gdhjhaw5pbmcgyw5kihrlc3qgc2v0cybc
   blhfdhjhaw4sifhfdgvzdcwgev90cmfpbiwgev90zxn0lcbpbwfnzxnfdhjhaw4sigltywd
   lc190zxn0id0gdhjhaw5fdgvzdf9zcgxpdchkawdpdhmuzgf0yswgzglnaxrzlnrhcmdldc
   wgzglnaxrzlmltywdlcywgdgvzdf9zaxplptaumjusihjhbmrvbv9zdgf0zt00milcblxui
   ybjbxbvcnqgdghligbzdm1gig1vzgvsxg5mcm9tihnrbgvhcm4gaw1wb3j0ihn2bvxuxg4j
   ienyzwf0zsb0agugu1zdig1vzgvsifxuc3zjx21vzgvsid0gc3ztllnwqyhnyw1tyt0wlja
   wmswgqz0xmdaulcbrzxjuzww9j2xpbmvhcicpxg5cbimgrml0ihrozsbkyxrhihrvihrozs
   btvkmgbw9kzwxcbnn2y19tb2rlbc5maxqowf90cmfpbiwgev90cmfpbikilcjzy3qioij0z
   xn0x2ltcg9ydchcinnrbgvhcm4uy3jvc3nfdmfsawrhdglvbi50cmfpbl90zxn0x3nwbgl0
   xcisihnhbwvfyxmgpsbucnvllcbub3rfaw1wb3j0zwrfbxnnid0gxcjeawqgew91igltcg9
   ydcbgdhjhaw5fdgvzdf9zcgxpdgagznjvbsbgc2tszwfybi5jcm9zc192ywxpzgf0aw9uyd
   9ciiwgaw5jb3jyzwn0x2fzx21zzya9ifwirg9uj3qgzm9yz2v0ihrvigltcg9ydcbgdhjha
   w5fdgvzdf9zcgxpdgagznjvbsbgc2tszwfybi5jcm9zc192ywxpzgf0aw9uycfciilcbnrl
   c3rfb2jqzwn0kfwiwf90cmfpblwilcbkb19ldmfspuzhbhnllcb1bmrlzmluzwrfbxnnpvw
   izglkihlvdsbkzwzpbmugyfhfdhjhaw5gp1wikvxudgvzdf9vymply3qoxcjyx3rlc3rcii
   wgzg9fzxzhbd1gywxzzswgdw5kzwzpbmvkx21zzz1cimrpzcb5b3ugzgvmaw5ligbyx3rlc
   3rgp1wikvxudgvzdf9vymply3qoxcj5x3ryywluxcisigrvx2v2yww9rmfsc2usihvuzgvm
   aw5lzf9tc2c9xcjkawqgew91igrlzmluzsbgev90cmfpbma/xcipxg50zxn0x29iamvjdch
   cinlfdgvzdfwilcbkb19ldmfspuzhbhnllcb1bmrlzmluzwrfbxnnpvwizglkihlvdsbkzw
   zpbmugyhlfdgvzdga/xcipxg50zxn0x29iamvjdchcimltywdlc190cmfpblwilcbkb19ld
   mfspuzhbhnllcb1bmrlzmluzwrfbxnnpvwizglkihlvdsbkzwzpbmugygltywdlc190cmfp
   bma/xcipxg50zxn0x29iamvjdchcimltywdlc190zxn0xcisigrvx2v2yww9rmfsc2usihv
   uzgvmaw5lzf9tc2c9xcjkawqgew91igrlzmluzsbgaw1hz2vzx3rlc3rgp1wikvxudgvzdf
   9pbxbvcnqoxcjza2xlyxjulnn2bvwilcbzyw1lx2fzid0gvhj1zswgbm90x2ltcg9ydgvkx
   21zzya9ifwirglkihlvdsbpbxbvcnqgyhn2bwagznjvbsbgc2tszwfybma/xcisigluy29y
   cmvjdf9hc19tc2cgpsbcikrvbid0igzvcmdldcb0bybpbxbvcnqgyhn2bwagznjvbsbgc2t
   szwfybmahxcipxg50zxn0x29iamvjdchcinn2y19tb2rlbfwilcbkb19ldmfspuzhbhnlkv
   xudgvzdf9mdw5jdglvbihcinn2y19tb2rlbc5maxrciiwgzg9fzxzhbd1gywxzzslcbnn1y
   2nlc3nfbxnnpvwir3jlyxqgam9iivwiin0=

   you see here that you make use of x_train and y_train to fit the data
   to the svc model. this is clearly different from id91. note also
   that in this example, you set the value of gamma manually. it is
   possible to automatically find good values for the parameters by using
   tools such as grid search and cross-validation.

   even though this is not the focus of this tutorial, you will see how
   you could have gone about this if you would have made use of grid
   search to adjust your parameters. you would have done something like
   the following:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbzdm1cbmzyb20gc2tszwfybibpbxbvcnqgzgf0yxnldhncbmzyb20gc2tszw
   fybi5jcm9zc192ywxpzgf0aw9uigltcg9ydcb0cmfpbl90zxn0x3nwbgl0xg5kawdpdhmgp
   sbkyxrhc2v0cy5sb2fkx2rpz2l0cygpiiwic2ftcgxlijoiiybtcgxpdcb0agugygrpz2l0
   c2agzgf0ysbpbnrvihr3byblcxvhbcbzzxrzxg5yx3ryywlulcbyx3rlc3qsihlfdhjhaw4
   sihlfdgvzdca9ihryywlux3rlc3rfc3bsaxqozglnaxrzlmrhdgesigrpz2l0cy50yxjnzx
   qsihrlc3rfc2l6zt0wljusihjhbmrvbv9zdgf0zt0wkvxuxg4jieltcg9ydcbhcmlku2vhc
   mnoq1zcbmzyb20gc2tszwfybi5ncmlkx3nlyxjjacbpbxbvcnqgr3jpzfnlyxjjaenwxg5c
   bimgu2v0ihrozsbwyxjhbwv0zxigy2fuzglkyxrlc1xucgfyyw1ldgvyx2nhbmrpzgf0zxm
   gpsbbxg4gihsnqyc6ifsxlcaxmcwgmtawlcaxmdawxswgj2tlcm5lbcc6ifsnbgluzwfyj1
   19lfxuicb7j0mnoibbmswgmtasidewmcwgmtawmf0sicdnyw1tysc6ifswljawmswgmc4wm
   daxxswgj2tlcm5lbcc6ifsncmjmj119lfxuxvxuxg4jienyzwf0zsbhignsyxnzawzpzxig
   d2l0acb0agugcgfyyw1ldgvyignhbmrpzgf0zxncbmnszia9iedyawrtzwfyy2hdvihlc3r
   pbwf0b3i9c3ztllnwqygplcbwyxjhbv9ncmlkpxbhcmftzxrlcl9jyw5kawrhdgvzlcbux2
   pvynm9ltepxg5cbimgvhjhaw4gdghlignsyxnzawzpzxigb24gdhjhaw5pbmcgzgf0yvxuy
   2xmlmzpdchyx3ryywlulcb5x3ryywlukvxuxg4jifbyaw50ig91dcb0agugcmvzdwx0cybc
   bnbyaw50kcdczxn0ihnjb3jligzvcib0cmfpbmluzybkyxrhoicsignszi5izxn0x3njb3j
   lxylcbnbyaw50kcdczxn0igbdydoid86nszi5izxn0x2vzdgltyxrvcl8uqylcbnbyaw50kc
   dczxn0igtlcm5lbdoid86nszi5izxn0x2vzdgltyxrvcl8ua2vybmvskvxuchjpbnqoj0jlc
   3qgygdhbw1hydoid86nszi5izxn0x2vzdgltyxrvcl8uz2ftbwepin0=

   next, you use the classifier with the classifier and parameter
   candidates that you have just created to apply it to the second part of
   your data set. next, you also train a new classifier using the best
   parameters found by the grid search. you score the result to see if the
   best parameters that were found in the grid search are actually
   working.
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbzdm1cbmzyb20gc2tszwfybibpbxbvcnqgzgf0yxnldhncbmzyb20gc2tszw
   fybi5jcm9zc192ywxpzgf0aw9uigltcg9ydcb0cmfpbl90zxn0x3nwbgl0xg5kawdpdhmgp
   sbkyxrhc2v0cy5sb2fkx2rpz2l0cygpxg5yx3ryywlulcbyx3rlc3qsihlfdhjhaw4sihlf
   dgvzdca9ihryywlux3rlc3rfc3bsaxqozglnaxrzlmrhdgesigrpz2l0cy50yxjnzxqsihr
   lc3rfc2l6zt0wljusihjhbmrvbv9zdgf0zt0wkvxuznjvbsbza2xlyxjulmdyawrfc2vhcm
   noigltcg9ydcbhcmlku2vhcmnoq1zcbnbhcmftzxrlcl9jyw5kawrhdgvzid0gw1xuicb7j
   0mnoibbmswgmtasidewmcwgmtawmf0sicdrzxjuzwwnoibbj2xpbmvhciddfsxcbiageydd
   jzogwzesidewlcaxmdasidewmdbdlcanz2ftbwenoibbmc4wmdesidaumdawmv0sicdrzxj
   uzwwnoibbj3jiziddfsxcbl1cbmnszia9iedyawrtzwfyy2hdvihlc3rpbwf0b3i9c3ztll
   nwqygplcbwyxjhbv9ncmlkpxbhcmftzxrlcl9jyw5kawrhdgvzlcbux2pvynm9ltepxg5jb
   gyuzml0kfhfdhjhaw4sihlfdhjhaw4piiwic2ftcgxlijoiiybbchbsesb0agugy2xhc3np
   zmllcib0byb0agugdgvzdcbkyxrhlcbhbmqgdmlldyb0agugywnjdxjhy3kgc2nvcmvcbmn
   szi5zy29yzshyx3rlc3qsihlfdgvzdckgifxuxg4jifryywluigfuzcbzy29yzsbhig5ldy
   bjbgfzc2lmawvyihdpdgggdghligdyawqgc2vhcmnoihbhcmftzxrlid98cbnn2bs5tvkmoq
   z0xmcwga2vybmvspsdyymynlcbnyw1tyt0wljawmskuzml0kfhfdhjhaw4sihlfdhjhaw4p
   lnnjb3jlkfhfdgvzdcwgev90zxn0ksj9

   the parameters indeed work well!

   now, what does this new knowledge tell you about the svc classifier
   that you had modeled before you had done the grid search?

   let   s back up to the model that you had made before.

   you see that in the id166 classifier, the penalty parameter c of the
   error term is specified at 100.. lastly, you see that the kernel has
   been explicitly specified as a linear one. the kernelargument specifies
   the kernel type that you   re going to use in the algorithm and by
   default, this is rbf. in other cases, you can specify others such as
   linear, poly,    

   but what is a kernel exactly?

   a kernel is a similarity function, which is used to compute the
   similarity between the training data points. when you provide a kernel
   to an algorithm, together with the training data and the labels, you
   will get a classifier, as is the case here. you will have trained a
   model that assigns new unseen objects into a particular category. for
   the id166, you will typically try to divide your data points linearly.

   however, the grid search tells you that an rbf kernel would   ve worked
   better. the penalty parameter and the gamma were specified correctly.

   tip: try out the classifier with an rbf kernel.

   for now, let   s say you continue with a linear kernel and predict the
   values for the test set:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuznjvbsbza2xlyxjulnbyzxbyb2nlc3npbmcgaw1wb3j0ih
   njywxlxg5mcm9tihnrbgvhcm4gaw1wb3j0ignsdxn0zxjcbmrpz2l0cya9igrhdgfzzxrzl
   mxvywrfzglnaxrzkclcbmrhdgegpsbzy2fszshkawdpdhmuzgf0yslcbmzyb20gc2tszwfy
   bi5jcm9zc192ywxpzgf0aw9uigltcg9ydcb0cmfpbl90zxn0x3nwbgl0xg5yx3ryywlulcb
   yx3rlc3qsihlfdhjhaw4sihlfdgvzdcwgaw1hz2vzx3ryywlulcbpbwfnzxnfdgvzdca9ih
   ryywlux3rlc3rfc3bsaxqozglnaxrzlmrhdgesigrpz2l0cy50yxjnzxqsigrpz2l0cy5pb
   wfnzxmsihrlc3rfc2l6zt0wlji1lcbyyw5kb21fc3rhdgu9ndipxg5mcm9tihnrbgvhcm4g
   aw1wb3j0ihn2bvxuc3zjx21vzgvsid0gc3ztllnwqyhnyw1tyt0wljawmswgqz0xmdaulcb
   rzxjuzww9j2xpbmvhcicpxg5zdmnfbw9kzwwuzml0kfhfdhjhaw4sihlfdhjhaw4piiwic2
   ftcgxlijoiiybqcmvkawn0ihrozsbsywjlbcbvzibgwf90zxn0yfxuchjpbnqoc3zjx21vz
   gvslnbyzwrpy3qox19fx19fkslcblxuiybqcmludcbgev90zxn0ycb0bybjagvjayb0agug
   cmvzdwx0c1xuchjpbnqox19fx19fksisinnvbhv0aw9uijoiiybqcmvkawn0ihrozsbsywj
   lbcbvzibgwf90zxn0yfxuchjpbnqoc3zjx21vzgvslnbyzwrpy3qowf90zxn0kslcblxuiy
   bqcmludcbgev90zxn0ycb0bybjagvjayb0agugcmvzdwx0c1xuchjpbnqoev90zxn0ksisi
   nnjdci6inrlc3rfznvuy3rpb24oxg4gicagxcjwcmludfwilfxuicagidesxg4gicagbm90
   x2nhbgxlzf9tc2c9xcjeawqgew91ihbyaw50ig91dcb0agugchjlzgljdgvkigxhymvscyb
   vzibgwf90zxn0yd9ciixcbiagicbpbmnvcnjly3rfbxnnpvwirg9uj3qgzm9yz2v0ihrvih
   byaw50ig91dcb0agugchjlzgljdgvkigxhymvscybvzibgwf90zxn0ycfciixcbiagicbkb
   19ldmfspuzhbhnlxg4pxg50zxn0x2z1bmn0aw9ukfxuicagifwichjpbnrciixcbiagicay
   lfxuicagig5vdf9jywxszwrfbxnnpvwirglkihlvdsbwcmludcbvdxqgdghlihrydwugbgf
   izwxzig9migb5x3rlc3rgp1wilfxuicagigluy29ycmvjdf9tc2c9xcjeb24ndcbmb3jnzx
   qgdg8gcmv2zwfsaw5nihrozsb0cnvligxhymvscybiesbwcmludgluzybvdxqgyhlfdgvzd
   gahxcisxg4gicagzg9fzxzhbd1gywxzzvxukvxuc3vjy2vzc19tc2coxcjxzwxsigrvbmuh
   xcipin0=

   you can also visualize the images and their predicted labels:
# import matplotlib
import matplotlib.pyplot as plt

# assign the predicted values to `predicted`
predicted = svc_model.predict(x_test)

# zip together the `images_test` and `predicted` values in `images_and_predictio
ns`
images_and_predictions = list(zip(images_test, predicted))

# for the first 4 elements in `images_and_predictions`
for index, (image, prediction) in enumerate(images_and_predictions[:4]):
    # initialize subplots in a grid of 1 by 4 at positions i+1
    plt.subplot(1, 4, index + 1)
    # don't show axes
    plt.axis('off')
    # display images in all subplots in the grid
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    # add a title to the plot
    plt.title('predicted: ' + str(prediction))

# show the plot
plt.show()

   this plot is very similar to the plot that you made when you were
   exploring the data:

   images and predicted labels visualized with matplotlib

   only this time, you zip together the images and the predicted values,
   and you only take the first 4 elements of images_and_predictions.

   but now the biggest question: how does this model perform?
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiznjvbsbza2xlyxj
   uigltcg9ydcbkyxrhc2v0c1xuznjvbsbza2xlyxjulnbyzxbyb2nlc3npbmcgaw1wb3j0ih
   njywxlxg5mcm9tihnrbgvhcm4gaw1wb3j0ignsdxn0zxjcbmrpz2l0cya9igrhdgfzzxrzl
   mxvywrfzglnaxrzkclcbmrhdgegpsbzy2fszshkawdpdhmuzgf0yslcbmzyb20gc2tszwfy
   bi5jcm9zc192ywxpzgf0aw9uigltcg9ydcb0cmfpbl90zxn0x3nwbgl0xg5yx3ryywlulcb
   yx3rlc3qsihlfdhjhaw4sihlfdgvzdcwgaw1hz2vzx3ryywlulcbpbwfnzxnfdgvzdca9ih
   ryywlux3rlc3rfc3bsaxqozglnaxrzlmrhdgesigrpz2l0cy50yxjnzxqsigrpz2l0cy5pb
   wfnzxmsihrlc3rfc2l6zt0wlji1lcbyyw5kb21fc3rhdgu9ndipxg5mcm9tihnrbgvhcm4g
   aw1wb3j0ihn2bvxuc3zjx21vzgvsid0gc3ztllnwqyhnyw1tyt0wljawmswgqz0xmdaulcb
   rzxjuzww9j2xpbmvhcicpxg5zdmnfbw9kzwwuzml0kfhfdhjhaw4sihlfdhjhaw4pxg5wcm
   vkawn0zwqgpsbzdmnfbw9kzwwuchjlzgljdchyx3rlc3qpiiwic2ftcgxlijoiiybjbxbvc
   nqgyg1ldhjpy3ngxg5mcm9tihnrbgvhcm4gaw1wb3j0ig1ldhjpy3ncblxuiybqcmludcb0
   agugy2xhc3npzmljyxrpb24gcmvwb3j0ig9migb5x3rlc3rgigfuzcbgchjlzgljdgvkyfx
   uchjpbnqobwv0cmljcy5jbgfzc2lmawnhdglvbl9yzxbvcnqox19fx19flcbfx19fx19fx1
   8pkvxuxg4jifbyaw50ihrozsbjb25mdxnpb24gbwf0cml4ig9migb5x3rlc3rgigfuzcbgc
   hjlzgljdgvkyfxuchjpbnqobwv0cmljcy5jb25mdxnpb25fbwf0cml4kf9fx19fxywgx19f
   x19fx19fkskilcjzb2x1dglvbii6iimgsw1wb3j0igbtzxryawnzyfxuznjvbsbza2xlyxj
   uigltcg9ydcbtzxryawnzxg5cbimguhjpbnqgdghlignsyxnzawzpy2f0aw9uihjlcg9ydc
   bvzibgev90zxn0ycbhbmqgyhbyzwrpy3rlzgbcbnbyaw50kg1ldhjpy3muy2xhc3npzmljy
   xrpb25fcmvwb3j0khlfdgvzdcwgchjlzgljdgvkkslcblxuiybqcmludcb0agugy29uznvz
   aw9uig1hdhjpefxuchjpbnqobwv0cmljcy5jb25mdxnpb25fbwf0cml4khlfdgvzdcwgchj
   lzgljdgvkkskilcjzy3qioij0zxn0x2ltcg9ydchcinnrbgvhcm4ubwv0cmljc1wilcbzyw
   1lx2fzid0gvhj1zswgbm90x2ltcg9ydgvkx21zzya9ifwirglkihlvdsbpbxbvcnqgyg1ld
   hjpy3ngigzyb20gyhnrbgvhcm5gp1wilcbpbmnvcnjly3rfyxnfbxnnid0gxcjeb24ndcbm
   b3jnzxqgdg8gaw1wb3j0igbtzxryawnzycbmcm9tigbza2xlyxjuycfciilcbm5vdf9jywx
   szwrfbxnnpvwirglkihlvdsbmawxsigluigb5x3rlc3rgigfuzcbgchjlzgljdgvkyd9cil
   xuaw5jb3jyzwn0x21zzz1cikrvbid0igzvcmdldcb0bybmawxsigluigb5x3rlc3rgigfzi
   hrozsbmaxjzdcbhcmd1bwvudcwgyhbyzwrpy3rlzgagyxmgdghlihnly29uzcbhcmd1bwvu
   dcfcilxudgvzdf9mdw5jdglvbihcinbyaw50xcisidesigrvx2v2yww9rmfsc2usig5vdf9
   jywxszwrfbxnnid0gbm90x2nhbgxlzf9tc2csigluy29ycmvjdf9tc2cgpsbpbmnvcnjly3
   rfbxnnkvxudgvzdf9mdw5jdglvbihcinbyaw50xcisidisigrvx2v2yww9rmfsc2usig5vd
   f9jywxszwrfbxnnid0gbm90x2nhbgxlzf9tc2csigluy29ycmvjdf9tc2cgpsbpbmnvcnjl
   y3rfbxnnkvxuc3vjy2vzc19tc2c9xcjxzwxsigrvbmuhie5vdywgy2hly2sgdghlihjlc3v
   sdhmgb2ygdghlignvbmz1c2lvbibtyxryaxguiervzxmgdghpcybtb2rlbcbwzxjmb3jtig
   jldhrlcj9ciij9

   you clearly see that this model performs a whole lot better than the
   id91 model that you used earlier.

   you can also see it when you visualize the predicted and the actual
   labels with the help of isomap():
# import `isomap()`
from sklearn.manifold import isomap

# create an isomap and fit the `digits` data to it
x_iso = isomap(n_neighbors=10).fit_transform(x_train)

# compute cluster centers and predict cluster index for each sample
predicted = svc_model.predict(x_train)

# create a plot with subplots in a grid of 1x2
fig, ax = plt.subplots(1, 2, figsize=(8, 4))

# adjust the layout
fig.subplots_adjust(top=0.85)

# add scatterplots to the subplots
ax[0].scatter(x_iso[:, 0], x_iso[:, 1], c=predicted)
ax[0].set_title('predicted labels')
ax[1].scatter(x_iso[:, 0], x_iso[:, 1], c=y_train)
ax[1].set_title('actual labels')


# add title
fig.suptitle('predicted versus actual labels', fontsize=14, fontweight='bold')

# show the plot
plt.show()

   this will give you the following scatterplots:

   isomap scatterplot visualization

   you   ll see that this visualization confirms your classification report,
   which is excellent news. :)

what's next?

digit recognition in natural images

   congratulations, you have reached the end of this scikit-learn
   tutorial, which was meant to introduce you to python machine learning!
   now it's your turn.

   firstly, make sure you get a hold of datacamp's [24]scikit-learn cheat
   sheet.

   next, start your own digit recognition project with different data. one
   dataset that you can already use is the mnist data, which you can
   download [25]here.

   the steps that you can take are very similar to the ones that you have
   gone through with this tutorial, but if you still feel that you can use
   some help, you should check out [26]this page, which works with the
   mnist data and applies the kmeans algorithm.

   working with the digits dataset was the first step in classifying
   characters with scikit-learn. if you   re done with this, you might
   consider trying out an even more challenging problem, namely,
   classifying alphanumeric characters in natural images.

   a well-known dataset that you can use for this problem is the chars74k
   dataset, which contains more than 74,000 images of digits from 0 to 9
   and both lowercase and higher case letters of the english alphabet. you
   can download the dataset [27]here.

data visualization and pandas

   whether you're going to start with the projects that have been
   mentioned above or not, this is definitely not the end of your journey
   of data science with python. if you choose not to widen your view just
   yet, consider deepening your data visualization and data manipulation
   knowledge.

   don't miss out on our [28]interactive data visualization with bokeh
   course to make sure you can impress your peers with a stunning data
   science portfolio or our [29]pandas foundation course, to learn more
   about working with data frames in python.
   129
   129
   [30]0
   related posts
   must read
   python
   +4

[31]keras tutorial: deep learning in python

   karlijn willems
   february 4th, 2019
   python
   +3

[32]scikit-learn tutorial: baseball analytics pt 1

   daniel poston
   may 4th, 2017
   must read
   machine learning
   +4

[33]detecting fake news with scikit-learn

   katharine jarmul
   august 24th, 2017
   (button)
   post a comment

   [34]subscribe to rss
   [35]about[36]terms[37]privacy

   want to leave a comment?

references

   visible links
   1. https://www.datacamp.com/users/sign_in
   2. https://www.datacamp.com/community/tutorials/machine-learning-python#comments
   3. https://www.datacamp.com/community/tutorials/machine-learning-python#explore
   4. https://www.datacamp.com/community/tutorials/machine-learning-python#preprocess
   5. https://www.datacamp.com/community/tutorials/machine-learning-python#kmeans
   6. https://www.datacamp.com/community/tutorials/machine-learning-python#id166
   7. https://www.datacamp.com/community/tutorials/machine-learning-in-r/
   8. https://www.datacamp.com/courses/supervised-learning-with-scikit-learn/
   9. https://www.datacamp.com/courses/unsupervised-learning-in-python/
  10. http://archive.ics.uci.edu/ml/datasets
  11. https://www.datacamp.com/community/tutorials/www.kaggle.com
  12. http://www.kdnuggets.com/datasets/index.html
  13. https://scikits.appspot.com/scikits
  14. http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/
  15. https://www.datacamp.com/courses/importing-data-in-python-part-1/
  16. https://www.datacamp.com/courses/
  17. http://matplotlib.org/
  18. http://matplotlib.org/examples/images_contours_and_fields/interpolation_methods.html
  19. http://www.lauradhamilton.com/introduction-to-principal-component-analysis-pca
  20. https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook/
  21. http://scikit-learn.org/stable/tutorial/machine_learning_map/
  22. https://www.datacamp.com/courses/
  23. http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html
  24. https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet/
  25. http://yann.lecun.com/exdb/mnist/
  26. http://johnloeber.com/docs/kmeans.html
  27. http://www.ee.surrey.ac.uk/cvssp/demos/chars74k/
  28. https://www.datacamp.com/courses/interactive-data-visualization-with-bokeh/
  29. https://www.datacamp.com/courses/pandas-foundations/
  30. https://www.datacamp.com/community/tutorials/machine-learning-python#comments
  31. https://www.datacamp.com/community/tutorials/deep-learning-python
  32. https://www.datacamp.com/community/tutorials/scikit-learn-tutorial-baseball-1
  33. https://www.datacamp.com/community/tutorials/scikit-learn-fake-news
  34. https://www.datacamp.com/community/rss.xml
  35. https://www.datacamp.com/about
  36. https://www.datacamp.com/terms-of-use
  37. https://www.datacamp.com/privacy-policy

   hidden links:
  39. https://www.datacamp.com/
  40. https://www.datacamp.com/community
  41. https://www.datacamp.com/community/tutorials
  42. https://www.datacamp.com/community/data-science-cheatsheets
  43. https://www.datacamp.com/community/open-courses
  44. https://www.datacamp.com/community/podcast
  45. https://www.datacamp.com/community/chat
  46. https://www.datacamp.com/community/blog
  47. https://www.datacamp.com/community/tech
  48. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/machine-learning-python
  49. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/machine-learning-python
  50. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/machine-learning-python
  51. https://www.datacamp.com/profile/karlijn
  52. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/machine-learning-python
  53. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/machine-learning-python
  54. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/machine-learning-python
  55. https://www.datacamp.com/profile/karlijn
  56. https://www.datacamp.com/profile/danielc4f06ad606fd4d6eb9cb444ef4d09494
  57. https://www.datacamp.com/profile/katharinecc6b90c27e1b40129c2745c9215cc689
  58. https://www.facebook.com/pages/datacamp/726282547396228
  59. https://twitter.com/datacamp
  60. https://www.linkedin.com/company/datamind-org
  61. https://www.youtube.com/channel/uc79gv3myp6zkiswyemeik9a
