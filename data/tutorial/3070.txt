   #[1]hello paperspace

   [2]hello paperspace
     * [3]    get paid to write
     * [4]machine learning
     * [5]tutorials
     * [6]     sign up

   20 march 2017 / [7]gpu

adversarial autoencoders (with pytorch)

   adversarial autoencoders (with pytorch)

     "most of human and animal learning is unsupervised learning. if
     intelligence was a cake, unsupervised learning would be the cake
     [base], supervised learning would be the icing on the cake, and
     id23 would be the cherry on the cake. we know how
     to make the icing and the cherry, but we don't know how to make the
     cake."

   director of ai research at facebook, professor yann lecunn repeatedly
   mentions this analogy at his talks. by unsupervised learning, he refers
   to the "ability of a machine to model the environment, predict possible
   futures and understand how the world works by observing it and acting
   in it."

   deep generative models are one of the techniques that attempt to solve
   the problem of unsupervised learning in machine learning. in this
   framework, a machine learning system is required to discover hidden
   structure within unlabelled data. deep generative models have many
   widespread applications, density estimation, image/audio denoising,
   compression, scene understanding, representation learning and
   semi-supervised classification amongst many others.

   id5 (vaes) allow us to formalize this problem in
   the framework of probabilistic id114 where we are maximizing
   a lower bound on the log likelihood of the data. in this post we will
   look at a recently developed architecture, adversarial autoencoders,
   which are inspired in vaes, but give us more flexibility in how we map
   our data into a latent dimension (if this is not clear as of now, don't
   worry, we will revisit this idea along the post). one of the most
   interesting ideas about adversarial autoencoders is how to impose a
   prior distribution to the output of a neural network by using
   adversarial learning.

   if you want to get your hands into the pytorch code, feel free to visit
   the [8]github repo. along the post we will cover some [9]background on
   denoising autoencoders and id5 first to then jump
   to [10]adversarial autoencoders, a pytorch [11]implementation, the
   [12]training procedure followed and some [13]experiments regarding
   disentanglement and semi-supervised learning using the mnist dataset.

background

denoising autoencoders (dae)

   the simplest version of an autoencoder is one in which we train a
   network to reconstruct its input. in other words, we would like the
   network to somehow learn the identity function $f(x) = x$. for this
   problem not to be trivial, we impose the condition to the network to go
   through an intermediate layer (latent space) whose dimensionality is
   much lower than the dimensionality of the input. with this bottleneck
   condition, the network has to compress the input information. the
   network is therefore divided in two pieces, the encoder receives the
   input and creates a latent or hidden representation of it, and the
   decoder takes this intermediate representation and tries to reconstruct
   the input. the loss of an autoencoder is called reconstruction loss,
   and can be defined simply as the squared error between the input and
   generated samples:

   $$l_r (x, x') = ||x - x'||^2$$

   another widely used reconstruction loss for the case when the input is
   normalized to be in the range $[0,1]^n$ is the cross-id178 loss.

id5 (vae)

   id5 impose a second constraint on how to construct
   the hidden representation. now the latent code has a prior distribution
   defined by design $p(x)$. in other words, the encoder can not use the
   entire latent space freely but has to restrict the hidden codes
   produced to be likely under this prior distribution $p(x)$. for
   isntance, if the prior distribution on the latent code is a gaussian
   distribution with mean 0 and standard deviation 1, then generating a
   latent code with value 1000 should be really unlikely.

   this can be seen as a second type of id173 on the amount of
   information that can be stored in the latent code. the benefit of this
   relies on the fact that now we can use the system as a generative
   model. to create a new sample that comes from the data distribution
   $p(x)$, we just have to sample from $p(z)$ and run this sample through
   the decoder to reconstruct a new image. if this condition is not
   imposed, then the latent code can be distributed among the latent space
   freely and therefore is not possible to sample a valid latent code to
   produce an output in a straightforward manner.

   in order to enforce this property a second term is added to the loss
   function in the form of a kullback-liebler (kl) divergence between the
   distribution created by the encoder and the prior distribution. since
   vae is based in a probabilistic interpretation, the reconstruction loss
   used is the cross-id178 loss mentioned earlier. putting this together
   we have,

   $$l(x, x') = l_r (x, x') + kl(q(z|x)||p(z))$$
   or
   $$l(x, x') = - \sum _{k=1} ^{n} x _k \ log(x' _k) + (1- x _k) \ log(1 -
   x' _k) + kl(q(z|x)|| p(z))$$

   where $q(z|x)$ is the encoder of our network and $p(z)$ is the prior
   distribution imposed on the latent code. now this architecture can be
   jointly trained using id26.

adversarial autoencoders (aae)

aae as generative model

   one of the main drawbacks of id5 is that the
   integral of the kl divergence term does not have a closed form
   analytical solution except for a handful of distributions. furthermore,
   it is not straightforward to use discrete distributions for the latent
   code $z$. this is because id26 through discrete variables is
   generally not possible, making the model difficult to train
   efficiently. one approach to do this in the vae setting was introduced
   [14]here.

   adversarial autoencoders avoid using the kl divergence altogether by
   using adversarial learning. in this architecture, a new network is
   trained to discriminatively predict whether a sample comes from the
   hidden code of the autoencoder or from the prior distribution $p(z)$
   determined by the user. the loss of the encoder is now composed by the
   reconstruction loss plus the loss given by the discriminator network.

   the image shows schematically how aaes work when we use a gaussian
   prior for the latent code (although the approach is generic and can use
   any distribution). the top row is equivalent to an autoencoder. first a
   sample $z$ is drawn according to the generator network $q(z|x)$, that
   sample is then sent to the decoder which generates $x'$ from $z$. the
   reconstruction loss is computed between $x$ and $x'$ and the gradient
   is backpropagated through $p$ and $q$ accordingly and its weights are
   updated.

                                   aae001

   figure 1. basic architecture of an aae. top row is an autoencoder while
   the bottom row is an adversarial network which forces the output to the
   encoder to follow the distribution $p(z)$.

   on the adversarial id173 part the discriminator recieves $z$
   distributed as $q(z|x)$ and $z'$ sampled from the true prior $p(z)$ and
   assigns a id203 to each of coming from $p(z)$. the loss incurred
   is backpropagated through the discriminator to update its weights. then
   the process is repeated and the generator updates its parameters.

   we can now use the loss incurred by the generator of the adversarial
   network (which is the encoder of the autoencoder) instead of a kl
   divergence for it to learn how to produce samples according to the
   distribution $p(z)$. this modification allows us to use a broader set
   of distributions as priors for the latent code.

   the loss of the discriminator is

   $$l_d = - \frac{1}{m} \sum _{k=1} ^m \ log(d(z')) + \ log(1 - d(z))$$
   ([15]plot)

   where $m$ is the minibatch size, $z$ is generated by the encoder and
   $z'$ is a sample from the true prior.

   for the adversarial generator we have

   $$l_g = - \frac{1}{m} \sum _{k=1} ^m log(d(z))$$ ([16]plot)

   by looking at the equations and the plots you should convince yourself
   that the loss defined this way will enforce the discriminator to be
   able to recognize fake samples while will push the generator to fool
   the discriminator.

network definition

   before getting into the training procedure used for this model, we look
   at how to implement what we have up to now in pytorch. for the encoder,
   decoder and discriminator networks we will use simple feed forward
   neural networks with three 1000 hidden state layers with relu nonlinear
   functions and dropout with id203 0.2.
#encoder
class q_net(nn.module):
    def __init__(self):
        super(q_net, self).__init__()
        self.lin1 = nn.linear(x_dim, n)
        self.lin2 = nn.linear(n, n)
        self.lin3gauss = nn.linear(n, z_dim)
    def forward(self, x):
        x = f.droppout(self.lin1(x), p=0.25, training=self.training)
        x = f.relu(x)
        x = f.droppout(self.lin2(x), p=0.25, training=self.training)
        x = f.relu(x)
        xgauss = self.lin3gauss(x)
        return xgauss

# decoder
class p_net(nn.module):
    def __init__(self):
        super(p_net, self).__init__()
        self.lin1 = nn.linear(z_dim, n)
        self.lin2 = nn.linear(n, n)
        self.lin3 = nn.linear(n, x_dim)
    def forward(self, x):
        x = self.lin1(x)
        x = f.dropout(x, p=0.25, training=self.training)
        x = f.relu(x)
        x = self.lin2(x)
        x = f.dropout(x, p=0.25, training=self.training)
        x = self.lin3(x)
        return f.sigmoid(x)

# discriminator
class d_net_gauss(nn.module):
    def __init__(self):
        super(d_net_gauss, self).__init__()
        self.lin1 = nn.linear(z_dim, n)
        self.lin2 = nn.linear(n, n)
        self.lin3 = nn.linear(n, 1)
    def forward(self, x):
        x = f.dropout(self.lin1(x), p=0.2, training=self.training)
        x = f.relu(x)
        x = f.dropout(self.lin2(x), p=0.2, training=self.training)
        x = f.relu(x)
        return f.sigmoid(self.lin3(x))

   some things to note from this definitions. first, since the output of
   the encoder has to follow a gaussian distribution, we do not use any
   nonlinearities at its last layer. the output of the decoder has a
   sigmoid nonlinearity, this is because we are using the inputs
   normalized in a way in which their values are within 0 and 1. the
   output of the discriminator network is just one number between 0 and 1
   representing the id203 of the input coming from the true prior
   distribution.

   once the networks classes are defined, we create an instance of each
   one and define the optimizers to be used. in order to have independence
   in the optimization procedure for the encoder (which is as well the
   generator of the adversarial network) we define two optimizers for this
   part of the network as follows:
torch.manual_seed(10)
q, p = q_net() = q_net(), p_net(0)     # encoder/decoder
d_gauss = d_net_gauss()                # discriminator adversarial
if torch.cuda.is_available():
    q = q.cuda()
    p = p.cuda()
    d_cat = d_gauss.cuda()
    d_gauss = d_net_gauss().cuda()
# set learning rates
gen_lr, reg_lr = 0.0006, 0.0008
# set optimizators
p_decoder = optim.adam(p.parameters(), lr=gen_lr)
q_encoder = optim.adam(q.parameters(), lr=gen_lr)
q_generator = optim.adam(q.parameters(), lr=reg_lr)
d_gauss_solver = optim.adam(d_gauss.parameters(), lr=reg_lr)

training procedure

   the training procedure for this architecture for each minibatch is
   performed as follows:

   1) do a forward path through the encoder/decoder part, compute the
   reconstruction loss and update the parameteres of the encoder q and
   decoder p networks.
    z_sample = q(x)
    x_sample = p(z_sample)
    recon_loss = f.binary_cross_id178(x_sample + tiny,
                                        x.resize(train_batch_size, x_dim) + tiny
)
    recon_loss.backward()
    p_decoder.step()
    q_encoder.step()

   2) create a latent representation z = q(x) and take a sample z    from
   the prior p(z), run each one through the discriminator and compute the
   score assigned to each (d(z) and d(z   )).
    q.eval()
    z_real_gauss = variable(torch.randn(train_batch_size, z_dim) * 5)   # sample
 from n(0,5)
    if torch.cuda.is_available():
        z_real_gauss = z_real_gauss.cuda()
    z_fake_gauss = q(x)

   3) compute the loss in the discriminator as and backpropagate it
   through the discriminator network to update its weights. in code,
    # compute discriminator outputs and loss
    d_real_gauss, d_fake_gauss = d_gauss(z_real_gauss), d_gauss(z_fake_gauss)
    d_loss_gauss = -torch.mean(torch.log(d_real_gauss + tiny) + torch.log(1 - d_
fake_gauss + tiny))
    d_loss.backward()       # backpropagate loss
    d_gauss_solver.step()   # apply optimization step

   4) compute the loss of the generator network and update q network
   accordingly.
# generator
q.train()   # back to use dropout
z_fake_gauss = q(x)
d_fake_gauss = d_gauss(z_fake_gauss)

g_loss = -torch.mean(torch.log(d_fake_gauss + tiny))
g_loss.backward()
q_generator.step()

generating images

   now we attempt to visualize at how the aae encodes images into a 2-d
   gaussian latent representation with standard deviation 5. for this we
   first train the model with a 2-d hidden state. then we generate uniform
   points on this latent space from (-10,-10) (upper left corner) to
   (10,10) (bottom right corner) and run them to through the decoder
   network.

                                    rec2d

   latent space. image reconstructions while exploring the 2d latent space
   uniformly from -10 to 10 in both the $x$ and $y$ axes.

aae to learn disentangled representations

   an ideal intermediate representation of the data would be one that
   captures the underlying causal factors of variation that generated the
   observed data. yoshua bengio and his colleagues note in [17]this paper
   that "we would like our representations to disentangle the factors of
   variation. different explanatory factors of the data tend to change
   independently of each other in the input distribution". they also
   mention "that the most robust approach to id171 is to
   disentangle as many factors as possible, discarding as little
   information about the data as is practical".

   in the case of [18]mnist data (which is a large dataset of handwritten
   digits), we can define two underlying causal factors, on the one hand,
   we have the number being generated, and on the other one the style or
   way of writing it.

supervised approach

   in this part, we go one step further than the previous architecture and
   try to impose certain structure in the latent code $z$. particularly we
   want the architecture to be able to separate the class information from
   the trace style in a fully supervised scenario. to do so, we extend the
   previous architecture to the one in the figure below. we split the
   latent dimension in two parts: the first one $z$ is analogous as the
   one we had in the previous example; the second part of the hidden code
   is now a [19]one-hot vector $y$ indicating the identity of the number
   being fed to the autoencoder.

                                  aae_semi

   supervised adversarial autoencoder architecture.

   in this setting, the decoder uses the one-hot vector $y$ and the hidden
   code $z$ to reconstruct the original image. the encoder is left with
   the task of encoding the style information in $z$. in the image below
   we can see the result of training this architecture with 10,000 labeled
   mnist samples. the figure shows the reconstructed images in which, for
   each row, the hidden code $z$ is fixed to a particular value and the
   class label $y$ ranges from 0 to 9. effectively the style is maintained
   across the columns.

                              disentanglement1

   image reconstruction by exploring the latent code $y$ while keeping $z$
   fixed from left to right.

   ##### semi-supervised approach as our last experiment we look an
   alternative to obtain similar disentanglement results for the case in
   which we have only few samples of labeled information. we can modify
   the previous architecture so that the aae produces a latent code
   composed by the concatenation of a vector $y$ indicating the class or
   label (using a softmax) and a continuous latent variable $z$ (using a
   linear layer). since we want the vector $y$ to behave as a one-hot
   vector, we impose it to follow a categorical distribution by using a
   second adversarial network with discriminator $d_{cat}$. the encoder
   now is $q(z, y|x)$. the decoder uses both the class label and the
   continuous hidden code to reconstruct the image.

                                   aae002

   semi-supervised adeversarial autoencoder architecture.

   the unlabeled data contributes to the training procedure by improving
   the way the encoder creates the hidden code based on the reconstruction
   loss and improving the generator and discriminator networks for which
   no labeled information is needed.

                                    disen

   disentanglement with semi supervised approach.

   it is worth noticing that now, not only we can generate images with
   fewer labeled information, but also we can classify the images for
   which we do not have labels by looking at the latent code $y$ and
   picking the one with the highest value. with the current setting, the
   classification loss is about 3% using 100 labeled samples and 47,000
   unlabeled ones.

on gpu training

   lastly, we do a short comparison in training time for this last
   algorithm in two different gpus and cpu in paperspace platform. even
   though this architecture is not highly complicated and it is composed
   by few linear layers, the improvement in training time is enormous when
   making use of gpu acceleration. 500 epochs training time goes down from
   almost 4 hours in cpu to around 9 minutes using the nvidia quadro m4000
   and further down to 6 minutes in the nvidia quadro p5000.

                                     tt

   training time with and without gpu acceleration

more material

     * [20]what is a variational autoencoder (tutorial)
     * [21]auto-encoding id58 (original paper)
     * [22]adversarial autoencoders (original paper)
     * [23]building machines that imagine and reason: principles and
       applications of deep generative models (video lecture)

   to get started with your own ml-in-a-box setup, [24]sign up here.

[25]felipe

   read [26]more posts by this author.
   [27]read more

       hello paperspace    

[28]gpu

     * [29]reproducible machine learning with pytorch and quilt
     * [30]build an ai to play dino run
     * [31]vectorization and broadcasting with pytorch

   [32]see all 15 posts    

   [33]gpu rendering with octane    in the cloud

   cloud

gpu rendering with octane    in the cloud

   gpu rendering is becoming an industry standard in the fields of visual
   effects, motion graphics, and advertising. by utilizing the high-end
   graphics cards of today, computers can see a performance increase of
   several

     * alexander

   10 min read

   r

a gentle introduction to data classification with r

   spam or ham? most of the data we generate is unstructured. this
   includes sources like text, audio, video, and images which an algorithm
   might not immediately comprehend. yet, imagine if a human had

     * caleb

   10 min read

   [34]hello paperspace icon hello paperspace
      
   adversarial autoencoders (with pytorch)
   share this

   [35]hello paperspace    2019

   [36]latest posts [37]facebook [38]twitter [39]ghost

   try paperspace
   [40](button) sign up

references

   visible links
   1. https://blog.paperspace.com/rss/
   2. https://blog.paperspace.com/
   3. https://blog.paperspace.com/write-for-paperspace/
   4. https://blog.paperspace.com/tag/machine-learning/
   5. https://blog.paperspace.com/tag/tutorial/
   6. https://www.paperspace.com/account/signup
   7. https://blog.paperspace.com/tag/gpu/
   8. https://github.com/fducau/aae_pytorch
   9. https://blog.paperspace.com/p/0862093d-f77a-42f4-8dc5-0b790d74fb38/#background
  10. https://blog.paperspace.com/p/0862093d-f77a-42f4-8dc5-0b790d74fb38/#aae
  11. https://blog.paperspace.com/p/0862093d-f77a-42f4-8dc5-0b790d74fb38/#implementation
  12. https://blog.paperspace.com/p/0862093d-f77a-42f4-8dc5-0b790d74fb38/#training
  13. https://blog.paperspace.com/p/0862093d-f77a-42f4-8dc5-0b790d74fb38/#generation
  14. https://arxiv.org/abs/1609.02200
  15. https://raw.githubusercontent.com/fducau/aae_pytorch/master/img/dloss.png
  16. https://raw.githubusercontent.com/fducau/aae_pytorch/master/img/gloss.png
  17. http://www.cl.uni-heidelberg.de/courses/ws14/deepl/bengioetal12.pdf
  18. http://yann.lecun.com/exdb/mnist/
  19. https://www.quora.com/what-is-one-hot-encoding-and-when-is-it-used-in-data-science
  20. https://jaan.io/what-is-variational-autoencoder-vae-tutorial
  21. https://arxiv.org/abs/1312.6114
  22. https://arxiv.org/abs/1511.05644
  23. http://videolectures.net/deeplearning2016_mohamed_generative_models/
  24. https://www.paperspace.com/account/signup?utm-campaign=pytorchblog
  25. https://blog.paperspace.com/author/felipe-2/
  26. https://blog.paperspace.com/author/felipe-2/
  27. https://blog.paperspace.com/author/felipe-2/
  28. https://blog.paperspace.com/tag/gpu/
  29. https://blog.paperspace.com/reproducible-data-with-pytorch-and-quilt/
  30. https://blog.paperspace.com/dino-run/
  31. https://blog.paperspace.com/pytorch-vectorization-and-broadcasting/
  32. https://blog.paperspace.com/tag/gpu/
  33. https://blog.paperspace.com/gpu-rendering-with-octane-in-the-cloud/
  34. https://blog.paperspace.com/
  35. https://blog.paperspace.com/
  36. https://blog.paperspace.com/
  37. https://www.facebook.com/hellopaperspace/
  38. https://twitter.com/hellopaperspace
  39. https://ghost.org/
  40. https://www.paperspace.com/account/signup

   hidden links:
  42. https://www.facebook.com/hellopaperspace/
  43. https://twitter.com/hellopaperspace
  44. https://feedly.com/i/subscription/feed/https://blog.paperspace.com/rss/
  45. https://blog.paperspace.com/gpu-rendering-with-octane-in-the-cloud/
  46. https://blog.paperspace.com/author/alexander/
  47. https://blog.paperspace.com/intro-to-datascience/
  48. https://blog.paperspace.com/author/caleb/
  49. https://twitter.com/share?text=adversarial%20autoencoders%20(with%20pytorch)&url=https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/
  50. https://www.facebook.com/sharer/sharer.php?u=https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/
