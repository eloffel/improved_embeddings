   #[1]adventures in machine learning    keras tutorial     build a
   convolutional neural network in 11 lines comments feed [2]alternate
   [3]alternate

   menu

     * [4]home
     * [5]about
     * [6]coding the deep learning revolution ebook
     * [7]contact
     * [8]ebook / newsletter sign-up

   search: ____________________

keras tutorial     build a convolutional neural network in 11 lines

   by [9]admin | [10]convolutional neural networks

     * you are here:
     * [11]home
     * [12]deep learning
     * [13]keras tutorial     build a convolutional neural network in 11
       lines

   may 17
   [14]5
   keras tutorial - keras logo

   in [15]a previous tutorial, i demonstrated how to create a
   convolutional neural network (id98) using tensorflow to classify the
   mnist handwritten digit dataset.  tensorflow is a brilliant tool, with
   lots of power and flexibility.  however, for quick prototyping work it
   can be a bit verbose.  enter [16]keras and this keras tutorial.  keras
   is a higher level library which operates over either tensorflow or
   [17]theano, and is intended to stream-line the process of building deep
   learning networks.  in fact, what was accomplished in the [18]previous
   tutorial in tensorflow in around 42 lines* can be replicated in only 11
   lines* in keras.  this keras tutorial will show you how to do this.

   *excluding input data preparation and visualisation

   this keras tutorial will show you how to build a id98 to achieve >99%
   accuracy with the mnist dataset.  it will be precisely the same
   structure as that built in [19]my previous convolutional neural network
   tutorial and the figure below shows the architecture of the network:
   keras tutorial - network

   convolutional neural network that will be built

   the full code of this keras tutorial can be found [20]here. if you   d
   like to check out more keras awesomeness after reading this post, have
   a look at my [21]keras lstm tutorial or my [22]keras reinforcement
   learning tutorial. also check out my tutorial on [23]convolutional
   neural networks in pytorch if you   re interested in the pytorch library.
     __________________________________________________________________

eager to learn more? get the book [24]here
     __________________________________________________________________


the main code in this keras tutorial

   the code below is the    guts    of the id98 structure that will be used in
   this keras tutorial:
model = sequential()
model.add(conv2d(32, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu',
                 input_shape=input_shape))
model.add(maxpooling2d(pool_size=(2, 2), strides=(2, 2)))
model.add(conv2d(64, (5, 5), activation='relu'))
model.add(maxpooling2d(pool_size=(2, 2)))
model.add(flatten())
model.add(dense(1000, activation='relu'))
model.add(dense(num_classes, activation='softmax'))

   i   ll go through most of the lines in turn, explaining as we go.
model = sequential()

   models in keras can come in two forms     sequential and via the
   functional api.  for most deep learning networks that you build, the
   sequential model is likely what you will use.  it allows you to easily
   stack sequential layers (and even recurrent layers) of the network in
   order from input to output.  the functional api allows you to build
   more complicated architectures, and it won   t be covered in this
   tutorial.

   the first line declares the model type as sequential().
model.add(conv2d(32, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu',
                 input_shape=input_shape))

   next, we add a 2d convolutional layer to process the 2d mnist input
   images.  the first argument passed to the [25]conv2d() layer function
   is the number of output channels     in this case we have 32 output
   channels (as per the architecture shown at the beginning).  the next
   input is the kernel_size, which in this case we have chosen to be a 5  5
   moving window, followed by the strides in the x and y directions (1,
   1).  next, the activation function is a rectified linear unit and
   finally we have to supply the model with the size of the input to the
   layer (which is declared in another part of the code     see [26]here).
   declaring the input shape is only required of the first layer     keras
   is good enough to work out the size of the tensors flowing through the
   model from there.

   also notice that we don   t have to declare any weights or bias variables
   like we do in [27]tensorflow, keras sorts that out for us.
model.add(maxpooling2d(pool_size=(2, 2), strides=(2, 2)))

   next we add a 2d max pooling layer.  the definition of the layer is
   dead easy.  we simply specify the size of the pooling in the x and y
   directions     (2, 2) in this case, and the strides.  that   s it.
model.add(conv2d(64, (5, 5), activation='relu'))
model.add(maxpooling2d(pool_size=(2, 2)))

   next we add another convolutional + max pooling layer, with 64 output
   channels.  the default strides argument in the conv2d() function is (1,
   1) in keras, so we can leave it out.  the default strides argument in
   keras is to make it equal ot the pool size, so again, we can leave it
   out.

   the input tensor for this layer is ([28]batch_size, 28, 28,  32)     the
   28 x 28 is the size of the image, and the 32 is the number of output
   channels from the previous layer.  however, notice we don   t have to
   explicitly detail what the shape of the input is     keras will work it
   out for us.  this allows rapid assembling of network architectures
   without having to worry too much about the sizes of the tensors flowing
   around our networks.
model.add(flatten())
model.add(dense(1000, activation='relu'))
model.add(dense(num_classes, activation='softmax'))

   now that we   ve built our convolutional layers in this keras tutorial,
   we want to flatten the output from these to enter our fully connected
   layers (all this is detailed in [29]the convolutional neural network
   tutorial in tensorflow).  in tensorflow, we had to figure out what the
   size of our output tensor from the convolutional layers was in order to
   flatten it, and also to determine explicitly the size of our weight and
   bias variables.  sure, this isn   t too difficult     but it just makes our
   life easier not to have to think about it too much.

   the next two lines declare our fully connected layers     using the
   dense() layer in keras.  again, it is very simple.  first we specify
   the size     in line with our architecture, we specify 1000 nodes, each
   activated by a relu function.  the second is our soft-max
   classification, or output layer, which is the size of the number of our
   classes (10 in this case, for our 10 possible hand-written digits).

   that   s it     we have successfully developed the architecture of our id98
   in only 8 lines.  now let   s see what we have to do to train the model
   and perform predictions.

training and evaluating our convolutional neural network

   we have now developed the architecture of the id98 in keras, but we
   haven   t specified the id168, or told the framework what type of
   optimiser to use (i.e. [30]id119, adam optimiser etc.).  in
   keras, this can be performed in one command:
model.compile(loss=keras.losses.categorical_crossid178,
              optimizer=keras.optimizers.sgd(lr=0.01),
              metrics=['accuracy'])

   keras supplies many id168s (or you can build your own) as can
   be seen [31]here.  in this case, we will use the standard cross id178
   for categorical class classification
   (keras.losses.categorical_crossid178).  keras also supplies many
   optimisers     as can be seen [32]here.  in this case, we   ll use the adam
   optimizer (keras.optimizers.adam) as we did in the [33]id98 tensorflow
   tutorial.  finally, we can specify a metric that will be calculated
   when we run evaluate() on the model.  in tensorflow we would have
   to [34]define an accuracy calculating operation which we would need to
   call in order to assess the accuracy.  in this case, keras makes it
   easy for us.  see [35]here for a list of metrics that can be used.

   next, we want to train our model.  this can be done by again running a
   single command in keras:
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test),
          callbacks=[history])

   this command looks similar to the syntax used in the very popular
   [36]scikit learn python machine learning library.  we first pass in all
   of our training data     in this case x_train and y_train.  the next
   argument is the batch size     we don   t have to explicitly handle the
   batching up of our data during training in keras, rather we just
   specify the batch size and it does it for us (i have a post on
   [37]mini-batch id119 if this is unfamiliar to you).  in this
   case we are using a batch size of 128.  next we pass the number of
   training epochs (10 in this case).  the verbose flag, set to 1 here,
   specifies if you want detailed information being printed in the console
   about the progress of the training.  during training, if verbose is set
   to 1, the following is output to the console:
3328/60000 [>.............................] - eta: 87s - loss: 0.2180 - acc: 0.9
336
3456/60000 [>.............................] - eta: 87s - loss: 0.2158 - acc: 0.9
349
3584/60000 [>.............................] - eta: 87s - loss: 0.2145 - acc: 0.9
350
3712/60000 [>.............................] - eta: 86s - loss: 0.2150 - acc: 0.9
348

   finally, we pass the validation or test data to the fit function so
   keras knows what data to test the metric against when evaluate() is run
   on the model.  ignore the callbacks argument for the moment     that will
   be discussed shortly.

   once the model is trained, we can then evaluate it and print the
   results:
score = model.evaluate(x_test, y_test, verbose=0)
print('test loss:', score[0])
print('test accuracy:', score[1])

   after 10 epochs of training the above model, we achieve an accuracy of
   99.2%, which is the same as what [38]we achieved in tensorflow for the
   same network.  you can see the improvement in the accuracy for each
   epoch in the figure below:
   keras tutorial - mnist training accuracy

   keras id98 mnist training accuracy

   keras makes things pretty easy, don   t you think? i hope this keras
   tutorial has demonstrated how it can be a useful framework for rapidly
   prototyping deep learning solutions.

   as a kind of appendix i   ll show you how to keep track of the accuracy
   as we go through the training epochs, which enabled me to generate the
   graph above.

logging metrics in keras

   keras has a useful utility titled    callbacks    which can be utilised to
   track all sorts of variables during training.  you can also use it to
   create checkpoints which saves the model at different stages in
   training to help you avoid work loss in case your poor overworked
   computer decides to crash.  it is passed to the .fit() function as
   observed above.  i   ll only show you a fairly simple use case below,
   which logs the accuracy.

   to create a callback we create an inherited class which inherits from
   keras.callbacks.callback:
class accuracyhistory(keras.callbacks.callback):
    def on_train_begin(self, logs={}):
        self.acc = []

    def on_epoch_end(self, batch, logs={}):
        self.acc.append(logs.get('acc'))

   the callback super class that the code above inherits from has a number
   of methods that can be overridden in our callback definition such
   as on_train_begin, on_epoch_end, on_batch_begin and on_batch_end.  the
   name of these methods are fairly self explanatory, and represent
   moments in the training process where we can    do stuff   .  in the code
   above, at the beginning of training we initialise a list self.acc = []
   to store our accuracy results.  using the on_epoch_end() method, we can
   extract the variable we want from the logs, which is a dictionary that
   holds, as a default, the loss and accuracy during training.  we then
   instantiate this callback like so:
history = accuracyhistory()

   now we can pass history to the .fit() function using the callback
   parameter name.  note that .fit() takes a list for the callback
   parameter, so you have to pass it history like this: [history].  to
   access the accuracy list that we created after the training is
   complete, you can simply call history.acc, which i then also plotted:
plt.plot(range(1,11), history.acc)
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.show()

   hope that helps.  have fun using keras. as i said at the beginning of
   the post, if you   d like to check out more keras awesomeness after
   reading this post, have a look at my [39]keras lstm tutorial.
     __________________________________________________________________

eager to learn more? get the book [40]here
     __________________________________________________________________

about the author

     yara says:
   [41]november 28, 2017 at 7:32 am

   thank you! i am new to nn, your post is of great help to me!

     ehsan taslimi says:
   [42]april 13, 2018 at 8:02 am

   thank you so much for the helpful post.

     gaurav srivastava says:
   [43]april 30, 2018 at 5:17 pm

   thank you. the article was helpful.

     [44]rukshar alam says:
   [45]october 13, 2018 at 7:06 am

   i   ve recently written a blog post reviewing the website. it can act as
   your guideline as a beginner to traverse through the wonderful neural
   network contents of this website. do check it out!!
   [46]https://aprogrammersexperience.blogspot.com/2018/10/beginners-guide
   -some-useful-machine.html
     * andy says:
       [47]october 13, 2018 at 8:12 am
       thanks rukshar!

   ____________________ (button)

   recent posts
     * [48]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [49]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [50]keras, eager and tensorflow 2.0     a new tf paradigm
     * [51]introduction to tensorboard and tensorflow visualization
     * [52]tensorflow eager tutorial

   recent comments
     * andry on [53]neural networks tutorial     a pathway to deep learning
     * sandipan on [54]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [55]neural networks tutorial     a pathway to deep learning
     * martin on [56]neural networks tutorial     a pathway to deep learning
     * uri on [57]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [58]march 2019
     * [59]january 2019
     * [60]october 2018
     * [61]september 2018
     * [62]august 2018
     * [63]july 2018
     * [64]june 2018
     * [65]may 2018
     * [66]april 2018
     * [67]march 2018
     * [68]february 2018
     * [69]november 2017
     * [70]october 2017
     * [71]september 2017
     * [72]august 2017
     * [73]july 2017
     * [74]may 2017
     * [75]april 2017
     * [76]march 2017

   categories
     * [77]amazon aws
     * [78]cntk
     * [79]convolutional neural networks
     * [80]cross id178
     * [81]deep learning
     * [82]gensim
     * [83]gpus
     * [84]keras
     * [85]id168s
     * [86]lstms
     * [87]neural networks
     * [88]nlp
     * [89]optimisation
     * [90]pytorch
     * [91]recurrent neural networks
     * [92]id23
     * [93]tensorboard
     * [94]tensorflow
     * [95]tensorflow 2.0
     * [96]weight initialization
     * [97]id97

   meta
     * [98]log in
     * [99]entries rss
     * [100]comments rss
     * [101]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [102]thrive themes | powered by [103]wordpress

   (button) close dialog

   session expired

   [104]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[105]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/feed/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
   3. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/&format=xml
   4. https://www.adventuresinmachinelearning.com/
   5. https://adventuresinmachinelearning.com/about/
   6. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   7. https://adventuresinmachinelearning.com/contact/
   8. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   9. https://adventuresinmachinelearning.com/author/admin/
  10. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
  11. https://adventuresinmachinelearning.com/
  12. https://adventuresinmachinelearning.com/category/deep-learning/
  13. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  14. http://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/#comments
  15. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  16. https://keras.io/
  17. http://www.deeplearning.net/software/theano/
  18. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  19. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  20. https://github.com/adventuresinml/adventures-in-ml-code
  21. https://adventuresinmachinelearning.com/keras-lstm-tutorial/
  22. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/
  23. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/
  24. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  25. https://keras.io/layers/convolutional/#conv2d
  26. https://github.com/adventuresinml/adventures-in-ml-code
  27. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  28. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  29. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  30. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  31. https://keras.io/losses/
  32. https://keras.io/optimizers/
  33. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  34. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  35. https://keras.io/metrics/
  36. http://scikit-learn.org/
  37. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  38. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  39. https://adventuresinmachinelearning.com/keras-lstm-tutorial/
  40. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  41. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/#comments/4791
  42. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/#comments/4800
  43. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/#comments/4802
  44. https://aprogrammersexperience.blogspot.com/
  45. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/#comments/4819
  46. https://aprogrammersexperience.blogspot.com/2018/10/beginners-guide-some-useful-machine.html
  47. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/#comments/4820
  48. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
  49. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  50. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
  51. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
  52. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  53. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
  54. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  55. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
  56. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
  57. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
  58. https://adventuresinmachinelearning.com/2019/03/
  59. https://adventuresinmachinelearning.com/2019/01/
  60. https://adventuresinmachinelearning.com/2018/10/
  61. https://adventuresinmachinelearning.com/2018/09/
  62. https://adventuresinmachinelearning.com/2018/08/
  63. https://adventuresinmachinelearning.com/2018/07/
  64. https://adventuresinmachinelearning.com/2018/06/
  65. https://adventuresinmachinelearning.com/2018/05/
  66. https://adventuresinmachinelearning.com/2018/04/
  67. https://adventuresinmachinelearning.com/2018/03/
  68. https://adventuresinmachinelearning.com/2018/02/
  69. https://adventuresinmachinelearning.com/2017/11/
  70. https://adventuresinmachinelearning.com/2017/10/
  71. https://adventuresinmachinelearning.com/2017/09/
  72. https://adventuresinmachinelearning.com/2017/08/
  73. https://adventuresinmachinelearning.com/2017/07/
  74. https://adventuresinmachinelearning.com/2017/05/
  75. https://adventuresinmachinelearning.com/2017/04/
  76. https://adventuresinmachinelearning.com/2017/03/
  77. https://adventuresinmachinelearning.com/category/amazon-aws/
  78. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  79. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
  80. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
  81. https://adventuresinmachinelearning.com/category/deep-learning/
  82. https://adventuresinmachinelearning.com/category/nlp/gensim/
  83. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
  84. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  85. https://adventuresinmachinelearning.com/category/loss-functions/
  86. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
  87. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
  88. https://adventuresinmachinelearning.com/category/nlp/
  89. https://adventuresinmachinelearning.com/category/optimisation/
  90. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
  91. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
  92. https://adventuresinmachinelearning.com/category/reinforcement-learning/
  93. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
  94. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
  95. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
  96. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
  97. https://adventuresinmachinelearning.com/category/nlp/id97/
  98. https://adventuresinmachinelearning.com/wp-login.php
  99. https://adventuresinmachinelearning.com/feed/
 100. https://adventuresinmachinelearning.com/comments/feed/
 101. https://wordpress.org/
 102. https://www.thrivethemes.com/
 103. http://www.wordpress.org/
 104. https://adventuresinmachinelearning.com/wp-login.php
 105. http://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/

   hidden links:
 107. https://adventuresinmachinelearning.com/author/admin/
