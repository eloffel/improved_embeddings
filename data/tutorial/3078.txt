   #[1]github [2]recent commits to practical-2:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]16
     * [35]star [36]88
     * [37]fork [38]85

[39]oxford-cs-deepnlp-2017/[40]practical-2

   [41]code [42]issues 1 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   oxford deep nlp 2017 course - practical 2: text classification
   [47]https://www.cs.ox.ac.uk/teaching/cour   
   [48]nlp [49]natural-language-processing [50]deep-learning
   [51]machine-learning [52]oxford
     * [53]4 commits
     * [54]1 branch
     * [55]0 releases
     * [56]fetching contributors

   branch: master (button) new pull request
   [57]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/o
   [58]download zip

downloading...

   want to be notified of new releases in
   oxford-cs-deepnlp-2017/practical-2?
   [59]sign in [60]sign up

launching github desktop...

   if nothing happens, [61]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [62]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [63]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [64]download the github extension for visual studio
   and try again.

   (button) go back
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [65]permalink
   type        name        latest commit message commit time
        failed to load latest commit information.
        [66]readme.md
        [67]practical2.pdf

readme.md

practical 2: text classification

   [chris dyer, yannis assael, brendan shillingford]

   ted stands for    technology, entertainment, and design   . each talk in
   the corpus is labeled with a series of open labels by annotators,
   including the labels    technology   ,    entertainment   , and    design   .
   although some talks are about more than one of these, and about half
   aren   t labeled as being about any of them! in this assignment, you will
   build a text classification model that predicts whether a talk is about
   technology, entertainment, or design--or none of these.

   this is an instance of what is called    multi-label classification   
   (mlc), where each instance may have many different labels. however, we
   will start off by converting it into an instance of multi-class
   classification, where each document receives a single label from a
   finite discrete set of possible labels.

setup and installation

   to answer the following questions you are allowed to use any machine
   learning framework of your taste. the practical demonstrators can
   provide help for:
     * [68]pytorch (example for [69]regression)
     * [70]tensorflow (example for [71]id28)

   other suggested frameworks: [72]cntk, [73]torch, [74]caffe.

multi-class classification

data preparation

   you should reserve the first 1585 documents of the [75]ted talks
   dataset for training, the subsequent 250 for validation, and the final
   250 for testing. each document will be represented as a pairs of (text,
   label).

text of talks

   using the training data, you should determine what vocabulary you want
   for your model. a good rule of thumb is to tokenise and lowercase the
   text (you did this in the intro practical).

   at test time, you will encounter words that were not present in the
   training set (and they will therefore not have an embedding). to deal
   with this, map these words to a special token. you will also want to
   ensure that your training set contains tokens.

labels

   each document should be labeled with label from the set: {too, oeo,
   ood, teo, tod, oed, ted, ooo}. you are called to generate labels from
   the <keywords> tag by checking the existence of one of the following
   tags: {technology, entertainment, design}.
     * none of the keywords     ooo
     *    technology        too
     *    entertainment        oeo
     *    design        ood
     *    technology    and    entertainment        teo
     *    technology    and    design        tod
     *    entertainment    and    design        oed
     *    technology    and    entertainment    and    design        ted

model

   a simple multilayer id88 classifier operates as follows:

   x = embedding(text)
   h = tanh(wx + b)
   u = vh + c
   p = softmax(u)
   if testing:
       prediction = arg max[y   ] p[y   ]
   else: # training, with y as the given gold label
       loss = -log(p[y]) # cross id178 criterion

   we will discuss the embedding function that represents the text as a
   vector (x) is discussed below. w and v are appropriately sized matrices
   of learned parameters, b and c are learned bias vectors. the other
   vectors are intermediate values.

text embedding function

   the text embedding function converts a sequence of words into a fixed
   sized vector representation. effective models for representing
   documents as vectors is an open area of research, but in general,
   trying a few different architectures is important since the optimal
   architecture depends both on the availability of data and the nature of
   the problem being solved.
     * an astoundingly simple but [76]effective embedding model is the
          bag-of-means    representation. let each word wi in the document
       (where i ranges over the tokens) be represented by an embedding
       vector x[i]. the bag of means representation is
           x = (1/n) sum[i] x[i].
       id27s can be learned as parameters in the model (either
       starting from random values or starting from a id27
       model, such as id97 or [77]glove), or they you can use fixed
       values (again, id97 or glove).
     * a more sophisticated model uses an bidirectional id56 (/lstm/gru) to
          read    the document (from left to right and from right to left),
       and then represents the document by pooling the hidden states
       across time (e.g., by simply taking their arithmetic average or
       componentwise maximum) and using that as the document vector. you
       can explore this next week for your practical assignment on id56s.

questions

   you are called to build a single-layer feed-forward neural network in
   your favourite framework. the network should treat the labels as 8
   independent classes. we suggest adam as optimiser, and training should
   place in batches for increased stability (e.g.~50).
    1. compare the learning curves of the model starting from random
       embeddings, starting from glove embeddings
       ([78]http://nlp.stanford.edu/data/glove.6b.zip; 50 dimensions) or
       fixed to be the glove values. training in batches is more stable
       (e.g. 50), which model works best on training vs. test? which model
       works best on held-out accuracy?
    2. what happens if you try alternative non-linearities (logistic
       sigmoid or relu instead of tanh)?
    3. what happens if you add dropout to the network?
    4. what happens if you vary the size of the hidden layer?
    5. how would the code change if you wanted to add a second hidden
       layer?
    6. how does the training algorithm affect the quality of the model?
    7. project the embeddings of the labels onto 2 dimensions and
       visualise (each row of the projection matrix v corresponds a label
       embedding). do you see anything interesting?

(optional, for enthusiastic students)

   try the same prediction task using a true multi-label classification
   (mlc) set up.
    1. one common approach is to make a bunch of binary classification
       decisions (one for each label).
    2. note, however, that separate binary classification problems don't
       model correlation structure. it may be the case that, for sake of
       argument, the label sequence "ood" never occurs in the data, but we
       do not know that aprior when designing the model. mlc is an
       interesting problem in its own right (the name of the game is
       modeling label decisions jointly, exploiting correlation structure
       between them), and neural networks offer some really interesting
       possibilities for modeling mlc problems that have yet to be
       adequately explored in the literature. you may want to try adding a
       crf at the output, for example.

handin

   on paper, show a practical demonstrator your response to these to get
   signed off.

     *    2019 github, inc.
     * [79]terms
     * [80]privacy
     * [81]security
     * [82]status
     * [83]help

     * [84]contact github
     * [85]pricing
     * [86]api
     * [87]training
     * [88]blog
     * [89]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [90]reload to refresh your
   session. you signed out in another tab or window. [91]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/oxford-cs-deepnlp-2017/practical-2/commits/master.atom
   3. https://github.com/oxford-cs-deepnlp-2017/practical-2#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/oxford-cs-deepnlp-2017/practical-2
  32. https://github.com/join
  33. https://github.com/login?return_to=/oxford-cs-deepnlp-2017/practical-2
  34. https://github.com/oxford-cs-deepnlp-2017/practical-2/watchers
  35. https://github.com/login?return_to=/oxford-cs-deepnlp-2017/practical-2
  36. https://github.com/oxford-cs-deepnlp-2017/practical-2/stargazers
  37. https://github.com/login?return_to=/oxford-cs-deepnlp-2017/practical-2
  38. https://github.com/oxford-cs-deepnlp-2017/practical-2/network/members
  39. https://github.com/oxford-cs-deepnlp-2017
  40. https://github.com/oxford-cs-deepnlp-2017/practical-2
  41. https://github.com/oxford-cs-deepnlp-2017/practical-2
  42. https://github.com/oxford-cs-deepnlp-2017/practical-2/issues
  43. https://github.com/oxford-cs-deepnlp-2017/practical-2/pulls
  44. https://github.com/oxford-cs-deepnlp-2017/practical-2/projects
  45. https://github.com/oxford-cs-deepnlp-2017/practical-2/pulse
  46. https://github.com/join?source=prompt-code
  47. https://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/
  48. https://github.com/topics/nlp
  49. https://github.com/topics/natural-language-processing
  50. https://github.com/topics/deep-learning
  51. https://github.com/topics/machine-learning
  52. https://github.com/topics/oxford
  53. https://github.com/oxford-cs-deepnlp-2017/practical-2/commits/master
  54. https://github.com/oxford-cs-deepnlp-2017/practical-2/branches
  55. https://github.com/oxford-cs-deepnlp-2017/practical-2/releases
  56. https://github.com/oxford-cs-deepnlp-2017/practical-2/graphs/contributors
  57. https://github.com/oxford-cs-deepnlp-2017/practical-2/find/master
  58. https://github.com/oxford-cs-deepnlp-2017/practical-2/archive/master.zip
  59. https://github.com/login?return_to=https://github.com/oxford-cs-deepnlp-2017/practical-2
  60. https://github.com/join?return_to=/oxford-cs-deepnlp-2017/practical-2
  61. https://desktop.github.com/
  62. https://desktop.github.com/
  63. https://developer.apple.com/xcode/
  64. https://visualstudio.github.com/
  65. https://github.com/oxford-cs-deepnlp-2017/practical-2/tree/c9bb97c232fef3c6a3b0d69d7273a663ee41e803
  66. https://github.com/oxford-cs-deepnlp-2017/practical-2/blob/master/readme.md
  67. https://github.com/oxford-cs-deepnlp-2017/practical-2/blob/master/practical2.pdf
  68. http://pytorch.org/
  69. https://github.com/pytorch/examples/tree/master/regression
  70. https://www.tensorflow.org/
  71. https://www.tensorflow.org/tutorials/wide/
  72. https://github.com/microsoft/cntk
  73. http://torch.ch/
  74. http://caffe.berkeleyvision.org/
  75. https://wit3.fbk.eu/mono.php?release=xml_releases&tinfo=cleanedhtml_ted
  76. https://cs.umd.edu/~miyyer/pubs/2015_acl_dan.pdf
  77. http://nlp.stanford.edu/projects/glove/
  78. http://nlp.stanford.edu/data/glove.6b.zip
  79. https://github.com/site/terms
  80. https://github.com/site/privacy
  81. https://github.com/security
  82. https://githubstatus.com/
  83. https://help.github.com/
  84. https://github.com/contact
  85. https://github.com/pricing
  86. https://developer.github.com/
  87. https://training.github.com/
  88. https://github.blog/
  89. https://github.com/about
  90. https://github.com/oxford-cs-deepnlp-2017/practical-2
  91. https://github.com/oxford-cs-deepnlp-2017/practical-2

   hidden links:
  93. https://github.com/
  94. https://github.com/oxford-cs-deepnlp-2017/practical-2
  95. https://github.com/oxford-cs-deepnlp-2017/practical-2
  96. https://github.com/oxford-cs-deepnlp-2017/practical-2
  97. https://help.github.com/articles/which-remote-url-should-i-use
  98. https://github.com/oxford-cs-deepnlp-2017/practical-2#practical-2-text-classification
  99. https://github.com/oxford-cs-deepnlp-2017/practical-2#setup-and-installation
 100. https://github.com/oxford-cs-deepnlp-2017/practical-2#multi-class-classification
 101. https://github.com/oxford-cs-deepnlp-2017/practical-2#data-preparation
 102. https://github.com/oxford-cs-deepnlp-2017/practical-2#text-of-talks
 103. https://github.com/oxford-cs-deepnlp-2017/practical-2#labels
 104. https://github.com/oxford-cs-deepnlp-2017/practical-2#model
 105. https://github.com/oxford-cs-deepnlp-2017/practical-2#text-embedding-function
 106. https://github.com/oxford-cs-deepnlp-2017/practical-2#questions
 107. https://github.com/oxford-cs-deepnlp-2017/practical-2#optional-for-enthusiastic-students
 108. https://github.com/oxford-cs-deepnlp-2017/practical-2#handin
 109. https://github.com/
