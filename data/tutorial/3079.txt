   #[1]github [2]recent commits to practical-3:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]15
     * [35]star [36]71
     * [37]fork [38]65

[39]oxford-cs-deepnlp-2017/[40]practical-3

   [41]code [42]issues 1 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   oxford deep nlp 2017 course - practical 3: text classification with
   id56s
   [47]deep-learning [48]machine-learning [49]natural-language-processing
   [50]nlp [51]oxford
     * [52]1 commit
     * [53]1 branch
     * [54]0 releases
     * [55]fetching contributors

   branch: master (button) new pull request
   [56]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/o
   [57]download zip

downloading...

   want to be notified of new releases in
   oxford-cs-deepnlp-2017/practical-3?
   [58]sign in [59]sign up

launching github desktop...

   if nothing happens, [60]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [61]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [62]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [63]download the github extension for visual studio
   and try again.

   (button) go back
   [64]@iassael
   [65]iassael [66]first commit
   [67]1
   latest commit [68]765faf3 feb 5, 2017
   [69]permalink
   type        name        latest commit message commit time
        failed to load latest commit information.
        [70]src
        [71]readme.md
        [72]practical3.pdf

readme.md

practical 3: text classification with id56s

   [chris dyer, phil blunsom, yannis assael, brendan shillingford, yishu
   miao]

   in this practical, you can explore one of two applications of id56s:
   text classification or language modelling (you are welcome to try both,
   too). we will be using the training/dev/test splits that we created in
   practical 2.

text classification (task 1)

   last week   s practical introduced text classification as a problem that
   could be solved with deep learning. the id194
   function we used was very simple: an average over the id27s
   in the document. this week, you will use id56s to compute the
   representations of the documents.

   in the figure below, on the left we show the id194
   function that was used in last week   s practical. your goal in this task
   is to adapt your code to use the architecture on the right.

   [73]practical 2 and 3

   note that in practical 3, x is defined to be the average of the id56
   hidden states (the h_t   s), just just the sum.

questions

    1. what are the benefits and downsides of the id56-based representation
       over the bag of words representation used last week? how would
       availability of data affect your answer?
    2. one possible architectural variant is to use only the final hidden
       state of the id56 as the id194 (i.e., x) rather
       than the average of the hidden states over time. how does this
       work? what are the potential benefits and downsides to this
       representation?
    3. try different id56 architectures, e.g., simple elman id56s or grus or
       lstms. which ones work best?
    4. what happens if you use a bidirectional lstm (i.e., the dashed
       arrows in the figure)?

   (optional, for enthusiastic students) id56s are expensive use as
      readers    on long sequences. truncated id26 through time
   (truncated bptt) can be used to get better parallelism. you are
   encouraged to use this to get better computational efficiency.

language modelling with id56s (task 2)

   as covered in lecture last week, id56 language models use the chain rule
   to decompose the id203 of a sequence into the product of
   probabilities of words, conditional of previously generated words:

   [74]id203 of a seq

   to avoid problems with floating point underflow, you it is customary to
   model this in log space.

   given a training sequence training graph for a language model looks
   like this:

   [75]seq graph

   your task is to train an id56 language model on the training portion of
   the ted data, using the validation set to determine when to stop
   optimising the parameters of the model.

   a language model can be evaluated quantitatively by computing the
   (per-word) perplexity of the model on a held-out test corpus,

   [76]ppl

   where |test set| is the length of the test set in words, including any
   <unk> tokens. (note: you can measure length in terms of any units,
   including characters, words, or sentences, these are just ways of
   quantifying how much uncertainty the model has about different units.)

   to evaluate the model qualitatively, generate random samples from the
   model by sampling from p(w_t | w_{<t} ) and then feeding the sampled
   value of wt into the id56 at time t+1.

questions

    1. if you change the preprocessing of your corpus (e.g., you turn more
       words in unk or you lowercase everything), is perplexity still
       comparable?
    2. to make training tractable you can either treat sentences as i.i.d.
       or you can use truncated bptt. is the i.i.d. assumption valid? what
       are the benefits and downsides of making this assumptions vs.
       truncated bptt? how do you think perplexities will compare on the
       held-out test set? (if you   re feeling adventurous, try it out!)
    3. rather than modeling documents as a sequence of words, you can
       model the document as a sequence of characters. are the per-word
       perplexities comparable between these two models? what benefits
       does modeling text at the character level have? what disadvantages?
    4. try a couple variations of the model using different definitions of
       id56s (e.g., lstms, grus, simple elman id56s) that were covered in
       class. how do perplexities compare?
    5. in text classification, using bidirectional id56s was suggested,
       could you use bidirectional id56s for the id38 task?
       why or why not?

handin

   on paper, show a practical demonstrator your response to these to get
   signed off.

     *    2019 github, inc.
     * [77]terms
     * [78]privacy
     * [79]security
     * [80]status
     * [81]help

     * [82]contact github
     * [83]pricing
     * [84]api
     * [85]training
     * [86]blog
     * [87]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [88]reload to refresh your
   session. you signed out in another tab or window. [89]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/oxford-cs-deepnlp-2017/practical-3/commits/master.atom
   3. https://github.com/oxford-cs-deepnlp-2017/practical-3#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/oxford-cs-deepnlp-2017/practical-3
  32. https://github.com/join
  33. https://github.com/login?return_to=/oxford-cs-deepnlp-2017/practical-3
  34. https://github.com/oxford-cs-deepnlp-2017/practical-3/watchers
  35. https://github.com/login?return_to=/oxford-cs-deepnlp-2017/practical-3
  36. https://github.com/oxford-cs-deepnlp-2017/practical-3/stargazers
  37. https://github.com/login?return_to=/oxford-cs-deepnlp-2017/practical-3
  38. https://github.com/oxford-cs-deepnlp-2017/practical-3/network/members
  39. https://github.com/oxford-cs-deepnlp-2017
  40. https://github.com/oxford-cs-deepnlp-2017/practical-3
  41. https://github.com/oxford-cs-deepnlp-2017/practical-3
  42. https://github.com/oxford-cs-deepnlp-2017/practical-3/issues
  43. https://github.com/oxford-cs-deepnlp-2017/practical-3/pulls
  44. https://github.com/oxford-cs-deepnlp-2017/practical-3/projects
  45. https://github.com/oxford-cs-deepnlp-2017/practical-3/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/deep-learning
  48. https://github.com/topics/machine-learning
  49. https://github.com/topics/natural-language-processing
  50. https://github.com/topics/nlp
  51. https://github.com/topics/oxford
  52. https://github.com/oxford-cs-deepnlp-2017/practical-3/commits/master
  53. https://github.com/oxford-cs-deepnlp-2017/practical-3/branches
  54. https://github.com/oxford-cs-deepnlp-2017/practical-3/releases
  55. https://github.com/oxford-cs-deepnlp-2017/practical-3/graphs/contributors
  56. https://github.com/oxford-cs-deepnlp-2017/practical-3/find/master
  57. https://github.com/oxford-cs-deepnlp-2017/practical-3/archive/master.zip
  58. https://github.com/login?return_to=https://github.com/oxford-cs-deepnlp-2017/practical-3
  59. https://github.com/join?return_to=/oxford-cs-deepnlp-2017/practical-3
  60. https://desktop.github.com/
  61. https://desktop.github.com/
  62. https://developer.apple.com/xcode/
  63. https://visualstudio.github.com/
  64. https://github.com/iassael
  65. https://github.com/oxford-cs-deepnlp-2017/practical-3/commits?author=iassael
  66. https://github.com/oxford-cs-deepnlp-2017/practical-3/commit/765faf390a3c7921b8624653e08b83ae9f498ce2
  67. https://github.com/oxford-cs-deepnlp-2017/practical-3/commit/765faf390a3c7921b8624653e08b83ae9f498ce2
  68. https://github.com/oxford-cs-deepnlp-2017/practical-3/commit/765faf390a3c7921b8624653e08b83ae9f498ce2
  69. https://github.com/oxford-cs-deepnlp-2017/practical-3/tree/765faf390a3c7921b8624653e08b83ae9f498ce2
  70. https://github.com/oxford-cs-deepnlp-2017/practical-3/tree/master/src
  71. https://github.com/oxford-cs-deepnlp-2017/practical-3/blob/master/readme.md
  72. https://github.com/oxford-cs-deepnlp-2017/practical-3/blob/master/practical3.pdf
  73. https://github.com/oxford-cs-deepnlp-2017/practical-3/blob/master/src/image03.png
  74. https://github.com/oxford-cs-deepnlp-2017/practical-3/blob/master/src/image01.png
  75. https://github.com/oxford-cs-deepnlp-2017/practical-3/blob/master/src/image02.png
  76. https://github.com/oxford-cs-deepnlp-2017/practical-3/blob/master/src/image00.png
  77. https://github.com/site/terms
  78. https://github.com/site/privacy
  79. https://github.com/security
  80. https://githubstatus.com/
  81. https://help.github.com/
  82. https://github.com/contact
  83. https://github.com/pricing
  84. https://developer.github.com/
  85. https://training.github.com/
  86. https://github.blog/
  87. https://github.com/about
  88. https://github.com/oxford-cs-deepnlp-2017/practical-3
  89. https://github.com/oxford-cs-deepnlp-2017/practical-3

   hidden links:
  91. https://github.com/
  92. https://github.com/oxford-cs-deepnlp-2017/practical-3
  93. https://github.com/oxford-cs-deepnlp-2017/practical-3
  94. https://github.com/oxford-cs-deepnlp-2017/practical-3
  95. https://help.github.com/articles/which-remote-url-should-i-use
  96. https://github.com/oxford-cs-deepnlp-2017/practical-3#practical-3-text-classification-with-id56s
  97. https://github.com/oxford-cs-deepnlp-2017/practical-3#text-classification-task-1
  98. https://github.com/oxford-cs-deepnlp-2017/practical-3#questions
  99. https://github.com/oxford-cs-deepnlp-2017/practical-3#language-modelling-with-id56s-task-2
 100. https://github.com/oxford-cs-deepnlp-2017/practical-3#questions-1
 101. https://github.com/oxford-cs-deepnlp-2017/practical-3#handin
 102. https://github.com/
