   #[1]rss

   [2][svg+xml,%3c%3fxml version%3d%221.0%22 encoding%3d%22utf-8%22
   standalone%3d%22no%22%3f%3e%0a%3csvg width%3d%22100px%22
   height%3d%22100px%22 viewbox%3d%220 0 100 100%22 version%3d%221.1%22
   xmlns%3d%22http%3a%2f%2fwww.w3.org%2f2000%2fsvg%22
   xmlns%3axlink%3d%22http%3a%2f%2fwww.w3.org%2f1999%2fxlink%22%3e%0a
   %3c%21-- generator%3a sketch 42 %2836781%29 -
   http%3a%2f%2fwww.bohemiancoding.com%2fsketch --%3e%0a
   %3ctitle%3elogo%3c%2ftitle%3e%0a %3cdesc%3ecreated with
   sketch.%3c%2fdesc%3e%0a %3cdefs%3e%0a %3crect id%3d%22path-1%22
   x%3d%2215%22 y%3d%2215%22 width%3d%2235%22
   height%3d%2236%22%3e%3c%2frect%3e%0a %3cmask id%3d%22mask-2%22
   maskcontentunits%3d%22userspaceonuse%22
   maskunits%3d%22objectboundingbox%22 x%3d%220%22 y%3d%220%22
   width%3d%2235%22 height%3d%2236%22 fill%3d%22white%22%3e%0a %3cuse
   xlink%3ahref%3d%22%23path-1%22%3e%3c%2fuse%3e%0a %3c%2fmask%3e%0a
   %3cellipse id%3d%22path-3%22 cx%3d%2215.1507839%22
   cy%3d%2249.0755757%22 rx%3d%2215.0944915%22
   ry%3d%2215.0944915%22%3e%3c%2fellipse%3e%0a %3cmask id%3d%22mask-4%22
   maskcontentunits%3d%22userspaceonuse%22
   maskunits%3d%22objectboundingbox%22 x%3d%220%22 y%3d%220%22
   width%3d%2230.188983%22 height%3d%2230.188983%22
   fill%3d%22white%22%3e%0a %3cuse
   xlink%3ahref%3d%22%23path-3%22%3e%3c%2fuse%3e%0a %3c%2fmask%3e%0a
   %3cellipse id%3d%22path-5%22 cx%3d%2248.8492161%22
   cy%3d%2215.3771436%22 rx%3d%2215.0944915%22
   ry%3d%2215.0944915%22%3e%3c%2fellipse%3e%0a %3cmask id%3d%22mask-6%22
   maskcontentunits%3d%22userspaceonuse%22
   maskunits%3d%22objectboundingbox%22 x%3d%220%22 y%3d%220%22
   width%3d%2230.188983%22 height%3d%2230.188983%22
   fill%3d%22white%22%3e%0a %3cuse
   xlink%3ahref%3d%22%23path-5%22%3e%3c%2fuse%3e%0a %3c%2fmask%3e%0a
   %3cellipse id%3d%22path-7%22 cx%3d%2248.8492161%22
   cy%3d%2249.0900343%22 rx%3d%2215.0944915%22
   ry%3d%2215.0944915%22%3e%3c%2fellipse%3e%0a %3cmask id%3d%22mask-8%22
   maskcontentunits%3d%22userspaceonuse%22
   maskunits%3d%22objectboundingbox%22 x%3d%220%22 y%3d%220%22
   width%3d%2230.188983%22 height%3d%2230.188983%22
   fill%3d%22white%22%3e%0a %3cuse
   xlink%3ahref%3d%22%23path-7%22%3e%3c%2fuse%3e%0a %3c%2fmask%3e%0a
   %3cellipse id%3d%22path-9%22 cx%3d%2215.3980442%22
   cy%3d%2215.3771436%22 rx%3d%2215.0944915%22
   ry%3d%2215.0944915%22%3e%3c%2fellipse%3e%0a %3cmask id%3d%22mask-10%22
   maskcontentunits%3d%22userspaceonuse%22
   maskunits%3d%22objectboundingbox%22 x%3d%220%22 y%3d%220%22
   width%3d%2230.188983%22 height%3d%2230.188983%22
   fill%3d%22white%22%3e%0a %3cuse
   xlink%3ahref%3d%22%23path-9%22%3e%3c%2fuse%3e%0a %3c%2fmask%3e%0a
   %3c%2fdefs%3e%0a %3cg id%3d%22page-1%22 stroke%3d%22none%22
   stroke-width%3d%221%22 fill%3d%22none%22
   fill-rule%3d%22evenodd%22%3e%0a %3cg id%3d%22logo%22
   stroke%3d%22%23000000%22 stroke-width%3d%222%22%3e%0a %3cg
   id%3d%22group%22 transform%3d%22translate%2850.000000%2c 50.000000%29
   rotate%28-315.000000%29 translate%28-50.000000%2c -50.000000%29
   translate%2818.000000%2c 17.500000%29%22%3e%0a %3cuse
   id%3d%22rectangle%22 mask%3d%22url%28%23mask-2%29%22
   xlink%3ahref%3d%22%23path-1%22%3e%3c%2fuse%3e%0a %3cuse
   id%3d%22oval-1%22 mask%3d%22url%28%23mask-4%29%22
   fill%3d%22%23ffffff%22 xlink%3ahref%3d%22%23path-3%22%3e%3c%2fuse%3e%0a
   %3cuse id%3d%22oval-1%22 mask%3d%22url%28%23mask-6%29%22
   fill%3d%22%23ffffff%22 xlink%3ahref%3d%22%23path-5%22%3e%3c%2fuse%3e%0a
   %3cuse id%3d%22oval-1%22 mask%3d%22url%28%23mask-8%29%22
   fill%3d%22%23ffffff%22 xlink%3ahref%3d%22%23path-7%22%3e%3c%2fuse%3e%0a
   %3cuse id%3d%22oval-1%22 mask%3d%22url%28%23mask-10%29%22
   fill%3d%22%23ffffff%22 xlink%3ahref%3d%22%23path-9%22%3e%3c%2fuse%3e%0a
   %3c%2fg%3e%0a %3c%2fg%3e%0a %3c%2fg%3e%0a%3c%2fsvg%3e]
   [3]image-to-image translation in tensorflow
   make discriminators do your work for you
   written by [4]christopher hesse     january 25^th, 2017
   i thought that the results from [5]pix2pix by isola et al. looked
   pretty cool and wanted to implement an adversarial net, so i ported the
   torch code to tensorflow. the single-file implementation is available
   as [6]pix2pix-tensorflow on github.
   here are some examples of what this thing does, from the original
   paper:
   [z]

   "the sorcerer's stone, a rock with enormous powers, such as: lead into
   gold, horses into gold, immortal life, giving ghosts restored bodies,
   frag trolls, trolls into gold, et cetera."

       [7]wizard people, dear readers

   pix2pix probably will not give ghosts restored bodies, but if you have
   a dataset of normal horses and their corresponding golden forms, it may
   be able to do something with that.
   running the code
   [fa2hxcyofoobtxggeucm3npksgcxzwfd0rxyotanmdukgwak1wko5zikncaaaaasuvork5
   cyii=]
   terminal
# make sure you have tensorflow 0.12.1 installed first
python -c "import tensorflow; print(tensorflow.__version__)"

# clone the repo
git clone https://github.com/affinelayer/pix2pix-tensorflow.git
cd pix2pix-tensorflow

# download the cmp facades dataset http://cmp.felk.cvut.cz/~tylecr1/facade/
python tools/download-dataset.py facades

# train the model
# this may take 1-9 hours depending on gpu, on cpu you will be waiting for a bit
python pix2pix.py \
  --mode train \
  --output_dir facades_train \
  --max_epochs 200 \
  --input_dir facades/train \
  --which_direction btoa

# test the model
python pix2pix.py \
  --mode test \
  --output_dir facades_test \
  --input_dir facades/val \
  --checkpoint facades_train

   after training this for a long while, you can expect output along the
   lines of:
   [9k=]
   how pix2pix works
   pix2pix uses a conditional generative adversarial network (cgan) to
   learn a mapping from an input image to an output image.
   the network is composed of two main pieces, the generator and the
   discriminator. the generator applies some transform to the input image
   to get the output image. the discriminator compares the input image to
   an unknown image (either a target image from the dataset or an output
   image from the generator) and tries to guess if this was produced by
   the generator.
   an example of a dataset would be that the input image is a black and
   white picture and the target image is the color version of the picture:
   [rt3h0abviqtiqaaaabjru5erkjggg==]
   the generator in this case is trying to learn how to colorize a black
   and white image:
   [cuxjmzoeoteaaaaasuvork5cyii=]
   the discriminator is looking at the generator's colorization attempts
   and trying to learn to tell the difference between the colorizations
   the generator provides and the true colorized target image provided in
   the dataset.
   [duwszikszikszikszikszikszikszikszikszikszwuwcqfov5z7i0aaaaasuvork5cyii
   =]
   why go through all this trouble? one of the main points of the paper is
   that the discriminator provides a id168 for training your
   generator and you didn't have to manually specify it, which is really
   neat. hand-engineered transformation code has been replaced with
   training neural nets, so why not replace the hand-engineered loss
   calculations as well? if this works, you can let the computer do the
   work while you relax in comfort and fear that computers will replace
   your job.
   let's look at the two parts of the adversarial network: the generator
   and the discriminator.
   the generator
   the generator has the job of taking an input image and performing the
   transform we want in order to produce the target image. an example
   input would be a black and white image, and we want the output to be a
   colorized version of that image. the structure of the generator is
   called an "encoder-decoder" and in pix2pix the encoder-decoder looks
   more or less like this:
   [c6aybhzgxaexiya5ebtxfcj8bgek13u8aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
   aaaaaaaaaaaaaaaaaaaaaaaahvsb8jlknoclq4aaaaaasuvork5cyii=]
   the volumes are there to give you a sense of the shape of the tensor
   dimensions next to them. the input in this example is a 256x256 image
   with 3 color channels (red, green, and blue, all equal for a black and
   white image), and the output is the same.
   the generator takes some input and tries to reduce it with a series of
   encoders (convolution + activation function) into a much smaller
   representation. the idea is that by compressing it this way we
   hopefully have a higher level representation of the data after the
   final encode layer. the decode layers do the opposite (deconvolution +
   activation function) and reverse the action of the encoder layers.
   [q9ytrkbpalsogaaaabjru5erkjggg==]
   in order to improve the performance of the image-to-image transform in
   the paper, the authors used a "u-net" instead of an encoder-decoder.
   this is the same thing, but with "skip connections" directly connecting
   encoder layers to decoder layers:
   [whjuuiwritpdqaaaabjru5erkjggg==]
   the skip connections give the network the option of bypassing the
   encoding/decoding part if it doesn't have a use for it.
   these diagrams are a slight simplification. for instance, the first and
   last layers of the network have no batch norm layer and a few layers in
   the middle have dropout units. the colorization mode used in the paper
   also has a different number of channels for the input and output
   layers.
   the discriminator
   the discriminator has the job of taking two images, an input image and
   an unknown image (which will be either a target or output image from
   the generator), and deciding if the second image was produced by the
   generator or not.
   [622hvf2ahstr86p+cmnlrv5ommnn74sndyunaqaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
   aaaaaaaaaaaaaaaaaaaaaaaaaaaad+c78aja9isddaiz8aaaaasuvork5cyii=]
   the structure looks a lot like the encoder section of the generator,
   but works a little differently. the output is a 30x30 image where each
   pixel value (0 to 1) represents how believable the corresponding
   section of the unknown image is. in the pix2pix implementation, each
   pixel from this 30x30 image corresponds to the believability of a 70x70
   patch of the input image (the patches overlap a lot since the input
   images are 256x256). the architecture is called a "patchgan".
   training
   to train this network, there are two steps: training the discriminator
   and training the generator.
   to train the discriminator, first the generator generates an output
   image. the discriminator looks at the input/target pair and the
   input/output pair and produces its guess about how realistic they look.
   the weights of the discriminator are then adjusted based on the
   classification error of the input/output pair and the input/target
   pair.
   [4bsxy3pdwh2kcaaaaasuvork5cyii=]
   the generator's weights are then adjusted based on the output of the
   discriminator as well as the difference between the output and target
   image.
   [zfhioiacerf4+hqgiabe4faoreaau1fplppc9roaaenhe6z1jvt0seabcbk0hagjafwome
   aebyjc+u4iaadbynyleqac4tdqffd8qeaaisink9b6aabcxdg4keraahnl7ztkpreaagfi2
   hxwqeaaitv+dqgqegoetrfu+qj7tdquegihscnkuiacepfoigaaqk04hagjaudqfcagaqb0
   preaacfvtcagao9he3xqqaeaxtoazaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabpxc+8qqjaarsqjaaaaaelftksuqmcc]
   the clever trick here is that when you train the generator on the
   output of the discriminator, you're actually calculating the gradients
   through the discriminator, which means that while the discriminator
   improves, you're training the generator to beat the discriminator.
   the theory is that as the discriminator gets better, so does the
   generator. if the discriminator is good at its job and the generator is
   capable of learning the correct mapping function through gradient
   descent, you should get generated outputs that could fool a human.
   validation
   validation of the code was performed on a linux machine with a ~1.3
   tflops nvidia gtx 750 ti gpu. due to a lack of compute power,
   validation is not extensive and only the facades dataset at 200 epochs
   was tested.
   [fa2hxcyofoobtxggeucm3npksgcxzwfd0rxyotanmdukgwak1wko5zikncaaaaasuvork5
   cyii=]
   terminal
git clone https://github.com/affinelayer/pix2pix-tensorflow.git
cd pix2pix-tensorflow
python tools/download-dataset.py facades

sudo nvidia-docker run \
  --volume $pwd:/prj \
  --workdir /prj \
  --env pythonunbuffered=x \
  affinelayer/pix2pix-tensorflow \
  python pix2pix.py \
    --mode train \
    --output_dir facades_train \
    --max_epochs 200 \
    --input_dir facades/train \
    --which_direction btoa

sudo nvidia-docker run \
  --volume $pwd:/prj \
  --workdir /prj \
  --env pythonunbuffered=x \
  affinelayer/pix2pix-tensorflow \
  python pix2pix.py \
    --mode test \
    --output_dir facades_test \
    --input_dir facades/val \
    --checkpoint facades_train

   for comparison, the first image in the validation set looks like this:
   [z]
   if you wish you can download the [8]full results on the validation set.
   implementation
   the implementation is a single file, [9]pix2pix.py, that does as much
   as possible inside the tensorflow graph.
   the porting process was mostly a matter of looking at the existing
   torch implementation as well as the torch source code to figure out
   what sorts of layers and settings were being used in order to make the
   tensorflow version as true to the original as possible. debugging a
   broken implementation can be time consuming, so i attempted to be
   careful about the conversion to avoid having to do extensive debugging.
   the implementation started with the creation of the generator graph,
   then the discriminator graph, then the training system. the generator
   and discriminator graph were printed at runtime using the torch pix2pix
   code. i looked in the torch framework source for the different layer
   types and found what settings and operations were present and
   implemented those in tensorflow.
   ideally i would have been able to export the pix2pix trained network
   weights into tensorflow to verify the graph construction, but that was
   annoying enough, or i am bad enough at torch, that i did not do it.
   most of the bugs in my code were related to the
   build-graph-then-execute model of tensorflow which can be a little
   surprising when you are used to imperative code.

   all code samples on this site are in the public domain unless otherwise
   stated

   [10][svg+xml,%3c%3fxml version%3d%221.0%22 encoding%3d%22utf-8%22
   standalone%3d%22no%22%3f%3e%0a%3csvg width%3d%22100px%22
   height%3d%22100px%22 viewbox%3d%220 0 100 100%22 version%3d%221.1%22
   xmlns%3d%22http%3a%2f%2fwww.w3.org%2f2000%2fsvg%22
   xmlns%3axlink%3d%22http%3a%2f%2fwww.w3.org%2f1999%2fxlink%22%3e%0a
   %3c%21-- generator%3a sketch 42 %2836781%29 -
   http%3a%2f%2fwww.bohemiancoding.com%2fsketch --%3e%0a
   %3ctitle%3etwitter%3c%2ftitle%3e%0a %3cdesc%3ecreated with
   sketch.%3c%2fdesc%3e%0a %3cdefs%3e%3c%2fdefs%3e%0a %3cg
   id%3d%22page-1%22 stroke%3d%22none%22 stroke-width%3d%221%22
   fill%3d%22none%22 fill-rule%3d%22evenodd%22%3e%0a %3cg
   id%3d%22twitter%22 fill-rule%3d%22nonzero%22
   fill%3d%22%23f92672%22%3e%0a %3cpath d%3d%22m35.7519134%2c81.2096791
   c64.7360437%2c81.2096791 80.5863373%2c57.1976441
   80.5863373%2c36.3752552 c80.5863373%2c35.6932286
   80.5724457%2c35.0142948 80.5418534%2c34.338428 c83.6186144%2c32.1141808
   86.2928038%2c29.338498 88.4020526%2c26.1787484 c85.5786385%2c27.4337309
   82.540279%2c28.2785644 79.3529525%2c28.6595127 c82.6062574%2c26.7086895
   85.1039029%2c23.6227275 86.2822369%2c19.943913 c83.2376919%2c21.7488105
   79.8660896%2c23.060622 76.2761914%2c23.7688595 c73.4007162%2c20.7060158
   69.3069591%2c18.7903209 64.7740329%2c18.7903209
   c56.0720928%2c18.7903209 49.0154906%2c25.8471809
   49.0154906%2c34.5457705 c49.0154906%2c35.7822996
   49.1537361%2c36.9850662 49.4240931%2c38.1387614 c36.327627%2c37.4797758
   24.7146452%2c31.209477 16.9425891%2c21.6750744 c15.5893091%2c24.0037531
   14.8089849%2c26.7087668 14.8089849%2c29.5950666
   c14.8089849%2c35.0620004 17.5909047%2c39.8882215
   21.8212579%2c42.7116355 c19.2359848%2c42.6317604
   16.8074106%2c41.9220847 14.6847599%2c40.7408384
   c14.6824403%2c40.8069509 14.6824403%2c40.8714061 14.6824403%2c40.942065
   c14.6824403%2c48.5734014 20.1140653%2c54.9449617
   27.3242736%2c56.3887559 c26.0001685%2c56.7497302
   24.6070436%2c56.9432842 23.1691772%2c56.9432842
   c22.1553769%2c56.9432842 21.1676588%2c56.8435021 20.2076207%2c56.659113
   c22.2137266%2c62.9201078 28.0309648%2c67.4762296
   34.9277754%2c67.6035474 c29.534552%2c71.8308078 22.7405749%2c74.348556
   15.3566577%2c74.348556 c14.0863146%2c74.348556 12.831358%2c74.2762993
   11.5979474%2c74.1304302 c18.5718189%2c78.6004707
   26.8526308%2c81.2086816 35.7527639%2c81.2086816%22
   id%3d%22shape%22%3e%3c%2fpath%3e%0a %3c%2fg%3e%0a
   %3c%2fg%3e%0a%3c%2fsvg%3e] [11][svg+xml,%3c%3fxml version%3d%221.0%22
   encoding%3d%22utf-8%22 standalone%3d%22no%22%3f%3e%0a%3csvg
   width%3d%22100px%22 height%3d%22100px%22 viewbox%3d%220 0 100 100%22
   version%3d%221.1%22 xmlns%3d%22http%3a%2f%2fwww.w3.org%2f2000%2fsvg%22
   xmlns%3axlink%3d%22http%3a%2f%2fwww.w3.org%2f1999%2fxlink%22%3e%0a
   %3c%21-- generator%3a sketch 42 %2836781%29 -
   http%3a%2f%2fwww.bohemiancoding.com%2fsketch --%3e%0a
   %3ctitle%3erss%3c%2ftitle%3e%0a %3cdesc%3ecreated with
   sketch.%3c%2fdesc%3e%0a %3cdefs%3e%3c%2fdefs%3e%0a %3cg
   id%3d%22page-1%22 stroke%3d%22none%22 stroke-width%3d%221%22
   fill%3d%22none%22 fill-rule%3d%22evenodd%22%3e%0a %3cg id%3d%22rss%22
   fill%3d%22%23f92672%22%3e%0a %3cellipse id%3d%22oval-1%22
   cx%3d%2228.4712624%22 cy%3d%2270.0384616%22 rx%3d%228.47900866%22
   ry%3d%228.2913302%22%3e%3c%2fellipse%3e%0a %3cpath
   d%3d%22m19%2c36.7735036 c41.6946018%2c38.6161117
   59.8351412%2c56.1456608 62.0519035%2c78.2440986
   l49.7815343%2c78.2440986 c47.6709502%2c62.736186
   34.9565604%2c50.5052613 19%2c48.7500136 l19%2c36.7735036 z%22
   id%3d%22oval-1%22%3e%3c%2fpath%3e%0a %3cpath d%3d%22m19%2c16.1912989
   c53.2966503%2c18.1111482 80.8090992%2c44.7986803
   83.1198892%2c78.2440986 l70.0797727%2c78.2440986
   c67.8187966%2c51.8248273 46.1124387%2c30.811446 19%2c28.9325028
   l19%2c16.1912989 z%22 id%3d%22oval-1%22%3e%3c%2fpath%3e%0a
   %3c%2fg%3e%0a %3c%2fg%3e%0a%3c%2fsvg%3e] enter your email address
   ____________________ subscribe

references

   1. https://affinelayer.com/feed.xml
   2. https://affinelayer.com/
   3. https://affinelayer.com/pix2pix/
   4. https://twitter.com/christophrhesse
   5. https://github.com/phillipi/pix2pix
   6. https://github.com/affinelayer/pix2pix-tensorflow
   7. http://www.erikkennedy.com/wizardpeopledearreaders.html
   8. https://mega.nz/#!b4oksq7r!rvsdvp8gl8fimvcjl-wf817l8btklkqr-_io4jd3vbm
   9. https://github.com/affinelayer/pix2pix-tensorflow/blob/master/pix2pix.py
  10. https://twitter.com/christophrhesse
  11. https://affinelayer.com/feed.xml
