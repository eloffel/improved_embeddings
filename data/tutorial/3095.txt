   #[1]github [2]recent commits to fast-weights:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]14
     * [35]star [36]238
     * [37]fork [38]67

[39]gokumohandas/[40]fast-weights

   [41]code [42]issues 0 [43]pull requests 3 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
        implementation of using fast weights to attend to the recent past.
   [47]machine-learning [48]tensorflow
     * [49]27 commits
     * [50]1 branch
     * [51]0 releases
     * [52]fetching contributors
     * [53]mit

    1. [54]python 100.0%

   (button) python
   branch: master (button) new pull request
   [55]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/g
   [56]download zip

downloading...

   want to be notified of new releases in gokumohandas/fast-weights?
   [57]sign in [58]sign up

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [60]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [61]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [62]download the github extension for visual studio
   and try again.

   (button) go back
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [63]permalink
   type      name      latest commit message commit time
        failed to load latest commit information.
        [64]fw
        [65]images
        [66].gitignore
        [67]license
        [68]readme.md

readme.md

implementation of using fast weights to attend to the recent past

using fast weights to attend to the recent past
jimmy ba, geoffrey hinton, volodymyr mnih, joel z. leibo, catalin ionescu
nips 2016, https://arxiv.org/abs/1610.06258

   more details @
   [69]https://theneuralperspective.com/2016/12/04/implementation-of-using
   -fast-weights-to-attend-to-the-recent-past/

   use fast weights to aid in learning associative tasks and store
   temporary memories of recent past. in a traditional recurrent
   architecture we have our slow weights which are used to determine the
   next hidden state and hold long-term memory. we introduce the concept
   of fast weights, in conjunction with the slow weights, in order to
   account for short-term knowledge. these weights are quick to update and
   decay as they change from the introduction of new hidden states.

   the overall task of the fast weights is to quickly be able to adjust to
   recent hidden states while remembering the recent past. you use the
   fast weights to determine the final hidden state at each time step.
   though bptt does not directly change our fast weights (these fast
   weights a are unique for each sample actually), bptt does affect the
   hidden states states' weights, and recall that fast weights do affects
   our hidden states (and vice versa). so with training, we affect our
   slow weights (wh and wx) which intern affects how our fast weights are
   determined. eventually, they will be determined as to keep track of the
   recent past so we can create hidden states that lead to the correct
   answer.

physiological motivations

   how do we store memories? we don't store memories by keeping track of
   the exact neural activity that occurred at the time of the memory.
   instead, we try to recreate the neural activity through a set of
   associative weights which can map to many other memories as well. this
   allows for efficient storage of many memories without storing separate
   weights for each instance. this associative network also allows for
   associative learning which is the ability to learn the recall the
   relationship between initially unrelated instances.(1)

   [70]ba

concept

     * in a traditional recurrent architecture we have our slow weights.
       these weights are used with the input and the previous hidden state
       to determine the next hidden state. these weights are responsible
       for the long-term knowledge of our systems. these weights are
       updated at the end of a batch, so they are quite slow to update and
       decay.
     * we introduce the concept of fast weights, in conjunction with the
       slow weights, in order to account for short-term knowledge. these
       weights are quick to update and decay as they change from the
       introduction of new hidden states.
     * for each connection in our network, the total weight is the sum of
       the results from both the slow and fast weights. the hidden state
       for each time step for each input is determined by the operations
       with the slow and fast weights. we use a fast memory weights matrix
       a to alter the hidden states to keep track of the features required
       for any associative learning tasks.
     * the fast weights memory matrix, a(t), starts with 0 at the
       beginning of the sequence. then all the inputs for the time step
       are processed and a(t) is updated with a scalar decay with the
       previous a(t) and the outer product of the hidden state with a
       scalar operation with learning rate eta.

   [71]eq1
     * for each time step, the same a(t) is used for all the inputs and
       for all s steps of the "inner loop". this inner loop is
       transforming the previous hidden state h(t) into the next output
       hidden state h(t+1). for our toy task, s=1 but for more complicated
       tasks where more associate learning is required, we need to
       increase s.

   [72]diagram.png

efficient implementation

     * if a neural net, we compute our fast weights matrix a by changing
       synapses but if we want to employ an efficient computer simulation,
       we need to utilize the fact that a will be <full rank matrix since
       number of time steps t < number of hidden units h. this means we
       might not need to calculate the fast weights matrix explicitly at
       all.

   [73]eq2
     * we can rewrite a by recursively applying it (assuming a=0 @
       beginning of seq.). this also allows us to compute the third
       component required for the inner loop's hidden state vector. we do
       not need to explicitly compute the fast weights matrix a at any
       point! the real advantage here is that now we do not have to store
       a fast weights matrix for each sequence in the minibatch, which
       becomes a major space issue for forward/back prop on multiple
       sequences in the minibatch. now, we just have to keep track of the
       previous hidden states (one row each time step).

   [74]eq3

   notice the last two terms when computing the inner loops next hidden
   vector. this is just the scalar product of the earlier hidden state
   vector, h(\tau ), and the current hidden state vector, hs(t+ 1) in the
   inner loop. so you can think of each iteration as attending to the past
   hidden vectors in proportion to the similarity with the current inner
   loop hidden vector.

   we do not use this method in our basic implementation because i wanted
   to explicitly show what the fast weights matrix looks like and having
   this "memory augmented" view does not really inhibit using minibatches
   (as you can see). but the problem an explicit fast weights matrix can
   create is the space issue, so using this efficient implementation will
   really help us out there.

   note that this 'efficient implementation' will be costly if our
   sequence length is greater than the hidden state dimensionality. the
   computations will scale quadratically now because since we need to
   attend to all previous hidden states with the current inner loop's
   hidden representation.

requirements

     * tensorflow (>0.10)

execution

     * to see the advantage behind the fast weights, ba et. al. used a
       very simple toy task.

   given: g1o2k3??g we need to predict 1.
     * you can think of each letter-number pair as a key/value pair. we
       are given a key at the end and we need to predict the appropriate
       value. the fast associative memory is required here in order to
       keep track of the key/value pairs it has just seen and retrieve the
       proper value given a key. after id26, the fast memory
       will give us a hidden state vector, for example after g and 1, with
       a part for g and another part for 1 and learn to associate the two
       together.
     * create datasets:

python data_utils.py

     * for training:

python train.py train <model_name: id56-ln-fw | id56-ln | control | gru-ln >

     * for sampling:

python train.py test <model_name: id56-ln-fw | id56-ln | control | gru-ln >

     * for plotting results:

python train.py plot

results

     * control: id56 without layer id172 (ln) or fast weights (fw)
       [75]results1 [76]results2

bag of tricks

     * initialize slow hidden weights with an identity matrix in id56 to
       avoid gradient issues.(2)
     * layer norm is required when using an id56 for convergence.
     * weights should be properly initialized in order to have unit
       variance after the dot product, prior to non-linearity or else
       things can blow up really quickly.
     * keep track of the gradient norm and tune accordingly.
     * no need to add extra input processing and extra layer after softmax
       as jimmy ba did. (he was doing that simply to compare with another
       task so it will just add extra computation if you blindly follow
       that).

extensions

     * i will be releasing my code comparing fast weights with an
       attention interface for language related sequence to sequence tasks
       (in a sep repo).
     * it will also be interesting to compare the computational demands of
       fast weights compared to lstm/grus and see which one is better for
       test time.

citations

    1. suzuki, wendy a. "associative learning and the hippocampus." apa.
       american psychological association, feb. 2005. web. 04 dec. 2016.
    2. hinton, geoffrey. "fieldslive video archive." fields institute for
       research in mathematical sciences. university of toronto, 13 oct.
       2016. web. 04 dec. 2016.

author

   goku mohandas ([77]gokumd@gmail.com)

     *    2019 github, inc.
     * [78]terms
     * [79]privacy
     * [80]security
     * [81]status
     * [82]help

     * [83]contact github
     * [84]pricing
     * [85]api
     * [86]training
     * [87]blog
     * [88]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [89]reload to refresh your
   session. you signed out in another tab or window. [90]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/gokumohandas/fast-weights/commits/master.atom
   3. https://github.com/gokumohandas/fast-weights#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/gokumohandas/fast-weights
  32. https://github.com/join
  33. https://github.com/login?return_to=/gokumohandas/fast-weights
  34. https://github.com/gokumohandas/fast-weights/watchers
  35. https://github.com/login?return_to=/gokumohandas/fast-weights
  36. https://github.com/gokumohandas/fast-weights/stargazers
  37. https://github.com/login?return_to=/gokumohandas/fast-weights
  38. https://github.com/gokumohandas/fast-weights/network/members
  39. https://github.com/gokumohandas
  40. https://github.com/gokumohandas/fast-weights
  41. https://github.com/gokumohandas/fast-weights
  42. https://github.com/gokumohandas/fast-weights/issues
  43. https://github.com/gokumohandas/fast-weights/pulls
  44. https://github.com/gokumohandas/fast-weights/projects
  45. https://github.com/gokumohandas/fast-weights/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/machine-learning
  48. https://github.com/topics/tensorflow
  49. https://github.com/gokumohandas/fast-weights/commits/master
  50. https://github.com/gokumohandas/fast-weights/branches
  51. https://github.com/gokumohandas/fast-weights/releases
  52. https://github.com/gokumohandas/fast-weights/graphs/contributors
  53. https://github.com/gokumohandas/fast-weights/blob/master/license
  54. https://github.com/gokumohandas/fast-weights/search?l=python
  55. https://github.com/gokumohandas/fast-weights/find/master
  56. https://github.com/gokumohandas/fast-weights/archive/master.zip
  57. https://github.com/login?return_to=https://github.com/gokumohandas/fast-weights
  58. https://github.com/join?return_to=/gokumohandas/fast-weights
  59. https://desktop.github.com/
  60. https://desktop.github.com/
  61. https://developer.apple.com/xcode/
  62. https://visualstudio.github.com/
  63. https://github.com/gokumohandas/fast-weights/tree/539fb10e3c384d5f782af2560bf28631cd0eaa61
  64. https://github.com/gokumohandas/fast-weights/tree/master/fw
  65. https://github.com/gokumohandas/fast-weights/tree/master/images
  66. https://github.com/gokumohandas/fast-weights/blob/master/.gitignore
  67. https://github.com/gokumohandas/fast-weights/blob/master/license
  68. https://github.com/gokumohandas/fast-weights/blob/master/readme.md
  69. https://theneuralperspective.com/2016/12/04/implementation-of-using-fast-weights-to-attend-to-the-recent-past/
  70. https://github.com/gokumohandas/fast-weights/blob/master/images/ba.png
  71. https://github.com/gokumohandas/fast-weights/blob/master/images/eq1.png
  72. https://github.com/gokumohandas/fast-weights/blob/master/images/diagram1.png
  73. https://github.com/gokumohandas/fast-weights/blob/master/images/eq2.png
  74. https://github.com/gokumohandas/fast-weights/blob/master/images/eq3.png
  75. https://github.com/gokumohandas/fast-weights/blob/master/images/loss.png
  76. https://github.com/gokumohandas/fast-weights/blob/master/images/accuracy.png
  77. mailto:gokumd@gmail.com
  78. https://github.com/site/terms
  79. https://github.com/site/privacy
  80. https://github.com/security
  81. https://githubstatus.com/
  82. https://help.github.com/
  83. https://github.com/contact
  84. https://github.com/pricing
  85. https://developer.github.com/
  86. https://training.github.com/
  87. https://github.blog/
  88. https://github.com/about
  89. https://github.com/gokumohandas/fast-weights
  90. https://github.com/gokumohandas/fast-weights

   hidden links:
  92. https://github.com/
  93. https://github.com/gokumohandas/fast-weights
  94. https://github.com/gokumohandas/fast-weights
  95. https://github.com/gokumohandas/fast-weights
  96. https://help.github.com/articles/which-remote-url-should-i-use
  97. https://github.com/gokumohandas/fast-weights#implementation-of-using-fast-weights-to-attend-to-the-recent-past
  98. https://github.com/gokumohandas/fast-weights#physiological-motivations
  99. https://github.com/gokumohandas/fast-weights#concept
 100. https://github.com/gokumohandas/fast-weights#efficient-implementation
 101. https://github.com/gokumohandas/fast-weights#requirements
 102. https://github.com/gokumohandas/fast-weights#execution
 103. https://github.com/gokumohandas/fast-weights#results
 104. https://github.com/gokumohandas/fast-weights#bag-of-tricks
 105. https://github.com/gokumohandas/fast-weights#extensions
 106. https://github.com/gokumohandas/fast-weights#citations
 107. https://github.com/gokumohandas/fast-weights#author
 108. https://github.com/
