   #[1]github [2]recent commits to tnml:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]14
     * [35]star [36]67
     * [37]fork [38]39

[39]emstoudenmire/[40]tnml

   [41]code [42]issues 1 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   tensor network machine learning. based on the paper "supervised
   learning with quantum inspired tensor networks"
   [47]http://arxiv.org/abs/1605.05775
   [48]machine-learning [49]tensor-networks [50]matrix-product-states
   [51]mps
     * [52]40 commits
     * [53]1 branch
     * [54]0 releases
     * [55]fetching contributors
     * [56]mit

    1. [57]c++ 100.0%

   (button) c++
   branch: master (button) new pull request
   [58]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/e
   [59]download zip

downloading...

   want to be notified of new releases in emstoudenmire/tnml?
   [60]sign in [61]sign up

launching github desktop...

   if nothing happens, [62]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [63]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [64]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [65]download the github extension for visual studio
   and try again.

   (button) go back
   [66]@emstoudenmire
   [67]emstoudenmire [68]update readme.md
   latest commit [69]d353a0e mar 25, 2019
   [70]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [71]mllib
   [72]sample_inputs
   [73].gitignore
   [74]license [75]initial commit oct 25, 2016
   [76]makefile.sample
   [77]readme.md
   [78]fixedl.cc
   [79]fulltest.cc
   [80]image.h
   [81]linear.cc
   [82]paralleldo.h
   [83]separate_fulltest.cc
   [84]single.cc [85]updated codes to work with newer (included) mllib aug
   29, 2018
   [86]single.h
   [87]util.h

readme.md

   note: these codes are research, proof-of-principle codes only, and are
   not intended to demonstrate the state of the art in terms of training
   times for matrix product states for machine learning

   if you are seeking fast approaches for optimizing mps, we recommend
   trying newer libraries which use stochastic gradient optimization
   methods, such as torchmps: [88]https://github.com/jemisjoky/torchmps

tensor network machine learning

   codes based on the paper "supervised learning with quantum-inspired
   tensor networks" by miles stoudenmire and david schwab.
   [89]http://arxiv.org/abs/1605.05775

   also see "tensor train polynomial models via riemannian optimization"
   by novikov, trofimov, and oseledets for a similar approach:
   [90]http://arxiv.org/abs/1605.03795

code overview

   fixedl -- optimize a matrix product state (mps) with a label index on
   the central tensor, similar to what is described in the paper
   arxiv:1605.05775, but where the label index stays fixed on the central
   tensor and does not move around during optimization. this mps
   parameterizes a model whose output is a vector of 10 numbers (for the
   case of mnist). the output entry with the largest value is the
   predicted label.

   fulltest -- given an mps ("wavefunction") generated by the fixedl
   program, report classification error for the mnist testing set

   single -- optimize an mps for a single label type, with no label index
   on the mps. this mps parameterizes a model whose output is positive for
   inputs of the correct type, and zero for all other inputs.

   separate_fulltest -- report classification error for the mnist testing
   set for a set of mps created by the "single" application. important:
   this program assumes that the mps w00, w01, w02, etc. made by running
   "single" reside in folders (which you have to create) named l00/, l01/,
   l02/ etc. so it looks for the files l00/w00, l01/w01, etc. under the
   folder where you run it.

compiling and running the programs

   dependencies:
     * itensor tensor network library: [91]http://itensor.org (on github
       at [92]http://github.com/itensor/itensor)
     * libpng (available through most package managers):
       [93]http://www.libpng.org
     * png++: [94]http://www.nongnu.org/pngpp/

   steps to install and run:
    1. install the above dependencies.
    2. do cp makefile.sample makefile to create a makefile from the sample
       provided.
    3. edit the following variables at the top of your makefile:
          + itensor_dir: this should be the folder where you git clone'd
            and installed itensor (where the options.mk file is located)
          + libpng_dir: folder where the file libpng16.so (or
            libpng16.dylib on mac) is located (or change the name of the
            library if you install a different version of libpng)
          + pngpp_dir: folder where the png++ header (.hpp) files are
            located
    4. run the command make, which should successfully build the fixedl
       application.
    5. copy one of the sample input files from the folder sample_inputs/
       to another folder of your choosing. run each app by doing ./appname
       input_file_name.
    6. edit the input file. at a minimum, change datadir to point to the
       location of the cppmnist folder (inside of this repo) on your
       computer. play around with the other settings such as ntrain (max
       number of training images per label) to check basic things about
       the code before trying a heavy-duty calculation.

   all of the codes require you to install the itensor tensor network
   library. you can obtain it from [95]http://github.com/itensor/itensor .
   the only software dependencies for itensor are a compiler that supports
   c++11 (language and standard library) and a blas/lapack distribution
   such as the "lapack" package on linux, the accelerate/veclib framework
   on macos, or the intel mkl library.

   see [96]http://itensor.org/ for help installing itensor and for more
   documentation on it.

   once itensor is installed, modify the first line of the provided
   makefile to point to the itensor installation folder. (note: itensor
   does not put files anywhere else on your computer; it just creates
   libraries inside its own folder.)

   to use the makefile, either just run make to build the default program
   (which is fixedl) or do make app=appname to compile the program appname
   (either fixedl, single, fulltest, or separate_fulltest).

input files

   sample input files for fixedl and single are provided in the
   sample_inputs/ folder.

   see below for a list of the possible input parameters to these programs
   and what they do.

fixedl program input parameters and code features

   fixedl optimizes a matrix product state (mps) with a label index on the
   central tensor, similar to what is described in the paper
   arxiv:1605.05775. this mps parameterizes a model whose output is a
   vector of 10 numbers (for the case of mnist). the output entry with the
   largest value is the predicted label.

   one difference from the algorithm described in the paper is that the
   label index always remains on the same mps tensor and is not moved
   around (although it can be moved, keeping it in a fixed position turns
   helps with the optimization).

   warning: fixedl can use a lot of ram. if this happens, adjust the
   nbatch parameter described below to make the program read smaller
   amounts of data into ram at each step.

   input parameters:
     * nthread (integer) [default: 1]: number of threads to use to
       parallelize gradient calculations. not recommended to set this
       larger than number of cores on your processor.
     * npass (integer) [default: 4]: maximum number of conjugate gradient
       passes to do at each bond.
     * nsweep (integer) [default: 50]: total number of sweeps
       (left-to-right passes over the mps) to do.
     * lambda (real) [default: 0.0]: size of the l2 (ridge) id173
       penalty to include in the cost function
     * maxm (integer) [default: 5000]: maximum bond dimension to allow
       when adaptively optimizing the mps tensors
     * minm (integer) [default: max(10,maxm/2)]: minimum bond dimension to
       allow when adaptively optimizing the mps tensors (sometimes it is
       not possible to reach the minm value if not enough non-zero
       singular values are available after the svd step)
     * cutoff (real) [default: 1e-10]: truncation error goal when
       optimizing the mps. smaller value means higher accuracy. the
       advantage of using a cutoff is that the bond dimension will
       automatically shrink when it does not need to be big, but can still
       grow where needed.
     * ntrain (integer) [default: 60000]: number of training images per
       label type to use when training. useful for speeding up the code
       for testing purposes or to study generalization / overfitting. if
       ntrain is set to a larger value than the number of training images
       available then the full set of training images will be used (so it
       is safe to do this).
     * feature (string) [default: normal]: local feature map type to use.
       "normal" means the [cos(pi/2x), sin(pi/2x)] local feature map.
       "series" uses the feature map [1,x/4] (motivated by the novikov et
       al. paper).
     * ninitial (integer) [default: 100]: number of training states per
       label type to use to make the initial mps by summing training mps
       together
     * replace (string: "yes" or "no") [default: no]: experimental feature
       which if set to "yes" will replace the new bond tensor with the old
       one if the cost function goes up when using the new bond tensor.
       this can happen if svd'ing the new bond tensor causes too big of an
       approximation and makes the cost function rise.
     * nbatch (integer) [default: 10]: number of "batches" into which to
       divide the "environment" tensors (i.e. the tensors representing
       each image projected into the "wings" of the weight mps). these
       environment tensors can take a huge amount of ram and so fixedl
       stores most of them on the hard disk (in the proj_images folder)
       and only reads them into memory in batches. by increasing the
       number of batches you can make the code read fewer environments
       into memory at a time.

   there are other input parameters of a more experimental nature, but the
   ones above are the most important.

   other code features:
     * if the code finds the file "w" (the weight mps written to disk) and
       the file "sites" it will read in the previous saved mps and use it
       as the initial value. this is extremely useful for restarting the
       code with different optimization parameters. for example, you could
       do two sweeps with maxm=10, stop the program, then do more sweeps
       with a larger maxm.
     * the code writes out a file called "sites" shortly after it begins.
       this holds what itensor calls a "siteset" which is a set of common
       reference indices to use to allow different mps tensors created to
       always share the same set of site indices.
     * if the code finds the file "write_wf" (this file can be empty:
       create it with the command touch write_wf) then after optimizing
       the current bond, the code will write the weight tensor mps to the
       file "w" (overwriting it if already present). once this happens,
       the code will delete the file "write_wf"

   tips for running fixedl:
     * getting a good initial weight mps is important before spending a
       lot of compute time optimizing over the full training set. a simple
       way to get a decent initial mps is to do some sweeps with a small
       maxm setting (maxm = 10, say) with ntrain set very low, like to
       100. then you can do some sweeps with ntrain=1000 and finally
       ntrain=10000 which uses the full training set (if ntrain is larger
       than the number of images per label, the code just includes every
       training image).
     * do some sweeps at a smaller maxm or larger cutoff, which keeps the
       typical mps bond dimension low, before doing the last few sweeps
       with a larger maxm and smaller cutoff.
     * another really excellent trick for initialization is described in
       the appendix of the novikov et al. paper (arxiv:1605.03795). train
       a linear classifer then define an mps which gives a model having
       the same output as the linear classifier model. this trick can be
       extended to the single-mps multi-task case that fixedl uses; this
       is left as an exercise to the reader but i may include a code for
       this here eventually.

single program input parameters and code features

   single optimizes an mps for a single label type, with no label index on
   the mps. this mps parameterizes a model whose output is (ideally)
   positive for inputs of the correct type, and zero for all other inputs.

   the input parameters accepted by single are mostly the same as for
   fixedl above.

   one important extra parameter needed by `single is the "label"
   parameter, which is an integer 0,1,...,9 telling the program which
   single label to "target" when optimizing the mps.

   when saving the currently optimized weight tensor mps to disk, the
   single app appends the label number which that mps is targeting. so if
   the label parameter is set to 3, the program will output the file "w03"
   (either when the program ends or the write_wf file is found).

tips for using the codes

     * using the "normal" feature map (i.e. [cos(pi/2x),sin(pi/2x)]) on
       larger image sizes can lead to a vanishing gradient problem. it's
       easy to understand why: this feature map is normalized so the
       overlap of two local feature vectors is strictly less than or equal
       to 1. to the extent that one can think of the weight mps "w" as a
       sum (or superposition) of various input training states (the
       representer theorem, which doesn't strictly apply to tensor
       networks by the way), then the projection of training vectors into
       the environment of the weight mps can lead to very small numbers.
       an interesting research direction would be to remedy this scenario
       by coming up with better initial states for w.
     * i have been finding that the single code can overfit the training
       data. this isn't too surprising since it produces a completely
       different mps for each label type, thus creates a model with many
       more parameters than the fixedl code does. it would be interesting
       to see if this overfitting can be remedied by explicit l2
       id173, or perhaps just by restricting the bond dimensions
       of the mps.

     *    2019 github, inc.
     * [97]terms
     * [98]privacy
     * [99]security
     * [100]status
     * [101]help

     * [102]contact github
     * [103]pricing
     * [104]api
     * [105]training
     * [106]blog
     * [107]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [108]reload to refresh your
   session. you signed out in another tab or window. [109]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/emstoudenmire/tnml/commits/master.atom
   3. https://github.com/emstoudenmire/tnml#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/emstoudenmire/tnml
  32. https://github.com/join
  33. https://github.com/login?return_to=/emstoudenmire/tnml
  34. https://github.com/emstoudenmire/tnml/watchers
  35. https://github.com/login?return_to=/emstoudenmire/tnml
  36. https://github.com/emstoudenmire/tnml/stargazers
  37. https://github.com/login?return_to=/emstoudenmire/tnml
  38. https://github.com/emstoudenmire/tnml/network/members
  39. https://github.com/emstoudenmire
  40. https://github.com/emstoudenmire/tnml
  41. https://github.com/emstoudenmire/tnml
  42. https://github.com/emstoudenmire/tnml/issues
  43. https://github.com/emstoudenmire/tnml/pulls
  44. https://github.com/emstoudenmire/tnml/projects
  45. https://github.com/emstoudenmire/tnml/pulse
  46. https://github.com/join?source=prompt-code
  47. http://arxiv.org/abs/1605.05775
  48. https://github.com/topics/machine-learning
  49. https://github.com/topics/tensor-networks
  50. https://github.com/topics/matrix-product-states
  51. https://github.com/topics/mps
  52. https://github.com/emstoudenmire/tnml/commits/master
  53. https://github.com/emstoudenmire/tnml/branches
  54. https://github.com/emstoudenmire/tnml/releases
  55. https://github.com/emstoudenmire/tnml/graphs/contributors
  56. https://github.com/emstoudenmire/tnml/blob/master/license
  57. https://github.com/emstoudenmire/tnml/search?l=c++
  58. https://github.com/emstoudenmire/tnml/find/master
  59. https://github.com/emstoudenmire/tnml/archive/master.zip
  60. https://github.com/login?return_to=https://github.com/emstoudenmire/tnml
  61. https://github.com/join?return_to=/emstoudenmire/tnml
  62. https://desktop.github.com/
  63. https://desktop.github.com/
  64. https://developer.apple.com/xcode/
  65. https://visualstudio.github.com/
  66. https://github.com/emstoudenmire
  67. https://github.com/emstoudenmire/tnml/commits?author=emstoudenmire
  68. https://github.com/emstoudenmire/tnml/commit/d353a0ef2b7d8c44115c395996e3b7871f947a8a
  69. https://github.com/emstoudenmire/tnml/commit/d353a0ef2b7d8c44115c395996e3b7871f947a8a
  70. https://github.com/emstoudenmire/tnml/tree/d353a0ef2b7d8c44115c395996e3b7871f947a8a
  71. https://github.com/emstoudenmire/tnml/tree/master/mllib
  72. https://github.com/emstoudenmire/tnml/tree/master/sample_inputs
  73. https://github.com/emstoudenmire/tnml/blob/master/.gitignore
  74. https://github.com/emstoudenmire/tnml/blob/master/license
  75. https://github.com/emstoudenmire/tnml/commit/b4a7ee7bb95d9065fae3f28d47399e68d1209ab0
  76. https://github.com/emstoudenmire/tnml/blob/master/makefile.sample
  77. https://github.com/emstoudenmire/tnml/blob/master/readme.md
  78. https://github.com/emstoudenmire/tnml/blob/master/fixedl.cc
  79. https://github.com/emstoudenmire/tnml/blob/master/fulltest.cc
  80. https://github.com/emstoudenmire/tnml/blob/master/image.h
  81. https://github.com/emstoudenmire/tnml/blob/master/linear.cc
  82. https://github.com/emstoudenmire/tnml/blob/master/paralleldo.h
  83. https://github.com/emstoudenmire/tnml/blob/master/separate_fulltest.cc
  84. https://github.com/emstoudenmire/tnml/blob/master/single.cc
  85. https://github.com/emstoudenmire/tnml/commit/d8d2a5db3f024be66b418aa06ded2b2ff27df2d2
  86. https://github.com/emstoudenmire/tnml/blob/master/single.h
  87. https://github.com/emstoudenmire/tnml/blob/master/util.h
  88. https://github.com/jemisjoky/torchmps
  89. http://arxiv.org/abs/1605.05775
  90. http://arxiv.org/abs/1605.03795
  91. http://itensor.org/
  92. http://github.com/itensor/itensor
  93. http://www.libpng.org/
  94. http://www.nongnu.org/pngpp/
  95. http://github.com/itensor/itensor
  96. http://itensor.org/
  97. https://github.com/site/terms
  98. https://github.com/site/privacy
  99. https://github.com/security
 100. https://githubstatus.com/
 101. https://help.github.com/
 102. https://github.com/contact
 103. https://github.com/pricing
 104. https://developer.github.com/
 105. https://training.github.com/
 106. https://github.blog/
 107. https://github.com/about
 108. https://github.com/emstoudenmire/tnml
 109. https://github.com/emstoudenmire/tnml

   hidden links:
 111. https://github.com/
 112. https://github.com/emstoudenmire/tnml
 113. https://github.com/emstoudenmire/tnml
 114. https://github.com/emstoudenmire/tnml
 115. https://help.github.com/articles/which-remote-url-should-i-use
 116. https://github.com/emstoudenmire/tnml#tensor-network-machine-learning
 117. https://github.com/emstoudenmire/tnml#code-overview
 118. https://github.com/emstoudenmire/tnml#compiling-and-running-the-programs
 119. https://github.com/emstoudenmire/tnml#input-files
 120. https://github.com/emstoudenmire/tnml#fixedl-program-input-parameters-and-code-features
 121. https://github.com/emstoudenmire/tnml#single-program-input-parameters-and-code-features
 122. https://github.com/emstoudenmire/tnml#tips-for-using-the-codes
 123. https://github.com/
