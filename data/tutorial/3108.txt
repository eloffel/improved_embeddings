  

  

rules   of   machine   learning:  
best   practices   for      ml   engineering  

martin      zinkevich  

  

this      document      is      intended      to      help      those      with      a      basic      knowledge      of      machine      learning      get      the  
benefit      of      best      practices      in      machine      learning      from      around      google.      it      presents      a      style      for      machine  
learning,      similar      to      the      google      c++      style      guide      and      other      popular      guides      to      practical  
programming.      if      you      have      taken      a      class      in      machine      learning,      or      built      or      worked      on      a  
machine  learned      model,      then      you      have      the      necessary      background      to      read      this      document.  
  

terminology  
overview  
before      machine      learning  

rule      #1:      don   t      be      afraid      to      launch      a      product      without      machine      learning.  
rule      #2:      make      metrics      design      and      implementation      a      priority.  
rule      #3:      choose      machine      learning      over      a      complex      heuristic.  

ml      phase      i:      your      first      pipeline  

rule      #4:      keep      the      first      model      simple      and      get      the      infrastructure      right.  
rule      #5:      test      the      infrastructure      independently      from      the      machine      learning.  
rule      #6:      be      careful      about      dropped      data      when      copying      pipelines.  
rule      #7:      turn      heuristics      into      features,      or      handle      them      externally.  

monitoring  

rule      #8:      know      the      freshness      requirements      of      your      system.  
rule      #9:      detect      problems      before      exporting      models.  
rule      #10:      watch      for      silent      failures.  
rule      #11:      give      feature      sets      owners      and      documentation.  

your      first      objective  

rule      #12:      don   t      overthink      which      objective      you      choose      to      directly      optimize.  
rule      #13:      choose      a      simple,      observable      and      attributable      metric      for      your      first  
objective.  
rule      #14:      starting      with      an      interpretable      model      makes      debugging      easier.  
rule      #15:      separate      spam      filtering      and      quality      ranking      in      a      policy      layer.  

ml      phase      ii:      feature      engineering  

rule      #16:      plan      to      launch      and      iterate.  
rule      #17:      start      with      directly      observed      and      reported      features      as      opposed      to      learned  
features.  

rule      #18:      explore      with      features      of      content      that      generalize      across      contexts.  
rule      #19:      use      very      specific      features      when      you      can.  
rule      #20:      combine      and      modify      existing      features      to      create      new      features      in  
human  understandable      ways.  
rule      #21:      the      number      of      feature      weights      you      can      learn      in      a      linear      model      is      roughly  
proportional      to      the      amount      of      data      you      have.  
rule      #22:      clean      up      features      you      are      no      longer      using.  

human      analysis      of      the      system  

rule      #23:      you      are      not      a      typical      end      user.  
rule      #24:      measure      the      delta      between      models.  
rule      #25:      when      choosing      models,      utilitarian      performance      trumps      predictive      power.  
rule      #26:      look      for      patterns      in      the      measured      errors,      and      create      new      features.  
rule      #27:      try      to      quantify      observed      undesirable      behavior.  
rule      #28:      be      aware      that      identical      short  term      behavior      does      not      imply      identical  
long  term      behavior.  
training  serving      skew  

rule      #29:      the      best      way      to      make      sure      that      you      train      like      you      serve      is      to      save      the      set  
of      features      used      at      serving      time,      and      then      pipe      those      features      to      a      log      to      use      them      at  
training      time.  
rule      #30:      importance      weight      sampled      data,      don   t      arbitrarily      drop      it!  
rule      #31:      beware      that      if      you      join      data      from      a      table      at      training      and      serving      time,      the  
data      in      the      table      may      change.  
rule      #32:      re  use      code      between      your      training      pipeline      and      your      serving      pipeline  
whenever      possible.  
rule      #33:      if      you      produce      a      model      based      on      the      data      until      january      5th,      test      the      model  
on      the      data      from      january      6th      and      after.  
rule      #34:      in      binary      classification      for      filtering      (such      as      spam      detection      or      determining  
interesting      e  mails),      make      small      short  term      sacrifices      in      performance      for      very      clean  
data.  
rule      #35:      beware      of      the      inherent      skew      in      ranking      problems.  
rule      #36:      avoid      feedback      loops      with      positional      features.  
rule      #37:      measure      training/serving      skew.  

ml      phase      iii:      slowed      growth,      optimization      refinement,      and      complex      models  

rule      #38:      don   t      waste      time      on      new      features      if      unaligned      objectives      have      become      the  
issue.  
rule      #39:      launch      decisions      will      depend      upon      more      than      one      metric.  
rule      #40:      keep      ensembles      simple.  
rule      #41:      when      performance      plateaus,      look      for      qualitatively      new      sources      of  
information      to      add      rather      than      refining      existing      signals.  
rule      #42:      don   t      expect      diversity,      personalization,      or      relevance      to      be      as      correlated  
with      popularity      as      you      think      they      are.  
rule      #43:      your      friends      tend      to      be      the      same      across      different      products.      your      interests  
tend      not      to      be.  

related      work  
acknowledgements  
appendix  

youtube      overview  
google      play      overview  
google      plus      overview  

  

terminology  
  
the      following      terms      will      come      up      repeatedly      in      our      discussion      of      effective      machine      learning:  
  
instance  :      the      thing      about      which      you      want      to      make      a      prediction.      for      example,      the      instance  
might      be      a      web      page      that      you      want      to      classify      as      either      "about      cats"      or      "not      about      cats".  
label  :      an      answer      for      a      prediction      task                either      the      answer      produced      by      a      machine      learning  
system,      or      the      right      answer      supplied      in      training      data.      for      example,      the      label      for      a      web      page  
might      be      "about      cats".  
feature  :      a      property      of      an      instance      used      in      a      prediction      task.      for      example,      a      web      page      might  
have      a      feature      "contains      the      word      'cat'".  
feature      column   :      a      set      of      related      features,      such      as      the      set      of      all      possible      countries      in      which  
users      might      live.      an      example      may      have      one      or      more      features      present      in      a      feature      column.      a  
feature      column      is      referred      to      as      a         namespace         in      the      vw      system      (at      yahoo/microsoft),      or      a  
field  .  
example  :      an      instance      (with      its      features)      and      a      label.    
model  :      a      statistical      representation      of      a      prediction      task.      you        train        a      model      on      examples      then      use  
the      model      to      make      predictions.  
metric  :      a      number      that      you      care      about.      may      or      may      not      be      directly      optimized.  
objective  :                a      metric      that      your      algorithm      is      trying      to      optimize.  
pipeline  :      the      infrastructure      surrounding      a      machine      learning      algorithm.      includes      gathering      the  
data      from      the      front      end,      putting      it      into      training      data      files,      training      one      or      more      models,      and  
exporting      the      models      to      production.    
  
  

1

overview  
to      make      great      products:  
do      machine      learning      like      the      great      engineer      you      are,      not      like      the      great      machine      learning  
expert      you      aren   t.  

1      google  specific      terminology.  

most      of      the      problems      you      will      face      are,      in      fact,      engineering      problems.      even      with      all      the  
resources      of      a      great      machine      learning      expert,      most      of      the      gains      come      from      great      features,      not  
great      machine      learning      algorithms.      so,      the      basic      approach      is:  

1. make      sure      your      pipeline      is      solid      end      to      end  
2. start      with      a      reasonable      objective  
3. add      common  sense      features      in      a      simple      way  
4. make      sure      that      your      pipeline      stays      solid.    

this      approach      will      make      lots      of      money      and/or      make      lots      of      people      happy      for      a      long      period      of  
time.      diverge      from      this      approach      only      when      there      are      no      more      simple      tricks      to      get      you      any  
farther.      adding      complexity      slows      future      releases.  
  
once      you've      exhausted      the      simple      tricks,      cutting  edge      machine      learning      might      indeed      be      in      your  
future.      see      the      section      on        phase      iii        machine      learning      projects.    
  
this      document      is      arranged      in      four      parts:  

1. the      first      part        should      help      you      understand      whether      the      time      is      right      for      building      a      machine  

learning      system.    

2. the      second      part        is      about      deploying      your      first      pipeline.    
3. the      third      part        is      about      launching      and      iterating      while      adding      new      features      to      your      pipeline,  

how      to      evaluate      models      and      training  serving      skew.    

4. the      final      part        is      about      what      to      do      when      you      reach      a      plateau.    
5. afterwards,      there      is      a      list      of        related      work        and      an        appendix        with      some      background      on      the  

systems      commonly      used      as      examples      in      this      document.  

before      machine      learning  

rule      #1:      don   t      be      afraid      to      launch      a      product      without      machine      learning.    
machine      learning      is      cool,      but      it      requires      data.      theoretically,      you      can      take      data      from      a      different  
problem      and      then      tweak      the      model      for      a      new      product,      but      this      will      likely      underperform      basic  
heuristics.      if      you      think      that      machine      learning      will      give      you      a      100%      boost,      then      a      heuristic      will      get  
you      50%      of      the      way      there.  
  
for      instance,      if      you      are      ranking      apps      in      an      app      marketplace,      you      could      use      the      install      rate      or  
number      of      installs.      if      you      are      detecting      spam,      filter      out      publishers      that      have      sent      spam      before.  
don   t      be      afraid      to      use      human      editing      either.      if      you      need      to      rank      contacts,      rank      the      most      recently  
used      highest      (or      even      rank      alphabetically).      if      machine      learning      is      not      absolutely      required      for      your  
product,      don't      use      it      until      you      have      data.  
  
  
  

rule      #2:      first,      design      and      implement      metrics.  
before      formalizing      what      your      machine      learning      system      will      do,      track      as      much      as      possible      in      your  
current      system.      do      this      for      the      following      reasons:  
  

1.
2.

3.

it      is      easier      to      gain      permission      from      the      system   s      users      earlier      on.  
if      you      think      that      something      might      be      a      concern      in      the      future,      it      is      better      to      get      historical  
data      now.  
if      you      design      your      system      with      metric      instrumentation      in      mind,      things      will      go      better      for  
you      in      the      future.      specifically,      you      don   t      want      to      find      yourself      grepping      for      strings      in      logs  
to      instrument      your      metrics!  

4. you      will      notice      what      things      change      and      what      stays      the      same.      for      instance,      suppose      you  
want      to      directly      optimize      one  day      active      users.      however,      during      your      early      manipulations  
of      the      system,      you      may      notice      that      dramatic      alterations      of      the      user      experience      don   t  
noticeably      change      this      metric.  

  
google      plus        team      measures      expands      per      read,      reshares      per      read,      plus  ones      per      read,  
comments/read,      comments      per      user,      reshares      per      user,      etc.      which      they      use      in      computing      the  
goodness      of      a      post      at      serving      time.        also,      note      that      an      experiment      framework,      where      you  
can      group      users      into      buckets      and      aggregate      statistics      by      experiment,      is      important  .      see  
rule      #12  .  
  
by      being      more      liberal      about      gathering      metrics,      you      can      gain      a      broader      picture      of      your      system.  
notice      a      problem?      add      a      metric      to      track      it!      excited      about      some      quantitative      change      on      the      last  
release?      add      a      metric      to      track      it!  
  
rule      #3:      choose      machine      learning      over      a      complex      heuristic.  
a      simple      heuristic      can      get      your      product      out      the      door.      a      complex      heuristic      is      unmaintainable.  
once      you      have      data      and      a      basic      idea      of      what      you      are      trying      to      accomplish,      move      on      to      machine  
learning.      as      in      most      software      engineering      tasks,      you      will      want      to      be      constantly      updating      your  
approach,      whether      it      is      a      heuristic      or      a      machine  learned      model,      and      you      will      find      that      the  
machine  learned      model      is      easier      to      update      and      maintain      (see        rule      #16  ).  

ml      phase      i:      your      first      pipeline  

focus      on      your      system      infrastructure      for      your      first      pipeline.      while      it      is      fun      to      think      about      all      the  
imaginative      machine      learning      you      are      going      to      do,      it      will      be      hard      to      figure      out      what      is      happening  
if      you      don   t      first      trust      your      pipeline.    
  
rule      #4:      keep      the      first      model      simple      and      get      the      infrastructure      right.    
the      first      model      provides      the      biggest      boost      to      your      product,      so      it      doesn't      need      to      be      fancy.      but  
you      will      run      into      many      more      infrastructure      issues      than      you      expect.      before      anyone      can      use      your  
fancy      new      machine      learning      system,      you      have      to      determine:  

1. how      to      get      examples      to      your      learning      algorithm.  
2. a      first      cut      as      to      what         good         and         bad         mean      to      your      system.  
3. how      to      integrate      your      model      into      your      application.      you      can      either      apply      the      model      live,      or  
pre  compute      the      model      on      examples      offline      and      store      the      results      in      a      table.      for      example,  
you      might      want      to      pre  classify      web      pages      and      store      the      results      in      a      table,      but      you      might  
want      to      classify      chat      messages      live.  

  
choosing      simple      features      makes      it      easier      to      ensure      that:  

1. the      features      reach      your      learning      algorithm      correctly.  
2. the      model      learns      reasonable      weights.  
3. the      features      reach      your      model      in      the      server      correctly.  

once      you      have      a      system      that      does      these      three      things      reliably,      you      have      done      most      of      the      work.  
your      simple      model      provides      you      with      baseline      metrics      and      a      baseline      behavior      that      you      can      use  
to      test      more      complex      models.      some      teams      aim      for      a         neutral         first      launch:      a      first      launch      that  
explicitly      de  prioritizes      machine      learning      gains,      to      avoid      getting      distracted.  
  
rule      #5:      test      the      infrastructure      independently      from      the      machine      learning.    
make      sure      that      the      infrastructure      is      testable,      and      that      the      learning      parts      of      the      system      are  
encapsulated      so      that      you      can      test      everything      around      it.      specifically:  

1. test      getting      data      into      the      algorithm.      check      that      feature      columns      that      should      be      populated  

are      populated.      where      privacy      permits,      manually      inspect      the      input      to      your      training  
algorithm.      if      possible,      check      statistics      in      your      pipeline      in      comparison      to      elsewhere,      such  
as      rasta.  

2. test      getting      models      out      of      the      training      algorithm.      make      sure      that      the      model      in      your  

training      environment      gives      the      same      score      as      the      model      in      your      serving      environment  
(see        rule      #37  ).  

  
machine      learning      has      an      element      of      unpredictability,      so      make      sure      that      you      have      tests      for      the  
code      for      creating      examples      in      training      and      serving,      and      that      you      can      load      and      use      a      fixed      model  
during      serving.      also,      it      is      important      to      understand      your      data:      see        practical      advice      for      analysis      of  
large,      complex      data      sets  .  
rule      #6:      be      careful      about      dropped      data      when      copying      pipelines.    
often      we      create      a      pipeline      by      copying      an      existing      pipeline      (i.e.      cargo      cult      programming),      and      the  
old      pipeline      drops      data      that      we      need      for      the      new      pipeline.      for      example,      the      pipeline      for        google  
plus        what   s      hot      drops      older      posts      (because      it      is      trying      to      rank      fresh      posts).      this      pipeline      was  
copied      to      use      for        google      plus        stream,      where      older      posts      are      still      meaningful,      but      the      pipeline  
was      still      dropping      old      posts.      another      common      pattern      is      to      only      log      data      that      was      seen      by      the  
user.      thus,      this      data      is      useless      if      we      want      to      model      why      a      particular      post      was      not      seen      by      the  
user,      because      all      the      negative      examples      have      been      dropped.      a      similar      issue      occurred      in      play.  
while      working      on      play      apps      home,      a      new      pipeline      was      created      that      also      contained      examples  
from      two      other      landing      pages      (play      games      home      and      play      home      home)      without      any      feature      to  
disambiguate      where      each      example      came      from.     
  

  
rule      #7:      turn      heuristics      into      features,      or      handle      them      externally.    
usually      the      problems      that      machine      learning      is      trying      to      solve      are      not      completely      new.      there      is  
an      existing      system      for      ranking,      or      classifying,      or      whatever      problem      you      are      trying      to      solve.      this  
means      that      there      are      a      bunch      of      rules      and      heuristics.        these      same      heuristics      can      give      you      a  
lift      when      tweaked      with      machine      learning  .      your      heuristics      should      be      mined      for      whatever  
information      they      have,      for      two      reasons.      first,      the      transition      to      a      machine      learned      system      will      be  
smoother.      second,      usually      those      rules      contain      a      lot      of      the      intuition      about      the      system      you      don   t  
want      to      throw      away.      there      are      four      ways      you      can      use      an      existing      heuristic:  

1. preprocess      using      the      heuristic  .      if      the      feature      is      incredibly      awesome,      then      this      is      an  

option.      for      example,      if,      in      a      spam      filter,      the      sender      has      already      been      blacklisted,      don   t      try  
to      relearn      what         blacklisted         means.      block      the      message.      this      approach      makes      the      most  
sense      in      binary      classification      tasks.  

2. create      a      feature  .      directly      creating      a      feature      from      the      heuristic      is      great.      for      example,      if  
you      use      a      heuristic      to      compute      a      relevance      score      for      a      query      result,      you      can      include      the  
score      as      the      value      of      a      feature.      later      on      you      may      want      to      use      machine      learning  
techniques      to      massage      the      value      (for      example,      converting      the      value      into      one      of      a      finite  
set      of      discrete      values,      or      combining      it      with      other      features)      but      start      by      using      the      raw  
value      produced      by      the      heuristic.  

3. mine      the      raw      inputs      of      the      heuristic  .      if      there      is      a      heuristic      for      apps      that      combines      the  
number      of      installs,      the      number      of      characters      in      the      text,      and      the      day      of      the      week,      then  
consider      pulling      these      pieces      apart,      and      feeding      these      inputs      into      the      learning  
separately.      some      techniques      that      apply      to      ensembles      apply      here      (  see      rule      #40  ).  

4. modify      the      label.        this      is      an      option      when      you      feel      that      the      heuristic      captures      information  

not      currently      contained      in      the      label.      for      example,      if      you      are      trying      to      maximize      the  
number      of      downloads,      but      you      also      want      quality      content,      then      maybe      the      solution      is      to  
multiply      the      label      by      the      average      number      of      stars      the      app      received.      there      is      a      lot      of  
space      here      for      leeway.      see      the      section      on         your      first      objective   .  

do      be      mindful      of      the      added      complexity      when      using      heuristics      in      an      ml      system.            using      old  
heuristics      in      your      new      machine      learning      algorithm      can      help      to      create      a      smooth      transition,      but  
think      about      whether      there      is      a      simpler      way      to      accomplish      the      same      effect.  
  

monitoring  

in      general,      practice      good      alerting      hygiene,      such      as      making      alerts      actionable      and      having      a  
dashboard      page.  
  
rule      #8:      know      the      freshness      requirements      of      your      system.  
how      much      does      performance      degrade      if      you      have      a      model      that      is      a      day      old?      a      week      old?      a  
quarter      old?      this      information      can      help      you      to      understand      the      priorities      of      your      monitoring.      if      you  
lose      10%      of      your      revenue      if      the      model      is      not      updated      for      a      day,      it      makes      sense      to      have      an  
engineer      watching      it      continuously.      most      ad      serving      systems      have      new      advertisements      to      handle  

every      day,      and      must      update      daily.      for      instance,      if      the      ml      model      for              google      play      search          is      not  
updated  ,      it      can      have      an      impact      on      revenue            in      under      a      month.      some      models      for      what   s      hot      in  
google      plus        have      no      post      identifier      in      their      model      so      they      can      export      these      models      infrequently.  
other      models      that      have      post      identifiers      are      updated      much      more      frequently.      also      notice      that  
freshness      can      change      over      time,      especially      when      feature      columns      are      added      or      removed      from  
your      model.  
  
rule      #9:      detect      problems      before      exporting      models.  
many      machine      learning      systems      have      a      stage      where      you      export      the      model      to      serving.      if      there      is  
an      issue      with      an      exported      model,      it      is      a      user  facing      issue.      if      there      is      an      issue      before,      then      it      is      a  
training      issue,      and      users      will      not      notice.  
  
do      sanity      checks      right      before      you      export      the      model.      specifically,      make      sure      that      the      model   s  
performance      is      reasonable      on      held      out      data.      or,      if      you      have      lingering      concerns      with      the      data,  
don   t      export      a      model.      many      teams      continuously      deploying      models      check      the        area      under      the  
roc      curve        (or      auc)      before      exporting.        issues      about      models      that      haven   t      been      exported  
require      an      e  mail      alert,      but      issues      on      a      user  facing      model      may      require      a      page.        so      better      to  
wait      and      be      sure      before      impacting      users.  
  
rule      #10:      watch      for      silent      failures.  
this      is      a      problem      that      occurs      more      for      machine      learning      systems      than      for      other      kinds      of  
systems.      suppose      that      a      particular      table      that      is      being      joined      is      no      longer      being      updated.      the  
machine      learning      system      will      adjust,      and      behavior      will      continue      to      be      reasonably      good,      decaying  
gradually.      sometimes      tables      are      found      that      were      months      out      of      date,      and      a      simple      refresh  
improved      performance      more      than      any      other      launch      that      quarter!      for      example,      the      coverage      of      a  
feature      may      change      due      to      implementation      changes:      for      example      a      feature      column      could      be  
populated      in      90%      of      the      examples,      and      suddenly      drop      to      60%      of      the      examples.      play      once      had      a  
table      that      was      stale      for      6      months,      and      refreshing      the      table      alone      gave      a      boost      of      2%      in      install  
rate.      if      you      track      statistics      of      the      data,      as      well      as      manually      inspect      the      data      on      occassion,      you  
can      reduce      these      kinds      of      failures.  
  
rule      #11:      give      feature      column      owners      and      documentation.  
if      the      system      is      large,      and      there      are      many      feature      columns,      know      who      created      or      is      maintaining  
each      feature      column.      if      you      find      that      the      person      who      understands      a      feature      column      is      leaving,  
make      sure      that      someone      has      the      information.      although      many      feature      columns      have      descriptive  
names,      it's      good      to      have      a      more      detailed      description      of      what      the      feature      is,      where      it      came      from,  
and      how      it      is      expected      to      help.  

your      first      objective  

you      have      many      metrics,      or      measurements      about      the      system      that      you      care      about,      but      your  
machine      learning      algorithm      will      often      require      a      single        objective,      a      number      that      your      algorithm  

is         trying         to      optimize.        i      distinguish      here      between      objectives      and      metrics:        a      metric      is      any  
number      that      your      system      reports  ,      which      may      or      may      not      be      important.      see      also        rule      #2  .  
  
rule      #12:      don   t      overthink      which      objective      you      choose      to      directly      optimize.    
you      want      to      make      money,      make      your      users      happy,      and      make      the      world      a      better      place.      there      are  
tons      of      metrics      that      you      care      about,      and      you      should      measure      them      all      (see        rule      #2  ).      however,  
early      in      the      machine      learning      process,      you      will      notice      them      all      going      up,      even      those      that      you      do  
not      directly      optimize.      for      instance,      suppose      you      care      about      number      of      clicks,      time      spent      on      the  
site,      and      daily      active      users.      if      you      optimize      for      number      of      clicks,      you      are      likely      to      see      the      time  
spent      increase.    
so,      keep      it      simple      and      don   t      think      too      hard      about      balancing      different      metrics      when      you      can      still  
easily      increase      all      the      metrics.      don   t      take      this      rule      too      far      though:      do      not      confuse      your      objective  
with      the      ultimate      health      of      the      system      (see        rule      #39  ).      and,        if      you      find      yourself      increasing      the  
directly      optimized      metric,      but      deciding      not      to      launch,      some      objective      revision      may      be  
required.  
  
rule      #13:      choose      a      simple,      observable      and      attributable      metric      for      your      first      objective.    
often      you      don't      know      what      the      true      objective      is.      you      think      you      do      but      then      you      as      you      stare      at  
the      data      and      side  by  side      analysis      of      your      old      system      and      new      ml      system,      you      realize      you      want  
to      tweak      it.      further,      different      team      members      often      can't      agree      on      the      true      objective.        the      ml  
objective      should      be      something      that      is      easy      to      measure      and      is      a      proxy      for      the         true     
objective   .      so      train      on      the      simple      ml      objective,      and      consider      having      a      "policy      layer"      on      top      that  
allows      you      to      add      additional      logic      (hopefully      very      simple      logic)      to      do      the      final      ranking.  
  
the      easiest      thing      to      model      is      a      user      behavior      that      is      directly      observed      and      attributable      to      an  
action      of      the      system:  

2

1. was      this      ranked      link      clicked?  
2. was      this      ranked      object      downloaded?  
3. was      this      ranked      object      forwarded/replied      to/e  mailed?  
4. was      this      ranked      object      rated?  
5. was      this      shown      object      marked      as      spam/pornography/offensive?  

avoid      modeling      indirect      effects      at      first:  
1. did      the      user      visit      the      next      day?  
2. how      long      did      the      user      visit      the      site?  
3. what      were      the      daily      active      users?  

indirect      effects      make      great      metrics,      and      can      be      used      during      a/b      testing      and      during      launch  
decisions.  
finally,      don   t      try      to      get      the      machine      learning      to      figure      out:  

1.
2.
3.

is      the      user      happy      using      the      product?  
is      the      user      satisfied      with      the      experience?  
is      the      product      improving      the      user   s      overall      well  being?  

2      there      is      often      no         true         objective.      see        rule      #39  .  

4. how      will      this      affect      the      company   s      overall      health?  

3

these      are      all      important,      but      also      incredibly      hard.      instead,      use      proxies:      if      the      user      is      happy,      they  
will      stay      on      the      site      longer.      if      the      user      is      satisfied,      they      will      visit      again      tomorrow.      insofar      as  
well  being      and      company      health      is      concerned,      human      judgement      is      required      to      connect      any  
machine      learned      objective      to      the      nature      of      the      product      you      are      selling      and      your      business      plan,  
so      we      don   t      end      up        here  .    
  
rule      #14:      starting      with      an      interpretable      model      makes      debugging      easier.  
linear      regression,      logistic      regression,      and      poisson      regression      are      directly      motivated      by      a  
probabilistic      model.      each      prediction      is      interpretable      as      a      id203      or      an      expected      value.      this  
makes      them      easier      to      debug      than      models      that      use      objectives      (zero  one      loss,      various      hinge  
losses,      et      cetera)      that      try      to      directly      optimize      classification      accuracy      or      ranking      performance.      for  
example,      if      probabilities      in      training      deviate      from      probabilities      predicted      in      side  by  sides      or      by  
inspecting      the      production      system,      this      deviation      could      reveal      a      problem  .    
  
for      example,      in      linear,      logistic,      or      poisson      regression,        there      are      subsets      of      the      data      where      the  
average      predicted      expectation      equals      the      average      label      (1  moment      calibrated,      or      just  
calibrated)   .      if      you      have      a      feature      which      is      either      1      or      0      for      each      example,      then      the      set      of  
examples      where      that      feature      is      1      is      calibrated.      also,      if      you      have      a      feature      that      is      1      for      every  
example,      then      the      set      of      all      examples      is      calibrated.  
  
with      simple      models,      it      is      easier      to      deal      with      feedback      loops      (see        rule      #36  ).  
often,      we      use      these      probabilistic      predictions      to      make      a      decision:      e.g.      rank      posts      in      decreasing  
expected      value      (i.e.      id203      of      click/download/etc.).        however,      remember      when      it      comes  
time      to      choose      which      model      to      use,      the      decision      matters      more      than      the      likelihood      of      the  
data      given      the      model      (see        rule      #27  )  .  
  
rule      #15:      separate      spam      filtering      and      quality      ranking      in      a      policy      layer.    
quality      ranking      is      a      fine      art,      but      spam      filtering      is      a      war.      the      signals      that      you      use      to      determine  
high      quality      posts      will      become      obvious      to      those      who      use      your      system,      and      they      will      tweak      their  
posts      to      have      these      properties.      thus,      your      quality      ranking      should      focus      on      ranking      content      that  
is      posted      in      good      faith.      you      should      not      discount      the      quality      ranking      learner      for      ranking      spam  
highly.        similarly,         racy         content      should      be      handled      separately      from      quality      ranking  .  
spam      filtering      is      a      different      story.      you      have      to      expect      that      the      features      that      you      need      to      generate  
will      be      constantly      changing.      often,      there      will      be      obvious      rules      that      you      put      into      the      system      (if      a  
post      has      more      than      three      spam      votes,      don   t      retrieve      it,      et      cetera).      any      learned      model      will      have      to  
be      updated      daily,      if      not      faster.      the      reputation      of      the      creator      of      the      content      will      play      a      great      role.  
  
at      some      level,      the      output      of      these      two      systems      will      have      to      be      integrated.      keep      in      mind,      filtering  
spam      in      search      results      should      probably      be      more      aggressive      than      filtering      spam      in      email  

3      this      is      true      assuming      that      you      have      no      id173      and      that      your      algorithm      has      converged.      it      is  
approximately      true      in      general.  

messages.      also,      it      is      a      standard      practice      to      remove      spam      from      the      training      data      for      the      quality  
classifier.  
  
  
  

ml      phase      ii:      feature      engineering  

in      the      first      phase      of      the      lifecycle      of      a      machine      learning      system,      the      important      issue      is      to      get      the  
training      data      into      the      learning      system,      get      any      metrics      of      interest      instrumented,      and      create      a  
serving      infrastructure.        after      you      have      a      working      end      to      end      system      with      unit      and      system  
tests      instrumented,      phase      ii      begins  .  
  
in      the      second      phase,      there      is      a      lot      of      low  hanging      fruit.      there      are      a      variety      of      obvious      features  
that      could      be      pulled      into      the      system.      thus,      the      second      phase      of      machine      learning      involves  
pulling      in      as      many      features      as      possible      and      combining      them      in      intuitive      ways.      during      this      phase,  
all      of      the      metrics      should      still      be      rising.      there      will      be      lots      of      launches,      and      it      is      a      great      time      to      pull  
in      lots      of      engineers      that      can      join      up      all      the      data      that      you      need      to      create      a      truly      awesome      learning  
system.    
  
rule      #16:      plan      to      launch      and      iterate.  
don   t      expect      that      the      model      you      are      working      on      now      will      be      the      last      one      that      you      will      launch,      or  
even      that      you      will      ever      stop      launching      models.      thus      consider      whether      the      complexity      you      are  
adding      with      this      launch      will      slow      down      future      launches.      many      teams      have      launched      a      model      per  
quarter      or      more      for      years.      there      are      three      basic      reasons      to      launch      new      models:  

1. you      are      coming      up      with      new      features,  
2. you      are      tuning      id173      and      combining      old      features      in      new      ways,      and/or  
3. you      are      tuning      the      objective.  

  
regardless,      giving      a      model      a      bit      of      love      can      be      good:      looking      over      the      data      feeding      into      the  
example      can      help      find      new      signals      as      well      as      old,      broken      ones.      so,      as      you      build      your      model,  
think      about      how      easy      it      is      to      add      or      remove      or      recombine      features.      think      about      how      easy      it      is      to  
create      a      fresh      copy      of      the      pipeline      and      verify      its      correctness.      think      about      whether      it      is      possible  
to      have      two      or      three      copies      running      in      parallel.      finally,      don   t      worry      about      whether      feature      16      of  
35      makes      it      into      this      version      of      the      pipeline.      you   ll      get      it      next      quarter.  
  
rule      #17:      start      with      directly      observed      and      reported      features      as      opposed      to      learned  
features.    
this      might      be      a      controversial      point,      but      it      avoids      a      lot      of      pitfalls.      first      of      all,      let   s      describe      what      a  
learned      feature      is.      a      learned      feature      is      a      feature      generated      either      by      an      external      system      (such  
as      an      unsupervised      id91      system)      or      by      the      learner      itself      (e.g.      via      a      factored      model      or      deep  

learning).      both      of      these      can      be      useful,      but      they      can      have      a      lot      of      issues,      so      they      should      not      be      in  
the      first      model.    
  
if      you      use      an      external      system      to      create      a      feature,      remember      that      the      system      has      its      own  
objective.      the      external      system's      objective            may      be      only      weakly      correlated      with      your      current  
objective.      if      you      grab      a      snapshot      of      the      external      system,      then      it      can      become      out      of      date.      if      you  
update      the      features      from      the      external      system,      then      the      meanings      may      change.      if      you      use      an  
external      system      to      provide      a      feature,      be      aware      that            they      require      a      great      deal      of      care.    
  
the      primary      issue      with      factored      models      and      deep      models      is      that      they      are      non  convex.      thus,  
there      is      no      guarantee      that      an      optimal      solution      can      be      approximated      or      found,      and      the      local  
minima      found      on      each      iteration      can      be      different.      this      variation      makes      it      hard      to      judge      whether  
the      impact      of      a      change      to      your      system      is      meaningful      or      random.      by      creating      a      model      without  
deep      features,      you      can      get      an      excellent      baseline      performance.      after      this      baseline      is      achieved,  
you      can      try      more      esoteric      approaches.  
  
rule      #18:      explore      with      features      of      content      that      generalize      across      contexts.  
often      a      machine      learning      system      is      a      small      part      of      a      much      bigger      picture.      for      example,      if      you  
imagine      a      post      that      might      be      used      in      what   s      hot,      many      people      will      plus  one,      re  share,      or  
comment      on      a      post      before      it      is      ever      shown      in      what   s      hot.      if      you      provide      those      statistics      to      the  
learner,      it      can      promote      new      posts      that      it      has      no      data      for      in      the      context      it      is      optimizing.        youtube  
watch      next      could      use      number      of      watches,      or      co  watches      (counts      of      how      many      times      one      video  
was      watched      after      another      was      watched)      from        youtube        search.      you      can      also      use      explicit      user  
ratings.      finally,      if      you      have      a      user      action      that      you      are      using      as      a      label,      seeing      that      action      on      the  
document      in      a      different      context      can      be      a      great      feature.      all      of      these      features      allow      you      to      bring  
new      content      into      the      context.      note      that      this      is      not      about      personalization:      figure      out      if      someone  
likes      the      content      in      this      context      first,      then      figure      out      who      likes      it      more      or      less.  
  
rule      #19:      use      very      specific      features      when      you      can.    
with      tons      of      data,      it      is      simpler      to      learn      millions      of      simple      features      than      a      few      complex      features.  
identifiers      of      documents      being      retrieved      and      canonicalized      queries      do      not      provide      much  
generalization,      but      align      your      ranking      with      your      labels      on      head      queries..      thus,      don   t      be      afraid      of  
groups      of      features      where      each      feature      applies      to      a      very      small      fraction      of      your      data,      but      overall  
coverage      is      above      90%.      you      can      use      id173      to      eliminate      the      features      that      apply      to      too  
few      examples.  
  
rule      #20:      combine      and      modify      existing      features      to      create      new      features      in  
human  understandable      ways.    
there      are      a      variety      of      ways      to      combine      and      modify      features.      machine      learning      systems      such      as  
tensorflow      allow      you      to      pre  process      your      data      through        transformations  .      the      two      most      standard  
approaches      are         discretizations         and         crosses         .  
  

f

c

m

   {

us, anada,

discretization      consists      of      taking      a      continuous      feature      and      creating      many      discrete      features      from  
it.      consider      a      continuous      feature      such      as      age.      you      can      create      a      feature      which      is      1      when      age      is  
less      than      18,      another      feature      which      is      1      when      age      is      between      18      and      35,      et      cetera.      don   t  
overthink      the      boundaries      of      these      histograms:      basic      quantiles      will      give      you      most      of      the      impact.  
  
crosses      combine      two      or      more      feature      columns.      a      feature      column,      in      tensorflow's      terminology,  
is      a      set      of      homogenous      features,      (e.g.      {male,      female},      {us,      canada,      mexico},      et      cetera).      a      cross  
male, emale}
{
exico}  
is      a      new      feature      column      with      features      in,      for      example,      
.
  
this      new      feature      column      will      contain      the      feature      (male,      canada).      if      you      are      using      tensorflow  
and      you      tell      tensorflow      to      create      this      cross      for      you,      this      (male,      canada)      feature      will      be      present  
in      examples      representing      male      canadians.      note      that      it      takes      massive      amounts      of      data      to      learn  
models      with      crosses      of      three,      four,      or      more      base      feature      columns.  
  
crosses      that      produce      very      large      feature      columns      may      overfit.      for      instance,      imagine      that      you      are  
doing      some      sort      of      search,      and      you      have      a      feature      column      with      words      in      the      query,      and      you  
have      a      feature      column      with      words      in      the      document.      you      can      combine      these      with      a      cross,      but  
you      will      end      up      with      a      lot      of      features      (see        rule      #21  ).      when      working      with      text      there      are      two  
alternatives.      the      most      draconian      is      a      dot      product.      a      dot      product      in      its      simplest      form      simply  
counts      the      number      of      common      words      between      the      query      and      the      document.      this      feature      can  
then      be      discretized.      another      approach      is      an      intersection:      thus,      we      will      have      a      feature      which      is  
present      if      and      only      if      the      word         pony         is      in      the      document      and      the      query,      and      another      feature  
which      is      present      if      and      only      if      the      word         the         is      in      the      document      and      the      query.    
  
rule      #21:      the      number      of      feature      weights      you      can      learn      in      a      linear      model      is      roughly  
proportional      to      the      amount      of      data      you      have.  
there      are      fascinating      statistical      learning      theory      results      concerning      the      appropriate      level      of  
complexity      for      a      model,      but      this      rule      is      basically      all      you      need      to      know.      i      have      had      conversations  
in      which      people      were      doubtful      that      anything      can      be      learned      from      one      thousand      examples,      or  
that      you      would      ever      need      more      than      1      million      examples,      because      they      get      stuck      in      a      certain  
method      of      learning.      the      key      is      to      scale      your      learning      to      the      size      of      your      data:  

1.

2.

3.

if      you      are      working      on      a      search      ranking      system,      and      there      are      millions      of      different      words  
in      the      documents      and      the      query      and      you      have      1000      labeled      examples,      then      you      should  
use      a      dot      product      between      document      and      query      features,        tf  idf  ,      and      a      half  dozen  
other      highly      human  engineered      features.      1000      examples,      a      dozen      features.  
if      you      have      a      million      examples,      then      intersect      the      document      and      query      feature      columns,  
using      id173      and      possibly      feature      selection.      this      will      give      you      millions      of      features,  
but      with      id173      you      will      have      fewer.      ten      million      examples,      maybe      a      hundred  
thousand      features.    
if      you      have      billions      or      hundreds      of      billions      of      examples,      you      can      cross      the      feature  
columns      with      document      and      query      tokens,      using      feature      selection      and      id173.  
you      will      have      a      billion      examples,      and      10      million      features.  

statistical      learning      theory      rarely      gives      tight      bounds,      but      gives      great      guidance      for      a      starting      point.  
in      the      end,      use        rule      #28        to      decide      what      features      to      use.  

  
rule      #22:      clean      up      features      you      are      no      longer      using.    
unused      features      create      technical      debt.      if      you      find      that      you      are      not      using      a      feature,      and      that  
combining      it      with      other      features      is      not      working,      then      drop      it      out      of      your      infrastructure.      you      want  
to      keep      your      infrastructure      clean      so      that      the      most      promising      features      can      be      tried      as      fast      as  
possible.      if      necessary,      someone      can      always      add      back      your      feature.  
  
  
keep      coverage      in      mind      when      considering      what      features      to      add      or      keep.      how      many      examples  
are      covered      by      the      feature?      for      example,      if      you      have      some      personalization      features,      but      only  
8%      of      your      users      have      any      personalization      features,      it      is      not      going      to      be      very      effective.    
  
at      the      same      time,      some      features      may      punch      above      their      weight.      for      example,      if      you      have      a  
feature      which      covers      only      1%      of      the      data,      but      90%      of      the      examples      that      have      the      feature      are  
positive,      then      it      will      be      a      great      feature      to      add.  

human      analysis      of      the      system  

before      going      on      to      the      third      phase      of      machine      learning,      it      is      important      to      focus      on      something      that  
is      not      taught      in      any      machine      learning      class:      how      to      look      at      an      existing      model,      and      improve      it.  
this      is      more      of      an      art      than      a      science,      and      yet      there      are      several      anti  patterns      that      it      helps      to  
avoid.  
  
rule      #23:      you      are      not      a      typical      end      user.    
this      is      perhaps      the      easiest      way      for      a      team      to      get      bogged      down.      while      there      are      a      lot      of      benefits  
to      fishfooding      (using      a      prototype      within      your      team)      and      dogfooding      (using      a      prototype      within  
your      company),      employees      should      look      at      whether      the      performance      is      correct.      while      a      change  
which      is      obviously      bad      should      not      be      used,      anything      that      looks      reasonably      near      production  
should      be      tested      further,      either      by      paying      laypeople      to      answer      questions      on      a      id104  
platform,      or      through      a      live      experiment      on      real      users.  
  
there      are      two      reasons      for      this.      the      first      is      that      you      are      too      close      to      the      code.      you      may      be  
looking      for      a      particular      aspect      of      the      posts,      or      you      are      simply      too      emotionally      involved      (e.g.  
confirmation      bias).      the      second      is      that      your      time      is      too      valuable.      consider      the      cost      of      9  
engineers      sitting      in      a      one      hour      meeting,      and      think      of      how      many      contracted      human      labels      that  
buys      on      a      id104      platform.  
  
if      you      really      want      to      have      user      feedback,        use      user      experience      methodologies  .      create      user  
personas      (one      description      is      in      bill      buxton   s        designing      user      experiences  )      early      in      a      process      and  
do      usability      testing      (one      description      is      in      steve      krug   s        don   t      make      me      think  )      later.      user  
personas      involve      creating      a      hypothetical      user.      for      instance,      if      your      team      is      all      male,      it      might      help  
to      design      a      35  year      old      female      user      persona      (complete      with      user      features),      and      look      at      the  
results      it      generates      rather      than      10      results      for      25  40      year      old      males.      bringing      in      actual      people      to  

watch      their      reaction      to      your      site      (locally      or      remotely)      in      usability      testing      can      also      get      you      a      fresh  
perspective.  
  
rule      #24:      measure      the      delta      between      models.    
one      of      the      easiest,      and      sometimes      most      useful      measurements      you      can      make      before      any      users  
have      looked      at      your      new      model      is      to      calculate      just      how      different      the      new      results      are      from  
production.      for      instance,      if      you      have      a      ranking      problem,      run      both      models      on      a      sample      of      queries  
through      the      entire      system,      and      look      at      the      size      of      the      symmetric      difference      of      the      results  
(weighted      by      ranking      position).      if      the      difference      is      very      small,      then      you      can      tell      without      running  
an      experiment      that      there      will      be      little      change.      if      the      difference      is      very      large,      then      you      want      to  
make      sure      that      the      change      is      good.      looking      over      queries      where      the      symmetric      difference      is      high  
can      help      you      to      understand      qualitatively      what      the      change      was      like.      make      sure,      however,      that      the  
system      is      stable.      make      sure      that      a      model      when      compared      with      itself      has      a      low      (ideally      zero)  
symmetric      difference.  
  
rule      #25:      when      choosing      models,      utilitarian      performance      trumps      predictive      power.    
your      model      may      try      to      predict      click  through  rate.      however,      in      the      end,      the      key      question      is      what  
you      do      with      that      prediction.      if      you      are      using      it      to      rank      documents,      then      the      quality      of      the      final  
ranking      matters      more      than      the      prediction      itself.      if      you      predict      the      id203      that      a      document      is  
spam      and      then      have      a      cutoff      on      what      is      blocked,      then      the      precision      of      what      is      allowed      through  
matters      more.      most      of      the      time,      these      two      things      should      be      in      agreement:      when      they      do      not  
agree,      it      will      likely      be      on      a      small      gain.      thus,      if      there      is      some      change      that      improves      log      loss      but  
degrades      the      performance      of      the      system,      look      for      another      feature.      when      this      starts      happening  
more      often,      it      is      time      to      revisit      the      objective      of      your      model.    
  
rule      #26:      look      for      patterns      in      the      measured      errors,      and      create      new      features.    
suppose      that      you      see      a      training      example      that      the      model      got         wrong   .      in      a      classification      task,      this  
could      be      a      false      positive      or      a      false      negative.      in      a      ranking      task,      it      could      be      a      pair      where      a      positive  
was      ranked      lower      than      a      negative.      the      most      important      point      is      that      this      is      an      example      that      the  
machine      learning      system        knows      it      got      wrong        and      would      like      to      fix      if      given      the      opportunity.      if      you  
give      the      model      a      feature      that      allows      it      to      fix      the      error,      the      model      will      try      to      use      it.    
  
on      the      other      hand,      if      you      try      to      create      a      feature      based      upon      examples      the      system      doesn   t      see  
as      mistakes,      the      feature      will      be      ignored.      for      instance,      suppose      that      in      play      apps      search,  
someone      searches      for         free      games   .      suppose      one      of      the      top      results      is      a      less      relevant      gag      app.  
so      you      create      a      feature      for         gag      apps   .      however,      if      you      are      maximizing      number      of      installs,      and  
people      install      a      gag      app      when      they      search      for      free      games,      the         gag      apps         feature      won   t      have      the  
effect      you      want.  
  
once      you      have      examples      that      the      model      got      wrong,      look      for      trends      that      are      outside      your      current  
feature      set.      for      instance,      if      the      system      seems      to      be      demoting      longer      posts,      then      add      post  
length.      don   t      be      too      specific      about      the      features      you      add.      if      you      are      going      to      add      post      length,  

don   t      try      to      guess      what      long      means,      just      add      a      dozen      features      and      the      let      model      figure      out      what  
to      do      with      them      (see        rule      #21  ).      that      is      the      easiest      way      to      get      what      you      want.  
  
rule      #27:      try      to      quantify      observed      undesirable      behavior.    
some      members      of      your      team      will      start      to      be      frustrated      with      properties      of      the      system      they      don   t  
like      which      aren   t      captured      by      the      existing      loss      function.      at      this      point,      they      should      do      whatever      it  
takes      to      turn      their      gripes      into      solid      numbers.      for      example,      if      they      think      that      too      many         gag      apps     
are      being      shown      in      play      search,      they      could      have      human      raters      identify      gag      apps.      (you      can  
feasibly      use      human  labelled      data      in      this      case      because      a      relatively      small      fraction      of      the      queries  
account      for      a      large      fraction      of      the      traffic.)      if      your      issues      are      measurable,      then      you      can      start      using  
them      as      features,      objectives,      or      metrics.      the      general      rule      is           measure      first,      optimize      second   .  
  
rule      #28:      be      aware      that      identical      short  term      behavior      does      not      imply      identical      long  term  
behavior.    
imagine      that      you      have      a      new      system      that      looks      at      every      doc_id      and      exact_query,      and      then  
calculates      the      id203      of      click      for      every      doc      for      every      query.      you      find      that      its      behavior      is  
nearly      identical      to      your      current      system      in      both      side      by      sides      and      a/b      testing,      so      given      its  
simplicity,      you      launch      it.      however,      you      notice      that      no      new      apps      are      being      shown.      why?      well,  
since      your      system      only      shows      a      doc      based      on      its      own      history      with      that      query,      there      is      no      way      to  
learn      that      a      new      doc      should      be      shown.  
  
the      only      way      to      understand      how      such      a      system      would      work      long  term      is      to      have      it      train      only      on  
data      acquired      when      the      model      was      live.      this      is      very      difficult.  

training  serving      skew  

training  serving      skew      is      a      difference      between      performance      during      training      and      performance  
during      serving.      this      skew      can      be      caused      by:  

    a      discrepancy      between      how      you      handle      data      in      the      training      and      serving      pipelines,      or  
    a      change      in      the      data      between      when      you      train      and      when      you      serve,      or    
    a      feedback      loop      between      your      model      and      your      algorithm.  

we      have      observed      production      machine      learning      systems      at      google      with      training  serving      skew  
that      negatively      impacts      performance.      the      best      solution      is      to      explicitly      monitor      it      so      that      system  
and      data      changes      don   t      introduce      skew      unnoticed.  
  
rule      #29:      the      best      way      to      make      sure      that      you      train      like      you      serve      is      to      save      the      set      of  
  
features      used      at      serving      time,      and      then      pipe      those      features      to      a      log      to      use      them      at  
training      time.  
  
even      if      you      can   t      do      this      for      every      example,      do      it      for      a      small      fraction,      such      that      you      can      verify      the  
consistency      between      serving      and      training      (see        rule      #37  ).      teams      that      have      made      this  
measurement      at      google      were      sometimes      surprised      by      the      results.        youtube        home      page  

switched      to      logging      features      at      serving      time      with      significant      quality      improvements      and      a  
reduction      in      code      complexity,      and      many      teams      are      switching      their      infrastructure      as      we      speak.  
  
rule      #30:      importance      weight      sampled      data,      don   t      arbitrarily      drop      it!  
when      you      have      too      much      data,      there      is      a      temptation      to      take      files      1  12,      and      ignore      files      13  99.  
this      is      a      mistake:      dropping      data      in      training      has      caused      issues      in      the      past      for      several      teams      (see  
rule      #6  ).      although      data      that      was      never      shown      to      the      user      can      be      dropped,      importance  
weighting      is      best      for      the      rest.      importance      weighting      means      that      if      you      decide      that      you      are      going  
to      sample      example      x      with      a      30%      id203,      then      give      it      a      weight      of      10/3.        with      importance  
weighting,      all      of      the      calibration      properties      discussed      in        rule      #14        still      hold  .  
  
rule      #31:      beware      that      if      you      join      data      from      a      table      at      training      and      serving      time,      the      data      in  
the      table      may      change.    
say      you      join      doc      ids      with      a      table      containing      features      for      those      docs      (such      as      number      of  
comments      or      clicks).      between      training      and      serving      time,      features      in      the      table      may      be      changed.  
your      model's      prediction      for      the      same      document      may      then      differ      between      training      and      serving.  
the      easiest      way      to      avoid      this      sort      of      problem      is      to      log      features      at      serving      time      (see              rule      #32  ).      if  
the      table      is      changing      only      slowly,      you      can      also      snapshot      the      table      hourly      or      daily      to      get  
reasonably      close      data.      note      that      this      still      doesn   t      completely      resolve      the      issue.  
  
rule      #32:      re  use      code      between      your      training      pipeline      and      your      serving      pipeline  
whenever      possible.    
batch      processing      is      different      than      online      processing.      in      online      processing,      you      must      handle  
each      request      as      it      arrives      (e.g.      you      must      do      a      separate      lookup      for      each      query),      whereas      in      batch  
processing,      you      can      combine      tasks      (e.g.      making      a      join).      at      serving      time,      you      are      doing      online  
processing,      whereas      training      is      a      batch      processing      task.      however,      there      are      some      things      that  
you      can      do      to      re  use      code.      for      example,      you      can      create      an      object      that      is      particular      to      your  
system      where      the      result      of      any      queries      or      joins      can      be      stored      in      a      very      human      readable      way,  
and      errors      can      be      tested      easily.      then,      once      you      have      gathered      all      the      information,      during  
serving      or      training,      you      run      a      common      method      to      bridge      between      the      human  readable      object  
that      is      specific      to      your      system,      and      whatever      format      the      machine      learning      system      expects.        this  
eliminates      a      source      of      training  serving      skew  .      as      a      corollary,      try      not      to      use      two      different  
programming      languages      between      training      and      serving              that      decision      will      make      it      nearly  
impossible      for      you      to      share      code.  
  
rule      #33:      if      you      produce      a      model      based      on      the      data      until      january      5th,      test      the      model      on  
the      data      from      january      6th      and      after.    
in      general,      measure      performance      of      a      model      on      the      data      gathered      after      the      data      you      trained      the  
model      on,      as      this      better      reflects      what      your      system      will      do      in      production.      if      you      produce      a      model  
based      on      the      data      until      january      5th,      test      the      model      on      the      data      from      january      6th.      you      will  
expect      that      the      performance      will      not      be      as      good      on      the      new      data,      but      it      shouldn   t      be      radically  
worse.      since      there      might      be      daily      effects,      you      might      not      predict      the      average      click      rate      or  

conversion      rate,      but      the      area      under      the      curve,      which      represents      the      likelihood      of      giving      the  
positive      example      a      score      higher      than      a      negative      example,      should      be      reasonably      close.  
  
rule      #34:      in      binary      classification      for      filtering      (such      as      spam      detection      or      determining  
interesting      e  mails),      make      small      short  term      sacrifices      in      performance      for      very      clean      data.    
in      a      filtering      task,      examples      which      are      marked      as      negative      are      not      shown      to      the      user.      suppose  
you      have      a      filter      that      blocks      75%      of      the      negative      examples      at      serving.      you      might      be      tempted      to  
draw      additional      training      data      from      the      instances      shown      to      users.      for      example,      if      a      user      marks      an  
email      as      spam      that      your      filter      let      through,      you      might      want      to      learn      from      that.  
  
but      this      approach      introduces      sampling      bias.      you      can      gather      cleaner      data      if      instead      during  
serving      you      label      1%      of      all      traffic      as         held      out   ,      and      send      all      held      out      examples      to      the      user.      now  
your      filter      is      blocking      at      least      74%      of      the      negative      examples.      these      held      out      examples      can  
become      your      training      data.  
  
note      that      if      your      filter      is      blocking      95%      of      the      negative      examples      or      more,      this      becomes      less  
viable.      even      so,      if      you      wish      to      measure      serving      performance,      you      can      make      an      even      tinier  
sample      (say      0.1%      or      0.001%).      ten      thousand      examples      is      enough      to      estimate      performance      quite  
accurately.  
  
rule      #35:      beware      of      the      inherent      skew      in      ranking      problems.    
when      you      switch      your      ranking      algorithm      radically      enough      that      different      results      show      up,      you  
have      effectively      changed      the      data      that      your      algorithm      is      going      to      see      in      the      future.      this      kind      of  
skew      will      show      up,      and      you      should      design      your      model      around      it.      there      are      multiple      different  
approaches.      these      approaches      are      all      ways      to      favor      data      that      your      model      has      already      seen.  
1. have      higher      id173      on      features      that      cover      more      queries      as      opposed      to      those  

features      that      are      on      for      only      one      query.      this      way,      the      model      will      favor      features      that      are  
specific      to      one      or      a      few      queries      over      features      that      generalize      to      all      queries.      this  
approach      can      help      prevent      very      popular      results      from      leaking      into      irrelevant      queries.      note  
that      this      is      opposite      the      more      conventional      advice      of      having      more      id173      on  
feature      columns      with      more      unique      values.  

2. only      allow      features      to      have      positive      weights.      thus,      any      good      feature      will      be      better      than      a  

feature      that      is         unknown   .  

3. don   t      have      document  only      features.      this      is      an      extreme      version      of      #1.      for      example,      even  
if      a      given      app      is      a      popular      download      regardless      of      what      the      query      was,      you      don   t      want      to  
show      it      everywhere .      not      having      document  only      features      keeps      that      simple.    

4

  

4      the      reason      you      don   t      want      to      show      a      specific      popular      app      everywhere      has      to      do      with      the      importance      of  
making      all      the      desired      apps        reachable  .      for      instance,      if      someone      searches      for         bird      watching      app   ,      they  
might      download         angry      birds   ,      but      that      certainly      wasn   t      their      intent.      showing      such      an      app      might      improve  
download      rate,      but      leave      the      user   s      needs      ultimately      unsatisfied.  

rule      #36:      avoid      feedback      loops      with      positional      features.    
the      position      of      content      dramatically      affects      how      likely      the      user      is      to      interact      with      it.      if      you      put      an  
app      in      the      first      position      it      will      be      clicked      more      often,      and      you      will      be      convinced      it      is      more      likely      to  
be      clicked.      one      way      to      deal      with      this      is      to      add      positional      features,      i.e.      features      about      the      position  
of      the      content      in      the      page.      you      train      your      model      with      positional      features,      and      it      learns      to      weight,  
for      example,      the      feature      "1st  position"      heavily.      your      model      thus      gives      less      weight      to      other      factors  
for      examples      with      "1st  position=true".      then      at      serving      you      don't      give      any      instances      the  
positional      feature,      or      you      give      them      all      the      same      default      feature,      because      you      are      scoring  
candidates        before        you      have      decided      the      order      in      which      to      display      them.  
  
note      that      it      is      important      to      keep      any      positional      features      somewhat      separate      from      the      rest      of      the  
model      because      of      this      asymmetry      between      training      and      testing.      having      the      model      be      the      sum      of  
a      function      of      the      positional      features      and      a      function      of      the      rest      of      the      features      is      ideal.      for  
example,      don   t      cross      the      positional      features      with      any      document      feature.  
  
rule      #37:      measure      training/serving      skew.    
there      are      several      things      that      can      cause      skew      in      the      most      general      sense.      moreover,      you      can  
divide      it      into      several      parts:  

1. the      difference      between      the      performance      on      the      training      data      and      the      holdout      data.      in  

general,      this      will      always      exist,      and      it      is      not      always      bad.  

2. the      difference      between      the      performance      on      the      holdout      data      and      the         next  day         data.  
again,      this      will      always      exist.        you      should      tune      your      id173      to      maximize      the  
next  day      performance  .      however,      large      drops      in      performance      between      holdout      and  
next  day      data      may      indicate      that      some      features      are      time  sensitive      and      possibly      degrading  
model      performance.  

3. the      difference      between      the      performance      on      the         next  day         data      and      the      live      data.      if      you  

apply      a      model      to      an      example      in      the      training      data      and      the      same      example      at      serving,      it  
should      give      you      exactly      the      same      result      (see        rule      #5  ).      thus,      a      discrepancy      here  
probably      indicates      an      engineering      error.    

  
    

ml      phase      iii:      slowed      growth,      optimization  

refinement,      and      complex      models  

there      will      be      certain      indications      that      the      second      phase      is      reaching      a      close.      first      of      all,      your  
monthly      gains      will      start      to      diminish.      you      will      start      to      have      tradeoffs      between      metrics:      you      will      see  
some      rise      and      others      fall      in      some      experiments.      this      is      where      it      gets      interesting.      since      the      gains  
are      harder      to      achieve,      the      machine      learning      has      to      get      more      sophisticated.  
  

a      caveat:      this      section      has      more      blue  sky      rules      than      earlier      sections.      we      have      seen      many      teams  
go      through      the      happy      times      of      phase      i      and      phase      ii      machine      learning.      once      phase      iii      has      been  
reached,      teams      have      to      find      their      own      path.  
rule      #38:      don   t      waste      time      on      new      features      if      unaligned      objectives      have      become      the  
issue.    
as      your      measurements      plateau,      your      team      will      start      to      look      at      issues      that      are      outside      the      scope  
of      the      objectives      of      your      current      machine      learning      system.      as      stated      before,      if      the      product      goals  
are      not      covered      by      the      existing      algorithmic      objective,      you      need      to      change      either      your      objective  
or      your      product      goals.      for      instance,      you      may      optimize      clicks,      plus  ones,      or      downloads,      but      make  
launch      decisions      based      in      part      on      human      raters.    
  
rule      #39:      launch      decisions      are      a      proxy      for      long  term      product      goals.    
alice      has      an      idea      about      reducing      the      logistic      loss      of      predicting      installs.      she      adds      a      feature.      the  
logistic      loss      drops.      when      she      does      a      live      experiment,      she      sees      the      install      rate      increase.  
however,      when      she      goes      to      a      launch      review      meeting,      someone      points      out      that      the      number      of  
daily      active      users      drops      by      5%.      the      team      decides      not      to      launch      the      model.      alice      is  
disappointed,      but      now      realizes      that      launch      decisions      depend      on      multiple      criteria,      only      some      of  
which      can      be      directly      optimized      using      ml.  
  
the      truth      is      that      the      real      world      is      not      dungeons      and      dragons:      there      are      no         hit      points         identifying  
the      health      of      your      product.      the      team      has      to      use      the      statistics      it      gathers      to      try      to      effectively  
predict      how      good      the      system      will      be      in      the      future.      they      need      to      care      about      engagement,      1      day  
active      users      (dau),      30      dau,      revenue,      and      advertiser   s      return      on      investment.      these      metrics      that  
are      measureable      in      a/b      tests      in      themselves      are      only      a      proxy      for      more      long  term      goals:      satisfying  
users,      increasing      users,      satisfying      partners,      and      profit,      which      even      then      you      could      consider  
proxies      for      having      a      useful,      high      quality      product      and      a      thriving      company      five      years      from      now.  
  
the      only      easy      launch      decisions      are      when      all      metrics      get      better      (or      at      least      do      not      get  
worse).        if      the      team      has      a      choice      between      a      sophisticated      machine      learning      algorithm,      and      a  
simple      heuristic,      if      the      simple      heuristic      does      a      better      job      on      all      these      metrics,      it      should      choose  
the      heuristic.      moreover,      there      is      no      explicit      ranking      of      all      possible      metric      values.      specifically,  
consider      the      following      two      scenarios:  
  

experiment  

daily      active      users  

revenue/day  

a  

b  

1      million  

2      million  

$4      million  

$2      million  

  
if      the      current      system      is      a,      then      the      team      would      be      unlikely      to      switch      to      b.      if      the      current      system      is  
b,      then      the      team      would      be      unlikely      to      switch      to      a.      this      seems      in      conflict      with      rational      behavior:  
however,      predictions      of      changing      metrics      may      or      may      not      pan      out,      and      thus      there      is      a      large      risk  
involved      with      either      change.      each      metric      covers      some      risk      with      which      the      team      is      concerned.  

moreover,      no      metric      covers      the      team   s      ultimate      concern,         where      is      my      product      going      to      be      five  
years      from      now   ?  
  
individuals,      on      the      other      hand,      tend      to      favor      one      objective      that      they      can      directly      optimize.  
most      machine      learning      tools      favor      such      an      environment.      an      engineer      banging      out      new      features  
can      get      a      steady      stream      of      launches      in      such      an      environment.      there      is      a      type      of      machine  
learning,      multi  objective      learning,      which      starts      to      address      this      problem.      for      instance,      one      can  
formulate      a      constraint      satisfaction      problem      that      has      lower      bounds      on      each      metric,      and      optimizes  
some      linear      combination      of      metrics.      however,      even      then,      not      all      metrics      are      easily      framed      as  
machine      learning      objectives:      if      a      document      is      clicked      on      or      an      app      is      installed,      it      is      because      that  
the      content      was      shown.      but      it      is      far      harder      to      figure      out      why      a      user      visits      your      site.      how      to  
predict      the      future      success      of      a      site      as      a      whole      is        ai  complete  ,      as      hard      as      computer      vision      or  
natural      language      processing.  
  
  
rule      #40:      keep      ensembles      simple.  
unified      models      that      take      in      raw      features      and      directly      rank      content      are      the      easiest      models      to  
debug      and      understand.      however,      an      ensemble      of      models      (a         model         which      combines      the      scores  
of      other      models)      can      work      better.        to      keep      things      simple,      each      model      should      either      be      an  
ensemble      only      taking      the      input      of      other      models,      or      a      base      model      taking      many      features,  
but      not      both.              if      you      have      models      on      top      of      other      models      that      are      trained      separately,      then  
combining      them      can      result      in      bad      behavior.  
  
use      a      simple      model      for      ensembling      that      takes      only      the      output      of      your         base         models      as      inputs.  
you      also      want      to      enforce      properties      on      these      ensemble      models.      for      example,      an      increase      in      the  
score      produced      by      a      base      model      should      not      decrease      the      score      of      the      ensemble.      also,      it      is      best  
if      the      incoming      models      are      semantically      interpretable      (for      example,      calibrated)      so      that      changes  
of      the      underlying      models      do      not      confuse      the      ensemble      model.      also,        enforce      that      an      increase  
in      the      predicted      id203      of      an      underlying      classifier      does      not      decrease      the      predicted  
id203      of      the      ensemble  .    
  
rule      #41:        when      performance      plateaus,      look      for      qualitatively      new      sources      of      information  
  
to      add      rather      than      refining      existing      signals.  
you   ve      added      some      demographic      information      about      the      user.      you've      added      some      information  
about      the      words      in      the      document.      you      have      gone      through      template      exploration,      and      tuned      the  
id173.      you      haven   t      seen      a      launch      with      more      than      a      1%      improvement      in      your      key      metrics  
in      a      few      quarters.      now      what?  
  
it      is      time      to      start      building      the      infrastructure      for      radically      different      features,      such      as      the      history      of  
documents      that      this      user      has      accessed      in      the      last      day,      week,      or      year,      or      data      from      a      different  
property.      use        wikidata        entities      or      something      internal      to      your      company      (such      as      google   s  
knowledge      graph  ).      use      deep      learning.      start      to      adjust      your      expectations      on      how      much      return      you  

expect      on      investment,      and      expand      your      efforts      accordingly.      as      in      any      engineering      project,      you  
have      to      weigh      the      benefit      of      adding      new      features      against      the      cost      of      increased      complexity.  
  
rule      #42:      don   t      expect      diversity,      personalization,      or      relevance      to      be      as      correlated      with  
popularity      as      you      think      they      are.    
diversity      in      a      set      of      content      can      mean      many      things,      with      the      diversity      of      the      source      of      the  
content      being      one      of      the      most      common.      personalization      implies      each      user      gets      their      own  
results.      relevance      implies      that      the      results      for      a      particular      query      are      more      appropriate      for      that  
query      than      any      other.      thus      all      three      of      these      properties      are      defined      as      being      different      from      the  
ordinary.  
  
the      problem      is      that      the      ordinary      tends      to      be      hard      to      beat.  
  
note      that      if      your      system      is      measuring      clicks,      time      spent,      watches,      +1s,      reshares,      et      cetera,      you  
are      measuring      the        popularity        of      the      content.      teams      sometimes      try      to      learn      a      personal      model  
with      diversity.      to      personalize,      they      add      features      that      would      allow      the      system      to      personalize  
(some      features      representing      the      user   s      interest)      or      diversify      (features      indicating      if      this      document  
has      any      features      in      common      with      other      documents      returned,      such      as      author      or      content),      and  
find      that      those      features      get      less      weight      (or      sometimes      a      different      sign)      than      they      expect.    
  
this      doesn   t      mean      that      diversity,      personalization,      or      relevance      aren   t      valuable.      as      pointed      out      in  
the      previous      rule,      you      can      do      post  processing      to      increase      diversity      or      relevance.      if      you      see  
longer      term      objectives      increase,      then      you      can      declare      that      diversity/relevance      is      valuable,      aside  
from      popularity.      you      can      then      either      continue      to      use      your      post  processing,      or      directly      modify      the  
objective      based      upon      diversity      or      relevance.  
  
rule      #43:      your      friends      tend      to      be      the      same      across      different      products.      your      interests      tend  
not      to      be.    
teams      at      google      have      gotten      a      lot      of      traction      from      taking      a      model      predicting      the      closeness      of      a  
connection      in      one      product,      and      having      it      work      well      on      another.      your      friends      are      who      they      are.      on  
the      other      hand,      i      have      watched      several      teams      struggle      with      personalization      features      across  
product      divides.      yes,      it      seems      like      it      should      work.      for      now,      it      doesn   t      seem      like      it      does.      what      has  
sometimes      worked      is      using      raw      data      from      one      property      to      predict      behavior      on      another.      also,  
keep      in      mind      that      even      knowing      that      a      user      has      a      history      on      another      property      can      help.      for  
instance,      the      presence      of      user      activity      on      two      products      may      be      indicative      in      and      of      itself.  
  

related      work  

there      are      many      documents      on      machine      learning      at      google      as      well      as      externally.  

    machine      learning      crash      course  :      an      introduction      to      applied      machine      learning  

    machine      learning:      a      probabilistic      approach        by      kevin      murphy      for      an      understanding      of  

the      field      of      machine      learning  

    practical      advice      for      the      analysis      of      large,      complex      data      sets  :      a      data      science      approach  

to      thinking      about      data      sets.  

  

    deep      learning        by      ian      goodfellow      et      al      for      learning      nonlinear      models  
    google      paper      on        technical      debt  ,      which      has      a      lot      of      general      advice.  
    tensorflow      documentation  

acknowledgements  
thanks      to      david      westbrook,      peter      brandt,      samuel      ieong,      chenyu      zhao,      li      wei,      michalis  
potamias,      evan      rosen,      barry      rosenberg,      christine      robson,      james      pine,      tal      shaked,      tushar  
chandra,      mustafa      ispir,      jeremiah      harmsen,      konstantinos      katsiapis,      glen      anderson,      dan  
duckworth,      shishir      birmiwal  ,        gal      elidan,      su      lin      wu,      jaihui      liu,      fernando      pereira,      and  
hrishikesh      aradhye      for      many      corrections,      suggestions,      and      helpful      examples      for      this      document.  
also,      thanks      to      kristen      lefevre,      suddha      basu,      and      chris      berg      who      helped      with      an      earlier  
version.      any      errors,      omissions,      or      (gasp!)      unpopular      opinions      are      my      own.  

appendix  
there      are      a      variety      of      references      to      google      products      in      this      document.      to      provide      more      context,  
i      give      a      short      description      of      the      most      common      examples      below.  

youtube      overview  
youtube      is      a      streaming      video      service.      both      youtube      watch      next      and      youtube      home      page  
teams      use      ml      models      to      rank      video      recommendations.      watch      next      recommends      videos      to  
watch      after      the      currently      playing      one,      while      home      page      recommends      videos      to      users      browsing  
the      home      page.  

google      play      overview  
google      play      has      many      models      solving      a      variety      of      problems.      play      search,      play      home      page  
personalized      recommendations,      and         users      also      installed         apps      all      use      machine      learning.    

google      plus      overview  
google      plus      uses      machine      learning      in      a      variety      of      situations:      ranking      posts      in      the         stream         of  
posts      being      seen      by      the      user,      ranking         what   s      hot         posts      (posts      that      are      very      popular      now),  
ranking      people      you      know,      et      cetera.  

  

