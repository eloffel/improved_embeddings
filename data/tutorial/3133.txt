   #[1]github [2]recent commits to gsoc:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]105
     * [35]star [36]180
     * [37]fork [38]179

[39]numfocus/[40]gsoc

   [41]code [42]issues 1 [43]pull requests 1 [44]projects 0 [45]wiki
   [46]insights
   [47]permalink
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [48]sign up
   branch: master
   [49]gsoc/[50]2017/[51]proposals/chinmaya_pancholi.md
   [52]find file copy path
   fetching contributors   
   cannot retrieve contributors at this time
   561 lines (407 sloc) 40.3 kb
   [53]raw [54]blame [55]history
   (button) (button)

gensim integration with scikit-learn and keras

contact information

   name : chinmaya pancholi
   university : [56]indian institute of technology, kharagpur
   email-id : [57]chinmayapancholi13@gmail.com
   github username : [58]chinmayapancholi13
   blog : [59]chinmayapancholi13.github.io
   time-zone : utc + 5:30

abstract

   gensim[60][1] is a topic-modeling package in python for unsupervised
   learning. this implies that to be able to usefully apply it to a real
   business problem, the output generated must go to a supervised
   classifier. presently, the most popular supervised learning packages
   are scikit-learn[61][2] (for simpler data analysis) and keras[62][3]
   (for id158s). hence, the objective of this project
   is to create wrappers for scikit-learn and keras around all gensim
   models for seaid113ss integration of gensim with these libraries.

technical details

background

unsupervised learning :

   unsupervised learning involves inferring a function to describe a
   hidden structure from datasets without labelled responses. an example
   of an unsupervised task is cluster analysis where the algorithm creates
   different clusters of the input data and would be able to assign an
   appropriate cluster to new unseen data after training. since the data
   being fed to the learning algorithm is unlabeled, usually there is no
   objective evaluation of the performance of output by the relevant
   algorithm.

supervised learning :

   supervised learning involves inferring a function from datasets which
   are labeled. the training data consists of a set of training examples
   where each example is a pair consisting of an input object and a
   desired output value, called the supervisory signal. in case of
   supervised tasks, the learning algorithm analyzes the training data and
   produces an inferred function, which can then be used for mapping new
   unseen examples. an example of a supervised task is image captioning
   where the system is fed pairs of image with the corresponding caption
   during training. after training, the system is expected to generate
   appropriate captions for new data input to it.

scikit-learn :

   scikit-learn is a machine learning library for python designed to
   interoperate with the libraries numpy[63][4] and scipy[64][5]. it
   features various classification, regression and id91 algorithms
   such as id166, id90, id116 id91, gradient boosting etc.
   today, for most people working on a data science project, scikit-learn
   is the most popular choice for access to easy-to-use and high-quality
   implementations of popular machine learning algorithms. in fact, it is
   the most popular and active open-source machine learning project in
   python in terms of the number of contributors and commits[65][6].

keras :

   keras is a deep learning library, written in python, for theano[66][7]
   and tensorflow[67][8]. it is a high-level neural networks api capable
   of running on top of either tensorflow or theano. it attempts to enable
   the user to go from idea to result as quickly as possible by providing
   fast implementations of neural networks constructs and hence allows for
   fast prototyping. keras supports both id98s (convolutional neural
   networks) and id56s (recurrent neural networks), as well as combinations
   of the two. the reason behind keras' popularity is that it is a
   self-contained wrapper for deep learning i.e. one could use keras to
   solve problems end-to-end without ever having to interact with the
   underlying backend engine, theano or tensorflow.

   we can observe that scikit-learn and keras are quite popular machine
   learning libraries used for supervised learning tasks and thus are very
   good choices for integration with gensim. hence, the project would
   enable harmonious use of gensim with the two libraries and would be of
   great use in industry as well as in academia.

a task for motivation : movie plot classification

   an interesting example of using gensim for a supervised problem is the
   movie plot classification problem[68][9]. the scenario is as follows :
   you run a movie studio. every day you receive thousands of proposals
   for movies to make and you need to send them to the right department
   for consideration. there is exactly one department per genre so you
   need to classify plots by genre.

   as can be seen [69]here, we can use gensim in conjunction with
   scikit-learn to tackle the above-mentioned problem. particularly, we
   have used gensim   s id97[70][10] and doc2vec[71][11] models with
   scikit-learn   s classifiers for this particular instance of automated
   text-tagging problem.

   more generally, we can use gensim in conjunction with scikit-learn and
   keras to address problems where we wish to utilise the output of any of
   gensim   s models further depending on the particular supervised task at
   hand. however, for any gensim model to be directly compatible with
   scikit-learn elements such as pipeline.pipeline and
   pipeline.featureunions, it must provide functions like fit, transform,
   fit_transform etc. similarly, for us to use any gensim model with
   keras, we need constructs like keras.layers.embedding for making the
   models compatible with the keras model. this is exactly what this
   project aims to address.

implementation details

   presently, the following models are present in gensim :
    1. author-topic model : this model trains the author-topic
       model[72][12] on documents and corresponding author-document
       dictionaries. the training is online and is constant in memory
       with-respect-to the number of documents but is not constant in
       memory with-respect-to the number of authors. the model can be
       updated with additional documents after training has been
       completed. it is also possible to continue training on the existing
       data.
    2. topic coherence model : this model calculates topic coherence in
       python by implementing the four-stage topic coherence pipeline
       (segmentation -> id203 estimation -> confirmation measure ->
       aggregation) based on [73][13]. implementation of this pipeline
       allows for the user to in essence make a coherence measure of one's
       choice by choosing a method in each of the pipelines.
    3. doc2vec model : this model enables us to learn representation for
       the input data through deep learning via the distributed memory and
       distributed id159s[74][14], using either hierarchical
       softmax or negative sampling.
    4. hierarchical dirichlet process model : this model encapsulates
       functionality for the online hierarchical dirichlet process
       algorithm[75][15]. it allows both model estimation from a training
       corpus and id136 of topic distribution on new, unseen
       documents.
    5. id44 model : this model is based on the lda
       technique for id96[76][16] and allows both lda model
       estimation from a training corpus and id136 of topic
       distribution on new, unseen documents. the model can also be
       updated with new documents for online training.
    6. logid178 model : this model realizes the transformation between
       word-document co-occurrence matrix into a weighted matrix using the
       log id178 id172, optionally normalizing the resulting
       documents to unit length.
    7. latent semantic analysis model : this model is based on the lsa
       technique[77][17] and implements fast truncated svd (singular value
       decomposition)[78][18]. the svd decomposition can be updated with
       new observations at any time for an online, incremental and
       memory-efficient training.
    8. id172 model : this model realizes the explicit
       id172 of vectors and supports 'l1' and 'l2' norms.
    9. random projections model : this model allows building and
       maintaining a model for the random projections (or random indexing)
       technique[79][19].
   10. term frequency-inverse document frequency model : this model is
       based on tf-idf weighting[80][20] and realizes the transformation
       between word-document co-occurrence matrix into a weighted tf-idf
       matrix.
   11. id97 model : the model is based on the id97
       technique[81][21][82][22] and produces word vectors with deep
       learning via id97   s skip-gram and cbow models, using either
       hierarchical softmax or negative sampling.
   12. dynamic topic model : this model is based on the dtm
       technique[83][23] and implements topics that change over time and
       represents how individual documents predict that change.
   13. text to bag-of-words model : this model transforms input text into
       gensim bag-of-words format corpus so that elements such as pipeline
       could take text as input directly, rather than always having to
       take a bag-of-words corpus. the idea for a wrapper for this model
       has been proposed recently during the review of my pr [84]#1244.

   further details about the above models can be seen from gensim's api
   reference[85][24].

example implementation for a wrapper for scikit-learn

   as can be seen from api guidelines for scikit-learn objects[86][25],
   developing a wrapper for a gensim model to work with scikit-learn would
   involve implementing the following functions of the new class :
    1. get_params : this method takes no arguments and returns a dict of
       the __init__ parameters of the estimator, together with their
       values. it must take one keyword argument, deep, which receives a
       boolean value that determines whether the method should return the
       parameters of sub-estimators (for most estimators, this can be
       ignored). the default value for deep should be true.
    2. set_params : this method takes as input a dict of the form
       'parameter': value and sets the parameter of the estimator using
       this dict.
    3. fit : method to learn from data. used like either estimator =
       obj.fit(data, targets) or estimator = obj.fit(data).
    4. transform : method to filter or modify the data, in a supervised or
       unsupervised way. used as new_data = obj.transform(data).
    5. partial_fit : method to enable the model to learn incrementally.
       used as estimator = obj.partial_fit(data).

   on similar lines, i have submitted pr [87]#1244 (which has now been
   accepted and merged) that creates a wrapper for the lsimodel. the code
   for the wrapper class has been explained through detailed comments
   below :
class sklearnwrapperlsimodel(models.lsimodel, transformermixin, baseestimator):
    """
    base lsi module
    """

    def __init__(self, corpus=none, num_topics=200,  word=none, chunksize=2000
0,
                 decay=1.0, onepass=true, power_iters=2, extra_samples=100):
        """
        sklearn wrapper for lsi model. class derived from gensim.model.lsimodel.
        """
        self.corpus = corpus
        self.num_topics = num_topics
        self. word =  word
        self.chunksize = chunksize
        self.decay = decay
        self.onepass = onepass
        self.extra_samples = extra_samples
        self.power_iters = power_iters

        # if 'fit' function is not used, then 'corpus' is given in init
        if self.corpus:
            models.lsimodel.__init__(self, corpus=self.corpus, num_topics=self.n
um_topics,
               word=self. word, chunksize=self.chunksize, decay=self.decay,
              onepass=self.onepass, power_iters=self.power_iters,
              extra_samples=self.extra_samples)

    def get_params(self, deep=true):
        """
        returns all parameters as dictionary.
        """
        return {"corpus": self.corpus, "num_topics": self.num_topics, " word":
 self. word,
                "chunksize": self.chunksize, "decay": self.decay, "onepass": sel
f.onepass,
                "extra_samples": self.extra_samples, "power_iters": self.power_i
ters}

    def set_params(self, **parameters):
        """
        set all parameters.
        """
        for parameter, value in parameters.items():
            self.parameter = value
        return self

    def fit(self, x,  y=none):
        """
        for fitting corpus into the class object.
        calls gensim.model.lsimodel:
        >>>gensim.models.lsimodel(corpus=corpus, num_topics=num_topics,
             word= word, chunksize=chunksize, decay=decay,
            onepass=onepass, power_iters=power_iters, extra_samples=extra_sample
s)
        """
        if sparse.issparse(x):
            self.corpus = matutils.sparse2corpus(x)
        else:
            self.corpus = x

        models.lsimodel.__init__(self, corpus=self.corpus, num_topics=self.num_t
opics,
           word=self. word, chunksize=self.chunksize, decay=self.decay,
          onepass=self.onepass, power_iters=self.power_iters,
          extra_samples=self.extra_samples)

        return self

    def transform(self, docs):
        """
        takes a list of documents as input ('docs').
        returns a matrix of topic distribution for the given document bow, where
 a_ij
        indicates (topic_i, topic_id203_j).
        """
        # the input as array of array
        check = lambda x: [x] if isinstance(x[0], tuple) else x
        docs = check(docs)
        x = [[] for i in range(0,len(docs))];

        for k,v in enumerate(docs):
            doc_topics = self[v]
            probs_docs = list(map(lambda x: x[1], doc_topics))
            # everything should be equal in length
            if len(probs_docs) != self.num_topics:
                probs_docs.extend([1e-12]*(self.num_topics - len(probs_docs)))
            x[k] = probs_docs
            probs_docs = []

        return np.reshape(np.array(x), (len(docs), self.num_topics))

    def partial_fit(self, x):
        """
        train model over x.
        """
        if sparse.issparse(x):
            x = matutils.sparse2corpus(x)
        self.add_documents(corpus=x)

   this class enables us to use gensim models with scikit-learn elements
   such as pipeline.pipeline and classifiers like
   linear_model.logisticregression as follows :
model = sklearnwrapperlsimodel(num_topics=2)

with open("/path/to/input/data/file",'rb') as f:
    compressed_content = f.read()
    uncompressed_content = codecs.decode(compressed_content, 'zlib_codec')
    cache = pickle.loads(uncompressed_content)

data = cache
 word = dictionary(map(lambda x : x.split(), data.data))
corpus = [ word.doc2bow(i.split()) for i in data.data]

clf = linear_model.logisticregression(penalty='l2', c=0.1)

text_lda = pipeline((('features', model,), ('classifier', clf)))
text_lda.fit(corpus, data.target)
score = text_lda.score(corpus, data.target)

   hence, creating wrappers for the remaining models would be on the same
   lines with the implementation for each function depending upon the
   particular model for which the wrapper is being created. for instance,
   for the text to bag-of-words model, the fit function would learn a
   vocabulary dictionary for the input text, transform function would
   construct and return the bag-of-words model using the vocabulary
   created earlier and partial_fit function would update the vocabulary
   for the new text and the return the new bag-of-words model.

   we should note that apart from the wrapper for lsi model mentioned
   above, gensim already has a scikit-learn wrapper for lda model[88][26].
   so, we need to develop scikit-learn wrappers for the remaining 11
   models.

   also, of the 11 models remaining to be implemented, topic coherence
   model, logid178 model, random projections model, dynamic topic model
   and term frequency-inverse document frequency model don't have an
   update function which allows the model to learn incrementally. without
   such a function, we can't add the function partial_fit in the wrapper
   which is supposed to wrap the update function. so, we could either
   implement the update function or not implement the partial_fit for
   these models for now.

example implementation for a wrapper for keras

   i have submitted pr [89]#1248 (this work is in progress) which creates
   a wrapper for id97 model in gensim for integration with keras.
from keras.layers import embedding
from gensim import models

class keraswrapperid97model(models.id97):
    """
    class to integrate keras with gensim's id97 model
    """

    def __init__(
            self, sentences=none, size=100, alpha=0.025, window=5, min_count=5,
            max_vocab_size=none, sample=1e-3, seed=1, workers=3, min_alpha=0.000
1,
            sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=hash, iter=5, null_word
=0,
            trim_rule=none, sorted_vocab=1, batch_words=10000):

        """
        keras wrapper for id97 model. class derived from gensim.model.word2v
ec.
        """
        self.sentences=sentences
        self.size=size
        self.alpha=alpha
        self.window=window
        self.min_count=min_count
        self.max_vocab_size=max_vocab_size
        self.sample=sample
        self.seed=seed
        self.workers=workers
        self.min_alpha=min_alpha
        self.sg=sg
        self.hs=hs
        self.negative=negative
        self.cbow_mean=cbow_mean
        self.hashfxn=hashfxn
        self.iter=iter
        self.null_word=null_word
        self.trim_rule=trim_rule
        self.sorted_vocab=sorted_vocab
        self.batch_words=batch_words

        models.id97.__init__(self, sentences=self.sentences, size=self.size,
            alpha=self.alpha, window=self.window, min_count=self.min_count,
            max_vocab_size=self.max_vocab_size, sample=self.sample, seed=self.se
ed,
            workers=self.workers, min_alpha=self.min_alpha, sg=self.sg, hs=self.
hs,
            negative=self.negative, cbow_mean=self.cbow_mean, hashfxn=self.hashf
xn,
            iter=self.iter, null_word=self.null_word, trim_rule=self.trim_rule,
            sorted_vocab=self.sorted_vocab, batch_words=self.batch_words)

    def get_embedding_layer(self):
        """
        return a keras 'embedding' layer with weights set as our id97 model'
s
        learned id27s
        """
        weights = self.wv.syn0
        layer = embedding(input_dim=weights.shape[0], output_dim=weights.shape[1
],
                    weights=[weights], trainable=false)
        return layer

   the wrapper could be used in a keras pipeline involving multiple layers
   using the embedding layer returned by the function
   get_embedding_layer(). the code below shows one such instance where the
   wrapper has been used to address the 20newsgroups problem[90][27].
from __future__ import print_function

import os
import sys
import numpy as np
from gensim.keras_integration.keras_wrapper_gensim_id97 import keraswrapperw
ord2vecmodel
from gensim.models import id97
from keras.engine import input
from keras.layers import merge
from keras.preprocessing.text import tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from keras.layers import dense, input, flatten
from keras.layers import conv1d, maxpooling1d, embedding
from keras.models import model

base_dir = ''
text_data_dir = base_dir + './path/to/text/data/dir'
max_sequence_length = 1000
embedding_dim = 100
validation_split = 0.2
batch_size = 128

# prepare text samples and their labels

texts = []  # list of text samples
labels_index = {}  # dictionary mapping label name to numeric id
labels = []  # list of label ids

for name in sorted(os.listdir(text_data_dir)):
    path = os.path.join(text_data_dir, name)
    if os.path.isdir(path):
        label_id = len(labels_index)
        labels_index[name] = label_id
        for fname in sorted(os.listdir(path)):
            if fname.isdigit():
                fpath = os.path.join(path, fname)
                if sys.version_info < (3,):
                    f = open(fpath)
                else:
                    f = open(fpath, encoding='latin-1')
                t = f.read()
                i = t.find('\n\n')  # skip header
                if 0 < i:
                    t = t[i:]
                texts.append(t)
                f.close()
                labels.append(label_id)

# vectorize the text samples into a 2d integer tensor
tokenizer = tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
data = pad_sequences(sequences, maxlen=max_sequence_length)
labels = to_categorical(np.asarray(labels))

# split the data into a training set and a validation set
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
num_validation_samples = int(validation_split * data.shape[0])

x_train = data[:-num_validation_samples]
y_train = labels[:-num_validation_samples]
x_val = data[-num_validation_samples:]
y_val = labels[-num_validation_samples:]

# train the embedding matrix
data1 = id97.linesentence('./path/to/input/data')
keras_w2v = keraswrapperid97model(data1, min_count=1)
embedding_layer = keras_w2v.get_embedding_layer()

sequence_input = input(shape=(max_sequence_length,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)

x = conv1d(128, 5, activation='relu')(embedded_sequences)
x = maxpooling1d(5)(x)
x = conv1d(128, 5, activation='relu')(x)
x = maxpooling1d(5)(x)
x = conv1d(128, 5, activation='relu')(x)
x = maxpooling1d(35)(x)  # global max pooling
x = flatten()(x)
x = dense(128, activation='relu')(x)
preds = dense(len(labels_index), activation='softmax')(x)

model = model(sequence_input, preds)
model.compile(loss='categorical_crossid178',
              optimizer='rmsprop',
              metrics=['acc'])

model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_siz
e)

   hence, creating wrappers for the remaining models would be on the same
   lines with the implementation varying upon the particular model for
   which the wrapper is being created.

related work

   this work would be a joint project with shorttext[91][28][92][29].
   shorttext already has implementations for integrating gensim with
   scikit-learn for id44 model, latent semantic
   analysis model and random projections model (see
   [93]latenttopicmodeling and [94]sklearnclassification). shorttext also
   has wrappers for integration of various neural network algorithms in
   keras (namely id98wordembed, doubleid98wordembed, clstmwordembed and
   densewordembed) with gensim where the input to the wrapper class is a
   pre-trained word-embedding model along with the class labels of the
   training data (see [95]nnlib and [96]sumvec).

   hence, we could take cues about the details of the implementation from
   shorttext wherever possible for the project.

schedule of deliverables

   note1 : my summer vacation starts on 28th april, 2017. so i will be
   able to start coding one full month earlier than the gsoc coding
   period, effectively giving me 4 months to complete my project. hence, i
   would be able to comfortably devote 50-60 hours a week for 4 months to
   work towards the proposed project.

   note2 : i would be writing tests for the wrappers that i develop along
   the way. this would assist me to spot and rectify bugs in my code
   quickly and also help me be confident about the correctness of the code
   that i write.

may 1st - may 28th, community bonding period

     * before the official time period begins i intend to complete my
       ongoing work. this includes completing the pr for the id97
       wrapper for keras ([97]#1248). i have planned to complete this pr
       before the start of the community bonding period. this is because
       the problems that i could possibly face as well as the learnings
       that i would gain from completing this pr (as well as those from pr
       [98]#1244) would help me to develop wrappers for other models as
       well. i would also like to wind-up pr [99]#1201 which got put on
       the back burner partly due to ongoing discussions about its
       implementation and partly due to my involvement with other prs.
     * in the past, i have used scikit-learn, tensorflow and theano often
       in my coursework as well as for my pet projects. this has enabled
       me to be fairly well-versed with the apis and usage of these
       libraries. however, i haven't used keras directly in any project so
       far. so, i would like to get my hands dirty working with keras by
       implementing some small programs involving neural networks. since
       keras is built on top of tensorflow and theano, this should not
       pose a problem for me.
     * i would also actually like to get started with the coding of the
       wrappers within this period. this would not just give me extra time
       to complete the work but also get fully versed with gensim's
       codebase. i would also try to help to fix some of the existing (and
       identify new) bugs in code as well as errors in documentation that
       i come across here. thus, this phase would involve triage of new
       and existing issues as well.
     * i have been fairly active on gensim   s [100]gitter channel as well
       as [101]the mailing list, resolving queries of other members
       whenever i could. the community too has been very forthcoming right
       from the start and on several occasions i have been guided in the
       right direction by the members. thus, i plan to do my bit by
       participating on both these mediums during the entire summer.
     * a big chunk of open-source development is sharing your ideas and
       work with others. for this, i've started a blog [102]here to report
       the work that i would be doing on a weekly basis.

may 29th - june 3rd

     * start developing wrappers for the models mentioned [103]here for
       scikit-learn.

june 5th - june 9th

     * continue wrapper implementation for integration with scikit-learn.
     * wrappers for half of the models should be implemented by now.

june 12th - june 16th

     * continue wrapper implementation for integration with scikit-learn.

june 19th - june 23th, end of phase 1

     * complete wrapper development for integration with scikit-learn.
     * the writing of wrappers for integration with scikit-learn is
       expected to be completed by the end of this week. however, if this
       is not the case, some part of the following week can be devoted to
       finish the remaining work.

june 26 - june 30th, begin of phase 2

     * finish incomplete wrappers for scikit-learn, if any.
     * start developing wrappers for the models mentioned [104]here for
       keras.
     * regularly update the blog as well.

july 3rd - july 7th

     * continue wrapper implementation for integration with keras.

july 10th - july 14th

     * continue wrapper implementation for integration with keras.
     * wrappers for half of the models should be implemented by now.

july 17th - july 21th, end of phase 2

     * complete wrapper development for integration with keras.
     * the writing of wrappers for integration with keras is expected to
       be completed by the end of this week. however, if this is not the
       case, some part of the following week can be devoted to finish the
       remaining work.

july 24th - july 28th, begin of phase 3

     * finish incomplete wrappers for keras, if any.
     * start creating ipython notebook tutorials for each wrapper to make
       it easier for others to utilise them. some of the use-cases
       implemented in ipython tutorials are expected to overlap with the
       unit-tests created earlier, so we could use our earlier code and
       learnings here.

july 31st - august 4th

     * continue working on ipython tutorial notebooks.

august 7th - august 11th

     * complete ipython notebook tutorials for all wrappers.

august 14th - august 18th

     * finish incomplete ipython tutorials, if any.
     * rectify possible bugs and problems our implementation would have at
       this point.
     * ensure all edge and corner cases have been handled appropriately.
     * optimize the code on the fronts of time, memory usage and accuracy
       for improved efficiency.

august 21st - august 25th, final week

     * document all useful information regarding aspects such as parameter
       selection and model tuning from the insights gained so far.
     * optimize and fine-tune the code further wherever possible.
     * scrub and wrap-up documentation.

august 28th - august 29th, submit final work

     * get the pull request merged.
     * write concluding blog post.

future works

   our primary reasons for attempting to integrate gensim with
   scikit-learn and keras are ease-of-use, popularity and the wide range
   of usage of these libraries. on similar grounds, we could integrate
   gensim with spacy[105][30] as well so that the tasks which spacy excels
   at, such as data pre-processing, could be done seaid113ssly while working
   with gensim. textacy[106][31] is one such library that provides
   wrappers around spacy's various functionalities for tasks such as text
   pre-processing, information extraction, id96 ([107]using
   scikit-learn) etc. hence, we could take some idea from textacy's code
   about implementation details for this work.

   apart from this, i plan to stick around and help others to resolve any
   issues related to this project which may emerge in the future.

development experience

   currently i am working with a tech startup called [108]pregbuddy. so
   far, i have developed a facebook messenger bot which was shipped in
   december 2016 and has helped us increase our number of users
   significantly. also, i worked on a google home bot using [109]api.ai
   which has not been released to users till now. currently, i am
   developing an in-app chatbot for our [110]android application which is
   expected to be shipped by the end of april, 2017.

   during the summers of 2016, i was an intern at [111]nomura, mumbai
   where i worked with their quantitative research and information
   technology teams to develop a system to automate the process of running
   very large scale risk-value computations and comparisons. these
   computations were supposed to be run everyday after the closing of
   stock markets of various countries monitored by nomura. the system was
   completed and released for use during the last week of my internship
   period across all offices of nomura globally. the internship not just
   gave me an experience of developing software while working in a large
   team but also enabled me to refine the skill of jumping into and
   working with large and unfamiliar codebase.

   both these experiences have helped immensely me to gain real-life
   understanding about what really goes into developing products for
   people on a large scale.

other experiences

   the most important reason behind my desire to contribute to gensim has
   been my previous experience with natural language processing and
   machine learning. as a part of my coursework, i have undertaken a
   course on machine learning, which had a lab component as well. the
   project which i worked on was anomaly detection in surveillance videos
   in which we developed a system to detect anomalies in surveillance
   video-feeds for pedestrian walkways. various machine learning
   approaches and techniques like optical flow and alexnet for the task of
   feature extraction and id90, support vector machine,
   k-nearest neighbors, na  ve bayes, among others, for classification were
   used. in addition, time series analysis and topic modelling approaches
   were also implemented and tested.

   as a sophomore, i was an intern in the [112]language technologies
   research center at international institute of information technology
   hyderabad where i worked on the project id53 systems
   using deep learning which aimed at solving the problem of open-domain
   single-relation question-answering. i have also worked on a
   semester-long project on citation recommendation using citation context
   which aimed to solve the problem of recommending citations to the
   author using different approaches for document similarity. in addition
   to this, we tested and applied various methods for spell correction and
   id183 (including gensim's [113]similar_by_word method) as
   well.

   right now, i am undertaking a course on deep learning which has enabled
   me to get a solid understanding of the recent research developments in
   the field and also given me an opportunity to implement some of these
   developments myself. i like to work with not-too-big and interesting
   problems whenever i get time by implementing and tinkering around their
   solutions in the form of pet-projects. some of them are :
     * [114]word-derivation task
     * [115]mnist using id98
     * [116]word-analogy
     * [117]mnist through multilayer id88 using tensorflow
     * [118]mnist through multilayer id88 without neural network
       libraries
     * [119]word similarity using glove
     * [120]search algorithms for 8-puzzle problem

   for getting a concise and complete view about my previous work, you can
   go through my [121]resume.

   hence, the above experiences have given me a strong understanding of
   the concepts and fundamentals associated with the fields of nlp and
   machine learning, in addition to a hands-on experience to write code to
   address a real task at hand.

contributions

   as of now, i have submitted 8 pull requests to gensim and 1 to
   shorttext. doing this has helped me immensely in getting comfortable
   with the codebase and also learning about the community's coding
   standards and practices.
    1. added scikit-learn wrapper for lsi model in gensim ([122]#1244) :
       this pr also involved adding unit-tests and updating the ipython
       notebook for the wrapper.
    2. added function predict_output_word to predict the output word given
       the context words ([123]#1209) : this fixed issue [124]#863.
    3. updated id97's ipython notebook for the function
       predict_output_word ([125]#1228)
    4. added keras wrapper for id97 model in gensim ([126]#1248) :
       this work is in progress currently as an ipython notebook needs to
       be created for the wrapper. i plan to finish this pr very soon in
       the next couple of days.
    5. allows training if model is not modified by _minimize_model
       ([127]#1207) : this pr was based on the bug in the code that i
       found for the function _minimize_model ([128]relevant google
       mailing list discussion). this pr also adds a deprecation warning
       for this function.
    6. computes training loss for skip gram model ([129]#1201) : this pr
       computes the loss while training a id97 model. this pr intends
       to fix issue [130]#999. this work is in progress as it got deferred
       temporarily due to the ongoing discussions about the implementation
       as well as my involvement with other prs.
    7. updated description for worker_loop function used in score function
       ([131]#1206)
    8. fixed duplicate imports and typos in id97.py ([132]#1240)
    9. fixed typographical errors in shorttext's documentation ([133]#2)

why this project?

   i have frequently used scikit-learn, tensorflow and theano in my
   projects, both within and outside my coursework. since this project
   aims to integrate gensim with these popular libraries, this project can
   be expected to help a lot of people in the industry and academia who
   prefer using gensim and are trying to solve a real-life supervised
   task. moreover, i plan to work in the domain of topic modelling for my
   m.tech. thesis as well.

   hence, seeing the potential impact of the project and its overlap with
   my interest and previous work, the project piqued my interest.

appendix

   below is a list of all the web articles, libraries, code snippets and
   research papers mentioned in the proposal.

   1 - [134]http://radimrehurek.com/gensim/
   2 - [135]http://scikit-learn.org/
   3 - [136]https://keras.io
   4 - [137]https://www.numpy.org/
   5 - [138]https://www.scipy.org/
   6 -
   [139]http://www.kdnuggets.com/2015/06/top-20-python-machine-learning-op
   en-source-projects.html
   7 - [140]https://github.com/theano/theano
   8 - [141]https://www.tensorflow.org/
   9 - [142]https://github.com/rare-technologies/movie-plots-by-genre
   10 - [143]https://radimrehurek.com/gensim/models/id97.html
   11 - [144]https://radimrehurek.com/gensim/models/doc2vec.html
   12 - [145]https://mimno.infosci.cornell.edu/info6150/readings/398.pdf
   13 -
   [146]http://svn.aksw.org/papers/2015/wsdm_topic_evaluation/public.pdf
   14 - [147]https://arxiv.org/pdf/1405.4053v2.pdf
   15 - [148]https://en.wikipedia.org/wiki/hierarchical_dirichlet_process
   16 - [149]https://en.wikipedia.org/wiki/latent_dirichlet_allocation
   17 - [150]https://en.wikipedia.org/wiki/latent_semantic_analysis
   18 - [151]https://en.wikipedia.org/wiki/singular_value_decomposition
   19 - [152]https://en.wikipedia.org/wiki/random_projection
   20 - [153]https://en.wikipedia.org/wiki/tf%e2%80%93idf
   21 - [154]https://arxiv.org/abs/1301.3781
   22 -
   [155]https://papers.nips.cc/paper/5021-distributed-representations-of-w
   ords-and-phrases-and-their-compositionality.pdf
   23 - [156]https://en.wikipedia.org/wiki/dynamic_topic_model
   24 - [157]http://radimrehurek.com/gensim/apiref.html
   25 -
   [158]http://scikit-learn.org/stable/developers/contributing.html#apis-o
   f-scikit-learn-objects
   26 - [159]https://github.com/rare-technologies/gensim/pull/932
   27 - [160]http://qwone.com/~jason/20newsgroups/
   28 - [161]https://pythonhosted.org/shorttext/
   29 - [162]http://gibbslda.sourceforge.net/fp224-phan.pdf
   30 - [163]https://spacy.io/
   31 - [164]http://textacy.readthedocs.io/en/latest/
   ____________________ (button) go

     *    2019 github, inc.
     * [165]terms
     * [166]privacy
     * [167]security
     * [168]status
     * [169]help

     * [170]contact github
     * [171]pricing
     * [172]api
     * [173]training
     * [174]blog
     * [175]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [176]reload to refresh your
   session. you signed out in another tab or window. [177]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/numfocus/gsoc/commits/master.atom
   3. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md
  32. https://github.com/join
  33. https://github.com/login?return_to=/numfocus/gsoc
  34. https://github.com/numfocus/gsoc/watchers
  35. https://github.com/login?return_to=/numfocus/gsoc
  36. https://github.com/numfocus/gsoc/stargazers
  37. https://github.com/login?return_to=/numfocus/gsoc
  38. https://github.com/numfocus/gsoc/network/members
  39. https://github.com/numfocus
  40. https://github.com/numfocus/gsoc
  41. https://github.com/numfocus/gsoc
  42. https://github.com/numfocus/gsoc/issues
  43. https://github.com/numfocus/gsoc/pulls
  44. https://github.com/numfocus/gsoc/projects
  45. https://github.com/numfocus/gsoc/wiki
  46. https://github.com/numfocus/gsoc/pulse
  47. https://github.com/numfocus/gsoc/blob/681bac048dda31bc7511e3e8b4e4cbd5ed6c0851/2017/proposals/chinmaya_pancholi.md
  48. https://github.com/join?source=prompt-blob-show
  49. https://github.com/numfocus/gsoc
  50. https://github.com/numfocus/gsoc/tree/master/2017
  51. https://github.com/numfocus/gsoc/tree/master/2017/proposals
  52. https://github.com/numfocus/gsoc/find/master
  53. https://github.com/numfocus/gsoc/raw/master/2017/proposals/chinmaya_pancholi.md
  54. https://github.com/numfocus/gsoc/blame/master/2017/proposals/chinmaya_pancholi.md
  55. https://github.com/numfocus/gsoc/commits/master/2017/proposals/chinmaya_pancholi.md
  56. http://www.iitkgp.ac.in/
  57. mailto:chinmayapancholi13@gmail.com
  58. https://github.com/chinmayapancholi13
  59. https://chinmayapancholi13.github.io/
  60. http://radimrehurek.com/gensim/
  61. http://scikit-learn.org/
  62. https://keras.io/
  63. https://www.numpy.org/
  64. https://www.scipy.org/
  65. http://www.kdnuggets.com/2015/06/top-20-python-machine-learning-open-source-projects.html
  66. https://github.com/theano/theano
  67. https://www.tensorflow.org/
  68. https://github.com/rare-technologies/movie-plots-by-genre
  69. https://github.com/rare-technologies/movie-plots-by-genre
  70. https://radimrehurek.com/gensim/models/id97.html
  71. https://radimrehurek.com/gensim/models/doc2vec.html
  72. https://mimno.infosci.cornell.edu/info6150/readings/398.pdf
  73. http://svn.aksw.org/papers/2015/wsdm_topic_evaluation/public.pdf
  74. https://arxiv.org/pdf/1405.4053v2.pdf
  75. https://en.wikipedia.org/wiki/hierarchical_dirichlet_process
  76. https://en.wikipedia.org/wiki/latent_dirichlet_allocation
  77. https://en.wikipedia.org/wiki/latent_semantic_analysis
  78. https://en.wikipedia.org/wiki/singular_value_decomposition
  79. https://en.wikipedia.org/wiki/random_projection
  80. https://en.wikipedia.org/wiki/tf   idf
  81. https://arxiv.org/abs/1301.3781
  82. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  83. https://en.wikipedia.org/wiki/dynamic_topic_model
  84. https://github.com/rare-technologies/gensim/pull/1244
  85. http://radimrehurek.com/gensim/apiref.html
  86. http://scikit-learn.org/stable/developers/contributing.html#apis-of-scikit-learn-objects
  87. https://github.com/rare-technologies/gensim/pull/1244
  88. https://github.com/rare-technologies/gensim/pull/932
  89. https://github.com/rare-technologies/gensim/pull/1248
  90. http://qwone.com/~jason/20newsgroups/
  91. https://pythonhosted.org/shorttext/
  92. http://gibbslda.sourceforge.net/fp224-phan.pdf
  93. https://github.com/stephenhky/pyshorttextcategorization/blob/master/shorttext/classifiers/bow/topic/latenttopicmodeling.py
  94. https://github.com/stephenhky/pyshorttextcategorization/blob/master/shorttext/classifiers/bow/topic/sklearnclassification.py
  95. https://github.com/stephenhky/pyshorttextcategorization/tree/master/shorttext/classifiers/embed/nnlib
  96. https://github.com/stephenhky/pyshorttextcategorization/tree/master/shorttext/classifiers/embed/sumvec
  97. https://github.com/rare-technologies/gensim/pull/1248
  98. https://github.com/rare-technologies/gensim/pull/1244
  99. https://github.com/rare-technologies/gensim/pull/1201
 100. https://gitter.im/rare-technologies/gensim
 101. https://groups.google.com/forum/#!forum/gensim
 102. https://chinmayapancholi13.github.io/
 103. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#implementation-details
 104. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#implementation-details
 105. https://spacy.io/
 106. http://textacy.readthedocs.io/en/latest/
 107. https://github.com/chartbeat-labs/textacy/blob/master/textacy/tm/topic_model.py
 108. http://www.pregbuddy.com/
 109. https://api.ai/
 110. https://www.google.co.in/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahukewjzr4lo-_vsahxkqi8khrl0dnyqfggfmae&url=https://play.google.com/store/apps/details?id=com.pregbuddy&hl=en&usg=afqjcngtijv3gq6wodqckxcj_g7gfkgkxa&sig2=bi17m_wm52kxbe5urxeusq
 111. http://www.nomura.com/
 112. http://ltrc.iiit.ac.in/
 113. https://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.similar_by_word
 114. https://github.com/chinmayapancholi13/derivational_task_mlp
 115. https://github.com/chinmayapancholi13/mnist_id98
 116. https://github.com/chinmayapancholi13/analogy_task_mlp
 117. https://github.com/chinmayapancholi13/mnist_tensorflow
 118. https://github.com/chinmayapancholi13/mnist_multilayerid88
 119. https://github.com/chinmayapancholi13/similarity_task_glove
 120. https://github.com/chinmayapancholi13/search_algorithms_eightpuzzleproblem
 121. https://drive.google.com/open?id=0b2iiuxztfxq_uwzwtknjqkxzdwc
 122. https://github.com/rare-technologies/gensim/pull/1244
 123. https://github.com/rare-technologies/gensim/pull/1209
 124. https://github.com/rare-technologies/gensim/issues/863
 125. https://github.com/rare-technologies/gensim/pull/1228
 126. https://github.com/rare-technologies/gensim/pull/1248
 127. https://github.com/rare-technologies/gensim/pull/1207
 128. https://groups.google.com/forum/#!searchin/gensim/_minimize_model$20in$20id97|sort:relevance/gensim/pxttbvslixc/sllehcjregaj
 129. https://github.com/rare-technologies/gensim/pull/1201
 130. https://github.com/rare-technologies/gensim/issues/999
 131. https://github.com/rare-technologies/gensim/pull/1206
 132. https://github.com/rare-technologies/gensim/pull/1240
 133. https://github.com/stephenhky/pyshorttextcategorization/pull/2
 134. http://radimrehurek.com/gensim/
 135. http://scikit-learn.org/
 136. https://keras.io/
 137. https://www.numpy.org/
 138. https://www.scipy.org/
 139. http://www.kdnuggets.com/2015/06/top-20-python-machine-learning-open-source-projects.html
 140. https://github.com/theano/theano
 141. https://www.tensorflow.org/
 142. https://github.com/rare-technologies/movie-plots-by-genre
 143. https://radimrehurek.com/gensim/models/id97.html
 144. https://radimrehurek.com/gensim/models/doc2vec.html
 145. https://mimno.infosci.cornell.edu/info6150/readings/398.pdf
 146. http://svn.aksw.org/papers/2015/wsdm_topic_evaluation/public.pdf
 147. https://arxiv.org/pdf/1405.4053v2.pdf
 148. https://en.wikipedia.org/wiki/hierarchical_dirichlet_process
 149. https://en.wikipedia.org/wiki/latent_dirichlet_allocation
 150. https://en.wikipedia.org/wiki/latent_semantic_analysis
 151. https://en.wikipedia.org/wiki/singular_value_decomposition
 152. https://en.wikipedia.org/wiki/random_projection
 153. https://en.wikipedia.org/wiki/tf   idf
 154. https://arxiv.org/abs/1301.3781
 155. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
 156. https://en.wikipedia.org/wiki/dynamic_topic_model
 157. http://radimrehurek.com/gensim/apiref.html
 158. http://scikit-learn.org/stable/developers/contributing.html#apis-of-scikit-learn-objects
 159. https://github.com/rare-technologies/gensim/pull/932
 160. http://qwone.com/~jason/20newsgroups/
 161. https://pythonhosted.org/shorttext/
 162. http://gibbslda.sourceforge.net/fp224-phan.pdf
 163. https://spacy.io/
 164. http://textacy.readthedocs.io/en/latest/
 165. https://github.com/site/terms
 166. https://github.com/site/privacy
 167. https://github.com/security
 168. https://githubstatus.com/
 169. https://help.github.com/
 170. https://github.com/contact
 171. https://github.com/pricing
 172. https://developer.github.com/
 173. https://training.github.com/
 174. https://github.blog/
 175. https://github.com/about
 176. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md
 177. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md

   hidden links:
 179. https://github.com/
 180. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md
 181. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md
 182. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md
 183. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#gensim-integration-with-scikit-learn-and-keras
 184. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#contact-information
 185. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#abstract
 186. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#technical-details
 187. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#background
 188. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#unsupervised-learning-
 189. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#supervised-learning-
 190. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#scikit-learn-
 191. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#keras-
 192. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#a-task-for-motivation--movie-plot-classification
 193. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#implementation-details
 194. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#example-implementation-for-a-wrapper-for-scikit-learn
 195. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#example-implementation-for-a-wrapper-for-keras
 196. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#related-work
 197. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#schedule-of-deliverables
 198. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#may-1st---may-28th-community-bonding-period
 199. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#may-29th---june-3rd
 200. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#june-5th---june-9th
 201. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#june-12th---june-16th
 202. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#june-19th---june-23th-end-of-phase-1
 203. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#june-26---june-30th-begin-of-phase-2
 204. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#july-3rd---july-7th
 205. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#july-10th---july-14th
 206. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#july-17th---july-21th-end-of-phase-2
 207. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#july-24th---july-28th-begin-of-phase-3
 208. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#july-31st---august-4th
 209. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#august-7th---august-11th
 210. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#august-14th---august-18th
 211. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#august-21st---august-25th-final-week
 212. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#august-28th---august-29th-submit-final-work
 213. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#future-works
 214. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#development-experience
 215. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#other-experiences
 216. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#contributions
 217. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#why-this-project
 218. https://github.com/numfocus/gsoc/blob/master/2017/proposals/chinmaya_pancholi.md#appendix
 219. https://github.com/
