   [1]tensorflow is an open source library for numerical computation,
   specializing in machine learning applications.

what you will build

   in this codelab, you will learn how to run tensorflow on a single
   machine, and will train a simple classifier to classify images of
   flowers.

   [3021186b83bc90c2.png]

   image cc-by by retinafunk
daisy (score = 0.99071)
sunflowers (score = 0.00595)
dandelion (score = 0.00252)
roses (score = 0.00049)
tulips (score = 0.00032)

   we will be using id21, which means we are starting with a
   model that has been already trained on another problem. we will then
   retrain it on a similar problem. deep learning from scratch can take
   days, but id21 can be done in short order.

   we are going to use a model trained on the [2]id163 large visual
   recognition challenge [3]dataset. these models can differentiate
   between 1,000 different classes, like dalmatian or dishwasher. you will
   have a choice of model architectures, so you can determine the right
   tradeoff between speed, size and accuracy for your problem.

   we will use this same model, but retrain it to tell apart a small
   number of classes based on our own examples.

what you'll learn

     * how to use python and tensorflow to train an image classifier
     * how to classify images with your trained classifier

what you need

     * a basic understanding of linux commands

install tensorflow

   before we can begin the tutorial you need to [4]install tensorflow
   version 1.7.

   this codelab was tested on tensorflow 1.7

   > pip install --upgrade "tensorflow==1.7.*"

clone the git repository

   all the code used in this codelab is contained in this git repository.
   clone the repository and cd into it. this is where we will be working.
git clone https://github.com/googlecodelabs/tensorflow-for-poets-2

cd tensorflow-for-poets-2

   before you start any training, you'll need a set of images to teach the
   model about the new classes you want to recognize. we've created an
   archive of creative-commons licensed flower photos to use initially.
   download the photos (218 mb) by invoking the following two commands:
curl http://download.tensorflow.org/example_images/flower_photos.tgz \
    | tar xz -c tf_files

   you should now have a copy of the flower photos. confirm the contents
   of your working directory by issuing the following command:
ls tf_files/flower_photos

   the preceding command should display the following objects:
daisy/
dandelion/
roses/
sunflowers/
tulip/
license.txt

configure your mobilenet

   in this exercise, we will retrain a [5]mobilenet. mobilenet is a a
   small efficient convolutional neural network. "convolutional" just
   means that the same calculations are performed at each location in the
   image.

   the mobilenet is configurable in two ways:
     * input image resolution: 128,160,192, or 224px. unsurprisingly,
       feeding in a higher resolution image takes more processing time,
       but results in better classification accuracy.
     * the relative size of the model as a fraction of the largest
       mobilenet: 1.0, 0.75, 0.50, or 0.25.

   we will use 224 0.5 for this codelab.

   with the recommended settings, it typically takes only a couple of
   minutes to retrain on a laptop. you will pass the settings inside linux
   shell variables. set those variables in your shell:
image_size=224
architecture="mobilenet_0.50_${image_size}"

  more about mobilenet performance (optional)

   the graph below shows the first-choice-accuracies of these
   configurations (y-axis), vs the number of calculations required
   (x-axis), and the size of the model (circle area).

   16 points are shown for mobilenet. for each of the 4 model sizes
   (circle area in the figure) there is one point for each image
   resolution setting. the 128px image size models are represented by the
   lower-left point in each set, while the 224px models are in the upper
   right.

   other notable architectures are also included for reference.
   "googlenet" in this figure is [6]"inception v1" in this table. an
   extended version of this figure is available in [7]slides 84-89 here.

   [70170cbb89d318b1.png]

start tensorboard

   before starting the training, launch tensorboard in the background.
   tensorboard is a monitoring and inspection tool included with
   tensorflow. you will use it to monitor the training progress.
tensorboard --logdir tf_files/training_summaries &

   this command will fail with the following error if you already have a
   tensorboard process running:
   error:tensorflow:tensorboard attempted to bind to port 6006, but it was
   already in use

   you can kill all existing tensorboard instances with:

   pkill -f "tensorboard"

investigate the retraining script

   the retrain script is from the [8]tensorflow hub repo, but it is not
   installed as part of the pip package. so for simplicity i've included
   it in the codelab repository. you can run the script using the python
   command. take a minute to skim its "help".
python -m scripts.retrain -h

run the training

   as noted in the introduction, id163 models are networks with
   millions of parameters that can differentiate a large number of
   classes. we're only training the final layer of that network, so
   training will end in a reasonable amount of time.

   start your retraining with one big command (note the --summaries_dir
   option, sending training progress reports to the directory that
   tensorboard is monitoring) :
python -m scripts.retrain \
  --bottleneck_dir=tf_files/bottlenecks \
  --how_many_training_steps=500 \
  --model_dir=tf_files/models/ \
  --summaries_dir=tf_files/training_summaries/"${architecture}" \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --architecture="${architecture}" \
  --image_dir=tf_files/flower_photos

   note that this step will take a while.

   this script downloads the pre-trained model, adds a new final layer,
   and trains that layer on the flower photos you've downloaded.

   if you are using docker and the above command fails reporting:
   erro[xxxx] error getting events from daemon: eof

   you have likely encountered [9]this bug. increase your docker cpu
   allocation to 4 or more, on osx you can set this by selecting
   "preferences..." from the docker menu, the setting is on the "advanced"
   tab.

   id163 does not include any of these flower species we're training on
   here. however, the kinds of information that make it possible for
   id163 to differentiate among 1,000 classes are also useful for
   distinguishing other objects. by using this pre-trained network, we are
   using that information as input to the final classification layer that
   distinguishes our flower classes.

   note: the rest of this section is not essential. if your model is done
   training and and you want to see how it did, feel free to skip ahead.
   "training and tensorboard" will show you how to check your progress in
   tensorboard.

   "using the retrained model" will show you how to try the retrained
   model.

  optional: i'm not in a hurry!

   the first retraining command iterates only 500 times. you can very
   likely get improved results (i.e. higher accuracy) by training for
   longer. to get this improvement, remove the parameter
   --how_many_training_steps to use the default 4,000 iterations.
python -m scripts.retrain \
  --bottleneck_dir=tf_files/bottlenecks \
  --model_dir=tf_files/models/"${architecture}" \
  --summaries_dir=tf_files/training_summaries/"${architecture}" \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --architecture="${architecture}" \
  --image_dir=tf_files/flower_photos

more about bottlenecks (optional)

   this section and the next provide background on how this retraining
   process works.

   the first phase analyzes all the images on disk and calculates the
   bottleneck values for each of them. what's a bottleneck?

   [ce5329e698bcd707.png]

   these id163 models are made up of many layers stacked on top of each
   other, a simplified picture of inception v3 from tensorboard, is shown
   above (all the details are available [10]in this paper, with a complete
   picture on page 6). these layers are pre-trained and are already very
   valuable at finding and summarizing information that will help classify
   most images. for this codelab, you are training only the last layer
   (final_training_ops in the figure below). while all the previous layers
   retain their already-trained state.

   [84a6154ed64fd0fb.png]

   in the above figure, the node labeled "softmax", on the left side, is
   the output layer of the original model. while all the nodes to the
   right of the "softmax" were added by the retraining script.

   the above figure is a screenshot from tensorboard. [11]you can open
   tensorboard in your browser, to get a better look at it. you will find
   it in the "graphs" tab.
   note that this will only work after the retrain script finished
   generating the "bottleneck" files.

   a bottleneck is an informal term we often use for the layer just before
   the final output layer that actually does the classification.
   "bottleneck" is not used to imply that the layer is slowing down the
   network. we use the term bottleneck because near the output, the
   representation is much more compact than in the main body of the
   network.

   every image is reused multiple times during training. calculating the
   layers behind the bottleneck for each image takes a significant amount
   of time. since these lower layers of the network are not being modified
   their outputs can be cached and reused.

   so the script is running the constant part of the network, everything
   below the node labeled bottlene... above, and caching the results.

   the command you ran saves these files to the bottlenecks/ directory. if
   you rerun the script, they'll be reused, so you don't have to wait for
   this part again.

   once the script finishes generating all the bottleneck files, the
   actual training of the final layer of the network begins.

   by default, this script runs 4,000 training steps. each step chooses 10
   images at random from the training set, finds their bottlenecks from
   the cache, and feeds them into the final layer to get predictions.
   those predictions are then compared against the actual labels, and the
   results of this comparison is used to update the final layer's weights
   through a id26 process.

   as it trains, you'll see a series of step outputs, each one showing
   training accuracy, validation accuracy, and the cross id178:
     * the training accuracy shows the percentage of the images used in
       the current training batch that were labeled with the correct
       class.
     * validation accuracy: the validation accuracy is the precision
       (percentage of correctly-labelled images) on a randomly-selected
       group of images from a different set.
     * cross id178 is a id168 that gives a glimpse into how well
       the learning process is progressing. (lower numbers are better.)

   the figures below show an example of the progress of the model's
   accuracy and cross id178 as it trains. if your model has finished
   generating the bottleneck files you can check your model's progress by
   [12]opening tensorboard, and clicking on the figure's name to show
   them. ignore any warnings that tensorboard prints to your command line.

   the first figure shows accuracy (y-axis) as a function of training
   progress (x-axis):

   [bc758910e1c6eee7.png]

   [ee5d91da57a84831.png]

   two lines are shown. the orange line shows the accuracy of the model on
   the training data. while the blue line shows the accuracy on the test
   set (which was not used for training). this is a much better measure of
   the true performance of the network. if the training accuracy continues
   to rise while the validation accuracy decreases then the model is said
   to be "overfitting". overfitting is when the model begins to memorize
   the training set instead of understanding general patterns in the data.

   as the process continues, you should see the reported accuracy improve.
   after all the training steps are complete, the script runs a final test
   accuracy evaluation on a set of images that are kept separate from the
   training and validation pictures. this test evaluation provides the
   best estimate of how the trained model will perform on the
   classification task.

   you should see an accuracy value of between 85% and 99%, though the
   exact value will vary from run to run since there's randomness in the
   training process. (if you are only training on two classes, you should
   expect higher accuracy.) this number value indicates the percentage of
   the images in the test set that are given the correct label after the
   model is fully trained.

   the retraining script writes data to the following two files:
     * tf_files/retrained_graph.pb, which contains a version of the
       selected network with a final layer retrained on your categories.
     * tf_files/retrained_labels.txt, which is a text file containing
       labels.

classifying an image

   the codelab repo also contains a copy of tensorflow's
   [13]label_image.py example, which you can use to test your network.
   take a minute to read the help for this script:
python -m scripts.label_image -h

   now, let's run the script on this image of a daisy:

   [3021186b83bc90c2.png]

   flower_photos/daisy/21652746_cc379e0eea_m.jpg

   image cc-by by retinafunk
python -m scripts.label_image \
    --graph=tf_files/retrained_graph.pb  \
    --image=tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpg

   each execution will print a list of flower labels, in most cases with
   the correct flower on top (though each retrained model may be slightly
   different).

   if you chose a mobilenet that takes a smaller input size, then be sure
   to set the --input_size flag using the shell variable you set earlier.

   --input_size=${image_size}

   you might get results like this for a daisy photo:
daisy (score = 0.99071)
sunflowers (score = 0.00595)
dandelion (score = 0.00252)
roses (score = 0.00049)
tulips (score = 0.00032)

   this indicates a high confidence (~99%) that the image is a daisy, and
   low confidence for any other label.

   you can use label_image.py to classify any image file you choose,
   either from your downloaded collection, or new ones. you just have to
   change the --image file name argument to the script.

   [28cf1a230dc7c51f.png]

   flower_photos/roses/2414954629_3708a1a04d.jpg

   image cc-by by lori branham
python -m scripts.label_image \
    --graph=tf_files/retrained_graph.pb  \
    --image=tf_files/flower_photos/roses/2414954629_3708a1a04d.jpg

   the retraining script has several other command line options you can
   use.

   you can read about these options in the help for the retrain script:
python -m scripts.retrain -h

   try adjusting some of these options to see if you can increase the
   final validation accuracy.

   for example, the --learning_rate parameter controls the magnitude of
   the updates to the final layer during training. so far we have left it
   out, so the program has used the default learning_rate value of 0.01.
   if you specify a small learning_rate, like 0.005, the training will
   take longer, but the overall precision might increase. higher values of
   learning_rate, like 1.0, could train faster, but typically reduces
   precision, or even makes training unstable.

   you need to experiment carefully to see what works for your case.

   it is very helpful for this type of work if you give each experiment a
   unique name, so they show up as separate entries in tensorboard.

   it's the --summaries_dir option that controls the name in tensorboard.
   earlier we used:

   --summaries_dir=training_summaries/basic

   tensorboard is monitoring the contents of the training_summaries
   directory, so setting --summarys_dir to training_summaries or any
   subdirectory of training_summaries will work.

   you may want to set the following two options together, so your results
   are clearly labeled:

   --learning_rate=0.5
   --summaries_dir=training_summaries/lr_0.5

   after you see the script working on the flower example images, you can
   start looking at teaching the network to recognize different
   categories.

   in theory, all you need to do is run the tool, specifying a particular
   set of sub-folders. each sub-folder is named after one of your
   categories and contains only images from that category.

   if you complete this step and pass the root folder of the
   subdirectories as the argument for the --image_dir parameter, the
   script should train the images that you've provided, just like it did
   for the flowers.

   the classification script uses the folder names as label names, and the
   images inside each folder should be pictures that correspond to that
   label, as you can see in the flower archive:

   [9444bbae4d5d9ab1.png]

   collect as many pictures of each label as you can and try it out!

   yufeng guo used these techniques to make his [14]candy sorter demo. one
   shortcut he used was to split the training images out of videos of
   objects from each category, using [15]ffmpeg.

   congratulations, you've taken your first steps into a larger world of
   deep learning!

   you can see more about using tensorflow at the [16]tensorflow website
   or the tensorflow github[17] project. there are lots of other resources
   available for tensorflow, including a [18]discussion group and
   [19]whitepaper.

   if you're interested in running tensorflow on mobile devices try the
   second part of this tutorial: there are three versions:
    1. [20]tflite android
    2. [21]tflite ios
    3. [22]tfmobile android

   or just go have some fun in the [23]tensorflow playground!

   this codelab is based on pete warden's [24]tensorflow for poets blog
   post and this [25]retraining tutorial.

references

   1. https://www.tensorflow.org/
   2. http://image-net.org/
   3. http://www.image-net.org/challenges/lsvrc/2012/
   4. https://www.tensorflow.org/versions/r1.7/install/
   5. https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html
   6. https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models
   7. http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf
   8. https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py
   9. https://github.com/moby/moby/issues/31220
  10. http://www.cs.unc.edu/~wliu/papers/googlenet.pdf
  11. http://0.0.0.0:6006/#graphs
  12. http://0.0.0.0:6006/
  13. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py
  14. https://youtu.be/enfynerscq8?t=4m17s
  15. http://stackoverflow.com/questions/10957412/fastest-way-to-extract-frames-using-ffmpeg
  16. https://www.tensorflow.org/
  17. https://github.com/tensorflow/
  18. https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss
  19. https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45166.pdf
  20. https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite
  21. https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-ios
  22. https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2
  23. http://playground.tensorflow.org/
  24. https://petewarden.com/2016/02/28/tensorflow-for-poets/
  25. https://www.tensorflow.org/tutorials/image_retraining
