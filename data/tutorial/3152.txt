   #[1]angeliki lazaridou

   [2]about [3]call for papers [4]shared task

shared task

the repeval 2017 shared task

introduction

   repeval 2017 features a shared task meant to evaluate natural language
   understanding models based on sentence encoders   that is, models that
   transform sentences into fixed-length vector representations and reason
   using those representations. the task will be natural language
   id136 (also known as recognizing id123, or rte) in the
   style of [5]snli   a three-class balanced classification problem over
   sentence pairs. the shared task will feature a new, dedicated dataset
   that spans several genres of text. participation is open and new teams
   may join at any time.

   the shared task includes two evaluations, a standard in-domain
   (matched) evaluation in which the training and test data are drawn from
   the same sources, and a cross-domain (mismatched) evaluation in which
   the training and test data differ substantially. this cross-domain
   evaluation will test the ability of submitted systems to learn
   representations of sentence meaning that capture broadly useful
   features.

results

   the following table presents the results as of the close of the
   competition on june 15. all numbers reflect accuracy on the hidden
   portion of the test set.
   team authors matched acc.  mismatched acc.
   alpha (ensemble) [6]qian chen et al. 74.9% 74.9%
   yixinnie-unc-nlp [7]yixin nie and mohit bansal 74.5% 73.5%
   alpha [8]qian chen et al. 73.5% 73.6%
   rivercorners (ensemble) [9]jorge balazs et al. 72.2% 72.8%
   rivercorners [10]jorge balazs et al. 72.1% 72.1%
   lct-malta [11]hoa vu et al. 70.7% 70.8%
   talp-upc [12]han yang et al. 67.9% 68.2%
   bilstm baseline [13]williams et al. 67.0% 67.6%

recap paper

   [14]competition recap paper

the data

   the training and development sections of the task data can be found
   [15]here. the unlabeled test data is available through the kaggle in
   class competiton pages ([16]matched, [17]mismatched).

   the task dataset (called the multi-genre nli corpus, or multinli)
   consist of 393k training examples drawn from five genres of text, and
   40k test and development examples drawn from those same five genres, as
   well as five more. data collection for the task dataset is be closely
   modeled on [18]snli, which is based on the genre of image captions, and
   may be used as additional training and development data, but will not
   be included in the evaluation.

   section name            training pairs  dev pairs  test pairs
   captions (snli corpus)  (550,152)       (10,000)   (10,000)
   fiction                 77,348          2,000      2,000
   government              77,350          2,000      2,000
   slate                   77,306          2,000      2,000
   telephone speech        83,348          2,000      2,000
   travel guides           77,350          2,000      2,000
   9/11 report             0               2,000      2,000
   face-to-face speech     0               2,000      2,000
   letters                 0               2,000      2,000
   nonfiction books (oup)  0               2,000      2,000
   magazine (verbatim)     0               2,000      2,000
   total                   392,702         20,000     20,000

   as in snli, each example will consist of two sentences and a label. the
   first sentence is drawn from a preexisting text source   either one of
   the sections of the [19]open american national corpus (openanc) or some
   other permissively licensed source. the second sentence is written by
   crowd workers as part of data collection. data for each genre will be
   collected in a separate id104 task. the labels will be
   entailment, neutral, and contradiction, in roughly equal proportions.
   some examples from the corpus can be seen below.

   premise label hypothesis
   fiction
   the old one always comforted ca'daan, except today. neutral ca'daan
   knew the old one very well.
   letters
   your gift is appreciated by each and every student who will benefit
   from your generosity. neutral hundreds of students will benefit from
   your generosity.
   telephone speech
   yes now you know if if everybody like in august when everybody's on
   vacation or something we can dress a little more casual or
   contradiction august is a black out month for vacations in the company.
   9/11 report
   at the other end of pennsylvania avenue, people began to line up for a
   white house tour. entailment people formed a line at the end of
   pennsylvania avenue.

   data is available in the same two formats as snli: tab-separated values
   and jsonl. it will take the form of five files in each format: train,
   in-domain development, cross-domain development, in-domain test, and
   cross-domain test. each individual example will be marked with a genre
   tag.

   we are also separately distributing a [20]small subset of the
   development data that has been manually annotated to facilitate error
   analysis.

rules and evaluation

   evaluation
     * evaluation will be done using the kaggle platform. during the
       evaluation period, participants download an unlabeled copy of the
       test set, use their systems to predict labels, and upload those
       labels. standard kaggle rules apply.
     * participants may submit to either or both of the two evaluations
       (in-domain or cross-domain).
     * multiple submissions from the same team are allowed, up to a limit
       of two per day during the two-week evaluation period. individual
       participants (i.e., pis) may join multiple teams within reason, but
       only when each team reflects a fully independent engineering
       effort, and each team has a different lead developer. (note: kaggle
       may not allow entrants to formally join multiple teams. in this
       case, pis and others spanning multiple teams should join at most a
       single kaggle team and only disclose their memberships in their
       paper submission.)

   systems
     * this competition is meant to evaluate the quality of vector
       representations of sentences, and all submitted systems should
       include a bottleneck in which sentences are represented as
       fixed-length vectors with no explicitly-imposed internal structure.
       typical attention and memory models that represent sentences as
       sets or sequences of vectors, though useful for tasks like nli, are
       not eligible for inclusion in this competition. (it is allowed,
       should it be useful, to use two separate models to encode the two
       input sentences.)
     * the development sets are to be used for model selection and the
       tuning of reasonable hyperparameters. models that are explicitly
       trained on the development data may be disqualified.
     * models should make their predictions for each example
       independantly. it is the case that different pairs sharing the same
       premise typically have different labels (as an artificact of how we
       created the data), but systems are not allowed to exploit this
       signal to model joint distributions over multiple examples at once.
       (if you aren   t sure whether this applies to your system, it
       probably doesn   t.)
     * for inclusion in the workshop and the final leaderboard,
       participants will have to upload a code package that can be used to
       reproduce the submitted results. this code package will not be used
       as the primary means of evaluation, but it will be made public to
       encourage both reproducibility and future extension of submitted
       models.
     * for inclusion in the workshop and the final leaderboard,
       participants will also be asked to provide the sentence vectors
       produced by their best performing model. when you submit your
       system, you will also be asked to upload a link to a sentence
       vector file with lines of the form    pairid \t p or h (for premise
       or hypothesis) \t sentence representation vector as
       whitespace-delimited (tab or space) values. for example, a line
       might look like:

   123  p       0.27204 -0.06203 -0.1884 0.023225 -0.018158 0.0067192 -0.13877 0
.17708 0.17709 ...

   you should supply vectors for every sentence (premise and hypothesis)
   in the test set(s) for the competition(s) you   re submitting to. in
   addition, you are also asked to submit vectors for a set of additional
   probe sentences. these sentences are [21]here, distributed in the same
   format as multinli, but with one sentence per line, marked as the
   premise, and no hypothesis. all your vectors should be concatenated
   into a single file.

   outside data
     * the use of outside data is allowed, including raw unannotated text
       from any source (including [22]openanc and the other sources from
       which multinli was derived), word vector packages, and knowledge
       resources like id138.
     * all outside data used must be publicly available to allow for
       reproducibility. widely-used data with restrictive licenses or
       licensing fees (such as ldc-distributed corpora) may be allowed at
       our discretion. please inquire at the qa forum below.

baselines

     * the [23]corpus description paper presents the following baselines:

   model                matched test acc.  mismatched test acc.
   most frequent class  36.5               35.6
   cbow                 65.2               64.6
   bilstm               67.5               67.1

     * note that the paper also presents results with an [24]esim. that
       model relies on attention between sentences and would be ineligible
       for inclusion in this competition.
     * both models are trained on a mix of multinli and snli and use
       [25]glove word vectors.
     * code (tensorflow/python) is available [26]here.

leaderboard and evaluation site

   to participate in the competition, evaluate your system, or view the
   current mid-competiton leaderboard (including systems that may not
   qualify for the final leaderboard), use these two kaggle in class
   competitions:
     * [27]multinli matched
     * [28]multinli mismatched

paper submission

   for inclusion in the workshop and the final leaderboard, you must
   submit:
     * a system description paper of 2   4 pages in emnlp format. system
       description papers will be reviewed for readability and soundness
       (but not novelty/technical merit) before acceptance.
     * a .zip code package (as a link from your paper) that can be used to
       reproduce the submitted results after being trained on
       widely-available data files.
     * a url for a vector package, as discussed above.

   paper prepration and uploading instructions can be found in the
   [29]call for papers.

key dates

     * march 24: [30]training and development data and draft data
       description paper available, competition begins
     * by may 15: [31]expert-tagged development data for error analysis
       available
     * june 1: unlabeled test data available, evaluation period begins,
       kaggle evaluation site opens
     * june 14 (gmt-11, 23:59:59): evaluation period ends, [32]system
       description papers and code packages due
     * june 16: results announced (see above)
     * july 3 (gmt-11, 23:59:59): reviews due
     * july 6: notification of presentation acceptance
     * july 21 (gmt-11, 23:59:59): camera ready papers due
     * september 8: workshop at emnlp 2017, copenhagen: shared task poster
       session and selected short talks

join us!

     * announcements list (all participants should subscribe): [33]google
       group
     * discussion/faq forum (ask questions here first): [34]google forum
     * private questions: [35]sam bowman

     * e-mail: pending. contact the organizers directly with concerns.

references

   visible links
   1. https://repeval2017.github.io/feed.xml
   2. https://repeval2017.github.io/
   3. https://repeval2017.github.io/call/
   4. https://repeval2017.github.io/shared/
   5. http://nlp.stanford.edu/projects/snli/
   6. http://aclweb.org/anthology/w17-5307
   7. http://aclweb.org/anthology/w17-5308
   8. http://aclweb.org/anthology/w17-5307
   9. http://aclweb.org/anthology/w17-5310
  10. http://aclweb.org/anthology/w17-5310
  11. http://aclweb.org/anthology/w17-5311
  12. http://aclweb.org/anthology/w17-5309
  13. https://www.nyu.edu/projects/bowman/multinli/paper.pdf
  14. http://aclweb.org/anthology/w17-5301
  15. http://nyu.edu/projects/bowman/multinli/
  16. https://inclass.kaggle.com/c/multinli-matched-evaluation/data
  17. https://inclass.kaggle.com/c/multinli-mismatched-evaluation/data
  18. http://nlp.stanford.edu/projects/snli/
  19. https://www.anc.org/oanc/
  20. https://www.nyu.edu/projects/bowman/multinli/multinli_0.9_annotations.zip
  21. http://nyu.edu/projects/bowman/multinli/extra-sentences-needing-vectors.zip
  22. https://www.anc.org/oanc/
  23. http://nyu.edu/projects/bowman/multinli
  24. https://arxiv.org/abs/1609.06038
  25. https://nlp.stanford.edu/projects/glove/
  26. https://github.com/nyu-mll/multinli
  27. https://inclass.kaggle.com/c/multinli-matched-evaluation
  28. https://inclass.kaggle.com/c/multinli-mismatched-evaluation
  29. https://repeval2017.github.io/call/
  30. http://nyu.edu/projects/bowman/multinli
  31. https://www.nyu.edu/projects/bowman/multinli/multinli_0.9_annotations.zip
  32. https://repeval2017.github.io/call/
  33. https://groups.google.com/forum/#!forum/repeval17-annouce
  34. https://groups.google.com/forum/#!forum/repeval17-qa/
  35. mailto:bowman@nyu.edu

   hidden links:
  37. https://repeval2017.github.io/shared/
