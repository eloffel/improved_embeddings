multinli

the multi-genre nli corpus

   [1]adina williams
   [2]nikita nangia
   [3]sam bowman
   nyu

introduction

   the multi-genre natural language id136 (multinli) corpus is a
   crowd-sourced collection of 433k sentence pairs annotated with textual
   entailment information. the corpus is modeled on the snli corpus, but
   differs in that covers a range of genres of spoken and written text,
   and supports a distinctive cross-genre generalization evaluation. the
   corpus served as the basis for the shared task of the [4]repeval 2017
   workshop at emnlp in copenhagen.

examples

   premise label hypothesis
   fiction
   the old one always comforted ca'daan, except today. neutral ca'daan
   knew the old one very well.
   letters
   your gift is appreciated by each and every student who will benefit
   from your generosity. neutral hundreds of students will benefit from
   your generosity.
   telephone speech
   yes now you know if if everybody like in august when everybody's on
   vacation or something we can dress a little more casual or
   contradiction august is a black out month for vacations in the company.
   9/11 report
   at the other end of pennsylvania avenue, people began to line up for a
   white house tour. entailment people formed a line at the end of
   pennsylvania avenue.

download

   multinli is distributed in a single zip file containing the corpus as
   both json lines (jsonl) and tab-separated text (txt).

   download: [5]multinli 1.0 (227mb, zip)

previous versions

   multinli 0.9 differs from multinli 1.0 only in the pairid and promptid
   fields in the training and development sets (and the attached paper),
   so results achieved on version 0.9 are still valid on 1.0. version 0.9
   can be downloaded [6]here.

the stanford nli corpus (snli)

   multinli is modeled after snli. the two corpora are distributed in the
   same formats, and for many applications, it may be productive to treat
   them as a single, larger corpus. you can find out more about snli
   [7]here and download it from an nyu mirror [8]here.

data description paper and citation

   a description of the data can be found [9]here (pdf) or in the corpus
   package zip. if you use the corpus in an academic paper, please cite
   us:

@inproceedings{n18-1101,
  author = "williams, adina
            and nangia, nikita
            and bowman, samuel",
  title = "a broad-coverage challenge corpus for
           sentence understanding through id136",
  booktitle = "proceedings of the 2018 conference of
               the north american chapter of the
               association for computational linguistics:
               human language technologies, volume 1 (long
               papers)",
  year = "2018",
  publisher = "association for computational linguistics",
  pages = "1112--1122",
  location = "new orleans, louisiana",
  url = "http://aclweb.org/anthology/n18-1101"
}

baselines, code, and analysis

   the [10]data description paper presents the following baselines:
   model               matched test acc.  mismatched test acc.
   most frequent class 36.5%              35.6%
   cbow                65.2%              64.6%
   bilstm              67.5%              67.1%
   esim                72.4%              71.9%

   note that the esim relies on attention between sentences and would be
   ineligible for inclusion in the repeval competition. all three models
   are trained on a mix of multinli and snli and use [11]glove word
   vectors. code (tensorflow/python) is available [12]here, alongside a
   script to reproduce the categories used in the error analysis in the
   paper.

   additional analysis-oriented datasets are available as part of [13]glue
   and [14]here.

test set and leaderboard

   to evaluate your system on the full test set, use the following kaggle
   in class competitions. you do not need to submit code to evaluate your
   model, and you may evaluate under a psuedonym, but you are expected to
   post a brief description of your model in the competition discussion
   board.
     * [15]multinli matched
     * [16]multinli mismatched

   these competitions will be open indefinitely. evaluations on a subset
   of the test set had previously been conducted with different
   leaderboards through the [17]repeval 2017 workshop.

   researchers interested in id72 and general-purpose
   representation learning can also access the test set through a separate
   leaderboard on the [18]glue platform.

   the best result (state of the art) that we've seen written up in a
   paper is 82.1/81.4 from [19]radford et al. 2018.

related resources

   the [20]xnli corpus provides additional development and test sets for
   multinli in fifteen languages.

   evaluations on the hard subset of the test set used in [21]gururangan
   et al. '18 are available separately ([22]matched/[23]mismatched).

license

   see details in the [24]data description paper.

thanks

   this work was made possible by a google faculty research award. we also
   thank george dahl, the organizers of the repeval 2016 and repeval 2017
   workshops, andrew drozdov, angeliki lazaridou, and our other nyu
   colleagues for help and advice.

references

   1. https://wp.nyu.edu/adinawilliams/
   2. https://woollysocks.github.io/
   3. http://www.nyu.edu/projects/bowman/
   4. https://repeval2017.github.io/shared/
   5. http://www.nyu.edu/projects/bowman/multinli/multinli_1.0.zip
   6. http://www.nyu.edu/projects/bowman/multinli/multinli_0.9.zip
   7. https://nlp.stanford.edu/projects/snli/
   8. http://www.nyu.edu/projects/bowman/multinli/snli_1.0.zip
   9. http://www.nyu.edu/projects/bowman/multinli/paper.pdf
  10. http://www.nyu.edu/projects/bowman/multinli/paper.pdf
  11. https://nlp.stanford.edu/projects/glove/
  12. https://github.com/nyu-mll/multinli
  13. https://gluebenchmark.com/
  14. https://abhilasharavichander.github.io/nli_stresstest/
  15. https://inclass.kaggle.com/c/multinli-matched-open-evaluation
  16. https://inclass.kaggle.com/c/multinli-mismatched-open-evaluation
  17. https://repeval2017.github.io/shared/
  18. https://gluebenchmark.com/
  19. https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
  20. http://www.nyu.edu/projects/bowman/xnli
  21. https://arxiv.org/pdf/1803.02324.pdf
  22. https://www.kaggle.com/c/multinli-matched-open-hard-evaluation/
  23. https://www.kaggle.com/c/multinli-mismatched-open-hard-evaluation/
  24. http://www.nyu.edu/projects/bowman/multinli/paper.pdf
