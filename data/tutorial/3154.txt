   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

how to train a neural network to code by itself ?

   [14]go to the profile of thibault neveu
   [15]thibault neveu (button) blockedunblock (button) followfollowing
   may 30, 2017
   [1*o1rrml rsjz95o_3rpia.jpeg]
   illustration by [16]l  o vallet

   let   s admit it would be quite crazy. a developer causing a neural
   network to replace it to code in its place ! ok, let   s do that.

   iframe: [17]/media/3fadb08c144b4c211ab9fc64d5c82fdb?postid=a432e8a120df

prerequisites

     * tensorflow + basic deep learning skills.
     * [18]the github repository of the project.
     * in this article, i will make a very quick reminder of recurrent
       neural network . however, if you don   t know much about the subject,
       these two resources seem good to me to begin to understand the
       subject: [19]a video and [20]an article. :)

   i will not explain in this article all the parts of the project. on the
   other hand, i will go into detail about the essential points that will
   enable you to understand it. take the time to execute each of the small
   pieces of code given in order to understand the logic. it   s important;
   it   s by doing that one learns.

   this time it   s the right one, we   ll go !

the database

   like any supervised workout, we   re going to need a dataset for our
   network. we are going to base this on code c (if it   s a language too
   easy, it   s not funny). to do this, our training data will be c scripts
   from the github linux repository. i have already extracted the .c that
   interests us [21]on the project.

   first problem : how to represent our data ?

   a neural network treats only numbers. everything else is unknown to
   him. thus, each character of our dataset should be represented in this
   form (a number / characters).
   [1*53ic3kqzaz5ixgf_5ilesa.png]
   example of character to int

   here, for example, the character    =    is assigned to the number 7. we
   will later represent each number in one hot encoding in order to better
   converge during the id26.

   iframe: [22]/media/752151d5e7fe7b564f3acd34b2b5eb6d?postid=a432e8a120df

   the three important variables to remember here are vocab_to_int,
   int_to_vocab and encoded. the first two allow us to easily switch
   between a character and an int and vice versa. the last is the
   representation of all our dataset in an encoder format. (only int
   instead of characters).

the first batch

   let   s create a simple batch consisting of two sequences where each
   sequence will consist of 10 numbers. this batch will serve as an
   example for the rest of this article.

   iframe: [23]/media/fea0e50254219d5e0a6ec505ae3e5c33?postid=a432e8a120df

batch inputs :
[20  6 58 27  6 27 97 86 56 49]
[ 36  32  32  37  27  12  94  60  89 101]
batch targets :
[ 6 58 27  6 27 97 86 56 49 57]
[ 32  32  37  27  12  94  60  89 101  77]

   that   s what the batch looks like. you can also display this
   translation:
['/', '*', '\n', ' ', '*', ' ', 'c', 'o', 'p', 'y']
['2', '0', '0', '4', ' ', 'e', 'v', 'g', 'e', 'n']

   now, we have some first values to work with. we want our network to be
   able to predict the next character to be typed knowing the n preceding
   characters. and not just the previous character. indeed if i say to my
   network that the last letter type is an    e    the possibilities of
   evolution are huge. however, if i tell him that the last type is    w   
      h       i       l    and    e    then it    s much more obvious that the next
   character to type will be a    (   .

   we must therefore create a neural network taking into account the
   temporal space of the characters type. to do this, we need to use a
   reccurent neural network.

recurrent neural network ?

   [1*frnak2uixix_tlwttxfdba.png]

   in order to illustrate the last example, a classic classifier (on the
   left of the diagram) takes the preceding letter; it   s passed by the
   hidden layer represented in blue in order to deduce an output. a
   recurring neural network is architecturally different. each cell
   (represented in red) is not only connected to the inputs, but also to
   the cell of the instant t-1. in order to solve our problem, we will use
   lstm (long short time memory) cells.

   feel free to spend time understanding the principle of recurrent neural
   networks in order to fully understand the code that will follow.

let   s build the model !

   [1*src4bcadhsro52alt_bqcw.png]
   tensorboard graph

   we will go into this article in detail of the 5 main parts. placeholder
   serving as an entry to our model. the initialization of our lstm cells
   used to create the id56.
   the output layer connected to each cell. the operation used to measure
   the model error. finally, we will define the training operation.

i) graph inputs

   iframe: [24]/media/b41025258c513cce4084fb31fcc6d7d1?postid=a432e8a120df

   the batch consists of two inputs of size 10, the shape expected for our
   input is therefore of size [2, 10], each entry of the batch being
   associated with a single output, we can define the same shape for our
   target. finally we define a placeholder for the value of the
   id203 used for the future dropout.

ii) lstm

   iframe: [25]/media/b7180a373c24b9731efecfa3d121127b?postid=a432e8a120df

   let   s study each part of this code :
     * create_cell() is used to create an lstm cell composed of 4 hidden
       neurons. this function also adds a dropout to the cell output
       before returning it to us.
     * tf.contrib.id56.multiid56cell is used to easily instantiate our id56.
       we give as a parameter an array of create_cell() because we want an
       id56 consisting of several layers. three in this example.
     * initial_state: knowing that each cell of an id56 depends on the
       previous state, we must instantiate an initial state filled with
       zero that will serve as input to the first entries of our batch.
     * x_one_hot transforms our batch into one hot encoding
     * cell_outputs gives us the output of each cell of our id56. here,
       each output will consist of 4 values (number of hidden neurons).
     * final_state returns the state of our last cell which can be used
       during training as a new initial state for a next batch. (assuming
       that the next batch is the logical sequel to the previous batch).

iii) graph outputs

   iframe: [26]/media/a63abe1f192cc29a6c9d238a8a80e53e?postid=a432e8a120df

   the values at the output of our cells are stored in a three-dimensional
   table [number of sequences, sequence size, number of neurons] or [2,
   10, 4]. we no longer need to separate the outputs by sequences. we then
   resize the output to get an array of dimension [20, 4] stored in the
   seq_out_reshape variable.

   finally, we apply a simple linear operation: tf.matmul (..) + b. the
   whole followed by a softmax in order to represent our outputs in the
   form of id203.

   iv) loss

   iframe: [27]/media/fbc2d0ef0c70d2d2bfba7dae8fcbd23b?postid=a432e8a120df

   in order to apply our error operation, the targets of our batch must be
   represented in the same way and in the same dimension as the output
   values of the model. we use tf.one_hot to represent our outputs under
   the same encoding as our inputs. then we resize the array (tf.reshape
   ()) to the same dimensions of the linear output: tf.matmul (..) + b. we
   can now use this function to calculate the error of the model.

   v) train

   iframe: [28]/media/e7586f54227811b399431894e85e29a3?postid=a432e8a120df

   we simply apply an adamoptimize to minimize our errors.

results !

   i think it   s finally one of the most rewarding parts: the results of
   the training. i have for this one used the following parameters:

       size of a sequence: 100
       size of a batch: 200
       number of neurons per cell: 512
       depth of id56: 2
       learning rate: 0.0005
       dropout: 0.5

   the results presented below were obtained after about two hours of
   training on my gpu. (geforce gtx 1060).

   let   s start with the evolution of the error:
   [1*rv7permpowuqlmzwfdgyfg.png]

   finally, let   s look at what type of code our model is capable of
   generating :

   iframe: [29]/media/486b7ce7a6fc21422dcd4e6b82c0dd00?postid=a432e8a120df

   it   s pretty cool to see that this model has clearly understood the
   general structure of a program. a function, parameters, initialization
   of variables, conditions     etc   .

   we will notice that there is absolutely no function named    super_fold   
   in our dataset. so i have a hard time understanding the usefulness of
   this function, it must be believed that this model is perhaps more
   intelligent than me   .

   iframe: [30]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=a432e8a120df

   [1*bqlrszfhjemf4q7pyrlgng.gif]
   [31][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [32][1*v-ppfkswhbvlwwamsvhhwg.png]
   [33][1*wt2auqisieaozxj-i7brdq.png]

     * [34]machine learning
     * [35]deep learning
     * [36]lstm
     * [37]artificial intelligence
     * [38]id56

   (button)
   (button)
   (button) 491 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [39]go to the profile of thibault neveu

[40]thibault neveu

   master   s degree student #ai #deeplearning #ml

     (button) follow
   [41]becoming human: artificial intelligence magazine

[42]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 491
     * (button)
     *
     *

   [43]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [44]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a432e8a120df
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/how-to-train-a-neural-network-to-code-by-itself-a432e8a120df&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/how-to-train-a-neural-network-to-code-by-itself-a432e8a120df&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_ggjj0wr9zzyu---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@thibo73800?source=post_header_lockup
  15. https://becominghuman.ai/@thibo73800
  16. https://www.linkedin.com/in/leovallet/
  17. https://becominghuman.ai/media/3fadb08c144b4c211ab9fc64d5c82fdb?postid=a432e8a120df
  18. https://github.com/thibo73800/deep_generation/tree/master/c_code
  19. https://www.youtube.com/watch?v=ix5v1wpxxky&t=2652s
  20. http://colah.github.io/posts/2015-08-understanding-lstms/
  21. https://github.com/thibo73800/deep_generation/tree/master/c_code/dataset
  22. https://becominghuman.ai/media/752151d5e7fe7b564f3acd34b2b5eb6d?postid=a432e8a120df
  23. https://becominghuman.ai/media/fea0e50254219d5e0a6ec505ae3e5c33?postid=a432e8a120df
  24. https://becominghuman.ai/media/b41025258c513cce4084fb31fcc6d7d1?postid=a432e8a120df
  25. https://becominghuman.ai/media/b7180a373c24b9731efecfa3d121127b?postid=a432e8a120df
  26. https://becominghuman.ai/media/a63abe1f192cc29a6c9d238a8a80e53e?postid=a432e8a120df
  27. https://becominghuman.ai/media/fbc2d0ef0c70d2d2bfba7dae8fcbd23b?postid=a432e8a120df
  28. https://becominghuman.ai/media/e7586f54227811b399431894e85e29a3?postid=a432e8a120df
  29. https://becominghuman.ai/media/486b7ce7a6fc21422dcd4e6b82c0dd00?postid=a432e8a120df
  30. https://becominghuman.ai/media/c43026df6fee7cdb1aab8aaf916125ea?postid=a432e8a120df
  31. https://medium.com/becoming-human/artificial-intelligence-communities-c305f28e674c
  32. https://upscri.be/8f5f8b
  33. https://medium.com/becoming-human/write-for-us-48270209de63
  34. https://becominghuman.ai/tagged/machine-learning?source=post
  35. https://becominghuman.ai/tagged/deep-learning?source=post
  36. https://becominghuman.ai/tagged/lstm?source=post
  37. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  38. https://becominghuman.ai/tagged/id56?source=post
  39. https://becominghuman.ai/@thibo73800?source=footer_card
  40. https://becominghuman.ai/@thibo73800
  41. https://becominghuman.ai/?source=footer_card
  42. https://becominghuman.ai/?source=footer_card
  43. https://becominghuman.ai/
  44. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  46. https://medium.com/p/a432e8a120df/share/twitter
  47. https://medium.com/p/a432e8a120df/share/facebook
  48. https://medium.com/p/a432e8a120df/share/twitter
  49. https://medium.com/p/a432e8a120df/share/facebook
