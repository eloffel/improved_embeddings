   [1]cs231n convolutional neural networks for visual recognition

   table of contents:
     * [2]gradient checks
     * [3]sanity checks
     * [4]babysitting the learning process
          + [5]id168
          + [6]train/val accuracy
          + [7]weights:updates ratio
          + [8]activation/gradient distributions per layer
          + [9]visualization
     * [10]parameter updates
          + [11]first-order (sgd), momentum, nesterov momentum
          + [12]annealing the learning rate
          + [13]second-order methods
          + [14]per-parameter adaptive learning rates (adagrad, rmsprop)
     * [15]hyperparameter optimization
     * [16]evaluation
          + [17]model ensembles
     * [18]summary
     * [19]additional references

learning

   in the previous sections we   ve discussed the static parts of a neural
   networks: how we can set up the network connectivity, the data, and the
   id168. this section is devoted to the dynamics, or in other
   words, the process of learning the parameters and finding good
   hyperparameters.

gradient checks

   in theory, performing a gradient check is as simple as comparing the
   analytic gradient to the numerical gradient. in practice, the process
   is much more involved and error prone. here are some tips, tricks, and
   issues to watch out for:

   use the centered formula. the formula you may have seen for the finite
   difference approximation when evaluating the numerical gradient looks
   as follows:

   where \(h\) is a very small number, in practice approximately 1e-5 or
   so. in practice, it turns out that it is much better to use the
   centered difference formula of the form:

   this requires you to evaluate the id168 twice to check every
   single dimension of the gradient (so it is about 2 times as expensive),
   but the gradient approximation turns out to be much more precise. to
   see this, you can use taylor expansion of \(f(x+h)\) and \(f(x-h)\) and
   verify that the first formula has an error on order of \(o(h)\), while
   the second formula only has error terms on order of \(o(h^2)\) (i.e. it
   is a second order approximation).

   use relative error for the comparison. what are the details of
   comparing the numerical gradient \(f   _n\) and analytic gradient
   \(f   _a\)? that is, how do we know if the two are not compatible? you
   might be temped to keep track of the difference \(\mid f   _a - f   _n \mid
   \) or its square and define the gradient check as failed if that
   difference is above a threshold. however, this is problematic. for
   example, consider the case where their difference is 1e-4. this seems
   like a very appropriate difference if the two gradients are about 1.0,
   so we   d consider the two gradients to match. but if the gradients were
   both on order of 1e-5 or lower, then we   d consider 1e-4 to be a huge
   difference and likely a failure. hence, it is always more appropriate
   to consider the relative error:

   which considers their ratio of the differences to the ratio of the
   absolute values of both gradients. notice that normally the relative
   error formula only includes one of the two terms (either one), but i
   prefer to max (or add) both to make it symmetric and to prevent
   dividing by zero in the case where one of the two is zero (which can
   often happen, especially with relus). however, one must explicitly keep
   track of the case where both are zero and pass the gradient check in
   that edge case. in practice:
     * relative error > 1e-2 usually means the gradient is probably wrong
     * 1e-2 > relative error > 1e-4 should make you feel uncomfortable
     * 1e-4 > relative error is usually okay for objectives with kinks.
       but if there are no kinks (e.g. use of tanh nonlinearities and
       softmax), then 1e-4 is too high.
     * 1e-7 and less you should be happy.

   also keep in mind that the deeper the network, the higher the relative
   errors will be. so if you are gradient checking the input data for a
   10-layer network, a relative error of 1e-2 might be okay because the
   errors build up on the way. conversely, an error of 1e-2 for a single
   differentiable function likely indicates incorrect gradient.

   use double precision. a common pitfall is using single precision
   floating point to compute gradient check. it is often that case that
   you might get high relative errors (as high as 1e-2) even with a
   correct gradient implementation. in my experience i   ve sometimes seen
   my relative errors plummet from 1e-2 to 1e-8 by switching to double
   precision.

   stick around active range of floating point. it   s a good idea to read
   through [20]   what every computer scientist should know about
   floating-point arithmetic   , as it may demystify your errors and enable
   you to write more careful code. for example, in neural nets it can be
   common to normalize the id168 over the batch. however, if your
   gradients per datapoint are very small, then additionally dividing them
   by the number of data points is starting to give very small numbers,
   which in turn will lead to more numerical issues. this is why i like to
   always print the raw numerical/analytic gradient, and make sure that
   the numbers you are comparing are not extremely small (e.g. roughly
   1e-10 and smaller in absolute value is worrying). if they are you may
   want to temporarily scale your id168 up by a constant to bring
   them to a    nicer    range where floats are more dense - ideally on the
   order of 1.0, where your float exponent is 0.

   kinks in the objective. one source of inaccuracy to be aware of during
   gradient checking is the problem of kinks. kinks refer to
   non-differentiable parts of an objective function, introduced by
   functions such as relu (\(max(0,x)\)), or the id166 loss, maxout neurons,
   etc. consider gradient checking the relu function at \(x = -1e6\).
   since \(x < 0\), the analytic gradient at this point is exactly zero.
   however, the numerical gradient would suddenly compute a non-zero
   gradient because \(f(x+h)\) might cross over the kink (e.g. if \(h >
   1e-6\)) and introduce a non-zero contribution. you might think that
   this is a pathological case, but in fact this case can be very common.
   for example, an id166 for cifar-10 contains up to 450,000 \(max(0,x)\)
   terms because there are 50,000 examples and each example yields 9 terms
   to the objective. moreover, a neural network with an id166 classifier
   will contain many more kinks due to relus.

   note that it is possible to know if a kink was crossed in the
   evaluation of the loss. this can be done by keeping track of the
   identities of all    winners    in a function of form \(max(x,y)\); that
   is, was x or y higher during the forward pass. if the identity of at
   least one winner changes when evaluating \(f(x+h)\) and then
   \(f(x-h)\), then a kink was crossed and the numerical gradient will not
   be exact.

   use only few datapoints. one fix to the above problem of kinks is to
   use fewer datapoints, since id168s that contain kinks (e.g. due
   to use of relus or margin losses etc.) will have fewer kinks with fewer
   datapoints, so it is less likely for you to cross one when you perform
   the finite different approximation. moreover, if your gradcheck for
   only ~2 or 3 datapoints then you would almost certainly gradcheck for
   an entire batch. using very few datapoints also makes your gradient
   check faster and more efficient.

   be careful with the step size h. it is not necessarily the case that
   smaller is better, because when \(h\) is much smaller, you may start
   running into numerical precision problems. sometimes when the gradient
   doesn   t check, it is possible that you change \(h\) to be 1e-4 or 1e-6
   and suddenly the gradient will be correct. this [21]wikipedia article
   contains a chart that plots the value of h on the x-axis and the
   numerical gradient error on the y-axis.

   gradcheck during a    characteristic    mode of operation. it is important
   to realize that a gradient check is performed at a particular (and
   usually random), single point in the space of parameters. even if the
   gradient check succeeds at that point, it is not immediately certain
   that the gradient is correctly implemented globally. additionally, a
   random initialization might not be the most    characteristic    point in
   the space of parameters and may in fact introduce pathological
   situations where the gradient seems to be correctly implemented but
   isn   t. for instance, an id166 with very small weight initialization will
   assign almost exactly zero scores to all datapoints and the gradients
   will exhibit a particular pattern across all datapoints. an incorrect
   implementation of the gradient could still produce this pattern and not
   generalize to a more characteristic mode of operation where some scores
   are larger than others. therefore, to be safe it is best to use a short
   burn-in time during which the network is allowed to learn and perform
   the gradient check after the loss starts to go down. the danger of
   performing it at the first iteration is that this could introduce
   pathological edge cases and mask an incorrect implementation of the
   gradient.

   don   t let the id173 overwhelm the data. it is often the case
   that a id168 is a sum of the data loss and the id173
   loss (e.g. l2 penalty on weights). one danger to be aware of is that
   the id173 loss may overwhelm the data loss, in which case the
   gradients will be primarily coming from the id173 term (which
   usually has a much simpler gradient expression). this can mask an
   incorrect implementation of the data loss gradient. therefore, it is
   recommended to turn off id173 and check the data loss alone
   first, and then the id173 term second and independently. one
   way to perform the latter is to hack the code to remove the data loss
   contribution. another way is to increase the id173 strength so
   as to ensure that its effect is non-negligible in the gradient check,
   and that an incorrect implementation would be spotted.

   remember to turn off dropout/augmentations. when performing gradient
   check, remember to turn off any non-deterministic effects in the
   network, such as dropout, random data augmentations, etc. otherwise
   these can clearly introduce huge errors when estimating the numerical
   gradient. the downside of turning off these effects is that you
   wouldn   t be gradient checking them (e.g. it might be that dropout isn   t
   backpropagated correctly). therefore, a better solution might be to
   force a particular random seed before evaluating both \(f(x+h)\) and
   \(f(x-h)\), and when evaluating the analytic gradient.

   check only few dimensions. in practice the gradients can have sizes of
   million parameters. in these cases it is only practical to check some
   of the dimensions of the gradient and assume that the others are
   correct. be careful: one issue to be careful with is to make sure to
   gradient check a few dimensions for every separate parameter. in some
   applications, people combine the parameters into a single large
   parameter vector for convenience. in these cases, for example, the
   biases could only take up a tiny number of parameters from the whole
   vector, so it is important to not sample at random but to take this
   into account and check that all parameters receive the correct
   gradients.

before learning: sanity checks tips/tricks

   here are a few sanity checks you might consider running before you
   plunge into expensive optimization:
     * look for correct loss at chance performance. make sure you   re
       getting the loss you expect when you initialize with small
       parameters. it   s best to first check the data loss alone (so set
       id173 strength to zero). for example, for cifar-10 with a
       softmax classifier we would expect the initial loss to be 2.302,
       because we expect a diffuse id203 of 0.1 for each class
       (since there are 10 classes), and softmax loss is the negative log
       id203 of the correct class so: -ln(0.1) = 2.302. for the
       weston watkins id166, we expect all desired margins to be violated
       (since all scores are approximately zero), and hence expect a loss
       of 9 (since margin is 1 for each wrong class). if you   re not seeing
       these losses there might be issue with initialization.
     * as a second sanity check, increasing the id173 strength
       should increase the loss
     * overfit a tiny subset of data. lastly and most importantly, before
       training on the full dataset try to train on a tiny portion (e.g.
       20 examples) of your data and make sure you can achieve zero cost.
       for this experiment it   s also best to set id173 to zero,
       otherwise this can prevent you from getting zero cost. unless you
       pass this sanity check with a small dataset it is not worth
       proceeding to the full dataset. note that it may happen that you
       can overfit very small dataset but still have an incorrect
       implementation. for instance, if your datapoints    features are
       random due to some bug, then it will be possible to overfit your
       small training set but you will never notice any generalization
       when you fold it your full dataset.

babysitting the learning process

   there are multiple useful quantities you should monitor during training
   of a neural network. these plots are the window into the training
   process and should be utilized to get intuitions about different
   hyperparameter settings and how they should be changed for more
   efficient learning.

   the x-axis of the plots below are always in units of epochs, which
   measure how many times every example has been seen during training in
   expectation (e.g. one epoch means that every example has been seen
   once). it is preferable to track epochs rather than iterations since
   the number of iterations depends on the arbitrary setting of batch
   size.

id168

   the first quantity that is useful to track during training is the loss,
   as it is evaluated on the individual batches during the forward pass.
   below is a cartoon diagram showing the loss over time, and especially
   what the shape might tell you about the learning rate:
   [learningrates.jpeg] [loss.jpeg]
   left: a cartoon depicting the effects of different learning rates. with
   low learning rates the improvements will be linear. with high learning
   rates they will start to look more exponential. higher learning rates
   will decay the loss faster, but they get stuck at worse values of loss
   (green line). this is because there is too much "energy" in the
   optimization and the parameters are bouncing around chaotically, unable
   to settle in a nice spot in the optimization landscape. right: an
   example of a typical id168 over time, while training a small
   network on cifar-10 dataset. this id168 looks reasonable (it
   might indicate a slightly too small learning rate based on its speed of
   decay, but it's hard to say), and also indicates that the batch size
   might be a little too low (since the cost is a little too noisy).

   the amount of    wiggle    in the loss is related to the batch size. when
   the batch size is 1, the wiggle will be relatively high. when the batch
   size is the full dataset, the wiggle will be minimal because every
   gradient update should be improving the id168 monotonically
   (unless the learning rate is set too high).

   some people prefer to plot their id168s in the log domain.
   since learning progress generally takes an exponential form shape, the
   plot appears as a slightly more interpretable straight line, rather
   than a hockey stick. additionally, if multiple cross-validated models
   are plotted on the same loss graph, the differences between them become
   more apparent.

   sometimes id168s can look funny [22]lossfunctions.tumblr.com.

train/val accuracy

   the second important quantity to track while training a classifier is
   the validation/training accuracy. this plot can give you valuable
   insights into the amount of overfitting in your model:
   [accuracies.jpeg]
   the gap between the training and validation accuracy indicates the
   amount of overfitting. two possible cases are shown in the diagram on
   the left. the blue validation error curve shows very small validation
   accuracy compared to the training accuracy, indicating strong
   overfitting (note, it's possible for the validation accuracy to even
   start to go down after some point). when you see this in practice you
   probably want to increase id173 (stronger l2 weight penalty,
   more dropout, etc.) or collect more data. the other possible case is
   when the validation accuracy tracks the training accuracy fairly well.
   this case indicates that your model capacity is not high enough: make
   the model larger by increasing the number of parameters.

ratio of weights:updates

   the last quantity you might want to track is the ratio of the update
   magnitudes to the value magnitudes. note: updates, not the raw
   gradients (e.g. in vanilla sgd this would be the gradient multiplied by
   the learning rate). you might want to evaluate and track this ratio for
   every set of parameters independently. a rough heuristic is that this
   ratio should be somewhere around 1e-3. if it is lower than this then
   the learning rate might be too low. if it is higher then the learning
   rate is likely too high. here is a specific example:
# assume parameter vector w and its gradient vector dw
param_scale = np.linalg.norm(w.ravel())
update = -learning_rate*dw # simple sgd update
update_scale = np.linalg.norm(update.ravel())
w += update # the actual update
print update_scale / param_scale # want ~1e-3

   instead of tracking the min or the max, some people prefer to compute
   and track the norm of the gradients and their updates instead. these
   metrics are usually correlated and often give approximately the same
   results.

activation / gradient distributions per layer

   an incorrect initialization can slow down or even completely stall the
   learning process. luckily, this issue can be diagnosed relatively
   easily. one way to do so is to plot activation/gradient histograms for
   all layers of the network. intuitively, it is not a good sign to see
   any strange distributions - e.g. with tanh neurons we would like to see
   a distribution of neuron activations between the full range of [-1,1],
   instead of seeing all neurons outputting zero, or all neurons being
   completely saturated at either -1 or 1.

first-layer visualizations

   lastly, when one is working with image pixels it can be helpful and
   satisfying to plot the first-layer features visually:
   [weights.jpeg] [id98weights.jpg]
   examples of visualized weights for the first layer of a neural network.
   left: noisy features indicate could be a symptom: unconverged network,
   improperly set learning rate, very low weight id173 penalty.
   right: nice, smooth, clean and diverse features are a good indication
   that the training is proceeding well.

parameter updates

   once the analytic gradient is computed with id26, the
   gradients are used to perform a parameter update. there are several
   approaches for performing the update, which we discuss next.

   we note that optimization for deep networks is currently a very active
   area of research. in this section we highlight some established and
   common techniques you may see in practice, briefly describe their
   intuition, but leave a detailed analysis outside of the scope of the
   class. we provide some further pointers for an interested reader.

sgd and bells and whistles

   vanilla update. the simplest form of update is to change the parameters
   along the negative gradient direction (since the gradient indicates the
   direction of increase, but we usually wish to minimize a loss
   function). assuming a vector of parameters x and the gradient dx, the
   simplest update has the form:
# vanilla update
x += - learning_rate * dx

   where learning_rate is a hyperparameter - a fixed constant. when
   evaluated on the full dataset, and when the learning rate is low
   enough, this is guaranteed to make non-negative progress on the loss
   function.

   momentum update is another approach that almost always enjoys better
   converge rates on deep networks. this update can be motivated from a
   physical perspective of the optimization problem. in particular, the
   loss can be interpreted as the height of a hilly terrain (and therefore
   also to the potential energy since \(u = mgh\) and therefore \( u
   \propto h \) ). initializing the parameters with random numbers is
   equivalent to setting a particle with zero initial velocity at some
   location. the optimization process can then be seen as equivalent to
   the process of simulating the parameter vector (i.e. a particle) as
   rolling on the landscape.

   since the force on the particle is related to the gradient of potential
   energy (i.e. \(f = - \nabla u \) ), the force felt by the particle is
   precisely the (negative) gradient of the id168. moreover, \(f =
   ma \) so the (negative) gradient is in this view proportional to the
   acceleration of the particle. note that this is different from the sgd
   update shown above, where the gradient directly integrates the
   position. instead, the physics view suggests an update in which the
   gradient only directly influences the velocity, which in turn has an
   effect on the position:
# momentum update
v = mu * v - learning_rate * dx # integrate velocity
x += v # integrate position

   here we see an introduction of a v variable that is initialized at
   zero, and an additional hyperparameter (mu). as an unfortunate
   misnomer, this variable is in optimization referred to as momentum (its
   typical value is about 0.9), but its physical meaning is more
   consistent with the coefficient of friction. effectively, this variable
   damps the velocity and reduces the kinetic energy of the system, or
   otherwise the particle would never come to a stop at the bottom of a
   hill. when cross-validated, this parameter is usually set to values
   such as [0.5, 0.9, 0.95, 0.99]. similar to annealing schedules for
   learning rates (discussed later, below), optimization can sometimes
   benefit a little from momentum schedules, where the momentum is
   increased in later stages of learning. a typical setting is to start
   with momentum of about 0.5 and anneal it to 0.99 or so over multiple
   epochs.

     with momentum update, the parameter vector will build up velocity in
     any direction that has consistent gradient.

   nesterov momentum is a slightly different version of the momentum
   update that has recently been gaining popularity. it enjoys stronger
   theoretical converge guarantees for convex functions and in practice it
   also consistenly works slightly better than standard momentum.

   the core idea behind nesterov momentum is that when the current
   parameter vector is at some position x, then looking at the momentum
   update above, we know that the momentum term alone (i.e. ignoring the
   second term with the gradient) is about to nudge the parameter vector
   by mu * v. therefore, if we are about to compute the gradient, we can
   treat the future approximate position x + mu * v as a    lookahead    -
   this is a point in the vicinity of where we are soon going to end up.
   hence, it makes sense to compute the gradient at x + mu * v instead of
   at the    old/stale    position x.
   [nesterov.jpeg]
   nesterov momentum. instead of evaluating gradient at the current
   position (red circle), we know that our momentum is about to carry us
   to the tip of the green arrow. with nesterov momentum we therefore
   instead evaluate the gradient at this "looked-ahead" position.

   that is, in a slightly awkward notation, we would like to do the
   following:
x_ahead = x + mu * v
# evaluate dx_ahead (the gradient at x_ahead instead of at x)
v = mu * v - learning_rate * dx_ahead
x += v

   however, in practice people prefer to express the update to look as
   similar to vanilla sgd or to the previous momentum update as possible.
   this is possible to achieve by manipulating the update above with a
   variable transform x_ahead = x + mu * v, and then expressing the update
   in terms of x_ahead instead of x. that is, the parameter vector we are
   actually storing is always the ahead version. the equations in terms of
   x_ahead (but renaming it back to x) then become:
v_prev = v # back this up
v = mu * v - learning_rate * dx # velocity update stays the same
x += -mu * v_prev + (1 + mu) * v # position update changes form

   we recommend this further reading to understand the source of these
   equations and the mathematical formulation of nesterov   s accelerated
   momentum (nag):
     * [23]advances in optimizing recurrent networks by yoshua bengio,
       section 3.5.
     * [24]ilya sutskever   s thesis (pdf) contains a longer exposition of
       the topic in section 7.2

annealing the learning rate

   in training deep networks, it is usually helpful to anneal the learning
   rate over time. good intuition to have in mind is that with a high
   learning rate, the system contains too much kinetic energy and the
   parameter vector bounces around chaotically, unable to settle down into
   deeper, but narrower parts of the id168. knowing when to decay
   the learning rate can be tricky: decay it slowly and you   ll be wasting
   computation bouncing around chaotically with little improvement for a
   long time. but decay it too aggressively and the system will cool too
   quickly, unable to reach the best position it can. there are three
   common types of implementing the learning rate decay:
     * step decay: reduce the learning rate by some factor every few
       epochs. typical values might be reducing the learning rate by a
       half every 5 epochs, or by 0.1 every 20 epochs. these numbers
       depend heavily on the type of problem and the model. one heuristic
       you may see in practice is to watch the validation error while
       training with a fixed learning rate, and reduce the learning rate
       by a constant (e.g. 0.5) whenever the validation error stops
       improving.
     * exponential decay. has the mathematical form \(\alpha = \alpha_0
       e^{-k t}\), where \(\alpha_0, k\) are hyperparameters and \(t\) is
       the iteration number (but you can also use units of epochs).
     * 1/t decay has the mathematical form \(\alpha = \alpha_0 / (1 + k t
       )\) where \(a_0, k\) are hyperparameters and \(t\) is the iteration
       number.

   in practice, we find that the step decay is slightly preferable because
   the hyperparameters it involves (the fraction of decay and the step
   timings in units of epochs) are more interpretable than the
   hyperparameter \(k\). lastly, if you can afford the computational
   budget, err on the side of slower decay and train for a longer time.

second order methods

   a second, popular group of methods for optimization in context of deep
   learning is based on [25]newton   s method, which iterates the following
   update:

   here, \(h f(x)\) is the [26]hessian matrix, which is a square matrix of
   second-order partial derivatives of the function. the term \(\nabla
   f(x)\) is the gradient vector, as seen in id119.
   intuitively, the hessian describes the local curvature of the loss
   function, which allows us to perform a more efficient update. in
   particular, multiplying by the inverse hessian leads the optimization
   to take more aggressive steps in directions of shallow curvature and
   shorter steps in directions of steep curvature. note, crucially, the
   absence of any learning rate hyperparameters in the update formula,
   which the proponents of these methods cite this as a large advantage
   over first-order methods.

   however, the update above is impractical for most deep learning
   applications because computing (and inverting) the hessian in its
   explicit form is a very costly process in both space and time. for
   instance, a neural network with one million parameters would have a
   hessian matrix of size [1,000,000 x 1,000,000], occupying approximately
   3725 gigabytes of ram. hence, a large variety of quasi-id77s
   have been developed that seek to approximate the inverse hessian. among
   these, the most popular is [27]l-bfgs, which uses the information in
   the gradients over time to form the approximation implicitly (i.e. the
   full matrix is never computed).

   however, even after we eliminate the memory concerns, a large downside
   of a naive application of l-bfgs is that it must be computed over the
   entire training set, which could contain millions of examples. unlike
   mini-batch sgd, getting l-bfgs to work on mini-batches is more tricky
   and an active area of research.

   in practice, it is currently not common to see l-bfgs or similar
   second-order methods applied to large-scale deep learning and
   convolutional neural networks. instead, sgd variants based on
   (nesterov   s) momentum are more standard because they are simpler and
   scale more easily.

   additional references:
     * [28]large scale distributed deep networks is a paper from the
       google brain team, comparing l-bfgs and sgd variants in large-scale
       distributed optimization.
     * [29]sfo algorithm strives to combine the advantages of sgd with
       advantages of l-bfgs.

per-parameter adaptive learning rate methods

   all previous approaches we   ve discussed so far manipulated the learning
   rate globally and equally for all parameters. tuning the learning rates
   is an expensive process, so much work has gone into devising methods
   that can adaptively tune the learning rates, and even do so per
   parameter. many of these methods may still require other hyperparameter
   settings, but the argument is that they are well-behaved for a broader
   range of hyperparameter values than the raw learning rate. in this
   section we highlight some common adaptive methods you may encounter in
   practice:

   adagrad is an adaptive learning rate method originally proposed by
   [30]duchi et al..
# assume the gradient dx and parameter vector x
cache += dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)

   notice that the variable cache has size equal to the size of the
   gradient, and keeps track of per-parameter sum of squared gradients.
   this is then used to normalize the parameter update step, element-wise.
   notice that the weights that receive high gradients will have their
   effective learning rate reduced, while weights that receive small or
   infrequent updates will have their effective learning rate increased.
   amusingly, the square root operation turns out to be very important and
   without it the algorithm performs much worse. the smoothing term eps
   (usually set somewhere in range from 1e-4 to 1e-8) avoids division by
   zero. a downside of adagrad is that in case of deep learning, the
   monotonic learning rate usually proves too aggressive and stops
   learning too early.

   rmsprop. rmsprop is a very effective, but currently unpublished
   adaptive learning rate method. amusingly, everyone who uses this method
   in their work currently cites [31]slide 29 of lecture 6 of geoff
   hinton   s coursera class. the rmsprop update adjusts the adagrad method
   in a very simple way in an attempt to reduce its aggressive,
   monotonically decreasing learning rate. in particular, it uses a moving
   average of squared gradients instead, giving:
cache = decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)

   here, decay_rate is a hyperparameter and typical values are [0.9, 0.99,
   0.999]. notice that the x+= update is identical to adagrad, but the
   cache variable is a    leaky   . hence, rmsprop still modulates the
   learning rate of each weight based on the magnitudes of its gradients,
   which has a beneficial equalizing effect, but unlike adagrad the
   updates do not get monotonically smaller.

   adam. [32]adam is a recently proposed update that looks a bit like
   rmsprop with momentum. the (simplified) update looks as follows:
m = beta1*m + (1-beta1)*dx
v = beta2*v + (1-beta2)*(dx**2)
x += - learning_rate * m / (np.sqrt(v) + eps)

   notice that the update looks exactly as rmsprop update, except the
      smooth    version of the gradient m is used instead of the raw (and
   perhaps noisy) gradient vector dx. recommended values in the paper are
   eps = 1e-8, beta1 = 0.9, beta2 = 0.999. in practice adam is currently
   recommended as the default algorithm to use, and often works slightly
   better than rmsprop. however, it is often also worth trying
   sgd+nesterov momentum as an alternative. the full adam update also
   includes a bias correction mechanism, which compensates for the fact
   that in the first few time steps the vectors m,v are both initialized
   and therefore biased at zero, before they fully    warm up   . with the
   bias correction mechanism, the update looks as follows:
# t is your iteration counter going from 1 to infinity
m = beta1*m + (1-beta1)*dx
mt = m / (1-beta1**t)
v = beta2*v + (1-beta2)*(dx**2)
vt = v / (1-beta2**t)
x += - learning_rate * mt / (np.sqrt(vt) + eps)

   note that the update is now a function of the iteration as well as the
   other parameters. we refer the reader to the paper for the details, or
   the course slides where this is expanded on.

   additional references:
     * [33]unit tests for stochastic optimization proposes a series of
       tests as a standardized benchmark for stochastic optimization.

   [opt2.gif] [opt1.gif]
   animations that may help your intuitions about the learning process
   dynamics. left: contours of a loss surface and time evolution of
   different optimization algorithms. notice the "overshooting" behavior
   of momentum-based methods, which make the optimization look like a ball
   rolling down the hill. right: a visualization of a saddle point in the
   optimization landscape, where the curvature along different dimension
   has different signs (one dimension curves up and another down). notice
   that sgd has a very hard time breaking symmetry and gets stuck on the
   top. conversely, algorithms such as rmsprop will see very low gradients
   in the saddle direction. due to the denominator term in the rmsprop
   update, this will increase the effective learning rate along this
   direction, helping rmsprop proceed. images credit: [34]alec radford.

hyperparameter optimization

   as we   ve seen, training neural networks can involve many hyperparameter
   settings. the most common hyperparameters in context of neural networks
   include:
     * the initial learning rate
     * learning rate decay schedule (such as the decay constant)
     * id173 strength (l2 penalty, dropout strength)

   but as we saw, there are many more relatively less sensitive
   hyperparameters, for example in per-parameter adaptive learning
   methods, the setting of momentum and its schedule, etc. in this section
   we describe some additional tips and tricks for performing the
   hyperparameter search:

   implementation. larger neural networks typically require a long time to
   train, so performing hyperparameter search can take many days/weeks. it
   is important to keep this in mind since it influences the design of
   your code base. one particular design is to have a worker that
   continuously samples random hyperparameters and performs the
   optimization. during the training, the worker will keep track of the
   validation performance after every epoch, and writes a model checkpoint
   (together with miscellaneous training statistics such as the loss over
   time) to a file, preferably on a shared file system. it is useful to
   include the validation performance directly in the filename, so that it
   is simple to inspect and sort the progress. then there is a second
   program which we will call a master, which launches or kills workers
   across a computing cluster, and may additionally inspect the
   checkpoints written by workers and plot their training statistics, etc.

   prefer one validation fold to cross-validation. in most cases a single
   validation set of respectable size substantially simplifies the code
   base, without the need for cross-validation with multiple folds. you   ll
   hear people say they    cross-validated    a parameter, but many times it
   is assumed that they still only used a single validation set.

   hyperparameter ranges. search for hyperparameters on log scale. for
   example, a typical sampling of the learning rate would look as follows:
   learning_rate = 10 ** uniform(-6, 1). that is, we are generating a
   random number from a uniform distribution, but then raising it to the
   power of 10. the same strategy should be used for the id173
   strength. intuitively, this is because learning rate and id173
   strength have multiplicative effects on the training dynamics. for
   example, a fixed change of adding 0.01 to a learning rate has huge
   effects on the dynamics if the learning rate is 0.001, but nearly no
   effect if the learning rate when it is 10. this is because the learning
   rate multiplies the computed gradient in the update. therefore, it is
   much more natural to consider a range of learning rate multiplied or
   divided by some value, than a range of learning rate added or
   subtracted to by some value. some parameters (e.g. dropout) are instead
   usually searched in the original scale (e.g. dropout = uniform(0,1)).

   prefer random search to grid search. as argued by bergstra and bengio
   in [35]random search for hyper-parameter optimization,    randomly chosen
   trials are more efficient for hyper-parameter optimization than trials
   on a grid   . as it turns out, this is also usually easier to implement.
   [gridsearchbad.jpeg]
   core illustration from [36]random search for hyper-parameter
   optimization by bergstra and bengio. it is very often the case that
   some of the hyperparameters matter much more than others (e.g. top
   hyperparam vs. left one in this figure). performing random search
   rather than grid search allows you to much more precisely discover good
   values for the important ones.

   careful with best values on border. sometimes it can happen that you   re
   searching for a hyperparameter (e.g. learning rate) in a bad range. for
   example, suppose we use learning_rate = 10 ** uniform(-6, 1). once we
   receive the results, it is important to double check that the final
   learning rate is not at the edge of this interval, or otherwise you may
   be missing more optimal hyperparameter setting beyond the interval.

   stage your search from coarse to fine. in practice, it can be helpful
   to first search in coarse ranges (e.g. 10 ** [-6, 1]), and then
   depending on where the best results are turning up, narrow the range.
   also, it can be helpful to perform the initial coarse search while only
   training for 1 epoch or even less, because many hyperparameter settings
   can lead the model to not learn at all, or immediately explode with
   infinite cost. the second stage could then perform a narrower search
   with 5 epochs, and the last stage could perform a detailed search in
   the final range for many more epochs (for example).

   bayesian hyperparameter optimization is a whole area of research
   devoted to coming up with algorithms that try to more efficiently
   navigate the space of hyperparameters. the core idea is to
   appropriately balance the exploration - exploitation trade-off when
   querying the performance at different hyperparameters. multiple
   libraries have been developed based on these models as well, among some
   of the better known ones are [37]spearmint, [38]smac, and [39]hyperopt.
   however, in practical settings with convnets it is still relatively
   difficult to beat random search in a carefully-chosen intervals. see
   some additional from-the-trenches discussion [40]here.

evaluation

model ensembles

   in practice, one reliable approach to improving the performance of
   neural networks by a few percent is to train multiple independent
   models, and at test time average their predictions. as the number of
   models in the ensemble increases, the performance typically
   monotonically improves (though with diminishing returns). moreover, the
   improvements are more dramatic with higher model variety in the
   ensemble. there are a few approaches to forming an ensemble:
     * same model, different initializations. use cross-validation to
       determine the best hyperparameters, then train multiple models with
       the best set of hyperparameters but with different random
       initialization. the danger with this approach is that the variety
       is only due to initialization.
     * top models discovered during cross-validation. use cross-validation
       to determine the best hyperparameters, then pick the top few (e.g.
       10) models to form the ensemble. this improves the variety of the
       ensemble but has the danger of including suboptimal models. in
       practice, this can be easier to perform since it doesn   t require
       additional retraining of models after cross-validation
     * different checkpoints of a single model. if training is very
       expensive, some people have had limited success in taking different
       checkpoints of a single network over time (for example after every
       epoch) and using those to form an ensemble. clearly, this suffers
       from some lack of variety, but can still work reasonably well in
       practice. the advantage of this approach is that is very cheap.
     * running average of parameters during training. related to the last
       point, a cheap way of almost always getting an extra percent or two
       of performance is to maintain a second copy of the network   s
       weights in memory that maintains an exponentially decaying sum of
       previous weights during training. this way you   re averaging the
       state of the network over last several iterations. you will find
       that this    smoothed    version of the weights over last few steps
       almost always achieves better validation error. the rough intuition
       to have in mind is that the objective is bowl-shaped and your
       network is jumping around the mode, so the average has a higher
       chance of being somewhere nearer the mode.

   one disadvantage of model ensembles is that they take longer to
   evaluate on test example. an interested reader may find the recent work
   from geoff hinton on [41]   dark knowledge    inspiring, where the idea is
   to    distill    a good ensemble back to a single model by incorporating
   the ensemble log likelihoods into a modified objective.

summary

   to train a neural network:
     * gradient check your implementation with a small batch of data and
       be aware of the pitfalls.
     * as a sanity check, make sure your initial loss is reasonable, and
       that you can achieve 100% training accuracy on a very small portion
       of the data
     * during training, monitor the loss, the training/validation
       accuracy, and if you   re feeling fancier, the magnitude of updates
       in relation to parameter values (it should be ~1e-3), and when
       dealing with convnets, the first-layer weights.
     * the two recommended updates to use are either sgd+nesterov momentum
       or adam.
     * decay your learning rate over the period of the training. for
       example, halve the learning rate after a fixed number of epochs, or
       whenever the validation accuracy tops off.
     * search for good hyperparameters with random search (not grid
       search). stage your search from coarse (wide hyperparameter ranges,
       training only for 1-5 epochs), to fine (narrower rangers, training
       for many more epochs)
     * form model ensembles for extra performance

additional references

     * [42]sgd tips and tricks from leon bottou
     * [43]efficient backprop (pdf) from yann lecun
     * [44]practical recommendations for gradient-based training of deep
       architectures from yoshua bengio

     * [45]cs231n
     * [46]cs231n
     * [47]karpathy@cs.stanford.edu

references

   1. http://cs231n.github.io/
   2. http://cs231n.github.io/neural-networks-3/#gradcheck
   3. http://cs231n.github.io/neural-networks-3/#sanitycheck
   4. http://cs231n.github.io/neural-networks-3/#baby
   5. http://cs231n.github.io/neural-networks-3/#loss
   6. http://cs231n.github.io/neural-networks-3/#accuracy
   7. http://cs231n.github.io/neural-networks-3/#ratio
   8. http://cs231n.github.io/neural-networks-3/#distr
   9. http://cs231n.github.io/neural-networks-3/#vis
  10. http://cs231n.github.io/neural-networks-3/#update
  11. http://cs231n.github.io/neural-networks-3/#sgd
  12. http://cs231n.github.io/neural-networks-3/#anneal
  13. http://cs231n.github.io/neural-networks-3/#second
  14. http://cs231n.github.io/neural-networks-3/#ada
  15. http://cs231n.github.io/neural-networks-3/#hyper
  16. http://cs231n.github.io/neural-networks-3/#eval
  17. http://cs231n.github.io/neural-networks-3/#ensemble
  18. http://cs231n.github.io/neural-networks-3/#summary
  19. http://cs231n.github.io/neural-networks-3/#add
  20. http://docs.oracle.com/cd/e19957-01/806-3568/ncg_goldberg.html
  21. http://en.wikipedia.org/wiki/numerical_differentiation
  22. http://lossfunctions.tumblr.com/
  23. http://arxiv.org/pdf/1212.0901v2.pdf
  24. http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf
  25. http://en.wikipedia.org/wiki/newton's_method_in_optimization
  26. http://en.wikipedia.org/wiki/hessian_matrix
  27. http://en.wikipedia.org/wiki/limited-memory_bfgs
  28. http://research.google.com/archive/large_deep_networks_nips2012.html
  29. http://arxiv.org/abs/1311.2115
  30. http://jmlr.org/papers/v12/duchi11a.html
  31. http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
  32. http://arxiv.org/abs/1412.6980
  33. http://arxiv.org/abs/1312.6055
  34. https://twitter.com/alecrad
  35. http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf
  36. http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf
  37. https://github.com/jaspersnoek/spearmint
  38. http://www.cs.ubc.ca/labs/beta/projects/smac/
  39. http://jaberg.github.io/hyperopt/
  40. http://nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html
  41. https://www.youtube.com/watch?v=ek61htlw8hy
  42. http://research.microsoft.com/pubs/192769/tricks-2012.pdf
  43. http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf
  44. http://arxiv.org/pdf/1206.5533v2.pdf
  45. https://github.com/cs231n
  46. https://twitter.com/cs231n
  47. mailto:karpathy@cs.stanford.edu
