   [1]cs231n convolutional neural networks for visual recognition

   (this page is currently in draft form)

visualizing what convnets learn

   several approaches for understanding and visualizing convolutional
   networks have been developed in the literature, partly as a response
   the common criticism that the learned features in a neural network are
   not interpretable. in this section we briefly survey some of these
   approaches and related work.

visualizing the activations and first-layer weights

   layer activations. the most straight-forward visualization technique is
   to show the activations of the network during the forward pass. for
   relu networks, the activations usually start out looking relatively
   blobby and dense, but as the training progresses the activations
   usually become more sparse and localized. one dangerous pitfall that
   can be easily noticed with this visualization is that some activation
   maps may be all zero for many different inputs, which can indicate dead
   filters, and can be a symptom of high learning rates.
   [act1.jpeg] [act2.jpeg]
   typical-looking activations on the first conv layer (left), and the 5th
   conv layer (right) of a trained alexnet looking at a picture of a cat.
   every box shows an activation map corresponding to some filter. notice
   that the activations are sparse (most values are zero, in this
   visualization shown in black) and mostly local.

   conv/fc filters. the second common strategy is to visualize the
   weights. these are usually most interpretable on the first conv layer
   which is looking directly at the raw pixel data, but it is possible to
   also show the filter weights deeper in the network. the weights are
   useful to visualize because well-trained networks usually display nice
   and smooth filters without any noisy patterns. noisy patterns can be an
   indicator of a network that hasn   t been trained for long enough, or
   possibly a very low id173 strength that may have led to
   overfitting.
   [filt1.jpeg] [filt2.jpeg]
   typical-looking filters on the first conv layer (left), and the 2nd
   conv layer (right) of a trained alexnet. notice that the first-layer
   weights are very nice and smooth, indicating nicely converged network.
   the color/grayscale features are clustered because the alexnet contains
   two separate streams of processing, and an apparent consequence of this
   architecture is that one stream develops high-frequency grayscale
   features and the other low-frequency color features. the 2nd conv layer
   weights are not as interpretable, but it is apparent that they are
   still smooth, well-formed, and absent of noisy patterns.

retrieving images that maximally activate a neuron

   another visualization technique is to take a large dataset of images,
   feed them through the network and keep track of which images maximally
   activate some neuron. we can then visualize the images to get an
   understanding of what the neuron is looking for in its receptive field.
   one such visualization (among others) is shown in [2]rich feature
   hierarchies for accurate id164 and semantic segmentation by
   ross girshick et al.:
   [pool5max.jpeg]
   maximally activating images for some pool5 (5th pool layer) neurons of
   an alexnet. the activation values and the receptive field of the
   particular neuron are shown in white. (in particular, note that the
   pool5 neurons are a function of a relatively large portion of the input
   image!) it can be seen that some neurons are responsive to upper
   bodies, text, or specular highlights.

   one problem with this approach is that relu neurons do not necessarily
   have any semantic meaning by themselves. rather, it is more appropriate
   to think of multiple relu neurons as the basis vectors of some space
   that represents in image patches. in other words, the visualization is
   showing the patches at the edge of the cloud of representations, along
   the (arbitrary) axes that correspond to the filter weights. this can
   also be seen by the fact that neurons in a convnet operate linearly
   over the input space, so any arbitrary rotation of that space is a
   no-op. this point was further argued in [3]intriguing properties of
   neural networks by szegedy et al., where they perform a similar
   visualization along arbitrary directions in the representation space.

embedding the codes with id167

   convnets can be interpreted as gradually transforming the images into a
   representation in which the classes are separable by a linear
   classifier. we can get a rough idea about the topology of this space by
   embedding images into two dimensions so that their low-dimensional
   representation has approximately equal distances than their
   high-dimensional representation. there are many embedding methods that
   have been developed with the intuition of embedding high-dimensional
   vectors in a low-dimensional space while preserving the pairwise
   distances of the points. among these, [4]id167 is one of the best-known
   methods that consistently produces visually-pleasing results.

   to produce an embedding, we can take a set of images and use the
   convnet to extract the id98 codes (e.g. in alexnet the 4096-dimensional
   vector right before the classifier, and crucially, including the relu
   non-linearity). we can then plug these into id167 and get 2-dimensional
   vector for each image. the corresponding images can them be visualized
   in a grid:
   [tsne.jpeg]
   id167 embedding of a set of images based on their id98 codes. images
   that are nearby each other are also close in the id98 representation
   space, which implies that the id98 "sees" them as being very similar.
   notice that the similarities are more often class-based and semantic
   rather than pixel and color-based. for more details on how this
   visualization was produced the associated code, and more related
   visualizations at different scales refer to [5]id167 visualization of
   id98 codes.

occluding parts of the image

   suppose that a convnet classifies an image as a dog. how can we be
   certain that it   s actually picking up on the dog in the image as
   opposed to some contextual cues from the background or some other
   miscellaneous object? one way of investigating which part of the image
   some classification prediction is coming from is by plotting the
   id203 of the class of interest (e.g. dog class) as a function of
   the position of an occluder object. that is, we iterate over regions of
   the image, set a patch of the image to be all zero, and look at the
   id203 of the class. we can visualize the id203 as a
   2-dimensional heat map. this approach has been used in matthew zeiler   s
   [6]visualizing and understanding convolutional networks:
   [occlude.jpeg]
   three input images (top). notice that the occluder region is shown in
   grey. as we slide the occluder over the image we record the id203
   of the correct class and then visualize it as a heatmap (shown below
   each image). for instance, in the left-most image we see that the
   id203 of pomeranian plummets when the occluder covers the face of
   the dog, giving us some level of confidence that the dog's face is
   primarily responsible for the high classification score. conversely,
   zeroing out other parts of the image is seen to have relatively
   negligible impact.

visualizing the data gradient and friends

   data gradient.

   [7]deep inside convolutional networks: visualising image classification
   models and saliency maps

   deconvnet.

   [8]visualizing and understanding convolutional networks

   guided id26.

   [9]striving for simplicity: the all convolutional net

reconstructing original images based on id98 codes

   [10]understanding deep image representations by inverting them

how much spatial information is preserved?

   [11]do convnets learn correspondence? (tldr: yes)

plotting performance as a function of image attributes

   [12]id163 large scale visual recognition challenge

fooling convnets

   [13]explaining and harnessing adversarial examples

comparing convnets to human labelers

   [14]what i learned from competing against a convnet on id163

     * [15]cs231n
     * [16]cs231n
     * [17]karpathy@cs.stanford.edu

references

   1. http://cs231n.github.io/
   2. http://arxiv.org/abs/1311.2524
   3. http://arxiv.org/abs/1312.6199
   4. http://lvdmaaten.github.io/tsne/
   5. http://cs.stanford.edu/people/karpathy/id98embed/
   6. http://arxiv.org/abs/1311.2901
   7. http://arxiv.org/abs/1312.6034
   8. http://arxiv.org/abs/1311.2901
   9. http://arxiv.org/abs/1412.6806
  10. http://arxiv.org/abs/1412.0035
  11. http://papers.nips.cc/paper/5420-do-convnets-learn-correspondence.pdf
  12. http://arxiv.org/abs/1409.0575
  13. http://arxiv.org/abs/1412.6572
  14. http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-id163/
  15. https://github.com/cs231n
  16. https://twitter.com/cs231n
  17. mailto:karpathy@cs.stanford.edu
