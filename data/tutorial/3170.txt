   [1]cs231n convolutional neural networks for visual recognition

   (these notes are currently in draft form and under development)

   table of contents:
     * [2]id21
     * [3]additional references

id21

   in practice, very few people train an entire convolutional network from
   scratch (with random initialization), because it is relatively rare to
   have a dataset of sufficient size. instead, it is common to pretrain a
   convnet on a very large dataset (e.g. id163, which contains 1.2
   million images with 1000 categories), and then use the convnet either
   as an initialization or a fixed feature extractor for the task of
   interest. the three major id21 scenarios look as follows:
     * convnet as fixed feature extractor. take a convnet pretrained on
       id163, remove the last fully-connected layer (this layer   s
       outputs are the 1000 class scores for a different task like
       id163), then treat the rest of the convnet as a fixed feature
       extractor for the new dataset. in an alexnet, this would compute a
       4096-d vector for every image that contains the activations of the
       hidden layer immediately before the classifier. we call these
       features id98 codes. it is important for performance that these
       codes are relud (i.e. thresholded at zero) if they were also
       thresholded during the training of the convnet on id163 (as is
       usually the case). once you extract the 4096-d codes for all
       images, train a linear classifier (e.g. linear id166 or softmax
       classifier) for the new dataset.
     * fine-tuning the convnet. the second strategy is to not only replace
       and retrain the classifier on top of the convnet on the new
       dataset, but to also fine-tune the weights of the pretrained
       network by continuing the id26. it is possible to
       fine-tune all the layers of the convnet, or it   s possible to keep
       some of the earlier layers fixed (due to overfitting concerns) and
       only fine-tune some higher-level portion of the network. this is
       motivated by the observation that the earlier features of a convnet
       contain more generic features (e.g. edge detectors or color blob
       detectors) that should be useful to many tasks, but later layers of
       the convnet becomes progressively more specific to the details of
       the classes contained in the original dataset. in case of id163
       for example, which contains many dog breeds, a significant portion
       of the representational power of the convnet may be devoted to
       features that are specific to differentiating between dog breeds.
     * pretrained models. since modern convnets take 2-3 weeks to train
       across multiple gpus on id163, it is common to see people
       release their final convnet checkpoints for the benefit of others
       who can use the networks for fine-tuning. for example, the caffe
       library has a [4]model zoo where people share their network
       weights.

   when and how to fine-tune? how do you decide what type of transfer
   learning you should perform on a new dataset? this is a function of
   several factors, but the two most important ones are the size of the
   new dataset (small or big), and its similarity to the original dataset
   (e.g. id163-like in terms of the content of images and the classes,
   or very different, such as microscope images). keeping in mind that
   convnet features are more generic in early layers and more
   original-dataset-specific in later layers, here are some common rules
   of thumb for navigating the 4 major scenarios:
    1. new dataset is small and similar to original dataset. since the
       data is small, it is not a good idea to fine-tune the convnet due
       to overfitting concerns. since the data is similar to the original
       data, we expect higher-level features in the convnet to be relevant
       to this dataset as well. hence, the best idea might be to train a
       linear classifier on the id98 codes.
    2. new dataset is large and similar to the original dataset. since we
       have more data, we can have more confidence that we won   t overfit
       if we were to try to fine-tune through the full network.
    3. new dataset is small but very different from the original dataset.
       since the data is small, it is likely best to only train a linear
       classifier. since the dataset is very different, it might not be
       best to train the classifier form the top of the network, which
       contains more dataset-specific features. instead, it might work
       better to train the id166 classifier from activations somewhere
       earlier in the network.
    4. new dataset is large and very different from the original dataset.
       since the dataset is very large, we may expect that we can afford
       to train a convnet from scratch. however, in practice it is very
       often still beneficial to initialize with weights from a pretrained
       model. in this case, we would have enough data and confidence to
       fine-tune through the entire network.

   practical advice. there are a few additional things to keep in mind
   when performing id21:
     * constraints from pretrained models. note that if you wish to use a
       pretrained network, you may be slightly constrained in terms of the
       architecture you can use for your new dataset. for example, you
       can   t arbitrarily take out conv layers from the pretrained network.
       however, some changes are straight-forward: due to parameter
       sharing, you can easily run a pretrained network on images of
       different spatial size. this is clearly evident in the case of
       conv/pool layers because their forward function is independent of
       the input volume spatial size (as long as the strides    fit   ). in
       case of fc layers, this still holds true because fc layers can be
       converted to a convolutional layer: for example, in an alexnet, the
       final pooling volume before the first fc layer is of size
       [6x6x512]. therefore, the fc layer looking at this volume is
       equivalent to having a convolutional layer that has receptive field
       size 6x6, and is applied with padding of 0.
     * learning rates. it   s common to use a smaller learning rate for
       convnet weights that are being fine-tuned, in comparison to the
       (randomly-initialized) weights for the new linear classifier that
       computes the class scores of your new dataset. this is because we
       expect that the convnet weights are relatively good, so we don   t
       wish to distort them too quickly and too much (especially while the
       new linear classifier above them is being trained from random
       initialization).

additional references

     * [5]id98 features off-the-shelf: an astounding baseline for
       recognition trains id166s on features from id163-pretrained
       convnet and reports several state of the art results.
     * [6]decaf reported similar findings in 2013. the framework in this
       paper (decaf) was a python-based precursor to the c++ caffe
       library.
     * [7]how transferable are features in deep neural networks? studies
       the id21 performance in detail, including some
       unintuitive findings about layer co-adaptations.

     * [8]cs231n
     * [9]cs231n
     * [10]karpathy@cs.stanford.edu

references

   1. http://cs231n.github.io/
   2. http://cs231n.github.io/transfer-learning/#tf
   3. http://cs231n.github.io/transfer-learning/#add
   4. https://github.com/bvlc/caffe/wiki/model-zoo
   5. http://arxiv.org/abs/1403.6382
   6. http://arxiv.org/abs/1310.1531
   7. http://arxiv.org/abs/1411.1792
   8. https://github.com/cs231n
   9. https://twitter.com/cs231n
  10. mailto:karpathy@cs.stanford.edu
