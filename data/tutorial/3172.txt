   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

medical image analysis with deep learning         ii

   [9]go to the profile of taposh dutta-roy
   [10]taposh dutta-roy (button) blockedunblock (button) followfollowing
   apr 4, 2017
   [1*v1po04yuhz0mlqww3zj_nw.jpeg]

   in the [11]last article we went through some basics of image-processing
   using [12]opencv and basics of [13]dicom image. in this article we will
   talk about basics of deep learning from the lens of convolutional
   neural nets. in the next article we will use [14]kaggle   s lung cancer
   data-set, review the key items to look for in a lung cancer dicom image
   and use kera   s to develop a model to predict lung cancer.

basic convolutional neural nets (id98)

   in order to understand basics of id98, we need to understand what is
   convolution.

   what is convolution?

   wikipedia defines convolution as    a mathematical operation on two
   functions (f and g); it produces a third function, that is typically
   viewed as a modified version of one of the original functions, giving
   the integral of the point-wise multiplication of the two functions as a
   function of the amount that one of the original functions is
   translated.    the easy way to understand this by thinking of it as a
   sliding window function applied to a matrix.
   [1*zcjpufrb6ehpri4eyp6aaa.gif]
   convolution with 3  3 filter. source:
   [15]http://deeplearning.stanford.edu/wiki/index.php/feature_extraction_
   using_convolution

   the figure above shows the sliding window applied on the matrix in
   green, where sliding window matrix is in red. the output is the
   convolved feature matrix. the figure below shows the convolution of two
   square pulses (blue and red) and the results.
   [1*pbwhcrckawaex_v5bjudpg.gif]
   source: wikipedia

   jeremy howard, in his [16]mooc explains convolution using an excel
   sheet, which is a great way to understand the fundamentals. consider 2
   matrices f and g. the output of convolution of f and g, is the third
   matrix    conv layer 1    given by the dot-product of of the 2 matrices.
   the dot product of 2 matrices is a scalar as shown below. an excellent
   source of math functions can be found [17]here.
   [1*m5enbfhctmzed5ian-yddq.png]
   dot product of 2 matrices.

   lets use excel as jeremy suggests, our input matrix is function f() and
   sliding window matrix is filter function g(). the dot product is the
   sum-product of the 2 matrices in excel as shown below.
   [1*tbme-vazgz1uee7ucvtw7g.png]
   convolution of 2 matrices

   lets extend this to an image of alphabet    a   . as we know any image is
   made of pixels. so our input matrix f is    a   . we select our sliding
   window function to be a random matrix g. then the convoluted output for
   the dot product of this matrix is shown below. send me a note if you
   would like a copy of this excel sheet.
   [1*0ds_ezez9i05sn9cef9ata.png]

   what are convolutional neural nets (id98) ?
   [1*larleaufgehgz0e0t8esha.png]
   source: [18]http://cs231n.github.io/convolutional-networks/

   in my point of view a simple convolutional neural net (id98) is a
   sequence of layers. each layer has some specific functions. each
   convolutional layer is 3 dimensional, so we use volume as the metric.
   further, each layer of a id98 transforms one volume of activations to
   another through a differentiable function. such a function is called
   activation or transfer function.

   the different types of entities of id98 are : input , filters (or
   kernels), convolutional layer, activation layer, pooling layer, and
   batch id172 layer. the combination of these layers in different
   permutations and of course some rules give us different deep learning
   architectures.

   input layer : the usual input to a id98 is an n-dimensional array. for
   an image we have input with 3 dimensions         length, width and depth
   (which are the color channels)
   [1*vz2d3bs9avtqzomvj-9vbq.png]
   source:
   [19]http://xrds.acm.org/blog/2016/06/convolutional-neural-networks-id98s
   -illustrated-explanation/

   filters or kernels : as shown in the figure from rivertrail below, a
   filter or kernel slides to every position of the image and computes a
   new pixel as a weighted sum of the pixels it floats over. in our excel
   example above our filter is g, moves over the input matrix f.
   [1*-gvlayh7set6tmzctmcn5w.png]
   source: [20]http://intellabs.github.io/rivertrail/tutorial/

   convolutional layer: a layer of dot product of input matrix and kernel
   gives a new matrix know as the convolutional matrix or layer.
   [1*6hh1hchaqbjt5eftph12hw.png]
   source: [21]https://docs.gimp.org/en/plug-in-convmatrix.html

   a very good visual chart understanding how padding, strides and
   transpose work can be found below.
   [1*yjlwtky_ktjzrrqbckynfw.png]
   source : [22]https://github.com/vdumoulin/conv_arithmetic

   activation layer :

   id180 can be classified into 2 categories
   based         saturated and non-saturated.
   [1*vignk-n-uptm1hj-ygsnag.png]

   saturated id180 are sigmoid and tanh, whereas
   non-saturated are relu and its variants.the advantage of using
   non-saturated activation function lies in two aspects:
    1. the first is to solve the so called    exploding/vanishing gradient   .
    2. the second is to accelerate the convergence speed.

   sigmoid: takes a real-valued input and squashes it to range between
   [0,1]

     (x) = 1 / (1 + exp(   x))

   tanh: takes a real-valued input and squashes it to the range [-1, 1]

   tanh(x) = 2  (2x)     1

   relu

   relu stands for rectified linear unit. it is the max function(x,0) with
   input x e.g. matrix from a convolved image. relu then sets all negative
   values in the matrix x to zero and all other values are kept
   constant.relu is computed after the convolution and therefore a
   nonlinear activation function like tanh or sigmoid. this was first
   discussed by geoff hinton in his nature paper.

elus

   exponential linear units try to make the mean activations closer to
   zero which speeds up learning. elus also avoid a vanishing gradient via
   the identity for positive values. it has been shown that elus obtain
   higher classification accuracy than relus. a very good detailed poster
   on elu can be found [23]here .
   [1*l_wr4a1er4neereulu086q.png]
   source:
   [24]http://image-net.org/challenges/posters/jku_en_rgb_schwarz_poster.p
   df [15 layer id98 with stacks of (1  96  6, 3  512  3, 5  768  3, 3  1024  3,
   2  4096  fc, 1  1000  fc) layers  units  receptive fields or fully-connected
   (fc). 2  2 max-pooling with a stride of 2 after each stack, spatial
   pyramid pooling with 3 levels before the first fc layer.]
   [1*whho0bvb137cbdqj2xhuja.png]
   source : wikipedia

leaky relus

   in contrast to relu, in which the negative part is totally dropped,
   leaky relu assigns a non-zero slope to it. leaky rectified linear
   activation is first introduced in acoustic model(maas et al., 2013).
   mathematically, we have
   [1*6fzflqjund-w1l8ocbzd1a.png]
   source: empirical evaluation of rectified activations in convolution
   network

   where ai is the fixed parameter in the range (1,+infinity).

   parametric rectified linear unit (prelu)

   prelu can be considered as a variant of leaky relu. in prelu, the
   slopes of negative part are learned form data rather than predefined.
   the authors claimed that prelu is the key factor of surpassing
   human-level performance on id163 classification (russakovsky et al.,
   2015) task. it is the same as leaky relu with the exception that ai is
   learned in the training via back propagation.

   randomized leaky rectified linear unit (rrelu)

   randomized rectified linear unit (rrelu) are also a variant of leaky
   relu. in rrelu, the slopes of negative parts are randomized in a given
   range in the training, and then fixed in the testing. the highlight of
   rrelu is that in training process, aji is a random number sampled from
   a uniform distribution u(l,u). formally, we have:
   [1*m9p-bpddcbivy5pbme0ilq.png]

   a comparison between relu, leaky relu, prelu and rrelu is shown below.
   [1*b_xwafogrwnmsoj59j5wvw.png]
   source :[25]https://arxiv.org/pdf/1505.00853.pdf relu, leaky relu,
   prelu and rrelu. for prelu, ai is learned and for leaky relu ai is
   fixed. for rrelu, aji is a random variable keeps sampling in a given
   range, and remains fixed in testing.

   noisy id180

   these are id180, extended to include [26]gaussian noise.
   good understanding on how noise helps can be found [27]here.
   [1*1lbvwjc85gsuhclx3nv2va.png]
   source: wikipedia

   pooling layer:

   the goal of a pooling layer is to progressively reduce the spatial size
   of the matrix to reduce the amount of parameters and computation in the
   network, and hence to also control overfitting. the pooling layer
   operates independently on every depth slice of the input and resizes it
   spatially, using the max or average operation. the most common form is
   a pooling layer with filters of size 2x2 applied with a stride of 2
   downsamples every depth slice in the input by 2 along both width and
   height, discarding 75% of the activations. every max operation would in
   this case be taking a max over 4 numbers (little 2x2 region in some
   depth slice). the depth dimension remains unchanged. more generally,
   the pooling layer:
   [1*fxqhdhi_1-lkasmwmoy83a.png]
   source: [28]http://cs231n.github.io/convolutional-networks/#pool
   [1*5q6gfkxs2yimhe_029xbtq.png]
   source :
   [29]https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/

   note: here we slide our 2 x 2 window by 2 cells (also called    stride   )
   and take the maximum value in each region.

   batch id172 layer:

   batch id172 is an effective way of normalizing each
   intermediate layer including the weights and id180.
   there are two main benefits for batchnorm:
    1. adding batchnorm to a model can result in 10x or more improvements
       in training speed
    2. because id172 greatly reduces the ability of a small number
       of outlying inputs to over-influence the training, it also tends to
       reduce overfitting.

   details about batch id172 can be found [30]here or check
   jeremy   s [31]mooc.

   fully connected layer:

   the fully connected layer is a traditional multi layer id88 that
   uses a softmax activation function in the output layer. the term    fully
   connected    implies that every neuron in the previous layer is connected
   to every neuron on the next layer. a softmax function is a
   generalization of the [32]logistic function that    squashes    a
   k-dimensional vector, of arbitrary real values to a k-dimensional
   vector of real values in the range (0, 1) that add up to 1.
   [1*t_n-nwrg4pt_gqhyfbycfq.png]
   source: wikipedia

   softmax activation is generally used at the final fully connected layer
   to get probabilities as it pushes the values between 0 and 1.

   now, we have an idea about the different layers in a id98. armed with
   this knowledge we will develop the deep learning architecture needed
   for lung cancer detection using keras in the next article.

   acknowledgements:
    1. jeremy howard   s mooc (course.fast.ai)
    2. [33]http://www.wildml.com/2015/11/understanding-convolutional-neura
       l-networks-for-nlp/
    3. [34]https://medium.com/towards-data-science/linear-algebra-cheat-sh
       eet-for-deep-learning-cd67aba4526c
    4. [35]https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets
       /
    5. [36]https://medium.com/technologymadeeasy/the-best-explanation-of-c
       onvolutional-neural-networks-on-the-internet-fbb8b1ad5df8
    6. [37]http://image-net.org/challenges/posters/jku_en_rgb_schwarz_post
       er.pdf

     * [38]machine learning
     * [39]artificial intelligence
     * [40]image analysis
     * [41]medical imaging
     * [42]dicom

   (button)
   (button)
   (button) 166 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [43]go to the profile of taposh dutta-roy

[44]taposh dutta-roy

     * (button)
       (button) 166
     * (button)
     *
     *

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/166532e964e6
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@taposhdr/medical-image-analysis-with-deep-learning-ii-166532e964e6&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@taposhdr/medical-image-analysis-with-deep-learning-ii-166532e964e6&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@taposhdr?source=post_header_lockup
  10. https://medium.com/@taposhdr
  11. https://medium.com/@taposhdr/medical-image-analysis-with-deep-learning-i-23d518abf531
  12. http://opencv.org/
  13. http://dicom.nema.org/
  14. https://www.kaggle.com/c/data-science-bowl-2017
  15. http://deeplearning.stanford.edu/wiki/index.php/feature_extraction_using_convolution
  16. http://course.fast.ai/
  17. https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c#.hbwzl7b7o
  18. http://cs231n.github.io/convolutional-networks/
  19. http://xrds.acm.org/blog/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/
  20. http://intellabs.github.io/rivertrail/tutorial/
  21. https://docs.gimp.org/en/plug-in-convmatrix.html
  22. https://github.com/vdumoulin/conv_arithmetic
  23. http://image-net.org/challenges/posters/jku_en_rgb_schwarz_poster.pdf
  24. http://image-net.org/challenges/posters/jku_en_rgb_schwarz_poster.pdf
  25. https://arxiv.org/pdf/1505.00853.pdf
  26. https://en.wikipedia.org/wiki/gaussian_noise
  27. http://jmlr.org/proceedings/papers/v48/gulcehre16.pdf
  28. http://cs231n.github.io/convolutional-networks/#pool
  29. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
  30. https://arxiv.org/abs/1502.03167
  31. http://course.fast.ai/
  32. https://en.wikipedia.org/wiki/logistic_function
  33. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
  34. https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c
  35. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
  36. https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8
  37. http://image-net.org/challenges/posters/jku_en_rgb_schwarz_poster.pdf
  38. https://medium.com/tag/machine-learning?source=post
  39. https://medium.com/tag/artificial-intelligence?source=post
  40. https://medium.com/tag/image-analysis?source=post
  41. https://medium.com/tag/medical-imaging?source=post
  42. https://medium.com/tag/dicom?source=post
  43. https://medium.com/@taposhdr?source=footer_card
  44. https://medium.com/@taposhdr

   hidden links:
  46. https://medium.com/p/166532e964e6/share/twitter
  47. https://medium.com/p/166532e964e6/share/facebook
  48. https://medium.com/p/166532e964e6/share/twitter
  49. https://medium.com/p/166532e964e6/share/facebook
