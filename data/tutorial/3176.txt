   #[1]rare technologies    feed [2]rare technologies    comments feed
   [3]rare technologies    author-topic models: why i am working on a new
   implementation comments feed [4]alternate [5]alternate

   [tr?id=1761346240851963&ev=pageview&noscript=1]

   iframe: [6]https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld

   [7]pragmatic machine learning rare technologies [8]navigation

     * [9]services
     * [10]products
          + [11]pii tools
          + [12]scaletext
     * [13]corporate training
          + [14]overview
          + [15]python best practices
          + [16]practical machine learning
          + [17]topic modelling
          + [18]deep learning in practice
     * [19]for students
          + [20]open source
          + [21]incubator
          + [22]competitions
     * [23]company
          + [24]careers
          + [25]our team
     * [26]blog
     * [27]contact
     * [28]search

     * [29]services
     * [30]products
          + [31]pii tools
          + [32]scaletext
     * [33]corporate training
          + [34]overview
          + [35]python best practices
          + [36]practical machine learning
          + [37]topic modelling
          + [38]deep learning in practice
     * [39]for students
          + [40]open source
          + [41]incubator
          + [42]competitions
     * [43]company
          + [44]careers
          + [45]our team
     * [46]blog
     * [47]contact
     * [48]search

author-topic models: why i am working on a new implementation

   [49]  lavur mortensen 2016-12-06[50] gensim, [51]machine learning,
   [52]open source, [53]programming, [54]student incubator

   author-topic models promise to give data scientists a tool to
   simultaneously gain insight about authorship and content in terms of
   latent topics.

   the model is closely related to id44 (lda).
   basically, each author can be associated with multiple documents, and
   each document can be attributed to multiple authors. the model learns
   topic representations for each author, so that we may ask questions
   like    what topic does author x typically write about?    or    how similar
   are authors x and y?   . more generally, the model enables us to learn
   topic representations of any labels on documents (more on this later).

   while there exist implementations of the author-topic model, they are
   slow, not scalable and have poor documentation and interfaces. so for
   the past few months, i have been working on getting us an
   implementation with all the nice properties we have become used to in
   gensim   s lda, that is,
     * fast training
     * scalability
     * streaming corpora

   among other things. in this post i will share my progress in this
   endeavor, and hopefully get some of you interested in trying the
   implementation once it   s released.

short review of available software

   there are, to the best of my knowledge, three implementations of the
   author-topic model:
     * the [55]python-topic-model module.
     * the [56]matlab id96 toolbox.
     * an [57]lda package in c++ by yoshua bengio.

   the python-topic-model version is quite slow. i tried training
   author-topic model on a small dataset. after about a minute my own
   algorithm started giving reasonable results, whereas the
   python-topic-model   s version yielded poor results even after 15 minutes
   of training.

   the c++ implementation has quite poor documentation, and as the github
   page describes the package as    practice of lda and other topic model
   based collapsed id150.    it doesn   t suggest that it is geared
   towards users.

   the matlab implementation seems to produce decent results quite
   quickly. a port of this algorithm over to python would not be a bad
   idea, so that data scientists without mathworks licenses (matlab is a
   proprietary piece of software) could use it. however, as the algorithm
   is based on id150, the scalability of the implementation is
   questionable (more on this in the next section). as this implementation
   does not compute any kind of likelihood, it will be difficult to
   objectively compare the scalability to my own implementation.

id136

   training models like the author-topic model and lda typically apply
   either id150 or variational id136. all the implementations
   discussed in the previous section apply some form of id150
   (perhaps blocking and/or collapsed id150).

id58

   id58 (vb) is known to enable fast and scalable id136.
   for this exact reason, gensim   s lda is trained using vb. it is also
   easy to formulate an [58]online algorithm (streamable corpus) within
   the vb framework. for these reasons, it was decided to implement an
   online vb algorithm to train the model. this required some theoretical
   work, as a vb algorithm has never been developed for the author-topic
   model. technically, the algorithm that was developed is a blocking
   id58 algorithm.

   vb lends itself quite well to parallel computation. while i have not
   implemented yet, it shouldn   t be a great challenge.

collapsed id58

   another candidate algorithm that was considered was collapsed
   id58 (cvb). the theoretical groundwork has been laid out
   for this model, by [59]ngo and co-authors in 2016, although no code has
   been made publicly available. [60]online cvb is also possible.

   cvb converges faster than standard vb, and would therefore be
   preferred. however, the natural first step would be to migrate the lda
   implementation over to cvb, before doing the same in the author-topic
   model. therefore standard vb was applied rather than cvb. it would be
   interesting to see a scalable, fast, online cvb implementation of lda
   in gensim, as it should be faster to train than the current
   implementation.

implementation

   algorithmic development has been the focal point of this project for
   the majority of its duration. recently, everything has been coming
   together nicely, and a refactor of the code is in progress so that it
   may be released. the refactored code will have similar interface and
   structure to gensim   s lda, and thus jumping into the model will be
   relatively easy for users and developers alike. at this point, it is
   not certain when the code will be released, but it shouldn   t be long.

not only authors

   while the name of the model suggests that it has something to do with
   authors, it is really a much more general abstraction of the standard
   topic model. the    authors    can represent any kind of label on
   documents, in particular when there are several labels per document and
   several documents per label; for example, tags on internet posts could
   be used. indeed, the model can be applied to any kind of data that have
   a similar structure, such as genomics data ([61]lda is used for this as
   well) or neuroimaging data (see the [62]cvb paper mentioned earlier).

final remark

   gensim has for a long time been at the forefront of bridging the gap
   between research and practice in natural language processing. while the
   author-topic model has not drawn a lot of attention yet, i hope that
   this implementation will provide people with the opportunity to
   experiment with it, and that we will see some interesting use cases as
   a result.

   [63]author-topic models[64]gensim[65]latent dirichlet
   allocation[66]topic modelling

author of post

     lavur mortensen

  lavur mortensen's bio:

   master's student in applied mathematics at the technical university of
   denmark, focused on machine learning, nlp, and id111. graduate of
   rare incubator.

need expert consulting in ml and nlp?

   ________________________________________

   ________________________________________


   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   ________________________________________
   please leave this field empty. ________________________________________

   send

categories

   categories[select category___________]

archives

   archives [select month__]

recent posts

     * [67]export pii drill-down reports
     * [68]personal data analytics
     * [69]scanning office 365 for sensitive pii information
     * [70]pivoted document length normalisation
     * [71]sent2vec: an unsupervised approach towards learning sentence
       embeddings

stay ahead of the curve

get our latest tutorials, updates and insights delivered straight to your
inbox.

   ____________________

   ____________________

   subscribe
   ____________________
   1-2 times a month, if lucky. your information will not be shared.

   [72][footer-logo.png]
     * [73]services
     * [74]careers
     * [75]our team
     * [76]corporate training
     * [77]blog
     * [78]incubator
     * [79]contact
     * [80]competitions
     * [81]site map

   rare technologies [82][email protected] sv  tova 5, prague, czech
   republic [83](eu) +420 776 288 853
   type and press    enter    to search ____________________

references

   visible links
   1. https://rare-technologies.com/feed/
   2. https://rare-technologies.com/comments/feed/
   3. https://rare-technologies.com/author-topic-models-why-i-am-working-on-a-new-implementation/feed/
   4. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/author-topic-models-why-i-am-working-on-a-new-implementation/
   5. https://rare-technologies.com/wp-json/oembed/1.0/embed?url=https://rare-technologies.com/author-topic-models-why-i-am-working-on-a-new-implementation/&format=xml
   6. https://www.googletagmanager.com/ns.html?id=gtm-t2pcjld
   7. https://rare-technologies.com/
   8. https://rare-technologies.com/author-topic-models-why-i-am-working-on-a-new-implementation/
   9. https://rare-technologies.com/services/
  10. https://rare-technologies.com/author-topic-models-why-i-am-working-on-a-new-implementation/
  11. https://pii-tools.com/
  12. https://scaletext.com/
  13. https://rare-technologies.com/corporate-training/
  14. https://rare-technologies.com/corporate-training/
  15. https://rare-technologies.com/python-best-practices/
  16. https://rare-technologies.com/practical-machine-learning/
  17. https://rare-technologies.com/topic-modelling-training/
  18. https://rare-technologies.com/deep_learning_training/
  19. https://rare-technologies.com/incubator
  20. https://github.com/rare-technologies/
  21. https://rare-technologies.com/incubator/
  22. https://rare-technologies.com/competitions/
  23. https://rare-technologies.com/#braintrust
  24. https://rare-technologies.com/careers/
  25. https://rare-technologies.com/our-team/
  26. https://rare-technologies.com/blog/
  27. https://rare-technologies.com/contact/
  28. https://rare-technologies.com/author-topic-models-why-i-am-working-on-a-new-implementation/
  29. https://rare-technologies.com/services/
  30. https://rare-technologies.com/author-topic-models-why-i-am-working-on-a-new-implementation/
  31. https://pii-tools.com/
  32. https://scaletext.com/
  33. https://rare-technologies.com/corporate-training/
  34. https://rare-technologies.com/corporate-training/
  35. https://rare-technologies.com/python-best-practices/
  36. https://rare-technologies.com/practical-machine-learning/
  37. https://rare-technologies.com/topic-modelling-training/
  38. https://rare-technologies.com/deep_learning_training/
  39. https://rare-technologies.com/incubator
  40. https://github.com/rare-technologies/
  41. https://rare-technologies.com/incubator/
  42. https://rare-technologies.com/competitions/
  43. https://rare-technologies.com/#braintrust
  44. https://rare-technologies.com/careers/
  45. https://rare-technologies.com/our-team/
  46. https://rare-technologies.com/blog/
  47. https://rare-technologies.com/contact/
  48. https://rare-technologies.com/author-topic-models-why-i-am-working-on-a-new-implementation/
  49. https://rare-technologies.com/author/olavur/
  50. https://rare-technologies.com/category/gensim/
  51. https://rare-technologies.com/category/machine-learning/
  52. https://rare-technologies.com/category/open-source/
  53. https://rare-technologies.com/category/programming/
  54. https://rare-technologies.com/category/student-incubator/
  55. https://github.com/arongdari/python-topic-model
  56. http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm
  57. https://github.com/ybenjo/lda
  58. https://www.cs.princeton.edu/~blei/papers/hoffmanbleibach2010b.pdf
  59. https://people.csail.mit.edu/ythomas/publications/2016authortopiccvb-prni.pdf
  60. https://arxiv.org/pdf/1305.2452v1.pdf
  61. https://www.ncbi.nlm.nih.gov/pubmed/25916734
  62. https://people.csail.mit.edu/ythomas/publications/2016authortopiccvb-prni.pdf
  63. https://rare-technologies.com/tag/author-topic-models/
  64. https://rare-technologies.com/tag/gensim/
  65. https://rare-technologies.com/tag/latent-dirichlet-allocation/
  66. https://rare-technologies.com/tag/topic-modelling/
  67. https://rare-technologies.com/personal-data-reports/
  68. https://rare-technologies.com/pii_analytics/
  69. https://rare-technologies.com/pii-scan-o365-connector/
  70. https://rare-technologies.com/pivoted-document-length-normalisation/
  71. https://rare-technologies.com/sent2vec-an-unsupervised-approach-towards-learning-sentence-embeddings/
  72. https://rare-technologies.com/author-topic-models-why-i-am-working-on-a-new-implementation/
  73. https://rare-technologies.com/services/
  74. https://rare-technologies.com/careers/
  75. https://rare-technologies.com/our-team/
  76. https://rare-technologies.com/corporate-training/
  77. https://rare-technologies.com/blog/
  78. https://rare-technologies.com/incubator/
  79. https://rare-technologies.com/contact/
  80. https://rare-technologies.com/competitions/
  81. https://rare-technologies.com/sitemap
  82. https://rare-technologies.com/cdn-cgi/l/email-protection#6c05020a032c1e0d1e094118090f04020300030b05091f420f0301
  83. tel:+420 776 288 853

   hidden links:
  85. https://rare-technologies.com/author-topic-models-why-i-am-working-on-a-new-implementation/#top
  86. https://www.facebook.com/raretechnologies
  87. https://twitter.com/raretechteam
  88. https://www.linkedin.com/company/6457766
  89. https://github.com/piskvorky/
  90. https://rare-technologies.com/feed/
