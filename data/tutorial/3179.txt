   #[1]magenta

   [2]magenta logo

   (button)
   [3]get started [4]demos [5]blog [6]research [7]community

   [8]get started [9]demos [10]blog [11]research [12]community

nsynth: neural audio synthesis

   apr 6, 2017
       nsynth

   one of the goals of magenta is to use machine learning to develop new
   avenues of human expression. and so today we are proud to announce
   nsynth (neural synthesizer), a novel approach to music synthesis
   designed to aid the creative process.

   unlike a traditional synthesizer which generates audio from
   hand-designed components like oscillators and wavetables, nsynth uses
   deep neural networks to generate sounds at the level of individual
   samples. learning directly from data, nsynth provides artists with
   intuitive control over timbre and dynamics and the ability to explore
   new sounds that would be difficult or impossible to produce with a
   hand-tuned synthesizer.

   the acoustic qualities of the learned instrument depend on both the
   model used and the available training data, so we are delighted to
   release improvements to both:
     * a [13]dataset of musical notes an order of magnitude larger than
       other publicly available corpora.
     * a novel [14]wavenet-style autoencoder model that learns codes that
       meaningfully represent the space of instrument sounds.

   a full description of the dataset and the algorithm can be found in our
   [15]arxiv paper.

the nsynth dataset

   we wanted to develop a creative tool for musicians and also provide a
   new challenge for the machine learning community to galvanize research
   in generative models for music. to satisfy both of these objectives, we
   built the nsynth dataset, a large collection of annotated musical notes
   sampled from individual instruments across a range of pitches and
   velocities. with ~300k notes from ~1000 instruments, it is an order of
   magnitude larger than comparable public datasets. you can download it
   [16]here.

   a motivation behind the nsynth dataset is that it lets us explicitly
   factorize the generation of music into notes and other musical
   qualities. we could further factorize those qualities, but for
   simplicity we don   t and get the following:

   the goal is to model (known as timbre) and it is assumed that comes
   from a higher-level    language model    of music, such as the note
   sequence id56s we   ve [17]previously described. while not perfect, this
   factorization is grounded in how instruments work and is surprisingly
   effective. indeed, much modern music production employs such a
   factorization, using midi for note sequences and software synthesizers
   for timbre. of course, this works better for some instruments (e.g.,
   piano and electronic synthesizer) than for others (e.g., guitar and
   saxophone) where note-to-note timbre dependencies are more pronounced.

   the nsynth dataset was inspired by image recognition datasets that have
   been core to recent progress in deep learning. similar to how many
   image datasets focus on a single object per example, the nsynth dataset
   hones in on single notes. we encourage the broader community to use it
   as a benchmark and entry point into audio machine learning. we hope
   that this serves as a building block for future datasets and envision a
   high-quality multi-note dataset for tasks like generation and
   transcription that involve learning complex language-like dependencies.

learning temporal embeddings

   [18]wavenet is an expressive model for temporal sequences such as
   speech and music. as a deep autoregressive network of dilated
   convolutions, it models sound one sample at a time, similar to a
   nonlinear infinite impulse response filter. since the context of this
   filter is currently limited to several thousand samples (about half a
   second), long-term structure requires a guiding external signal.
   [19]prior work demonstrated this in the case of text-to-speech and used
   previously learned linguistic embeddings to create impressive results.

   in this work, we removed the need for conditioning on external features
   by employing a wavenet-style autoencoder to learn its own temporal
   embeddings.
   [nsynth_blog_figs_wavenetae_diagram.png]

   the temporal encoder looks very much like a wavenet and has the same
   dilation block structure. however, its convolutions are not causal, so
   it sees the entire context of the input chunk. after thirty layers of
   computation, there is then a final average pooling to create a temporal
   embedding of 16 dimensions for every 512 samples. consequently, the
   embedding can be thought of as a 32x compression of the original data.

   we condition the vanilla wavenet decoder with this embedding by
   upsampling it to the original time resolution, applying a 1x1
   convolution, and finally adding this result as a bias to each of the
   decoder   s thirty layers. note that this conditioning is not external as
   it   s learned by the model. since the embeddings bias the autoregressive
   system, we can imagine it acting as a driving function for a nonlinear
   oscillator. this interpretation is corroborated by the fact that the
   magnitude contours of the embeddings mimic those of the audio itself.
   [nsynth_blog_figs_three_reconstructions.png]
   bass original
   bass wavenet
   glockenspiel original
   glockenspiel wavenet
   flugelhorn original
   flugelhorn wavenet

      rainbowgrams    of audio and reconstructions for three different
   instruments. these are [20]cqt spectrograms with magnitude represented
   by intensity and instantaneous frequency by color. frequency is on the
   vertical axis and time is on the horizontal axis. for the embeddings,
   the different colors represent the 16 different dimensions at 125
   timesteps (32ms per step). there is a slight built-in distortion due to
   the compression of the 8-bit mu-law encoding. it is a minor effect for
   many samples, but is more pronounced for lower frequencies.

a latent space of timbre and dynamics

   we   ll soon be releasing an instrument as an interactive demonstration,
   but in the meantime here   s an illuminating example of what you can do
   with this technology.

   below, there are two columns of audio clips matching the two rows of
   rainbowgrams. the top row of rainbowgrams corresponds to the left
   column of audio and is the result of evenly interpolating across
   instruments by adding the signals. the bottom row of rainbowgrams
   corresponds to the right column of audio and is the result of using
   nsynth to linearly interpolate in the embedding space. try playing the
   clips starting with the bass, then the bassflute, and so on. what you
   hear in the left column is the linear addition of the signals in the
   audio output space. as expected, it sounds like the two instruments
   being played at the same time. in the right column, however, the new
   notes combine semantic aspects of the two original sounds to create a
   unique sound that is still musical.
   [nsynth_blog_figs_interpolatez.png]
   bass original
   bass wavenet
   bass+flute original
   bass+flute wavenet
   flute original
   flute wavenet
   flute+organ original
   flute+organ wavenet
   organ original
   organ wavenet
   organ+bass original
   organ+bass wavenet
   bass original
   bass wavenet

   further, the learned embeddings only capture a local context, much like
   a spectrogram, which enables them to generalize in time. despite having
   only trained on short single notes, the model can successfully
   reconstruct both a whole series of notes as well as notes played for
   longer than three seconds.
   [nsynth_blog_figs_extrapolation.png]
   scale original
   scale wavenet

   while the wavenet autoencoder adds more harmonics to the original
   timbre, it follows the fundamental frequency up and down two octaves.
   the fact that it has never seen a transition between two notes is clear
   as its best approximation is to just smoothly glissando between them.

release++

   besides the music examples and the dataset, we are also releasing the
   code for both the wavenet autoencoder powering nsynth as well as our
   best baseline spectral autoencoder model. in addition, we are releasing
   the trained weights as a tensorflow checkpoint and a script to save
   embeddings from your own wav files. you can find all the code at our
   [21]repository and the checkpoint tarball can be downloaded [22]here.

   part of the goal of magenta is to close the loop between artistic
   creativity and machine learning, so we have also released [23]playable
   instruments for you to make your own music with these techonologies.

a group effort

   this work was a collaboration between the [24]google brain team and
   [25]deepmind. the main contributors were jesse engel, cinjon resnick,
   adam roberts, sander dieleman, karen simonyan, mohammad norouzi, and
   doug eck. we especially want to acknowledge sander and karen for their
   key algorithmic contributions and adam for handling the brunt of the
   dataset.

   we also want to acknowledge hans bernhard, colin raffel, sageev oore,
   yotam mann, ron weiss, and rif saurus. your help was much appreciated.

     * [26]twitter
     * [27]blog
     * [28]github
     * [29]privacy
     * [30]terms

references

   visible links
   1. https://magenta.tensorflow.org/feed.xml
   2. https://magenta.tensorflow.org/
   3. https://magenta.tensorflow.org/get-started
   4. https://magenta.tensorflow.org/demos
   5. https://magenta.tensorflow.org/blog
   6. https://magenta.tensorflow.org/research
   7. https://magenta.tensorflow.org/community
   8. https://magenta.tensorflow.org/get-started
   9. https://magenta.tensorflow.org/demos
  10. https://magenta.tensorflow.org/blog
  11. https://magenta.tensorflow.org/research
  12. https://magenta.tensorflow.org/community
  13. https://magenta.tensorflow.org/datasets/nsynth
  14. https://github.com/tensorflow/magenta/tree/master/magenta/models/nsynth
  15. https://arxiv.org/abs/1704.01279
  16. https://magenta.tensorflow.org/datasets/nsynth
  17. https://magenta.tensorflow.org/2016/06/10/recurrent-neural-network-generation-tutorial
  18. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  19. https://arxiv.org/abs/1609.03499
  20. https://en.wikipedia.org/wiki/constant-q_transform
  21. https://github.com/tensorflow/magenta/tree/master/magenta/models/nsynth
  22. http://download.magenta.tensorflow.org/models/nsynth/wavenet-ckpt.tar
  23. https://magenta.tensorflow.org/nsynth-instrument
  24. https://research.google.com/teams/brain/
  25. https://deepmind.com/
  26. https://twitter.com/search?q=#madewithmagenta
  27. https://magenta.tensorflow.org/blog
  28. https://github.com/tensorflow/magenta
  29. https://www.google.com/policies/privacy/
  30. https://www.google.com/policies/terms/

   hidden links:
  32. https://ai.google/
