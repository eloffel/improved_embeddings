   #[1]mcneela.github.io rss feed

[2]daniel mcneela

     * [3]about me
     * [4]resume
     * [5]portfolio
     * [6]machine learning
     * [7]mathematics
     * [8]computer science
     * [9]books

     * [10]search ____________________
     * [11]menu

   ____________________

     * [12]

home
       return to the homepage
     * [13]

resume
       my curriculum vitae
     * [14]

portfolio
       my coding portfolio
     * [15]

mathematics
       my ruminations on all things mathematical
     * [16]

computer science
       my thoughts on various topics in computer science

           the universal approximation theorem for neural networks

   in 1989, hornik, stinchombe, and white published a proof of the fact
   that for any continuous function $f$ on a compact set $k$, there exists
   a feedforward neural network, having only a single hidden layer, which
   uniformly approximates $f$ to within an arbitrary $\varepsilon > 0$ on
   $k$. i present here an elegant proof of the same fact given by cybenko.
   in my exposition, i'll attempt to clear up some points of confusion
   which i encountered on my first read through as well as clarify some of
   the finer mathematical details that cybenko glossed over in his paper.
   that said, the crux of the the theorem is simple, and begins with a few
   definitions. as i proceed through the proof, i'll try to bolster the
   intuition behind it. knowledge of measure theory and functional
   analysis is ideal for understanding the details of the proof, although
   it might be possible to grasp the general argument given a strong
   background in elementary real analysis. by $i_n$, we denote the
   $n$-dimensional unit cube which we can represent as a cartesian product
   of unit intervals $[0, 1]^n$.
   [i3.png]
   as is the convention in mathematical analysis, we write $c(i_n)$ to
   refer to the space of continuous functions (with codomain $\mathbb{c}$)
   on $i_n$. in general, this is a vector space. additionally, we define
   $m(i_n)$ to be the space of finite, signed regular borel measures on
   $i_n$. for those unfamiliar, a measure $\mu$ is regular if and only if
   the following three conditions are satisfied
    1. $\mu(k) < \infty$ for all compact sets $k$
    2. $\mu(e) = \inf\{\mu(u) : e \subseteq u, u \text{ open } \}$
    3. $\mu(e) = \sup\{\mu(k) : k \subseteq e, k \text{ compact} \}$

   intuitively, a single measure imposes some notion of size or volume on
   a set. while different measures may define different sizes for a single
   set, they all ideally convey some idea of how much space that set takes
   up relative to the larger space in which it resides. all the
   integration to be done throughout this proof will be with respect to
   regular measures on $c(i_n)$. why is this important? for one thing, as
   their name suggests, regular measures exhibit a certain structural
   "regularity". they behave in the way that, in some sense, one would
   intuitively expect. for example, the measure of a compact set, has
   finite measure when the measure being applied is regular. this makes
   sense. in a metric space, compact sets are closed and bounded, and it
   would be weird for a set $k$ to be compact yet somehow take up infinite
   space. similarly, if we are measuring a set $e$ and we take the lower
   bound on the measure of open sets subsuming $e$, we should get the
   measure of $e$. the same goes for when we approximate $e$ by compact
   sets from below. another reason for restricting ourselves to regular
   measures is that this will be a prerequisite for invoking the riesz
   representation theorem in our proof. more on this in a bit. now, recall
   the uniform norm of a function $f:a \to b$, $$ \|f\| = \sup \{|f(x)| :
   x \in a \} $$ in other words, the uniform norm gives an upper bound on
   all possible values of $f$, and for two functions $f, g$ we see that
   $\| f - g \|$ gives a uniform bound on the extent to which $f$ and $g$
   differ from one another. what we aim to show is that the set of
   feedforward neural networks is dense in $c(i_n)$ with respect to the
   uniform norm. a set $a$ is dense in $x$ if $\overline{a} = x$ (the
   closure of $a$ is the entire space $x$). since we are working in a
   metric space where we have a notion of distance between functions (here
   the metric is the uniform norm), we can define "denseness" in more
   intuitive terms. in this case, we say $a$ is dense in $x$ if for every
   $x\in x$ there exists a sequence $(a_n)_{n\in \mathbb{n}} \in a$ such
   that $\lim_{n \to \infty} a_n = x$. with this in mind, we can
   reformulate the guarantee of the uniform approximation theorem in a few
   different ways
    1. the set of all feedforward neural networks $\mathcal{n}$ is dense
       in $c(i_n)$.
    2. for every continuous function $f \in c(i_n)$, there exists a
       sequence of neural networks $(n_j) \in \mathcal{n}$ converging to
       $f$, i.e. $\lim_{j \to \infty} n_j = f$.
    3. for every continuous function $f \in c(i_n)$ and $\varepsilon > 0$
       there exists a neural network $g \in \mathcal{n}$ such that $\|g -
       f\| < \varepsilon$.

   that last formulation may be the most intuitive. effectively, it's
   saying that you can specify an arbitrary precision $\varepsilon$ to
   within which you'd like to approximate a continuous function $f$ on
   $i_n$ and a neural network exists that will accomplish this. of course,
   the proof gives us no way to go about explicitly constructing or
   finding such a neural network, but hopefully the guarantee still
   provides you with some theoretical peace of mind. having formulated our
   problem, we begin by giving an explicit mathematical definition of a
   feedforward neural network. definition: a feedforward [17]neural
   network having $n$ units or neurons arranged in a single hidden layer
   is a function $y : \mathbb{r}^d \to \mathbb{r}$ of the form
   $$y(\mathbf{x}) = \sum_{i=1}^n \alpha_i \sigma
   \left(\mathbf{w}_i^{t}\mathbf{x} + b_i \right)$$ where $\mathbf{w}_i,
   \mathbf{x} \in \mathbb{r}^d$, $\alpha_i, b_i \in \mathbb{r}$, and
   $\sigma$ is a nonlinear sigmoidal activation function. the
   $\mathbf{w}_i$ are the weights of the individual units and are applied
   to the input $\mathbf{x}$. the $\alpha_i$ are the network weights and
   are applied to the output of each unit in the hidden layer. finally,
   $b_i$ is the bias of unit $i$. in effect, a neural network of this sort
   is just a linear combination of affine transformations of its inputs
   under sigmoidal activations. by an [18]affine transformation we mean
   the $\mathbf{w}_i\mathbf{x} + b_i$ part of the definition. but what's a
   sigmoidal activation function? definition: a sigmoidal activation
   function $\sigma : \mathbb{r} \to \mathbb{r}$ satisfies \[ \sigma(t)
   \to \begin{cases} 1 & \text{ as } t \to \infty \\ 0 & \text{ as } t \to
   -\infty \end{cases} \] in other words, as $t$ gets exponentially large,
   $\sigma(t)$ limits towards 1, and as $t$ gets exponentially negative,
   $\sigma(t)$ limits towards 0.
   pictured above is the softmax function, one example of a sigmoidal
   function. as $x, y$ tend to $-\infty$ we see that $z$ tends to 0.
   conversely, if $x,y$ tend to $\infty$, $z$ tends to 1. definition: we
   say $\sigma$ is discriminatory if for $\mu \in m(i_n)$ and \[
   \int_{i_n} \sigma(\mathbf{w}_i^t\mathbf{x} + b_i)\ d\mu(\mathbf{x}) = 0
   \] for all $\mathbf{w}_i \in \mathbb{r}^d, b_i \in \mathbb{r}$ then
   $\mu = 0$. this definition tells us something interesting about
   discriminatory $\sigma$. in some sense, a discriminatory $\sigma$ is
   volumetrically non-destructive when it acts on linear transformations
   of input. that is, the definition tells us that for nonzero $\mu$,
   there exist $\mathbf{w}_i, b_i$ such that $\int_{i_n}
   \sigma(\mathbf{w}_i^t\mathbf{x} + b_i)\ d\mu(x) \neq 0$. in other
   words, the two opposing limit properties of $\sigma$ prevent it from
   "losing" the information conveyed in the weighted-then-shifted input
   $\mathbf{x}$ by, for example, ensuring that it can't send the affine
   space $\mathbf{w}_i\mathbf{x} + b_i$ to a set of measure zero. we are
   now ready to present the universal approximation theorem and its proof.
   theorem 1 if the $\sigma$ in the neural network definition is a
   continuous, discriminatory function, then the set of all neural
   networks is dense in $c(i_n)$. proof: let $\mathcal{n} \subset c(i_n)$
   be the set of neural networks. as mentioned earlier, $\mathcal{n}$ is a
   linear subspace of $c(i_n)$. to prove that $\mathcal{n}$ is dense in
   $c(i_n)$, we will prove that its closure is $c(i_n)$. by way of
   contradiction, suppose that $\overline{\mathcal{n}} \neq c(i_n)$. then
   $\overline{\mathcal{n}}$ is a closed, proper subspace of $c(i_n)$. now,
   let's take a quick digression here to review the statement of the
   hahn-banach theorem. hahn-banach theorem let $x$ be a real vector
   space, $p$ a sublinear functional on $x$, $m$ a subspace of $x$, and
   $f$ a linear functional on $m$ such that $f(x) \leq p(x)$ for all $x
   \in m$. then there exists a linear functional $f$ on $x$ such that
   $f(x) \leq p(x)$ for all $x \in x$ and $f|m = f$. for us, the
   hahn-banach theorem gives us a way to extend a bounded linear
   functional defined on $\mathcal{n}$ to one defined on all of $c(i_n)$
   in a way that will eventually allow us to derive a contradiction. by
   the hahn-banach theorem, there exists a bounded linear functional on
   $c(i_n)$, call it $l$, such that $l(\mathcal{n}) =
   l(\overline{\mathcal{n}}) = 0$ but $l \neq 0$. recall that a [19]linear
   functional is just a linear map from a vector space $x$ to $a$ where $a
   = \mathbb{r} \text{ or } \mathbb{c}$. it should be obvious that
   $\int_{i_n}$ acts as a linear functional on $c(i_n)$ (if it's not,
   consider the domain and codomain of lebesgue integration). now we give
   the statement of the riesz representation theorem. riesz representation
   theorem if $i$ is a positive linear functional on $c_c(x)$, there is a
   unique radon measure $\mu$ on $x$ such that $i(f) = \int f\ d\mu$ for
   all $f \in c_c(x)$. moreover, $\mu$ satisfies $$\mu(u) = \sup\{i(f) : f
   \in c_c(x), 0 \leq f \leq 1, \text{supp}(f) \subset u\}\text{ for all
   open } u \subset x$$ and $$\mu(k) = \inf\{i(f) : f \in c_c(x), 0 \leq f
   \leq 1, f \geq \chi_k\}\text{ for all compact } k \subset x$$ where
   $c_c(x)$ denotes the functions of compact support defined on $x$ and
   $\chi_k$ is the indicator function of the compact set $k$. thankfully,
   in our case $c(i_n)$ is $\sigma$-compact so instead of dealing with
   radon measures, we can restrict ourselves to the regular borel measures
   introduced earlier. by the riesz representation theorem, we can write
   the functional $l$ in the following form $$l(h) = \int_{i_n} h(x)\ d\mu
   (x)$$ for some $\mu \in m(i_n)$, for all $h \in c(i_n)$. in particular,
   since by definition any neural network is a member of $\mathcal{n}$,
   and $l$ is identically zero on $\mathcal{n}$, we have $$\int_{i_n}
   h(x)\ d\mu (x) = 0$$ since we took $\sigma$ to be discriminatory, we
   must have $\mu = 0$. but this contradicts our determination that $l
   \neq 0$, because $$\mu = 0 \implies \int_{i_n} h(x)\ d\mu(x) = 0 \quad
   \text{ for all } h \in c(i_n)$$ therefore, $\mathcal{n}$ is dense in
   $c(i_n)$. $\square$ we have arrived at our desired result, but it all
   hinges on our assumption that the sigmoid function $\sigma(t)$ is
   discriminatory, a fact that we have not yet proved. let's do that now.
   lemma any bounded, measurable sigmoidal function $\sigma$ is
   discriminatory. in particular, any continuous sigmoidal function is
   discriminatory. proof: for any $x, y, \theta, \varphi$ we have
   \begin{align*} \sigma_{\lambda}(x) = \sigma(\lambda(y^tx + \theta) +
   \varphi) = \begin{cases} \to 1 & \text{ for } y^tx + \theta > 0 \text{
   as } \lambda \to \infty \\ \to 0 & \text{ for } y^tx + \theta < 0
   \text{ as } \lambda \to \infty \\ = \sigma(\varphi) & \text{ for } y^tx
   + \theta = 0 \text{ for all } \lambda \end{cases} \end{align*} this can
   be seen by applying the properties of the sigmoidal function to its
   input for varying values of $\lambda$. in other words, as $\lambda \to
   \infty$ for $y^tx + \theta > 0$ we are in essence calculating
   $\sigma(t)$ for $t \to \infty$. similarly, as $\lambda \to \infty$ for
   $y^tx + \theta < 0$ we get $\sigma(t)$ for $t \to -\infty$. the third
   case is obvious. thus the functions parameterized by $\lambda$,
   $\sigma_{\lambda}(x)$ converge pointwise and boundedly to
   \begin{align*} \gamma(x) = \begin{cases} 1 & \text{ for } y^tx + \theta
   > 0 \\ 0 & \text{ for } y^tx + \theta < 0 \\ \sigma(\varphi) & \text{
   for } y^tx + \theta = 0 \\ \end{cases} \end{align*} as $\lambda \to
   \infty$. this follows directly from the above. let $\pi_{y, \theta} =
   \{x | y^tx + \theta = 0\}$ be an affine hyperplane and let $h_{y,
   \theta}$ be the open half-space defined by $\{y^tx + \theta > 0\}$.
   note that $|\sigma_\lambda(x)| \leq \max(1, \sigma(\varphi))$ for all
   $x$. therefore, we can apply the dominated convergence theorem to get
   \begin{align*} \lim_{\lambda \to \infty} \int_{i_n}
   \sigma_{\lambda}(x)\ d\mu(x) = \int_{i_n} \lim_{\lambda \to \infty}
   \sigma_{\lambda}(x)\ d\mu(x) = \int_{i_n} \gamma(x)\ d\mu(x) =
   \sigma(\varphi)\mu(\pi_{y, \theta}) + \mu(h_{y, \theta}) \end{align*}
   for all $\varphi, \theta, y$. we want to show that if the above
   integral is equal to zero, then $\mu$ is forced to be 0. equivalently,
   we must show that the measure of all half-planes being 0 implies that
   the measure $\mu$ equals 0. to do this, we need to fix $y$. for a
   bounded, measurable function $h$, define a linear functional $f$ $$f(h)
   = \int_{i_n} h(y^tx)\ d\mu(x)$$ note that $f$ is a bounded linear
   functional on $l^{\infty}(\mathbb{r})$ since $\mu$ is a finite signed
   measure. this is because when we integrate with respect to a finite
   measure, we can't get an infinite result. let $h$ be the indicator
   function for the interval $[\theta, \infty)$ so that $$f(h) =
   \int_{i_n}h(y^tx)\ d\mu(x) = \mu(\pi_{y, -\theta}) + \mu(h_{y,
   -\theta}) = 0$$ to see why this is true, recall that the indicator
   function $\chi_a:x \to \{0, 1\}$ for a set $a \subseteq x$ is defined
   as follows $$\chi_a(x) = \begin{cases}1 & \text{ if } x \in a \\ 0 &
   \text{ otherwise }\end{cases}$$ thus, if $y^tx \in [\theta, \infty)$,
   then $y^tx - \theta \geq 0$. for the integral, this decomposes into the
   measure of two disjoint sets, the hyperplane $\pi_{y, -\theta} = \{x :
   y^tx - \theta = 0\}$ and the half-space $h_{y, -\theta} = \{x : y^tx -
   \theta > 0\}$. similarly, $f(h) = 0$ if $h$ is the indicator function
   for the open interval $(\theta, \infty)$. by linearity, $f$ is 0 for
   the indicator function on any interval and hence for any simple
   function. simple functions are dense in $l^{\infty}(\mathbb{r})$, so $f
   = 0$. in particular, the bounded, measurable functions $s(u) = \sin(m
   \cdot u)$ and $c(u) = \cos(m \cdot u)$ give $$f(s + ic) = \int_{i_n} =
   \cos(m^tx) + i\sin(m^tx)\ d\mu(x) = \int_{i_n} e^{im^tx}\ d\mu(x) = 0$$
   for all $m$. thus, the fourier transform of $\mu$ is 0 and so $\mu$
   must be zero as well. therefore $\sigma$ is discriminatory. here we
   chose a specific function $h$ which drives the measure $\mu$ to zero.
   the fourier transform is a clearly reasonable choice, although it seems
   apparent to me that other suitable functions exist.

references

   1. http://mcneela.github.io/feed.xml
   2. http://mcneela.github.io/index.html
   3. http://mcneela.github.io/about.html
   4. http://mcneela.github.io/resume/website_resume.pdf
   5. http://mcneela.github.io/portfolio.html
   6. http://mcneela.github.io/machinelearning.html
   7. http://mcneela.github.io/math.html
   8. http://mcneela.github.io/compsci.html
   9. http://mcneela.github.io/books.html
  10. http://mcneela.github.io/machine_learning/2017/03/21/universal-approximation-theorem.html#search
  11. http://mcneela.github.io/machine_learning/2017/03/21/universal-approximation-theorem.html#menu
  12. http://mcneela.github.io/machine_learning/2017/03/21/index.html
  13. http://mcneela.github.io/machine_learning/2017/03/21/resume.html
  14. http://mcneela.github.io/machine_learning/2017/03/21/portfolio.html
  15. http://mcneela.github.io/machine_learning/2017/03/21/math.html
  16. http://mcneela.github.io/machine_learning/2017/03/21/compsci.html
  17. https://en.wikipedia.org/wiki/artificial_neural_network
  18. https://en.wikipedia.org/wiki/affine_transformation
  19. https://en.wikipedia.org/wiki/linear_form
