   #[1]you canalytics-    feed [2]you canalytics-    comments feed [3]you
   canalytics-    intuitive machine learning : id119 simplified
   comments feed [4]alternate [5]alternate

   [6]linkedin
   [7]logo

     * [8]home
     * [9]blog-navigation
     * [10]art
     * [11]about
     * [12]contact

[13]intuitive machine learning : id119 simplified

      [14]roopam upadhyay [15]19 comments
   [16]gravity and id119 gravity and id119

   id119 and gravity     by roopam
     __________________________________________________________________

   how do machines learn? they learn the same way as humans. humans learn
   from experience and so do machines. for machines, the experience is in
   the form of data. machines use powerful algorithms to make sense of the
   data. they identify underlining patterns within the data to learn
   things about the world. like humans, machines use this learning to make
   decisions. for example, amazon.com uses machine learning to
   recommend products to you based on your surfing patterns on their
   website. forrester research estimates sales conversion through this
   recommendation engine as high as 60%. companies across the globe are
   making huge profits through decision making by machine learning
   algorithms.

   many powerful machine learning algorithms use id119
   optimization to identify patterns and learn from data. id119
   powers machine learning algorithms such as id75, logistic
   regression, neural networks, and support vector machines. in this
   article, we will gain an intuitive understanding of id119
   optimization. for this let   s take a dive towards these concepts with
   some help from   

gravity

   a few years ago my wife told me this incident about her friend who has
   a young daughter. one day my wife   s friend discovered that her young
   daughter had dropped and broken a lamp while playing. when she inquired
   from her daughter about how that happened, the little girl responded:
      grabili did it, mommy, it   s not [entirely] my fault, it   s grabili   .
   the girl was referring to gravity and she was right that gravity was an
   equal accomplice in her mischief. like the little girl, we all know
   through the works of galileo and newton that gravity is responsible for
   everything that falls.

   gravity has important lessons that will help us gain an intuitive and
   simplified understanding of id119.

gravity & id119 optimization

   consider these 3 situations where a ball is placed in a glass bowl at 3
   different positions. we all know through our experience with gravity
   that the force of gravity will pull the ball towards the bottom of the
   bowl.

   [17]picture1 picture1

   in situation 1, the ball will move from position b[1] to c[1] and not
   a[1]. this ball will continue to travel till it reaches the bottom of
   the bowl. in situation 2, since the static ball is already at the
   bottom it will stay at b[2 ]and it won   t move at all. the ball always
   tries to move from a higher potential energy state to a lower. gravity
   is responsible for these different potential energy states of the ball.

   now, you must ask: how is this similar to id119
   optimization? actually, this is exactly how id119
   optimization works. the only major difference is that id119
   optimization tries to minimize a id168 instead of potential
   energy.

   [18]bowl and ball bowl and ball before we explore more about the
   id168 (aka cost function) and create its connection with
   potential energy, let us look at how the ball will roll down when it is
   placed in situation 1. in terms of the coordinate system, the potential
   energy reduces as the ball rolls down the z-axis (vertical axis). the
   ball tried to modify its position on the x and y-axes to determine the
   lowest possible potential energy or the minimum possible value on the
   z-axis.

   also, notice that the ball experienced different pulls at different
   stages while it journeyed towards the bottom of the bowl. these pulls
   can be evaluated through the gradient or the slope of the orange arrows
   shown in the adjacent picture. the steeper the orange arrows larger is
   the force of gravity on the ball.

   for id119 optimization, the z-axis is the id168 and
   x &  y-axes are the coefficients of the model. the id168 is
   equivalent to the potential energy of the ball in the bowl. the idea is
   to minimize id168 by adjusting x & y-axes (i.e coefficients of
   the model).

   if this sounds a bit vague wait till we will solve an example of a
   id75 model later in this article. but before that let us
   refresh a few concepts from high school mathematics involving
   id128 and finding the minimum value for a function.

id119     finding minimum value of a function

   there are essentially two methods for identification of minimum value
   for a function. we will explore both these methods in this section.
   this will pave the way for us to understand id119. let us
   consider the following function and identify the minimum values or
   minima for the function.
 f(x) : x^{4}-10x^{2}+9   f(x) : x^{4}-10x^{2}+9

   this is the plot for the above function with different values of x.

   [19]picture1 picture1

   as you must have noticed, point a and c are the bottom points or
   minimum values on this curve. now we want to find the values of point a
   and c through two different methods i.e. logical and numeric/iterative
   solutions.

method-1: id128

   the first method is a logical method which requires solving an
   equation. we can find the minimum or maximum values for this function
   at the points where the gradient or the slope of the function becomes
   zero similar to the bottom of the bowl. mathematically, this expression
   is obtained by equating the first differential of the function to zero
   as shown below:
 \frac{d}{dx}f(x)= 4 x^{3}-20x = 0   \frac{d}{dx}f(x)= 4 x^{3}-20x = 0

   if we will solve this equation we will get      5 as the value of x where
   this function has the minimum values.
 minima f(x) : x = \pm \sqrt{5} =\pm 2.24   minima f(x) : x = \pm \sqrt{5} =\pm
2.24

   this makes sense since even if we look at the plot the minimum value
   for x is slightly away from x=   2.

method-2: iterative calculation

   it is great to solve the equation with the first method but
   unfortunately for quite a lot of complicated functions, it is not
   possible to solve equations the way we do in method-1. hence we need to
   identify some numeric methods to solve such equations. numeric methods
   to solve an equation require iterative calculation. it is similar to
   rolling a ball downwards, and measuring its position with each
   calculation till it reaches the bottom. the idea is to measure the
   speed of the ball based on the gradient or the slope of the curve. this
   is precisely how id119 optimization works.

   [20]picture2 picture2

   the first thing you must have noticed is that this function has 2
   minimum values. hence, initial positions of rolling the ball will take
   you to different values (    5 or -   5). this is a slight problem but
   luckily for many machine learning problems we just have one minimum
   value. this special property of a function is referred to as convexity.
   in other words, convexity is the property of a bowl to have just
   one bottom.

   moreover, to roll the ball we need to know the gradient or slope of the
   function (bowl) at different points. but how does one identify
   gradients of the curve at different points? we will learn this in the
   next segment by solving an example for id75.

id75     id168 & id119

   there are several business problems that use id75 to
   estimate values. for instance, if we have data for 100 professionals
   about their salaries and years of experiences we can fit a curve on
   this data and identify patterns between these two variables. using the
   data we are trying to estimate an equation of this nature:
 \hat{salary}= 5000\times (years\ of\ experience)+20000   \hat{salary}= 5000\tim
es (years\ of\ experience)+20000

   now, if some professional tells you she has 10 years of experience. you
   could easily estimate her salary as 70,000 (5000  10+20000). the most
   important factors here are the values of the coefficients for the line
   (i.e. m = 5000 & c=20000). we can estimate these coefficients with the
   data through either the ordinary least square (ols) method or gradient
   descent optimization. this example is quite simple but imagine if you
   had 8000 more variables in addition to years of experience that   s when
   you need machine learning and id119.

   let me tell you upfront that id119 is not the best way to
   solve a traditional id75 problem with fewer predictor
   variables. id75 models could be solved for their
   coefficients more efficiently through the ordinary least square (ols)
   method. ols is kind of like method-1 in the previous section i.e. more
   straightforward. id119, on the other hand, is like method-2
   and requires iterative calculation.

   despite inefficiency, traditional id75 offers an easier
   way for us to understand id119. more importantly,
   non-traditional regression in machine learning with thousands of
   predictor variables involves id75 with id173
   (i.e. ridge regression and lasso). these id75 models with
   id173 coefficient are efficiently solved using gradient
   descent method.

   ok so now let   s move to our example with simulated data for x and y
   (similar to years of experience and salary). consider this data set
   with different values of x and y. we will try to find the equation of
   the orange line through both ols and id119 methods.[21]
   rplot03 rplot03

method-1: solving id75 by ols

   we are interested in finding an equation for the line through linear
   regression.
 \hat{y}= m\times x+c   \hat{y}= m\times x+c

   here, m and c are constants or coefficients of a regression equation.

   as discussed earlier, we can easily solve this linear equation with
   ols. using r   s lm function we get this equation of the line for our
   data (m = 1.013 & c=1.003). lm function uses matrix inversion. [22]r
   code for id75.
 \hat{y} = 1.013 \times x+1.003   \hat{y} = 1.013 \times x+1.003

method-2: solving id75 by id119

   now, let   s try to find the values of coefficients (m & c) using
   id119. for this first, we need to identify the error or the
   id168 for the id75 model. remember the loss
   function will determine the shape of the bowl on which we are rolling
   the ball. the generalized equation of id168 for any problem is:
 loss\ function\ (lf)=\frac{1}{n} \sum (y-\hat{y})^{2}   loss\ function\ (lf)=\f
rac{1}{n} \sum (y-\hat{y})^{2}

   this is the original value of y minus expected value. for the linear
   regression problem, the expected value is the equation of the line,
   therefore:
 loss\ function\ (lf)=\frac{1}{n} \sum (y-(mx+c))^{2}   loss\ function\ (lf)=\fr
ac{1}{n} \sum (y-(mx+c))^{2}

   if we plot id168 (z-axis) with respect to the coefficients of
   id75 (m and c) the 3d plot looks like this.

   [23]picture1 picture1

   alright, this looks like a bowl. now, we just need to roll down the
   ball to identify the minimum value of id168 by adjusting the
   coefficients (m and c). id119 reaches the bottom of the
   curve by iterative calculation of both m & c. for this, we need to
   figure out the way to roll the ball down the slope at each step. keep
   the following analogy between gravity and id119
   which minimizes id168 by adjusting for m & c.
   gravity analogy id119
   potential energy id168
   objective: minimize potential energy objective: minimize id168
   horizontal plan (x & y axes) regression coefficient (m & c)
   modify x & y to achieve the objective modify m & c to achieve the
   objective
   the shape of the bowl relationship btw id168, m, & c
   bottom of the bowl minimum value of loss (error) on lf

   firstly, id119 requires the direction of the downward slope
   at each point on the id168 with respect to both m & c.
   secondly, it needs to control the speed at which the ball will roll.
   this entire thing is captured in this equation for m:
 m_{n+1}=m_{n}-\alpha \frac{\partial}{\partial m_{n}} lf (m_{n})  m_{n+1}=m_{n}-
\alpha \frac{\partial}{\partial m_{n}} lf (m_{n})

   here, the new position of m i.e. (m[n+1]) is decided by the previous
   position of m i.e. (m[n]) by rolling the ball down the slope (
   \frac{\partial}{\partial m_{n}} lf (m_{n}) \frac{\partial}{\partial
   m_{n}} lf (m_{n}) ) . the partial differentiation term determines the
   gradient or slope of the curve. additionally,    is learning rate which
   determines the speed at which the ball will roll.
 c_{n+1}=c_{n}-\alpha \frac{\partial}{\partial c_{n}} lf (c_{n})  c_{n+1}=c_{n}-
\alpha \frac{\partial}{\partial c_{n}} lf (c_{n})

   similarly, we can iteratively estimate the value of the other
   coefficient of the id75 model i.e c. the best fit value
   for c and m is when the slope becomes flat or the ball gets to the
   bottom of the bowl.

   now we just need to find the gradient term and supplement them in the
   above equations. the gradient term for m is:
 \frac{\partial}{\partial m}lf = \frac{2}{n} \sum (y-(mx+c)) \times x   \frac{\p
artial}{\partial m}lf = \frac{2}{n} \sum (y-(mx+c)) \times x

   similarly, the gradient term for c is:
 \frac{\partial}{\partial c}lf = \frac{2}{n} \sum (y-(mx+c))  \frac{\partial}{\p
artial c}lf = \frac{2}{n} \sum (y-(mx+c))

   finally, we choose the constant value for learning rate (  ) as 0.01. we
   are now ready to run the id119 optimization to find values
   of m & c through iterative calculation. this [24]r code for gradient
   descent captures all the equations discussed above and does the
   iterative calculation for m & c to minimize id168. gradient
   descent optimization gives the same value for the coefficients as ols
   i.e. m = 1.013 & c=1.003. please run the code and check for yourself.

   the following heat map points to the value of id168 for
   different values of m & c. as expected, this points to the minimum
   value at m = 1.013 & c=1.003.[25] rplot01 rplot01

sign-off note

   so gravity and id119 have so many similarities. next time
   when you see something falling, do think of id119. a
   question you may want to ponder: does nature do so many iterative
   calculations to make things fall?
     * [26]share
     *

     * [27]twitter
     * [28]facebook
     *
     * [29]email
     * [30]linkedin
     *
     *

related

   posted in [31]id119, [32]machine learning and artificial
   intelligence |
      [33]what is the right sample size for your analysis?
   [34]in conversation with naeem siddiqi     author credit risk scorecards
   and credit scoring guru   

19 thoughts on    intuitive machine learning : id119 simplified   

    1.
   nigel clark says:
       [35]april 5, 2016 at 12:05 am
       fantastic post! congrats roopam!
       thanx for taking time to explain this
       [36]reply
    2.
   younes says:
       [37]april 6, 2016 at 6:05 pm
       analogy between nature and science is the key of learning .
       [38]reply
    3.
   pavan m says:
       [39]april 7, 2016 at 4:59 pm
       earlier i found little difficulty to understand intuition behind
       the gradient decent. it   s explained in very simple way..thanks
       roopam.
       if you get a chance, kindly provide intuition behind local optima
       and global optima and how to identify if we have reached global
       optima or local optima..thanks in advance
       -pavan
       [40]reply
          +
        roopam upadhyay says:
            [41]april 14, 2016 at 7:49 pm
            thanks pavan, i appreciate that.
            to understand global optima i.e. maxima / minima let us
            consider the function that we have discussed in this post
            null
            for this function point b is a local maxima. how do we know
            that? because between points a and c that is the maximum value
            for the function. however, unless we go beyond these points
            there is no way for us to identify that it is a local maxima.
            for this function a global maxima is infinity since the value
            of function will continue to rise till eternity with an
            incremental value of x. hence, there is no way for us to
            estimate that with both the methods we have used to find
            optimal value of the function.
            method-1 uses id128 to identify all the points
            where the slope of the function becomes zero. however, for a
            continuously increasing / decreasing function we won   t find
            the maxima or minima with this method since it is plus / minus
            infinity.
            method-2 and id119 method have a bigger challenge
            since the rolling ball will just stop at any point where it
            finds a flat surface. one of the practical solutions for this
            is to make the ball roll from different initial points and
            hope that you have arrived at the minima. luckily for the
            optimization problems with squared error function there is
            just one minima.
            [42]reply
    4.
   jay sorenson says:
       [43]april 8, 2016 at 2:32 pm
       hi:
       id119 sounds like a variation of a id135
       algorithm.
       regards,
       jay
       [44]reply
          +
        roopam upadhyay says:
            [45]april 15, 2016 at 11:44 am
            that   s true jay, id119 is a minimization problem
            subjected to model constraints hence it is similar to linear
            programming (lp). for fewer predictor variables you could also
            use excel solver (optimization) to build linear or logistic
            regression models. however for a large number of predictor
            variables, common in machine learning problems, gradient
            descent works better.
            [46]reply
               o
             ronny says:
                 [47]september 25, 2017 at 12:06 pm
                 hi dear roopam
                 great explanation, huge thanks
                 but i have one query, so after so many iterations for
                 finding lowest point of the id119
                 how do you understand that lowest point is reached.
                 i.e 1. m=5, c=4
                 2. m=4.9 c=3.9
                 3. m=4.7 c=3.7
                    
                    
                    
                 m=0.5 c=0.6
                 when should i stop understanding that optimal minimum is
                 reached
                 what is the signal or value that gives me the lowest
                 point.
                 [48]reply
                    #
                  roopam upadhyay says:
                      [49]september 25, 2017 at 3:02 pm
                      at the optimum or minimum value for the loss
                      function, the gradients d(lf)/d(m) and d(lf)/d(c)
                      become zero. this also indicate that your ball has
                      reached the bottom of the bowl since the slope is
                      zero. in practice, you could check if the gradients
                      are less than a certain threshold say 10^-6 before
                      stopping the algorithm.
                      [50]reply
                         @
                       aru says:
                           [51]october 13, 2017 at 5:16 pm
                           roopam,
                           apologize if my questions sounds naive. the
                           id119 algorithm finds the
                           coefficients (m and c here) that provides the
                           local optima. for this purpose, we start with
                           an arbitrary coefficient values and reduce them
                           till the lowest point is reached. will the
                           coefficient values get close to zero always to
                           achieve local optima? if so, why don   t we start
                           the id119 search with m=0 and c=0
                           and increase gradually?
                         @
                       roopam upadhyay says:
                           [52]october 16, 2017 at 10:20 pm
                           since we are working with a convex loss
                           function, id119, in this case, will
                           always find global optima. remember, a convex
                           id168 is shaped like a bowl and has
                           just one minimum value.
                           the coefficient values don   t necessarily get
                           close to zero     there is no such condition and
                           the coefficient values depends on the dataset.
                           one of the approaches is to normalise the
                           dataset to keep the coefficient values in a
                           relatively narrow range but again no reason to
                           believe that they will converge closer to zero.
    5.
   muhammad atif says:
       [53]april 13, 2016 at 1:04 pm
       excellent and very easy to understand explanation. thank you.
       [54]reply
    6.
   ajay says:
       [55]july 21, 2017 at 5:25 am
                          
       [56]reply
    7.
   ajay says:
       [57]july 21, 2017 at 5:26 am
                                                     
       [58]reply
    8.
   shreya says:
       [59]october 8, 2017 at 12:57 am
       hello. thank you very much for this post. i have a hard time
       understanding mathematical concepts, and this helped me a lot in
       studying for my machine learning exam. thanks!     
       [60]reply
    9.
   mahmood abd elrazek says:
       [61]february 24, 2018 at 7:22 pm
       but i have a question    where is the minus    that is with -mx because
       the equation is    y-mx-c   
       please answer it <3 ?
       [62]reply
          +
        roopam upadhyay says:
            [63]february 25, 2018 at 9:09 am
            not sure if i have understood your question completely. are
            you talking about this equation?
            this one has the same form as you expected, notice the
            brackets.
            [64]reply
   10.
   narwadeshwar says:
       [65]may 7, 2018 at 6:12 pm
       thankyou roopam this is very helpful
       [66]reply
   11.
   dr. abdul majid sandhu says:
       [67]may 27, 2018 at 10:01 am
       dear roopam, thanks for an easy explanation. could you tell me the
       difference between this method and    steepest descent method    of
       geometry optimization?
       regards,
       [68]reply
          +
        roopam upadhyay says:
            [69]may 27, 2018 at 10:39 am
            thanks. to answer your question, the method of steepest
            descent is also called the id119 method. in
            computer science and machine learning, you will almost always
            refer to the method as id119.
            [70]reply

leave a comment [71]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   [ ] notify me of follow-up comments by email.

   [ ] notify me of new posts by email.
   post comment

   this site uses akismet to reduce spam. [72]learn how your comment data
   is processed.

subscribe to blog

   provide your email address to receive notifications of new posts

   email address ____________________

   (button) subscribe

   ____________________

must read

   career in data science - interview preparation - best practices

   free books - machine learning - data science - artificial intelligence

case-studies

   - marketing campaign management - revenue estimation & optimization

   customer segmentation - cluster analysis - segment wise business
   strategy

   - risk management - credit scorecards

   - sales forecasting - time series models

credit

   i must thank my wife, swati patankar, for being the editor of this
   blog.

pages

     * [73]blog-navigation
     * [74]art
     * [75]about
     * [76]contact

      roopam upadhyay

     * [77]blog-navigation
     * [78]art
     * [79]about
     * [80]contact

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________ loading send email [81]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

references

   visible links
   1. http://ucanalytics.com/blogs/feed/
   2. http://ucanalytics.com/blogs/comments/feed/
   3. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/feed/
   4. http://ucanalytics.com/blogs/wp-json/oembed/1.0/embed?url=http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/
   5. http://ucanalytics.com/blogs/wp-json/oembed/1.0/embed?url=http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/&format=xml
   6. http://www.linkedin.com/in/roopamup
   7. http://ucanalytics.com/blogs
   8. http://ucanalytics.com/blogs
   9. http://ucanalytics.com/blogs/navigation/
  10. http://ucanalytics.com/blogs/art-gallery/
  11. http://ucanalytics.com/blogs/about-me/
  12. http://ucanalytics.com/blogs/contact/
  13. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/
  14. http://ucanalytics.com/blogs/author/roopam/
  15. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comments
  16. https://i1.wp.com/ucanalytics.com/blogs/wp-content/uploads/2016/04/gravity-and-gradient-descent.jpg
  17. https://i0.wp.com/ucanalytics.com/blogs/wp-content/uploads/2016/03/picture1.jpg
  18. https://i0.wp.com/ucanalytics.com/blogs/wp-content/uploads/2016/03/bowl-and-ball.jpg
  19. https://i1.wp.com/ucanalytics.com/blogs/wp-content/uploads/2016/03/picture1-2-e1459331131958.jpg
  20. https://i0.wp.com/ucanalytics.com/blogs/wp-content/uploads/2016/03/picture2-e1459331186740.jpg
  21. https://i2.wp.com/ucanalytics.com/blogs/wp-content/uploads/2016/03/rplot03.jpeg
  22. http://ucanalytics.com/blogs/wp-content/uploads/2016/04/linear-regression-code-lm.txt
  23. https://i1.wp.com/ucanalytics.com/blogs/wp-content/uploads/2016/03/picture1-1.jpg
  24. http://ucanalytics.com/blogs/wp-content/uploads/2016/04/gradient-descent.txt
  25. https://i1.wp.com/ucanalytics.com/blogs/wp-content/uploads/2016/03/rplot01.png
  26. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/
  27. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/?share=twitter
  28. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/?share=facebook
  29. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/?share=email
  30. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/?share=linkedin
  31. http://ucanalytics.com/blogs/category/gradient-descent/
  32. http://ucanalytics.com/blogs/category/machine-learning-and-artificial-intelligence/
  33. http://ucanalytics.com/blogs/right-sample-size-analysis/
  34. http://ucanalytics.com/blogs/conversation-naeem-siddiqi-author-credit-risk-scorecards-credit-scoring-guru/
  35. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1458
  36. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1458
  37. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1459
  38. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1459
  39. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1460
  40. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1460
  41. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1466
  42. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1466
  43. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1461
  44. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1461
  45. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1467
  46. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1467
  47. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-2587
  48. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-2587
  49. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-2590
  50. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-2590
  51. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-2924
  52. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-2966
  53. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1464
  54. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-1464
  55. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-2056
  56. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-2056
  57. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-2057
  58. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-2057
  59. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-2794
  60. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-2794
  61. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-5264
  62. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-5264
  63. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-5280
  64. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-5280
  65. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-7623
  66. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-7623
  67. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-8603
  68. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-8603
  69. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-8605
  70. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#comment-8605
  71. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#respond
  72. https://akismet.com/privacy/
  73. http://ucanalytics.com/blogs/navigation/
  74. http://ucanalytics.com/blogs/art-gallery/
  75. http://ucanalytics.com/blogs/about-me/
  76. http://ucanalytics.com/blogs/contact/
  77. http://ucanalytics.com/blogs/navigation/
  78. http://ucanalytics.com/blogs/art-gallery/
  79. http://ucanalytics.com/blogs/about-me/
  80. http://ucanalytics.com/blogs/contact/
  81. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/#cancel

   hidden links:
  83. http://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/
  84. http://ucanalytics.com/blogs/category/data-science-career/
  85. http://ucanalytics.com/blogs/category/python-and-r/
  86. http://ucanalytics.com/blogs/category/marketing-analytics/retail-case-study-example/
  87. http://ucanalytics.com/blogs/category/marketing-analytics/telecom-case-study-example/
  88. http://ucanalytics.com/blogs/category/risk-analytics/banking-risk-case-study-example/
  89. http://ucanalytics.com/blogs/category/manufacturing-case-study-example/
