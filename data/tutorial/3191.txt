   #[1]the morning paper    feed [2]the morning paper    comments feed
   [3]the morning paper    a miscellany of fun deep learning papers
   comments feed [4]recurrent neural network models [5]ethically aligned
   design [6]alternate [7]alternate [8]the morning paper [9]wordpress.com

   [10]skip to content

   [11]the morning paper

an interesting/influential/important paper from the world of cs every weekday
morning, as selected by adrian colyer

     * [12]home
     * [13]about
     * [14]infoq qr editions
     * [15]subscribe
     * [16]privacy

a miscellany of fun deep learning papers

   march 24, 2017
   tags: [17]deep learning

   to round out the week, i thought i   d take a selection of fun papers
   from the    more papers from 2016    section of [18]top 100 awesome deep
   learning papers list.
     * [19]colorful image colorization, zhang et al., 2016
     * [20]texture networks: feed-forward synthesis of textures and
       stylized images
     * [21]generative visual manipulation on the natural image manifold,
       zhu et al., 2016
     * [22]wavenet: a generative model for raw audio, van den oord et al.,
       2016
     * [23]google   s id4 system: bridging the gap
       between human and machine translation, wu et al., 2016

   the texture networks paper we   ve covered before, so the link in the
   above list is to the morning paper write-up (but i felt like it
   belonged in this group nevertheless).

colorful image colorization

     given a grayscale photograph as input, this paper attacks the
     problem of hallucinating a plausible color version of the
     photograph.

   how is this possible? well, we   ve seen that networks can learn what
   various parts of the image represent. if you see enough images you can
   learn that grass is (usually) green, the sky is (sometimes!) blue, and
   ladybirds are red. the network doesn   t have to recover the actual
   ground truth colour, just a plausible colouring.

     therefore, our task becomes much more achievable: to model enough of
     the statistical dependencies between the semantics and the textures
     of grayscale images and their color versions in order to produce
     visually compelling results.

   results like this:

   training data for the colourisation task is plentiful     pretty much any
   colour photo will do. the tricky part is finding a good id168    
   as we   ll see soon, many id168s produce images that look
   desaturated, whereas we want vibrant realistic images.

   the network is based on image data using the [24]cie lab colourspace.
   grayscale images have only the lightness, l, channel, and the goal is
   to predict the a (green-red) and b (blue-yellow) colour channels. the
   overall network architecture should look familiar by now, indeed so
   familiar that supplementary details are pushed to an [25]accompanying
   website.

   (that website page is well worth checking out by the way, it even
   includes a link to a demo site on algorithmia where you can [26]try the
   system out for yourself on your own images).

   colour prediction is inherently multi-modal, objects can take on
   several plausible colourings. apples for example may be red, green, or
   yellow, but are unlikely to be blue or orange. to model this, the
   prediction is a distribution of possible colours for each pixel. a
   typical objective function might use e.g. euclidean loss between
   predicted and ground truth colours.

     however, this loss is not robust to the inherent ambiguity and
     multimodal nature of the colorization problem. if an object can take
     on a set of distinct ab values, the optimal solution to the
     euclidean loss will be the mean of the set. in color prediction,
     this averaging effect favors grayish, desaturated results.
     additionally, if the set of plausible colorizations is non-convex,
     the solution will in fact be out of the set, giving implausible
     results.

   what can we do instead? the ab output space is divided into bins with
   grid size 10, and the top q = 313 in-gamut (within the range of colours
   we want to use) are kept:

   the network learns a mapping to a id203 distribution over these q
   colours (a q-dimensional vector). the ground truth colouring is also
   translated into a q-dimensional vector, and the two are compared using
   a multinomial cross id178 loss. notably this includes a weighting
   term to rebalance loss based on colour-class rarity.

     the distribution of ab values in natural images is strongly biased
     towards values with low ab values, due to the appearance of
     backgrounds such as clouds, pavement, dirt, and walls. figure 3(b)
     [below] shows the empirical distribution of pixels in ab space,
     gathered from 1.3m training images in id163. observethat the
     number of pixels in natural images at desaturated values are orders
     of magnitude higher than for saturated values. without accounting
     for this, the id168 is dominated by desaturated ab values.

   the final predicted distribution then needs to be mapped to a point
   estimated in ab space. taking the mode of the predicted distribution
   leads to vibrant but sometimes spatially inconsistent results (see rh
   column below). taking the mean brings back another form of the
   desaturation problem (see lh column below).

     to try to get the best of both worlds, we interpolate by
     re-adjusting the temperature t of the softmax distribution, and
     taking the mean of the result. we draw inspiration from the
     simulated annealing technique, and thus refer to the operation as
     taking the annealed-mean of the distribution.

   here are some more colourings from a network trained on id163, which
   were rated by amazon mechanical turk participants to see how lifelike
   they are.

   and now for my very favourite part of the paper:

     since our model was trained using    fake    grayscale images generated
     by stripping ab channels from color photos, we also ran our method
     on real legacy black and white photographs, as shown in figure 8
     (additional results can be viewed on our project webpage). one can
     see that our model is still able to produce good colorizations, even
     though the low-level image statistics of the legacy photographs are
     quite different from those of the modern-day photos on which it was
     trained.

   aren   t they fabulous! especially the migrant mother colouring.

   the representations learned by the network also proved useful for
   object classification, detection, and segmentation tasks.

generative visual manipulation on the natural image manifold

   so we   ve just seen that neural networks can help us with our colouring.
   but what about those of us that are more artistically challenged and
   have a few wee issues making realistic looking drawings (or alterations
   to existing drawings) in the first place? in turns out that
   [27]generative adversarial neural networks can help.

   it   s a grand sounding paper title, but you can think of it as    fiddling
   about with images while ensuring they still look natural.    i guess that
   wouldn   t look quite so good in the conference proceedings ;).

     today, visual communication is sadly one-sided. we all perceive
     information in the visual form (through photographs, paintings,
     sculpture, etc), but only a chosen few are talented enough to
     effectively express themselves visually    one reason is the lack of
        safety wheels    in image editing: any less-than-perfect edit
     immediately makes the image look completely unrealistic. to put
     another way, classic visual manipulation paradigm does not prevent
     the user from    falling off    the manifold of natural images.

   as we know, [28]gans can be trained to learn effective representations
   of natural looking images (   the manifold of natural images   ). so let   s
   do that, but then instead of using the trained gan to generate images,
   use it as a constraint on the output of various image manipulation
   operations, to make sure the results lie on the learned manifold at all
   times. the result is an interactive tool that helps you make realistic
   looking alterations to existing images. it helps to see the tool in
   action, you can see a [29]video here. the authors also demonstrate
      generative transformation    of one image to look more like another, and
   my favourite, creating a new image from scratch based on a user   s
   sketch.

   the intuition for using gans to learn manifold approximations is that
   they have been shown to produce high-quality samples, and that
   euclidean distance in the latent space often corresponds to a
   perceptually meaningful visual similarity. this means we can also
   perform interpolation between points in the latent space.

   here   s what happens when the latent vector is updated based on user
   edits (top row, adding black colour and changing the shape):

   in the interactive tool, each update step takes about 50-100ms, working
   only on the mapped representation of the original image. when the user
   is done, the generated image captures roughly the desired change, but
   the quality is degraded as compared to the original image.

     to address this issue, we develop a dense correspondence algorithm
     to estimate both the geometric and color changes induced by the
     editing process.

   this motion and colour flow algorithm is used to estimate the colour
   and shape changes in the generated image sequence (as user editing
   progressed), and then transfer them back on top of the original photo
   to generate photo-realistic images.

   the user interface gives the user a colouring brush for changing the
   colour of regions, a sketching brush to outline shapes or add fine
   details, and a warping    brush    for more explicit shape modifications.

   here are some results from user edits:

   transformations between two images also take place in the gan-learned
   representation space and are mapped back in the same way:

   it   s also possible to use the brush tools to create an image from
   scratch, and then add more scribbles to refine the result. how good is
   this! :

wavenet: a generative model for raw audio

   enough with the images already! what about generating sound? how about
   text-to-speech sound generation yielding state of the art performance?
   hearing is believing, so check out these samples:
     * [30]wavenet speaking in us english
     * [31]wavenet trained without the accompanying text sequence, so that
       it has to make up what to say:
     * [32]wavenet trained on classical piano music (just the audio, no
       score!) and left to its own devices to generate whatever it wants
       to:

   (you can find more on the deepmind blog at
   [33]https://deepmind.com/blog/wavenet-generative-model-raw-audio/).

     we show that wavenets can generate raw speech signals with
     subjective naturalness never before reported in the field of
     text-to-speech (tts), as assessed by human raters.

   the architecture of wavenet is inspired by pixelid56 (see    [34]id56
   models for image generation    from a couple of weeks ago). the
   foundation is very simple     take a waveform \mathbf{x} with with t
   values, {x_1, ..., x_t} . and let the id203 of the x_t be
   conditioned on all of the values that precede it: p(x_t | x_1, ...,
   x_{t-1}) . now the joint id203 of the overall waveform is
   modelled by:

   p(\mathbf{x}) = \prod_{t=1}^{t} p(x_t | x_1, ..., x_{t-1})

   this can be modelled by a stack of convolutional layers.    by using
   causal convolutions, we make sure the model cannot violate the ordering
   in which we model the data       (the prediction at timestep t cannot
   depend on any of the future timestamps). this can be implemented by
   shifting the output of a normal convolution by one or more timesteps.

     at training time, the conditional predictions for all timesteps can
     be made in parallel because all timesteps of ground truth x are
     known. when generating with the model, the predictions are
     sequential: after each sample is predicted, it is fed back into the
     network to predict the next sample.

   causal convolutions need lots of layers to increase their receptive
   field. wavenet uses dilated convolutions to increase receptive fields
   by orders of magnitude, without greatly increasing computational cost.

     a dilated convolution is a convolution where the filter is applied
     over an area large than its length by skipping input values with a
     certain step.

   wavenet uses dilation doubling in every layer up to a limit of 512,
   before repeating (1,2,4,    , 512, 1,2,4,    , 512,    ).

   a straight softmax output layer would need 65,356 probabilities per
   timestep to model all possible values for raw audio stored as a
   sequence of 16-bit integer values. the data is quantized to 256
   possible values using a non-linear quantization scheme which was found
   to produce a significantly better reconstruction than a simple linear
   scheme:
   f(x_t) = sign(x_t) \frac{\ln(1 + \mu|x_t|)}{\ln(1 + \mu)}

   where -1 < x_t < 1 and \mu = 255 . the network uses both residual and
   parameterised skip connections throughout to speed up convergence. by
   conditioning the model on additional inputs, wavenet can be guided to
   produce audio with the required characteristics (e.g., a certain
   speaker   s voice). for tts, information about the text is fed as an
   extra input. > for the first experiment we looked at free-form speech
   generation (not conditioned on text). we used the english multi-speaker
   corpus from cstr voice cloning toolkit (vctk) (yamagishi, 2012) and
   conditioned wavenet only on the speaker. the conditioning was applied
   by feeding the speaker id to the model in the form of a one-hot vector.
   the dataset consisted of 44 hours of data from 109 different speakers.

   since it wasn   t conditioned on text, the model generates made-up but
   human language-like words in a smooth way (see second audio clip at the
   top of this section). it can model speech from any of the speakers by
   conditioning it on the one-hot encoding     thus the model is powerful
   enough to capture the characteristics of all 109 speakers from the
   dataset in a single model.

   the second experiment trained wavenet on the same single-speaker speech
   databases that google   s north american and mandarin chinese tts systems
   are built on. wavenet was conditioned on linguistic features derived
   from the input texts. in subjective paired comparison tests, wavenet
   beat the best baselines:

   wavenet also achieved the highest ever score in a mean opinion score
   test where users had to rate naturalness on a scale of 1-5 (scoring
   over 4 on average). (see the first speech sample at the top of this
   section).

   the third set of experiments trained wavenet on two music datasets (see
   the third speech sample at the top of this section).       the samples were
   often harmonic and aesthetically pleasing, even when produced by
   unconditional models.   

   from the [35]wavenet blog post on the deepmind site:

     wavenets open up a lot of possibilities for tts, music generation
     and audio modelling in general. the fact that directly generating
     timestep per timestep with deep neural networks works at all for
     16khz audio is really surprising, let alone that it outperforms
     state-of-the-art tts systems. we are excited to see what we can do
     with them next.

google   s id4 system: bridging the gap between human
and machine translation

   google   s id4 (gid4) system is an end-to-end
   learning system for automated translation. previous id4 systems
   suffered in one or more of three key areas: training and id136
   speed, coping with rare words, and sometimes failing to translate all
   of the words in a source sentence. gid4 is now in production at google,
   having handsomely beaten the phrase-based machine translation (pbmt)
   system used in production at google beforehand.

   understanding how it all fits together will draw upon many of the
   papers we   ve looked at so far. at the core it   s a sequence-to-sequence
   learning network with an encoder network, a decoder network, and an
   [36]attention network.

     the encoder transforms a source sentence into a list of vectors, one
     vector per input symbol. given this list of vectors, the decoder
     produces one symbol at a time, until the special end-of-sentence
     symbol (eos) is produced. the encoder and decoder are connected
     through an attention module which allows the decoder to focus on
     different regions of the source sentence during the course of
     decoding.

   the decoder is a combination of an id56 network and a softmax layer.
   deeper models give better accuracy, but the team found that lstm layers
   worked well up to 4 layers, barely with 6 layers, and very poorly
   beyond 8 layers. what to do? we learned the answer earlier this week,
   add residual connections:

   since in translation words in the source sentence may appear anywhere
   in the output sentence, the encoder network uses a bi-directional id56
   for the encoder. only the bottom layer is bi-direction     one lstm layer
   processes the sentence left-to-right, while its twin processes the
   sentence right-to-left.

   the encoder and decoder networks are placed on multiple gpus, with each
   layer running on a different gpu. as well as using multiple gpus, to
   get id136 time down quantized id136 involving reduce precision
   arithmetic is also used.

     one of the main challenges in deploying our neural machine
     translation model to our interactive production translation service
     is that it is computationally intensive at id136, making low
     latency translation difficult, and high volume deployment
     computationally expensive. quantized id136 using reduced
     precision arithmetic is one technique that can significantly reduce
     the cost of id136 for these models, often providing efficiency
     improvements on the same computational devices.

   the model is trained using full-precision floats, it is only for
   production id136 that approximation is used. here are the decoding
   times for 6003 english-french sentences across cpu, gpu, and google   s
   tensor processing unit (tpu) respectively:

   firstly, note that the tpu beats the cpu and gpu hands-down. the cpu
   beats the gpu because    our current decoder implementation is not fully
   utilizing the computation capacities that a gpu can theoretically offer
   during id136.   

dealing with out of vocabulary words

     id4 models often operate with fixed word
     vocabularies even though translation is fundamentally an open
     vocabulary problem (names, numbers, dates etc.)    our most successful
     approach [   ] adopts the wordpiece model (wpm) implementation
     initially developed to solve a japanese/korean segmentation problem
     for the google id103 system. this approach is
     completely data-driven and guaranteed to generate a deterministic
     segmentation for any possible sequence of characters.

   for example,    jet makers feud over seat width with big orders at stake   
   turns into the word pieces:    _j et _makers _fe ud _over _seat _width
   _with _big _orders _at _stake.    the words    jet    and    feud    are both
   broken into two word pieces.

     given a training corpus and a number of desired tokens d, the
     optimization problem is to select d wordpieces such that the
     resulting corpus is minimal in the number of wordpieces when
     segmented according to the chosen wordpiece model.

overall model performance.

   the following chart shows side-by-side scores for translations made by
   the previous production system (pbmt), the new gid4 system, and humans
   fluent in both languages.

     side-by-side scores range from 0 to 6, with a score of 0 meaning
        completely nonsense translation   , and a score of 6 meaning    perfect
     translation: the meaning of the translation is completely consistent
     with the source, and the grammar is correct   . a translation is given
     a score of 4 if    the sentence retains most of the meaning of the
     source sentence, but may have some grammar mistakes   , and a
     translation is given a score of 2 if    the sentence preserves some of
     the meaning of the source sentence but misses significant parts   .
     these scores are generated by human raters who are fluent in both
     languages and hence often capture translation quality better than
     id7 scores.

share this:

     * [37]twitter
     * [38]linkedin
     * [39]email
     * [40]print
     *

like this:

   like loading...

related

   from     [41]machine learning, [42]uncategorized
   [43]    recurrent neural network models
   [44]ethically aligned design    
   2 comments [45]leave one    
    1. andre [46]permalink
       march 24, 2017 6:09 pm
       the papers here are very interesting. adrian, i must say i am
       astounded by these daily reviews. just found this site and
       subscribed.
       [47]reply
    2. atcold [48]permalink
       march 26, 2017 5:51 pm
       in    colorful image colorization   , figure 2, they mention that they
       haven   t used    pool    layers but    spatial down/up-sampling   , which is
       exactly the same    i   m not sure why is this even relevant; perhaps
       i   m missing something.
       [49]reply

leave a reply [50]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [51]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [52]log out /
   [53]change )
   google photo

   you are commenting using your google account. ( [54]log out /
   [55]change )
   twitter picture

   you are commenting using your twitter account. ( [56]log out /
   [57]change )
   facebook photo

   you are commenting using your facebook account. ( [58]log out /
   [59]change )
   [60]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

   this site uses akismet to reduce spam. [61]learn how your comment data
   is processed.
     * subscribe

                        [62][mail-new-icon.png?w=600]
      never miss an issue! the morning paper delivered straight to your
                                   inbox.
     * search
       ____________________
     * archives archives [select month________]
     * most read in the last few days
          + [63]ginseng: keeping secrets in registers when you distrust
            the operating system
          + [64]amazon aurora: design considerations for high throughput
            cloud-native id208
          + [65]establishing software root of trust unconditionally
          + [66]amazon aurora: on avoiding distributed consensus for i/os,
            commits, and membership changes
          + [67]calvin: fast distributed transactions for partitioned
            database systems
          + [68]the amazing power of word vectors
          + [69]on the criteria to be used in decomposing systems into
            modules
          + [70]neural ordinary differential equations
          + [71]applying the universal scalability law to organisations
          + [72]programming paradigms for dummies: what every programmer
            should know
     * rss
          + [73]rss - posts
          + [74]rss - comments
     * live on twitter[75]my tweets

   [76]blog at wordpress.com.

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [77]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [78]likes-master

   %d bloggers like this:

references

   visible links
   1. https://blog.acolyer.org/feed/
   2. https://blog.acolyer.org/comments/feed/
   3. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/feed/
   4. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/
   5. https://blog.acolyer.org/2017/03/27/ethically-aligned-design/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/&for=wpcom-auto-discovery
   8. https://blog.acolyer.org/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/#content
  11. https://blog.acolyer.org/
  12. https://blog.acolyer.org/
  13. https://blog.acolyer.org/about/
  14. https://blog.acolyer.org/infoq-quarterly-review-editions/
  15. https://blog.acolyer.org/email-subs/
  16. https://blog.acolyer.org/privacy/
  17. https://blog.acolyer.org/tag/deep-learning/
  18. https://github.com/terryum/awesome-deep-learning-papers
  19. https://arxiv.org/pdf/1603.08511
  20. https://blog.acolyer.org/2016/09/23/texture-networks-feed-forward-synthesis-of-textures-and-stylized-images/
  21. https://arxiv.org/pdf/1609.03552
  22. https://arxiv.org/pdf/1609.03499v2
  23. https://arxiv.org/pdf/1609.08144
  24. https://en.m.wikipedia.org/wiki/lab_color_space
  25. http://richzhang.github.io/colorization/
  26. http://demos.algorithmia.com/colorize-photos/
  27. https://adriancolyer.files.wordpress.com/2017/03/colouring-fig-8.jpeg
  28. https://adriancolyer.files.wordpress.com/2017/03/colouring-fig-8.jpeg
  29. https://people.eecs.berkeley.edu/~junyanz/projects/gvm/
  30. https://storage.googleapis.com/deepmind-media/pixie/us-english/parametric-1.wav
  31. https://storage.googleapis.com/deepmind-media/pixie/knowing-what-to-say/first-list/speaker-1.wav
  32. https://storage.googleapis.com/deepmind-media/pixie/making-music/sample_1.wav
  33. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  34. https://blog.acolyer.org/2017/03/03/id56-models-for-image-generation/
  35. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  36. https://blog.acolyer.org/2016/03/09/neural-turing-machines/
  37. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/?share=twitter
  38. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/?share=linkedin
  39. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/?share=email
  40. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/#print
  41. https://blog.acolyer.org/category/machine-learning/
  42. https://blog.acolyer.org/category/uncategorized/
  43. https://blog.acolyer.org/2017/03/23/recurrent-neural-network-models/
  44. https://blog.acolyer.org/2017/03/27/ethically-aligned-design/
  45. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/#respond
  46. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/#comment-10680
  47. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/?replytocom=10680#respond
  48. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/#comment-10697
  49. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/?replytocom=10697#respond
  50. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/#respond
  51. https://gravatar.com/site/signup/
  52. javascript:highlandercomments.doexternallogout( 'wordpress' );
  53. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/
  54. javascript:highlandercomments.doexternallogout( 'googleplus' );
  55. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/
  56. javascript:highlandercomments.doexternallogout( 'twitter' );
  57. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/
  58. javascript:highlandercomments.doexternallogout( 'facebook' );
  59. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/
  60. javascript:highlandercomments.cancelexternalwindow();
  61. https://akismet.com/privacy/
  62. http://eepurl.com/bbzpm9
  63. https://blog.acolyer.org/2019/04/05/ginseng-keeping-secrets-in-registers-when-you-distrust-the-operating-system/
  64. https://blog.acolyer.org/2019/03/25/amazon-aurora-design-considerations-for-high-throughput-cloud-native-relational-databases/
  65. https://blog.acolyer.org/2019/04/03/establishing-software-root-of-trust-unconditionally/
  66. https://blog.acolyer.org/2019/03/27/amazon-aurora-on-avoiding-distributed-consensus-for-i-os-commits-and-membership-changes/
  67. https://blog.acolyer.org/2019/03/29/calvin-fast-distributed-transactions-for-partitioned-database-systems/
  68. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
  69. https://blog.acolyer.org/2016/09/05/on-the-criteria-to-be-used-in-decomposing-systems-into-modules/
  70. https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/
  71. https://blog.acolyer.org/2015/04/29/applying-the-universal-scalability-law-to-organisations/
  72. https://blog.acolyer.org/2019/01/25/programming-paradigms-for-dummies-what-every-programmer-should-know/
  73. https://blog.acolyer.org/feed/
  74. https://blog.acolyer.org/comments/feed/
  75. https://twitter.com/519408925733425153
  76. https://wordpress.com/?ref=footer_blog
  77. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/#cancel
  78. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  80. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/#comment-form-guest
  81. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/#comment-form-load-service:wordpress.com
  82. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/#comment-form-load-service:twitter
  83. https://blog.acolyer.org/2017/03/24/a-miscellany-of-fun-deep-learning-papers/#comment-form-load-service:facebook
