   #[1]openai

     * [2]about
     * [3]progress
     * [4]resources
     * [5]blog

   (button)

   (button)

     * [6]about
         ______________________________________________________________

     * [7]progress
         ______________________________________________________________

     * [8]resources
         ______________________________________________________________

     * [9]blog
         ______________________________________________________________

     * [10]jobs
     __________________________________________________________________

   february 24, 2017     7 minute read

attacking machine learning with adversarial examples

   [11]adversarial examples are inputs to machine learning models that an
   attacker has intentionally designed to cause the model to make a
   mistake; they're like optical illusions for machines. in this post
   we'll show how adversarial examples work across different mediums, and
   will discuss why securing systems against them can be difficult.

   at openai, we think adversarial examples are a good aspect of security
   to work on because they represent a concrete problem in ai safety that
   can be addressed in the short term, and because fixing them is
   difficult enough that it requires a serious research effort. (though
   we'll need to explore many aspects of machine learning security to
   achieve our [12]goal of building safe, widely distributed ai.)

   to get an idea of what adversarial examples look like, consider this
   demonstration from [13]explaining and harnessing adversarial examples:
   starting with an image of a panda, the attacker adds a small
   perturbation that has been calculated to make the image be recognized
   as a gibbon with high confidence.
   [adversarial_img_1.png] an adversarial input, overlaid on a typical
   image, can cause a classifier to miscategorize a panda as a gibbon.

   the approach is quite robust; [14]recent research has shown adversarial
   examples can be printed out on standard paper then photographed with a
   standard smartphone, and still fool systems.
   [adversarial_img_2.png] adversarial examples can be printed out on
   normal paper and photographed with a standard resolution smartphone and
   still cause a classifier to, in this case, label a "washer" as a
   "safe".

   adversarial examples have the potential to be dangerous. for example,
   attackers could target autonomous vehicles by using stickers or paint
   to create an adversarial stop sign that the vehicle would interpret as
   a 'yield' or other sign, as discussed in [15]practical black-box
   attacks against deep learning systems using adversarial examples.

   id23 agents can also be manipulated by adversarial
   examples, according to new research from uc berkeley, openai, and
   pennsylvania state university, [16]adversarial attacks on neural
   network policies, and research from the university of nevada at reno,
   [17]vulnerability of deep id23 to policy induction
   attacks. the research shows that widely-used rl algorithms, such as
   [18]id25, [19]trpo, and [20]a3c, are vulnerable to adversarial inputs.
   these can lead to degraded performance even in the presence of
   pertubations too subtle to be percieved by a human, causing an agent to
   move [21]a pong paddle down when it should go up, or interfering with
   its ability to spot enemies in seaquest.

   iframe: [22]https://www.youtube.com/embed/r2jm0nrjzdi?ecver=1

   if you want to experiment with breaking your own models, you can use
   [23]cleverhans, an open source library developed jointly by [24]ian
   goodfellow and [25]nicolas papernot to test your ai's vulnerabilities
   to adversarial examples.
     __________________________________________________________________

adversarial examples give us some traction on ai safety

   when we think about the study of ai safety, we usually think about some
   of the most difficult problems in that field     how can we ensure that
   sophisticated id23 agents that are significantly more
   intelligent than human beings behave in ways that their designers
   intended?

   adversarial examples show us that even simple modern algorithms, for
   both supervised and id23, can already behave in
   surprising ways that we do not intend.
     __________________________________________________________________

attempted defenses against adversarial examples

   traditional techniques for making machine learning models more robust,
   such as weight decay and dropout, generally do not provide a practical
   defense against adversarial examples. so far, only two methods have
   provided a significant defense.

   adversarial training: this is a brute force solution where we simply
   generate a lot of adversarial examples and explicitly train the model
   not to be fooled by each of them. an open-source implementation of
   adversarial training is available in the [26]cleverhans library and its
   use illustrated in the following [27]tutorial.

   [28]defensive distillation: this is a strategy where we train the model
   to output probabilities of different classes, rather than hard
   decisions about which class to output. the probabilities are supplied
   by an earlier model, trained on the same task using hard class labels.
   this creates a model whose surface is smoothed in the directions an
   adversary will typically try to exploit, making it difficult for them
   to discover adversarial input tweaks that lead to incorrect
   categorization. (distillation was originally introduced in
   [29]distilling the knowledge in a neural network as a technique for
   model compression, where a small model is trained to imitate a large
   one, in order to obtain computational savings.)

   yet even these specialized algorithms can easily be broken by giving
   more computational firepower to the attacker.
     __________________________________________________________________

a failed defense:    gradient masking   

   to give an example of how a simple defense can fail, let's consider why
   a technique called "gradient masking" does not work.

   "gradient masking" is a term introduced in [30]practical black-box
   attacks against deep learning systems using adversarial examples. to
   describe an entire category of failed defense methods that work by
   trying to deny the attacker access to a useful gradient.

   most adversarial example construction techniques use the gradient of
   the model to make an attack. in other words, they look at a picture of
   an airplane, they test which direction in picture space makes the
   id203 of the    cat    class increase, and then they give a little
   push (in other words, they perturb the input) in that direction. the
   new, modified image is mis-recognized as a cat.

   but what if there were no gradient     what if an infinitesimal
   modification to the image caused no change in the output of the model?
   this seems to provide some defense because the attacker does not know
   which way to    push    the image.

   we can easily imagine some very trivial ways to get rid of the
   gradient. for example, most image classification models can be run in
   two modes: one mode where they output just the identity of the most
   likely class, and one mode where they output probabilities. if the
   model   s output is    99.9% airplane, 0.1% cat   , then a little tiny change
   to the input gives a little tiny change to the output, and the gradient
   tells us which changes will increase the id203 of the    cat   
   class. if we run the model in a mode where the output is just
      airplane   , then a little tiny change to the input will not change the
   output at all, and the gradient does not tell us anything.

   let   s run a thought experiment to see how well we could defend our
   model against adversarial examples by running it in    most likely class   
   mode instead of    id203 mode.    the attacker no longer knows where
   to go to find inputs that will be classified as cats, so we might have
   some defense. unfortunately, every image that was classified as a cat
   before is still classified as a cat now. if the attacker can guess
   which points are adversarial examples, those points will still be
   misclassified. we haven   t made the model more robust; we have just
   given the attacker fewer clues to figure out where the holes in the
   models defense are.

   even more unfortunately, it turns out that the attacker has a very good
   strategy for guessing where the holes in the defense are. the attacker
   can train their own model, a smooth model that has a gradient, make
   adversarial examples for their model, and then deploy those adversarial
   examples against our non-smooth model. very often, our model will
   misclassify these examples too. in the end, our thought experiment
   reveals that hiding the gradient didn   t get us anywhere.

   the defense strategies that perform gradient masking typically result
   in a model that is very smooth in specific directions and neighborhoods
   of training points, which makes it harder for the adversary to find
   gradients indicating good candidate directions to perturb the input in
   a damaging way for the model. however, the adversary can train a
   substitute model: a copy that imitates the defended model by observing
   the labels that the defended model assigns to inputs chosen carefully
   by the adversary.
   [adversarial_img_3.png]

   a procedure for performing such a model extraction attack was
   introduced in the black-box attacks paper. the adversary can then use
   the substitute model   s gradients to find adversarial examples that are
   misclassified by the defended model as well. in the figure above,
   reproduced from the discussion of gradient masking found in [31]towards
   the science of security and privacy in machine learning, we illustrate
   this attack strategy with a one-dimensional ml problem. the gradient
   masking phenomenon would be exacerbated for higher dimensionality
   problems, but harder to depict.

   we find that both adversarial training and defensive distillation
   accidentally perform a kind of gradient masking. neither algorithm was
   explicitly designed to perform gradient masking, but gradient masking
   is apparently a defense that machine learning algorithms can invent
   relatively easily when they are trained to defend themselves and not
   given specific instructions about how to do so. if we transfer
   adversarial examples from one model to a second model that was trained
   with either adversarial training or defensive distillation, the attack
   often succeeds, even when a direct attack on the second model would
   fail. this suggests that both training techniques do more to flatten
   out the model and remove the gradient than to make sure it classifies
   more points correctly.
     __________________________________________________________________

why is it hard to defend against adversarial examples?

   adversarial examples are hard to defend against because it is difficult
   to construct a theoretical model of the adversarial example crafting
   process. adversarial examples are solutions to an optimization problem
   that is non-linear and non-convex for many ml models, including neural
   networks. because we don   t have good theoretical tools for describing
   the solutions to these complicated optimization problems, it is very
   hard to make any kind of theoretical argument that a defense will rule
   out a set of adversarial examples.

   adversarial examples are also hard to defend against because they
   require machine learning models to produce good outputs for every
   possible input. most of the time, machine learning models work very
   well but only work on a very small amount of all the many possible
   inputs they might encounter.

   every strategy we have tested so far fails because it is not adaptive:
   it may block one kind of attack, but it leaves another vulnerability
   open to an attacker who knows about the defense being used. designing a
   defense that can protect against a powerful, adaptive attacker is an
   important research area.
     __________________________________________________________________

conclusion

   adversarial examples show that many modern machine learning algorithms
   can be broken in surprising ways. these failures of machine learning
   demonstrate that even simple algorithms can behave very differently
   from what their designers intend. we encourage machine learning
   researchers to get involved and design methods for preventing
   adversarial examples, in order to close this gap between what designers
   intend and how algorithms behave. if you're interested in working on
   adversarial examples, consider [32]joining openai.

for more information

   to learn more about machine learning security, follow ian and nicolas's
   machine learning security blog [33]cleverhans.io.
     __________________________________________________________________

   authors
   [34]ian goodfellow[35]nicolas papernot[36]sandy huang[37]rocky
   duan[38]pieter abbeel[39]jack clark
     __________________________________________________________________

     * [40]about
     * [41]progress
     * [42]resources
     * [43]blog
     * [44]charter
     * [45]jobs
     * [46]press

     *
     *

   sign up for our newsletter
   ____________________ (button)
   right

references

   visible links
   1. https://openai.com/blog/rss/
   2. https://openai.com/about/
   3. https://openai.com/progress/
   4. https://openai.com/resources/
   5. https://openai.com/blog/
   6. https://openai.com/about/
   7. https://openai.com/progress/
   8. https://openai.com/resources/
   9. https://openai.com/blog/
  10. https://openai.com/jobs/
  11. https://arxiv.org/abs/1312.6199
  12. https://openai.com/blog/openai-technical-goals/
  13. https://arxiv.org/abs/1412.6572
  14. https://arxiv.org/abs/1607.02533
  15. https://arxiv.org/abs/1602.02697
  16. http://rll.berkeley.edu/adversarial/
  17. https://arxiv.org/abs/1701.04143
  18. https://arxiv.org/abs/1312.5602
  19. https://arxiv.org/abs/1502.05477
  20. https://arxiv.org/abs/1602.01783
  21. https://www.youtube.com/watch?v=yye26lkl-ie
  22. https://www.youtube.com/embed/r2jm0nrjzdi?ecver=1
  23. https://github.com/openai/cleverhans
  24. https://twitter.com/goodfellow_ian
  25. https://twitter.com/nicolaspapernot
  26. https://github.com/openai/cleverhans
  27. https://github.com/openai/cleverhans/blob/master/tutorials/mnist_tutorial_tf.md
  28. https://arxiv.org/abs/1511.04508
  29. https://arxiv.org/abs/1503.02531
  30. https://arxiv.org/abs/1602.02697
  31. https://arxiv.org/abs/1611.03814
  32. https://openai.com/jobs/
  33. http://cleverhans.io/
  34. https://openai.com/blog/authors/ian/
  35. https://openai.com/blog/authors/nicolas/
  36. https://openai.com/blog/authors/sandy/
  37. https://openai.com/blog/authors/rocky/
  38. https://openai.com/blog/authors/pieter/
  39. https://openai.com/blog/authors/jack-clark/
  40. https://openai.com/about/
  41. https://openai.com/progress/
  42. https://openai.com/resources/
  43. https://openai.com/blog/
  44. https://openai.com/charter/
  45. https://openai.com/jobs/
  46. https://openai.com/press/

   hidden links:
  48. https://openai.com/
  49. https://openai.com/
  50. https://twitter.com/openai
  51. https://www.facebook.com/openai.research
