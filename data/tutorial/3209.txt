   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]chatbots life
     * [9]     start a project
     * [10]     tools
     * [11]    business
     * [12]     voice
     * [13]     tutorials
     * [14]     design
     * [15]            ai & nlp
     * [16]     ai for shopify
     __________________________________________________________________

deep learning in 7 lines of code

   [17]go to the profile of gk_
   [18]gk_ (button) blockedunblock (button) followfollowing
   apr 7, 2017

   the essence of machine learning is recognizing patterns within data.
   this boils down to 3 things: data, software and math. what can be done
   in seven lines of code you ask? a lot.
   [1*icbj3kcycukiym6dc-wt5w.jpeg]
   steve mcqueen and yul brynner in    the magnificent seven    (1960)

   the way to reduce a deep learning problem to a few lines of code is to
   use layers of abstraction, otherwise known as    frameworks   . today we   ll
   use tensorflow and tflearn.

   abstraction is an essential property of software: the app you are using
   to view this piece is an abstraction layer above some operating system
   that knows how to read files, display images, etc. and this is an
   abstraction above lower level functions. ultimately there is cpu-level
   code that moves bits         the    bare metal   .

     software frameworks are abstraction layers.

   our reduction is achieved by using [19]tflearn, a layer above
   [20]tensorflow, a layer above a [21]python. as always we   ll use
   [22]ipython notebook as a tool to facilitate our work.

let   s start at the beginning

   in    [23]how neural networks work    we built a neural network in python
   (no frameworks), and we showed how machine learning could    learn    from
   patterns of data, using a    toy data    example. recall the    toy    data is
   purposefully simple so that we can intuitively grok the patterns within
   it.

   the notebook for this    zero abstraction    code is [24]here. each
   mathematical operation within the model is detailed.
   [1*ccqpggeblgej32mvf2lalg.png]
   2-hidden layer ann

   extending our model to use 2 [25]hidden layers and [26]id119
   such as the one [27]we built for analyzing text, we have ~80 lines of
   code, again sans frameworks. this is an example of    deep learning, the
      depth    comes from the hidden layers.

   the definition of our model is relatively straight-forward, albeit
   laborious, most of the code is applied to training:

   iframe: [28]/media/be970e14b1c3cd825e63d5d335add99c?postid=7879a8ef8cfb

   this worked quite well         we then abstracted it using a framework.
   [1*kxcdbcu7gjkjfvktip-nta.png]

abstracting with tensorflow

   in    [29]tensorflow demystified    we built the same neural network, again
   we showed how machine learning could    learn    from patterns of data.

   this simplified our code (same underlying mathematical structures), for
   example the handling of id119 and id168 is
   conveniently reduced to 2 lines of code.

   iframe: [30]/media/77956deee961964f899b51217ad80d75?postid=7879a8ef8cfb

   [31]https://github.com/ugik/notebooks/blob/master/tensorflow%20ann.ipyn
   b

   our model   s definition is also simplified, the math and common
   functions (eg. sigmoid) are encapsulated for us in the framework.

   iframe: [32]/media/f0f48ec9b6704111595611227edbbcac?postid=7879a8ef8cfb

   you can imagine a complex neural network    flow   , like [33]alexnet,
   using tensorflow to simplify the definition and work for its
   mathematical    flow   .
   [1*wxms_k-zevgqittdjkatlq.png]
   [34]https://papers.nips.cc/paper/4824-id163-classification-with-deep
   -convolutional-neural-networks.pdf

abstracting again

   still unsatisfied with the amount of code and complexity involved, we
   abstract again using tflearn, which describes itself as:

     tflearn: deep learning library featuring a higher-level api for
     tensorflow.

   by    higher-level    they mean higher abstraction level, which is what
   we   re after. so we have our 7 lines of code for a multi-layer neural
   net.

   iframe: [35]/media/a351c069b2a507efc6b87a11232fca1f?postid=7879a8ef8cfb

   this is magnificent         5 lines of code to define our neural net
   structure (input +2 hidden +output +regression), 2 lines to train it.

   our notebook code is [36]here.

   let   s go through this in detail, you   ll notice that the data and
   learning intent is identical to [37]our earlier example.

framework installation

   make sure you have tensorflow 1.0.x installed, the tflearn framework
   will not work with tensorflow prior to version 1.0

   iframe: [38]/media/07bc6e17e9a911592ea23d16df011d46?postid=7879a8ef8cfb

'1.0.1'

   it may be helpful (on linux with pip) to use:

     python -m pip install         upgrade tensorflow tflearn

data

   next we setup our data, same toy data as [39]our tensorflow example.
   the training data was explained in detail there         should be
   self-explanatory. notice we no longer need to carve out testing data,
   the tflearn framework can do this for us.

   iframe: [40]/media/b2c2f79133fd04548a1d82c13af0046f?postid=7879a8ef8cfb

magnificent seven

   our deep-learning code:

   iframe: [41]/media/a351c069b2a507efc6b87a11232fca1f?postid=7879a8ef8cfb

   the first 5 lines define our neural    net    with a sequence of tflearn
   functions: from [42]tflearn.input_data to [43]tflearn.fully_connected,
   to [44]tflearn.regression. this    flow    is identical to our tensorflow
   example: our input data has 5 features, we   ll use 32 nodes in each
   hidden layer and our output has 2 classes.

   next we instantiate a deep neural network: [45]tflearn.dnn with our
   network, with a [46]tensorboard parameter to enable logging.

   and finally we fit our model with the training data. notice the sweet
   interface for the training metrics. change the n_epochs to see impact
   to accuracy.
   [1*5uiqnedbzsytxj81weu-vg.gif]
   interactive metrics while training model
training step: 1999  | total loss: 0.01591 | time: 0.003s
| adam | epoch: 1000 | loss: 0.01591 - acc: 0.9997 -- iter: 16/22
training step: 2000  | total loss: 0.01561 | time: 0.006s
| adam | epoch: 1000 | loss: 0.01561 - acc: 0.9997 -- iter: 22/22
--

predictions

   we now can use our model to predict output. be sure to remove any test
   patterns from your training data (comment out lines containing the
   patterns you want to test), otherwise the model is cheating.

   iframe: [47]/media/41d21bb8d10e1d9a51744f3838061d98?postid=7879a8ef8cfb

[[0.004509848542511463, 0.9954901337623596]]
[[0.9810173511505127, 0.018982617184519768]]

   our model correctly recognizes the [1, _, _, _, 1] pattern with output
   [1, 0]

   as a convenience when working iteratively with notebook, we reset our
   model   s graph by adding 2 lines directly above our model code:

   iframe: [48]/media/1f7472326c35f05c418104fd0f88b33c?postid=7879a8ef8cfb

   by abstracting, we can focus on preparing our data and using our model
   to make predictions.

tensorboard

   our tflearn framework automatically passes data to [49]tensorboard: a
   visualization tool for tensorflow. because we provided a log file with
   [50]tflearn.dnn we can have a quick look.

     $ tensorboard         logdir=tflearn_logs

     starting tensorboard b   41' on port 6006
     (you can navigate to [51]http://127.0.1.1:6006)

   here we can see a graph of our    flow   :
   [1*hxyae_jipnunhygaipwdlw.png]
   tensorboard graphs view

   and our accuracy and id168 performance:
   [1*uprkwyfqzjzfez6zfey0iq.png]
   tensorboard scalars view

   it   s clear we don   t need as many epochs in our training to achieve
   solid accuracy.

other examples

   here   s a tflearn setup for an [52]lstm id56 (long-short-term-memory
   recurrent neural-net), often used to learn sequences of data with
   memory. notice a different setup for the network and the tflearn.lstm,
   but mostly the same basic concept.

   iframe: [53]/media/4fe33e53fc49579964aeac17991b73da?postid=7879a8ef8cfb

   [54]https://github.com/tflearn/tflearn/blob/master/examples/nlp/lstm.py

   and here   s a tflearn setup for a [55]convolutional neural network,
   often used for image recognition. notice again all we   re doing is
   providing the mathematical sequence for the network   s mathematical
   equations, then feeding it data.

   iframe: [56]/media/ee06021b6436291be0d443effe4a2075?postid=7879a8ef8cfb

   [57]https://github.com/tflearn/tflearn/blob/master/examples/images/conv
   net_mnist.py

   we started by learning from code without any frameworks, this showed us
   precisely what was going on. no    black box   . once we have a solid
   understanding of the underlying code, we use frameworks to simplify our
   work, knowing that what   s inside.

   deep-learning frameworks simplify your work by encapsulating the
   underlying functions necessary. as the frameworks evolve and improve,
   we inherit those improvements automatically, consequentially we go from
      black box    to    black boxes within a black box   .

        so let it be written, so let it be done   

   [1*r37lnu2faoc9xe83h41gww.jpeg]
   yul brenner    the king and i    (1954)

   iframe: [58]/media/47a0a8491046a6d8bdbb86880dd8fc1e?postid=7879a8ef8cfb

   [1*bqlrszfhjemf4q7pyrlgng.gif]
   [59][1*6xuspt9josq0w0fi35hiaa.png]
   [60][1*c1ldmh5vbniz9rmaka8hwg.png]
   [61][1*d0jf3di6zthtqcfwdyy7mg.png]

     * [62]machine learning
     * [63]artificial intelligence
     * [64]tensorflow
     * [65]ai
     * [66]python

   (button)
   (button)
   (button) 989 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [67]go to the profile of gk_

[68]gk_

   philosopher, entrepreneur, investor

     (button) follow
   [69]chatbots life

[70]chatbots life

   best place to learn about chatbots. we share the latest bot news, info,
   ai & nlp, tools, tutorials & more.

     * (button)
       (button) 989
     * (button)
     *
     *

   [71]chatbots life
   never miss a story from chatbots life, when you sign up for medium.
   [72]learn more
   never miss a story from chatbots life
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://chatbotslife.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/7879a8ef8cfb
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://chatbotslife.com/deep-learning-in-7-lines-of-code-7879a8ef8cfb&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://chatbotslife.com/deep-learning-in-7-lines-of-code-7879a8ef8cfb&source=--------------------------nav_reg&operation=register
   8. https://chatbotslife.com/?source=logo-lo_kmgksdj0wxzj---a49517e4c30b
   9. https://chatbotslife.com/chatbot-development-hire-top-ai-chatbot-developers-c8b45ef7f207
  10. https://chatbotslife.com/chatbot-tools-62dfc60d2b8a
  11. https://chatbotslife.com/bots-for-business/home
  12. https://chatbotslife.com/tagged/voice-assistant
  13. https://chatbotslife.com/tagged/how-to
  14. https://chatbotslife.com/tagged/user-experience
  15. https://chatbotslife.com/tagged/artificial-intelligence
  16. https://www.gobeyond.ai/
  17. https://chatbotslife.com/@gk_?source=post_header_lockup
  18. https://chatbotslife.com/@gk_
  19. http://tflearn.org/
  20. https://www.tensorflow.org/
  21. https://www.python.org/
  22. https://ipython.org/notebook.html
  23. https://medium.com/@gk_/how-neural-networks-work-ff4c7ad371f7
  24. https://github.com/ugik/notebooks/blob/master/simple_neural_network.ipynb
  25. https://en.wikipedia.org/wiki/multilayer_id88
  26. https://en.wikipedia.org/wiki/gradient_descent
  27. https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6
  28. https://chatbotslife.com/media/be970e14b1c3cd825e63d5d335add99c?postid=7879a8ef8cfb
  29. https://chatbotslife.com/tensorflow-demystified-80987184faf7
  30. https://chatbotslife.com/media/77956deee961964f899b51217ad80d75?postid=7879a8ef8cfb
  31. https://github.com/ugik/notebooks/blob/master/tensorflow ann.ipynb
  32. https://chatbotslife.com/media/f0f48ec9b6704111595611227edbbcac?postid=7879a8ef8cfb
  33. https://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks.pdf
  34. https://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks.pdf
  35. https://chatbotslife.com/media/a351c069b2a507efc6b87a11232fca1f?postid=7879a8ef8cfb
  36. https://github.com/ugik/notebooks/blob/master/tflearn toy ann.ipynb
  37. https://chatbotslife.com/tensorflow-demystified-80987184faf7
  38. https://chatbotslife.com/media/07bc6e17e9a911592ea23d16df011d46?postid=7879a8ef8cfb
  39. https://chatbotslife.com/tensorflow-demystified-80987184faf7
  40. https://chatbotslife.com/media/b2c2f79133fd04548a1d82c13af0046f?postid=7879a8ef8cfb
  41. https://chatbotslife.com/media/a351c069b2a507efc6b87a11232fca1f?postid=7879a8ef8cfb
  42. http://tflearn.org/layers/core/
  43. http://tflearn.org/layers/core/
  44. http://tflearn.org/layers/estimator/#regression
  45. http://tflearn.org/models/dnn/
  46. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  47. https://chatbotslife.com/media/41d21bb8d10e1d9a51744f3838061d98?postid=7879a8ef8cfb
  48. https://chatbotslife.com/media/1f7472326c35f05c418104fd0f88b33c?postid=7879a8ef8cfb
  49. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  50. http://tflearn.org/models/dnn/
  51. http://127.0.1.1:6006/
  52. https://en.wikipedia.org/wiki/long_short-term_memory
  53. https://chatbotslife.com/media/4fe33e53fc49579964aeac17991b73da?postid=7879a8ef8cfb
  54. https://github.com/tflearn/tflearn/blob/master/examples/nlp/lstm.py
  55. https://en.wikipedia.org/wiki/convolutional_neural_network
  56. https://chatbotslife.com/media/ee06021b6436291be0d443effe4a2075?postid=7879a8ef8cfb
  57. https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_mnist.py
  58. https://chatbotslife.com/media/47a0a8491046a6d8bdbb86880dd8fc1e?postid=7879a8ef8cfb
  59. https://chatbotslife.com/bot-communities-mastermind-group-d2dae9876709#.53x0py6ou
  60. https://m.me/chatbotslife
  61. https://chatbotslife.com/how-to-get-a-free-chatbot-b1fb9dfe109#.z9dtp2sy0
  62. https://chatbotslife.com/tagged/machine-learning?source=post
  63. https://chatbotslife.com/tagged/artificial-intelligence?source=post
  64. https://chatbotslife.com/tagged/tensorflow?source=post
  65. https://chatbotslife.com/tagged/ai?source=post
  66. https://chatbotslife.com/tagged/python?source=post
  67. https://chatbotslife.com/@gk_?source=footer_card
  68. https://chatbotslife.com/@gk_
  69. https://chatbotslife.com/?source=footer_card
  70. https://chatbotslife.com/?source=footer_card
  71. https://chatbotslife.com/
  72. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  74. https://medium.com/p/7879a8ef8cfb/share/twitter
  75. https://medium.com/p/7879a8ef8cfb/share/facebook
  76. https://medium.com/p/7879a8ef8cfb/share/twitter
  77. https://medium.com/p/7879a8ef8cfb/share/facebook
