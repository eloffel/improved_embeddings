   #[1]tensorflow tutorial: 10 minutes practical tensorflow lesson for
   quick learners [2]a quick complete tutorial to save and restore
   tensorflow models

   ____________________

   [3]cv-tricks.com learn machine learning, ai & id161
   [4]login

     * [5]home
     * [6]tensorflow tutorials
     * [7]about

   [8]search

tensorflow tutorial 2: image classifier using convolutional neural network

   by ankit sachan

   in this tensorflow tutorial, we shall build a convolutional neural
   network based image classifier using tensorflow. if you are just
   getting started with tensorflow, then it would be a good idea to
   [9]read the basic tensorflow tutorial here.

   to demonstrate how to build a convolutional neural network based image
   classifier, we shall build a 6 layer neural network that will identify
   and separate images of dogs from that of cats. this network that we
   shall build is a very small network that you can run on a cpu as well.
   traditional neural networks that are very good at doing image
   classification have many more paramters and take a lot of time if
   trained on cpu. however, in this post, my objective is to show you how
   to build a real-world convolutional neural network using tensorflow
   rather than participating in [10]ilsvrc. before we start with
   tensorflow tutorial, let   s cover basics of convolutional neural
   network. if you are already familiar with conv-nets(and call them
   conv-nets), you can move to part-2 i.e. tensorflow tutorial.

part-1: basics of convolutional neural network (id98):

   neural networks are essentially mathematical models to solve an
   optimization problem. they are made of neurons, the basic computation
   unit of neural networks. a neuron takes an input(say x), do some
   computation on it(say: multiply it with a variable w and adds another
   variable b ) to produce a value (say; z= wx+b). this value is passed to
   a non-linear function called activation function(f) to produce the
   final output(activation) of a neuron. there are many kinds of
   id180. one of the popular activation function is
   sigmoid, which is:

   sigmoid function in tensorflow tutorial

   the neuron which uses sigmoid function as an activation function will
   be called sigmoid neuron. depending on the id180,
   neurons are named and there are many kinds of them like relu, tanh
   etc(remember this). one neuron can be connected to multiple neurons,
   like this:

   neuron in tensorflow tutorial

   in this example, you can see that the weights are the property of the
   connection, i.e. each connection has a different weight value while
   bias is the property of the neuron. this is the complete picture of a
   sigmoid neuron which produces output y:

   sigmoid in tensorflow tutorial

layers:

   if you stack neurons in a single line, it   s called a  layer; which is
   the next building block of neural networks.

   neural network shown in tensorflow tutorial

   as you can see above, the neurons in green make 1 layer which is the
   first layer of the network through which input data is passed to the
   network. similarly, the last layer is called output layer as shown in
   red. the layers in between input and output layer are called hidden
   layers. in this example, we have only 1 hidden layer shown in blue. the
   networks which have many hidden layers tend to be more accurate and are
   called deep network and hence machine learning algorithms which uses
   these deep networks are called deep learning.

types of layers:

   typically, all the neurons in one layer, do similar kind of
   mathematical operations and that   s how that a layer gets its
   name(except for input and output layers as they do little mathematical
   operations). here are the most popular kinds of layers you should know
   about:
    1. convolutional layer:
       convolution is a mathematical operation that   s used in single
       processing to filter signals, find patterns in signals etc. in a
       convolutional layer, all neurons apply convolution operation to the
       inputs, hence they are called convolutional neurons. the most
       important parameter in a convolutional neuron is the filter size,
       let   s say we have a layer with filter size 5*5*3. also, assume that
       the input that   s fed to convolutional neuron is an input image of
       size of 32*32 with 3 channels.   convolution in tensorflow tutorial
       let   s pick one 5*5*3(3 for number of channels in a colored image)
       sized chunk from image and calculate convolution(dot product) with
       our filter(w). this one convolution operation will result in a
       single number as output. we shall also add the bias(b) to this
       output.

   convolution explained during tensorflow tutorial

   in order to calculate the dot product, it   s mandatory for the 3rd
   dimension of the filter to be same as the number of channels in the
   input. i.e. when we calculate the dot product it   s a matrix
   multiplication of 5*5*3 sized chunk with 5*5*3 sized filter.

   we shall slide convolutional filter over whole input image to calculate
   this output across the image as shown by a schematic below: sliding a
   convolutional filter across image in tensorflow tutorial

   in this case, we slide our window by 1 pixel at a time. if some cases,
   people slide the windows by more than 1 pixel. this number is called
   stride.

   if you concatenate all these outputs in 2d, we shall have an output
   activation map of size 28*28(can you think of why 28*28 from 32*32 with
   the filter of 5*5 and stride of 1). typically, we use more than 1
   filter in one convolution layer. if we have 6 filters in our example,
   we shall have an output of size 28*28*6.

   tensorflow tutorial

   as you can see, after each convolution, the output reduces in size(as
   in this case we are going from 32*32 to 28*28). in a deep neural
   network with many layers, the output will become very small this way,
   which doesn   t work very well. so, it   s a standard practice to add zeros
   on the boundary of the input layer such that the output is the same
   size as input layer. so, in this example, if we add a padding of size 2
   on both sides of the input layer, the size of the output layer will be
   32*32*6 which works great from the implementation purpose as well.
   let   s say you have an input of size n*n, filter size is f, you are
   using s as stride and input is added with 0 pad of size p. then, the
   output size will be:

   (n-f+2p)/s +1

2. pooling layer:

   pooling layer is mostly used immediately after the convolutional layer
   to reduce the spatial size(only width and height, not depth). this
   reduces the number of parameters, hence computation is reduced. also,
   less number of parameters avoid overfitting(don   t worry about it now,
   will describe it little later). the most common form of pooling is max
   pooling where we take a filter of size f*f and apply the maximum
   operation over the f*f sized part of the image.

   if you take the average in place of taking maximum, it will be called
   average pooling, but it   s not very popular.

   if your input is of size w1*h1*d1 and the size of the filter is f*f
   with stride s. then the output sizes w2*h2*d2 will be:

   w2= (w1-f)/s +1

   h2=(h1-f)/s +1

   d2=d1

   most common pooling is done with the filter of size 2*2 with a stride
   of 2. as you can calculate using the above formula, it essentially
   reduces the size of input by half.

   max pooling with filter size of 2*2 during tensorflow tutorial

3. fully connected layer:

   if each neuron in a layer receives input from all the neurons in the
   previous layer, then this layer is called fully connected layer. the
   output of this layer is computed by id127 followed by
   bias offset.

understanding training process:

   deep neural networks are nothing but mathematical models of
   intelligence which to a certain extent mimic human brains. when we are
   trying to train a neural network, there are two fundamental things we
   need to do:
    1. the architecture of the network:
       when designing the architecture of a neural network you have to
       decide on: how do you arrange layers? which layers to use? how many
       neurons to use in each layer etc.? designing the architecture is
       slightly complicated and advanced topic and takes a lot of
       research. there are many standard architectures which work great
       for many standard problems. examples being alexnet, googlenet,
       inceptionresnet, vgg etc. in the beginning, you should only use the
       standard network architectures. you could start designing networks
       after you get a lot of experience with neural nets. hence, let   s
       not worry about it now.
    2. correct weights/parameters:

   once you have decided the architecture of the network; the second
   biggest variable is the weights(w) and biases(b) or the parameters of
   the network. the objective of the training is to get the best possible
   values of the all these parameters which solve the problem reliably.
   for example, when we are trying to build the classifier between dog and
   cat, we are looking to find parameters such that output layer gives out
   id203 of dog as 1(or at least higher than cat) for all images of
   dogs and id203 of cat as 1((or at least higher than dog) for all
   images of cats.

   you can find the best set of parameters using a process called backward
   propagation, i.e. you start with a random set of parameters and keep
   changing these weights such that for every training image we get the
   correct output. there are many optimizer methods to change the weights
   that are mathematically quick in finding the correct weights.
   gradientdescent is one such method(backward propagation and optimizer
   methods to change the gradient is a very complicated topic. but we
   don   t need to worry about it now as tensorflow takes care of it).

   so, let   s say, we start with some initial values of parameters and feed
   1 training image(in reality multiple images are fed together) of dog
   and we calculate the output of the network as 0.1 for it being a dog
   and 0.9 of it being a cat. now, we do backward propagation to slowly
   change the parameters such that the id203 of this image being a
   dog increases in the next iteration. there is a variable that is used
   to govern how fast do we change the parameters of the network during
   training, it   s called learning rate.  if you think about it, we want to
   maximise the total correct classifications by the network i.e. we care
   for the whole training set; we want to make these changes such that the
   number of correct classifications by the network increases. so we
   define a single number called cost which indicates if the training is
   going in the right direction. typically cost is defined in such a way
   that; as the cost is reduced, the accuracy of the network increases.
   so, we keep an eye on the cost and we keep doing many iterations of
   forward and backward propagations(10s of thousands sometimes) till cost
   stops decreasing. there are many ways to define cost. one of the simple
   one is mean root square cost. let   s say \(y_{prediction}\) is the
   vector containing the output of the network for all the training images
   and \(y_{actual}\) is the vector containing actual values(also called
   ground truth) of these labeled images. so, if we minimize the distance
   between these two variables, it would be a good indicator of the
   training. so, we define the cost as the average of these distances for
   all the images:

   $$ cost=0.5 \sum_{i=0}^n (y_{actual}-y_{prediction})^2 $$

   this is a very simple example of cost, but in actual training, we use
   much more complicated cost measures, like cross-id178 cost. but
   tensorflow implements many of these costs so we don   t need to worry
   about the details of these costs at this point in time.

   after training is done, these parameters and architecture will be saved
   in a binary file(called model). in production set-up when we get a new
   image of dog/cat to classify, we load this model in the same network
   architecture and calculate the id203 of the new image being a
   cat/dog. this is called id136 or prediction.

   for computational simplicity, not all training data is fed to the
   network at once. rather, let   s say we have total 1600 images, we divide
   them in small batches say of size 16 or 32 called batch-size. hence, it
   will take 100 or 50 rounds(iterations) for complete data to be used for
   training. this is called one epoch, i.e. in one epoch the networks sees
   all the training images once. there are a few more things that are done
   to improve accuracy but let   s not worry about everything at once.

part-2: tensorflow tutorial-> building a small neural network based image
classifier:

   network that we will implement in this tutorial is smaller and simpler
   (than the ones that are used to solve real-world problems) so that you
   can train this on your cpu as well. while training, images from both
   the classes(dogs/cats) are fed to a convolutional layer which is
   followed by 2 more convolutional layers. after convolutional layers, we
   flatten the output and add two fully connected layer in the end. the
   second fully connected layer has only two outputs which represent the
   id203 of an image being a cat or a dog.

   tensorflow tutorial

a) pre-requisites:

   i) opencv: we use opencv to read images of cats/dogs so you will have
   to install it.

   ii) shape function:

   if you have multi-dimensional tensor in tf, you can get the shape of it
   by doing this:
   python

   a = tf.truncated_normal([16,128,128,3])_____________________
   sess = tf.session()_________________________________________
   sess.run(tf.initialize_all_variables())_____________________
   sess.run(tf.shape(a))_______________________________________
   1
   2
   3
   4
   5

   a = tf.truncated_normal([16,128,128,3])
   sess = tf.session()
   sess.run(tf.initialize_all_variables())
   sess.run(tf.shape(a))

   output will be: array([ 16, 128, 128,   3], dtype=int32)

   you can reshape this to a new 2d tensor of shape[16  128*128*3]= [16
   49152].
   python

   b=tf.reshape(a,[16,49152])__________________________________
   sess.run(tf.shape(b))_______________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   2
   3

   b=tf.reshape(a,[16,49152])
   sess.run(tf.shape(b))

   output: array([16, 49152], dtype=int32)

   iii) softmax: is a function that converts k-dimensional vector    x   
   containing real values to the same shaped vector of real values in the
   range of (0,1), whose sum is 1. we shall apply the softmax function to
   the output of our convolutional neural network in order to, convert the
   output to the id203 for each class.

   $$    o(x)_{j}= \frac{e^{x_{i}}}{ \sum_{n=1}^n e^{x_{n}}}    \,\, for
   \, j=1   .n   $$

b) reading inputs:

   i have used 2000 images of dogs and cats each from [11]kaggle dataset
   but you could use any n image folders on your computer which contain
   different kinds of objects. typically, we divide our input data into 3
   parts:
    1. training data: we shall use 80% i.e. 0 images for training.
    2. validation data: 20% images will be used for validation. these
       images are taken out of training data to calculate accuracy
       independently during the training process.
    3. test set: separate independent data for testing which has around
       400 images. sometimes due to something called overfitting; after
       training, neural networks start working very well on the training
       data(and very similar images) i.e. the cost becomes very small, but
       they fail to work well for other images. for example, if you are
       training a classifier between dogs and cats and you get training
       data from someone who takes all images with white backgrounds. it   s
       possible that your network works very well on this validation
       data-set, but if you try to run it on an image with a cluttered
       background, it will most likely fail. so, that   s why we try to get
       our test-set from an independent source.

   python

   classes = ['dogs', 'cats']__________________________________
   num_classes = len(classes)__________________________________
   ____________________________________________________________
   train_path='training_data'__________________________________
   ____________________________________________________________
   # validation split__________________________________________
   validation_size = 0.2_______________________________________
   ____________________________________________________________
   # batch size________________________________________________
   batch_size = 16_____________________________________________
   ____________________________________________________________
   data = dataset.read_train_sets(train_path, img_size, classes
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13

   classes = ['dogs', 'cats']
   num_classes = len(classes)

   train_path='training_data'

   # validation split
   validation_size = 0.2

   # batch size
   batch_size = 16

   data = dataset.read_train_sets(train_path, img_size, classes,
   validation_size=validation_size)

   dataset is a class that i have created to read the input data. this is
   a simple python code that reads images from the provided training and
   testing data folders.

   the objective of our training is to learn the correct values of
   weights/biases for all the neurons in the network that work to do
   classification between dog and cat. the initial value of these weights
   can be taken anything but it works better if you take normal
   distributions(with mean zero and small variance). there are other
   methods to initialize the network but normal distribution is more
   prevalent. let   s create functions to create initial weights quickly
   just by specifying the shape(remember we talked about truncated_normal
   function in the [12]earlier post).
   python

   def create_weights(shape):__________________________________
       return tf.variable(tf.truncated_normal(shape, stddev=0.0
   ____________________________________________________________
   def create_biases(size):____________________________________
       return tf.variable(tf.constant(0.05, shape=[size]))_____
   1
   2
   3
   4
   5
   6

   def create_weights(shape):
       return tf.variable(tf.truncated_normal(shape, stddev=0.05))

   def create_biases(size):
       return tf.variable(tf.constant(0.05, shape=[size]))

c) creating network layers:

i) building convolution layer in tensorflow:

   tf.nn.conv2d function can be used to build a convolutional layer which
   takes these inputs:

   input= the output(activation) from the previous layer. this should be a
   4-d tensor. typically, in the first convolutional layer, you pass n
   images of size width*height*num_channels, then this has the size [n
   width height num_channels]

   filter= trainable variables defining the filter. we start with a random
   normal distribution and learn these weights. it   s a 4d tensor whose
   specific shape is predefined as part of network design. if your filter
   is of size filter_size and input fed has num_input_channels and you
   have num_filters filters in your current layer, then filter will have
   following shape:

   [filter_size filter_size num_input_channels num_filters]

   strides= defines how much you move your filter when doing convolution.
   in this function, it needs to be a tensor of size>=4 i.e. [batch_stride
   x_stride y_stride depth_stride]. batch_stride is always 1 as you don   t
   want to skip images in your batch. x_stride and y_stride are same
   mostly and the choice is part of network design and we shall use them
   as 1 in our example. depth_stride is always set as 1 as you don   t skip
   along the depth.

   padding=same means we shall 0 pad the input such a way that output x,y
   dimensions are same as that of input.

   after convolution, we add the biases of that neuron, which are also
   learnable/trainable. again we start with random normal distribution and
   learn these values during training.

   now, we apply max-pooling using tf.nn.max_pool function that has a very
   similar signature as that of conv2d function.
   python

   tf.nn.max_pool(value=layer,_________________________________
                                  ksize=[1, 2, 2, 1],__________
                                  strides=[1, 2, 2, 1],________
                                  padding='same')______________
   1
   2
   3
   4
   5

   tf.nn.max_pool(value=layer,
                                  ksize=[1, 2, 2, 1],
                                  strides=[1, 2, 2, 1],
                                  padding='same')

   notice that we are using k_size/filter_size as 2*2 and stride of 2 in
   both x and y direction. if you use the formula (w2= (w1-f)/s
   +1; h2=(h1-f)/s +1 ) mentioned earlier we can see that output is
   exactly half of input. these are most commonly used values for max
   pooling.

   finally, we use a relu as our activation function which simply takes
   the output of max_pool and applies relu using tf.nn.relu

   all these operations are done in a single convolution layer. let   s
   create a function to define a complete convolutional layer.
   python

   def create_convolutional_layer(input,_______________________
                  num_input_channels, _________________________
                  conv_filter_size,        ____________________
                  num_filters):  ______________________________
       ________________________________________________________
       ## we shall define the weights that will be trained usin
       weights = create_weights(shape=[conv_filter_size, conv_f
       ## we create biases using the create_biases function. th
       biases = create_biases(num_filters)_____________________
   ____________________________________________________________
       ## creating the convolutional layer_____________________
       layer = tf.nn.conv2d(input=input,_______________________
                        filter=weights,________________________
                        strides=[1, 1, 1, 1],__________________
                        padding='same')________________________
   ____________________________________________________________
       layer += biases_________________________________________
   ____________________________________________________________
       ## we shall be using max-pooling.  _____________________
       layer = tf.nn.max_pool(value=layer,_____________________
                               ksize=[1, 2, 2, 1],_____________
                               strides=[1, 2, 2, 1],___________
                               padding='same')_________________
       ## output of pooling is fed to relu which is the activat
       layer = tf.nn.relu(layer)_______________________________
   ____________________________________________________________
       return layer____________________________________________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   25
   26
   27
   28

   def create_convolutional_layer(input,
                  num_input_channels,
                  conv_filter_size,
                  num_filters):

       ## we shall define the weights that will be trained using
   create_weights function.
       weights = create_weights(shape=[conv_filter_size, conv_filter_size,
   num_input_channels, num_filters])
       ## we create biases using the create_biases function. these are
   also trained.
       biases = create_biases(num_filters)

       ## creating the convolutional layer
       layer = tf.nn.conv2d(input=input,
                        filter=weights,
                        strides=[1, 1, 1, 1],
                        padding='same')

       layer += biases

       ## we shall be using max-pooling.
       layer = tf.nn.max_pool(value=layer,
                               ksize=[1, 2, 2, 1],
                               strides=[1, 2, 2, 1],
                               padding='same')
       ## output of pooling is fed to relu which is the activation
   function for us.
       layer = tf.nn.relu(layer)

       return layer

ii) flattening layer:

   the output of a convolutional layer is a multi-dimensional tensor. we
   want to convert this into a one-dimensional tensor. this is done in the
   flattening layer. we simply use the reshape operation to create a
   single dimensional tensor as defined below:
   python

   def create_flatten_layer(layer):____________________________
       layer_shape = layer.get_shape()_________________________
       num_features = layer_shape[1:4].num_elements()__________
       layer = tf.reshape(layer, [-1, num_features])___________
   ____________________________________________________________
       return layer____________________________________________
   1
   2
   3
   4
   5
   6
   7

   def create_flatten_layer(layer):
       layer_shape = layer.get_shape()
       num_features = layer_shape[1:4].num_elements()
       layer = tf.reshape(layer, [-1, num_features])

       return layer

iii) fully connected layer:

   now, let   s define a function to create a fully connected layer. just
   like any other layer, we declare weights and biases as random normal
   distributions. in fully connected layer, we take all the inputs, do the
   standard z=wx+b operation on it. also sometimes you would want to add a
   non-linearity(relu) to it. so, let   s add a condition that allows the
   caller to add relu to the layer.
   python

   def create_fc_layer(input,          ________________________
                num_inputs,    ________________________________
                num_outputs,___________________________________
                use_relu=true):________________________________
       ________________________________________________________
       #let's define trainable weights and biases._____________
       weights = create_weights(shape=[num_inputs, num_outputs]
       biases = create_biases(num_outputs)_____________________
   ____________________________________________________________
       layer = tf.matmul(input, weights) + biases______________
       if use_relu:____________________________________________
           layer = tf.nn.relu(layer)___________________________
   ____________________________________________________________
       return layer____________________________________________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15

   def create_fc_layer(input,
                num_inputs,
                num_outputs,
                use_relu=true):

       #let's define trainable weights and biases.
       weights = create_weights(shape=[num_inputs, num_outputs])
       biases = create_biases(num_outputs)

       layer = tf.matmul(input, weights) + biases
       if use_relu:
           layer = tf.nn.relu(layer)

       return layer

   so, we have finished defining the building blocks of the network.

iv) placeholders and input:

   now, let   s create a placeholder that will hold the input training
   images. all the input images are read in dataset.py file and resized to
   128 x 128 x 3 size. input placeholder x is created in the shape of
   [none, 128, 128, 3]. the first dimension being none means you can pass
   any number of images to it. for this program, we shall pass images in
   the batch of 16 i.e. shape will be [16 128 128 3]. similarly, we create
   a placeholder y_true for storing the predictions. for each image, we
   have two outputs i.e. probabilities for each class. hence y_pred is of
   the shape [none 2] (for batch size 16 it will be [16 2].
   python

   x = tf.placeholder(tf.float32, shape=[none, img_size,img_siz
   ____________________________________________________________
   y_true = tf.placeholder(tf.float32, shape=[none, num_classes
   y_true_cls = tf.argmax(y_true, dimension=1)_________________
   1
   2
   3
   4
   5

   x = tf.placeholder(tf.float32, shape=[none,
   img_size,img_size,num_channels], name='x')

   y_true = tf.placeholder(tf.float32, shape=[none, num_classes],
   name='y_true')
   y_true_cls = tf.argmax(y_true, dimension=1)

v) network design:

   we use the functions defined above to create various layers of the
   network.
   python

   layer_conv1 = create_convolutional_layer(input=x,___________
                  num_input_channels=num_channels,_____________
                  conv_filter_size=filter_size_conv1,__________
                  num_filters=num_filters_conv1)_______________
   ____________________________________________________________
   layer_conv2 = create_convolutional_layer(input=layer_conv1,_
                  num_input_channels=num_filters_conv1,________
                  conv_filter_size=filter_size_conv2,__________
                  num_filters=num_filters_conv2)_______________
   ____________________________________________________________
   layer_conv3= create_convolutional_layer(input=layer_conv2,__
                  num_input_channels=num_filters_conv2,________
                  conv_filter_size=filter_size_conv3,__________
                  num_filters=num_filters_conv3)_______________
             __________________________________________________
   layer_flat = create_flatten_layer(layer_conv3)______________
   ____________________________________________________________
   layer_fc1 = create_fc_layer(input=layer_flat,_______________
                        num_inputs=layer_flat.get_shape()[1:4].
                        num_outputs=fc_layer_size,_____________
                        use_relu=true)_________________________
   ____________________________________________________________
   layer_fc2 = create_fc_layer(input=layer_fc1,________________
                        num_inputs=fc_layer_size,______________
                        num_outputs=num_classes,_______________
                        use_relu=false)________________________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   25
   26
   27

   layer_conv1 = create_convolutional_layer(input=x,
                  num_input_channels=num_channels,
                  conv_filter_size=filter_size_conv1,
                  num_filters=num_filters_conv1)

   layer_conv2 = create_convolutional_layer(input=layer_conv1,
                  num_input_channels=num_filters_conv1,
                  conv_filter_size=filter_size_conv2,
                  num_filters=num_filters_conv2)

   layer_conv3= create_convolutional_layer(input=layer_conv2,
                  num_input_channels=num_filters_conv2,
                  conv_filter_size=filter_size_conv3,
                  num_filters=num_filters_conv3)

   layer_flat = create_flatten_layer(layer_conv3)

   layer_fc1 = create_fc_layer(input=layer_flat,

   num_inputs=layer_flat.get_shape()[1:4].num_elements(),
                        num_outputs=fc_layer_size,
                        use_relu=true)

   layer_fc2 = create_fc_layer(input=layer_fc1,
                        num_inputs=fc_layer_size,
                        num_outputs=num_classes,
                        use_relu=false)

vi) predictions:

   as mentioned above, you can get the id203 of each class by
   applying softmax to the output of fully connected layer.

   y_pred = tf.nn.softmax(layer_fc2,name="y_pred")

   y_pred contains the predicted id203 of each class for each input
   image. the class having higher id203 is the prediction of the
   network. y_pred_cls = tf.argmax(y_pred, dimension=1)

   now, let   s define the cost that will be minimized to reach the optimum
   value of weights. we will use a simple cost that will be calculated
   using a tensorflow function softmax_cross_id178_with_logits which
   takes the output of last fully connected layer and actual labels to
   calculate cross_id178 whose average will give us the cost.
   python

   cross_id178 = tf.nn.softmax_cross_id178_with_logits(logi
                                                       labels=y
   cost = tf.reduce_mean(cross_id178)________________________
   ____________________________________________________________
   1
   2
   3
   4

   cross_id178 =
   tf.nn.softmax_cross_id178_with_logits(logits=layer_fc2,
                                                       labels=y_true)
   cost = tf.reduce_mean(cross_id178)

vii) optimization:

   tensorflow implements most of the optimisation functions. we shall use
   adamoptimizer for gradient calculation and weight optimization. we
   shall specify that we are trying to minimise cost with a learning rate
   of 0.0001.

   optimizer = tf.train.adamoptimizer(learning_rate=1e-4).minimize(cost)

   as you know, if we run optimizer operation inside session.run(), in
   order to calculate the value of cost, the whole network will have to be
   run and we will pass the training images in a feed_dict(does that make
   sense? think about, what variable would you need to calculate cost and
   keep going up in the code). training images are passed in a batch of
   16(batch_size) in each iteration.
   python

   batch_size = 16_____________________________________________
   ____________________________________________________________
   x_batch, y_true_batch, _, cls_batch = data.train.next_batch(
   ____________________________________________________________
   feed_dict_train = {x: x_batch,______________________________
                              y_true: y_true_batch}____________
   ____________________________________________________________
   session.run(optimizer, feed_dict=feed_dict_tr)______________
   1
   2
   3
   4
   5
   6
   7
   8
   9

   batch_size = 16

   x_batch, y_true_batch, _, cls_batch = data.train.next_batch(batch_size)

   feed_dict_train = {x: x_batch,
                              y_true: y_true_batch}

   session.run(optimizer, feed_dict=feed_dict_tr)

   where next_batch is a simple python function in dataset.py file that
   returns the next 16 images to be passed for training. similarly, we
   pass the validation batch of images independently to in another
   session.run() call.
   python

   x_valid_batch, y_valid_batch, _, valid_cls_batch = data.vali
   ____________________________________________________________
   feed_dict_val = {x: x_valid_batch,__________________________
                         y_true: y_valid_batch}________________
   ____________________________________________________________
   val_loss = session.run(cost, feed_dict=feed_dict_val)_______
   1
   2
   3
   4
   5
   6
   7

   x_valid_batch, y_valid_batch, _, valid_cls_batch =
   data.valid.next_batch(train_batch_size)

   feed_dict_val = {x: x_valid_batch,
                         y_true: y_valid_batch}

   val_loss = session.run(cost, feed_dict=feed_dict_val)

   note that in this case, we are passing cost in the session.run() with a
   batch of validation images as opposed to training images. in order to
   calculate the cost, the whole network(3 convolution+1 flattening+2 fc
   layers) will have to be executed to produce layer_fc2(which is required
   to calculate cross_id178, hence cost). however, as opposed to
   training, this time optimization  optimizer =
   tf.train.adamoptimizer(learning_rate=1e-4).minimize(cost) will not be
   run(as we only have to calculate cost). this is what changes the
   gradients and weights and is very computationally expensive. we can
   calculate the accuracy on validataion set using true labels(y_true) and
   predicted labels(y_pred).
   python

   correct_prediction = tf.equal(y_pred_cls, y_true_cls)_______
   accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.flo
   ____________________________________________________________
   ____________________________________________________________
   1
   2
   3

   correct_prediction = tf.equal(y_pred_cls, y_true_cls)
   accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

   we can calculate the validation accuracy by passing accuracy in
   session.run() and providing validation images in a feed_dict.

   val_acc = session.run(accuracy,feed_dict=feed_dict_validate)

   similarly, we also report the accuracy for the training images.

   acc = session.run(accuracy, feed_dict=feed_dict_train)

   as, training images along with labels are used for training, so in
   general training accuracy will be higher than validation. we report
   training accuracy to know that we are at least moving in the right
   direction and are at least improving accuracy in the training dataset.
   after each epoch, we report the accuracy numbers and save the model
   using saver object in tensorflow.
   python

   saver.save(session, 'dogs-cats-model')______________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   2

   saver.save(session, 'dogs-cats-model')

   so, this is how the complete train function looks like:
   python

   def train(num_iteration):___________________________________
       global total_iterations_________________________________
       ________________________________________________________
       for i in range(total_iterations,________________________
                      total_iterations + num_iteration):_______
   ____________________________________________________________
           x_batch, y_true_batch, _, cls_batch = data.train.nex
           x_valid_batch, y_valid_batch, _, valid_cls_batch = d
   ____________________________________________________________
           ____________________________________________________
           feed_dict_tr = {x: x_batch,_________________________
                              y_true: y_true_batch}____________
           feed_dict_val = {x: x_valid_batch,__________________
                                 y_true: y_valid_batch}________
   ____________________________________________________________
           session.run(optimizer, feed_dict=feed_dict_tr)______
   ____________________________________________________________
           if i % int(data.train.num_examples/batch_size) == 0:
               val_loss = session.run(cost, feed_dict=feed_dict
               epoch = int(i / int(data.train.num_examples/batc
               ________________________________________________
               show_progress(epoch, feed_dict_tr, feed_dict_val
               saver.save(session, 'dogs-cats-model') _________
   ____________________________________________________________
   ____________________________________________________________
       total_iterations += num_iteration_______________________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24
   25
   26
   27

   def train(num_iteration):
       global total_iterations

       for i in range(total_iterations,
                      total_iterations + num_iteration):

           x_batch, y_true_batch, _, cls_batch =
   data.train.next_batch(batch_size)
           x_valid_batch, y_valid_batch, _, valid_cls_batch =
   data.valid.next_batch(batch_size)


           feed_dict_tr = {x: x_batch,
                              y_true: y_true_batch}
           feed_dict_val = {x: x_valid_batch,
                                 y_true: y_valid_batch}

           session.run(optimizer, feed_dict=feed_dict_tr)

           if i % int(data.train.num_examples/batch_size) == 0:
               val_loss = session.run(cost, feed_dict=feed_dict_val)
               epoch = int(i /
   int(data.train.num_examples/batch_size))

               show_progress(epoch, feed_dict_tr, feed_dict_val, val_loss)
               saver.save(session, 'dogs-cats-model')


       total_iterations += num_iteration

   this code is slightly long as it   s a real world example. so, please go
   [13]here, clone the code and run the train.py file to start the
   training. this is how the output will look like:

   tensorflow tutorial using convolutional neural networks

   this is a small network and is not state-of-the-art to build an image
   classifier but it   s very good for learning specially when you are just
   getting started. for our training, we get more than 80% accuracy on
   validation set. as we save the model during training, we shall use this
   to run on our own images.

prediction:

   after you are done with training, you shall notice that there are many
   new files in the folder:
    1. dogs-cats-model.meta
    2. dogs-cats-model.data-00000-of-00001
    3. dogs-cats-model.index
    4. checkpoint

   file dogs-cats-model.meta contains the complete network graph and we
   can use this to recreate the graph later. we shall use a saver object
   provided by tensorflow to do this.
   python

   saver = tf.train.import_meta_graph('flowers-model.meta')____
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   2

   saver = tf.train.import_meta_graph('flowers-model.meta')

   the file dogs-cats-model.data-00000-of-00001 contains the trained
   weights(values of variables) of the network. so, once we have recreated
   the graph, we shall restore the weights.
   python

   saver.restore(sess, tf.train.latest_checkpoint('./'))_______
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   2

   saver.restore(sess, tf.train.latest_checkpoint('./'))

   in order to get the prediction of the network, we need to read &
   pre-process the input image in the same way(as training), get hold of
   y_pred on the graph and pass it the new image in a feed dict. so, let   s
   do that:
   python

   image = cv2.imread(filename)________________________________
   # resizing the image to our desired size and________________
   # preprocessing will be done exactly as done during training
   image = cv2.resize(image, (image_size, image_size), cv2.inte
   images.append(image)________________________________________
   images = np.array(images, dtype=np.uint8)___________________
   images = images.astype('float32')___________________________
   images = np.multiply(images, 1.0/255.0) ____________________
   #the input to the network is of shape [none image_size image
   x_batch = images.reshape(1, image_size,image_size,num_channe
   ____________________________________________________________
   ____________________________________________________________
   graph = tf.get_default_graph()______________________________
   ____________________________________________________________
   y_pred = graph.get_tensor_by_name("y_pred:0")_______________
   ____________________________________________________________
   ## let's feed the images to the input placeholders__________
   x= graph.get_tensor_by_name("x:0") _________________________
   y_true = graph.get_tensor_by_name("y_true:0") ______________
   y_test_images = np.zeros((1, 2)) ___________________________
   ____________________________________________________________
   feed_dict_testing = {x: x_batch, y_true: y_test_images}_____
   result=sess.run(y_pred, feed_dict=feed_dict_testing)________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   18
   19
   20
   21
   22
   23
   24

   image = cv2.imread(filename)
   # resizing the image to our desired size and
   # preprocessing will be done exactly as done during training
   image = cv2.resize(image, (image_size, image_size), cv2.inter_linear)
   images.append(image)
   images = np.array(images, dtype=np.uint8)
   images = images.astype('float32')
   images = np.multiply(images, 1.0/255.0)
   #the input to the network is of shape [none image_size image_size
   num_channels]. hence we reshape.
   x_batch = images.reshape(1, image_size,image_size,num_channels)


   graph = tf.get_default_graph()

   y_pred = graph.get_tensor_by_name("y_pred:0")

   ## let's feed the images to the input placeholders
   x= graph.get_tensor_by_name("x:0")
   y_true = graph.get_tensor_by_name("y_true:0")
   y_test_images = np.zeros((1, 2))

   feed_dict_testing = {x: x_batch, y_true: y_test_images}
   result=sess.run(y_pred, feed_dict=feed_dict_testing)

   finally, we can run a new image of dog/cat using predict script.
   python

   python predict.py test_dog.jpg______________________________
   [[ 0.99398661   0.00601341]]_________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   2
   3

   python predict.py test_dog.jpg
   [[ 0.99398661  0.00601341]]

   output contains the probabilities of the input image being a dog or a
   cat. in this example, id203 of being dog is much higher than that
   of cat.

   congratulations! you have learnt how to build and train an image
   classifier using convolutional neural networks.

   trained model and data: in the git repository, i have only added 500
   images for each class. but it takes more than 500 images of dogs/cats
   to train even a decent classifier. so, i have trained this model on
   2400 images of each class. you can download these images from here.
   this mini-cat-dog-dataset is a subset of kaggle dog-cat dataset and is
   not owned by us. you can also use[14] my trained model available here
   to generate the prediction.

   the complete code is available [15]here. please let me know your
   questions and feedback in the comments below. these comments and
   feedback are my motivation to create more tutorials      .

     practice exercises:  1. for the fun of it, you can use the same
     script to train another classifier on your own dataset(take at least
     500 images of each class). depending on the kind of problem you
     choose, you would notice:

     depending on the problem, the sample network does better or worse.
     for example, if you train a classifier on bikes, airplanes and cars
     it will take lesser training data and you will get higher accuracy.
     but if you take a problem which is harder then you would need a lot
     of data and it may still not be enough.

     2. a very standard practice in id161 is to augment the
     data, i.e. you can slightly rotate, crop, zoom-in, flip the original
     image to generate new training examples. this in general leads to
     improved accuracy. for practice, you can augment and train again,
     try and report how many minimum images would you need to get the
     same level of accuracy or how much accuracy gain will be achieved by
     this.


   [16]how-to, [17]image recognition, [18]tensorflow tutorial

most popular
     __________________________________________________________________

     * tensorflow tutorial 2: image classifier using convolutional neural
       network
     * [19]tensorflow tutorials [20]a quick complete tutorial to save and
       restore tensorflow models
     * [21]resnet, alexnet, vggnet, inception: understanding various
       architectures of convolutional networks
     * [22]zero to hero: guide to id164 using deep learning:
       ...
     * [23]keras tensorflow tutorial [24]keras tutorial: practical guide
       from getting started to developing complex ...

[25]rss [26]cv-tricks rss feed
     __________________________________________________________________

     * [27]human pose estimation using deep learning in opencv
     * [28]deep learning based image colorization with opencv
     * [29]deep learning based edge detection in opencv
     __________________________________________________________________

share this article

   [30]share on facebook [31]share on twitter [32]share on pinterest

   copyright    2017 cv-tricks.com

references

   visible links
   1. https://cv-tricks.com/artificial-intelligence/deep-learning/deep-learning-frameworks/tensorflow-tutorial/
   2. https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
   3. https://cv-tricks.com/
   4. https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/
   5. https://cv-tricks.com/
   6. https://cv-tricks.com/category/tensorflow-tutorial
   7. https://cv-tricks.com/about-us/
   8. https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/
   9. https://cv-tricks.com/artificial-intelligence/deep-learning/deep-learning-frameworks/tensorflow/tensorflow-tutorial/
  10. http://image-net.org/challenges/lsvrc/
  11. https://www.kaggle.com/c/dogs-vs-cats
  12. https://cv-tricks.com/artificial-intelligence/deep-learning/deep-learning-frameworks/tensorflow/tensorflow-tutorial/
  13. https://github.com/sankit1/cv-tricks.com
  14. https://drive.google.com/open?id=0b2l-gjqoc67tb3h5wuw2djvusvk
  15. https://github.com/sankit1/cv-tricks.com
  16. https://cv-tricks.com/tag/how-to/
  17. https://cv-tricks.com/tag/image-recognition/
  18. https://cv-tricks.com/tag/tensorflow-tutorial/
  19. https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
  20. https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
  21. https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/
  22. https://cv-tricks.com/object-detection/faster-r-id98-yolo-ssd/
  23. https://cv-tricks.com/tensorflow-tutorial/keras/
  24. https://cv-tricks.com/tensorflow-tutorial/keras/
  25. https://cv-tricks.com/feed/
  26. https://cv-tricks.com/
  27. https://cv-tricks.com/pose-estimation/using-deep-learning-in-opencv/
  28. https://cv-tricks.com/opencv/deep-learning-image-colorization/
  29. https://cv-tricks.com/opencv-dnn/edge-detection-hed/
  30. http://www.facebook.com/share.php?u=https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/
  31. https://twitter.com/share?url=https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/
  32. http://pinterest.com/pin/create/button/?url=https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/

   hidden links:
  34. https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/
  35. https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/
  36. https://cv-tricks.com/object-detection/faster-r-id98-yolo-ssd/
  37. https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/#top
