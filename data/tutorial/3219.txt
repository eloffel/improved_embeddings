   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets   
   parallelism in machine learning: gpus, cuda, and practical applications
   comments feed [5]sap.io start-up incubator: senior data scientist
   [6]transform the future with predictive analytics

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2016    [28]nov    [29]tutorials,
   overviews    parallelism in machine learning: gpus, cuda, and practical
   applications ( [30]16:n40 )

parallelism in machine learning: gpus, cuda, and practical applications

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 491
   tags: [33]algorithms, [34]cuda, [35]gpu, [36]nvidia, [37]parallelism

   the lack of parallel processing in machine learning tasks inhibits
   economy of performance, yet it may very well be worth the trouble. read
   on for an introductory overview to gpu-based parallelism, the cuda
   framework, and some thoughts on practical implementation.

   pages: [38]1 2
     __________________________________________________________________

   by [39]matthew mayo, kdnuggets.

   importantly, hosts and devices possess their own memory spaces,
   independent of one another. a cuda device shares a single global memory
   space. the first requirement for launching kernels and spawning
   numerous device threads for computation is to copy the required data
   from host to device memory. once computation is complete, it is
   necessary to copy the results back in the reverse direction. this is
   all facilitated via cuda extensions, and occurs at a heavily abstracted
   layer from the programmer   s point of view.

   when managing device-side memory, proper allocation of blocks to
   kernels is critical; too few leads to a lack of computational power,
   while too many results in wasted threads, which could have been
   assigned to other simultaneously-executing kernels. as a specific
   example, this could translate to too few threads allocated to a
   particular fold during k-fold cross-validation model-building, leading
   to a much longer validation process than intended.

   conversely, it could result in too many threads being assigned during
   k-fold cross-validation model building, leaving large numbers of unused
   threads and extending the amount of time required for all folds to
   complete their model validation.

   fortunately, the management of device memory, including the number of
   threads assigned to blocks, and, ultimately, to kernels, is
   user-definable (within some upper bounds, such as a maximum of 1024
   threads per block). cuda provides some clever ways of semi-automation
   of this management as well, allowing memory management functions to
   take mathematical expressions as arguments, so that, for example, a
   kernel can, upon execution, calculate the size of a data structure such
   as an array or a matrix and allocate the amount and dimensions of
   memory that would be appropriate for its computations.

                           cuda grid organization
                       fig. 4: cuda grid organization.

   consider id127, an aspect of id75 which we
   propose to parallelize, and its implementation on the cuda
   architecture. proceeding at a high level, without regard to matrix
   sizes, we can say that we have 2 matrices to multiply, m and n, and
   that the result will be stored in matrix p. first, we allocate space
   for matrices m and n in device global memory, as well as space for the
   resulting matrix p. we then copy matrices m and n to the device.

   assuming for simplicity that all matrices fit into a single block, we
   will have each block thread compute an element of p. to accomplish
   this, each thread loads a row of m and a column of n, computes the dot
   product, and stores it in the appropriate element of p. as each of
   these dot products are computed in parallel, the total time it will
   take to perform the id127 is the time that it takes to
   perform a single dot product computation. once complete, matrix p is
   then copied from device memory back to host memory, where it can be
   further used by serial code, if necessary. typically such a kernel
   operation would be followed by deallocation of device memory.

   this is a high level overview. in practice, additional tasks need to be
   performed, such as determining block sizes, as stated above. it is also
   a single, specific example; however, the memory management and device
   computation techniques, while they will be, by necessity, quite
   different by algorithmic situation, generalize to our various tasks:
   identify parallelizable computations, allocate device memory, copy data
   to device, perform parallelized computation, copy result back to host,
   continue with serial code. note that the memory allocation and data
   copying overhead can easily become a bottleneck here, with any
   potential computational time savings stymied by these processes.

algorithmic applications in machine learning


   given the proper data, knowledge of algorithm implementation, and
   ambition, there is no limit to what you can attempt with parallel
   processing in machine learning. of course, and as mentioned above,
   identifying parallelizable portions of code is the most difficult task,
   and there may not be any in a given algorithm.

   a good place to start is id127, as treated above, which
   is a well-used method for implementing id75 algorithms. an
   implementation of id75 on gpgpu can be found [40]here. the
   paper "performance improvement of data mining in weka through gpu
   acceleration" notes speed increases, and the paper provides some
   additional insight into conceptualizing parallelism algorithmically.

   another common task used in machine learning which is ripe for
   parallelization is distance calculation. euclidean distance is a very
   common metric which requires calculation over and over again in
   numerous algorithms, including id116 id91. since the individual
   distance calculations of successive iterations are not dependent on
   other calculations of the same iteration, these calculations could be
   performed in parallel (if we forget our memory management overhead as a
   potential bottleneck to contend with).

                           k-fold cross-validation
                      fig. 5: k-fold cross-validation.

   while these aforementioned shared statistical tasks could benefit from
   efficiency of execution, there is an additional aspect of the machine
   learning data flow which could potentially allow for even more
   significant gains. a common evaluation technique regularly employed in
   machine learning model validating is k-fold cross-validation, involving
   the intensive, not-necessarily sequential processing of dataset
   segments. k-fold cross-validation is a deterministic method for model
   building, achieved by leaving out one of k segments, or folds, of a
   dataset, training on all k-1 segments, and using the remaining kth
   segment for testing; this process is then repeated k times, with the
   individual prediction error results being combined and averaged in a
   single, integrated model. this provides variability, with the goal of
   producing the most accurate predictive models possible.

   this extensive model validating, when performed sequentially, can be
   relatively time-consuming, especially when each fold is paired with a
   computationally expensive algorithm task such as id75
   id127. as k-fold cross-validation is a standard method
   for predicting a given machine learning algorithm   s error rate,
   attempting to increase the speed by which this prediction occurs seems
   particularly worthy of effort. a very high level view of doing so is
   implied previously in this article.

   a consideration for those using python goes beyond algorithm design,
   and relates to optimized native code and runtime comparisons with
   parallel implementations. while beyond the scope of this discussion,
   you may want to read more about this topic [41]here.

   thinking algorithmically is necessary to leverage finite computational
   resources in any situation, and is no different with machine learning.
   with some clever thinking, an in-depth understanding of what you are
   attempting, and a collection of tools and their documentation, you
   never know what you may be able to achieve. trust me when i say that,
   after doing some related work in grad school, finding opportunities to
   experiment with take much less time than you might think. familiarize
   yourself with a code base, read a few tutorials, and get to work.

   parallel computing, gpus, and traditional machine learning can be good
   friends, and i challenge you to dig deeper and discover the potential
   for yourself.

   related:
     * [42]comparing id91 techniques: a concise technical overview
     * [43]support vector machines: a concise technical overview
     * [44]data science basics: data mining vs. statistics

   pages: [45]1 2
     __________________________________________________________________

   [46][prv.gif] previous post
   [47]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [48]another 10 free must-read books for machine learning and data
       science
    2. [49]9 must-have skills you need to become a data scientist, updated
    3. [50]who is a typical data scientist in 2019?
    4. [51]the pareto principle for data scientists
    5. [52]what no one will tell you about data science job applications
    6. [53]19 inspiring women in ai, big data, data science, machine
       learning
    7. [54]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [55]id158s optimization using genetic algorithm
       with python
    2. [56]who is a typical data scientist in 2019?
    3. [57]8 reasons why you should get a microsoft azure certification
    4. [58]the pareto principle for data scientists
    5. [59]r vs python for data visualization
    6. [60]how to work in data science, ai, big data
    7. [61]the deep learning toolset     an overview

[62]latest news

     * [63]what is missing when ai makes a decision?
     * [64]spatio-temporal statistics: a primer
     * [65]another 10 free must-see courses for machine learning a...
     * [66]download your datax guide to ai in marketing
     * [67]kdnuggets offer: save 20% on strata in london
     * [68]training a champion: building deep neural nets for big ...

more recent stories

     * [69]training a champion: building deep neural nets for big data
       an...
     * [70]building a recommender system
     * [71]predict age and gender using convolutional neural network and
       ...
     * [72]top tweets, mar 27     apr 02: here is a great explanat...
     * [73]odsc east is selling out; odsc india announced
     * [74]accelerate ai and data science career expo, may 4, 2019
     * [75]grow your data career at datasciencego, san diego, sep 27-29
     * [76]getting started with nlp using the pytorch framework
     * [77]how to diy your data science education
     * [78]top 8 data science use cases in gaming
     * [79]kdnuggets 19:n13, apr 3: top 10 data scientist coding mista...
     * [80]make better data-driven business decisions
     * [81]top stories, mar 25-31: r vs python for data visualization;
       th...
     * [82]two predictive analytics world events in europe this fall
     * [83]7 qualities your big data visualization tools absolutely must
       ...
     * [84]yeshiva university: tenure-track faculty in ai and machine
       lea...
     * [85]which face is real?
     * [86]yeshiva university: program director / tenure track faculty
       me...
     * [87]top 10 coding mistakes made by data scientists
     * [88]uber   s case study at paw industry 4.0: machine learning ...

   [89]kdnuggets home    [90]news    [91]2016    [92]nov    [93]tutorials,
   overviews    parallelism in machine learning: gpus, cuda, and practical
   applications ( [94]16:n40 )
      2019 kdnuggets. [95]about kdnuggets.  [96]privacy policy. [97]terms
   of service

   [98]subscribe to kdnuggets news
   [99][tw_c48.png] [100]facebook [101]linkedin
   x
   [envelope.png] [102]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2016/11/parallelism-machine-learning-gpu-cuda-threading.html/feed
   5. https://www.kdnuggets.com/jobs/16/11-09-sap-senior-data-scientist.html
   6. https://www.kdnuggets.com/2016/11/sap-transform-future-predictive-analytics.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2016/index.html
  28. https://www.kdnuggets.com/2016/11/index.html
  29. https://www.kdnuggets.com/2016/11/tutorials.html
  30. https://www.kdnuggets.com/2016/n40.html
  31. https://www.kdnuggets.com/jobs/16/11-09-sap-senior-data-scientist.html
  32. https://www.kdnuggets.com/2016/11/sap-transform-future-predictive-analytics.html
  33. https://www.kdnuggets.com/tag/algorithms
  34. https://www.kdnuggets.com/tag/cuda
  35. https://www.kdnuggets.com/tag/gpu
  36. https://www.kdnuggets.com/tag/nvidia
  37. https://www.kdnuggets.com/tag/parallelism
  38. https://www.kdnuggets.com/2016/11/parallelism-machine-learning-gpu-cuda-threading.html
  39. https://www.kdnuggets.com/author/matt-mayo
  40. http://www.sciencedirect.com/science/article/pii/s1877050914006024
  41. https://lambdafu.net/2013/08/07/replacing-native-code-with-cython/
  42. https://www.kdnuggets.com/2016/09/comparing-id91-techniques-concise-technical-overview.html
  43. https://www.kdnuggets.com/2016/09/support-vector-machines-concise-technical-overview.html
  44. https://www.kdnuggets.com/2016/09/data-science-basics-data-mining-statistics.html
  45. https://www.kdnuggets.com/2016/11/parallelism-machine-learning-gpu-cuda-threading.html
  46. https://www.kdnuggets.com/jobs/16/11-09-sap-senior-data-scientist.html
  47. https://www.kdnuggets.com/2016/11/sap-transform-future-predictive-analytics.html
  48. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  49. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  50. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  51. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  52. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  53. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  54. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  55. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  56. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  57. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  58. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  59. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  60. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  61. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  62. https://www.kdnuggets.com/news/index.html
  63. https://www.kdnuggets.com/2019/04/ai-makes-decision.html
  64. https://www.kdnuggets.com/2019/04/spatio-temporal-statistics-primer.html
  65. https://www.kdnuggets.com/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html
  66. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  67. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  68. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  69. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  70. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  71. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  72. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  73. https://www.kdnuggets.com/2019/04/odsc-east-selling-out-india-announced.html
  74. https://www.kdnuggets.com/jobs/19/04-03-accelerate-ai-data-science-career-expo-2019.html
  75. https://www.kdnuggets.com/2019/04/formulated-data-career-datasciencego-san-diego.html
  76. https://www.kdnuggets.com/2019/04/nlp-pytorch.html
  77. https://www.kdnuggets.com/2019/04/diy-your-data-science-education.html
  78. https://www.kdnuggets.com/2019/04/top-8-data-science-use-cases-gaming.html
  79. https://www.kdnuggets.com/2019/n13.html
  80. https://www.kdnuggets.com/2019/04/psu-make-better-data-driven-business-decisions.html
  81. https://www.kdnuggets.com/2019/04/top-news-week-0325-0331.html
  82. https://www.kdnuggets.com/2019/04/paw-two-predictive-analytics-world-events-europe-fall.html
  83. https://www.kdnuggets.com/2019/04/7-qualities-big-data-visualization-tools.html
  84. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-faculty-ai-machine-learning.html
  85. https://www.kdnuggets.com/2019/04/which-face-real-stylegan.html
  86. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-program-director-faculty-artificial-intelligence-machine-learning.html
  87. https://www.kdnuggets.com/2019/04/top-10-coding-mistakes-data-scientists.html
  88. https://www.kdnuggets.com/2019/04/paw-uber-case-study-machine-learning-enforce-mobile-performance.html
  89. https://www.kdnuggets.com/
  90. https://www.kdnuggets.com/news/index.html
  91. https://www.kdnuggets.com/2016/index.html
  92. https://www.kdnuggets.com/2016/11/index.html
  93. https://www.kdnuggets.com/2016/11/tutorials.html
  94. https://www.kdnuggets.com/2016/n40.html
  95. https://www.kdnuggets.com/about/index.html
  96. https://www.kdnuggets.com/news/privacy-policy.html
  97. https://www.kdnuggets.com/terms-of-service.html
  98. https://www.kdnuggets.com/news/subscribe.html
  99. https://twitter.com/kdnuggets
 100. https://facebook.com/kdnuggets
 101. https://www.linkedin.com/groups/54257
 102. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
 104. https://www.kdnuggets.com/
 105. https://www.kdnuggets.com/
