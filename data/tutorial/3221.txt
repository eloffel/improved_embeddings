   #[1]alternate

   (button) close

world class articles, delivered weekly.

   ____________________ (button) sign me up

   subscription implies consent to our [2]privacy policy

   thank you!
   check out your inbox to confirm your invite.
   (button)
   (button) not right now, thanks.

   [3]developers
   hiring? toptal handpicks [4]top machine learning engineers to suit your
   needs.
     * [5]top 3%
     * [6]why
     * [7]clients
     * [8]enterprise
     * [9]community
     * [10]blog
     * [11]about us

   (button)
     * [12]start hiring
     * [13]log in
     * [14]go to your profile

   [15]engineeringall blogs
   ____________________
   (button) icon close (button) search
   filter by
   (*) all ( ) engineering ( ) design ( ) finance ( ) projects ( ) product
   ( ) toptal insights
   (button) view all results
   engineering
   design
   finance
   projects
   product
   toptal insights

   [16]data science and databases
   21 minute read

a deep learning tutorial: from id88s to deep networks

   [17]ivan vasilev
   ivan is an enthusiastic senior developer with an entrepreneurial
   spirit. his primary focuses are in java, javascript and machine
   learning.
   share
     *
     *
     *
     *

   read the [18]spanish es version of this article translated by marisela
   ordaz

   in recent years, there   s been a resurgence in the field of artificial
   intelligence. it   s spread beyond the academic world with major players
   like [19]google, microsoft, and [20]facebook creating their own
   research teams and making some impressive [21]acquisitions.

   some this can be attributed to the abundance of raw data generated by
   social network users, much of which needs to be analyzed, the rise of
   advanced [22]data science solutions, as well as to the cheap
   computational power available via [23]gpgpus.

   but beyond these phenomena, this resurgence has been powered in no
   small part by a new trend in ai, specifically in [24]machine learning,
   known as    deep learning   . in this tutorial, i   ll introduce you to the
   key concepts and algorithms behind deep learning, beginning with the
   simplest unit of composition and building to the concepts of machine
   learning in java.

   (for full disclosure: i   m also the author of a java deep learning
   library, available [25]here, and the examples in this article are
   implemented using the above library. if you like it, you can support it
   by giving it a star on github, for which i would be grateful. usage
   instructions are available on the [26]homepage.)

a thirty second tutorial on machine learning

   in case you   re not familiar, check out this [27]introduction to machine
   learning:

   the general procedure is as follows:
    1. we have some algorithm that   s given a handful of labeled examples,
       say 10 images of dogs with the label 1 (   dog   ) and 10 images of
       other things with the label 0 (   not dog   )   note that we   re mainly
       sticking to [28]supervised, [29]binary classification for this
       post.
    2. the algorithm    learns    to identify images of dogs and, when fed a
       new image, hopes to produce the correct label (1 if it   s an image
       of a dog, and 0 otherwise).

   this setting is incredibly general: your data could be symptoms and
   your labels illnesses; or your data could be images of handwritten
   characters and your labels the actual characters they represent.

id88s: early deep learning algorithms

   one of the earliest supervised training algorithms is that of the
   id88, a basic neural network building block.

   say we have n points in the plane, labeled    0    and    1   . we   re given a
   new point and we want to guess its label (this is akin to the    dog    and
      not dog    scenario above). how do we do it?

   one approach might be to look at the closest neighbor and return that
   point   s label. but a slightly more intelligent way of going about it
   would be to pick a line that best separates the labeled data and use
   that as your classifier.

   a depiction of input data in relation to a linear classifier is a basic
   approach to deep learning.

   in this case, each piece of input data would be represented as a vector
   x = (x_1, x_2) and our function would be something like       0    if below
   the line,    1    if above   .

   to represent this mathematically, let our separator be defined by a
   vector of weights w and a vertical offset (or bias) b. then, our
   function would combine the inputs and weights with a weighted sum
   transfer function:

   weighted sum transfer function

   the result of this transfer function would then be fed into an
   activation function to produce a labeling. in the example above, our
   activation function was a threshold cutoff (e.g., 1 if greater than
   some value):

   result of this transfer function

training the id88

   the training of the id88 consists of feeding it multiple training
   samples and calculating the output for each of them. after each sample,
   the weights w are adjusted in such a way so as to minimize the output
   error, defined as the difference between the desired (target) and the
   actual outputs. there are other error functions, like the [30]mean
   square error, but the basic principle of training remains the same.

single id88 drawbacks

   the single id88 approach to deep learning has one major drawback:
   it can only learn [31]linearly separable functions. how major is this
   drawback? take xor, a relatively simple function, and notice that it
   can   t be classified by a linear separator (notice the failed attempt,
   below):

   the drawback to this deep learning approach is that some functions
   cannot be classified by a linear separator.

   to address this problem, we   ll need to use a multilayer id88,
   also known as feedforward neural network: in effect, we   ll compose a
   bunch of these id88s together to create a more powerful mechanism
   for learning.

feedforward neural networks for deep learning

   a neural network is really just a composition of id88s, connected
   in different ways and operating on different id180.

   feedforward neutral network deep learning is a more complex approach
   than single id88s.

   for starters, we   ll look at the feedforward neural network, which has
   the following properties:
     * an input, output, and one or more hidden layers. the figure above
       shows a network with a 3-unit input layer, 4-unit hidden layer and
       an output layer with 2 units (the terms units and neurons are
       interchangeable).
     * each unit is a single id88 like the one described above.
     * the units of the input layer serve as inputs for the units of the
       hidden layer, while the hidden layer units are inputs to the output
       layer.
     * each connection between two neurons has a weight w (similar to the
       id88 weights).
     * each unit of layer t is typically connected to every unit of the
       previous layer t - 1 (although you could disconnect them by setting
       their weight to 0).
     * to process input data, you    clamp    the input vector to the input
       layer, setting the values of the vector as    outputs    for each of
       the input units. in this particular case, the network can process a
       3-dimensional input vector (because of the 3 input units). for
       example, if your input vector is [7, 1, 2], then you   d set the
       output of the top input unit to 7, the middle unit to 1, and so on.
       these values are then propagated forward to the hidden units using
       the weighted sum transfer function for each hidden unit (hence the
       term forward propagation), which in turn calculate their outputs
       (activation function).
     * the output layer calculates it   s outputs in the same way as the
       hidden layer. the result of the output layer is the output of the
       network.

beyond linearity

   what if each of our id88s is only allowed to use a linear
   activation function? then, the final output of our network will still
   be some linear function of the inputs, just adjusted with a ton of
   different weights that it   s collected throughout the network. in other
   words, a linear composition of a bunch of linear functions is still
   just a linear function. if we   re restricted to linear activation
   functions, then the feedforward neural network is no more powerful than
   the id88, no matter how many layers it has.
   a linear composition of a bunch of linear functions is still just a
   linear function, so most neural networks use non-linear activation
   functions.

   because of this, most neural networks use non-linear activation
   functions like the [32]logistic, [33]tanh, [34]binary or [35]rectifier.
   without them the network can only learn functions which are [36]linear
   combinations of its inputs.

training id88s

   the most common deep learning algorithm for supervised training of the
   multilayer id88s is known as id26. the basic
   procedure:
    1. a training sample is presented and propagated forward through the
       network.
    2. the output error is calculated, typically the mean squared error:
       mean squared error
       where t is the target value and y is the actual network output.
       other error calculations are also acceptable, but the mse is a good
       choice.
    3. network error is minimized using a method called [37]stochastic
       id119. id119
       id119 is universal, but in the case of neural networks,
       this would be a graph of the training error as a function of the
       input parameters. the optimal value for each weight is that at
       which the error achieves a global minimum. during the training
       phase, the weights are updated in small steps (after each training
       sample or a mini-batch of several samples) in such a way that they
       are always trying to reach the global minimum   but this is no easy
       task, as you often end up in local minima, like the one on the
       right. for example, if the weight has a value of 0.6, it needs to
       be changed towards 0.4.
       this figure represents the simplest case, that in which error
       depends on a single parameter. however, network error depends on
       every network weight and the error function is much, much more
       complex.
       thankfully, id26 provides a method for updating each
       weight between two neurons with respect to the output error. the
       [38]derivation itself is quite complicated, but the weight update
       for a given node has the following (simple) form:
       example form
       where e is the output error, and w_i is the weight of input i to
       the neuron.
       essentially, the goal is to move in the direction of the gradient
       with respect to weight i. the key term is, of course, the
       derivative of the error, which isn   t always easy to calculate: how
       would you find this derivative for a random weight of a random
       hidden node in the middle of a large network?
       the answer: through id26. the errors are first
       calculated at the output units where the formula is quite simple
       (based on the difference between the target and predicted values),
       and then propagated back through the network in a clever fashion,
       allowing us to efficiently update our weights during training and
       (hopefully) reach a minimum.

hidden layer

   the hidden layer is of particular interest. by the [39]universal
   approximation theorem, a single hidden layer network with a finite
   number of neurons can be trained to approximate an arbitrarily random
   function. in other words, a single hidden layer is powerful enough to
   learn any function. that said, we often learn better in practice with
   multiple hidden layers (i.e., deeper nets).
   the hidden layer is where the network stores it's internal abstract
   representation of the training data.

   the hidden layer is where the network stores it   s internal abstract
   representation of the training data, similar to the way that a human
   brain (greatly simplified analogy) has an internal representation of
   the real world. going forward in the tutorial, we   ll look at different
   ways to play around with the hidden layer.

an example network

   you can see a simple (4-2-3 layer) feedforward neural network that
   classifies the [40]iris dataset implemented in java [41]here through
   the testmlpsigmoidbp method. the dataset contains three classes of iris
   plants with features like sepal length, petal length, etc. the network
   is provided 50 samples per class. the features are clamped to the input
   units, while each output unit corresponds to a single class of the
   dataset:    1/0/0    indicates that the plant is of class setosa,    0/1/0   
   indicates versicolour, and    0/0/1    indicates virginica). the
   classification error is 2/150 (i.e., it misclassifies 2 samples out of
   150).

the problem with large networks

   a neural network can have more than one hidden layer: in that case, the
   higher layers are    building    new abstractions on top of previous
   layers. and as we mentioned before, you can often learn better
   in-practice with larger networks.

   however, increasing the number of hidden layers leads to two known
   issues:
    1. [42]vanishing gradients: as we add more and more hidden layers,
       id26 becomes less and less useful in passing information
       to the lower layers. in effect, as information is passed back, the
       gradients begin to vanish and become small relative to the weights
       of the networks.
    2. [43]overfitting: perhaps the central problem in machine learning.
       briefly, overfitting describes the phenomenon of fitting the
       training data too closely, maybe with hypotheses that are too
       complex. in such a case, your learner ends up fitting the training
       data really well, but will perform much, much more poorly on real
       examples.

   let   s look at some deep learning algorithms to address these issues.

autoencoders

   most introductory machine learning classes tend to stop with
   feedforward neural networks. but the space of possible nets is far
   richer   so let   s continue.

   an autoencoder is typically a feedforward neural network which aims to
   learn a compressed, distributed representation (encoding) of a dataset.

   an autoencoder is a neural deep learning network that aims to learn a
   certain representation of a dataset.

   conceptually, the network is trained to    recreate    the input, i.e., the
   input and the target data are the same. in other words: you   re trying
   to output the same thing you were input, but compressed in some way.
   this is a confusing approach, so let   s look at an example.

compressing the input: grayscale images

   say that the training data consists of 28x28 grayscale images and the
   value of each pixel is clamped to one input layer neuron (i.e., the
   input layer will have 784 neurons). then, the output layer would have
   the same number of units (784) as the input layer and the target value
   for each output unit would be the grayscale value of one pixel of the
   image.

   the intuition behind this architecture is that the network will not
   learn a    mapping    between the training data and its labels, but will
   instead learn the internal structure and features of the data itself.
   (because of this, the hidden layer is also called feature detector.)
   usually, the number of hidden units is smaller than the input/output
   layers, which forces the network to learn only the most important
   features and achieves a id84.
   we want a few small nodes in the middle to learn the data at a
   conceptual level, producing a compact representation.

   in effect, we want a few small nodes in the middle to really learn the
   data at a conceptual level, producing a compact representation that in
   some way captures the core features of our input.

flu illness

   to further demonstrate autoencoders, let   s look at one more
   application.

   in this case, we   ll use a simple dataset consisting of flu symptoms
   (credit to this [44]blog post for the idea). if you   re interested, the
   code for this example can be found [45]in the testaeid26
   method.

   here   s how the data set breaks down:
     * there are six binary input features.
     * the first three are symptoms of the illness. for example, 1 0 0 0 0
       0 indicates that this patient has a high temperature, while 0 1 0 0
       0 0 indicates coughing, 1 1 0 0 0 0 indicates coughing and high
       temperature, etc.
     * the final three features are    counter    symptoms; when a patient has
       one of these, it   s less likely that he or she is sick. for example,
       0 0 0 1 0 0 indicates that this patient has a flu vaccine. it   s
       possible to have combinations of the two sets of features: 0 1 0 1
       0 0 indicates a vaccines patient with a cough, and so forth.

   we   ll consider a patient to be sick when he or she has at least two of
   the first three features and healthy if he or she has at least two of
   the second three (with ties breaking in favor of the healthy patients),
   e.g.:
     * 111000, 101000, 110000, 011000, 011100 = sick
     * 000111, 001110, 000101, 000011, 000110 = healthy

   we   ll train an autoencoder (using id26) with six input and
   six output units, but only two hidden units.

   after several hundred iterations, we observe that when each of the
      sick    samples is presented to the machine learning network, one of the
   two the hidden units (the same unit for each    sick    sample) always
   exhibits a higher activation value than the other. on the contrary,
   when a    healthy    sample is presented, the other hidden unit has a
   higher activation.

going back to machine learning

   essentially, our two hidden units have learned a compact representation
   of the flu symptom data set. to see how this relates to learning, we
   return to the problem of overfitting. by training our net to learn a
   compact representation of the data, we   re favoring a simpler
   representation rather than a highly complex hypothesis that overfits
   the training data.

   in a way, by favoring these simpler representations, we   re attempting
   to learn the data in a truer sense.

restricted id82s

   the next logical step is to look at a [46]restricted id82s
   (rbm), a generative stochastic neural network that can learn a
   id203 distribution over its set of inputs.

   in machine learning, restricted botlzmann machines are composed of
   visible and hidden units.

   rbms are composed of a hidden, visible, and bias layer. unlike the
   feedforward networks, the connections between the visible and hidden
   layers are undirected (the values can be propagated in both the
   visible-to-hidden and hidden-to-visible directions) and fully connected
   (each unit from a given layer is connected to each unit in the next   if
   we allowed any unit in any layer to connect to any other layer, then
   we   d have a boltzmann (rather than a restricted boltzmann) machine).

   the standard rbm has binary hidden and visible units: that is, the unit
   activation is 0 or 1 under a [47]bernoulli distribution, but there are
   variants with other [48]non-linearities.

   while researchers have known about rbms for some time now, the recent
   introduction of the [49]contrastive divergence unsupervised training
   algorithm has renewed interest.

contrastive divergence

   the single-step contrastive divergence algorithm (cd-1) works like
   this:
    1. positive phase:
          + an input sample v is clamped to the input layer.
          + v is propagated to the hidden layer in a similar manner to the
            feedforward networks. the result of the hidden layer
            activations is h.
    2. negative phase:
          + propagate h back to the visible layer with result v    (the
            connections between the visible and hidden layers are
            undirected and thus allow movement in both directions).
          + propagate the new v    back to the hidden layer with activations
            result h   .
    3. weight update:
       weight update
       where a is the learning rate and v, v   , h, h   , and w are vectors.

   the intuition behind the algorithm is that the positive phase (h given
   v) reflects the network   s internal representation of the real world
   data. meanwhile, the negative phase represents an attempt to recreate
   the data based on this internal representation (v    given h). the main
   goal is for the generated data to be as close as possible to the real
   world and this is reflected in the weight update formula.

   in other words, the net has some perception of how the input data can
   be represented, so it tries to reproduce the data based on this
   perception. if its reproduction isn   t close enough to reality, it makes
   an adjustment and tries again.

returning to the flu

   to demonstrate contrastive divergence, we   ll use the same symptoms data
   set as before. the test network is an rbm with six visible and two
   hidden units. we   ll train the network using contrastive divergence with
   the symptoms v clamped to the visible layer. during testing, the
   symptoms are again presented to the visible layer; then, the data is
   propagated to the hidden layer. the hidden units represent the
   sick/healthy state, a very similar architecture to the autoencoder
   (propagating data from the visible to the hidden layer).

   after several hundred iterations, we can observe the same result as
   with autoencoders: one of the hidden units has a higher activation
   value when any of the    sick    samples is presented, while the other is
   always more active for the    healthy    samples.

   you can see this example in action [50]in the testcontrastivedivergence
   method.

deep networks

   we   ve now demonstrated that the hidden layers of autoencoders and rbms
   act as effective feature detectors; but it   s rare that we can use these
   features directly. in fact, the data set above is more an exception
   than a rule. instead, we need to find some way to use these detected
   features indirectly.

   luckily, [51]it was discovered that these structures can be stacked to
   form deep networks. these networks can be trained greedily, one layer
   at a time, to help to overcome the vanishing gradient and overfitting
   problems associated with classic id26.

   the resulting structures are often quite powerful, producing impressive
   results. take, for example, google   s famous [52]   cat    paper in which
   they use special kind of deep autoencoders to    learn    human and cat
   face detection based on unlabeled data.

   let   s take a closer look.

stacked autoencoders

   as the name suggests, this network consists of multiple stacked
   autoencoders.

   stacked autoencoders have a series of inputs, outputs, and hidden
   layers that contribute to machine learning outcomes.

   the hidden layer of autoencoder t acts as an input layer to autoencoder
   t + 1. the input layer of the first autoencoder is the input layer for
   the whole network. the greedy layer-wise training procedure works like
   this:
    1. train the first autoencoder (t=1, or the red connections in the
       figure above, but with an additional output layer) individually
       using the id26 method with all available training data.
    2. train the second autoencoder t=2 (green connections). since the
       input layer for t=2 is the hidden layer of t=1 we are no longer
       interested in the output layer of t=1 and we remove it from the
       network. training begins by clamping an input sample to the input
       layer of t=1, which is propagated forward to the output layer of
       t=2. next, the weights (input-hidden and hidden-output) of t=2 are
       updated using id26. t=2 uses all the training samples,
       similar to t=1.
    3. repeat the previous procedure for all the layers (i.e., remove the
       output layer of the previous autoencoder, replace it with yet
       another autoencoder, and train with back propagation).
    4. steps 1-3 are called pre-training and leave the weights properly
       initialized. however, there   s no mapping between the input data and
       the output labels. for example, if the network is trained to
       recognize images of handwritten digits it   s still not possible to
       map the units from the last feature detector (i.e., the hidden
       layer of the last autoencoder) to the digit type of the image. in
       that case, the most common solution is to add one or more fully
       connected layer(s) to the last layer (blue connections). the whole
       network can now be viewed as a multilayer id88 and is trained
       using id26 (this step is also called fine-tuning).

   stacked auto encoders, then, are all about providing an effective
   pre-training method for initializing the weights of a network, leaving
   you with a complex, multi-layer id88 that   s ready to train (or
   fine-tune).

id50

   as with autoencoders, we can also stack id82s to create a
   class known as id50 (dbns).

   id50 are comprised of a stack of id82s.

   in this case, the hidden layer of rbm t acts as a visible layer for rbm
   t+1. the input layer of the first rbm is the input layer for the whole
   network, and the greedy layer-wise pre-training works like this:
    1. train the first rbm t=1 using contrastive divergence with all the
       training samples.
    2. train the second rbm t=2. since the visible layer for t=2 is the
       hidden layer of t=1, training begins by clamping the input sample
       to the visible layer of t=1, which is propagated forward to the
       hidden layer of t=1. this data then serves to initiate contrastive
       divergence training for t=2.
    3. repeat the previous procedure for all the layers.
    4. similar to the stacked autoencoders, after pre-training the network
       can be extended by connecting one or more fully connected layers to
       the final rbm hidden layer. this forms a multi-layer id88
       which can then be fine tuned using id26.

   this procedure is akin to that of stacked autoencoders, but with the
   autoencoders replaced by rbms and id26 replaced with the
   contrastive divergence algorithm.

   (note: for more on constructing and training stacked autoencoders or
   id50, check out the sample code [53]here.)

convolutional networks

   as a final deep learning architecture, let   s take a look at
   convolutional networks, a particularly interesting and special class of
   feedforward networks that are very well-suited to image recognition.

   convolutional networks are a special class of deep learning feedforward
   networks. image via [54]deeplearning.net

   before we look at the actual structure of convolutional networks, we
   first define an image filter, or a square region with associated
   weights. a filter is applied across an entire input image, and you will
   often apply multiple filters. for example, you could apply four 6x6
   filters to a given input image. then, the output pixel with coordinates
   1,1 is the weighted sum of a 6x6 square of input pixels with top left
   corner 1,1 and the weights of the filter (which is also 6x6 square).
   output pixel 2,1 is the result of input square with top left corner 2,1
   and so on.

   with that covered, these networks are defined by the following
   properties:
     * convolutional layers apply a number of filters to the input. for
       example, the first convolutional layer of the image could have four
       6x6 filters. the result of one filter applied across the image is
       called feature map (fm) and the number feature maps is equal to the
       number of filters. if the previous layer is also convolutional, the
       filters are applied across all of it   s fms with different weights,
       so each input fm is connected to each output fm. the intuition
       behind the shared weights across the image is that the features
       will be detected regardless of their location, while the
       multiplicity of filters allows each of them to detect different set
       of features.
     * subsampling layers reduce the size of the input. for example, if
       the input consists of a 32x32 image and the layer has a subsampling
       region of 2x2, the output value would be a 16x16 image, which means
       that 4 pixels (each 2x2 square) of the input image are combined
       into a single output pixel. there are multiple ways to subsample,
       but the most popular are [55]max pooling, [56]average pooling, and
       [57]stochastic pooling.
     * the last subsampling (or convolutional) layer is usually connected
       to one or more fully connected layers, the last of which represents
       the target data.
     * training is performed using modified id26 that takes the
       subsampling layers into account and updates the convolutional
       filter weights based on all values to which that filter is applied.

   you can see several examples of convolutional networks trained (with
   id26) on the [58]mnist data set (grayscale images of
   handwritten letters) [59]here, specifically in the the testlenet*
   methods (i would recommend testlenettiny2 as it achieves a low error
   rate of about 2% in a relatively short period of time). there   s also a
   nice javascript visualization of a similar network [60]here.

implementation

   now that we   ve covered the most common neural network variants, i
   thought i   d write a bit about the challenges posed during
   implementation of these deep learning structures.

   broadly speaking, my goal in creating a [61]deep learning library was
   (and still is) to build a neural network-based framework that satisfied
   the following criteria:
     * a common architecture that is able to represent diverse models (all
       the variants on neural networks that we   ve seen above, for
       example).
     * the ability to use diverse training algorithms (back propagation,
       contrastive divergence, etc.).
     * decent performance.

   to satisfy these requirements, i took a tiered (or modular) approach to
   the design of the software.

structure

   let   s start with the basics:
     * [62]neuralnetworkimpl is the base class for all neural network
       models.
     * each network contains a set of [63]layers.
     * each layer has a list of [64]connections, where a connection is a
       link between two layers such that the network is a directed acyclic
       graph.

   this structure is agile enough to be used for classic feedforward
   networks, as well as for [65]rbms and more complex architectures like
   [66]id163.

   it also allows a layer to be part of more than one network. for
   example, the layers in a [67]deep belief network are also layers in
   their corresponding rbms.

   in addition, this architecture allows a dbn to be viewed as a list of
   stacked rbms during the pre-training phase and a feedforward network
   during the fine-tuning phase, which is both intuitively nice and
   programmatically convenient.

data propagation

   the next module takes care of propagating data through the network, a
   two-step process:
    1. determine the order of the layers. for example, to get the results
       from a multilayer id88, the data is    clamped    to the input
       layer (hence, this is the first layer to be calculated) and
       propagated all the way to the output layer. in order to update the
       weights during id26, the output error has to be
       propagated through every layer in breadth-first order, starting
       from the output layer. this is achieved using various
       implementations of [68]layerorderstrategy, which takes advantage of
       the graph structure of the network, employing different graph
       traversal methods. some examples include the [69]breadth-first
       strategy and the [70]targeting of a specific layer. the order is
       actually determined by the connections between the layers, so the
       strategies return an ordered list of connections.
    2. calculate the activation value. each layer has an associated
       [71]connectioncalculator which takes it   s list of connections (from
       the previous step) and input values (from other layers) and
       calculates the resulting activation. for example, in a simple
       sigmoidal feedforward network, the hidden layer   s
       connectioncalculator takes the values of the input and bias layers
       (which are, respectively, the input data and an array of 1s) and
       the weights between the units (in case of fully connected layers,
       the weights are actually stored in a [72]fullyconnected connection
       as a matrix), calculates the weighted sum, and feeds the result
       into the sigmoid function. the connection calculators implement a
       variety of transfer (e.g., weighted sum, convolutional) and
       activation (e.g., logistic and tanh for multilayer id88,
       binary for rbm) functions. most of them can be executed on a gpu
       using [73]aparapi and usable with mini-batch training.

gpu computation with aparapi

   as i mentioned earlier, one of the reasons that neural networks have
   made a resurgence in recent years is that their training methods are
   highly conducive to parallelism, allowing you to speed up training
   significantly with the use of a gpgpu. in this case, i chose to work
   with the [74]aparapi library to add gpu support.

   aparapi imposes some important restrictions on the connection
   calculators:
     * only one-dimensional arrays (and variables) of primitive data types
       are allowed.
     * only member-methods of the aparapi kernel class itself are allowed
       to be called from the gpu executable code.

   as such, most of the data (weights, input, and output arrays) is stored
   in matrix instances, which use one-dimensional float arrays internally.
   all aparapi connection calculators use either [75]aparapiweightedsum
   (for fully connected layers and weighted sum input functions),
   [76]aparapisubsampling2d (for subsampling layers), or [77]aparapiconv2d
   (for convolutional layers). some of these limitations can be overcome
   with the introduction of [78]heterogeneous system architecture. aparapi
   also allows to run the same code on both cpu and gpu.

training

   the [79]training module implements various training algorithms. it
   relies on the previous two modules. for example,
   [80]id26trainer (all the trainers are using the [81]trainer
   base class) uses feedforward layer calculator for the feedforward phase
   and a special breadth-first layer calculator for propagating the error
   and updating the weights.

   my latest work is on java 8 support and some other improvements, will
   soon be merged into [82]master.

conclusion

   the aim of this java deep learning tutorial was to give you a brief
   introduction to the field of deep learning algorithms, beginning with
   the most basic unit of composition (the id88) and progressing
   through various effective and popular architectures, like that of the
   restricted id82.

   the ideas behind neural networks have been around for a long time; but
   today, you can   t step foot in the machine learning community without
   hearing about deep networks or some other take on deep learning. hype
   shouldn   t be mistaken for justification, but with the advances of gpgpu
   computing and the impressive progress made by researchers like geoffrey
   hinton, yoshua bengio, yann lecun and andrew ng, the field certainly
   shows a lot of promise. there   s no better time to get familiar and get
   involved like the present.

appendix: resources

   if you   re interested in learning more, i found the following resources
   quite helpful during my work:
     * [83]deeplearning.net: a portal for all things deep learning. it has
       some nice [84]tutorials, [85]software library and a great
       [86]reading list.
     * an active [87]google+ community.
     * two very good courses: [88]machine learning and [89]neural networks
       for machine learning, both offered on coursera.
     * the [90]stanford neural networks tutorial.

   related: [91]schooling flappy bird: a id23 tutorial

tags

   [92]java[93]machinelearning[94]deeplearning

   ivan vasilev

   java developer

about the author

   ivan is an enthusiastic software engineer and machine learning
   researcher. his experiences range across a number of fields and
   technologies, but his primary focuses are in deep learning, python, and
   java.
   [95]hire ivan

comments

   beeharry shiam
   very nice article and clearly explained. keep it up. ivan
   ron barak
   erratum: "some this can be attributed"
   francisco claria
   superb article ivan!!! i've always been atracted to these subjects and
   self organizing maps, i hope you have some spare time soon to write an
   article with applied exercices to real world situations like clasify
   people based on their facebook public profile interests ;)
   vojimir golem
   great article, looking forward to dive in into your source code!
   random
   excellent write up!
   berkus
   great article!
   royi
   amazing introduction. bravo!! tell me, how did you create those
   beautiful diagrams?
   photo shop
   i am saying thanks for your information............ <a
   href="http://www.imeshlab.com/networking.im">networking training in
   chandigarh</a>
   vijay ram
   excellent article
   matic d.b.
   nice article. i really liked it reading. i have on comment though ...
   in section feedforward neural networks you mentioned that example
   network can process 3-dimensional input vector. i think you should
   write you can proces vector with length 3, because vectors are 1d. it's
   probably just a typo;)
   venkatanaresh
   thanks a lot...its useful to many...the best tutorial....:)
   prasad gabbur
   this is probably the most succinct and comprehensive introduction to
   deep learning i have come across, giving me the confidence and
   curiosity to dig into the math and code. great job!
   heng huang
   great tutorial
   rodrigo alves
   this article is certainly a reference to the topic. sure it is
   something i'm falling back to whenever i really need to go deep into
   the field. thanks
   jinhan kim
   this is the best article i have read for deep learning recent 2 months.
   great work! thank you.
   pay to do my essay
   this type of learning looks helpful and good for those people who
   wanted to improve their knowledge about programming. it can help them
   in terms of discovering many new ways on how to develop a certain
   website or program by that kind of thing.
   berkay celik
   great explanation, thank you. if you may give some examples along with
   the code, it may be more useful for us.
   smindler
   this is a fantastic article.
   serban balamaci
   loved the easy explanation in the article. you have a nack for
   explaining complex things and making the simple. thanks !
   ghulam gilanie
   too much informative article
   distroslv34
   really good article. i would love to get some results about the
   performance with real problems, especially for id98
   born to sing paras
   i need simple implementation of back propagation neural network where i
   can put the training input samples and corresponding output and the
   system should recognize the out while testing with tests data. please
   help me. my email is : [96][email protected]
   godar stalli
   grande articolo! great article!
   disqus_gxzeszhdhn
   the hidden layer is where the network stores *its* internal abstract
   representation of the training data.
   liang
   thank you for sharing this
   anil singh
   awesome post. thank you http://www.code-sample.com/
   lipika dey
   excellent article!
   sivampillai
   really nice work! the way you have covered the concepts from the very
   basic makes it so much easier for a newcomer like me! thanks a lot for
   your effort!
   humoyun ahmedov
   awesome article, well done brother
   chenyd5
   amazing post. thanks a lot!
   vasiliu mirel
   congratulations, a great article!
   gary frost
   a great article. i am the original inventor/coder/designer of aparapi.
   great to see it being used for deep learning.
   jeremy orange
   my business partners needed to fill out a form several days ago and
   found a website that has lots of sample forms . if others have been
   needing it too , here's <code>https://goo.gl/qsciwf</code>.
   muhammad magdi
   what a good gist ! :))
   leapoahead
   is rbm unsupervised learning?
   moobly
   thanks very much for writing this - this is the best concise intro to
   this subject that i've come across. you should print up some t-shirts
   ;)
   deepak
   superb article. i have one question here. we know the input and output
   layers with number of nodes but how would we decide on number of hidden
   layers and number nodes in the hidden layers.
   tanaji khadtare
   superb !!! great simplification !! good flow of thoughts !
   muhammad usman
   amazing explanation...
   acelin angela
   nice content <a href="http://bandarq.mobi/">situs bandarq</a> <a
   href="http://situsbandarqq.com/">situs bandarq</a> <a
   href="http://situstogelsingapore.com/">situs togel singapore</a>
   please enable javascript to view the [97]comments powered by
   disqus.[98]comments powered by disqus

world class articles, delivered weekly.

   ____________________ (button) get great content

   subscription implies consent to our [99]privacy policy

   thank you!
   check out your inbox to confirm your invite.
   (button)

trending articles

   [100]engineering
   [101]web front-end

[102]react tutorial: components, hooks, and performance

   [103]engineering
   [104]web front-end

[105]guide to monorepos for front-end code

   [106]engineering
   [107]technology

[108]working with google sheets and apps script

   [109]engineering
   [110]web front-end

[111]working with the react context api

see our related talents

   [112]machine learning[113]java[114]data science

hire the author

   ivan vasilev

   java developer

read next

   [115]engineering
   [116]back-end

[117]do the math: scaling microservices applications with orchestrators

world class articles, delivered weekly.

   ____________________ (button) sign me up

   subscription implies consent to our [118]privacy policy

   thank you!
   check out your inbox to confirm your invite.
   (button)

world class articles, delivered weekly.

   ____________________ (button) sign me up

   subscription implies consent to our [119]privacy policy

   thank you!
   check out your inbox to confirm your invite.
   (button)

toptal developers

     * [120]android developers
     * [121]angularjs developers
     * [122]back-end developers
     * [123]c++ developers
     * [124]data analysts
     * [125]data engineers
     * [126]data scientists
     * [127]devops engineers
     * [128]ember.js developers
     * [129]freelance developers
     * [130]front-end developers
     * [131]full-stack developers
     * [132]html5 developers
     * [133]ios developers
     * [134]java developers
     * [135]javascript developers
     * [136]machine learning engineers
     * [137]magento developers
     * [138]mixed reality developers
     * [139]mobile app developers
     * [140].net developers
     * [141]node.js developers
     * [142]php developers
     * [143]python developers
     * [144]react.js developers
     * [145]ruby developers
     * [146]ruby on rails developers
     * [147]salesforce developers
     * [148]scala developers
     * [149]software architects
     * [150]software developers
     * [151]unity or unity3d developers
     * [152]virtual reality developers
     * [153]web developers
     * [154]wordpress developers
     * [155]view more

join the toptal^   community.

   [156]hire a developeror[157]apply as a developer

most in-demand talent

     * [158]ios developers
     * [159]front-end developers
     * [160]ux designers
     * [161]ui designers
     * [162]financial modeling consultants
     * [163]interim cfos

about

     * [164]top 3%
     * [165]clients
     * [166]freelance developers
     * [167]freelance designers
     * [168]freelance finance experts
     * [169]freelance project managers
     * [170]about us

contact

     * [171]contact us
     * [172]press center
     * [173]careers
     * [174]faq

social

     *
     *
     *
     *

   hire the top 3% of freelance talent
   copyright 2010 - 2019 toptal, llc
     * [175]privacy policy
     * [176]website terms

   [tr?id=463369723801939&amp;ev=viewcontent&amp;noscript=1]

   iframe:
   [177]https://www.attributiontracker.com/tracker/account_visit?tt_visit=
   1

references

   visible links
   1. https://www.toptal.com/machine-learning/un-tutorial-de-aprendizaje-profundo-de-id88es-a-redes-profundas
   2. https://www.toptal.com/privacy
   3. https://www.toptal.com/developers
   4. https://www.toptal.com/machine-learning
   5. https://www.toptal.com/top-3-percent
   6. https://www.toptal.com/why
   7. https://www.toptal.com/clients
   8. https://www.toptal.com/enterprise
   9. https://www.toptal.com/community
  10. https://www.toptal.com/blog
  11. https://www.toptal.com/about
  12. https://www.toptal.com/machine-learning
  13. https://www.toptal.com/users/login
  14. https://www.toptal.com/secure_redirect/default
  15. https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-id88s-to-deep-networks
  16. https://www.toptal.com/developers/blog/data-science-and-databases
  17. https://www.toptal.com/resume/ivan-vasilev
  18. https://www.toptal.com/machine-learning/un-tutorial-de-aprendizaje-profundo-de-id88es-a-redes-profundas
  19. http://www.wired.com/wiredenterprise/2013/03/google_hinton/
  20. http://techcrunch.com/2013/12/09/facebook-artificial-intelligence-lab-lecun/
  21. http://www.theverge.com/2014/1/26/5348640/google-deepmind-acquisition-robotics-ai
  22. https://www.toptal.com/services/data-science
  23. https://en.wikipedia.org/wiki/general-purpose_computing_on_graphics_processing_units
  24. https://www.toptal.com/machine-learning
  25. https://github.com/ivan-vasilev/neuralnetworks
  26. https://github.com/ivan-vasilev/neuralnetworks
  27. https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer
  28. https://en.wikipedia.org/wiki/supervised_learning
  29. https://en.wikipedia.org/wiki/binary_classification
  30. https://en.wikipedia.org/wiki/mean_squared_error
  31. https://en.wikipedia.org/wiki/linearly_separable
  32. https://en.wikipedia.org/wiki/sigmoid_function
  33. https://en.wikipedia.org/wiki/tanh
  34. https://en.wikipedia.org/wiki/artificial_neuron#step_function
  35. https://en.wikipedia.org/wiki/rectified_linear_unit
  36. http://stackoverflow.com/questions/11677508/why-do-sigmoid-functions-work-in-neural-nets
  37. https://en.wikipedia.org/wiki/stochastic_gradient_descent
  38. https://en.wikipedia.org/wiki/id26#derivation
  39. https://en.wikipedia.org/wiki/universal_approximation_theorem
  40. http://archive.ics.uci.edu/ml/datasets/iris
  41. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-samples/src/test/java/com/github/neuralnetworks/samples/test/iristest.java
  42. http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf
  43. https://en.wikipedia.org/wiki/overfitting
  44. http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/
  45. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/test/java/com/github/neuralnetworks/test/aetest.java
  46. https://en.wikipedia.org/wiki/restricted_boltzmann_machine
  47. https://en.wikipedia.org/wiki/bernoulli_distribution
  48. http://www.cs.toronto.edu/~hinton/absps/guidetr.pdf
  49. http://deeplearning.net/tutorial/rbm.html#id1
  50. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/test/java/com/github/neuralnetworks/architecture/types/rbmtest.java
  51. http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf
  52. http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/38115.pdf
  53. https://github.com/ivan-vasilev/neuralnetworks/blob/d2bbc296eca926d07d09b860b29c5a5a3f632f63/nn-core/src/test/java/com/github/neuralnetworks/test/dnntest.java
  54. http://deeplearning.net/
  55. http://deeplearning.net/tutorial/lenet.html#maxpooling
  56. http://ufldl.stanford.edu/wiki/index.php/ufldl_tutorial
  57. http://techtalks.tv/talks/stochastic-pooling-for-id173-of-deep-convolutional-neural-networks/58106/
  58. https://en.wikipedia.org/wiki/mnist_database
  59. https://github.com/ivan-vasilev/neuralnetworks/blob/9e569aa7c9a4d724cf3c1aed8a8036af272ec58f/nn-samples/src/test/java/com/github/neuralnetworks/samples/test/mnisttest.java
  60. http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html
  61. https://github.com/ivan-vasilev/neuralnetworks
  62. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/architecture/neuralnetworkimpl.java
  63. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/architecture/layer.java
  64. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/architecture/connectionsimpl.java
  65. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/architecture/types/rbm.java
  66. http://www.cs.toronto.edu/~hinton/absps/id163.pdf
  67. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/architecture/types/dbn.java
  68. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/calculation/layerorderstrategy.java
  69. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/calculation/breadthfirstorderstrategy.java
  70. https://github.com/ivan-vasilev/neuralnetworks/blob/a7314674cca70f705b6d993eed8ce567ea1832b9/nn-core/src/main/java/com/github/neuralnetworks/calculation/targetlayerorderstrategy.java
  71. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/calculation/connectioncalculator.java
  72. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/architecture/fullyconnected.java
  73. https://code.google.com/p/aparapi/
  74. https://code.google.com/p/aparapi/
  75. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/calculation/operations/aparapi/aparapiweightedsum.java
  76. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/calculation/operations/aparapi/aparapisubsampling2d.java
  77. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/calculation/operations/aparapi/aparapiconv2d.java
  78. http://developer.amd.com/resources/heterogeneous-computing/what-is-heterogeneous-system-architecture-hsa/
  79. https://github.com/ivan-vasilev/neuralnetworks/tree/master/nn-core/src/main/java/com/github/neuralnetworks/training
  80. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/training/id26/id26trainer.java
  81. https://github.com/ivan-vasilev/neuralnetworks/blob/master/nn-core/src/main/java/com/github/neuralnetworks/training/trainer.java
  82. https://github.com/ivan-vasilev/neuralnetworks
  83. http://deeplearning.net/
  84. http://deeplearning.net/reading-list/tutorials/
  85. http://deeplearning.net/software_links/
  86. http://deeplearning.net/reading-list/
  87. https://plus.google.com/u/0/communities/112866381580457264725
  88. https://www.coursera.org/course/ml
  89. https://www.coursera.org/course/neuralnets
  90. http://ufldl.stanford.edu/wiki/index.php/ufldl_tutorial
  91. https://www.toptal.com/deep-learning/pytorch-reinforcement-learning-tutorial
  92. https://www.toptal.com/developers/blog/tags/java
  93. https://www.toptal.com/developers/blog/tags/machinelearning
  94. https://www.toptal.com/developers/blog/tags/deeplearning
  95. https://www.toptal.com/hire?interested_in=developers&skill=machine-learning
  96. https://www.toptal.com/cdn-cgi/l/email-protection
  97. https://disqus.com/?ref_noscript
  98. https://disqus.com/
  99. https://www.toptal.com/privacy
 100. https://www.toptal.com/developers/blog
 101. https://www.toptal.com/developers/blog/web-front-end
 102. https://www.toptal.com/react/react-tutorial-pt2
 103. https://www.toptal.com/developers/blog
 104. https://www.toptal.com/developers/blog/web-front-end
 105. https://www.toptal.com/front-end/guide-to-monorepos
 106. https://www.toptal.com/developers/blog
 107. https://www.toptal.com/developers/blog/technology
 108. https://www.toptal.com/google-docs/extending-google-sheets-app-scripts
 109. https://www.toptal.com/developers/blog
 110. https://www.toptal.com/developers/blog/web-front-end
 111. https://www.toptal.com/react/react-context-api
 112. https://www.toptal.com/machine-learning
 113. https://www.toptal.com/java
 114. https://www.toptal.com/data-science
 115. https://www.toptal.com/developers/blog
 116. https://www.toptal.com/developers/blog/back-end
 117. https://www.toptal.com/devops/scaling-microservices-applications
 118. https://www.toptal.com/privacy
 119. https://www.toptal.com/privacy
 120. https://www.toptal.com/android
 121. https://www.toptal.com/angular-js
 122. https://www.toptal.com/back-end
 123. https://www.toptal.com/c-plus-plus
 124. https://www.toptal.com/data-analysis
 125. https://www.toptal.com/data-engineer
 126. https://www.toptal.com/data-science
 127. https://www.toptal.com/devops
 128. https://www.toptal.com/emberjs
 129. https://www.toptal.com/freelance
 130. https://www.toptal.com/front-end
 131. https://www.toptal.com/full-stack
 132. https://www.toptal.com/html5
 133. https://www.toptal.com/ios
 134. https://www.toptal.com/java
 135. https://www.toptal.com/javascript
 136. https://www.toptal.com/machine-learning
 137. https://www.toptal.com/magento
 138. https://www.toptal.com/mixed-reality
 139. https://www.toptal.com/app
 140. https://www.toptal.com/dot-net
 141. https://www.toptal.com/nodejs
 142. https://www.toptal.com/php
 143. https://www.toptal.com/python
 144. https://www.toptal.com/react
 145. https://www.toptal.com/ruby
 146. https://www.toptal.com/ruby-on-rails
 147. https://www.toptal.com/salesforce
 148. https://www.toptal.com/scala
 149. https://www.toptal.com/software-architect
 150. https://www.toptal.com/software
 151. https://www.toptal.com/unity-unity3d
 152. https://www.toptal.com/virtual-reality
 153. https://www.toptal.com/web
 154. https://www.toptal.com/wordpress
 155. https://www.toptal.com/developers/all
 156. https://www.toptal.com/hire?interested_in=developers
 157. https://www.toptal.com/developers/join
 158. https://www.toptal.com/ios
 159. https://www.toptal.com/front-end
 160. https://www.toptal.com/designers/ux
 161. https://www.toptal.com/designers/ui
 162. https://www.toptal.com/finance/financial-modeling
 163. https://www.toptal.com/finance/interim-cfos
 164. https://www.toptal.com/top-3-percent
 165. https://www.toptal.com/clients
 166. https://www.toptal.com/developers
 167. https://www.toptal.com/designers
 168. https://www.toptal.com/finance
 169. https://www.toptal.com/project-managers
 170. https://www.toptal.com/about
 171. https://www.toptal.com/contact
 172. https://www.toptal.com/press-center
 173. https://www.toptal.com/careers
 174. https://www.toptal.com/faq
 175. https://www.toptal.com/privacy
 176. https://www.toptal.com/tos
 177. https://www.attributiontracker.com/tracker/account_visit?tt_visit=1

   hidden links:
 179. https://www.toptal.com/developers/blog
 180. https://www.toptal.com/designers/blog
 181. https://www.toptal.com/finance/blog
 182. https://www.toptal.com/project-managers/blog
 183. https://www.toptal.com/product-managers/blog
 184. https://www.toptal.com/insights
 185. https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-id88s-to-deep-networks
 186. https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-id88s-to-deep-networks
 187. https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-id88s-to-deep-networks
 188. https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-id88s-to-deep-networks
 189. https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-id88s-to-deep-networks#blog_post-article-author
 190. https://www.toptal.com/devops/scaling-microservices-applications
 191. https://www.linkedin.com/company/toptal
 192. https://twitter.com/toptal
 193. https://www.facebook.com/toptal
 194. https://www.instagram.com/toptal/
 195. https://www.toptal.com/
