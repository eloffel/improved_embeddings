   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

big picture machine learning: classifying text with neural networks and
tensorflow

   [11]go to the profile of d  borah mesquita
   [12]d  borah mesquita (button) blockedunblock (button) followfollowing
   apr 9, 2017
   [1*w2vzgrxr1ua5kn-x0m9omw.jpeg]

   developers often say that if you want to get started with machine
   learning, you should first learn how the algorithms work. but my
   experience shows otherwise.

   i say you should first be able to see the big picture: how the
   applications work. once you understand this, it becomes much easier to
   dive in deep and explore the inner workings of the algorithms.

   so how do you [13]develop an intuition and achieve this big-picture
   understanding of machine learning? a good way to do this is by creating
   machine learning models.

   assuming you still don   t know how to create all these algorithms from
   scratch, you   ll want to use a library that has all these algorithms
   already implemented for you. and that library is tensorflow.

   in this article, we   ll create a machine learning model to classify
   texts into categories. we   ll cover the following topics:
    1. how tensorflow works
    2. what is a machine learning model
    3. what is a neural network
    4. how the neural network learns
    5. how to manipulate data and pass it to the neural network inputs
    6. how to run the model and get the prediction results

   you will probably learn a lot of new things, so let   s start!     

tensorflow

   [14]tensorflow is an open-source library for machine learning, first
   created by google. the name of the library help us understand how we
   work with it: tensors are multidimensional arrays that flow through the
   nodes of a graph.

tf.graph

   every computation in tensorflow is represented as a dataflow graph.
   this graph has two elements:
     * a set of tf.operation, that represents units of computation
     * a set of tf.tensor, that represents units of data

   to see how all this works you will create this dataflow graph:
   [1*hgu9p-k0xnhvldmdhxtprg.png]
   a graph that computes x+y

   you   ll define x = [1,3,6] and y = [1,1,1]. as the graph works with
   tf.tensor to represent units of data, you will create constant tensors:
import tensorflow as tf
x = tf.constant([1,3,6])
y = tf.constant([1,1,1])

   now you   ll define the operation unit:
import tensorflow as tf
x = tf.constant([1,3,6])
y = tf.constant([1,1,1])
op = tf.add(x,y)

   you have all the graph elements. now you need to build the graph:
import tensorflow as tf
my_graph = tf.graph()
with my_graph.as_default():
    x = tf.constant([1,3,6])
    y = tf.constant([1,1,1])
    op = tf.add(x,y)

   this is how the tensorflow workflow works: you first create a graph,
   and only then can you make the computations (really    running    the graph
   nodes with operations). to run the graph you   ll need to create a
   tf.session.

tf.session

   a tf.session object encapsulates the environment in which operation
   objects are executed, and tensor objects are evaluated (from [15]the
   docs). to do that, we need to define which graph will be used in the
   session:
import tensorflow as tf
my_graph = tf.graph()
with tf.session(graph=my_graph) as sess:
    x = tf.constant([1,3,6])
    y = tf.constant([1,1,1])
    op = tf.add(x,y)

   to execute the operations, you   ll use the method tf.session.run(). this
   method executes one    step    of the tensorflow computation, by running
   the necessary graph fragment to execute every operation objects and
   evaluate every tensor passed in the argument fetches. in your case you
   will run a step of the sum operations:
import tensorflow as tf
my_graph = tf.graph()
with tf.session(graph=my_graph) as sess:
    x = tf.constant([1,3,6])
    y = tf.constant([1,1,1])
    op = tf.add(x,y)
    result = sess.run(fetches=op)
    print(result)
>>> [2 4 7]
     __________________________________________________________________

a predictive model

   now that you know how tensorflow works, you have to learn how to create
   a predictive model. in short,

   machine learning algorithm + data = predictive model

   the process to construct a model is like this:
   [1*4zimlh2nuwsn08cvihw3hw.png]
   the process to create a predictive model

   as you can see, the model consists of a machine learning algorithm
      trained    with data. when you have the model you will get results like
   this:
   [1*knvhdida3-jnqxnrywfnew.png]
   prediction workflow

   the goal of the model you will create is to classify texts in
   categories, we define that:

   input: text, result: category

   we have a training dataset with all the texts labeled (every text has a
   label indicating to which category it belongs). in machine learning
   this type of task is denominated supervised learning.

        we know the correct answers. the algorithm iteratively makes
     predictions on the training data and is corrected by the
     teacher.         [16]   jason brownlee

   you   ll classify data into categories, so it   s also a classification
   task.

   to create the model, we   re going to use neural networks.
     __________________________________________________________________

neural networks

   a neural network is a computational model (a way to describe a system
   using mathematical language and mathematical concepts). these systems
   are self-learning and trained, rather than explicitly programmed.

   neural networks are inspired by our central nervous system. they have
   connected nodes that are similar to our neurons.
   [1*52jlj1ijw0ebblsbwtevug.png]
   a neural network

   the id88 was the first neural network algorithm. [17]this article
   explains really well the inner working of a id88 (the    inside an
   artificial neuron    animation is fantastic).

   to understand how a neural network works we will actually build a
   neural network architecture with tensorflow. this architecture was used
   by [18]aymeric damien in [19]this example.

neural network architecture

   the neural network will have 2 hidden layers ([20]you have to choose
   how many hidden layers the network will have, is part of the
   architecture design). the job of each hidden layer is to [21]transform
   the inputs into something that the output layer can use.

   hidden layer 1
   [1*8suxr4macilejnic v4g.png]
   input layer and 1st hidden layer

   you also need to define how many nodes the 1st hidden layer will have.
   these nodes are also called features or neurons, and in the image above
   they are represented by each circle.

   in the input layer every node corresponds to a word of the dataset (we
   will see how this works later).

   as explained [22]here, each node (neuron) is multiplied by a weight.
   every node has a weight value, and during the training phase the neural
   network adjusts these values in order to produce a correct output
   (wait, we will learn more about this in a minute).

   in addition to multiplying each input node by a weight, the network
   also adds a bias ([23]role of bias in neural networks).

   in your architecture after multiplying the inputs by the weights and
   sum the values to the bias, the data also pass by an activation
   function. this activation function defines the final output of each
   node. an analogy: imagine that each node is a lamp, the activation
   function tells if the lamp will light or not.

   there are [24]many types of id180. you will use the
   rectified linear unit (relu). this function is defined this way:

   f(x) = max(0,x) [the output is x or 0 (zero), whichever is larger]

   examples: ifx = -1, then f(x) = 0(zero); if x = 0.7, then f(x) = 0.7.

   hidden layer 2

   the 2nd hidden layer does exactly what the 1st hidden layer does, but
   now the input of the 2nd hidden layer is the output of the 1st one.
   [1*dn3acc-ohpd_jxmeu6t0uw.png]
   1st and 2nd hidden layers

   output layer

   and we finally got to the last layer, the output layer. you will use
   the [25]one-hot encoding to get the results of this layer. in this
   encoding only one bit has the value 1 and all the other ones got a zero
   value. for example, if we want to encode three categories (sports,
   space and computer graphics):
+-------------------+-----------+
|    category       |   value   |
+-------------------|-----------+
|      sports       |    001    |
|      space        |    010    |
| computer graphics |    100    |
|-------------------|-----------|

   so the number of output nodes is the number of classes of the input
   dataset.

   the output layer values are also multiplied by the weights and we also
   add the bias, but now the activation function is different.

   you want to label each text with a category, and these categories are
   mutually exclusive (a text doesn   t belong to two categories at the same
   time). to consider this, instead of using the relu activation function
   you will use the [26]softmax function. this function transforms the
   output of each unity to a value between 0 and 1 and also makes sure
   that the sum of all units equals 1. this way the output will tell us
   the id203 of each text for each category.
| 1.2                    0.46|
| 0.9   -> [softmax] ->  0.34|
| 0.4                    0.20|

   and now you have the data flow graph of your neural network.
   translating everything we saw so far into code, the result is:
# network parameters
n_hidden_1 = 10        # 1st layer number of features
n_hidden_2 = 5         # 2nd layer number of features
n_input = total_words  # words in vocab
n_classes = 3          # categories: graphics, space and baseball
def multilayer_id88(input_tensor, weights, biases):
    layer_1_multiplication = tf.matmul(input_tensor, weights['h1'])
    layer_1_addition = tf.add(layer_1_multiplication, biases['b1'])
    layer_1_activation = tf.nn.relu(layer_1_addition)
# hidden layer with relu activation
    layer_2_multiplication = tf.matmul(layer_1_activation, weights['h2'])
    layer_2_addition = tf.add(layer_2_multiplication, biases['b2'])
    layer_2_activation = tf.nn.relu(layer_2_addition)
# output layer with linear activation
    out_layer_multiplication = tf.matmul(layer_2_activation, weights['out'])
    out_layer_addition = out_layer_multiplication + biases['out']
return out_layer_addition

   (we   ll talk about the code for the output layer activation function
   later.)

how the neural network learns

   as we saw earlier the weight values are updated while the network is
   trained. now we will see how this happens in the tensorflow
   environment.

tf.variable

   the weights and biases are stored in variables (tf.variable). these
   variables maintain state in the graph across calls to run(). in machine
   learning we usually start the weight and bias values through a
   [27]normal distribution.
weights = {
    'h1': tf.variable(tf.random_normal([n_input, n_hidden_1])),
    'h2': tf.variable(tf.random_normal([n_hidden_1, n_hidden_2])),
    'out': tf.variable(tf.random_normal([n_hidden_2, n_classes]))
}
biases = {
    'b1': tf.variable(tf.random_normal([n_hidden_1])),
    'b2': tf.variable(tf.random_normal([n_hidden_2])),
    'out': tf.variable(tf.random_normal([n_classes]))
}

   when we run the network for the first time (that is, the weight values
   are the ones defined by the normal distribution):
input values: x
weights: w
bias: b
output values: z
expected values: expected

   to know if the network is learning or not, you need to compare the
   output values (z) with the expected values (expected). and how do we
   compute this difference (loss)? there are many methods to do that.
   because we are working with a classification task, the best measure for
   the loss is the [28]cross-id178 error.

   [29]james d. mccaffrey wrote [30]a brilliant explanation about why this
   is the best method for this kind of task.

   with tensorflow you will compute the cross-id178 error using the
   tf.nn.softmax_cross_id178_with_logits() method (here is the softmax
   activation function) and calculate the mean error (tf.reduce_mean()).
# construct model
prediction = multilayer_id88(input_tensor, weights, biases)
# define loss
id178_loss = tf.nn.softmax_cross_id178_with_logits(logits=prediction, labels
=output_tensor)
loss = tf.reduce_mean(id178_loss)

   you want to find the best values for the weights and biases in order to
   minimize the output error (the difference between the value we got and
   the correct value). to do that you will use the gradient [31]descent
   method. to be more specific, you will use the [32]stochastic gradient
   descent.
   [1*hmsql5sviarvkynvuaebya.png]
   id119. source:
   [33]https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html

   there are also many algorithms to compute the id119, you
   will use the [34]adaptive moment estimation (adam). to use this
   algorithm in tensorflow you need to pass the learning_rate value, that
   determines the incremental steps of the values to find the best weight
   values.

   the method tf.train.adamoptimizer(learning_rate).minimize(loss) is a
   [35]syntactic sugar that does two things:
    1. compute_gradients(loss, <list of variables>)
    2. apply_gradients(<list of variables>)

   the method updates all the tf.variables with the new values, so we
   don   t need to pass the list of variables. and now you have the code to
   train the network:
learning_rate = 0.001
# construct model
prediction = multilayer_id88(input_tensor, weights, biases)
# define loss
id178_loss = tf.nn.softmax_cross_id178_with_logits(logits=prediction, labels
=output_tensor)
loss = tf.reduce_mean(id178_loss)
optimizer = tf.train.adamoptimizer(learning_rate=learning_rate).minimize(loss)
     __________________________________________________________________

data manipulation

   the dataset you will use has many texts in english and we need to
   manipulate this data to pass it to the neural network. to do that you
   will do two things:
    1. create an index for each word
    2. create a matrix for each text, where the values are 1 if a word is
       in the text and 0 if not

   let   s see the code to understand this process:
import numpy as np    #numpy is a package for scientific computing
from collections import counter
vocab = counter()
text = "hi from brazil"
#get all words
for word in text.split(' '):
    vocab[word]+=1

#convert words to indexes
def get_word_2_index(vocab):
    word2index = {}
    for i,word in enumerate(vocab):
        word2index[word] = i

    return word2index
#now we have an index
word2index = get_word_2_index(vocab)
total_words = len(vocab)
#this is how we create a numpy array (our matrix)
matrix = np.zeros((total_words),dtype=float)
#now we fill the values
for word in text.split():
    matrix[word2index[word]] += 1
print(matrix)
>>> [ 1.  1.  1.]

   in the example above the text was    hi from brazil    and the matrix was [
   1. 1. 1.]. what if the text was only    hi   ?
matrix = np.zeros((total_words),dtype=float)
text = "hi"
for word in text.split():
    matrix[word2index[word.lower()]] += 1
print(matrix)
>>> [ 1.  0.  0.]

   you will to the same with the labels (categories of the texts), but now
   you will use the one-hot encoding:
y = np.zeros((3),dtype=float)
if category == 0:
    y[0] = 1.        # [ 1.  0.  0.]
elif category == 1:
    y[1] = 1.        # [ 0.  1.  0.]
else:
     y[2] = 1.       # [ 0.  0.  1.]
     __________________________________________________________________

running the graph and getting the results

   now comes the best part: getting the results from the model. first
   let   s take a closer look at the input dataset.

the dataset

   you will use the [36]20 newsgroups, a dataset with 18.000 posts about
   20 topics. to load this dataset you will use the [37]scikit-learn
   library. we will use only 3 categories: comp.graphics, sci.space and
   rec.sport.baseball. the scikit-learn has two subsets: one for training
   and one for testing. the recommendation is that you should never look
   at the test data, because this can interfere in your choices while
   creating the model. you don   t want to create a model to predict this
   specific test data, you want to create a model with a good
   generalization.

   this is how you will load the datasets:
from sklearn.datasets import fetch_20newsgroups
categories = ["comp.graphics","sci.space","rec.sport.baseball"]
newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)
newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)

training the model

   in the [38]neural network terminology, one epoch = one forward pass
   (getting the output values) and one backward pass (updating the
   weights) of all the training examples.

   remember the tf.session.run() method? let   s take a closer look at it:

   tf.session.run(fetches, feed_dict=none, options=none,
   run_metadata=none)

   in the dataflow graph of the beginning of this article you used the sum
   operation, but we can also pass a list of things to run. in this neural
   network run you will pass two things: the loss calculation and the
   optimization step.

   the feed_dict parameter is where we pass the data for each run step. to
   pass this data we need to define tf.placeholders (to feed the
   feed_dict).

   as the tensorflow documentation says:

        a placeholder exists solely to serve as the target of feeds. it is
     not initialized and contains no data.            [39]source

   so you will define your placeholders like this:
n_input = total_words # words in vocab
n_classes = 3         # categories: graphics, sci.space and baseball
input_tensor = tf.placeholder(tf.float32,[none, n_input],name="input")
output_tensor = tf.placeholder(tf.float32,[none, n_classes],name="output")

   you will separate the training data in batches:

        if you use placeholders for feeding input, you can specify a
     variable batch dimension by creating the placeholder with
     tf.placeholder(   , shape=[none,    ]). the none element of the shape
     corresponds to a variable-sized dimension.            [40]source

   we will feed the dict with a larger batch while testing the model,
   that   s why you need to the define a variable batch dimension.

   the get_batches() function gives us the number of texts with the size
   of the batch. and now we can run the model:
training_epochs = 10
# launch the graph
with tf.session() as sess:
    sess.run(init) #inits the variables (normal distribution, remember?)
    # training cycle
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = int(len(newsgroups_train.data)/batch_size)
        # loop over all batches
        for i in range(total_batch):
            batch_x,batch_y = get_batch(newsgroups_train,i,batch_size)
            # run optimization op (backprop) and cost op (to get loss value)
            c,_ = sess.run([loss,optimizer], feed_dict={input_tensor: batch_x, o
utput_tensor:batch_y})

   now you have the model, trained. to test it, you   ll also need to create
   graph elements. we   ll measure the accuracy of the model, so you need to
   get the index of the predicted value and the index of the correct value
   (because we are using the one-hot encoding), check is they are equal
   and calculate the mean to all the test dataset:
    # test model
    index_prediction = tf.argmax(prediction, 1)
    index_correct = tf.argmax(output_tensor, 1)
    correct_prediction = tf.equal(index_prediction, index_correct)
    # calculate accuracy
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    total_test_data = len(newsgroups_test.target)
    batch_x_test,batch_y_test = get_batch(newsgroups_test,0,total_test_data)
    print("accuracy:", accuracy.eval({input_tensor: batch_x_test, output_tensor:
 batch_y_test}))
>>> epoch: 0001 loss= 1133.908114347
    epoch: 0002 loss= 329.093700409
    epoch: 0003 loss= 111.876660109
    epoch: 0004 loss= 72.552971845
    epoch: 0005 loss= 16.673050320
    epoch: 0006 loss= 16.481995190
    epoch: 0007 loss= 4.848220565
    epoch: 0008 loss= 0.759822878
    epoch: 0009 loss= 0.000000000
    epoch: 0010 loss= 0.079848485
    optimization finished!
    accuracy: 0.75

   and that   s it! you created a model using a neural network to classify
   texts into categories. congratulations!     

   you can see the notebook with the final code [41]here.

   tip: modify the values we defined to see how the changes affect the
   training time and the model accuracy.

   any questions or suggestions? leave them in the comments. oh, and
   thank   s for reading!             

   did you found this article helpful? i try my best to write a deep dive
   article each month, you can [42]receive an email when i publish a new
   one.

   it would mean a lot if you click the      and share with friends. follow
   me for more articles about data science and machine learning.

     * [43]machine learning
     * [44]data science
     * [45]artificial intelligence
     * [46]tensorflow
     * [47]technology

   (button)
   (button)
   (button) 3.3k claps
   (button) (button) (button) 36 (button) (button)

     (button) blockedunblock (button) followfollowing
   [48]go to the profile of d  borah mesquita

[49]d  borah mesquita

   award-winning data scientist                 loves to write and explain things in
   different ways    - [50]http://deborahmesquita.com/

     (button) follow
   [51]freecodecamp.org

[52]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 3.3k
     * (button)
     *
     *

   [53]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [54]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d94036ac2274
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/big-picture-machine-learning-classifying-text-with-neural-networks-and-tensorflow-d94036ac2274&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/big-picture-machine-learning-classifying-text-with-neural-networks-and-tensorflow-d94036ac2274&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_gsvib0ibzqde---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@dehhmesquita?source=post_header_lockup
  12. https://medium.freecodecamp.org/@dehhmesquita
  13. https://www.quora.com/how-does-one-develop-intuitive-learning-and-what-are-the-key-differences-between-intuition-based-learning-and-learning-by-proof-and-which-one-do-you-prefer
  14. https://www.tensorflow.org/
  15. https://www.tensorflow.org/api_docs/python/tf/session
  16. http://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/
  17. https://appliedgo.net/id88/
  18. https://github.com/aymericdamien
  19. https://github.com/aymericdamien/tensorflow-examples/blob/master/notebooks/3_neuralnetworks/multilayer_id88.ipynb
  20. http://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw
  21. http://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute
  22. https://appliedgo.net/id88/
  23. http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks
  24. https://en.wikipedia.org/wiki/activation_function
  25. https://en.wikipedia.org/wiki/one-hot
  26. https://en.wikipedia.org/wiki/softmax_function
  27. https://en.wikipedia.org/wiki/normal_distribution
  28. https://en.wikipedia.org/wiki/cross_id178
  29. https://jamesmccaffrey.wordpress.com/
  30. https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-id178-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/
  31. https://en.wikipedia.org/wiki/gradient_descent
  32. https://en.wikipedia.org/wiki/stochastic_gradient_descent
  33. https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html
  34. http://sebastianruder.com/optimizing-gradient-descent/index.html#adam
  35. https://en.wikipedia.org/wiki/syntactic_sugar
  36. http://qwone.com/~jason/20newsgroups/
  37. http://scikit-learn.org/stable/index.html
  38. http://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks
  39. https://www.tensorflow.org/programmers_guide/reading_data
  40. https://www.tensorflow.org/versions/r0.11/resources/faq
  41. https://github.com/dmesquita/understanding_tensorflow_nn
  42. https://goo.gl/forms/slrjdrgtxgaoilkt1
  43. https://medium.freecodecamp.org/tagged/machine-learning?source=post
  44. https://medium.freecodecamp.org/tagged/data-science?source=post
  45. https://medium.freecodecamp.org/tagged/artificial-intelligence?source=post
  46. https://medium.freecodecamp.org/tagged/tensorflow?source=post
  47. https://medium.freecodecamp.org/tagged/technology?source=post
  48. https://medium.freecodecamp.org/@dehhmesquita?source=footer_card
  49. https://medium.freecodecamp.org/@dehhmesquita
  50. http://deborahmesquita.com/
  51. https://medium.freecodecamp.org/?source=footer_card
  52. https://medium.freecodecamp.org/?source=footer_card
  53. https://medium.freecodecamp.org/
  54. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  56. https://medium.com/p/d94036ac2274/share/twitter
  57. https://medium.com/p/d94036ac2274/share/facebook
  58. https://medium.com/p/d94036ac2274/share/twitter
  59. https://medium.com/p/d94036ac2274/share/facebook
