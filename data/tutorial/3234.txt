   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

a    brief    history of neural nets and deep learning, part 4

   [9]go to the profile of andrey kurenkov
   [10]andrey kurenkov (button) blockedunblock (button) followfollowing
   feb 17, 2017

   originally published at
   [11]http://www.andreykurenkov.com[12]/writing/a-brief-history-of-neural
   -nets-and-deep-learning-part-4/. best read there!

   this is the fourth part in    a brief history of neural nets and deep
   learning   . parts 1   3 [13]here, [14]here, and [15]here. in this part, we
   will get to the end of our story and see how deep learning emerged from
   the slump neural nets found themselves in by the late 90s, and the
   amazing state of the art results it has achieved since.

        ask anyone in machine learning what kept neural network research
     alive and they will probably mention one or all of these three
     names: geoffrey hinton, fellow canadian yoshua bengio and yann
     lecun, of facebook and new york university.   [16]1

the deep learning conspiracy

   when you want a revolution, start with a conspiracy. with the ascent of
   support vector machines and the failure of id26, the early
   2000s were a dark time for neural net research. lecun and hinton
   variously mention how in this period their papers or the papers of
   their students were routinely rejected from being published due to
   their subject being neural nets. the above quote is probably an
   exaggeration         certainly research in machine learning and ai was still
   very active, and other people were also still working with neural
   nets         but citation counts from the time make it clear that the
   excitement had leveled off, even if it did not completely evaporate.
   still, they persevered. and they found a strong ally outside the
   research realm: the canadian government. funding from the canadian
   institute for advanced research (cifar), which encourages basic
   research without direct application, was what motivated hinton to move
   to canada in 1987, and funded his work afterward. but, the funding was
   ended in the mid 90s just as sentiment towards neural nets was becoming
   negative again. rather than relenting and switching his focus, hinton
   fought to continue work on neural nets, and managed to secure more
   funding from cifar as told well in [17]this exemplary piece[18]1:

        but in 2004, hinton asked to lead a new program on neural
     computation. the mainstream machine learning community could not
     have been less interested in neural nets.

        it was the worst possible time,    says bengio, a professor at the
     universit   de montr  al and co-director of the cifar program since it
     was renewed last year.    everyone else was doing something different.
     somehow, geoff convinced them.   

        we should give (cifar) a lot of credit for making that gamble.   

     cifar    had a huge impact in forming a community around deep
     learning,    adds lecun, the cifar program   s other co-director.    we
     were outcast a little bit in the broader machine learning community:
     we couldn   t get our papers published. this gave us a place where we
     could exchange ideas.      

   the funding was modest, but sufficient to enable a small group of
   researchers to keep working on the topic. as hinton tells it, they
   hatched a conspiracy:    rebrand    the frowned-upon field of neural nets
   with the moniker    deep learning    [19]1. then, what every researcher
   must dream of actually happened: hinton, simon osindero, and yee-whye
   teh published a paper in 2006 that was seen as a breakthrough, a
   breakthrough significant enough to rekindle interest in neural nets:
   [20]a fast learning algorithm for deep belief nets[21]2. though, as
   we   ll see, the approaches used in the paper have been superceded by
   newer work, the movement that is    deep learning    can very persuasively
   be said to have started precisely with this paper. but, more important
   than the name was the idea         that neural networks with many layers
   really could be trained well, if the weights are initialized in a
   clever way rather than randomly. hinton [22]once expressed the need for
   such an advance at the time:

        historically, this was very important in overcoming the belief that
     these deep neural networks were no good and could never be trained.
     and that was a very strong belief. a friend of mine sent a paper to
     icml [international conference on machine learning], not that long
     ago, and the referee said it should not accepted by icml, because it
     was about neural networks and it was not appropriate for icml. in
     fact if you look at icml last year, there were no papers with
        neural    in the title accepted, so icml should not accept papers
     about neural networks. that was only a few years ago. and one of the
     ieee journals actually had an official policy of [not accepting your
     papers]. so, it was a strong belief.   

   [0*jdpliue4syk3hard.]
   a restricted id82. [23](source)

   so what was the clever way of initializing weights? the basic idea is
   to train each layer one by one with unsupervised training, which starts
   off the weights much better than just giving them random values, and
   then finishing with a round of supervised learning just as is normal
   for neural nets. each layer starts out as a restricted boltzmann
   machine (rbm), which is just a id82 without connections
   between hidden and visible units as illustrated above, and is taught a
   generative model of data in an unsupervised fashion. it turns out that
   this form of id82 can be trained in an efficient manner
   introduced by hinton in the 2002 [24]   training products of experts by
   minimizing contrastive divergence   [25]3. basically, this algorithm
   maximizes something other than the id203 of the units generating
   the training data, which allows for a nice approximation and turns out
   to still work well. so, using this method, the algorithm is as such:
    1. train an rbm on the training data using contrastive-divergence.
       this is the first layer of the belief net.
    2. generate the hidden values of the trained rbm for the data, and
       train another rbm using those hidden values. this is the second
       layer            stack    it on top of the first and keep weights in just one
       direction to form a belief net.
    3. keep doing step 2 for as many layers as are desired for the belief
       net.
    4. if classification is desired, add a small set of hidden units that
       correspond to the classification labels and do a variation on the
       wake-sleep algorithm to    fine-tune    the weights. such combinations
       of unsupervised and supervised learning are often called
       semi-supervised learning.

   [0*kvpbuvfx9dgtxran.]
   the layerwise pre-training that hinton introduced. [26](source)

   the paper concluded by showing that id50 (dbns) had
   state of the art performance on the standard mnist character
   recognition dataset, significantly outperforming normal neural nets
   with only a few layers. yoshua bengio et al. followed up on this work
   in 2007 with [27]   greedy layer-wise training of deep networks   [28]4, in
   which they present a strong argument that deep machine learning methods
   (that is, methods with many processing steps, or equivalently with
   hierarchical feature representations of the data) are more efficient
   for difficult problems than shallow methods (which two-layer anns or
   support vector machines are examples of).
   [0*io9ngyoba1qczguc.]
   another view of unsupervised pre-training, using autoencoders instead
   of rbms. [29](source)

   they also present reasons for why the addition of unsupervised
   pre-training works, and conclude that this not only initializes the
   weights in a more optimal way, but perhaps more importantly leads to
   more useful learned representations of the data. in fact, using rbms is
   not that important         unsupervised pre-training of normal neural net
   layers using id26 with plain autoencoders layers proved to
   also work well. likewise, at the same time another approach called
   sparse coding also showed that unsupervised id171 was a
   powerful approach for improving supervised learning performance.

   so, the key really was having many layers of computing units so that
   good high-level representation of data could be learned         in complete
   disagreement with the traditional approach of hand-designing some nice
   feature extraction steps and only then doing learning using those
   features. hinton and bengio   s work had empirically demonstrated that
   fact, but more importantly, showed the premise that deep neural nets
   could not be trained well to be false. this, lecun had already
   demonstrated with id98s throughout the 90s, but neural nets still went
   out of favor. bengio, in collaboration with yann lecun, reiterated this
   on [30]   scaling algorithms towards ai   [31]5:

        until recently, many believed that training deep architectures was
     too difficult an optimization problem. however, at least two
     different approaches have worked well in training such
     architectures: simple id119 applied to convolutional
     networks [lecun et al., 1989, lecun et al., 1998] (for signals and
     images), and more recently, layer-by-layer unsupervised learning
     followed by id119 [hinton et al., 2006, bengio et al.,
     2007, ranzato et al., 2006]. research on deep architectures is in
     its infancy, and better learning algorithms for deep architectures
     remain to be discovered. taking a larger perspective on the
     objective of discovering learning principles that can lead to ai has
     been a guiding perspective of this work. we hope to have helped
     inspire others to seek a solution to the problem of scaling machine
     learning towards ai.   

   and inspire they did. or at least, they started; though deep learning
   had not yet gained the tsumani momentum that it has today, the wave had
   unmistakably begun. still, the results at that point were not that
   impressive         most of the demonstrated performance in the papers up to
   this point was for the mnist dataset, a classic machine learning task
   that had been the standard benchmark for algorithms for about a decade.
   hinton   s 2006 publication demonstrated a very impressive error rate of
   only 1.25% on the test set, but id166s had already gotten an error rate
   of 1.4%, and even simple algorithms could get error rates in the low
   single digits. and, as was pointed out in the paper, yann lecun already
   demonstrated error rates of 0.95% in 1998 using id98s.

   so, doing well on mnist was not necessarily that big a deal. aware of
   this and confident that it was time for deep learning to take the
   stage, hinton and two of his graduate students, abdel-rahman mohamed
   and george dahl, demonstrated their effectiveness at a far more
   challenging ai task: [32]id103[33]6. using dbns, the two
   students and hinton managed to improve on a decade-old performance
   record on a standard id103 dataset. this was an impressive
   achievement, but in retrospect seems like only a hint at what was
   coming         in short, many more broken records.

the importance of brute force

   the algorithmic advances described above were undoubtedly important to
   the emergence of deep learning, but there was another essential
   component that had emerged in the decade since the 1990s: pure
   computational power. following moore   s law, computers got dozens of
   times faster since the slow days of the 90s, making learning with large
   datasets and many layers much more tractable. but even this was not
   enough         cpus were starting to hit a ceiling in terms of speed growth,
   and computer power was starting to increase mainly through weakly
   parallel computations with several cpus. to learn the millions of
   weights typical in deep models, the limitations of weak cpu parallelism
   had to be left behind and replaced with the massively parallel
   computing powers of gpus. realizing this is, in part, how abdel-rahman
   mohamed, george dahl, and geoff hinton accomplished their record
   breaking id103 performance[34]7:

        inspired by one of hinton   s lectures on deep neural networks,
     mohamed began applying them to speech         but deep neural networks
     required too much computing power for conventional computers         so
     hinton and mohamed enlisted dahl. a student in hinton   s lab, dahl
     had discovered how to train and simulate neural networks efficiently
     using the same high-end graphics cards which make vivid computer
     games feasible on personal computers.

     they applied the same method to the problem of recognizing fragments
     of phonemes in very short windows of speech,    said hinton.    they got
     significantly better results than previous methods on a standard
     three-hour benchmark.   

   it   s hard to say just how much faster using gpus over cpus was in this
   case, but the paper [35]   large-scale deep unsupervised learning using
   graphics processors   [36]8of the same year suggests a number: 70 times
   faster. yes, 70 times         reducing weeks of work into days, even a single
   day. the authors, who had previously developed sparse coding, included
   the prolific machine learning researcher andrew ng, who increasingly
   realized that making use of lots of training data and of fast
   computation had been greatly undervalued by researchers in favor of
   incremental changes in learning algorithms. this idea was strongly
   supported by 2010   s [37]   deep big simple neural nets excel on
   handwritten digit recognition   [38]9(notably co-written by j.
   schmidhuber, one of the inventors of the recurrent ltsm networks),
   which showed a whopping %0.35 error rate could be achieved on the mnist
   dataset without anything more special than really big neural nets, a
   lot of variations on the input, and efficient gpu implementations of
   id26. these ideas had existed for decades, so although it
   could not be said that algorithmic advancements did not matter, this
   result did strongly support the notion that the brute force approach of
   big training sets and fast parallelized computations were also crucial.

   dahl and mohamed   s use of a gpu to get record breaking results was an
   early and relatively modest success, but it was sufficient to incite
   excitement and for the two to be invited to intern at microsoft
   research[39]1. here, they would have the benefit from another trend in
   computing that had emerged by then: big data. that loosest of terms,
   which in the context of machine learning is easy to understand         lots
   of training data. and lots of training data is important, because
   without it neural nets still did not do great         they tended to overfit
   (perfectly work on the training data, but not generalize to new test
   data). this makes sense         the complexity of what large neural nets can
   compute is such that a lot of data is needed to avoid them learning
   every little unimportant aspect of the training set         but was a major
   challenge for researchers in the past. so now, the computing and data
   gathering powers of large companies proved invaluable. the two students
   handily proved the power of deep learning during their three month
   internship, and microsoft research has been at the forefront of deep
   learning id103 ever since.

   microsoft was not the only bigcompany to recognize the power of deep
   learning (though it was handily the first). navdeep jaitly, another
   student of hinton   s, went off to a summer internship at google in 2011.
   there, he worked on google   s id103, and showed their
   existing setup could be much improved by incorporating deep learning.
   the revised approach soon powered android   s id103,
   replacing much of google   s carefully crafted prior solution [40]1.

   besides the impressive effects of humble phd interns on these gigantic
   companies    products, what is notable here is that both companies were
   making use of the same ideas         ideas that were out in the open for
   anyone to work with. and in fact, the work by microsoft and google, as
   well as ibm and hinton   s lab, resulted in the impressively titled
   [41]   deep neural networks for acoustic modeling in id103:
   the shared views of four research groups   [42]10in 2012. four research
   groups         three from companies that could certainly benefit from a
   briefcase full of patents on the emerging wonder technology of deep
   learning, and the university research group that popularized that
   technology         working together and publishing their results to the
   broader research community. if there was ever an ideal scenario for
   industry adopting an idea from research, this seems like it.

   not to say the companies were doing this for charity. this was the
   beginning of all of them exploring how to commercialize the technology,
   and most of all google. but it was perhaps not hinton, but andrew ng
   who incited the company to become likely the world   s biggest commercial
   adopter and proponent of the technology. in 2011, ng [43]incidentally
   met with the legendary googler jeff dean while visiting the company,
   and chatted about his efforts to train neural nets with google   s
   fantastic computational resources. this intrigued dean, and together
   with ng they formed google brain         an effort to build truly giant
   neural nets and explore what they could do. the work resulted in
   unsupervised neural net learning of an unprecedented scale         16,000 cpu
   cores powering the learning of a whopping 1 billion weights (for
   comparison, hinton   s breakthrough 2006 dbn had about 1 million
   weights). the neural net was trained on youtube videos, entirely
   without labels, and learned to recognize the most common objects in
   those videos         leading of course to the internet   s collective glee over
   the net   s discovery of cats:
   [0*ulvl4v2pmlscfm-n.]
   google   s famous neural-net learned cat. this is the optimal input to
   one of the neurons. [44](source)

   cute as that was, it was also useful. as they reported in a regularly
   published paper, the features learned by the model could be used for
   record setting performance on a standard id161
   benchmark[45]11. with that, google   s internal tools for training
   massive neural nets were born, and they have only continued to evolve
   since. the wave of deep learning research that began in 2006 had now
   undeniably made it into industry.

the ascendance of deep learning

   while deep learning was making it into industry, the research community
   was hardly keeping still. the discovery that efficient use of gpus and
   computing power in general was so important made people examine
   long-held assumptions and ask questions that should have perhaps been
   asked long ago         namely, why exactly does id26 not work
   well? the insight to ask why the old approaches did not work, rather
   than why the new approaches did, led xavier glort and yoshua bengio to
   write [46]   understanding the difficulty of training deep feedforward
   neural networks    in 2010[47]12. in it, they discussed two very
   meaningful findings:
    1. the particular non-linear activation function chosen for neurons in
       a neural net makes a big impact on performance, and the one often
       used by default is not a good choice.
    2. it was not so much choosing random weights that was problematic, as
       choosing random weights without consideration for which layer the
       weights are for. the old vanishing gradient problem happens,
       basically, because id26 involves a sequence of
       multiplications that invariably result in smaller derivatives for
       earlier layers. that is, unless weights are chosen with difference
       scales according to the layer they are in         making this simple
       change results in significant improvements.

   [0*j63baaor5iikosxo.]
   different id180. the relu is the **rectified linear
   unit**. [48](source)

   the second point is quite clear, but the first opens the question:
      what, then, is the best activation function   ? three different groups
   explored the question (a group with lecun, with [49]   what is the best
   multi-stage architecture for object recognition?   [50]13, a group with
   hinton, in [51]   rectified linear units improve restricted boltzmann
   machines   [52]14, and a group with bengio -[53]   deep sparse rectifier
   neural networks   [54]15), and they all found the same surprising answer:
   the very much non-differentiable and very simple function f(x)=max(0,x)
   tends to be the best. surprising, because the function is kind of
   weird         it is not strictly differentiable, or rather is not
   differentiable precisely at zero, so on paper as far as math goes it
   looks pretty ugly. but, clearly the zero case is a pretty small
   mathematical quibble         a bigger question is why such a simple function,
   with constant derivatives on either side of 0, is so good. the answer
   is not precisely known, but a few ideas seem pretty well established:
    1. rectified activation leads to sparse representations, meaning not
       many neurons actually end up needing to output non-zero values for
       any given input. in the years leading up to this point sparsity was
       shown to be beneficial for deep learning, both because it
       represents information in a more robust manner and because it leads
       to significant computational efficiency (if most of your neurons
       are outputting zero, you can in effect ignore most of them and
       compute things much faster). incidentally, researchers in
       computational neuroscience first introduced the importance of
       sparse computation in the context of the brain   s visual system, a
       decade before it was explored in the context of machine learning.
    2. the simplicity of the function, and its derivatives, makes it much
       faster to work with than the exponential sigmoid or the
       trigonometric tanh. as with the use of gpus, this turns out to not
       just be a small boost but really important for being able to scale
       neural nets to the point where they perform well on challenging
       problems.
    3. a later analysis titled [55]   rectifier nonlinearities improve
       neural network acoustic models   [56]16, co-written by andrew ng,
       also showed the constant 0 or 1 derivative of the relu not too
       detrimental to learning. in fact, it helps avoid the vanishing
       gradient problem that was the bane of id26. furthermore,
       beside producing more sparse representations, it also produces more
       distributed representations         meaning is derived from the
       combination of multiple values of different neurons, rather than
       being localized to individual neurons.

   at this point, with all these discoveries since 2006, it had become
   clear that unsupervised pre-training is not essential to deep learning.
   it was helpful, no doubt, but it was also shown that in some cases
   well-done, purely supervised training (with the correct starting weight
   scales and activation function) could outperform training that included
   the unsupervised step. so, why indeed, did purely supervised learning
   with id26 not work well in the past? geoffrey hinton
   [57]summarized the findings up to today in these four points:
    1. our labeled datasets were thousands of times too small.
    2. our computers were millions of times too slow.
    3. we initialized the weights in a stupid way.
    4. we used the wrong type of non-linearity.

   so here we are. deep learning. the culmination of decades of research,
   all leading to this:

     deep learning =
     lots of training data + parallel computation + scalable, smart
     algorithms

   [0*klvi4a3w2gquu2tc.]
   i wish i was first to come up with this delightful equation, but it
   seems others came up with it before me. [58](source)

   not to say all there was to figure out was figured out by this point.
   far from it. what had been figured out is exactly the opposite: that
   peoples    intuition was often wrong, and in particular unquestioned
   decisions and assumptions were often very unfounded. asking simple
   questions, trying simple things         these had the power to greatly
   improve state of the art techniques. and precisely that has been
   happening, with many more ideas and approaches being explored and
   shared in deep learning since. an example: [59]   improving neural
   networks by preventing co-adaptation of feature detectors   [60]17by g.
   e. hinton et al. the idea is very simple: to prevent overfitting,
   randomly pretend some neurons are not there while training. this
   straightforward idea         called dropout         is a very efficient means of
   implementing the hugely powerful approach of id108, which
   just means learning in many different ways from the training data.
   id79s, a dominating technique in machine learning to this day,
   is chiefly effective due to being a form of id108. training
   many different neural nets is possible but is far too computationally
   expensive, yet this simple idea in essence achieves the same thing and
   indeed significantly improves performance.

   still, having all these research discoveries since 2006 is not what
   made the id161 or other research communities again respect
   neural nets. what did do it was something somewhat less noble:
   completely destroying non-deep learning methods on a modern competitive
   benchmark. geoffrey hinton enlisted two of his dropout co-writers, alex
   krizhevsky and ilya sutskever, to apply the ideas discovered to create
   an entry to the ilsvrc-2012 id161 competition. to me, it is
   very striking to now understand that their work, described in
   [61]   id163 classification with deep convolutional neural
   networks   [62]18, is the combination of very old concepts (a id98 with
   pooling and convolution layers, variations on the input data) with
   several new key insight (very efficient gpu implementation, relu
   neurons, dropout), and that this, precisely this, is what modern deep
   learning is. so, how did they do? far, far better than the next closest
   entry: their error rate was %15.3, whereas the second closest was
   %26.2. this, the first and only id98 entry in that competition, was an
   undisputed sign that id98s, and deep learning in general, had to be
   taken seriously for id161. now, almost all entries to the
   competition are id98s         a neural net model yann lecun was working with
   since 1989. and, remember lstm recurrent neural nets, devised in the
   90s by sepp hochreiter and j  rgen schmidhuber to solve the
   id26 problem? those, too, are now state of the art for
   sequential tasks such as speech processing.

   this was the turning point. a mounting wave of excitement about
   possible progress has culminated in undeniable achievements that far
   surpassed what other known techniques could manage. the tsunami
   metaphor that we started with in part 1, this is where it began, and it
   has been growing and intensifying to this day. deep learning is here,
   and no winter is in sight.
   [0*_olvvltc5wdjflcf.]
   the citation counts for some of the key people we have seen develop
   deep learning. i believe i don   t need to point out the exponential
   trends since 2012. from google scholar.

epilogue: state of the art

   if this were a movie, the 2012 id163 competition would likely have
   been the climax, and now we would have a progression of text describing
      where are they now   . yann lecun         facebook. geoffrey hinton         google.
   andrew ng         coursera, google, baidu. bengio, schmidhuber, and
   hochreiter actually still in academia         but presumably with many more
   citations and/or grad students[63]19. though the ideas and achievements
   of deep learning are definitely exciting, while writing this i was
   inevitably also moved that these people, who worked in this field for
   decades (even as most abandoned it), are now rich, successful, and most
   of all better situated to do research than ever. all these peoples   
   ideas are still very much out in the open, and in fact basically all
   these companies are open sourcing their deep learning frameworks, like
   some sort of utopian vision of industry-led research. what a story.

   i was foolish enough to hope i could fit a summary of the most
   impressive results of the past several years in this part, but at this
   point it is clear i will not have the space to do so. perhaps one day
   there will be a part five of this that can finish out the tale by
   describing these things, but for now let me provide a brief list:

   1         the resurgence of ltsm id56s + representing    ideas    with
   [64]distributed representations
   [0*6mvcjkqkk-gifas9.]

   [65]skype real time translation

   2         using deep learning for id23 (again, but better)

   [66]https://www.youtube.com/watch?v=v1eynij0rnk

   [67]chess playing!

   3         adding external memory writable and readable to by the neural net

     * [68]machine learning
     * [69]artificial intelligence
     * [70]deep learning
     * [71]neural networks
     * [72]how to

   (button)
   (button)
   (button) 69 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [73]go to the profile of andrey kurenkov

[74]andrey kurenkov

   an eclectic artistically inclined engineer who says things sometimes.
   [75]www.andreykurenkov.com

     * (button)
       (button) 69
     * (button)
     *
     *

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/61be90639182
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@andreykurenkov/a-brief-history-of-neural-nets-and-deep-learning-part-4-61be90639182&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@andreykurenkov/a-brief-history-of-neural-nets-and-deep-learning-part-4-61be90639182&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@andreykurenkov?source=post_header_lockup
  10. https://medium.com/@andreykurenkov
  11. http://www.andreykurenkov.com/
  12. https://medium.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/
  13. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning
  14. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-2
  15. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-3
  16. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:1
  17. http://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html
  18. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:1
  19. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:1
  20. https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf
  21. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:2
  22. https://youtu.be/vshmxxqtdds?t=6m59s
  23. http://deeplearning.net/tutorial/rbm.html
  24. http://www.cs.toronto.edu/~fritz/absps/tr00-004.pdf
  25. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:3
  26. http://deeplearning.net/tutorial/rbm.html
  27. http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf
  28. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:4
  29. https://commons.wikimedia.org/wiki/file:stacked_autoencoders.png?uselang=ru
  30. http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf
  31. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:5
  32. http://www.cs.toronto.edu/~gdahl/papers/dbnphonerec.pdf
  33. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:6
  34. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:7
  35. http://www.machinelearning.org/archive/icml2009/papers/218.pdf
  36. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:8
  37. http://arxiv.org/pdf/1003.0358.pdf
  38. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:9
  39. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:1
  40. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:1
  41. http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf
  42. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:10
  43. https://medium.com/backchannel/google-search-will-be-your-next-brain-5207c26e4523#.b3x9b7ods
  44. https://googleblog.blogspot.com/2012/06/using-large-scale-brain-simulations-for.html
  45. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:11
  46. http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf
  47. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:12
  48. https://imiloainf.wordpress.com/2013/11/06/rectifier-nonlinearities/
  49. http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf
  50. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:13
  51. http://www.cs.toronto.edu/~fritz/absps/reluicml.pdf
  52. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:14
  53. https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf
  54. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:15
  55. http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf
  56. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:16
  57. https://youtu.be/icomkxaw5va?t=21m29s
  58. http://www.computervisionblog.com/2015/05/deep-learning-vs-big-data-who-owns-what.html
  59. http://arxiv.org/pdf/1207.0580.pdf
  60. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:17
  61. http://www.cs.toronto.edu/~fritz/absps/id163.pdf
  62. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:18
  63. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/#fn:19
  64. http://machinelearning.wustl.edu/mlpapers/paper_files/bengiodvj03.pdf
  65. http://blogs.microsoft.com/blog/2014/05/27/microsoft-demos-breakthrough-in-real-time-translated-conversations/
  66. https://www.youtube.com/watch?v=v1eynij0rnk
  67. http://arxiv.org/abs/1509.01549
  68. https://medium.com/tag/machine-learning?source=post
  69. https://medium.com/tag/artificial-intelligence?source=post
  70. https://medium.com/tag/deep-learning?source=post
  71. https://medium.com/tag/neural-networks?source=post
  72. https://medium.com/tag/how-to?source=post
  73. https://medium.com/@andreykurenkov?source=footer_card
  74. https://medium.com/@andreykurenkov
  75. http://www.andreykurenkov.com/

   hidden links:
  77. https://medium.com/p/61be90639182/share/twitter
  78. https://medium.com/p/61be90639182/share/facebook
  79. https://medium.com/p/61be90639182/share/twitter
  80. https://medium.com/p/61be90639182/share/facebook
