   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

back-propagation is very simple. who made it complicated ?

learning outcome: you will be able to build your own neural network on
a paper.

   [9]go to the profile of prakash jay
   [10]prakash jay (button) blockedunblock (button) followfollowing
   apr 20, 2017
   [1*fnu_3mgmfp0lbizrpx42-w.png]
   feed forward neural network

   almost 6 months back when i first wanted to try my hands on neural
   network, i scratched my head for a long time on how back-propagation
   works. when i talk to peers around my circle, i see a lot of people
   facing this problem. most people consider it as a black-box and use
   libraries like keras, tensorflow and pytorch which provide automatic
   differentiation. though it is not necessary to write your own code on
   how to compute gradients and backprop errors, having knowledge on it
   helps you in understanding a few concepts like vanishing gradients,
   saturation of neurons and reasons for random initialization of weights

   iframe: [11]/media/b6c52d4e837d433674b3626d69280699?postid=97b794c97e5c

   signup for my ai newsletter

more about why is it important to understand?

   andrej karapathy wrote a blog-post on it and i found it useful.
   [12]yes you should understand backprop
   when we offered cs231n (deep learning class) at stanford, we
   intentionally designed the programming assignments to   medium.com

approach

     * build a small neural network as defined in the architecture below.
     * initialize the weights and bias randomly.
     * fix the input and output.
     * forward pass the inputs. calculate the cost.
     * compute the gradients and errors.
     * backprop and adjust the weights and bias accordingly

architecture:

     * build a feed forward neural network with 2 hidden layers. all the
       layers will have 3 neurons each.
     * 1st and 2nd hidden layer will have relu and sigmoid respectively as
       id180. final layer will have softmax.
     * error is calculated using cross-id178.

initializing the network

   i have taken inputs, weights and bias randomly
   [1*fnu_3mgmfp0lbizrpx42-w.png]
   [1*se4kinkd20ffrrdxj_xhia.png]
   initializing the network

layer-1

   [1*wmgvoyf6tsyomaczwcm4ka.png]
   neural network layer-1

   matrix operation:
   [1*z22u84kixx9lzvstxt6kew.png]
   layer-1 matrix operation

   relu operation:
   [1*zydya39kbx6jolutrinewq.png]
   layer-1 relu operation

   example:
   [1*pwyg8v_vepzrvskbv8z6lw.png]
   layer-1 example

layer-2

   [1*kdddeupqvesmq7evhho_ua.png]
   layer-2 neural network

   matrix operation:
   [1*-mtc2rvgxay5ofm23jq-xq.png]
   layer-2 matrix operation

   sigmoid operation:
   [1*bnrzdcuxyqn5whfcmcm36q.png]
   sigmoid operation

   example:
   [1*7bv4iqzrhrkgmwrrmyoufw.png]
   layer-2 example

layer-3

   [1*6pqvkcdsxwogeutdnkvtea.png]
   layer-3 neural network

   matrix operation:
   [1*kik9zfkcqm2t--m1jkwaug.png]
   layer-3 matrix operation

   softmax operation:
   [1*jgofihoqauknwhmd1_hjrg.png]
   softmax formula

   example:
   [1*7bzpaixwjl8cib4f5phg_w.png]
   layer-3 output example

   edit1: as [13]jmuth pointed out in the comments, the output from
   softmax would be [0.19858, 0.28559, 0.51583] instead of [0.26980,
   0.32235, 0.40784]. i have done a/sum(a) while the correct answer would
   be exp(a)/sum(exp(a)) . please adjust your calculations from here on
   using these values. thank you.

   analysis:
     * the actual output should be [1.0, 0.0, 0.0] but we got [0.2698,
       0.3223, 0.4078].
     * to calculate error lets use cross-id178

error:

   cross-id178:
   [1*6yctn8sqfjb4p6namglbug.png]
   cross-id178 formula

   example:
   [1*drx0o_3p5ficxp1ahgvllw.png]
   cross-id178 calculation

   we are done with forward pass. now let us see backward pass

important derivatives:

sigmoid:

   [1*9gk0k_cqurehs_2ccjkmlw.png]
   derivative of sigmoid

relu:

   [1*vuxxfyips0l51d3yfkrf6w.png]
   derivative of relu

softmax:

   [1*l6gntfihuu0euumughmwpa.png]
   derivative of softmax

backpropagating the error         (hidden layer2         output layer) weights

   [1*yuz65bgzmk6vmystdyzoiw.png]
   backpropagating layer-3 weights

   let us calculate a few derivatives upfront so these become handy and we
   can reuse them whenever necessary. here are we are using only one
   example (batch_size=1), if there are more examples, we just need to
   average everything.
   [1*ycj_l8yqzniwtxvgbdjvnq.png]
   example: derivative of cross-id178

   by symmetry we can calculate other derivatives also
   [1*fddrkouj5ck2k4aa2bsaaa.png]
   matrix of cross-id178 derivatives wrt output

   in our example,
   [1*mnimid864igwtr-ukq_xbeq.png]
   values of derivative of cross-id178 wrt output.

   next let us calculate the derivative of each output with respect to
   their input.
   [1*y0lhl4xf972mfl_bez7uia.png]
   example: derivative of softmax wrt output layer input

   by symmetry we can calculate other derivatives also
   [1*xwyzdij1a-rpkqpnbm9lrw.png]
   matrix of derivative of softmax wrt output layer input.

   in our example,
   [1*jo8iczpkkdvrcyqwpxmbwa.png]
   values of derivative of softmax wrt output layer input .

   for each input to neuron let us calculate the derivative with respect
   to each weight. now let us look at the final derivative
   [1*nwdvdhjrddpck2gqtfm7yg.png]
   example: derivative of input to output layer wrt weight

   by symmetry we can calculate other derivatives also
   [1*p5urzmci3wxbkwu733qb7w.png]
   values of derivative of input to output layer wrt weights.

   finally let us calculate the change in
   [1*4r3ky40plsphq6iu-fpvdq.png]
   weight from k1 to l1 neuron

   which will be simply
   [1*8ahwgjwle1u37-x1vn5o0q.png]
   derivative of error wrt weight

   using chain rule:
   [1*m3bw07a9usaprg7eeqqhea.png]
   chain rule breakdown of error derivative

   by symmetry:
   [1*p3itnpdubbkrtc-bz_fiha.png]
   matrix form of all derivatives in layer-3

   all of the above values are calculated before. we just need to
   substitute the results.
   [1*fmmmkzggyogyw0lkkt_6gg.png]
   example: calculation of all the values

   considering a learning rate of 0.01 we get our final weight matrix as
   [1*c9i0oxxlijhhnoyyzzaid86.png]
   modified weights of kl neurons after backprop

   so, we have calculated new weight matrix for w_{kl}. now let us move to
   the next layer:

backpropagating the error         (hidden layer1         hidden layer 2) weights

   [1*uvd-nrefd1rtk8zmvdi81w.png]
   backpropagating errors to 2nd layer

   let us calculate a few handy derivatives before we actually calculate
   the error derivatives wrt weights in this layer.
   [1*nwb_tahwvbwjc0hnm0liuq.png]
   example: derivative of sigmoid output wrt layer 2 input

   in our example:
   [1*wjqmhqc4jxxn575vddjcew.png]
   values of derivative of output of layer-2 wrt input of layer1

   for each input to neuron let us calculate the derivative with respect
   to each weight. now let us look at the final derivative
   [1*pu_imzzc1hzlboeje_vxng.png]
   derivative of layer 2 input wrt weight

   by symmetry we can calculate:
   [1*x--fpdrcchydy6w0w18hla.png]
   values of derivative layer 2 input wrt of weight

   now we will calculate the derivative of
   [1*5f0ll3rjgskxpolq9fft_a.png]
   weight from j3 to k1

   which will be simply.
   [1*ogtc-qhys5nvqf_nea9ffa.png]
   derivative of error wrt weight j3-k1

   using chain rule,
   [1*qbpa5rfby9ljz_twl1dvbg.png]
   chain rule of derivative of error wrt weight

   by symmetry we get the final matrix as,
   [1*jvj87pgk9j_0gjkosbqftq.png]
   final matrix of derivatives of weights_{jk}

   we have already calculated the 2nd and 3rd term in each matrix. we need
   to check on the 1st term. if we see the matrix, the first term is
   common in all the columns. so there are only three values. let us look
   into one value
   [1*xpvo_apuou1jijzlc25hwg.png]
   breakdown of error.

   lets see what each individual term boils down too.
   [1*83kcfysjvbrnmcy5excsyq.png]
   breakdown of each error derivative

   by symmetry we get the final matrix as,
   [1*pfkvtj5comzorzo_xt4ztw.png]
   derivative of error wrt output of hidden layer 2

   again the first two values are already calculated by us when dealing
   with derivatives of w_{kl}. we just need to calculate the third one,
   which is the derivative of input to each output layer wrt output of
   hidden layer-2. it is nothing but the corresponding weight which
   connects both the layers.
   [1*eopo2msythmjzahpe34xhq.png]
   derivative of input of output layer wrt hidden layer -2
   [1*snkgxvihssh7b2n-rvhdvq.png]
   final matrix of derivative of total error wrt output of hidden layer-2

   all values are calculated before we just need to impute the
   corresponding values for our example.
   [1*rvkcvgjllxmeuvooic0eoa.png]
   calculations using an example

   let us look at the final matrix
   [1*qwqzpzh6h6wg5bx0jlbeqg.png]
   our final matrix of derivatives of weights connecting hidden layer-1
   and hidden layer-2

   in our example,
   [1*qiy4a3padk24ozlbkkmqfg.png]
   calculations from our examples

   consider a learning rate (lr) of 0.01 we get our final weight matrix as
   [1*ui5v3-jnhi0wyl_t7liumq.png]
   final modified matrix of w_{jk}

   so, we have calculated new weight matrix for w_{jk}. now let us move to
   the next layer:

backpropagating the error         (input layer         hidden layer 1) weights.

   edit:1 the following calculations from here are wrong. i took only
   wj1k1 and ignored wj1k2 and wj1k3. this was pointed by an user in
   comments. i would like someone to edit the jupyter notebook attached at
   the end. please refer to some other implementations if u still didn   t
   understand back-prop here.
   [1*5wuezefqt5hy-euewj_gca.png]
   backpropagating errors to 1st layer

   let us calculate a few handy derivatives before we actually calculate
   the error derivatives wrt weights in this layer.
   [1*7a1ujwrna7inrbzfpbzvoq.png]
   derivative of hidden layer 1 output wrt to its input

   we already know the derivative of relu (we have seen it at the
   beginning of the post). since all inputs are positive, we will get
   output as 1
   [1*rqopwnkopb_3uv_jj-n6ug.png]
   calculations

   for each input to neuron let us calculate the derivative with respect
   to each weight. now let us look at the final derivative.
   [1*9yud65-wxnz8tkvdwv5a-a.png]
   derivative of input to hidden layer wrt to weights

   by symmetry we can write,
   [1*a8yzzi2xjqdbarfzfvvkza.png]
   final derivative calculations

   now we will calculate the change in
   [1*cyumqhvnq71eftzhtajgja.png]
   weight connecting i2 neuron to j1

   and generalize it to all variables. this will be simply
   [1*e9klthww9pkngcq5k2i5rq.png]
   derivative of error wrt to weight

   using chain rule,
   [1*i3p_amzxm1ihhaqfa9s2qa.png]
   chain rule for calculating error

   by symmetry,
   [1*694omkkfagoiel3j3wai6a.png]
   final matrix using symmertry

   we know the 2nd and 3rd derivatives in each cell in the above matrix.
   let us look at how to get to derivative of 1st term in each cell.
   [1*esevf4m3skrhk4so7ipyxg.png]
   watching for the first term

   we have calculated all the values previously except the last one in
   each cell, which is a simple derivative of linear terms.
   [1*rs49rptpz8rrrgally8big.png]
   calculations in our example

   in our example,
   [1*v2c2bzyfcu536kdyc4yuow.png]
   final matrix
   [1*iwlmjt6d8yfrobi-waigba.png]
   calculations in our example

   consider a learning rate (lr) of 0.01 we get our final weight matrix as
   [1*ygqxqf0kybm75h0cjsnonq.png]
   using learning rate we get final matrix

the end of calculations

our initial weights:

   [1*jre924t9vam_gsah_zhvdq.png]

our final weights:

   [1*sjr26ibys56wc-yeujspvg.png]

important notes:

     * i have completely eliminated bias when differentiating. do you know
       why ?
     * backprop of bias should be straightforward. try on your own.
     * i have taken only one example. what will happen if we take batch of
       examples?
     * though i have not mentioned directly about vanishing gradients. do
       you see why it occurs?
     * what would happen if all the weights are the same number instead of
       random ?

references used:

     * [14]http://csrgxtu.github.io/2015/03/20/writing-mathematic-fomulars
       -in-markdown/
     * [15]https://mattmazur.com/2015/03/17/a-step-by-step-id26
       -example/
     * [16]http://eli.thegreenplace.net/2016/the-softmax-function-and-its-
       derivative/

code available on github :

     * [17]https://github.com/prakashvanapalli/tensorflow/blob/master/blog
       posts/backpropogation_with_images.ipynb

   i have hand calculated everything. let me know your feedback. if you
   like it, please recommend and share it. thank you.

     * [18]neural networks
     * [19]id26
     * [20]deep learning
     * [21]artificial intelligence
     * [22]tensorflow

   (button)
   (button)
   (button) 4.5k claps
   (button) (button) (button) 67 (button) (button)

     (button) blockedunblock (button) followfollowing
   [23]go to the profile of prakash jay

[24]prakash jay

   senior data scientist [25]@fractalanalytics

     * (button)
       (button) 4.5k
     * (button)
     *
     *

   [26]go to the profile of prakash jay
   never miss a story from prakash jay, when you sign up for medium.
   [27]learn more
   never miss a story from prakash jay
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/97b794c97e5c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@14prakash?source=post_header_lockup
  10. https://medium.com/@14prakash
  11. https://medium.com/media/b6c52d4e837d433674b3626d69280699?postid=97b794c97e5c
  12. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b
  13. https://medium.com/@jmuth
  14. http://csrgxtu.github.io/2015/03/20/writing-mathematic-fomulars-in-markdown/
  15. https://mattmazur.com/2015/03/17/a-step-by-step-id26-example/
  16. http://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/
  17. https://github.com/prakashvanapalli/tensorflow/blob/master/blogposts/backpropogation_with_images.ipynb
  18. https://medium.com/tag/neural-networks?source=post
  19. https://medium.com/tag/id26?source=post
  20. https://medium.com/tag/deep-learning?source=post
  21. https://medium.com/tag/artificial-intelligence?source=post
  22. https://medium.com/tag/tensorflow?source=post
  23. https://medium.com/@14prakash?source=footer_card
  24. https://medium.com/@14prakash
  25. http://twitter.com/fractalanalytics
  26. https://medium.com/@14prakash
  27. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  29. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b
  30. https://medium.com/p/97b794c97e5c/share/twitter
  31. https://medium.com/p/97b794c97e5c/share/facebook
  32. https://medium.com/p/97b794c97e5c/share/twitter
  33. https://medium.com/p/97b794c97e5c/share/facebook
