   #[1]alternate [2]alternate

   menu

     * [3]home
     * [4]about
     * [5]coding the deep learning revolution ebook
     * [6]contact
     * [7]ebook / newsletter sign-up

   search: ____________________

neural networks tutorial     a pathway to deep learning

   by [8]andy | [9]deep learning

     * you are here:
     * [10]home
     * [11]deep learning
     * [12]neural networks tutorial     a pathway to deep learning

   mar 18
   [13]0

   chances are, if you are searching for a tutorial on artificial neural
   networks (ann) you already have some idea of what they are, and what
   they are capable of doing.  but did you know that neural networks are
   the foundation of the new and exciting field of deep learning?  deep
   learning is the field of machine learning that is making many
   state-of-the-art advancements, from beating players at [14]go and
   [15]poker ([16]id23), to speeding up [17]drug
   discovery and [18]assisting self-driving cars.  if these types of
   cutting edge applications excite you like they excite me, then you will
   be interesting in learning as much as you can about deep learning.
   however, that requires you to know quite a bit about how neural
   networks work.  this tutorial article is designed to help you get up to
   speed in neural networks as quickly as possible.

   in this tutorial i   ll be presenting some concepts, code and maths that
   will enable you to build and understand a simple neural network.  some
   tutorials focus only on the code and skip the maths     but this impedes
   understanding. i   ll take things as slowly as possible, but it might
   help to brush up on your [19]matrices and [20]differentiation if you
   need to. the code will be in python, so it will be beneficial if you
   have a basic understanding of how python works.  you   ll pretty much get
   away with knowing about python functions, loops and the basics of
   the [21]numpy library.  by the end of this neural networks tutorial
   you   ll be able to build an ann in python that will correctly classify
   handwritten digits in images with a fair degree of accuracy.

   once you   re done with this tutorial, you can dive a little deeper with
   the following posts:

   [22]python tensorflow tutorial     build a neural network
   [23]improve your neural networks     part 1 [tips and tricks]
   [24]stochastic id119     mini-batch and more
     __________________________________________________________________

eager to learn more? get the book [25]here
     __________________________________________________________________

   all of the relevant code in this tutorial can be found [26]here.


   here   s an outline of the tutorial, with links, so you can easily
   navigate to the parts you want:

   [27]1 what are id158s?
   [28]2 the structure of an ann
   [29]2.1 the artificial neuron
   [30]2.2 nodes
   [31]2.3 the bias
   [32]2.4 putting together the structure
   [33]2.5 the notation
   [34]3 the feed-forward pass
   [35]3.1 a feed-forward example
   [36]3.2 our first attempt at a feed-forward function
   [37]3.3 a more efficient implementation
   [38]3.4 vectorisation in neural networks
   [39]3.5 id127
   [40]4 id119 and optimisation
   [41]4.1 a simple example in code
   [42]4.2 the cost function
   [43]4.3 id119 in neural networks
   [44]4.4 a two dimensional id119 example
   [45]4.5 id26 in depth
   [46]4.6 propagating into the hidden layers
   [47]4.7 vectorisation of id26
   [48]4.8 implementing the id119 step
   [49]4.9 the final id119 algorithm
   [50]5 implementing the neural network in python
   [51]5.1 scaling data
   [52]5.2 creating test and training datasets
   [53]5.3 setting up the output layer
   [54]5.4 creating the neural network
   [55]5.5 assessing the accuracy of the trained model

1 what are id158s?

   id158s (anns) are software implementations of the
   neuronal structure of our brains.  we don   t need to talk about the
   complex biology of our brain structures, but suffice to say, the brain
   contains neurons which are kind of like organic switches.  these can
   change their output state depending on the strength of their electrical
   or chemical input.  the neural network in a person   s brain is a hugely
   interconnected network of neurons, where the output of any given neuron
   may be the input to thousands of other neurons.  learning occurs by
   repeatedly activating certain neural connections over others, and this
   reinforces those connections.  this makes them more likely to produce a
   desired outcome given a specified input.  this learning involves
   feedback     when the desired outcome occurs, the neural connections
   causing that outcome become strengthened.

   id158s attempt to simplify and mimic this brain
   behaviour.  they can be trained in a supervised or unsupervised
   manner.  in a supervised ann, the network is trained by providing
   matched input and output data samples, with the intention of getting
   the ann to provide a desired output for a given input.  an example is
   an e-mail spam filter     the input training data could be the count of
   various words in the body of the e-mail, and the output training data
   would be a classification of whether the e-mail was truly spam or not.
   if many examples of e-mails are passed through the neural network this
   allows the network to learn what input data makes it likely that an
   e-mail is spam or not.  this learning takes place be adjusting the
   weights of the ann connections, but this will be discussed further in
   the next section.

   unsupervised learning in an ann is an attempt to get the ann to
      understand    the structure of the provided input data    on its own   .
   this type of ann will not be discussed in this post.

2 the structure of an ann

2.1 the artificial neuron

   the biological neuron is simulated in an ann by an activation function.
   in classification tasks (e.g. identifying spam e-mails) this activation
   function has to have a    switch on    characteristic     in other words,
   once the input is greater than a certain value, the output should
   change state i.e. from 0 to 1, from -1 to 1 or from 0 to >0. this
   simulates the    turning on    of a biological neuron. a common activation
   function that is used is the sigmoid function:

   \begin{equation*}
   f(z) = \frac{1}{1+exp(-z)}
   \end{equation*}

   which looks like this:
import matplotlib.pylab as plt
import numpy as np
x = np.arange(-8, 8, 0.1)
f = 1 / (1 + np.exp(-x))
plt.plot(x, f)
plt.xlabel('x')
plt.ylabel('f(x)')
plt.show()

   as can be seen in the figure above, the function is    activated    i.e. it
   moves from 0 to 1 when the input x is greater than a certain value. the
   sigmoid function isn   t a step function however, the edge is    soft   , and
   the output doesn   t change instantaneously. this means that there is a
   derivative of the function and this is important for the training
   algorithm which is discussed more in [56]section 4.

2.2 nodes

   as mentioned previously, biological neurons are connected hierarchical
   networks, with the outputs of some neurons being the inputs to others.
   we can represent these networks as connected layers of nodes. each node
   takes multiple weighted inputs, applies the activation function to the
   summation of these inputs, and in doing so generates an output. i   ll
   break this down further, but to help things along, consider the diagram
   below:

   figure 2. node with inputs

   the circle in the image above represents the node. the node is the
      seat    of the activation function, and takes the weighted inputs, sums
   them, then inputs them to the activation function. the output of the
   activation function is shown as h in the above diagram. note: a node as
   i have shown above is also called a id88 in some literature.

   what about this    weight    idea that has been mentioned? the weights are
   real valued numbers (i.e. not binary 1s or 0s), which are multiplied by
   the inputs and then summed up in the node. so, in other words, the
   weighted input to the node above would be:

   \begin{equation*}
   x_1w_1 + x_2w_2 + x_3w_3 + b
   \end{equation*}

   here the $w_i$ values are weights (ignore the $b$ for the moment).
   what are these weights all about?  well, they are the variables that
   are changed during the learning process, and, along with the input,
   determine the output of the node.  the $b$ is the weight of the +1 bias
   element     the inclusion of this bias enhances the flexibility of the
   node, which is best demonstrated in an example.

2.3 the bias

   let   s take an extremely simple node, with only one input and one
   output:


   figure 2. simple node

   the input to the activation function of the node in this case is simply
   $x_1w_1$.  what does changing $w_1$ do in this simple network?
w1 = 0.5
w2 = 1.0
w3 = 2.0
l1 = 'w = 0.5'
l2 = 'w = 1.0'
l3 = 'w = 2.0'
for w, l in [(w1, l1), (w2, l2), (w3, l3)]:
    f = 1 / (1 + np.exp(-x*w))
    plt.plot(x, f, label=l)
plt.xlabel('x')
plt.ylabel('h_w(x)')
plt.legend(loc=2)
plt.show()

   figure 4. effect of adjusting weights

   here we can see that changing the weight changes the slope of the
   output of the sigmoid activation function, which is obviously useful if
   we want to model different strengths of relationships between the input
   and output variables.  however, what if we only want the output to
   change when x is greater than 1?  this is where the bias comes in    
   let   s consider the same network with a bias input:

   figure 5. effect of bias


w = 5.0
b1 = -8.0
b2 = 0.0
b3 = 8.0
l1 = 'b = -8.0'
l2 = 'b = 0.0'
l3 = 'b = 8.0'
for b, l in [(b1, l1), (b2, l2), (b3, l3)]:
    f = 1 / (1 + np.exp(-(x*w+b)))
    plt.plot(x, f, label=l)
plt.xlabel('x')
plt.ylabel('h_wb(x)')
plt.legend(loc=2)
plt.show()

   figure 6. effect of bias adjusments

   in this case, the $w_1$ has been increased to simulate a more defined
      turn on    function.  as you can see, by varying the bias    weight    $b$,
   you can change when the node activates.  therefore, by adding a bias
   term, you can make the node simulate a generic if function, i.e. if (x
   > z) then 1 else 0.  without a bias term, you are unable to vary the z
   in that if statement, it will be always stuck around 0.  this is
   obviously very useful if you are trying to simulate conditional
   relationships.

2.4 putting together the structure

   hopefully the previous explanations have given you a good overview of
   how a given node/neuron/id88 in a neural network operates.
   however, as you are probably aware, there are many such interconnected
   nodes in a fully fledged neural network.  these structures can come in
   a myriad of different forms, but the most common simple neural network
   structure consists of an input layer, a hidden layer and an output
   layer.  an example of such a structure can be seen below:

   figure 10. three layer neural network

   the three layers of the network can be seen in the above figure     layer
   1 represents the input layer, where the external input data enters the
   network. layer 2 is called the hidden layer as this layer is not part
   of the input or output. note: neural networks can have many hidden
   layers, but in this case for simplicity i have just included one.
   finally, layer 3 is the output layer. you can observe the many
   connections between the layers, in particular between layer 1 (l1) and
   layer 2 (l2). as can be seen, each node in l1 has a connection to all
   the nodes in l2. likewise for the nodes in l2 to the single output node
   l3. each of these connections will have an associated weight.

2.5 the notation

   the maths below requires some fairly precise notation so that we know
   what we are talking about.  the notation i am using here is similar to
   that used in the stanford deep learning tutorial.  in the upcoming
   equations, each of these weights are identified with the following
   notation: ${w_{ij}}^{(l)}$. $i$ refers to the node number of the
   connection in layer $l+1$ and $j$ refers to the node number of the
   connection in layer $l$. take special note of this order. so, for the
   connection between node 1 in layer 1 and node 2 in layer 2, the weight
   notation would be ${w_{21}}^{(1)}$. this notation may seem a bit odd,
   as you would expect the *i* and *j* to refer the node numbers in layers
   $l$ and $l+1$ respectively (i.e. in the direction of input to output),
   rather than the opposite. however, this notation makes more sense when
   you add the bias.

   as you can observe in the figure above     the (+1) bias is connected to
   each of the nodes in the subsequent layer. so the bias in layer 1 is
   connected to the all the nodes in layer two. because the bias is not a
   true node with an activation function, it has no inputs (it always
   outputs the value +1). the notation of the bias weight is
   ${b_i}^{(l)}$, where *i* is the node number in the layer $l+1$     the
   same as used for the normal weight notation ${w_{21}}^{(1)}$. so, the
   weight on the connection between the bias in layer 1 and the second
   node in layer 2 is given by ${b_2}^{(1)}$.

   remember, these values     ${w_{ji}}^{(1)}$ and ${b_i}^{(l)}$     all need
   to be calculated in the training phase of the ann.

   finally, the node output notation is ${h_j}^{(l)}$, where $j$ denotes
   the node number in layer $l$ of the network. as can be observed in the
   three layer network above, the output of node 2 in layer 2 has the
   notation of ${h_2}^{(2)}$.

   now that we have the notation all sorted out, it is now time to look at
   how you calculate the output of the network when the input and the
   weights are known. the process of calculating the output of the neural
   network given these values is called the feed-forward pass or process.

3 the feed-forward pass

   to demonstrate how to calculate the output from the input in neural
   networks, let   s start with the specific case of the three layer neural
   network that was presented above. below it is presented in equation
   form, then it will be demonstrated with a concrete example and some
   python code:

   \begin{align}
   h_1^{(2)} &= f(w_{11}^{(1)}x_1 + w_{12}^{(1)} x_2 + w_{13}^{(1)} x_3 +
   b_1^{(1)}) \\
   h_2^{(2)} &= f(w_{21}^{(1)}x_1 + w_{22}^{(1)} x_2 + w_{23}^{(1)} x_3 +
   b_2^{(1)}) \\
   h_3^{(2)} &= f(w_{31}^{(1)}x_1 + w_{32}^{(1)} x_2 + w_{33}^{(1)} x_3 +
   b_3^{(1)}) \\
   h_{w,b}(x) &= h_1^{(3)} = f(w_{11}^{(2)}h_1^{(2)} + w_{12}^{(2)}
   h_2^{(2)} + w_{13}^{(2)} h_3^{(2)} + b_1^{(2)})
   \end{align}

   in the equation above $f(\bullet)$ refers to the node activation
   function, in this case the sigmoid function. the first line,
   ${h_1}^{(2)}$ is the output of the first node in the second layer, and
   its inputs are $w_{11}^{(1)}x_1$, $w_{12}^{(1)} x_2$, $w_{13}^{(1)}x_3$
   and $b_1^{(1)}$. these inputs can be traced in the three-layer
   connection diagram above. they are simply summed and then passed
   through the activation function to calculate the output of the first
   node. likewise, for the other two nodes in the second layer.

   the final line is the output of the only node in the third and final
   layer, which is ultimate output of the neural network. as can be
   observed, rather than taking the weighted input variables ($x_1, x_2,
   x_3$), the final node takes as input the weighted output of the nodes
   of the second layer ($h_{1}^{(2)}$, $h_{2}^{(2)}$, $h_{3}^{(2)}$), plus
   the weighted bias. therefore, you can see in equation form the
   hierarchical nature of id158s.

3.1 a feed-forward example

   now, let   s do a simple first example of the output of this neural
   network in python. first things first, notice that the weights between
   layer 1 and 2 ($w_{11}^{(1)}, w_{12}^{(1)}, \dots$) are ideally suited
   to matrix representation? observe:

   \begin{equation}
   w^{(1)} =
   \begin{pmatrix}
   w_{11}^{(1)} & w_{12}^{(1)} & w_{13}^{(1)} \\
   w_{21}^{(1)} & w_{22}^{(1)} & w_{23}^{(1)} \\
   w_{31}^{(1)} & w_{32}^{(1)} & w_{33}^{(1)} \\
   \end{pmatrix}
   \end{equation}

   this matrix can be easily represented using numpy arrays:
import numpy as np
w1 = np.array([[0.2, 0.2, 0.2], [0.4, 0.4, 0.4], [0.6, 0.6, 0.6]])

   here i have just filled up the layer 1 weight array with some example
   weights. we can do the same for the layer 2 weight array:

   \begin{equation}
   w^{(2)} =
   \begin{pmatrix}
   w_{11}^{(2)} & w_{12}^{(2)} & w_{13}^{(2)}
   \end{pmatrix}
   \end{equation}
w2 = np.zeros((1, 3))
w2[0,:] = np.array([0.5, 0.5, 0.5])

   we can also setup some dummy values in the layer 1 bias weight
   array/vector, and the layer 2 bias weight (which is only a single value
   in this neural network structure     i.e. a scalar):
b1 = np.array([0.8, 0.8, 0.8])
b2 = np.array([0.2])

   finally, before we write the main program to calculate the output from
   the neural network, it   s handy to setup a separate python function for
   the activation function:
def f(x):
    return 1 / (1 + np.exp(-x))

3.2 our first attempt at a feed-forward function

   below is a simple way of calculating the output of the neural network,
   using nested loops in python.  we   ll look at more efficient ways of
   calculating the output shortly.
def simple_looped_nn_calc(n_layers, x, w, b):
    for l in range(n_layers-1):
        #setup the input array which the weights will be multiplied by for each
layer
        #if it's the first layer, the input array will be the x input vector
        #if it's not the first layer, the input to the next layer will be the
        #output of the previous layer
        if l == 0:
            node_in = x
        else:
            node_in = h
        #setup the output array for the nodes in layer l + 1
        h = np.zeros((w[l].shape[0],))
        #loop through the rows of the weight array
        for i in range(w[l].shape[0]):
            #setup the sum inside the activation function
            f_sum = 0
            #loop through the columns of the weight array
            for j in range(w[l].shape[1]):
                f_sum += w[l][i][j] * node_in[j]
            #add the bias
            f_sum += b[l][i]
            #finally use the activation function to calculate the
            #i-th output i.e. h1, h2, h3
            h[i] = f(f_sum)
    return h

   this function takes as input the number of layers in the neural
   network, the x input array/vector, then python tuples or lists of the
   weights and bias weights of the network, with each element in the
   tuple/list representing a layer $l$ in the network.  in other words,
   the inputs are setup in the following:
w = [w1, w2]
b = [b1, b2]
#a dummy x input vector
x = [1.5, 2.0, 3.0]

   the function first checks what the input is to the layer of
   nodes/weights being considered. if we are looking at the first layer,
   the input to the second layer nodes is the input vector $x$ multiplied
   by the relevant weights. after the first layer though, the inputs to
   subsequent layers are the output of the previous layers. finally, there
   is a nested loop through the relevant $i$ and $j$ values of the weight
   vectors and the bias. the function uses the dimensions of the weights
   for each layer to figure out the number of nodes and therefore the
   structure of the network.

   calling the function:
simple_looped_nn_calc(3, x, w, b)

   gives the output of 0.8354.  we can confirm this results by manually
   performing the calculations in the original equations:

   \begin{align}
   h_1^{(2)} &= f(0.2*1.5 + 0.2*2.0 + 0.2*3.0 + 0.8) = 0.8909 \\
   h_2^{(2)} &= f(0.4*1.5 + 0.4*2.0 + 0.4*3.0 + 0.8) = 0.9677 \\
   h_3^{(2)} &= f(0.6*1.5 + 0.6*2.0 + 0.6*3.0 + 0.8) = 0.9909 \\
   h_{w,b}(x) &= h_1^{(3)} = f(0.5*0.8909 + 0.5*0.9677 + 0.5*0.9909 + 0.2)
   = 0.8354
   \end{align}

3.3 a more efficient implementation

   as was stated earlier     using loops isn   t the most efficient way of
   calculating the feed forward step in python. this is because the loops
   in python are notoriously slow. an alternative, more efficient
   mechanism of doing the feed forward step in python and numpy will be
   discussed shortly. we can benchmark how efficient the algorithm is by
   using the %timeit function in ipython, which runs the function a number
   of times and returns the average time that the function takes to run:
%timeit simple_looped_nn_calc(3, x, w, b)

   running this tells us that the looped feed forward takes $40\mu s$. a
   result in the tens of microseconds sounds very fast, but when applied
   to very large practical nns with 100s of nodes per layer, this speed
   will become prohibitive, especially when training the network, as will
   become clear later in this tutorial.  if we try a four layer neural
   network using the same code, we get significantly worse performance    
   $70\mu s$ in fact.

3.4 vectorisation in neural networks

   there is a way to write the equations even more compactly, and to
   calculate the feed forward process in neural networks more efficiently,
   from a computational perspective.  firstly, we can introduce a new
   variable $z_{i}^{(l)}$ which is the summated input into node $i$ of
   layer $l$, including the bias term.  so in the case of the first node
   in layer 2, $z$ is equal to:

   $$z_{1}^{(2)} = w_{11}^{(1)}x_1 + w_{12}^{(1)} x_2 + w_{13}^{(1)} x_3 +
   b_1^{(1)} = \sum_{j=1}^{n} w_{ij}^{(1)}x_i + b_{i}^{(1)}$$

   where n is the number of nodes in layer 1.  using this notation, the
   unwieldy previous set of equations for the example three layer network
   can be reduced to:

   \begin{align}
   z^{(2)} &= w^{(1)} x + b^{(1)} \\
   h^{(2)} &= f(z^{(2)}) \\
   z^{(3)} &= w^{(2)} h^{(2)} + b^{(2)} \\
   h_{w,b}(x) &= h^{(3)} = f(z^{(3)})
   \end{align}

   note the use of capital w to denote the matrix form of the weights.  it
   should be noted that all of the elements in the above equation are now
   matrices / vectors.  if you   re unfamiliar with these concepts, they
   will be explained more fully in the next section. can the above
   equation be simplified even further?  yes, it can.  we can forward
   propagate the calculations through any number of layers in the neural
   network by generalising:

   \begin{align}
   z^{(l+1)} &= w^{(l)} h^{(l)} + b^{(l)}   \\
   h^{(l+1)} &= f(z^{(l+1)})
   \end{align}

   here we can see the general feed forward process, where the output of
   layer $l$ becomes the input to layer $l+1$. we know that $h^{(1)}$ is
   simply the input layer $x$ and $h^{(n_l)}$ (where $n_l$ is the number
   of layers in the network) is the output of the output layer. notice in
   the above equations that we have dropped references to the node numbers
   $i$ and $j$     how can we do this? don   t we still have to loop through
   and calculate all the various node inputs and outputs?

   the answer is that we can use id127s to do this more
   simply. this process is called    vectorisation    and it has two benefits
       first, it makes the code less complicated, as you will see shortly.
   second, we can use fast id202 routines in python (and other
   languages) rather than using loops, which will speed up our programs.
   numpy can handle these calculations easily. first, for those who aren   t
   familiar with matrix operations, the next section is a brief recap.

3.5 id127

   let   s expand out $z^{(l+1)} = w^{(l)} h^{(l)} + b^{(l)}$ in explicit
   matrix/vector form for the input layer (i.e. $h^{(l)} = x$):

   \begin{align}
   z^{(2)} &=
   \begin{pmatrix}
   w_{11}^{(1)} & w_{12}^{(1)} & w_{13}^{(1)} \\
   w_{21}^{(1)} & w_{22}^{(1)} & w_{23}^{(1)} \\
   w_{31}^{(1)} & w_{32}^{(1)} & w_{33}^{(1)} \\
   \end{pmatrix}
   \begin{pmatrix}
   x_{1} \\
   x_{2} \\
   x_{3} \\
   \end{pmatrix} +
   \begin{pmatrix}
   b_{1}^{(1)} \\
   b_{2}^{(1)} \\
   b_{3}^{(1)} \\
   \end{pmatrix} \\
   &=
   \begin{pmatrix}
   w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{2} + w_{13}^{(1)}x_{3} \\
   w_{21}^{(1)}x_{1} + w_{22}^{(1)}x_{2} + w_{23}^{(1)}x_{3} \\
   w_{31}^{(1)}x_{1} + w_{32}^{(1)}x_{2} + w_{33}^{(1)}x_{3} \\
   \end{pmatrix} +
   \begin{pmatrix}
   b_{1}^{(1)} \\
   b_{2}^{(1)} \\
   b_{3}^{(1)} \\
   \end{pmatrix} \\
   &=
   \begin{pmatrix}
   w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{2} + w_{13}^{(1)}x_{3} + b_{1}^{(1)}
   \\
   w_{21}^{(1)}x_{1} + w_{22}^{(1)}x_{2} + w_{23}^{(1)}x_{3} + b_{2}^{(1)}
   \\
   w_{31}^{(1)}x_{1} + w_{32}^{(1)}x_{2} + w_{33}^{(1)}x_{3} + b_{3}^{(1)}
   \\
   \end{pmatrix} \\
   \end{align}

   for those who aren   t aware of how id127 works, it is a
   good idea to scrub up on matrix operations. there are many [57]sites
   which cover this well. however, just quickly, when the weight matrix is
   multiplied by the input layer vector, each element in the $row$ of the
   weight matrix is multiplied by each element in the single $column$ of
   the input vector, then summed to create a new (3 x 1) vector. then you
   can simply add the bias weights vector to achieve the final result.

   you can observe how each row of the final result above corresponds to
   the argument of the activation function in the original non-matrix set
   of equations above. if the activation function is capable of being
   applied element-wise (i.e. to each row separately in the $z^{(1)}$
   vector), then we can do all our calculations using matrices and vectors
   rather than slow python loops. thankfully, numpy allows us to do just
   that, with reasonably fast matrix operations and element-wise
   functions. let   s have a look at a much more simplified (and faster)
   version of the simple_looped_nn_calc:
def matrix_feed_forward_calc(n_layers, x, w, b):
    for l in range(n_layers-1):
        if l == 0:
            node_in = x
        else:
            node_in = h
        z = w[l].dot(node_in) + b[l]
        h = f(z)
    return h

   note line 7  where the id127 occurs     if you just use
   the $*$ symbol when multiplying the weights by the node input vector in
   numpy it will attempt to perform some sort of element-wise
   multiplication, rather than the true id127 that we
   desire. therefore you need to use the a.dot(b) notation when performing
   id127 in numpy.

   if we perform %timeit again using this new function and a simple
   4 layer network, we only get an improvement of $24\mu s$ (a reduction
   from $70\mu s$ to $46\mu s$).  however, if we increase the size of the
   4 layer network to layers of 100-100-50-10 nodes the results are much
   more impressive.  the python looped based method takes a whopping
   $41ms$     note, that is milliseconds, and the vectorised implementation
   only takes $84\mu s$ to forward propagate through the neural network.
   by using vectorised calculations instead of python loops we have
   increased the efficiency of the calculation 500 fold! that   s a huge
   improvement. there is even the possibility of faster implementations of
   matrix operations using deep learning packages such as [58]tensorflow
   and [59]theano which utilise your computer   s gpu (rather than the cpu),
   the architecture of which is more suited to fast matrix computations
   (i have a [60]tensorflow tutorial post also).

   that brings us to an end of the feed-forward introduction for neural
   networks.  the next section will deal with how to actually train a
   neural network so that it can perform classification tasks, using
   id119 and id26.

4 id119 and optimisation

   as mentioned in [61]section 1, the setting of the values of the weights
   which link the layers in the network is what constitutes the training
   of the system. in supervised learning, the idea is to reduce the error
   between the input and the desired output. so if we have a neural
   network with one output layer, and given some input $x$ we want
   the neural network to output a 2, yet the network actually produces a
   5, a simple expression of the error is $abs(2-5)=3$. for the
   mathematically minded, this would be the $l^1$ norm of the error (don   t
   worry about it if you don   t know what this is).

   the idea of supervised learning is to provide many input-output pairs
   of known data and vary the weights based on these samples so that the
   error expression is minimised. we can specify these input-output pairs
   as $\{ (x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)}) \}$ where $m$ is
   the number of training samples that we have on hand to train the
   weights of the network. each of these inputs or outputs can be vectors
       that is $x^{(1)}$ is not necessarily just one value, it could be an
   $n$ dimensional series of values. for instance, let   s say that we   re
   training a spam-detection neural network     in such a case $x^{(1)}$
   could be a count of all the different significant words in an e-mail
   e.g.:

   \begin{align}
   x^{(1)} &=
   \begin{pmatrix}
   no. of    prince    \\
   no. of    nigeria    \\
   no. of    extension    \\
   \vdots \\
   no. of    mum    \\
   no. of    burger    \\
   \end{pmatrix} \\
   &=
   \begin{pmatrix}
   2 \\
   2 \\
   0 \\
   \vdots \\
   0 \\
   1 \\
   \end{pmatrix}
   \end{align}

   $y^{(1)}$ in this case could be a single scalar value, either a 1 or a
   0 to designate whether the e-mail is spam or not. or, in other
   applications it could be a $k$ dimensional vector. as an example, say
   we have input $x$ that is a vector of the pixel greyscale readings of a
   photograph.  we also have an output $y$ that is a 26 dimensional vector
   that designates, with a 1 or 0, what letter of the alphabet is shown in
   the photograph i.e. $(1, 0, \ldots, 0)$ for a, $(0, 1, \ldots, 0)$ for
   b and so on.  this 26 dimensional output vector could be used to
   classify letters in photographs.

   in training the network with these $(x, y)$ pairs, the goal is to get
   the neural network better and better at predicting the correct $y$
   given $x$. this is performed by varying the weights so as to minimize
   the error. how do we know how to vary the weights, given an error in
   the output of the network? this is where the concept of gradient
   descent comes in handy. consider the diagram below:


   figure 8. simple, one-dimensional id119

   in this diagram we have a blue plot of the error depending on a single
   scalar weight value, $w$. the minimum possible error is marked by the
   black cross, but we don   t know what $w$ value gives that minimum error.
   we start out at a random value of $w$, which gives an error marked by
   the red dot on the curve labelled with    1   . we need to change $w$ in a
   way to approach that minimum possible error, the black cross. one of
   the most common ways of approaching that value is called gradient
   descent.

   to proceed with this method, first the gradient of the error with
   respect to $w$ is calculated at point    1   . for those who don   t know,
   the gradient is the slope of the error curve at that point. it is shown
   in the diagram above by the black arrow which    pierces    point    1   . the
   gradient also gives directional information     if it is positive with
   respect to an increase in $w$, a step in that direction will lead to an
   increase in the error. if it is negative with respect to an increase in
   $w$ (as it is in the diagram above), a step in that will lead to a
   decrease in the error. obviously, we wish to make a step in $w$ that
   will lead to a decrease in the error. the magnitude of the gradient or
   the    steepness    of the slope, gives an indication of how fast the error
   curve or function is changing at that point. the higher the magnitude
   of the gradient, the faster the error is changing at that point with
   respect to $w$.

   the id119 method uses the gradient to make an informed step
   change in $w$ to lead it towards the minimum of the error curve. this
   is an iterative method, that involves multiple steps. each time, the
   $w$ value is updated according to:

   \begin{equation}
   w_{new} = w_{old}     \alpha * \nabla error
   \end{equation}

   here $w_{new}$ denotes the new $w$ position, $w_{old}$ denotes the
   current or old $w$ position, $\nabla error$ is the gradient of the
   error at $w_{old}$ and $\alpha$ is the step size. the step size
   $\alpha$ will determine how quickly the solution converges on the
   minimum error. however, this parameter has to be tuned     if it is too
   large, you can imagine the solution bouncing around on either side of
   the minimum in the above diagram. this will result in an optimisation
   of $w$ that does not converge. as this iterative algorithm approaches
   the minimum, the gradient or change in the error with each step will
   reduce. you can see in the graph above that the gradient lines will
      flatten out    as the solution point approaches the minimum. as the
   solution approaches the minimum error, because of the decreasing
   gradient, it will result in only small improvements to the error.  when
   the solution approaches this    flattening    out of the error we want to
   exit the iterative process.  this exit can be performed by either
   stopping after a certain number of iterations or via some sort of    stop
   condition   .  this stop condition might be when the change in the error
   drops below a certain limit, often called the precision.

4.1 a simple example in code

   below is an example of a simple python implementation of gradient
   descent for solving the minimum of the equation $f(x) = x^4     3x^3 + 2$
   taken from [62]wikipedia.  the gradient of this function is able to be
   calculated analytically (i.e. we can do it easily using calculus, which
   we can   t do with many real world applications) and is $f'(x) = 4x^3    
   9x^2$. this means at every value of $x$, we can calculate the gradient
   of the function by using a simple equation. again, using calculus we
   can know that the exact minimum of this equation is $x = 2.25$ .
x_old = 0 # the value does not matter as long as abs(x_new - x_old) > precision
x_new = 6 # the algorithm starts at x=6
gamma = 0.01 # step size
precision = 0.00001

def df(x):
    y = 4 * x**3 - 9 * x**2
    return y

while abs(x_new - x_old) > precision:
    x_old = x_new
    x_new += -gamma * df(x_old)

print("the local minimum occurs at %f" % x_new)

   this function prints    the local minimum occurs at 2.249965   , which
   agrees with the exact solution within the precision.  this code
   implements the weight adjustment algorithm that i showed above, and can
   be seen to find the minimum of the function correctly within the given
   precision. this is a very simple example of id119, and
   finding the gradient works quite differently when training neural
   networks. however, the main idea remains     we figure out the gradient
   of the neural network then adjust the weights in a step to try to get
   closer to the minimum error that we are trying to find. another
   difference between this toy example of id119 is that the
   weight vector is multi-dimensional, and therefore the id119
   method must search a multi-dimensional space for the minimum point.

   the way we figure out the gradient of a neural network is via the
   famous id26 method, which will be discussed shortly. first
   however, we have to look at the error function more closely.

4.2 the cost function

   previously, we   ve talked about iteratively minimising the error of the
   output of the neural network by varying the weights in gradient
   descent. however, as it turns out, there is a mathematically more
   generalised way of looking at things that allows us to reduce the error
   while also preventing things like overfitting (this will be discussed
   more in later articles). this more general optimisation formulation
   revolves around minimising what   s called the cost function. the
   equivalent cost function of a single training pair ($x^z$, $y^z$) in a
   neural network is:

   \begin{align}
   j(w,b,x,y) &= \frac{1}{2} \parallel y^z     h^{(n_l)}(x^z) \parallel ^2
   \\
   &= \frac{1}{2} \parallel y^z     y_{pred}(x^z) \parallel ^2
   \end{align}

   this shows the cost function of the $z_{th}$ training sample, where
   $h^{(n_l)}$ is the output of the final layer of the neural network i.e.
   the output of the neural network. i   ve also represented $h^{(n_l)}$ as
   $y_{pred}$ to highlight the prediction of the neural network given
   $x^z$. the two vertical lines represent the $l^2$ norm of the error, or
   what is known as the sum-of-squares error (sse). sse is a very common
   way of representing the error of a machine learning system. instead of
   taking just the absolute error $abs(y_{pred}(x^z)     y^z)$, we use the
   square of the error. there are many reasons why the sse is often used
   which will not be discussed here     suffice to say that this is a very
   common way of representing the errors in machine learning. the
   $\frac{1}{2}$ out the front is just a constant added that tidies things
   up when we differentiate the cost function, which we   ll be doing when
   we perform id26.

   note that the formulation for the cost function above is for a single
   $(x,y)$ training pair. we want to minimise the cost function over all
   of our $m$ training pairs. therefore, we want to find the minimum *mean
   squared error* (mse) over all the training samples:

   \begin{align}
   j(w,b) &= \frac{1}{m} \sum_{z=0}^m \frac{1}{2} \parallel y^z    
   h^{(n_l)}(x^z) \parallel ^2 \\
   &= \frac{1}{m} \sum_{z=0}^m j(w, b, x^{(z)}, y^{(z)})
   \end{align}

   so, how do you use the cost function $j$ above to train the weights of
   our network? using id119 and id26. first, let   s
   look at id119 more closely in neural networks.

4.3 id119 in neural networks

   id119 for every weight $w_{(ij)}^{(l)}$ and every bias
   $b_i^{(l)}$ in the neural network looks like the following:

   \begin{align}
   w_{ij}^{(l)} &= w_{ij}^{(l)}     \alpha \frac{\partial}{\partial
   w_{ij}^{(l)}} j(w,b) \\
   b_{i}^{(l)} &= b_{i}^{(l)}     \alpha \frac{\partial}{\partial
   b_{i}^{(l)}} j(w,b)
   \end{align}

   basically, the equation above is similiar to the previously shown
   id119 algorithm: $w_{new} = w_{old}     \alpha * \nabla
   error$. the new and old subscripts are missing, but the values on the
   left side of the equation are new and the values on the right side are
   old. again, we have an iterative process whereby the weights are
   updated in each iteration, this time based on the cost function
   $j(w,b)$.

   the values $\frac{\partial}{\partial w_{ij}^{(l)}}$ and
   $\frac{\partial}{\partial b_{i}^{(l)}}$ are the partial derivatives of
   the single sample cost function based on the weight values. what does
   this mean? recall that for the simple id119 example
   mentioned previously, each step depends on the slope of the error/cost
   term with respect to the weights. another word for slope or gradient is
   the derivative. a normal derivative has the notation $\frac{d}{dx}$. if
   $x$ in this instance is a vector, then such a derivative will also be a
   vector, displaying the gradient in all the dimensions of $x$.

4.4 a two dimensional id119 example

   let   s take the example of a standard two-dimensional id119
   problem. below is a diagram of an iterative two-dimensional gradient
   descent run:

   figure 9. two-dimensional id119

   the blue lines in the above diagram are the contour lines of the cost
   function     designating regions with an error value that is
   approximately the same. as can be observed in the diagram above, each
   step ($p_1 \to p_2 \to p_3$) in the id119 involves a
   gradient or derivative that is an arrow/vector.  this vector spans both
   the $[x_1, x_2]$ dimensions, as the solution works its way towards the
   minimum in the centre. so, for instance, the derivative evaluated at
   $p_1$ might be $\frac {d}{dx} = [2.1, 0.7]$, where the derivative is a
   vector to designate the two directions. the partial derivative $\frac
   {\partial}{\partial x_1}$ in this case would be a scalar $\to [2.1]$    
   in other words, it is the gradient in only one direction of the search
   space ($x_1$). in id119, it is often the case that the
   partial derivative of all the possible search directions are
   calculated, then    gathered up    to determine a new, complete, step
   direction.

   in neural networks, we don   t have a simple cost function where we can
   easily evaluate the gradient, like we did in our toy id119
   example ($f(x) = x^4     3x^3 + 2$). in fact, things are even trickier.
   while we can compare the output of the neural network to our expected
   training value, $y^{(z)}$ and feasibly look at how changing the weights
   of the output layer would change the cost function for the sample (i.e.
   calculating the gradient), how on earth do we do that for all the
   hidden layers of the network?

   the answer to that is the id26 method. this method allows us
   to    share    the cost function or error to all the weights in the network
       or in other words, it allows us to determine how much of the error is
   caused by any given weight.

4.5 id26 in depth

   in this section, i   m going to delve into the maths a little. if you   re
   wary of the maths of how id26 works, then it may be best to
   skip this section.  the next [63]section will show you how to implement
   id26 in code     so if you want to skip straight on to using
   this method, feel free to skip the rest of this section. however, if
   you don   t mind a little bit of maths, i encourage you to push on to the
   end of this section as it will give you a good depth of
   understanding in training neural networks. this will be invaluable to
   understanding some of the key ideas in deep learning, rather than just
   being a code cruncher who doesn   t really understand how the code works.

   first let   s recall some of the foundational equations from [64]section
   3 for the following three layer neural network:

   figure 10. three layer neural network (again)

   the output of this neural network can be calculated by:

   \begin{equation}
   h_{w,b}(x) = h_1^{(3)} = f(w_{11}^{(2)}h_1^{(2)} + w_{12}^{(2)}
   h_2^{(2)} + w_{13}^{(2)} h_3^{(2)} + b_1^{(2)})
   \end{equation}

   we can also simplify the above to $h_1^{(3)} = f(z_1^{(2)})$ by
   defining $z_1^{(2)}$ as:

   $$z_{1}^{(2)} = w_{11}^{(2)}h_1^{(2)} + w_{12}^{(2)} h_2^{(2)} +
   w_{13}^{(2)} h_3^{(2)} + b_1^{(2)}$$

   let   s say we want to find out how much a change in the weight
   $w_{12}^{(2)}$ has on the cost function $j$. this is to evaluate $\frac
   {\partial j}{\partial w_{12}^{(2)}}$. to do so, we have to use
   something called the chain function:

   $$\frac {\partial j}{\partial w_{12}^{(2)}} = \frac {\partial
   j}{\partial h_1^{(3)}} \frac {\partial h_1^{(3)}}{\partial z_1^{(2)}}
   \frac {\partial z_1^{(2)}}{\partial w_{12}^{(2)}}$$

   if you look at the terms on the right     the numerators    cancel out    the
   denominators, in the same way that $\frac {2}{5} \frac {5}{2} = \frac
   {2}{2} = 1$. therefore we can construct $\frac {\partial j}{\partial
   w_{12}^{(2)}}$ by stringing together a few partial derivatives (which
   are quite easy, thankfully). let   s start with $\frac {\partial
   z_1^{(2)}}{\partial w_{12}^{(2)}}$:

   \begin{align}
   \frac {\partial z_1^{(2)}}{\partial w_{12}^{(2)}} &= \frac
   {\partial}{\partial w_{12}^{(2)}} (w_{11}^{(1)}h_1^{(2)} + w_{12}^{(1)}
   h_2^{(2)} + w_{13}^{(1)} h_3^{(2)} + b_1^{(1)})\\
   &= \frac {\partial}{\partial w_{12}^{(2)}} (w_{12}^{(1)} h_2^{(2)})\\
   &= h_2^{(2)}
   \end{align}

   the partial derivative of $z_1^{(2)}$ with respect $w_{12}^{(2)}$ only
   operates on one term within the parentheses, $w_{12}^{(1)} h_2^{(2)}$,
   as all the other terms don   t vary at all when $w_{12}^{(2)}$ does. the
   derivative of a constant is 1, therefore $\frac {\partial}{\partial
   w_{12}^{(2)}} (w_{12}^{(1)} h_2^{(2)})$ collapses to just $h_2^{(2)}$,
   which is simply the output of the second node in layer 2.

   the next partial derivative in the chain is $\frac {\partial
   h_1^{(3)}}{\partial z_1^{(2)}}$, which is the partial derivative of the
   activation function of the $h_1^{(3)}$ output node. because of the
   requirement to be able to derive this derivative, the activation
   functions in neural networks need to be differentiable. for the common
   sigmoid activation function (shown in [65]section 2.1), the derivative
   is:

   $$\frac {\partial h}{\partial z} = f'(z) = f(z)(1-f(z))$$

   where $f(z)$ is the activation function. so far so good     now we have
   to work out how to deal with the first term $\frac {\partial
   j}{\partial h_1^{(3)}}$. remember that $j(w,b,x,y)$ is the mean squared
   error id168, which looks like (for our case):

   $$j(w,b,x,y) = \frac{1}{2} \parallel y_1     h_1^{(3)}(z_1^{(2)})
   \parallel ^2$$

   here $y_1$ is the training target for the output node. again using the
   chain rule:

   \begin{align}
   &let\ u = \parallel y_1     h_1^{(3)}(z_1^{(2)}) \parallel\ and\ j =
   \frac {1}{2} u^2\\
   &using\ \frac {\partial j}{\partial h} = \frac {\partial j}{\partial u}
   \frac {\partial u}{\partial h}:\\
   &\frac {\partial j}{\partial h} = -(y_1     h_1^{(3)})
   \end{align}

   so we   ve now figured out how to calculate $\frac {\partial j}{\partial
   w_{12}^{(2)}}$, at least for the weights connecting the output layer.
   before we move to any hidden layers (i.e. layer 2 in our example case),
   let   s introduce some simplifications to tighten up our notation and
   introduce $\delta$:

   $$\delta_i^{(n_l)} = -(y_i     h_i^{(n_l)})\cdot f^\prime(z_i^{(n_l)})$$

   where $i$ is the node number of the output layer. in our selected
   example there is only one such layer, therefore $i=1$ always in this
   case. now we can write the complete cost function derivative as:

   \begin{align}
   \frac{\partial}{\partial w_{ij}^{(l)}} j(w,b,x, y) &= h^{(l)}_j
   \delta_i^{(l+1)} \\
   \end{align}

   where, for the output layer in our case, $l$ = 2 and $i$ remains the
   node number.

4.6 propagating into the hidden layers

   what about for weights feeding into any hidden layers (layer 2 in our
   case)? for the weights connecting the output layer, the $\frac
   {\partial j}{\partial h} = -(y_i     h_i^{(n_l)})$ derivative made sense,
   as the cost function can be directly calculated by comparing the output
   layer to the training data. the output of the hidden nodes, however,
   have no such direct reference, rather, they are connected to the cost
   function only through mediating weights and potentially other layers of
   nodes. how can we find the variation in the cost function from changes
   to weights embedded deep within the neural network? as mentioned
   previously, we use the id26 method.

   now that we   ve done the hard work using the chain rule, we   ll now take
   a more graphical approach. the term that needs to propagate back
   through the network is the $\delta_i^{(n_l)}$ term, as this is the
   network   s ultimate connection to the cost function. what about node j
   in the second layer (hidden layer)? how does it contribute to
   $\delta_i^{(n_l)}$ in our test network? it contributes via the weight
   $w_{ij}^{(2)}$     see the diagram below for the case of $j=1$ and $i=1$.

   figure 11. simple id26 illustration

   as can be observed from above, the output layer $\delta$ is
   communicated to the hidden node by the weight of the connection. in the
   case where there is only one output layer node, the generalised hidden
   layer $\delta$ is defined as:

   $$\delta_j^{(l)} = \delta_1^{(l+1)} w_{1j}^{(l)}\ f^\prime(z_j)^{(l)}$$

   where $j$ is the node number in layer $l$. what about the case where
   there are multiple output nodes? in this case, the weighted sum of all
   the communicated errors are taken to calculate $\delta_j^{(l)}$, as
   shown in the diagram below:

   figure 12. id26 illustration with multiple outputs

   as can be observed from the above, each $\delta$ value from the output
   layer is included in the sum used to calculate $\delta_1^{(2)}$, but
   each output $\delta$ is weighted according to the appropriate
   $w_{i1}^{(2)}$ value. in other words, node 1 in layer 2 contributes to
   the error of three output nodes, therefore the measured error (or cost
   function value) at each of these nodes has to be    passed back    to the
   $\delta$ value for this node. now we can develop a generalised
   expression for the $\delta$ values for nodes in the hidden layers:

   $$\delta_j^{(l)} = (\sum_{i=1}^{s_{(l+1)}} w_{ij}^{(l)}
   \delta_i^{(l+1)})\ f^\prime(z_j^{(l)})$$

   where $j$ is the node number in layer $l$ and $i$ is the node number in
   layer $l+1$ (which is the same notation we have used from the start).
   the value $s_{(l+1)}$ is the number of nodes in layer $(l+1)$.

   so we now know how to calculate: $$\frac{\partial}{\partial
   w_{ij}^{(l)}} j(w,b,x, y) = h^{(l)}_j \delta_i^{(l+1)}$$ as shown
   previously. what about the bias weights? i   m not going to derive them
   as i did with the normal weights in the interest of saving time /
   space. however, the reader shouldn   t have too many issues following the
   same steps, using the chain rule, to arrive at:

   $$\frac{\partial}{\partial b_{i}^{(l)}} j(w,b,x, y) =
   \delta_i^{(l+1)}$$

   great     so we now know how to perform our original id119
   problem for neural networks:

   \begin{align}
   w_{ij}^{(l)} &= w_{ij}^{(l)}     \alpha \frac{\partial}{\partial
   w_{ij}^{(l)}} j(w,b) \\
   b_{i}^{(l)} &= b_{i}^{(l)}     \alpha \frac{\partial}{\partial
   b_{i}^{(l)}} j(w,b)
   \end{align}

   however, to perform this id119 training of the weights, we
   would have to resort to loops within loops. as previously shown
   in [66]section 3.4 of this neural network tutorial, performing such
   calculations in python using loops is slow for large networks.
   therefore, we need to figure out how to vectorise such calculations,
   which the next section will show.

4.7 vectorisation of id26

   to consider how to vectorise the id119 calculations in
   neural networks, let   s first look at a na  ve vectorised version of the
   gradient of the cost function (warning: this is not in a correct form
   yet!):

   \begin{align}
   \frac{\partial j}{\partial w^{(l)}} &= h^{(l)} \delta^{(l+1)}\\
   \frac{\partial j}{\partial b^{(l)}} &= \delta^{(l+1)}
   \end{align}

   now, let   s look at what element of the above equations. what does
   $h^{(l)}$ look like? pretty simple, just a $(s_l \times 1)$ vector,
   where $s_l$ is the number of nodes in layer $l$. what does the
   multiplication of $h^{(l)} \delta^{(l+1)}$ look like? well, because we
   know that $\alpha \times \frac{\partial j}{\partial w^{(l)}}$ must be
   the same size of the weight matrix $w^{(l)}$, we know that the outcome
   of $h^{(l)} \delta^{(l+1)}$ must also be the same size as the weight
   matrix for layer $l$. in other words it has to be of size $(s_{l+1}
   \times s_{l})$.

   we know that $\delta^{(l+1)}$ has the dimension $(s_{l+1} \times 1)$
   and that $h^{(l)}$ has the dimension of $(s_l \times 1)$. the rules of
   id127 show that a matrix of dimension $(\mathbf n
   \times m)$ multiplied by a matrix of dimension $(o \times \mathbf p)$
   will have a product matrix of size $(\mathbf n \times \mathbf p)$. if
   we perform a straight multiplication between $h^{(l)}$ and
   $\delta^{(l+1)}$, the number of columns of the first vector (i.e. 1
   column) will not equal the number of rows of the second vector (i.e. 3
   rows), therefore we can   t perform a proper id127. the
   only way we can get a proper outcome of size $(s_{l+1} \times s_{l})$
   is by using a matrix transpose. a transpose swaps the dimensions of a
   matrix around e.g. a $(s_l \times 1)$ sized vector becomes a $(1 \times
   s_l)$ sized vector, and is denoted by a superscript of $t$. therefore,
   we can do the following:

   $$\delta^{(l+1)} (h^{(l)})^t = (s_{l+1} \times 1) \times (1 \times s_l)
   = (s_{l+1} \times s_l)$$

   as can be observed below, by using the transpose operation we can
   arrive at the outcome we desired.

   a final vectorisation that can be performed is during the weighted
   addition of the errors in the id26 step:

   $$\delta_j^{(l)} = (\sum_{i=1}^{s_{(l+1)}} w_{ij}^{(l)}
   \delta_i^{(l+1)})\ f^\prime(z_j^{(l)}) = \left((w^{(l)})^t
   \delta^{(l+1)}\right) \bullet f'(z^{(l)})$$

   the $\bullet$ symbol in the above designates an element-by-element
   multiplication (called the hadamard product), not a matrix
   multiplication.  note that the id127 $\left((w^{(l)})^t
   \delta^{(l+1)}\right)$ performs the necessary summation of the weights
   and $\delta$ values     the reader can check that this is the case.

4.8 implementing the id119 step

   now, how do we integrate this new vectorisation into the gradient
   descent steps of our soon-to-be coded algorithm? first, we have to look
   again at the overall cost function we are trying to minimise (not just
   the sample-by-sample cost function shown in the preceding equation):

   \begin{align}
   j(w,b) &= \frac{1}{m} \sum_{z=0}^m j(w, b, x^{(z)}, y^{(z)})
   \end{align}

   as we can observe, the total cost function is the mean of all the
   sample-by-sample cost function calculations. also remember the gradient
   descent calculation (showing the element-by-element version along with
   the vectorised version):

   \begin{align}
   w_{ij}^{(l)} &= w_{ij}^{(l)}     \alpha \frac{\partial}{\partial
   w_{ij}^{(l)}} j(w,b)\\
   w^{(l)} &= w^{(l)}     \alpha \frac{\partial}{\partial w^{(l)}} j(w,b)\\
   &= w^{(l)}     \alpha \left[\frac{1}{m} \sum_{z=1}^{m} \frac
   {\partial}{\partial w^{(l)}} j(w,b,x^{(z)},y^{(z)}) \right]\\
   \end{align}

   so that means as we go along through our training samples or batches,
   we have to have a term that is summing up the partial derivatives of
   the individual sample cost function calculations. this term will gather
   up all the values for the mean calculation. let   s call this    summing
   up    term $\delta w^{(l)}$. likewise, the equivalent bias term can be
   called $\delta b^{(l)}$. therefore, at each sample iteration of the
   final training algorithm, we have to perform the following steps:

   \begin{align}
   \delta w^{(l)} &= \delta w^{(l)} + \frac {\partial}{\partial w^{(l)}}
   j(w,b,x^{(z)},y^{(z)})\\
   &= \delta w^{(l)} + \delta^{(l+1)} (h^{(l)})^t\\
   \delta b^{(l)} &= \delta b^{(1)} + \delta^{(l+1)}
   \end{align}

   by performing the above operations at each iteration, we slowly build
   up the previously mentioned sum $\sum_{z=1}^{m} \frac
   {\partial}{\partial w^{(l)}} j(w,b,x^{(z)},y^{(z)})$ (and the same for
   $b$). once all the samples have been iterated through, and the $\delta$
   values have been summed up, we update the weight parameters :

   \begin{align}
   w^{(l)} &= w^{(l)}     \alpha \left[\frac{1}{m} \delta w^{(l)} \right] \\
   b^{(l)} &= b^{(l)}     \alpha \left[\frac{1}{m} \delta b^{(l)}\right]
   \end{align}

4.9 the final id119 algorithm

   so, no we   ve finally made it to the point where we can specify the
   entire id26-based id119 training of our neural
   networks. it has taken quite a few steps to show, but hopefully it has
   been instructive. the final id26 algorithm is as follows:

   randomly initialise the weights for each layer $w^{(l)}$
   while iterations < iteration limit:
   1. set $\delta w$ and $\delta b$ to zero
   2. for samples 1 to m:
   a. perform a feed foward pass through all the $n_l$ layers. store the
   activation function outputs $h^{(l)}$
   b. calculate the $\delta^{(n_l)}$ value for the output layer
   c. use id26 to calculate the $\delta^{(l)}$ values for
   layers 2 to $n_l-1$
   d. update the $\delta w^{(l)}$ and $\delta b^{(l)}$ for each layer
   3. perform a id119 step using:

   $w^{(l)} = w^{(l)}     \alpha \left[\frac{1}{m} \delta w^{(l)} \right]$
   $b^{(l)} = b^{(l)}     \alpha \left[\frac{1}{m} \delta b^{(l)}\right]$

   as specified in the algorithm above, we would repeat the gradient
   descent routine until we are happy that the average cost function has
   reached a minimum. at this point, our network is trained and (ideally)
   ready for use.

   the next part of this neural networks tutorial will show how to
   implement this algorithm to train a neural network that recognises
   hand-written digits.

5 implementing the neural network in python

   in the last [67]section we looked at the theory surrounding gradient
   descent training in neural networks and the id26 method. in
   this article, we are going to apply that theory to develop some code to
   perform training and prediction on the mnist dataset. the mnist dataset
   is a kind of go-to dataset in neural network and deep learning
   examples, so we   ll stick with it here too. what it consists of is a
   record of images of hand-written digits with associated labels that
   tell us what the digit is. each image is 8 x 8 pixels in size, and the
   image data sample is represented by 64 data points which denote the
   pixel intensity. in this example, we   ll be using the mnist dataset
   provided in the python machine learning library called [68]scikit
   learn. an example of the image (and the extraction of the data from the
   scikit learn dataset) is shown in the code below (for an image of 1):
from sklearn.datasets import load_digits
digits = load_digits()
print(digits.data.shape)
import matplotlib.pyplot as plt
plt.gray()
plt.matshow(digits.images[1])
plt.show()

   figure 13. mnist digit    1   

   the code above prints (1797, 64) to show the shape of input data matrix
   and the pixelated digit    1    in the image above.  the code we are going
   to write in this neural networks tutorial will try and estimate the
   digits that these pixels represent (using neural networks of course).
   first things first, we need to get the input data in shape. to do so,
   we need to do two things:
    1. scale the data
    2. split the data into test and train sets

5.1 scaling data

   why do we need to scale the input data?  first, have a look at one of
   the dataset pixel representations:
digits.data[0,:]
out[2]:
array([  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.,   0.,   0.,  13.,
        15.,  10.,  15.,   5.,   0.,   0.,   3.,  15.,   2.,   0.,  11.,
         8.,   0.,   0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.,   0.,
         5.,   8.,   0.,   0.,   9.,   8.,   0.,   0.,   4.,  11.,   0.,
         1.,  12.,   7.,   0.,   0.,   2.,  14.,   5.,  10.,  12.,   0.,
         0.,   0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.])

   notice that the input data ranges from 0 up to 15?  it   s standard
   practice to scale the input data so that it all fits mostly between
   either 0 to 1 or with a small range centred around 0 i.e. -1 to 1.
   why?  well, it can help the convergence of the neural network and is
   especially important if we are combining different data types.
   thankfully, this is easily done using sci-kit learn:
from sklearn.preprocessing import standardscaler
x_scale = standardscaler()
x = x_scale.fit_transform(digits.data)
x[0,:]
out[3]:
array([ 0.        , -0.33501649, -0.04308102,  0.27407152, -0.66447751,
       -0.84412939, -0.40972392, -0.12502292, -0.05907756, -0.62400926,
        0.4829745 ,  0.75962245, -0.05842586,  1.12772113,  0.87958306,
       -0.13043338, -0.04462507,  0.11144272,  0.89588044, -0.86066632,
       -1.14964846,  0.51547187,  1.90596347, -0.11422184, -0.03337973,
        0.48648928,  0.46988512, -1.49990136, -1.61406277,  0.07639777,
        1.54181413, -0.04723238,  0.        ,  0.76465553,  0.05263019,
       -1.44763006, -1.73666443,  0.04361588,  1.43955804,  0.        ,
       -0.06134367,  0.8105536 ,  0.63011714, -1.12245711, -1.06623158,
        0.66096475,  0.81845076, -0.08874162, -0.03543326,  0.74211893,
        1.15065212, -0.86867056,  0.11012973,  0.53761116, -0.75743581,
       -0.20978513, -0.02359646, -0.29908135,  0.08671869,  0.20829258,
       -0.36677122, -1.14664746, -0.5056698 , -0.19600752])

   the scikit learn standard scaler by default normalises the data by
   subtracting the mean and dividing by the standard deviation.  as can be
   observed, most of the data points are centered around zero and
   contained within -2 and 2.  this is a good starting point.  there is no
   real need to scale the output data $y$.

5.2 creating test and training datasets

   in machine learning, there is a phenomenon called    overfitting   . this
   occurs when models, during training, become too complex     they become
   really well adapted to predict the training data, but when they are
   asked to predict something based on new data that they haven   t    seen   
   before, they perform poorly. in other words, the models don   t
   generalise very well. to make sure that we are not creating models
   which are too complex, it is common practice to split the dataset into
   a training set and a test set. the training set is, obviously, the data
   that the model will be trained on, and the test set is the data that
   the model will be tested on after it has been trained. the amount of
   training data is always more numerous than the testing data, and is
   usually between 60-80% of the total dataset.

   again, scikit learn makes this splitting of the data into training and
   testing sets easy:
from sklearn.model_selection import train_test_split
y = digits.target
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4)

   in this case, we   ve made the test set to be 40% of the total data,
   leaving 60% to train with. the train_test_split function in scikit
   learn pushes the data randomly into the different datasets     in other
   words, it doesn   t take the first 60% of rows as the training set and
   the second 40% of rows as the test set. this avoids data collection
   artefacts from degrading the performance of the model.

5.3 setting up the output layer

   as you would have been able to gather, we need the output layer to
   predict whether the digit represented by the input pixels is between 0
   and 9. therefore, a sensible neural network architecture would be to
   have an output layer of 10 nodes, with each of these nodes representing
   a digit from 0 to 9. we want to train the network so that when, say, an
   image of the digit    5    is presented to the neural network, the node in
   the output layer representing 5 has the highest value. ideally, we
   would want to see an output looking like this: [0, 0, 0, 0, 0, 1, 0, 0,
   0, 0]. however, in reality, we can settle for something like this:
   [0.01, 0.1, 0.2, 0.05, 0.3, 0.8, 0.4, 0.03, 0.25, 0.02]. in this case,
   we can take the maximum index of the output array and call that our
   predicted digit.

   for the mnist data supplied in the scikit learn dataset, the    targets   
   or the classification of the handwritten digits is in the form of a
   single number. we need to convert that single number into a vector so
   that it lines up with our 10 node output layer. in other words, if the
   target value in the dataset is    1    we want to convert it into the
   vector: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]. the code below does just that:
import numpy as np
def convert_y_to_vect(y):
    y_vect = np.zeros((len(y), 10))
    for i in range(len(y)):
        y_vect[i, y[i]] = 1
    return y_vect
y_v_train = convert_y_to_vect(y_train)
y_v_test = convert_y_to_vect(y_test)
y_train[0], y_v_train[0]
out[8]:
(1, array([ 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))


   as can be observed above, the mnist target (1) has been converted into
   the vector [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], which is what we want.

5.4 creating the neural network

   the next step is to specify the structure of the neural network. for
   the input layer, we know we need 64 nodes to cover the 64 pixels in the
   image. as discussed, we need 10 output layer nodes to predict the
   digits. we   ll also need a hidden layer in our network to allow for the
   complexity of the task. usually, the number of hidden layer nodes is
   somewhere between the number of input layers and the number of output
   layers. let   s define a simple python list that designates the structure
   of our network:
nn_structure = [64, 30, 10]

   we   ll use sigmoid id180 again, so let   s setup the
   sigmoid function and its derivative:
def f(x):
    return 1 / (1 + np.exp(-x))
def f_deriv(x):
    return f(x) * (1 - f(x))

   ok, so we now have an idea of what our neural network will look like.
   how do we train it? remember the algorithm from [69]section 4.9 , which
   we   ll repeat here for ease of access and review:

   randomly initialise the weights for each layer $w^{(l)}$
   while iterations < iteration limit:
   1. set $\delta w$ and $\delta b$ to zero
   2. for samples 1 to m:
   a. perform a feed foward pass through all the $n_l$ layers. store the
   activation function outputs $h^{(l)}$
   b. calculate the $\delta^{(n_l)}$ value for the output layer
   c. use id26 to calculate the $\delta^{(l)}$ values for
   layers 2 to $n_l-1$
   d. update the $\delta w^{(l)}$ and $\delta b^{(l)}$ for each layer
   3. perform a id119 step using:

   $w^{(l)} = w^{(l)}     \alpha \left[\frac{1}{m} \delta w^{(l)} \right]$
   $b^{(l)} = b^{(l)}     \alpha \left[\frac{1}{m} \delta b^{(l)}\right]$

   so the first step is to initialise the weights for each layer. to make
   it easy to organise the various layers, we   ll use python dictionary
   objects (initialised by {}). finally, the weights have to be
   initialised with random values     this is to ensure that the neural
   network will converge correctly during training. we use the numpy
   library random_sample function to do this. the weight initialisation
   code is shown below:
import numpy.random as r
def setup_and_init_weights(nn_structure):
    w = {}
    b = {}
    for l in range(1, len(nn_structure)):
        w[l] = r.random_sample((nn_structure[l], nn_structure[l-1]))
        b[l] = r.random_sample((nn_structure[l],))
    return w, b

   the next step is to set the mean accumulation values $\delta w$ and
   $\delta b$ to zero (they need to be the same size as the weight and
   bias matrices):
def init_tri_values(nn_structure):
    tri_w = {}
    tri_b = {}
    for l in range(1, len(nn_structure)):
        tri_w[l] = np.zeros((nn_structure[l], nn_structure[l-1]))
        tri_b[l] = np.zeros((nn_structure[l],))
    return tri_w, tri_b

   if we now step into the id119 loop, the first step is to
   perform a feed forward pass through the network. the code below is a
   variation on the feed forward function created in [70]section 3:
def feed_forward(x, w, b):
    h = {1: x}
    z = {}
    for l in range(1, len(w) + 1):
        # if it is the first layer, then the input into the weights is x, otherw
ise,
        # it is the output from the last layer
        if l == 1:
            node_in = x
        else:
            node_in = h[l]
        z[l+1] = w[l].dot(node_in) + b[l] # z^(l+1) = w^(l)*h^(l) + b^(l)
        h[l+1] = f(z[l+1]) # h^(l) = f(z^(l))
    return h, z

   finally, we have to then calculate the output layer delta
   $\delta^{(n_l)}$ and any hidden layer delta values $\delta^{(l)}$ to
   perform the id26 pass:
def calculate_out_layer_delta(y, h_out, z_out):
    # delta^(nl) = -(y_i - h_i^(nl)) * f'(z_i^(nl))
    return -(y-h_out) * f_deriv(z_out)

def calculate_hidden_delta(delta_plus_1, w_l, z_l):
    # delta^(l) = (transpose(w^(l)) * delta^(l+1)) * f'(z^(l))
    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)

   now we can put all the steps together into the final function:
def train_nn(nn_structure, x, y, iter_num=3000, alpha=0.25):
    w, b = setup_and_init_weights(nn_structure)
    cnt = 0
    m = len(y)
    avg_cost_func = []
    print('starting id119 for {} iterations'.format(iter_num))
    while cnt < iter_num:
        if cnt%1000 == 0:
            print('iteration {} of {}'.format(cnt, iter_num))
        tri_w, tri_b = init_tri_values(nn_structure)
        avg_cost = 0
        for i in range(len(y)):
            delta = {}
            # perform the feed forward pass and return the stored h and z values
, to be used in the
            # id119 step
            h, z = feed_forward(x[i, :], w, b)
            # loop from nl-1 to 1 backpropagating the errors
            for l in range(len(nn_structure), 0, -1):
                if l == len(nn_structure):
                    delta[l] = calculate_out_layer_delta(y[i,:], h[l], z[l])
                    avg_cost += np.linalg.norm((y[i,:]-h[l]))
                else:
                    if l > 1:
                        delta[l] = calculate_hidden_delta(delta[l+1], w[l], z[l]
)
                    # triw^(l) = triw^(l) + delta^(l+1) * transpose(h^(l))
                    tri_w[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(h[
l][:,np.newaxis]))
                    # trib^(l) = trib^(l) + delta^(l+1)
                    tri_b[l] += delta[l+1]
        # perform the id119 step for the weights in each layer
        for l in range(len(nn_structure) - 1, 0, -1):
            w[l] += -alpha * (1.0/m * tri_w[l])
            b[l] += -alpha * (1.0/m * tri_b[l])
        # complete the average cost calculation
        avg_cost = 1.0/m * avg_cost
        avg_cost_func.append(avg_cost)
        cnt += 1
    return w, b, avg_cost_func

   the function above deserves a bit of explanation. first, we aren   t
   setting a termination of the id119 process based on some
   change or precision of the cost function. rather, we are just running
   it for a set number of iterations (3,000 in this case) and we   ll
   monitor how the average cost function changes as we progress through
   the training (avg_cost_func list in the above code). in each iteration
   of the id119, we cycle through each training sample
   (range(len(y)) and perform the feed forward pass and then the
   id26. the id26 step is an iteration through the
   layers starting at the output layer and working backwards    
   range(len(nn_structure), 0, -1). we calculate the average cost, which
   we are tracking during the training, at the output layer (l ==
   len(nn_structure)). we also update the mean accumulation values,
   $\delta w$ and $\delta b$, designated as tri_w and tri_b, for every
   layer apart from the output layer (there are no weights connecting the
   output layer to any further layer).

   finally, after we have looped through all the training samples,
   accumulating the tri_w and tri_b values, we perform a id119
   step change in the weight and bias values:
   $$w^{(l)} = w^{(l)}     \alpha \left[\frac{1}{m} \delta w^{(l)} \right]$$
   $$b^{(l)} = b^{(l)}     \alpha \left[\frac{1}{m} \delta b^{(l)}\right]$$

   after the process is completed, we return the trained weight and bias
   values, along with our tracked average cost for each iteration. now
   it   s time to run the function     note: this may take a few minutes
   depending on the capabilities of your computer.
w, b, avg_cost_func = train_nn(nn_structure, x_train, y_v_train)

   now we can have a look at how the average cost function decreased as we
   went through the id119 iterations of the training, slowly
   converging on a minimum in the function:
plt.plot(avg_cost_func)
plt.ylabel('average j')
plt.xlabel('iteration number')
plt.show()

   we can see in the above plot, that by 3,000 iterations of our gradient
   descent our average cost function value has started to    plateau    and
   therefore any further increases in the number of iterations isn   t
   likely to improve the performance of the network by much.

5.5 assessing the accuracy of the trained model

   now that we   ve trained our mnist neural network, we want to see how it
   performs on the test set. is our model any good? given a test input (64
   pixels), we need to find what the output of our neural network is     we
   do that by simply performing a feed forward pass through the network
   using our trained weight and bias values. as discussed previously, we
   assess the prediction of the output layer by taking the node with the
   maximum output as the predicted digit. we can use the numpy.argmax
   function for this, which returns the index of the array value with the
   highest value:
def predict_y(w, b, x, n_layers):
    m = x.shape[0]
    y = np.zeros((m,))
    for i in range(m):
        h, z = feed_forward(x[i, :], w, b)
        y[i] = np.argmax(h[n_layers])
    return y

   finally, we can assess the accuracy of the prediction (i.e. the
   percentage of times the network predicted the handwritten digit
   correctly), by using the scikit learn accuracy_score function:
from sklearn.metrics import accuracy_score
y_pred = predict_y(w, b, x_test, 3)
accuracy_score(y_test, y_pred)*100

   this gives an 86% accuracy of predicting the digits.  sounds pretty
   good right? well actually no, it   s pretty bad. the current
   state-of-the-art deep learning algorithms achieve accuracy scores of
   99.7% (see [71]here), so we are a fair way off that sort of accuracy.
   there are many more exciting things to learn     my next post will cover
   some tips and tricks on how to improve the accuracy substantially
   on this simple neural network.  however, beyond that, we have a whole
   realm of state-of-the-art deep learning algorithms to learn and
   investigate, from convolution neural networks to deep belief nets and
   recurrent neural networks.  if you followed along ok with this post,
   you will be in a good position to advance to these newer techniques.

   stick around to find out more about this rapidly advancing area of
   machine learning.  as a start, check out these posts:
   [72]python tensorflow tutorial     build a neural network
   [73]improve your neural networks     part 1 [tips and tricks]
   [74]stochastic id119     mini-batch and more
     __________________________________________________________________

eager to learn more? get the book [75]here
     __________________________________________________________________

about the author

     [76]liping yang says:
   [77]april 11, 2017 at 1:45 am

   such a great post     concise and to the point!!thank you for your
   efforts and for sharing with the world!
     * andy says:
       [78]april 11, 2017 at 1:52 am
       thanks liping! i appreciate it

     cylux says:
   [79]april 11, 2017 at 3:54 am

   thank you for your posting,
   by the way, in 4.5, when you simplify h1(3) as f(z1(2)) by defining
   z1(2) , z1(2) seems have changed. is this intentional?
     * andy says:
       [80]april 11, 2017 at 9:30 pm
       thanks cylux     good pickup. i   ve fixed up the weight indexing
       mistake now

     [81]data science weekly     issue 177 | a bunch of data says:
   [82]april 13, 2017 at 1:20 pm

   [   ] neural networks tutorial     a pathway to deep learning in this
   tutorial i   ll be presenting some concepts, code and maths that will
   enable you to build and understand a simple neural network    [   ]

     nicky says:
   [83]april 20, 2017 at 7:50 pm

   great explanations. thanks.
     * andy says:
       [84]april 22, 2017 at 12:06 am
       thanks nicky, glad it has been useful for you

     ernest bonat says:
   [85]april 21, 2017 at 12:03 am

   hi andy!

   i think this deep learning tutorial is one of the best online today    
   thank you andy!

   could you let me know best math-technical books (ebooks) and technical
   papers where this algorithm is explained more clear in deep?
     * andy says:
       [86]april 22, 2017 at 12:08 am
       thanks for your kind comments ernest.
       in my opinion the best book on deep learning is    deep learning    by
       ian goodfellow. hope that helps

     mallam says:
   [87]april 23, 2017 at 6:45 pm

   thank you for this excellent tutorial. one of the best.
   if you do not mind i do have a question.
   why did you use the sse as the cost function and not the cross id178
   cost function?
   as far as i know i think that cross id178 cost function works better
   with classification problems.
     * andy says:
       [88]april 25, 2017 at 6:34 am
       hi mallam, you are welcome     glad you have found it useful. you are
       correct, the cross id178 cost function is better for
       classification. however, it is more difficult to explain for this
       introductory tutorial, so i stuck with sse as it is more readily
       associated with    error   . in my tensorflow tutorials, i have used
       the cross id178 cost function, and i hope to dedicate a post to
       this function soon     it is an interesting concept. thanks again,
       all the best
          + mallam says:
            [89]may 4, 2017 at 5:54 am
            i do have another comment just for the sake of optimization.
            there is no need for the if condition in the feed_forward
            function.
            you already stated in the first line that the first item in
            the h dictionary is x
            so you can remove the if condition and put h[l] instead of
            node_in in the line computing z[l+1] and it will give the same
            results.

     [90]christoper says:
   [91]april 28, 2017 at 4:37 pm

   hi, i log on to your blogs regularly. your writing style is awesome,
   keep doing
   what you   re doing!
   [92]http://bh-sj.com/index.php/easyblog/blogger/listings/lrmtrent255513
   2
     * andy says:
       [93]april 30, 2017 at 1:30 am
       thanks for the kind comments christopher, glad you liked it

     [94]motoapk hihi says:
   [95]april 30, 2017 at 12:53 am

   thanks-a-mundo for the article post.much thanks again. fantastic.
     * andy says:
       [96]april 30, 2017 at 1:29 am
       you   re very welcome, glad it was helpful



   deepu says:
   [97]november 2, 2017 at 5:54 pm

   andy, great article to start learning neural network. appreciate your
   time and effort. thanks for making machine learning concepts simple to
   understand!

     * andy says:
       [98]november 2, 2017 at 7:01 pm
       thanks deepu, glad you have found the articles useful



   kurt says:
   [99]november 10, 2017 at 1:12 am

   great post with reasonable amount of explanation.enough to get me
   hooked on to it.



   alessio says:
   [100]december 16, 2017 at 11:03 pm

   this is exactly the kickstart i was looking for. thank yo



   ram says:
   [101]january 3, 2018 at 1:51 pm

   i have read many articles that explain ml in way too many equations
   assuming that the reader will understand (no pre-req. mentioned). this
   is the first article i have seen where there is math and with real
   program examples explains how these equations tie to numbers. this
   makes learning ml less intimidating.

   thanks for posting such excellent tutorials.



   adolphe says:
   [102]january 18, 2018 at 9:59 pm

   very simple and powerful explanation,
   thank you very much



   mattc says:
   [103]march 13, 2018 at 9:46 pm

   great post andy. i   ll be checking out your other blogs!



   rukshar alam says:
   [104]may 14, 2018 at 4:48 am

   thanks for the awesome tutorial    this really helped for my thesis on
   neural networks     

     * andy says:
       [105]may 14, 2018 at 6:27 am
       you   re welcome rukshar, glad it helped



   shiv jetwal says:
   [106]june 18, 2018 at 5:21 pm

   thanks andy for such a great post.everything is clearly defined.

     * andy says:
       [107]june 19, 2018 at 1:28 am
       thanks shiv, glad it was helpful for you



   chris says:
   [108]july 24, 2018 at 1:01 pm

   hi andy, this is by far the most understandable explanation of the
   topic and will contribute significantly to the success of my thesis on
   ml. i especially like the way you visualize the concept and show the
   mathematical connections. the code samples also fit very well.

     * andy says:
       [109]july 24, 2018 at 9:19 pm
       thanks chris, glad it was useful



   danz says:
   [110]december 6, 2018 at 3:26 pm

   hello andy, many readers already recognized the value of the post, but
   i still feel the need to add my own opinion.
   there is no single resource that a reader should consult to have a good
   understanding of this domain, however, this post is one of the bests
   introductory ones i have ever seen since i am studying the subject.
   even the math somehow seems more accessible here until some point
   -which is farther than other publications (i have to admit, i did not
   fully grasp the math described at back propagation).
   what i especially like, is the progressive establishment of building
   knowledge blocks, in a way that leads to a    feeling    of the solution.
   i really like how your platform is structured and you seem to address
   more advanced topics like id56 and rl in other posts, which i will
   consult for sure.
   thanks again for the exceptional material!

     * andy says:
       [111]december 6, 2018 at 9:55 pm
       thanks very much for your kind comments danz     glad the blog is a
       help to you



   thomas evangelidis says:
   [112]december 26, 2018 at 4:18 pm

   this tutorial is simply excellent! i haven   t found any clearer and
   in-depth theoretical description of the basic nn principles accompanied
   by python code implementations!

     * andy says:
       [113]december 26, 2018 at 7:38 pm
       thanks thomas, glad it has been a help



   sepideh malek taji says:
   [114]january 20, 2019 at 1:03 am

   this tutorial was really the greatest ever!!!
   thank you so much for sharing.



   martin says:
   [115]february 2, 2019 at 8:05 pm

   best nn tutorial i have ever seen!

     * andy says:
       [116]february 2, 2019 at 9:17 pm
       thanks martin!



   andry says:
   [117]march 10, 2019 at 1:00 pm

   very good tutorial for beginner like me. how do i cite it in an
   article?

   ____________________ (button)

   recent posts
     * [118]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [119]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [120]keras, eager and tensorflow 2.0     a new tf paradigm
     * [121]introduction to tensorboard and tensorflow visualization
     * [122]tensorflow eager tutorial

   recent comments
     * andry on [123]neural networks tutorial     a pathway to deep learning
     * sandipan on [124]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [125]neural networks tutorial     a pathway to deep learning
     * martin on [126]neural networks tutorial     a pathway to deep
       learning
     * uri on [127]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [128]march 2019
     * [129]january 2019
     * [130]october 2018
     * [131]september 2018
     * [132]august 2018
     * [133]july 2018
     * [134]june 2018
     * [135]may 2018
     * [136]april 2018
     * [137]march 2018
     * [138]february 2018
     * [139]november 2017
     * [140]october 2017
     * [141]september 2017
     * [142]august 2017
     * [143]july 2017
     * [144]may 2017
     * [145]april 2017
     * [146]march 2017

   categories
     * [147]amazon aws
     * [148]cntk
     * [149]convolutional neural networks
     * [150]cross id178
     * [151]deep learning
     * [152]gensim
     * [153]gpus
     * [154]keras
     * [155]id168s
     * [156]lstms
     * [157]neural networks
     * [158]nlp
     * [159]optimisation
     * [160]pytorch
     * [161]recurrent neural networks
     * [162]id23
     * [163]tensorboard
     * [164]tensorflow
     * [165]tensorflow 2.0
     * [166]weight initialization
     * [167]id97

   meta
     * [168]log in
     * [169]entries rss
     * [170]comments rss
     * [171]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [172]thrive themes | powered by [173]wordpress

   (button) close dialog

   session expired

   [174]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[175]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/neural-networks-tutorial/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/neural-networks-tutorial/&format=xml
   3. https://www.adventuresinmachinelearning.com/
   4. https://adventuresinmachinelearning.com/about/
   5. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   6. https://adventuresinmachinelearning.com/contact/
   7. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   8. https://adventuresinmachinelearning.com/author/andyt81/
   9. https://adventuresinmachinelearning.com/category/deep-learning/
  10. https://adventuresinmachinelearning.com/
  11. https://adventuresinmachinelearning.com/category/deep-learning/
  12. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  13. http://adventuresinmachinelearning.com/neural-networks-tutorial/#comments
  14. http://www.sciencemag.org/news/2016/01/huge-leap-forward-computer-mimics-human-brain-beats-professional-game-go
  15. http://www.sciencemag.org/news/2017/03/artificial-intelligence-goes-deep-beat-humans-poker
  16. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/
  17. http://www.nanalyze.com/2016/01/4-companies-using-deep-learning-for-drug-discovery/
  18. http://spectrum.ieee.org/cars-that-think/transportation/self-driving/driveai-brings-deep-learning-to-selfdriving-cars
  19. https://www.khanacademy.org/math/precalculus/precalc-matrices
  20. https://www.khanacademy.org/math/differential-calculus
  21. http://www.numpy.org/
  22. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  23. https://adventuresinmachinelearning.com/improve-neural-networks-part-1/
  24. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  25. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  26. https://github.com/adventuresinml/adventures-in-ml-code
  27. http://adventuresinmachinelearning.com/neural-networks-tutorial/#what-are-anns
  28. http://adventuresinmachinelearning.com/neural-networks-tutorial/#structure-ann
  29. http://adventuresinmachinelearning.com/neural-networks-tutorial/#the-artificial-neuron
  30. http://adventuresinmachinelearning.com/neural-networks-tutorial/#nodes
  31. http://adventuresinmachinelearning.com/neural-networks-tutorial/#the-bias
  32. http://adventuresinmachinelearning.com/neural-networks-tutorial/#putting-together-the-structure
  33. http://adventuresinmachinelearning.com/neural-networks-tutorial/#the-notation
  34. http://adventuresinmachinelearning.com/neural-networks-tutorial/#the-feed-forward-pass
  35. http://adventuresinmachinelearning.com/neural-networks-tutorial/#the-feed-forward-pass
  36. http://adventuresinmachinelearning.com/neural-networks-tutorial/#first-attempt-feed-forward
  37. http://adventuresinmachinelearning.com/neural-networks-tutorial/#more-efficient-implementation
  38. http://adventuresinmachinelearning.com/neural-networks-tutorial/#vectorisation
  39. http://adventuresinmachinelearning.com/neural-networks-tutorial/#matrix-mult
  40. http://adventuresinmachinelearning.com/neural-networks-tutorial/#gradient-desc-opt
  41. http://adventuresinmachinelearning.com/neural-networks-tutorial/#simple-example
  42. http://adventuresinmachinelearning.com/neural-networks-tutorial/#the-cost-function
  43. http://adventuresinmachinelearning.com/neural-networks-tutorial/#gradient-descent-in-nn
  44. http://adventuresinmachinelearning.com/neural-networks-tutorial/#two-dimensional
  45. http://adventuresinmachinelearning.com/neural-networks-tutorial/#backprop-in-depth
  46. http://adventuresinmachinelearning.com/neural-networks-tutorial/#prop-in-hidden-layers
  47. http://adventuresinmachinelearning.com/neural-networks-tutorial/#vector-backprop
  48. http://adventuresinmachinelearning.com/neural-networks-tutorial/#imp-gradient-desc
  49. http://adventuresinmachinelearning.com/neural-networks-tutorial/#final-gradient-desc-algo
  50. http://adventuresinmachinelearning.com/neural-networks-tutorial/#implementing-nn
  51. http://adventuresinmachinelearning.com/neural-networks-tutorial/#scaling-data
  52. http://adventuresinmachinelearning.com/neural-networks-tutorial/#test-and-train
  53. http://adventuresinmachinelearning.com/neural-networks-tutorial/#setting-up-output
  54. http://adventuresinmachinelearning.com/neural-networks-tutorial/#creating-nn
  55. http://adventuresinmachinelearning.com/neural-networks-tutorial/#creating-nn
  56. http://adventuresinmachinelearning.com/neural-networks-tutorial/#gradient-desc-opt
  57. http://stattrek.com/tutorials/matrix-algebra-tutorial.aspx
  58. https://www.tensorflow.org/
  59. http://www.deeplearning.net/software/theano/
  60. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  61. http://adventuresinmachinelearning.com/neural-networks-tutorial/#what-are-anns
  62. https://en.wikipedia.org/wiki/gradient_descent
  63. http://adventuresinmachinelearning.com/neural-networks-tutorial/#implementing-nn
  64. http://adventuresinmachinelearning.com/neural-networks-tutorial/#the-feed-forward-pass
  65. http://adventuresinmachinelearning.com/neural-networks-tutorial/#the-artificial-neuron
  66. http://adventuresinmachinelearning.com/neural-networks-tutorial/#vectorisation
  67. http://adventuresinmachinelearning.com/neural-networks-tutorial/#gradient-desc-opt
  68. http://scikit-learn.org/
  69. http://adventuresinmachinelearning.com/neural-networks-tutorial/#final-gradient-desc-algo
  70. http://adventuresinmachinelearning.com/neural-networks-tutorial/#the-feed-forward-pass
  71. http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html
  72. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  73. https://adventuresinmachinelearning.com/improve-neural-networks-part-1/
  74. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  75. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  76. http://deeplearning.lipingyang.org/
  77. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/3
  78. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/4
  79. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/5
  80. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/6
  81. http://abunchofdata.com/data-science-weekly-issue-177/
  82. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/7
  83. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/9
  84. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/11
  85. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/10
  86. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/12
  87. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/13
  88. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/14
  89. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/20
  90. http://bh-sj.com/index.php/easyblog/blogger/listings/lrmtrent2555132
  91. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/16
  92. http://bh-sj.com/index.php/easyblog/blogger/listings/lrmtrent2555132
  93. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/19
  94. http://motoapk.com/
  95. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/17
  96. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/18
  97. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/83
  98. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/84
  99. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/85
 100. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/87
 101. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/88
 102. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/90
 103. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/92
 104. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/100
 105. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/101
 106. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/104
 107. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/105
 108. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/110
 109. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/111
 110. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/128
 111. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/129
 112. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/131
 113. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/132
 114. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/133
 115. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
 116. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
 117. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
 118. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
 119. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
 120. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
 121. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
 122. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
 123. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
 124. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
 125. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
 126. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
 127. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
 128. https://adventuresinmachinelearning.com/2019/03/
 129. https://adventuresinmachinelearning.com/2019/01/
 130. https://adventuresinmachinelearning.com/2018/10/
 131. https://adventuresinmachinelearning.com/2018/09/
 132. https://adventuresinmachinelearning.com/2018/08/
 133. https://adventuresinmachinelearning.com/2018/07/
 134. https://adventuresinmachinelearning.com/2018/06/
 135. https://adventuresinmachinelearning.com/2018/05/
 136. https://adventuresinmachinelearning.com/2018/04/
 137. https://adventuresinmachinelearning.com/2018/03/
 138. https://adventuresinmachinelearning.com/2018/02/
 139. https://adventuresinmachinelearning.com/2017/11/
 140. https://adventuresinmachinelearning.com/2017/10/
 141. https://adventuresinmachinelearning.com/2017/09/
 142. https://adventuresinmachinelearning.com/2017/08/
 143. https://adventuresinmachinelearning.com/2017/07/
 144. https://adventuresinmachinelearning.com/2017/05/
 145. https://adventuresinmachinelearning.com/2017/04/
 146. https://adventuresinmachinelearning.com/2017/03/
 147. https://adventuresinmachinelearning.com/category/amazon-aws/
 148. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
 149. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
 150. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
 151. https://adventuresinmachinelearning.com/category/deep-learning/
 152. https://adventuresinmachinelearning.com/category/nlp/gensim/
 153. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
 154. https://adventuresinmachinelearning.com/category/deep-learning/keras/
 155. https://adventuresinmachinelearning.com/category/loss-functions/
 156. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
 157. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
 158. https://adventuresinmachinelearning.com/category/nlp/
 159. https://adventuresinmachinelearning.com/category/optimisation/
 160. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
 161. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
 162. https://adventuresinmachinelearning.com/category/reinforcement-learning/
 163. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
 164. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
 165. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
 166. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
 167. https://adventuresinmachinelearning.com/category/nlp/id97/
 168. https://adventuresinmachinelearning.com/wp-login.php
 169. https://adventuresinmachinelearning.com/feed/
 170. https://adventuresinmachinelearning.com/comments/feed/
 171. https://wordpress.org/
 172. https://www.thrivethemes.com/
 173. http://www.wordpress.org/
 174. https://adventuresinmachinelearning.com/wp-login.php
 175. http://adventuresinmachinelearning.com/neural-networks-tutorial/

   hidden links:
 177. https://adventuresinmachinelearning.com/author/andyt81/
