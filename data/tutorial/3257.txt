   #[1]adventures in machine learning    stochastic id119    
   mini-batch and more comments feed [2]alternate [3]alternate

   menu

     * [4]home
     * [5]about
     * [6]coding the deep learning revolution ebook
     * [7]contact
     * [8]ebook / newsletter sign-up

   search: ____________________

stochastic id119     mini-batch and more

   by [9]andy | [10]deep learning

     * you are here:
     * [11]home
     * [12]deep learning
     * [13]stochastic id119     mini-batch and more

   mar 30
   [14]10

   in the [15]neural network tutorial, i introduced the id119
   algorithm which is used to train the weights in an artificial neural
   network.  in reality, for deep learning and big data tasks standard
   id119 is not often used.  rather, a variant of gradient
   descent called stochastic id119 and in particular its cousin
   mini-batch id119 is used.  that is the focus of this post.

id119 review

   the id119 optimisation algorithm aims to minimise some
   cost/id168 based on that function   s gradient.  successive
   iterations are employed to progressively approach either a local or
   global minimum of the cost function.  the figure below shows an example
   of id119 operating in a single dimension:
   stochastic id119 - simple id119 example

   simple, one-dimensional id119

   when training weights in a neural network, normal batch gradient
   descent usually takes the mean squared error of all the training
   samples when it is updating the weights of the network:

   $$w = w     \alpha \nabla j(w,b)$$

   where $w$ are the weights, $\alpha$ is the learning rate and $\nabla$
   is the gradient of the cost function $j(w,b)$ with respect to changes
   in the weights.  more details can be found in the [16]neural networks
   tutorial, but in that tutorial the cost function $j$ was defined as:

   \begin{align}
   j(w,b) &= \frac{1}{m} \sum_{z=0}^m j(w, b, x^{(z)}, y^{(z)})
   \end{align}

   as can be observed, the overall cost function (and therefore the
   gradient) depends on the mean cost function calculated on all of the m
   training samples ($x^{(z)}$ and $y^{(z)}$ refer to each training sample
   pair).  is this the best way of doing things?  batch id119
   is good because the training progress is nice and smooth     if you plot
   the average value of the cost function over the number of iterations /
   epochs it will look something like this:
   stochastic id119 - batch gradient example

   example batch id119 progress

   as you can see, the line is mostly smooth and predictable.  however, a
   problem with batch id119 in neural networks is that for
   every id119 update in the weights, you have to cycle through
   every training sample.  for big data sets i.e. > 50,000 training
   samples, this can be time prohibitive.  batch id119 also has
   the following disadvantages:
     * it requires the loading of the whole dataset into memory, which can
       be problematic for big data sets
     * batch id119 can   t be efficiently parallelised (compared
       to the techniques about to be presented)     this is because each
       update in the weight parameters requires a mean calculation of the
       cost function over all the training samples.
     * the smooth nature of the reducing cost function tends to ensure
       that the neural network training will get stuck in local minimums,
       which makes it less likely that a global minimum of the cost
       function will be found.

   stochastic id119 is an algorithm that attempts to
   address some of these issues.

stochastic id119

   stochastic id119 updates the weight parameters after
   evaluation the cost function after each sample.  that is, rather than
   summing up the cost function results for all the sample then taking the
   mean, stochastic id119 (or sgd) updates the weights after
   every training sample is analysed.  therefore, the updates look like
   this:

   $$w = w     \alpha \nabla j(w,b, x^{(z)}, y^{(z)})$$

   notice that an update to the weights (and bias) is performed after
   every sample $z$ in $m$.  this is easily implemented by a minor
   variation of the batch id119 code in python, by simply
   shifting the update component into the sample loop (the original
   train_nn function can be found in the [17]neural networks tutorial and
   [18]here):
def train_nn_sgd(nn_structure, x, y, iter_num=3000, alpha=0.25, lamb=0.000):
    w, b = setup_and_init_weights(nn_structure)
    cnt = 0
    m = len(y)
    avg_cost_func = []
    print('starting id119 for {} iterations'.format(iter_num))
    while cnt < iter_num:
        if cnt%50 == 0:
            print('iteration {} of {}'.format(cnt, iter_num))
        tri_w, tri_b = init_tri_values(nn_structure)
        avg_cost = 0
        for i in range(len(y)):
            delta = {}
            # perform the feed forward pass and return the stored h and z values
,
            # to be used in the id119 step
            h, z = feed_forward(x[i, :], w, b)
            # loop from nl-1 to 1 backpropagating the errors
            for l in range(len(nn_structure), 0, -1):
                if l == len(nn_structure):
                    delta[l] = calculate_out_layer_delta(y[i,:], h[l], z[l])
                    avg_cost += np.linalg.norm((y[i,:]-h[l]))
                else:
                    if l > 1:
                        delta[l] = calculate_hidden_delta(delta[l+1], w[l], z[l]
)
                    # triw^(l) = triw^(l) + delta^(l+1) * transpose(h^(l))
                    tri_w[l] = np.dot(delta[l+1][:,np.newaxis],
                                       np.transpose(h[l][:,np.newaxis]))
                    # trib^(l) = trib^(l) + delta^(l+1)
                    tri_b[l] = delta[l+1]
            # perform the id119 step for the weights in each layer
            for l in range(len(nn_structure) - 1, 0, -1):
                w[l] += -alpha * (tri_w[l] + lamb * w[l])
                b[l] += -alpha * (tri_b[l])
        # complete the average cost calculation
        avg_cost = 1.0/m * avg_cost
        avg_cost_func.append(avg_cost)
        cnt += 1
    return w, b, avg_cost_func

   in the above function, to implement stochastic id119,
   the following code was simply indented into the sample loop    for i in
   range(len(y)):    (and the averaging over m samples removed):
for l in range(len(nn_structure) - 1, 0, -1):
    w[l] += -alpha * (tri_w[l] + lamb * w[l])
    b[l] += -alpha * (tri_b[l])

   in other words, a very easy transition from batch to stochastic
   id119.  where does the    stochastic    part come in?  the
   stochastic component is in the selection of the random selection
   of training sample.  however, if we use the scikit-learn
   [19]test_train_split function the random selection has already
   occurred, so we can simply iterate through each training sample, which
   has a randomised order.

stochastic id119 performance

   so how does sgd perform?  let   s take a look.  the plot below shows the
   average cost versus the number of training epochs / iterations for
   batch id119 and sgd on the scikit-learn mnist dataset.  note
   that both of these are operating off the same optimised learning
   parameters (i.e. learning rate, regularisation parameter) which were
   determined according to the methods described in [20]this post.
   stochastic id119 - bgd vs sgd

   batch id119 versus sgd

   some interesting things can be noted from the above figure.  first, sgd
   converges much more rapidly than batch id119.  in fact, sgd
   converges on a minimum j after < 20 iterations.  secondly, despite what
   the average cost function plot says, batch id119 after 1000
   iterations outperforms sgd.  on the mnist test set, the sgd run has an
   accuracy of 94% compared to a bgd accuracy of 96%.  why is that?  let   s
   zoom into the sgd run to have a closer look:
   stochastic id119 - noisy sgd

   noisy sgd

   as you can see in the figure above, sgd is noisy.  that is because it
   responds to the effects of each and every sample, and the samples
   themselves will no doubt contain an element of noisiness.  while this
   can be a benefit in that it can act to    kick    the id119 out
   of local minimum values of the cost function, it can also hinder it
   settling down into a good minimum.  this is why, eventually, batch
   id119 has outperformed sgd after 1000 iterations.  it might
   be argued that this is a worthwhile pay-off, as the running time of sgd
   versus bgd is greatly reduced.  however, you might ask     is there a
   middle road, a trade-off?

   there is, and it is called mini-batch id119.

mini-batch id119

   mini-batch id119 is a trade-off between stochastic gradient
   descent and batch id119.  in mini-batch id119,
   the cost function (and therefore gradient) is averaged over a small
   number of samples, from around 10-500.  this is opposed to the sgd
   batch size of 1 sample, and the bgd size of all the training samples.
   it looks like this:

   $$w = w     \alpha \nabla j(w,b, x^{(z:z+bs)}, y^{(z:z+bs)})$$

   where $bs$ is the mini-batch size and the cost function is:

   $$j(w,b, x^{(z:z+bs)}, y^{(z:z+bs)}) = \frac{1}{bs} \sum_{z=0}^{bs}
   j(w, b, x^{(z)}, y^{(z)})$$

   what   s the benefit of doing it this way?  first, it smooths out some of
   the noise in sgd, but not all of it, thereby still allowing the    kick   
   out of local minimums of the cost function.  second, the mini-batch
   size is still small, thereby keeping the performance benefits of sgd.

   to create the mini-batches, we can use the following function:
from numpy import random
def get_mini_batches(x, y, batch_size):
    random_idxs = random.choice(len(y), len(y), replace=false)
    x_shuffled = x[random_idxs,:]
    y_shuffled = y[random_idxs]
    mini_batches = [(x_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) f
or
                   i in range(0, len(y), batch_size)]
    return mini_batches

   then our new neural network training algorithm looks like this:
def train_nn_mbgd(nn_structure, x, y, bs=100, iter_num=3000, alpha=0.25, lamb=0.
000):
    w, b = setup_and_init_weights(nn_structure)
    cnt = 0
    m = len(y)
    avg_cost_func = []
    print('starting id119 for {} iterations'.format(iter_num))
    while cnt < iter_num:
        if cnt%1000 == 0:
            print('iteration {} of {}'.format(cnt, iter_num))
        tri_w, tri_b = init_tri_values(nn_structure)
        avg_cost = 0
        mini_batches = get_mini_batches(x, y, bs)
        for mb in mini_batches:
            x_mb = mb[0]
            y_mb = mb[1]
            # pdb.set_trace()
            for i in range(len(y_mb)):
                delta = {}
                # perform the feed forward pass and return the stored h and z va
lues,
                # to be used in the id119 step
                h, z = feed_forward(x_mb[i, :], w, b)
                # loop from nl-1 to 1 backpropagating the errors
                for l in range(len(nn_structure), 0, -1):
                    if l == len(nn_structure):
                        delta[l] = calculate_out_layer_delta(y_mb[i,:], h[l], z[
l])
                        avg_cost += np.linalg.norm((y_mb[i,:]-h[l]))
                    else:
                        if l > 1:
                            delta[l] = calculate_hidden_delta(delta[l+1], w[l],
z[l])
                        # triw^(l) = triw^(l) + delta^(l+1) * transpose(h^(l))
                        tri_w[l] += np.dot(delta[l+1][:,np.newaxis],
                                          np.transpose(h[l][:,np.newaxis]))
                        # trib^(l) = trib^(l) + delta^(l+1)
                        tri_b[l] += delta[l+1]
            # perform the id119 step for the weights in each layer
            for l in range(len(nn_structure) - 1, 0, -1):
                w[l] += -alpha * (1.0/bs * tri_w[l] + lamb * w[l])
                b[l] += -alpha * (1.0/bs * tri_b[l])
        # complete the average cost calculation
        avg_cost = 1.0/m * avg_cost
        avg_cost_func.append(avg_cost)
        cnt += 1
    return w, b, avg_cost_func

   let   s see how it performs with a min-batch size of 100 samples:
   stochastic id119 - mini-batch vs the rest

   mini-batch id119 versus the rest

   as can be observed in the figure above, mini-batch id119
   appears be the superior method of id119 to be used in neural
   networks training.  the jagged decline in the average cost function is
   evidence that mini-batch id119 is    kicking    the cost
   function out of local minimum values to reach better, perhaps even the
   best, minimum.  however, it is still able to find a good minimum and
   stick to it.  this is confirmed in the test data     the mini-batch
   method achieves an accuracy of 98% compared to the next best, batch
   id119, which has an accuracy of 96%.  the great thing is    
   it gets to these levels of accuracy after only 150 iterations or so.

   one final benefit of mini-batch id119 is that it can be
   performed in a distributed manner.  that is, each mini-batch can be
   computed in parallel by    workers    across multiple servers, cpus and
   gpus to achieve significant improvements in training speeds.  there are
   multiple algorithms and architectures to perform this parallel
   operation, but that is a topic for another day.  in the mean-time,
   enjoy trying out mini-batch id119 in your neural networks.


about the author

     aziz says:
   [21]august 16, 2018 at 1:18 am

   thanks a lot

     aziz says:
   [22]august 16, 2018 at 1:18 am

   thanks a lot

     zephyr says:
   [23]september 20, 2018 at 11:51 pm

   a very clear explaination!, thanks!

     zephyr says:
   [24]september 20, 2018 at 11:51 pm

   a very clear explaination!, thanks!

     salim sazzed says:
   [25]october 5, 2018 at 4:12 am

   nice tutorial, clearly explained..

     salim sazzed says:
   [26]october 5, 2018 at 4:12 am

   nice tutorial, clearly explained..

     nils says:
   [27]november 24, 2018 at 4:18 pm

   adventure in machine learning is one of the best tutorial websites i
   have ever found! well done and many thanks for all these amazing
   tutorials!
     * andy says:
       [28]november 24, 2018 at 10:39 pm
       thanks nils, glad it   s a help

     nils says:
   [29]november 24, 2018 at 4:18 pm

   adventure in machine learning is one of the best tutorial websites i
   have ever found! well done and many thanks for all these amazing
   tutorials!
     * andy says:
       [30]november 24, 2018 at 10:39 pm
       thanks nils, glad it   s a help

   ____________________ (button)

   recent posts
     * [31]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [32]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [33]keras, eager and tensorflow 2.0     a new tf paradigm
     * [34]introduction to tensorboard and tensorflow visualization
     * [35]tensorflow eager tutorial

   recent comments
     * andry on [36]neural networks tutorial     a pathway to deep learning
     * sandipan on [37]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [38]neural networks tutorial     a pathway to deep learning
     * martin on [39]neural networks tutorial     a pathway to deep learning
     * uri on [40]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [41]march 2019
     * [42]january 2019
     * [43]october 2018
     * [44]september 2018
     * [45]august 2018
     * [46]july 2018
     * [47]june 2018
     * [48]may 2018
     * [49]april 2018
     * [50]march 2018
     * [51]february 2018
     * [52]november 2017
     * [53]october 2017
     * [54]september 2017
     * [55]august 2017
     * [56]july 2017
     * [57]may 2017
     * [58]april 2017
     * [59]march 2017

   categories
     * [60]amazon aws
     * [61]cntk
     * [62]convolutional neural networks
     * [63]cross id178
     * [64]deep learning
     * [65]gensim
     * [66]gpus
     * [67]keras
     * [68]id168s
     * [69]lstms
     * [70]neural networks
     * [71]nlp
     * [72]optimisation
     * [73]pytorch
     * [74]recurrent neural networks
     * [75]id23
     * [76]tensorboard
     * [77]tensorflow
     * [78]tensorflow 2.0
     * [79]weight initialization
     * [80]id97

   meta
     * [81]log in
     * [82]entries rss
     * [83]comments rss
     * [84]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [85]thrive themes | powered by [86]wordpress

   (button) close dialog

   session expired

   [87]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[88]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/stochastic-gradient-descent/feed/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/stochastic-gradient-descent/
   3. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/stochastic-gradient-descent/&format=xml
   4. https://www.adventuresinmachinelearning.com/
   5. https://adventuresinmachinelearning.com/about/
   6. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   7. https://adventuresinmachinelearning.com/contact/
   8. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   9. https://adventuresinmachinelearning.com/author/andyt81/
  10. https://adventuresinmachinelearning.com/category/deep-learning/
  11. https://adventuresinmachinelearning.com/
  12. https://adventuresinmachinelearning.com/category/deep-learning/
  13. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  14. http://adventuresinmachinelearning.com/stochastic-gradient-descent/#comments
  15. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  16. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  17. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  18. https://github.com/adventuresinml/adventures-in-ml-code
  19. http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
  20. https://adventuresinmachinelearning.com/improve-neural-networks-part-1/
  21. https://adventuresinmachinelearning.com/stochastic-gradient-descent/#comments/658
  22. https://adventuresinmachinelearning.com/stochastic-gradient-descent/#comments/720
  23. https://adventuresinmachinelearning.com/stochastic-gradient-descent/#comments/659
  24. https://adventuresinmachinelearning.com/stochastic-gradient-descent/#comments/722
  25. https://adventuresinmachinelearning.com/stochastic-gradient-descent/#comments/660
  26. https://adventuresinmachinelearning.com/stochastic-gradient-descent/#comments/723
  27. https://adventuresinmachinelearning.com/stochastic-gradient-descent/#comments/661
  28. https://adventuresinmachinelearning.com/stochastic-gradient-descent/#comments/662
  29. https://adventuresinmachinelearning.com/stochastic-gradient-descent/#comments/725
  30. https://adventuresinmachinelearning.com/stochastic-gradient-descent/#comments/727
  31. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
  32. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  33. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
  34. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
  35. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  36. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
  37. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  38. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
  39. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
  40. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
  41. https://adventuresinmachinelearning.com/2019/03/
  42. https://adventuresinmachinelearning.com/2019/01/
  43. https://adventuresinmachinelearning.com/2018/10/
  44. https://adventuresinmachinelearning.com/2018/09/
  45. https://adventuresinmachinelearning.com/2018/08/
  46. https://adventuresinmachinelearning.com/2018/07/
  47. https://adventuresinmachinelearning.com/2018/06/
  48. https://adventuresinmachinelearning.com/2018/05/
  49. https://adventuresinmachinelearning.com/2018/04/
  50. https://adventuresinmachinelearning.com/2018/03/
  51. https://adventuresinmachinelearning.com/2018/02/
  52. https://adventuresinmachinelearning.com/2017/11/
  53. https://adventuresinmachinelearning.com/2017/10/
  54. https://adventuresinmachinelearning.com/2017/09/
  55. https://adventuresinmachinelearning.com/2017/08/
  56. https://adventuresinmachinelearning.com/2017/07/
  57. https://adventuresinmachinelearning.com/2017/05/
  58. https://adventuresinmachinelearning.com/2017/04/
  59. https://adventuresinmachinelearning.com/2017/03/
  60. https://adventuresinmachinelearning.com/category/amazon-aws/
  61. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  62. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
  63. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
  64. https://adventuresinmachinelearning.com/category/deep-learning/
  65. https://adventuresinmachinelearning.com/category/nlp/gensim/
  66. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
  67. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  68. https://adventuresinmachinelearning.com/category/loss-functions/
  69. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
  70. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
  71. https://adventuresinmachinelearning.com/category/nlp/
  72. https://adventuresinmachinelearning.com/category/optimisation/
  73. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
  74. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
  75. https://adventuresinmachinelearning.com/category/reinforcement-learning/
  76. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
  77. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
  78. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
  79. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
  80. https://adventuresinmachinelearning.com/category/nlp/id97/
  81. https://adventuresinmachinelearning.com/wp-login.php
  82. https://adventuresinmachinelearning.com/feed/
  83. https://adventuresinmachinelearning.com/comments/feed/
  84. https://wordpress.org/
  85. https://www.thrivethemes.com/
  86. http://www.wordpress.org/
  87. https://adventuresinmachinelearning.com/wp-login.php
  88. http://adventuresinmachinelearning.com/stochastic-gradient-descent/

   hidden links:
  90. https://adventuresinmachinelearning.com/author/andyt81/
