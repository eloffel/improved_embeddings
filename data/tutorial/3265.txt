        image-to-image translation with conditional adversarial nets
                              [1]phillip isola
                               [2]jun-yan zhu
                              [3]tinghui zhou
                             [4]alexei a. efros
                      univerity of california, berkeley
                                in cvpr 2017
                                 [5][paper]
                                 [6][github]

                                          [7][teaser_v3.jpg]
   example results on several image-to-image translation problems. in each
     case we use the same architecture and objective, simply training on
                               different data.

                                  abstract

   we investigate conditional adversarial networks as a general-purpose
   solution to image-to-image translation problems. these networks not
   only learn the mapping from input image to output image, but also learn
   a id168 to train this mapping. this makes it possible to apply
   the same generic approach to problems that traditionally would require
   very different loss formulations. we demonstrate that this approach is
   effective at synthesizing photos from label maps, reconstructing
   objects from edge maps, and colorizing images, among other tasks. as a
   community, we no longer hand-engineer our mapping functions, and this
   work suggests we can achieve reasonable results without
   hand-engineering our id168s either.
     __________________________________________________________________

                                try our code

                           [8][torch]  [9][pytorch]

   ports of our code:
   [10][tensorflow] (implementation by [11]christopher hesse)
   [12][tensorflow] (implementation by [13]yen-chen lin)
   [14][chainer] (implementation by [15]pfnet-research)
   [16][keras] (implementation by [17]thibault de boissiere)
     __________________________________________________________________

                                    paper

                       [18][paper_pdf_thumb.png] [19]
                               [download] 7mb

                                [20][bibtex]
     __________________________________________________________________

                            [21]interactive demo
                       (made by [22]christopher hesse)

                            [23][edges2cats.jpg]
     __________________________________________________________________

                       expository articles and videos

                            [24]two-minute papers

            iframe: [25]https://www.youtube.com/embed/u7kq5lnfufg

   karoly zsolnai-feher made the above as part of his very cool
   [26]"two-minute papers" series.

                          [27]affinelayer blog post
                          [28][hesse_blog_post.png]

   great explanation by christopher hesse, also documenting his
   [29]tensorflow port of our code.
     __________________________________________________________________

                                 experiments

   here we show comprehensive results from each experiment in our paper.
   please see the paper for details on these experiments.

                           effect of the objective
                               [30]cityscapes
                                 [31]facades
                    effect of the generator architecture
                               [32]cityscapes
                   effect of the discriminator patch scale
                               [33]cityscapes
                                 [34]facades
                             additional results
                              [35]map to aerial
                              [36]aerial to map
                          [37]semantic segmentation
                              [38]day to night
                            [39]edges to handbags
                             [40]edges to shoes
                          [41]sketches to handbags
                            [42]sketches to shoes
     __________________________________________________________________

                    community contributions: [43]#pix2pix

   people have used our code for many creative applications, often posted
   on twitter with the hashtag #pix2pix. check them out [44]here! below we
                      highlight just a few of the many:
                               [45]#edges2cats
                           [46][cats_example.jpg]

   christopher hesse trained our model on converting edge maps to photos
   of cats, and included this in his [47]interactive demo. apparently,
   this is what the internet wanted most, and #edges2cats briefly [48]went
   viral. the above cats were designed by vitaly vidmirov ([49]@vvid).

                            [50]alternative face
                           [51][hardy_conway.jpg]

   mario klingemann used our code to translate the appearance of french
   singer francoise hardy onto kellyanne conway's infamous "alternative
   facts" interview. interesting articles about it can be read [52]here
   and [53]here.

                            [54]person-to-person
                          [55][dorsey_kurzweil.jpg]

   brannon dorsey recorded himself mimicking frames from a video of ray
   kurzweil giving a talk. he then used this data to train a
   dorsey   kurzweil translator, allowing him to become a kind of puppeter
   in control of kurzweil's appearance.

                            [56]interactive anime
                             [57][ipokemon.jpg]

   bertrand gondouin trained our method to translate sketches   pokemon,
   resulting in an interactive drawing tool.

                           [58]background masking
                        [59][background_masking.jpg]

   kaihu chen performed [60]a number of interesting experiments using our
   method, including getting it to mask out the background of a portrait
   as shown above.

                        [61]color palette completion
                          [62][color_palettes.jpg]

   [63]colormind adapted our code to predict a complete 5-color palette
   given a subset of the palette as input. this application stretches the
   definition of what counts as "image-to-image translation" in an
   exciting way: if you can visualize your input/output data as images,
   then image-to-image methods are applicable! (not that this is
   necessarily the best choice of representation, just one to think
   about.)
     __________________________________________________________________

                             recent related work

    [64]id3 have been vigorously explored in
    the last two years, and many conditional variants have been proposed.
    please see the discussion of related work in [65]our paper. below we
      point out three papers that especially influenced this work: the
     original gan paper from goodfellow et al., the [66]dcgan framework,
   from which our code is derived, and the igan paper, from our lab, that
      first explored the idea of using gans for mapping user strokes to
                                   images.
     ian j. goodfellow, jean pouget-abadie, mehdi mirza, bing xu, david
      warde-farley, sherjil ozair, aaron courville, and yoshua bengio.
           id3. nips, 2014. [67][pdf]
   alec radford, luke metz, soumith chintala. unsupervised representation
   learning with deep convolutional id3. iclr,
                          2016. [[68]pdf][[69]code]
      jun-yan zhu, philipp krahenbuhl, eli shechtman, alexei a. efros.
     generative visual manipulation on the natural image manifold. eccv,
                   2016. [[70]pdf][[71]webpage][[72]code]
   also, please check out our follow-up work on image-to-image translation
                     *without* paired training examples:
    jun-yan zhu*, taesung park*, phillip isola, alexei a. efros. unpaired
   image-to-image translation using cycle-consistent adversarial networks.
                arxiv, 2017. [[73]pdf][[74]webpage][[75]code]
     __________________________________________________________________

                              acknowledgements

   we thank richard zhang, deepak pathak, and shubham tulsiani for helpful
   discussions. thanks to saining xie for help with the hed edge detector.
    thanks to the online community for exploring many applications of our
   work and pointing out typos and errors in the paper and code. this work
   was supported in part by nsf sma-1514512, nga nuri, iarpa via air force
     research laboratory, intel corp, berkeley deep drive, and hardware
    donations by nvidia. disclaimer: the views and conclusions contained
      herein are those of the authors and should not be interpreted as
   necessarily representing the official policies or endorsements, either
        expressed or implied, of iarpa, afrl or the u.s. government.

references

   1. http://people.eecs.berkeley.edu/~isola/
   2. https://people.eecs.berkeley.edu/~junyanz/
   3. https://people.eecs.berkeley.edu/~tinghuiz/
   4. http://www.eecs.berkeley.edu/~efros/
   5. https://arxiv.org/abs/1611.07004
   6. https://github.com/phillipi/pix2pix
   7. https://phillipi.github.io/pix2pix/images/teaser_v3.png
   8. https://github.com/phillipi/pix2pix
   9. https://github.com/junyanz/pytorch-cyclegan-and-pix2pix
  10. https://github.com/affinelayer/pix2pix-tensorflow
  11. https://github.com/christopherhesse
  12. https://github.com/yenchenlin/pix2pix-tensorflow
  13. https://github.com/yenchenlin
  14. https://github.com/pfnet-research/chainer-pix2pix
  15. https://github.com/pfnet-research
  16. https://github.com/tdeboissiere/deeplearningimplementations/tree/master/pix2pix
  17. https://github.com/tdeboissiere
  18. https://arxiv.org/abs/1611.07004
  19. https://arxiv.org/abs/1611.07004
  20. https://phillipi.github.io/pix2pix/resources/bibtex_arxiv_pix2pix.txt
  21. https://affinelayer.com/pixsrv/
  22. https://github.com/christopherhesse
  23. https://affinelayer.com/pixsrv/
  24. https://www.youtube.com/embed/u7kq5lnfufg
  25. https://www.youtube.com/embed/u7kq5lnfufg
  26. https://www.youtube.com/channel/ucbfypyitq-7l4upox8nvctg
  27. https://affinelayer.com/pix2pix/
  28. https://affinelayer.com/pix2pix/
  29. https://github.com/affinelayer/pix2pix-tensorflow
  30. https://phillipi.github.io/pix2pix/images/index_cityscapes_loss_variations.html
  31. https://phillipi.github.io/pix2pix/images/index_facades2_loss_variations.html
  32. https://phillipi.github.io/pix2pix/images/index_cityscapes_generator_architecture_variations.html
  33. https://phillipi.github.io/pix2pix/images/index_cityscapes_patchsize_variations.html
  34. https://phillipi.github.io/pix2pix/images/index_facades2_patchsize_variations.html
  35. https://phillipi.github.io/pix2pix/images/map2sat1_btoa/latest_net_g_val/index.html
  36. https://phillipi.github.io/pix2pix/images/sat2map1_atob/latest_net_g_val/index.html
  37. https://phillipi.github.io/pix2pix/images/cityscapes_cgan_atob/latest_net_g_val/index.html
  38. https://phillipi.github.io/pix2pix/images/att_night/latest_net_g_val/index.html
  39. https://phillipi.github.io/pix2pix/images/sketch2photo_handbag/latest_net_g_val/index.html
  40. https://phillipi.github.io/pix2pix/images/sketch2photo_shoes/latest_net_g_val/index.html
  41. https://phillipi.github.io/pix2pix/images/sketch2photo_handbag/latest_net_g_sketch/index.html
  42. https://phillipi.github.io/pix2pix/images/sketch2photo_shoes/latest_net_g_sketch/index.html
  43. https://twitter.com/search?vertical=default&q=pix2pix&src=typd
  44. https://twitter.com/search?vertical=default&q=pix2pix&src=typd
  45. https://twitter.com/search?vertical=default&q=edges2cats&src=typd
  46. https://twitter.com/search?vertical=default&q=edges2cats&src=typd
  47. https://affinelayer.com/pixsrv/
  48. https://www.youtube.com/watch?v=vbue5clrcdy
  49. https://twitter.com/vvid/status/834976420942204933
  50. https://www.youtube.com/watch?v=af_9lxhceby
  51. https://www.youtube.com/watch?v=af_9lxhceby
  52. http://nymag.com/selectall/2017/03/pix2pix-cat-drawing-tool-is-ai-at-its-best.html
  53. http://www.alphr.com/art/1005324/alternative-face-the-machine-that-puts-kellyanne-conway-s-words-into-a-french-singer-s
  54. https://twitter.com/brannondorsey/status/808461108881268736
  55. https://twitter.com/brannondorsey/status/808461108881268736
  56. https://twitter.com/bgondouin/status/818571935529377792
  57. https://twitter.com/bgondouin/status/818571935529377792
  58. http://www.k4ai.com/imageops/index.html
  59. http://www.k4ai.com/imageops/index.html
  60. http://www.k4ai.com/tag/gan/index.html
  61. http://colormind.io/blog/
  62. http://colormind.io/blog/
  63. http://colormind.io/blog/
  64. https://arxiv.org/abs/1406.2661
  65. https://arxiv.org/abs/1611.07004
  66. https://github.com/soumith/dcgan.torch
  67. https://arxiv.org/pdf/1406.2661v1.pdf
  68. https://arxiv.org/pdf/1511.06434v2.pdf
  69. https://github.com/soumith/dcgan.torch
  70. https://arxiv.org/pdf/1609.03552v2.pdf
  71. https://people.eecs.berkeley.edu/~junyanz/projects/gvm/
  72. https://github.com/junyanz/igan
  73. https://arxiv.org/abs/1703.10593
  74. https://junyanz.github.io/cyclegan/
  75. https://github.com/junyanz/cyclegan
