   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

deep learning the stock market

   [9]go to the profile of tal perry
   [10]tal perry (button) blockedunblock (button) followfollowing
   dec 3, 2016
     __________________________________________________________________

   update 25.1.17         took me a while but [11]here is an ipython notebook
   with a rough implementation
   [1*ihzf1r_nywbw9k_yzgph8a.png]

   in the past few months i   ve been fascinated with    deep learning   ,
   especially its applications to language and text. i   ve spent the bulk
   of my career in financial technologies, mostly in algorithmic trading
   and alternative data services. you can see where this is going.

   i wrote this to get my ideas straight in my head. while i   ve become a
      deep learning    enthusiast, i don   t have too many opportunities to
   brain dump an idea in most of its messy glory. i think that a decent
   indication of a clear thought is the ability to articulate it to people
   not from the field. i hope that i   ve succeeded in doing that and that
   my articulation is also a pleasurable read.

   why nlp is relevant to stock prediction

   in many nlp problems we end up taking a sequence and encoding it into a
   single fixed size representation, then decoding that representation
   into another sequence. for example, we might tag entities in the text,
   translate from english to french or convert audio frequencies to text.
   there is a torrent of work coming out in these areas and a lot of the
   results are achieving state of the art performance.

   in my mind the biggest difference between the nlp and financial
   analysis is that language has some guarantee of structure, it   s just
   that the rules of the structure are vague. markets, on the other hand,
   don   t come with a promise of a learnable structure, that such a
   structure exists is the assumption that this project would prove or
   disprove (rather it might prove or disprove if i can find that
   structure).

   assuming the structure is there, the idea of summarizing the current
   state of the market in the same way we encode the semantics of a
   paragraph seems plausible to me. if that doesn   t make sense yet, keep
   reading. it will.
     __________________________________________________________________

   you shall know a word by the company it keeps (firth, j. r. 1957:11)

   there is tons of literature on id27s. [12]richard socher   s
   lecture is a great place to start. in short, we can make a geometry of
   all the words in our language, and that geometry captures the meaning
   of words and relationships between them. you may have seen the example
   of    king-man +woman=queen    or something of the sort.
   [1*t-1epa3cit7laglbr_3yeq.jpeg]
   geometry between words. an embedding captures a geometrical pattern
   between companies and their ceos just by seeing these words together.
   image from the glove project
   ([13]http://nlp.stanford.edu/projects/glove/)

   embeddings are cool because they let us represent information in a
   condensed way. the old way of representing words was holding a vector
   (a big list of numbers) that was as long as the number of words we
   know, and setting a 1 in a particular place if that was the current
   word we are looking at. that is not an efficient approach, nor does it
   capture any meaning. with embeddings, we can represent all of the words
   in a fixed number of dimensions (300 seems to be plenty, 50 works
   great) and then leverage their higher dimensional geometry to
   understand them.

   the picture below shows an example. an embedding was trained on more or
   less the entire internet. after a few days of intensive calculations,
   each word was embedded in some high dimensional space. this    space    has
   a geometry, concepts like distance, and so we can ask which words are
   close together. the authors/inventors of that method made an example.
   here are the words that are closest to frog.
   [1*sa6estup7wjibpuabnvusg.png]
   the nearest neighbors of the vector for the word frog, according to the
   glove algorithm (a kind of id27). notice how it knows about
   words you   ve never heard of and captures their similarity to from. from
   glove project ([14]http://nlp.stanford.edu/projects/glove/)

   but we can embed more than just words. we can do, say , stock market
   embeddings.
     __________________________________________________________________

   market2vec

   the first id27 algorithm i heard about was id97. i want
   to get the same effect for the market, though i   ll be using a different
   algorithm. my input data is a csv, the first column is the date, and
   there are 4*1000 columns corresponding to the high low open closing
   price of 1000 stocks. that is my input vector is 4000 dimensional,
   which is too big. so the first thing i   m going to do is stuff it into a
   lower dimensional space, say 300 because i liked the movie.
   [1*ayp49wmgnrbkcl_acgxk2w.jpeg]
   the face you make when you give the extra effort to embedd 4000
   dimensions into 300

   taking something in 4000 dimensions and stuffing it into a
   300-dimensional space my sound hard but its actually easy. we just need
   to multiply matrices. a matrix is a big excel spreadsheet that has
   numbers in every cell and no formatting problems. imagine an excel
   table with 4000 columns and 300 rows, and when we basically bang it
   against the vector a new vector comes out that is only of size 300. i
   wish that   s how they would have explained it in college.

   iframe: [15]/media/65e72f9c5099b4110f3037d7a918fb5b?postid=df853d139e02

   the fanciness starts here as we   re going to set the numbers in our
   matrix at random, and part of the    deep learning    is to update those
   numbers so that our excel spreadsheet changes. eventually this matrix
   spreadsheet (i   ll stick with matrix from now on) will have numbers in
   it that bang our original 4000 dimensional vector into a concise 300
   dimensional summary of itself.

   we   re going to get a little fancier here and apply what they call an
   activation function. we   re going to take a function, and apply it to
   each number in the vector individually so that they all end up between
   0 and 1 (or 0 and infinity, it depends). why ? it makes our vector more
   special, and makes our learning process able to understand more
   complicated things. [16]how?

   so what? what i   m expecting to find is that that new embedding of the
   market prices (the vector) into a smaller space captures all the
   essential information for the task at hand, without wasting time on the
   other stuff. so i   d expect they   d capture correlations between other
   stocks, perhaps notice when a certain sector is declining or when the
   market is very hot. i don   t know what traits it will find, but i assume
   they   ll be useful.
     __________________________________________________________________

   now what

   lets put aside our market vectors for a moment and talk about language
   models. [17]andrej karpathy wrote the epic post    [18]the unreasonable
   effectiveness of recurrent neural networks   . if i   d summarize in the
   most liberal fashion the post boils down to
    1. if we look at the works of shakespeare and go over them character
       by character, we can use    deep learning    to learn a language model.
    2. a language model (in this case) is a magic box. you put in the
       first few characters and it tells you what the next one will be.
    3. if we take the character that the language model predicted and feed
       it back in we can keep going forever.

   and then as a punchline, he generated a bunch of text that looks like
   shakespeare. and then he did it again with the linux source code. and
   then again with a textbook on algebraic geometry.

   so i   ll get back to the mechanics of that magic box in a second, but
   let me remind you that we want to predict the future market based on
   the past just like he predicted the next word based on the previous
   one. where karpathy used characters, we   re going to use our market
   vectors and feed them into the magic black box. we haven   t decided what
   we want it to predict yet, but that is okay, we won   t be feeding its
   output back into it either.

   going deeper

   i want to point out that this is where we start to get into the deep
   part of deep learning. so far we just have a single layer of learning,
   that excel spreadsheet that condenses the market. now we   re going to
   add a few more layers and stack them, to make a    deep    something.
   that   s the deep in deep learning.

   so karpathy shows us some sample output from the linux source code,
   this is stuff his black box wrote.
static void action_new_function(struct s_stat_info *wb)
{
  unsigned long flags;
  int lel_idx_bit = e->edd, *sys & ~((unsigned long) *first_compat);
  buf[0] = 0xffffffff & (bit << 4);
  min(inc, slist->bytes);
  printk(kern_warning "memory allocated %02x/%02x, "
    "original mll instead\n"),
    min(min(multi_run - s->len, max) * num_data_in),
    frame_pos, sz + first_seg);
  div_u64_w(val, inb_p);
  spin_unlock(&disk->queue_lock);
  mutex_unlock(&s->sock->mutex);
  mutex_unlock(&func->mutex);
  return disassemble(info->pending_bh);
}

   notice that it knows how to open and close parentheses, and respects
   indentation conventions; the contents of the function are properly
   indented and the multi-line printk statement has an inner indentation.
   that means that this magic box understands long range dependencies.
   when it   s indenting within the print statement it knows it   s in a print
   statement and also remembers that it   s in a function( or at least
   another indented scope). that   s nuts. it   s easy to gloss over that but
   an algorithm that has the ability to capture and remember long term
   dependencies is super useful because    we want to find long term
   dependencies in the market.
     __________________________________________________________________

   inside the magical black box

   what   s inside this magical black box? it is a type of recurrent neural
   network (id56) called an lstm. an id56 is a deep learning algorithm that
   operates on sequences (like sequences of characters). at every step, it
   takes a representation of the next character (like the embeddings we
   talked about before) and operates on the representation with a matrix,
   like we saw before. the thing is, the id56 has some form of internal
   memory, so it remembers what it saw previously. it uses that memory to
   decide how exactly it should operate on the next input. using that
   memory, the id56 can    remember    that it is inside of an intended scope
   and that is how we get properly nested output text.
   [1*nkhwsoynut5xu7pyf6znhg.png]
   a basic id56. graphics from @christopher olah   s blog
   [19]http://colah.github.io/posts/2015-08-understanding-lstms/

   a fancy version of an id56 is called a long short term memory (lstm).
   lstm has cleverly designed memory that allows it to
    1. selectively choose what it remembers
    2. decide to forget
    3. select how much of it   s memory it should output.

   [1*j5w8frasmi93z81nlaui4w.png]
   best illustartion every explain an lstm. graphics from @christopher
   olah   s blog
   [20]http://colah.github.io/posts/2015-08-understanding-lstms/

   so an lstm can see a    {    and say to itself    oh yeah, that   s important i
   should remember that    and when it does, it essentially remembers an
   indication that it is in a nested scope. once it sees the corresponding
      }    it can decide to forget the original opening brace and thus forget
   that it is in a nested scope.

   we can have the lstm learn more abstract concepts by stacking a few of
   them on top of each other, that would make us    deep    again. now each
   output of the previous lstm becomes the inputs of the next lstm, and
   each one goes on to learn higher abstractions of the data coming in. in
   the example above (and this is just illustrative speculation), the
   first layer of lstms might learn that characters separated by a space
   are    words   . the next layer might learn word types like (static void
   action_new_function).the next layer might learn the concept of a
   function and its arguments and so on. it   s hard to tell exactly what
   each layer is doing, though karpathy   s blog has a really nice example
   of how he did visualize exactly that.
     __________________________________________________________________

   connecting market2vec and lstms

   the studious reader will notice that karpathy used characters as his
   inputs, not embeddings (technically a one-hot encoding of characters).
   but, lars eidnes actually used id27s when he wrote
   [21]auto-generating clickbait with recurrent neural network
   [1*ce8go1kpq8o-xuqzurfi5a.png]
   lars eidnes network for [22]auto-generating clickbait with recurrent
   neural network (thank you lars)

   the figure above shows the network he used. ignore the softmax part
   (we   ll get to it later). for the moment, check out how on the bottom he
   puts in a sequence of words vectors at the bottom and each one.
   (remember, a    word vector    is a representation of a word in the form of
   a bunch of numbers, like we saw in the beginning of this post). lars
   inputs a sequence of word vectors and each one of them:
    1. influences the first lstm
    2. makes it   s lstm output something to the lstm above it
    3. makes it   s lstm output something to the lstm for the next word

   we   re going to do the same thing with one difference, instead of word
   vectors we   ll input    marketvectors   , those market vectors we described
   before. to recap, the marketvectors should contain a summary of what   s
   happening in the market at a given point in time. by putting a sequence
   of them through lstms i hope to capture the long term dynamics that
   have been happening in the market. by stacking together a few layers of
   lstms i hope to capture higher level abstractions of the market   s
   behavior.
     __________________________________________________________________

   what comes out

   thus far we haven   t talked at all about how the algorithm actually
   learns anything, we just talked about all the clever transformations
   we   ll do on the data. we   ll defer that conversation to a few paragraphs
   down, but please keep this part in mind as it is the se up for the
   punch line that makes everything else worthwhile.

   in karpathy   s example, the output of the lstms is a vector that
   represents the next character in some abstract representation. in
   eidnes    example, the output of the lstms is a vector that represents
   what the next word will be in some abstract space. the next step in
   both cases is to change that abstract representation into a id203
   vector, that is a list that says how likely each character or word
   respectively is likely to appear next. that   s the job of the softmax
   function. once we have a list of likelihoods we select the character or
   word that is the most likely to appear next.

   in our case of    predicting the market   , we need to ask ourselves what
   exactly we want to market to predict? some of the options that i
   thought about were:
    1. predict the next price for each of the 1000 stocks
    2. predict the value of some index (s&p, vix etc) in the next n
       minutes.
    3. predict which of the stocks will move up by more than x% in the
       next n minutes
    4. (my personal favorite) predict which stocks will go up/down by 2x%
       in the next n minutes while not going down/up by more than x% in
       that time.
    5. (the one we   ll follow for the remainder of this article). predict
       when the vix will go up/down by 2x% in the next n minutes while not
       going down/up by more than x% in that time.

   1 and 2 are regression problems, where we have to predict an actual
   number instead of the likelihood of a specific event (like the letter n
   appearing or the market going up). those are fine but not what i want
   to do.

   3 and 4 are fairly similar, they both ask to predict an event (in
   technical jargon         a class label). an event could be the letter n
   appearing next or it could be moved up 5% while not going down more
   than 3% in the last 10 minutes. the trade-off between 3 and 4 is that 3
   is much more common and thus easier to learn about while 4 is more
   valuable as not only is it an indicator of profit but also has some
   constraint on risk.

   5 is the one we   ll continue with for this article because it   s similar
   to 3 and 4 but has mechanics that are easier to follow. the [23]vix is
   sometimes called the fear index and it represents how volatile the
   stocks in the s&p500 are. it is derived by observing the [24]implied
   volatility for specific options on each of the stocks in the index.

   sidenote         why predict the vix

   what makes the vix an interesting target is that
    1. it is only one number as opposed to 1000s of stocks. this makes it
       conceptually easier to follow and reduces computational costs.
    2. it is the summary of many stocks so most if not all of our inputs
       are relevant
    3. it is not a linear combination of our inputs. implied volatility is
       extracted from a complicated, non-linear formula stock by stock.
       the vix is derived from a complex formula on top of that. if we can
       predict that, it   s pretty cool.
    4. it   s tradeable so if this actually works we can use it.

   back to our lstm outputs and the softmax

   how do we use the formulations we saw before to predict changes in the
   vix a few minutes in the future? for each point in our dataset, we   ll
   look what happened to the vix 5 minutes later. if it went up by more
   than 1% without going down more than 0.5% during that time we   ll output
   a 1, otherwise a 0. then we   ll get a sequence that looks like:

     0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,1,1,1,0,0,0,0,0    .

   we want to take the vector that our lstms output and squish it so that
   it gives us the id203 of the next item in our sequence being a 1.
   the squishing happens in the softmax part of the diagram above.
   (technically, since we only have 1 class now, we use a sigmoid ).

   so before we get into how this thing learns, let   s recap what we   ve
   done so far
    1. we take as input a sequence of price data for 1000 stocks
    2. each timepoint in the sequence is a snapshot of the market. our
       input is a list of 4000 numbers. we use an embedding layer to
       represent the key information in just 300 numbers.
    3. now we have a sequence of embeddings of the market. we put those
       into a stack of lstms, timestep by timestep. the lstms remember
       things from the previous steps and that influences how they process
       the current one.
    4. we pass the output of the first layer of lstms into another layer.
       these guys also remember and they learn higher level abstractions
       of the information we put in.
    5. finally, we take the output from all of the lstms and    squish them   
       so that our sequence of market information turns into a sequence of
       probabilities. the id203 in question is    how likely is the
       vix to go up 1% in the next 5 minutes without going down 0.5%   
     __________________________________________________________________

   how does this thing learn?

   now the fun part. everything we did until now was called the forward
   pass, we   d do all of those steps while we train the algorithm and also
   when we use it in production. here we   ll talk about the backward pass,
   the part we do only while in training that makes our algorithm learn.

   so during training, not only did we prepare years worth of historical
   data, we also prepared a sequence of prediction targets, that list of 0
   and 1 that showed if the vix moved the way we want it to or not after
   each observation in our data.

   to learn, we   ll feed the market data to our network and compare its
   output to what we calculated. comparing in our case will be simple
   subtraction, that is we   ll say that our model   s error is

     error = (((precomputed)    (predicted id203))   )^(1/2)

   or in english, the square root of the square of the difference between
   what actually happened and what we predicted.

   here   s the beauty. that   s a differential function, that is, we can tell
   by how much the error would have changed if our prediction would have
   changed a little. our prediction is the outcome of a differentiable
   function, the softmax the inputs to the softmax, the lstms are all
   mathematical functions that are differentiable. now all of these
   functions are full of parameters, those big excel spreadsheets i talked
   about ages ago. so at this stage what we do is take the derivative of
   the error with respect to every one of the millions of parameters in
   all of those excel spreadsheets we have in our model. when we do that
   we can see how the error will change when we change each parameter, so
   we   ll change each parameter in a way that will reduce the error.

   this procedure propagates all the way to the beginning of the model. it
   tweaks the way we embed the inputs into marketvectors so that our
   marketvectors represent the most significant information for our task.

   it tweaks when and what each lstm chooses to remember so that their
   outputs are the most relevant to our task.

   it tweaks the abstractions our lstms learn so that they learn the most
   important abstractions for our task.

   which in my opinion is amazing because we have all of this complexity
   and abstraction that we never had to specify anywhere. it   s all
   inferred mathamagically from the specification of what we consider to
   be an error.
   [1*z4eh4w8cxkvsoqchcck0aw.jpeg]

   what   s next

   now that i   ve laid this out in writing and it still makes sense to me i
   want
    1. to see if anyone bothers reading this.
    2. to fix all of the mistakes my dear readers point out
    3. consider if this is still feasible
    4. and build it

   so, if you   ve come this far please point out my errors and share your
   inputs.
     __________________________________________________________________

   other thoughts

   here are some mostly more advanced thoughts about this project, what
   other things i might try and why it makes sense to me that this may
   actually work.

   liquidity and efficient use of capital

   generally the more liquid a particular market is the more efficient
   that is. i think this is due to a chicken and egg cycle, whereas a
   market becomes more liquid it is able to absorb more capital moving in
   and out without that capital hurting itself. as a market becomes more
   liquid and more capital can be used in it, you   ll find more
   sophisticated players moving in. this is because it is expensive to be
   sophisticated, so you need to make returns on a large chunk of capital
   in order to justify your operational costs.

   a quick corollary is that in less liquid markets the competition isn   t
   quite as sophisticated and so the opportunities a system like this can
   bring may not have been traded away. the point being were i to try and
   trade this i would try and trade it on less liquid segments of the
   market, that is maybe the tase 100 instead of the s&p 500.

   this stuff is new

   the knowledge of these algorithms, the frameworks to execute them and
   the computing power to train them are all new at least in the sense
   that they are available to the average joe such as myself. i   d assume
   that top players have figured this stuff out years ago and have had the
   capacity to execute for as long but, as i mention in the above
   paragraph, they are likely executing in liquid markets that can support
   their size. the next tier of market participants, i assume, have a
   slower velocity of technological assimilation and in that sense, there
   is or soon will be a race to execute on this in as yet untapped
   markets.

   multiple time frames

   while i mentioned a single stream of inputs in the above, i imagine
   that a more efficient way to train would be to train market vectors (at
   least) on multiple time frames and feed them in at the id136 stage.
   that is, my lowest time frame would be sampled every 30 seconds and i   d
   expect the network to learn dependencies that stretch hours at most.

   i don   t know if they are relevant or not but i think there are patterns
   on multiple time frames and if the cost of computation can be brought
   low enough then it is worthwhile to incorporate them into the model.
   i   m still wrestling with how best to represent these on the
   computational graph and perhaps it is not mandatory to start with.

   marketvectors

   when using word vectors in nlp we usually start with a pretrained model
   and continue adjusting the embeddings during training of our model. in
   my case, there are no pretrained market vector available nor is tehre a
   clear algorithm for training them.

   my original consideration was to use an auto-encoder like in [25]this
   paper but end to end training is cooler.

   a more serious consideration is the success of sequence to sequence
   models in translation and id103, where a sequence is
   eventually encoded as a single vector and then decoded into a different
   representation (like from speech to text or from english to french). in
   that view, the entire architecture i described is essentially the
   encoder and i haven   t really laid out a decoder.

   but, i want to achieve something specific with the first layer, the one
   that takes as input the 4000 dimensional vector and outputs a 300
   dimensional one. i want it to find correlations or relations between
   various stocks and compose features about them.

   the alternative is to run each input through an lstm, perhaps
   concatenate all of the output vectors and consider that output of the
   encoder stage. i think this will be inefficient as the interactions and
   correlations between instruments and their features will be lost, and
   thre will be 10x more computation required. on the other hand, such an
   architecture could naively be paralleled across multiple gpus and hosts
   which is an advantage.

   id98s

   recently there has been a spur of papers on character level machine
   translation. this [26]paper caught my eye as they manage to capture
   long range dependencies with a convolutional layer rather than an id56.
   i haven   t given it more than a brief read but i think that a
   modification where i   d treat each stock as a channel and convolve over
   channels first (like in rgb images) would be another way to capture the
   market dynamics, in the same way that they essentially encode semantic
   meaning from characters.

     * [27]machine learning
     * [28]artificial intelligence
     * [29]deep learning
     * [30]stock market
     * [31]finance

   (button)
   (button)
   (button) 2.8k claps
   (button) (button) (button) 66 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of tal perry

[33]tal perry

   founder of lighttag.io, platform to annotate text for nlp. google
   developer expert in ml. former nlp@citi cto@superfly

     * (button)
       (button) 2.8k
     * (button)
     *
     *

   [34]go to the profile of tal perry
   never miss a story from tal perry, when you sign up for medium.
   [35]learn more
   never miss a story from tal perry
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/df853d139e02
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@talperry/deep-learning-the-stock-market-df853d139e02&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@talperry/deep-learning-the-stock-market-df853d139e02&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@talperry?source=post_header_lockup
  10. https://medium.com/@talperry
  11. https://github.com/talolard/marketvectors/blob/master/preparedata.ipynb
  12. https://www.youtube.com/watch?v=xhhol3tnyjs&index=2&list=plmimxx8char9ig0zhsytqgsdhb9weegam
  13. http://nlp.stanford.edu/projects/glove/
  14. http://nlp.stanford.edu/projects/glove/
  15. https://medium.com/media/65e72f9c5099b4110f3037d7a918fb5b?postid=df853d139e02
  16. https://lmgtfy.com/?q=why+does+deep+learning+use+non+linearities
  17. https://medium.com/@karpathy
  18. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  19. http://colah.github.io/posts/2015-08-understanding-lstms/
  20. http://colah.github.io/posts/2015-08-understanding-lstms/
  21. https://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/
  22. https://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/
  23. https://en.wikipedia.org/wiki/vix
  24. https://en.wikipedia.org/wiki/implied_volatility
  25. http://cs229.stanford.edu/proj2013/takeuchilee-applyingdeeplearningtoenhancemomentumtradingstrategiesinstocks.pdf
  26. https://arxiv.org/pdf/1610.03017v2.pdf
  27. https://medium.com/tag/machine-learning?source=post
  28. https://medium.com/tag/artificial-intelligence?source=post
  29. https://medium.com/tag/deep-learning?source=post
  30. https://medium.com/tag/stock-market?source=post
  31. https://medium.com/tag/finance?source=post
  32. https://medium.com/@talperry?source=footer_card
  33. https://medium.com/@talperry
  34. https://medium.com/@talperry
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://medium.com/p/df853d139e02/share/twitter
  38. https://medium.com/p/df853d139e02/share/facebook
  39. https://medium.com/p/df853d139e02/share/twitter
  40. https://medium.com/p/df853d139e02/share/facebook
