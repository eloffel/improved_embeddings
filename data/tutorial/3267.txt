decoding the thought vector

   neural networks have the rather uncanny knack for turning meaning into
   numbers. data flows from the input to the output, getting pushed
   through a series of transformations which process the data into
   increasingly abstruse vectors of representations. these numbers, the
   activations of the network, carry useful information from one layer of
   the network to the next, and are believed to represent the data at
   different layers of abstraction. but the vectors themselves have thus
   far defied interpretation.

   in this blog post i put forward a possible interpretation of these
   vectors. i argue we shouldn't take these vectors literally, but rather
   as an encoding for a simpler, sparse data structure. this gives rise to
   a simple technique (the -svd) for reverse engineering this data
   structure, and gives us the tools to decode the vectors' meaning.

   applying this trick to a variational autoencoder, trained on a dataset
   of faces, produces a decomposition of [1]this face [yann.jpg] into its
   bare components

   and applying it to the image captioning system neuraltalk2 yields a
   breakdown of a sentence into the sum of a few simple sentence
   fragments. [2]this [cat.jpg] image gets captioned as

   unlike approaches like infogan ([3]chen et al.) or the -vae ([4]higgins
   et al.) this requires no special training. and this idea can be in
   principle applied to a large class of neural networks.

the encoder/decoder network design

   let's restrict our attention to a common pattern in neural network
   design. a little old school, perhaps, but still elegant, this pattern
   combines two powerhouses of deep learning, an encoder and a decoder,
   via composition to produce our net,

   the encoder is the lens in which we see the data. it sees the raw data,
   and through a sequence of linear transformations and activations,
   distills all of its salient information into an incomprehensible array
   of numbers (one typically smaller than the input size). the decoder
   takes this vector, and through another poorly understood process,
   interprets it, returning the desired output. this formula is simple,
   but effective. a good number of papers in deep learning are a result of
   finding such fruitful combinations.
                                         c2c                         c2r r2c r2r
   autoencoder
   [5]lamb et al.
  [6]kingma et al. image captioning [7]vinyals et al. [8]neuraltalk2
   image synthesis
   [9]reed et al. id195
   [10]sutskever et al.
   [11]vinyals et al.

   the defining characteristic of these architectures is the information
   bottleneck. this bottleneck is the output of and the input of and is
   illustrated by the solid black rectangle in the above diagrams. at
   least in principle, the information bottleneck forces a compression of
   the input data by mapping it into a lower dimension, so that only its
   meaningful dimensions of variation are captured - and the rest,
   'noise', is discarded.

   this vector has been called, by various people, an "embedding", a
   "representational vector" or a "latent vector". but geoff hinton, in a
   stroke of marketing genius, gave it the name "thought vector".

the geometry of thought vectors

   thought vectors have been observed empirically to possess some curious
   properties. the most fascinating among these is known colloquially as
   "linear structure" - the observation that certain directions in
   thought-space can be given semantic meaning.

   [12]white observes the following property, for example. take an
   autoencoder trained on faces. first we identify (by hand) a few (say )
   images containing a certain qualitative feature - a smile, for example.
   encode and average them to get a vector, .

   equation_smile

   this output, which [13]white calls the smile vector, can be interpreted
   as a code for an abstract smile. we can visualize this vector by
   looking at its output on the decoder.

   decoder_smile

   the background of this image is grey - a clue about its indifference to
   background color. and though the face is vaguely feminine, it is also
   generic, apart from an exaggerated, toothy grin. we can add to any
   thought vector makes the decoder's output smile.

   equation_fake_smile

   these vectors are used to great effect for image processing in
   [14]upchurch et al.

ideas in sparse superposition

   there is a very straightforward line of thought which follows from the
   observation of linear structure. taking the dogma that directions are
   meanings to its logical conclusion, it's a small leap to conjecture the
   whole thought vector is nothing more than sum of these directions.
   let's refer to these directions as atoms. if were a picture of a man in
   short hair wearing sunglasses, for example, a decomposition might look
   like

   if we had access to the full complement of atoms (now numbered) in , we
   can write

   where is small. in this interpretation of a thought vector, the 's
   themselves have no meaning - they just need to, collectively, satisfy a
   few simple mathematical properties. in essence, they just need to be
   "different enough". different enough that its presence in the sum can
   be reliably determined.

   the fact that detecting these atoms is even possible may seem
   counterintuitive. adding numbers together is typically an irreversible
   operation. say if we add , there's no way to recover from ( works just
   as well). the story is different, however, in higher dimensions. if the
   's were orthogonal, we can check for 's presence by probing the
   encoding on the left by taking the dot product with .

   this allows for the storage of atoms in a vector of size . this is
   rather obvious, but bear with me. we can in fact store far more atoms
   than if is sparse. by the magic of [15]sparse recovery, we can store as
   many as atoms in a humble vector of length nondestructively. needs to
   satisfy a few technical properties, of course, but what is critical is
   that (the number of nonzero entries in ) is small. the sparser is, the
   more atoms we can handle. think of this as a tradeoff between storing
   information in and storing information in the nonzero locations of .

   this is more than an information theoretic trick, however - it's also a
   natural way of representing information. think of this as a [16]tagging
   system, where the nonzero in represents a tag. every data point can be
   tagged with at most tags from a list of tags (e.g sunglasses, facial
   hair, blonde, etc), which can be large. each tag has its own rating,
   but it is the tags themselves, not the individual ratings, which
   contain the salient information. this is basis of sparse coding.

   sparse structure has already been found in some shallow embedding
   system such as [17]id97 by [18]arora et. al. and if such sparse
   structure exists in deep networks, we can derive a simple 2 step
   process to interpret thought vectors.
    1. take the data the model was trained on

    2. try to recover and via dictionary learning

   the second step in this process can be achieved heuristically with the
   -svd. for these experiments, i used [19]pyksvd.

   once we have , we can decompose any output of (even those not in the
   training set) via sparse coding

   i use [20]pyksvd for this too, which solves this problem heuristically
   using [21]orthogonal matching pursuit.

faces

   my first experiment will be on a variational autoencoder designed by
   [22]lamb et al., trained on the [23]celeb-a dataset of faces. the
   thought vector is a element vector, and the neural net does a noble job
   of capturing most of the dimensions of variation in the images.

   using a dictionary of total atoms, with each element a sum of sparse
   atoms, i can produce a pretty respectable reconstruction of most of the
   faces. the atoms are signed, and the positive atoms look like [24]this
   and the negative atoms look like [25]this. the reconstructions can be
   visualized incrementally, by finding the best reconstruction for
   increasing values of .

   (move your mouse over the icons to see how the truncated reconstruction
   looks)

   notice how the reconstructor works much like an artist does. first it
   paints a fairly generic face in broad strokes. then it fills in small
   local details which make the face recognizable. here is another example

   on some of the more generic faces, the reconstruction is pretty decent
   even at . this allows us to visualize each atom's exact contribution to
   the overall picture.

   true to our hypothesis, each vector in the dictionary has meaning. like
   the smile vector, adding these vectors to existing images produces
   interesting effects. the usual suspects are all present, corresponding
   to

   angle

   facial hair and accessories

   lighting

   and facial expressions.

   but there are also a few interesting surprises. like the following
   atoms for

   face shape

   or a whole slew of atoms dedicated to modeling the forehead and hair.

   and we can run this entire thing in reverse. this provides a way of
   querying our training set, to find, for example, all faces which are
   lit strongly from the front

   people wearing sunglasses

   and fans of heavy metal music.

neuraltalk2

   we move on from interpolating images to interpolating sentences. here
   we investigate the image captioning system [26]neuraltalk2. this
   network is a hybrid of a convolutional net (the vggnet) and a recurrent
   net, fine tuned to the captioning task on the coco dataset. this
   network as thought vector of size . the network takes a picture, turns
   it into a thought vector, and turns that vector into a sentence using a
   recurrent neural net. the captions look like this:

   (1) (2) (3) (4)
   [cat.jpg] [blackandwhite.jpg] [skateboarders.jpg] [knifedude.jpg]
   a woman is holding a cat in a kitchen a couple of people walking down a
   street holding umbrellas a group of four different types of computer
   equipment a man in a suit and tie standing in a room

   which range in quality from the technically correct to pretty good. a
   conspicuous mistake the system often makes is the omission of certain
   critical, but somewhat out of place elements of a picture.

   the captioning system does marvelously on (1) and (2), picking up on
   subtle cues that the woman is holding a dog, and is in a kitchen. but
   where is the knife in (3), and the lego figurines in (4)? our method of
   analysis provides a means to debug such problems. surprisingly, using
   only a sparsity of and a dictionary size of was enough.

   let us visualize these thought vectors. our language model does not
   generate a single sentence, but a id203 distribution over
   possible sentences. so we visualize its output in the form of a
   [27]dialog tree. each path from the root to a leaf represents a
   sentence, with its id203 the product of the thicknesses of the
   respective edges. i generated these figures by sampling from this
   distribution a few times, and combining the data in a trie.

   [28]image (1) [cat.jpg] produces the following output.

   i've taken the liberty of giving them my own labels to facilitate
   navigation. the output of the algorithm is an excellent synthesis of
   the concepts of "dog", "woman at counter", "woman in front of cake",
   and surprisingly, the verb/noun combo "holding a cat". we can visualize
   each atom by looking at examples of images for which thought vector
   contains these elements.

   dog

   holding a cat

   girl/woman and cake

   woman at counter

caption 2

   [29]image (2) [blackandwhite.jpg] consists of two meaningful atoms (i
   have omitted two atoms with small weights which were
   misclassifications). the first corresponds to "a black and white
   picture", and the other to "man with an umbrella".

   and true to form, we can look at all the other pictures which contain
   these atoms

   black and white photo of

   people with umbrellas

image 3

   [30]image (3) [skateboarders.jpg] produces the following output.

   the keyboard is detected, of course. but surprisingly, so are the
   presence of the lego figures as "skateboarders". failing to combine
   these two elements, the language model simply chose to omit what wasn't
   convenient to articulate.

   keyboard

   skateboarder

image 4

   [31]image (4) [knifedude.jpg] demonstrates the same problem.

   the atoms are visualized here:

   scissors

   tie

   this "failure to synthesize" is surprisingly common. taking linear
   combinations of two unrelated sentences, for example, would often
   result in the outputs interpolating in a discontinuous manner. usually
   one or the other would dominate the output depending on their relative
   weights. this "bug" can be interpreted as a feature, however. the
   language models resistance to combing two semantically alien ideas
   seems to be an emergence of a kind of "common sense" - but this
   sensibility comes at the expense of a more free-form creativity.

   here is another example of this behavior. take this atom for statue. we
   can combine it with other atoms

   but the language model itself dictates how the sentences are merged.
   you might want the language model to spit out "a statue of a man riding
   a snowboard". but the language model actually returns "a statue of a
   man riding a horse". in a way, the model can be forgiven for this - the
   model has never seen a statue of a man on a snowboard, and so is
   reluctant to caption it so. the model has an internal idea of what
   statues are.

   the dictionary elements are not restricted to nouns. it permits certain
   modifiers too. this element stands for "a group of".

   and here a similar atom seems to be able to count up to 4.

   rather curiously, it turns "airplanes" into "knives". i do not
   understand why this happens.

   a final note. unlike the previous model, the dictionary elements
   produced by -svd are largely unsigned. though some negative vectors do
   bleed into the 's they are generally small in magnitude and don't seem
   to have meaningful interpretations. this seems to be a consequence of
   the thought vectors being taken after a activation - forcing the entire
   vector to be positive. since never sees a negative vector, the entirety
   of is "dead space", and the atoms can only interact constructively.

conclusion

   the final question that should be asked is why this structure should
   even exist in the first place. how does this structure emerge from
   training? and how does the decoder work?

   identifying sparse elements in a thought vector may not be as difficult
   a task as it initially seems. given the right conditions on it can be
   done quite efficiently by solving the convex sparse coding problem:

   this is pretty encouraging. it has been hypothesized by [32]gregor et
   al. that the decoder might be implementing an unfolded sparse coding
   algorithm, at least for a single iteration. perhaps this theory can be
   confirmed by correlating various constellations of activations to the
   atoms of our dictionary. and perhaps there's a possibility we can read
   right off the decoder.

   the former riddle is more difficult to answer. and it breaks down into
   a bevy of minor mysteries when probed. is this structure specific to
   certain neural architectures (perhaps those which use activations)? or
   does it come from the data? was this structure discovered
   automatically, or were the assumptions of sparsity hidden in the
   network structure? does sparse structure exist in all levels of
   representation, or only encoder/decoder networks? is sparse coding even
   the true model for the data, or is this just an approximation to how
   the data is really represented? but lacking any formal theory of deep
   learning, these questions are still open to investigation. i hope to
   have convinced you, at least, that this is an avenue worth
   investigating.

   thanks [33]http://distill.pub for the inspiration in design and a
   certain yellow icon.

   this is my [34]website, [35]blog and twitter [36]follow @gabeeegoooh

references

   1. javascript:void(0)
   2. javascript:void(0)
   3. https://arxiv.org/find/cs/1/au:+chen_x/0/1/0/all/0/1
   4. http://openreview.net/pdf?id=sy2fzu9gl
   5. https://arxiv.org/abs/1602.03220
   6. https://arxiv.org/abs/1312.6114
   7. https://arxiv.org/abs/1609.06647
   8. https://github.com/karpathy/neuraltalk2
   9. https://arxiv.org/abs/1605.05396
  10. https://arxiv.org/abs/1409.3215
  11. https://arxiv.org/abs/1506.05869
  12. https://arxiv.org/abs/1609.04468
  13. https://arxiv.org/abs/1609.04468
  14. https://arxiv.org/abs/1611.05507
  15. http://www.math.ucla.edu/~wotaoyin/summer2013/slides/lec03_sparserecoveryguarantees.pdf
  16. http://www.imdb.com/search/keyword/
  17. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  18. https://arxiv.org/abs/1601.03764
  19. https://github.com/hoytak/pyksvd
  20. https://github.com/hoytak/pyksvd
  21. https://en.wikipedia.org/wiki/matching_pursuit
  22. https://github.com/vdumoulin/discgen
  23. http://mmlab.ie.cuhk.edu.hk/projects/celeba.html
  24. http://gabgoh.github.io/thoughtvectors/allatomsp.jpg
  25. http://gabgoh.github.io/thoughtvectors/allatomsn.jpg
  26. https://github.com/karpathy/neuraltalk2
  27. https://i.imgur.com/ejxdkne.jpg
  28. javascript:void(0)
  29. javascript:void(0)
  30. javascript:void(0)
  31. http://gabgoh.github.io/thoughtvectors/
  32. http://yann.lecun.com/exdb/publis/pdf/gregor-icml-10.pdf
  33. http://distill.pub/
  34. http://gabgoh.github.io/
  35. http://gabgoh.github.io/blog/
  36. https://twitter.com/gabeeegoooh
