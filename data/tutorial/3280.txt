   #[1]openai

     * [2]about
     * [3]progress
     * [4]resources
     * [5]blog

   (button)

   (button)

     * [6]about
         ______________________________________________________________

     * [7]progress
         ______________________________________________________________

     * [8]resources
         ______________________________________________________________

     * [9]blog
         ______________________________________________________________

     * [10]jobs
     __________________________________________________________________

   unsupervised sentiment neuron

unsupervised sentiment neuron

   april 6, 2017
   6 minute read

   we   ve [11]developed an [12]unsupervised system which learns an
   excellent representation of sentiment, despite being trained only to
   predict the next character in the text of amazon reviews.

   a [13]linear model using this representation achieves state-of-the-art
   id31 accuracy on a small but extensively-studied dataset,
   the stanford sentiment treebank (we get 91.8% accuracy versus the
   previous best of 90.2%), and can match the performance of previous
   supervised systems using 30-100x fewer labeled examples. our
   representation also contains a distinct    [14]sentiment neuron    which
   contains almost all of the sentiment signal.

   [15]view on github[16]view on arxiv

   our system beats other approaches on stanford sentiment treebank while
   using dramatically less data.

   comparison of our models' id31 accuracy compared to
   supervised model benchmarks.
   the number of labeled examples it takes two variants of our model (the
   green and blue lines) to match fully supervised approaches, each
   trained with 6,920 examples (the dashed gray lines). our l1-regularized
   model (pretrained in an unsupervised fashion on amazon reviews) matches
   [multichannel id98](https://arxiv.org/abs/1408.5882) performance with
   only 11 labeled examples, and state-of-the-art [17]ct-lstm ensembles
   with 232 examples.

   we were very surprised that our model learned an interpretable feature,
   and that simply [18]predicting the next character in amazon reviews
   resulted in discovering the concept of sentiment. we believe the
   phenomenon is not specific to our model, but is instead a general
   property of certain large neural networks that are trained to predict
   the next step or dimension in their inputs.

methodology

   we first trained a [19]multiplicative lstm with 4,096 units on a corpus
   of 82 million amazon reviews to predict the next character in a chunk
   of text. training took one month across four nvidia pascal gpus, with
   our model processing 12,500 characters per second.

   these 4,096 units (which are just a vector of floats) can be regarded
   as a feature vector representing the string read by the model. after
   training the mlstm, we turned the model into a sentiment classifier by
   taking a linear combination of these units, learning the weights of the
   combination via the available supervised data.

sentiment neuron

   while training the linear model with l1 id173, we noticed it
   used surprisingly few of the learned units. digging in, we realized
   there actually existed a single    sentiment neuron    that's highly
   predictive of the sentiment value.

   distribution of the value of the sentiment neuron across positive and
   negative reviews
   the sentiment neuron within our model can classify reviews as negative
   or positive, even though the model is trained only to predict the next
   character in the text.

   just like with similar models, our model can be used to generate text.
   unlike those models, we have a direct dial to control the sentiment of
   the resulting text: we simply overwrite the value of the sentiment
   neuron.
   sentiment fixed to positive sentiment fixed to negative
   just what i was looking for. nice fitted pants, exactly matched seam to
   color contrast with other pants i own. highly recommended and also very
   happy! the package received was blank and has no barcode. a waste of
   time and money.
   this product does what it is supposed to. i always keep three of these
   in my kitchen just in case ever i need a replacement cord. great little
   item. hard to put on the crib without some kind of embellishment. my
   guess is just like the screw kind of attachment i had.
   best hammock ever! stays in place and holds it's shape. comfy (i love
   the deep neon pictures on it), and looks so cute. they didn't fit
   either. straight high sticks at the end. on par with other buds i have.
   lesson learned to avoid.
   dixie is getting her doolittle newsletter we'll see another new one
   coming out next year. great stuff. and, here's the contents -
   information that we hardly know about or forget. great product but no
   seller. couldn't ascertain a cause. broken product. i am a prolific
   consumer of this company all the time.
   i love this weapons look . like i said beautiful !!! i recommend it to
   all. would suggest this to many roleplayers, and i stronge to get them
   for every one i know. a must watch for any man who love chess! like the
   cover, fits good. . however, an annoying rear piece like garbage should
   be out of this one. i bought this hoping it would help with a huge pull
   down my back & the black just doesn't stay. scrap off everytime i use
   it.... very disappointed.
   examples of synthetic text generated by the trained model. above, we
   select random samples from the model after fixing the sentiment unit's
   value to determine the sentiment of the review. below, we also pass the
   prefix "i couldn't figure out" through the model and select
   high-likelihood samples only.
   sentiment fixed to positive sentiment fixed to negative
   i couldn't figure out the shape at first but it definitely does what
   it's meant to do. it's a great product and i recommend it highly i
   couldn't figure out how to use the product. it did not work.at least
   there was no quality control; this tablet does not work. i would have
   given it zero stars, but that was not an option.
   i couldn't figure out why this movie had been discontinued! now i can
   enjoy it anytime i like. so glad to have found it again. i couldn't
   figure out how to set it up being that there was no warning on the box.
   i wouldn't recommend this to anyone.
   i couldn't figure out how to use the video or the book that goes along
   with it, but it is such a fantastic book on how to put it into
   practice! i couldn't figure out how to use the gizmo. what a waste of
   time and money. might as well through away this junk.
   i couldn't figure out how to use just one and my favorite running app.
   i use it all the time. good quality, you cant beat the price. i
   couldn't figure out how to stop this drivel. at worst, it was going
   absolutely nowhere, no matter what i did.needles to say, i skim-read
   the entire book. don't waste your time.
   i couldn't figure out how to attach these balls to my little portable
   drums, but these fit the bill and were well worth every penny. i
   couldn't figure out how to play it.

example

   the diagram below represents the character-by-character value of the
   sentiment neuron, displaying negative values as red and positive values
   as green. note that strongly indicative words like    best    or
      horrendous    cause particularly big shifts in the color.

   [20]animation of model predicting sentiment as it reads an amazon
   review character by character. final image:
   https://openai.com/blog/content/images/2017/04/sentiment-prediction.png
   the sentiment neuron adjusting its value on a character-by-character
   basis.

   it's interesting to note that the system also makes large updates after
   the completion of sentences and phrases. for example, in    and about
   99.8 percent of that got lost in the film   , there   s a negative update
   after    lost    and a larger update at the sentence   s end, even though    in
   the film    has no sentiment content on its own.

unsupervised learning

   labeled data are the fuel for today's machine learning. collecting data
   is easy, but scalably labeling that data is hard. it   s only feasible to
   generate labels for important problems where the reward is worth the
   effort, like machine translation, id103, or self-driving.

   machine learning researchers have long dreamed of developing
   [21]unsupervised [22]learning [23]algorithms [24]to [25]learn a good
   representation of a dataset, which can then be used to solve tasks
   using only a few labeled examples. our research implies that simply
   training large unsupervised next-step-prediction models on large
   amounts of data may be a good approach to use when creating systems
   with good representation learning capabilities.

next steps

   our results are a promising step towards general unsupervised
   representation learning. we found the results by exploring whether we
   could learn good quality representations as a side effect of language
   modeling, and scaled up an existing model on a carefully-chosen
   dataset. yet the underlying phenomena remain more mysterious than
   clear.
     * these results were not as strong for datasets of long documents. we
       suspect our character-level model struggles to remember information
       over hundreds to thousands of timesteps. we think it   s worth trying
       id187 that can adapt the timescales at which they
       operate. further scaling up these models may further improve
       representation fidelity and performance on id31 and
       similar tasks.
     * the model struggles the more the input text diverges from review
       data. it   s worth verifying that broadening the corpus of text
       samples results in an equally informative representation that also
       applies to broader domains.
     * our results suggest that there exist settings where very large
       next-step-prediction models learn excellent unsupervised
       representations. training a large neural network to predict the
       next frame in a large collection of videos may result in
       unsupervised representations for object, scene, and action
       classifiers.

   overall, it's important to understand the properties of models,
   training regimes, and datasets that reliably lead to such excellent
   representations.
     __________________________________________________________________

   cover artwork
   ludwig pettersson
     __________________________________________________________________

   authors
   [26]alec radford[27]ilya sutskever[28]rafa   j  zefowicz[29]jack
   clark[30]greg brockman
     __________________________________________________________________

     * [31]about
     * [32]progress
     * [33]resources
     * [34]blog
     * [35]charter
     * [36]jobs
     * [37]press

     *
     *

   sign up for our newsletter
   ____________________ (button)
   right

references

   visible links
   1. https://openai.com/blog/rss/
   2. https://openai.com/about/
   3. https://openai.com/progress/
   4. https://openai.com/resources/
   5. https://openai.com/blog/
   6. https://openai.com/about/
   7. https://openai.com/progress/
   8. https://openai.com/resources/
   9. https://openai.com/blog/
  10. https://openai.com/jobs/
  11. https://arxiv.org/abs/1704.01444
  12. https://openai.com/blog/unsupervised-sentiment-neuron/#unsupervisedlearning
  13. https://openai.com/blog/unsupervised-sentiment-neuron/#methodology
  14. https://openai.com/blog/unsupervised-sentiment-neuron/#sentimentneuron
  15. https://github.com/openai/generating-reviews-discovering-sentiment
  16. https://arxiv.org/abs/1704.01444
  17. https://arxiv.org/abs/1702.02181
  18. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  19. https://arxiv.org/abs/1609.07959
  20. https://openai.com/content/images/2017/04/sentiment-prediction.png
  21. https://en.wikipedia.org/wiki/id97
  22. https://arxiv.org/abs/1506.06726
  23. http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.2006.18.7.1527
  24. http://www-cs.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf
  25. https://arxiv.org/abs/1505.05192
  26. https://openai.com/blog/authors/alec/
  27. https://openai.com/blog/authors/ilya/
  28. https://openai.com/blog/authors/rafal/
  29. https://openai.com/blog/authors/jack-clark/
  30. https://openai.com/blog/authors/greg/
  31. https://openai.com/about/
  32. https://openai.com/progress/
  33. https://openai.com/resources/
  34. https://openai.com/blog/
  35. https://openai.com/charter/
  36. https://openai.com/jobs/
  37. https://openai.com/press/

   hidden links:
  39. https://openai.com/
  40. https://openai.com/
  41. https://twitter.com/openai
  42. https://www.facebook.com/openai.research
