   #[1]openai

     * [2]about
     * [3]progress
     * [4]resources
     * [5]blog

   (button)

   (button)

     * [6]about
         ______________________________________________________________

     * [7]progress
         ______________________________________________________________

     * [8]resources
         ______________________________________________________________

     * [9]blog
         ______________________________________________________________

     * [10]jobs
     __________________________________________________________________

   evolution strategies as a scalable alternative to reinforcement
   learning

evolution strategies as a scalable alternative to id23

   march 24, 2017
   12 minute read

   we've [11]discovered that evolution strategies (es), an optimization
   technique that's been known for decades, rivals the performance of
   standard id23 (rl) techniques on modern rl benchmarks
   (e.g. atari/mujoco), while overcoming many of rl's inconveniences.

   in particular, es is simpler to implement (there is no need for
   [12]id26), it is easier to scale in a distributed setting,
   it does not suffer in settings with sparse rewards, and has fewer
   [13]hyperparameters. this outcome is surprising because es resembles
   simple hill-climbing in a high-dimensional space based only on
   [14]finite differences along a few random directions at each step.

   [15]view on github[16]view on arxiv

   our finding continues the modern trend of achieving strong results with
   decades-old ideas. for example, in 2012, the [17]"alexnet" paper showed
   how to design, scale and train convolutional neural networks (id98s) to
   achieve extremely strong results on image recognition tasks, at a time
   when most researchers thought that id98s were not a promising approach
   to id161. similarly, in 2013, the [18]deep id24 paper
   showed how to combine id24 with id98s to successfully solve atari
   games, reinvigorating rl as a research field with exciting experimental
   (rather than theoretical) results. likewise, our work demonstrates that
   es achieves strong performance on rl benchmarks, dispelling the common
   belief that es methods are impossible to apply to high dimensional
   problems.

   es is easy to implement and scale. running on a computing cluster of 80
   machines and 1,440 cpu cores, our implementation is able to train a 3d
   mujoco humanoid walker in only 10 minutes (a3c on 32 cores takes about
   10 hours). using 720 cores we can also obtain comparable performance to
   a3c on atari while cutting down the training time from 1 day to 1 hour.

   in what follows, we'll first briefly describe the conventional rl
   approach, contrast that with our es approach, discuss the tradeoffs
   between es and rl, and finally highlight some of our experiments.

id23

   let's briefly look at how rl works. suppose we are given some
   environment (e.g. a game) that we'd like to train an agent on. to
   describe the behavior of the agent, we define a policy function (the
   brain of the agent), which computes how the agent should act in any
   given situation. in practice, the policy is usually a neural network
   that takes the current state of the game as an input and calculates the
   id203 of taking any of the allowed actions. a typical policy
   function might have about 1,000,000 parameters, so our task comes down
   to finding the precise setting of these parameters such that the policy
   plays well (i.e. wins a lot of games).
   [first-graphic-1.png] above: in the game of pong, the policy could take
   the pixels of the screen and compute the id203 of moving the
   player's paddle (in green, on right) up, down, or neither.

   the training process for the policy works as follows. starting from a
   random initialization, we let the agent interact with the environment
   for a while and collect episodes of interaction (e.g. each episode is
   one game of pong). we thus obtain a complete recording of what
   happened: what sequence of states we encountered, what actions we took
   in each state, and what the reward was at each step. as an example,
   below is a diagram of three episodes that each took 10 time steps in a
   hypothetical environment. each rectangle is a state, and rectangles are
   colored green if the reward was positive (e.g. we just got the ball
   past our opponent) and red if the reward was negative (e.g. we missed
   the ball):

   this diagram suggests a recipe for how we can improve the policy;
   whatever we happened to do leading up to the green states was good, and
   whatever we happened to do in the states leading up to the red areas
   was bad. we can then use id26 to compute a small update on
   the network's parameters that would make the green actions more likely
   in those states in the future, and the red actions less likely in those
   states in the future. we expect that the updated policy works a bit
   better as a result. we then iterate the process: collect another batch
   of episodes, do another update, etc.

   exploration by injecting noise in the actions. the policies we usually
   use in rl are stochastic, in that they only compute probabilities of
   taking any action. this way, during the course of training, the agent
   may find itself in a particular state many times, and at different
   times it will take different actions due to the sampling. this provides
   the signal needed for learning; some of those actions will lead to good
   outcomes, and get encouraged, and some of them will not work out, and
   get discouraged. we therefore say that we introduce exploration into
   the learning process by injecting noise into the agent's actions, which
   we do by sampling from the action distribution at each time step. this
   will be in contrast to es, which we describe next.

evolution strategies

   on "evolution". before we dive into the es approach, it is important to
   note that despite the word "evolution", es has very little to do with
   biological evolution. early versions of these techniques may have been
   inspired by biological evolution and the approach can, on an abstract
   level, be seen as sampling a population of individuals and allowing the
   successful individuals to dictate the distribution of future
   generations. however, the mathematical details are so heavily
   abstracted away from biological evolution that it is best to think of
   es as simply a class of black-box stochastic optimization techniques.

   black-box optimization. in es, we forget entirely that there is an
   agent, an environment, that there are neural networks involved, or that
   interactions take place over time, etc. the whole setup is that
   1,000,000 numbers (which happen to describe the parameters of the
   policy network) go in, 1 number comes out (the total reward), and we
   want to find the best setting of the 1,000,000 numbers. mathematically,
   we would say that we are optimizing a function f(w) with respect to the
   input vector w (the parameters / weights of the network), but we make
   no assumptions about the structure of f, except that we can evaluate it
   (hence "black box").

   the es algorithm. intuitively, the optimization is a "guess and check"
   process, where we start with some random parameters and then repeatedly
   1) tweak the guess a bit randomly, and 2) move our guess slightly
   towards whatever tweaks worked better. concretely, at each step we take
   a parameter vector w and generate a population of, say, 100 slightly
   different parameter vectors w1 ... w100 by jittering w with gaussian
   noise. we then evaluate each one of the 100 candidates independently by
   running the corresponding policy network in the environment for a
   while, and add up all the rewards in each case. the updated parameter
   vector then becomes the weighted sum of the 100 vectors, where each
   weight is proportional to the total reward (i.e. we want the more
   successful candidates to have a higher weight). mathematically, you'll
   notice that this is also equivalent to estimating the gradient of the
   expected reward in the parameter space using finite differences, except
   we only do it along 100 random directions. yet another way to see it is
   that we're still doing rl (policy gradients, or [19]reinforce
   specifically), where the agent's actions are to emit entire parameter
   vectors using a gaussian policy.
   [evo.png] above: es optimization process, in a setting with only two
   parameters and a reward function (red = high, blue = low). at each
   iteration we show the current parameter value (in white), a population
   of jittered samples (in black), and the estimated gradient (white
   arrow). we keep moving the parameters to the top of the arrow until we
   converge to a local optimum. you can reproduce this figure with
   [20]this notebook.

   code sample. to make the core algorithm concrete and to highlight its
   simplicity, here is a short example of optimizing a quadratic function
   using es (or see this [21]longer version with more comments):
# simple example: minimize a quadratic around some solution point
import numpy as np
solution = np.array([0.5, 0.1, -0.3])
def f(w): return -np.sum((w - solution)**2)

npop = 50      # population size
sigma = 0.1    # noise standard deviation
alpha = 0.001  # learning rate
w = np.random.randn(3) # initial guess
for i in range(300):
  n = np.random.randn(npop, 3)
  r = np.zeros(npop)
  for j in range(npop):
    w_try = w + sigma*n[j]
    r[j] = f(w_try)
  a = (r - np.mean(r)) / np.std(r)
  w = w + alpha/(npop*sigma) * np.dot(n.t, a)

   injecting noise in the parameters. notice that the objective is
   identical to the one that rl optimizes: the expected reward. however,
   rl injects noise in the action space and uses id26 to
   compute the parameter updates, while es injects noise directly in the
   parameter space. another way to describe this is that rl is a "guess
   and check" on actions, while es is a "guess and check" on parameters.
   since we're injecting noise in the parameters, it is possible to use
   deterministic policies (and we do, in our experiments). it is also
   possible to add noise in both actions and parameters to potentially
   combine the two approaches.

tradeoffs between es and rl

   es enjoys multiple advantages over rl algorithms (some of them are a
   little technical):
     * no need for id26. es only requires the forward pass of
       the policy and does not require id26 (or value function
       estimation), which makes the code shorter and between 2-3 times
       faster in practice. on memory-constrained systems, it is also not
       necessary to keep a record of the episodes for a later update.
       there is also no need to worry about exploding gradients in id56s.
       lastly, we can explore a much larger function class of policies,
       including networks that are not differentiable (such as in binary
       networks), or ones that include complex modules (e.g. pathfinding,
       or various optimization layers).
     * highly parallelizable. es only requires workers to communicate a
       few scalars between each other, while in rl it is necessary to
       synchronize entire parameter vectors (which can be millions of
       numbers). intuitively, this is because we control the random seeds
       on each worker, so each worker can locally reconstruct the
       perturbations of the other workers. thus, all that we need to
       communicate between workers is the reward of each perturbation. as
       a result, we observed linear speedups in our experiments as we
       added on the order of thousands of cpu cores to the optimization.
     * higher robustness. several hyperparameters that are difficult to
       set in rl implementations are side-stepped in es. for example, rl
       is not "scale-free", so one can achieve very different learning
       outcomes (including a complete failure) with different settings of
       the frame-skip hyperparameter in atari. as we show in our work, es
       works about equally well with any frame-skip.
     * structured exploration. some rl algorithms (especially policy
       gradients) initialize with random policies, which often manifests
       as random jitter on spot for a long time. this effect is mitigated
       in id24 due to epsilon-greedy policies, where the max
       operation can cause the agents to perform some consistent action
       for a while (e.g. holding down a left arrow). this is more likely
       to do something in a game than if the agent jitters on spot, as is
       the case with policy gradients. similar to id24, es does not
       suffer from these problems because we can use deterministic
       policies and achieve consistent exploration.
     * credit assignment over long time scales. by studying both es and rl
       gradient estimators mathematically we can see that es is an
       attractive choice especially when the number of time steps in an
       episode is long, where actions have longlasting effects, or if no
       good value function estimates are available.

   conversely, we also found some challenges to applying es in practice.
   one core problem is that in order for es to work, adding noise in
   parameters must lead to different outcomes to obtain some gradient
   signal. as we elaborate on in our paper, we found that the use of
   virtual batchnorm can help alleviate this problem, but further work on
   effectively parameterizing neural networks to have variable behaviors
   as a function of noise is necessary. as an example of a related
   difficulty, we found that in montezuma's revenge, one is very unlikely
   to get the key in the first level with a random network, while this is
   occasionally possible with random actions.

es is competitive with rl

   we compared the performance of es and rl on two standard rl benchmarks:
   mujoco control tasks and atari game playing. each mujoco task (see
   examples below) contains a physically-simulated articulated figure,
   where the policy receives the positions of all joints and has to output
   the torques to apply at each joint in order to move forward. below are
   some example agents trained on three mujoco control tasks, where the
   objective is to move forward:

   we usually compare the performance of algorithms by looking at their
   efficiency of learning from data; as a function of how many states
   we've seen, what is our average reward? here are the example learning
   curves that we obtain, in comparison to rl (the [22]trpo algorithm in
   this case):

   data efficiency comparison. the comparisons above show that es (orange)
   can reach a comparable performance to trpo (blue), although it doesn't
   quite match or surpass it in all cases. moreover, by scanning
   horizontally we can see that es is less efficient, but no worse than
   about a factor of 10 (note the x-axis is in log scale).

   wall clock comparison. instead of looking at the raw number of states
   seen, one can argue that the most important metric to look at is the
   wall clock time: how long (in number of seconds) does it take to solve
   a given problem? this quantity ultimately dictates the achievable speed
   of iteration for a researcher. since es requires negligible
   communication between workers, we were able to solve one of the hardest
   mujoco tasks (a 3d humanoid) using 1,440 cpus across 80 machines in
   only 10 minutes. as a comparison, in a typical setting 32 a3c workers
   on one machine would solve this task in about 10 hours. it is also
   possible that the performance of rl could also improve with more
   algorithmic and engineering effort, but we found that naively scaling
   a3c in a standard cloud cpu setting is challenging due to high
   communication bandwidth requirements.

   below are a few videos of 3d humanoid walkers trained with es. as we
   can see, the results have quite a bit of variety, based on which local
   minimum the optimization ends up converging into.

   on atari, es trained on 720 cores in 1 hour achieves comparable
   performance to a3c trained on 32 cores in 1 day. below are some result
   snippets on pong, seaquest and beamrider. these videos show the
   preprocessed frames, which is exactly what the agent sees when it is
   playing:

   in particular, note that the submarine in seaquest correctly learns to
   go up when its oxygen reaches low levels.

related work

   es is an algorithm from the neuroevolution literature, which has a long
   history in ai and a complete literature review is beyond the scope of
   this post. however, we encourage an interested reader to look at
   [23]wikipedia, [24]scholarpedia, and j  rgen schmidhuber's [25]review
   article (section 6.6). the work that most closely informed our approach
   is [26]natural evolution strategies by wierstra et al. 2014. compared
   to this work and much of the work it has inspired, our focus is
   specifically on scaling these algorithms to large-scale, distributed
   settings, finding components that make the algorithms work better with
   deep neural networks (e.g. [27]virtual batch norm), and evaluating them
   on modern rl benchmarks.

   it is also worth noting that neuroevolution-related approaches have
   seen some recent resurgence in the machine learning literature, for
   example with [28]hypernetworks, [29]"large-scale evolution of image
   classifiers" and [30]"convolution by evolution".

conclusion

   our work suggests that neuroevolution approaches can be competitive
   with id23 methods on modern agent-environment
   benchmarks, while offering significant benefits related to code
   complexity and ease of scaling to large-scale distributed settings. we
   also expect that more exciting work can be done by revisiting other
   ideas from this line of work, such as indirect encoding methods, or
   evolving the network structure in addition to the parameters.

   note on supervised learning. it is also important to note that
   supervised learning problems (e.g. image classification, speech
   recognition, or most other tasks in the industry), where one can
   compute the exact gradient of the id168 with id26,
   are not directly impacted by these findings. for example, in our
   preliminary experiments we found that using es to estimate the gradient
   on the mnist digit recognition task can be as much as 1,000 times
   slower than using id26. it is only in rl settings, where one
   has to estimate the gradient of the expected reward by sampling, where
   es becomes competitive.

   code release. finally, if you'd like to try running es yourself, we
   encourage you to dive into the full details by reading [31]our paper or
   looking at our code on this [32]github repo.
     __________________________________________________________________

   cover artwork
   ben barry
     __________________________________________________________________

   authors
   [33]andrej karpathy[34]tim salimans[35]jonathan ho[36]peter
   chen[37]ilya sutskever[38]john schulman[39]greg brockman[40]szymon
   sidor
     __________________________________________________________________

     * [41]about
     * [42]progress
     * [43]resources
     * [44]blog
     * [45]charter
     * [46]jobs
     * [47]press

     *
     *

   sign up for our newsletter
   ____________________ (button)
   right

references

   visible links
   1. https://openai.com/blog/rss/
   2. https://openai.com/about/
   3. https://openai.com/progress/
   4. https://openai.com/resources/
   5. https://openai.com/blog/
   6. https://openai.com/about/
   7. https://openai.com/progress/
   8. https://openai.com/resources/
   9. https://openai.com/blog/
  10. https://openai.com/jobs/
  11. https://arxiv.org/abs/1703.03864
  12. http://neuralnetworksanddeeplearning.com/chap2.html
  13. https://www.quora.com/what-are-hyperparameters-in-machine-learning
  14. https://en.wikipedia.org/wiki/finite_difference
  15. https://github.com/openai/evolution-strategies-starter
  16. https://arxiv.org/abs/1703.03864
  17. https://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks
  18. https://arxiv.org/abs/1312.5602
  19. http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf
  20. https://github.com/karpathy/randomfun/blob/master/es.ipynb
  21. https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d
  22. https://arxiv.org/abs/1502.05477
  23. https://en.wikipedia.org/wiki/neuroevolution
  24. http://www.scholarpedia.org/article/neuroevolution
  25. https://arxiv.org/abs/1404.7828
  26. http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf
  27. https://arxiv.org/abs/1606.03498
  28. https://arxiv.org/abs/1609.09106
  29. https://arxiv.org/abs/1703.01041
  30. https://arxiv.org/abs/1606.02580
  31. https://arxiv.org/abs/1703.03864
  32. https://github.com/openai/evolution-strategies-starter
  33. https://openai.com/blog/authors/andrej/
  34. https://openai.com/blog/authors/tim/
  35. https://openai.com/blog/authors/jonathan-ho/
  36. https://openai.com/blog/authors/peter-chen/
  37. https://openai.com/blog/authors/ilya/
  38. https://openai.com/blog/authors/john/
  39. https://openai.com/blog/authors/greg/
  40. https://openai.com/blog/authors/szymon-sidor/
  41. https://openai.com/about/
  42. https://openai.com/progress/
  43. https://openai.com/resources/
  44. https://openai.com/blog/
  45. https://openai.com/charter/
  46. https://openai.com/jobs/
  47. https://openai.com/press/

   hidden links:
  49. https://openai.com/
  50. https://openai.com/
  51. https://twitter.com/openai
  52. https://www.facebook.com/openai.research
