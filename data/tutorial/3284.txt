   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]machine learnings
     * [9]all articles
     * [10]non-technical guide to ai
     * [11]newsletter     
     __________________________________________________________________

deep spelling

rethinking id147 in the 21st century

   [12]go to the profile of tal weiss
   [13]tal weiss (button) blockedunblock (button) followfollowing
   mar 22, 2016

   i implemented my first spelling corrector years ago based on peter
   norvig   s [14]excellent tutorial         a spelling corrector in 21 lines of
   python code.

   and it sucked.

   so i tried to fix it. i piled on double metaphone phonetic similarity,
   unicode support, multi-word expressions, weighted damerau-levenshtein
   edit-distance, efficient trie and smart caching.

   and it still sucked.

   and the reason it failed to go beyond a simple toy is very simple         i
   am not google (and neither are you).

   even in its simplest form, spelling a short word took a long time         say
   ~0.1 seconds. that might be ok for some uses, but when you are dealing
   with real-time chat, faced with the upcoming [15]bot gold rush,
   interactivity reigns supreme. and don   t forget that spelling is only a
   single chain in the long processing pipeline of delivering value via
   natural language understanding, dialog management, business logic and
   of course the actual application.

   the root cause of the abysmal performance is that the speller is trying
   to brute-force its way to the right solution. here is the core of
   norvig   s code:
   [1*uwczm4pwvqv2w1ww6iyahq.png]
   nasty brute force

   the function looks at every possible edit to the input         a deletion of
   any character, a transposition of any 2 adjacent characters, replacing
   any character in the input with a random character or simply inserting
   a random character. and for each of the results in the set of edited
   strings         it calculates every possible edit again!

   the result is a challenging amount of computation that needs to happen,
   and which grows exponentially with respect to the length of the input
   string.
     __________________________________________________________________

   iana-neuroscientist but i   m pretty confident our brain does not spell
   correct this way. we look at jumbled words and we automatically
   compensate for the noise.

     can yu read this massage despite thehorible sppeling msitakes?
     i bet you kan.

   sometimes this process happens subconsciously and so intuitively that
   we miss the fact that there is a mistake in the text at all.

   do you think our brain implements edit-3 distance functions?
   there must be a better way than brute force.
   there must be a way for a computer to learn this    intuition   .
     __________________________________________________________________

   so i tried a different approach. it   s called deep learning         a branch
   of machine learning based on a set of algorithms that attempt to model
   high-level abstractions in data by using multiple processing layers
   composed of multiple non-linear transformations (i   m quoting
   [16]wikipedia here, sorry   ).

   deep learning was [17]popularized recently when an ai named alphago
   defeated the top go player in the world in 4 out of 5 games.

   the specifics of my approach are less important as i think many
   different deep learning architectures can achieve similar results due
   to the fact that they are all based on the same premise:
    1. build an artificial network of sufficient    computational power   
       (artificial neurons, connectivity)
    2. feed it a lot of data until it figures out what to do with it.

   nonetheless, here are some technical details:
     * open source tools: python / [18]theano / [19]keras
     * did not use google   s [20]tensorflow as it is [21]a pain to install
       on ec2 and is [22]slower(?!)
       (funny thing         google is marking    [23]tensorflow   , the term it
       coined, as a spelling mistake in my chrome. the irony).
     * i used a sequence-to-sequence architecture as the input size can be
       different than the output size. the input size can also vary
       greatly.
       this is very similar to models used for machine translation tasks.
     * character based; i preprocessed very little and kept the 75 most
       popular characters.
       yes, this means digits and some punctuation as well.
       yes, the embedded    knowledge    of the system is basically a
       character based language model. i don   t see a reason to tokenize
       the input (break it up into words) adding noise to the process and
       applying    [24]feature engineering   . iana-linguist either and my
       model learns features much better than i ever could design them.
       besides         how would you tokenize    whereis th elove   ?
       there is a reason the space bar is so much larger than other keys
       on most keyboards         people tend to misplace it.
       i did reverse the order of the input because it generates many
       short term dependencies between the    question    and the target
          answer    which made id64 the optimization problem easier.
     * [25]long short term memory layers (2 for the encoder and 2 for the
       decoder) with plenty of [26]dropout (0.3).
     * gaussian initialization scaled by fan-in ([27]he et al.)
     * [28]adam, stochastic optimization.
     * my objective was [29]categorical cross-id178: also known as
       multiclass logloss.
     * trained on the amazon cloud (ec2 g2.2xlarge gpu instances).
       no, i do not have multiple [30]32 tesla k40 gpu setups for 3 weeks.
     * it took 12 hours to achieve 90% accuracy and 3 more days to reach
       95.5% on the validation set.
       total cost of training = $40.
     __________________________________________________________________

   id158s are very loosely based on biological neurons
   in our brains and i am not trying to imply that this algorithm is in
   any way related to how our brain actually works.
     __________________________________________________________________

   you might be asking where the data came from. data is key for spelling
   (and for deep learning in general) and if you are google and have in
   your database thousands of common misspellings of    britney spears    and
      arnold schwarzenegger    you can leverage this treasure to implement
   very effective algorithms.
   but if you are a startup you need to cleverly and creatively improvise.

   i used a [31]billion word dataset that google released for language
   modeling research, and injected artificial noise. the noise is the
   simulated spelling mistakes and the model tries to learn how to correct
   the input by comparing the output to the original text         an
   [32]autoencoder.

   we also already have several millions of logs at [33]evature which i
   plan to use for id20.
     __________________________________________________________________

   here are some examples of input and output:
              all the examples look like this:
q the messsed up text
a the original text
    the output of the algorithm, given the 1st line as input

   the algorithm quickly learned the identity function:
q and it can render further applications
a and it can render further applications
    and it can render further applications

   but then it also learned to remove added noise:
q he had dated fori much of the past
a he had dated for much of the past
    he had dated for much of the past

   to replace erroneous characters:
q since then, the bigjest players in
a since then, the biggest players in
    since then, the biggest players in

   fix transposed characters:
q strong summer film slatew ith plenty
a strong summer film slate with plenty
    strong summer film slate with plenty

   and re-insert missing characters:
q in te third quarter of last year,
t in the third quarter of last year,
    in the third quarter of last year,

   some of the errors are not really errors:
q in addition to personal-injury and
a in addition to personal-injury and
    in addition to personal injury and

   some might say this is even better than the original:
q had learned of ca secret plan y iran
t had learned of a secret plan by iran
    had learned of a secret plan i ran

   yet some errors are even introduced by the algorithm   
q post-thanksgiving performances, but
a post-thanksgiving performances, but
    post-thanks gving performances, but
     __________________________________________________________________

   here is the heart of the algorithm in 8 lines of code.
   apples and oranges, of course. this is high level keras code and not
   pure python.
model = sequential()
model.add(recurrent.lstm(hidden_size, input_shape=(none, len(chars)), init="he_n
ormal", dropout_w=dropout_w, dropout_u=dropout_u))
model.add(repeatvector(output_len))
for _ in range(layers):
    model.add(recurrent.lstm(hidden_size, return_sequences=true, init=initializa
tion), dropout_w=dropout_w, dropout_u=dropout_u)
model.add(timedistributeddense(len(chars), init="he_normal"))
model.add(activation(   softmax   ))
model.compile(loss=   categorical_crossid178   , optimizer=   adam   )
     __________________________________________________________________

   todo:
     * continue training
     * optimize hyperparameters, maybe using [34]spearmint
     * add    [35]attention    to the model
     * add more [36]regularizers.
     * publish code to github.
     * publish ipython notebook.
     * expand on the theory.
     * adapt to other domains         maybe try the [37]reddit corpus released a
       few months ago.
     * try to extract real world spelling mistakes from wikipedia as
       suggested by this awesome [38]stack overflow answer.
     * use smoothed version of confusion matrix to generate the noise,
       such as the ones from [39]id203 scoring for spelling
       correction by church and gale.
     __________________________________________________________________

from the life of a machine learning engineer:

   6:10 am         my phone sounds an annoying alarm and it is time to get up
   and prepare for the new day (yeah, kids, yay).
   but before i get out of bed i ssh to the server and tmux a to see the
   latest numbers. every morning is like christmas as performance
   improves.

     > memoryerror

   damn!

parting thoughts:

     * this post and task is about engineering, not science. that   s where
       the state of the art is right now. the technology works and it is
       up to us, the lowly engineers to put is to good use.
     * you might be wondering about language support. nothing about this
       task used any prior knowledge about the input language, so the
       algorithm should work, as is, for any language.
     * i got some good results on getting from noisy characters to
       id147 but there is no reason to stop there. i see no
       reason why you cannot continue to higher level tasks such as
       [40]id39 as demonstrated in one of my favorite
       papers n[41]atural language processing (almost) from scratch, only
       using noisy character streams as the input and having the model
       magically    handle    the spelling mistakes automatically.
     __________________________________________________________________

   join 30,000+ people who read the weekly [42]    machine learnings    
   newsletter to understand how ai will impact the way they work and live.

     * [43]machine learning
     * [44]neural networks
     * [45]deep learning
     * [46]artificial intelligence
     * [47]technology

   (button)
   (button)
   (button) 1.1k claps
   (button) (button) (button) 27 (button) (button)

     (button) blockedunblock (button) followfollowing
   [48]go to the profile of tal weiss

[49]tal weiss

   founder, evature

     (button) follow
   [50]machine learnings

[51]machine learnings

   understand how machine learning and artificial intelligence will change
   your work & life.

     * (button)
       (button) 1.1k
     * (button)
     *
     *

   [52]machine learnings
   never miss a story from machine learnings, when you sign up for medium.
   [53]learn more
   never miss a story from machine learnings
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://machinelearnings.co/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/9ffef96a24f6
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://machinelearnings.co/deep-spelling-9ffef96a24f6&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://machinelearnings.co/deep-spelling-9ffef96a24f6&source=--------------------------nav_reg&operation=register
   8. https://machinelearnings.co/?source=logo-lo_kztt2kpiomfq---6b021343882e
   9. https://machinelearnings.co/tagged/machine-learning
  10. https://machinelearnings.co/a-humans-guide-to-machine-learning-e179f43b67a0
  11. http://subscribe.machinelearnings.co/?utm_source=machine_learnings&utm_medium=blog&utm_campaign=publication_homepage&utm_content=navigation_cta
  12. https://machinelearnings.co/@majortal?source=post_header_lockup
  13. https://machinelearnings.co/@majortal
  14. http://norvig.com/spell-correct.html
  15. http://techcrunch.com/2016/03/17/facebooks-messenger-in-a-bot-store/
  16. https://en.wikipedia.org/wiki/deep_learning
  17. http://www.theverge.com/2016/3/15/11213518/alphago-deepmind-go-match-5-result
  18. http://deeplearning.net/software/theano/
  19. http://keras.io/
  20. https://www.tensorflow.org/
  21. http://eatcodeplay.com/installing-gpu-enabled-tensorflow-with-python-3-4-in-ec2/
  22. https://www.reddit.com/r/machinelearning/comments/48gfop/tensorflow_speed_questions/
  23. https://www.tensorflow.org/
  24. https://en.wikipedia.org/wiki/feature_engineering
  25. https://en.wikipedia.org/wiki/long_short-term_memory
  26. https://en.wikipedia.org/wiki/dropout_(neural_networks)
  27. http://arxiv.org/abs/1502.01852
  28. http://arxiv.org/abs/1412.6980v8
  29. http://deeplearning.net/software/theano/library/tensor/nnet/nnet.html#tensor.nnet.categorical_crossid178
  30. http://arxiv.org/pdf/1602.02410.pdf
  31. http://research.google.com/pubs/pub41880.html
  32. https://en.wikipedia.org/wiki/autoencoder
  33. http://www.eva.ai/
  34. https://github.com/hips/spearmint
  35. http://arxiv.org/pdf/1409.0473v6.pdf
  36. http://keras.io/regularizers/
  37. https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/
  38. http://stackoverflow.com/a/3429534/78234
  39. http://www.denizyuret.com/ref/church/published_1991_hand.ps.gz
  40. https://en.wikipedia.org/wiki/named-entity_recognition
  41. http://arxiv.org/abs/1103.0398
  42. http://subscribe.machinelearnings.co/?utm_source=machine_learnings&utm_medium=blog&utm_campaign=guest_post&utm_content=tal_weiss
  43. https://machinelearnings.co/tagged/machine-learning?source=post
  44. https://machinelearnings.co/tagged/neural-networks?source=post
  45. https://machinelearnings.co/tagged/deep-learning?source=post
  46. https://machinelearnings.co/tagged/artificial-intelligence?source=post
  47. https://machinelearnings.co/tagged/technology?source=post
  48. https://machinelearnings.co/@majortal?source=footer_card
  49. https://machinelearnings.co/@majortal
  50. https://machinelearnings.co/?source=footer_card
  51. https://machinelearnings.co/?source=footer_card
  52. https://machinelearnings.co/
  53. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  55. https://medium.com/p/9ffef96a24f6/share/twitter
  56. https://medium.com/p/9ffef96a24f6/share/facebook
  57. https://medium.com/p/9ffef96a24f6/share/twitter
  58. https://medium.com/p/9ffef96a24f6/share/facebook
