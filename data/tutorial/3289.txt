   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]athelas
   [7]athelas
   [8]sign in[9]get started
     __________________________________________________________________

a brief history of id98s in image segmentation: from r-id98 to mask r-id98

   [10]go to the profile of dhruv parthasarathy
   [11]dhruv parthasarathy (button) blockedunblock (button)
   followfollowing
   apr 22, 2017

   at [12]athelas, we use convolutional neural networks(id98s) for a lot
   more than just classification! in this post, we   ll see how id98s can be
   used, with great results, in image instance segmentation.
     __________________________________________________________________

   ever since [13]alex krizhevsky, geoff hinton, and ilya sutskever won
   id163 in 2012, convolutional neural networks(id98s) have become the
   gold standard for image classification. in fact, since then, id98s have
   improved to the point where they now outperform humans on the id163
   challenge!
   [1*bgtawfxqwzc5yv1_szdrwq.png]
   id98s now outperform humans on the id163 challenge. the y-axis in the
   above graph is the error rate on id163.

   while these results are impressive, image classification is far simpler
   than the complexity and diversity of true human visual understanding.
   [1*8gvucx9yhnl21kctcyfdrq.png]
   an example of an image used in the classification challenge. note how
   the image is well framed and has just one object.

   in classification, there   s generally an image with a single object as
   the focus and the task is to say what that image is (see above). but
   when we look at the world around us, we carry out far more complex
   tasks.
   [1*ejjj2tvuvzdivstcnzh7fa.png]
   sights in real life are often composed of a multitude of different,
   overlapping objects, backgrounds, and actions.

   we see complicated sights with multiple overlapping objects, and
   different backgrounds and we not only classify these different objects
   but also identify their boundaries, differences, and relations to one
   another!
   [1*ndwfhmrw3rpj5sw_vqtwvw.png]
   in image segmentation, our goal is to classify the different objects in
   the image, and identify their boundaries. source: mask r-id98 paper.

   can id98s help us with such complex tasks? namely, given a more
   complicated image, can we use id98s to identify the different objects in
   the image, and their boundaries? as has been shown by ross girshick and
   his peers over the last few years, the answer is conclusively yes.

goals of this post

   through this post, we   ll cover the intuition behind some of the main
   techniques used in id164 and segmentation and see how
   they   ve evolved from one implementation to the next. in particular,
   we   ll cover r-id98 (regional id98), the original application of id98s to
   this problem, along with its descendants fast r-id98, and faster r-id98.
   finally, we   ll cover mask r-id98, a paper released recently by facebook
   research that extends such id164 techniques to provide pixel
   level segmentation. here are the papers referenced in this post:
    1. r-id98: [14]https://arxiv.org/abs/1311.2524
    2. fast r-id98: [15]https://arxiv.org/abs/1504.08083
    3. faster r-id98: [16]https://arxiv.org/abs/1506.01497
    4. mask r-id98: [17]https://arxiv.org/abs/1703.06870
     __________________________________________________________________

2014: r-id98 - an early application of id98s to id164

   [1*r9elexnk1b1zhnrredw9ow.png]
   id164 algorithms such as r-id98 take in an image and identify
   the locations and classifications of the main objects in the image.
   source: [18]https://arxiv.org/abs/1311.2524.

   inspired by the research of hinton   s lab at the university of toronto,
   a small team at uc berkeley, led by professor jitendra malik, asked
   themselves what today seems like an inevitable question:

     to what extent do [krizhevsky et. al   s results] generalize to object
     detection?

   id164 is the task of finding the different objects in an
   image and classifying them (as seen in the image above). the team,
   comprised of ross girshick (a name we   ll see again), jeff donahue, and
   trevor darrel found that this problem can be solved with krizhevsky   s
   results by testing on the pascal voc challenge, a popular object
   detection challenge akin to id163. they write,

     this paper is the first to show that a id98 can lead to dramatically
     higher id164 performance on pascal voc as compared to
     systems based on simpler hog-like features.

   let   s now take a moment to understand how their architecture, regions
   with id98s (r-id98) works.

   understanding r-id98

   the goal of r-id98 is to take in an image, and correctly identify where
   the main objects (via a bounding box) in the image.
     * inputs: image
     * outputs: bounding boxes + labels for each object in the image.

   but how do we find out where these bounding boxes are? r-id98 does what
   we might intuitively do as well - propose a bunch of boxes in the image
   and see if any of them actually correspond to an object.
   [1*zq03ib84byiofkoho5hnkg.png]
   selective search looks through windows of multiple scales and looks for
   adjacent pixels that share textures, colors, or intensities. image
   source:
   [19]https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf

   r-id98 creates these bounding boxes, or region proposals, using a
   process called selective search which you can read about [20]here. at a
   high level, selective search (shown in the image above) looks at the
   image through windows of different sizes, and for each size tries to
   group together adjacent pixels by texture, color, or intensity to
   identify objects.
   [0*sdj6skdrqyzpo6oh.]
   after creating a set of region proposals, r-id98 passes the image
   through a modified version of alexnet to determine whether or not it is
   a valid region. source: [21]https://arxiv.org/abs/1311.2524.

   once the proposals are created, r-id98 warps the region to a standard
   square size and passes it through to a modified version of alexnet (the
   winning submission to id163 2012 that inspired r-id98), as shown
   above.

   on the final layer of the id98, r-id98 adds a support vector machine
   (id166) that simply classifies whether this is an object, and if so what
   object. this is step 4 in the image above.

   improving the bounding boxes

   now, having found the object in the box, can we tighten the box to fit
   the true dimensions of the object? we can, and this is the final step
   of r-id98. r-id98 runs a simple id75 on the region proposal
   to generate tighter bounding box coordinates to get our final result.
   here are the inputs and outputs of this regression model:
     * inputs: sub-regions of the image corresponding to objects.
     * outputs: new bounding box coordinates for the object in the
       sub-region.

   so, to summarize, r-id98 is just the following steps:
    1. generate a set of proposals for bounding boxes.
    2. run the images in the bounding boxes through a pre-trained alexnet
       and finally an id166 to see what object the image in the box is.
    3. run the box through a id75 model to output tighter
       coordinates for the box once the object has been classified.
     __________________________________________________________________

2015: fast r-id98 - speeding up and simplifying r-id98

   [1*3xnxhbeaz6fgzb-eehxtka.png]
   ross girshick wrote both r-id98 and fast r-id98. he continues to push the
   boundaries of id161 at facebook research.

   r-id98 works really well, but is really quite slow for a few simple
   reasons:
    1. it requires a forward pass of the id98 (alexnet) for every single
       region proposal for every single image (that   s around 2000 forward
       passes per image!).
    2. it has to train three different models separately - the id98 to
       generate image features, the classifier that predicts the class,
       and the regression model to tighten the bounding boxes. this makes
       the pipeline extremely hard to train.

   in 2015, ross girshick, the first author of r-id98, solved both these
   problems, leading to the second algorithm in our short history - fast
   r-id98. let   s now go over its main insights.

   fast r-id98 insight 1: roi (region of interest) pooling

   for the forward pass of the id98, girshick realized that for each image,
   a lot of proposed regions for the image invariably overlapped causing
   us to run the same id98 computation again and again (~2000 times!). his
   insight was simple         why not run the id98 just once per image and then
   find a way to share that computation across the ~2000 proposals?
   [1*4k_bq1ahaste9vlt0wsdxq.png]
   in roipool, a full forward pass of the image is created and the conv
   features for each region of interest are extracted from the resulting
   forward pass. source: stanford   s cs231n slides by fei fei li, andrei
   karpathy, and justin johnson.

   this is exactly what fast r-id98 does using a technique known as roipool
   (region of interest pooling). at its core, roipool shares the forward
   pass of a id98 for an image across its subregions. in the image above,
   notice how the id98 features for each region are obtained by selecting a
   corresponding region from the id98   s feature map. then, the features in
   each region are pooled (usually using max pooling). so all it takes us
   is one pass of the original image as opposed to ~2000!

   fast r-id98 insight 2: combine all models into one network
   [1*e_p1vaebgt4hnyjqmtiz4g.png]
   fast r-id98 combined the id98, classifier, and bounding box regressor
   into one, single network. source:
   [22]https://www.slideshare.net/simplyinsimple/detection-52781995.

   the second insight of fast r-id98 is to jointly train the id98,
   classifier, and bounding box regressor in a single model. where earlier
   we had different models to extract image features (id98), classify
   (id166), and tighten bounding boxes (regressor), fast r-id98 instead used
   a single network to compute all three.

   you can see how this was done in the image above. fast r-id98 replaced
   the id166 classifier with a softmax layer on top of the id98 to output a
   classification. it also added a id75 layer parallel to the
   softmax layer to output bounding box coordinates. in this way, all the
   outputs needed came from one single network! here are the inputs and
   outputs to this overall model:
     * inputs: images with region proposals.
     * outputs: object classifications of each region along with tighter
       bounding boxes.
     __________________________________________________________________

2016: faster r-id98 - speeding up region proposal

   even with all these advancements, there was still one remaining
   bottleneck in the fast r-id98 process         the region proposer. as we saw,
   the very first step to detecting the locations of objects is generating
   a bunch of potential bounding boxes or regions of interest to test. in
   fast r-id98, these proposals were created using selective search, a
   fairly slow process that was found to be the bottleneck of the overall
   process.
   [1*xy9rmw06kzwqlnipk6itqa.png]
   jian sun, a principal researcher at microsoft research, led the team
   behind faster r-id98. source:
   [23]https://blogs.microsoft.com/next/2015/12/10/microsoft-researchers-w
   in-id163-computer-vision-challenge/#sm.00017fqnl1bz6fqf11amuo0d9ttdp

   in the middle 2015, a team at microsoft research composed of shaoqing
   ren, kaiming he, ross girshick, and jian sun, found a way to make the
   region proposal step almost cost free through an architecture they
   (creatively) named faster r-id98.

   the insight of faster r-id98 was that region proposals depended on
   features of the image that were already calculated with the forward
   pass of the id98 (first step of classification). so why not reuse those
   same id98 results for region proposals instead of running a separate
   selective search algorithm?
   [0*_nni03esxm2p6yxo.]
   in faster r-id98, a single id98 is used for region proposals, and
   classifications. source: [24]https://arxiv.org/abs/1506.01497.

   indeed, this is just what the faster r-id98 team achieved. in the image
   above, you can see how a single id98 is used to both carry out region
   proposals and classification. this way, only one id98 needs to be
   trained and we get region proposals almost for free! the authors write:

     our observation is that the convolutional feature maps used by
     region-based detectors, like fast r- id98, can also be used for
     generating region proposals [thus enabling nearly cost-free region
     proposals].

   here are the inputs and outputs of their model:
     * inputs: images (notice how region proposals are not needed).
     * outputs: classifications and bounding box coordinates of objects in
       the images.

   how the regions are generated

   let   s take a moment to see how faster r-id98 generates these region
   proposals from id98 features. faster r-id98 adds a fully convolutional
   network on top of the features of the id98 creating what   s known as the
   region proposal network.
   [0*n6pzeyvw47nlcdqz.]
   the region proposal network slides a window over the features of the
   id98. at each window location, the network outputs a score and a
   bounding box per anchor (hence 4k box coordinates where k is the number
   of anchors). source: [25]https://arxiv.org/abs/1506.01497.

   the region proposal network works by passing a sliding window over the
   id98 feature map and at each window, outputting k potential bounding
   boxes and scores for how good each of those boxes is expected to be.
   what do these k boxes represent?
   [1*pj3otvxjtp9vwfbopsnwiw.png]
   we know that the bounding boxes for people tend to be rectangular and
   vertical. we can use this intuition to guide our region proposal
   networks through creating an anchor of such dimensions. image source:
   [26]http://vlm1.uta.edu/~athitsos/courses/cse6367_spring2011/assignment
   s/assignment1/bbox0062.jpg.

   intuitively, we know that objects in an image should fit certain common
   aspect ratios and sizes. for instance, we know that we want some
   rectangular boxes that resemble the shapes of humans. likewise, we know
   we won   t see many boxes that are very very thin. in such a way, we
   create k such common aspect ratios we call anchor boxes. for each such
   anchor box, we output one bounding box and score per position in the
   image.

   with these anchor boxes in mind, let   s take a look at the inputs and
   outputs to this region proposal network:
     * inputs: id98 feature map.
     * outputs: a bounding box per anchor. a score representing how likely
       the image in that bounding box will be an object.

   we then pass each such bounding box that is likely to be an object into
   fast r-id98 to generate a classification and tightened bounding boxes.
     __________________________________________________________________

2017: mask r-id98 - extending faster r-id98 for pixel level segmentation

   [1*e_5qbtrotlzclyaxsekbmq.png]
   the goal of image instance segmentation is to identify, at a pixel
   level, what the different objets in a scene are. source:
   [27]https://arxiv.org/abs/1703.06870.

   so far, we   ve seen how we   ve been able to use id98 features in many
   interesting ways to effectively locate different objects in an image
   with bounding boxes.

   can we extend such techniques to go one step further and locate exact
   pixels of each object instead of just bounding boxes? this problem,
   known as image segmentation, is what kaiming he and a team of
   researchers, including girshick, explored at facebook ai using an
   architecture known as mask r-id98.
   [1*cyw3edkx75stl1ereatdfw.png]
   kaiming he, a researcher at facebook ai, is lead author of mask r-id98
   and also a coauthor of faster r-id98.

   much like fast r-id98, and faster r-id98, mask r-id98   s underlying
   intuition is straight forward. given that faster r-id98 works so well
   for id164, could we extend it to also carry out pixel level
   segmentation?
   [1*birpf-ogjxarqf5lxi17jw.png]
   in mask r-id98, a fully convolutional network (fcn) is added on top of
   the id98 features of faster r-id98 to generate a mask (segmentation
   output). notice how this is in parallel to the classification and
   bounding box regression network of faster r-id98. source:
   [28]https://arxiv.org/abs/1703.06870.

   mask r-id98 does this by adding a branch to faster r-id98 that outputs a
   binary mask that says whether or not a given pixel is part of an
   object. the branch (in white in the above image), as before, is just a
   fully convolutional network on top of a id98 based feature map. here are
   its inputs and outputs:
     * inputs: id98 feature map.
     * outputs: matrix with 1s on all locations where the pixel belongs to
       the object and 0s elsewhere (this is known as a [29]binary mask).

   but the mask r-id98 authors had to make one small adjustment to make
   this pipeline work as expected.

   roialign - realigning roipool to be more accurate
   [0*ktazfpueryqwh4rx.]
   instead of roipool, the image gets passed through roialign so that the
   regions of the feature map selected by roipool correspond more
   precisely to the regions of the original image. this is needed because
   pixel level segmentation requires more fine-grained alignment than
   bounding boxes. source: [30]https://arxiv.org/abs/1703.06870.

   when run without modifications on the original faster r-id98
   architecture, the mask r-id98 authors realized that the regions of the
   feature map selected by roipool were slightly misaligned from the
   regions of the original image. since image segmentation requires pixel
   level specificity, unlike bounding boxes, this naturally led to
   inaccuracies.

   the authors were able to solve this problem by cleverly adjusting
   roipool to be more precisely aligned using a method known as roialign.
   [1*vdgql5vdblwu3johrmzwfq.jpeg]
   how do we accurately map a region of interest from the original image
   onto the feature map?

   imagine we have an image of size 128x128 and a feature map of size
   25x25. let   s imagine we want features the region corresponding to the
   top-left 15x15 pixels in the original image (see above). how might we
   select these pixels from the feature map?

   we know each pixel in the original image corresponds to ~ 25/128 pixels
   in the feature map. to select 15 pixels from the original image, we
   just select 15 * 25/128 ~= 2.93 pixels.

   in roipool, we would round this down and select 2 pixels causing a
   slight misalignment. however, in roialign, we avoid such rounding.
   instead, we use [31]bilinear interpolation to get a precise idea of
   what would be at pixel 2.93. this, at a high level, is what allows us
   to avoid the misalignments caused by roipool.

   once these masks are generated, mask r-id98 combines them with the
   classifications and bounding boxes from faster r-id98 to generate such
   wonderfully precise segmentations:
   [1*6cclgikh8zhzjmcftfnoeq.png]
   mask r-id98 is able to segment as well as classify the objects in an
   image. source: [32]https://arxiv.org/abs/1703.06870.
     __________________________________________________________________

code

   if you   re interested in trying out these algorithms yourselves, here
   are relevant repositories:

   faster r-id98
     * caffe: [33]https://github.com/rbgirshick/py-faster-rid98
     * pytorch: [34]https://github.com/longcw/faster_rid98_pytorch
     * matlab: [35]https://github.com/shaoqingren/faster_rid98

   mask r-id98
     * pytorch: [36]https://github.com/felixgwu/mask_rid98_pytorch
     * tensorflow: [37]https://github.com/charlesshang/fastmaskrid98

looking forward

   in just 3 years, we   ve seen how the research community has progressed
   from krizhevsky et. al   s original result to r-id98, and finally all the
   way to such powerful results as mask r-id98. seen in isolation, results
   like mask r-id98 seem like incredible leaps of genius that would be
   unapproachable. yet, through this post, i hope you   ve seen how such
   advancements are really the sum of intuitive, incremental improvements
   through years of hard work and collaboration. each of the ideas
   proposed by r-id98, fast r-id98, faster r-id98, and finally mask r-id98
   were not necessarily quantum leaps, yet their sum products have led to
   really remarkable results that bring us closer to a human level
   understanding of sight.

   what particularly excites me, is that the time between r-id98 and mask
   r-id98 was just three years! with continued funding, focus, and support,
   how much further can id161 improve over the next three years?
     __________________________________________________________________

   if you see any errors or issues in this post, please contact me at
   dhruv@getathelas.com and i   ll immediately correct them!

   if you   re interested in applying such techniques, come join us at
   [38]athelas where we apply id161 to blood diagnostics daily:
   [39]athelas
   in-home blood diagnosticathelas.com

   other posts we   ve written:
   [40]classifying white blood cells with deep learning (code and data
   included!)
   you can follow all the code and recreate the results of this post
   here.blog.athelas.com
   [41]baidu deep voice explained: part 1         the id136 pipeline
   this post is the first in what i hope to be a series covering recently
   published ml/ai papers that i think are   blog.athelas.com
   [42]write an ai to win at pong from scratch with id23
   there   s a huge difference between reading about id23
   and actually implementing it.medium.com

   thanks to bharath ramsundar, [43]pranav ramkrishnan, tanay tandon, and
   [44]oliver cameron for help with this post!

   thanks to [45]pranav ramkrishnan and [46]oliver cameron.
     * [47]machine learning
     * [48]artificial intelligence
     * [49]ai
     * [50]deep learning
     * [51]id161

   (button)
   (button)
   (button) 6.2k claps
   (button) (button) (button) 32 (button) (button)

     (button) blockedunblock (button) followfollowing
   [52]go to the profile of dhruv parthasarathy

[53]dhruv parthasarathy

   [54]@dhruvp. vp eng [55]@athelas. mit math and cs undergrad    13. mit cs
   masters    14. previously: director of ai programs @ udacity.

     (button) follow
   [56]athelas

[57]athelas

   blood diagnostics through deep learning [58]http://athelas.com

     * (button)
       (button) 6.2k
     * (button)
     *
     *

   [59]athelas
   never miss a story from athelas, when you sign up for medium. [60]learn
   more
   never miss a story from athelas
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.athelas.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/34ea83205de4
   4. https://medium.com/
   5. https://medium.com/
   6. https://blog.athelas.com/?source=avatar-lo_xfpsytvgz1za-67fc953f9b17
   7. https://blog.athelas.com/?source=logo-lo_xfpsytvgz1za---67fc953f9b17
   8. https://medium.com/m/signin?redirect=https://blog.athelas.com/a-brief-history-of-id98s-in-image-segmentation-from-r-id98-to-mask-r-id98-34ea83205de4&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://blog.athelas.com/a-brief-history-of-id98s-in-image-segmentation-from-r-id98-to-mask-r-id98-34ea83205de4&source=--------------------------nav_reg&operation=register
  10. https://blog.athelas.com/@dhruvp?source=post_header_lockup
  11. https://blog.athelas.com/@dhruvp
  12. http://athelas.com/
  13. https://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks
  14. https://arxiv.org/abs/1311.2524
  15. https://arxiv.org/abs/1504.08083
  16. https://arxiv.org/abs/1506.01497
  17. https://arxiv.org/abs/1703.06870
  18. https://arxiv.org/abs/1311.2524
  19. https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf
  20. http://www.cs.cornell.edu/courses/cs7670/2014sp/slides/visionseminar14.pdf
  21. https://arxiv.org/abs/1311.2524
  22. https://www.slideshare.net/simplyinsimple/detection-52781995
  23. https://blogs.microsoft.com/next/2015/12/10/microsoft-researchers-win-id163-computer-vision-challenge/#sm.00017fqnl1bz6fqf11amuo0d9ttdp
  24. https://arxiv.org/abs/1506.01497
  25. https://arxiv.org/abs/1506.01497
  26. http://vlm1.uta.edu/~athitsos/courses/cse6367_spring2011/assignments/assignment1/bbox0062.jpg
  27. https://arxiv.org/abs/1703.06870
  28. https://arxiv.org/abs/1703.06870
  29. https://en.wikipedia.org/wiki/mask_(computing)
  30. https://arxiv.org/abs/1703.06870
  31. https://en.wikipedia.org/wiki/bilinear_interpolation
  32. https://arxiv.org/abs/1703.06870
  33. https://github.com/rbgirshick/py-faster-rid98
  34. https://github.com/longcw/faster_rid98_pytorch
  35. https://github.com/shaoqingren/faster_rid98
  36. https://github.com/felixgwu/mask_rid98_pytorch
  37. https://github.com/charlesshang/fastmaskrid98
  38. http://athelas.com/
  39. https://athelas.com/
  40. https://blog.athelas.com/classifying-white-blood-cells-with-convolutional-neural-networks-2ca6da239331
  41. https://blog.athelas.com/paper-1-baidus-deep-voice-675a323705df
  42. https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0
  43. https://medium.com/@pranavramkr
  44. https://medium.com/@olivercameron
  45. https://medium.com/@pranavramkr?source=post_page
  46. https://medium.com/@olivercameron?source=post_page
  47. https://blog.athelas.com/tagged/machine-learning?source=post
  48. https://blog.athelas.com/tagged/artificial-intelligence?source=post
  49. https://blog.athelas.com/tagged/ai?source=post
  50. https://blog.athelas.com/tagged/deep-learning?source=post
  51. https://blog.athelas.com/tagged/computer-vision?source=post
  52. https://blog.athelas.com/@dhruvp?source=footer_card
  53. https://blog.athelas.com/@dhruvp
  54. http://twitter.com/dhruvp
  55. http://twitter.com/athelas
  56. https://blog.athelas.com/?source=footer_card
  57. https://blog.athelas.com/?source=footer_card
  58. http://athelas.com/
  59. https://blog.athelas.com/
  60. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  62. https://athelas.com/
  63. https://blog.athelas.com/classifying-white-blood-cells-with-convolutional-neural-networks-2ca6da239331
  64. https://blog.athelas.com/paper-1-baidus-deep-voice-675a323705df
  65. https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0
  66. https://medium.com/p/34ea83205de4/share/twitter
  67. https://medium.com/p/34ea83205de4/share/facebook
  68. https://medium.com/p/34ea83205de4/share/twitter
  69. https://medium.com/p/34ea83205de4/share/facebook
