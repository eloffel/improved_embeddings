   [1]

linkedin

     * [2]sign in
     * [3]join now

   id84 technique -tsne vs. pca

                id84 technique -tsne vs. pca

   published on may 3, 2017may 3, 2017     65 likes     12 comments

   [4]zenodia charpy

[5]zenodia charpy[6]follow

helping organizations with machine learning in the cloud

     * (button) like65
     * (button) comment12
     * [ ] share
          + (button) linkedin
          + (button) facebook
          + (button) twitter
       11

   sure, almost everyone who has heard of or use pca quite frequently,
   however, perhaps not as many people would have heard of tsne...

   i found it very intrigging when i first stepping into deep learning
   domain...

   btw, did you know that tsne is developed by the god father of neural
   network : geoffrey everest hinton ?

   oki , so what is tsne=t-distributed stochastic neighbor embedding, you
   can read about an in-depth explaination in this paper from the god
   father himself : http://www.cs.toronto.edu/~hinton/absps/tsne.pdf

   oki, so first , we need some data, i took the notmnist dataset ( since
   i am a bit tired of the mnist dataset as well as the iris dataset ) ,
   and i have already downloaded the notmnist data from google's public
   dataset, which you can find via udacity's google deep learning course (
   it's free, and very cool, so if you haven't check it out, i suggest
   that you do that :p )

   the notmnist dataset is images of english letter from a to j (=10
   classes) , randmly selected sample letters from the downloaded and
   pre-processed dataset are displayed below

   [:0]

   i've done quite a bit of pre-processing and then get the original
   dataset into nicely ''picked' files , so we start from there ( since
   image data pre-processing is not really the focus of this post anyways,
   so we skip forward the boring stuff ...)

   in [1]:
from __future__ import print_function
import numpy as np
from random import shuffle
import tensorflow as tf
from six.moves import cpickle as pickle
from six.moves import range
import matplotlib.pyplot as plt
import math
import os
import sys
from scipy import ndimage
import matplotlib.pyplot as plt
#import matplotlib
from sklearn.manifold import tsne
from sklearn.decomposition import pca
import os
import sys

%matplotlib inline

   in [2]:
pickle_file = 'notmnist.pickle'
#load pickled files
with open(pickle_file, 'rb') as f:
  save = pickle.load(f)
  train_dataset = save['train_dataset']
  train_labels = save['train_labels']
  valid_dataset = save['valid_dataset'] # we actually dont need this, i load it
anyways in case i change my mind
  valid_labels = save['valid_labels'] # we actually dont need this either

  del save  # hint to help free up memory
  #print('training set', train_dataset.shape, train_labels.shape)
  #print('validation set', valid_dataset.shape, valid_labels.shape)
#training set (200000, 28, 28) (200000,)
#validation set (10000, 28, 28) (10000,)

   in [3]:
# reshapping the data so it is a 2d array =(num_of_samples,features=784=28*28)
xtrain=train_dataset.reshape((-1, 28 * 28)).astype(np.float32)
ytrain=train_labels # we are not one hot encoding the lables yet
xtest=valid_dataset.reshape((-1, 28 * 28)).astype(np.float32)
ytest=valid_labels # we are not gonna ohe the labels yet
#print(xtrain.shape)
#print(xtrain.ndim)
#print(set(ytrain))
#(200000, 784)
#2
#set([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

   in [4]:
indices =np.arange(xtrain.shape[0])
shuffle(indices)
n_train_samples = 5000 # see the comparison of sample sizes 1000,2000,3000,4000
to 5000
x = xtrain[indices[:n_train_samples]] # we only take 5000 samples
y = ytrain[indices[:n_train_samples]] # match the labels with 5000 samples

x_pca=pca(n_components=50).fit_transform(x)

tsne =  tsne(n_components=2, perplexity=40, verbose=2)
z = tsne.fit_transform(x)
print(type(z),z.shape)


   out [4]:
[id167] computing pairwise distances...
[id167] computing 121 nearest neighbors...
[id167] computed conditional probabilities for sample 1000 / 5000
[id167] computed conditional probabilities for sample 2000 / 5000
[id167] computed conditional probabilities for sample 3000 / 5000
[id167] computed conditional probabilities for sample 4000 / 5000
[id167] computed conditional probabilities for sample 5000 / 5000
[id167] mean sigma: 0.000000
[id167] iteration 25: error = 1.6184427, gradient norm = 0.0107759
[id167] iteration 50: error = 1.5508032, gradient norm = 0.0067521
[id167] iteration 75: error = 1.3033309, gradient norm = 0.0024789
[id167] iteration 100: error = 1.2544495, gradient norm = 0.0023307
[id167] kl divergence after 100 iterations with early exaggeration: 1.254449
[id167] iteration 125: error = 1.1744450, gradient norm = 0.0022337
[id167] iteration 150: error = 1.1492559, gradient norm = 0.0015884
[id167] iteration 175: error = 1.1428556, gradient norm = 0.0016543
[id167] iteration 200: error = 1.1397188, gradient norm = 0.0021076
[id167] iteration 225: error = 1.1402746, gradient norm = 0.0021348
[id167] iteration 250: error = 1.1393719, gradient norm = 0.0023150
[id167] iteration 275: error = 1.1399436, gradient norm = 0.0025634
[id167] iteration 300: error = 1.1407571, gradient norm = 0.0026940
[id167] iteration 300: did not make any progress during the last 30 episodes. fi
nished.
[id167] error after 300 iterations: 1.254449
<type 'numpy.ndarray'> (5000, 2)


   in [5]: plot pca vs tsne in 2d
import matplotlib
label=['a','b','c','d','e','f','g','h','i','j']
colors = ['black','gray','blue','red','salmon','orange','yellow','green','purple
','hotpink']

fig = plt.figure(figsize=(15,15))
plt.scatter(z[:,0], z[:,1], s=100, c=y, alpha=0.5,cmap=matplotlib.colors.listedc
olormap(colors))

cb = plt.colorbar()
loc = np.arange(0,max(y),max(y)/float(len(colors)))
cb.set_ticks(loc)
cb.set_ticklabels(label)
plt.title('tsne on 5000 notmnist data')
plt.show()

fig = plt.figure(figsize=(15,15))
plt.scatter(x_pca[:,0],x_pca[:,1],c=y, cmap=matplotlib.colors.listedcolormap(col
ors))
plt.title('pca on 5000 notmnist data')
plt.show


   out[5]:

   [:0]

<function matplotlib.pyplot.show>

   #note : one can see that clearly the seperation of the 10 classes were
   much better looking with tsne than pca in 2 dimensions space

   in [6]: # now we are to fit pca and tsne transformed data into vanilla
   id158 and see how they perform ,seperatedly ...
# hold out last 500 as testing set
indices =np.arange(x_pca.shape[0])
y=(np.arange(10) == y[:,none]).astype(np.float32)
x_pca.astype(np.float32) # ensure it is float32
tot=x_pca.shape[0]
sz = int(tot*0.8)
print(tot,sz)
x_pca_train= x_pca[indices[:sz]]
x_pca_test= x_pca[indices[sz:tot]]
y_pca_train = y[indices[:sz]]
y_pca_test=y[indices[sz:tot]]
print(x_pca_train.shape)
print(y_pca_train.shape)
print(x_pca.shape)


   in [7] : note that the comparison of the model architecture as well as
   hyper-parameters were all fixed for both pca and tsne, in order to
   compare them properly.
#run the model with pca
num_steps = 100

# network parameters
n_hidden_1 = 1024 #  1st layer number of features
#n_input=2 # for tsne
n_input=2 #for pca
#n_input = 784     #  data input (img shape: 28*28)
n_classes = 10    #  total classes (0-9 digits)

# store layers weight & bias using python dictionary-like syntax
weights = {
    'h1': tf.variable(tf.truncated_normal([n_input, n_hidden_1])),
    'out': tf.variable(tf.truncated_normal([n_hidden_1, n_classes]))
} # you can access hidden layer1=h1's matrix via weights['h1'] as well as output
 from activation function via weight['out']

biases = {
    'b1': tf.variable(tf.zeros([n_hidden_1])),
    'out': tf.variable(tf.zeros([n_classes]))
}

# tf graph input

x = tf.placeholder(tf.float32, [none, n_input])
y = tf.placeholder(tf.float32, [none, n_classes])

#tf_valid_dataset = tf.constant(valid_dataset)
tf_test_dataset = tf.constant(x_pca_test)

def multilayer_id88(x, weights, biases):
    # hidden layer with relu activation
    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])
    layer_1 = tf.nn.relu(layer_1)

    # output layer with linear activation
    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']
    return out_layer

# construct model
pred = multilayer_id88(x, weights, biases)


# define loss and optimizer
loss = tf.reduce_mean(tf.nn.softmax_cross_id178_with_logits(labels=y, logits=p
red))
optimizer = tf.train.gradientdescentoptimizer(0.5).minimize(loss)

# initializing the variables
init = tf.global_variables_initializer()

# accuracy
def accuracy(predictions, labels):
  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
         / predictions.shape[0])

# launch the graph
with tf.session() as sess:
    sess.run(init)
    print("initialized!")
    for step in range(num_steps):
        # the key of the dictionary is the placeholder node of the graph to be f
ed,
        # and the value is the numpy array to feed to it.
        feed_dict = {x : x_pca_train, y : y_pca_train}
        _, l, predictions = sess.run([optimizer, loss, pred], feed_dict=feed_dic
t)
        if (step % 10 == 0):
            print(" loss at step %d: %f" % (step, l))
            print(" accuracy: %.1f%%" % accuracy(predictions, y_pca_train))
            feed_dict = {x : x_pca_test, y : y_pca_test}
            _, l, testpred = sess.run([optimizer, loss, pred], feed_dict=feed_di
ct)
            print("test accuracy: %.1f%%" % accuracy(testpred, y_pca_test))


   out [7]:
 initialized!
 loss at step 0: 175.529495
 accuracy: 12.2%
test accuracy: 17.0%
 loss at step 10: 328.700989
 accuracy: 14.3%
test accuracy: 16.8%
 loss at step 20: 131.815903
 accuracy: 15.8%
test accuracy: 13.1%
 loss at step 30: 16.042177
 accuracy: 17.1%
test accuracy: 16.3%
 loss at step 40: 2.914466
 accuracy: 19.5%
test accuracy: 16.6%
 loss at step 50: 1.931446
 accuracy: 27.9%
test accuracy: 24.0%
 loss at step 60: 1.929332
 accuracy: 27.6%
test accuracy: 24.9%
 loss at step 70: 1.913266
 accuracy: 27.4%
test accuracy: 24.2%
 loss at step 80: 1.917354
 accuracy: 27.6%
test accuracy: 25.1%
 loss at step 90: 1.909120
 accuracy: 27.1%
test accuracy: 23.3%


   now that i have only taken first 2 pca components in order to match
   tsne, it's perforance drop to about round 23% accuracy on the testing
   set for the last epoch

   in [8]:#let's do tsne then
# hold out last 0.2% of the sample sizes as testing set
indicesz =np.arange(z.shape[0])
z.astype(np.float32) # ensure it is float32
totz=z.shape[0]
szz = int(tot*0.8)
#print(totz,szz)
x_tsne_train= z[indicesz[:szz]]
x_tsne_test= z[indicesz[szz:totz]]
y_tsne_train = y[indicesz[:szz]]
y_tsne_test=y[indicesz[szz:totz]]

#print(x_tsne_train.shape)
#print(y_tsne_train.shape)


   in [9]: now run the exact the same model with exactly the same setup
   with tsne
#now run the model with tsne
num_steps = 100

# network parameters
n_hidden_1 = 1024 #  1st layer number of features
n_input=2 # for tsne
#n_input=50 #for pca
#n_input = 784     #  data input (img shape: 28*28)
n_classes = 10    #  total classes (0-9 digits)

# store layers weight & bias using python dictionary-like syntax
weights = {
    'h1': tf.variable(tf.truncated_normal([n_input, n_hidden_1])),
    'out': tf.variable(tf.truncated_normal([n_hidden_1, n_classes]))
} # you can access hidden layer1=h1's matrix via weights['h1'] as well as output
 from activation function via weight['out']

biases = {
    'b1': tf.variable(tf.zeros([n_hidden_1])),
    'out': tf.variable(tf.zeros([n_classes]))
}

# tf graph input

x = tf.placeholder(tf.float32, [none, n_input])
y = tf.placeholder(tf.float32, [none, n_classes])

#tf_valid_dataset = tf.constant(valid_dataset)
tf_test_dataset = tf.constant(x_pca_test)

def multilayer_id88(x, weights, biases):
    # hidden layer with relu activation
    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])
    layer_1 = tf.nn.relu(layer_1)

    # output layer with linear activation
    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']
    return out_layer

# construct model
pred = multilayer_id88(x, weights, biases)
#valid_pred = multilayer_id88(tf_valid_dataset, weights, biases)
#test_pred = multilayer_id88(tf_test_dataset, weights, biases)

# define loss and optimizer
loss = tf.reduce_mean(tf.nn.softmax_cross_id178_with_logits(labels=y, logits=p
red))
optimizer = tf.train.gradientdescentoptimizer(0.5).minimize(loss)

# initializing the variables
init = tf.global_variables_initializer()

# accuracy
def accuracy(predictions, labels):
  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
      / predictions.shape[0])

# launch the graph
with tf.session() as sess:
    sess.run(init)
    print("initialized!")
    for step in range(num_steps):
        # the key of the dictionary is the placeholder node of the graph to be f
ed,
        # and the value is the numpy array to feed to it.
        feed_dict = {x : x_tsne_train, y : y_tsne_train}
        _, l, predictions = sess.run([optimizer, loss, pred], feed_dict=feed_dic
t)
        if (step % 10 == 0):
            print(" loss at step %d: %f" % (step, l))
            print(" accuracy: %.1f%%" % accuracy(predictions, y_tsne_train))
            feed_dict = {x : x_tsne_test, y : y_tsne_test}
            _, l, testpred = sess.run([optimizer, loss, pred], feed_dict=feed_di
ct)
            print("test accuracy: %.1f%%" % accuracy(testpred, y_tsne_test))



   out [9]:
 initialized!
 loss at step 0: 266.517761
 accuracy: 14.6%
test accuracy: 30.7%
 loss at step 10: 378.249481
 accuracy: 22.9%
test accuracy: 24.1%
 loss at step 20: 225.392090
 accuracy: 18.0%
test accuracy: 20.2%
 loss at step 30: 98.855362
 accuracy: 26.9%
test accuracy: 33.6%
 loss at step 40: 7.851514
 accuracy: 40.4%
test accuracy: 24.5%
 loss at step 50: 1.506628
 accuracy: 50.0%
test accuracy: 52.1%
 loss at step 60: 1.440037
 accuracy: 54.4%
test accuracy: 52.5%
 loss at step 70: 1.399115
 accuracy: 54.4%
test accuracy: 53.0%
 loss at step 80: 1.373763
 accuracy: 55.1%
test accuracy: 53.6%
 loss at step 90: 1.357892
 accuracy: 51.5%
test accuracy: 48.2%


   that was quite an improvement, but what if we run exactly the same
   thing with differnt sample sizes ? say 1000, 2000, 3000, 4000, 5000 (
   after 5000 my computer crashed, so can't do further :p )

   [:0]

   interesting... so before the sample size exceeding 3000, pca is doing a
   lot better than tsne ( once again, with all the model parameters fixed,
   and take only top 2 component for pca in order to match tsne , we only
   varying the sample sizes in this table)

   it is interesting to see that , although tsne is an interesting
   algorithm , however, we should use it with care, not just throw away
   pca ( or other id84 technique) but rather, use it
   when conditions suit tsne than pca, otherwise i will always make an
   effort to try them both and do more experiments before drawing any
   definate conclusions.

   oki that is for now and hope you guys enjoy this post ^__^b

   end note : i find this article that explain tsne concept from math
   perspective exceptionally well, so if you are interested in the actual
   math behind tsne , please refer to this article below :
   [7]https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-
   python/

   [8]zenodia charpy

[9]zenodia charpy

helping organizations with machine learning in the cloud

   [10]follow

   12 comments
   article-comment__guest-image
   [11]sign in to leave your comment
   show more comments.
     __________________________________________________________________

more from zenodia charpy

   [12]44 articles
   deploy your deep learning model to kubernetes (aks) with azure machine
   learning services python sdk

[13]deploy your deep learning model to   

   march 5, 2019

   distributed deep learning with kubernetes with keras +tensorflow on
   horovod step-by-step guide

[14]distributed deep learning with kubernetes   

   february 11, 2019

   an experiment of combining images features with tabular data

[15]an experiment of combining images features   

   january 31, 2019

     *    2019
     * [16]about
     * [17]user agreement
     * [18]privacy policy
     * [19]cookie policy
     * [20]copyright policy
     * [21]brand policy
     * [22]manage subscription
     * [23]community guidelines
     * [ ]
          + (button) bahasa indonesia
          + (button) bahasa malaysia
          + (button)   e  tina
          + (button) dansk
          + (button) deutsch
          + (button) english
          + (button) espa  ol
          + (button)             
          + (button) fran  ais
          + (button)          
          + (button) italiano
          + (button)             
          + (button) nederlands
          + (button)          
          + (button) norsk
          + (button) polski
          + (button) portugu  s
          + (button) rom  n  
          + (button)               
          + (button) svenska
          + (button) tagalog
          + (button)                      
          + (button) t  rk  e
          + (button)               
       languagelanguage

references

   1. https://www.linkedin.com/?trk=header_logo
   2. https://www.linkedin.com/uas/login?trk=header_signin
   3. https://www.linkedin.com/start/join?trk=header_join
   4. https://se.linkedin.com/in/zenodia-charpy-1b23174?trk=author_mini-profile_image
   5. https://se.linkedin.com/in/zenodia-charpy-1b23174?trk=author_mini-profile_title
   6. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/dimensionality-reduction-technique-tsne-vs-pca-zenodia-charpy&trk=author-info__follow-button
   7. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/
   8. https://se.linkedin.com/in/zenodia-charpy-1b23174?trk=author_mini-profile_image
   9. https://se.linkedin.com/in/zenodia-charpy-1b23174?trk=author_mini-profile_title
  10. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/dimensionality-reduction-technique-tsne-vs-pca-zenodia-charpy&trk=author-info__follow-button-bottom
  11. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/dimensionality-reduction-technique-tsne-vs-pca-zenodia-charpy&trk=article-reader_leave-comment
  12. https://www.linkedin.com/today/author/zenodia-charpy-1b23174
  13. https://www.linkedin.com/pulse/deploy-your-deep-learning-model-kubernetes-aks-azure-machine-charpy?trk=related_artice_deploy your deep learning model to kubernetes (aks) with azure machine learning services python sdk_article-card_title
  14. https://www.linkedin.com/pulse/distributed-deep-learning-kubenetes-keras-tensorflow-horovod-charpy?trk=related_artice_distributed deep learning with kubernetes with keras +tensorflow on horovod step-by-step guide_article-card_title
  15. https://www.linkedin.com/pulse/experiment-combining-images-features-tabular-data-zenodia-charpy?trk=related_artice_an experiment of combining images features with tabular data_article-card_title
  16. https://press.linkedin.com/about-linkedin?trk=article_reader_footer_footer-about
  17. https://www.linkedin.com/legal/user-agreement?trk=article_reader_footer_footer-user-agreement
  18. https://www.linkedin.com/legal/privacy-policy?trk=article_reader_footer_footer-privacy-policy
  19. https://www.linkedin.com/legal/cookie-policy?trk=article_reader_footer_footer-cookie-policy
  20. https://www.linkedin.com/legal/copyright-policy?trk=article_reader_footer_footer-copyright-policy
  21. https://brand.linkedin.com/policies?trk=article_reader_footer_footer-brand-policy
  22. https://www.linkedin.com/psettings/guest-controls?trk=article_reader_footer_footer-manage-sub
  23. https://www.linkedin.com/help/linkedin/answer/34593?lang=en&trk=article_reader_footer_footer-community-guide
