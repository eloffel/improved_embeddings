   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    comprehensive guide on id167 algorithm with
   implementation in r & python comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]machine learning [94]comprehensive guide on id167
   algorithm with implementation in r & python

   [95]machine learning[96]python[97]r

comprehensive guide on id167 algorithm with implementation in r & python

   [98]saurabh.jaju2, january 22, 2017

introduction

   imagine you get a dataset with hundreds of features (variables) and
   have little understanding about the domain the data belongs to. you are
   expected to identify hidden patterns in the data, explore and analyze
   the dataset. and not just that, you have to find out if there is a
   pattern in the data     is it signal or is it just noise?

   does that thought make you uncomfortable? it made my hands sweat when i
   came across this situation for the first time. do you wonder how to
   explore a multidimensional dataset? it is one of the frequently asked
   question by many data scientists. in this article, i will take you
   through a very powerful way to exactly do this.

what about pca?

   by now, some of you would be screaming    i   ll use pca for dimensionality
   reduction and visualization   . well, you are right! pca is definitely a
   good choice for id84 and visualization for datasets
   with a large number of features. but, what if you could use something
   more advanced than pca? (if you don   t know pca, i would strongly
   recommend to [99]read this article first)

   what if you could easily search for a pattern in non-linear style? in
   this article, i will tell you about a new algorithm called id167
   (2008), which is much more effective than pca (1933). i will take you
   through the basics of id167 algorithm first and then will walk you
   through why id167 is a good fit for id84
   algorithms.

   you will also, get hands-on knowledge for using id167 in both r and
   python.

   read on!

table of content

    1. what is id167?
    2. what is id84?
    3. how does id167 fit in the id84 algorithm space
    4. algorithmic details of id167
          + algorithm
          + time and space complexity
    5. what does id167 actually do?
    6. use cases
    7.  id167 compared to other id84 algorithm
    8. example implementations
          + in r
               o hyper parameter tuning
               o code
               o implementation time
               o interpreting results
          + in python
               o hyper parameter tuning
               o code
               o implementation time
    9. where and when to use
          + data scientist
          + machine learning competition enthusiast
          + student
   10. common fallacies


1. what is id167?

   (id167) t-distributed stochastic neighbor embedding is a non-linear
   id84 algorithm used for exploring
   high-dimensional data. it maps multi-dimensional data to two or more
   dimensions suitable for human observation. with help of the id167
   algorithms, you may have to plot fewer exploratory data analysis plots
   next time you work with high dimensional data.

     [20131959678_bf1a8e3fcc_b-289x300.jpg]


2. what is id84?

   in order to understand how id167 works, let   s first understand what is
   id84?

   well, in simple terms, id84 is the technique of
   representing multi-dimensional data (data with multiple features having
   a correlation with each other) in 2 or 3 dimensions.

   some of you might question why do we need id84 when
   we can plot the data using scatter plots, histograms & boxplots and
   make sense of the pattern in data using descriptive statistics.

   well, even if you can understand the patterns in data and present it on
   simple charts, it is still difficult for anyone without statistics
   background to make sense of it. also, if you have hundreds of features,
   you have to study thousands of charts before you can make sense of this
   data. (read more about [100]id84 here)

   with the help of id84 algorithm, you will be able
   to present the data explicitly.


3. how does id167 fit in the id84 algorithm space?

   now that you have an understanding of what is id84,
   let   s look at how we can use id167 algorithm for reducing dimensions.

   following are a few id84 algorithms that you can
   check out:
    1. pca (linear)
    2. id167 (non-parametric/ nonlinear)
    3. sammon mapping (nonlinear)
    4. isomap (nonlinear)
    5. lle (nonlinear)
    6. cca (nonlinear)
    7. sne (nonlinear)
    8. mvu (nonlinear)
    9. laplacian eigenmaps (nonlinear)

   the good news is that you need to study only two of the algorithms
   mentioned above to effectively visualize data in lower dimensions     pca
   and id167.


limitations of pca

   pca is a linear algorithm. it will not be able to interpret complex
   polynomial relationship between features. on the other hand, id167 is
   based on id203 distributions with random walk on neighborhood
   graphs to find the structure within the data.

   a major problem with, linear id84 algorithms is
   that they concentrate on placing dissimilar data points far apart in a
   lower dimension representation. but in order to represent high
   dimension data on low dimension, non-linear manifold, it is important
   that similar datapoints must be represented close together, which is
   not what linear id84 algorithms do.

   now, you have a brief understanding of what pca endeavors to do.

   local approaches seek to map nearby points on the manifold to nearby
   points in the low-dimensional representation. global approaches on the
   other hand attempt to preserve geometry at all scales, i.e mapping
   nearby points to nearby points and far away points to far away points

   it is important to know that most of the nonlinear techniques other
   than id167 are not capable of retaining both the local and global
   structure of the data at the same time.


4. algorithmic details of id167 (optional read)

   this section is for the people interested in understanding the
   algorithm in depth. you can safely skip this section if you do not want
   to go through the math in detail.

   let   s understand why you should know about id167 and the algorithmic
   details of id167.  id167 is an improvement on the stochastic neighbor
   embedding (sne) algorithm.


4.1 algorithm

step 1

   stochastic neighbor embedding (sne) starts by converting the
   high-dimensional euclidean distances between data points into
   conditional probabilities that represent similarities. the similarity
   of datapoint [xi.png]  to datapoint [xj.png]  is the conditional
   id203,  ,  [xi.png] would pick  [xj.png] as its neighbor if
   neighbors were picked in proportion to their id203 density under
   a gaussian centered at [xi.png] .

   for nearby datapoints,  is relatively high, whereas for widely
   separated datapoints,  will be almost infinitesimal (for reasonable
   values of the variance of the gaussian, ). mathematically, the
   id155  is given by

   [tsn1.png]

   where  is the variance of the gaussian that is centered on datapoint
   [xi.png]

   if you are not interested in the math, think about it in this way, the
   algorithm starts by converting the shortest distance (a straight line)
   between the points into id203 of similarity of points. where,
   the similarity between points is: the id155 that
   [xi.png]  would pick  [xj.png]  as its neighbor if neighbors were
   picked in proportion to their id203 density under a gaussian
   (normal distribution) centered at [xi.png] .


step 2

   for the low-dimensional counterparts  [yi.png] and [yj.png]  of the
   high-dimensional datapoints [xi.png]  and  [xj.png]  it is possible to
   compute a similar id155, which we denote by  .

   [tsn2.png]

   note that, pi|i and pj|j are set to zero as we only want to model pair
   wise similarity.

   in simple terms step 1 and step2 calculate the id155
   of similarity between a pair of points in
    1. high dimensional space
    2. in low dimensional space

    for the sake of simplicity, try to understand this in detail.

   let us map 3d space to 2d space. what step1 and step2 are doing is
   calculating the id203 of similarity of points in 3d space and
   calculating the id203 of similarity of points in the
   corresponding 2d space.

   logically, the conditional probabilities  and must be equal for
   a perfect representation of the similarity of the datapoints in the
   different dimensional spaces, i.e the difference between  and  must be
   zero for the perfect replication of the plot in high and low
   dimensions.

   by this logic sne attempts to minimize this difference of conditional
   id203.


step 3

   now here is the difference between the sne and id167 algorithms.

   to measure the minimization of sum of difference of conditional
   id203 sne minimizes the sum of [101]id181s
   overall data points using a id119 method. we must know that
   kl divergences are asymmetric in nature.

   in other words, the sne cost function focuses on retaining the local
   structure of the data in the map (for reasonable values of the variance
   of the gaussian in the high-dimensional space, ).

   additionally, it is very difficult (computationally inefficient) to
   optimize this cost function.

   so id167 also tries to minimize the sum of the difference in
   conditional probabilities. but it does that by using the symmetric
   version of the sne cost function, with simple gradients. also, id167
   employs a heavy-tailed distribution in the low-dimensional space to
   alleviate both the crowding problem (the area of the two-dimensional
   map that is available to accommodate moderately distant data points
   will not be nearly large enough compared with the area available to
   accommodate nearby data points)  and the optimization problems of sne.


step 4

   if we see the equation to calculate the id155, we
   have left out the variance from the discussion as of now. the remaining
   parameter to be selected is the variance of the student   s
   t-distribution that is centered over each high-dimensional datapoint
   [xi.png] . it is not likely that there is a single value of  that is
   optimal for all data points in the data set because the density of the
   data is likely to vary. in dense regions, a smaller value of   is
   usually more appropriate than in sparser regions. any particular value
   of  induces a id203 distribution,  , over all of the other data
   points. this distribution has an

   this distribution has an id178 which increases as  increases. id167
   performs a binary search for the value of   that produces a  with a
   fixed perplexity that is specified by   the user. the perplexity is
   defined as

   where h( ) is the [102]shannon id178 of   measured in bits

   [tsn4.png]

   the perplexity can be interpreted as a smooth measure of the effective
   number of neighbors. the performance of sne is fairly robust to changes
   in the perplexity, and typical values are between 5 and 50.

   the minimization of the cost function is performed using gradient
   decent. and physically, the gradient may be interpreted as the
   resultant force created by a set of springs between the map point
   [yi.png] and all other map points [yj.png]  . all springs exert a force
   along the direction ( [yi.png]      [yj.png] ). the spring between
   [yi.png]  and [yj.png] repels or attracts the map points depending on
   whether the distance between the two in the map is too small or too
   large to represent the similarities between the two high-dimensional
   datapoints. the force exerted by the spring between and  is
   proportional to its length, and also proportional to its stiffness,
   which is the mismatch (pj|i     qj|i + p i| j     q i| j ) between the
   pairwise similarities of the data points and the map points[1].-


4.2 time and space complexity

   now that we have understood the algorithm, it is time to analyze its
   performance. as you might have observed, that the algorithm computes
   pairwise conditional probabilities and tries to minimize the sum of the
   difference of the probabilities in higher and lower dimensions. this
   involves a lot of calculations and computations. so the algorithm is
   quite heavy on the system resources.

   id167 has a quadratic time and space complexity in the number of data
   points. this makes it particularly slow and resource draining while
   applying it to data sets comprising of more than 10,000 observations.


5. what does id167 actually do?

   after we have looked into the mathematical description of how does the
   algorithms works, to sum up, what we have learned above. here is a
   brief explanation of how id167 works.

   it   s quite simple actually, id167 a non-linear id84
   algorithm finds patterns in the data by identifying observed clusters
   based on similarity of data points with multiple features. but it is
   not a id91 algorithm it is a id84 algorithm.
   this is because it maps the multi-dimensional data to a lower
   dimensional space, the input features are no longer identifiable. thus
   you cannot make any id136 based only on the output of id167. so
   essentially it is mainly a data exploration and visualization
   technique.

   but id167 can be used in the process of classification and id91
   by using its output as the input feature for other classification
   algorithms.


6. use cases

   you may ask, what are the use cases of such an algorithm. id167 can be
   used on almost all high dimensional data sets. but it is extensively
   applied in image processing, nlp, genomic data and speech processing.
   it has been utilized for improving the analysis of brain and heart
   scans. below are a few examples:

6.1 facial expression recognition

   a lot of progress has been made on fer and many algorithms like pca
   have been studied for fer. but, fer still remains a challenge due to
   the difficulties of dimension reduction and classification.
   t-stochastic neighbor embedding (id167) is used for reducing the
   high-dimensional data into a relatively low-dimensional subspace and
   then using other algorithms like adaboostm2, id79s, logistic
   regression, nns and others as multi-classifier for the expression
   classification.

   in one such attempt for facial recognition based on the japanese female
   facial expression (jaffe) database with id167 and adaboostm2.
   experimental results showed that the proposed new algorithm applied to
   fer gained the better performance compared with those traditional
   algorithms, such as pca, lda, lle and sne.[2]

   the flowchart for implementing such a combination on the data could be
   as follows:

   preprocessing     id172     id167    classification algorithm

                         pca      lda    lle     sne    id167

   id166               73.5%  74.3%  84.7%  89.6%  90.3%

   adaboostm2   75.4%  75.9%  87.7%  90.6%  94.5%


6.2 identifying tumor subpopulations (medical imaging)

   mass spectrometry imaging (msi) is a technology that simultaneously
   provides the spatial distribution for hundreds of biomolecules directly
   from tissue. spatially mapped t-distributed stochastic neighbor
   embedding (id167), a nonlinear visualization of the data that is able
   to better resolve the biomolecular intratumor heterogeneity.

   in an unbiased manner, id167 can uncover tumor subpopulations that are
   statistically linked to patient survival in gastric cancer and
   metastasis status in primary tumors of breast cancer. survival analysis
   performed on each id167 clusters will provide significantly useful
   results.[3]


6.3 text comparison using wordvec

   word vector representations capture many linguistic properties such as
   gender, tense, plurality and even semantic concepts like    capital city
   of   . using id84, a 2d map can be computed where
   semantically similar words are close to each other. this combination of
   techniques can be used to provide a bird   s-eye view of different text
   sources, including text summaries and their source material. this
   enables users to explore a text source like a geographical map.[4]


7. id167 compared to other id84 algorithms

   while comparing the performance of id167 with other algorithms, we will
   compare id167 with other algorithms based on the achieved accuracy
   rather than the time and resource requirements with relation to
   accuracy.

   id167 outputs provide better results than pca and other linear
   id84 models. this is because a linear method such
   as classical scaling is not good at modeling curved manifolds. it
   focuses on preserving the distances between widely separated data
   points rather than on preserving the distances between nearby data
   points.

   the gaussian kernel employed in the high-dimensional space by id167
   defines a soft border between the local and global structure of the
   data. and for pairs of data points that are close together relative to
   the standard deviation of the gaussian, the importance of modeling
   their separations is almost independent of the magnitudes of those
   separations. moreover, id167 determines the local neighborhood size for
   each datapoint separately based on the local density of the data (by
   forcing each id155 distribution to have the same
   perplexity)[1]. this is because the algorithm defines a soft border
   between the local and global structure of the data.  and unlike other
   non-linear id84 algorithms, it performs better than
   any of them.


8. example implementations

   let   s implement the id167 algorithm on mnist handwritten digit
   database. this is one of the most explored dataset for image
   processing.

1. in r

   the    rtsne    package has an implementation of id167 in r. the    rtsne   
   package can be installed in r using the following command typed in the
   r console:
install.packages(   rtsne   )
     * hyper parameter tuning
       [tsne1.png]

     * code
       mnist data can be downloaded from the mnist website and can be
       converted into a csv file with small amount of code.for this
       example, please download the following preprocessed mnist data.
       [103]link

## calling the installed package
train<- read.csv(file.choose()) ## choose the train.csv file downloaded from the
 link above
library(rtsne)
## curating the database for analysis with both id167 and pca
labels<-train$label
train$label<-as.factor(train$label)
## for plotting
colors = rainbow(length(unique(train$label)))
names(colors) = unique(train$label)

## executing the algorithm on curated data
tsne <- rtsne(train[,-1], dims = 2, perplexity=30, verbose=true, max_iter = 500)
exetimetsne<- system.time(rtsne(train[,-1], dims = 2, perplexity=30, verbose=tru
e, max_iter = 500))

## plotting
plot(tsne$y, t='n', main="tsne")
text(tsne$y, labels=train$label, col=colors[train$label])


     * implementation time

exetimetsne
  user       system  elapsed
  118.037   0.000  118.006

exectutiontimepca
   user     system   elapsed
  11.259   0.012     11.360

   as can be seen id167 takes considerably longer time to execute on the
   same sample size of data than pca.


     * interpreting results

   the plots can be used for exploratory analysis. the output x & y
   co-ordinates and as well as cost can be used as features in
   classification algorithms.

   [tsn5.png]

   [tsn61.png]

2. in python

   an important thing to note is that the    pip install tsne    produces an
   error. installing    tsne    package is not recommended. id167 algorithm
   can be accessed from sklearn package.
     * hyper parameter tuning

   [tsne2.png]
     * code

   the following code is taken from the sklearn examples on the sklearn
   website.
## importing the required packages
from time import time
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import offsetbox
from sklearn import (manifold, datasets, decomposition, ensemble,
             discriminant_analysis, random_projection)
## loading and curating the data
digits = datasets.load_digits(n_class=10)
x = digits.data
y = digits.target
n_samples, n_features = x.shape
n_neighbors = 30
## function to scale and visualize the embedding vectors
def plot_embedding(x, title=none):
    x_min, x_max = np.min(x, 0), np.max(x, 0)
    x = (x - x_min) / (x_max - x_min)
    plt.figure()
    ax = plt.subplot(111)
    for i in range(x.shape[0]):
        plt.text(x[i, 0], x[i, 1], str(digits.target[i]),
                 color=plt.cm.set1(y[i] / 10.),
                 fontdict={'weight': 'bold', 'size': 9})
    if hasattr(offsetbox, 'annotationbbox'):
        ## only print thumbnails with matplotlib > 1.0
        shown_images = np.array([[1., 1.]])  # just something big
        for i in range(digits.data.shape[0]):
            dist = np.sum((x[i] - shown_images) ** 2, 1)
            if np.min(dist) < 4e-3:
                ## don't show points that are too close
                continue
            shown_images = np.r_[shown_images, [x[i]]]
            imagebox = offsetbox.annotationbbox(
                offsetbox.offsetimage(digits.images[i], cmap=plt.cm.gray_r),
                x[i])
            ax.add_artist(imagebox)
    plt.xticks([]), plt.yticks([])
    if title is not none:
        plt.title(title)

#----------------------------------------------------------------------
## plot images of the digits
n_img_per_row = 20
img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))
for i in range(n_img_per_row):
    ix = 10 * i + 1
    for j in range(n_img_per_row):
        iy = 10 * j + 1
        img[ix:ix + 8, iy:iy + 8] = x[i * n_img_per_row + j].reshape((8, 8))
plt.imshow(img, cmap=plt.cm.binary)
plt.xticks([])
plt.yticks([])
plt.title('a selection from the 64-dimensional digits dataset')
## computing pca
print("computing pca projection")
t0 = time()
x_pca = decomposition.truncatedsvd(n_components=2).fit_transform(x)
plot_embedding(x_pca,
               "principal components projection of the digits (time %.2fs)" %
               (time() - t0))
## computing id167
print("computing id167 embedding")
tsne = manifold.tsne(n_components=2, init='pca', random_state=0)
t0 = time()
x_tsne = tsne.fit_transform(x)
plot_embedding(x_tsne,
               "id167 embedding of the digits (time %.2fs)" %
               (time() - t0))
plt.show()


     * implementation time

tsne: 13.40 s

pca: 0.01 s

   [tsn7.png] [tsn8.png]

9. where and when to use id167?

9.1 data scientist

   well for the data scientist the main problem while using id167 is the
   black box type nature of the algorithm. this impedes the process of
   providing id136s and insights based on the results. also, another
   problem with the algorithm is that it doesn   t always provide a similar
   output on successive runs.

   so then how could you use the algorithm? the best way to used the
   algorithm is to use it for exploratory data analysis. it will give you
   a very good sense of patterns hidden inside the data. it can also be
   used as an input parameter for other classification & id91
   algorithms.


9.2 machine learning hacker

   reduce the dataset to 2 or 3 dimensions and stack this with a
   non-linear stacker. using a holdout set for stacking / blending. then
   you can boost the id167 vectors using xgboost to get better results.


9.3 data science enthusiasts

   for data science enthusiasts who are beginning to work with data
   science, this algorithm presents the best opportunities in terms of
   research and performance enhancements. there have been a few research
   papers attempting to improve the time complexity of the algorithm by
   utilizing linear functions. but an optimal solution is still required.
   research papers on implementing id167 for a variety of nlp problems and
   image processing applications is an unexplored territory and has enough
   scope.


10. common fallacies

   following are a few common fallacies to avoid while interpreting the
   results of id167:
    1. for the algorithm to execute properly, the perplexity should be
       smaller than the number of points. also, the suggested perplexity
       is in the range of (5 to 50)
    2. sometimes, different runs with same hyper parameters may produce
       different results.
    3. cluster sizes in any id167 plot must not be evaluated for standard
       deviation, dispersion or any other similar measures. this is
       because id167 expands denser clusters and contracts sparser
       clusters to even out cluster sizes. this is one of the reasons for
       the crisp and clear plots it produces.
    4. distances between clusters may change because global geometry is
       closely related to optimal perplexity. and in a dataset with many
       clusters with different number of elements one perplexity cannot
       optimize distances for all clusters.
    5. patterns may be found in random noise as well, so multiple runs of
       the algorithm with different sets of hyperparameter must be checked
       before deciding if a pattern exists in the data.
    6. different cluster shapes may be observed at different perplexity
       levels.
    7. topology cannot be analyzed based on a single id167 plot, multiple
       plots must be observed before making any assessment.


reference

   [1] l.j.p. van der maaten and g.e. hinton. visualizing high-dimensional
   data using  id167. journal of machine learning research
   9(nov):2579-2605, 2008

   [2] jizheng yi et.al. facial expression recognition based on id167 and
   adaboostm2.

   ieee international conference on green computing and communications and
   ieee internet of things and ieee cyber,physical and social computing
   (2013)

   [3]  walid m. abdelmoulaa et.al. data-driven identification of
   prognostic tumor subpopulations using spatially mapped id167 of mass
   spectrometry imaging data.

   12244   12249 | pnas | october 25, 2016 | vol. 113 | no. 43

   [4]  hendrik heuer. text comparison using word vector representations
   and id84. 8th eur. conf. on python in science
   (euroscipy 2015)


end notes

   i hope you enjoyed reading this article.  in this article, i have tried
   to explore all the aspects to help you get started with id167. i   m sure
   you must be excited to explore more on id167 algorithm and use it at
   your end.

   share your experience of working with id167 algorithm and if you think
   its better than pca. if you have any doubts or questions, feel free to
   post it in the comments section.

[104]learn, [105]compete, hack and [106]get hired

   you can also read this article on analytics vidhya's android app
   [107]get it on google play

share this:

     * [108]click to share on linkedin (opens in new window)
     * [109]click to share on facebook (opens in new window)
     * [110]click to share on twitter (opens in new window)
     * [111]click to share on pocket (opens in new window)
     * [112]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [113]id84, [114]machine learning,
   [115]machine learning algorithm, [116]machine learning application,
   [117]id167, [118]id167 algorithm, [119]id167 examples, [120]id167 in
   python, [121]id167 in r, [122]time and space complexity
   next article

sas admin- mumbai, pune, bangalore (5-6 years of experience)

   previous article

mystory: how i became a data science hacker from being a delivery head

[123]saurabh.jaju2

   saurabh is a data scientist and software engineer skilled at analyzing
   variety of datasets and developing smart applications. he is currently
   pursuing a masters degree in information and data science from
   university of california,berkeley and is passionate about developing
   data science based smart resource management systems.
     *
     *

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [124]discussion portal to get your queries resolved

16 comments

     * dr.d.k.samuel says:
       [125]january 23, 2017 at 5:28 am
       nice keep them coming
       but it says
       error: object    train    not found
       > labels train$label<-as.factor(train$label)
       error in is.factor(x) : object 'train' not found
       [126]reply
          + dataslee says:
            [127]january 23, 2017 at 6:30 am
            the train data you must have yourself
            [128]reply
          + saurabh jugalkishor jaju says:
            [129]january 23, 2017 at 7:32 am
            dear dr samuel,
            the data to execute the r code must be loaded in the r
            environment. i will provide the link to the dataset. please
            download the data, read it in the r environment and then run
            the code.
            please download the data curated for the r code from this
            link:
            [130]https://drive.google.com/file/d/0b6e7d59tv2zwyljlzhdgeuyy
            dlk/view?usp=sharing
            then load the data in r using : read.csv(file.choose())
            command in r.
            thankyou for reading the article and please feel free to
            contact me with anything else relating to the article.
            regards,
            saurabh
            [131]reply
     * pete says:
       [132]january 23, 2017 at 4:33 pm
       hi,
       this is a great article covering the topic comprehensively.
       would appreciate if you can you share the complete r code.
       looking forward to more articles from you on advanced topics.
       thanks
       pete
       [133]reply
     * mantej singh dhanjal says:
       [134]january 24, 2017 at 3:14 pm
       nicely explained. another great article from analytics vidhya.
       thank you very much for such a useful information!
       i   ll look into that example.
       [135]reply
     * [136]padhma says:
       [137]january 31, 2017 at 1:21 pm
       i simply want to tell you that i   m all new to blogs and truly liked
       you   re blog site. very likely i   m likely to bookmark your site .you
       surely come with remarkable articles. cheers for sharing your
       website page.
       [138]reply
     * kern paillard says:
       [139]february 2, 2017 at 12:15 pm
       great article!
       i   m curious to know of your thoughts on kernel pca   compated to
       id167.
       essentially, kernel pca also maps the high dimensions using
       non-linear methods   
       there is a scikit implementation of it here,
       [140]http://scikit-learn.org/stable/auto_examples/decomposition/plo
       t_kernel_pca.html
       i   ve found performance issues when using kernel pca on even
       relatively medium sized data sets for e.g. ~= 100k rows and had to
       resort to carrying out kernel pca on chunks of data..
       [141]reply
     * [142]donald e. foss says:
       [143]february 10, 2017 at 10:30 pm
       can you explain the large time delta in the execution in r versus
       python? i assume the data set was the same.
       pca
       r: 11.360 seconds
       python: 0.01 seconds
       tsne
       r: 118.006 seconds
       python: 13.40 seconds
       the delta with tsne is nearly a magnitude, and the delta with pca
       is incredible.
       [144]reply
          + saurabh.jaju2 says:
            [145]february 11, 2017 at 3:56 am
            actually the datasets are not the same.the point of the code
            was to show the implementations of tsne and pca and compare
            them in each language. comparison of performance of python
            code to r code was not intended .the dataset for r is provided
            as a link in the article and the dataset for python is loaded
            sklearn package.
            [146]reply
     * [147]                                          40                        (         ) | 36          says:
       [148]april 27, 2017 at 1:00 am
       [   ] r-python         id167                    [   ]
       [149]reply
     * samira mzn says:
       [150]april 27, 2017 at 2:29 pm
       hello,
       i tested the r code for the id167 method and it worked very well, i
       would also like to know if you have informations on the two
       id84 methods : nerv and jse, with the codes r ?
       thanks,
       [151]reply
     * [152]                                          40                        (         ) -                 says:
       [153]june 7, 2017 at 9:28 pm
       [   ] r-python         id167                    [   ]
       [154]reply
     * [155]cynthia says:
       [156]october 12, 2017 at 11:22 am
       great guide     thanks for this!
       if you want to check out an interesting use of id167, displayr
       recently used it to analyze middle eastern politics. fascinating
       read:
       [157]https://www.displayr.com/using-machine-learning-id167-to-under
       stand-middle-eastern-politics/
       thanks,
       cynthia
       [158]reply
     * shane says:
       [159]december 1, 2017 at 8:49 pm
       in order to use this technique for machine learning, the id167
       model would need to convert future observations (e.g. from a test
       set), based on the results from the training set
       so is it possible for example, to do something like
       test_set_transformed <- predict(tsne, newdata = test_set)
       where tsne is the model/output from rtsne() and the training set in
       your code
       [160]reply
     * queen ohandjo says:
       [161]december 20, 2017 at 1:24 pm
       excellent post.
       code worked well
       now i have to prepare my dataset in this format for a successful
       run.
       thanks alot.
       *crossing fingers* code works on my data
       [162]reply
     * queen ohandjo says:
       [163]january 10, 2018 at 1:02 am
       hi,
       i was able to successfully run the train.csv file.
       but when i run my dataset i get the error below
       any help will be much appreciated
       > ## calling the installed package
       > prostate library(rtsne)
       >
       > ## curating the database for analysis with both id167 and pca
       > labels prostate$label<-as.factor(prostate$label)
       error in `$
       > ## for plotting
       > colors = rainbow(length(unique(prostate$label)))
       > names(colors) = unique(prostate$label)
       >
       > ## executing the algorithm on curated data
       > tsne exetimetsne
       > ## plotting
       > plot(tsne$y, t=   n   , main=   tsne   )
       > text(tsne$y, labels=prostate$label, col=colors[prostate$label])
       [164]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [165]srk       3924
   2    [2.jpg?date=2019-04-05] [166]mark12    3510
   3    [3.jpg?date=2019-04-05] [167]nilabha   3261
   4    [4.jpg?date=2019-04-05] [168]nitish007 3237
   5    [5.jpg?date=2019-04-05] [169]tezdhar   3082
   [170]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [171]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [172]understanding support vector machine algorithm from examples
       (along with code)
     * [173]essentials of machine learning algorithms (with python and r
       codes)
     * [174]a complete tutorial to learn data science with python from
       scratch
     * [175]7 types of regression techniques you should know!
     * [176]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [177]a simple introduction to anova (with applications in excel)
     * [178]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [179]top 5 machine learning github repositories and reddit discussions
   from march 2019

[180]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [181]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[182]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [183]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[184]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [185]16 opencv functions to start your id161 journey (with
   python code)

[186]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [187][ds-finhack.jpg]

   [188][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [189]about us
     * [190]our team
     * [191]career
     * [192]contact us
     * [193]write for us

   [194]about us
   [195]   
   [196]our team
   [197]   
   [198]careers
   [199]   
   [200]contact us

data scientists

     * [201]blog
     * [202]hackathon
     * [203]discussions
     * [204]apply jobs
     * [205]leaderboard

companies

     * [206]post jobs
     * [207]trainings
     * [208]hiring hackathons
     * [209]advertising
     * [210]reach us

   don't have an account? [211]sign up here.

join our community :

   [212]46336 [213]followers
   [214]20224 [215]followers
   [216]followers
   [217]7513 [218]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [219]privacy policy
     * [220]terms of use
     * [221]refund policy

   don't have an account? [222]sign up here

   iframe: [223]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [224](button) join now

   subscribe!

   iframe: [225]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [226](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/machine-learning/
  94. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/
  95. https://www.analyticsvidhya.com/blog/category/machine-learning/
  96. https://www.analyticsvidhya.com/blog/category/python-2/
  97. https://www.analyticsvidhya.com/blog/category/r/
  98. https://www.analyticsvidhya.com/blog/author/saurabh-jaju2/
  99. https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
 100. https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/
 101. https://en.wikipedia.org/wiki/kullback   leibler_divergence
 102. https://en.wikipedia.org/wiki/id178_(information_theory)
 103. https://drive.google.com/file/d/0b6e7d59tv2zwyljlzhdgeuyydlk/view?usp=sharing
 104. https://www.analyticsvidhya.com/blog
 105. https://datahack.analyticsvidhya.com/
 106. https://www.analyticsvidhya.com/jobs/#/user/
 107. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 108. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/?share=linkedin
 109. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/?share=facebook
 110. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/?share=twitter
 111. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/?share=pocket
 112. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/?share=reddit
 113. https://www.analyticsvidhya.com/blog/tag/dimensionality-reduction/
 114. https://www.analyticsvidhya.com/blog/tag/machine-learning/
 115. https://www.analyticsvidhya.com/blog/tag/machine-learning-algorithm/
 116. https://www.analyticsvidhya.com/blog/tag/machine-learning-application/
 117. https://www.analyticsvidhya.com/blog/tag/id167/
 118. https://www.analyticsvidhya.com/blog/tag/id167-algorithm/
 119. https://www.analyticsvidhya.com/blog/tag/id167-examples/
 120. https://www.analyticsvidhya.com/blog/tag/id167-in-python/
 121. https://www.analyticsvidhya.com/blog/tag/id167-in-r/
 122. https://www.analyticsvidhya.com/blog/tag/time-and-space-complexity/
 123. https://www.analyticsvidhya.com/blog/author/saurabh-jaju2/
 124. https://discuss.analyticsvidhya.com/
 125. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-121445
 126. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-121445
 127. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-121452
 128. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-121452
 129. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-121455
 130. https://drive.google.com/file/d/0b6e7d59tv2zwyljlzhdgeuyydlk/view?usp=sharing
 131. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-121455
 132. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-121478
 133. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-121478
 134. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-121523
 135. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-121523
 136. http://www.backtooriginal.com/
 137. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-121911
 138. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-121911
 139. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-122052
 140. http://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html
 141. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-122052
 142. http://donaldfoss.com/
 143. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-122489
 144. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-122489
 145. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-122506
 146. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-122506
 147. http://www.36dsj.com/archives/81770
 148. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-127729
 149. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-127729
 150. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-127764
 151. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-127764
 152. http://hadoopdoc.com/2017/06/08/                                          40             %
 153. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-129967
 154. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-129967
 155. http://www.displayr.com/
 156. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-139370
 157. https://www.displayr.com/using-machine-learning-id167-to-understand-middle-eastern-politics/
 158. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-139370
 159. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-145814
 160. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-145814
 161. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-148799
 162. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-148799
 163. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-150695
 164. https://www.analyticsvidhya.com/blog/2017/01/id167-implementation-r-python/#comment-150695
 165. https://datahack.analyticsvidhya.com/user/profile/srk
 166. https://datahack.analyticsvidhya.com/user/profile/mark12
 167. https://datahack.analyticsvidhya.com/user/profile/nilabha
 168. https://datahack.analyticsvidhya.com/user/profile/nitish007
 169. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 170. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 171. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 172. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 173. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 174. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 175. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 176. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 177. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 178. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 179. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 180. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 181. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 182. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 183. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 184. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 185. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 186. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 187. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 188. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 189. http://www.analyticsvidhya.com/about-me/
 190. https://www.analyticsvidhya.com/about-me/team/
 191. https://www.analyticsvidhya.com/career-analytics-vidhya/
 192. https://www.analyticsvidhya.com/contact/
 193. https://www.analyticsvidhya.com/about-me/write/
 194. http://www.analyticsvidhya.com/about-me/
 195. https://www.analyticsvidhya.com/about-me/team/
 196. https://www.analyticsvidhya.com/about-me/team/
 197. https://www.analyticsvidhya.com/about-me/team/
 198. https://www.analyticsvidhya.com/career-analytics-vidhya/
 199. https://www.analyticsvidhya.com/about-me/team/
 200. https://www.analyticsvidhya.com/contact/
 201. https://www.analyticsvidhya.com/blog
 202. https://datahack.analyticsvidhya.com/
 203. https://discuss.analyticsvidhya.com/
 204. https://www.analyticsvidhya.com/jobs/
 205. https://datahack.analyticsvidhya.com/users/
 206. https://www.analyticsvidhya.com/corporate/
 207. https://trainings.analyticsvidhya.com/
 208. https://datahack.analyticsvidhya.com/
 209. https://www.analyticsvidhya.com/contact/
 210. https://www.analyticsvidhya.com/contact/
 211. https://datahack.analyticsvidhya.com/signup/
 212. https://www.facebook.com/analyticsvidhya/
 213. https://www.facebook.com/analyticsvidhya/
 214. https://twitter.com/analyticsvidhya
 215. https://twitter.com/analyticsvidhya
 216. https://plus.google.com/+analyticsvidhya
 217. https://in.linkedin.com/company/analytics-vidhya
 218. https://in.linkedin.com/company/analytics-vidhya
 219. https://www.analyticsvidhya.com/privacy-policy/
 220. https://www.analyticsvidhya.com/terms/
 221. https://www.analyticsvidhya.com/refund-policy/
 222. https://id.analyticsvidhya.com/accounts/signup/
 223. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 224. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 225. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 226. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 228. https://www.facebook.com/analyticsvidhya
 229. https://twitter.com/analyticsvidhya
 230. https://plus.google.com/+analyticsvidhya/posts
 231. https://in.linkedin.com/company/analytics-vidhya
 232. https://datahack.analyticsvidhya.com/contest/skillpower-machine-learning/
 233. https://www.analyticsvidhya.com/blog/2017/01/sas-admin-mumbai-pune-bangalore-5-6-years-of-experience/
 234. https://www.analyticsvidhya.com/blog/2017/01/delivery-head-to-data-science-hacker/
 235. https://www.analyticsvidhya.com/blog/author/saurabh-jaju2/
 236. https://in.linkedin.com/in/saurabh-jaju
 237. https://github.com/saurabhjaju2
 238. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 239. https://www.facebook.com/analyticsvidhya/
 240. https://twitter.com/analyticsvidhya
 241. https://plus.google.com/+analyticsvidhya
 242. https://plus.google.com/+analyticsvidhya
 243. https://in.linkedin.com/company/analytics-vidhya
 244. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 245. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 246. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 247. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 248. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 249. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 250. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 251. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 252. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 253. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 254. javascript:void(0);
 255. javascript:void(0);
 256. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 257. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 258. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 259. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 260. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 261. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 262. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 263. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 264. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 265. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f01%2fid167-implementation-r-python%2f&linkname=comprehensive%20guide%20on%20id167%20algorithm%20with%20implementation%20in%20r%20%26amp%3b%20python
 266. javascript:void(0);
 267. javascript:void(0);
