   #[1]salesforce research

   [2]salesforce logo [3]mission [4]research [5]careers [6]ethics
   [7]products [8]press

   [9]publications [10]blog [11]open source [12]grants

   [13]salesforce logo [14]   
   [15]mission [16]research [17]publications [18]blog [19]open source
   [20]grants [21]careers [22]ethics [23]products [24]press

   [25]salesforce logo [26]mission [27]research [28]careers [29]ethics
   [30]products [31]press

   [32]publications [33]blog [34]open source [35]grants

your tldr by an ai: a deep reinforced model for abstractive summarization

   by: [36]romain paulus

   the last few decades have witnessed a fundamental change in the
   challenge of taking in new information. the bottleneck is no longer
   access to information; now it   s our ability to keep up. we all have to
   read more and more to keep up-to-date with our jobs, the news, and
   social media. we   ve looked at how ai can improve people   s work by
   helping with this information deluge and one potential answer is to
   have algorithms automatically summarize longer texts.

   training a model that can generate long, coherent, and meaningful
   summaries remains an open research problem. in fact, generating any
   kind of longer text is hard for even the most advanced deep learning
   algorithms. in order to make summarization successful, we introduce two
   separate improvements: a more contextual word generation model and a
   new way of training summarization models via id23
   (rl).

   the combination of the two training methods enables the system to
   create relevant and highly readable multi-sentence summaries of long
   text, such as news articles, significantly improving on previous
   results. our algorithm can be trained on a variety of different types
   of texts and summary lengths. in this blog post, we present the main
   contributions of our model and an overview of the natural language
   challenges specific to text summarization.
   [summ-weights-800.gif] figure 1: illustration of our model generating a
   multi-sentence summary from a news article. for each generated word,
   the model pays attention to specific words of the input and the
   previously generated output.

extractive vs. abstractive summarization

   id54 models can work in one of two ways: by
   extraction or by abstraction. extractive models perform
   "copy-and-paste" operations: they select relevant phrases of the input
   document and concatenate them to form a summary. they are quite robust
   since they use existing natural-language phrases that are taken
   straight from the input, but they lack in flexibility since they cannot
   use novel words or connectors. they also cannot paraphrase like people
   sometimes do. in contrast, abstractive models generate a summary based
   on the actual    abstracted    content: they can use words that were not in
   the original input. this gives them a lot more potential to produce
   fluent and coherent summaries but it is also a much harder problem as
   you now require the model to generate coherent phrases and connectors.

   even though abstractive models are more powerful in theory, it is
   common for them to make mistakes in practice. typical mistakes include
   incoherent, irrelevant or repeated phrases in generated summaries,
   especially when trying to create long text outputs. they historically
   lacked a sense of general coherence, flow and readability. in this
   work, we tackle these issues and design a more robust and coherent
   abstractive summarization model.

   in order to understand our new abstractive model, let   s first define
   the basic building blocks and then introduce our new training scheme.

reading and generating text with encoder-decoder models

   [37]recurrent neural networks (id56s) are deep learning models that can
   process sequences (e.g. text) of variable length and compute useful
   representations (or hidden state) for each phrase. these networks
   process each element of the sequence (in this case, each word) one by
   one; for each new input in the sequence, the network outputs a new
   hidden state as a function of that input and the previous hidden state.
   in this sense, the hidden state calculated at each word is a function
   of all the words read up to that point.
   [summ-encoder.svg] figure 2: a recurrent neural network reads an input
   sentence by applying the same function (in green) on individual words.

   id56s can also be used to generate output sequences in a similar
   fashion. at each step, the id56 hidden state is used to generate a new
   word that is added to the final output text and fed in as the next
   input.
   [summ-decoder-2.svg] figure 3: id56s can generate output sequences, and
   re-use the output word as the input of the next function.

   the input (reading) and output (generating) id56s can be combined in a
   joint model where the final hidden state of the input id56 is used as
   the initial hidden state of the output id56. combined in this way, the
   joint model is able to read any text and generate a different text from
   it. this framework is called an encoder-decoder id56 (or id195) and is
   the basis of our summarization model. in addition, we replace the
   traditional encoder id56 by a bidirectional encoder, which uses two
   different id56s to read the input sequence: one that reads the text from
   left-to-right (as illustrated in figure 4) and another that reads from
   right-to-left. this helps our model to have a better representation of
   the input context.
   [summ-encoder-decoder.svg] figure 4: encoder-decoder id56 models can be
   used to solve sequence-to-sequence tasks in natural language such as
   summarization.

a new attention and decoding mechanism

   to make our model outputs more coherent, we allow the decoder to look
   back at parts of the input document when generating a new word with a
   technique called temporal attention. instead of relying entirely on its
   own hidden state, the decoder can incorporate contextual information
   about different parts of the input with an attention function. this
   attention is then modulated to ensure that the model uses different
   parts of the input when generating the output text, hence increasing
   information coverage of the summary.

   in addition, to make sure that our model doesn't repeat itself, we also
   allow it to look back at the previous hidden states from the decoder.
   in a similar fashion, we define an intra-decoder attention function
   that can look back at previous hidden states of the decoder id56s.
   finally, the decoder combines the context vector from the temporal
   attention with the one from the intra-decoder attention to generate the
   next word in the output summary. figure 5 illustrates the combination
   of these two attention functions at a given decoding step.
   [summ-attentions.svg] figure 5: two context vectors (marked    c   ) are
   computed from attending over the encoder hidden states and decoder
   hidden states. using these two contexts and the current decoder hidden
   state (   h   ), a new word is generated (on the right) and added to the
   output sequence.

supervised learning vs. id23

   to train this model on real-world data like news articles, a common way
   is to use the teacher forcing algorithm: a model generates a summary
   while using a reference summary, and the model is assigned a
   word-by-word error (or    local supervision   , as shown in figure 6) each
   time it generates a new word.
   [summ-teacher-forcing.svg] figure 6: model training with supervised
   learning. each generated word gets a training supervision signal,
   calculated by comparing it against the ground truth summary word at the
   same position.

   this method can be used to train any sequence generation model based on
   recurrent neural networks, with very decent results. however, for our
   particular task, summaries don't have to match a reference sequence
   word by word in order to be correct. as you can imagine, two humans may
   generate very different summaries of the same news article, sometimes
   using different styles, words or sentence orders, while still being
   considered good summaries. the problem with teacher forcing here is
   that as soon as the first few words are generated, the training is
   misguided: it sticks strictly to the one officially correct summary and
   cannot adjust to a potentially correct but different beginning.

   taking this into consideration, we can do better than the word-by-word
   approach of teacher forcing. a different kind of training called
   id23 (rl) can be applied here. at first, the rl
   algorithm lets the model generate its own summary, then it uses an
   external scorer to compare the generated summary against the ground
   truth. this scorer then indicates to the model how "good" the generated
   summary was. if the score is high, then the model can update itself to
   make such summaries more likely to appear in the future. otherwise, if
   the score is low, the model will get penalized and change its
   generation procedure to prevent similar summaries. this reinforced
   model is very good at increasing the summarization score that evaluates
   the entire sequence rather than a word-by-word prediction.
   [summ-rl.svg] figure 7: in id23, the model doesn   t
   have a local supervision signal for every predicted word, but instead
   is trained with a reward signal that depends on the entire output and
   the reference summary.

how to evaluate summarization

   what exactly is this scorer, and how does it tell if summaries are
   "good"? since asking a human to manually evaluate millions of summaries
   is long and impractical at scale, we rely on an automated evaluation
   metric called [38]id8 (recall-oriented understudy for gisting
   evaluation). id8 works by comparing matching sub-phrases in the
   generated summaries against sub-phrases in the ground truth reference
   summaries, even if they are not perfectly aligned. different variants
   of id8 (id8-1, id8-2, id8-l) all work in the same fashion but
   use different sub-sequence lengths.

   while id8 scores have a good correlation with human judgment in
   general, the summaries with the highest id8 aren't necessarily the
   most readable or natural ones. this became an issue when we trained our
   model to maximize the id8 score with id23 alone. we
   observed that our models with the highest id8 scores also generated
   barely-readable summaries.

   to bring the best of both worlds, our model is trained with teacher
   forcing and id23 at the same time, being able to make
   use of both word-level and whole-summary-level supervision to make it
   more coherent and readable. in particular, we find that id8-optimized
   rl helps improve recall (i.e all important information that needs to be
   summarized is indeed summarized) and word level learning supervision
   ensures good language flow, making the summary more coherent and
   readable.
   [summ-hybrid-training.svg] figure 8: combination of supervised learning
   (in red) and id23 (in purple), showing how our model
   can learn both local and global rewards and optimize both for
   readability and overall id8 score.

   until recently, the highest id8-1 score for abstractive summarization
   on the id98/daily mail dataset was 35.46. the combination of our
   intra-decoder attention id56 model with joint supervised and rl training
   improves this score to 39.87, and 41.16 with rl only. figure 9 shows
   other summarization scores for existing models and ours. even though
   our pure rl model has higher id8 scores, our supervised+rl model has
   a higher readability, hence is more relevant for this summarization
   task. note that see et al. use a slightly different data format, hence
   their results are not directly comparable with ours and the others but
   still give a good reference point.
                        model                      id8-1 id8-l
   [39]nallapati et al. 2016 (abstractive)         35.46   32.65
   [40]nallapati et al. 2017 (extractive baseline) 39.2    35.5
   [41]nallapati et al. 2017 (extractive)          39.6    35.3
   [42]see et al. 2017 (abstractive)               39.53*  36.38*
   our model (rl only)                             41.16   39.08
   our model (supervised + rl)                     39.87   36.90
   figure 9: summarization results on the id98/daily mail dataset,
   comparing our model against existing abstractive and extractive
   approaches.

sample outputs

   what does such a large improvement mean in terms of real summaries?
   here we show a couple of multi-sentence summaries based on documents
   from the development split of the dataset. our model and its simpler
   baselines generated these, after training on the [43]id98/daily mail
   dataset. as you can see, the summaries have significantly improved but
   there   s still more work needed to make them perfect.

   as a non-banking institution, google wallet, along with competitors
   paypal and venmo, is not legally required to be federally insured. with
   the new change to its policy, funds in wallet balance are protected if
   anything were to happen to the company like bankruptcy. the mobile
   transfer service (similar to paypal and venmo) in multiple
   federally-insured banking institutions. google's user agreement says
   funds are not protected by the federal deposit insurance corporation.
   four balls in a single shot-and for those who miss it there's a slow
   motion version.
   article summary (ground truth) summary (our model)
   google wallet says it has changed its policy when storing users' funds
   as they will now be federally-insured (file photo) for those who use
   google wallet, their money just became safer with federal-level
   insurance. google confirmed to yahoo finance in a statement that its
   current policy changed - meaning the company will store the balances
   for users of the mobile transfer service (similar to paypal and venmo)
   in multiple federally-insured banking institutions. this is good news
   for people who place large amounts of money in their wallet balance
   because the federal deposit insurance corporation insures funds for
   banking institutions up to $250,000. currently, google's user agreement
   says funds are not protected by the fdic. however, a google
   spokesperson told yahoo finance that the current policy has changed.
   (...)
   talk about a chain reaction! this is the moment a billiards player
   performs a complex trick shot by setting up a domino train to pot four
   balls. video footage shows a white ball being rolled down a positioned
   cue. it then bounces off one side of the red-clothed table and hits the
   first in a long line of dominoes. one by one the small counters fall
   down, tapping balls into various pockets as they go. first a yellow,
   then a blue, then a red. finally, the last domino gently hits an orange
   ball, causing it to roll down another positioned cue lying on the
   table. the orb then knocks a green ball into the center pocket. in less
   than 30 seconds the stunt comes to a close. (...) video footage shows a
   white ball being rolled down a jumper. it then bounces off one side of
   the red-clothed table and hits the first in a long line of dominoes.
   one by one the small counters fall down, tapping balls into pockets as
   they go-first a yellow. it comes to a close. the clip was uploaded by
   youtube user honda4ridered.
   kelly osbourne didn't always want to grow up to be like her famous mom
   - but in a letter published in the new book a letter to my mom, the tv
   personality admitted that she is now proud to be sharon osbourne's
   daughter. for author lisa erspamer's third collection of tributes,
   celebrities such as melissa rivers, shania twain, will.i.am, christy
   turlington burns, and kristin chenoweth all composed messages of love
   and gratitude to the women who raised them. and the heartwarming
   epistolary book, which was published last week, has arrived just in
   time for mother's day on may 10. 'like all teenage girls i had this
   ridiculous fear of growing up and becoming just like you,' kelly
   osbourne wrote in her letter, republished on yahoo parenting. 'i was so
   ignorant and adamant about creating my "own" identity.' scroll down for
   video mini-me: in lisa erspamer's new book a letter to my mom, kelly
   osbourne (r) wrote a letter to her mother sharon (l) saying that she's
   happy to have grown up to be just like her (...) author lisa erspamer
   invited celebrities and a number of other people to write heartfelt
   notes to their mothers for her new book a letter to my mom. stars such
   as melissa rivers, will.i.am, and christy turlington participated in
   the moving project. kelly didn't always want to grow up to be like her
   famous mom. lisa erspamer's third collection of tributes, celebrities
   such as melissa rivers, shania twain, will.i.am, christy turlington,
   and kristin chenoweth all composed messages of love and gratitude to
   the women who raised them. kelly wrote a letter to her mom before
   joan's death last year. she has arrived just in time for mother's day
   on may 10.
   figure 10: more examples of summaries generated by our model, compared
   against summaries written by humans for the same article.

   in order to illustrate the impact of our main contributions on text
   summarization, figure 11 shows how the output of our model diverges if
   we remove intra-attention and rl training.
   article
   tony blair has said he does not want to retire until he is 91     as he
   unveiled plans to set up a    cadre    of ex-leaders to advise governments
   around the world. the defiant 61-year-old former prime minister said he
   had    decades    still in him and joked that he would    turn to drink    if
   he ever stepped down from his multitude of global roles. he told
   newsweek magazine that his latest ambition was to recruit former heads
   of government to go round the world to advise presidents and prime
   ministers on how to run their countries. in an interview with the
   magazine newsweek mr blair said he did not want to retire until he was
   91 years old mr blair said his latest ambition is to recruit former
   heads of government to advise presidents and prime ministers on how to
   run their countries mr blair said he himself had been    mentored    by us
   president bill clinton when he took office in 1997. and he said he
   wanted to build up his organisations, such as his faith foundation, so
   they are    capable of changing global policy   . last night, tory mps
   expressed horror at the prospect of mr blair remaining in public life
   for another 30 years. andrew bridgen said:    we all know weak ed
   miliband   s called on tony to give his flailing campaign a boost, but
   the attention   s clearly gone to his head.    (...)
   summary (ground truth, written by a human)
   the former prime minister claimed he has 'decades' of work left in him.
   joked he would 'turn to drink' if he ever stepped down from global
   roles. wants to recruit former government heads to advise current
   leaders. he was 'mentored' by us president bill clinton when he started
   in 1997.
   summary (our model)
   blair said he did not want to retire until he was 91 years old.
   61-year-old former prime minister said he would 'turn to drink' if he
   ever stepped down from his own. he said he wanted to build up his
   charity to advise presidents and prime ministers on how to run their
   countries. mr blair says he is to recruit former heads of government to
   go round the world to advise ministers. he says he wants to emulate
   ex-israeli president shimon peres.
   summary (without intra-attention and id23)
   61-year-old former prime minister said he did not want to retire until
   he was 91 years old. he said he wanted to build up his organisations,
   such as his faith foundation. he said he wanted to emulate ex-israeli
   president shimon peres. mr blair said he wanted to emulate ex-israeli
   president shimon peres. he said he wanted to be seeing someone when he
   took office in 1997. mr blair said he wanted to be seeing someone when
   he took office in 1997. mr blair said he wanted to
   figure 11: example summaries generated by our model, with and without
   our main contributions. new words that are not present in the original
   article are shown in green. repeated phrases in the summaries are shown
   in red.

conclusion

   our model significantly improves the state-of-the-art in multi-sentence
   summary generation, outperforming existing abstractive models and
   extractive baselines. we believe that our contributions - the
   intra-decoder attention module and the combined training objective -
   could improve other sequence generation tasks, especially for long
   outputs.

   our work also touches on the limit of automatic id74 such
   as id8, and shows that better metrics are required to evaluate - and
   optimize - summarization models. an ideal metric will correlate well
   with human judgment both in terms of summary coherence and readability.
   when such a metric is used with our reinforced summarization model
   summaries may improve even further.

citation credit

   romain paulus, caiming xiong, and richard socher. 2017.
   [44]a deep reinforced model for abstractive summarization

acknowledgements

   special thanks to [45]melvin gruesbeck for his help with visuals and
   figures.

   [46]salesforce logo [47]content [48]terms [49]privacy
      copyright 2017 [50]salesforce.com, inc. [51]all rights reserved.
   rights of albert einstein are used with permission of the hebrew
   university of jerusalem. represented exclusively by greenlight.

references

   1. https://blog.einstein.ai/rss/
   2. https://www.einstein.ai/
   3. https://einstein.ai/mission
   4. https://einstein.ai/research
   5. https://einstein.ai/careers
   6. https://einstein.ai/ethics
   7. https://einstein.ai/products
   8. https://einstein.ai/press
   9. https://einstein.ai/research/publications
  10. https://einstein.ai/research/blog
  11. https://einstein.ai/research/open-source
  12. https://einstein.ai/research/grants
  13. https://www.einstein.ai/
  14. javascript:void(0);
  15. https://einstein.ai/mission
  16. https://einstein.ai/research
  17. https://einstein.ai/research/publications
  18. https://einstein.ai/research/blog
  19. https://einstein.ai/research/open-source
  20. https://einstein.ai/research/grants
  21. https://einstein.ai/careers
  22. https://einstein.ai/ethics
  23. https://einstein.ai/products
  24. https://einstein.ai/press
  25. https://www.einstein.ai/
  26. https://einstein.ai/mission
  27. https://einstein.ai/research
  28. https://einstein.ai/careers
  29. https://einstein.ai/ethics
  30. https://einstein.ai/products
  31. https://einstein.ai/press
  32. https://einstein.ai/research/publications
  33. https://einstein.ai/research/blog
  34. https://einstein.ai/research/open-source
  35. https://einstein.ai/research/grants
  36. https://blog.einstein.ai/author/romain/
  37. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  38. https://en.wikipedia.org/wiki/id8_(metric)
  39. https://arxiv.org/abs/1602.06023
  40. https://arxiv.org/abs/1611.04230
  41. https://arxiv.org/abs/1611.04230
  42. https://arxiv.org/abs/1704.04368
  43. http://cs.nyu.edu/~kcho/dmqa/
  44. https://arxiv.org/abs/1705.04304
  45. http://www.melvingruesbeck.com/
  46. https://www.salesforce.com/
  47. https://einstein.ai/content-takedown
  48. https://einstein.ai/terms-of-service
  49. https://einstein.ai/privacy
  50. https://www.salesforce.com/
  51. https://www.salesforce.com/company/legal/intellectual.jsp
