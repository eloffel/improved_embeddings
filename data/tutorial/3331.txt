   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]ai

                         caption this, with tensorflow

   how to build and train an image caption generator using a tensorflow
   notebook.

   by [27]raul puri[28]daniel ricciardelli

   march 28, 2017

   the image id134 model. the image id134 model.
   (source: shannon shih from machine learning at berkeley. horse image
   sourced from ms coco.)

   attention readers: we invite you to access the corresponding [29]python
   code and ipython notebooks for this article on github.

   [30]image id134 models combine recent advances in computer
   vision and machine translation to produce realistic image captions
   using neural networks. neural image caption models are trained to
   maximize the likelihood of producing a caption given an input image,
   and can be used to generate novel image descriptions. for example, the
   following are possible captions generated using a neural image caption
   generator trained on the [31]ms coco data set.
   possible captions generated using a neural image caption generator
   figure 1. credit: raul puri, with images sourced from [32]ms coco data
   set.

   in this article, we will walk through an intermediate-level tutorial on
   how to train an image caption generator on the [33]flickr30k data set
   using an [34]adaptation of google   s show and tell model. we use the
   [35]tensorflow framework to construct, train, and test our model
   because it   s relatively easy to use and has a growing online community.

why id134?

   recent successes in applying deep neural networks to id161
   and natural language processing tasks have inspired ai researchers to
   explore new research opportunities at the intersection of these
   previously separate domains. id134 models have to balance
   an understanding of both visual cues and natural language.

   the intersection of these two traditionally unrelated fields has the
   possibility to effect change on a wide scale. while there are some
   straightforward applications of this technology, such as generating
   summaries for youtube videos or captioning unlabeled images, more
   creative applications can drastically improve the quality of life for a
   wide cross section of the population. similar to how traditional
   id161 seeks to make the world more accessible and
   understandable for computers, this technology has the potential to make
   our world more accessible and understandable for us humans. it can
   serve as a tour guide, and can even serve as a visual aid for daily
   life, such as in the case of the [36]horus wearable device from the
   italian ai firm [37]eyra.

some assembly required

   before we begin, we   ll need to do some housekeeping.

   first, you will need to install tensorflow. if this is your first time
   working with tensorflow, we recommend that you first review the
   following article: [38]hello, tensorflow! building and training your
   first tensorflow model.

   you will need the pandas, opencv2, and jupyter libraries to run the
   associated code. however, to simplify the install process we highly
   recommend that you follow the docker install instructions on our
   associated [39]github repo.

   you will also need to download the image embeddings and image captions
   for the flickr30k data set. download links are also provided on our
   [40]github repo.

   now, let's begin!

the image id134 model

   image id134 model figure 2. credit: shannon shih from
   machine learning at berkeley. horse image sourced from ms coco.

   at a high-level, this is the model we will be training. each image will
   be encoded by a deep convolutional neural network into a 4,096
   dimensional vector representation. a language generating id56, or
   recurrent neural network, will then decode that representation
   sequentially into a natural language description.

id134 as an extension of image classification

   image classification is a id161 task with a lot of history
   and many strong models behind it. classification requires models that
   can piece together relevant visual information about the shapes and
   objects present in an image, to place that image into an object
   category. machine learning models for other id161 tasks such
   as [41]id164 and [42]image segmentation build on this by not
   only recognizing when information is present, but also by learning how
   to interpret 2d space, reconcile the two understandings, and determine
   where an object's information is distributed in the image. for caption
   generation, this raises two questions:
    1. how can we build upon the success of image classification models,
       in retrieving important information from images?
    2. how can our model learn to reconcile an understanding of language,
       with an understanding of images?

leveraging id21

   we can take advantage of pre-existing models to help caption images.
   [43]id21 allows us to take the data transformations
   learned by neural networks trained on different tasks and apply them to
   our data. in our case, the [44]vgg-16 image classification model takes
   in 224x224 pixel images and produces a 4,096 dimensional feature vector
   useful for categorizing images.

   we can take the representation (known as the image embedding) from the
   vgg-16 model and use it to train the rest of our model. for the scope
   of this article, we have abstracted away the architecture of vgg-16 and
   have pre-computed the 4,096 dimensional features to speed up training.

   loading the vgg image features and image captions is relatively
   straightforward:
def get_data(annotation_path, feature_path):
    annotations = pd.read_table(annotation_path, sep='\t', header=none, names=['
image', 'caption'])
    return np.load(feature_path,'r'), annotations['caption'].values

understanding captions

   now that we have an image representation, we need our model to learn to
   decode that representation into an understandable caption. due to the
   serial nature of text, we leverage recurrence in an id56/lstm network
   (to learn more, read "[45]understanding id137"). these networks
   are trained to predict the next word in a series given previous words
   and the image representation.

   long short-term memory (lstm) cells allow the model to better select
   what information to use in the sequence of caption words, what to
   remember, and what information to forget. tensorflow provides a wrapper
   function to generate an lstm layer for a given input and output
   dimension.

   to transform words into a fixed-length representation suitable for lstm
   input, we use an embedding layer that learns to map words to 256
   dimensional features (or word-embeddings). word-embeddings help us
   represent our words as vectors, where similar word-vectors are
   semantically similar. to learn more about how word-embeddings capture
   the relationships between different words, check out "[46]capturing
   semantic meaning using deep learning."

   in the vgg-16 image classifier, the convolutional layers extract a
   4,096 dimensional representation to pass through a final softmax layer
   for classification. because the lstm cells expect 256 dimensional
   textual features as input, we need to translate the image
   representation into the representation used for the target captions. to
   do this, we utilize another embedding layer that learns to map the
   4,096 dimensional image features into the space of 256 dimensional
   textual features.

building and training the model

   all together, this is what the show and tell model looks like:
   show and tell model figure 3. source: "[47]show and tell: lessons
   learned from the 2015 mscoco image captioning challenge."

   in this diagram, {s[0], s[1], ..., s[n]} represent the words of the
   caption we are trying to predict and {w[e]s[0], w[e]s[1][,] ...,
   w[e]s[n][-1]} are the id27 vectors for each word. the outputs
   {p1, p2, ..., pn} of the lstm are id203 distributions generated
   by the model for the next word in the sentence. the model is trained to
   minimize the negative sum of the log probabilities of each word.
def build_model(self):
        # declaring the placeholders for our extracted image feature vectors, ou
r caption, and our mask
        # (describes how long our caption is with an array of 0/1 values of leng
th `maxlen`
        img = tf.placeholder(tf.float32, [self.batch_size, self.dim_in])
        caption_placeholder = tf.placeholder(tf.int32, [self.batch_size, self.n_
lstm_steps])
        mask = tf.placeholder(tf.float32, [self.batch_size, self.n_lstm_steps])

        # getting an initial lstm embedding from our image_imbedding
        image_embedding = tf.matmul(img, self.img_embedding) + self.img_embeddin
g_bias

        # setting initial state of our lstm
        state = self.lstm.zero_state(self.batch_size, dtype=tf.float32)

       total_ loss = 0.0
        with tf.variable_scope("id56"):
            for i in range(self.n_lstm_steps):
                if i > 0:
                   #if this isn't the first iteration of our lstm we need to get
 the word_embedding corresponding
                   # to the (i-1)th word in our caption
                    with tf.device("/cpu:0"):
                        current_embedding = tf.nn.embedding_lookup(self.word_emb
edding, caption_placeholder[:,i-1]) + self.embedding_bias
                else:
                     #if this is the first iteration of our lstm we utilize the
embedded image as our input
                     current_embedding = image_embedding
                if i > 0:
                    # allows us to reuse the lstm tensor variable on each iterat
ion
                    tf.get_variable_scope().reuse_variables()

                out, state = self.lstm(current_embedding, state)

                print (out,self.word_encoding,self.word_encoding_bias)

                if i > 0:
                    #get the one-hot representation of the next word in our capt
ion
                    labels = tf.expand_dims(caption_placeholder[:, i], 1)
                    ix_range=tf.range(0, self.batch_size, 1)
                    ixs = tf.expand_dims(ix_range, 1)
                    concat = tf.concat([ixs, labels],1)
                    onehot = tf.sparse_to_dense(
                            concat, tf.stack([self.batch_size, self.n_words]), 1
.0, 0.0)

                    #perform a softmax classification to generate the next word
in the caption
                    logit = tf.matmul(out, self.word_encoding) + self.word_encod
ing_bias
                    xid178 = tf.nn.softmax_cross_id178_with_logits(logits=lo
git, labels=onehot)
                    xid178 = xid178 * mask[:,i]

                    loss = tf.reduce_sum(xid178)
                    total_loss += loss

            total_loss = total_loss / tf.reduce_sum(mask[:,1:])
            return total_loss, img,  caption_placeholder, mask

generating captions using id136

   after training, we have a model that gives the id203 of a word
   appearing next in a caption, given the image and all previous words.
   how can we use this to generate new captions?

   the simplest approach is to take an input image and iteratively output
   the next most probable word, building up a single caption.
def build_generator(self, maxlen, batchsize=1):
        #same setup as `build_model` function
        img = tf.placeholder(tf.float32, [self.batch_size, self.dim_in])
        image_embedding = tf.matmul(img, self.img_embedding) + self.img_embeddin
g_bias
        state = self.lstm.zero_state(batchsize,dtype=tf.float32)

       #declare list to hold the words of our generated captions
        all_words = []
        print (state,image_embedding,img)
        with tf.variable_scope("id56"):
           # in the first iteration we have no previous word, so we directly pas
s in the image embedding
           # and set the `previous_word` to the embedding of the start token ([0
]) for the future iterations
            output, state = self.lstm(image_embedding, state)
            previous_word = tf.nn.embedding_lookup(self.word_embedding, [0]) + s
elf.embedding_bias

            for i in range(maxlen):
                tf.get_variable_scope().reuse_variables()

                out, state = self.lstm(previous_word, state)


               # get a one-hot word encoding from the output of the lstm
                logit = tf.matmul(out, self.word_encoding) + self.word_encoding_
bias
                best_word = tf.argmax(logit, 1)

                with tf.device("/cpu:0"):
                   # get the embedding of the best_word to use as input to the n
ext iteration of our lstm
                    previous_word = tf.nn.embedding_lookup(self.word_embedding,
best_word)

                previous_word += self.embedding_bias

                all_words.append(best_word)

        return img, all_words

   in many cases this works, but by "greedily" taking the most probable
   words, we may not end up with the most probable caption overall.

   one possible way to circumvent this is by using a method called
   "[48]id125." the algorithm iteratively considers the set of the k
   best sentences up to length t as candidates to generate sentences of
   size t + 1, and keeps only the resulting best k of them. this allows
   one to explore a larger space of good captions while keeping id136
   computationally tractable. in the example below, the algorithm
   maintains a list of k = 2 candidate sentences shown by the path to each
   bold word for each vertical time step.
   id125 method figure 4.

   credit: daniel ricciardelli.

limitations and discussion

   the neural image caption generator gives a useful framework for
   learning to map from images to human-level image captions. by training
   on large numbers of image-caption pairs, the model learns to capture
   relevant semantic information from visual features.

   however, with a static image, embedding our caption generator will
   focus on features of our images useful for image classification and not
   necessarily features useful for id134. to improve the
   amount of task-relevant information contained in each feature, we can
   train the image embedding model (the vgg-16 network used to encode
   features) as a piece of the id134 model, allowing us to
   fine-tune the image encoder to better fit the role of generating
   captions.

   also, if we actually look closely at the captions generated, we notice
   that they are rather mundane and commonplace. take this possible
   image-caption pair for instance:
   image-caption pair figure 5.

   credit: raul puri, with images sourced from the ms coco data set.

   this is most certainly a "giraffe standing next to a tree." however, if
   we look at other pictures, we will likely notice that it generates a
   caption of "a giraffe next to a tree" for any picture with a giraffe
   because giraffes in the training set often appear near trees.

next steps

   first, if you want to improve on the model explained here, take a look
   at google's open source [49]show and tell network, trainable with the
   ms coco data set and an inception-v3 image embedding.

   current state-of-the-art image captioning models include a visual
   attention mechanism, which allows the model to identify areas of
   interest in the image to selectively focus on when generating captions.

   also, if you are interested in this state-of-the-art implementation of
   id134, check out the following paper: [50]show, attend,
   and tell: neural image id134 with visual attention.

   note: don't forget to access the corresponding [51]python code and
   ipython notebooks for this article on github!

   this post is a collaboration between o'reilly and [52]tensorflow.
   [53]see our statement of editorial independence.
   article image: the image id134 model. (source: shannon
   shih from machine learning at berkeley. horse image sourced from ms
   coco.).
   tags: [54]all about tensorflow

   share
    1. [55]tweet
    2.
    3.
     __________________________________________________________________

[56]raul puri

   raul puri is a graduating undergraduate researcher at uc berkeley co
   2017. raul has contributed to research projects in several fields
   including but not limited to: robotics and automation, id161,
   medical imaging, biomems devices, etc. however, the bulk of his
   research work is focused on machine learning and machine learning
   systems with applications to security, anomaly detection, nlp, and
   id161 and robotics. raul is also passionate about giving back
   to the community by teaching applied ml concepts and is a teaching
   assista...
   [57]more

[58]daniel ricciardelli

   dan ricciardelli is an undergraduate researcher at the university of
   california, berkeley. his research interests include natural language
   processing for finance and industrial applications, id161,
   deep active learning, and automated knowledge discovery. dan is excited
   about making machine learning more accessible to technical and
   non-technical students and professionals alongside machine learning at
   berkeley.
   [59]more
     __________________________________________________________________

   [60]bots landscape.

   [61]ai

[62]infographic: the bot platform ecosystem

   by [63]jon bruner

   a look at the artificial intelligence and messaging platforms behind
   the fast-growing chatbot community

   video
   [64]vertical forest, milan.

   [65]ai

[66]evolve ai

   by [67]naveen rao

   naveen rao explains how intel nervana is evolving the ai stack from
   silicon to the cloud.

   [68]welcome sign at o'reilly ai conference 2016

   [69]ai

[70]highlights from the o'reilly ai conference in new york 2016

   by [71]mac slocum

   watch highlights covering artificial intelligence, machine learning,
   intelligence engineering, and more. from the o'reilly ai conference in
   new york 2016.

   video
   [72]close up of uber's self driving car in pittsburgh.

   [73]ai

[74]how ai is propelling driverless cars, the future of surface transport

   by [75]shahin farshchi

   shahin farshchi examines role artificial intelligence will play in
   driverless cars.

about us

     * [76]our company
     * [77]teach/speak/write
     * [78]careers
     * [79]customer service
     * [80]contact us

site map

     * [81]ideas
     * [82]learning
     * [83]topics
     * [84]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [85]terms of service     [86]privacy policy     [87]editorial independence

   animal

   iframe: [88]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/ai
  27. https://www.oreilly.com/people/raul-puri
  28. https://www.oreilly.com/people/daniel-ricciardelli
  29. https://goo.gl/mjruhw
  30. https://arxiv.org/abs/1609.06647
  31. http://mscoco.org/
  32. http://mscoco.org/
  33. http://web.engr.illinois.edu/~bplumme2/flickr30kentities/
  34. https://github.com/mlberkeley/oreilly-captions
  35. https://www.tensorflow.org/
  36. https://horus.tech/horus/?l=en_us
  37. https://eyra.io/
  38. https://www.oreilly.com/learning/hello-tensorflow
  39. https://github.com/mlberkeley/oreilly-captions
  40. https://github.com/mlberkeley/oreilly-captions
  41. https://en.wikipedia.org/wiki/object_detection
  42. https://en.wikipedia.org/wiki/image_segmentation
  43. http://cs231n.github.io/transfer-learning/
  44. https://arxiv.org/abs/1409.1556
  45. http://colah.github.io/posts/2015-08-understanding-lstms/
  46. https://www.oreilly.com/learning/capturing-semantic-meanings-using-deep-learning
  47. https://arxiv.org/abs/1609.06647
  48. https://en.wikipedia.org/wiki/beam_search
  49. https://github.com/tensorflow/models/tree/master/im2txt
  50. https://arxiv.org/abs/1502.03044
  51. https://goo.gl/a9kkhq
  52. https://goo.gl/igouk3
  53. http://www.oreilly.com/about/editorial_independence.html
  54. https://www.oreilly.com/tags/all-about-tensorflow
  55. https://twitter.com/share
  56. https://www.oreilly.com/people/raul-puri
  57. https://www.oreilly.com/people/raul-puri
  58. https://www.oreilly.com/people/daniel-ricciardelli
  59. https://www.oreilly.com/people/daniel-ricciardelli
  60. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  61. https://www.oreilly.com/topics/ai
  62. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  63. https://www.oreilly.com/people/b1d73-jon-bruner
  64. https://www.oreilly.com/ideas/evolve-ai
  65. https://www.oreilly.com/topics/ai
  66. https://www.oreilly.com/ideas/evolve-ai
  67. https://www.oreilly.com/people/14d38-naveen-rao
  68. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  69. https://www.oreilly.com/topics/ai
  70. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  71. https://www.oreilly.com/people/0d2c1-mac-slocum
  72. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  73. https://www.oreilly.com/topics/ai
  74. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  75. https://www.oreilly.com/people/c7521-shahin-farshchi
  76. http://oreilly.com/about/
  77. http://oreilly.com/work-with-us.html
  78. http://oreilly.com/careers/
  79. http://shop.oreilly.com/category/customer-service.do
  80. http://shop.oreilly.com/category/customer-service.do
  81. https://www.oreilly.com/ideas
  82. https://www.oreilly.com/topics/oreilly-learning
  83. https://www.oreilly.com/topics
  84. https://www.oreilly.com/all
  85. http://oreilly.com/terms/
  86. http://oreilly.com/privacy.html
  87. http://www.oreilly.com/about/editorial_independence.html
  88. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  90. https://www.oreilly.com/
  91. https://www.facebook.com/oreilly/
  92. https://twitter.com/oreillymedia
  93. https://www.youtube.com/user/oreillymedia
  94. https://plus.google.com/+oreillymedia
  95. https://www.linkedin.com/company/oreilly-media
  96. https://www.oreilly.com/
