   #[1]chris mccormick

[2]chris mccormick    [3]about    [4]tutorials    [5]archive

id97 tutorial - the skip-gram model

   19 apr 2016

   this tutorial covers the skip gram neural network architecture for
   id97. my intention with this tutorial was to skip over the usual
   introductory and abstract insights about id97, and get into more of
   the details. specifically here i   m diving into the skip gram neural
   network model.

the model

   the skip-gram neural network model is actually surprisingly simple in
   its most basic form; i think it   s all of the little tweaks and
   enhancements that start to clutter the explanation.

   let   s start with a high-level insight about where we   re going. id97
   uses a trick you may have seen elsewhere in machine learning. we   re
   going to train a simple neural network with a single hidden layer to
   perform a certain task, but then we   re not actually going to use that
   neural network for the task we trained it on! instead, the goal is
   actually just to learn the weights of the hidden layer   we   ll see that
   these weights are actually the    word vectors    that we   re trying to
   learn.
   another place you may have seen this trick is in unsupervised feature
   learning, where you train an auto-encoder to compress an input vector
   in the hidden layer, and decompress it back to the original in the
   output layer. after training it, you strip off the output layer (the
   decompression step) and just use the hidden layer--it's a trick for
   learning good image features without having labeled training data.

the fake task

   so now we need to talk about this    fake    task that we   re going to build
   the neural network to perform, and then we   ll come back later to how
   this indirectly gives us those word vectors that we are really after.

   we   re going to train the neural network to do the following. given a
   specific word in the middle of a sentence (the input word), look at the
   words nearby and pick one at random. the network is going to tell us
   the id203 for every word in our vocabulary of being the    nearby
   word    that we chose.
   when i say "nearby", there is actually a "window size" parameter to the
   algorithm. a typical window size might be 5, meaning 5 words behind and
   5 words ahead (10 in total).

   the output probabilities are going to relate to how likely it is find
   each vocabulary word nearby our input word. for example, if you gave
   the trained network the input word    soviet   , the output probabilities
   are going to be much higher for words like    union    and    russia    than
   for unrelated words like    watermelon    and    kangaroo   .

   we   ll train the neural network to do this by feeding it word pairs
   found in our training documents. the below example shows some of the
   training samples (word pairs) we would take from the sentence    the
   quick brown fox jumps over the lazy dog.    i   ve used a small window size
   of 2 just for the example. the word highlighted in blue is the input
   word.

   [6]training data

   the network is going to learn the statistics from the number of times
   each pairing shows up. so, for example, the network is probably going
   to get many more training samples of (   soviet   ,    union   ) than it is of
   (   soviet   ,    sasquatch   ). when the training is finished, if you give it
   the word    soviet    as input, then it will output a much higher
   id203 for    union    or    russia    than it will for    sasquatch   .

   [7]the inner workings of id97

model details

   so how is this all represented?

   first of all, you know you can   t feed a word just as a text string to a
   neural network, so we need a way to represent the words to the network.
   to do this, we first build a vocabulary of words from our training
   documents   let   s say we have a vocabulary of 10,000 unique words.

   we   re going to represent an input word like    ants    as a one-hot vector.
   this vector will have 10,000 components (one for every word in our
   vocabulary) and we   ll place a    1    in the position corresponding to the
   word    ants   , and 0s in all of the other positions.

   the output of the network is a single vector (also with 10,000
   components) containing, for every word in our vocabulary, the
   id203 that a randomly selected nearby word is that vocabulary
   word.

   here   s the architecture of our neural network.

   [8]skip-gram neural network architecture

   there is no activation function on the hidden layer neurons, but the
   output neurons use softmax. we   ll come back to this later.

   when training this network on word pairs, the input is a one-hot vector
   representing the input word and the training output is also a one-hot
   vector representing the output word. but when you evaluate the trained
   network on an input word, the output vector will actually be a
   id203 distribution (i.e., a bunch of floating point values, not a
   one-hot vector).

the hidden layer

   for our example, we   re going to say that we   re learning word vectors
   with 300 features. so the hidden layer is going to be represented by a
   weight matrix with 10,000 rows (one for every word in our vocabulary)
   and 300 columns (one for every hidden neuron).
   300 features is what google used in their published model trained on
   the google news dataset (you can download it from [9]here). the number
   of features is a "hyper parameter" that you would just have to tune to
   your application (that is, try different values and see what yields the
   best results).

   if you look at the rows of this weight matrix, these are actually what
   will be our word vectors!

   [10]hidden layer weight matrix

   so the end goal of all of this is really just to learn this hidden
   layer weight matrix     the output layer we   ll just toss when we   re done!

   let   s get back, though, to working through the definition of this model
   that we   re going to train.

   now, you might be asking yourself      that one-hot vector is almost all
   zeros    what   s the effect of that?    if you multiply a 1 x 10,000 one-hot
   vector by a 10,000 x 300 matrix, it will effectively just select the
   matrix row corresponding to the    1   . here   s a small example to give you
   a visual.

   [11]effect of id127 with a one-hot vector

   this means that the hidden layer of this model is really just operating
   as a lookup table. the output of the hidden layer is just the    word
   vector    for the input word.

the output layer

   the 1 x 300 word vector for    ants    then gets fed to the output layer.
   the output layer is a softmax regression classifier. there   s an
   in-depth tutorial on softmax regression [12]here, but the gist of it is
   that each output neuron (one per word in our vocabulary!) will produce
   an output between 0 and 1, and the sum of all these output values will
   add up to 1.

   specifically, each output neuron has a weight vector which it
   multiplies against the word vector from the hidden layer, then it
   applies the function exp(x) to the result. finally, in order to get the
   outputs to sum up to 1, we divide this result by the sum of the results
   from all 10,000 output nodes.

   here   s an illustration of calculating the output of the output neuron
   for the word    car   .

   [13]behavior of the output neuron
   note that neural network does not know anything about the offset of the
   output word relative to the input word. it does not learn a different
   set of probabilities for the word before the input versus the word
   after. to understand the implication, let's say that in our training
   corpus, every single occurrence of the word 'york' is preceded by the
   word 'new'. that is, at least according to the training data, there is
   a 100% id203 that 'new' will be in the vicinity of 'york'.
   however, if we take the 10 words in the vicinity of 'york' and randomly
   pick one of them, the id203 of it being 'new' is not 100%; you
   may have picked one of the other words in the vicinity.

intuition

   ok, are you ready for an exciting bit of insight into this network?

   if two different words have very similar    contexts    (that is, what
   words are likely to appear around them), then our model needs to output
   very similar results for these two words. and one way for the network
   to output similar context predictions for these two words is if the
   word vectors are similar. so, if two words have similar contexts, then
   our network is motivated to learn similar word vectors for these two
   words! ta da!

   and what does it mean for two words to have similar contexts? i think
   you could expect that synonyms like    intelligent    and    smart    would
   have very similar contexts. or that words that are related, like
      engine    and    transmission   , would probably have similar contexts as
   well.

   this can also handle id30 for you     the network will likely learn
   similar word vectors for the words    ant    and    ants    because these
   should have similar contexts.

next up

   you may have noticed that the skip-gram neural network contains a huge
   number of weights    for our example with 300 features and a vocab of
   10,000 words, that   s 3m weights in the hidden layer and output layer
   each! training this on a large dataset would be prohibitive, so the
   id97 authors introduced a number of tweaks to make training
   feasible. these are covered in [14]part 2 of this tutorial.

other resources

   [15]the inner workings of id97

   did you know that the id97 model can also be applied to non-text
   data for recommender systems and ad targeting? instead of learning
   vectors from a sequence of words, you can learn vectors from a sequence
   of user actions. read more about this in my new post [16]here.

cite

   mccormick, c. (2016, april 19). id97 tutorial - the skip-gram
   model. retrieved from http://www.mccormickml.com
   [ins: :ins]
   please enable javascript to view the [17]comments powered by disqus.

related posts

     * [18]the inner workings of id97 12 mar 2019
     * [19]applying id97 to recommenders and advertising 15 jun 2018
     * [20]product quantizers for id92 tutorial part 2 22 oct 2017

      2019. all rights reserved.

references

   1. http://mccormickml.com/atom.xml
   2. http://mccormickml.com/
   3. http://mccormickml.com/about/
   4. http://mccormickml.com/tutorials/
   5. http://mccormickml.com/archive/
   6. http://mccormickml.com/assets/id97/training_data.png
   7. http://bit.ly/2fuhcvx
   8. http://mccormickml.com/assets/id97/skip_gram_net_arch.png
   9. https://code.google.com/archive/p/id97/
  10. http://mccormickml.com/assets/id97/id97_weight_matrix_lookup_table.png
  11. http://mccormickml.com/assets/id97/matrix_mult_w_one_hot.png
  12. http://ufldl.stanford.edu/tutorial/supervised/softmaxregression/
  13. http://mccormickml.com/assets/id97/output_weights_function.png
  14. http://mccormickml.com/2017/01/11/id97-tutorial-part-2-negative-sampling/
  15. http://bit.ly/2fuhcvx
  16. http://mccormickml.com/2018/06/15/applying-id97-to-recommenders-and-advertising/
  17. https://disqus.com/?ref_noscript
  18. http://mccormickml.com/2019/03/12/the-inner-workings-of-id97/
  19. http://mccormickml.com/2018/06/15/applying-id97-to-recommenders-and-advertising/
  20. http://mccormickml.com/2017/10/22/product-quantizer-tutorial-part-2/
