   #[1]chris mccormick

[2]chris mccormick    [3]about    [4]tutorials    [5]archive

interpreting lsi document similarity

   04 nov 2016

   in this post i   m sharing a technique i   ve found for showing which words
   in a piece of text contribute most to its similarity with another piece
   of text when using id45 (lsi) to represent the two
   documents. this has proven valuable to me in debugging bad search
   results from    concept search    using lsi. you   ll find the equations for
   the technique as well as example python code.

my fun nlp project

   i   ve been having a lot of fun playing with id45
   (lsi) on a personal project. i   m working on making my personal
   journals, as well as some books i   ve read, searchable using lsi. that
   is, i can take one of my journal entries, and search for other journal
   entries (or even paragraphs from one of the books) that are
   conceptually similar.

   i   m basing the project on the awesome    id96    package gensim
   in python. i   m sharing the fundamental code for this project (just not
   my journals :-p) on github in a project i   ve called [6]simsearch.

   it also contains a working example based on the public domain work
   matthew henry   s commentary on the bible (which comes from the early
   1700s!). i   ll show you some of that example a little farther down.

   so far, though, the quality of the search results with my journals has
   been pretty hit-or-miss. for the really bad results, i   m left
   wondering, what went wrong?! why does lsi think these two pieces of
   text are related? how can i interpret this match?

impact of each word on similarity

   turns out it   s possible to look at the impact of individual words on
   the total lsi similarity, allowing you to interpret the results some.

   first, a little refresher on how lsi works. the first step in comparing
   the two pieces of text is to produce tf-idf vectors for them, which
   contain one element per word in the vocabulary. these tf-idf vectors
   are then projected down to, e.g., 100 topics with lsi. finally, the two
   lsi vectors are compared using cosine similarity, which produces a
   value between 0.0 and 1.0.

   [7]tf-idf conversion

   [8]lsi projection

   given that the tf-idf vectors contain a separate component for each
   word, it seemed reasonable to me to ask,    how much does each word
   contribute, positively or negatively, to the final similarity value?   

   at first, i made some reckless attempts at modifying the code to give
   me what i wanted, but it became clear that the math wasn   t right   stuff
   wasn   t adding up right. so i had to bite the bullet and do some actual
   math :).

   originally i was hoping to calculate, for example, how the word
      sabbath    contributes to the total similarity, independent of which
   document it comes from. turns out you can   t do exactly this, but you
   can do something close that i   m content with.

   instead of asking how the word    sabbath    in both documents contributes
   to the total similarity, we can instead ask, how does the word
      sabbath    in document 1 contribute to its similarity with document 2?

   i   ve shared the equations for this technique down in the next section.
   but first let   s look at the working result!

working example

   in my simsearch project, i   ve included an example with matthew henry   s
   commentary on the bible. i take a section of the commentary on genesis
   2 which talks about the seventh day of creation, when god rested, and
   use this to search against the rest of the commentary. the top result
   is a great match   it   s commentary on exodus 20, where god gives moses
   the fourth commandment to remember the sabbath, a day of rest.
   definitely conceptually similar.

   using the technique i worked out, you can see how each word contributes
   to the total similarity value of 0.954 between these two pieces of
   text.

   here are the top 10 words in    document 1    (the commentary on genesis)
   that contribute most to the similarity. if you add up all the terms,
   they sum to 0.954.
total similarity: 0.954
breakdown by word:
     sabbath    0.354
         day    0.315
      honour    0.032
        work    0.027
        rest    0.027
        holy    0.024
        week    0.015
     blessed    0.014
     seventh    0.013
        days    0.012
         ...

   you can do the same for    document 2    (the commentary on the ten
   commandments). again, if you sum all terms you get the total similarity
   of 0.954.
total similarity: 0.954
breakdown by word:
     sabbath    0.423
         day    0.250
        work    0.034
     seventh    0.032
        rest    0.017
        days    0.016
     blessed    0.016
        holy    0.010
    blessing    0.009
  sanctified    0.009
         ...

   note how the results are a little different taken from the perspective
   of document 1 vs. document 2. this is the compromise i mentioned
   earlier   you have to look at the two documents separately.

equation for the technique

   first, let   s define our variables.
     * \( x^{(1)} \) and \( x^{(2)} \) are the tf-idf vectors representing
       the two documents.
     * \( z^{(1)} \) and \( z^{(2)} \) are the corresponding lsi vectors
       for these two documents.

   to cut down on the number of variables, let   s assume that we have a
   vocabulary of 5,000 words (so our tf-idf vectors are 5,000 components
   long), and that we are using lsi with 100 topics (so our lsi vectors
   are 100 components long).

   to calculate how much word \( j \) from document 1 contributes to the
   total similarity, we can use the following equation.
     * \( u \) is the lsi project matrix which is [100 x 5,000].
     * \( u_{*j} \) is the column for the word \( j \). this is [100 x 1]
     * \( x_{j}^{(1)} \) is the tf-idf value in document 1 for word \( j
       \).
     * \( z^{(2)} \) is the [100 x 1] lsi vector for document 2.
     * \( \left| z^{(1)} \right| \) and \( \left| z^{(2)} \right| \) are
       the magnitudes of the two lsi vectors (recall that the cosine
       similarity involves normalizing the lsi vectors   dividing them by
       their magnitudes).

derivation

   to convert the tf-idf vector into an lsi vector, we just take the
   product of the [100 x 5,000] lsi projection matrix \( u \) and the
   [5,000 x 1] tfidf vector \( x \):

   we want to look at the individual contribution of each word to our
   final similarity, so let   s expand the dot-product into the element-wise
   equation:
   for all of these equations, i will use `i` to iterate over the 100
   topics in the lsi vectors, and `j` to iterate over the 5,000 words in
   the tf-idf vectors.

   to calculate the similarity between our two documents, we compare their
   lsi vectors using the cosine similarity.

   the cosine similarity between the vectors is found by normalizing them
   and taking their dot-product:

   let   s expand that dot product to see the element-wise operations:

   this equation would allow us to see the similarity contribution for
   each of the 100 topics. however, what we really want is the similarity
   contribution from each word.

   time to make things a little nasty. let   s substitute in our equation
   for \( z_{i} \) to put this equation in terms of the original tf-idf
   vectors.

   what i originally wanted to do here was to see, for example, how the
   word    sabbath    contributes to the total similarity, independent of
   which document it comes from. in order to do that, i would need to take
   the above equation and find a way to express it as a sum over \( j \).
   that way i could separate it into 5,000 terms, each term representing
   the similarity contribution from the corresponding word.

   problem is, i don   t think that   s possible. you would need to
   consolidate those two sums over \( j \). but    product of sums    is not
   the same as the    sum of products   .

   there is an alternate solution, though, that seems perfectly acceptable
   to me. instead of asking how the word    sabbath    in both documents
   contributes to the total similarity, we can instead ask, how does the
   word    sabbath    in document 1 contribute to its similarity with document
   2?

   we   re going to take a step backwards, and remove \( x^{(2)} \) from the
   equation.

   by the distributive property, we can then move the \( z_{i}^{(2)} \)
   term into the sum:

   finally, by the commutative property of addition, we   re allowed to
   switch the order of those two sums:

   we did it! we   ve managed to express the similarity between documents 1
   and 2 as a sum of 5,000 terms. now we can sort these terms to see which
   words in document 1 are contributing most to the total similarity.

   if you want to look at just the contribution from a specific word \( j
   \) from document 1:

   or, in vector form:

sample code

   you can find all of the python code for the example i   ve discussed in
   my [9]simsearch project on github. follow the instructions in the
   readme to get it going. note that there are a few dependencies you may
   have to grab if you don   t already have them, such as gensim and nltk.
   [ins: :ins]
   please enable javascript to view the [10]comments powered by disqus.

related posts

     * [11]the inner workings of id97 12 mar 2019
     * [12]applying id97 to recommenders and advertising 15 jun 2018
     * [13]product quantizers for id92 tutorial part 2 22 oct 2017

      2019. all rights reserved.

references

   1. http://mccormickml.com/atom.xml
   2. http://mccormickml.com/
   3. http://mccormickml.com/about/
   4. http://mccormickml.com/tutorials/
   5. http://mccormickml.com/archive/
   6. https://github.com/chrisjmccormick/simsearch
   7. http://mccormickml.com/assets/lsi/tf-idf_conversion.png
   8. http://mccormickml.com/assets/lsi/lsi_projection.png
   9. https://github.com/chrisjmccormick/simsearch
  10. https://disqus.com/?ref_noscript
  11. http://mccormickml.com/2019/03/12/the-inner-workings-of-id97/
  12. http://mccormickml.com/2018/06/15/applying-id97-to-recommenders-and-advertising/
  13. http://mccormickml.com/2017/10/22/product-quantizer-tutorial-part-2/
