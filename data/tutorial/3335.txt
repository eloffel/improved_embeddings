   #[1]chris mccormick

[2]chris mccormick    [3]about    [4]tutorials    [5]archive

id97 resources

   27 apr 2016

   while researching id97, i came across a lot of different resources
   of varying usefullness, so i thought i   d share my collection of links
   and notes on what they contain.
     * [6]original papers & resources from google team
          + [7]efficient estimation of word representations in vector
            space
          + [8]distributed representations of words and phrases and their
            compositionality
          + [9]presentation on id97
          + [10]c code implementation
     * [11]tutorials
          + [12]alex minnaar   s tutorials
          + [13]kaggle id97 tutorial
          + [14]folgert karsdorp   s id97 tutorial
          + [15]discussions on quora
     * [16]implementations
     * [17]my own stuff

original papers & resources from google team

   id97 was presented in two initial papers released within a month of
   each other. the original authors are a team of researchers from google.

efficient estimation of word representations in vector space

   [18]link to paper

   this was the first paper, dated september 7th, 2013.

   this paper introduces the continuous bag of words (cbow) and skip-gram
   models. however, don   t expect a particularly thorough description of
   these models in this paper   

   i believe the reason for this is that these two new models are
   presented more as modifications to previously existing models for
   learning word vectors. some of the terminology and concepts in this
   id97 paper come from these past papers and are not redifined in
   google   s paper.

   a good example are the labels    projection layer    and    hidden layer   
   which come from the    nnlm    model. the term    projection layer    is used
   to refer to a middle layer of the neural network with no activation
   function, whereas    hidden layer    implies a non-linear activation.

distributed representations of words and phrases and their compositionality

   [19]link to paper

   this was a follow-up paper, dated october 16th, 2013.

   this paper adds a few more innovations which address the high compute
   cost of training the skip-gram model on a large dataset. these added
   tweaks are fundamental to the id97 algorithm, and are implemented
   in google   s c version as well as the python implementation in gensim.

   these innovations are:
    1. subsampling common words (that is, eliminating some training
       samples).
    2.    negative sampling    - a modification of the optimization objective
       which causes each training sample to update only a small percentage
       of the model   s weights.

   additionally, they point out the value in recognizing common    phrases   
   and treating them as single words in the model (e.g.,    united_states   
   or    new_york   ).

presentation on id97

   [20]link to presentation

   this was presented december 9th, 2013 at nips 2013 by tomas mikolov
   from google.

   i think this is mainly a re-hash of the content in the two papers.
   seeing it presented differently may help you pull out some additional
   insights, though.

c code implementation

   [21]link to code

   the above link is to the home page for google   s own id97
   implementation in c.

   you can also find here some pre-trained models that they have provided.
   note that it   s possible to load these pre-trained models into gensim if
   you want to work with them in python.

tutorials

alex minnaar   s tutorials

   the best tutorials i found online were done by [22]alex minnaar.

   he   s since taken the tutorials down, but i have pdf copies here:
     * [23]part i - the skip-gram model
     * [24]part ii - continuous bag-of-words model

kaggle id97 tutorial

   [25]link to tutorial

   this is pretty cool. it   s a kaggle competition that   s really just a
   python tutorial to teach you about using id97 with gensim. it   s
   well written and will walk you through all of the steps carefully. it
   does very little to explain the algorithms used, but is great on the
   practical implementation side.

   it uses a id31 task (on the imdb movie review dataset) as
   an example project. while the tutorial is great for showing you how to
   get set up with gensim and even train your own id97 model on the
   data, you   ll discover that it essentially fails at applying id97
   effectively on the example task of id31! to get good
   results on the imdb dataset, you   ll want to check out google   s doc2vec
   technique (which isn   t covered in this tutorial).

   here   s what the tutorial covers.

   part 1:
     * cleaning and tokening the text data.
     * vectorizing the documents using word counts.
     * classification using a id79.

   part 2:
     * setting up gensim
     * training a id97 model (learning word vectors from the dataset)
       using gensim

   part 3:
     * this section attempts two rather unsuccessful ways of applying the
       word vectors to create vector representations of each review.
       neither manages to outperform the simpler word-count approach from
       part 1.
          + creating vector representations of each movie review by
            averaging its word vectors.
          + id91 the word vectors to identify sets of synonyms, then
            using the word-count approach, but this time combining
            synonyms into a single bucket.

   part 4:
     * points to google   s doc2vec as a superior solution to this task, but
       doesn   t provide implementation details.

folgert karsdorp   s id97 tutorial

   [26]link to tutorial

   i haven   t read this tutorial in depth    it covers the continuous bag of
   words model (instead of the skip-gram model). it even includes some of
   the backprop equations.

discussions on quora

     * [27]https://www.quora.com/what-are-the-continuous-bag-of-words-and-
       skip-gram-architectures-in-laymans-terms
     * [28]https://www.quora.com/how-does-id97-work
     * [29]https://www.quora.com/what-are-some-interesting-id97-result
       s/answer/omer-levy

implementations

   the below implementations also include some tutorials; i haven   t gone
   through them in detail yet.
     * id97 and doc2vec in python in gensim [30]here and [31]here
     * id97 in java in [32]deeplearning4j
     * java version from [33]medallia
     * id97 implementation in [34]spark mllib
     * id97 implementation / tutorial in google   s [35]tensorflow

my own stuff

     * i have my own tutorial on the skip-gram model of id97 [36]here.
     * [37]part 2 of my tutorial covers subsampling of frequent words and
       the negative sampling technique.
     * i created a project called [38]inspec_id97 that uses gensim in
       python to load up google   s large pre-trained model, and inspect
       some of the details of the vocabulary.
     * i   m working on a matlab implementation of id97,
       [39]id97_matlab. my goal is less about practical useage and
       more about understanding the model. for now, it doesn   t support the
       most important part   actually training a id97 model. what it
       does do currently is allow you to play with a paired-down (or,
       really, cleaned-up!) version of google   s pre-trained model in
       matlab.

   [ins: :ins]
   please enable javascript to view the [40]comments powered by disqus.

related posts

     * [41]the inner workings of id97 12 mar 2019
     * [42]applying id97 to recommenders and advertising 15 jun 2018
     * [43]product quantizers for id92 tutorial part 2 22 oct 2017

      2019. all rights reserved.

references

   1. http://mccormickml.com/atom.xml
   2. http://mccormickml.com/
   3. http://mccormickml.com/about/
   4. http://mccormickml.com/tutorials/
   5. http://mccormickml.com/archive/
   6. http://mccormickml.com/2016/04/27/id97-resources/#original-papers--resources-from-google-team
   7. http://mccormickml.com/2016/04/27/id97-resources/#efficient-estimation-of-word-representations-in-vector-space
   8. http://mccormickml.com/2016/04/27/id97-resources/#distributed-representations-of-words-and-phrases-and-their-compositionality
   9. http://mccormickml.com/2016/04/27/id97-resources/#presentation-on-id97
  10. http://mccormickml.com/2016/04/27/id97-resources/#c-code-implementation
  11. http://mccormickml.com/2016/04/27/id97-resources/#tutorials
  12. http://mccormickml.com/2016/04/27/id97-resources/#alex-minnaars-tutorials
  13. http://mccormickml.com/2016/04/27/id97-resources/#kaggle-id97-tutorial
  14. http://mccormickml.com/2016/04/27/id97-resources/#folgert-karsdorps-id97-tutorial
  15. http://mccormickml.com/2016/04/27/id97-resources/#discussions-on-quora
  16. http://mccormickml.com/2016/04/27/id97-resources/#implementations
  17. http://mccormickml.com/2016/04/27/id97-resources/#my-own-stuff
  18. http://arxiv.org/pdf/1301.3781.pdf
  19. http://arxiv.org/pdf/1310.4546.pdf
  20. https://docs.google.com/file/d/0b7xkcwpi5kdyrwrnd1rzwxq2twc/edit
  21. https://code.google.com/archive/p/id97/
  22. http://alexminnaar.com/
  23. http://mccormickml.com/assets/id97/alex_minnaar_id97_tutorial_part_i_the_skip-gram_model.pdf
  24. http://mccormickml.com/assets/id97/alex_minnaar_id97_tutorial_part_ii_the_continuous_bag-of-words_model.pdf
  25. https://www.kaggle.com/c/id97-nlp-tutorial/
  26. http://www.folgertkarsdorp.nl/id97-an-introduction/
  27. https://www.quora.com/what-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms
  28. https://www.quora.com/how-does-id97-work
  29. https://www.quora.com/what-are-some-interesting-id97-results/answer/omer-levy
  30. http://radimrehurek.com/2013/09/deep-learning-with-id97-and-gensim/
  31. http://rare-technologies.com/id97-tutorial/
  32. http://deeplearning4j.org/id97.html
  33. https://github.com/medallia/id97java
  34. https://spark.apache.org/docs/latest/mllib-feature-extraction.html#id97
  35. https://www.tensorflow.org/versions/r0.8/tutorials/id97/index.html
  36. http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-gram-model/
  37. http://mccormickml.com/2017/01/11/id97-tutorial-part-2-negative-sampling/
  38. https://github.com/chrisjmccormick/inspect_id97
  39. https://github.com/chrisjmccormick/id97_matlab
  40. https://disqus.com/?ref_noscript
  41. http://mccormickml.com/2019/03/12/the-inner-workings-of-id97/
  42. http://mccormickml.com/2018/06/15/applying-id97-to-recommenders-and-advertising/
  43. http://mccormickml.com/2017/10/22/product-quantizer-tutorial-part-2/
