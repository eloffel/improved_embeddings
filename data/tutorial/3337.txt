   #[1]chris mccormick

[2]chris mccormick    [3]about    [4]tutorials    [5]archive

latent semantic analysis (lsa) for text classification tutorial

   25 mar 2016
   in this post i'll provide a tutorial of latent semantic analysis as
   well as some python example code that shows the technique in action.

why lsa?

   latent semantic analysis is a technique for creating a vector
   representation of a document. having a vector representation of a
   document gives you a way to compare documents for their similarity by
   calculating the distance between the vectors. this in turn means you
   can do handy things like classifying documents to determine which of a
   set of known topics they most likely belong to.

   classification implies you have some known topics that you want to
   group documents into, and that you have some labelled training data. if
   you want to identify natural groupings of the documents without any
   labelled data, you can use id91 (see my post on id91 with
   lsa [6]here).

tf-idf

   the first step in lsa is actually a separate algorithm that you may
   already be familiar with. it   s called [7]term frequency-inverse
   document frequency, or tf-idf for short.

   tf-idf is pretty simple and i won   t go into it here, but the gist of it
   is that each position in the vector corresponds to a different word,
   and you represent a document by counting the number of times each word
   appears. additionally, you normalize each of the word counts by the
   frequency of that word in your overall document collection, to give
   less frequent terms more weight.

   there   s some thorough material on tf-idf in the stanford nlp course
   available on youtube [8]here   specifically, check out the lectures 19-1
   to 19-7. or if you prefer some (dense) reading, you can check out the
   tf-idf chapter of the stanford nlp textbook [9]here.

lsa

   [10]latent semantic analysis takes tf-idf one step further.
   side note: "latent semantic analysis (lsa)" and "latent semantic
   indexing (lsi)" are the same thing, with the latter name being used
   sometimes when referring specifically to indexing a collection of
   documents for search ("information retrieval").

   lsa is quite simple, you just use svd to perform dimensionality
   reduction on the tf-idf vectors   that   s really all there is to it!
   if you're unfamiliar with id84, this topic was
   covered well in the machine learning course on coursera. you can also
   find the lecture on youtube [11]here.

   you might think to do this even if you had never heard of    lsa      the
   tf-idf vectors tend to be long and unwieldy since they have one
   component for every word in the vocabulary. for instance, in my example
   python code, these vectors have 10,000 components. so dimensionality
   reduction makes them more manageable for further operations like
   id91 or classification.

   however, the svd step does more than just reduce the computational
   load   you are trading a large number of features for a smaller set of
   better features.

   what makes the lsa features better? i think the challenging thing with
   interpereting lsa is that you can talk about the behaviors that it is
   theoretically capable of doing, but ultimately what it does is dictated
   by the mathematical operations of svd.

   for example. a linear combination of terms is capable of handling
   pysnonyms: if you assign the words    car    and    automobile    the same
   weight, then they will contribute equally to the resulting lsa
   component, and it doesn   t matter which term the author uses.

   you can find a little more discussion of the interpretation of lsa
   [12]here. also, the python code associated with this post performs some
   inspection of the lsa results to try to gain some intuition.

lsa python code

   note: if you're less interested in learning lsa and just want to use
   it, you might consider checking out the nice [13]gensim package in
   python, it's built specifically for working with topic-modeling
   techniques like lsa.

   i implemented an example of document classification with lsa in python
   using scikit-learn. my code is available on github, you can either
   visit the project page [14]here, or download the source [15]directly.

   scikit-learn already includes a [16]document classification example.
   however, that example uses plain tf-idf rather than lsa, and is geared
   towards demonstrating batch training on large datasets. still, i
   borrowed code from that example for things like retrieving the reuters
   dataset.

   i wanted to put the emphasis on the feature extraction and not the
   classifier, so i used simple [17]id92 classification with k = 5
   (majority wins).

inspecting lsa

   a little background on this reuters dataset. these are news articles
   that were sent over the reuters newswire in 1987. the dataset contains
   about 100 categories such as    mergers and acquisitions   ,    interset
   rates   ,    wheat   ,    silver    etc. articles can be assigned multiple
   categories. the distribution of articles among categories is highly
   non-uniform; for example, the    earnings    category contains 2,709
   articles. and 75 of the categories contain less than 10 docs each!

   armed with that background, let   s see what lsa is learning from the
   dataset.

   you can look at component 0 of the svd matrix, and look at the terms
   which are given the highest weight by this component.

   top 10 terms in component 0

   these terms are all very common to articles in the    earnings    category,
   which seem to be very terse. here are some of the abbreviations used:
     * vs -    versus   
     * cts -    cents   
     * mln -    million   
     * shr -    share   
     * dlrs -    dollars   

   here   s an example earnings article from the dataset to give you some
   context:
cobanco inc cbco> year net

shr 34 cts vs 1.19 dlrs net 807,000 vs 2,858,000 assets 510.2 mln vs 479.7 mln d
eposits 472.3 mln vs 440.3 mln loans 299.2 mln vs 327.2 mln note: 4th qtr not av
ailable. year includes 1985 extraordinary gain from tax carry forward of 132,000
 dlrs, or five cts per shr. reuter

   [ins: :ins]
   please enable javascript to view the [18]comments powered by disqus.

related posts

     * [19]the inner workings of id97 12 mar 2019
     * [20]applying id97 to recommenders and advertising 15 jun 2018
     * [21]product quantizers for id92 tutorial part 2 22 oct 2017

      2019. all rights reserved.

references

   1. http://mccormickml.com/atom.xml
   2. http://mccormickml.com/
   3. http://mccormickml.com/about/
   4. http://mccormickml.com/tutorials/
   5. http://mccormickml.com/archive/
   6. https://chrisjmccormick.wordpress.com/2015/08/05/document-id91-example-in-scikit-learn/
   7. https://en.wikipedia.org/wiki/tf   idf
   8. https://www.youtube.com/watch?v=5gz3hp217io&index=80&list=pl6397e4b26d00a269
   9. http://nlp.stanford.edu/ir-book/html/htid113dition/scoring-term-weighting-and-the-vector-space-model-1.html
  10. https://en.wikipedia.org/wiki/latent_semantic_analysis
  11. https://www.youtube.com/watch?v=n5ynbdhqngu
  12. https://en.wikipedia.org/wiki/latent_semantic_analysis#rank_lowering
  13. https://radimrehurek.com/gensim/
  14. https://github.com/chrisjmccormick/lsa_classification
  15. https://github.com/chrisjmccormick/lsa_classification/archive/master.zip
  16. http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html
  17. http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighborsclassifier.html
  18. https://disqus.com/?ref_noscript
  19. http://mccormickml.com/2019/03/12/the-inner-workings-of-id97/
  20. http://mccormickml.com/2018/06/15/applying-id97-to-recommenders-and-advertising/
  21. http://mccormickml.com/2017/10/22/product-quantizer-tutorial-part-2/
