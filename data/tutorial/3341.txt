   #[1]chris mccormick

[2]chris mccormick    [3]about    [4]tutorials    [5]archive

deep learning tutorial - self-taught learning & deep networks

   24 jun 2014

   this post covers the sections of the tutorial titled    self-taught
   learning and unsupervised id171    and    building deep networks
   for classification   .

self-taught learning

   in the self-taught learning section, we train an auto-encoder on the
   digits 5 - 9, then use this auto-encoder as a feature extractor for
   recognizing the digits 0 - 4. the exercise takes the training data for
   the digits 0 - 4 and divides them in half, with one half used for
   training and the other used for testing.

   this is a straight-forward exercise that just involves combining some
   of the code that you   ve already written.

   note for octave users

   i did run into some memory issues on this exercise using octave.  there
   are a total of 29,404 examples of the digits 5 - 9 used to train the
   autoencoder, and my training code runs out of memory on this large of a
   dataset. to work around this, i randomly selected 10,000 examples using
      randperm   , and trained the autoencoder on just these 10k examples. i
   still got 98.28% accuracy overall, so this didn   t appear to affect the
   accuracy too dramatically.

deep networks

   in the deep networks exercise, you   ll be building a deep mlp which
   achieves impressive performance on the handwritten digits dataset.

   the deep network will consist of two stacked autoencoders with 200
   neurons each, followed by a softmax regression classifier on the
   output.

   the key step in this exercise will be the fine-tuning of the network to
   improve its overall accuracy. during the fine tuning step, you   ll be
   treating the network as a regular mlp network without the sparsity
   constraints on the hidden layers. in other words, you   ll be using
   ordinary backpropogation to train the network on the labeled dataset.

   note that the output layer (the softmax classifier) has already been
   optimized over the training set. the real intent of the    fine-tuning   
   step is to tweak the parameters in the autoencoder layers to improve
   performance. of course, the softmax parameters will be updated as well
   as we adjust the earlier layers.

   my implementation achieved 92.05% accuracy on the test set prior to
   fine-tuning, and 98.13% accuracy after fine-tuning. this is a pretty
   dramatic improvement! looking at the performance table on the mnist
   website, 92.05% is pretty poor. this seems to suggest that the
   auto-encoders alone aren   t fantastic feature extractors, and that the
   autoencoder technique just provides very good initial weight values to
   use in training the deep mlp. really, it   s the ordinary backprop
   training that produces the strong performance, the autoencoder training
   steps just give the backprop training a good head-start.

id26

   the [6]tutorial notes on id26 provide a nice summary of the
   equations you need to implement. however, there is a key difference,
   which is that we are using the logarithmic cost function instead of the
   mean-squared error.

   log cost

   when calculating the cost, use the same cost function which you used in
   the softmax regression exercise.

   this also affects the delta term for the output layer. using the mse
   cost, this term is given by:

   [7]incorrectoutputdelta

   but when using the log cost, we   ll drop that sigmoid prime term. see my
   equations below.

   bias terms

   one of the trickier bits of implementing back propagation is keeping
   track of the bias term. remember that in the notation the weight matrix
      w    does not include the bias term. the bias term is stored sepearately
   as the vector    b   .     theta   , however, includes both w and b.

   also, the starter code for the softmax regression exercise doesn   t seem
   to be set up to include a bias term. i implemented softmax regression
   without a bias term and it seems to work fine. it could be that the
   softmax classifier is able to learn its model without a bias term, or
   it could be that it   s just able to learn reasonable parameters even
   without the bias. in any case, if you have a bias term in your softmax
   parameters, you may need to modify my equations slightly.

   equations for gradients

   in the equations below,    m    is the number of training examples.

   i   ve found that, to vectorize matlab code, the best approach is to look
   at the dimensions of the matrices to ensure that the dimensions all
   line up. if the dimensions match up correctly, you   ve probably
   implemented the equation correctly. under each equation you   ll find a
   table showing the matrix dimensions [rows x columns] for each term in
   the equation.

   delta terms

   the first step is to calculate the delta terms, working backwards from
   the output layer.

   i got the equation for the output layer delta term from the backprop
   assignment in the coursera course on machine learning, where we also
   used the log cost.

   you could probably also derive this equation by working through the
   derivative of the cost function to verify this.

   the gradient checking code verified my implementation.

   [8]delta4_eq

   in calculating the layer 3 and layer 2 deltas, note that w does not
   include the bias term, and neither does a.

   [9]delta3_eq

   [10]delta2_eq

   gradients

   once you have the delta terms, you can calculate the gradient values.

   my softmax regression model doesn   t include a bias term, so i   m only
   showing the equation for gradients for w. also, note that we are
   applying id173 to the output layer, and so i   ve included the
   lambda term.

   [11]layer3_grads_eq

   for the hidden layers, i do have a bias term. also, we are not applying
   id173 to these layers, so there is no lambda term.

   [12]layer2_grads_eq

   [13]layer1_grads_eq

   note for octave users

   i am looking at purchasing matlab, and currently have a trial version
   which i used to complete this exercise.

   i think that you should still be able to complete this exercise using
   octave, but you may just need to use a smaller training set size for
   the auto-encoders. for example, you could try training on a random
   selection of 10,000 images instead of the full 60,000. the final
   accuracy of your system may not be as good, but you should still be
   able to implement it and verify it   s correctness using the gradient
   checking code they provide.
   [ins: :ins]
   please enable javascript to view the [14]comments powered by disqus.

related posts

     * [15]the inner workings of id97 12 mar 2019
     * [16]applying id97 to recommenders and advertising 15 jun 2018
     * [17]product quantizers for id92 tutorial part 2 22 oct 2017

      2019. all rights reserved.

references

   1. http://mccormickml.com/atom.xml
   2. http://mccormickml.com/
   3. http://mccormickml.com/about/
   4. http://mccormickml.com/tutorials/
   5. http://mccormickml.com/archive/
   6. http://ufldl.stanford.edu/wiki/index.php/id26_algorithm
   7. https://chrisjmccormick.files.wordpress.com/2014/06/incorrectoutputdelta.png
   8. https://chrisjmccormick.files.wordpress.com/2014/06/delta4_eq1.png
   9. https://chrisjmccormick.files.wordpress.com/2014/06/delta3_eq.png
  10. https://chrisjmccormick.files.wordpress.com/2014/06/delta2_eq.png
  11. https://chrisjmccormick.files.wordpress.com/2014/06/layer3_grads_eq.png
  12. https://chrisjmccormick.files.wordpress.com/2014/06/layer2_grads_eq.png
  13. https://chrisjmccormick.files.wordpress.com/2014/06/layer1_grads_eq.png
  14. https://disqus.com/?ref_noscript
  15. http://mccormickml.com/2019/03/12/the-inner-workings-of-id97/
  16. http://mccormickml.com/2018/06/15/applying-id97-to-recommenders-and-advertising/
  17. http://mccormickml.com/2017/10/22/product-quantizer-tutorial-part-2/
