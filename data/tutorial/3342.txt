   #[1]chris mccormick

[2]chris mccormick    [3]about    [4]tutorials    [5]archive

deep learning tutorial - softmax regression

   13 jun 2014

   softmax regression is a generalized form of id28 which
   can be used in multi-class classification problems where the classes
   are mutually exclusive. the hand-written digit dataset used in this
   tutorial is a perfect example. a softmax regression classifier trained
   on the hand written digits will output a separate id203 for each
   of the ten digits, and the probabilities will all add up to 1.

   softmax regression consists of ten linear classifiers of the form:

   [6]softmaxregression

   the output of this equation is a vector, with one value for each
   hand-written digit. the first component is the id203 that an
   input image of a digit, x, is the number    1    (belongs to class y = 1).

   the tutorial notes don   t show this classifier is derived, and the
   relationship between this equation and the original binary logistic
   regression classifier is certainly not obvious.

   one bit of intuition you can derive about it, though, is the
   id172. by dividing each component by the sum of all the
   components, we ensure that the sum of all the elements in the vector
   will always equal 1. this makes sense given that the classes are all
   mutually exclusive.

   below is the plot of e^x. there   s not much intuition to be derived from
   this plot, other than that the output of e^x is always positive.

   [7]graph_exponential

over-parameterization

   the tutorial makes the point that you could fix the vector of
   parameters for one of the 10 classifiers (say, theta_1) to a vector of
   all zeros, and the classifier would still be able to function by
   learning the parameters for the other 9 classifiers.

   how is this possible? if you were to set theta_1 to all zeros, then the
   first component of the un-normalized output vector would always be
   equal to 1, no matter what the input is.

   however, the id172 allows this component to take on different
   values depending on the un-normalized outputs of the other 9
   classifiers.

id28 as a specific case of softmax regression

   while the notes don   t provide a derivation of softmax regression, they
   do show how you can arrive at id28 by looking at the
   special case of softmax regression with two classes.

cost function

   below is the cost function (with weight decay) for softmax regression
   from the tutorial.

   [8]softmaxregression_cost

   the indicator function denoted by 1{y^(i) = j} means that only the
   output of the classifier corresponding to the correct class label is
   included in the cost. that is, when computing the cost for an example
   of the digit    4   , only the output of classifier 4 contributes to the
   cost.

   below is a plot of log(x). note that the input will only range from 0
   to 1. the costs in this range are all negative, so note the negative
   sign at the beginning of our cost function to account for this.

   [9]graphlogx

   the output of log(x) ranges from negative infinity to 0. if the
   classifier outputs 1 for the training example, then the cost is zero.
   the cost increases exponentially as the classifier   s output decreases
   towards 0. sounds like good behavior for our cost function!

gradients

   below is the gradient function from the tutorial.

   [10]softmaxregression_thetagrad

   note that this function computes the gradients for a single class j.
   the expression inside the parentheses evaluates to a single value
   between 0 and 1. this is multiplied by the vector x^(i) to get the
   weight updates for a single training example i and a single class j.

   let   s say that you evaluate the inner expression for every class and
   every training example, such that you have a matrix m which is
   [numclasses x numexamples].

   we want a gradient matrix    grad    which is [numclasses x inputsize]. we
   also have our data matrix    data    which is [inputsize x numexamples].

   to compute the    grad    matrix for all examples and all classes
   simultaneously, we compute

   grad = m * data   

   you can confirm that the dimensions match up:

   grad [numclasses x inputsize] = m [numclasses x numexamples] * data   
   [numexamples x inputsize]

   to calculate the final gradient matrix, you also need to take the
   average of these gradients by dividing them by the number of training
   examples. finally, you need to add in the id173 term of the
   gradient.

softmax regression exercise

   this exercise is considerably easier than the sparse auto-encoder.

   since the first step of calculating the cost is to evaluate the model
   over the training set, i recommend starting by completing the
      softmaxpredict.m    function. i modified my version of softmaxpredict to
   return both the class labels and the matrix of confidences so that i
   could call it from my    softmaxcost.m    function.

   note for octave users

   octave does not support    mex    files, so you will need to add the line
      options.usemex = false;    before calling minfunc in softmaxtrain.m.

   fortunately, memory was not an issue for this exercise   i was able to
   run the full 100 training iterations.
   [ins: :ins]
   please enable javascript to view the [11]comments powered by disqus.

related posts

     * [12]the inner workings of id97 12 mar 2019
     * [13]applying id97 to recommenders and advertising 15 jun 2018
     * [14]product quantizers for id92 tutorial part 2 22 oct 2017

      2019. all rights reserved.

references

   1. http://mccormickml.com/atom.xml
   2. http://mccormickml.com/
   3. http://mccormickml.com/about/
   4. http://mccormickml.com/tutorials/
   5. http://mccormickml.com/archive/
   6. http://chrisjmccormick.files.wordpress.com/2014/06/softmaxregression.png
   7. https://chrisjmccormick.files.wordpress.com/2014/06/graph_exponential.png
   8. https://chrisjmccormick.files.wordpress.com/2014/06/softmaxregression_cost.png
   9. https://chrisjmccormick.files.wordpress.com/2014/06/graphlogx.png
  10. https://chrisjmccormick.files.wordpress.com/2014/06/softmaxregression_thetagrad.png
  11. https://disqus.com/?ref_noscript
  12. http://mccormickml.com/2019/03/12/the-inner-workings-of-id97/
  13. http://mccormickml.com/2018/06/15/applying-id97-to-recommenders-and-advertising/
  14. http://mccormickml.com/2017/10/22/product-quantizer-tutorial-part-2/
