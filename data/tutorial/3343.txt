   #[1]chris mccormick

[2]chris mccormick    [3]about    [4]tutorials    [5]archive

deep learning tutorial - pca and whitening

   03 jun 2014

principal component analysis

   pca is a method for reducing the number of dimensions in the vectors in
   a dataset. essentially, you   re compressing the data by exploiting
   correlations between some of the dimensions.

covariance matrix

   pca starts with computing the covariance matrix. i found [6]this
   tutorial helpful for getting a basic understanding of covariance
   matrices (i only read a little bit of it to get the basic idea).

   the following equation is presented for computing the covariance
   matrix.

   [7]covariance

   note that the placement of the transpose operator creates a matrix
   here, not a single value.

   note that this function only computes the covariance matrix if the mean
   is zero. the proper function would be based on (x - mu)(x - mu)^t. for
   the tutorial example, the dataset has been adjusted to have a mean of
   0.

   for 2d data, here are the equations for each individual cell of the 2x2
   covariance matrix, so that you can get more of a feel for what each
   element represents.

   [8]covariancematrix

   if you subtract the means from the dataset ahead of time, then you can
   drop the    minus mu    terms from these equations. subtracting the means
   causes the dataset to be centered around (0, 0).

   the top-left corner is just a measure of how much the data varies along
   the x_1 dimension. similarly, the bottom-right corner is the variance
   in the x_2 dimension.

   the bottom-left and top-right corners are identical. these indicate the
   correlation between x_1 and x_2.

   if the data is mainly in quadrants one and three, then all of the x_1 *
   x_2 products are going to be positive, so there   s a
   positive correlation between x_1 and x_2.

   [9]positivecorrelation

   if the data is all in quadrants two and four, then the all of the
   products will be negative, so there   s a negative correlation between
   x_1 and x_2.

   [10]negativecorrelation

   if the data is evenly dispersed in all four quadrants, then the
   positive and negative products will cancel out, and the covariance will
   be roughly zero. this indicates that there is _no _correlation.

   [11]nocorrelation

   eigenvectors

   after calculating the covariance matrix for the dataset, the next step
   is to compute the  eigenvectors of the covariance matrix. i found
   [12]this tutorial helpful. again, i only skimmed it and got a high
   level understanding.

   the eigenvectors of the covariance matrix have the property that they
   point along the major directions of variation in the data.

   pca-u1.png

   why this is the case is beyond me. i suspect you   d have to be more
   intimately acquainted with how the eigenvectors are found in order to
   understand why they have this property. so i   m just taking it as a
   given for now.

projecting onto an eigenvector

   the tutorial explains that taking the dot product between a data point,
   x, and an eigen vector, u_1, gives you    the length (magnitude) of the
   projection of \textstyle x onto the vector \textstyle u_1 .    

   take a look at the wikipedia article on [13]scalar projection to help
   understand what this means.

   the resulting scalar value is a point along the line formed by the
   eigen vector.

   what   s actually involved, then, in reducing a 256 dimensional vector
   down to 50 dimensional vector? you will be taking the dot product
   between your 256-dimensional vector x and each of the top 50 eigen
   vectors.

   it was interesting to me to note that this is equivalent to evaluating
   a neural network with 256 inputs and 50 hidden units, but with no
   activation function on the hidden units (i.e., no sigmoid function).

whitening

   there are two things we are trying to accomplish with whitening:
    1. make the features less correlated with one another.
    2. give all of the features the same variance.

   whitening has two simple steps:
    1. project the dataset onto the eigenvectors. this rotates the dataset
       so that there is no correlation between the components.
    2. normalize the the dataset to have a variance of 1 for all
       components. this is done by simply dividing each component by the
       square root of its eigenvalue.

   i asked a neural network expect i   m connected with, [14]pavel
   skribtsov, for more of an explanation on why this technique is
   beneficial:

     "this is a common trick to simplify optimization process to find
     weights. if the input signal has correlating inputs (some linear
     dependency) then the [cost] function will tend to have "river-like"
     minima regions rather than minima points in weights space. as to
     input whitening - similar thing - if you don't do it - error
     function will tend to have non-symmetrical minima "caves" and since
     some training algorithms have equal speed of update for all weights
     - the minimization may tend to skip good places in narrow dimensions
     of the minima while trying to please the wider ones. so it does not
     directly relate to deep learning. if your optimization process
     converges well - you can skip this pre-processing."

pca in 2d exercise

   this exercise is pretty straightforward. a few notes, though:
     * note that you don   t need to adjust the data to have a mean of 0,
       it   s already close enough.
     * in step 1a, where it plots the eigen vectors, your plot area needs
       to be square in order for it to look right. mine was a rectangle at
       first (from a previous plot) and it threw me off   it made the second
       eigen vector look wrong.
          + the command axis(   square   ) is supposed to do this, but for
            seem reason it gives my plot a 5:4 ratio, not 1:1. what a
            pain!

pca and whitening exercise

     * if you are using octave instead of matlab, there   s a modification
       you   ll need to make to line 93 of display_network.m. remove the
       arguments    erasemode    and    none   .
     * when subtracting the mean, the instructions say to calculate the
       mean per image, but the code says to calculate it per row (per
       pixel). this [15]section of the tutorial describes why they compute
       it per image for natural images.
     * you can use the command    colorbar    to add a color legend to the
       plot for the imagesc command.

   using 116 out of 144 principal components preserved 99% of the
   variance.

   here is the final output of my code, showing the original image patches
   and the whitened images.

   before whitening:

   [16]imagepatches_prewhitening

   after whitening:

   [17]imagepatches_withwhitening
   [ins: :ins]
   please enable javascript to view the [18]comments powered by disqus.

related posts

     * [19]the inner workings of id97 12 mar 2019
     * [20]applying id97 to recommenders and advertising 15 jun 2018
     * [21]product quantizers for id92 tutorial part 2 22 oct 2017

      2019. all rights reserved.

references

   1. http://mccormickml.com/atom.xml
   2. http://mccormickml.com/
   3. http://mccormickml.com/about/
   4. http://mccormickml.com/tutorials/
   5. http://mccormickml.com/archive/
   6. http://stattrek.com/matrix-algebra/covariance-matrix.aspx
   7. https://chrisjmccormick.files.wordpress.com/2014/06/covariance.png
   8. https://chrisjmccormick.files.wordpress.com/2014/06/covariancematrix.png
   9. https://chrisjmccormick.files.wordpress.com/2014/06/positivecorrelation.png
  10. https://chrisjmccormick.files.wordpress.com/2014/06/negativecorrelation.png
  11. https://chrisjmccormick.files.wordpress.com/2014/06/nocorrelation.png
  12. http://www.math.hmc.edu/calculus/tutorials/eigenstuff/
  13. http://en.wikipedia.org/wiki/scalar_projection
  14. http://www.pawlin.com/
  15. http://ufldl.stanford.edu/wiki/index.php/pca#pca_on_images
  16. https://chrisjmccormick.files.wordpress.com/2014/06/imagepatches_prewhitening.png
  17. https://chrisjmccormick.files.wordpress.com/2014/06/imagepatches_withwhitening.png
  18. https://disqus.com/?ref_noscript
  19. http://mccormickml.com/2019/03/12/the-inner-workings-of-id97/
  20. http://mccormickml.com/2018/06/15/applying-id97-to-recommenders-and-advertising/
  21. http://mccormickml.com/2017/10/22/product-quantizer-tutorial-part-2/
