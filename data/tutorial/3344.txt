   #[1]chris mccormick

[2]chris mccormick    [3]about    [4]tutorials    [5]archive

deep learning tutorial - sparse autoencoder

   30 may 2014

   this post contains my notes on the autoencoder section of stanford   s
   deep learning tutorial / cs294a. it also contains my notes on the
   sparse autoencoder exercise, which was easily the most challenging
   piece of matlab code i   ve ever written!!!

autoencoders and sparsity

     * autoencoder - by training a neural network to produce an output
       that   s identical to the input, but having fewer nodes in the hidden
       layer than in the input, you   ve built a tool for compressing the
       data.
          + going from the input to the hidden layer is the compression
            step. you take, e.g., a 100 element vector and compress it to
            a 50 element vector.
          + going from the hidden layer to the output layer is the
            decompression step. you take the 50 element vector and compute
            a 100 element vector that   s ideally close to the original
            input.
     * sparse activation - alternatively, you could allow for a large
       number of hidden units, but require that, for a given input, most
       of the hidden neurons only produce a very small activation.
          + for a given hidden node, it   s average activation value (over
            all the training samples) should be a small value close to
            zero, e.g., 0.5
          + a term is added to the cost function which increases the cost
            if the above is not true.

visualizing a trained autoencoder

   in this section, we   re trying to gain some insight into what the
   trained autoencoder neurons are looking for. for a given neuron, we
   want to figure out what input vector will cause the neuron to produce
   it   s largest response.

   that   s tricky, because really the answer is an input vector whose
   components are all set to either positive or negative infinity
   depending on the sign of the corresponding weight.

   so we have to put a constraint on the problem. specifically, we   re
   constraining the magnitude of the input, and stating that the squared
   magnitude of the input vector should be no larger than 1.

   given this constraint, the input vector which will produce the largest
   response is one which is pointing in the same direction as the weight
   vector.

   the below examples show the dot product between two vectors. the
   magnitude of the dot product is largest when the vectors  are parallel.

   [6]dotproductmagnitudea

   [7]dotproductmagnitudeb

   ok, that   s great. but in the real world, the magnitude of the input
   vector is not constrained. the reality is that a vector with larger
   magnitude components (corresponding, for example, to a higher contrast
   image) could produce a stronger response than a vector with lower
   magnitude components (a lower contrast image), even if the smaller
   vector is more in alignment with the weight vector.

   given this fact, i don   t have a strong answer for why the visualization
   is still meaningful. i suspect that the    whitening    preprocessing step
   may have something to do with this, since it may ensure that the inputs
   tend to all be high contrast.

sparse autoencoder exercise

   for the exercise, you   ll be implementing a sparse autoencoder. no
   simple task!

   the work essentially boils down to taking the equations provided in the
   lecture notes and expressing them in matlab code.

   i won   t be providing my source code for the exercise since that would
   ruin the learning process. however, i will offer my notes and
   interpretations of the functions, and provide some tips on how to
   convert these into vectorized matlab expressions (note that the next
   exercise in the tutorial is to vectorize your sparse autoencoder cost
   function, so you may as well do that now).

   if you are using octave, like myself, there are a few tweaks you   ll
   need to make. see my    notes for octave users    at the end of the post.

   step 1: compute cost

   the first step is to compute the current cost given the current values
   of the weights.

   step 1.1: feedforward evaluation

   in order to calculate the network   s error over the training set, the
   first step is to actually evaluate the network for every single
   training example and store the resulting neuron activation values.
   we   ll need these activation values both for calculating the cost and
   for calculating the gradients later on.

   step 1.2: mean squared error (mse) cost

   once you have the network   s outputs for all of the training examples,
   we can use the first part of equation (8) in the lecture notes to
   compute the average squared difference between the network   s output and
   the training output (the    mean squared error   ).

   step 1.3: id173 cost

   next, we need to add in the id173 cost term (also a part of
   equation (8)).

   [8]id173term

   this term is a complex way of describing a fairly simple step. you just
   need to square every single weight value in both weight matrices (w1
   and w2), and sum all of them up. finally, multiply the result by lambda
   over 2.

   note that in the notation used in this course, the bias terms are
   stored in a separate variable _b. _this means they   re not included in
   the id173 term, which is good, because they should not be.

   step 1.4: sparsity cost

   next, we need add in the sparsity constraint.

   first we   ll need to calculate the average activation value for each
   hidden neuron.

   [9]avghiddenneuronactivation

   if a2 is a matrix containing the hidden neuron activations with one row
   per hidden neuron and one column per training example, then you can
   just sum along the rows of a2 and divide by m.

   the result is phat, a column vector with one row per hidden neuron.

   once you have phat, you can calculate the sparsity cost term.

   [10]sparsitycostterm

   to vectorize this equation:
     * use the phat column vector from the previous step in place of
       phat_j
     * replicate p into a column vector.
     * use element-wise operators. that is, use    .*    for multiplication
       and    ./    for division.

   this will give you a column vector containing the sparisty cost for
   each hidden neuron; take the sum of this vector as the final sparsity
   cost.

   the final cost value is just the sum of the base mse, the
   id173 term, and the sparsity term.

   step 2: computing gradients

   this part is quite the challenge, but remarkably, it boils down to only
   ten lines of code.

   note: i   ve described here how to calculate the gradients for the weight
   matrix w, but not for the bias terms b. the bias term gradients are
   simpler, so i   m leaving them to you.

   i think it helps to look first at where we   re headed. the final goal is
   given by the update rule on page 10 of the lecture notes.

   [11]updaterule

   this is the update rule for id119. however, we   re not
   strictly using id119   we   re using a fancier optimization
   routine called    l-bfgs    which just needs the current cost, plus the
   average gradients given by the following term (which is    w1grad    in the
   code):

   [equation 2.1]

   [12]w1grad

   we need to compute this for both w1grad and w2grad.

   the key term here which we have to work hard to calculate is the matrix
   of weight gradients (the second term in the table).

   to understand how the weight gradients are calculated, it   s most clear
   when you look at this equation (from page 8 of the lecture notes) which
   gives you the gradient value for a single weight value relative to
   a single training example. this equation needs to be evaluated for
   every combination of j and i, leading to a matrix with same dimensions
   as the weight matrix. then it needs to be evaluated for every training
   example, and the resulting matrices are summed.

   [13]gradientforsingleweight

   in the lecture notes, step 4 at the top of page 9 shows you how to
   vectorize this over all of the weights for a single training example:

   [14]wlgrad_vector

   finally, step 2  at the bottom of page 9 shows you how to sum these up
   for every training example.

   instead of looping over the training examples, though, we can express
   this as a matrix operation:

   [equation 2.2]

   [15]w1grad_total

   [16]w2grad_total

   so we can see that there are ultimately four matrices that we   ll need:
   a1, a2, delta2, and delta3. once we have these four, we   re ready to
   calculate the final gradient matrices w1grad and w2grad. we already
   have a1 and a2 from step 1.1, so we   re halfway there, ha!

   delta3 can be calculated with the following. i   ve taken the equations
   from the lecture notes and modified them slightly to be matrix
   operations, so they translate pretty directly into matlab code; you   re
   welcome :).

   [17]delta3

   next, the below equations show you how to calculate delta2. again i   ve
   modified the equations into a vectorized form. here the notation gets a
   little wacky, and i   ve even resorted to making up my own symbols!
   hopefully the table below will explain the operations clearly, though.
   just be careful in looking at whether each operation is a regular
   matrix product, an element-wise product, etc.

   [18]delta2

   now that you have delta3 and delta2, you can evaluate [equation 2.2],
   then plug the result into [equation 2.1] to get your final matrices
   w1grad and w2grad. whew!

   use the lecture notes to figure out how to calculate b1grad and b2grad.
   it   s not too tricky, since they   re also based on the delta2 and delta3
   matrices that we   ve already computed.

   good luck!

vectorization exercise

   the next segment covers vectorization of your matlab / octave code. you
   may have already done this during the sparse autoencoder exercise, as i
   did.

   in that case, you   re just going to apply your sparse autoencoder to a
   dataset containing hand-written digits (called the mnist dataset)
   instead of patches from natural images.

   they don   t provide a code zip file for this exercise, you just modify
   your code from the sparse autoencoder exercise.

   one important note, i think, is that the gradient checking part runs
   extremely slow on this mnist dataset, so you   ll probably want to
   disable that section of the    train.m    file.

   here is my visualization of the final trained weights. the weights
   appeared to be mapped to pixel values such that a negative weight value
   is black, a weight value close to zero is grey, and a positive weight
   value is white.

   [19]myfinalweights

notes for octave users

   i implemented these exercises in octave rather than matlab, and so i
   had to make a few changes.
     * in    display_network.m   , replace the line:
          h=imagesc(array,   erasemode   ,   none   ,[-1 1]);    with
          h=imagesc(array, [-1 1]);    the octave version of    imagesc    doesn   t
       support this    erasemode    parameter.
     * the    print    command didn   t work for me. instead, at the end of
          display_network.m   , i added the following line:    imwrite((array +
       1) ./ 2,    visualization.png   );    this will save the visualization to
          visualization.png   .
     * octave doesn   t support    mex    code, so when setting the options for
          minfunc    in train.m, add the following line:    options.usemex =
       false;   
     * perhaps because it   s not using the mex code, minfunc would run out
       of memory before completing. this was an issue for me with the
       mnist dataset (from the vectorization exercise), but not for the
       natural images. to work around this, instead of running minfunc for
       400 iterations, i ran it for 50 iterations and did this 8 times.
       after each run, i used the learned weights as the initial weights
       for the next run (i.e., set    theta = opttheta   ).

   [ins: :ins]
   please enable javascript to view the [20]comments powered by disqus.

related posts

     * [21]the inner workings of id97 12 mar 2019
     * [22]applying id97 to recommenders and advertising 15 jun 2018
     * [23]product quantizers for id92 tutorial part 2 22 oct 2017

      2019. all rights reserved.

references

   1. http://mccormickml.com/atom.xml
   2. http://mccormickml.com/
   3. http://mccormickml.com/about/
   4. http://mccormickml.com/tutorials/
   5. http://mccormickml.com/archive/
   6. http://chrisjmccormick.files.wordpress.com/2014/05/dotproductmagnitudea.png
   7. http://chrisjmccormick.files.wordpress.com/2014/05/dotproductmagnitudeb.png
   8. https://chrisjmccormick.files.wordpress.com/2014/05/id173term.png
   9. https://chrisjmccormick.files.wordpress.com/2014/05/avghiddenneuronactivation.png
  10. https://chrisjmccormick.files.wordpress.com/2014/05/sparsitycostterm.png
  11. https://chrisjmccormick.files.wordpress.com/2014/05/updaterule.png
  12. https://chrisjmccormick.files.wordpress.com/2014/05/w1grad1.png
  13. https://chrisjmccormick.files.wordpress.com/2014/05/gradientforsingleweight.png
  14. https://chrisjmccormick.files.wordpress.com/2014/05/wlgrad_vector1.png
  15. https://chrisjmccormick.files.wordpress.com/2014/05/w1grad_total.png
  16. https://chrisjmccormick.files.wordpress.com/2014/05/w2grad_total.png
  17. https://chrisjmccormick.files.wordpress.com/2014/05/delta32.png
  18. https://chrisjmccormick.files.wordpress.com/2014/05/delta2.png
  19. https://chrisjmccormick.files.wordpress.com/2014/05/myfinalweights.png
  20. https://disqus.com/?ref_noscript
  21. http://mccormickml.com/2019/03/12/the-inner-workings-of-id97/
  22. http://mccormickml.com/2018/06/15/applying-id97-to-recommenders-and-advertising/
  23. http://mccormickml.com/2017/10/22/product-quantizer-tutorial-part-2/
