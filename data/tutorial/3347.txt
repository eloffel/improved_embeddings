   #[1]chris mccormick

[2]chris mccormick    [3]about    [4]tutorials    [5]archive

id166 tutorial - part i

   16 apr 2013

   i found it really hard to get a basic understanding of support vector
   machines. to learn how id166s work, i ultimately went through andrew ng   s
   machine learning course (available freely from stanford).  i think the
   reason id166 tutorials are so challenging is that training an id166 is a
   complex optimization problem, which requires a lot of math and theory
   to explain.

   however, just by looking at an id166 that   s been trained on a simple data
   set, i think you can gain some of the most important insights into how
   id166s work.

   i fabricated a very simple id166 example in order to help myself
   understand some of the concepts. i   ve included the results and
   illustrations here, as well as the data files so that you can run the
   algorithm yourself.

   in excel, i created two sets of points (two    classes   ) that i placed
   arbitrarily. i placed the points such that you can easily draw a
   straight line to separate the two classes (the classes are    linearly
   separable   ). these points are my training set which i used to train the
   id166.

   [6]trivialdataset

   here is the [7]excel spreadsheet containing the data values and the
   plots in this post.

id166 scoring function

   a trained support vector machine has a scoring function which computes
   a score for a new input. a support vector machine is a binary (two
   class) classifier; if the output of the scoring function is negative
   then the input is classified as belonging to class y = -1. if the score
   is positive, the input is classified as belonging to class y = 1.

   let   s look at the equation for the scoring function, used to compute
   the score for an input vector x.

   [8]scoringfunction
     * this function operates over every data point in a training set (i =
       1 through m).
          + where x^(i), y^(i) represents the _i_th training example.
            (don   t confuse this as    x to the ith power   )
          + x^(i) is an input vector which may be any dimension.
          + y^(i) is a class label, which has one of only two values,
            either -1 or 1.
          +   _i is the coefficient associated with the _i_th training
            example.
     * x is the input vector that we are trying to classify
     * k is what is called a id81.
          + it operates on two vectors and the output is a scalar.
          + there are different possible choices of id81, we   ll
            look at this more later.
     * b is just a scalar value.

support vectors

   this scoring function looks really expensive to compute. you   ll have to
   perform an operation on every single training point just to classify a
   new input x   what if your training set contains millions of data points?
    as it turns out, the coefficient   _i will be zero for all of the
   training points except for the    support vectors   .

   in the below plot, you can see the support vectors chosen by the
   id166   the three training points closest to the decision boundary.

   [9]trivialdataset_supportvectors

training the id166 in weka

   to train an id166 on this data set, i used the freely available [10]weka
   toolset.
    1. in the weka explorer, on the    preprocess    tab, open [11]this .csv
       file containing the data set.

   [12]openfile
    1. on the    classify    tab, press the    choose    button to select
       classifier weka->classifiers->functions->smo (smo is an
       optimization algorithm used to train an id166 on a data set).

   [13]choosesmo
    1. click on the classifier command line to bring up a dialog for
       editing the command line arguments.

   in the dialog, change the    filtertype    property to    no
   id172/standardization   . this will make the results easier to
   interpret.

   also, click the command line of the    kernel    property. this will bring
   up another dialog to allow you to specify properties of the kernel
   function. set the    exponent    property to 2.

   [14]setuppolykernel

   a note for those who are already familiar with kernels: since our data
   set is linearly separable, we don   t really need an exponent of 2 on the
   kernel. this is necessary, though, to force weka to use support
   vectors.  otherwise, it will just give you a simple linear equation for
   the scoring function, which doesn   t help us in our understanding of
   id166s.
    1. click    start    to run the training algorithm.

   towards the middle of the output, you should see something like the
   following equation:
    0.0005 * <7 2 > * x]
 -  0.0006 * <9 5 > * x]
 +  0.0001 * <4 7 > * x]
 +  2.7035

   this is essentially the scoring function that we saw at the beginning
   of the post, but now with the values filled in.

   the numbers in angle brackets are our three support vectors <7  2>, <9
    5>, and <4  7> (these are the points i marked in the scatter plot).

   the coefficient beside each support vector is the computed    alpha   
   value for that data point.

   the sign of the coefficient comes from the class label. for example, <9
    5> belongs to class y = -1, and <7  2> belongs to class y = 1. in the
   original scoring function, there was the term   _i * y^(i). the alpha
   values will always be greater than or equal to 0, so the sign of the
   coefficient comes from the class label y^(i).

   the final value in the expression, 2.7035, is b.

visualizing the scoring function

   now we can take our scoring equation:

   [15]scoringfunction

   and plug in the values we   ve found to get:

   [16]scoringfunctionvalues

   where x1 and x2 are the components of the input vector _x _that we want
   to compute a score for.

   note that the original summation was over every point in the training
   set, but we   ve only included the terms for the support vectors here.
   alpha is zero for all of the other data points, so those terms
   disappear.

   you can plot the above equation using google; paste the following into
   a google search bar:

   plot z = 0.0005(7x + 2y)^2-0.0006(9x+5y)^2+0.0001(4x+7y)^2+2.7035

   change the ranges to x: 0 - 16 and y: 0 - 16 and you should get
   something like the following:

   [17]hypothesis

   the scoring function forms a surface in three dimensions. where it
   intersects the z = 0 plane it forms a line; this is our decision
   boundary. to the left of the decision boundary, inputs receive a score
   higher than 0 and are assigned to class y = 1. to the right inputs
   receive a score less than 0 and are assigned to class y = -1.

   in the next example, we   ll look at a slightly more complex
   classification problem where the classes are not linearly separable. in
   that example, we   ll go into more detail about the id81 and
   how it   s used to achieve non-linear classification.
   [ins: :ins]
   please enable javascript to view the [18]comments powered by disqus.

related posts

     * [19]the inner workings of id97 12 mar 2019
     * [20]applying id97 to recommenders and advertising 15 jun 2018
     * [21]product quantizers for id92 tutorial part 2 22 oct 2017

      2019. all rights reserved.

references

   1. http://mccormickml.com/atom.xml
   2. http://mccormickml.com/
   3. http://mccormickml.com/about/
   4. http://mccormickml.com/tutorials/
   5. http://mccormickml.com/archive/
   6. http://chrisjmccormick.files.wordpress.com/2013/04/trivialdataset.png
   7. http://chrisjmccormick.files.wordpress.com/2013/04/supportvectormachines.xlsx
   8. http://chrisjmccormick.files.wordpress.com/2013/04/scoringfunction.png
   9. http://chrisjmccormick.files.wordpress.com/2013/04/trivialdataset_supportvectors.png
  10. http://www.cs.waikato.ac.nz/ml/weka/
  11. https://docs.google.com/file/d/0b-kwgxjrqkq7vgszztjvyjc1z0k/edit?usp=sharing
  12. http://chrisjmccormick.files.wordpress.com/2013/04/openfile.png
  13. http://chrisjmccormick.files.wordpress.com/2013/04/choosesmo.png
  14. http://chrisjmccormick.files.wordpress.com/2013/04/setuppolykernel.png
  15. http://chrisjmccormick.files.wordpress.com/2013/04/scoringfunction.png
  16. http://chrisjmccormick.files.wordpress.com/2013/04/scoringfunctionvalues.png
  17. http://chrisjmccormick.files.wordpress.com/2013/04/hypothesis.png
  18. https://disqus.com/?ref_noscript
  19. http://mccormickml.com/2019/03/12/the-inner-workings-of-id97/
  20. http://mccormickml.com/2018/06/15/applying-id97-to-recommenders-and-advertising/
  21. http://mccormickml.com/2017/10/22/product-quantizer-tutorial-part-2/
