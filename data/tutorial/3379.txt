   lambda [1]ruben fiszel's website
   [2]home [3]about [4]contact

id23 and id25, learning to play from pixels

   posted on august 24, 2016 by ruben fiszel

introduction

   my 2 month summer internship at skymind (the company behind the open
   source deeplearning library [5]dl4j) comes to an end and this is a post
   to summarize what i have been working on: building a deep reinforcement
   learning library for dl4j:     (drums roll)     rl4j! this post begins by
   an introduction to id23 and is then followed by a
   detailed explanation of id25 (deep q-network) for pixel inputs and is
   concluded by an rl4j example. i will assume from the reader some
   familiarity with neural networks. but first, lets talk about the core
   concepts of id23.
   cartpole

   cartpole

preliminaries

     a    simple aspect of science    may be defined as one which, through
     good fortune, i happen to understand. (isaac asimov)

   id23 is an exciting area of machine learning. it is
   basically the learning of an efficient strategy in a given environment.
   informally, this is very similar to pavlovian conditioning: you assign
   a reward for a given behavior and over time, the agents learn to
   reproduce that behavior in order to receive more rewards. it is an
   iterative trial and error process.

markov decision process

   formally, an environment is defined as a markov decision process (mdp).
   behind this scary name is nothing else than the combination of
   (5-tuple):
     * a set of states \(s\) (eg: in chess, a state is the board
       configuration)
     * a set of possible actions \(a\) (in chess, all the move that could
       be possible in every configuration possible, eg: e4-e5)
     * the conditional distribution \(p(s'| s, a)\) of next states given a
       current state and an action. (in a deterministic environment like
       chess, transitioning from state \(s\) with action \(a\), there is
       only one state s    with id203 1, and all the others have
       id203 0. nevertheless, in a stochastic (involving randomness,
       eg: a coin toss) environment, the distribution is not as simple.)
     * the reward function of transitionning from state s to s   : \(r(s,
       s')\) (eg: in chess, +1 for a final move that leads to a victory,
       -1 for a final move that leads to a defeat, 0 otherwise. in
       cartpole, +1 for each step.).
     * the discount factor: \(\gamma\). this is the preference for present
       rewards compared to future rewards. (a concept very common in
       [6]finance.)

   note: it is usually more convenient to use the set of action \(a_s\)
   which is the set of available move from a given state, than the
   complete set a. \(a_s\) is simply the elements \(a\) in \(a\) such that
   \(p(s' | s, a) > 0\).

   the markov property is to be memoryless. once you reach a state, the
   past history (the states visited before) should not affect the next
   transitions and rewards. only the present state matters.
   schema of a mdp

   schema of a mdp

some terminologies

   final/terminal states: the states that have no available actions are
   final/terminal states.

   episode: an episode is a complete play from one of the initial state to
   a final state. \[s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_n\]

   cumulative reward: the cumulative reward is the discounted sum of
   reward accumulated throughout an episode: \[r=\sum_{t=0}^n \gamma^t
   r_{t+1}\]

   policy: a policy is the agent   s strategy to choose an action at each
   state. it is noted by \(\pi\).

   optimal policy: the optimal policy is the theoretical policy that
   maximizes the expectation of cumulative reward. from the definition of
   expectation and the law of large numbers, this policy has the highest
   average cumulative rewards given sufficient episode. this policy might
   be intractable.

   the objective of id23 is to train an agent such that
   his policy converges to the theoretical optimal policy.

different settings

        qui peut le plus peut le moins    (he who can do the greater
     things, can do the lesser things)

model-free

   the conditional distribution and the reward function constitute the
   model of the environment. in a game of backgammon, we know the model
   (each possible transition is decided by the known dice distribution,
   and we can predict each reward from transition without realizing them
   (because we can calculate the new value of the board)). the algorithm
   [7]td-gammon use that fact to learn the v-function (see below).

   some id23 algorithms can work without being given the
   model. nevertheless, in order to learn the best strategy, they
   additionaly have to learn the model during the training. this is called
   model-free id23. model-free algorithms are very
   important because a large majority of real world complex problems fall
   in that category. futhermore, model free is simply an additional
   constraint. it is simply more powerful since it is a superset of model
   based id23.

observation setting

   instead of being given access to the state, you might being given
   access to a partial observation of the state only. it is the same idea
   behind hidden markov chain. this is the difference between the partial
   and fully observed setting. for instance, our field of vision is a very
   partial observation of the full state of the universe (the position and
   energy of every particule in the universe). fortunately, the partial
   observation setting can be reduced to a fully observed setting with the
   use a history (the state becomes an accumulation of previous states).

   nevertheless, it is most common to not accumulate the whole history.
   either only the last h observations are stacked (in a windowed fashion)
   or you can use a recurrent neural network (id56) to learn what to keep
   in memory and what to forget (that is essentially how a [8]lstm works).

   abusing the language slighty for consistency purposes with the existing
   notation, history (even truncated ones) will also be called    state    and
   also symbolized \(s_t\)

single player and adversarial games

   a single player game has a natural translation into a mdp. the states
   represent the moment where the player is in control. the observations
   from those states are all the information accumulated between states
   (eg: as many pixel frame as there are in-between frames of control). an
   action is all the available command at the disposal of the player (in
   doom, go up, right, left, shoot, etc    ).

   id23 can also be applied to adversarial games by
   self-play: the agent plays against itself. often in this setting, there
   exists a [9]nash equilibrium such that it is always in your interest to
   play as if your opponent was a perfect player. this makes sense in
   chess by example. if given a board configuration, a good move against a
   chess master, would still be a good move against a beginner.[10]^1
   whatever is the current level of the agent, by playing against himself,
   the agent stills get information about the quality of his previous
   moves (seen as good moves if he won, bad moves if he lost).

   of course the information, which is a gradient in the context of a
   neural network, is of   higher quality   if he played directly against a
   very good agent from the start. but it is really mind-blowing that an
   agent can learn to increase his level of play by playing against
   himself, an agent of the same level. that is actually the method of
   training employed by [11]alphago (the go agent from deepmind that beat
   the world champion). the policy was bootstrapped (initially trained) on
   a dataset of master moves, then it used id23 and self
   play to increase furthermore the level (quantified with elo). in the
   end, the agent got better than policy it was learning from the original
   dataset. after all, it beat the master above all the masters. to
   compute the final policy, they used their policy gradient in
   combination with a monte-carlo search tree on a massive amount of
   computation power.

   this setting is a bit different from learning from pixels. firstly,
   because the input is not as high-dimensional. the [12]manifold is a lot
   closer to its embedding space. nevertheless, a convolutional layer was
   still used in this case to use efficiently the locality of some subgrid
   board patterns. secondly, because alphago is not model-free (it is
   deterministic). in the following of this post, i will talk exclusively
   about the model-free 1-player setting.

id24

     i am no friend of id203 theory, i have hated it from the first
     moment when our dear friend max born gave it birth. for it could be
     seen how easy and simple it made everything, in principle,
     everything ironed and the true problems concealed. (erwin
     schr  dinger)

from policy to neural network

   our goal is to learn the optimal policy \(\pi^*\) that maximize:
   \[e[r_0]=e[\sum_{t=0}^n \gamma^t r_{t+1}]\] let   s introduce an
   auxilliary function: \[v_\pi(s) = e \{ r_t + \gamma r_{t+1} + \gamma^2
   r_{t+2} + \ldots + \gamma^n r_{n} \mid s_t = s, \text{policy followed
   at each state is }\pi \}\] which is the expected cumulative reward from
   a state \(s\) following the policy \(\pi\). let suppose an oracle
   \[v_{\pi^*}(s)\] the v function of the optimal policy. from it, we
   could retrieve the optimal policy by defining the policy that among all
   available actions at the current state, choose the action that maximize
   the expectation of \(v_{\pi^*}(s)\). this is a greedy behavior. the
   optimal policy is the greedy policy w.r.t to \(v_{\pi^*}\). \[ \pi^*(s)
   ~\text{chooses a s.t}~a= arg \max_a[e_\pi(r_t + \gamma v(s_{t+1}) \mid
   s_t=s, a_t=a)] \] if you were very attentive, something would sound
   wrong here. in the model-free setting, we cannot predict the
   after-state \(s_{t+1}\) from \(s_{t}\) because we ignore the transition
   model. even with that oracle, our model is still not computable!

   to solve this very annoying issue, we are gonna use another auxiliarry
   function, the q-function: \[q_{\pi^*}(s, a) = e_\pi[r_t + \gamma
   v_{\pi^*}(s_{t+1}) \mid s_t, a_t = a]\] in a greedy setting, we have
   the relationship: \[v_\pi(s_t) = \max_a q_\pi(s_t, a)\] now, let
   suppose instead of the v oracle, we have the q oracle. we can now
   redefine \(\pi^*\). \[ \pi^*(s) ~ \text{chooses a s.t}~ a= arg
   \max_a[q_{\pi^*}(s, a)] \] no more uncomputable expectations, neat

   nevertheless, we have only moved the expectation from outside to inside
   the oracle. and unfortunately, oracles do not exist in the real world.

   the trick here is that we have reduced an abstract notion that is a
   policy into a numerical function that might be relatively    smooth   
   (continuous) thanks to the expectation. fortunately for us, there is
   one weapon at our disposal to approximate such complex functions:
   neural networks.

   neural networks are universal function approximators. they can
   approximate any continuous differentiable function. although they can
   get stuck in local extrema and many proofs of convergence from
   id23 are not valid anymore when throwing neural
   networks in the equation. this is because their learning is not as
   deterministic or boundable as their tabular counterparts. nonetheless,
   in most case, with the right hyperparameters, they are
   [13]unreasonnably powerful. using deep learning with reinforcement
   learning is called deep id23.

policy iteration

   now machine learning knowledge and common sense tells you that there is
   still something missing about our approach. neural networks can
   approximate functions that already have labels. unfortunately for us
   oracles are not summonable, so we will have to get our labels another
   way. (:<).

   this is where the magic of monte carlo come in. monte carlo methods are
   methods that rely on repeated random sampling to calculate an
   estimator. (a famous example is the [14]pi calculation).

   if we play randomly from a given state, the better states should get
   better rewards on average (thank you [15]law of large numbers). so
   without knowing anything about the environment, you can get some
   information about the expected value of a state. for instance, at
   poker, better hands will win more often on average than lesser hands
   even when every decision is taken randomly. the monte carlo search tree
   are also based on this property (shocking isn   t it?). this is a phase
   of exploration that lead to unsupervised learning and enable us to
   extract meaningful label.

   more formally,

   given a policy \(\pi\), a state s and an action a, in order to get an
   approximation of \(q(s, a)\) we sample it according to its definition:
   \begin{align*} q_\pi(s, a) &= e[r_t + \gamma r_{t+1} + \ldots +
   \gamma^n r_{n} \mid s_t = s, a_t = a] \end{align*}

   in plain english, we can get a label for \(q_\pi(s, a)\) by playing a
   sufficient number of time from s according to the policy \(\pi\).

   from an aggregate of signals:
   one signal

   one signal

   the actual learning is done by standard id119, using the
   labels in batches. the gradient is the standard mean-square error one
   such that the td-error gets minimised at each iteration.

   we use the mean-square error id168 (l2 loss) with a learning
   rate of \(\alpha\) and apply stochastic id119 (on a batch of
   size 1) of: \[q_\pi(s_t, a_t) \leftarrow q_\pi(s_t, a_t) + \alpha
   [r_t-q_\pi(s_t, a_t)]\]

   \((s_t, a_t)\) is the input, \(q_\pi(s_t, a_t) + \alpha [r_t-q_\pi(s_t,
   a_t)]\) is the label aka target.

   note: even if we use mse, there is no square in the formula because the
   loss is applied afterwards on the difference of the expected output
   \(q_\pi(s_t, a_t)\) and the label \(\alpha [r_t-q_\pi(s_t, a_t)]\).

   repeat many times: sampling from \(\pi\) \[q_\pi(s_t, a_t) \leftarrow
   e_\pi[r_t] = e_{s_t, a_t, ..., s_n \sim \pi}[\sum_{i=t}^n
   \gamma^{i-t}r_i]\] we can converge to the rightful expectation
   many signals

   many signals

   so we can now design a naive prototype of our learning algorithm (in
   scala but it is intelligible without any scala knowledge):
//a randomly uninitialized neural network
val neuralnet: neuralnet

//iterate until you reach max epoch
for (t <- (1 to maxepoch))
        epoch()


def epoch() = {

        //pick a random state and action
        val state = randomstate
        val action = randomaction(state)

        //transition to a new state, initalize thethe reward
        var (new_state, accureward) = transitition(state, action)

        //play until terminal state and accumulate the reward
        accureward += playrandomly(state)

        //do sgd over input and label!
        fit((state, action), accureward)
}



// mdp specific, return the new state and the reward
def transition(state: state, action: action): (state, double)

//return a randomly sampled state among all the state space
def randomstate: state

//play until terminal state
def playrandomly(state): double = {
        var s = state
        var accureward = 0
        var k = 0
        while (!s.isterminal) {
                val action = randomaction(s)
                val (state, reward) = transition(s, action)
                accureward += math.pow(gamma, k) * reward
                k += 1
                s = state
        }
        accureward
}


//choose a random action among all the available actions at this state
def randomaction(state: state): action =
        oneof(state.available_action)

//helper function, pick one among
def oneof(seq: seq[action]): action =
        seq.get(random.nextint(seq.size))


//how it would be roughly done with dl4j
def fit(input: (state, action), label: double) =
        neuralnet.fit(totensor(input), totensor(label))

//return an indarray from nd4j
def totensor(array: array[_]): tensor =
        nd4j.create(array)

   there are multipqle issues: this should work but this is terribly
   inefficient. we are playing a full game with n state and n actions for
   a single label and that label might not be very meaningful (if the
   interesting trajectories are hard to reach at random).

the exploration/exploitation dilemma

   exploring at random the environment will converge to the optimal policy
       but only guaranteed after an almost infinite time: you will have to
   visit every possible trajectories (a trajectory is the ordered list of
   al the states visited and actions choosen during an episode) at least
   once. considering how many states and branching there is, it is
   impossible. the branching issue is the reason why go is so hard but
   chess is ok. in the real world, we do not have infinite time (and time
   is money).

   thus, we should exploit the past informations and our learning of them
   to focus our exploration on the most promising possible trajectories.
   this can be achieved through different ways, and one of them is
   \(\epsilon\)-greedy exploration. \(\epsilon\)-greedy exploration is
   fairly simple. it is a policy that choose an action at random with odd
   \(\epsilon\) or the best action as deemed by our current policy with
   odd \((1-\epsilon)\). usually \(\epsilon\) is annealed over time to
   privilege exploitation over exploration after enough exploration. this
   is a trade-off between exploration and exploitation.

   at each new information, our actual q functions gets more accurate
   about the present policy and the exploration is focused on better
   paths. the policy based on our new q function gets better (since q is
   more accurate) and the \(\epsilon\)-greedy exploration reach better
   paths. focused on those better paths, our q function explore even more
   the better parts and has to update its returns according to the new
   policy. this is an iterative cycle that enable convergence to the
   optimal policy called policy iteration. unfortunately, the convergence
   can take infinite time and is not even guaranteed when q is
   approximated by neural networks. nevertheless, impressive results can
   make up for the lack of formal convergence proofs.
   policy iteration

   policy iteration

   this algorithm also requires you to be able to sample the states in a
     good   manner: it should be proportionally representative of the states
   that are usally present in a game (or at least the kind of game at the
   targeted agent   s level). on a sidenote, this is possible in some case,
   see [16]giraffe that uses td-lambda.

   fortunately, some rearranging and optimisations are possible:

bellman equation

   we can transform the q equation into a bellman equation: \begin{align*}
   q_\pi(s, a) &= e[r_t + \gamma r_{t+1} + \ldots + \gamma^n r_{n} \mid
   s_t = s, a_t = a] \\ &= e[r_t + \gamma r_{t+1} + v(s_{t+1}) \mid s_t =
   s, a_t = a] \\ &= e[r_t + \gamma r_{t+1} + \ldots + \gamma \max_{a'}
   q(s_{t+1}, a')\} \mid s_t = s, a_t = a] \\ \end{align*}

   as in the monte-carlo method, we can do many updates of q.

   mse: \[q_\pi(s_t, a_t) \leftarrow q_\pi(s_t, a_t) + \alpha
   [(\underbrace{\underbrace{r_t+\max_a q_\pi(s_{t+1},
   a)}_{\text{target}}-q_\pi(s_t, a_t)}_{\text{td-error}})]\]

   td-error is the    temporal difference error   . indeed, we are actually
   calculating the difference between what the q approximation expects in
   the future plus the realized reward and its present value as evaluated
   by the neural net.

   that bellman equation only make sense with some boundary conditions. if
   s is terminal: \[v(s) = 0\] and for any a \[q(s_{t-1}, a) = r_t\]

   the states near the terminal states are the first to converge because
   they are closer in the chain to the   true   label, the known boundary
   conditions. in go or chess, id23 is applied by
   assigning +1 to the transitions that lead to a final winning board
   (respectively -1 for a loosing board) and 0 otherwise. it diffuses the
   q-values by finding a point between the two extremes [-1; 1]. a
   transition with q value close to 0 represents a transition leading to a
   balanced board. a transition with q value close to 1 represents a near
   certain victory.

   it could be surprising that the moves do not have only -1 and 1 values
   (since deviating from the optimal path should be fatal). one
   interesting aspect of calculating q-values is the realization that in
   many games/mdp, no mistake in itself is ever really fatal. it is the
   accumulation of them that really kill you. ai is full of life lessons
   ;). moreover, the expected accumulated reward space is a lot smoother
   than often thought. one possible explanation is that expectations
   always have an average effect: an expectation is nothing else than a
   weighted average with probabilities as weights. furthermore, gamma
   being \(< 1\) the very long-term effects are not too preponderant.
   isn   t it exciting to be able to calculate directly the odd of winning a
   game for every transition ?

   as long as we sample sufficiently enough transitions near the terminal
   states, id24 is able to converge. the incredible power of deep
   id23 is that it will be able to generalize its
   learning from visited states to unvisited states. it should be able to
   understand what is a balanced or winning board even if it has never
   seen it before. this is because the network should be able to abstract
   patterns and understand the strength of an action based on previously
   seen pattern (eg: shoot an enemy when recognizing its form).

offline and online id23

   to learn more about the differences between online and offline
   id23, see this excellent post from [17]kofzor.

initial state sampling

   in a 1 player setting (like the atari game): we do not actually need to
   learn to play well in every situation (although, if we did, that would
   show that we would have reached a very good level of generalization).
   we only need to learn to play efficiently from the states that our
   policy encounters. thus, we can sample from states that are simply
   reachable by playing with our current policy from an initial state.
   this enables to sample directly from a played episode by our agent.

id24 implementation

   so we can now design a naive prototype of our id24:

def epoch() = {

        //sample among the initial state space
        //(often unique state)
        var state = initstate

        //while the state is not terminal,
        //play an episode and do a q-update at each transition
        while(!state.isterminal)  {

                //sample action from eps-greddy policy
                val action = epsilongreedyaction(state)

                //interaction with the environment
                val (nextstate, reward) = transition(state, action)

                //q-update
                update(state, action, reward, nextstate)

                state = nextstate
        }
}

//our q-update as explained above
def update(state: state, action: action, reward: double, nextstate: state) = {
        val target = reward + maxq(nextstate)
        fit((state, action), target)
}


//the eps-greedy policy implementation
def epsilongreedyaction(state: state) = {
        if (random.float() < epsilon)
                randomaction(state)
        else
                maxqaction(state)
}


//retrive max q value
def maxq(state: state) =
        actionswithq(state).maxby(_._2)._2

//retrive action of the max q value
def maxqaction(state: state) =
        actionswithq(state).maxby(_._2)._1

//return a list of actions and the q-value of their transition from the state
def actionswithq(state: state) = {
        val stateactionlist = available_actions.map(action => (state, action))
        available_actions.zip(neural_net.output(totensor(state_action_list)))

def initstate: state


modeling q(s, a)

   instead of having \(a\) as an additional input of the neural net
   combined with the state, the state is the only input and the output
   contains the q value of every action possible. this makes sense only
   when the availables actions are consistent accross the full episode
   (else the neural output layer would have to be different at each
   state). it is many times solvable by having the full set \(a\) of
   actions as output and ignore the impossible actions (some papers put
   the target of impossible actions at 0).
   q modeling source

   q modeling [18]source

experience replay

   there is one issue with using neural network as q approximator. the
   transitions are very correlated. this reduce the overall variance of
   the transition. after all, they are all extracted from the same
   episode. imagine if you had to learn a task without any memory (not
   even short-term), you would always optimise your learning based on the
   last episode.

   the google deepmind research team used experience replay, which is a
   windowed buffer of the last n transitions (n being a million in the
   original paper) with id25 and greatly improved their performances on
   atari. instead of updating from the last transition, you store it
   inside the experience replay and update from a batch of randomly
   sampled transitions from the same experience replay.

   epoch() becomes:
def epoch() = {

        //sample among the initial state space
        //(often unique state)
        var state = initstate

        //while the state is not terminal,
        //play an episode and do a q-update at each transition
        while(!state.isterminal)  {

                //sample action from eps-greddy policy
                val action = epsilongreedyaction(state)

                //interaction with the environment
                val (nextstate, reward) = transition(state, action)

                //store transition (exp replay is just a ring buffer)
                expreplay.store(state, action, reward, nextstate)

                //q update in batch
                updatefrombatch(expreplay.getbatch())

                state = nextstate
        }
}

compression

   nd4j, the tensor library of dl4j, does not support as first class type
   uint8. however, pixels in grayscaling are encoded with that precision.
   to avoid wasting too much space on memory, indarray were compressed to
   uint8.

convolutional layers and image preprocessing

convolutional layers

   convolutional layer source

   convolutional layer [19]source

   convolutional layers are layers that are excellent to detect local
   patterns in images. for pixels, it is used as a processor that is
   required to reduce the dimension of the input into its real manifold.
   given the proper manifold of observations, the decision becomes much
   easier.

image processing

   you could feed the neural network with the rgb directly, but then the
   network would have to also learn that additional pattern. it seems like
   the brain is hard-wired to combine colors (fortunately!). thus, it
   would seem reasonnable to tolerate that preprocessing.

   what you see:

   what the neural net see:
   neural net input

   neural net input

resizing

   the image is resized into 84x84. convolutional layers needs for memory
   and computations grow with the size of their input. the fine details of
   the image are not required to play the game correctly. indeed, many are
   purely aesthetic. resizing to a more reasonnable size speed up the
   training.

skip frame

   in the original [20]atari paper, only 1 in 4 frames is actually
   processed. for the following 3 images, the last action is repeated. it
   speeds up roughly by 4 time the training without loosing much
   information. indeed, atari game are not supposed to be played frame
   perfect and for most action it makes more sense to keep them for at
   least 4 frames.

history processing

   to give information to the neural network about the current momentum,
   the last 4 frame (with skip frame, you pick 1 every 4) are stacked into
   4 channels. those 4 frames represent a history as previously discussed
   in the observation setting section.
   slowed input

   slowed input
   stacking

   stacking

   to fill the first frames of the history, a random policy or a noop
   replay is used. (sidenote, [21]random starts can be used for fair
   evaluation)

double id24

   the idea behind [22]double id25 is that the network is frozen every m
   update (hard update) or smoothly averaged (target = target * (smooth) +
   current * (1-smooth)) every update (soft update). indeed, it adds
   stability to the learning by using a q evaluation to use in the
   td-error formula that is less prone to    jiggering   . the q update
   becomes:

   \[y_\text{target} = r_t + \gamma*(q_\text{target}(s_t+1, arg \max_a
   q_\text(s_t+1, a))) \]

clipping

   the td-error can be clipped (bounded between two limit values) such
   that no outlier update can have too much impact on the learning.

scaling rewards

   scaling the rewards such that q-values are lower (in a range of [-1; 1]
   is similar to id172). it can dramatically alter the efficiency
   of the learning. this is an important hyperparameter to not neglect.

prioritized replay

   the idea behind [23]prioritized replay is that not all transitions are
   born equal. some are more important than others. one way to sort them
   is through their td-error. indeed, a high td-error is correlated to a
   high level of information (in the sense of surprise). those transitions
   should be sampled more often than the others.

graph, visualisation and mean-q

   to visualize and debug the training or a method of rl, it is useful to
   have a visual monitoring of the agent   s progress. this is why i built
   the dashboard [24]webapp-rl4j.
   webapp-rl4j

   webapp-rl4j

   the most important is to keep track of cumulative rewards. this is a
   way to check that the agents effectively gets better. it is important
   to notice that it represents the epsilon greedy strategy and not the
   directly derived policy from the q approximation.
   cumulative reward graph

   cumulative reward graph

   but you might want to also track the loss (score of the neural network)
   and mean q-values:
   score and mean-q graph

   score and mean-q graph

   unlike with classic supervised learning, the loss does not necessarily
   always decrease because the learning impacts the labels!

   if used with target network, you should see some discontinuities from
   the non continuous evaluation of different target networks. loss should
   decrease w.r.t to a single target network. the mean q-values should
   smoothly converge towards a value proportionnal to the mean expected
   reward.

rl4j

   rl4j is available on [25]github. currently id25 with experience replay,
   double id24 and clipping is implemented. asynchronous
   id23 with a3c and async n-step id24 is included
   too. it is possible to play both from pixels or low-dimensional
   problems (like cartpole). async id23 is experimental.
   hopefully, contributions will enrich the library.

   here is a working example with rl4j to play cartpole with a simple id25.
   you can play doom too. check [26]rl4j-examples for more examples. it is
   also possible to provide your own constructed neural network model as
   an argument to any training method.
    public static qlearning.qlconfiguration cartpole_ql =
            new qlearning.qlconfiguration(
                    123,    //random seed
                    200,    //max step by epoch
                    150000, //max step
                    150000, //max size of experience replay
                    32,     //size of batches
                    500,    //target update (hard)
                    10,     //num step noop warmup
                    0.01,   //reward scaling
                    0.99,   //gamma
                    1.0,    //td-error clipping
                    0.1f,   //min epsilon
                    1000,   //num step for eps greedy anneal
                    true    //double id25
            );

    public static id25factorystddense.configuration cartpole_net =
            new id25factorystddense.configuration(
                    3,         //number of layers
                    16,        //number of hidden nodes
                    0.001,     //learning rate
                    0.00       //l2 id173
            );

    public static void main( string[] args )
    {

        //record the training data in rl4j-data in a new folder (save)
        datamanager manager = new datamanager(true);

        //define the mdp from gym (name, render)
        gymenv<box, integer, discretespace> mdp = new gymenv("cartpole-v0", fals
e, false);

        //define the training
        qlearningdiscretedense<box> dql = new qlearningdiscretedense(mdp, cartpo
le_net, cartpole_ql, manager);

        //train
        dql.train();

        //get the final policy
        id25policy<box> pol = dql.getpolicy();

        //serialize and save (serialization showcase, but not required)
        pol.save("/tmp/pol1");

        //close the mdp (close http)
        mdp.close();


    }

conclusion

   this was an exciting journey through deep id23. from
   equations to code, id24 is a powerful, yet a somewhat simple
   algorithm. the field of rl is very active and promising. in fact,
   supervised learning could be considered a subset of reinforcement
   learning (by setting the labels as rewards). maybe one day,
   id23 will be the panacea of ai. until then, we can
   expect to be awed by its diverse applications into more and more
   mind-blowing problems. as a word of acknowledgement, i would like to
   thank skymind and its amazing team for this very enriching internship.

to feed your appetite

   bookworm

   bookworm

   i hope that thanks to this introduction, you are excited about rl
   research. here is a brief summary of important research in deep
   id23.

continuous domain

   when the action space is not discrete, you cannot use id25. but many
   problems cannot be discretized. [27]continuous control is achieved
   through a normal distribution parametrized (mean and variance) by the
   output of the neural network. at each step, the action is sampled from
   the distribution. it also uses soft update of the target network.

policy gradient

   policy gradient works by directly learning the stochastic policy from
   cross id178 of the distribution scaled bythe advantage. this
   excellent post from [28]karpathy   s blog has more details on the matter.
   using a stochastic policy feels more natural as it encourages
   exploration and exploit more fairly the uncertainty we have between the
   different values of the move. indeed, a max operation can end by
   ignoring fully a branch that is \(\epsilon\) below in q-value of
   another. although, this can be solved in id25 by using boltzmann
   exploration.

   policy gradient were used by alphago in combination with montecarlo
   search tree. the neural network was bootstrapped (pretrained) on a
   dataset of master move before they let it improve itself with rl.

   nowadays, policy gradients are getting more popular. for example, a3c
   (see below) is based on it.

asynchronous methods for deep id23

   a3c (asynchronous actor critic) and async nstep id24 are included
   in rl4j. it bypasses the need for an experience replay by using
   multiple agents exploring in parrallel the environment. the original
   [29]paper uses [30]hogwild!. in rl4j, a workaround is to use a central
   thread and accumulate gradient from    slave    agents. having multiple
   agents exploring the environment enable to decorrelate the experience
   from the past episode and enable to gather more experience (instead of
   replaying multiple time the same transition). it is very efficient and
   the authors were able to train efficiently on a single machine!

   for the curious, here is some scala-pseudo-code of a3c:
//a randomly uninitialized neural network
val globalneuralnet: neuralnet

//iterate until you reach max epoch
for (t <- (1 to numthread))
        launchthread()

def launchactor) =
  new actor(globalneuralnet).run()

var globalt: int = 0

class actor(globalneuralnet: neuralnet) {

  def run() = {
    while(globalt < maxstep) {

      val neuralnet = globalneuralnet.clone()

      var state = initstate
      var i = 0
      val stack:stack[(state, action, reward)] = new stack()

      while(i < tmax && !state.isterminal){
        globalt += 1
        i += 1

        //sample action from stochastic policy
        val action = stochasticpolicy(neuralnet, state)

        //interaction with the environment
        val (nextstate, reward) = transition(state, action)
        stack.add((state, action, reward))
        state = nextstate

      }

      var r =
        if (state.isterminal)
          0
        else
          criticoutput(neuralnet, state)

      val inputs = new tensor()
      val actorlabels = new tensor()
      val criticlabels = new tensor()

      while(!stack.isempty) {
        val trans = stack.dequeue()
        r = trans.reward + gamma*r

        inputs.appendrow(trans.state)
        criticlabels.appendrow(r)

        val prevcriticoutput = criticoutput(neuralnet, trans.state)
        val advantage = r - prevcriticoutput
        val prevactoroutput = actoroutput(neuralnet, trans.state)

        actorlabel.appendrow(prevactoroutput.addscalar(trans.action, r))

      }

      asyncupdate(globalneuralnet, inputs, criticlabels, actorlabels)

    }
  }

  def stochasticpolicy(neuralnet: neuralnet, state: state) = {
    val distribution = actoroutput(neuralnet, state)
    chooseaccordingtodistribution(state.availableactions, distribution)
  }


}

deep exploration

   deep exploration was the subject of a semester project during my
   master. i wrote the scala library [31]scala-drl about it. deep
   exploration is defined as the multi-step ahead planning of the
   exploration.

   in [32]bootstrapped id25, the term id64 comes from
   [33]statistics. multiple neural network are constructured in parralel.
   at each epoch, one neural network explore the environment. then the
   transition are randomly redistributed to each model. this has a similar
   effect than resampling. having had differences experiences to learn
   from, each model has its own opinion about the best move and its own
   uncertainty about the environment. this encourage deep exploration.

   another way is to use [34]autoencoder to quantify the uncertainty about
   a novel state and attribute an exploration bonus based on the
   reconstruction loss.

other interesting papers to discover by yourself.

deterministic policy gradient

   [35]http://jmlr.org/proceedings/papers/v32/silver14.pdf

trusted region policy optimisation

   [36]https://arxiv.org/abs/1502.05477

dueling network architectures for deep id23

   [37]http://arxiv.org/abs/1511.06581

references

   [38]karpathy   s post about policy gradient

   [39]playing atari with deep id23

   [40]deep id23 with double id24

   [41]asynchronous methods for deep id23

   [42]prioritized experience replay

   [43]continuous control with deep id23

   [44]giraffe: using deep id23 to play chess

   [45]deep exploration via bootstrapped id25

   [46]incentivizing exploration in id23 with deep
   predictive models

   [47]reinforcejs

   [48]sutton holy bible on id23
     __________________________________________________________________

    1. (note: as my semester project advisor remarked, this is not always
       true. imagine you play against a beginner at chess and you are
       badly losing, you might want to    bait    him to reverse the
       situation. a bait is only beneficial when the opponent has a low
       level of play. this proves that chess is not a real nash
       equilibrium. this does not matter much because the goal is often to
       build a high level of play agent that plays against other high
       quality agents. at this level of play, you do not loose if given a
       significant advantage.[49]   

   [50]licence creative commons
     __________________________________________________________________

references

   visible links
   1. https://rubenfiszel.github.io/
   2. https://rubenfiszel.github.io/
   3. https://rubenfiszel.github.io/about.html
   4. https://rubenfiszel.github.io/contact.html
   5. http://deeplearning4j.org/
   6. https://en.wikipedia.org/wiki/discounted_utility
   7. https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node108.html
   8. http://colah.github.io/posts/2015-08-understanding-lstms/
   9. https://en.wikipedia.org/wiki/nash_equilibrium
  10. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#fn1
  11. https://deepmind.com/alpha-go
  12. http://colah.github.io/posts/2014-03-nn-manifolds-topology/
  13. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  14. http://mathfaculty.fullerton.edu/mathews/n2003/montecarlopimod.html
  15. https://en.wikipedia.org/wiki/law_of_large_numbers
  16. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#giraffe
  17. https://kofzor.github.io/reinforcement_learning_101/#comparing-reinforcement-learning-algorithms
  18. https://www.nervanasys.com/demystifying-deep-reinforcement-learning/
  19. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#atari
  20. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#atari
  21. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#doubleq
  22. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#doubleq
  23. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#prio
  24. https://github.com/rubenfiszel/webapp-rl4j
  25. https://github.com/deeplearning4j/rl4j
  26. https://github.com/rubenfiszel/rl4j-examples
  27. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#continuous
  28. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#karpathy
  29. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#atari
  30. https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahukewjaiqzll9boahuu3mmkhyl8bm0qfggemaa&url=https://www.eecs.berkeley.edu/~brecht/papers/hogwildtr.pdf&usg=afqjcne9xrk7aeeqxmc2xkwxxrfyc90y2a&sig2=cecuhurshrx9cl-mb4hk6a
  31. https://github.com/rubenfiszel/scala-drl
  32. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#bootstrapped
  33. https://en.wikipedia.org/wiki/id64_(statistics)
  34. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#autoencoder
  35. http://jmlr.org/proceedings/papers/v32/silver14.pdf
  36. https://arxiv.org/abs/1502.05477
  37. http://arxiv.org/abs/1511.06581
  38. http://karpathy.github.io/2016/05/31/rl/
  39. http://arxiv.org/abs/1312.5602
  40. http://arxiv.org/abs/1509.06461
  41. https://arxiv.org/abs/1602.01783
  42. http://arxiv.org/abs/1511.05952
  43. http://arxiv.org/abs/1509.02971
  44. https://arxiv.org/abs/1509.01549
  45. http://arxiv.org/abs/1602.04621
  46. http://arxiv.org/abs/1507.00814
  47. http://cs.stanford.edu/people/karpathy/reinforcejs/index.html
  48. https://webdocs.cs.ualberta.ca/~sutton/book/ebook/
  49. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html#fnref1
  50. http://creativecommons.org/licenses/by/4.0/

   hidden links:
  52. https://ch.linkedin.com/in/rubenfiszel
  53. https://github.com/rubenfiszel
  54. mailto:ruben.fiszel@epfl.ch
  55. https://rubenfiszel.github.io/posts/rl4j/assets/rubenfiszel_resume.pdf
