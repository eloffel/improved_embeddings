   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

machine learning is fun part 5: language translation with deep learning and
the magic of sequences

   [9]go to the profile of adam geitgey
   [10]adam geitgey (button) blockedunblock (button) followfollowing
   aug 21, 2016

   update: this article is part of a series. check out the full series:
   [11]part 1, [12]part 2, [13]part 3, [14]part 4, [15]part 5, [16]part 6,
   [17]part 7 and [18]part 8! you can also read this article in
   [19]         ,[20]               , [21]         , [22]ti   ng vi   t or [23]italiano.

   giant update: [24]i   ve written a new book based on these articles! it
   not only expands and updates all my articles, but it has tons of brand
   new content and lots of hands-on coding projects. [25]check it out now!

   we all know and love [26]google translate, the website that can
   instantly translate between 100 different human languages as if by
   magic. it is even available on our phones and smartwatches:
   [1*q_xg5wyxxl2hchy5k8xvgg.gif]

   the technology behind google translate is called machine translation.
   it has changed the world by allowing people to communicate when it
   wouldn   t otherwise be possible.

   but we all know that high school students have been using google
   translate to    umm    assist with their spanish homework for 15 years.
   isn   t this old news?
   [1*zak7kbzftpfodu4cyaudyw.gif]

   it turns out that over the past two years, deep learning has totally
   rewritten our approach to machine translation. deep learning
   researchers who know almost nothing about language translation are
   throwing together relatively simple machine learning solutions that are
   beating the best expert-built language translation systems in the
   world.

   the technology behind this breakthrough is called sequence-to-sequence
   learning. it   s very powerful technique that be used to solve many kinds
   problems. after we see how it is used for translation, we   ll also learn
   how the exact same algorithm can be used to write ai id70 and
   describe pictures.

   let   s go!
     __________________________________________________________________

making computers translate

   so how do we program a computer to translate human language?

   the simplest approach is to replace every word in a sentence with the
   translated word in the target language. here   s a simple example of
   translating from spanish to english word-by-word:
   [1*aqn40w6gpzk53yzlsifnoq.png]
   we just replace each spanish word with the matching english word.

   this is easy to implement because all you need is a dictionary to look
   up each word   s translation. but the results are bad because it ignores
   grammar and context.

   so the next thing you might do is start adding language-specific rules
   to improve the results. for example, you might translate common
   two-word phrases as a single group. and you might swap the order nouns
   and adjectives since they usually appear in reverse order in spanish
   from how they appear in english:
   [1*gkdstlnohkao--alw0_zaa.png]

   that worked! if we just keep adding more rules until we can handle
   every part of grammar, our program should be able to translate any
   sentence, right?

   this is how the earliest machine translation systems worked. linguists
   came up with complicated rules and programmed them in one-by-one. some
   of the smartest linguists in the world labored for years during the
   cold war to [27]create translation systems as a way to interpret
   russian communications more easily.

   unfortunately this only worked for simple, plainly-structured documents
   like weather reports. it didn   t work reliably for real-world documents.

   the problem is that human language doesn   t follow a fixed set of rules.
   human languages are full of special cases, regional variations, and
   just flat out rule-breaking. the way we speak english more influenced
   by [28]who invaded who hundreds of years ago than it is by someone
   sitting down and defining grammar rules.

making computers translate better using statistics

   after the failure of rule-based systems, new translation approaches
   were developed using models based on id203 and statistics instead
   of grammar rules.

   building a statistics-based translation system requires lots of
   training data where the exact same text is translated into at least two
   languages. this double-translated text is called parallel corpora. in
   the same way that the [29]rosetta stone was used by scientists in the
   1800s to figure out egyptian hieroglyphs from greek, computers can use
   parallel corpora to guess how to convert text from one language to
   another.

   luckily, there   s lots of double-translated text already sitting around
   in strange places. for example, the european parliament translates
   their proceedings into 21 languages. so researchers often use [30]that
   data to help build translation systems.
   [1*gtarmuvb-azarxxiixubxw.png]
   training data is usually exciting! but this is just millions and
   millions of lines of dry government documents   

thinking in probabilities

   the fundamental difference with statistical translation systems is that
   they don   t try to generate one exact translation. instead, they
   generate thousands of possible translations and then they rank those
   translations by likely each is to be correct. they estimate how
      correct    something is by how similar it is to the training data.
   here   s how it works:

step 1: break original sentence into chunks

   first, we break up our sentence into simple chunks that can each be
   easily translated:
   [1*zyf5rgsi4nlpm_yuuqbgjg.png]

step 2: find all possible translations for each chunk

   next, we will translate each of these chunks by finding all the ways
   humans have translated those same chunks of words in our training data.

   it   s important to note that we are not just looking up these chunks in
   a simple translation dictionary. instead, we are seeing how actual
   people translated these same chunks of words in real-world sentences.
   this helps us capture all of the different ways they can be used in
   different contexts:
   [1*lnfhvnrdstd4xcrvgi9t7a.png]
   even the most common phrases have lots of possible translations.

   some of these possible translations are used more frequently than
   others. based on how frequently each translation appears in our
   training data, we can give it a score.

   for example, it   s much more common for someone to say    quiero    to mean
      i want    than to mean    i try.    so we can use how frequently    quiero   
   was translated to    i want    in our training data to give that
   translation more weight than a less frequent translation.

step 3: generate all possible sentences and find the most likely one

   next, we will use every possible combination of these chunks to
   generate a bunch of possible sentences.

   just from the chunk translations we listed in step 2, we can already
   generate nearly 2,500 different variations of our sentence by combining
   the chunks in different ways. here are some examples:

     i love | to leave | at | the seaside | more tidy.
     i mean | to be on | to | the open space | most lovely.
     i like | to be |on | per the seaside | more lovely.
     i mean | to go | to | the open space | most tidy.

   but in a real-world system, there will be even more possible chunk
   combinations because we   ll also try different orderings of words and
   different ways of chunking the sentence:

     i try | to run | at | the prettiest | open space.
     i want | to run | per | the more tidy | open space.
     i mean | to forget | at | the tidiest | beach.
     i try | to go | per | the more tidy | seaside.

   now need to scan through all of these generated sentences to find the
   one that is that sounds the    most human.   

   to do this, we compare each generated sentence to millions of real
   sentences from books and news stories written in english. the more
   english text we can get our hands on, the better.

   take this possible translation:

     i try | to leave | per | the most lovely | open space.

   it   s likely that no one has ever written a sentence like this in
   english, so it would not be very similar to any sentences in our data
   set. we   ll give this possible translation a low id203 score.

   but look at this possible translation:

     i want | to go | to | the prettiest | beach.

   this sentence will be similar to something in our training set, so it
   will get a high id203 score.

   after trying all possible sentences, we   ll pick the sentence that has
   the most likely chunk translations while also being the most similar
   overall to real english sentences.

   our final translation would be    i want to go to the prettiest beach.   
   not bad!

id151 was a huge milestone

   id151 systems perform much better than
   rule-based systems if you give them enough training data. [31]franz
   josef och improved on these ideas and used them to build google
   translate in the early 2000s. machine translation was finally available
   to the world.

   in the early days, it was surprising to everyone that the    dumb   
   approach to translating based on id203 worked better than
   rule-based systems designed by linguists. this led to a (somewhat mean)
   saying among researchers in the 80s:

        every time i fire a linguist, my accuracy goes up.   

         [32]frederick jelinek

the limitations of id151

   id151 systems work well, but they are
   complicated to build and maintain. every new pair of languages you want
   to translate requires experts to tweak and tune a new multi-step
   translation pipeline.

   because it is so much work to build these different pipelines,
   trade-offs have to be made. if you are asking google to translate
   georgian to telegu, it has to internally translate it into english as
   an intermediate step because there   s not enough georgain-to-telegu
   translations happening to justify investing heavily in that language
   pair. and it might do that translation using a less advanced
   translation pipeline than if you had asked it for the more common
   choice of french-to-english.

   wouldn   t it be cool if we could have the computer do all that annoying
   development work for us?

making computers translate better         without all those expensive people

   the holy grail of machine translation is a black box system that learns
   how to translate by itself    just by looking at training data. with
   id151, humans are still needed to build and
   tweak the multi-step statistical models.

   in 2014, [33]kyunghyun cho   s team made a breakthrough. they found a way
   to apply deep learning to build this black box system. their deep
   learning model takes in a parallel corpora and and uses it to learn how
   to translate between those two languages without any human
   intervention.

   two big ideas make this possible         recurrent neural networks and
   encodings. by combining these two ideas in a clever way, we can build a
   self-learning translation system.

recurrent neural networks

   we   ve already [34]talked about recurrent neural networks in part 2, but
   let   s quickly review.

   a regular (non-recurrent) neural network is a generic machine learning
   algorithm that takes in a list of numbers and calculates a result
   (based on previous training). neural networks can be used as a black
   box to solve lots of problems. for example, we can use a neural network
   to calculate the approximate value of a house based on attributes of
   that house:
   [1*xvhk7sbupbpvgnyieljazq.png]

   but like most machine learning algorithms, neural networks are
   stateless. you pass in a list of numbers and the neural network
   calculates a result. if you pass in those same numbers again, it will
   always calculate the same result. it has no memory of past
   calculations. in other words, 2 + 2 always equals 4.

   a recurrent neural network (or id56 for short) is a slightly tweaked
   version of a neural network where the previous state of the neural
   network is one of the inputs to the next calculation. this means that
   previous calculations change the results of future calculations!
   [1*9afnuvfhmftlmdwydt_79w.png]
   humans hate him: 1 weird trick that makes machines smarter!

   why in the world would we want to do this? shouldn   t 2 + 2 always equal
   4 no matter what we last calculated?

   this trick allows neural networks to learn patterns in a sequence of
   data. for example, you can use it to predict the next most likely word
   in a sentence based on the first few words:
   [1*ykxwetzgt4axmnath0nhhg.gif]
   this is one way you could implement    autocorrect    for a smart phone   s
   keyboard app

   id56s are useful any time you want to learn patterns in data. because
   human language is just one big, complicated pattern, id56s are
   increasingly used in many areas of natural language processing.

   if you want to learn more about id56s, [35]you can read part 2 where we
   used one to generate a fake ernest hemingway book and then used another
   one to generate fake super mario brothers levels.

encodings

   the other idea we need to review is encodings. we [36]talked about
   encodings in part 4 as part of face recognition. to explain encodings,
   let   s take a slight detour into how we can tell two different people
   apart with a computer.

   when you are trying to tell two faces apart with a computer, you
   collect different measurements from each face and use those
   measurements to compare faces. for example, we might measure the size
   of each ear or the spacing between the eyes and compare those
   measurements from two pictures to see if they are the same person.

   you   re probably already familiar with this idea from watching any
   primetime detective show like csi:
   [1*_gnyjr3jlpos9grtivmkfq.gif]
   i love this dumb gif from csi so much that i   ll use it again         because
   it is somehow manages to demonstrate this idea clearly while also being
   total nonsense.

   the idea of turning a face into a list of measurements is an example of
   an encoding. we are taking raw data (a picture of a face) and turning
   it into a list of measurements that represent it (the encoding).

   but like we saw in [37]part 4, we don   t have to come up with a specific
   list of facial features to measure ourselves. instead, we can use a
   neural network to generate measurements from a face. the computer can
   do a better job than us in figuring out which measurements are best
   able to differentiate two similar people:
   [1*6kmmqlt4ubcrn7htqnhmkw.png]
   these facial feature measurements are generated by a neural net that
   was trained to make sure different people   s faces resulted in different
   numbers.

   this is our encoding. it lets us represent something very complicated
   (a picture of a face) with something simple (128 numbers). now
   comparing two different faces is much easier because we only have to
   compare these 128 numbers for each face instead of comparing full
   images.

   guess what? we can do the same thing with sentences! we can come up
   with an encoding that represents every possible different sentence as a
   series of unique numbers:
   [1*oa4mrbjwmgmcsevwupg7jq.png]
   this list of numbers represents the english sentence    machine learning
   is fun!   . a different sentence would be represented by a different set
   of numbers.

   to generate this encoding, we   ll feed the sentence into the id56, one
   word at time. the final result after the last word is processed will be
   the values that represent the entire sentence:
   [1*hbpw6frw7mrc607fwm9rkw.gif]
   because the id56 has a    memory    of each word that passed through it, the
   final encoding it calculates represents all the words in the sentence.

   great, so now we have a way to represent an entire sentence as a set of
   unique numbers! we don   t know what each number in the encoding means,
   but it doesn   t really matter. as long as each sentence is uniquely
   identified by it   s own set of numbers, we don   t need to know exactly
   how those numbers were generated.

let   s translate!

   ok, so we know how to use an id56 to encode a sentence into a set of
   unique numbers. how does that help us? here   s where things get really
   cool!

   what if we took two id56s and hooked them up end-to-end? the first id56
   could generate the encoding that represents a sentence. then the second
   id56 could take that encoding and just do the same logic in reverse to
   decode the original sentence again:
   [1*b24hdd3ngjfi4y3enlnogw.png]

   of course being able to encode and then decode the original sentence
   again isn   t very useful. but what if (and here   s the big idea!) we
   could train the second id56 to decode the sentence into spanish instead
   of english? we could use our parallel corpora training data to train it
   to do that:
   [1*fgzlsewnewfo2wo9kzivbw.png]

   and just like that, we have a generic way of converting a sequence of
   english words into an equivalent sequence of spanish words!

   this is a powerful idea:
     * this approach is mostly limited by the amount of training data you
       have and the amount of computer power you can throw at it. machine
       learning researchers only invented this two years ago, but it   s
       already performing as well as id151
       systems that took 20 years to develop.
     * this doesn   t depend on knowing any rules about human language. the
       algorithm figures out those rules itself. this means you don   t need
       experts to tune every step of your translation pipeline. the
       computer does that for you.
     * this approach works for almost any kind of sequence-to-sequence
       problem! and it turns out that lots of interesting problems are
       sequence-to-sequence problems. read on for other cool things you
       can do!

   note that we glossed over some things that are required to make this
   work with real-world data. for example, there   s additional work you
   have to do to deal with different lengths of input and output sentences
   (see [38]bucketing and padding). there   s also issues with
   [39]translating rare words correctly.

building your own sequence-to-sequence translation system

   if you want to build your own language translation system, [40]there   s
   a working demo included with tensorflow that will translate between
   english and french. however, this is not for the faint of heart or for
   those with limited budgets. this technology is still new and very
   resource intensive. even if you have a fast computer with a high-end
   video card, it might take [41]about a month of continuous processing
   time to train your own language translation system.

   also, sequence-to-sequence language translation techniques are
   improving so rapidly that it   s hard to keep up. many recent
   improvements (like adding an [42]attention mechanism or [43]tracking
   context) are significantly improving results but these developments are
   so new that there aren   t even wikipedia pages for them yet. if you want
   to do anything serious with sequence-to-sequence learning, you   ll need
   to keep with new developments as they occur.

the ridiculous power of sequence-to-sequence models

   so what else can we do with sequence-to-sequence models?

   about a year ago, [44]researchers at google showed that you can use
   sequence-to-sequence models to build ai bots. the idea is so simple
   that it   s amazing it works at all.

   first, they captured chat logs between google employees and google   s
   tech support team. then they trained a sequence-to-sequence model where
   the employee   s question was the input sentence and the tech support
   team   s response was the    translation    of that sentence.
   [1*y7ub13jxxmmh-euia7q7wg.png]

   when a user interacted with the bot, they would    translate    each of the
   user   s messages with this system to get the bot   s response.

   the end result was a semi-intelligent bot that could (sometimes) answer
   real tech support questions. here   s part of a sample conversation
   between a user and the bot [45]from their paper:

     human: connection refused or something like that
     machine: may i know the version of network connect you connect?
     human: i am not sure i know that
     machine: is the network connect prompting for any user certificate?
     human: yes
     machine: are you at home or at work at the moment? office?
     human: i am at home
     machine: try this. goto <url redacted> page and enter your name and
     password
     human: done
     machine: try logging in with <name redacted> and check if its
     working fine now
     human: yes, now it works!
     machine: great. anything else that i can help?

   they also tried building a chat bot based on millions of movie
   subtitles. the idea was to use conversations between movie characters
   as a way to train a bot to talk like a human. the input sentence is a
   line of dialog said by one character and the    translation    is what the
   next character said in response:
   [1*_mf74evw0srnxe0omhmeva.png]

   this produced really interesting results. not only did the bot converse
   like a human, but it displayed a small bit of intelligence:

     human: my name is david. what is my name?
     machine: david.
     human: my name is john. what is my name?
     machine: john.
     human: are you a leader or a follower?
     machine: i   m a leader.
     human: are you a follower or a leader?
     machine: i   m a leader.

   this is only the beginning of the possibilities. we aren   t limited to
   converting one sentence into another sentence. it   s also possible to
   make an image-to-sequence model that can turn an image into text!

   [46]a different team at google did this by replacing the first id56 with
   a convolutional neural network ([47]like we learned about in part 3).
   this allows the input to be a picture instead of a sentence. the rest
   works basically the same way:
   [1*e-vldp_dgnii7wbivem05a.png]

   and just like that, we can turn pictures into words (as long as we have
   lots and lots of training data)!

   [48]andrej karpathy [49]expanded on these ideas to build a system
   capable of describing images in great detail by processing multiple
   regions of an image separately:
   [1*1xaplpnllfzzlxm1rxt19a.png]
   image from [50]this paper by [51]andrej karpathy

   this makes it possible to build [52]image search engines that are
   capable of finding images that match oddly specific search queries:
   [1*sxrtnr7buqrqqyopke21kw.gif]
   example from
   [53]http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/

   there   s even [54]researchers working on the reverse problem, generating
   an entire picture based on just a text description!

   just from these examples, you can start to imagine the possibilities.
   so far, there have been sequence-to-sequence [55]applications in
   everything from id103 to id161. i bet there will
   be a lot more over the next year.

   if you want to learn more in depth about sequence-to-sequence models
   and translation, here   s some recommended resources:
     * [56]richard socher   s cs224d lecture    fancy recurrent neural
       networks for machine translation (video)
     * [57]thang luong   s cs224d lecture         neural machine transation (pdf)
     * [58]tensorflow   s description of id195 modeling
     * [59]the deep learning book   s chapter on sequence to sequence
       learning (pdf)
     __________________________________________________________________

   if you liked this article, please consider [60]signing up for my
   machine learning is fun! email list. i   ll only email you when i have
   something new and awesome to share. it   s the best way to find out when
   i write more articles like this.

   you can also follow me on twitter at [61]@ageitgey, [62]email me
   directly or [63]find me on linkedin. i   d love to hear from you if i can
   help you or your team with machine learning.

   now continue on to [64]machine learning is fun! part 6!

     * [65]machine learning
     * [66]artificial intelligence
     * [67]translation

   (button)
   (button)
   (button) 8.8k claps
   (button) (button) (button) 59 (button) (button)

     (button) blockedunblock (button) followfollowing
   [68]go to the profile of adam geitgey

[69]adam geitgey

   interested in computers and machine learning. likes to write about it.

     * (button)
       (button) 8.8k
     * (button)
     *
     *

   [70]go to the profile of adam geitgey
   never miss a story from adam geitgey, when you sign up for medium.
   [71]learn more
   never miss a story from adam geitgey
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/2ace0acca0aa
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@ageitgey?source=post_header_lockup
  10. https://medium.com/@ageitgey
  11. https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471
  12. https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3
  13. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721
  14. https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78
  15. https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa
  16. https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a
  17. https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7
  18. https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196
  19. https://zhuanlan.zhihu.com/p/24590838
  20. http://algotravelling.com/ru/                -                -      -            -5/
  21. https://medium.com/@jongdae.lim/      -      -machine-learning-   -         -part-5-83b7a44b797a
  22. https://viblo.asia/p/machine-learning-that-thu-vi-5-dich-ngon-ngu-va-mo-ta-anh-eb85ojnml2g
  23. https://medium.com/botsupply/il-machine-learning-  -divertente-parte-5-5e9083caf8f3
  24. https://www.machinelearningisfun.com/get-the-book/
  25. https://www.machinelearningisfun.com/get-the-book/
  26. https://translate.google.com/
  27. https://en.wikipedia.org/wiki/georgetown   ibm_experiment
  28. https://en.wikipedia.org/wiki/history_of_english#middle_english
  29. https://en.wikipedia.org/wiki/rosetta_stone
  30. http://www.statmt.org/europarl/
  31. https://en.wikipedia.org/wiki/franz_josef_och
  32. https://en.wikipedia.org/wiki/frederick_jelinek
  33. http://arxiv.org/abs/1406.1078
  34. https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3
  35. https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3
  36. https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78
  37. https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78
  38. https://www.tensorflow.org/versions/r0.10/tutorials/id195/index.html#bucketing-and-padding
  39. https://cs224d.stanford.edu/papers/addressing.pdf
  40. https://www.tensorflow.org/versions/r0.10/tutorials/id195/index.html
  41. https://github.com/tensorflow/tensorflow/issues/600#issuecomment-226333266
  42. http://stanford.edu/~lmthang/data/papers/emnlp15_attn.pdf
  43. http://arxiv.org/abs/1607.00578
  44. https://arxiv.org/pdf/1506.05869.pdf
  45. https://arxiv.org/pdf/1506.05869.pdf
  46. http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/vinyals_show_and_tell_2015_cvpr_paper.pdf
  47. https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.yzzx24elr
  48. http://cs.stanford.edu/people/karpathy/
  49. http://cs.stanford.edu/people/karpathy/cvpr2015.pdf
  50. http://cs.stanford.edu/people/karpathy/cvpr2015.pdf
  51. http://cs.stanford.edu/people/karpathy/
  52. http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/
  53. http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/
  54. http://arxiv.org/pdf/1506.03500.pdf
  55. https://github.com/kjw0612/awesome-id56#applications
  56. https://www.youtube.com/watch?v=qglmw2n4s1w
  57. http://cs224d.stanford.edu/lectures/cs224d-lecture15.pdf
  58. https://www.tensorflow.org/versions/r0.10/tutorials/id195/index.html
  59. http://www.deeplearningbook.org/contents/id56.html
  60. http://eepurl.com/b9fg2t
  61. https://twitter.com/ageitgey
  62. mailto:ageitgey@gmail.com
  63. https://www.linkedin.com/in/ageitgey
  64. https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a
  65. https://medium.com/tag/machine-learning?source=post
  66. https://medium.com/tag/artificial-intelligence?source=post
  67. https://medium.com/tag/translation?source=post
  68. https://medium.com/@ageitgey?source=footer_card
  69. https://medium.com/@ageitgey
  70. https://medium.com/@ageitgey
  71. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  73. https://medium.com/p/2ace0acca0aa/share/twitter
  74. https://medium.com/p/2ace0acca0aa/share/facebook
  75. https://medium.com/p/2ace0acca0aa/share/twitter
  76. https://medium.com/p/2ace0acca0aa/share/facebook
