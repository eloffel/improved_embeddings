    #[1]id111 online    feed [2]id111 online    comments feed
   [3]id111 online    dive into nltk, part iii: part-of-speech
   tagging and pos tagger comments feed [4]getting started with mbsp
   [5]dive into nltk, part iv: id30 and lemmatization [6]alternate
   [7]alternate

   [ins: :ins]

   [8]   



   javascript is disabled. please enable javascript on your browser to
   best view this site.

[9]id111 online

   search for: ____________________ search

id111 | text analysis | text process | natural language processing

   id111 online
     * [10]home
     * [11]textanalysis
     * [12]keywordextraction
     * [13]textsummarization
     * [14]wordsimilarity
     * [15]about

   [16]home   [17]nltk   dive into nltk, part iii: part-of-speech tagging and
   pos tagger
   [ins: :ins]

post navigation

   [18]    getting started with mbsp
   [19]dive into nltk, part iv: id30 and lemmatization    

dive into nltk, part iii: part-of-speech tagging and pos tagger

   posted on [20]july 9, 2014 by [21]textminermarch 26, 2017
   [22]deep learning specialization on coursera

   this is the third article in the series    [23]dive into nltk   , here is
   an index of all the articles in the series that have been published to
   date:
   [ins: :ins]

   [24]part i: getting started with nltk
   [25]part ii: sentence tokenize and word tokenize
   [26]part iii: part-of-speech tagging and pos tagger
   [27]part iv: id30 and lemmatization
   [28]part v: using stanford text analysis tools in python
   [29]part vi: add stanford word segmenter interface for python nltk
   [30]part vii: a preliminary study on text classification
   [31]part viii: using external maximum id178 modeling libraries for
   text classification
   [32]part ix: from text classification to id31
   [33]part x: play with id97 models based on nltk corpus

   part-of-speech tagging is one of the most important [34]text analysis
   tasks used to classify words into their part-of-speech and label them
   according the tagset which is a collection of tags used for the pos
   tagging. part-of-speech tagging also known as word classes or lexical
   categories. here is the definition from [35]wikipedia:

     in corpus linguistics, part-of-speech tagging ([36]id52 or
     post), also called grammatical tagging or word-category
     disambiguation, is the process of marking up a word in a text
     (corpus) as corresponding to a particular part of speech, based on
     both its definition, as well as its context   i.e. relationship with
     adjacent and related words in a phrase, sentence, or paragraph. a
     simplified form of this is commonly taught to school-age children,
     in the identification of words as nouns, verbs, adjectives, adverbs,
     etc.

     once performed by hand, id52 is now done in the context of
     computational linguistics, using algorithms which associate discrete
     terms, as well as hidden parts of speech, in accordance with a set
     of descriptive tags. pos-tagging algorithms fall into two
     distinctive groups: rule-based and stochastic. e. brill   s tagger,
     one of the first and most widely used english pos-taggers, employs
     rule-based algorithms.

   how to use [37]id52 in nltk
   after import nltk in python interpreter, you should use
   [38]word_tokenize before id52, which referred as pos_tag method:

   >>> import nltk
   >>> text = nltk.word_tokenize(   dive into nltk: part-of-speech tagging
   and pos tagger   )
   >>> text
   [   dive   ,    into   ,    nltk   ,    :   ,    part-of-speech   ,    tagging   ,    and   ,
      pos   ,    tagger   ]
   >>> nltk.pos_tag(text)
   [(   dive   ,    jj   ), (   into   ,    in   ), (   nltk   ,    nnp   ), (   :   ,    :   ),
   (   part-of-speech   ,    jj   ), (   tagging   ,    nn   ), (   and   ,    cc   ), (   pos   ,
      nnp   ), (   tagger   ,    nnp   )]

   nltk provides documentation for each tag, which can be queried using
   the tag, e.g., nltk.help.upenn_tagset(   rb   ), or a regular expression,
   e.g., nltk.help.upenn_brown_tagset(   nn.*   ):

   >>> nltk.help.upenn_tagset(   jj   )
   jj: adjective or numeral, ordinal
   third ill-mannered pre-war regrettable oiled calamitous first separable
   ectoplasmic battery-powered participatory fourth still-to-be-named
   multilingual multi-disciplinary    
   >>> nltk.help.upenn_tagset(   in   )
   in: preposition or conjunction, subordinating
   astride among uppon whether out inside pro despite on by throughout
   below within for towards near behind atop around if like until below
   next into if beside    
   >>> nltk.help.upenn_tagset(   nnp   )
   nnp: noun, proper, singular
   motown venneboerger czestochwa ranzer conchita trumplane christos
   oceanside escobar kreisler sawyer cougar yvette ervin odi darryl ctca
   shannon a.k.c. meltex liverpool    
   >>>

   nltk also provide batch id52 method for document id52,
   which is batch_pos_tag method:

   >>> nltk.batch_pos_tag([[   this   ,    is   ,    batch   ,    tag   ,    test   ],
   [   nltk   ,    is   ,    text   ,    analysis   ,    tool   ]])
   [[(   this   ,    dt   ), (   is   ,    vbz   ), (   batch   ,    nn   ), (   tag   ,    nn   ),
   (   test   ,    nn   )], [(   nltk   ,    nn   ), (   is   ,    vbz   ), (   text   ,    jj   ),
   (   analysis   ,    nn   ), (   tool   ,    nn   )]]
   >>>

   the pre-trained [39]pos tagger model in nltk

   you can find the pre-trained id52 model in nltk_data/taggers:

     yangtekimacbook-pro:taggers textminer$ pwd
     /users/textminer/nltk_data/taggers
     yangtekimacbook-pro:taggers textminer$ ls
     total 11304
     drwxr-xr-x 3 textminer staff 102 7 9 14:06 id48_treebank_pos_tagger
     -rw-r   r    1 textminer staff 750857 5 26 2013
     id48_treebank_pos_tagger.zip
     drwxr-xr-x 3 textminer staff 102 7 24 2013
     maxent_treebank_pos_tagger
     -rw-r   r    1 textminer staff 5031883 5 26 2013
     maxent_treebank_pos_tagger.zip

   the default pos tagger model using in nltk is
   maxent_treebanck_pos_tagger model, you can find the code in
   nltk-master/nltk/tag/__init__.py:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

# standard treebank pos tagger
_pos_tagger = 'taggers/maxent_treebank_pos_tagger/english.pickle'
def pos_tag(tokens):
    """
    use nltk's currently recommended part of speech tagger to
    tag the given list of tokens.

        >>> from nltk.tag import pos_tag # doctest: +skip
        >>> from nltk.tokenize import word_tokenize # doctest: +skip
        >>> pos_tag(word_tokenize("john's big idea isn't all that bad.")) # doct
est: +skip
        [('john', 'nnp'), ("'s", 'pos'), ('big', 'jj'), ('idea', 'nn'), ('is',
        'vbz'), ("n't", 'rb'), ('all', 'dt'), ('that', 'dt'), ('bad', 'jj'),
        ('.', '.')]

    :param tokens: sequence of tokens to be tagged
    :type tokens: list(str)
    :return: the tagged tokens
    :rtype: list(tuple(str, str))
    """
    tagger = load(_pos_tagger)
    return tagger.tag(tokens)

def batch_pos_tag(sentences):
    """
    use nltk's currently recommended part of speech tagger to tag the
    given list of sentences, each consisting of a list of tokens.
    """
    tagger = load(_pos_tagger)
    return tagger.batch_tag(sentences)

   # standard treebank pos tagger _pos_tagger =
   'taggers/maxent_treebank_pos_tagger/english.pickle' def
   pos_tag(tokens): """ use nltk's currently recommended part of speech
   tagger to tag the given list of tokens. >>> from nltk.tag import
   pos_tag # doctest: +skip >>> from nltk.tokenize import word_tokenize #
   doctest: +skip >>> pos_tag(word_tokenize("john's big idea isn't all
   that bad.")) # doctest: +skip [('john', 'nnp'), ("'s", 'pos'), ('big',
   'jj'), ('idea', 'nn'), ('is', 'vbz'), ("n't", 'rb'), ('all', 'dt'),
   ('that', 'dt'), ('bad', 'jj'), ('.', '.')] :param tokens: sequence of
   tokens to be tagged :type tokens: list(str) :return: the tagged tokens
   :rtype: list(tuple(str, str)) """ tagger = load(_pos_tagger) return
   tagger.tag(tokens) def batch_pos_tag(sentences): """ use nltk's
   currently recommended part of speech tagger to tag the given list of
   sentences, each consisting of a list of tokens. """ tagger =
   load(_pos_tagger) return tagger.batch_tag(sentences)

   how to train a id52 model or pos tagger in nltk
   you have used the maxent treebank id52 model in nltk by default,
   and nltk provides not only the maxent pos tagger, but other pos taggers
   like crf, id48, brill, tnt and interfaces with stanford pos tagger,
   hunpos pos tagger and senna postaggers:

     -rwxr-xr-x@ 1 textminer staff 4.4k 7 22 2013 __init__.py
     -rwxr-xr-x@ 1 textminer staff 2.9k 7 22 2013 api.py
     -rwxr-xr-x@ 1 textminer staff 56k 7 22 2013 brill.py
     -rwxr-xr-x@ 1 textminer staff 31k 7 22 2013 crf.py
     -rwxr-xr-x@ 1 textminer staff 48k 7 22 2013 id48.py
     -rwxr-xr-x@ 1 textminer staff 5.1k 7 22 2013 hunpos.py
     -rwxr-xr-x@ 1 textminer staff 11k 7 22 2013 senna.py
     -rwxr-xr-x@ 1 textminer staff 26k 7 22 2013 sequential.py
     -rwxr-xr-x@ 1 textminer staff 3.3k 7 22 2013 simplify.py
     -rwxr-xr-x@ 1 textminer staff 6.4k 7 22 2013 stanford.py
     -rwxr-xr-x@ 1 textminer staff 18k 7 22 2013 tnt.py
     -rwxr-xr-x@ 1 textminer staff 2.3k 7 22 2013 util.py

   here we will show you how to train a tnt pos tagger model, you can find
   the details about tnt in tnt.py:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83

# natural language toolkit: tnt tagger
#
# copyright (c) 2001-2013 nltk project
# author: sam huston <sjh900@gmail.com>
#
# url: <http://www.nltk.org/>
# for license information, see license.txt

'''
implementation of 'tnt - a statisical part of speech tagger'
by thorsten brants

http://acl.ldc.upenn.edu/a/a00/a00-1031.pdf
'''
from __future__ import print_function
from math import log

from operator import itemgetter

from nltk.id203 import freqdist, conditionalfreqdist
from nltk.tag.api import taggeri

class tnt(taggeri):
    '''
    tnt - statistical pos tagger

    important notes:

    * does not automatically deal with unseen words

      - it is possible to provide an untrained pos tagger to
        create tags for unknown words, see __init__ function

    * should be used with sentence-delimited input

      - due to the nature of this tagger, it works best when
        trained over sentence delimited input.
      - however it still produces good results if the training
        data and testing data are separated on all punctuation eg: [,.?!]
      - input for training is expected to be a list of sentences
        where each sentence is a list of (word, tag) tuples
      - input for tag function is a single sentence
        input for tagdata function is a list of sentences
        output is of a similar form

    * function provided to process text that is unsegmented

      - please see basic_sent_chop()


    tnt uses a second order markov model to produce tags for
    a sequence of input, specifically:

      argmax [proj(p(t_i|t_i-1,t_i-2)p(w_i|t_i))] p(t_t+1 | t_t)

    ie: the maximum projection of a set of probabilities

    the set of possible tags for a given word is derived
    from the training data. it is the set of all tags
    that exact word has been assigned.

    to speed up and get more precision, we can use log addition
    to instead multiplication, specifically:

      argmax [sigma(log(p(t_i|t_i-1,t_i-2))+log(p(w_i|t_i)))] +
             log(p(t_t+1|t_t))

    the id203 of a tag for a given word is the linear
    interpolation of 3 markov models; a zero-order, first-order,
    and a second order model.

      p(t_i| t_i-1, t_i-2) = l1*p(t_i) + l2*p(t_i| t_i-1) +
                             l3*p(t_i| t_i-1, t_i-2)

    a id125 is used to limit the memory usage of the algorithm.
    the degree of the beam can be changed using n in the initialization.
    n represents the maximum number of possible solutions to maintain
    while tagging.

    it is possible to differentiate the tags which are assigned to
    capitalized words. however this does not result in a significant
    gain in the accuracy of the results.
    '''

   # natural language toolkit: tnt tagger # # copyright (c) 2001-2013 nltk
   project # author: sam huston <sjh900@gmail.com> # # url:
   <http://www.nltk.org/> # for license information, see license.txt '''
   implementation of 'tnt - a statisical part of speech tagger' by
   thorsten brants http://acl.ldc.upenn.edu/a/a00/a00-1031.pdf ''' from
   __future__ import print_function from math import log from operator
   import itemgetter from nltk.id203 import freqdist,
   conditionalfreqdist from nltk.tag.api import taggeri class
   tnt(taggeri): ''' tnt - statistical pos tagger important notes: * does
   not automatically deal with unseen words - it is possible to provide an
   untrained pos tagger to create tags for unknown words, see __init__
   function * should be used with sentence-delimited input - due to the
   nature of this tagger, it works best when trained over sentence
   delimited input. - however it still produces good results if the
   training data and testing data are separated on all punctuation eg:
   [,.?!] - input for training is expected to be a list of sentences where
   each sentence is a list of (word, tag) tuples - input for tag function
   is a single sentence input for tagdata function is a list of sentences
   output is of a similar form * function provided to process text that is
   unsegmented - please see basic_sent_chop() tnt uses a second order
   markov model to produce tags for a sequence of input, specifically:
   argmax [proj(p(t_i|t_i-1,t_i-2)p(w_i|t_i))] p(t_t+1 | t_t) ie: the
   maximum projection of a set of probabilities the set of possible tags
   for a given word is derived from the training data. it is the set of
   all tags that exact word has been assigned. to speed up and get more
   precision, we can use log addition to instead multiplication,
   specifically: argmax [sigma(log(p(t_i|t_i-1,t_i-2))+log(p(w_i|t_i)))] +
   log(p(t_t+1|t_t)) the id203 of a tag for a given word is the
   linear interpolation of 3 markov models; a zero-order, first-order, and
   a second order model. p(t_i| t_i-1, t_i-2) = l1*p(t_i) + l2*p(t_i|
   t_i-1) + l3*p(t_i| t_i-1, t_i-2) a id125 is used to limit the
   memory usage of the algorithm. the degree of the beam can be changed
   using n in the initialization. n represents the maximum number of
   possible solutions to maintain while tagging. it is possible to
   differentiate the tags which are assigned to capitalized words. however
   this does not result in a significant gain in the accuracy of the
   results. '''

   first you need the train dada and test data, we use the treebank data
   from nltk.corpus:

   >>> from nltk.corpus import treebank
   >>> len(treebank.tagged_sents())
   3914
   >>> train_data = treebank.tagged_sents()[:3000]
   >>> test_data = treebank.tagged_sents()[3000:]
   >>> train_data[0]
   [(u   pierre   , u   nnp   ), (u   vinken   , u   nnp   ), (u   ,   , u   ,   ), (u   61   ,
   u   cd   ), (u   years   , u   nns   ), (u   old   , u   jj   ), (u   ,   , u   ,   ), (u   will   ,
   u   md   ), (u   join   , u   vb   ), (u   the   , u   dt   ), (u   board   , u   nn   ), (u   as   ,
   u   in   ), (u   a   , u   dt   ), (u   nonexecutive   , u   jj   ), (u   director   , u   nn   ),
   (u   nov.   , u   nnp   ), (u   29   , u   cd   ), (u   .   , u   .   )]
   >>> test_data[0]
   [(u   at   , u   in   ), (u   tokyo   , u   nnp   ), (u   ,   , u   ,   ), (u   the   , u   dt   ),
   (u   nikkei   , u   nnp   ), (u   index   , u   nn   ), (u   of   , u   in   ), (u   225   ,
   u   cd   ), (u   selected   , u   vbn   ), (u   issues   , u   nns   ), (u   ,   , u   ,   ),
   (u   which   , u   wdt   ), (u   *t*-1   , u   -none-   ), (u   gained   , u   vbd   ),
   (u   132   , u   cd   ), (u   points   , u   nns   ), (u   tuesday   , u   nnp   ), (u   ,   ,
   u   ,   ), (u   added   , u   vbd   ), (u   14.99   , u   cd   ), (u   points   , u   nns   ),
   (u   to   , u   to   ), (u   35564.43   , u   cd   ), (u   .   , u   .   )]
   >>>

   we use the first 3000 treebank tagged sentences as the train_data, and
   last 914 tagged sentences as the test_data, now we train tnt pos tagger
   by the train_data and evaluate it by the test_data:

   >>> from nltk.tag import tnt
   >>> tnt_pos_tagger = tnt.tnt()
   >>> tnt_pos_tagger.train(train_data)
   >>> tnt_pos_tagger.evaluate(test_data)
   0.8755881718109216

   you can save this pos tagger model as a pickle file:

   >>> import pickle
   >>> f = open(   tnt_treebank_pos_tagger.pickle   ,    w   )
   >>> pickle.dump(tnt_pos_tagger, f)
   >>> f.close()

   and you can use it any time you want:

   >>> tnt_tagger.tag(nltk.word_tokenize(   this is a tnt treebank tnt
   tagger   ))
   [(   this   , u   dt   ), (   is   , u   vbz   ), (   a   , u   dt   ), (   tnt   ,    unk   ),
   (   treebank   ,    unk   ), (   tnt   ,    unk   ), (   tagger   ,    unk   )]
   >>>

   that   s it, now you can train your [40]id52 model by yourself,
   just do it.

   posted by [41]textminer

related posts:

    1. [42]getting started with pattern
    2. [43]dive into nltk, part v: using stanford text analysis tools in
       python
    3. [44]text analysis online no longer provides nltk stanford nlp api
       interface
    4. [45]nltk id138 word lemmatizer api for english word with pos tag
       only

   [46]deep learning specialization on coursera

   posted in [47]nltk, [48]text analysis, [49]id111, [50]text
   processing tagged [51]brill pos tagger, [52]crf pos tagger, [53]id48 pos
   tagger, [54]maxent pos tagger, [55]nltk, [56]nltk pos tagger, [57]nltk
   id52, [58]part-of-speech tagging, [59]pos, [60]pos tagger,
   [61]id52, [62]stanford pos tagger, [63]tnt pos tagger, [64]train
   a pos tagger [65]permalink

post navigation

   [66]    getting started with mbsp
   [67]dive into nltk, part iv: id30 and lemmatization    
     __________________________________________________________________

comments

dive into nltk, part iii: part-of-speech tagging and pos tagger     5 comments

    1.
   joe on [68]november 6, 2015 at 1:39 am said:
       thank you for this tutorial! it   s very easy to follow and your
       explanations are clear.
       towards the end, i got an error message at two places. one was at
       pickle.dump(   ). i changed it to    wb    at f=open(      w   ) and that fixed
       the problem. also, tnt_tagger.tag(   ) did not work, but
       tnt_pos_tagger.tag(   ) worked fine.
       [69]reply    
    2.
   bastien on [70]june 8, 2016 at 10:08 am said:
       same error as joe here : tnt_tagger.tag(   ) does not work.
       [71]reply    
          +
        bastien on [72]june 8, 2016 at 10:11 am said:
            so as joe also mentionnened tnt_pos_tagger.tag works fine for
            me too
            [73]reply    
    3.
   theoriginalg on [74]august 10, 2016 at 9:02 pm said:
       great comment joe, that helped me out too!
       [75]reply    
    4.
   [76]gereziher on [77]january 27, 2017 at 7:19 pm said:
       works better but still with a very nice dangerious accuracy of
       0.25% over unknown words,lol!.
       [78]reply    

leave a reply [79]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   [ ] save my name, email, and website in this browser for the next time
   i comment.

   post comment

   [80][dlai-logo-final-minus-font-plus-white-backg.png]
   [show?id=9iqcvd3eeqc&bids=541296.11421701896&type=2&subid=0]

   search for: ____________________ search

   [ins: :ins]

recent posts

     * [81]deep learning practice for nlp: large movie review data
       id31 from scratch
     * [82]best coursera courses for data science
     * [83]best coursera courses for machine learning
     * [84]best coursera courses for deep learning
     * [85]dive into nlp with deep learning, part i: getting started with
       dl4nlp

recent comments

     * textminer on [86]training id97 model on english wikipedia by
       gensim
     * ankit ramani on [87]training id97 model on english wikipedia by
       gensim
     * vincent on [88]training id97 model on english wikipedia by
       gensim
     * muhammad amin nadim on [89]andrew ng deep learning specialization:
       best deep learning course for beginners and deep learners
     * saranya on [90]training id97 model on english wikipedia by
       gensim

archives

     * [91]november 2018
     * [92]august 2018
     * [93]july 2018
     * [94]june 2018
     * [95]january 2018
     * [96]october 2017
     * [97]september 2017
     * [98]august 2017
     * [99]july 2017
     * [100]may 2017
     * [101]april 2017
     * [102]march 2017
     * [103]december 2016
     * [104]october 2016
     * [105]august 2016
     * [106]july 2016
     * [107]june 2016
     * [108]may 2016
     * [109]april 2016
     * [110]february 2016
     * [111]december 2015
     * [112]november 2015
     * [113]september 2015
     * [114]may 2015
     * [115]april 2015
     * [116]march 2015
     * [117]february 2015
     * [118]january 2015
     * [119]december 2014
     * [120]november 2014
     * [121]october 2014
     * [122]september 2014
     * [123]july 2014
     * [124]june 2014
     * [125]may 2014
     * [126]april 2014
     * [127]january 2014

categories

     * [128]ainlp
     * [129]coursera course
     * [130]data science
     * [131]deep learning
     * [132]dl4nlp
     * [133]how to use mashape api
     * [134]keras
     * [135]machine learning
     * [136]id39
     * [137]nlp
     * [138]nlp tools
     * [139]nltk
     * [140]id31
     * [141]tensorflow
     * [142]text analysis
     * [143]text classification
     * [144]id111
     * [145]text processing
     * [146]text similarity
     * [147]text summarization
     * [148]textanalysis api
     * [149]uncategorized
     * [150]id27
     * [151]id40

meta

     * [152]log in
     * [153]entries rss
     * [154]comments rss
     * [155]wordpress.org

     [156]text analysis online

     [157]text summarizer

     [158]text processing

     [159]word similarity

     [160]best coursera course

     [161]best coursera courses

     [162]elastic patent

     2019 - [163]id111 online - [164]weaver xtreme theme

   [165]   

references

   visible links
   1. https://textminingonline.com/feed
   2. https://textminingonline.com/comments/feed
   3. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger/feed
   4. https://textminingonline.com/getting-started-with-mbsp
   5. https://textminingonline.com/dive-into-nltk-part-iv-id30-and-lemmatization
   6. https://textminingonline.com/wp-json/oembed/1.0/embed?url=https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger
   7. https://textminingonline.com/wp-json/oembed/1.0/embed?url=https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger&format=xml
   8. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger#page-bottom
   9. https://textminingonline.com/
  10. https://textminingonline.com/
  11. http://textanalysisonline.com/#new_tab
  12. http://keywordextraction.net/#new_tab
  13. http://textsummarization.net/#new_tab
  14. https://wordsimilarity.com/#new_tab
  15. https://textminingonline.com/about
  16. https://textminingonline.com/
  17. https://textminingonline.com/category/nltk
  18. https://textminingonline.com/getting-started-with-mbsp
  19. https://textminingonline.com/dive-into-nltk-part-iv-id30-and-lemmatization
  20. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger
  21. https://textminingonline.com/author/yuzhen
  22. https://click.linksynergy.com/fs-bin/click?id=9iqcvd3eeqc&offerid=467035.416&subid=0&type=4
  23. http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk
  24. http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk
  25. http://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize
  26. http://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger
  27. http://textminingonline.com/dive-into-nltk-part-iv-id30-and-lemmatization
  28. http://textminingonline.com/dive-into-nltk-part-v-using-stanford-text-analysis-tools-in-python
  29. http://textminingonline.com/dive-into-nltk-part-vi-add-stanford-word-segmenter-interface-for-python-nltk
  30. http://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification
  31. http://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
  32. http://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis
  33. http://textminingonline.com/?p=872
  34. http://textanalysisonline.com/
  35. http://en.wikipedia.org/wiki/part-of-speech_tagging
  36. http://textanalysisonline.com/nltk-pos-tagging
  37. http://textanalysisonline.com/nltk-pos-tagging
  38. http://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize
  39. http://textanalysisonline.com/nltk-pos-tagging
  40. http://textanalysisonline.com/nltk-pos-tagging
  41. http://textminingonline.com/
  42. https://textminingonline.com/getting-started-with-pattern
  43. https://textminingonline.com/dive-into-nltk-part-v-using-stanford-text-analysis-tools-in-python
  44. https://textminingonline.com/text-analysis-online-no-longer-provides-nltk-stanford-nlp-api-interface
  45. https://textminingonline.com/nltk-id138-word-lemmatizer-api-for-english-word-with-pos-tag-only
  46. https://click.linksynergy.com/fs-bin/click?id=9iqcvd3eeqc&offerid=467035.414&subid=0&type=4
  47. https://textminingonline.com/category/nltk
  48. https://textminingonline.com/category/text-analysis
  49. https://textminingonline.com/category/text-mining
  50. https://textminingonline.com/category/text-processing
  51. https://textminingonline.com/tag/brill-pos-tagger
  52. https://textminingonline.com/tag/crf-pos-tagger
  53. https://textminingonline.com/tag/id48-pos-tagger
  54. https://textminingonline.com/tag/maxent-pos-tagger
  55. https://textminingonline.com/tag/nltk
  56. https://textminingonline.com/tag/nltk-pos-tagger
  57. https://textminingonline.com/tag/nltk-pos-tagging
  58. https://textminingonline.com/tag/part-of-speech-tagging
  59. https://textminingonline.com/tag/pos
  60. https://textminingonline.com/tag/pos-tagger
  61. https://textminingonline.com/tag/pos-tagging
  62. https://textminingonline.com/tag/stanford-pos-tagger
  63. https://textminingonline.com/tag/tnt-pos-tagger
  64. https://textminingonline.com/tag/train-a-pos-tagger
  65. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger
  66. https://textminingonline.com/getting-started-with-mbsp
  67. https://textminingonline.com/dive-into-nltk-part-iv-id30-and-lemmatization
  68. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger#comment-113278
  69. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger?replytocom=113278#respond
  70. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger#comment-119780
  71. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger?replytocom=119780#respond
  72. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger#comment-119781
  73. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger?replytocom=119781#respond
  74. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger#comment-121163
  75. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger?replytocom=121163#respond
  76. http://-/
  77. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger#comment-128147
  78. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger?replytocom=128147#respond
  79. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger#respond
  80. https://click.linksynergy.com/link?id=9iqcvd3eeqc&offerid=541296.11421701896&type=2&murl=https://www.coursera.org/specializations/deep-learning
  81. https://textminingonline.com/deep-learning-practice-for-nlp-large-movie-review-data-sentiment-analysis-from-scratch
  82. https://textminingonline.com/best-coursera-courses-for-data-science
  83. https://textminingonline.com/best-coursera-courses-for-machine-learning
  84. https://textminingonline.com/best-coursera-courses-for-deep-learning
  85. https://textminingonline.com/dive-into-nlp-with-deep-learning-part-i-getting-started-with-dl4nlp
  86. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-138841
  87. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-138807
  88. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-138723
  89. https://textminingonline.com/andrew-ng-deep-learning-specialization-best-deep-learning-course-for-beginners-and-deep-learners#comment-138475
  90. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-137923
  91. https://textminingonline.com/2018/11
  92. https://textminingonline.com/2018/08
  93. https://textminingonline.com/2018/07
  94. https://textminingonline.com/2018/06
  95. https://textminingonline.com/2018/01
  96. https://textminingonline.com/2017/10
  97. https://textminingonline.com/2017/09
  98. https://textminingonline.com/2017/08
  99. https://textminingonline.com/2017/07
 100. https://textminingonline.com/2017/05
 101. https://textminingonline.com/2017/04
 102. https://textminingonline.com/2017/03
 103. https://textminingonline.com/2016/12
 104. https://textminingonline.com/2016/10
 105. https://textminingonline.com/2016/08
 106. https://textminingonline.com/2016/07
 107. https://textminingonline.com/2016/06
 108. https://textminingonline.com/2016/05
 109. https://textminingonline.com/2016/04
 110. https://textminingonline.com/2016/02
 111. https://textminingonline.com/2015/12
 112. https://textminingonline.com/2015/11
 113. https://textminingonline.com/2015/09
 114. https://textminingonline.com/2015/05
 115. https://textminingonline.com/2015/04
 116. https://textminingonline.com/2015/03
 117. https://textminingonline.com/2015/02
 118. https://textminingonline.com/2015/01
 119. https://textminingonline.com/2014/12
 120. https://textminingonline.com/2014/11
 121. https://textminingonline.com/2014/10
 122. https://textminingonline.com/2014/09
 123. https://textminingonline.com/2014/07
 124. https://textminingonline.com/2014/06
 125. https://textminingonline.com/2014/05
 126. https://textminingonline.com/2014/04
 127. https://textminingonline.com/2014/01
 128. https://textminingonline.com/category/ainlp
 129. https://textminingonline.com/category/coursera-course
 130. https://textminingonline.com/category/data-science
 131. https://textminingonline.com/category/deep-learning
 132. https://textminingonline.com/category/dl4nlp
 133. https://textminingonline.com/category/how-to-use-mashape-api
 134. https://textminingonline.com/category/keras
 135. https://textminingonline.com/category/machine-learning
 136. https://textminingonline.com/category/named-entity-recognition
 137. https://textminingonline.com/category/nlp
 138. https://textminingonline.com/category/nlp-tools
 139. https://textminingonline.com/category/nltk
 140. https://textminingonline.com/category/sentiment-analysis
 141. https://textminingonline.com/category/tensorflow
 142. https://textminingonline.com/category/text-analysis
 143. https://textminingonline.com/category/text-classification
 144. https://textminingonline.com/category/text-mining
 145. https://textminingonline.com/category/text-processing
 146. https://textminingonline.com/category/text-similarity
 147. https://textminingonline.com/category/text-summarization
 148. https://textminingonline.com/category/textanalysis-api-2
 149. https://textminingonline.com/category/uncategorized
 150. https://textminingonline.com/category/word-embedding
 151. https://textminingonline.com/category/word-segmentation
 152. https://textminingonline.com/wp-login.php
 153. https://textminingonline.com/feed
 154. https://textminingonline.com/comments/feed
 155. https://wordpress.org/
 156. http://textanalysisonline.com/
 157. http://textsummarization.net/
 158. http://textprocessing.org/
 159. http://wordsimilarity.com/
 160. https://bestcourseracourse.com/
 161. https://bestcourseracourses.com/
 162. https://elasticpatent.com/
 163. https://textminingonline.com/
 164. https://weavertheme.com/
 165. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger#page-top

   hidden links:
 167. https://wordpress.org/
