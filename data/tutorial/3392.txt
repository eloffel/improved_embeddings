    #[1]id111 online    feed [2]id111 online    comments feed
   [3]id111 online    dive into nltk, part vii: a preliminary study
   on text classification comments feed [4]dive into nltk, part vi: add
   stanford word segmenter interface for python nltk [5]dive into nltk,
   part viii: using external maximum id178 modeling libraries for text
   classification [6]alternate [7]alternate

   [ins: :ins]

   [8]   



   javascript is disabled. please enable javascript on your browser to
   best view this site.

[9]id111 online

   search for: ____________________ search

id111 | text analysis | text process | natural language processing

   id111 online
     * [10]home
     * [11]textanalysis
     * [12]keywordextraction
     * [13]textsummarization
     * [14]wordsimilarity
     * [15]about

   [16]home   [17]nltk   dive into nltk, part vii: a preliminary study on text
   classification
   [ins: :ins]

post navigation

   [18]    dive into nltk, part vi: add stanford word segmenter interface
   for python nltk
   [19]dive into nltk, part viii: using external maximum id178 modeling
   libraries for text classification    

dive into nltk, part vii: a preliminary study on text classification

   posted on [20]october 30, 2014 by [21]textminermarch 26, 2017
   [22]deep learning specialization on coursera

   this is the seventh article in the series    [23]dive into nltk   , here is
   an index of all the articles in the series that have been published to
   date:
   [ins: :ins]

   [24]part i: getting started with nltk
   [25]part ii: sentence tokenize and word tokenize
   [26]part iii: part-of-speech tagging and pos tagger
   [27]part iv: id30 and lemmatization
   [28]part v: using stanford text analysis tools in python
   [29]part vi: add stanford word segmenter interface for python nltk
   [30]part vii: a preliminary study on text classification
   [31]part viii: using external maximum id178 modeling libraries for
   text classification
   [32]part ix: from text classification to id31
   [33]part x: play with id97 models based on nltk corpus

   text classification is very useful technique in [34]text analysis, such
   as it can be used in spam filtering, [35]id46,
   [36]id31, genre classification and etc. according
   wikipedia, text classification also refer as document classification:

     document classification or document categorization is a problem in
     library science, information science and computer science. the task
     is to assign a document to one or more classes or categories. this
     may be done    manually    (or    intellectually   ) or algorithmically. the
     intellectual classification of documents has mostly been the
     province of library science, while the algorithmic classification of
     documents is used mainly in information science and computer
     science. the problems are overlapping, however, and there is
     therefore also interdisciplinary research on document
     classification.

   and in this article we focus on the automatic text (document)
   classification, and if you are not familiar with this technique, we
   strongly recommend you to learn the stanford nlp course in coursera
   first, where the week 3 lecture show you what is text classification
   and naive bayes model   week 4 lecture show you the discriminative model
   with maximum id178 classifiers: [37]natural language processing by
   dan jurafsky, christopher manning

   here we will directly dive into nltk and talk all text classification
   related things in nltk. you can find the nltk classifier code in the
   nltk/nltk/classify directory, by the __init__.py file, we can learn
   something about the nltk classifier interfaces:
# natural language toolkit: classifiers
#
# copyright (c) 2001-2014 nltk project
# author: edward loper <edloper@gmail.com>
# url: <http://nltk.org/>
# for license information, see license.txt

"""
classes and interfaces for labeling tokens with category labels (or
"class labels").  typically, labels are represented with strings
(such as ``'health'`` or ``'sports'``).  classifiers can be used to
perform a wide range of classification tasks.  for example,
classifiers can be used...

- to classify documents by topic
- to classify ambiguous words by which word sense is intended
- to classify acoustic signals by which phoneme they represent
- to classify sentences by their author

features
========
in order to decide which category label is appropriate for a given
token, classifiers examine one or more 'features' of the token.  these
"features" are typically chosen by hand, and indicate which aspects
of the token are relevant to the classification decision.  for
example, a document classifier might use a separate feature for each
word, recording how often that word occurred in the document.

featuresets
===========
the features describing a token are encoded using a "featureset",
which is a dictionary that maps from "feature names" to "feature
values".  feature names are unique strings that indicate what aspect
of the token is encoded by the feature.  examples include
``'prevword'``, for a feature whose value is the previous word; and
``'contains-word(library)'`` for a feature that is true when a document
contains the word ``'library'``.  feature values are typically
booleans, numbers, or strings, depending on which feature they
describe.

featuresets are typically constructed using a "feature detector"
(also known as a "feature extractor").  a feature detector is a
function that takes a token (and sometimes information about its
context) as its input, and returns a featureset describing that token.
for example, the following feature detector converts a document
(stored as a list of words) to a featureset describing the set of
words included in the document:

    >>> # define a feature detector function.
    >>> def document_features(document):
    ...     return dict([('contains-word(%s)' % w, true) for w in document])

feature detectors are typically applied to each token before it is fed
to the classifier:

    >>> # classify each gutenberg document.
    >>> from nltk.corpus import gutenberg
    >>> for fileid in gutenberg.fileids(): # doctest: +skip
    ...     doc = gutenberg.words(fileid) # doctest: +skip
    ...     print fileid, classifier.classify(document_features(doc)) # doctest:
 +skip

the parameters that a feature detector expects will vary, depending on
the task and the needs of the feature detector.  for example, a
feature detector for id51 (wsd) might take as its
input a sentence, and the index of a word that should be classified,
and return a featureset for that word.  the following feature detector
for wsd includes features describing the left and right contexts of
the target word:

    >>> def wsd_features(sentence, index):
    ...     featureset = {}
    ...     for i in range(max(0, index-3), index):
    ...         featureset['left-context(%s)' % sentence[i]] = true
    ...     for i in range(index, max(index+3, len(sentence))):
    ...         featureset['right-context(%s)' % sentence[i]] = true
    ...     return featureset

training classifiers
====================
most classifiers are built by training them on a list of hand-labeled
examples, known as the "training set".  training sets are represented
as lists of ``(featuredict, label)`` tuples.
"""

from nltk.classify.api import classifieri, multiclassifieri
from nltk.classify.megam import config_megam, call_megam
from nltk.classify.weka import wekaclassifier, config_weka
from nltk.classify.naivebayes import naivebayesclassifier
from nltk.classify.positivenaivebayes import positivenaivebayesclassifier
from nltk.classify.decisiontree import decisiontreeclassifier
from nltk.classify.rte_classify import rte_classifier, rte_features, rtefeaturee
xtractor
from nltk.classify.util import accuracy, apply_features, log_likelihood
from nltk.classify.scikitlearn import sklearnclassifier
from nltk.classify.maxent import (maxentclassifier, binarymaxentfeatureencoding,
                                  typedmaxentfeatureencoding,
                                  conditionalexponentialclassifier)

   # natural language toolkit: classifiers # # copyright (c) 2001-2014
   nltk project # author: edward loper <edloper@gmail.com> # url:
   <http://nltk.org/> # for license information, see license.txt """
   classes and interfaces for labeling tokens with category labels (or
   "class labels"). typically, labels are represented with strings (such
   as ``'health'`` or ``'sports'``). classifiers can be used to perform a
   wide range of classification tasks. for example, classifiers can be
   used... - to classify documents by topic - to classify ambiguous words
   by which word sense is intended - to classify acoustic signals by which
   phoneme they represent - to classify sentences by their author features
   ======== in order to decide which category label is appropriate for a
   given token, classifiers examine one or more 'features' of the token.
   these "features" are typically chosen by hand, and indicate which
   aspects of the token are relevant to the classification decision. for
   example, a document classifier might use a separate feature for each
   word, recording how often that word occurred in the document.
   featuresets =========== the features describing a token are encoded
   using a "featureset", which is a dictionary that maps from "feature
   names" to "feature values". feature names are unique strings that
   indicate what aspect of the token is encoded by the feature. examples
   include ``'prevword'``, for a feature whose value is the previous word;
   and ``'contains-word(library)'`` for a feature that is true when a
   document contains the word ``'library'``. feature values are typically
   booleans, numbers, or strings, depending on which feature they
   describe. featuresets are typically constructed using a "feature
   detector" (also known as a "feature extractor"). a feature detector is
   a function that takes a token (and sometimes information about its
   context) as its input, and returns a featureset describing that token.
   for example, the following feature detector converts a document (stored
   as a list of words) to a featureset describing the set of words
   included in the document: >>> # define a feature detector function. >>>
   def document_features(document): ... return dict([('contains-word(%s)'
   % w, true) for w in document]) feature detectors are typically applied
   to each token before it is fed to the classifier: >>> # classify each
   gutenberg document. >>> from nltk.corpus import gutenberg >>> for
   fileid in gutenberg.fileids(): # doctest: +skip ... doc =
   gutenberg.words(fileid) # doctest: +skip ... print fileid,
   classifier.classify(document_features(doc)) # doctest: +skip the
   parameters that a feature detector expects will vary, depending on the
   task and the needs of the feature detector. for example, a feature
   detector for id51 (wsd) might take as its input a
   sentence, and the index of a word that should be classified, and return
   a featureset for that word. the following feature detector for wsd
   includes features describing the left and right contexts of the target
   word: >>> def wsd_features(sentence, index): ... featureset = {} ...
   for i in range(max(0, index-3), index): ...
   featureset['left-context(%s)' % sentence[i]] = true ... for i in
   range(index, max(index+3, len(sentence))): ...
   featureset['right-context(%s)' % sentence[i]] = true ... return
   featureset training classifiers ==================== most classifiers
   are built by training them on a list of hand-labeled examples, known as
   the "training set". training sets are represented as lists of
   ``(featuredict, label)`` tuples. """ from nltk.classify.api import
   classifieri, multiclassifieri from nltk.classify.megam import
   config_megam, call_megam from nltk.classify.weka import wekaclassifier,
   config_weka from nltk.classify.naivebayes import naivebayesclassifier
   from nltk.classify.positivenaivebayes import
   positivenaivebayesclassifier from nltk.classify.decisiontree import
   decisiontreeclassifier from nltk.classify.rte_classify import
   rte_classifier, rte_features, rtefeatureextractor from
   nltk.classify.util import accuracy, apply_features, log_likelihood from
   nltk.classify.scikitlearn import sklearnclassifier from
   nltk.classify.maxent import (maxentclassifier,
   binarymaxentfeatureencoding, typedmaxentfeatureencoding,
   conditionalexponentialclassifier)

   the most basic thing for a supervised text classifier is the labeled
   category data, which can be used as a training data. as an example, we
   use the nltk name corpus to train a gender identification classifier:
in [1]: from nltk.corpus import names

in [2]: import random

in [3]: names = ([(name, 'male') for name in names.words('male.txt')] + [(name,
'female') for name in names.words('female.txt')])

in [4]: random.shuffle(names)

in [5]: len(names)
out[5]: 7944

in [6]: names[0:10]
out[6]:
[(u'marthe', 'female'),
 (u'elana', 'female'),
 (u'ernie', 'male'),
 (u'colleen', 'female'),
 (u'lynde', 'female'),
 (u'barclay', 'male'),
 (u'skippy', 'male'),
 (u'marcelia', 'female'),
 (u'charlena', 'female'),
 (u'ronnica', 'female')]

   in [1]: from nltk.corpus import names in [2]: import random in [3]:
   names = ([(name, 'male') for name in names.words('male.txt')] + [(name,
   'female') for name in names.words('female.txt')]) in [4]:
   random.shuffle(names) in [5]: len(names) out[5]: 7944 in [6]:
   names[0:10] out[6]: [(u'marthe', 'female'), (u'elana', 'female'),
   (u'ernie', 'male'), (u'colleen', 'female'), (u'lynde', 'female'),
   (u'barclay', 'male'), (u'skippy', 'male'), (u'marcelia', 'female'),
   (u'charlena', 'female'), (u'ronnica', 'female')]

   the most important thing for a text classifier is feature, which can be
   very flexible, and defined by human engineer. here, we just use the
   final letter of a given name as the feature, and build a dictionary
   containing relevant information about a given name:
in [7]: def gender_features(word):
   ...:     return {'last_letter': word[-1]}
   ...:

in [8]: gender_features('gary')
out[8]: {'last_letter': 'y'}

   in [7]: def gender_features(word): ...: return {'last_letter':
   word[-1]} ...: in [8]: gender_features('gary') out[8]: {'last_letter':
   'y'}

   the dictionary that is returned by this function is called a feature
   set and maps from features    names to their values. feature set is core
   part for nltk classifier, we can use the feature extractor to extract
   feature sets for nltk classifier and segment them into training set and
   testing set:
in [9]: featuresets = [(gender_features(n), g) for (n, g) in names]

in [10]: len(featuresets)
out[10]: 7944

in [11]: featuresets[0:10]
out[11]:
[({'last_letter': u'e'}, 'female'),
 ({'last_letter': u'a'}, 'female'),
 ({'last_letter': u'e'}, 'male'),
 ({'last_letter': u'n'}, 'female'),
 ({'last_letter': u'e'}, 'female'),
 ({'last_letter': u'y'}, 'male'),
 ({'last_letter': u'y'}, 'male'),
 ({'last_letter': u'a'}, 'female'),
 ({'last_letter': u'a'}, 'female'),
 ({'last_letter': u'a'}, 'female')]

in [12]: train_set, test_set = featuresets[500:], featuresets[:500]

in [13]: len(train_set)
out[13]: 7444

in [14]: len(test_set)
out[14]: 500

   in [9]: featuresets = [(gender_features(n), g) for (n, g) in names] in
   [10]: len(featuresets) out[10]: 7944 in [11]: featuresets[0:10]
   out[11]: [({'last_letter': u'e'}, 'female'), ({'last_letter': u'a'},
   'female'), ({'last_letter': u'e'}, 'male'), ({'last_letter': u'n'},
   'female'), ({'last_letter': u'e'}, 'female'), ({'last_letter': u'y'},
   'male'), ({'last_letter': u'y'}, 'male'), ({'last_letter': u'a'},
   'female'), ({'last_letter': u'a'}, 'female'), ({'last_letter': u'a'},
   'female')] in [12]: train_set, test_set = featuresets[500:],
   featuresets[:500] in [13]: len(train_set) out[13]: 7444 in [14]:
   len(test_set) out[14]: 500

   a learning algorithm is very useful for a classifier, here we will show
   you how to use the naive bayes and maximum id178 model to train a
   naivebayes and maxent classifier, where naive bayes is the generative
   model and maxent is discriminative model.

   here is how to train a naive bayes classifier for gender
   identification:
in [16]: from nltk import naivebayesclassifier

in [17]: nb_classifier = naivebayesclassifier.train(train_set)

in [18]: nb_classifier.classify(gender_features('gary'))
out[18]: 'female'

in [19]: nb_classifier.classify(gender_features('grace'))
out[19]: 'female'

in [20]: from nltk import classify

in [21]: classify.accuracy(nb_classifier, test_set)
out[21]: 0.73

in [22]: nb_classifier.show_most_informative_features(5)
most informative features
             last_letter = u'a'           female : male   =     38.4 : 1.0
             last_letter = u'k'             male : female =     33.4 : 1.0
             last_letter = u'f'             male : female =     16.7 : 1.0
             last_letter = u'p'             male : female =     11.9 : 1.0
             last_letter = u'v'             male : female =     10.6 : 1.0

   in [16]: from nltk import naivebayesclassifier in [17]: nb_classifier =
   naivebayesclassifier.train(train_set) in [18]:
   nb_classifier.classify(gender_features('gary')) out[18]: 'female' in
   [19]: nb_classifier.classify(gender_features('grace')) out[19]:
   'female' in [20]: from nltk import classify in [21]:
   classify.accuracy(nb_classifier, test_set) out[21]: 0.73 in [22]:
   nb_classifier.show_most_informative_features(5) most informative
   features last_letter = u'a' female : male = 38.4 : 1.0 last_letter =
   u'k' male : female = 33.4 : 1.0 last_letter = u'f' male : female = 16.7
   : 1.0 last_letter = u'p' male : female = 11.9 : 1.0 last_letter = u'v'
   male : female = 10.6 : 1.0

   here is how to train a maximum id178 classifier for gender
   identification:
in [23]: from nltk import maxentclassifier

in [24]: me_classifier = maxentclassifier.train(train_set)
  ==> training (100 iterations)

      iteration    log likelihood    accuracy
      ---------------------------------------
             1          -0.69315        0.369
             2          -0.37066        0.765
             3          -0.37029        0.765
             4          -0.37007        0.765
             5          -0.36992        0.765
             6          -0.36981        0.765
             7          -0.36973        0.765
             8          -0.36967        0.765
             9          -0.36962        0.765
            10          -0.36958        0.765
            11          -0.36955        0.765
            12          -0.36952        0.765
            13          -0.36949        0.765
            14          -0.36947        0.765
            15          -0.36945        0.765
            16          -0.36944        0.765
            17          -0.36942        0.765
            18          -0.36941        0.765
            ....
in [25]: me_classifier.classify(gender_features('gary'))
out[25]: 'female'

in [26]: me_classifier.classify(gender_features('grace'))
out[26]: 'female'

in [27]: classify.accuracy(me_classifier, test_set)
out[27]: 0.728

in [28]: me_classifier.show_most_informative_features(5)
   6.644 last_letter==u'c' and label is 'male'
  -5.082 last_letter==u'a' and label is 'male'
  -3.565 last_letter==u'k' and label is 'female'
  -2.700 last_letter==u'f' and label is 'female'
  -2.248 last_letter==u'p' and label is 'female'

   in [23]: from nltk import maxentclassifier in [24]: me_classifier =
   maxentclassifier.train(train_set) ==> training (100 iterations)
   iteration log likelihood accuracy
   --------------------------------------- 1 -0.69315 0.369 2 -0.37066
   0.765 3 -0.37029 0.765 4 -0.37007 0.765 5 -0.36992 0.765 6 -0.36981
   0.765 7 -0.36973 0.765 8 -0.36967 0.765 9 -0.36962 0.765 10 -0.36958
   0.765 11 -0.36955 0.765 12 -0.36952 0.765 13 -0.36949 0.765 14 -0.36947
   0.765 15 -0.36945 0.765 16 -0.36944 0.765 17 -0.36942 0.765 18 -0.36941
   0.765 .... in [25]: me_classifier.classify(gender_features('gary'))
   out[25]: 'female' in [26]:
   me_classifier.classify(gender_features('grace')) out[26]: 'female' in
   [27]: classify.accuracy(me_classifier, test_set) out[27]: 0.728 in
   [28]: me_classifier.show_most_informative_features(5) 6.644
   last_letter==u'c' and label is 'male' -5.082 last_letter==u'a' and
   label is 'male' -3.565 last_letter==u'k' and label is 'female' -2.700
   last_letter==u'f' and label is 'female' -2.248 last_letter==u'p' and
   label is 'female'

   it seems that naive bayes and maxent model have the same result on this
   gender task, but that   s not true. choosing right features and deciding
   how to encode them for the task have an big impact on the performance.
   here we define a more complex feature extractor function and train the
   model again:
in [29]: def gender_features2(name):
   ....:     features = {}
   ....:     features["firstletter"] = name[0].lower()
   ....:     features["lastletter"] = name[-1].lower()
   ....:     for letter in 'abcdefghijklmnopqrstuvwxyz':
   ....:         features["count(%s)" % letter] = name.lower().count(letter)
   ....:         features["has(%s)" % letter] = (letter in name.lower())
   ....:     return features
   ....:

in [30]: gender_features2('gary')
out[30]:
{'count(a)': 1,
 'count(b)': 0,
 'count(c)': 0,
 'count(d)': 0,
 'count(e)': 0,
 'count(f)': 0,
 'count(g)': 1,
 'count(h)': 0,
 'count(i)': 0,
 'count(j)': 0,
 'count(k)': 0,
 'count(l)': 0,
 'count(m)': 0,
 'count(n)': 0,
 'count(o)': 0,
 'count(p)': 0,
 'count(q)': 0,
 'count(r)': 1,
 'count(s)': 0,
 'count(t)': 0,
 'count(u)': 0,
 'count(v)': 0,
 'count(w)': 0,
 'count(x)': 0,
 'count(y)': 1,
 'count(z)': 0,
 'firstletter': 'g',
 'has(a)': true,
 'has(b)': false,
 'has(c)': false,
 'has(d)': false,
 'has(e)': false,
 'has(f)': false,
 'has(g)': true,
 'has(h)': false,
 'has(i)': false,
 'has(j)': false,
 'has(k)': false,
 'has(l)': false,
 'has(m)': false,
 'has(n)': false,
 'has(o)': false,
 'has(p)': false,
 'has(q)': false,
 'has(r)': true,
 'has(s)': false,
 'has(t)': false,
 'has(u)': false,
 'has(v)': false,
 'has(w)': false,
 'has(x)': false,
 'has(y)': true,
 'has(z)': false,
 'lastletter': 'y'}

in [32]: featuresets = [(gender_features2(n), g) for (n, g) in names]

in [34]: train_set, test_set = featuresets[500:], featuresets[:500]

in [35]: nb2_classifier = naivebayesclassifier.train(train_set)

in [36]: classify.accuracy(nb2_classifier, test_set)
out[36]: 0.774

in [37]: me2_classifier = maxentclassifier.train(train_set)
  ==> training (100 iterations)

      iteration    log likelihood    accuracy
      ---------------------------------------
             1          -0.69315        0.369
             2          -0.61051        0.631
             3          -0.59637        0.631
             4          -0.58304        0.631
             5          -0.57050        0.637
             6          -0.55872        0.651
             7          -0.54766        0.672
             8          -0.53728        0.689
             ....
            93          -0.33632        0.805
            94          -0.33588        0.805
            95          -0.33545        0.805
            96          -0.33503        0.805
            97          -0.33462        0.805
            98          -0.33421        0.805
            99          -0.33382        0.805
         final          -0.33343        0.805

in [38]: classify.accuracy(me2_classifier, test_set)
out[38]: 0.78

   in [29]: def gender_features2(name): ....: features = {} ....:
   features["firstletter"] = name[0].lower() ....: features["lastletter"]
   = name[-1].lower() ....: for letter in 'abcdefghijklmnopqrstuvwxyz':
   ....: features["count(%s)" % letter] = name.lower().count(letter) ....:
   features["has(%s)" % letter] = (letter in name.lower()) ....: return
   features ....: in [30]: gender_features2('gary') out[30]: {'count(a)':
   1, 'count(b)': 0, 'count(c)': 0, 'count(d)': 0, 'count(e)': 0,
   'count(f)': 0, 'count(g)': 1, 'count(h)': 0, 'count(i)': 0, 'count(j)':
   0, 'count(k)': 0, 'count(l)': 0, 'count(m)': 0, 'count(n)': 0,
   'count(o)': 0, 'count(p)': 0, 'count(q)': 0, 'count(r)': 1, 'count(s)':
   0, 'count(t)': 0, 'count(u)': 0, 'count(v)': 0, 'count(w)': 0,
   'count(x)': 0, 'count(y)': 1, 'count(z)': 0, 'firstletter': 'g',
   'has(a)': true, 'has(b)': false, 'has(c)': false, 'has(d)': false,
   'has(e)': false, 'has(f)': false, 'has(g)': true, 'has(h)': false,
   'has(i)': false, 'has(j)': false, 'has(k)': false, 'has(l)': false,
   'has(m)': false, 'has(n)': false, 'has(o)': false, 'has(p)': false,
   'has(q)': false, 'has(r)': true, 'has(s)': false, 'has(t)': false,
   'has(u)': false, 'has(v)': false, 'has(w)': false, 'has(x)': false,
   'has(y)': true, 'has(z)': false, 'lastletter': 'y'} in [32]:
   featuresets = [(gender_features2(n), g) for (n, g) in names] in [34]:
   train_set, test_set = featuresets[500:], featuresets[:500] in [35]:
   nb2_classifier = naivebayesclassifier.train(train_set) in [36]:
   classify.accuracy(nb2_classifier, test_set) out[36]: 0.774 in [37]:
   me2_classifier = maxentclassifier.train(train_set) ==> training (100
   iterations) iteration log likelihood accuracy
   --------------------------------------- 1 -0.69315 0.369 2 -0.61051
   0.631 3 -0.59637 0.631 4 -0.58304 0.631 5 -0.57050 0.637 6 -0.55872
   0.651 7 -0.54766 0.672 8 -0.53728 0.689 .... 93 -0.33632 0.805 94
   -0.33588 0.805 95 -0.33545 0.805 96 -0.33503 0.805 97 -0.33462 0.805 98
   -0.33421 0.805 99 -0.33382 0.805 final -0.33343 0.805 in [38]:
   classify.accuracy(me2_classifier, test_set) out[38]: 0.78

   it seems that more features make maximum id178 model more accuracy,
   but more slow when training it. we can define the third feature
   extractor function and train naive bayes and maxent classifier models
   again:
in [49]: def gender_features3(name):
    features = {}
    features["fl"] = name[0].lower()
    features["ll"] = name[-1].lower()
    features["fw"] = name[:2].lower()
    features["lw"] = name[-2:].lower()
    return features

in [50]: gender_features3('gary')
out[50]: {'fl': 'g', 'fw': 'ga', 'll': 'y', 'lw': 'ry'}

in [51]: gender_features3('g')
out[51]: {'fl': 'g', 'fw': 'g', 'll': 'g', 'lw': 'g'}

in [52]: gender_features3('gary')
out[52]: {'fl': 'g', 'fw': 'ga', 'll': 'y', 'lw': 'ry'}

in [53]: featuresets = [(gender_features3(n), g) for (n, g) in names]

in [54]: featuresets[0]
out[54]: ({'fl': u'm', 'fw': u'ma', 'll': u'e', 'lw': u'he'}, 'female')

in [55]: len(featuresets)
out[55]: 7944

in [56]: train_set, test_set = featuresets[500:], featuresets[:500]

in [57]: nb3_classifier = naivebayesclassifier.train(train_set)

in [59]: classify.accuracy(nb3_classifier, test_set)
out[59]: 0.77
in [60]: me3_classifier = maxentclassifier.train(train_set)
  ==> training (100 iterations)

      iteration    log likelihood    accuracy
      ---------------------------------------
             1          -0.69315        0.369
             2          -0.40398        0.800
             3          -0.34739        0.821
             4          -0.32196        0.825
             5          -0.30766        0.827
             ......
            95          -0.25608        0.839
            96          -0.25605        0.839
            97          -0.25601        0.839
            98          -0.25598        0.839
            99          -0.25595        0.839
         final          -0.25591        0.839

in [61]: classify.accuracy(me3_classifier, test_set)
out[61]: 0.798

   in [49]: def gender_features3(name): features = {} features["fl"] =
   name[0].lower() features["ll"] = name[-1].lower() features["fw"] =
   name[:2].lower() features["lw"] = name[-2:].lower() return features in
   [50]: gender_features3('gary') out[50]: {'fl': 'g', 'fw': 'ga', 'll':
   'y', 'lw': 'ry'} in [51]: gender_features3('g') out[51]: {'fl': 'g',
   'fw': 'g', 'll': 'g', 'lw': 'g'} in [52]: gender_features3('gary')
   out[52]: {'fl': 'g', 'fw': 'ga', 'll': 'y', 'lw': 'ry'} in [53]:
   featuresets = [(gender_features3(n), g) for (n, g) in names] in [54]:
   featuresets[0] out[54]: ({'fl': u'm', 'fw': u'ma', 'll': u'e', 'lw':
   u'he'}, 'female') in [55]: len(featuresets) out[55]: 7944 in [56]:
   train_set, test_set = featuresets[500:], featuresets[:500] in [57]:
   nb3_classifier = naivebayesclassifier.train(train_set) in [59]:
   classify.accuracy(nb3_classifier, test_set) out[59]: 0.77 in [60]:
   me3_classifier = maxentclassifier.train(train_set) ==> training (100
   iterations) iteration log likelihood accuracy
   --------------------------------------- 1 -0.69315 0.369 2 -0.40398
   0.800 3 -0.34739 0.821 4 -0.32196 0.825 5 -0.30766 0.827 ...... 95
   -0.25608 0.839 96 -0.25605 0.839 97 -0.25601 0.839 98 -0.25598 0.839 99
   -0.25595 0.839 final -0.25591 0.839 in [61]:
   classify.accuracy(me3_classifier, test_set) out[61]: 0.798

   it seems that with a proper feature extraction, maximum id178
   classifier can get better performance on the test set. actually,
   sometimes selecting right feature is more important in a supervised
   text classification. you need spend a lot of time to choose features
   and select a good learning algorithm with parameters adjustment for
   your text classifier.

   our preliminary study on text classification in nltk is over, and next
   chapter we will dive into text classification with a useful example.

   posted by [38]textminer

related posts:

    1. [39]dive into nltk, part viii: using external maximum id178
       modeling libraries for text classification
    2. [40]dive into nltk, part ix: from text classification to sentiment
       analysis
    3. [41]dive into nltk, part iii: part-of-speech tagging and pos tagger
    4. [42]text analysis online no longer provides nltk stanford nlp api
       interface

   [43]deep learning specialization on coursera

   posted in [44]nltk, [45]text analysis, [46]text classification,
   [47]id111, [48]text processing tagged [49]automatic
   classification, [50]automatic classifier, [51]automatic document
   classification, [52]automatic text classification, [53]classification,
   [54]classifier, [55]document classification, [56]document classifier,
   [57]maxent classifier, [58]maxent model, [59]maximum id178
   classifier, [60]naive bayes, [61]naive bayes classifier, [62]nltk,
   [63]nltk classification, [64]nltk text classification, [65]text
   classifcation, [66]text classifier [67]permalink

post navigation

   [68]    dive into nltk, part vi: add stanford word segmenter interface
   for python nltk
   [69]dive into nltk, part viii: using external maximum id178 modeling
   libraries for text classification    
     __________________________________________________________________

comments

dive into nltk, part vii: a preliminary study on text classification     1
comment

    1.
   hiroshi perera on [70]december 28, 2016 at 12:06 am said:
       very useful
       [71]reply    

leave a reply [72]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   [ ] save my name, email, and website in this browser for the next time
   i comment.

   post comment

   [73][dlai-logo-final-minus-font-plus-white-backg.png]
   [show?id=9iqcvd3eeqc&bids=541296.11421701896&type=2&subid=0]

   search for: ____________________ search

   [ins: :ins]

recent posts

     * [74]deep learning practice for nlp: large movie review data
       id31 from scratch
     * [75]best coursera courses for data science
     * [76]best coursera courses for machine learning
     * [77]best coursera courses for deep learning
     * [78]dive into nlp with deep learning, part i: getting started with
       dl4nlp

recent comments

     * textminer on [79]training id97 model on english wikipedia by
       gensim
     * ankit ramani on [80]training id97 model on english wikipedia by
       gensim
     * vincent on [81]training id97 model on english wikipedia by
       gensim
     * muhammad amin nadim on [82]andrew ng deep learning specialization:
       best deep learning course for beginners and deep learners
     * saranya on [83]training id97 model on english wikipedia by
       gensim

archives

     * [84]november 2018
     * [85]august 2018
     * [86]july 2018
     * [87]june 2018
     * [88]january 2018
     * [89]october 2017
     * [90]september 2017
     * [91]august 2017
     * [92]july 2017
     * [93]may 2017
     * [94]april 2017
     * [95]march 2017
     * [96]december 2016
     * [97]october 2016
     * [98]august 2016
     * [99]july 2016
     * [100]june 2016
     * [101]may 2016
     * [102]april 2016
     * [103]february 2016
     * [104]december 2015
     * [105]november 2015
     * [106]september 2015
     * [107]may 2015
     * [108]april 2015
     * [109]march 2015
     * [110]february 2015
     * [111]january 2015
     * [112]december 2014
     * [113]november 2014
     * [114]october 2014
     * [115]september 2014
     * [116]july 2014
     * [117]june 2014
     * [118]may 2014
     * [119]april 2014
     * [120]january 2014

categories

     * [121]ainlp
     * [122]coursera course
     * [123]data science
     * [124]deep learning
     * [125]dl4nlp
     * [126]how to use mashape api
     * [127]keras
     * [128]machine learning
     * [129]id39
     * [130]nlp
     * [131]nlp tools
     * [132]nltk
     * [133]id31
     * [134]tensorflow
     * [135]text analysis
     * [136]text classification
     * [137]id111
     * [138]text processing
     * [139]text similarity
     * [140]text summarization
     * [141]textanalysis api
     * [142]uncategorized
     * [143]id27
     * [144]id40

meta

     * [145]log in
     * [146]entries rss
     * [147]comments rss
     * [148]wordpress.org

     [149]text analysis online

     [150]text summarizer

     [151]text processing

     [152]word similarity

     [153]best coursera course

     [154]best coursera courses

     [155]elastic patent

     2019 - [156]id111 online - [157]weaver xtreme theme

   [158]   

references

   visible links
   1. https://textminingonline.com/feed
   2. https://textminingonline.com/comments/feed
   3. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification/feed
   4. https://textminingonline.com/dive-into-nltk-part-vi-add-stanford-word-segmenter-interface-for-python-nltk
   5. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
   6. https://textminingonline.com/wp-json/oembed/1.0/embed?url=https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification
   7. https://textminingonline.com/wp-json/oembed/1.0/embed?url=https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification&format=xml
   8. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification#page-bottom
   9. https://textminingonline.com/
  10. https://textminingonline.com/
  11. http://textanalysisonline.com/#new_tab
  12. http://keywordextraction.net/#new_tab
  13. http://textsummarization.net/#new_tab
  14. https://wordsimilarity.com/#new_tab
  15. https://textminingonline.com/about
  16. https://textminingonline.com/
  17. https://textminingonline.com/category/nltk
  18. https://textminingonline.com/dive-into-nltk-part-vi-add-stanford-word-segmenter-interface-for-python-nltk
  19. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
  20. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification
  21. https://textminingonline.com/author/yuzhen
  22. https://click.linksynergy.com/fs-bin/click?id=9iqcvd3eeqc&offerid=467035.416&subid=0&type=4
  23. http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk
  24. http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk
  25. http://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize
  26. http://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger
  27. http://textminingonline.com/dive-into-nltk-part-iv-id30-and-lemmatization
  28. http://textminingonline.com/dive-into-nltk-part-v-using-stanford-text-analysis-tools-in-python
  29. http://textminingonline.com/dive-into-nltk-part-vi-add-stanford-word-segmenter-interface-for-python-nltk
  30. http://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification
  31. http://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
  32. http://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis
  33. http://textminingonline.com/?p=872
  34. http://textanalysisonline.com/
  35. http://textanalysisonline.com/langid-language-detection
  36. http://textanalysisonline.com/textblob-sentiment-analysis
  37. https://class.coursera.org/nlp/lecture/preview
  38. http://textminingonline.com/
  39. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
  40. https://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis
  41. https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger
  42. https://textminingonline.com/text-analysis-online-no-longer-provides-nltk-stanford-nlp-api-interface
  43. https://click.linksynergy.com/fs-bin/click?id=9iqcvd3eeqc&offerid=467035.414&subid=0&type=4
  44. https://textminingonline.com/category/nltk
  45. https://textminingonline.com/category/text-analysis
  46. https://textminingonline.com/category/text-classification
  47. https://textminingonline.com/category/text-mining
  48. https://textminingonline.com/category/text-processing
  49. https://textminingonline.com/tag/automatic-classification
  50. https://textminingonline.com/tag/automatic-classifier
  51. https://textminingonline.com/tag/automatic-document-classification
  52. https://textminingonline.com/tag/automatic-text-classification
  53. https://textminingonline.com/tag/classification
  54. https://textminingonline.com/tag/classifier
  55. https://textminingonline.com/tag/document-classification
  56. https://textminingonline.com/tag/document-classifier
  57. https://textminingonline.com/tag/maxent-classifier
  58. https://textminingonline.com/tag/maxent-model
  59. https://textminingonline.com/tag/maximum-id178-classifier
  60. https://textminingonline.com/tag/naive-bayes
  61. https://textminingonline.com/tag/naive-bayes-classifier
  62. https://textminingonline.com/tag/nltk
  63. https://textminingonline.com/tag/nltk-classification
  64. https://textminingonline.com/tag/nltk-text-classification
  65. https://textminingonline.com/tag/text-classifcation
  66. https://textminingonline.com/tag/text-classifier
  67. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification
  68. https://textminingonline.com/dive-into-nltk-part-vi-add-stanford-word-segmenter-interface-for-python-nltk
  69. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
  70. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification#comment-127100
  71. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification?replytocom=127100#respond
  72. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification#respond
  73. https://click.linksynergy.com/link?id=9iqcvd3eeqc&offerid=541296.11421701896&type=2&murl=https://www.coursera.org/specializations/deep-learning
  74. https://textminingonline.com/deep-learning-practice-for-nlp-large-movie-review-data-sentiment-analysis-from-scratch
  75. https://textminingonline.com/best-coursera-courses-for-data-science
  76. https://textminingonline.com/best-coursera-courses-for-machine-learning
  77. https://textminingonline.com/best-coursera-courses-for-deep-learning
  78. https://textminingonline.com/dive-into-nlp-with-deep-learning-part-i-getting-started-with-dl4nlp
  79. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-138841
  80. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-138807
  81. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-138723
  82. https://textminingonline.com/andrew-ng-deep-learning-specialization-best-deep-learning-course-for-beginners-and-deep-learners#comment-138475
  83. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-137923
  84. https://textminingonline.com/2018/11
  85. https://textminingonline.com/2018/08
  86. https://textminingonline.com/2018/07
  87. https://textminingonline.com/2018/06
  88. https://textminingonline.com/2018/01
  89. https://textminingonline.com/2017/10
  90. https://textminingonline.com/2017/09
  91. https://textminingonline.com/2017/08
  92. https://textminingonline.com/2017/07
  93. https://textminingonline.com/2017/05
  94. https://textminingonline.com/2017/04
  95. https://textminingonline.com/2017/03
  96. https://textminingonline.com/2016/12
  97. https://textminingonline.com/2016/10
  98. https://textminingonline.com/2016/08
  99. https://textminingonline.com/2016/07
 100. https://textminingonline.com/2016/06
 101. https://textminingonline.com/2016/05
 102. https://textminingonline.com/2016/04
 103. https://textminingonline.com/2016/02
 104. https://textminingonline.com/2015/12
 105. https://textminingonline.com/2015/11
 106. https://textminingonline.com/2015/09
 107. https://textminingonline.com/2015/05
 108. https://textminingonline.com/2015/04
 109. https://textminingonline.com/2015/03
 110. https://textminingonline.com/2015/02
 111. https://textminingonline.com/2015/01
 112. https://textminingonline.com/2014/12
 113. https://textminingonline.com/2014/11
 114. https://textminingonline.com/2014/10
 115. https://textminingonline.com/2014/09
 116. https://textminingonline.com/2014/07
 117. https://textminingonline.com/2014/06
 118. https://textminingonline.com/2014/05
 119. https://textminingonline.com/2014/04
 120. https://textminingonline.com/2014/01
 121. https://textminingonline.com/category/ainlp
 122. https://textminingonline.com/category/coursera-course
 123. https://textminingonline.com/category/data-science
 124. https://textminingonline.com/category/deep-learning
 125. https://textminingonline.com/category/dl4nlp
 126. https://textminingonline.com/category/how-to-use-mashape-api
 127. https://textminingonline.com/category/keras
 128. https://textminingonline.com/category/machine-learning
 129. https://textminingonline.com/category/named-entity-recognition
 130. https://textminingonline.com/category/nlp
 131. https://textminingonline.com/category/nlp-tools
 132. https://textminingonline.com/category/nltk
 133. https://textminingonline.com/category/sentiment-analysis
 134. https://textminingonline.com/category/tensorflow
 135. https://textminingonline.com/category/text-analysis
 136. https://textminingonline.com/category/text-classification
 137. https://textminingonline.com/category/text-mining
 138. https://textminingonline.com/category/text-processing
 139. https://textminingonline.com/category/text-similarity
 140. https://textminingonline.com/category/text-summarization
 141. https://textminingonline.com/category/textanalysis-api-2
 142. https://textminingonline.com/category/uncategorized
 143. https://textminingonline.com/category/word-embedding
 144. https://textminingonline.com/category/word-segmentation
 145. https://textminingonline.com/wp-login.php
 146. https://textminingonline.com/feed
 147. https://textminingonline.com/comments/feed
 148. https://wordpress.org/
 149. http://textanalysisonline.com/
 150. http://textsummarization.net/
 151. http://textprocessing.org/
 152. http://wordsimilarity.com/
 153. https://bestcourseracourse.com/
 154. https://bestcourseracourses.com/
 155. https://elasticpatent.com/
 156. https://textminingonline.com/
 157. https://weavertheme.com/
 158. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification#page-top

   hidden links:
 160. https://wordpress.org/
