    #[1]id111 online    feed [2]id111 online    comments feed
   [3]id111 online    dive into nltk, part viii: using external
   maximum id178 modeling libraries for text classification comments
   feed [4]dive into nltk, part vii: a preliminary study on text
   classification [5]we have launched the professional text sentiment
   analysis api on mashape [6]alternate [7]alternate

   [ins: :ins]

   [8]   



   javascript is disabled. please enable javascript on your browser to
   best view this site.

[9]id111 online

   search for: ____________________ search

id111 | text analysis | text process | natural language processing

   id111 online
     * [10]home
     * [11]textanalysis
     * [12]keywordextraction
     * [13]textsummarization
     * [14]wordsimilarity
     * [15]about

   [16]home   [17]nlp   dive into nltk, part viii: using external maximum
   id178 modeling libraries for text classification
   [ins: :ins]

post navigation

   [18]    dive into nltk, part vii: a preliminary study on text
   classification
   [19]we have launched the professional text id31 api on
   mashape    

dive into nltk, part viii: using external maximum id178 modeling libraries
for text classification

   posted on [20]november 26, 2014 by [21]textminermarch 26, 2017
   [22]deep learning specialization on coursera

   this is the eighth article in the series    [23]dive into nltk   , here is
   an index of all the articles in the series that have been published to
   date:
   [ins: :ins]

   [24]part i: getting started with nltk
   [25]part ii: sentence tokenize and word tokenize
   [26]part iii: part-of-speech tagging and pos tagger
   [27]part iv: id30 and lemmatization
   [28]part v: using stanford text analysis tools in python
   [29]part vi: add stanford word segmenter interface for python nltk
   [30]part vii: a preliminary study on text classification
   [31]part viii: using external maximum id178 modeling libraries for
   text classification
   [32]part ix: from text classification to id31
   [33]part x: play with id97 models based on nltk corpus

   maximum id178 modeling, also known as multinomial logistic
   regression, is one of the most popular framework for [34]text analysis
   tasks since first introduced into the nlp area by berger and della
   pietra at 1996. a lot of maximum id178 model tools and libraries are
   implemented by several programming languages since then, and you can
   find a complete list of maximum id178 models related software by this
   website which maintained by doctor le zhang: [35]maximum id178
   modeling, which also contains very useful materials for maximum id178
   models.

   nltk provides several learning algorithms for text classification, such
   as naive bayes, id90, and also includes maximum id178
   models, you can find them all in the nltk/classify module. for maximum
   id178 modeling, you can find the details in the maxent.py:
# natural language toolkit: maximum id178 classifiers
#
# copyright (c) 2001-2014 nltk project
# author: edward loper <edloper@gmail.com>
#         dmitry chichkov <dchichkov@gmail.com> (typedmaxentfeatureencoding)
# url: <http://nltk.org/>
# for license information, see license.txt

"""
a classifier model based on maximum id178 modeling framework.  this
framework considers all of the id203 distributions that are
empirically consistent with the training data; and chooses the
distribution with the highest id178.  a id203 distribution is
"empirically consistent" with a set of training data if its estimated
frequency with which a class and a feature vector value co-occur is
equal to the actual frequency in the data.

terminology: 'feature'
======================
the term *feature* is usually used to refer to some property of an
unlabeled token.  for example, when performing word sense
disambiguation, we might define a ``'prevword'`` feature whose value is
the word preceding the target word.  however, in the context of
maxent modeling, the term *feature* is typically used to refer to a
property of a "labeled" token.  in order to prevent confusion, we
will introduce two distinct terms to disambiguate these two different
concepts:

  - an "input-feature" is a property of an unlabeled token.
  - a "joint-feature" is a property of a labeled token.

in the rest of the ``nltk.classify`` module, the term "features" is
used to refer to what we will call "input-features" in this module.

in literature that describes and discusses maximum id178 models,
input-features are typically called "contexts", and joint-features
are simply referred to as "features".

converting input-features to joint-features
-------------------------------------------
in maximum id178 models, joint-features are required to have numeric
values.  typically, each input-feature ``input_feat`` is mapped to a
set of joint-features of the form:

|   joint_feat(token, label) = { 1 if input_feat(token) == feat_val
|                              {      and label == some_label
|                              {
|                              { 0 otherwise

for all values of ``feat_val`` and ``some_label``.  this mapping is
performed by classes that implement the ``maxentfeatureencodingi``
interface.
"""
from __future__ import print_function, unicode_literals
__docformat__ = 'epytext en'

try:
    import numpy
except importerror:
    pass

import time
import tempfile
import os
import gzip
from collections import defaultdict

from nltk import compat
from nltk.data import gzip_open_unicode
from nltk.util import ordereddict
from nltk.id203 import dictionaryprobdist

from nltk.classify.api import classifieri
from nltk.classify.util import cutoffchecker, accuracy, log_likelihood
from nltk.classify.megam import call_megam, write_megam_file, parse_megam_weight
s
from nltk.classify.tadm import call_tadm, write_tadm_file, parse_tadm_weights
...

   # natural language toolkit: maximum id178 classifiers # # copyright
   (c) 2001-2014 nltk project # author: edward loper <edloper@gmail.com> #
   dmitry chichkov <dchichkov@gmail.com> (typedmaxentfeatureencoding) #
   url: <http://nltk.org/> # for license information, see license.txt """
   a classifier model based on maximum id178 modeling framework. this
   framework considers all of the id203 distributions that are
   empirically consistent with the training data; and chooses the
   distribution with the highest id178. a id203 distribution is
   "empirically consistent" with a set of training data if its estimated
   frequency with which a class and a feature vector value co-occur is
   equal to the actual frequency in the data. terminology: 'feature'
   ====================== the term *feature* is usually used to refer to
   some property of an unlabeled token. for example, when performing word
   sense disambiguation, we might define a ``'prevword'`` feature whose
   value is the word preceding the target word. however, in the context of
   maxent modeling, the term *feature* is typically used to refer to a
   property of a "labeled" token. in order to prevent confusion, we will
   introduce two distinct terms to disambiguate these two different
   concepts: - an "input-feature" is a property of an unlabeled token. - a
   "joint-feature" is a property of a labeled token. in the rest of the
   ``nltk.classify`` module, the term "features" is used to refer to what
   we will call "input-features" in this module. in literature that
   describes and discusses maximum id178 models, input-features are
   typically called "contexts", and joint-features are simply referred to
   as "features". converting input-features to joint-features
   ------------------------------------------- in maximum id178 models,
   joint-features are required to have numeric values. typically, each
   input-feature ``input_feat`` is mapped to a set of joint-features of
   the form: | joint_feat(token, label) = { 1 if input_feat(token) ==
   feat_val | { and label == some_label | { | { 0 otherwise for all values
   of ``feat_val`` and ``some_label``. this mapping is performed by
   classes that implement the ``maxentfeatureencodingi`` interface. """
   from __future__ import print_function, unicode_literals __docformat__ =
   'epytext en' try: import numpy except importerror: pass import time
   import tempfile import os import gzip from collections import
   defaultdict from nltk import compat from nltk.data import
   gzip_open_unicode from nltk.util import ordereddict from
   nltk.id203 import dictionaryprobdist from nltk.classify.api
   import classifieri from nltk.classify.util import cutoffchecker,
   accuracy, log_likelihood from nltk.classify.megam import call_megam,
   write_megam_file, parse_megam_weights from nltk.classify.tadm import
   call_tadm, write_tadm_file, parse_tadm_weights ...

   within nltk, the maxent training algorithms support gis(generalized
   iterative scaling), iis(improved iterative scaling), and lm-bfgs. the
   first two are implemented in nltk by python but seems very slow and
   costs large memory for the same training data. and the lbfgs algorithm,
   support external libraries like [36]megam(mega model optimization
   package), which is very elegant.

   megam is based on the ocaml system, which is the main implementation of
   the [37]caml language. caml is a general-purpose programming language,
   designed with program safety and reliability in mind. in order use
   megam in your system, you need install ocaml first. in my ubuntu 12.04
   vps, it   s very easy to install the latest ocaml version of 4.02:

     wget http://caml.inria.fr/pub/distrib/ocaml-4.02/ocaml-4.02.1.tar.gz
     tar -zxvf ocaml-4.02.1.tar.gz
     ./configure
     make world.opt
     sudo make install

   after install ocaml, it   s time to install megam:

     wget http://hal3.name/megam/megam_src.tgz
     tar -zxvf megam_src.tgz
     cd megam_0.92

   from readme, install megam is very easy:

     to build a safe but slow version, just execute:

     make

     which will produce an executable megam, unless something goes wrong.

     to build a fast but not so safe version, execuate

     make opt

     which will produce an executable megam.opt that will be much much
     faster. if you encounter any bugs, please let me know (if something
     crashes, it   s probably easiest to switch to the safe by slow
     version,
     run it, and let me know what the error message is).

   but when we execute the    make    first, we met the error like this:

        
     ocamlc -g -custom -o megam str.cma -cclib -lstr bigarray.cma -cclib
     -lbigarray unix.cma -cclib -lunix -i /usr/lib/ocaml/caml fastdot_c.c
     fastdot.cmo inthashtbl.cmo arry.cmo util.cmo data.cmo bitvec.cmo
     cg.cmo wsemlm.cmo bfgs.cmo pa.cmo id88.cmo radapt.cmo
     kernelmap.cmo abffs.cmo main.cmo
     fastdot_c.c:4:19: fatal error: alloc.h: no such file or directory
        

   here you should use the    ocamlc -where    to find the right ocmal
   library: /usr/local/lib/ocaml , and then edit the makefile 74 line
   (note that this editing is on my ubuntu 12.04 vps):

     #withclibs =-i /usr/lib/ocaml/caml
     withclibs =-i /usr/local/lib/ocaml/caml

   then execute the    make    again, but met another problem:

        
     ocamlc -g -custom -o megam str.cma -cclib -lstr bigarray.cma -cclib
     -lbigarray unix.cma -cclib -lunix -i /usr/local/lib/ocaml/caml
     fastdot_c.c fastdot.cmo inthashtbl.cmo arry.cmo util.cmo data.cmo
     bitvec.cmo cg.cmo wsemlm.cmo bfgs.cmo pa.cmo id88.cmo
     radapt.cmo kernelmap.cmo abffs.cmo main.cmo
     /usr/bin/ld: cannot find -lstr
           

   here you should edit the makefile again, changed the 62 line -lstr to
   -lcamlstr:

     #withstr =str.cma -cclib -lstr
     withstr =str.cma -cclib -lcamlstr

   then you can type    make    make a slower version of executable file
      megam    and type    make opt    to get a faster version of executable file
      megam.opt    in the makefile directory.

   but that   s not the end, if you want use it in the nltk, you should tell
   the nltk where to find the    megam    or    megam.opt    binary, nltk use a
   config_megam to lookup the binary version:
_megam_bin = none
def config_megam(bin=none):
    """
    configure nltk's interface to the ``megam`` maxent optimization
    package.

    :param bin: the full path to the ``megam`` binary.  if not specified,
        then nltk will search the system for a ``megam`` binary; and if
        one is not found, it will raise a ``lookuperror`` exception.
    :type bin: str
    """
    global _megam_bin
    _megam_bin = find_binary(
        'megam', bin,
        env_vars=['megam'],
        binary_names=['megam.opt', 'megam', 'megam_686', 'megam_i686.opt'],
        url='http://www.umiacs.umd.edu/~hal/megam/index.html')

   _megam_bin = none def config_megam(bin=none): """ configure nltk's
   interface to the ``megam`` maxent optimization package. :param bin: the
   full path to the ``megam`` binary. if not specified, then nltk will
   search the system for a ``megam`` binary; and if one is not found, it
   will raise a ``lookuperror`` exception. :type bin: str """ global
   _megam_bin _megam_bin = find_binary( 'megam', bin, env_vars=['megam'],
   binary_names=['megam.opt', 'megam', 'megam_686', 'megam_i686.opt'],
   url='http://www.umiacs.umd.edu/~hal/megam/index.html')

   the best way to do this to copy the binary version megam into the
   system binary path like /usr/bin or /usr/local/bin, then you never need
   use the to config_megam every time you use it for maximum id178
   modeling in the nltk.

     sudo cp megam /usr/local/bin/
     sudo cp megam.opt /usr/local/bin/

   just use it like this:

in [1]: import random

in [2]: from nltk.corpus import names

in [3]: from nltk import maxentclassifier

in [5]: from nltk import classify

in [7]: names = ([(name, 'male') for name in names.words('male.txt')] + [(name,
'female') for name in names.words('female.txt')])

in [8]: random.shuffle(names)

in [10]: def gender_features3(name):
        features = {}
        features["fl"] = name[0].lower()
        features["ll"] = name[-1].lower()
        features["fw"] = name[:2].lower()
        features["lw"] = name[-2:].lower()
        return features
   ....:

in [11]: featuresets = [(gender_features3(n), g) for (n, g) in names]

in [12]: train_set, test_set = featuresets[500:], featuresets[:500]

in [17]: me3_megam_classifier = maxentclassifier.train(train_set, "megam")
[found megam: megam]
scanning file...7444 train, 0 dev, 0 test, reading...done
warning: there only appear to be two classes, but we're
         optimizing with bfgs...using binary optimization
         with cg would be much faster
optimizing with lambda = 0
it 1   dw 4.640e-01 pp 6.38216e-01 er 0.37413
it 2   dw 2.065e-01 pp 5.74892e-01 er 0.37413
it 3   dw 3.503e-01 pp 5.43226e-01 er 0.24328
it 4   dw 1.209e-01 pp 5.29406e-01 er 0.22394
it 5   dw 4.864e-01 pp 5.27097e-01 er 0.26115
it 6   dw 5.765e-01 pp 4.92409e-01 er 0.23415
it 7   dw 0.000e+00 pp 4.92409e-01 er 0.23415
-------------------------
it 1 dw 1.802e-01 pp 4.74930e-01 er 0.21588
it 2   dw 3.478e-02 pp 4.70876e-01 er 0.21548
it 3   dw 1.963e-01 pp 4.61761e-01 er 0.21709
it 4   dw 9.624e-02 pp 4.56257e-01 er 0.21574
it 5   dw 3.442e-01 pp 4.54401e-01
......
it 10  dw 2.399e-03 pp 3.71020e-01 er 0.16967
it 11  dw 2.202e-02 pp 3.71017e-01 er 0.16980
it 12  dw 0.000e+00 pp 3.71017e-01 er 0.16980
-------------------------
it 1 dw 2.620e-02 pp 3.70816e-01 er 0.17020
it 2   dw 2.285e-02 pp 3.70721e-01 er 0.16953
it 3   dw 1.074e-02 pp 3.70631e-01 er 0.16980
it 4   dw 3.152e-02 pp 3.70580e-01 er 0.16994
it 5   dw 2.263e-02 pp 3.70504e-01 er 0.16940
it 6   dw 1.115e-01 pp 3.70370e-01 er 0.16886
it 7   dw 1.938e-01 pp 3.70318e-01 er 0.16913
it 8   dw 1.365e-01 pp 3.69815e-01 er 0.16940
it 9   dw 2.634e-01 pp 3.69366e-01 er 0.16873
it 10  dw 2.498e-01 pp 3.69290e-01 er 0.17007
it 11  dw 2.515e-01 pp 3.69240e-01 er 0.16994
it 12  dw 3.027e-01 pp 3.69234e-01 er 0.16994
it 13  dw 9.850e-03 pp 3.69233e-01 er 0.16994
it 14  dw 1.152e-01 pp 3.69214e-01 er 0.16994
it 15  dw 0.000e+00 pp 3.69214e-01 er 0.16994

in [18]: classify.accuracy(me3_megam_classifier, test_set)
out[18]: 0.812

   in [1]: import random in [2]: from nltk.corpus import names in [3]:
   from nltk import maxentclassifier in [5]: from nltk import classify in
   [7]: names = ([(name, 'male') for name in names.words('male.txt')] +
   [(name, 'female') for name in names.words('female.txt')]) in [8]:
   random.shuffle(names) in [10]: def gender_features3(name): features =
   {} features["fl"] = name[0].lower() features["ll"] = name[-1].lower()
   features["fw"] = name[:2].lower() features["lw"] = name[-2:].lower()
   return features ....: in [11]: featuresets = [(gender_features3(n), g)
   for (n, g) in names] in [12]: train_set, test_set = featuresets[500:],
   featuresets[:500] in [17]: me3_megam_classifier =
   maxentclassifier.train(train_set, "megam") [found megam: megam]
   scanning file...7444 train, 0 dev, 0 test, reading...done warning:
   there only appear to be two classes, but we're optimizing with
   bfgs...using binary optimization with cg would be much faster
   optimizing with lambda = 0 it 1 dw 4.640e-01 pp 6.38216e-01 er 0.37413
   it 2 dw 2.065e-01 pp 5.74892e-01 er 0.37413 it 3 dw 3.503e-01 pp
   5.43226e-01 er 0.24328 it 4 dw 1.209e-01 pp 5.29406e-01 er 0.22394 it 5
   dw 4.864e-01 pp 5.27097e-01 er 0.26115 it 6 dw 5.765e-01 pp 4.92409e-01
   er 0.23415 it 7 dw 0.000e+00 pp 4.92409e-01 er 0.23415
   ------------------------- it 1 dw 1.802e-01 pp 4.74930e-01 er 0.21588
   it 2 dw 3.478e-02 pp 4.70876e-01 er 0.21548 it 3 dw 1.963e-01 pp
   4.61761e-01 er 0.21709 it 4 dw 9.624e-02 pp 4.56257e-01 er 0.21574 it 5
   dw 3.442e-01 pp 4.54401e-01 ...... it 10 dw 2.399e-03 pp 3.71020e-01 er
   0.16967 it 11 dw 2.202e-02 pp 3.71017e-01 er 0.16980 it 12 dw 0.000e+00
   pp 3.71017e-01 er 0.16980 ------------------------- it 1 dw 2.620e-02
   pp 3.70816e-01 er 0.17020 it 2 dw 2.285e-02 pp 3.70721e-01 er 0.16953
   it 3 dw 1.074e-02 pp 3.70631e-01 er 0.16980 it 4 dw 3.152e-02 pp
   3.70580e-01 er 0.16994 it 5 dw 2.263e-02 pp 3.70504e-01 er 0.16940 it 6
   dw 1.115e-01 pp 3.70370e-01 er 0.16886 it 7 dw 1.938e-01 pp 3.70318e-01
   er 0.16913 it 8 dw 1.365e-01 pp 3.69815e-01 er 0.16940 it 9 dw
   2.634e-01 pp 3.69366e-01 er 0.16873 it 10 dw 2.498e-01 pp 3.69290e-01
   er 0.17007 it 11 dw 2.515e-01 pp 3.69240e-01 er 0.16994 it 12 dw
   3.027e-01 pp 3.69234e-01 er 0.16994 it 13 dw 9.850e-03 pp 3.69233e-01
   er 0.16994 it 14 dw 1.152e-01 pp 3.69214e-01 er 0.16994 it 15 dw
   0.000e+00 pp 3.69214e-01 er 0.16994 in [18]:
   classify.accuracy(me3_megam_classifier, test_set) out[18]: 0.812

   the megam using in nltk depends the python subprocess.popen, and it   s
   very fast and costs fewer resources than the original gis or iis maxent
   module in nltk. i have also compiled it in my local mac pro, and meet
   the same problems like the linux ubuntu compile process. you can also
   reference this article to compile the megam in mac os: [38]compiling
   megam on macos x.

   that   s the end, just enjoy using megam in your nltk or python projet.

   posted by [39]textminer

related posts:

    1. [40]dive into nltk, part ix: from text classification to sentiment
       analysis
    2. [41]dive into nltk, part vii: a preliminary study on text
       classification
    3. [42]text analysis online no longer provides nltk stanford nlp api
       interface
    4. [43]dive into nltk, part x: play with id97 models based on nltk
       corpus

   [44]deep learning specialization on coursera

   posted in [45]nlp, [46]nlp tools, [47]nltk, [48]text analysis, [49]text
   classification, [50]id111, [51]text processing tagged [52]caml,
   [53]maxent model, [54]maximum id178, [55]maximum id178 classifier,
   [56]maximum id178 libraries, [57]maximum id178 model, [58]maximum
   id178 modeling, [59]maximum id178 models, [60]megam, [61]nltk
   maximum id178 model, [62]ocaml, [63]text classification, [64]text
   classifier [65]permalink

post navigation

   [66]    dive into nltk, part vii: a preliminary study on text
   classification
   [67]we have launched the professional text id31 api on
   mashape    
     __________________________________________________________________

comments

dive into nltk, part viii: using external maximum id178 modeling libraries
for text classification     2 comments

    1.
   ramya sruthi on [68]august 21, 2017 at 5:19 am said:
       hi, can you explain the output of megam classifier. why there are
       so many iterations.
       [69]reply    
          +
        textminer on [70]august 31, 2017 at 5:24 am said:
            it   s training iterations which lower the cost.
            [71]reply    

leave a reply [72]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   [ ] save my name, email, and website in this browser for the next time
   i comment.

   post comment

   [73][dlai-logo-final-minus-font-plus-white-backg.png]
   [show?id=9iqcvd3eeqc&bids=541296.11421701896&type=2&subid=0]

   search for: ____________________ search

   [ins: :ins]

recent posts

     * [74]deep learning practice for nlp: large movie review data
       id31 from scratch
     * [75]best coursera courses for data science
     * [76]best coursera courses for machine learning
     * [77]best coursera courses for deep learning
     * [78]dive into nlp with deep learning, part i: getting started with
       dl4nlp

recent comments

     * textminer on [79]training id97 model on english wikipedia by
       gensim
     * ankit ramani on [80]training id97 model on english wikipedia by
       gensim
     * vincent on [81]training id97 model on english wikipedia by
       gensim
     * muhammad amin nadim on [82]andrew ng deep learning specialization:
       best deep learning course for beginners and deep learners
     * saranya on [83]training id97 model on english wikipedia by
       gensim

archives

     * [84]november 2018
     * [85]august 2018
     * [86]july 2018
     * [87]june 2018
     * [88]january 2018
     * [89]october 2017
     * [90]september 2017
     * [91]august 2017
     * [92]july 2017
     * [93]may 2017
     * [94]april 2017
     * [95]march 2017
     * [96]december 2016
     * [97]october 2016
     * [98]august 2016
     * [99]july 2016
     * [100]june 2016
     * [101]may 2016
     * [102]april 2016
     * [103]february 2016
     * [104]december 2015
     * [105]november 2015
     * [106]september 2015
     * [107]may 2015
     * [108]april 2015
     * [109]march 2015
     * [110]february 2015
     * [111]january 2015
     * [112]december 2014
     * [113]november 2014
     * [114]october 2014
     * [115]september 2014
     * [116]july 2014
     * [117]june 2014
     * [118]may 2014
     * [119]april 2014
     * [120]january 2014

categories

     * [121]ainlp
     * [122]coursera course
     * [123]data science
     * [124]deep learning
     * [125]dl4nlp
     * [126]how to use mashape api
     * [127]keras
     * [128]machine learning
     * [129]id39
     * [130]nlp
     * [131]nlp tools
     * [132]nltk
     * [133]id31
     * [134]tensorflow
     * [135]text analysis
     * [136]text classification
     * [137]id111
     * [138]text processing
     * [139]text similarity
     * [140]text summarization
     * [141]textanalysis api
     * [142]uncategorized
     * [143]id27
     * [144]id40

meta

     * [145]log in
     * [146]entries rss
     * [147]comments rss
     * [148]wordpress.org

     [149]text analysis online

     [150]text summarizer

     [151]text processing

     [152]word similarity

     [153]best coursera course

     [154]best coursera courses

     [155]elastic patent

     2019 - [156]id111 online - [157]weaver xtreme theme

   [158]   

references

   visible links
   1. https://textminingonline.com/feed
   2. https://textminingonline.com/comments/feed
   3. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification/feed
   4. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification
   5. https://textminingonline.com/we-have-launched-the-professional-text-sentiment-analysis-api-on-mashape
   6. https://textminingonline.com/wp-json/oembed/1.0/embed?url=https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
   7. https://textminingonline.com/wp-json/oembed/1.0/embed?url=https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification&format=xml
   8. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification#page-bottom
   9. https://textminingonline.com/
  10. https://textminingonline.com/
  11. http://textanalysisonline.com/#new_tab
  12. http://keywordextraction.net/#new_tab
  13. http://textsummarization.net/#new_tab
  14. https://wordsimilarity.com/#new_tab
  15. https://textminingonline.com/about
  16. https://textminingonline.com/
  17. https://textminingonline.com/category/nlp
  18. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification
  19. https://textminingonline.com/we-have-launched-the-professional-text-sentiment-analysis-api-on-mashape
  20. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
  21. https://textminingonline.com/author/yuzhen
  22. https://click.linksynergy.com/fs-bin/click?id=9iqcvd3eeqc&offerid=467035.416&subid=0&type=4
  23. http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk
  24. http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk
  25. http://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize
  26. http://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger
  27. http://textminingonline.com/dive-into-nltk-part-iv-id30-and-lemmatization
  28. http://textminingonline.com/dive-into-nltk-part-v-using-stanford-text-analysis-tools-in-python
  29. http://textminingonline.com/dive-into-nltk-part-vi-add-stanford-word-segmenter-interface-for-python-nltk
  30. http://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification
  31. http://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
  32. http://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis
  33. http://textminingonline.com/?p=872
  34. http://textanalysisonline.com/
  35. http://homepages.inf.ed.ac.uk/lzhang10/maxent.html
  36. http://www.umiacs.umd.edu/~hal/megam/
  37. http://caml.inria.fr/
  38. http://vene.ro/blog/compiling-megam-on-macos-x.html
  39. http://textminingonline.com/
  40. https://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis
  41. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification
  42. https://textminingonline.com/text-analysis-online-no-longer-provides-nltk-stanford-nlp-api-interface
  43. https://textminingonline.com/dive-into-nltk-part-x-play-with-id97-models-based-on-nltk-corpus
  44. https://click.linksynergy.com/fs-bin/click?id=9iqcvd3eeqc&offerid=467035.414&subid=0&type=4
  45. https://textminingonline.com/category/nlp
  46. https://textminingonline.com/category/nlp-tools
  47. https://textminingonline.com/category/nltk
  48. https://textminingonline.com/category/text-analysis
  49. https://textminingonline.com/category/text-classification
  50. https://textminingonline.com/category/text-mining
  51. https://textminingonline.com/category/text-processing
  52. https://textminingonline.com/tag/caml
  53. https://textminingonline.com/tag/maxent-model
  54. https://textminingonline.com/tag/maximum-id178
  55. https://textminingonline.com/tag/maximum-id178-classifier
  56. https://textminingonline.com/tag/maximum-id178-libraries
  57. https://textminingonline.com/tag/maximum-id178-model
  58. https://textminingonline.com/tag/maximum-id178-modeling
  59. https://textminingonline.com/tag/maximum-id178-models
  60. https://textminingonline.com/tag/megam
  61. https://textminingonline.com/tag/nltk-maximum-id178-model
  62. https://textminingonline.com/tag/ocaml
  63. https://textminingonline.com/tag/text-classification-2
  64. https://textminingonline.com/tag/text-classifier
  65. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
  66. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification
  67. https://textminingonline.com/we-have-launched-the-professional-text-sentiment-analysis-api-on-mashape
  68. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification#comment-132533
  69. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification?replytocom=132533#respond
  70. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification#comment-132827
  71. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification?replytocom=132827#respond
  72. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification#respond
  73. https://click.linksynergy.com/link?id=9iqcvd3eeqc&offerid=541296.11421701896&type=2&murl=https://www.coursera.org/specializations/deep-learning
  74. https://textminingonline.com/deep-learning-practice-for-nlp-large-movie-review-data-sentiment-analysis-from-scratch
  75. https://textminingonline.com/best-coursera-courses-for-data-science
  76. https://textminingonline.com/best-coursera-courses-for-machine-learning
  77. https://textminingonline.com/best-coursera-courses-for-deep-learning
  78. https://textminingonline.com/dive-into-nlp-with-deep-learning-part-i-getting-started-with-dl4nlp
  79. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-138841
  80. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-138807
  81. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-138723
  82. https://textminingonline.com/andrew-ng-deep-learning-specialization-best-deep-learning-course-for-beginners-and-deep-learners#comment-138475
  83. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-137923
  84. https://textminingonline.com/2018/11
  85. https://textminingonline.com/2018/08
  86. https://textminingonline.com/2018/07
  87. https://textminingonline.com/2018/06
  88. https://textminingonline.com/2018/01
  89. https://textminingonline.com/2017/10
  90. https://textminingonline.com/2017/09
  91. https://textminingonline.com/2017/08
  92. https://textminingonline.com/2017/07
  93. https://textminingonline.com/2017/05
  94. https://textminingonline.com/2017/04
  95. https://textminingonline.com/2017/03
  96. https://textminingonline.com/2016/12
  97. https://textminingonline.com/2016/10
  98. https://textminingonline.com/2016/08
  99. https://textminingonline.com/2016/07
 100. https://textminingonline.com/2016/06
 101. https://textminingonline.com/2016/05
 102. https://textminingonline.com/2016/04
 103. https://textminingonline.com/2016/02
 104. https://textminingonline.com/2015/12
 105. https://textminingonline.com/2015/11
 106. https://textminingonline.com/2015/09
 107. https://textminingonline.com/2015/05
 108. https://textminingonline.com/2015/04
 109. https://textminingonline.com/2015/03
 110. https://textminingonline.com/2015/02
 111. https://textminingonline.com/2015/01
 112. https://textminingonline.com/2014/12
 113. https://textminingonline.com/2014/11
 114. https://textminingonline.com/2014/10
 115. https://textminingonline.com/2014/09
 116. https://textminingonline.com/2014/07
 117. https://textminingonline.com/2014/06
 118. https://textminingonline.com/2014/05
 119. https://textminingonline.com/2014/04
 120. https://textminingonline.com/2014/01
 121. https://textminingonline.com/category/ainlp
 122. https://textminingonline.com/category/coursera-course
 123. https://textminingonline.com/category/data-science
 124. https://textminingonline.com/category/deep-learning
 125. https://textminingonline.com/category/dl4nlp
 126. https://textminingonline.com/category/how-to-use-mashape-api
 127. https://textminingonline.com/category/keras
 128. https://textminingonline.com/category/machine-learning
 129. https://textminingonline.com/category/named-entity-recognition
 130. https://textminingonline.com/category/nlp
 131. https://textminingonline.com/category/nlp-tools
 132. https://textminingonline.com/category/nltk
 133. https://textminingonline.com/category/sentiment-analysis
 134. https://textminingonline.com/category/tensorflow
 135. https://textminingonline.com/category/text-analysis
 136. https://textminingonline.com/category/text-classification
 137. https://textminingonline.com/category/text-mining
 138. https://textminingonline.com/category/text-processing
 139. https://textminingonline.com/category/text-similarity
 140. https://textminingonline.com/category/text-summarization
 141. https://textminingonline.com/category/textanalysis-api-2
 142. https://textminingonline.com/category/uncategorized
 143. https://textminingonline.com/category/word-embedding
 144. https://textminingonline.com/category/word-segmentation
 145. https://textminingonline.com/wp-login.php
 146. https://textminingonline.com/feed
 147. https://textminingonline.com/comments/feed
 148. https://wordpress.org/
 149. http://textanalysisonline.com/
 150. http://textsummarization.net/
 151. http://textprocessing.org/
 152. http://wordsimilarity.com/
 153. https://bestcourseracourse.com/
 154. https://bestcourseracourses.com/
 155. https://elasticpatent.com/
 156. https://textminingonline.com/
 157. https://weavertheme.com/
 158. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification#page-top

   hidden links:
 160. https://wordpress.org/
