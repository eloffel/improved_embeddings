    #[1]id111 online    feed [2]id111 online    comments feed
   [3]id111 online    dive into nltk, part ix: from text
   classification to id31 comments feed [4]dive into
   tensorflow, part iii: gtx 1080+ubuntu16.04+cuda8.0+cudnn5.0+tensorflow
   [5]dive into tensorflow, part iv: hello mnist [6]alternate [7]alternate

   [ins: :ins]

   [8]   



   javascript is disabled. please enable javascript on your browser to
   best view this site.

[9]id111 online

   search for: ____________________ search

id111 | text analysis | text process | natural language processing

   id111 online
     * [10]home
     * [11]textanalysis
     * [12]keywordextraction
     * [13]textsummarization
     * [14]wordsimilarity
     * [15]about

   [16]home   [17]nlp   dive into nltk, part ix: from text classification to
   id31
   [ins: :ins]

post navigation

   [18]    dive into tensorflow, part iii: gtx
   1080+ubuntu16.04+cuda8.0+cudnn5.0+tensorflow
   [19]dive into tensorflow, part iv: hello mnist    

dive into nltk, part ix: from text classification to id31

   posted on [20]july 24, 2016 by [21]textminermarch 26, 2017
   [22]deep learning specialization on coursera

   this is the ninth article in the series    [23]dive into nltk   , here is
   an index of all the articles in the series that have been published to
   date:
   [ins: :ins]

   [24]part i: getting started with nltk
   [25]part ii: sentence tokenize and word tokenize
   [26]part iii: part-of-speech tagging and pos tagger
   [27]part iv: id30 and lemmatization
   [28]part v: using stanford text analysis tools in python
   [29]part vi: add stanford word segmenter interface for python nltk
   [30]part vii: a preliminary study on text classification
   [31]part viii: using external maximum id178 modeling libraries for
   text classification
   [32]part ix: from text classification to id31
   [33]part x: play with id97 models based on nltk corpus

   according [34]wikipedia, [35]id31 is defined like this:

     id31 (also known as opinion mining) refers to the use
     of natural language processing, text analysis and computational
     linguistics to identify and extract subjective information in source
     materials. id31 is widely applied to reviews and
     social media for a variety of applications, ranging from marketing
     to customer service.

     generally speaking, id31 aims to determine the
     attitude of a speaker or a writer with respect to some topic or the
     overall contextual polarity of a document. the attitude may be his
     or her judgment or evaluation (see appraisal theory), affective
     state (that is to say, the emotional state of the author when
     writing), or the intended emotional communication (that is to say,
     the emotional effect the author wishes to have on the reader).

   generally speaking, [36]id31 can be seen as one task of
   text classification. based on the movie review data from nltk, we can
   train a basic text classification model for id31:
python 2.7.6 (default, jun  3 2014, 07:43:23)
type "copyright", "credits" or "license" for more information.

ipython 3.1.0 -- an enhanced interactive python.
?         -> introduction and overview of ipython's features.
%quickref -> quick reference.
help      -> python's own help system.
object?   -> details about 'object', use 'object??' for extra details.

in [1]: import nltk

in [2]: from nltk.corpus import movie_reviews

in [3]: from random import shuffle

in [4]: documents = [(list(movie_reviews.words(fileid)), category)
for category in movie_reviews.categories()
for fileid in movie_reviews.fileids(category)]

in [5]: shuffle(documents)

in [6]: print documents[0]
([u'the', u'general', u"'", u's', u'daughter', u'will', u'probably', u'be', u'th
e', u'cleverest', u'stupid', u'film', u'we', u"'", u'll', u'see', u'this', u'yea
r', u'--', u'or', u'perhaps', u'the', u'stupidest', u'clever', u'film', u'.', u'
it', u"'", u's', u'confusing', u'to', u'a', u'critic', u'when', u'so', u'much',
u'knuckleheaded', u'plotting', u'and', u'ostentatious', u'direction', u'shares',
 u'the', u'screen', u'with', u'so', u'much', u'snappy', u'dialogue', u'and', u'c
risp', u'character', u'interaction', u'.', u'that', u',', u'however', u',', u'is
', u'what', u'happens', u'when', u'legendary', u'screenwriter', u'william', u'go
ldman', u'takes', u'a', u'pass', u'at', u'an', u'otherwise', u'brutally', u'pred
ictable', u'conspiracy', u'thriller', u'.', u'the', u'punched', u'-', u'up', u'p
unch', u'lines', u'are', u'ever', u'on', u'the', u'verge', u'of', u'convincing',
 u'you', u'the', u'general', u"'", u's', u'daughter', u'has', u'a', u'brain', u'
in', u'its', u'head', u',', u'even', u'as', u'the', u'remaining', u'75', u'%', u
'of', u'the', u'narrative', u'punches', u'you', u'in', u'the', u'face', u'with',
 u'its', u'lack', u'of', u'common', u'sense', u'.', u'our', u'hero', u'is', u'wa
rrant', u'officer', u'paul', u'brenner', u',', u'a', u'brash', u'investigator',
u'for', u'the', u'u', u'.', u's', u'.', u'army', u"'", u's', u'criminal', u'inve
stigation', u'division', u'.', u'his', u'latest', u'case', u'is', u'the', u'murd
er', u'of', u'captain', u'elisabeth', u'campbell', u'(', u'leslie', u'stefanson'
, u')', u'at', u'a', u'georgia', u'base', u',', u'the', u'victim', u'found', u't
ied', u'to', u'the', u'ground', u'after', u'an', u'apparent', u'sexual', u'assau
lt', u'and', u'strangulation', u'.', u'complicating', u'the', u'case', u'is', u'
the', u'fact', u'that', u'capt', u'.', u'campbell', u'is', u'the', u'daughter',
u'of', u'general', u'joe', u'campbell', u'(', u'james', u'cromwell', u')', u',',
 u'a', u'war', u'hero', u'and', u'potential', u'vice', u'-', u'presidential', u'
nominee', u'.',
......
u'general', u'campbell', u'wants', u'to', u'keep', u'the', u'case', u'out', u'of
', u'the', u'press', u',', u'which', u'gives', u'brenner', u'only', u'the', u'36
', u'hours', u'before', u'the', u'fbi', u'steps', u'in', u'.', u'teamed', u'with
', u'rape', u'investigator', u'sarah', u'sunhill', u'(', u'madeleine', u'stowe',
 u')', u'--', u'who', u',', u'coincidentally', u'enough', u',', u'once', u'had',
 u'a', u'romantic', u'relationship', u'with', u'brenner', u'--', u'brenner', u'b
egins', u'uncovering', \ u'evidence', u'out', u'of', u'the', u'corner', u'of', u
'his', u'eye', u')', u'.', u'by', u'the', u'time', u'the', u'general', u"'", u's
', u'daughter', u'wanders', u'towards', u'its', u'over', u'-', u'wrought', u',',
 u'psycho', u'-', u'in', u'-', u'the', u'-', u'rain', u'finale', u',', u'west',
u"'", u's', u'heavy', u'hand', u'has', u'obliterated', u'most', u'of', u'what',
u'made', u'the', u'film', u'occasionally', u'fun', u'.', u'it', u"'", u's', u'si
lly', u'and', u'pretentious', u'film', u'-', u'making', u',', u'but', u'at', u'l
east', u'it', u'provides', u'a', u'giggle', u'or', u'five', u'.', u'goldman', u'
should', u'tear', u'the', u'15', u'decent', u'pages', u'out', u'of', u'this', u'
script', u'and', u'turn', u'them', u'into', u'a', u'stand', u'-', u'up', u'routi
ne', u'.'], u'neg')


# the total number of movie reviews documents in nltk is 2000
in [7]: len(documents)
out[7]: 2000


# construct a list of the 2,000 most frequent words in the overall corpus
in [8]: all_words = nltk.freqdist(w.lower() for w in movie_reviews.words())

in [9]: word_features = all_words.keys()[:2000]

# define a feature extractor that simply checks whether each of these words is p
resent in a given document.
in [10]: def document_features(document):
   ....:     document_words = set(document)
   ....:     features = {}
   ....:     for word in word_features:
   ....:         features['contains(%s)' % word] = (word in document_words)
   ....:     return features
   ....:


in [11]: print document_features(movie_reviews.words('pos/cv957_8737.txt'))
{u'contains(waste)': false, u'contains(lot)': false, u'contains(*)': true, u'con
tains(black)': false, u'contains(rated)': false, u'contains(potential)': false,
u'contains(m)': false, u'contains(understand)': false, u'contains(drug)': true,
u'contains(case)': false, u'contains(created)': false, u'contains(kiss)': false,
 u'contains(needed)': false, u'contains(c)': false, u'contains(about)': true, u'
contains(toy)': false, u'contains(longer)': false, u'contains(ready)': false, u'
contains(certainly)': false,
......
u'contains(good)': false, u'contains(live)': false, u'contains(appropriate)': fa
lse, u'contains(towards)': false, u'contains(smile)': false, u'contains(cross)':
 false}

# generate the feature sets for the movie review documents one by one
in [12]: featuresets = [(document_features(d), c) for (d, c) in documents]

# define the train set (1900 documents) and test set (100 documents)
in [13]: train_set, test_set = featuresets[100:], featuresets[:100]

# train a naive bayes classifier with train set by nltk
in [14]: classifier = nltk.naivebayesclassifier.train(train_set)

# get the accuracy of the naive bayes classifier with test set
in [15]: print nltk.classify.accuracy(classifier, test_set)
0.81

# debug info: show top n most informative features
in [16]: classifier.show_most_informative_features(10)
most informative features
   contains(outstanding) = true              pos : neg    =     13.3 : 1.0
         contains(mulan) = true              pos : neg    =      8.8 : 1.0
        contains(seagal) = true              neg : pos    =      8.0 : 1.0
   contains(wonderfully) = true              pos : neg    =      6.5 : 1.0
         contains(damon) = true              pos : neg    =      6.2 : 1.0
         contains(awful) = true              neg : pos    =      6.0 : 1.0
        contains(wasted) = true              neg : pos    =      5.9 : 1.0
          contains(lame) = true              neg : pos    =      5.8 : 1.0
         contains(flynt) = true              pos : neg    =      5.5 : 1.0
        contains(poorly) = true              neg : pos    =      5.1 : 1.0

   python 2.7.6 (default, jun 3 2014, 07:43:23) type "copyright",
   "credits" or "license" for more information. ipython 3.1.0 -- an
   enhanced interactive python. ? -> introduction and overview of
   ipython's features. %quickref -> quick reference. help -> python's own
   help system. object? -> details about 'object', use 'object??' for
   extra details. in [1]: import nltk in [2]: from nltk.corpus import
   movie_reviews in [3]: from random import shuffle in [4]: documents =
   [(list(movie_reviews.words(fileid)), category) for category in
   movie_reviews.categories() for fileid in
   movie_reviews.fileids(category)] in [5]: shuffle(documents) in [6]:
   print documents[0] ([u'the', u'general', u"'", u's', u'daughter',
   u'will', u'probably', u'be', u'the', u'cleverest', u'stupid', u'film',
   u'we', u"'", u'll', u'see', u'this', u'year', u'--', u'or', u'perhaps',
   u'the', u'stupidest', u'clever', u'film', u'.', u'it', u"'", u's',
   u'confusing', u'to', u'a', u'critic', u'when', u'so', u'much',
   u'knuckleheaded', u'plotting', u'and', u'ostentatious', u'direction',
   u'shares', u'the', u'screen', u'with', u'so', u'much', u'snappy',
   u'dialogue', u'and', u'crisp', u'character', u'interaction', u'.',
   u'that', u',', u'however', u',', u'is', u'what', u'happens', u'when',
   u'legendary', u'screenwriter', u'william', u'goldman', u'takes', u'a',
   u'pass', u'at', u'an', u'otherwise', u'brutally', u'predictable',
   u'conspiracy', u'thriller', u'.', u'the', u'punched', u'-', u'up',
   u'punch', u'lines', u'are', u'ever', u'on', u'the', u'verge', u'of',
   u'convincing', u'you', u'the', u'general', u"'", u's', u'daughter',
   u'has', u'a', u'brain', u'in', u'its', u'head', u',', u'even', u'as',
   u'the', u'remaining', u'75', u'%', u'of', u'the', u'narrative',
   u'punches', u'you', u'in', u'the', u'face', u'with', u'its', u'lack',
   u'of', u'common', u'sense', u'.', u'our', u'hero', u'is', u'warrant',
   u'officer', u'paul', u'brenner', u',', u'a', u'brash', u'investigator',
   u'for', u'the', u'u', u'.', u's', u'.', u'army', u"'", u's',
   u'criminal', u'investigation', u'division', u'.', u'his', u'latest',
   u'case', u'is', u'the', u'murder', u'of', u'captain', u'elisabeth',
   u'campbell', u'(', u'leslie', u'stefanson', u')', u'at', u'a',
   u'georgia', u'base', u',', u'the', u'victim', u'found', u'tied', u'to',
   u'the', u'ground', u'after', u'an', u'apparent', u'sexual', u'assault',
   u'and', u'strangulation', u'.', u'complicating', u'the', u'case',
   u'is', u'the', u'fact', u'that', u'capt', u'.', u'campbell', u'is',
   u'the', u'daughter', u'of', u'general', u'joe', u'campbell', u'(',
   u'james', u'cromwell', u')', u',', u'a', u'war', u'hero', u'and',
   u'potential', u'vice', u'-', u'presidential', u'nominee', u'.', ......
   u'general', u'campbell', u'wants', u'to', u'keep', u'the', u'case',
   u'out', u'of', u'the', u'press', u',', u'which', u'gives', u'brenner',
   u'only', u'the', u'36', u'hours', u'before', u'the', u'fbi', u'steps',
   u'in', u'.', u'teamed', u'with', u'rape', u'investigator', u'sarah',
   u'sunhill', u'(', u'madeleine', u'stowe', u')', u'--', u'who', u',',
   u'coincidentally', u'enough', u',', u'once', u'had', u'a', u'romantic',
   u'relationship', u'with', u'brenner', u'--', u'brenner', u'begins',
   u'uncovering', \ u'evidence', u'out', u'of', u'the', u'corner', u'of',
   u'his', u'eye', u')', u'.', u'by', u'the', u'time', u'the', u'general',
   u"'", u's', u'daughter', u'wanders', u'towards', u'its', u'over', u'-',
   u'wrought', u',', u'psycho', u'-', u'in', u'-', u'the', u'-', u'rain',
   u'finale', u',', u'west', u"'", u's', u'heavy', u'hand', u'has',
   u'obliterated', u'most', u'of', u'what', u'made', u'the', u'film',
   u'occasionally', u'fun', u'.', u'it', u"'", u's', u'silly', u'and',
   u'pretentious', u'film', u'-', u'making', u',', u'but', u'at',
   u'least', u'it', u'provides', u'a', u'giggle', u'or', u'five', u'.',
   u'goldman', u'should', u'tear', u'the', u'15', u'decent', u'pages',
   u'out', u'of', u'this', u'script', u'and', u'turn', u'them', u'into',
   u'a', u'stand', u'-', u'up', u'routine', u'.'], u'neg') # the total
   number of movie reviews documents in nltk is 2000 in [7]:
   len(documents) out[7]: 2000 # construct a list of the 2,000 most
   frequent words in the overall corpus in [8]: all_words =
   nltk.freqdist(w.lower() for w in movie_reviews.words()) in [9]:
   word_features = all_words.keys()[:2000] # define a feature extractor
   that simply checks whether each of these words is present in a given
   document. in [10]: def document_features(document): ....:
   document_words = set(document) ....: features = {} ....: for word in
   word_features: ....: features['contains(%s)' % word] = (word in
   document_words) ....: return features ....: in [11]: print
   document_features(movie_reviews.words('pos/cv957_8737.txt'))
   {u'contains(waste)': false, u'contains(lot)': false, u'contains(*)':
   true, u'contains(black)': false, u'contains(rated)': false,
   u'contains(potential)': false, u'contains(m)': false,
   u'contains(understand)': false, u'contains(drug)': true,
   u'contains(case)': false, u'contains(created)': false,
   u'contains(kiss)': false, u'contains(needed)': false, u'contains(c)':
   false, u'contains(about)': true, u'contains(toy)': false,
   u'contains(longer)': false, u'contains(ready)': false,
   u'contains(certainly)': false, ...... u'contains(good)': false,
   u'contains(live)': false, u'contains(appropriate)': false,
   u'contains(towards)': false, u'contains(smile)': false,
   u'contains(cross)': false} # generate the feature sets for the movie
   review documents one by one in [12]: featuresets =
   [(document_features(d), c) for (d, c) in documents] # define the train
   set (1900 documents) and test set (100 documents) in [13]: train_set,
   test_set = featuresets[100:], featuresets[:100] # train a naive bayes
   classifier with train set by nltk in [14]: classifier =
   nltk.naivebayesclassifier.train(train_set) # get the accuracy of the
   naive bayes classifier with test set in [15]: print
   nltk.classify.accuracy(classifier, test_set) 0.81 # debug info: show
   top n most informative features in [16]:
   classifier.show_most_informative_features(10) most informative features
   contains(outstanding) = true pos : neg = 13.3 : 1.0 contains(mulan) =
   true pos : neg = 8.8 : 1.0 contains(seagal) = true neg : pos = 8.0 :
   1.0 contains(wonderfully) = true pos : neg = 6.5 : 1.0 contains(damon)
   = true pos : neg = 6.2 : 1.0 contains(awful) = true neg : pos = 6.0 :
   1.0 contains(wasted) = true neg : pos = 5.9 : 1.0 contains(lame) = true
   neg : pos = 5.8 : 1.0 contains(flynt) = true pos : neg = 5.5 : 1.0
   contains(poorly) = true neg : pos = 5.1 : 1.0

   based on the top-2000 word features, we can train a maximum id178
   classifier model with [37]nltk and megam:
in [17]: maxent_classifier = nltk.maxentclassifier.train(train_set, "megam")
[found megam: /usr/local/bin/megam]
scanning file...1900 train, 0 dev, 0 test, reading...done
warning: there only appear to be two classes, but we're
         optimizing with bfgs...using binary optimization
         with cg would be much faster
optimizing with lambda = 0
it 1   dw 2.415e-03 pp 6.85543e-01 er 0.49895
it 2   dw 1.905e-03 pp 6.72937e-01 er 0.48895
it 3   dw 7.755e-03 pp 6.53779e-01 er 0.19526
it 4   dw 1.583e-02 pp 6.30863e-01 er 0.33526
it 5   dw 4.763e-02 pp 5.89126e-01 er 0.33895
it 6   dw 8.723e-02 pp 5.09921e-01 er 0.21211
it 7   dw 2.223e-01 pp 4.13823e-01 er 0.17000
it 8   dw 2.183e-01 pp 3.81889e-01 er 0.16526
it 9   dw 3.448e-01 pp 3.79054e-01 er 0.17421
it 10  dw 7.749e-02 pp 3.73549e-01 er 0.17105
it 11  dw 1.413e-01 pp 3.61806e-01 er 0.15842
it 12  dw 1.380e-01 pp 3.61716e-01 er 0.16000
it 13  dw 5.230e-02 pp 3.59953e-01 er 0.16053
it 14  dw 1.092e-01 pp 3.58713e-01 er 0.16211
it 15  dw 1.252e-01 pp 3.58669e-01 er 0.16000
it 16  dw 1.370e-01 pp 3.57027e-01 er 0.16105
it 17  dw 2.213e-01 pp 3.56230e-01 er 0.15684
it 18  dw 1.397e-01 pp 3.51368e-01 er 0.15579
it 19  dw 7.718e-01 pp 3.38156e-01 er 0.14947
it 20  dw 6.426e-02 pp 3.36342e-01 er 0.14947
it 21  dw 1.531e-01 pp 3.33402e-01 er 0.15053
it 22  dw 1.047e-01 pp 3.33287e-01 er 0.14895
it 23  dw 1.379e-01 pp 3.30814e-01 er 0.14895
it 24  dw 1.480e+00 pp 3.02938e-01 er 0.12842
it 25  dw 0.000e+00 pp 3.02938e-01 er 0.12842
-------------------------
......
......
-------------------------
it 1 dw 1.981e-05 pp 8.59536e-02 er 0.00684
it 2   dw 4.179e-05 pp 8.58979e-02 er 0.00684
it 3   dw 3.792e-04 pp 8.56536e-02 er 0.00684
it 4   dw 1.076e-03 pp 8.52961e-02 er 0.00737
it 5   dw 2.007e-03 pp 8.49459e-02 er 0.00737
it 6   dw 4.055e-03 pp 8.42942e-02 er 0.00737
it 7   dw 2.664e-02 pp 8.16976e-02 er 0.00526
it 8   dw 1.888e-02 pp 8.12042e-02 er 0.00316
it 9   dw 5.093e-02 pp 8.08672e-02 er 0.00316
it 10  dw 3.968e-03 pp 8.08624e-02 er 0.00316
it 11  dw 0.000e+00 pp 8.08624e-02 er 0.00316

in [18]: print nltk.classify.accuracy(maxent_classifier, test_set)
0.89

in [19]: maxent_classifier.show_most_informative_features(10)
  -1.843 contains(waste)==false and label is u'neg'
  -1.006 contains(boring)==false and label is u'neg'
  -0.979 contains(worst)==false and label is u'neg'
  -0.973 contains(bad)==false and label is u'neg'
  -0.953 contains(unfortunately)==false and label is u'neg'
  -0.864 contains(lame)==false and label is u'neg'
  -0.850 contains(attempt)==false and label is u'neg'
  -0.833 contains(supposed)==false and label is u'neg'
  -0.815 contains(seen)==true and label is u'neg'
  -0.783 contains(laughable)==false and label is u'neg'

   in [17]: maxent_classifier = nltk.maxentclassifier.train(train_set,
   "megam") [found megam: /usr/local/bin/megam] scanning file...1900
   train, 0 dev, 0 test, reading...done warning: there only appear to be
   two classes, but we're optimizing with bfgs...using binary optimization
   with cg would be much faster optimizing with lambda = 0 it 1 dw
   2.415e-03 pp 6.85543e-01 er 0.49895 it 2 dw 1.905e-03 pp 6.72937e-01 er
   0.48895 it 3 dw 7.755e-03 pp 6.53779e-01 er 0.19526 it 4 dw 1.583e-02
   pp 6.30863e-01 er 0.33526 it 5 dw 4.763e-02 pp 5.89126e-01 er 0.33895
   it 6 dw 8.723e-02 pp 5.09921e-01 er 0.21211 it 7 dw 2.223e-01 pp
   4.13823e-01 er 0.17000 it 8 dw 2.183e-01 pp 3.81889e-01 er 0.16526 it 9
   dw 3.448e-01 pp 3.79054e-01 er 0.17421 it 10 dw 7.749e-02 pp
   3.73549e-01 er 0.17105 it 11 dw 1.413e-01 pp 3.61806e-01 er 0.15842 it
   12 dw 1.380e-01 pp 3.61716e-01 er 0.16000 it 13 dw 5.230e-02 pp
   3.59953e-01 er 0.16053 it 14 dw 1.092e-01 pp 3.58713e-01 er 0.16211 it
   15 dw 1.252e-01 pp 3.58669e-01 er 0.16000 it 16 dw 1.370e-01 pp
   3.57027e-01 er 0.16105 it 17 dw 2.213e-01 pp 3.56230e-01 er 0.15684 it
   18 dw 1.397e-01 pp 3.51368e-01 er 0.15579 it 19 dw 7.718e-01 pp
   3.38156e-01 er 0.14947 it 20 dw 6.426e-02 pp 3.36342e-01 er 0.14947 it
   21 dw 1.531e-01 pp 3.33402e-01 er 0.15053 it 22 dw 1.047e-01 pp
   3.33287e-01 er 0.14895 it 23 dw 1.379e-01 pp 3.30814e-01 er 0.14895 it
   24 dw 1.480e+00 pp 3.02938e-01 er 0.12842 it 25 dw 0.000e+00 pp
   3.02938e-01 er 0.12842 ------------------------- ...... ......
   ------------------------- it 1 dw 1.981e-05 pp 8.59536e-02 er 0.00684
   it 2 dw 4.179e-05 pp 8.58979e-02 er 0.00684 it 3 dw 3.792e-04 pp
   8.56536e-02 er 0.00684 it 4 dw 1.076e-03 pp 8.52961e-02 er 0.00737 it 5
   dw 2.007e-03 pp 8.49459e-02 er 0.00737 it 6 dw 4.055e-03 pp 8.42942e-02
   er 0.00737 it 7 dw 2.664e-02 pp 8.16976e-02 er 0.00526 it 8 dw
   1.888e-02 pp 8.12042e-02 er 0.00316 it 9 dw 5.093e-02 pp 8.08672e-02 er
   0.00316 it 10 dw 3.968e-03 pp 8.08624e-02 er 0.00316 it 11 dw 0.000e+00
   pp 8.08624e-02 er 0.00316 in [18]: print
   nltk.classify.accuracy(maxent_classifier, test_set) 0.89 in [19]:
   maxent_classifier.show_most_informative_features(10) -1.843
   contains(waste)==false and label is u'neg' -1.006
   contains(boring)==false and label is u'neg' -0.979
   contains(worst)==false and label is u'neg' -0.973 contains(bad)==false
   and label is u'neg' -0.953 contains(unfortunately)==false and label is
   u'neg' -0.864 contains(lame)==false and label is u'neg' -0.850
   contains(attempt)==false and label is u'neg' -0.833
   contains(supposed)==false and label is u'neg' -0.815
   contains(seen)==true and label is u'neg' -0.783
   contains(laughable)==false and label is u'neg'

   it seems that the maxent classifier has the better classifier result on
   the test set. let   s classify a test text with the naive bayes
   classifier and maxent classifier:
in [22]:  test_text = "i love this movie, very interesting"

in [23]: test_set = document_features(test_text.split())

in [24]: test_set
out[24]:
{u'contains(waste)': false,
 u'contains(lot)': false,
 u'contains(*)': false,
 u'contains(black)': false,
 u'contains(rated)': false,
 u'contains(potential)': false,
 u'contains(m)': false,
 u'contains(understand)': false,
 u'contains(drug)': false,
 u'contains(case)': false,
 u'contains(created)': false,
 u'contains(kiss)': false,
 u'contains(needed)': false,
 ......
 u'contains(happens)': false,
 u'contains(suddenly)': false,
 u'contains(almost)': false,
 u'contains(evil)': false,
 u'contains(building)': false,
 u'contains(michael)': false,
 ...}

# naivebayes classifier get the wrong result
in [25]: print classifier.classify(test_set)
neg

# maxent classifier done right
in [26]: print maxent_classifier.classify(test_set)
pos

# let's see the id203 result
in [27]: prob_result = classifier.prob_classify(test_set)

in [28]: prob_result
out[28]: <probdist with 2 samples>

in [29]: prob_result.max()
out[29]: u'neg'

in [30]: prob_result.prob("neg")
out[30]: 0.99999917093621

in [31]: prob_result.prob("pos")
out[31]: 8.29063793272753e-07

# maxent classifier id203 result
in [32]: print maxent_classifier.classify(test_set)
pos

in [33]: prob_result = maxent_classifier.prob_classify(test_set)

in [33]: prob_result.prob("pos")
out[33]: 0.67570114045832497

in [34]: prob_result.prob("neg")
out[34]: 0.32429885954167498

   in [22]: test_text = "i love this movie, very interesting" in [23]:
   test_set = document_features(test_text.split()) in [24]: test_set
   out[24]: {u'contains(waste)': false, u'contains(lot)': false,
   u'contains(*)': false, u'contains(black)': false, u'contains(rated)':
   false, u'contains(potential)': false, u'contains(m)': false,
   u'contains(understand)': false, u'contains(drug)': false,
   u'contains(case)': false, u'contains(created)': false,
   u'contains(kiss)': false, u'contains(needed)': false, ......
   u'contains(happens)': false, u'contains(suddenly)': false,
   u'contains(almost)': false, u'contains(evil)': false,
   u'contains(building)': false, u'contains(michael)': false, ...} #
   naivebayes classifier get the wrong result in [25]: print
   classifier.classify(test_set) neg # maxent classifier done right in
   [26]: print maxent_classifier.classify(test_set) pos # let's see the
   id203 result in [27]: prob_result =
   classifier.prob_classify(test_set) in [28]: prob_result out[28]:
   <probdist with 2 samples> in [29]: prob_result.max() out[29]: u'neg' in
   [30]: prob_result.prob("neg") out[30]: 0.99999917093621 in [31]:
   prob_result.prob("pos") out[31]: 8.29063793272753e-07 # maxent
   classifier id203 result in [32]: print
   maxent_classifier.classify(test_set) pos in [33]: prob_result =
   maxent_classifier.prob_classify(test_set) in [33]:
   prob_result.prob("pos") out[33]: 0.67570114045832497 in [34]:
   prob_result.prob("neg") out[34]: 0.32429885954167498

   till now, we just used the top-n word features, and for this sentiment
   analysis machine learning problem, add more features may be get better
   result. so we redesign the word features:
in [40]: def bag_of_words(words):
   ....:     return dict([(word, true) for word in words])
   ....:

in [43]: data_sets = [(bag_of_words(d), c) for (d, c) in documents]

in [44]: len(data_sets)
out[44]: 2000

in [45]: train_set, test_set = data_sets[100:], data_sets[:100]

in [46]: bayes_classifier = nltk.naivebayesclassifier.train(train_set)

in [47]: print nltk.classify.accuracy(bayes_classifier, test_set)
0.8

in [48]: bayes_classifier.show_most_informative_features(10)
most informative features
             outstanding = true              pos : neg    =     13.9 : 1.0
                  avoids = true              pos : neg    =     13.1 : 1.0
              astounding = true              pos : neg    =     11.7 : 1.0
                 insipid = true              neg : pos    =     11.0 : 1.0
                    3000 = true              neg : pos    =     11.0 : 1.0
               insulting = true              neg : pos    =     10.6 : 1.0
            manipulation = true              pos : neg    =     10.4 : 1.0
             fascination = true              pos : neg    =     10.4 : 1.0
                    slip = true              pos : neg    =     10.4 : 1.0
               ludicrous = true              neg : pos    =     10.1 : 1.0

in [49]: maxent_bg_classifier = nltk.maxentclassifier.train(train_set, "megam")
scanning file...1900 train, 0 dev, 0 test, reading...done
warning: there only appear to be two classes, but we're
         optimizing with bfgs...using binary optimization
         with cg would be much faster
optimizing with lambda = 0
it 1   dw 1.255e-01 pp 3.91521e-01 er 0.15368
it 2   dw 1.866e-02 pp 3.82995e-01 er 0.14684
it 3   dw 3.912e-02 pp 3.46794e-01 er 0.13368
it 4   dw 5.916e-02 pp 3.26135e-01 er 0.13684
it 5   dw 2.929e-02 pp 3.23077e-01 er 0.13474
it 6   dw 2.552e-02 pp 3.15917e-01 er 0.13526
it 7   dw 2.765e-02 pp 3.14291e-01 er 0.13526
it 8   dw 8.298e-02 pp 2.35472e-01 er 0.07263
it 9   dw 1.357e-01 pp 2.20265e-01 er 0.08684
it 10  dw 6.186e-02 pp 2.03567e-01 er 0.07158
it 11  dw 2.057e-01 pp 1.69049e-01 er 0.05316
it 12  dw 1.319e-01 pp 1.61575e-01 er 0.05263
it 13  dw 8.872e-02 pp 1.59902e-01 er 0.05526
it 14  dw 5.907e-02 pp 1.59254e-01 er 0.05632
it 15  dw 4.443e-02 pp 1.54540e-01 er 0.05368
it 16  dw 3.677e-01 pp 1.48646e-01 er 0.03842
it 17  dw 2.500e-01 pp 1.47460e-01 er 0.03947
it 18  dw 9.548e-01 pp 1.44516e-01 er 0.03842
it 19  dw 3.466e-01 pp 1.42935e-01 er 0.04211
it 20  dw 1.872e-02 pp 1.42847e-01 er 0.04263
it 21  dw 1.452e-01 pp 1.28344e-01 er 0.02737
it 22  dw 1.248e-01 pp 1.24428e-01 er 0.02526
it 23  dw 4.071e-01 pp 1.18201e-01 er 0.02211
it 24  dw 3.979e-01 pp 1.08352e-01 er 0.01526
it 25  dw 1.871e-01 pp 1.08345e-01 er 0.01632
it 26  dw 8.477e-02 pp 1.07972e-01 er 0.01579
it 27  dw 0.000e+00 pp 1.07972e-01 er 0.01579
-------------------------
.......
-------------------------
it 12  dw 4.018e-02 pp 1.73432e-05 er 0.00000
it 13  dw 3.898e-02 pp 1.62334e-05 er 0.00000
it 14  dw 9.937e-02 pp 1.52647e-05 er 0.00000
it 15  dw 5.558e-02 pp 1.31892e-05 er 0.00000
it 16  dw 5.646e-02 pp 1.30511e-05 er 0.00000
it 17  dw 1.100e-01 pp 1.23914e-05 er 0.00000
it 18  dw 4.541e-02 pp 1.17382e-05 er 0.00000
it 19  dw 1.316e-01 pp 1.04446e-05 er 0.00000
it 20  dw 1.919e-01 pp 9.04729e-06 er 0.00000
it 21  dw 1.039e-02 pp 9.02896e-06 er 0.00000
it 22  dw 2.843e-01 pp 8.92068e-06 er 0.00000
it 23  dw 1.100e-01 pp 8.54637e-06 er 0.00000
it 24  dw 2.199e-01 pp 8.36371e-06 er 0.00000
it 25  dw 2.428e-02 pp 8.24041e-06 er 0.00000
it 26  dw 0.000e+00 pp 8.24041e-06 er 0.00000

in [50]: print nltk.classify.accuracy(maxent_bg_classifier, test_set)
0.89

in [51]: maxent_bg_classifier.show_most_informative_features(10)
  -4.151 get==true and label is u'neg'
  -2.961 get==true and label is u'pos'
  -2.596 all==true and label is u'neg'
  -2.523 out==true and label is u'pos'
  -2.400 years==true and label is u'neg'
  -2.397 its==true and label is u'pos'
  -2.340 them==true and label is u'neg'
  -2.327 out==true and label is u'neg'
  -2.324 ,==true and label is u'neg'
  -2.259 (==true and label is u'neg'

   in [40]: def bag_of_words(words): ....: return dict([(word, true) for
   word in words]) ....: in [43]: data_sets = [(bag_of_words(d), c) for
   (d, c) in documents] in [44]: len(data_sets) out[44]: 2000 in [45]:
   train_set, test_set = data_sets[100:], data_sets[:100] in [46]:
   bayes_classifier = nltk.naivebayesclassifier.train(train_set) in [47]:
   print nltk.classify.accuracy(bayes_classifier, test_set) 0.8 in [48]:
   bayes_classifier.show_most_informative_features(10) most informative
   features outstanding = true pos : neg = 13.9 : 1.0 avoids = true pos :
   neg = 13.1 : 1.0 astounding = true pos : neg = 11.7 : 1.0 insipid =
   true neg : pos = 11.0 : 1.0 3000 = true neg : pos = 11.0 : 1.0
   insulting = true neg : pos = 10.6 : 1.0 manipulation = true pos : neg =
   10.4 : 1.0 fascination = true pos : neg = 10.4 : 1.0 slip = true pos :
   neg = 10.4 : 1.0 ludicrous = true neg : pos = 10.1 : 1.0 in [49]:
   maxent_bg_classifier = nltk.maxentclassifier.train(train_set, "megam")
   scanning file...1900 train, 0 dev, 0 test, reading...done warning:
   there only appear to be two classes, but we're optimizing with
   bfgs...using binary optimization with cg would be much faster
   optimizing with lambda = 0 it 1 dw 1.255e-01 pp 3.91521e-01 er 0.15368
   it 2 dw 1.866e-02 pp 3.82995e-01 er 0.14684 it 3 dw 3.912e-02 pp
   3.46794e-01 er 0.13368 it 4 dw 5.916e-02 pp 3.26135e-01 er 0.13684 it 5
   dw 2.929e-02 pp 3.23077e-01 er 0.13474 it 6 dw 2.552e-02 pp 3.15917e-01
   er 0.13526 it 7 dw 2.765e-02 pp 3.14291e-01 er 0.13526 it 8 dw
   8.298e-02 pp 2.35472e-01 er 0.07263 it 9 dw 1.357e-01 pp 2.20265e-01 er
   0.08684 it 10 dw 6.186e-02 pp 2.03567e-01 er 0.07158 it 11 dw 2.057e-01
   pp 1.69049e-01 er 0.05316 it 12 dw 1.319e-01 pp 1.61575e-01 er 0.05263
   it 13 dw 8.872e-02 pp 1.59902e-01 er 0.05526 it 14 dw 5.907e-02 pp
   1.59254e-01 er 0.05632 it 15 dw 4.443e-02 pp 1.54540e-01 er 0.05368 it
   16 dw 3.677e-01 pp 1.48646e-01 er 0.03842 it 17 dw 2.500e-01 pp
   1.47460e-01 er 0.03947 it 18 dw 9.548e-01 pp 1.44516e-01 er 0.03842 it
   19 dw 3.466e-01 pp 1.42935e-01 er 0.04211 it 20 dw 1.872e-02 pp
   1.42847e-01 er 0.04263 it 21 dw 1.452e-01 pp 1.28344e-01 er 0.02737 it
   22 dw 1.248e-01 pp 1.24428e-01 er 0.02526 it 23 dw 4.071e-01 pp
   1.18201e-01 er 0.02211 it 24 dw 3.979e-01 pp 1.08352e-01 er 0.01526 it
   25 dw 1.871e-01 pp 1.08345e-01 er 0.01632 it 26 dw 8.477e-02 pp
   1.07972e-01 er 0.01579 it 27 dw 0.000e+00 pp 1.07972e-01 er 0.01579
   ------------------------- ....... ------------------------- it 12 dw
   4.018e-02 pp 1.73432e-05 er 0.00000 it 13 dw 3.898e-02 pp 1.62334e-05
   er 0.00000 it 14 dw 9.937e-02 pp 1.52647e-05 er 0.00000 it 15 dw
   5.558e-02 pp 1.31892e-05 er 0.00000 it 16 dw 5.646e-02 pp 1.30511e-05
   er 0.00000 it 17 dw 1.100e-01 pp 1.23914e-05 er 0.00000 it 18 dw
   4.541e-02 pp 1.17382e-05 er 0.00000 it 19 dw 1.316e-01 pp 1.04446e-05
   er 0.00000 it 20 dw 1.919e-01 pp 9.04729e-06 er 0.00000 it 21 dw
   1.039e-02 pp 9.02896e-06 er 0.00000 it 22 dw 2.843e-01 pp 8.92068e-06
   er 0.00000 it 23 dw 1.100e-01 pp 8.54637e-06 er 0.00000 it 24 dw
   2.199e-01 pp 8.36371e-06 er 0.00000 it 25 dw 2.428e-02 pp 8.24041e-06
   er 0.00000 it 26 dw 0.000e+00 pp 8.24041e-06 er 0.00000 in [50]: print
   nltk.classify.accuracy(maxent_bg_classifier, test_set) 0.89 in [51]:
   maxent_bg_classifier.show_most_informative_features(10) -4.151
   get==true and label is u'neg' -2.961 get==true and label is u'pos'
   -2.596 all==true and label is u'neg' -2.523 out==true and label is
   u'pos' -2.400 years==true and label is u'neg' -2.397 its==true and
   label is u'pos' -2.340 them==true and label is u'neg' -2.327 out==true
   and label is u'neg' -2.324 ,==true and label is u'neg' -2.259 (==true
   and label is u'neg'

   now we can test the bigrams feature in the classifier model:
in [52]: from nltk import ngrams

in [53]: def bag_of_ngrams(words, n=2):
   ....:     ngs = [ng for ng in iter(ngrams(words, n))]
   ....:     return bag_of_words(ngs)
   ....:

in [54]: data_sets = [(bag_of_ngrams(d), c) for (d, c) in documents]

in [55]: train_set, test_set = data_sets[100:], data_sets[:100]

in [56]: nb_bi_classifier = nltk.naivebayesclassifier.train(train_set)

in [57]: print nltk.classify.accuracy(nb_bi_classifier, test_set)
0.83

in [59]: nb_bi_classifier.show_most_informative_features(10)
most informative features
    (u'is', u'terrific') = true              pos : neg    =     17.1 : 1.0
      (u'not', u'funny') = true              neg : pos    =     16.9 : 1.0
     (u'boring', u'and') = true              neg : pos    =     13.6 : 1.0
     (u'and', u'boring') = true              neg : pos    =     13.6 : 1.0
        (u'our', u'own') = true              pos : neg    =     13.1 : 1.0
        (u'why', u'did') = true              neg : pos    =     12.9 : 1.0
    (u'enjoyable', u',') = true              pos : neg    =     12.4 : 1.0
     (u'works', u'well') = true              pos : neg    =     12.4 : 1.0
      (u'.', u'cameron') = true              pos : neg    =     12.4 : 1.0
     (u'well', u'worth') = true              pos : neg    =     12.4 : 1.0

in [60]: maxent_bi_classifier = nltk.maxentclassifier.train(train_set, "megam")
scanning file...1900 train, 0 dev, 0 test, reading...done
warning: there only appear to be two classes, but we're
         optimizing with bfgs...using binary optimization
         with cg would be much faster
optimizing with lambda = 0
it 1   dw 6.728e-02 pp 4.68710e-01 er 0.25895
it 2   dw 6.127e-02 pp 3.37578e-01 er 0.13789
it 3   dw 1.712e-02 pp 2.94106e-01 er 0.11737
it 4   dw 2.538e-02 pp 2.68465e-01 er 0.11526
it 5   dw 3.965e-02 pp 2.46789e-01 er 0.10684
it 6   dw 1.240e-01 pp 1.98149e-01 er 0.07947
it 7   dw 1.640e-02 pp 1.62956e-01 er 0.05895
it 8   dw 1.320e-01 pp 1.07163e-01 er 0.02789
it 9   dw 1.233e-01 pp 8.79358e-02 er 0.01368
it 10  dw 2.815e-01 pp 5.51191e-02 er 0.00737
it 11  dw 1.127e-01 pp 3.91500e-02 er 0.00421
it 12  dw 3.463e-01 pp 2.95846e-02 er 0.00211
it 13  dw 1.114e-01 pp 2.90701e-02 er 0.00053
it 14  dw 1.453e-01 pp 1.95422e-02 er 0.00053
it 15  dw 1.976e-01 pp 1.54022e-02 er 0.00105
......
it 44  dw 2.544e-01 pp 9.05755e-15 er 0.00000
it 45  dw 4.974e-02 pp 9.02763e-15 er 0.00000
it 46  dw 9.311e-07 pp 9.02483e-15 er 0.00000
it 47  dw 0.000e+00 pp 9.02483e-15 er 0.00000

in [61]: print nltk.classify.accuracy(maxent_bi_classifier, test_set)
0.9

in [62]: maxent_bi_classifier.show_most_informative_features(10)
 -14.152 (u'a', u'man')==true and label is u'neg'
  12.821 (u'"', u'the')==true and label is u'neg'
 -12.399 (u'of', u'the')==true and label is u'neg'
 -11.881 (u'a', u'man')==true and label is u'pos'
  10.020 (u',', u'which')==true and label is u'neg'
   8.418 (u'and', u'that')==true and label is u'neg'
  -8.022 (u'and', u'the')==true and label is u'neg'
  -7.191 (u'on', u'a')==true and label is u'neg'
  -7.185 (u'on', u'a')==true and label is u'pos'
   7.107 (u',', u'which')==true and label is u'pos'

   in [52]: from nltk import ngrams in [53]: def bag_of_ngrams(words,
   n=2): ....: ngs = [ng for ng in iter(ngrams(words, n))] ....: return
   bag_of_words(ngs) ....: in [54]: data_sets = [(bag_of_ngrams(d), c) for
   (d, c) in documents] in [55]: train_set, test_set = data_sets[100:],
   data_sets[:100] in [56]: nb_bi_classifier =
   nltk.naivebayesclassifier.train(train_set) in [57]: print
   nltk.classify.accuracy(nb_bi_classifier, test_set) 0.83 in [59]:
   nb_bi_classifier.show_most_informative_features(10) most informative
   features (u'is', u'terrific') = true pos : neg = 17.1 : 1.0 (u'not',
   u'funny') = true neg : pos = 16.9 : 1.0 (u'boring', u'and') = true neg
   : pos = 13.6 : 1.0 (u'and', u'boring') = true neg : pos = 13.6 : 1.0
   (u'our', u'own') = true pos : neg = 13.1 : 1.0 (u'why', u'did') = true
   neg : pos = 12.9 : 1.0 (u'enjoyable', u',') = true pos : neg = 12.4 :
   1.0 (u'works', u'well') = true pos : neg = 12.4 : 1.0 (u'.',
   u'cameron') = true pos : neg = 12.4 : 1.0 (u'well', u'worth') = true
   pos : neg = 12.4 : 1.0 in [60]: maxent_bi_classifier =
   nltk.maxentclassifier.train(train_set, "megam") scanning file...1900
   train, 0 dev, 0 test, reading...done warning: there only appear to be
   two classes, but we're optimizing with bfgs...using binary optimization
   with cg would be much faster optimizing with lambda = 0 it 1 dw
   6.728e-02 pp 4.68710e-01 er 0.25895 it 2 dw 6.127e-02 pp 3.37578e-01 er
   0.13789 it 3 dw 1.712e-02 pp 2.94106e-01 er 0.11737 it 4 dw 2.538e-02
   pp 2.68465e-01 er 0.11526 it 5 dw 3.965e-02 pp 2.46789e-01 er 0.10684
   it 6 dw 1.240e-01 pp 1.98149e-01 er 0.07947 it 7 dw 1.640e-02 pp
   1.62956e-01 er 0.05895 it 8 dw 1.320e-01 pp 1.07163e-01 er 0.02789 it 9
   dw 1.233e-01 pp 8.79358e-02 er 0.01368 it 10 dw 2.815e-01 pp
   5.51191e-02 er 0.00737 it 11 dw 1.127e-01 pp 3.91500e-02 er 0.00421 it
   12 dw 3.463e-01 pp 2.95846e-02 er 0.00211 it 13 dw 1.114e-01 pp
   2.90701e-02 er 0.00053 it 14 dw 1.453e-01 pp 1.95422e-02 er 0.00053 it
   15 dw 1.976e-01 pp 1.54022e-02 er 0.00105 ...... it 44 dw 2.544e-01 pp
   9.05755e-15 er 0.00000 it 45 dw 4.974e-02 pp 9.02763e-15 er 0.00000 it
   46 dw 9.311e-07 pp 9.02483e-15 er 0.00000 it 47 dw 0.000e+00 pp
   9.02483e-15 er 0.00000 in [61]: print
   nltk.classify.accuracy(maxent_bi_classifier, test_set) 0.9 in [62]:
   maxent_bi_classifier.show_most_informative_features(10) -14.152 (u'a',
   u'man')==true and label is u'neg' 12.821 (u'"', u'the')==true and label
   is u'neg' -12.399 (u'of', u'the')==true and label is u'neg' -11.881
   (u'a', u'man')==true and label is u'pos' 10.020 (u',', u'which')==true
   and label is u'neg' 8.418 (u'and', u'that')==true and label is u'neg'
   -8.022 (u'and', u'the')==true and label is u'neg' -7.191 (u'on',
   u'a')==true and label is u'neg' -7.185 (u'on', u'a')==true and label is
   u'pos' 7.107 (u',', u'which')==true and label is u'pos'

   and again, we can use the words feature and ngrams (bigrams) feature
   together:
in [63]: def bag_of_all(words, n=2):
   ....:     all_features = bag_of_words(words)
   ....:     ngram_features = bag_of_ngrams(words, n=n)
   ....:     all_features.update(ngram_features)
   ....:     return all_features
   ....:

in [64]: data_sets = [(bag_of_all(d), c) for (d, c) in documents]

in [65]: train_set, test_set = data_sets[100:], data_sets[:100]

in [66]: nb_all_classifier = nltk.naivebayesclassifier.train(train_set)

in [67]: print nltk.classify.accuracy(nb_all_classifier, test_set)
0.83

in [68]: nb_all_classifier.show_most_informative_features(10)
most informative features
    (u'is', u'terrific') = true              pos : neg    =     17.1 : 1.0
      (u'not', u'funny') = true              neg : pos    =     16.9 : 1.0
             outstanding = true              pos : neg    =     13.9 : 1.0
     (u'boring', u'and') = true              neg : pos    =     13.6 : 1.0
     (u'and', u'boring') = true              neg : pos    =     13.6 : 1.0
                  avoids = true              pos : neg    =     13.1 : 1.0
        (u'our', u'own') = true              pos : neg    =     13.1 : 1.0
        (u'why', u'did') = true              neg : pos    =     12.9 : 1.0
    (u'enjoyable', u',') = true              pos : neg    =     12.4 : 1.0
     (u'works', u'well') = true              pos : neg    =     12.4 : 1.0


in [71]: maxent_all_classifier = nltk.maxentclassifier.train(train_set, "megam")

scanning file...1900 train, 0 dev, 0 test, reading...done
warning: there only appear to be two classes, but we're
         optimizing with bfgs...using binary optimization
         with cg would be much faster
optimizing with lambda = 0
it 1   dw 8.715e-02 pp 3.82841e-01 er 0.17684
it 2   dw 2.846e-02 pp 2.97371e-01 er 0.11632
it 3   dw 1.299e-02 pp 2.79797e-01 er 0.11421
it 4   dw 2.456e-02 pp 2.64735e-01 er 0.11053
it 5   dw 4.200e-02 pp 2.47440e-01 er 0.10789
it 6   dw 1.417e-01 pp 2.04814e-01 er 0.08737
it 7   dw 1.330e-02 pp 2.03060e-01 er 0.08737
it 8   dw 3.177e-02 pp 1.92654e-01 er 0.08421
it 9   dw 5.613e-02 pp 1.38725e-01 er 0.05789
it 10  dw 1.339e-01 pp 7.92844e-02 er 0.02368
it 11  dw 1.734e-01 pp 6.71341e-02 er 0.01316
it 12  dw 1.313e-01 pp 6.55828e-02 er 0.01263
it 13  dw 2.036e-01 pp 6.38482e-02 er 0.01421
it 14  dw 1.230e-02 pp 5.96907e-02 er 0.01368
it 15  dw 9.719e-02 pp 4.03190e-02 er 0.00842
it 16  dw 4.004e-02 pp 3.98276e-02 er 0.00737
it 17  dw 1.598e-01 pp 2.68187e-02 er 0.00316
it 18  dw 1.900e-01 pp 2.57116e-02 er 0.00211
it 19  dw 4.355e-01 pp 2.14572e-02 er 0.00263
it 20  dw 1.029e-01 pp 1.91407e-02 er 0.00211
it 21  dw 1.347e-01 pp 1.46859e-02 er 0.00105
it 22  dw 2.231e-01 pp 1.26997e-02 er 0.00053
it 23  dw 2.942e-01 pp 1.20663e-02 er 0.00000
it 24  dw 3.836e-01 pp 1.14817e-02 er 0.00000
it 25  dw 4.213e-01 pp 9.89037e-03 er 0.00000
it 26  dw 1.875e-01 pp 7.06744e-03 er 0.00000
it 27  dw 2.865e-01 pp 5.61255e-03 er 0.00000
it 28  dw 5.903e-01 pp 4.94776e-03 er 0.00000
it 29  dw 0.000e+00 pp 4.94776e-03 er 0.00000
-------------------------
.......
-------------------------
.......
it 8   dw 2.024e-01 pp 8.14623e-10 er 0.00000
it 9   dw 9.264e-02 pp 7.87683e-10 er 0.00000
it 10  dw 5.845e-02 pp 7.38397e-10 er 0.00000
it 11  dw 2.418e-01 pp 6.34000e-10 er 0.00000
it 12  dw 5.081e-01 pp 6.19061e-10 er 0.00000
it 13  dw 0.000e+00 pp 6.19061e-10 er 0.00000

in [72]: print nltk.classify.accuracy(maxent_all_classifier, test_set)
0.91

in [73]: maxent_all_classifier.show_most_informative_features(10)
  11.220 to==true and label is u'neg'
   3.415 ,==true and label is u'neg'
   3.360 '==true and label is u'neg'
   3.310 this==true and label is u'neg'
   3.243 a==true and label is u'neg'
  -3.218 (u'does', u'a')==true and label is u'neg'
   3.164 have==true and label is u'neg'
  -3.024 what==true and label is u'neg'
   2.966 more==true and label is u'neg'
  -2.891 (u',', u'which')==true and label is u'neg'

   in [63]: def bag_of_all(words, n=2): ....: all_features =
   bag_of_words(words) ....: ngram_features = bag_of_ngrams(words, n=n)
   ....: all_features.update(ngram_features) ....: return all_features
   ....: in [64]: data_sets = [(bag_of_all(d), c) for (d, c) in documents]
   in [65]: train_set, test_set = data_sets[100:], data_sets[:100] in
   [66]: nb_all_classifier = nltk.naivebayesclassifier.train(train_set) in
   [67]: print nltk.classify.accuracy(nb_all_classifier, test_set) 0.83 in
   [68]: nb_all_classifier.show_most_informative_features(10) most
   informative features (u'is', u'terrific') = true pos : neg = 17.1 : 1.0
   (u'not', u'funny') = true neg : pos = 16.9 : 1.0 outstanding = true pos
   : neg = 13.9 : 1.0 (u'boring', u'and') = true neg : pos = 13.6 : 1.0
   (u'and', u'boring') = true neg : pos = 13.6 : 1.0 avoids = true pos :
   neg = 13.1 : 1.0 (u'our', u'own') = true pos : neg = 13.1 : 1.0
   (u'why', u'did') = true neg : pos = 12.9 : 1.0 (u'enjoyable', u',') =
   true pos : neg = 12.4 : 1.0 (u'works', u'well') = true pos : neg = 12.4
   : 1.0 in [71]: maxent_all_classifier =
   nltk.maxentclassifier.train(train_set, "megam") scanning file...1900
   train, 0 dev, 0 test, reading...done warning: there only appear to be
   two classes, but we're optimizing with bfgs...using binary optimization
   with cg would be much faster optimizing with lambda = 0 it 1 dw
   8.715e-02 pp 3.82841e-01 er 0.17684 it 2 dw 2.846e-02 pp 2.97371e-01 er
   0.11632 it 3 dw 1.299e-02 pp 2.79797e-01 er 0.11421 it 4 dw 2.456e-02
   pp 2.64735e-01 er 0.11053 it 5 dw 4.200e-02 pp 2.47440e-01 er 0.10789
   it 6 dw 1.417e-01 pp 2.04814e-01 er 0.08737 it 7 dw 1.330e-02 pp
   2.03060e-01 er 0.08737 it 8 dw 3.177e-02 pp 1.92654e-01 er 0.08421 it 9
   dw 5.613e-02 pp 1.38725e-01 er 0.05789 it 10 dw 1.339e-01 pp
   7.92844e-02 er 0.02368 it 11 dw 1.734e-01 pp 6.71341e-02 er 0.01316 it
   12 dw 1.313e-01 pp 6.55828e-02 er 0.01263 it 13 dw 2.036e-01 pp
   6.38482e-02 er 0.01421 it 14 dw 1.230e-02 pp 5.96907e-02 er 0.01368 it
   15 dw 9.719e-02 pp 4.03190e-02 er 0.00842 it 16 dw 4.004e-02 pp
   3.98276e-02 er 0.00737 it 17 dw 1.598e-01 pp 2.68187e-02 er 0.00316 it
   18 dw 1.900e-01 pp 2.57116e-02 er 0.00211 it 19 dw 4.355e-01 pp
   2.14572e-02 er 0.00263 it 20 dw 1.029e-01 pp 1.91407e-02 er 0.00211 it
   21 dw 1.347e-01 pp 1.46859e-02 er 0.00105 it 22 dw 2.231e-01 pp
   1.26997e-02 er 0.00053 it 23 dw 2.942e-01 pp 1.20663e-02 er 0.00000 it
   24 dw 3.836e-01 pp 1.14817e-02 er 0.00000 it 25 dw 4.213e-01 pp
   9.89037e-03 er 0.00000 it 26 dw 1.875e-01 pp 7.06744e-03 er 0.00000 it
   27 dw 2.865e-01 pp 5.61255e-03 er 0.00000 it 28 dw 5.903e-01 pp
   4.94776e-03 er 0.00000 it 29 dw 0.000e+00 pp 4.94776e-03 er 0.00000
   ------------------------- ....... ------------------------- ....... it
   8 dw 2.024e-01 pp 8.14623e-10 er 0.00000 it 9 dw 9.264e-02 pp
   7.87683e-10 er 0.00000 it 10 dw 5.845e-02 pp 7.38397e-10 er 0.00000 it
   11 dw 2.418e-01 pp 6.34000e-10 er 0.00000 it 12 dw 5.081e-01 pp
   6.19061e-10 er 0.00000 it 13 dw 0.000e+00 pp 6.19061e-10 er 0.00000 in
   [72]: print nltk.classify.accuracy(maxent_all_classifier, test_set)
   0.91 in [73]: maxent_all_classifier.show_most_informative_features(10)
   11.220 to==true and label is u'neg' 3.415 ,==true and label is u'neg'
   3.360 '==true and label is u'neg' 3.310 this==true and label is u'neg'
   3.243 a==true and label is u'neg' -3.218 (u'does', u'a')==true and
   label is u'neg' 3.164 have==true and label is u'neg' -3.024 what==true
   and label is u'neg' 2.966 more==true and label is u'neg' -2.891 (u',',
   u'which')==true and label is u'neg'

   we get the best [38]id31 performance on this case,
   although there were some problems, such as the punctuations and stop
   words were not discarded. just as a case study, we encourage you to
   test on more data or more features, or better machine learning models
   such as deep learning method.

   posted by [39]textminer

related posts:

    1. [40]dive into nltk, part viii: using external maximum id178
       modeling libraries for text classification
    2. [41]dive into nltk, part vii: a preliminary study on text
       classification
    3. [42]fasttext for fast id31
    4. [43]text analysis online no longer provides nltk stanford nlp api
       interface

   [44]deep learning specialization on coursera

   posted in [45]nlp, [46]nlp tools, [47]nltk, [48]id31,
   [49]text classification, [50]id111, [51]text processing tagged
   [52]maxent model, [53]maximum id178, [54]maximum id178 classifier,
   [55]maximum id178 libraries, [56]maximum id178 model, [57]maximum
   id178 modeling, [58]maximum id178 models, [59]megam, [60]naive
   bayes classifier, [61]naivebayesclassifier, [62]nltk maximum id178
   model, [63]id31, [64]id31 api,
   [65]sentiment analyzer, [66]text classification, [67]text classifier
   [68]permalink

post navigation

   [69]    dive into tensorflow, part iii: gtx
   1080+ubuntu16.04+cuda8.0+cudnn5.0+tensorflow
   [70]dive into tensorflow, part iv: hello mnist    
     __________________________________________________________________

comments

dive into nltk, part ix: from text classification to id31     no
comments

leave a reply [71]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   [ ] save my name, email, and website in this browser for the next time
   i comment.

   post comment

   [72][dlai-logo-final-minus-font-plus-white-backg.png]
   [show?id=9iqcvd3eeqc&bids=541296.11421701896&type=2&subid=0]

   search for: ____________________ search

   [ins: :ins]

recent posts

     * [73]deep learning practice for nlp: large movie review data
       id31 from scratch
     * [74]best coursera courses for data science
     * [75]best coursera courses for machine learning
     * [76]best coursera courses for deep learning
     * [77]dive into nlp with deep learning, part i: getting started with
       dl4nlp

recent comments

     * textminer on [78]training id97 model on english wikipedia by
       gensim
     * ankit ramani on [79]training id97 model on english wikipedia by
       gensim
     * vincent on [80]training id97 model on english wikipedia by
       gensim
     * muhammad amin nadim on [81]andrew ng deep learning specialization:
       best deep learning course for beginners and deep learners
     * saranya on [82]training id97 model on english wikipedia by
       gensim

archives

     * [83]november 2018
     * [84]august 2018
     * [85]july 2018
     * [86]june 2018
     * [87]january 2018
     * [88]october 2017
     * [89]september 2017
     * [90]august 2017
     * [91]july 2017
     * [92]may 2017
     * [93]april 2017
     * [94]march 2017
     * [95]december 2016
     * [96]october 2016
     * [97]august 2016
     * [98]july 2016
     * [99]june 2016
     * [100]may 2016
     * [101]april 2016
     * [102]february 2016
     * [103]december 2015
     * [104]november 2015
     * [105]september 2015
     * [106]may 2015
     * [107]april 2015
     * [108]march 2015
     * [109]february 2015
     * [110]january 2015
     * [111]december 2014
     * [112]november 2014
     * [113]october 2014
     * [114]september 2014
     * [115]july 2014
     * [116]june 2014
     * [117]may 2014
     * [118]april 2014
     * [119]january 2014

categories

     * [120]ainlp
     * [121]coursera course
     * [122]data science
     * [123]deep learning
     * [124]dl4nlp
     * [125]how to use mashape api
     * [126]keras
     * [127]machine learning
     * [128]id39
     * [129]nlp
     * [130]nlp tools
     * [131]nltk
     * [132]id31
     * [133]tensorflow
     * [134]text analysis
     * [135]text classification
     * [136]id111
     * [137]text processing
     * [138]text similarity
     * [139]text summarization
     * [140]textanalysis api
     * [141]uncategorized
     * [142]id27
     * [143]id40

meta

     * [144]log in
     * [145]entries rss
     * [146]comments rss
     * [147]wordpress.org

     [148]text analysis online

     [149]text summarizer

     [150]text processing

     [151]word similarity

     [152]best coursera course

     [153]best coursera courses

     [154]elastic patent

     2019 - [155]id111 online - [156]weaver xtreme theme

   [157]   

references

   visible links
   1. https://textminingonline.com/feed
   2. https://textminingonline.com/comments/feed
   3. https://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis/feed
   4. https://textminingonline.com/dive-into-tensorflow-part-iii-gtx-1080-ubuntu16-04-cuda8-0-cudnn5-0-tensorflow
   5. https://textminingonline.com/dive-into-tensorflow-part-iv-hello-mnist
   6. https://textminingonline.com/wp-json/oembed/1.0/embed?url=https://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis
   7. https://textminingonline.com/wp-json/oembed/1.0/embed?url=https://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis&format=xml
   8. https://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis#page-bottom
   9. https://textminingonline.com/
  10. https://textminingonline.com/
  11. http://textanalysisonline.com/#new_tab
  12. http://keywordextraction.net/#new_tab
  13. http://textsummarization.net/#new_tab
  14. https://wordsimilarity.com/#new_tab
  15. https://textminingonline.com/about
  16. https://textminingonline.com/
  17. https://textminingonline.com/category/nlp
  18. https://textminingonline.com/dive-into-tensorflow-part-iii-gtx-1080-ubuntu16-04-cuda8-0-cudnn5-0-tensorflow
  19. https://textminingonline.com/dive-into-tensorflow-part-iv-hello-mnist
  20. https://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis
  21. https://textminingonline.com/author/yuzhen
  22. https://click.linksynergy.com/fs-bin/click?id=9iqcvd3eeqc&offerid=467035.416&subid=0&type=4
  23. http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk
  24. http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk
  25. http://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize
  26. http://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger
  27. http://textminingonline.com/dive-into-nltk-part-iv-id30-and-lemmatization
  28. http://textminingonline.com/dive-into-nltk-part-v-using-stanford-text-analysis-tools-in-python
  29. http://textminingonline.com/dive-into-nltk-part-vi-add-stanford-word-segmenter-interface-for-python-nltk
  30. http://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification
  31. http://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
  32. http://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis
  33. http://textminingonline.com/?p=872
  34. https://en.wikipedia.org/wiki/sentiment_analysis
  35. http://sentimentanalysis.net/
  36. http://sentimentanalysis.net/
  37. http://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
  38. http://sentimentanalysis.net/
  39. http://textminingonline.com/
  40. https://textminingonline.com/dive-into-nltk-part-viii-using-external-maximum-id178-modeling-libraries-for-text-classification
  41. https://textminingonline.com/dive-into-nltk-part-vii-a-preliminary-study-on-text-classification
  42. https://textminingonline.com/fasttext-for-fast-sentiment-analysis
  43. https://textminingonline.com/text-analysis-online-no-longer-provides-nltk-stanford-nlp-api-interface
  44. https://click.linksynergy.com/fs-bin/click?id=9iqcvd3eeqc&offerid=467035.414&subid=0&type=4
  45. https://textminingonline.com/category/nlp
  46. https://textminingonline.com/category/nlp-tools
  47. https://textminingonline.com/category/nltk
  48. https://textminingonline.com/category/sentiment-analysis
  49. https://textminingonline.com/category/text-classification
  50. https://textminingonline.com/category/text-mining
  51. https://textminingonline.com/category/text-processing
  52. https://textminingonline.com/tag/maxent-model
  53. https://textminingonline.com/tag/maximum-id178
  54. https://textminingonline.com/tag/maximum-id178-classifier
  55. https://textminingonline.com/tag/maximum-id178-libraries
  56. https://textminingonline.com/tag/maximum-id178-model
  57. https://textminingonline.com/tag/maximum-id178-modeling
  58. https://textminingonline.com/tag/maximum-id178-models
  59. https://textminingonline.com/tag/megam
  60. https://textminingonline.com/tag/naive-bayes-classifier
  61. https://textminingonline.com/tag/naivebayesclassifier
  62. https://textminingonline.com/tag/nltk-maximum-id178-model
  63. https://textminingonline.com/tag/sentiment-analysis
  64. https://textminingonline.com/tag/sentiment-analysis-api
  65. https://textminingonline.com/tag/sentiment-analyzer
  66. https://textminingonline.com/tag/text-classification-2
  67. https://textminingonline.com/tag/text-classifier
  68. https://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis
  69. https://textminingonline.com/dive-into-tensorflow-part-iii-gtx-1080-ubuntu16-04-cuda8-0-cudnn5-0-tensorflow
  70. https://textminingonline.com/dive-into-tensorflow-part-iv-hello-mnist
  71. https://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis#respond
  72. https://click.linksynergy.com/link?id=9iqcvd3eeqc&offerid=541296.11421701896&type=2&murl=https://www.coursera.org/specializations/deep-learning
  73. https://textminingonline.com/deep-learning-practice-for-nlp-large-movie-review-data-sentiment-analysis-from-scratch
  74. https://textminingonline.com/best-coursera-courses-for-data-science
  75. https://textminingonline.com/best-coursera-courses-for-machine-learning
  76. https://textminingonline.com/best-coursera-courses-for-deep-learning
  77. https://textminingonline.com/dive-into-nlp-with-deep-learning-part-i-getting-started-with-dl4nlp
  78. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-138841
  79. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-138807
  80. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-138723
  81. https://textminingonline.com/andrew-ng-deep-learning-specialization-best-deep-learning-course-for-beginners-and-deep-learners#comment-138475
  82. https://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim#comment-137923
  83. https://textminingonline.com/2018/11
  84. https://textminingonline.com/2018/08
  85. https://textminingonline.com/2018/07
  86. https://textminingonline.com/2018/06
  87. https://textminingonline.com/2018/01
  88. https://textminingonline.com/2017/10
  89. https://textminingonline.com/2017/09
  90. https://textminingonline.com/2017/08
  91. https://textminingonline.com/2017/07
  92. https://textminingonline.com/2017/05
  93. https://textminingonline.com/2017/04
  94. https://textminingonline.com/2017/03
  95. https://textminingonline.com/2016/12
  96. https://textminingonline.com/2016/10
  97. https://textminingonline.com/2016/08
  98. https://textminingonline.com/2016/07
  99. https://textminingonline.com/2016/06
 100. https://textminingonline.com/2016/05
 101. https://textminingonline.com/2016/04
 102. https://textminingonline.com/2016/02
 103. https://textminingonline.com/2015/12
 104. https://textminingonline.com/2015/11
 105. https://textminingonline.com/2015/09
 106. https://textminingonline.com/2015/05
 107. https://textminingonline.com/2015/04
 108. https://textminingonline.com/2015/03
 109. https://textminingonline.com/2015/02
 110. https://textminingonline.com/2015/01
 111. https://textminingonline.com/2014/12
 112. https://textminingonline.com/2014/11
 113. https://textminingonline.com/2014/10
 114. https://textminingonline.com/2014/09
 115. https://textminingonline.com/2014/07
 116. https://textminingonline.com/2014/06
 117. https://textminingonline.com/2014/05
 118. https://textminingonline.com/2014/04
 119. https://textminingonline.com/2014/01
 120. https://textminingonline.com/category/ainlp
 121. https://textminingonline.com/category/coursera-course
 122. https://textminingonline.com/category/data-science
 123. https://textminingonline.com/category/deep-learning
 124. https://textminingonline.com/category/dl4nlp
 125. https://textminingonline.com/category/how-to-use-mashape-api
 126. https://textminingonline.com/category/keras
 127. https://textminingonline.com/category/machine-learning
 128. https://textminingonline.com/category/named-entity-recognition
 129. https://textminingonline.com/category/nlp
 130. https://textminingonline.com/category/nlp-tools
 131. https://textminingonline.com/category/nltk
 132. https://textminingonline.com/category/sentiment-analysis
 133. https://textminingonline.com/category/tensorflow
 134. https://textminingonline.com/category/text-analysis
 135. https://textminingonline.com/category/text-classification
 136. https://textminingonline.com/category/text-mining
 137. https://textminingonline.com/category/text-processing
 138. https://textminingonline.com/category/text-similarity
 139. https://textminingonline.com/category/text-summarization
 140. https://textminingonline.com/category/textanalysis-api-2
 141. https://textminingonline.com/category/uncategorized
 142. https://textminingonline.com/category/word-embedding
 143. https://textminingonline.com/category/word-segmentation
 144. https://textminingonline.com/wp-login.php
 145. https://textminingonline.com/feed
 146. https://textminingonline.com/comments/feed
 147. https://wordpress.org/
 148. http://textanalysisonline.com/
 149. http://textsummarization.net/
 150. http://textprocessing.org/
 151. http://wordsimilarity.com/
 152. https://bestcourseracourse.com/
 153. https://bestcourseracourses.com/
 154. https://elasticpatent.com/
 155. https://textminingonline.com/
 156. https://weavertheme.com/
 157. https://textminingonline.com/dive-into-nltk-part-ix-from-text-classification-to-sentiment-analysis#page-top

   hidden links:
 159. https://wordpress.org/
