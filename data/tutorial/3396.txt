   [ ]
     * download
     * documentation
     * resources
     * community
     * jobs
     * commercial support
     * faq
     * fork on github

   an open source and collaborative framework for extracting the data you
   need from websites.

   in a fast, simple, yet extensible way.
   [1]pypi version [2]wheel status [3]coverage report
   install the latest version of scrapy

   scrapy 1.6
   pip install scrapy
   [4](button) pypi [5](button) conda [6](button) release notes

   terminal   
 pip install scrapy
 cat > myspider.py <<eof

import scrapy

class blogspider(scrapy.spider):
    name = 'blogspider'
    start_urls = ['https://blog.scrapinghub.com']

    def parse(self, response):
        for title in response.css('.post-header>h2'):
            yield {'title': title.css('a ::text').get()}

        for next_page in response.css('a.next-posts-link'):
            yield response.follow(next_page, self.parse)

   eof scrapy runspider myspider.py

   build and run your
   web spiders

   terminal   
 pip install shub
 shub login
insert your scrapinghub api key: <api_key>

# deploy the spider to scrapy cloud
 shub deploy

# schedule the spider for execution
 shub schedule blogspider
spider blogspider scheduled, watch it running here:
https://app.scrapinghub.com/p/26731/job/1/8

# retrieve the scraped data
 shub items 26731/1/8

{"title": "improved frontera: web crawling at scale with python 3 support"}
{"title": "how to crawl the web politely with scrapy"}
...

   deploy them to
   [7]scrapy cloud

   or use [8]scrapyd to host the spiders on your own server

fast and powerful

   write the rules to extract the data and let scrapy do the rest

easily extensible

   extensible by design, plug new functionality easily without having to
   touch the core

portable, python

   written in python and runs on linux, windows, mac and bsd

healthy community

     * - 31k stars, 7.5k forks and 1.8k watchers on [9]github
     * - 4.5k followers on [10]twitter
     * - 11k questions on [11]stackoverflow

want to know more?

     * [12]- discover scrapy at a glance
     * [13]- meet the companies using scrapy

   iframe:
   [14]https://ghbtns.com/github-btn.html?user=scrapy&repo=scrapy&type=wat
   ch&count=true

   iframe:
   [15]https://ghbtns.com/github-btn.html?user=scrapy&repo=scrapy&type=for
   k&count=true

   [16]@scrapyproject

   maintained by [17]scrapinghub and [18]many other contributors

references

   visible links
   1. https://pypi.org/project/scrapy
   2. https://pypi.org/project/scrapy
   3. https://codecov.io/github/scrapy/scrapy?branch=master
   4. https://pypi.org/project/scrapy/
   5. https://anaconda.org/conda-forge/scrapy
   6. https://docs.scrapy.org/en/latest/news.html
   7. https://scrapinghub.com/scrapy-cloud/
   8. https://github.com/scrapy/scrapyd
   9. https://github.com/scrapy/scrapy
  10. https://twitter.com/scrapyproject
  11. http://stackoverflow.com/tags/scrapy/info
  12. http://docs.scrapy.org/en/1.6/intro/overview.html
  13. https://scrapy.org/companies/
  14. https://ghbtns.com/github-btn.html?user=scrapy&repo=scrapy&type=watch&count=true
  15. https://ghbtns.com/github-btn.html?user=scrapy&repo=scrapy&type=fork&count=true
  16. https://twitter.com/scrapyproject
  17. https://scrapinghub.com/
  18. https://github.com/scrapy/scrapy/graphs/contributors

   hidden links:
  20. https://scrapy.org/
  21. https://scrapy.org/download/
  22. https://scrapy.org/doc/
  23. https://scrapy.org/resources/
  24. https://scrapy.org/community/
  25. https://scrapy.org/jobs/
  26. https://scrapy.org/companies/
  27. http://docs.scrapy.org/en/latest/faq.html
  28. https://github.com/scrapy/scrapy
