   #[1]quora

   [2]quora
   ____________________

   sign in
   [3]what is an intuitive explanation of lstms and grus?
   [4]edwin chen
   [5]edwin chen, pilot
   [6]updated 94w ago    upvoted by
   [7]luis argerich, data science professor at uba since 1997. and
   [8]abhinav sharma, product designer at quora, former ml engineer at
   facebook

   imagine you have a sequence of images from a movie, and you want to
   label each image with an activity (is this a fight?, are the characters
   talking?, are the characters eating?).

   how do you do this?
     __________________________________________________________________

   one way is to ignore the sequential nature of the images, and build a
   per-image classifier that considers each image in isolation. for
   example, given enough images and labels:
     * your algorithm might first learn to detect low-level patterns like
       shapes and edges.
     * with more data, it might learn to combine these patterns into more
       complex ones, like faces (two circular things atop a triangular
       thing atop an oval thing) or cats.
     * and with even more data, it might learn to map these higher-level
       patterns into activities themselves (scenes with mouths, steaks,
       and forks are probably about eating).

   this, then, is a deep neural network: it takes an image input, returns
   an activity output, and     just as you might learn to detect patterns in
   puppy behavior without knowing anything about dogs (after seeing enough
   corgis, you discover common characteristics like fluffy butts and
   drumstick legs; next, you learn advanced features like splooting)     in
   between it learns to represent images through hidden layers of
   representations.
     __________________________________________________________________

   ignoring the sequential aspect of the movie images is pretty ml 101,
   though. if you see a scene of a beach, you should boost beach
   activities in future frames: an image of someone in the water should
   probably be labeled swimming, not bathing, and an image of someone
   lying with their eyes closed is probably suntanning. if you remember
   that bob just arrived at a supermarket, then even without any
   distinctive supermarket features, an image of bob holding a slab of
   bacon should probably be categorized as shopping instead of cooking.

   so what you   d like is to let your model track the state of the world:
    1. after seeing each image, the model outputs a label and also updates
       the knowledge it   s been learning. for example, the model might
       learn to automatically discover and track information like location
       (are scenes currently in a house or beach?), time of day (if a
       scene contains an image of the moon, the model should remember that
       it's nighttime), and within-movie progress (is this image the first
       frame or the 100th?). importantly, just as a neural network
       automatically discovers hidden patterns like edges, shapes, and
       faces without being fed them, your model should automatically
       discover useful information by itself.
    2. when given a new image, the model should incorporate the knowledge
       it's gathered to do a better job.

   this, then, is a recurrent neural network. instead of simply taking an
   image and returning an activity, an id56 also maintains internal
   memories about the world (weights assigned to different pieces of
   information) to help perform its classifications.
     __________________________________________________________________

   let's think about how your model updates its knowledge of the world. so
   far, you've placed no constraints on this update, so its knowledge can
   change pretty chaotically: at one frame it thinks the characters are in
   the us, at the next frame it sees the characters eating sushi and
   thinks they're in japan, and at the next frame it sees polar bears and
   thinks they're on hydra island. or perhaps it has a wealth of
   information to suggest that alice is an investment analyst, but decides
   she's a professional assassin after seeing her cook.

   this chaos means information quickly transforms and vanishes, and it's
   difficult for the model to keep a long-term memory. so what you   d like
   is for the network to learn how to update its beliefs (scenes without
   bob shouldn't change bob-related information, scenes with alice should
   focus on gathering details about her), in a way that its knowledge of
   the world evolves more gently.

   this is how you do it.
    1. adding a forgetting mechanism. if a scene ends, for example, the
       model should forget the current scene location, the time of day,
       and reset any scene-specific information; however, if a character
       dies in the scene, it should continue remembering that he's no
       longer alive. thus, you want the model to learn a separate
       forgetting/remembering mechanism: when new inputs come in, it needs
       to know which beliefs to keep or throw away.
    2. adding a saving mechanism. when the model sees a new image, it
       needs to learn whether any information about the image is worth
       using and saving. maybe your mom sent you an article about the
       kardashians, but who cares?
    3. so when new a input comes in, the model first forgets any long-term
       information it decides it no longer needs. then it learns which
       parts of the new input are worth using, and saves them into its
       long-term memory.
    4. focusing long-term memory into working memory. finally, the model
       needs to learn which parts of its long-term memory are immediately
       useful. for example, bob's age may be a useful piece of information
       to keep in the long term (children are more likely to be crawling,
       adults are more likely to be working), but is probably irrelevant
       if he's not in the current scene. so instead of using the full
       long-term memory all the time, it learns which parts to focus on
       instead.

   and now you have a long short-term memory network. whereas an id56 can
   overwrite its memory at each time step in a fairly uncontrolled
   fashion, an lstm transforms its memory in a very precise way: by using
   specific learning mechanisms that help it keep track of information
   over longer periods of time.
     __________________________________________________________________

   let   s now describe lstms more mathematically. (skip this section if you
   want to keep everything light.)

   at time [math]t[/math], we receive a new input [math]x_t[/math]. we
   also have our long-term and working memories passed on from the
   previous time step, [math]ltm_{t   1}[/math] and [math]wm_{t   1}[/math]
   (both n-length vectors, which we can think of as weights assigned to n
   pieces of information), which we want to update.

   we'll start with our long-term memory. first, we need to know which
   pieces of long-term memory to continue remembering and which to
   discard, so we want to use the new input and our working memory to
   learn a remember gate of n numbers between 0 and 1, each of which
   determines how much of a long-term memory element to keep. (a 1 means
   to keep it, a 0 means to forget it entirely.)

   we   ll use small neural networks to build up all the different learning
   mechanisms that form our overall lstm, so we   ll start with this
   remember gate:

   [math]remember_t = rememberneuralnetwork(x_t, wm_{t   1})[/math]

   (this neural network uses the current input and our working memory to
   produce n numbers between 0 and 1, determining how much of our
   long-term memory to keep or discard.)

   next, we need to compute the information we can learn from
   [math]x_t[/math], i.e., a candidate addition to our long-term memory:

   [math]ltm   _t = candidatememoryneuralnetwork(x_t, wm_{t-1})[/math]

   (to figure out what we can learn from a new input, the neural network
   uses the input and its working memory.)

   before we add the candidate into our memory, though, we want to learn
   which parts of it are actually worth using and saving:

   [math]save_t = saveneuralnetwork(x_t, wm_{t   1})[/math]

   (think of what happens when you read something on the web. while a news
   article might contain information about hillary, you should ignore it
   if the source is breitbart.)

   let's now combine all these steps. after forgetting memories we don't
   think we'll ever need again and saving useful pieces of incoming
   information, we have our updated long-term memory:

   [math]ltm_t = remember_t     ltm_{t   1} + save_t     ltm'_t[/math]

   where [math]   [/math] denotes element-wise multiplication.

   next, let's update our working memory. we want to learn how to focus
   our long-term memory into information that will be immediately useful.
   so we learn a focus/attention vector:

   [math]focus_t = focusneuralnetwork(x_t, wm_{t   1})[/math]

   (to figure out what to focus on, we have a neural subnetwork that uses
   the input and our working memory.)

   our new working memory is then

   [math]wm_t = focus_t       (ltm_t)[/math]

   where [math]  [/math] is an activation function that squashes our
   long-term memories (which can be any real numbers from negative to
   positive infinity) into a smaller, more focused range (usually between
   -1 and +1). thus, we pay full attention to elements where the focus is
   1, and ignore elements where the focus is 0.

   and we're done!

   if you   d like to understand the math better, i wrote a longer post
   here: [9]exploring lstms
     __________________________________________________________________

   in pokemon form, the evolution from neural network to id56 to lstm might
   look like this.

   neural network:
   [main-qimg-dc73185e3bde51fe372a930922a905a4]

   recurrent neural network:
   [main-qimg-6118719b2860a4ae8b839c1c8201081e]

   lstm:
   [main-qimg-1bd7a1e0260df62f5b6093929a296aa9]
     __________________________________________________________________

   let   s turn to a real example to get more intuition.

   first, i built a trump-mimicking lstm, by feeding it sequences of
   characters in a tweet and training it to predict the next character
   (like a trump auto-completer, if you will).

   here are some of the proclamations it generates:
   [main-qimg-0eb8bbacee64b795f3f6bcadf78131ea]

   how is it able to ramble so well? in order to pass itself off as trump
   tweeting, the lstm should know how to do things like create hashtags
   and make populist appeals     and if we look at the internal neurons of
   the lstm, indeed it does.

   here, for example, is a neuron that tracks its position within
   hashtags, urls, and @mentions. (as the lstm moves through the sequence,
   its neurons fire at varying intensities. the picture represents one
   particular neuron, where characters are color-coded according to the
   intensity; dark blue shades indicate large positive numbers, and dark
   red shades indicate very negative numbers.)
   [main-qimg-d1997adb1d406202e0cc594def7b7413]

   here is a quote attributor:
   [main-qimg-b3f1995b79c572fcf3142bf42d096af9]

   and here is that maga neuron:
   [main-qimg-9e79a08881a68bbe2b5a769041285bbe]
     __________________________________________________________________

   here   s another example. this time i trained a software-writing lstm, by
   feeding it the apache commons lang codebase. here   s a program it codes
   up after a couple hours:
/*
 * licensed to the apache software foundation (asf) under one or more
 * contributor license agreements. see the notice file distributed with
 * this work for additional information regarding copyright ownership.
 * the asf licenses this file to you under the apache license, version 2.0
 * (the "license"); you may not use this file except in compliance with
 * the license. you may obtain a copy of the license at
 *
 * http://www.apache.org/licenses/license-2.0
 *
 * unless required by applicable law or agreed to in writing, software
 * distributed under the license is distributed on an "as is" basis,
 * without warranties or conditions of any kind, either express or implied.
 * see the license for the specific language governing permissions and
 * limitations under the license.
 */

package org.apache.commons.math4.linear;

import java.text.numberformat;
import java.io.bytearrayinputstream;
import java.io.objectoutputstream;
import java.io.objectoutputstream;
import java.util.arraylist;
import java.util.list;

import org.apache.commons.math4.optim.nonlinear.scalar.goaltype;
import org.apache.commons.math4.ml.neuralnet.sofm.neuronsquaremesh2d;
import org.apache.commons.math4.distribution.descriptivestatistics;
import org.apache.commons.math4.optim.nonlinear.scalar.nodefieldintegrator;
import org.apache.commons.math4.optim.nonlinear.scalar.gradientfunction;
import org.apache.commons.math4.optim.pointvaluepair;
import org.apache.commons.numbers.core.precision;

/**
 * <p>natural infinite is defined in basic eigenvalues of a transform are in a s
ubconsider for the optimization ties.</p>
 *
 * <p>this implementation is the computation at a collection of a set of the sol
vers.</p>
 * <p>
 * this class is returned the default precision parameters after a new value for
 the interpolation interpolators for barycenter.
 * <p>
 * the distribution values do not ratio example function containing this interfa
ce, which should be used in uniform real distributions.</p>
 * <p>
 * this class generates a new standard deviation of the following conventions, t
he variance was reached as
 * constructor, and invoke the interpolation arrays</li>
 * <li>{@code a < 1} and {@code this} the regressions returned by calling
 * the same special corresponding to a representation.
 * </p>
 *
 * @since 1.2
 */
public class sinoutionintegrator implements serializable {

  /** serializable version identifier */
  private static final long serialversionuid = -7989543519820244888l;

  /**
  * start distance between the instance and a result (does not all lead to the n
umber of seconds).
  * <p>
  * note that this implementation this can prevent the permutation of the prenev
ed statistics.
  * </p>
  * <p>
  * <strong>preconditions</strong>: <ul>
  * <li>returns number of samples and the designated subarray, or
  * if it is null, {@code null}. it does not dofine the base number.</p>
  *
  * @param source the number of left size of the specified value
  * @param numberofpoints number of points to be checked
  * @return the parameters for a public function.
  */
  public static double fitness(final double[] sample) {
    double additionalcomputed = double.positive_infinity;
    for (int i = 1; i < dim; i++) {
      final double coefficients[i] = point[i] * coefficients[i];
      double diff = a * fastmath.cos(point[i]);
      final double sum = fastmath.max(random.nextdouble(), alpha);
      final double sum = fastmath.sin(optimal[i].getreal() - cholenghat);
      final double lower = gamma * chessian;
      final double fs = factor * maxiterationcount;
      if (temp > numberofpoints - 1) {
        final int pma = points.size();
        boolean partial = points.tostring();
        final double segments = new double[2];
        final double sign = pti * x2;
        double n = 0;
        for (int i = 0; i < n; i++) {
          final double ds = normalizedstate(i, k, difference * factor);
          final double inv = alpha + temp;
          final double rsigx = fastmath.sqrt(max);
          return new string(degree, e);
        }
      }
      // perform the number to the function parameters from one count of the val
ues
      final pointvaluepair part = new pointvaluepair[n];
      for (int i = 0; i < n; i++) {
        if (i == 1) {
          numberofpoints = 1;
        }
        final double dev = fastmath.log(perturb(g, norm), values[i]);
        if (double.isnan(y) &&
                             nan) {
            sum /= samples.length;
        }
        double i = 1;
        for (int i = 0; i < n; i++) {
          statistics[i] = fastmath.abs(point[i].sign() + rhs[i]);
        }
        return new pointvaluepair(true, params);
      }
    }
  }

  /**
   * test creates a new polynomialfunction function
   * @see #applyto(double)
   */
  @test
  public void testcosise() {
    final double p = 7.7;
    final double expected = 0.0;
    final searchinterval d = new power(1.0, 0.0);
    final double penalty = 1e-03;
    final double init = 0.245;
    final double t = 0.2;
    final double result = (x + 1.0) / 2.0;
    final double numeratoradd = 13;
    final double bhigh = 2 * (k - 1) * math.acos();

    assert.assertequals(0.0, true);
    assert.asserttrue(percentile.evaluate(singletonarray), 0);
    assert.assertequals( 0.0, getnumberoftrials(0, 0), 1e-10);
    assert.assertequals(0.201949230731, percentile.evaluate(specialvalues), 1.0e
-3);
    assert.assertequals(-10.0, distribution.inversecumulativeid203(0.50),
0);
    assert.assertequals(0.0, solver.solve(100, f, 1.0, 0.5), 1.0e-10);
  }

   while the code certainly isn't perfect, it's better than a lot of data
   scientists i know. and we can see that the lstm has learned a lot of
   interesting (and correct!) coding behavior:
     * it knows how to structure classes: a license up top, followed by
       packages and imports, followed by comments and a class definition,
       followed by variables and methods. similarly, it knows how to
       create methods: comments follow the correct orders (description,
       then @param, then @return, etc.), decorators are properly placed,
       and non-void methods end with appropriate return statements.
       crucially, this behavior spans long ranges of code     see how giant
       the blocks are!
     * it can also track subroutines and nesting levels: indentation is
       always correct, and if statements and for loops are always closed
       out.
     * it even knows how to create tests.
     __________________________________________________________________

   but how exactly is an lstm able to do these things? we talked about
   their different saving, forgetting, and focusing attention mechanisms    
   what role do these different memory modules play?

   to investigate, let   s look at some simpler examples.

   long-term and working memories

   we described the lstm as having two types of memories: a long-term
   memory (traditionally called the cell state), and a working memory
   (traditionally called the hidden state) that pulls out and focuses the
   long-term memories when they   re needed.

   so when a memory is currently irrelevant, we expect the hidden state to
   turn off     and that's exactly what happens for this sequence copying
   neuron.
   [main-qimg-06662049a9a9eb5f1c69fa7dcefa1327]

   above, i trained an lstm to copy sequences (given a sequence of 3
   characters, followed by a delimiter x, the lstm should learn to
   reproduce the initial sequence; think of how the java lstm was able to
   copy apache licenses, and the trump lstm was able to copy #maga
   hashtags). the picture shows a neuron whose working memory (hidden
   state) is turned off when reading the initial sequence in each row,
   while everything is being fed solely to the long-term memory (cell
   state).

   if you   d like to play with the neuron yourself, i built a web app here:
   [10]lstm explorer

   tracking old information

   we also described a specific remembering mechanism inside the lstm,
   whose job is to keep or discard information from the long-term memory.
   we expect it to fully activate when it needs to remember something
   exactly, and to turn off when information is never going to be needed
   again.

   so i trained an lstm that would need to remember whether a sequence
   started with an    a    or a    b   , and that's exactly what we see with this
   "a" memorizing neuron: the remembering mechanism (traditionally called
   a forget gate) is firing hard to remember that it's in an "a" state
   while it passes through the x's, and turns off once it's ready to
   generate the final character.
   [main-qimg-c143e6b22ecc56cff656cc036dba7c21]

   if you   d like to play with the neuron, go here: [11]lstm explorer

   saving new information

   we also described a specific saving mechanism (traditionally called the
   input gate) that decides whether or not information from a new input is
   useful enough to keep. thus, it should turn off at worthless
   information.

   so i trained an lstm that needs to count a   s and ignore random x   s, in
   order to generate a proper number of b   s, and that's exactly what this
   selective counting neuron does:
   [main-qimg-732722bcb0bab0de4e4f9f10fb98a247]

   go here to play around: [12]lstm explorer
     __________________________________________________________________

   let's recap by talking about how you could have discovered lstms by
   yourself.

   first, many of the problems we'd like to solve are sequential or
   temporal of some sort, so we should incorporate past learnings into our
   models. but we already know that hidden layers of neural networks
   encode useful information, so why not use these hidden layers as the
   memories we pass from one time step to the next? and so we get id56s.

   but we know from our own behavior that we don't keep track of knowledge
   willy-nilly; when we read a new article about politics, we don't
   immediately believe whatever it tells us and incorporate it into our
   beliefs of the world. we selectively decide what information to save,
   what information to discard, and what pieces of information to use to
   make decisions the next time we read the news. thus, we want to learn
   how to gather, update, and apply information     and why not learn these
   things through their own mini neural networks? and so we get lstms.

   and now that we've gone through this process, we can come up with our
   own modifications.
     * for example, maybe you think it's silly for lstms to distinguish
       between long-term and working memories     why not have one? or maybe
       you find separate remember gates and save gates kind of redundant    
       anything we forget should be replaced by new information, and
       vice-versa. and now you've come up with one popular lstm variant,
       the [13]gru.
     * or maybe you think that when deciding what information to remember,
       save, and focus on, we shouldn't rely on our working memory alone    
       why not use our long-term memory as well? and now you've discovered
       [14]peephole lstms.
     __________________________________________________________________

   finally, i   ll end with a picture:
   [main-qimg-2d7a5bbeade9b225ea841fcd2bfcace3]

   if you you want to learn more about lstms, i also wrote a longer post:
   [15]exploring lstms

   and if you want to visually explore lstms, you can play around here:
   [16]lstm explorer
   18.5k views    [17]view 241 upvoters
   view 5 other answers to this question

about the author

   [18]edwin chen

[19]edwin chen

   pilot
   1.3m answer views6.8k this month
   [20]published writerforbes

   [21]about    [22]careers    [23]privacy    [24]terms    [25]contact

references

   visible links
   1. https://www.quora.com/opensearch/description.xml
   2. https://www.quora.com/
   3. https://www.quora.com/what-is-an-intuitive-explanation-of-lstms-and-grus
   4. https://www.quora.com/profile/edwin-chen-1
   5. https://www.quora.com/profile/edwin-chen-1
   6. https://www.quora.com/what-is-an-intuitive-explanation-of-lstms-and-grus/answer/edwin-chen-1
   7. https://www.quora.com/profile/luis-argerich
   8. https://www.quora.com/profile/abhinav-sharma
   9. http://blog.echen.me/2017/05/30/exploring-lstms/
  10. http://blog.echen.me/lstm-explorer/#/neuron?file=copy_machine&layer=1&n=5
  11. http://blog.echen.me/lstm-explorer/#/neuron?file=state_memorizer&layer=1&n=8
  12. http://blog.echen.me/lstm-explorer/#/neuron?file=selective_counter&layer=1&n=20
  13. https://arxiv.org/abs/1412.3555
  14. http://machinelearning.wustl.edu/mlpapers/paper_files/gersss02.pdf
  15. http://blog.echen.me/2017/05/30/exploring-lstms/
  16. http://blog.echen.me/lstm-explorer/
  17. https://www.quora.com/what-is-an-intuitive-explanation-of-lstms-and-grus/answer/edwin-chen-1
  18. https://www.quora.com/profile/edwin-chen-1
  19. https://www.quora.com/profile/edwin-chen-1
  20. https://www.quora.com/profile/edwin-chen-1/answers/published
  21. https://www.quora.com/about
  22. https://www.quora.com/careers
  23. https://www.quora.com/about/privacy
  24. https://www.quora.com/about/tos
  25. https://www.quora.com/contact

   hidden links:
  27. https://www.quora.com/what-is-an-intuitive-explanation-of-lstms-and-grus
