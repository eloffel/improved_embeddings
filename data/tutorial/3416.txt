   #[1]quora

   [2]quora
   ____________________

   sign in
   [3]what is a good explanation of id44?
   [4]edwin chen
   [5]edwin chen, pilot
   [6]updated 396w ago    upvoted by
   [7]oscar annen, mphil machine learning, university of cambridge (2017)
   and
   [8]alan davis, m.s. computer science & machine learning, the university
   of texas at dallas (2010)
   suppose you have the following set of sentences:
     * i ate a banana and spinach smoothie for breakfast
     * i like to eat broccoli and bananas.
     * chinchillas and kittens are cute.
     * my sister adopted a kitten yesterday.
     * look at this cute hamster munching on a piece of broccoli.

   id44 is a way of automatically discovering
   topics that these sentences contain. for example, given these sentences
   and asked for 2 topics, lda might produce something like
     * sentences 1 and 2: 100% topic a
     * sentences 3 and 4: 100% topic b
     * sentence 5: 60% topic a, 40% topic b
     * topic a: 30% broccoli, 15% bananas, 10% breakfast, 10% munching,
       ... (at which point, you could interpret topic a to be about food)
     * topic b: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, ...
       (at which point, you could interpret topic b to be about cute
       animals)

   the question, of course, is: how does lda perform this discovery?
   lda model
   in more detail, lda represents documents as mixtures of topics that
   spit out words with certain probabilities. it assumes that documents
   are produced in the following fashion: when writing each document, you
     * decide on the number of words n the document will have (say,
       according to a poisson distribution).

     * choose a topic mixture for the document (according to a dirichlet
       distribution over a fixed set of k topics). for example, assuming
       that we have the two food and cute animal topics above, you might
       choose the document to consist of 1/3 food and 2/3 cute animals.
     * generate each word in the document by:
     * ....first picking a topic (according to the multinomial
       distribution that you sampled above; for example, you might pick
       the food topic with 1/3 id203 and the cute animals topic with
       2/3 id203).
     * ....then using the topic to generate the word itself (according to
       the topic's multinomial distribution). for instance, the food topic
       might output the word "broccoli" with 30% id203, "bananas"
       with 15% id203, and so on.

   assuming this generative model for a collection of documents, lda then
   tries to backtrack from the documents to find a set of topics that are
   likely to have generated the collection.
   example
   let's make an example. according to the above process, when generating
   some particular document d, you might
     * decide that d will be 1/2 about food and 1/2 about cute animals.
     * pick 5 to be the number of words in d.
     * pick the first word to come from the food topic, which then gives
       you the word "broccoli".
     * pick the second word to come from the cute animals topic, which
       gives you "panda".
     * pick the third word to come from the cute animals topic, giving you
       "adorable".
     * pick the fourth word to come from the food topic, giving you
       "cherries".
     * pick the fifth word to come from the food topic, giving you
       "eating".

   so the document generated under the lda model will be "broccoli panda
   adorable cherries eating" (note that lda is a bag-of-words model).
   learning
   so now suppose you have a set of documents. you've chosen some fixed
   number of k topics to discover, and want to use lda to learn the topic
   representation of each document and the words associated to each topic.
   how do you do this? one way (known as collapsed id150*) is the
   following:
     * go through each document, and randomly assign each word in the
       document to one of the k topics.
     * notice that this random assignment already gives you both topic
       representations of all the documents and word distributions of all
       the topics (albeit not very good ones).
     * so to improve on them, for each document d...
     * ....go through each word w in d...
     * ........and for each topic t, compute two things: 1) p(topic t |
       document d) = the proportion of words in document d that are
       currently assigned to topic t, and 2) p(word w | topic t) = the
       proportion of assignments to topic t over all documents that come
       from this word w. reassign w a new topic, where you choose topic t
       with id203 p(topic t | document d) * p(word w | topic t)
       (according to our generative model, this is essentially the
       id203 that topic t generated word w, so it makes sense that
       we resample the current word's topic with this id203). (also,
       i'm glossing over a couple of things here, such as the use of
       priors/pseudocounts in these probabilities.)
     * ........in other words, in this step, we're assuming that all topic
       assignments except for the current word in question are correct,
       and then updating the assignment of the current word using our
       model of how documents are generated.
     * after repeating the previous step a large number of times, you'll
       eventually reach a roughly steady state where your assignments are
       pretty good. so use these assignments to estimate the topic
       mixtures of each document (by counting the proportion of words
       assigned to each topic within that document) and the words
       associated to each topic (by counting the proportion of words
       assigned to each topic overall).

   layman's explanation
   in case the discussion above was a little eye-glazing, here's another
   way to look at lda in a different domain.
   suppose you've just moved to a new city. you're a hipster and an anime
   fan, so you want to know where the other hipsters and anime geeks tend
   to hang out. of course, as a hipster, you know you can't just ask, so
   what do you do?
   here's the scenario: you scope out a bunch of different establishments
   (documents) across town, making note of the people (words) hanging out
   in each of them (e.g., alice hangs out at the mall and at the park, bob
   hangs out at the movie theater and the park, and so on). crucially, you
   don't know the typical interest groups (topics) of each establishment,
   nor do you know the different interests of each person.
   so you pick some number k of categories to learn (i.e., you want to
   learn the k most important kinds of categories people fall into), and
   start by making a guess as to why you see people where you do. for
   example, you initially guess that alice is at the mall because people
   with interests in x like to hang out there; when you see her at the
   park, you guess it's because her friends with interests in y like to
   hang out there; when you see bob at the movie theater, you randomly
   guess it's because the z people in this city really like to watch
   movies; and so on.
   of course, your random guesses are very likely to be incorrect (they're
   random guesses, after all!), so you want to improve on them. one way of
   doing so is to:
     * pick a place and a person (e.g., alice at the mall).
     * why is alice likely to be at the mall? probably because other
       people at the mall with the same interests sent her a message
       telling her to come.
     * in other words, the more people with interests in x there are at
       the mall and the stronger alice is associated with interest x (at
       all the other places she goes to), the more likely it is that alice
       is at the mall because of interest x.
     * so make a new guess as to why alice is at the mall, choosing an
       interest with some id203 according to how likely you think it
       is.

   go through each place and person over and over again. your guesses keep
   getting better and better (after all, if you notice that lots of geeks
   hang out at the bookstore, and you suspect that alice is pretty geeky
   herself, then it's a good bet that alice is at the bookstore because
   her geek friends told her to go there; and now that you have a better
   idea of why alice is probably at the bookstore, you can use this
   knowledge in turn to improve your guesses as to why everyone else is
   where they are), and so eventually you can stop updating. then take a
   snapshot (or multiple snapshots) of your guesses, and use it to get all
   the information you want:
     * for each category, you can count the people assigned to that
       category to figure out what people have this particular interest.
       by looking at the people themselves, you can interpret the category
       as well (e.g., if category x contains lots of tall people wearing
       jerseys and carrying around basketballs, you might interpret x as
       the "basketball players" group).
     * for each place p and interest category c, you can compute the
       proportions of people at p because of c (under the current set of
       assignments), and these give you a representation of p. for
       example, you might learn that the people who hang out at barnes &
       noble consist of 10% hipsters, 50% anime fans, 10% jocks, and 30%
       college students.

   real-world example
   finally, let's go through a real-world example. i applied lda to a set
   of sarah palin's emails a little while ago (see
   [9]http://blog.echen.me/2011/06/27/... for a blog post, or
   [10]http://sarah-palin.heroku.com/ for an app that allows you to browse
   through the emails by the lda-learned topics), so here are the some of
   the topics that the algorithm learned:
     * trig/family/inspiration: family, web, mail, god, son, from,
       congratulations, children, life, child, down, trig, baby, birth,
       love, you, syndrome, very, special, bless, old, husband, years,
       thank, best, ...
     * wildlife/bp corrosion: game, fish, moose, wildlife, hunting, bears,
       polar, bear, subsistence, management, area, board, hunt, wolves,
       control, department, year, use, wolf, habitat, hunters, caribou,
       program, denby, fishing, ...
     * energy/fuel/oil/mining: energy, fuel, costs, oil, alaskans, prices,
       cost, nome, now, high, being, home, public, power, mine, crisis,
       price, resource, need, community, fairbanks, rebate, use, mining,
       villages, ...
     * gas: gas, oil, pipeline, agia, project, natural, north, producers,
       companies, tax, company, energy, development, slope, production,
       resources, line, gasline, transcanada, said, billion, plan,
       administration, million, industry, ...
     * education/waste: school, waste, education, students, schools,
       million, read, email, market, policy, student, year, high, news,
       states, program, first, report, business, management, bulletin,
       information, reports, 2008, quarter, ...
     * presidential campaign/elections: mail, web, from, thank, you, box,
       mccain, sarah, very, good, great, john, hope, president, sincerely,
       wasilla, work, keep, make, add, family, republican, support, doing,
       p.o, ...

   here's an example of an email which fell 99% into the
   trig/family/inspiration category (particularly representative words are
   highlighted in blue):
   [main-qimg-705d947d4ec236b5e6df820b0334f6e6]
   and here's an excerpt from an email which fell 10% into the
   presidential campaign/election category (in red), 90% into the
   wildlife/bp corrosion category (in green), and is precisely a
   wildlife-based protest against palin as a choice for vp:
   [main-qimg-8b27690ca11a718740ff96dc85e1b21a]
   173.8k views    [11]view 1.6k upvoters
   view 14 other answers to this question

about the author

   [12]edwin chen

[13]edwin chen

   pilot
   1.3m answer views6.8k this month
   [14]published writerforbes

   [15]about    [16]careers    [17]privacy    [18]terms    [19]contact

references

   visible links
   1. https://www.quora.com/opensearch/description.xml
   2. https://www.quora.com/
   3. https://www.quora.com/what-is-a-good-explanation-of-latent-dirichlet-allocation
   4. https://www.quora.com/profile/edwin-chen-1
   5. https://www.quora.com/profile/edwin-chen-1
   6. https://www.quora.com/what-is-a-good-explanation-of-latent-dirichlet-allocation/answer/edwin-chen-1
   7. https://www.quora.com/profile/oscar-annen
   8. https://www.quora.com/profile/alan-davis
   9. http://blog.echen.me/2011/06/27/topic-modeling-the-sarah-palin-emails/
  10. http://sarah-palin.heroku.com/
  11. https://www.quora.com/what-is-a-good-explanation-of-latent-dirichlet-allocation/answer/edwin-chen-1
  12. https://www.quora.com/profile/edwin-chen-1
  13. https://www.quora.com/profile/edwin-chen-1
  14. https://www.quora.com/profile/edwin-chen-1/answers/published
  15. https://www.quora.com/about
  16. https://www.quora.com/careers
  17. https://www.quora.com/about/privacy
  18. https://www.quora.com/about/tos
  19. https://www.quora.com/contact

   hidden links:
  21. https://www.quora.com/what-is-a-good-explanation-of-latent-dirichlet-allocation
