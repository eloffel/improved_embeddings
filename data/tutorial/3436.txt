   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]ai

 interpretability via attentional and memory-based interfaces, using tensorflow

   a closer look at the reasoning inside your deep networks.

   by [27]goku mohandas

   april 12, 2017

   attention gru. attention gru. (source: goku mohandas, used with
   permission.)

   attention readers: you can access all of the code on [28]github and
   view the ipython notebook [29]here.

   this article is a gentle introduction to attentional and memory-based
   interfaces in deep neural architectures, using [30]tensorflow.
   incorporating attention mechanisms is very simple and can offer
   transparency and interpretability to our complex models. we conclude
   with extensions and caveats of the interfaces. as you read the article,
   please access all of the code on [31]github and view the ipython
   notebook [32]here; all code is compatible with tensorflow version 1.0.
   the intended audience for this notebook are developers and researchers
   who have some basic understanding of tensorflow and fundamental deep
   learning concepts. check out this [33]post for a nice introduction to
   tensorflow.

applying selective attention to deep learning

   attentional interfaces in deep neural networks are loosely based on
   [34]visual attention mechanisms, found in many animals. these
   mechanisms allow the organisms to dynamically focus on pertinent parts
   of a visual input and respond accordingly. this basic idea of selective
   attention has been carried over to deep learning, where it is being
   used in [35]image analysis, [36]translation, [37]question/answering,
   [38]speech, and a variety of other tasks.
   attention on an image figure 1. attention on an image for a specific
   (underlined) word in the caption. credit: [39]kelvin xu, jimmy ba, ryan
   kiros, kyunghyun cho, aaron courville, ruslan salakhutdinov, richard
   zemel:    show, attend and tell: neural image id134 with
   visual attention   . used with permission.

   these interfaces also offer much needed model interpretability, by
   allowing us to see which parts of the input are attended to at any
   point in time. a common disadvantage with deep neural architectures is
   the lack of interpretability and the associated "black box" stigma.
   implementation of these interfaces has not only been shown to increase
   model performance, but also offer more transparent and sensible
   results. and as you will see in our implementation below, they produce
   some interesting visualizations that are consistent with how we would
   attend to the inputs.

   research on attentional interfaces is very popular these days because
   they offer multiple benefits. there are increasingly complex variants
   to the attention mechanisms, but the overall foundation remains the
   same.
   attention heatmaps figure 2. attention heatmaps offering
   interpretability on where the model is looking to respond. credit:
   [40]karl moritz hermann, tom     ko  isk  , edward grefenstette, lasse
   espeholt, will kay, mustafa suleyman:    teaching machines to read and
   comprehend   . used with permission.

   in this article, we will take a look at the fundamental attentional
   interface, implement it with a small model, and then discuss some
   recent variants. we will be using attention in natural language
   understanding (nlu) tasks but will briefly explore other areas as well.

overview of attentional interfaces

   first, let's take a closer look at the fundamental idea behind an
   attentional interface.

   the attention model takes in j inputs (h[1], h[2], ... h[j]), along
   with some information s[{i-1}] (memory), and outputs vector c, which is
   the summary of the hidden states focusing on the information s[{i-1}].
   before we see what goes on internally, it is important to point out
   there are a couple of different options for attention (i.e., soft,
   hard), but the type we will focus on will be completely differentiable
   (i.e., soft). this is because we want to be able to learn where to
   focus. this also means that we will be focusing everywhere at all
   times, but we will learn where to place more attention.

   we can think of our s[{i-1}] as the generated context describing what
   we should focus on. we take the dot product of each input item within
   this context in order to produce a score. the scores (e from equations
   on figure 3) are fed into a softmax to create our attention
   distribution. we multiply these normalized scores with our original
   inputs to receive a weighted summary vector for timestep/input i.
   internal operations for making the summarizing vector c figure 3.
   internal operations for making the summarizing vector c. credit: goku
   mohandas.

   one of the main advantages of using an attentional interface is the
   opportunity to interpret and visualize the attention scores. we can see
   for each input where the model places more attention and how that
   impacts our final result. there are many ways to implement fully
   differential attentional interfaces, and the internal intricacies can
   differ. the example we have above is very commonly seen in neural
   translation models.

   with translation, we don   t want just one summarizing vector using all
   of the inputs   we want a summarizing vector for each input. this is
   because translation is not always a one-to-one output. words in one
   language may result in multiple words in another, so we need to attend
   to the entire sentence for all inputs, while using that context
   information so we know what we have seen so far.

   we can go ahead and plot the attention scores for translation, and we
   will see something like figure 4. notice how some words, like "me,"
   very clearly only need to attend to one word. but other words, or
   groups of words, require attention across multiple words from the
   source language in order to translate properly.
   architecture and attention visualization for a basic neural machine
   translation task figure 4. architecture and attention visualization for
   a basic id4 task. goku mohandas.

basic id31 and overview architecture

   we will be using a id31 task in order to implement a
   basic attentional interface. this simple use case will be a nice
   introduction to how we can add attention to existing models for
   improved performance and interpretability.

   we will be using the [41]large movie review data set, which contains
   50,000 reviews that are split evenly into 25,000 train and 25,000 test
   sets. the data set includes text reviews with ratings from 0-9, but we
   will use ratings 0-4 as positive and 5-9 as negative sentiment. more
   specifics, including unlabeled data for unsupervised learning, and data
   collection details are provided in the readme file contained in [42]the
   zipped data set.

   here is the overview of the architecture, including the attentional
   interface. we will cover the details of preprocessing, the model, and
   the training procedure below. in figure 5, you will notice how the
   attention layer is almost just like a pluggable interface. we could
   have just as easily taken the last (relevant) hidden state from the
   encoder and applied a nonlinearity, followed by id172, to
   receive a predicted sentiment. adding this attentional layer, however,
   gives us the chance to look inside the model and gain interpretability
   from the id136s.
   model architecture overview figure 5. model architecture overview.
   credit: goku mohandas.

setup

   we will fetch all the data (trained embeddings, processed reviews, and
   trained model) by executing the commands detailed in the [43]ipython
   notebook for this article. we will be using preprocessed data, but for
   optimal id136 performance, you could retrain the model from
   scratch, using all 25,000 training/test samples. the main focus of this
   article, however, is on attention mechanisms and how you can add them
   to your networks for increased interpretability.

preprocessing components

   in this section, we will preprocess our raw input data. the main
   components are the vocab class, which we initialize using our vocab.txt
   file. this file contains all of the tokens (words) from our raw input,
   sorted by descending frequency. the next helper function we need is
   ids_to_tokens(), which will convert a list of ids into tokens we can
   understand. we will use this for reading our input and associating the
   word with its respective attention score.

sample the data

   now, we will see what our inputs will look like. the processed_review
   represents our reviews with ids. the review_seq_len tells us how long
   the review is. unless we use dynamic computation graphs, we need to
   feed in fixed-sized inputs into our tensorflow models per batch. this
   means that we will have some padding (with pads), and we do not want
   these to influence our model. in this implementation, the pads do not
   prove to be too problematic, since id136 will depend on the entire
   summarized context (so no loss masking is needed). and we also want to
   keep the pad tokens, even when determining the attention scores, to
   show how the model learns not to focus on the pads over time.

the model

   we will start by talking about operation functions.
   _xavier_weight_init() is a little function we made to properly
   initialize our weights, depending on the nonlinearity that will be
   applied to them. the initialization is such that we will receive
   outputs with unit variance prior to sending to the activation function.

   this is an optimization technique we use so we do not have large values
   when applying the nonlinearity, as that will lead to saturation at the
   extremes   and lead to gradient issues. we also have a helper function
   for layer id172, ln(), which is another optimization technique
   that will normalize our inputs into the gated recurrent unit (gru)
   before applying the activation function. this will allow us to control
   gradient issues and even allow us to use larger learning rates. the
   layer id172 is applied in the custom_gru() function prior to
   the sigmoid and tanh operations. the last helper function is
   add_dropout_and_layers(), which will add dropout to our recurrent
   outputs and will allow us to create multi-layered recurrent
   architectures.

   let's briefly describe the model pipelines and see how our inputs
   undergo representation changes. first, we will initialize our
   placeholders, which will hold the reviews, lens, sentiment, embeddings,
   etc. then we will build the encoder, which will take our input review
   and first embed using the glove embeddings. we will then feed the
   embedded tokens into a gru in order to encode the input. we will use
   the output from each timestep in the gru as our inputs to the
   attentional layer. notice that we could have completely removed the
   attentional interface, and just used the last relevant hidden state
   from the encoder gru in order to receive our predicted sentiment, but
   adding this attention layer allows us to see how the model processes
   the input review.

   in the attentional layer, we apply a nonlinearity followed by another
   one in order to reduce our representation to one dimension. now, we can
   normalize to compute our attention scores. these scores are then
   broadcasted and multiplied with the original inputs to receive our
   summarized vector. we use this vector to receive our predicted
   sentiment via id172 in the decoder. notice that we do not use a
   previous state (s[{i-1}]) since the task involves creating just one
   context and extracting the sentiment from that.

   we then define our loss as the cross id178 between the predicted and
   the ground truth sentiment. we use a bit of decay for our learning rate
   with an absolute minimum and use the adam optimizer. with all of these
   components, we have built our graph.

attention visualization to predict sentiment

   mapping of the attention scores figure 6. mapping of the attention
   scores for a sample review from our data set. credit: goku mohandas.

   you can see how the model starts with equal attention distribution to
   all words, and then starts to learn which words are important in order
   to predict the sentiment. the darker the color, the stronger that
   particular word is attended to. the first sanity check is that the pad
   tokens receive almost no attention; then, we start to see focus on
   really strong/influential words. check out a few of the other reviews
   as well and you will see that almost always, there is quite a bit of
   attention at the ends of reviews, which is where people give a
   concluding statement that summarizes their sentiment succinctly.

   we used the id31 task in order to see how simple it is to
   add an attentional interface to a model. though we were able to receive
   some meaningful attention scores for this task, it is not a trivial
   task to interpret, even with attention. most reviews (discarding the
   extremes) will usually talk about the plot for most of the review, so
   it is interesting that we can use attention to pick up on the brief
   moments of sentiment. an interesting extension of this implementation
   would be to only use the extreme reviews, where there is a spew of
   emotion throughout the review.

attentional interface variants

   our implementation of the soft attentional interface is one of the
   basic forms of attention, and we can already start to see some
   interpretability. this is currently a very active area of research
   where there are developments in different types of tasks using
   attention (vqa, translation, etc.), different types of attention, and
   better attentional interface architecture.

   so far, we have seen how attentional processing can be specialized to
   the task. with the translation task, the attentional interface is
   applied to each input word and a summarized context is made for each
   time step. this is because translation is not always a one-to-one task
   and a word may depend on several words in the target language for the
   correct translation. however, for our id31
   implementation, it wouldn't make sense to have a summarizing vector for
   each input because our output is a binary sentiment. similarly, there
   are also processing variants for tasks, such as question and answering
   (i.e., pointer-based attention to specific spans in the input) and
   image-based attention (i.e., focusing on different parts of the
   represented image).

   we can also have variants to the architecture of the interface as well.
   this too is a large, open field of research, and we may cover some of
   the interesting variants in future articles. here, we   ll introduce one
   of the architecture variants that is relevant to our sentiment
   analysis, or any natural language understanding task.

   in the attentional interface we implemented, you may notice that we
   completely disregard the order of the input review. we process all of
   the words in the review, but when we read a review and try to determine
   the sentiment, it is important to have context for what we have already
   read. a small tweak in the attentional interface can help us overcome
   this issue.
   attention gru figure 7. [44]attention gru. credit: goku mohandas.

   in order to incorporate order of the input into our attentional
   interface, all we have to do is, first, compute the attention scores
   for each token in the input. then, we feed in one token at a time, with
   the scalar (or vector) attention score in place of the update gate in
   the gru. we do this for all tokens in the input and we take the final
   (relevant output) or all of the outputs   which, of course, depends on
   our task and model architecture. this tweak allows us to incorporate
   the order of the input, along attentional scores, giving us both
   interpretability and a logical architecture.

caveats

   as you may notice, this increased interpretability does come with an
   increased computation cost. this can be a major factor when using these
   models for production systems, so it is really important to understand
   the types of tasks in which these interfaces prove to be really
   valuable. for the tasks we talked about here, adding these interfaces
   allows us to overcome the information and interpretability bottlenecks.
   for the specific task of binary id31, we can get similar
   performance with just a bag-of-words model, but the model ultimately
   depends on the performance you are looking for and the computational
   constraints.

interpretability and why it   s important

   as we start to develop increasingly deeper models, it will be important
   to maintain a high level of interpretability. this becomes more
   valuable as ai starts to have a greater impact on our everyday lives
   and we start integrating it everywhere. attentional interfaces are just
   the beginning for this field in terms of having transparent models, and
   we need to keep pushing for increased interpretability.

   this post is a collaboration between o'reilly and [45]tensorflow.
   [46]see our statement of editorial independence.
   article image: attention gru. (source: goku mohandas, used with
   permission.).
   tags: [47]all about tensorflow

   share
    1. [48]tweet
    2.
    3.
     __________________________________________________________________

   [49]goku mohandas

[50]goku mohandas

   goku mohandas is an artificial intelligence (ai) researcher in silicon
   valley with a focus on using deep learning for natural language tasks.
   his interests include research on ai for intelligent search and
   id53 augmented by attentional and memory-based
   interfaces. he also strongly believes in the democratization of ai with
   a focus on interpretability and transparency. previous work includes
   working on the intersection of ai and biotechnology at the johns
   hopkins university applied physics laboratory.
   [51]more
     __________________________________________________________________

   [52]bots landscape.

   [53]ai

[54]infographic: the bot platform ecosystem

   by [55]jon bruner

   a look at the artificial intelligence and messaging platforms behind
   the fast-growing chatbot community

   video
   [56]vertical forest, milan.

   [57]ai

[58]evolve ai

   by [59]naveen rao

   naveen rao explains how intel nervana is evolving the ai stack from
   silicon to the cloud.

   [60]welcome sign at o'reilly ai conference 2016

   [61]ai

[62]highlights from the o'reilly ai conference in new york 2016

   by [63]mac slocum

   watch highlights covering artificial intelligence, machine learning,
   intelligence engineering, and more. from the o'reilly ai conference in
   new york 2016.

   video
   [64]close up of uber's self driving car in pittsburgh.

   [65]ai

[66]how ai is propelling driverless cars, the future of surface transport

   by [67]shahin farshchi

   shahin farshchi examines role artificial intelligence will play in
   driverless cars.

about us

     * [68]our company
     * [69]teach/speak/write
     * [70]careers
     * [71]customer service
     * [72]contact us

site map

     * [73]ideas
     * [74]learning
     * [75]topics
     * [76]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [77]terms of service     [78]privacy policy     [79]editorial independence

   animal

   iframe: [80]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/ai
  27. https://www.oreilly.com/people/goku-mohandas
  28. https://github.com/ajarai/attentional-interfaces-o-reilly
  29. https://github.com/ajarai/attentional-interfaces-o-reilly/blob/master/attention.ipynb
  30. https://www.tensorflow.org/
  31. https://github.com/ajarai/attentional-interfaces-o-reilly
  32. https://github.com/ajarai/attentional-interfaces-o-reilly/blob/master/attention.ipynb
  33. https://www.oreilly.com/learning/hello-tensorflow
  34. http://www.annualreviews.org/doi/abs/10.1146/annurev.ne.18.030195.001205
  35. http://arxiv.org/abs/1502.03044
  36. http://arxiv.org/abs/1409.0473
  37. http://arxiv.org/abs/1611.01603
  38. http://arxiv.org/abs/1508.01211
  39. http://arxiv.org/abs/1502.03044
  40. http://arxiv.org/abs/1506.03340
  41. http://ai.stanford.edu/~amaas/data/sentiment/
  42. https://github.com/ajarai/attentional-interfaces-o-reilly/blob/master/data/processed_reviews.zip
  43. https://github.com/ajarai/attentional-interfaces-o-reilly
  44. http://arxiv.org/abs/1603.01417
  45. https://www.tensorflow.org/
  46. http://www.oreilly.com/about/editorial_independence.html
  47. https://www.oreilly.com/tags/all-about-tensorflow
  48. https://twitter.com/share
  49. https://www.oreilly.com/people/goku-mohandas
  50. https://www.oreilly.com/people/goku-mohandas
  51. https://www.oreilly.com/people/goku-mohandas
  52. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  53. https://www.oreilly.com/topics/ai
  54. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  55. https://www.oreilly.com/people/b1d73-jon-bruner
  56. https://www.oreilly.com/ideas/evolve-ai
  57. https://www.oreilly.com/topics/ai
  58. https://www.oreilly.com/ideas/evolve-ai
  59. https://www.oreilly.com/people/14d38-naveen-rao
  60. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  61. https://www.oreilly.com/topics/ai
  62. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  63. https://www.oreilly.com/people/0d2c1-mac-slocum
  64. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  65. https://www.oreilly.com/topics/ai
  66. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  67. https://www.oreilly.com/people/c7521-shahin-farshchi
  68. http://oreilly.com/about/
  69. http://oreilly.com/work-with-us.html
  70. http://oreilly.com/careers/
  71. http://shop.oreilly.com/category/customer-service.do
  72. http://shop.oreilly.com/category/customer-service.do
  73. https://www.oreilly.com/ideas
  74. https://www.oreilly.com/topics/oreilly-learning
  75. https://www.oreilly.com/topics
  76. https://www.oreilly.com/all
  77. http://oreilly.com/terms/
  78. http://oreilly.com/privacy.html
  79. http://www.oreilly.com/about/editorial_independence.html
  80. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  82. https://www.oreilly.com/
  83. https://www.facebook.com/oreilly/
  84. https://twitter.com/oreillymedia
  85. https://www.youtube.com/user/oreillymedia
  86. https://plus.google.com/+oreillymedia
  87. https://www.linkedin.com/company/oreilly-media
  88. https://www.oreilly.com/
