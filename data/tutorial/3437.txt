   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    an intuitive understanding of id27s:
   from count vectors to id97 comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]deep learning [94]an intuitive understanding of word
   embeddings: from count vectors to id97

   [95]deep learning[96]machine learning[97]nlp[98]python

an intuitive understanding of id27s: from count vectors to id97

   [99]nss, june 4, 2017

introduction

   before we start, have a look at the below examples.
    1. you open google and search for a news article on the ongoing
       champions trophy and get hundreds of search results in return about
       it.
    2. nate silver analysed millions of tweets and correctly predicted the
       results of 49 out of 50 states in 2008 u.s presidential elections.
    3. you type a sentence in google translate in english and get an
       equivalent chinese conversion.


   so what do the above examples have in common?

   you possible guessed it right     text processing. all the above three
   scenarios deal with humongous amount of text to perform different range
   of tasks like id91 in the google search example, classification
   in the second and machine translation in the third.

   humans can deal with text format quite intuitively but provided we have
   millions of documents being generated in a single day, we cannot have
   humans performing the above the three tasks. it is neither scalable nor
   effective.

   so, how do we make computers of today perform id91,
   classification etc on a text data since we know that they are generally
   inefficient at handling and processing strings or texts for any
   fruitful outputs?

   sure, a computer can match two strings and tell you whether they are
   same or not. but how do we make computers tell you about football or
   ronaldo when you search for messi? how do you make a computer
   understand that    apple    in    apple is a tasty fruit    is a fruit that can
   be eaten and not a company?

   the answer to the above questions lie in creating a representation for
   words that capture their meanings, semantic relationships and the
   different types of contexts they are used in.

   and all of these are implemented by using id27s or numerical
   representations of texts so that computers may handle them.

   below, we will see formally what are id27s and their
   different types and how we can actually implement them to perform the
   tasks like returning efficient google search results.


table of contents

    1. what are id27s?
    2. different types of id27s
       2.1  frequency based embedding
       2.1.1 count vectors
       2.1.2 tf-idf
       2.1.3 co-occurrence matrix
       2.2  prediction based embedding
       2.2.1 cbow
       2.2.2 skip-gram
    3. id27s use case scenarios(what all can be done using word
       embeddings? eg: similarity, odd one out etc.)
    4. using pre-trained word vectors
    5. training your own word vectors
    6. end notes


1. what are id27s?

   in very simplistic terms, id27s are the texts converted into
   numbers and there may be different numerical representations of the
   same text. but before we dive into the details of id27s, the
   following question should be asked     why do we need id27s?

   as it turns out, many machine learning algorithms and almost all deep
   learning architectures are incapable of processing strings or plain
   text in their raw form. they require numbers as inputs to perform any
   sort of job, be it classification, regression etc. in broad terms. and
   with the huge amount of data that is present in the text format, it is
   imperative to extract knowledge out of it and build applications. some
   real world applications of text applications are     id31
   of reviews by amazon etc., document or news classification or
   id91 by google etc.

   let us now define id27s formally. a id27 format
   generally tries to map a word using a dictionary to a vector. let us
   break this sentence down into finer details to have a clear view.

   take a look at this example     sentence=    id27s are word
   converted into numbers    

   a word in this sentence may be    embeddings    or    numbers     etc.

   a dictionary may be the list of all unique words in the sentence. so, a
   dictionary may look like    
   [   word   ,   embeddings   ,   are   ,   converted   ,   into   ,   numbers   ]

   a vector representation of a word may be a one-hot encoded vector where
   1 stands for the position where the word exists and 0 everywhere else.
   the vector representation of    numbers    in this format according to the
   above dictionary is [0,0,0,0,0,1] and of converted is[0,0,0,1,0,0].

   this is just a very simple method to represent a word in the vector
   form. let us look at different types of id27s or word vectors
   and their advantages and disadvantages over the rest.


2. different types of id27s

   the different types of id27s can be broadly classified into
   two categories-
    1. frequency based embedding
    2. prediction based embedding

   let us try to understand each of these methods in detail.


2.1 frequency based embedding

   there are generally three types of vectors that we encounter under this
   category.
    1. count vector
    2. tf-idf vector
    3. co-occurrence vector

   let us look into each of these vectorization methods in detail.


2.1.1 count vector

   consider a corpus c of d documents {d1,d2   ..dd} and n unique tokens
   extracted out of the corpus c. the n tokens will form our dictionary
   and the size of the count vector matrix m will be given by d x n. each
   row in the matrix m contains the frequency of tokens in document d(i).

   let us understand this using a simple example.

   d1: he is a lazy boy. she is also lazy.

   d2: neeraj is a lazy person.

   the dictionary created may be a list of unique tokens(words) in the
   corpus =[   he   ,   she   ,   lazy   ,   boy   ,   neeraj   ,   person   ]

   here, d=2, n=6

   the count matrix m of size 2 x 6 will be represented as    
      he she lazy boy neeraj person
   d1 1  1   2    1   0      0
   d2 0  0   1    0   1      1

   now, a column can also be understood as word vector for the
   corresponding word in the matrix m. for example, the word vector for
      lazy    in the above matrix is [2,1] and so on.here, the rows correspond
   to the documents in the corpus and the columns correspond to the tokens
   in the dictionary. the second row in the above matrix may be read as    
   d2 contains    lazy   : once,    neeraj   : once and    person    once.

   now there may be quite a few variations while preparing the above
   matrix m. the variations will be generally in-
    1. the way dictionary is prepared.
       why? because in real world applications we might have a corpus
       which contains millions of documents. and with millions of
       document, we can extract hundreds of millions of unique words. so
       basically, the matrix that will be prepared like above will be a
       very sparse one and inefficient for any computation. so an
       alternative to using every unique word as a dictionary element
       would be to pick say top 10,000 words based on frequency and then
       prepare a dictionary.
    2. the way count is taken for each word.
       we may either take the frequency (number of times a word has
       appeared in the document) or the presence(has the word appeared in
       the document?) to be the entry in the count matrix m. but
       generally, frequency method is preferred over the latter.

   below is a representational image of the matrix m for easy
   understanding.



2.1.2 tf-idf vectorization

   this is another method which is based on the frequency method but it is
   different to the count vectorization in the sense that it takes into
   account not just the occurrence of a word in a single document but in
   the entire corpus. so, what is the rationale behind this? let us try to
   understand.

   common words like    is   ,    the   ,    a    etc. tend to appear quite frequently
   in comparison to the words which are important to a document. for
   example, a document a on lionel messi is going to contain more
   occurences of the word    messi    in comparison to other documents. but
   common words like    the    etc. are also going to be present in higher
   frequency in almost every document.

   ideally, what we would want is to down weight the common words
   occurring in almost all documents and give more importance to words
   that appear in a subset of documents.

   tf-idf works by penalising these common words by assigning them lower
   weights while giving importance to words like messi in a particular
   document.

   so, how exactly does tf-idf work?

   consider the below sample table which gives the count of
   terms(tokens/words) in two documents.

   now, let us define a few terms related to tf-idf.


   tf = (number of times term t appears in a document)/(number of terms in
   the document)

   so, tf(this,document1) = 1/8

   tf(this, document2)=1/5

   it denotes the contribution of the word to the document i.e words
   relevant to the document should be frequent. eg: a document about messi
   should contain the word    messi    in large number.

   idf = log(n/n), where, n is the number of documents and n is the number
   of documents a term t has appeared in.

   where n is the number of documents and n is the number of documents a
   term t has appeared in.

   so, idf(this) = log(2/2) = 0.

   so, how do we explain the reasoning behind idf? ideally, if a word has
   appeared in all the document, then probably that word is not relevant
   to a particular document. but if it has appeared in a subset of
   documents then probably the word is of some relevance to the documents
   it is present in.

   let us compute idf for the word    messi   .

   idf(messi) = log(2/1) = 0.301.

   now, let us compare the tf-idf for a common word    this    and a word
      messi    which seems to be of relevance to document 1.

   tf-idf(this,document1) = (1/8) * (0) = 0

   tf-idf(this, document2) = (1/5) * (0) = 0

   tf-idf(messi, document1) = (4/8)*0.301 = 0.15

   as, you can see for document1 , tf-idf method heavily penalises the
   word    this    but assigns greater weight to    messi   . so, this may be
   understood as    messi    is an important word for document1 from the
   context of the entire corpus.


2.1.3 co-occurrence matrix with a fixed context window

   the big idea     similar words tend to occur together and will have
   similar context for example     apple is a fruit. mango is a fruit.
   apple and mango tend to have a similar context i.e fruit.

   before i dive into the details of how a co-occurrence matrix is
   constructed, there are two concepts that need to be clarified    
   co-occurrence and context window.

   co-occurrence     for a given corpus, the co-occurrence of a pair of
   words say w1 and w2 is the number of times they have appeared together
   in a context window.

   context window     context window is specified by a number and the
   direction. so what does a context window of 2 (around) means? let us
   see an example below,


   quick brown fox jump over the lazy dog

   the green words are a 2 (around) context window for the word    fox    and
   for calculating the co-occurrence only these words will be counted. let
   us see context window for the word    over   .


   quick brown fox jump over the lazy dog


   now, let us take an example corpus to calculate a co-occurrence matrix.

   corpus = he is not lazy. he is intelligent. he is smart.


               he is not lazy intelligent smart
       he      0  4   2   1        2        1
       is      4  0   1   2        2        1
       not     2  1   0   1        0        0
      lazy     1  2   1   0        0        0
   intelligent 2  2   0   0        0        0
      smart    1  1   0   0        0        0

   let us understand this co-occurrence matrix by seeing two examples in
   the table above. red and the blue box.

   red box- it is the number of times    he    and    is    have appeared in the
   context window 2 and it can be seen that the count turns out to be 4.
   the below table will help you visualise the count.
   he is not lazy he is intelligent he is smart
   he is not lazy he is intelligent he is smart
   he is not lazy he is intelligent he is smart
   he is not lazy he is intelligent he is smart

   while the word    lazy    has never appeared with    intelligent    in the
   context window and therefore has been assigned 0 in the blue box.


   variations of co-occurrence matrix

   let   s say there are v unique words in the corpus. so vocabulary size =
   v. the columns of the co-occurrence matrix form the context words. the
   different variations of co-occurrence matrix are-
    1. a co-occurrence matrix of size v x v. now, for even a decent corpus
       v gets very large and difficult to handle. so generally, this
       architecture is never preferred in practice.
    2. a co-occurrence matrix of size v x n where n is a subset of v and
       can be obtained by removing irrelevant words like stopwords etc.
       for example. this is still very large and presents computational
       difficulties.

   but, remember this co-occurrence matrix is not the word vector
   representation that is generally used. instead, this co-occurrence
   matrix is decomposed using techniques like pca, svd etc. into factors
   and combination of these factors forms the word vector representation.

   let me illustrate this more clearly. for example, you perform pca on
   the above matrix of size vxv. you will obtain v principal components.
   you can choose k components out of these v components. so, the new
   matrix will be of the form v x k.

   and, a single word, instead of being represented in v dimensions will
   be represented in k dimensions while still capturing almost the same
   semantic meaning. k is generally of the order of hundreds.

   so, what pca does at the back is decompose co-occurrence matrix into
   three matrices, u,s and v where u and v are both orthogonal matrices.
   what is of importance is that dot product of u and s gives the word
   vector representation and v gives the word context representation.


   advantages of co-occurrence matrix
    1. it preserves the semantic relationship between words. i.e man and
       woman tend to be closer than man and apple.
    2. it uses svd at its core, which produces more accurate word vector
       representations than existing methods.
    3. it uses factorization which is a well-defined problem and can be
       efficiently solved.
    4. it has to be computed once and can be used anytime once computed.
       in this sense, it is faster in comparison to others.


   disadvantages of co-occurrence matrix
    1. it requires huge memory to store the co-occurrence matrix.
       but, this problem can be circumvented by factorizing the matrix out
       of the system for example in hadoop clusters etc. and can be saved.


2.2 prediction based vector

   pre-requisite: this section assumes that you have a working knowledge
   of how a neural network works and the mechanisms by which weights in an
   nn are updated. if you are new to neural network, i would suggest you
   go through [100]this awesome article by sunil to gain a very good
   understanding of how nn works.

   so far, we have seen deterministic methods to determine word vectors.
   but these methods proved to be limited in their word representations
   until mitolov etc. el introduced id97 to the nlp community. these
   methods were prediction based in the sense that they provided
   probabilities to the words and proved to be state of the art for tasks
   like word analogies and word similarities. they were also able to
   achieve tasks like king -man +woman = queen, which was considered a
   result almost magical. so let us look at the id97 model used as of
   today to generate word vectors.

   id97 is not a single algorithm but a combination of two techniques
       cbow(continuous bag of words) and skip-gram model. both of these are
   shallow neural networks which map word(s) to the target variable which
   is also a word(s). both of these techniques learn weights which act as
   word vector representations. let us discuss both these methods
   separately and gain intuition into their working.


2.2.1 cbow (continuous bag of words)

   the way cbow work is that it tends to predict the id203 of a word
   given a context. a context may be a single word or a group of words.
   but for simplicity, i will take a single context word and try to
   predict a single target word.

   suppose, we have a corpus c =    hey, this is sample corpus using only
   one context word.    and we have defined a context window of 1. this
   corpus may be converted into a training set for a cbow model as follow.
   the input is shown below. the matrix on the right in the below image
   contains the one-hot encoded from of the input on the left.

   the target for a single datapoint say datapoint 4 is shown as below
   hey this is sample corpus using only one context word
   0   0    0  1      0      0     0    0   0       0


   this matrix shown in the above image is sent into a shallow neural
   network with three layers: an input layer, a hidden layer and an output
   layer. the output layer is a softmax layer which is used to sum the
   probabilities obtained in the output layer to 1. now let us see how the
   forward propagation will work to calculate the hidden layer activation.

   let us first see a diagrammatic representation of the cbow model.

   the matrix representation of the above image for a single data point is
   below.

   the flow is as follows:
    1. the input layer and the target, both are one- hot encoded of size
       [1 x v]. here v=10 in the above example.
    2. there are two sets of weights. one is between the input and the
       hidden layer and second between hidden and output layer.
       input-hidden layer matrix size =[v x n] , hidden-output layer
       matrix  size =[n x v] : where n is the number of dimensions we
       choose to represent our word in. it is arbitary and a
       hyper-parameter for a neural network. also, n is the number of
       neurons in the hidden layer. here, n=4.
    3. there is a no activation function between any layers.( more
       specifically, i am referring to linear activation)
    4. the input is multiplied by the input-hidden weights and called
       hidden activation. it is simply the corresponding row in the
       input-hidden matrix copied.
    5. the hidden input gets multiplied by hidden- output weights and
       output is calculated.
    6. error between output and target is calculated and propagated back
       to re-adjust the weights.
    7. the weight  between the hidden layer and the output layer is taken
       as the word vector representation of the word.

   we saw the above steps for a single context word. now, what about if we
   have multiple context words? the image below describes the architecture
   for multiple context words.

   below is a matrix representation of the above architecture for an easy
   understanding.

   the image above takes 3 context words and predicts the id203 of a
   target word. the input can be assumed as taking three one-hot encoded
   vectors in the input layer as shown above in red, blue and green.

   so, the input layer will have 3 [1 x v] vectors in the input as shown
   above and 1 [1 x v] in the output layer. rest of the architecture is
   same as for a 1-context cbow.

   the steps remain the same, only the calculation of hidden activation
   changes. instead of just copying the corresponding rows of the
   input-hidden weight matrix to the hidden layer, an average is taken
   over all the corresponding rows of the matrix. we can understand this
   with the above figure. the average vector calculated becomes the hidden
   activation. so, if we have three context words for a single target
   word, we will have three initial hidden activations which are then
   averaged element-wise to obtain the final activation.

   in both a single context word and multiple context word, i have shown
   the images till the calculation of the hidden activations since this is
   the part where cbow differs from a simple mlp network. the steps after
   the calculation of hidden layer are same as that of the mlp as
   mentioned in this article     [101]understanding and coding neural
   networks from scratch.

   the differences between mlp and cbow are  mentioned below for
   clarification:
    1. the objective function in mlp is a mse(mean square error) whereas
       in cbow it is negative log likelihood of a word given a set of
       context i.e -log(p(wo/wi)), where p(wo/wi) is given as

   wo : output word
   wi: context words

   2. the gradient of error with respect to hidden-output weights and
   input-hidden weights are different since mlp has  sigmoid
   activations(generally) but cbow has linear activations. the method
   however to calculate the gradient is same as an mlp.


   advantages of cbow:
    1. being probabilistic is nature, it is supposed to perform superior
       to deterministic methods(generally).
    2. it is low on memory. it does not need to have huge ram requirements
       like that of co-occurrence matrix where it needs to store three
       huge matrices.


   disadvantages of cbow:
    1. cbow takes the average of the context of a word (as seen above in
       calculation of hidden activation). for example, apple can be both a
       fruit and a company but cbow takes an average of both the contexts
       and places it in between a cluster for fruits and companies.
    2. training a cbow from scratch can take forever if not properly
       optimized.


2.2.2 skip     gram model

   skip     gram follows the same topology as of cbow. it just flips cbow   s
   architecture on its head. the aim of skip-gram is to predict the
   context given a word. let us take the same corpus that we built our
   cbow model on. c=   hey, this is sample corpus using only one context
   word.    let us construct the training data.

   the input vector for skip-gram is going to be similar to a 1-context
   cbow model. also, the calculations up to hidden layer activations are
   going to be the same. the difference will be in the target variable.
   since we have defined a context window of 1 on both the sides, there
   will be    two    one hot encoded target variables and    two    corresponding
   outputs as can be seen by the blue section in the image.

   two separate errors are calculated with respect to the two target
   variables and the two error vectors obtained are added element-wise to
   obtain a final error vector which is propagated back to update the
   weights.

   the weights between the input and the hidden layer are taken as the
   word vector representation after training. the id168 or the
   objective is of the same type as of the cbow model.

   the skip-gram architecture is shown below.


   for a better understanding, matrix style structure with calculation has
   been shown below.


   let us break down the above image.

   input layer  size     [1 x v], input hidden weight matrix size     [v x n],
   number of neurons in hidden layer     n, hidden-output weight matrix size
       [n x v], output layer size     c [1 x v]

   in the above example, c is the number of context words=2, v= 10, n=4
    1. the row in red is the hidden activation corresponding to the input
       one-hot encoded vector. it is basically the corresponding row of
       input-hidden matrix copied.
    2. the yellow matrix is the weight between the hidden layer and the
       output layer.
    3. the blue matrix is obtained by the id127 of hidden
       activation and the hidden output weights. there will be two rows
       calculated for two target(context) words.
    4. each row of the blue matrix is converted into its softmax
       probabilities individually as shown in the green box.
    5. the grey matrix contains the one hot encoded vectors of the two
       context words(target).
    6. error is calculated by substracting the first row of the grey
       matrix(target) from the first row of the green matrix(output)
       element-wise. this is repeated for the next row. therefore, for
       n target context words, we will have n error vectors.
    7. element-wise sum is taken over all the error vectors to obtain a
       final error vector.
    8. this error vector is propagated back to update the weights.

advantages of skip-gram model

    1. skip-gram model can capture two semantics for a single word. i.e it
       will have two vector representations of apple. one for the company
       and other for the fruit.
    2. skip-gram with negative sub-sampling outperforms every other method
       generally.


   [102]this is an excellent interactive tool to visualise cbow and skip
   gram in action. i would suggest you to really go through this link for
   a better understanding.

3. id27s use case scenarios

   since id27s or word vectors are numerical representations of
   contextual similarities between words, they can be manipulated and made
   to perform amazing tasks like-
    1. finding the degree of similarity between two words.
       model.similarity('woman','man')
       0.73723527
    2. finding odd one out.
       model.doesnt_match('breakfast cereal dinner lunch';.split())
       'cereal'
    3. amazing things like woman+king-man =queen
       model.most_similar(positive=['woman','king'],negative=['man'],topn=
       1)
       queen: 0.508
    4. id203 of a text under the model
       model.score(['the fox jumped over the lazy dog'.split()])
       0.21

   below is one interesting visualisation of id97.

   the above image is a id167 representation of word vectors in 2
   dimension and you can see that two contexts of apple have been
   captured. one is a fruit and the other company.

   5.  it can be used to perform machine translation.

   the above graph is a bilingual embedding with chinese in green and
   english in yellow. if we know the words having similar meanings in
   chinese and english, the above bilingual embedding can be used to
   translate one language into the other.


4. using pre-trained word vectors

   we are going to use google   s pre-trained model. it contains word
   vectors for a vocabulary of 3 million words trained on around 100
   billion words from the google news dataset. the downlaod link for the
   model is [103]this. beware it is a 1.5 gb download.

   from gensim.models import id97

   #loading the downloaded model
   model =
   id97.load_id97_format('googlenews-vectors-negative300.bin',
   binary=true, norm_only=true)

   #the model is loaded. it can be used to perform all of the tasks
   mentioned above.

   # getting word vectors of a word
   dog = model['dog']

   #performing king queen magic
   print(model.most_similar(positive=['woman', 'king'], negative=['man']))

   #picking odd one out
   print(model.doesnt_match("breakfast cereal dinner lunch".split()))

   #printing similarity index
   print(model.similarity('woman', 'man'))


5. training your own word vectors

   we will be training our own id97 on a custom corpus. for training
   the model we will be using gensim and the steps are illustrated as
   below.

   id97 requires that a format of list of list for training where
   every document is contained in a list and every list contains list of
   tokens of that documents. i won   t be covering the pre-preprocessing
   part here. so let   s take an example list of list to train our id97
   model.

   sentence=[[   neeraj   ,   boy   ],[   sarwan   ,   is   ],[   good   ,   boy   ]]

   #training id97 on 3 sentences
   model = gensim.models.id97(sentence,
   min_count=1,size=300,workers=4)

   let us try to understand the parameters of this model.

   sentence     list of list of our corpus
   min_count=1 -the threshold value for the words. word with frequency
   greater than this only are going to be included into the model.
   size=300     the number of dimensions in which we wish to represent our
   word. this is the size of the word vector.
   workers=4     used for parallelization

   #using the model
   #the new trained model can be used similar to the pre-trained ones.

   #printing similarity index
   print(model.similarity('woman', 'man'))


projects

   now, its time to take the plunge and actually play with some other real
   datasets. so are you ready to take on the challenge? accelerate your
   nlp journey with the following practice problems:
   [104]practice problem: identify the sentiments identify the sentiment
   of tweets
   [105]practice problem : twitter id31 to detect hate
   speech in tweets


6. end notes

   id27s is an active research area trying to figure out better
   word representations than the existing ones. but, with time they have
   grown large in number and more complex. this article was aimed at
   simplying some of the workings of these embedding models without
   carrying the mathematical overhead. if you feel think that i was able
   to clear some of your confusion, comment below. any changes or
   suggestions would be welcomed.

   note: we also have a [106]video course on natural language processing
   covering many nlp topics including id27s. do check it out!
   you can also read this article on analytics vidhya's android app
   [107]get it on google play

share this:

     * [108]click to share on linkedin (opens in new window)
     * [109]click to share on facebook (opens in new window)
     * [110]click to share on twitter (opens in new window)
     * [111]click to share on pocket (opens in new window)
     * [112]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [113]id158, [114]deep learning, [115]machine
   learning, [116]natural language processing, [117]nlp, [118]text
   processing, [119]id97
   next article

senior data analyst-chennai (2-4 years of experience)

   previous article

data science evangelist- gurgaon (2 to 3 years of experience)

[120]nss

   i am a perpetual, quick learner and keen to explore the realm of data
   analytics and science. i am deeply excited about the times we live in
   and the rate at which data is being generated and being transformed as
   an asset. i am well versed with a few tools for dealing with data and
   also in the process of learning some other tools and knowledge required
   to exploit data.
     *

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [121]discussion portal to get your queries resolved

38 comments

     * preeti agarwal says:
       [122]june 6, 2017 at 10:55 am
       very nicely explained    had read somewhere on tuning the word matrix
       further     will post the link shortly!!
       [123]reply
          + nss says:
            [124]june 9, 2017 at 2:34 pm
            sure and thank you.
            [125]reply
     * sandip says:
       [126]june 6, 2017 at 12:21 pm
       very nice article
       [127]reply
          + nss says:
            [128]june 9, 2017 at 2:34 pm
            thank you.
            [129]reply
     * ajit balakrishnan says:
       [130]june 6, 2017 at 1:25 pm
       excellent summary
       [131]reply
          + nss says:
            [132]june 9, 2017 at 2:34 pm
            thank you.
            [133]reply
     * yousra says:
       [134]june 6, 2017 at 3:11 pm
       very good article. .it helped me to better understand id97.
       .thanks
       [135]reply
          + nss says:
            [136]june 9, 2017 at 2:33 pm
            thank you.
            [137]reply
     * zach smith says:
       [138]june 6, 2017 at 8:28 pm
       nice article. although there is an inconsistency in section 2.1.1.
       in your written example you say documents = rows and terms =
       columns, but the visualization of m that you show has that
       switched. am i wrong in thinking that the visualization is wrong
       and that if matters that documents are assigned to rows and terms
       are assigned to columns?
       [139]reply
          + nss says:
            [140]june 6, 2017 at 9:45 pm
            @zach smith   . it doesn   t matter in this case. the entries i.e
            the occurrences of terms in a document are still going to be
            the same.
            [141]reply
     * zunwenyou says:
       [142]june 8, 2017 at 7:07 am
       great article. thank you
       [143]reply
          + nss says:
            [144]june 9, 2017 at 2:33 pm
            thank you.
            [145]reply
     * [146]ravi theja says:
       [147]june 8, 2017 at 9:59 am
       great article.
       does each word have two vector representation   one representation
       for word acting as context word and other representation for word
       acting as central word??
       [148]reply
          + nss says:
            [149]june 9, 2017 at 2:32 pm
            each word has just one vector representation. you can use it
            either as a context or an input. thanks.
            [150]reply
               o [151]ravi theja says:
                 [152]june 9, 2017 at 11:26 pm
                 i am confused now.
                 in the following lecture at 42:04 chris manning was
                 explaining that each word has two vector representations.
                 one for central word and other for context word.
                 [153]https://www.youtube.com/watch?v=eribwqs9p38&index=2&
                 list=pl3fw7lu3i5jsnh1rnuwq_tcylnr7ekre6
                 and stack overflow is giving some explainations
                 [154]https://stackoverflow.com/questions/29381505/why-doe
                 s-id97-use-2-representations-for-each-word .
                 please correct me if i am wrong. thanks.
                 [155]reply
                    # nss says:
                      [156]june 10, 2017 at 12:50 am
                      i went through the video and the stack exchange link
                      that you forwarded. let us just focus on the formula
                      manning was describing in the video exp(u.v), where
                      v is the center word and u is the context. now i
                      want you to look at the skip gram excel sheet that i
                      included in my article. you will notice that the
                      context vector u is obtained from the weight matrix
                      between the hidden and the output layer while the
                      center one is given by the rows of the weight matrix
                      between the input layer and the hidden layer. so,
                      effectively as word is being represented by two
                      vectors or you can interpret it as a word (one hot
                      encoded one) projected into two different vectors.
                      this is just a representational purpose. what i was
                      talking in my previous comment was that, when you
                      pass an input into the neural network, you create
                      one vector representation(one hot encoded one) and
                      that can be used as both the center word and the
                      context word. hope that, i made things clear now.
                      [157]reply
                         @ [158]ravi theja says:
                           [159]june 19, 2017 at 2:25 pm
                           yes, things are clear now. thank you.
                           [160]reply
     * pallavi says:
       [161]june 8, 2017 at 3:30 pm
       thanks for this nice article! i have been waiting for word
       embedding related detailed article since long.
       however, i have a query. in your excel calculations for skip gram,
       is the id127 shown for hidden activation row and
       each column from hidden output weight matrix? if yes, i am getting
       some different answer for output matrix. can you please check? or
       am i missing something?
       [162]reply
          + nss says:
            [163]june 9, 2017 at 2:30 pm
            yes, the calculation shown is between hidden activation and
            hidden-output matrix. you can paste your output vector below
            and i can have a look.
            [164]reply
     * pallavi says:
       [165]june 8, 2017 at 4:01 pm
       thanks for the nice article!
       [166]reply
     * wouter deketelaere says:
       [167]june 9, 2017 at 1:13 am
       great explanation.
       i do have a question about the co-occurrence matrix, though.
       shouldn   t the red box contain the number 5 for a context size of 2
       (around)?
       around the word    intelligent    you have two instances of    he is    on
       the left and the right of    intelligent   .
       you only count the    is    on the left and    he    on the right.
       why is that?
       [168]reply
          + nss says:
            [169]june 9, 2017 at 2:26 pm
            read the co-occurrence matrix for the red box as     for the
            word    he   , how many times has    is    appeared in the 2-context
            window. the word    intelligent    has nothing to do with the
            co-occurrence of    he    and    is   .
            [170]reply
     * cory says:
       [171]june 16, 2017 at 2:15 am
       great job! very good flow and well explained. i do have one
       question though regarding the output layer. for your case, you
       should have two vectors representing the context words we wish to
          predict    once training is complete. when performing the
       multiplication between the input hidden weight matrix and hidden
       output weight matrix, your entries for each output vector are
       identical. this makes sense to me since you are using the same
       hidden output weight matrix and same input hidden matrix value for
       each of the output layers. so, after applying the softmax function,
       the vectors should still be identical right? a post from so(last
       post of the question:
       [172]https://stats.stackexchange.com/questions/194011/how-does-word
       2vecs-skip-gram-model-generate-the-output-vectors) states that all
       c distributions are different, because of the softmax function.
       this doesn   t make sense to me since given the format of softmax,
       each element inside of one of the c output layers would just be
       (for the case of the first element):
       element(1)/(element1+..element10). does the change occur after the
       first error propogation?
       a lecture from stanford also displays the output layer (before
       softmax) to not be identical for each of the c
       windows([173]https://www.youtube.com/watch?v=eribwqs9p38,
       time=39.22). i   m very confused on this as i   ve had many conflicting
       opinions.
       thanks so much for your help!
       [174]reply
          + nss says:
            [175]june 19, 2017 at 11:30 am
            yes, the change occurs after the first error propagation.
            [176]reply
     * [177]selvin says:
       [178]june 21, 2017 at 11:28 am
       very very amazing explaintion   .many things gather about
       yourself   yes realy i enjoy it
       [179]reply
     * james wong says:
       [180]june 23, 2017 at 7:48 am
       hi nss. really great tutorial with respect to id27s, the
       best i   ve seen by far. however there   s still a question baffling me
       all the time. in the cbow algorithm, you point out that    the weight
       between the hidden layer and the output layer is taken as the word
       vector representation of the word   . but under the skip-gram
       section, you then say,    the weights between the input and the
       hidden layer are taken as the word vector representation after
       training   . could you please explain a little bit about what   s the
       difference between the two weight matrices when it comes to word
       embedding representations?
       [181]reply
          + nss says:
            [182]june 24, 2017 at 10:59 am
            it is more of an empirical choice. you can choose either
            weights but generally what people use is the weight matrix
            near to the single word as vector representation. for example
            in cbow, the output is a single word so weight matrix between
            hidden and output is preferred while in skip gram, input word
            is a single word, so the weight matrix between input and
            hidden is preferred.
            [183]reply
     * pizza boy says:
       [184]october 6, 2017 at 8:35 pm
       very good explanation about id97s. also illustration is
       wonderful. however, i want to learn how you can acquire this
       knowledge.      please make more content like this.
       [185]reply
     * [186]yongyong fan says:
       [187]october 10, 2017 at 9:30 am
       hey bro, that   s really an awesome article, very well explained!!!
       the original paper is hard to understand for nlp beginners. thank
       you!!!
       [188]reply
     * tansu says:
       [189]october 22, 2017 at 3:36 pm
       excellent resource. clear and very well organized. i was trying to
       understand the concepts for two days and this is the best one.
       thanks.
       [190]reply
     * sharath chandra says:
       [191]october 25, 2017 at 2:18 am
       thanks for the great intuition tutorial . i have one question
       regarding context window.
       for the sample corpus c =    hey, this is sample corpus using only
       one context word.    in cbow and skip-gram models, where context
       window is 1, the input-outputs you have shown are [hey,
       this],[this, hey], [is, this], [is, sample] but why we skipped
       [this, is]
       is it for any specific reason. please help me if i   m understanding
       the context window wrongly.
       [192]reply
     * [193]fofie says:
       [194]november 2, 2017 at 8:06 pm
       very nice article thanks a lot   ..
       [195]reply
     * veena says:
       [196]november 4, 2017 at 1:07 pm
       very good article.
       [197]reply
     * shahbaz hassan wasti says:
       [198]november 14, 2017 at 11:44 am
       thanks man for this great contribution, really by far the best
       tutorial to lean id97 and related concepts. the amazing thing
       about your explanation is that you have provided a comprehensive
       understanding of the concepts yet in a simplest possible way.
       [199]reply
     * ali says:
       [200]february 21, 2018 at 5:38 pm
       that is a really great article !
       things are simple yet powerful and clear
       [201]reply
     * saul goodman says:
       [202]march 22, 2018 at 12:12 am
       thank you. very minor edit
       in cbow example this, is is missing as the third data point.
       [203]reply
          + faizan shaikh says:
            [204]march 22, 2018 at 6:00 pm
            thanks saul for the feedback. i have updated the article
            [205]reply
     * aakash dubey says:
       [206]may 19, 2018 at 1:30 pm
       hey nss,
       very useful article bro      thank you for sharing it
       [207]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [208]srk       3924
   2    [2.jpg?date=2019-04-05] [209]mark12    3510
   3    [3.jpg?date=2019-04-05] [210]nilabha   3261
   4    [4.jpg?date=2019-04-05] [211]nitish007 3237
   5    [5.jpg?date=2019-04-05] [212]tezdhar   3082
   [213]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [214]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [215]understanding support vector machine algorithm from examples
       (along with code)
     * [216]essentials of machine learning algorithms (with python and r
       codes)
     * [217]a complete tutorial to learn data science with python from
       scratch
     * [218]7 types of regression techniques you should know!
     * [219]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [220]a simple introduction to anova (with applications in excel)
     * [221]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [222]top 5 machine learning github repositories and reddit discussions
   from march 2019

[223]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [224]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[225]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [226]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[227]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [228]16 opencv functions to start your id161 journey (with
   python code)

[229]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [230][ds-finhack.jpg]

   [231][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [232]about us
     * [233]our team
     * [234]career
     * [235]contact us
     * [236]write for us

   [237]about us
   [238]   
   [239]our team
   [240]   
   [241]careers
   [242]   
   [243]contact us

data scientists

     * [244]blog
     * [245]hackathon
     * [246]discussions
     * [247]apply jobs
     * [248]leaderboard

companies

     * [249]post jobs
     * [250]trainings
     * [251]hiring hackathons
     * [252]advertising
     * [253]reach us

   don't have an account? [254]sign up here.

join our community :

   [255]46336 [256]followers
   [257]20224 [258]followers
   [259]followers
   [260]7513 [261]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [262]privacy policy
     * [263]terms of use
     * [264]refund policy

   don't have an account? [265]sign up here

   iframe: [266]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [267](button) join now

   subscribe!

   iframe: [268]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [269](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/deep-learning/
  94. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
  95. https://www.analyticsvidhya.com/blog/category/deep-learning/
  96. https://www.analyticsvidhya.com/blog/category/machine-learning/
  97. https://www.analyticsvidhya.com/blog/category/nlp/
  98. https://www.analyticsvidhya.com/blog/category/python-2/
  99. https://www.analyticsvidhya.com/blog/author/nss/
 100. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
 101. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
 102. http://bit.ly/wevi-online
 103. https://drive.google.com/file/d/0b7xkcwpi5kdynlnuttlss21pqmm/edit
 104. https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&utm_medium=blog
 105. https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&utm_medium=blog
 106. https://trainings.analyticsvidhya.com/courses/course-v1:analyticsvidhya+nlp101+2018_t1/about?utm_source=blog&utm_medium=word-embeddings-count-word2veec
 107. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 108. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/?share=linkedin
 109. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/?share=facebook
 110. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/?share=twitter
 111. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/?share=pocket
 112. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/?share=reddit
 113. https://www.analyticsvidhya.com/blog/tag/artificial-neural-network/
 114. https://www.analyticsvidhya.com/blog/tag/deep-learning/
 115. https://www.analyticsvidhya.com/blog/tag/machine-learning/
 116. https://www.analyticsvidhya.com/blog/tag/natural-language-processing/
 117. https://www.analyticsvidhya.com/blog/tag/nlp/
 118. https://www.analyticsvidhya.com/blog/tag/text-processing/
 119. https://www.analyticsvidhya.com/blog/tag/id97/
 120. https://www.analyticsvidhya.com/blog/author/nss/
 121. https://discuss.analyticsvidhya.com/
 122. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129869
 123. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129869
 124. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130115
 125. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130115
 126. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129871
 127. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129871
 128. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130116
 129. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130116
 130. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129875
 131. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129875
 132. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130117
 133. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130117
 134. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129880
 135. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129880
 136. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130114
 137. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130114
 138. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129896
 139. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129896
 140. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129898
 141. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129898
 142. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129987
 143. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129987
 144. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130113
 145. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130113
 146. http://wwww.datafuture.wordpress.com/
 147. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129992
 148. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-129992
 149. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130112
 150. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130112
 151. http://www.datafuture.wordpress.com/
 152. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130173
 153. https://www.youtube.com/watch?v=eribwqs9p38&index=2&list=pl3fw7lu3i5jsnh1rnuwq_tcylnr7ekre6
 154. https://stackoverflow.com/questions/29381505/why-does-id97-use-2-representations-for-each-word
 155. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130173
 156. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130178
 157. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130178
 158. http://www.datafuture.wordpress.com/
 159. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130738
 160. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130738
 161. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130020
 162. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130020
 163. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130111
 164. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130111
 165. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130024
 166. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130024
 167. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130064
 168. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130064
 169. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130110
 170. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130110
 171. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130560
 172. https://stats.stackexchange.com/questions/194011/how-does-id97s-skip-gram-model-generate-the-output-vectors
 173. https://www.youtube.com/watch?v=eribwqs9p38
 174. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130560
 175. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130728
 176. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130728
 177. http://www.iperidigi.com/
 178. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130863
 179. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130863
 180. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130961
 181. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-130961
 182. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-131033
 183. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-131033
 184. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-138901
 185. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-138901
 186. http://www.fanyeong.com/
 187. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-139194
 188. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-139194
 189. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-140408
 190. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-140408
 191. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-140703
 192. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-140703
 193. http://www.bidiwe.com/
 194. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-141942
 195. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-141942
 196. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-142173
 197. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-142173
 198. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-143663
 199. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-143663
 200. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-151522
 201. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-151522
 202. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-152072
 203. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-152072
 204. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-152085
 205. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-152085
 206. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-153393
 207. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#comment-153393
 208. https://datahack.analyticsvidhya.com/user/profile/srk
 209. https://datahack.analyticsvidhya.com/user/profile/mark12
 210. https://datahack.analyticsvidhya.com/user/profile/nilabha
 211. https://datahack.analyticsvidhya.com/user/profile/nitish007
 212. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 213. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 214. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 215. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 216. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 217. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 218. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 219. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 220. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 221. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 222. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 223. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 224. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 225. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 226. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 227. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 228. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 229. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 230. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 231. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 232. http://www.analyticsvidhya.com/about-me/
 233. https://www.analyticsvidhya.com/about-me/team/
 234. https://www.analyticsvidhya.com/career-analytics-vidhya/
 235. https://www.analyticsvidhya.com/contact/
 236. https://www.analyticsvidhya.com/about-me/write/
 237. http://www.analyticsvidhya.com/about-me/
 238. https://www.analyticsvidhya.com/about-me/team/
 239. https://www.analyticsvidhya.com/about-me/team/
 240. https://www.analyticsvidhya.com/about-me/team/
 241. https://www.analyticsvidhya.com/career-analytics-vidhya/
 242. https://www.analyticsvidhya.com/about-me/team/
 243. https://www.analyticsvidhya.com/contact/
 244. https://www.analyticsvidhya.com/blog
 245. https://datahack.analyticsvidhya.com/
 246. https://discuss.analyticsvidhya.com/
 247. https://www.analyticsvidhya.com/jobs/
 248. https://datahack.analyticsvidhya.com/users/
 249. https://www.analyticsvidhya.com/corporate/
 250. https://trainings.analyticsvidhya.com/
 251. https://datahack.analyticsvidhya.com/
 252. https://www.analyticsvidhya.com/contact/
 253. https://www.analyticsvidhya.com/contact/
 254. https://datahack.analyticsvidhya.com/signup/
 255. https://www.facebook.com/analyticsvidhya/
 256. https://www.facebook.com/analyticsvidhya/
 257. https://twitter.com/analyticsvidhya
 258. https://twitter.com/analyticsvidhya
 259. https://plus.google.com/+analyticsvidhya
 260. https://in.linkedin.com/company/analytics-vidhya
 261. https://in.linkedin.com/company/analytics-vidhya
 262. https://www.analyticsvidhya.com/privacy-policy/
 263. https://www.analyticsvidhya.com/terms/
 264. https://www.analyticsvidhya.com/refund-policy/
 265. https://id.analyticsvidhya.com/accounts/signup/
 266. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 267. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 268. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 269. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 271. https://www.facebook.com/analyticsvidhya
 272. https://twitter.com/analyticsvidhya
 273. https://plus.google.com/+analyticsvidhya/posts
 274. https://in.linkedin.com/company/analytics-vidhya
 275. https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&utm_medium=blog
 276. https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&utm_medium=blog
 277. https://www.analyticsvidhya.com/blog/2017/06/senior-data-analyst-chennai-2-4-years-of-experience/
 278. https://www.analyticsvidhya.com/blog/2017/06/data-science-evangelist-gurgaon-2-to-3-years-of-experience/
 279. https://www.analyticsvidhya.com/blog/author/nss/
 280. https://in.linkedin.com/in/neeraj-singh-sarwan-a84b6965
 281. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 282. https://www.facebook.com/analyticsvidhya/
 283. https://twitter.com/analyticsvidhya
 284. https://plus.google.com/+analyticsvidhya
 285. https://plus.google.com/+analyticsvidhya
 286. https://in.linkedin.com/company/analytics-vidhya
 287. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 288. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 289. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 290. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 291. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 292. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 293. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 294. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 295. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 296. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 297. javascript:void(0);
 298. javascript:void(0);
 299. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 300. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 301. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 302. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 303. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 304. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 305. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 306. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 307. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 308. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2fword-embeddings-count-word2veec%2f&linkname=intuitive%20understanding%20of%20word%20embeddings%3a%20count%20vectors%20to%20id97
 309. javascript:void(0);
 310. javascript:void(0);
