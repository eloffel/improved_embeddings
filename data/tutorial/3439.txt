   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    a comprehensive beginners guide to id202
   for data scientists comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]business analytics [94]a comprehensive beginners guide to
   id202 for data scientists

   [95]business analytics[96]machine learning

a comprehensive beginners guide to id202 for data scientists

   [97]vikas kumar yadav, may 25, 2017

introduction

   one of the most common question we get on analytics vidhya is:

     how much maths do i need to learn to be a data scientist?

   even though the question sounds simple, there is no simple answer to
   the the question. usually, we say that you need to know basic
   descriptive and inferential statistics to start. that is good to start.

   but, once you have covered the basic concepts in machine learning, you
   will need to learn some more math. you need it to understand how these
   algorithms work. what are their limitations and in case they make any
   underlying assumptions. now, there could be a lot of areas to study
   including algebra, calculus, statistics, 3-d geometry etc.

   if you get confused (like i did) and ask experts what should you learn
   at this stage, most of them would suggest / agree that you go ahead
   with id202.

   but, the problem does not stop there. the next challenge is to figure
   out how to learn id202. you can get lost in the detailed
   mathematics and derivation and learning them would not help as much! i
   went through that journey myself and hence decided to write this
   comprehensive guide.

   if you have faced this question about how to learn & what to learn in
   id202     you are at the right place. just follow this guide.


table of contents

    1. motivation     why learn id202?
    2. representation of problems in id202
       2.1. visualising the problem: line
       2.2. complicate the problem
       2.3. planes
    3. matrix
       3.1 terms related to matrix
       3.2 basic operations on matrix
       3.3 representing in matrix form
    4. solving the problem
       4.1. row echelon form
       4.2. inverse of a matrix
       4.2.1  finding inverse
       4.2.2 the power of matrices: solving the equations in one go
       4.2.3 use of inverse in data science
    5. eigenvalues and eigenvectors
       5.1 finding eigenvectors
       5.2 use of eigenvectors in data science: pca algorithm
    6. singular value decomposition of a matrix
    7. end notes


1. motivation     why learn id202?

   i would like to present 4 scenarios to showcase why learning linear
   algebra is important, if you are learning data science and machine
   learning.

scenario 1:

   what do you see when you look at the image above? you most likely said
   flower, leaves -not too difficult. but, if i ask you to write that
   logic so that a computer can do the same for you     it will be a very
   difficult task (to say the least).

   you were able to identify the flower because the human brain has gone
   through million years of evolution. we do not understand what goes in
   the background to be able to tell whether the colour in the picture is
   red or black. we have somehow trained our brains to automatically
   perform this task.

   but making a computer do the same task is not an easy task, and is an
   active area of research in machine learning and computer science in
   general. but before we work on identifying attributes in an image, let
   us ponder over a particular question- how does a machine stores this
   image?

   you probably know that computers of today are designed to process only
   0 and 1. so how can an image such as above with multiple attributes
   like colour be stored in a computer? this is achieved by storing the
   pixel intensities in a construct called matrix. then, this matrix can
   be processed to identify colours etc.

   so any operation which you want to perform on this image would likely
   use id202 and matrices at the back end.


scenario 2:

   if you are somewhat familiar with the data science domain, you might
   have heard about the world    xgboost        an algorithm employed most
   frequently by winners of data science competitions. it stores the
   numeric data in the form of matrix to give predictions. it enables
   xgboost to process data faster and provide more accurate results.
   moreover, not just xgboost but various other algorithms use matrices to
   store and process data.


scenario 3:

   deep learning- the new buzz word in town employs matrices to store
   inputs such as image or speech or text to give a state-of-the-art
   solution to these problems. weights learned by a neural network are
   also stored in matrices. below is a graphical representation of weights
   stored in a matrix.


scenario 4:

   another active area of research in machine learning is dealing with
   text and the most common techniques employed are bag of words, term
   document matrix etc. all these techniques in a very similar manner
   store counts(or something similar) of words in documents and store this
   frequency count in a matrix form to perform tasks like semantic
   analysis, language translation, language generation etc.


   so, now you would understand the importance of id202 in
   machine learning. we have seen image, text or any data, in general,
   employing matrices to store and process data. this should be motivation
   enough to go through the material below to get you started on linear
   algebra. this is a relatively long guide, but it builds linear
   algebra from the ground up.


2. representation of problems in id202

   let   s start with a simple problem. suppose that price of 1 ball & 2 bat
   or 2 ball and 1 bat is 100 units. we need to find price of a ball and a
   bat.

   suppose the price of a bat is rs    x    and the price of a ball is rs    y   .
   values of    x    and    y    can be anything depending on the situation i.e.
      x    and    y    are variables.

   let   s translate this in mathematical form    

   2x + y = 100 ...........(1)

   similarly, for the second condition-

   x + 2y  =  100 ..............(2)

   now, to find the prices of bat and ball, we need the values of    x    and
      y    such that it satisfies both the equations. the basic problem of
   id202 is to find these values of    x    and    y    i.e. the solution
   of a set of linear equations.

   broadly speaking, in id202 data is represented in the form of
   linear equations. these linear equations are in turn represented in the
   form of matrices and vectors.

   the number of variables as well as the number of equations may vary
   depending upon the condition, but the representation is in form of
   matrices and vectors.


2.1 visualise the problem

   it is usually helpful to visualize data problems. let us see if that
   helps in this case.

   linear equations represent flat objects. we will start with the
   simplest one to understand i.e. line. a line corresponding to an
   equation is the set of all the points which satisfy the given equation.
   for example,

   points (50,0) , (0,100), (100/3,100/3) and (30,40) satisfy our
   equation (1) . so these points should lie on the line corresponding to
   our equation (1). similarly, (0,50),(100,0),(100/3,100/3) are some of
   the points that satisfy equation (2).

   now in this situation, we want both of the conditions to be satisfied
   i.e. the point which lies on both the lines.  intuitively, we want to
   find the intersection point of both the lines as shown in the figure
   below.

   let   s solve the problem by elementary algebraic operations like
   addition, subtraction and substitution.

   2x + y = 100 .............(1)

   x + 2y = 100 ..........(2)

   from equation (1)-

   y = (100- x)/2

   put value of y in equation (2)-

   x + 2*(100-x)/2 = 100......(3)

   now, since the equation (3) is an equation in single variable x, it can
   be solved for x and subsequently y.

   that looks simple     let   s go one step further and explore.


2.2 let   s complicate the problem

   now, suppose you are given a set of three conditions with three
   variables each as given below and asked to find the values of all the
   variables. let   s solve the problem and see what happens.

   x+y+z=1.......(4)

   2x+y=1......(5)

   5x+3y+2z=4.......(6)

   from equation (4) we get,

   z=1-x-y....(7)

   substituting value of z in equation (6), we get    

   5x+3y+2(1-x-y)=4

   3x+y=2.....(8)

   now, we can solve equations (8) and (5) as a case of two variables to
   find the values of    x    and    y    in the problem of bat and ball above.
   once we know   x    and    y   , we can use (7)  to find the value of    z   .

   as you might see, adding an extra variable has tremendously increased
   our efforts for finding the solution of the problem. now imagine having
   10 variables and 10 equations. solving 10 equations simultaneously can
   prove to be tedious and time consuming. now dive into data science. we
   have millions of data points. how do you solve those problems?

   we have millions of data points in a real data set. it is going to be a
   nightmare to reach to solutions using the approach mentioned above. and
   imagine if we have to do it again and again and again. it   s going to
   take ages before we can solve this problem. and now if i tell you that
   it   s just one part of the battle, what would you think? so, what should
   we do? should we quit and let it go? definitely no. then?

   matrix is used to solve a large set of linear equations. but before we
   go further and take a look at matrices, let   s visualise the physical
   meaning of our problem. give a little bit of thought to the next topic.
   it directly relates to the usage of matrices.


2.3 planes

   a linear equation in 3 variables represents the set of all points whose
   coordinates satisfy the equations. can you figure out the physical
   object represented by such an equation? try to think of 2 variables at
   a time in any equation and then add the third one. you should figure
   out that it represents a three-dimensional analogue of line.

   basically, a linear equation in three variables represents a plane.
   more technically, a plane is a flat geometric object which extends up
   to infinity.

   as in the case of a line, finding solutions to 3 variables linear
   equation means we want to find the intersection of those planes. now
   can you imagine, in how many ways a set of three planes can intersect?
   let me help you out. there are 4 possible cases    
    1. no intersection at all.
    2. planes intersect in a line.
    3. they can intersect in a plane.
    4. all the three planes intersect at a point.

   can you imagine the number of solutions in each case? try doing this.
   here is an aid picked from wikipedia to help you visualise.

   so, what was the point of having you to visualise all graphs above?

   normal humans like us and most of the super mathematicians can only
   visualise things in 3-dimensions, and having to visualise things in 4
   (or 10000) dimensions is [del: difficult :del]  impossible for mortals.
   so, how do mathematicians deal with higher dimensional data so
   efficiently? they have tricks up their sleeves and matrices is one such
   trick employed by mathematicians to deal with higher dimensional data.

   now let   s proceed with our main focus i.e. matrix.


3. matrix

   matrix is a way of writing similar things together to handle and
   manipulate them as per our requirements easily. in data science, it is
   generally used to store information like weights in an artificial
   neural network while training various algorithms. you will be able to
   understand my point by the end of this article.

   technically, a matrix is a 2-d array of numbers (as far as data science
   is concerned). for example look at the matrix a below.
   1 2 3
   4 5 6
   7 8 9

   generally, rows are denoted by    i    and column are denoted by    j   .  the
   elements are indexed by    i   th row and    j   th column.we denote the matrix
   by some alphabet e.g.  a and its elements by a(ij).

   in above matrix

   a12 =  2

   to reach to the result, go along first row and reach to second column.


3.1 terms related to matrix

   order of matrix     if a matrix has 3 rows and 4 columns, order of the
   matrix is 3*4 i.e. row*column.

   square matrix     the matrix in which the number of rows is equal to the
   number of columns.

   diagonal matrix     a matrix with all the non-diagonal elements equal to
   0 is called a diagonal matrix.

   upper triangular matrix     square matrix with all the elements below
   diagonal equal to 0.

   lower triangular matrix     square matrix with all the elements above the
   diagonal equal to 0.

   scalar matrix     square matrix with all the diagonal elements equal to
   some constant k.

   identity matrix     square matrix with all the diagonal elements equal to
   1 and all the non-diagonal elements equal to 0.

   column matrix      the matrix which consists of only 1 column. sometimes,
   it is used to represent a vector.

   row matrix      a matrix consisting only of row.

   trace     it is the sum of all the diagonal elements of a square matrix.


3.2 basic operations on matrix

   let   s play with matrices and realise the capabilities of matrix
   operations.

   addition     addition of matrices is almost similar to basic arithmetic
   addition. all you need is the order of all the matrices being added
   should be same. this point will become obvious once you will do matrix
   addition by yourself.

   suppose we have 2 matrices    a    and    b    and the resultant matrix after
   the addition is    c   . then

   cij  =   aij + bij

   for example, let   s take two matrices and solve them.

   a      =
   1 0
   2 3

   b    =
   4 -1
   0  5

   then,

   c        =
   5 -1
   2  8

   observe that to get the elements of c matrix, i have added a and b
   element-wise i.e. 1 to 4, 3 to 5 and so on.

   scalar multiplication      multiplication of a matrix with a scalar
   constant is called scalar multiplication. all we have to do in a scalar
   multiplication is to multiply each element of the matrix with the given
   constant.  suppose we have a constant scalar    c    and a matrix    a   .
   then multiplying    c    with    a     gives-

   c[aij] =  [c*aij]

   transposition     transposition simply means interchanging the row and
   column index. for example-

   a[ij]^t= a[ji]

   transpose is used in vectorized implementation of linear and logistic
   regression.

   code in python

   import numpy as np
   import pandas as pd

   #create a 3*3 matrix
   a= np.arange(21,30).reshape(3,3)

   #print the matrix
   a
array([[21, 22, 23],
       [24, 25, 26],
       [27, 28, 29]])

   #take the transpose
   a.transpose()
array([[21, 24, 27],
       [22, 25, 28],
       [23, 26, 29]])

   id127

   id127 is one of the most frequently used operations in
   id202. we will learn to multiply two matrices as well as go
   through its important properties.

   before landing to algorithms, there are a few points to be kept in
   mind.
    1. the multiplication of two matrices of orders i*j and j*k results
       into a matrix of order i*k.  just keep the outer indices in order
       to get the indices of the final matrix.
    2. two matrices will be compatible for multiplication only if the
       number of columns of the first matrix and the number of rows of the
       second one are same.
    3. the third point is that order of multiplication matters.

   don   t worry if you can   t get these points. you will be able to
   understand by the end of this section.

   suppose, we are given two matrices a and b to multiply. i will write
   the final expression first and then will explain the steps.

   i have picked this image from wikipedia for your better understanding.

   in the first illustration, we know that the order of the resulting
   matrix should be 3*3. so first of all, create a matrix of order 3*3. to
   determine (ab)ij , multiply each element of    i   th row of a with    j   th
   column of b one at a time and add all the terms. to help you understand
   element-wise multiplication, take a look at the code below.

   import numpy as np

   a=np.arange(21,30).reshape(3,3)
   b=np.arange(31,40).reshape(3,3)

   a.dot(b)
ab= array([[2250, 2316, 2382],
       [2556, 2631, 2706],
       [2862, 2946, 3030]]) b.dot(a)

ba= array([[2310, 2406, 2502],
       [2526, 2631, 2736],
       [2742, 2856, 2970]])

   so, how did we get 2250 as first element of ab matrix?
   2250=21*31+22*34+23*37. similarly, for other elements.

   notice the difference between ab and ba.

   properties of id127
    1. id127 is associative provided the given matrices
       are compatible for multiplication i.e.

   abc =  (ab)c = a(bc)

   import numpy as np
   a=np.arange(21,30).reshape(3,3)
   b=np.arange(31,40).reshape(3,3)
   c=np.arange(41,50).reshape(3,3)

   temp1=(a.dot(b)).dot(c)
array([[306108, 313056, 320004],
       [347742, 355635, 363528],
       [389376, 398214, 407052]])

   temp2=a.dot((b.dot(c)))
array([[306108, 313056, 320004],
       [347742, 355635, 363528],
       [389376, 398214, 407052]])

   2. id127 is not commutative i.e. ab and  ba are not
   equal. we have verified this result above.

   id127 is used in linear and id28 when we
   calculate the value of output variable by parameterized vector method.
   as we have learned the basics of matrices, it   s time to apply them.


3.3 representing equations in matrix form

   let me do something exciting for you.  take help of pen and paper and
   try to find the value of the id127 shown below


   it can be verified very easily that the expression contains our three
   equations. we will name our matrices as    a   ,    x    and    z   .

   it explicitly verifies that we can write our equations together in one
   place as

   ax   = z

   next step has to be solution methods.we will go through two methods to
   find the solution.


4. solving the problem

   now, we will look in detail the two methods to solve matrix equations.
    1. row echelon form
    2. inverse of a matrix

4.1 row echelon form

   now you have visualised what an equation in 3 variables represents and
   had a warm up on matrix operations. let   s find the solution of the set
   of equations given to us to understand our first method of interest and
   explore it later in detail.

   i have already illustrated that solving the equations by substitution
   method can prove to be tedious and time taking. our first method
   introduces you with a neater and more systematic method to accomplish
   the job in which, we manipulate our original equations systematically
   to find the solution.  but what are those valid manipulations? are
   there any qualifying criteria they have to fulfil? well, yes. there are
   two conditions which have to be fulfilled by any manipulation to be
   valid.
    1. manipulation should preserve the solution i.e. solution should not
       be altered on imposing the manipulation.
    2. manipulation should be reversible.

   so, what are those manipulations?
    1. we can swap the order of equations.
    2. we can multiply both sides of equations by any non-zero constant
          c   .
    3. we can multiply an equation by any non-zero constant and then add
       to other equation.

   these points will become more clear once you go through the algorithm
   and practice it. the basic idea is to clear variables in successive
   equations and form an upper triangular matrix. equipped with
   prerequisites, let   s get started. but before that, it is strongly
   recommended to go through this [98]link for better understanding.


   i will solve our original problem as an illustration. let   s do it in
   steps.
    1. make an augmented matrix from the matrix    a    and    z   .

   what i have done is i have just concatenated the two matrices. the
   augmented matrix simply tells that the elements in a row are
   coefficients of    x   ,    y    and    z    and last element in the row is
   right-hand side of the equation.
    2. multiply row (1) with 2 and subtract from row (2). similarly,
       multiply equation 1 with 5 and subtract from row (3).

    3. in order to make an upper triangular matrix, multiply row (2) by 2
       and then subtract from row (3).

   remember to make each leading coefficient, also called pivot equal to
   1, by suitable manipulations; in this case multiplying row 2 with -1.
   also, if a row consists of 0 only, it should be below each row which
   consists of a non-zero entry. the resulting form of matrix is called
   row echelon form. notice that the planes corresponding to new equations
   formed by manipulation are not equivalent. doing these operations, we
   are just conserving the solution of equations and trying to reach to
   it.
    4. now we have simplified our job, let   s retrieve the modified
       equations. we will start from the simplest i.e. the one with the
       minimum number of remaining variables. if you follow the
       illustrated procedure, you will find that last equation comes to be
       the simplest one.

   0*x+0*y+1*z=1
   z=1

   now retrieve equation (2) and put the value of    z    in it to find    y   .
   do the same for equation (1).

   isn   t it pretty simple and clean?

   let   s ponder over another point. will we always be able to make an
   upper triangular matrix which gives a unique solution? are there
   different cases possible? recall that planes can intersect in multiple
   ways. take your time to figure it out and then proceed further.

   different possible cases-
    1. it   s possible that we get a unique solution as illustrated in above
       example. it indicates that all the three planes intersect in a
       point.
    2. we can get a case like shown below

   note that in last equation, 0=0 which is always true but it seems like
   we have got only 2 equations. one of the equations is redundant. in
   many cases, it   s also possible that the number of redundant equations
   is more than one. in this case, the number of solutions is infinite.
    3. there is another case where echelon matrix looks as shown below

   let   s retrieve the last equation.

   0*x+0*y+0*z=4

   0=4

   is it possible? very clear cut intuition is no. but, does this signify
   something? it   s analogous to saying that it is impossible to find a
   solution and indeed, it is true. we can   t find a solution for such a
   set of equations. can you think what is happening actually in terms of
   planes? go back to the section where we saw planes intersecting and
   find it out.

   note that this method is efficient for a set of 5-6 equations. although
   the method is quite simple, if equation set gets larger, the number of
   times you have to manipulate the equations becomes enormously high and
   the method becomes inefficient.

   rank of a matrix     rank of a matrix is equal to the maximum number of
   linearly independent row vectors in a matrix.

   a set of vectors is linearly dependent if we can express at least one
   of the vectors as a linear combination of remaining vectors in the set.


4.2 inverse of a matrix

   for solving a large number of equations in one go, the inverse is used.
   don   t panic if you are not familiar with the inverse. we will do a good
   amount of work on all the required concepts. let   s start with a few
   terms and operations.

   determinant of a matrix     the concept of determinant is applicable to
   square matrices only. i will lead you to the generalised expression of
   determinant in steps. to start with, let   s take a 2*2 matrix  a.

   for now, just focus on 2*2 matrix. the expression of determinant of the
   matrix a will be:

   det(a) =a*d-b*c

   note that det(a) is a standard notation for determinant. notice that
   all you have to do to find determinant in this case is to multiply
   diagonal elements together and put a positive or negative sign before
   them. for determining the sign, sum the indices of a particular
   element. if the sum is an even number, put a positive sign before the
   multiplication and if the sum is odd, put a negative sign.  for
   example, the sum of indices of element    a11    is 2. similarly the sum of
   indices of element    d    is 4. so we put a positive sign before the first
   term in the expression.  do the same thing for the second term
   yourself.

   now take a 3*3 matrix    b    and find its determinant.

   i am writing the expression first and then will explain the procedure
   step by step.

   each term consists of two parts basically i.e. a submatrix and a
   coefficient. first of all, pick a constant. observe that coefficients
   are picked from the first row only. to start with, i have picked the
   first element of the first row. you can start wherever you want. once
   you have picked the coefficient, just delete all the elements in the
   row and column corresponding to the chosen coefficient. next, make a
   matrix of the remaining elements; each one in its original position
   after deleting the row and column and find the determinant of this
   submatrix . repeat the same procedure for each element in the first
   row. now, for determining the sign of the terms, just add the indices
   of the coefficient element. if it is even, put a positive sign and if
   odd, put a negative sign. finally, add all the terms to find the
   determinant. now, let   s take a higher order matrix    c    and generalise
   the concept.


   try to relate the expression to what we have done already and figure
   out the final expression.

   code in python

   import numpy as np
   #create a 4*4 matrix
   arr = np.arange(100,116).reshape(4,4)
array([[100, 101, 102, 103],
       [104, 105, 106, 107],
       [108, 109, 110, 111],
       [112, 113, 114, 115]])

   #find the determinant
   np.linalg.det(arr)
-2.9582283945788078e-31

   minor of a matrix

   let   s take a square matrix a. then minor corresponding to an element
   a(ij)  is the determinant of the submatrix formed by deleting the    i   th
    row and    j   th column of the matrix. hope you can relate with what i
   have explained already in the determinant section. let   s take an
   example.

   to find the minor corresponding to element a11, delete first row and
   first column to find the submatrix.

   now find the determinant of this matrix as explained already. if you
   calculate the determinant of this matrix, you should get 4. if we
   denote minor by m11, then

   m11 = 4

   similarly, you can do for other elements.

   cofactor of a matrix

   in the above discussion of minors, if we consider signs of minor terms,
   the resultant we get is called cofactor of a matrix. to assign the
   sign, just sum the indices of the corresponding element. if it turns
   out to be even, assign positive sign. else assign negative. let   s take
   above illustration as an example. if we add the indices i.e. 1+1=2, so
   we should put a positive sign. let   s say it c11. then

   c11 = 4

   you should find cofactors corresponding to other elements by yourself
   for a good amount of practice.

   cofactor matrix

   find the cofactor corresponding to each element. now in the original
   matrix, replace the original element by the corresponding cofactor. the
   matrix thus found is called the cofactor matrix corresponding to the
   original matrix.

   for example, let   s take our matrix a. if you have found out the
   cofactors corresponding to each element, just put them in a matrix
   according to rule stated above. if you have done it right, you should
   get cofactor matrix

   adjoint of a matrix     in our journey to find inverse, we are almost at
   the end. just keep hold of the article for a couple of minutes and we
   will be there. so, next we will find the adjoint of a matrix.

   suppose we have to find the adjoint of a matrix a. we will do it in two
   steps.

   in step 1, find the cofactor matrix of a.

   in step 2, just transpose the cofactor matrix.

   the resulting matrix is the adjoint of the original matrix. for
   illustration, lets find the adjoint of our matrix a. we already have
   cofactor matrix c. transpose of cofactor matrix should be

   finally, in the next section, we will find the inverse.


4.2.1 finding inverse of a matrix

   do you remember the concept of the inverse of a number in elementary
   algebra? well, if there exist two numbers such that upon their
   multiplication gives 1 then those two numbers are called inverse of
   each other. similarly in id202, if there exist two matrices
   such that their multiplication yields an identity matrix then the
   matrices are called inverse of each other. if you can not get what i
   explained, just go with the article. it will come intuitively to you.
   the best way to learning is learning by doing. so, let   s jump straight
   to the algorithm for finding the inverse of a matrix a. again, we will
   do it in two steps.

   step 1: find out the adjoint of the matrix a by the procedure explained
   in previous sections.

   step2: multiply the adjoint matrix by the inverse of determinant of the
   matrix a. the resulting matrix is the inverse of a.

   for example, let   s take our matrix a and find it   s inverse. we already
   have the adjoint matrix. determinant of matrix a comes to be -2. so,
   its inverse will be

   now suppose that the determinant comes out to be 0. what happens when
   we invert the determinant i.e. 0?  does it make any sense?  it
   indicates clearly that we can   t find the inverse of such a matrix.
   hence, this matrix is non-invertible. more technically, this type of
   matrix is called a singular matrix.

   keep in mind that the resultant of multiplication of a matrix and its
   inverse is an identity matrix. this property is going to be used
   extensively in equation solving.

   inverse is used in finding parameter vector corresponding to minimum
   cost function in id75.


4.2.2 power of matrices

   what happens when we multiply a number by 1? obviously it remains the
   same. the same is applicable for an identity matrix i.e. if we multiply
   a matrix with an identity matrix of the same order, it remains same.

   lets solve our original problem with the help of matrices. our original
   problem represented in matrix was as shown below

   ax = z i.e.

   what happens when we pre multiply both the sides with inverse of
   coefficient matrix i.e. a. lets find out by doing.

   a^-1 a x =a^-1 z

   we can manipulate it as,

   (a^-1 a) x = a ^-1z

   but we know multiply a matrix with its inverse gives an identity
   matrix. so,

   ix =  a ^-1z

   where i is the identity matrix of the corresponding order.

   if you observe keenly, we have already reached to the solution.
   multiplying identity matrix to x does not change it. so the equation
   becomes

   x = a ^-1z

   for solving the equation, we have to just find the inverse. it can be
   very easily done by executing a few lines of codes. isn   t it a really
   powerful method?

   code for inverse in python

   import numpy as np
   #create an array arr1
   arr1 = np.arange(5,21).reshape(4,4)

   #find the inverse
   np.linalg.inv(arr1)


4.2.3 application of inverse in data science

   inverse is used to calculate parameter vector by normal equation in
   linear equation. here is an illustration. suppose we are given a data
   set as shown below-
   team league year rs  ra  w  obp   slg   ba    g   oobp  oslg
   ari  nl     2012 734 688 81 0.328 0.418 0.259 162 0.317 0.415
   atl  nl     2012 700 600 94  0.32 0.389 0.247 162 0.306 0.378
   bal  al     2012 712 705 93 0.311 0.417 0.247 162 0.315 0.403
   bos  al     2012 734 806 69 0.315 0.415  0.26 162 0.331 0.428
   chc  nl     2012 613 759 61 0.302 0.378  0.24 162 0.335 0.424
   chw  al     2012 748 676 85 0.318 0.422 0.255 162 0.319 0.405
   cin  nl     2012 669 588 97 0.315 0.411 0.251 162 0.305  0.39
   cle  al     2012 667 845 68 0.324 0.381 0.251 162 0.336  0.43
   col  nl     2012 758 890 64  0.33 0.436 0.274 162 0.357  0.47
   det  al     2012 726 670 88 0.335 0.422 0.268 162 0.314 0.402
   hou  nl     2012 583 794 55 0.302 0.371 0.236 162 0.337 0.427
   kcr  al     2012 676 746 72 0.317   0.4 0.265 162 0.339 0.423
   laa  al     2012 767 699 89 0.332 0.433 0.274 162  0.31 0.403
   lad  nl     2012 637 597 86 0.317 0.374 0.252 162  0.31 0.364


   it describes the different variables of different baseball teams to
   predict whether it makes to playoffs or not. but for right now to make
   it a regression problem, suppose we are interested in predicting oobp
   from the rest of the variables. so,    oobp    is our target variable. to
   solve this problem using id75, we have to find parameter
   vector. if you are familiar with normal equation method, you should
   have the idea that to do it, we need to make use of matrices. lets
   proceed further and denote our independent variables below as matrix
      x   .this data is a part of a data set taken from analytics edge. here
   is the [99]link for the data set.

   so,  x=
   734 688 81 0.328 0.418 0.259
   700 600 94 0.32  0.389 0.247
   712 705 93 0.311 0.417 0.247
   734 806 69 0.315 0.415 0.26
   613 759 61 0.302 0.378 0.24
   748 676 85 0.318 0.422 0.255
   669 588 97 0.315 0.411 0.251
   667 845 68 0.324 0.381 0.251
   758 890 64 0.33  0.436 0.274
   726 670 88 0.335 0.422 0.268
   583 794 55 0.302 0.371 0.236
   676 746 72 0.317 0.4   0.265
   767 699 89 0.332 0.433 0.274
   637 597 86 0.317 0.374 0.252


   to find the final parameter vector(  ) assuming our initial function is
   parameterised by    and x , all you have to do is to find the inverse of
   (x^t x) which can be accomplished very easily by using code as shown
   below.

   first of all, let me make the id75 formulation easier for
   you to comprehend.

   f [  ] (x)=   ^t x, where    is the parameter we wish to calculate and x
   is the column vector of features or independent variables.

   import pandas as pd
   import numpy as np

   #you don   t need to bother about the following. it just #transforms the
   data from original source into matrix

   df = pd.read_csv( "../baseball.csv   )
   df1 = df.head(14)

   # we are just taking 6 features to calculate   .
   x = df1[[   rs   ,    ra   ,    w   ,    obp   ,'slg','ba']]
   y=df1['oobp']

   #converting x to matrix
   x = np.asmatrix(x)

   #taking transpose of x and assigning it to x
   x= np.transpose(x)

   #finding multiplication
   t= x.dot(x)

   #inverse of t - provided it is invertible otherwise we use
   pseudoinverse
   inv=np.linalg.inv(t)

   #calculating   
   theta=(inv.dot(x.t)).dot(y)

   imagine if you had to solve this set of equations without using linear
   algebra. let me remind you that this data set is less than even 1% of
   original date set. now imagine if you had to find parameter vector
   without using id202. it would have taken a lots of time and
   effort and could be even impossible to solve sometimes.

   one major drawback of normal equation method when the number of
   features is large is that it is computationally very costly. the reason
   is that if there are    n    features, the matrix (x^t x) comes to be the
   order n*n and its solution costs time of order o( n*n*n). generally,
   normal equation method is applied when a number of features is of the
   order of 1000 or 10,000. data sets with a larger number of features are
   handled with the help another method called id119.

   next, we will go through another advanced concept of id202
   called eigenvectors.


5. eigenvalues and eigenvectors

   eigenvectors find a lot of applications in different domains like
   id161, physics and machine learning. if you have studied
   machine learning and are familiar with principal component analysis
   algorithm, you must know how important the algorithm is when handling a
   large data set. have you ever wondered what is going on behind that
   algorithm? actually, the concept of eigenvectors is the backbone of
   this algorithm. let us explore eigen vectors and eigen values for a
   better understanding of it.

   let   s multiply a 2-dimensional vector with a 2*2 matrix and see what
   happens.

   this operation on a vector is called linear transformation.  notice
   that the directions of input and output vectors are different. note
   that the column matrix denotes a vector here.

   i will illustrate my point with the help of a picture as shown below.


   in the above picture, there are two types of vectors coloured in red
   and yellow and the picture is showing the change in vectors after a
   linear transformation. note that on applying a linear transformation to
   yellow coloured vector, its direction changes but the direction of the
   red coloured vector doesn   t change even after applying the linear
   transformation. the vector coloured in red is an example of
   eigenvector.

   precisely, for a particular matrix; vectors whose direction remains
   unchanged even after applying linear transformation with the matrix are
   called eigenvectors for that particular matrix. remember that the
   concept of eigen values and vectors is applicable to square matrices
   only. another thing to know is that i have taken a case of
   two-dimensional vectors but the concept of eigenvectors is applicable
   to a space of any number of dimensions.


5.1 how to find eigenvectors of a matrix?

   suppose we have a matrix a and an eigenvector    x    corresponding to the
   matrix. as explained already, after multiplication with matrix the
   direction of    x    doesn   t change. only change in magnitude is permitted.
   let us write it as an equation-

   ax = cx

   (a-c)x = 0        .(1)

   please note that in the term (a-c),    c    denotes an identity matrix of
   the order equal to    a    multiplied by a scalar    c   

   we have two unknowns    c    and    x    and only one equation. can you think
   of a trick to solve this equation?

   in equation (1), if we put the vector    x    as zero vector, it makes no
   sense. hence, the only choice is that (a-c) is a singular matrix. and
   singular matrix has a property that its determinant equals to 0. we
   will use this property to find the value of    c   .

   det(a-c) = 0

   once you find the determinant of the matrix (a-c) and equate to 0, you
   will get an equation in    c    of the order depending upon the given
   matrix a. all you have to do is to find the solution of the equation.
   suppose that we find solutions as    c1    ,    c2    and so on. put    c1    in
   equation (1) and find the vector    x1    corresponding to    c1   . the vector
      x1    that you just found is an eigenvector of a. now, repeat the same
   procedure with    c2   ,    c3    and so on.

   code for finding eigenvectors in python

   import  numpy as np

   #create an array
   arr = np.arange(1,10).reshape(3,3)

   #finding the eigenvalue and eigenvectors of arr
   np.linalg.eig(arr)


5.2 use of eigenvectors in data science

   the concept of eigenvectors is applied in a machine learning algorithm
   principal component analysis. suppose you have a data with a large
   number of features i.e. it has a very high dimensionality. it is
   possible that there are redundant features in that data. apart from
   this, a large number of features will cause reduced efficiency and more
   disk space. what pca does is that it craps some of lesser important
   features. but how to determine those features? here, eigenvectors come
   to our rescue.let   s go through the algorithm of pca. suppose we have an
      n    dimensional data and we want to reduce it to    k    dimensions. we
   will do it in steps.

   step 1: data is mean normalised and feature scaled.

   step 2: we find out the covariance matrix of our data set.

   now we want to reduce the number of features i.e. dimensions. but
   cutting off features means loss of information. we want to minimise the
   loss of information i.e. we want to keep the maximum variance. so, we
   want to find out the directions in which variance is maximum. we will
   find these directions in the next step.

   step 3: we find out the eigenvectors of the covariance matrix. you
   don   t need to bother much about covariance matrix. it   s an advanced
   concept of statistics.  as we have data in    n    dimensions, we will find
      n    eigenvectors corresponding to    n    eigenvalues.

   step 4: we will select    k    eigenvectors corresponding to the    k   
   largest eigenvalues and will form a matrix in which each eigenvector
   will constitute a column. we will call this matrix as u.

   now it   s the time to find the reduced data points. suppose you want to
   reduce a data point    a    in the data set to    k    dimensions.  to do so,
   you have to just transpose the matrix u and multiply it with the vector
      a   . you will get the required vector in    k    dimensions.

   once we are done with eigenvectors, let   s talk about another advanced
   and highly useful concept in id202 called singular value
   decomposition, popularly called as svd. its complete understanding
   needs  a rigorous study of id202.  in fact, svd is a complete
   blog in itself. we will come up with another blog completely devoted to
   svd. stay tuned for a better experience. for now, i will just give you
   a glimpse of how svd helps in data science.


6. singular value decomposition

   suppose you are given a feature matrix a. as suggested by name, what we
   do is we decompose our matrix a in three constituent matrices for a
   special purpose.  sometimes, it is also said that svd is some sort of
   generalisation of eigen value decomposition.  i will not go into its
   mathematics for the reason already explained and will stick to our plan
   i.e. use of svd in data science.

   svd is used to remove the redundant features in a data set. suppose you
   have a data set which comprises of 1000 features. definitely, any real
   data set with such a large number of features is bound to contain
   redundant features. if you have run ml, you should be familiar with the
   fact that redundant features cause a lots of problems in running
   machine learning algorithms. also, running an algorithm on the original
   data set will be time inefficient and will require a lot of memory. so,
   what should you to do handle such a problem? do we have a choice?  can
   we omit some features? will it lead to significant amount of
   information loss? will we be able to get an efficient enough algorithm
   even after omitting the rows? i will answer these questions with the
   help of an illustration.

   look at the pictures shown below taken from this [100]link

   we can convert this tiger into black and white and can think of it as a
   matrix whose elements represent the pixel intensity as relevant
   location. in simpler words, the matrix contains information about the
   intensity of pixels of the image in the form of rows and columns. but,
   is it necessary to have all the columns in the intensity matrix? will
   we be able to represent the tiger with a lesser amount of information?
   the next picture will clarify my point. in this picture, different
   images are shown corresponding to different ranks with different
   resolution. for now, just assume that higher rank implies the larger
   amount of information about pixel intensity. the image is taken from
   this [101]link

   it is clear that we can reach to a pretty well image with 20 or 30
   ranks instead of 100 or 200 ranks and that   s what we want to do in a
   case of highly redundant data. what i want to convey is that to get a
   reasonable hypothesis, we don   t have to retain all the information
   present in the original dataset. even, some of the features cause a
   problem in reaching a solution to the best algorithm. for the example,
   presence of redundant features causes multi co-linearity in linear
   regression. also, some features are not significant for our model.
   omitting these features helps to find a better fit of algorithm along
   with time efficiency and lesser disk space. singular value
   decomposition is used to get rid of the redundant features present in
   our data.


7. end notes

   if you have made this far     give yourself a pat at the back. we have
   covered different aspects of id202 in this article. i have
   tried to give sufficient amount of information as well as keep the flow
   such that everybody can understand the concepts and be able to do
   necessary calculations. still, if you get stuck somewhere, feel free to
   comment below or post on [102]discussion portal.


[103]learn, [104]compete, hack and [105]get hired!

   you can also read this article on analytics vidhya's android app
   [106]get it on google play

share this:

     * [107]click to share on linkedin (opens in new window)
     * [108]click to share on facebook (opens in new window)
     * [109]click to share on twitter (opens in new window)
     * [110]click to share on pocket (opens in new window)
     * [111]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [112]eigenvalues, [113]eigenvectors, [114]inverse of matrix,
   [115]id202, [116]matrices, [117]matrix, [118]matrix methods,
   [119]matrix operations, [120]svd
   next article

business intelligence analyst- bangalore (3-6 years of experience)

   previous article

business analyst/senior business analyst     data analytics     bfsi gurgaon (2-5
years of experience)

[121]vikas kumar yadav

   i am an undergraduate student majoring in mechanical engineering at
   indian institute of technology, roorkee. i am an aspiring data
   scientist and a machine learning enthusiast. i am really passionate
   about changing the world by using artificial intelligence.

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [122]discussion portal to get your queries resolved

30 comments

     * nikhil says:
       [123]may 25, 2017 at 3:38 pm
       great article vikas! can you please write an article on how svd is
       used in id126!
       [124]reply
          + vikas kumar yadav says:
            [125]may 25, 2017 at 8:31 pm
            thanks nikhil ! we will be coming up with a article on svd in
            near future. stay tuned.
            [126]reply
     * aanish singla says:
       [127]may 25, 2017 at 5:15 pm
       excellent article. it improved my understanding of solving linear
       equations using matrix by a great extent.
       [128]reply
          + vikas kumar yadav says:
            [129]may 25, 2017 at 8:32 pm
            thanks aanish!
            [130]reply
     * [131]yuriy zaletskyy says:
       [132]may 25, 2017 at 10:18 pm
       good job vikas
       [133]reply
          + vikas kumar yadav says:
            [134]may 26, 2017 at 10:23 am
            thanks yuriy
            !
            [135]reply
     * sarvesh shukla says:
       [136]may 26, 2017 at 2:30 pm
       great job bro    
       according to my experience id202 is really a terrifying
       subject..hahaha
       [137]reply
          + vikas kumar yadav says:
            [138]may 29, 2017 at 10:38 am
            thank you sarvesh!
            [139]reply
     * anand yadav says:
       [140]may 26, 2017 at 7:48 pm
       good job!!
       [141]reply
          + vikas kumar yadav says:
            [142]may 29, 2017 at 10:36 am
            thanks anand!
            [143]reply
     * artem says:
       [144]may 26, 2017 at 9:53 pm
       hi vikas,
       thank you so much for your time and effort. it   s really nice
       article. very helpful, keep it up, please!
       [145]reply
          + vikas kumar yadav says:
            [146]may 29, 2017 at 10:36 am
            thanks artem!
            [147]reply
     * sourabh says:
       [148]may 27, 2017 at 3:16 pm
       great post vikas.
       two clarifications:
       i) does    x    should also have a column of 1 to include intercept
       coefficient (theta_zero)?
       ii) it seems that the equation for theta should be
       theta=(inv.dot(x.t)).dot(x).dot(y) instead of
       theta=(inv.dot(x.t)).dot(y).
       kindly, let me know if i misunderstood something.
       [149]reply
          + sourabh says:
            [150]may 27, 2017 at 3:23 pm
            sorry. i think i did not write the correct equation as well.
            kindly ignore my second clarification in previous post.
            i meant that the equation for theta should be
            theta=(inv.dot(x)).dot(y) instead of
            theta=(inv.dot(x.t)).dot(y).
            is there a typo in your equation.?    x.t    in your equation
            should be replaced with    x   .
            [151]reply
               o vikas kumar yadav says:
                 [152]june 3, 2017 at 11:05 am
                 @sourabh
                 thanks for commenting. there is no typo in the equation.
                 the equation for theta is
                 (x^{t}x)^{-1}x^{t}y
                 the part where you have doubt just calculates the x[t]y.
                 hope this helps.
                 [153]reply
     * sahar says:
       [154]may 28, 2017 at 11:07 pm
       thank you, vikas. it was an easy read. looking forward to reading
       your article on svd.     
       [155]reply
          + vikas kumar yadav says:
            [156]may 29, 2017 at 10:35 am
            thank you sahar!
            [157]reply
     * victor says:
       [158]may 30, 2017 at 4:01 am
       thank you for the blog. it   s very well written and i can follow it
       until i am lost at stage 4.2.3. i don   t understand why we need the
       inverse of (xt x). from your previous section, should it be just
       the inverse of x? just like you can calculate parameter victor= a
       -1z in the previous section. could you clarify a bit?
       also, i wonder why don   t we use row echelon form? the amount of
       calculation seems to be much less than inverse the matrix.
       [159]reply
          + neerja says:
            [160]june 4, 2017 at 8:47 am
            i have a similar question about xtx too. the blog is easy to
            understand. it   s very helpful.
            [161]reply
     * praneeth says:
       [162]may 30, 2017 at 9:35 am
       great post vikas..
       just a small correction:
       2x + y = 100             .(1)
       x + 2y = 100          .(2)
       from equation (1)-
       y = (100- x)/2
       put value of y in equation (2)-
       x + 2*(100-x)/2 = 100      (3)
       here you said from eq(1), but that is from eq(2). and then it is
       substituted again in eq(2). i agree that the approach is correct.
       [163]reply
          + vikas kumar yadav says:
            [164]may 30, 2017 at 9:45 am
            yes! you got it right. and thanks for bringing it to notice.
            [165]reply
     * anup says:
       [166]june 11, 2017 at 12:20 am
       thanks! your article explains lot of important concepts in data
       science
       [167]reply
     * upendra says:
       [168]july 4, 2017 at 7:24 pm
       thank you for sharing ..and it is very informative    
       [169]reply
     * shashwat says:
       [170]august 25, 2017 at 12:38 pm
       suppose that price of 1 ball & 2 bat or 2 bat and 1 ball is 100
       units.
       this should be suppose that price of 1 ball & 2 bat or 2 ball and 1
       bat is 100 units.
       [171]reply
          + vikas kumar yadav says:
            [172]august 25, 2017 at 12:54 pm
            oh yes   you got it right! and thanks for pointing it out.
            [173]reply
     * bakhtawar says:
       [174]april 27, 2018 at 5:58 pm
       how this become 2250?
       2250=21*31+22*34+23*37
       when i multiply and plus this become 886446
       [175]reply
          + aishwarya singh says:
            [176]may 7, 2018 at 1:44 pm
            hi bakhtawar,
            here is how you can solve it:
            (21*31) + (22*34) + (23*37)
            =651+748+814
            =2,250
            [177]reply
     * rajeev says:
       [178]may 5, 2018 at 1:50 am
       well written to start and made it easy on how to proceed,,,,        
       [179]reply
     * amit kumar yadav says:
       [180]may 7, 2018 at 11:52 am
       really nice! good job
       [181]reply
          + faizan shaikh says:
            [182]may 7, 2018 at 1:47 pm
            thanks amit!
            [183]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [184]srk       3924
   2    [2.jpg?date=2019-04-05] [185]mark12    3510
   3    [3.jpg?date=2019-04-05] [186]nilabha   3261
   4    [4.jpg?date=2019-04-05] [187]nitish007 3237
   5    [5.jpg?date=2019-04-05] [188]tezdhar   3082
   [189]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [190]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [191]understanding support vector machine algorithm from examples
       (along with code)
     * [192]essentials of machine learning algorithms (with python and r
       codes)
     * [193]a complete tutorial to learn data science with python from
       scratch
     * [194]7 types of regression techniques you should know!
     * [195]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [196]a simple introduction to anova (with applications in excel)
     * [197]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [198]top 5 machine learning github repositories and reddit discussions
   from march 2019

[199]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [200]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[201]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [202]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[203]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [204]16 opencv functions to start your id161 journey (with
   python code)

[205]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [206][ds-finhack.jpg]

   [207][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [208]about us
     * [209]our team
     * [210]career
     * [211]contact us
     * [212]write for us

   [213]about us
   [214]   
   [215]our team
   [216]   
   [217]careers
   [218]   
   [219]contact us

data scientists

     * [220]blog
     * [221]hackathon
     * [222]discussions
     * [223]apply jobs
     * [224]leaderboard

companies

     * [225]post jobs
     * [226]trainings
     * [227]hiring hackathons
     * [228]advertising
     * [229]reach us

   don't have an account? [230]sign up here.

join our community :

   [231]46336 [232]followers
   [233]20224 [234]followers
   [235]followers
   [236]7513 [237]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [238]privacy policy
     * [239]terms of use
     * [240]refund policy

   don't have an account? [241]sign up here

   iframe: [242]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [243](button) join now

   subscribe!

   iframe: [244]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [245](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/business-analytics/
  94. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/
  95. https://www.analyticsvidhya.com/blog/category/business-analytics/
  96. https://www.analyticsvidhya.com/blog/category/machine-learning/
  97. https://www.analyticsvidhya.com/blog/author/vikas_10/
  98. https://www.youtube.com/watch?v=0-gaihnicmo&index=17&list=plawxtw4syaplh16ry8kgdwcimzpxcncx_
  99. https://d37djvu3ytnwxt.cloudfront.net/assets/courseware/v1/dfb1bb5463c388fb167745888e3a6dd9/asset-v1:mitx+15.071x_3+1t2016+type@asset+block/baseball.csv
 100. http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/images/tiger.jpg
 101. http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/images/tigers.png
 102. https://discuss.analyticsvidhya.com/
 103. https://www.analyticsvidhya.com/blog
 104. https://datahack.analyticsvidhya.com/
 105. https://www.analyticsvidhya.com/jobs/#/user/
 106. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 107. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/?share=linkedin
 108. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/?share=facebook
 109. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/?share=twitter
 110. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/?share=pocket
 111. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/?share=reddit
 112. https://www.analyticsvidhya.com/blog/tag/eigenvalues/
 113. https://www.analyticsvidhya.com/blog/tag/eigenvectors/
 114. https://www.analyticsvidhya.com/blog/tag/inverse-of-matrix/
 115. https://www.analyticsvidhya.com/blog/tag/linear-algebra/
 116. https://www.analyticsvidhya.com/blog/tag/matrices/
 117. https://www.analyticsvidhya.com/blog/tag/matrix/
 118. https://www.analyticsvidhya.com/blog/tag/matrix-methods/
 119. https://www.analyticsvidhya.com/blog/tag/matrix-operations/
 120. https://www.analyticsvidhya.com/blog/tag/svd/
 121. https://www.analyticsvidhya.com/blog/author/vikas_10/
 122. https://discuss.analyticsvidhya.com/
 123. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129194
 124. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129194
 125. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129216
 126. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129216
 127. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129204
 128. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129204
 129. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129218
 130. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129218
 131. http://blog.zaletskyy.com/
 132. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129230
 133. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129230
 134. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129251
 135. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129251
 136. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129259
 137. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129259
 138. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129394
 139. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129394
 140. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129273
 141. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129273
 142. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129392
 143. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129392
 144. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129278
 145. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129278
 146. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129393
 147. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129393
 148. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129311
 149. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129311
 150. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129312
 151. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129312
 152. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129688
 153. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129688
 154. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129372
 155. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129372
 156. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129391
 157. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129391
 158. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129439
 159. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129439
 160. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129733
 161. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129733
 162. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129455
 163. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129455
 164. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129456
 165. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-129456
 166. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-130262
 167. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-130262
 168. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-131530
 169. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-131530
 170. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-135085
 171. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-135085
 172. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-135087
 173. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-135087
 174. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-152886
 175. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-152886
 176. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-153081
 177. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-153081
 178. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-153051
 179. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-153051
 180. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-153078
 181. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-153078
 182. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-153082
 183. https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#comment-153082
 184. https://datahack.analyticsvidhya.com/user/profile/srk
 185. https://datahack.analyticsvidhya.com/user/profile/mark12
 186. https://datahack.analyticsvidhya.com/user/profile/nilabha
 187. https://datahack.analyticsvidhya.com/user/profile/nitish007
 188. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 189. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 190. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 191. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 192. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 193. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 194. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 195. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 196. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 197. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 198. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 199. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 200. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 201. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 202. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 203. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 204. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 205. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 206. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 207. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 208. http://www.analyticsvidhya.com/about-me/
 209. https://www.analyticsvidhya.com/about-me/team/
 210. https://www.analyticsvidhya.com/career-analytics-vidhya/
 211. https://www.analyticsvidhya.com/contact/
 212. https://www.analyticsvidhya.com/about-me/write/
 213. http://www.analyticsvidhya.com/about-me/
 214. https://www.analyticsvidhya.com/about-me/team/
 215. https://www.analyticsvidhya.com/about-me/team/
 216. https://www.analyticsvidhya.com/about-me/team/
 217. https://www.analyticsvidhya.com/career-analytics-vidhya/
 218. https://www.analyticsvidhya.com/about-me/team/
 219. https://www.analyticsvidhya.com/contact/
 220. https://www.analyticsvidhya.com/blog
 221. https://datahack.analyticsvidhya.com/
 222. https://discuss.analyticsvidhya.com/
 223. https://www.analyticsvidhya.com/jobs/
 224. https://datahack.analyticsvidhya.com/users/
 225. https://www.analyticsvidhya.com/corporate/
 226. https://trainings.analyticsvidhya.com/
 227. https://datahack.analyticsvidhya.com/
 228. https://www.analyticsvidhya.com/contact/
 229. https://www.analyticsvidhya.com/contact/
 230. https://datahack.analyticsvidhya.com/signup/
 231. https://www.facebook.com/analyticsvidhya/
 232. https://www.facebook.com/analyticsvidhya/
 233. https://twitter.com/analyticsvidhya
 234. https://twitter.com/analyticsvidhya
 235. https://plus.google.com/+analyticsvidhya
 236. https://in.linkedin.com/company/analytics-vidhya
 237. https://in.linkedin.com/company/analytics-vidhya
 238. https://www.analyticsvidhya.com/privacy-policy/
 239. https://www.analyticsvidhya.com/terms/
 240. https://www.analyticsvidhya.com/refund-policy/
 241. https://id.analyticsvidhya.com/accounts/signup/
 242. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 243. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 244. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 245. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 247. https://www.facebook.com/analyticsvidhya
 248. https://twitter.com/analyticsvidhya
 249. https://plus.google.com/+analyticsvidhya/posts
 250. https://in.linkedin.com/company/analytics-vidhya
 251. https://www.analyticsvidhya.com/blog/2017/05/business-intelligence-analyst-bangalore-3-6-years-of-experience/
 252. https://www.analyticsvidhya.com/blog/2017/05/business-analystsenior-business-analyst-data-analytics-bfsi-gurgaon-2-5-years-of-experience/
 253. https://www.analyticsvidhya.com/blog/author/vikas_10/
 254. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 255. https://www.facebook.com/analyticsvidhya/
 256. https://twitter.com/analyticsvidhya
 257. https://plus.google.com/+analyticsvidhya
 258. https://plus.google.com/+analyticsvidhya
 259. https://in.linkedin.com/company/analytics-vidhya
 260. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 261. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 262. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 263. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 264. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 265. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 266. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 267. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 268. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 269. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 270. javascript:void(0);
 271. javascript:void(0);
 272. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 273. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 274. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 275. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 276. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 277. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 278. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 279. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 280. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 281. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fcomprehensive-guide-to-linear-algebra%2f&linkname=a%20comprehensive%20beginners%20guide%20to%20linear%20algebra%20for%20data%20scientists
 282. javascript:void(0);
 283. javascript:void(0);
