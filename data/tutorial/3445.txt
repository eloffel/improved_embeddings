   #[1]github [2]recent commits to fairseq:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]217
     * [35]star [36]3,382
     * [37]fork [38]590

[39]facebookresearch/[40]fairseq

   [41]code [42]issues 8 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   facebook ai research sequence-to-sequence toolkit
     * [47]28 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors
     * [51]view license

    1. [52]lua 95.7%
    2. [53]c++ 1.9%
    3. [54]python 1.2%
    4. other 1.2%

   (button) lua c++ python other
   branch: master (button) new pull request
   [55]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/f
   [56]download zip

downloading...

   want to be notified of new releases in facebookresearch/fairseq?
   [57]sign in [58]sign up

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [60]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [61]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [62]download the github extension for visual studio
   and try again.

   (button) go back
   [63]@jgehring
   [64]jgehring [65]fix surplus commas in bibtex entries (button)    
fixes [66]#134

   latest commit [67]39e98d6 feb 25, 2019
   [68]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [69]data [70]add missing copyright headers oct 24, 2018
   [71]fairseq
   [72]rocks [73]add missing copyright headers oct 24, 2018
   [74]scripts [75]fix comment describing makealigndict.lua nov 28, 2017
   [76]test [77]initial commit may 9, 2017
   [78].gitignore
   [79]cmakelists.txt
   [80]code_of_conduct.md
   [81]contributing.md
   [82]license
   [83]patents
   [84]readme.md
   [85]fairseq.gif
   [86]generate-lines.lua
   [87]generate.lua
   [88]help.lua
   [89]optimize-fconv.lua
   [90]preprocess.lua
   [91]run.lua
   [92]score.lua
   [93]tofloat.lua [94]initial commit may 9, 2017
   [95]train.lua

readme.md

introduction

   this is fairseq, a sequence-to-sequence learning toolkit for [96]torch
   from facebook ai research tailored to id4 (id4).
   it implements the convolutional id4 models proposed in
   [97]convolutional sequence to sequence learning and [98]a convolutional
   encoder model for id4 as well as a standard
   lstm-based model. it features multi-gpu training on a single machine as
   well as fast id125 generation on both cpu and gpu. we provide
   pre-trained models for english to french, english to german and english
   to romanian translation.

   note, there is now a pytorch version [99]fairseq-py of this toolkit and
   new development efforts will focus on it.

   [100]model

citation

   if you use the code in your paper, then please cite it as:
@article{gehring2017convs2s,
  author          = {gehring, jonas and auli, michael and grangier, david and ya
rats, denis and dauphin, yann n},
  title           = "{convolutional sequence to sequence learning}",
  journal         = {arxiv e-prints},
  archiveprefix   = "arxiv",
  eprinttype      = {arxiv},
  eprint          = {1705.03122},
  primaryclass    = "cs.cl",
  keywords        = {computer science - computation and language},
  year            = 2017,
  month           = may,
}

   and
@article{gehring2016convenc,
  author          = {gehring, jonas and auli, michael and grangier, david and da
uphin, yann n},
  title           = "{a convolutional encoder model for neural machine translati
on}",
  journal         = {arxiv e-prints},
  archiveprefix   = "arxiv",
  eprinttype      = {arxiv},
  eprint          = {1611.02344},
  primaryclass    = "cs.cl",
  keywords        = {computer science - computation and language},
  year            = 2016,
  month           = nov,
}

requirements and installation

     * a computer running macos or linux
     * for training new models, you'll also need a nvidia gpu and
       [101]nccl
     * a [102]torch installation. for maximum speed, we recommend using
       luajit and [103]intel mkl.
     * a recent version [104]nn. the minimum required version is from may
       5th, 2017. a simple luarocks install nn is sufficient to update
       your locally installed version.

   install fairseq by cloning the github repository and running
luarocks make rocks/fairseq-scm-1.rockspec

   luarocks will fetch and build any additional dependencies that may be
   missing. in order to install the cpu-only version (which is only useful
   for translating new data with an existing model), do
luarocks make rocks/fairseq-cpu-scm-1.rockspec

   the luarocks installation provides a command-line tool that includes
   the following functionality:
     * fairseq preprocess: data pre-processing: build vocabularies and
       binarize training data
     * fairseq train: train a new model on one or multiple gpus
     * fairseq generate: translate pre-processed data with a trained model
     * fairseq generate-lines: translate raw text with a trained model
     * fairseq score: id7 scoring of generated translations against
       reference translations
     * fairseq tofloat: convert a trained model to a cpu model
     * fairseq optimize-fconv: optimize a fully convolutional model for
       generation. this can also be achieved by passing the -fconvfast
       flag to the generation scripts.

quick start

evaluating pre-trained models

   first, download a pre-trained model along with its vocabularies:
$ curl https://s3.amazonaws.com/fairseq/models/wmt14.en-fr.fconv-cuda.tar.bz2 |
tar xvjf -

   this will unpack vocabulary files and a serialized model for english to
   french translation to wmt14.en-fr.fconv-cuda/.

   alternatively, use a cpu-based model:
$ curl https://s3.amazonaws.com/fairseq/models/wmt14.en-fr.fconv-float.tar.bz2 |
 tar xvjf -

   let's use fairseq generate-lines to translate some text. this model
   uses a [105]byte pair encoding (bpe) vocabulary, so we'll have to apply
   the encoding to the source text. this can be done with
   [106]apply_bpe.py using the bpecodes file in within
   wmt14.en-fr.fconv-cuda/. @@ is used as a continuation marker and the
   original text can be easily recovered with e.g. sed s/@@ //g. prior to
   bpe, input text needs to be tokenized using tokenizer.perl from
   [107]mosesdecoder. here, we use a beam size of 5:
$ fairseq generate-lines -path wmt14.en-fr.fconv-cuda/model.th7 -sourcedict wmt1
4.en-fr.fconv-cuda/dict.en.th7 \
    -targetdict wmt14.en-fr.fconv-cuda/dict.fr.th7 -beam 5
| [target] dictionary: 44666 types
| [source] dictionary: 44409 types
> why is it rare to discover new marine mam@@ mal species ?
s       why is it rare to discover new marine mam@@ mal species ?
o       why is it rare to discover new marine mam@@ mal species ?
h       -0.068684287369251      pourquoi est-il rare de d  couvrir de nouvelles e
sp  ces de mammif  res marins ?
a       1 1 4 4 6 6 7 11 9 9 9 12 13

   this generation script produces four types of output: a line prefixed
   with s shows the supplied source sentence after applying the
   vocabulary; o is a copy of the original source sentence; h is the
   hypothesis along with an average log-likelihood and a are attention
   maxima for each word in the hypothesis (including the end-of-sentence
   marker which is omitted from the text).

   check [108]below for a full list of pre-trained models available.

training a new model

data pre-processing

   the fairseq source distribution contains an example pre-processing
   script for the iwslt14 german-english corpus. pre-process and binarize
   the data as follows:
$ cd data/
$ bash prepare-iwslt14.sh
$ cd ..
$ text=data/iwslt14.tokenized.de-en
$ fairseq preprocess -sourcelang de -targetlang en \
  -trainpref $text/train -validpref $text/valid -testpref $text/test \
  -thresholdsrc 3 -thresholdtgt 3 -destdir data-bin/iwslt14.tokenized.de-en

   this will write binarized data that can be used for model training to
   data-bin/iwslt14.tokenized.de-en.

training

   use fairseq train to train a new model. here a few example settings
   that work well for the iwslt14 dataset:
# standard bi-directional lstm model
$ mkdir -p trainings/blstm
$ fairseq train -sourcelang de -targetlang en -datadir data-bin/iwslt14.tokenize
d.de-en \
  -model blstm -nhid 512 -dropout 0.2 -dropout_hid 0 -optim adam -lr 0.0003125 -
savedir trainings/blstm

# fully convolutional sequence-to-sequence model
$ mkdir -p trainings/fconv
$ fairseq train -sourcelang de -targetlang en -datadir data-bin/iwslt14.tokenize
d.de-en \
  -model fconv -nenclayer 4 -nlayer 3 -dropout 0.2 -optim nag -lr 0.25 -clip 0.1
 \
  -momentum 0.99 -timeavg -bptt 0 -savedir trainings/fconv

# convolutional encoder, lstm decoder
$ mkdir -p trainings/convenc
$ fairseq train -sourcelang de -targetlang en -datadir data-bin/iwslt14.tokenize
d.de-en \
  -model conv -nenclayer 6 -dropout 0.2 -dropout_hid 0 -savedir trainings/conven
c

   by default, fairseq train will use all available gpus on your machine.
   use the [109]cuda_visible_devices environment variable to select
   specific gpus or -ngpus to change the number of gpu devices that will
   be used.

generation

   once your model is trained, you can translate with it using fairseq
   generate (for binarized data) or fairseq generate-lines (for text).
   here, we'll do it for a fully convolutional model:
# optional: optimize for generation speed
$ fairseq optimize-fconv -input_model trainings/fconv/model_best.th7 -output_mod
el trainings/fconv/model_best_opt.th7

# translate some text
$ data=data-bin/iwslt14.tokenized.de-en
$ fairseq generate-lines -sourcedict $data/dict.de.th7 -targetdict $data/dict.en
.th7 \
  -path trainings/fconv/model_best_opt.th7 -beam 10 -nbest 2
| [target] dictionary: 24738 types
| [source] dictionary: 35474 types
> eine sprache ist ausdruck des menschlichen geistes .
s       eine sprache ist ausdruck des menschlichen geistes .
o       eine sprache ist ausdruck des menschlichen geistes .
h       -0.23804219067097       a language is expression of human mind .
a       2 2 3 4 5 6 7 8 9
h       -0.23861141502857       a language is expression of the human mind .
a       2 2 3 4 5 7 6 7 9 9

cpu generation

   use fairseq tofloat to convert a trained model to use cpu-only
   operations (this has to be done on a gpu machine):
# optional: optimize for generation speed
$ fairseq optimize-fconv -input_model trainings/fconv/model_best.th7 -output_mod
el trainings/fconv/model_best_opt.th7

# convert to float
$ fairseq tofloat -input_model trainings/fconv/model_best_opt.th7 \
  -output_model trainings/fconv/model_best_opt-float.th7

# translate some text
$ fairseq generate-lines -sourcedict $data/dict.de.th7 -targetdict $data/dict.en
.th7 \
  -path trainings/fconv/model_best_opt-float.th7 -beam 10 -nbest 2
> eine sprache ist ausdruck des menschlichen geistes .
s       eine sprache ist ausdruck des menschlichen geistes .
o       eine sprache ist ausdruck des menschlichen geistes .
h       -0.2380430996418        a language is expression of human mind .
a       2 2 3 4 5 6 7 8 9
h       -0.23861189186573       a language is expression of the human mind .
a       2 2 3 4 5 7 6 7 9 9

pre-trained models

   we provide the following pre-trained fully convolutional
   sequence-to-sequence models:
     * [110]wmt14.en-fr.fconv-cuda.tar.bz2: pre-trained model for
       [111]wmt14 english-french including vocabularies
     * [112]wmt14.en-fr.fconv-float.tar.bz2: cpu version of the above
     * [113]wmt14.en-de.fconv-cuda.tar.bz2: pre-trained model for
       [114]wmt14 english-german including vocabularies
     * [115]wmt14.en-de.fconv-float.tar.bz2: cpu version of the above
     * [116]wmt16.en-ro.fconv-cuda.tar.bz2: pre-trained model for wmt16
       english-romanian including vocabularies. this model was trained on
       the [117]original wmt bitext as well as [118]back-translated data
       provided by rico sennrich.
     * [119]wmt16.en-ro.fconv-float.tar.bz2: cpu version of the above

   in addition, we provide pre-processed and binarized test sets for the
   models above:
     * [120]wmt14.en-fr.newstest2014.tar.bz2: newstest2014 test set for
       wmt14 english-french
     * [121]wmt14.en-fr.ntst1213.tar.bz2: newstest2012 and newstest2013
       test sets for wmt14 english-french
     * [122]wmt14.en-de.newstest2014.tar.bz2: newstest2014 test set for
       wmt14 english-german
     * [123]wmt16.en-ro.newstest2014.tar.bz2: newstest2016 test set for
       wmt16 english-romanian

   generation with the binarized test sets can be run in batch mode as
   follows, e.g. for english-french on a gtx-1080ti:
$ curl https://s3.amazonaws.com/fairseq/data/wmt14.en-fr.newstest2014.tar.bz2 |
tar xvjf -

$ fairseq generate -sourcelang en -targetlang fr -datadir data-bin/wmt14.en-fr -
dataset newstest2014 \
  -path wmt14.en-fr.fconv-cuda/model.th7 -beam 5 -batchsize 128 | tee /tmp/gen.o
ut
...
| translated 3003 sentences (95451 tokens) in 136.3s (700.49 tokens/s)
| timings: setup 0.1s (0.1%), encoder 1.9s (1.4%), decoder 108.9s (79.9%), searc
h_results 0.0s (0.0%), search_prune 12.5s (9.2%)
| id74 = 43.43, 68.2/49.2/37.4/28.8 (bp=0.996, ratio=1.004, sys_len=92087, ref_
len=92448)

# word-level id7 scoring:
$ grep ^h /tmp/gen.out | cut -f3- | sed 's/@@ //g' > /tmp/gen.out.sys
$ grep ^t /tmp/gen.out | cut -f2- | sed 's/@@ //g' > /tmp/gen.out.ref
$ fairseq score -sys /tmp/gen.out.sys -ref /tmp/gen.out.ref
id74 = 40.55, 67.6/46.5/34.0/25.3 (bp=1.000, ratio=0.998, sys_len=81369, ref_le
n=81194)

join the fairseq community

     * facebook page: [124]https://www.facebook.com/groups/fairseq.users
     * google group:
       [125]https://groups.google.com/forum/#!forum/fairseq-users
     * contact: [126]jgehring@fb.com, [127]michaelauli@fb.com

license

   fairseq is bsd-licensed. the license applies to the pre-trained models
   as well. we also provide an additional patent grant.

     *    2019 github, inc.
     * [128]terms
     * [129]privacy
     * [130]security
     * [131]status
     * [132]help

     * [133]contact github
     * [134]pricing
     * [135]api
     * [136]training
     * [137]blog
     * [138]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [139]reload to refresh your
   session. you signed out in another tab or window. [140]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/facebookresearch/fairseq/commits/master.atom
   3. https://github.com/facebookresearch/fairseq#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/facebookresearch/fairseq
  32. https://github.com/join
  33. https://github.com/login?return_to=/facebookresearch/fairseq
  34. https://github.com/facebookresearch/fairseq/watchers
  35. https://github.com/login?return_to=/facebookresearch/fairseq
  36. https://github.com/facebookresearch/fairseq/stargazers
  37. https://github.com/login?return_to=/facebookresearch/fairseq
  38. https://github.com/facebookresearch/fairseq/network/members
  39. https://github.com/facebookresearch
  40. https://github.com/facebookresearch/fairseq
  41. https://github.com/facebookresearch/fairseq
  42. https://github.com/facebookresearch/fairseq/issues
  43. https://github.com/facebookresearch/fairseq/pulls
  44. https://github.com/facebookresearch/fairseq/projects
  45. https://github.com/facebookresearch/fairseq/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/facebookresearch/fairseq/commits/master
  48. https://github.com/facebookresearch/fairseq/branches
  49. https://github.com/facebookresearch/fairseq/releases
  50. https://github.com/facebookresearch/fairseq/graphs/contributors
  51. https://github.com/facebookresearch/fairseq/blob/master/license
  52. https://github.com/facebookresearch/fairseq/search?l=lua
  53. https://github.com/facebookresearch/fairseq/search?l=c++
  54. https://github.com/facebookresearch/fairseq/search?l=python
  55. https://github.com/facebookresearch/fairseq/find/master
  56. https://github.com/facebookresearch/fairseq/archive/master.zip
  57. https://github.com/login?return_to=https://github.com/facebookresearch/fairseq
  58. https://github.com/join?return_to=/facebookresearch/fairseq
  59. https://desktop.github.com/
  60. https://desktop.github.com/
  61. https://developer.apple.com/xcode/
  62. https://visualstudio.github.com/
  63. https://github.com/jgehring
  64. https://github.com/facebookresearch/fairseq/commits?author=jgehring
  65. https://github.com/facebookresearch/fairseq/commit/39e98d640a74bc527cac3d8fe10ea5a4d77c8551
  66. https://github.com/facebookresearch/fairseq/issues/134
  67. https://github.com/facebookresearch/fairseq/commit/39e98d640a74bc527cac3d8fe10ea5a4d77c8551
  68. https://github.com/facebookresearch/fairseq/tree/39e98d640a74bc527cac3d8fe10ea5a4d77c8551
  69. https://github.com/facebookresearch/fairseq/tree/master/data
  70. https://github.com/facebookresearch/fairseq/commit/cea5fc4031c6ffefe17dc3de10af7b8d5c04f64b
  71. https://github.com/facebookresearch/fairseq/tree/master/fairseq
  72. https://github.com/facebookresearch/fairseq/tree/master/rocks
  73. https://github.com/facebookresearch/fairseq/commit/cea5fc4031c6ffefe17dc3de10af7b8d5c04f64b
  74. https://github.com/facebookresearch/fairseq/tree/master/scripts
  75. https://github.com/facebookresearch/fairseq/commit/7d017f0a46a3cddfc420a4778d9541ba38b6a43d
  76. https://github.com/facebookresearch/fairseq/tree/master/test
  77. https://github.com/facebookresearch/fairseq/commit/b08530ae7332d4f8ca2d9ad470ea651fd5e22ba5
  78. https://github.com/facebookresearch/fairseq/blob/master/.gitignore
  79. https://github.com/facebookresearch/fairseq/blob/master/cmakelists.txt
  80. https://github.com/facebookresearch/fairseq/blob/master/code_of_conduct.md
  81. https://github.com/facebookresearch/fairseq/blob/master/contributing.md
  82. https://github.com/facebookresearch/fairseq/blob/master/license
  83. https://github.com/facebookresearch/fairseq/blob/master/patents
  84. https://github.com/facebookresearch/fairseq/blob/master/readme.md
  85. https://github.com/facebookresearch/fairseq/blob/master/fairseq.gif
  86. https://github.com/facebookresearch/fairseq/blob/master/generate-lines.lua
  87. https://github.com/facebookresearch/fairseq/blob/master/generate.lua
  88. https://github.com/facebookresearch/fairseq/blob/master/help.lua
  89. https://github.com/facebookresearch/fairseq/blob/master/optimize-fconv.lua
  90. https://github.com/facebookresearch/fairseq/blob/master/preprocess.lua
  91. https://github.com/facebookresearch/fairseq/blob/master/run.lua
  92. https://github.com/facebookresearch/fairseq/blob/master/score.lua
  93. https://github.com/facebookresearch/fairseq/blob/master/tofloat.lua
  94. https://github.com/facebookresearch/fairseq/commit/b08530ae7332d4f8ca2d9ad470ea651fd5e22ba5
  95. https://github.com/facebookresearch/fairseq/blob/master/train.lua
  96. http://torch.ch/
  97. https://arxiv.org/abs/1705.03122
  98. https://arxiv.org/abs/1611.02344
  99. https://github.com/facebookresearch/fairseq-py
 100. https://github.com/facebookresearch/fairseq/blob/master/fairseq.gif
 101. https://github.com/nvidia/nccl
 102. http://torch.ch/docs/getting-started.html
 103. https://software.intel.com/en-us/intel-mkl
 104. https://github.com/torch/nn
 105. https://arxiv.org/abs/1508.07909
 106. https://github.com/rsennrich/subword-id4/blob/master/apply_bpe.py
 107. https://github.com/moses-smt/mosesdecoder
 108. https://github.com/facebookresearch/fairseq#pre-trained-models
 109. http://acceleware.com/blog/cudavisibledevices-masking-gpus
 110. https://s3.amazonaws.com/fairseq/models/wmt14.en-fr.fconv-cuda.tar.bz2
 111. http://statmt.org/wmt14/translation-task.html#download
 112. https://s3.amazonaws.com/fairseq/models/wmt14.en-fr.fconv-float.tar.bz2
 113. https://s3.amazonaws.com/fairseq/models/wmt14.en-de.fconv-cuda.tar.bz2
 114. https://nlp.stanford.edu/projects/id4
 115. https://s3.amazonaws.com/fairseq/models/wmt14.en-de.fconv-float.tar.bz2
 116. https://s3.amazonaws.com/fairseq/models/wmt16.en-ro.fconv-cuda.tar.bz2
 117. http://statmt.org/wmt16/translation-task.html#download
 118. http://data.statmt.org/rsennrich/wmt16_backtranslations/en-ro
 119. https://s3.amazonaws.com/fairseq/models/wmt16.en-ro.fconv-float.tar.bz2
 120. https://s3.amazonaws.com/fairseq/data/wmt14.en-fr.newstest2014.tar.bz2
 121. https://s3.amazonaws.com/fairseq/data/wmt14.en-fr.ntst1213.tar.bz2
 122. https://s3.amazonaws.com/fairseq/data/wmt14.en-de.newstest2014.tar.bz2
 123. https://s3.amazonaws.com/fairseq/data/wmt16.en-ro.newstest2016.tar.bz2
 124. https://www.facebook.com/groups/fairseq.users
 125. https://groups.google.com/forum/#!forum/fairseq-users
 126. mailto:jgehring@fb.com
 127. mailto:michaelauli@fb.com
 128. https://github.com/site/terms
 129. https://github.com/site/privacy
 130. https://github.com/security
 131. https://githubstatus.com/
 132. https://help.github.com/
 133. https://github.com/contact
 134. https://github.com/pricing
 135. https://developer.github.com/
 136. https://training.github.com/
 137. https://github.blog/
 138. https://github.com/about
 139. https://github.com/facebookresearch/fairseq
 140. https://github.com/facebookresearch/fairseq

   hidden links:
 142. https://github.com/
 143. https://github.com/facebookresearch/fairseq
 144. https://github.com/facebookresearch/fairseq
 145. https://github.com/facebookresearch/fairseq
 146. https://help.github.com/articles/which-remote-url-should-i-use
 147. https://github.com/facebookresearch/fairseq#introduction
 148. https://github.com/facebookresearch/fairseq#citation
 149. https://github.com/facebookresearch/fairseq#requirements-and-installation
 150. https://github.com/facebookresearch/fairseq#quick-start
 151. https://github.com/facebookresearch/fairseq#evaluating-pre-trained-models
 152. https://github.com/facebookresearch/fairseq#training-a-new-model
 153. https://github.com/facebookresearch/fairseq#data-pre-processing
 154. https://github.com/facebookresearch/fairseq#training
 155. https://github.com/facebookresearch/fairseq#generation
 156. https://github.com/facebookresearch/fairseq#cpu-generation
 157. https://github.com/facebookresearch/fairseq#pre-trained-models
 158. https://github.com/facebookresearch/fairseq#join-the-fairseq-community
 159. https://github.com/facebookresearch/fairseq#license
 160. https://github.com/
