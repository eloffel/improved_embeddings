community

   news
   beta
   tutorials
   cheat sheets
   open courses
   podcast - dataframed
   chat
   new

datacamp

   official blog
   tech thoughts
   (button)
   search
   [1](button)
   log in
   (button)
   create account
   (button)
   share an article
   (button)
   back to tutorials
   tutorials
   [2]0
   88
   88
   karlijn willems
   march 28th, 2017
   python
   +2

apache spark in python: beginner's guide

   a beginner's guide to spark in python based on 9 popular questions,
   such as how to install pyspark in jupyter notebook, best practices,...

   you might already know apache spark as a fast and general engine for
   big data processing, with built-in modules for streaming, sql, machine
   learning and graph processing. it   s well-known for its speed, ease of
   use, generality and the ability to run virtually everywhere. and even
   though spark is one of the most asked tools for data engineers, also
   data scientists can benefit from spark when doing exploratory data
   analysis, feature extraction, supervised learning and model evaluation.

   today   s post will introduce you to some basic spark in python topics,
   based on 9 of the most frequently asked questions, such as
     * [3]what language to pick when you   re working with spark: python or
       scala? what are the benefits of using one over the other?
     * next, you   ll see how you can [4]work with spark in python: locally
       or via the jupyter notebook. you   ll learn how to install spark and
       how to run spark applications with jupyter notebooks, either by
       adding pyspark as any other library, by working with a kernel or by
       running pyspark with jupyter in docker containers.
     * now that you have made sure that you can work with spark in python,
       you   ll get to know one of the basic building blocks that you will
       frequently use when you   re working with pyspark: the rdd. you   ll
       learn [5]how the rdd differs from the dataframe api and the dataset
       api and when you should use which structure.
     * then, you   ll learn more about the differences between [6]spark
       dataframes and pandas dataframes and how you can switch from one to
       the other. if you want to go further into pandas dataframes,
       consider datacamp   s [7]pandas foundations course.
     * when you   re working with rdds, it   s very important to understand
       the differences between [8]rdd actions and transformations,
     * and why you need to [9]cache or persist rdds, or when you need to
       [10]broadcast a variable.
     * to round up, you   ll get introduced to some of the [11]best
       practices in spark, like using dataframes and the spark ui,
     * and you   ll also see how you can [12]turn off the logging for
       pyspark.

spark: python or scala?

   when you start out, you   ll probably read a lot about using spark with
   python or with scala. there has been some discussion about it on
   forums.

spark performance: scala or python?

   in general, most developers seem to agree that scala wins in terms of
   performance and concurrency: it   s definitely faster than python when
   you   re working with spark, and when you   re talking about concurrency,
   it   s sure that scala and the play framework make it easy to write clean
   and performant async code that is easy to reason about. play is fully
   asynchronous, which make it possible to have many concurrent
   connections without dealing with threads. it will be easier to make i/o
   calls in paralllel to improve performance and enables the use of
   real-time, streaming, and server push technologies.

   note that asynchronous code allows for non-blocking i/o when making
   calls to remote services. let   s state it differently with an example:
   when you have two lines of code of which the first one queries a
   database and the next prints something to the console, synchronous
   programming will wait for the query to finish before printing
   something. your program is (momentarily) blocked. if your programming
   language doesn   t support asynchronous programming, you   ll need to make
   threads to execute lines of code in parallel. asynchronous programming,
   on the other hand, will already print to the console while the database
   is being queried. the query will be processed on the background.

   in short, the above explains why it   s still strongly recommended to use
   scala over python when you   re working with streaming data, even though
   structured streaming in spark seems to reduce the gap already.

   but streaming data is not the only performance consideration that you
   might make.

   when you   re working with the dataframe api, there isn   t really much of
   a difference between python and scala, but you do need to be wary of
   user defined functions (udfs), which are less efficient than its scala
   equivalents. that   s why you should favor built-in expressions if you   re
   working with python. when you   re working with python, also make sure
   not to pass your data between dataframe and rdd unnecessarily, as the
   serialization and deserialization of the data transfer is particularly
   expensive.

   remember that serialization is a process of converting an object into a
   sequence of bytes which can be persisted to a disk or database or can
   be sent through streams. the reverse process, creating object from
   sequence of bytes, is called deserialization. in a more practical
   example, you can have a movie application, for example, with a server
   and clients. whenever the application from a client send queries to the
   server to retrieve, for example, a list of movies. the server needs to
   pass a list of available movie objects back to the clients, the object
   needs to be serialized.

learning spark: python or scala?

   python seems to be a better choice in terms of learning curve and easy
   to use, as it   s less verbose and more readable than scala. it   s ideal
   for those who don   t have too much programming experience.

   but even for those who have some programming experience, working with
   spark in python isn   t far fetched at all, as you   ll see in the
   following paragraphs.

spark and type safety: scala or python?

   two things that are somewhat in the middle are the type safety and the
   amount of advanced features that you get when you   re working with
   either python or scala. for what concerns the type safety, you can say
   that python is a good choice when you   re doing smaller ad hoc
   experiments, while scala is great when you   re working on bigger
   projects in production. this is mostly because statically typed
   language like scala are much easier and hassle-free when you   re
   refactoring.

   remember that when a language is statically typed, every variable name
   is bound both to a type and an object. type checking happens at compile
   time. typical examples are java or scala. note that in scala   s case,
   the type systemcan deduce the type of a variable, so there is a form of
   type id136 that will make your work a bit quicker. in dynamically
   typed languages, every variable name is bound only to an object, unless
   it is null, of course. type checking happens at run time. as a
   developer, this generally means that you can work quicker because you
   don   t have to specify types every time. typical examples here are
   python or ruby.

   let   s make this visual. consider the following java and python code
   chunks:
# java
string str = "hello";
str = 5;

# python
str = "hello"
str = 5

   assigning the integer 5 to str in java will give an error, since you
   declared it to be a string. in python, this won   t be a problem.

   note that the spark datasets, which are statically typed, don   t really
   have much of a place in python. you   ll read more about this later on.

spark and advanced features: python or scala?

   and, lastly, there are some advanced features that might sway you to
   use either python or scala. here, you would have to argue that python
   has the main advantage if you   re talking about data science, as it
   provides the user with a lot of great tools for machine learning and
   natural language processing, such as sparkmlib.

conclusion

   in a nutshell, both languages have their advantages and disadvantages
   when you   re working with spark. deciding for one or the other depends
   on your projects    needs, your own or your teams    capabilities,     the
   general advice that is given is to use scala unless you   re already
   proficient in it or if you don   t have much programming experience. that
   means that, in the end, it   s important that you know how to work with
   both!

   ps. if you want to get started with pyspark, don   t miss datacamp   s
   [13]pyspark cheat sheet.

how to install spark

   installing spark and getting to work with it can be a daunting task.
   this section will go deeper into how you can install it and what your
   options are to start working with it.

   first, check if you have the java [14]jdk installed. then, go to the
   [15]spark download page. keep the default options in the first three
   steps and you   ll find a downloadable link in step 4. click to download
   it.

   next, make sure that you untar the directory that appears in your
      downloads    folder. next, move the untarred folder to /usr/local/spark.
$ mv spark-2.1.0-bin-hadoop2.7 /usr/local/spark

   now that you   re all set to go, open the readme file in
   /usr/local/spark. you   ll see that you   ll need to run a command to build
   spark if you have a version that has not been built yet. so, make sure
   you run the command:
$ build/mvn -dskiptests clean package run

   this might take a while, but after this, you   re all set to go!

interactive spark shell

   next, you can immediately start working in the spark shell by typing
   ./bin/pyspark in the same folder in which you left off at the end of
   the last section. it can take a bit of time, but eventually, you   ll see
   something like this:
welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.1.0
      /_/

using python version 2.7.13 (v2.7.13:a06454b1afa1, dec 17 2016 12:39:47)
sparksession available as 'spark'.

   you   re ready to start working interactively!

   note that the sparkcontext has already been initialized. you don   t need
   to import sparkcontext from pyspark to begin working.

   of course, you can adjust the command to start the spark shell
   according to the options that you want to change. in the following
   command, you see that the --master argument allows you to specify to
   which master the sparkcontext connects to. in this case, you see that
   the local mode is activated. the number in between the brackets
   designates the number of cores that are being used; in this case, you
   use all cores, while local[4] would only make use of four cores.
$ ./bin/pyspark --master local[*]

   note that the application ui is available at localhost:4040. open up a
   browser, paste in this location and you   ll get to see a dashboard with
   tabs designating jobs, stages, storage, etc. this will definitely come
   in handy when you   re executing jobs and looking to tune them. you   ll
   read more about this further on.

running spark applications using jupyter notebooks

   when you have downloaded a spark distribution, you can also start
   working with jupyter notebook. if you want to try it out first, go
   [16]here and make sure you click on the    welcome to spark with python   
   notebook. the demo will show you how you can interactively train two
   classifiers to predict survivors in the titanic data set with spark
   mllib.

   there are various options to get spark in your jupyter notebook: you
   can run pyspark notebooks in your docker container, you can set up your
   jupyter notebook with spark or you can make sure you add a kernel to
   work with it in your notebook.

   in any case, make sure you have the jupyter notebook application ready.
   if you don   t, consider installing [17]anaconda, which includes the
   application, or install it with the help of pip pip3 install jupyter.
   you can find more information on the installation process or running
   specific notebooks with spark in python in a docker container, consult
   datacamp   s [18]definitive guide to jupyter notebook.

spark in jupyter notebook

   now that you have all that you need to get started, you can launch the
   jupyter notebook application by typing the following:
pyspark_driver_python="jupyter" pyspark_driver_python_opts="notebook" pyspark

   or you can launch jupyter notebook normally with jupyter notebook and
   run the following code before importing pyspark:
! pip install findspark

   with findspark, you can add pyspark to sys.path at runtime. next, you
   can just import pyspark just like any other regular library:
import findspark
findspark.init()

# or the following command
findspark.init("/path/to/spark_home")

from pyspark import sparkcontext, sparkconf

   note that if you haven   t installed spark with brew and in accordance
   with the instructions that are listed above, it could be that you need
   to add the path to spark_home to findspark.init(). if you   re still in
   doubt where spark_home is located at, you can call findspark.find() to
   automatically detect the location of where spark is installed.

   tip: you can read more about findspark [19]here.

jupyter notebook with spark kernel

   next, if you want to install a kernel, you want to make sure you get
   apache toree installed. install toree via pip with pip install toree.
   next, install a jupyter application toree:
jupyter toree install --spark_home=/usr/local/bin/apache-spark/ --interpreters=s
cala,pyspark

   make sure that you fill out the spark_home argument correctly and also
   note that if you don   t specify pyspark in the interpreters argument,
   that the scala kernel will be installed by default. this path should
   point to the unzipped directory that you have downloaded earlier from
   the spark download page. next, verify whether the kernel is included in
   the following list:
jupyter kernelspec list

   start jupyter notebook as usual with jupyter notebook or configure
   spark even further with, for example, the following line:
spark_opts='--master=local[4]' jupyter notebook

   in which you specify to run spark locally with 4 threads.

running pyspark with jupyter in docker containers

   one of the other options to run the jupyter notebook application is to
   run it in docker containers. all you need to do is set up docker and
   download a docker image that best fits your porject. first, consult
   [20]this section for the docker installation instructions if you
   haven   t gotten around installing docker yet.

   once you have set up, go to [21]dockerhub and go for an image like
   [22]jupyter/pyspark-notebook to kickstart your journey with pyspark in
   jupyter. for other images, check out [23]this repository.

   do you want to get a better overview of the evolution of the jupyter
   project and the components of the jupyter notebook? consider reading up
   on our [24]ipython or jupyter? post.

   [25]learn python for data science with datacamp

spark apis: rdd, dataset and dataframe

   these three apis can seem very confusing for anyone who   s just getting
   acquainted with spark. you will agree with me when i say that it   s
   sometimes hard to see the forest for the trees    this section will focus
   on making the distinction between these three a little bit more clear
   and also explains when you should be using which api.

rdds

   rdds are the building blocks of spark. it   s the original api that spark
   exposed and pretty much all the higher level apis decompose to rdds.
   from a developer   s perspective, an rdd is simply a set of java or scala
   objects representing data.

   rdds have three main characteristics: they are compile-time type safe
   (they have a type!), they are lazy and they are based on the scala
   collections api.

   the advantages of rdds are manifold, but there are also some problems.
   for example, it   s easy to build inefficient transformation chains, they
   are slow with non-jvm languages such as python, they can not be
   optimized by spark. lastly, it   s difficult to understand what is going
   on when you   re working with them, because, for example, the
   transformation chains are not very readable in the sense that you don   t
   immediately see what will be the solution, but how you are doing it.

dataframes

   because of the disadvantages that you can experience while working with
   rdds, the dataframe api was conceived: it provides you with a higher
   level abstraction that allows you to use a query language to manipulate
   the data. this higher level abstraction is a logical plan that
   represents data and a schema. this means that the frontend to
   interacting with your data is a lot easier! because the logical plan
   will be converted to a physical plan for execution, you   re actually a
   lot closer to what you   re doing when you   re working with them rather
   than how you   re trying to do it, because you let spark figure out the
   most efficient way to do what you want to do.

   remember though that dataframes are still built on top of rdds!

   and exactly because you let spark worry about the most efficient way to
   do things, dataframes are optimized: more intelligent decisions will be
   made when you   re transforming data and that also explains why they are
   faster than rdds.

   more specifically, the performance improvements are due to two things,
   which you   ll often come across when you   re reading up dataframes:
   custom memory management (project tungsten), which will make sure that
   your spark jobs much faster given cpu constraints, and optimized
   execution plans (catalyst optimizer), of which the logical plan of the
   dataframe is a part.

datasets

   the only downside to using dataframes is that you   ve lost compile-time
   type safety when you work with dataframes, which makes your code more
   prone to errors. this is part of the reason why they have moved more to
   the notion of datasets: getting back some type safety and the use of
   lambda functions, which means that you want to go a bit back to the
   advantage that rdds has to offer, but you don   t want to lose all the
   optimalizations that the dataframes offer.

   [content_unified-apache-spark-2_0-api-1.png]

   the notion of the dataset has developed and has become the second main
   spark api, besides the rdd api. as a result, the dataset can take on
   two distinct characteristics: a strongly-typed api and an untyped api.
   this means that the dataframe is still there conceptually, as a synonym
   for a dataset: any dataframe is now a synonym for dataset[row] in
   scala, where row is a generic untyped jvm object. the dataset is a
   collection of strongly-typed jvm objects.

   note that, since python has no compile-time type-safety, only the
   untyped dataframe api is available. or, in other words, spark datasets
   are statically typed, while python is a dynamically typed programming
   language. that explains why the dataframes or the untyped api is
   available when you want to work with spark in python. also, remember
   that datasets are built on top of rdds, just like dataframes.

   to summarize, the clear advantage of working with the dataset api
   (which includes both datasets and dataframes) are the static typing and
   the runtime type safety, the higher level abstraction over the data,
   and the performance and optimization. of course, it also helps that the
   dataset api basically forces you to work with more structured data,
   which also adds to the ease of use of the api itself.

when to use which?

   all of the above explains why it   s generally advised to use dataframes
   when you   re working with pyspark, also because they are so close to the
   dataframe structure that you might already know from the pandas
   library. also note that what has been pointed out in the section above:
   there is not really a place for datasets in python because of the lack
   of compile-time type-safety of the python language. however, that
   doesn   t mean that the dataset api, which includes both a strongly-typed
   and untyped api, can   t come in handy: it is ideal for when you want to
   use high-level expressions, sql queries, columnar access and the use of
   lambda functions,     on semi-structured data. however, if you might
   still want to have more control, you can always fall back on the rdds.

   you can use rdds when you want to perform low-level transformations and
   actions on your unstructured data. this means that you don   t care about
   imposing a schema while processing or accessing the attributes by name
   or column. in addition, you don   t necessarily need the optimization and
   performance benefits that dataframes and datasets can offer for (semi-)
   structured data. also, you usually use rdds when you want to manipulate
   the data with functional programming constructs rather than domain
   specific expressions.

   in short, use the api that best fits your use case, but also remember
   that it   s fairly easy to switch from a dataframe to an rdd, as you will
   see in the next section!

the difference between spark dataframes and pandas dataframes

   dataframes are often compared to tables in a relational database or a
   data frame in r or python: they have a scheme, with column names and
   types and logic for rows and columns. this mimics the implementation of
   dataframes in pandas!

   note that, even though the spark, python and r data frames can be very
   similar, there are also a lot of differences: as you have read above,
   spark dataframes carry the specific optimalization under the hood and
   can use distributed memory to handle big data, while pandas dataframes
   and r data frames can only run on one computer.

   however, these differences don   t mean that the two of them can   t work
   together: you can reuse your existing pandas dataframes to scale up to
   larger data sets. if you want to convert your spark dataframe to a
   pandas dataframe and you expect the resulting pandas   s dataframe to be
   small, you can use the following lines of code:
df.topandas()

   you see, the two integrate very well: you can parallelize the work load
   thanks to the spark dataframe, you can make use of the wealth of
   libraries that python and r dataframes have to offer, which make
   visualization or machine learning a whole lot more easy!

   note that you do need to make sure that the dataframe needs to be small
   enough because all the data is loaded into the driver   s memory!

rdd actions versus transformations

   rdds support two types of operations: transformations, which create a
   new dataset from an existing one, and actions, which return a value to
   the driver program after running a computation on the dataset. for
   example, map() is a transformation that passes each dataset element
   through a function and returns a new rdd representing the results. on
   the other hand, reduce() is an action that aggregates all the elements
   of the rdd using some function and returns the final result to the
   driver program. note, however, that there is also a reducebykey() that
   returns a distributed dataset.

   all transformations in spark are lazy, in that they do not compute
   their results right away: instead, they just remember the
   transformations applied to some base dataset. the transformations are
   only computed when an action requires a result to be returned to the
   driver program.

   with these two types of rdd operations, spark can run more efficiently:
   a dataset created through map() operation will be used in a consequent
   reduce() operation and will return only the result of the the last
   reduce function to the driver. that way, the reduced data set rather
   than the larger mapped data set will be returned to the user. this is
   more efficient, without a doubt!

why do you need to cache or persist rdds?

   by default, each transformed rdd may be recomputed each time you run an
   action on it. however, you may also persist an rdd in memory using the
   persist (or cache) method, in which case spark will keep the elements
   around on the cluster for much faster access the next time you query
   it. there is also support for persisting rdds on disk, or replicated
   across multiple nodes.

   a couple of use cases for caching or persisting rdds are the use of
   iterative algorithms and fast interactive rdd use.

   if you   re going to persist rdds, take a look at the level of
   persistence that you   re going to define. you can find more information
   [26]here.

persist or broadcast variable?

   rdds are divided into partitions: each partition can be considered as
   an immutable subset of the entire rdd. when you execute your spark
   program, each partition gets sent to a worker. this means that each
   worker operates on the subset of the data. each worker can cache the
   data if the rdd needs to be re-iterated: the partitions that it
   elaborates are stored in memory and will be reused in other actions. as
   you read in the above paragraph, by persisting, spark will have faster
   access to that data partition next time an operation makes use of it.

   but you need to keep in mind that when you pass a function to a spark
   operation, it is executed on separate cluster nodes. every node
   receives a copy of the variable inside the function, and so every
   change to the local value of the variable is not propagated to the
   driver program.

   a typical use case in which this might happen is when you have to
   redistribute intermediate results of operations, such as trained
   models, or as static lookup tables in cases where you want to perform
   lookups against a small table to join it together with your bigger data
   set.

   instead of creating a copy of the variable for each machine, you use
   broadcast variables to send some immutable state once to each worker.
   broadcast variables allow the programmer to keep a cached read-only
   variable in every machine. in short, you use these variables when you
   want a local copy of a variable.

   you can create a broadcast variable with
   sparkcontext.broadcast(variable). this will return the reference of the
   broadcast variable.

   as you can see, persisting an rdd or using a broadcast variable are two
   different solutions to different problems.

what are the best practices in spark?

   there are tons of possibilities when you   re working with pyspark, but
   that doesn   t mean that there are some simple and general best practices
   that you can follow:

use spark dataframes

   consider the section above to see whether you should use rdds or
   dataframes. as you already read above, spark dataframes are optimized
   and therefore also faster than rdds. especially when you   re working
   with structured data, you should really consider switching your rdd to
   a dataframe.

rdd best practices

don   t call collect() on large rdds

   by calling collect() on any rdd, you drag data back into your
   applications from the nodes. each rdd element will be copy onto the
   single driver program, which will run out of memory and crash. given
   the fact that you want to make use of spark in the most efficient way
   possible, it   s not a good idea to call collect() on large rdds.

   other functions that you can use to inspect your data are take() or
   takesample(), but also countbykey(), countbyvalue() or collectasmap()
   can help you out. if you really need to take a look at the complete
   data, you can always write out the rdd to files or export it to a
   database that is large enough to keep your data.

reduce your rdd before joining

   the fact that you can chain operations comes in handy when you   re
   working with spark rdds, but what you might not realize is that you
   have a responsibility to build efficient transformation chains, too.
   taking care of the efficiency is also a way of tuning your spark jobs   
   efficiency and performance.

   one of the most basic rules that you can apply when you   re revising the
   chain of operations that you have written down is to make sure that you
   filter or reduce your data before joining it. this way, you avoid
   sending too much data over the network that you   ll throw away after the
   join, which is already a good reason, right?

   but there is more. the join operation is one of the most expensive
   operations that you can use in spark, so that   s why it makes sense to
   be wary of this. when you reduce the data before the join, you avoid
   shuffling your data around too much.

avoid groupbykey() on large rdds

   on big data sets, you   re better off making use of other functions, such
   as reducebykey(), combinebykey() or foldbykey(). when you use
   groupbykey(), all key-value pairs are shuffled around in the cluster. a
   lot of unnecessary data is being transferred over the network.
   additionally, this also means that if more data is shuffled onto a
   single machine than can fit in memory, the data will be spilled to
   disk. this heavily impacts the performance of your spark job.

   when you make use of reducebykey(), for example, the pairs with the
   same key are already combined before the data is shuffled. as a result,
   you   ll have to send less data over the network. next, the reduce
   function is called again so that all the values from each partition are
   reduced.

broadcast variables

   since you already know what broadcast variables are and in which
   situations they can come in handy, you   ll also have gathered that this
   is one of the best practices for when you   re working with spark because
   you can reduce the cost of launching a job over the cluster.

avoid flatmap(), join() and groupby() pattern

   when you have two datasets that are grouped by key and you want to join
   them, but still keep them grouped, use cogroup() instead of the above
   pattern.

spark ui

   you already read about it in one of the sections above, but making use
   of the spark ui is really something that you can not miss. this web
   interface allows you to monitor and inspect the execution of your jobs
   in a web browser, which is extremely important if you want to exercise
   control over your jobs.

   the spark ui allows you to maintain an overview off your active,
   completed and failed jobs. you can see when you submitted the job, and
   how long it took for the job to run. besides the schematic overview,
   you can also see the event timeline section in the    jobs    tab. make
   sure to also find out more about your jobs by clicking the jobs
   themselves. you   ll get more information on the stages of tasks inside
   it.

   the    stages    tab in the ui shows you the current stage of all stages of
   all jobs in a spark application, while the    storage    tab will give you
   more insights on the rdd size and the memory use.

   [27]learn python for data science with datacamp

how to turn off pyspark logging

   go to the spark directory and execute the following command:
cp conf/log4j.properties.template conf/log4j.properties

   note that this command copies the file log4j.properties.template into
   the same folder conf, but under a different name, namely
   log4j.properties instead of the original name
   log4j.properties.template. next, you should also know wthat apache
   log4j is a java-based logging utility. in short, you   re copying a
   logging template.

   next, before you start editing anything, take your time to inspect the
   template file log4j.properties.template with a text editor. you   ll see
   a section like this:
# set everything to be logged to the console
log4j.rootcategory=info, console
log4j.appender.console=org.apache.log4j.consoleappender
log4j.appender.console.target=system.err
log4j.appender.console.layout=org.apache.log4j.patternlayout
log4j.appender.console.layout.conversionpattern=%d{yy/mm/dd hh:mm:ss} %p %c{1}:
%m%n

   you   ll see that the log4j rootcategory attribute is set to info by
   default. that means that you   ll get to see logs that are at info level
   and that offer    informational messages that highlight the progress of
   the application at coarse-grained level   . it   s not hard to imagine that
   this level of logging could offer you many, if not too many, logs.
   however, turning the logging entirely off is usually not a good option
   either because you usually do want to stay in the loop with whatever
   your application is doing: this makes debugging or spotting anomalies
   prematurely a lot easier.

   consider setting the level to warn, which will indicate potentially
   harmful situations in the logs, or error, which indicate error events
   that might not block your application from running. you can find the
   other levels [28]here.

   you know now what to do: edit log4j.properties with nano
   log4j.properties or vim and replace the info with the desired level of
   logging.

   save the file correctly and restart your shell. run the application
   again and you   ll see that the logging has reduced to the level that you
   defined in the properties file.

spark up your data analyses!

   you   ve reached the end of this tutorial so now it   s time for you to
   start working with spark if you   ve just read and not followed the
   tutorial!

   if you are interested in learning more about pyspark, consider taking
   datacamp   s [29]introduction to pyspark course.
   88
   88
   [30]0
   related posts
   python
   +1

[31]pyspark cheat sheet: spark dataframes in python

   karlijn willems
   june 15th, 2017
   machine learning
   +3

[32]apache spark tutorial: ml with pyspark

   karlijn willems
   july 28th, 2017
   big data
   +1

[33]fast track apache spark

   tobi bosede
   december 15th, 2017
   (button)
   post a comment

   [34]subscribe to rss
   [35]about[36]terms[37]privacy

   want to leave a comment?

references

   visible links
   1. https://www.datacamp.com/users/sign_in
   2. https://www.datacamp.com/community/tutorials/apache-spark-python#comments
   3. https://www.datacamp.com/community/tutorials/apache-spark-python#language
   4. https://www.datacamp.com/community/tutorials/apache-spark-python#pyspark
   5. https://www.datacamp.com/community/tutorials/apache-spark-python#difference
   6. https://www.datacamp.com/community/tutorials/apache-spark-python#pandas
   7. https://www.datacamp.com/courses/pandas-foundations/
   8. https://www.datacamp.com/community/tutorials/apache-spark-python#rdd
   9. https://www.datacamp.com/community/tutorials/apache-spark-python#cache
  10. https://www.datacamp.com/community/tutorials/broadcast
  11. https://www.datacamp.com/community/tutorials/apache-spark-python#bestpractices
  12. https://www.datacamp.com/community/tutorials/apache-spark-python#logging
  13. https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python/
  14. http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
  15. https://spark.apache.org/downloads.html
  16. https://try.jupyter.org/
  17. https://www.continuum.io/downloads
  18. https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook/
  19. https://github.com/minrk/findspark
  20. https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook##installjupyter
  21. https://hub.docker.com/
  22. https://hub.docker.com/r/jupyter/pyspark-notebook/
  23. https://github.com/jupyter/docker-stacks
  24. https://www.datacamp.com/community/blog/ipython-jupyter/
  25. https://www.datacamp.com/courses/
  26. http://spark.apache.org/docs/1.2.0/programming-guide.html#rdd-persistence
  27. https://www.datacamp.com/courses/
  28. https://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/level.html
  29. https://www.datacamp.com/courses/introduction-to-pyspark
  30. https://www.datacamp.com/community/tutorials/apache-spark-python#comments
  31. https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet
  32. https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning
  33. https://www.datacamp.com/community/blog/fast-track-apache-spark
  34. https://www.datacamp.com/community/rss.xml
  35. https://www.datacamp.com/about
  36. https://www.datacamp.com/terms-of-use
  37. https://www.datacamp.com/privacy-policy

   hidden links:
  39. https://www.datacamp.com/
  40. https://www.datacamp.com/community
  41. https://www.datacamp.com/community/tutorials
  42. https://www.datacamp.com/community/data-science-cheatsheets
  43. https://www.datacamp.com/community/open-courses
  44. https://www.datacamp.com/community/podcast
  45. https://www.datacamp.com/community/chat
  46. https://www.datacamp.com/community/blog
  47. https://www.datacamp.com/community/tech
  48. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/apache-spark-python
  49. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/apache-spark-python
  50. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/apache-spark-python
  51. https://www.datacamp.com/profile/karlijn
  52. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/apache-spark-python
  53. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/apache-spark-python
  54. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/apache-spark-python
  55. https://www.datacamp.com/profile/karlijn
  56. https://www.datacamp.com/profile/karlijn
  57. https://www.datacamp.com/profile/anitobib
  58. https://www.facebook.com/pages/datacamp/726282547396228
  59. https://twitter.com/datacamp
  60. https://www.linkedin.com/company/datamind-org
  61. https://www.youtube.com/channel/uc79gv3myp6zkiswyemeik9a
