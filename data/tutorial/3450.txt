   #[1]facebook research    feed [2]facebook research    comments feed
   [3]alternate [4]alternate

   [5]skip to content

   iframe: [6]https://www.googletagmanager.com/ns.html?id=gtm-wspss3d

   [7]facebook research
   search ____________________ (button) submit
   [8]toggle search (button)

     * [9]research areas (button) show dropdown menu
          + [10]ar/vr
          + [11]connectivity
          + [12]computational photography & intelligent cameras
          + [13]id161
          + [14]data science
          + [15]economics & computation
          + [16]facebook ai research
          + [17]human computer interaction & ux
          + [18]machine learning
          + [19]natural language processing & speech
          + [20]security & privacy
          + [21]systems & networking
     * [22]publications
     * [23]people
     * [24]academic programs (button) show dropdown menu
          + [25]facebook fellowship program
          + [26]research awards
          + [27]research collaborations
          + [28]research award recipients
          + [29]visiting researchers and postdocs
     * [30]downloads & projects
     * [31]careers
     * [32]blog

   search ____________________ (button) submit
   [33]close search

babi

the babi project

   this page gather resources related to the babi project of facebook ai
   research which is organized towards the goal of automatic text
   understanding and reasoning. the datasets we have released consist of:
     * the (20) qa babi tasks
     * the (6) dialog babi tasks
     * the children   s book test
     * the movie dialog dataset
     * the wikimovies dataset
     * the dialog-based language learning dataset
     * the simplequestions dataset
     * hitl dialogue simulator

the babi tasks

   this section presents the first set of 20 tasks for testing text
   understanding and reasoning in the babi project. the tasks are
   described in detail in the paper:

   jason weston, antoine bordes, sumit chopra, alexander m. rush, bart van
   merri  nboer, armand joulin and tomas mikolov. [34]towards ai complete
   id53: a set of prerequisite toy tasks, arxiv:1502.05698.

   please also see the following slides:

   antoine bordes [35]artificial tasks for artificial intelligence, iclr
   keynote, 2015.

   the aim is that each task tests a unique aspect of text and reasoning,
   and hence test different capabilities of learning models. more tasks
   are planned in the future to capture more aspects.

   training set size: for each task, there are 1000 questions for
   training, and 1000 for testing. however, we emphasize that the goal is
   to use as little data as possible to do well on the tasks (i.e. if you
   can use less than 1000 that   s even better)     and without resorting to
   engineering task-specific tricks that will not generalize to other
   tasks, as they may not be of much use subsequently. note that the aim
   during evaluation is to use the _same_ learner across all tasks to
   evaluate its skills and capabilities.

   supervision signal: further while the memnn results in the paper use
   full supervision (including of the supporting facts) results with weak
   supervision would also be ultimately preferable as this kind of data is
   easier to collect. hence results of that form are very welcome. e.g.
   [36]this paper does include weakly supervised results.

   for the reasons above there are currently several directories:
     * 1) en/     the tasks in english, readable by humans.
     * 2) hn/     the tasks in hindi, readable by humans.
     * 3) shuffled/     the same tasks with shuffled letters so they are not
       readable by humans, and for existing parsers and taggers cannot be
       used in a straight-forward fashion to leverage extra resources    in
       this case the learner is more forced to rely on the given training
       data. this mimics a learner being first presented a language and
       having to learn from scratch.
     * 4) en-10k/ shuffled-10k/ and hn-10k/     the same tasks in the three
       formats, but with 10,000 training examples, rather than 1000
       training examples. note the results in the paper use 1000 training
       examples.

   the file format for each task is as follows:
id text
id text
id text
id question[tab]answer[tab]supporting fact ids.
...

   the ids for a given    story    start at 1 and increase. when the ids in a
   file reset back to 1 you can consider the following sentences as a new
      story   . supporting fact ids only ever reference the sentences within a
      story   .

   for example:
1 mary moved to the bathroom.
2 john went to the hallway.
3 where is mary?        bathroom        1
4 daniel went back to the hallway.
5 sandra moved to the garden.
6 where is daniel?      hallway 4
7 john moved to the office.
8 sandra journeyed to the bathroom.
9 where is daniel?      hallway 4
10 mary moved to the hallway.
11 daniel travelled to the office.
12 where is daniel?     office  11
13 john went back to the garden.
14 john moved to the bedroom.
15 where is sandra?     bathroom        8
1 sandra travelled to the office.
2 sandra went to the bathroom.
3 where is sandra?      bathroom        2

   versions: some small updates since the original release have been made
   (see the readme in the data download for more details). you can also
   get [37]v1.0 and [38]v1.1 here.

   data stats: some data statistics including overlap between train and
   test (which is minimal) can be found [39]here. code code to generate
   tasks is available on [40]github. we hope this will encourage the
   machine learning community to work on, and develop more, of these
   tasks.

the (6) dialog babi tasks

   this section presents the set of 6 tasks for testing end-to-end dialog
   systems in the restaurant domain described in the paper:

   [41]antoine bordes[42], [43]y-lan boureau[44], [45]jason
   weston, [46]learning end-to-end goal-oriented dialog, arxiv:1605.07683.

   each task tests a unique aspect of dialog. tasks are designed to
   complement the set of 20 babi tasks for story understanding of the
   previous section.

   for each task, there are 1000 dialogs for training, 1000 for
   development and 1000 for testing. for tasks 1-5, we also include a
   second test set (with suffix -oov.txt) that contains dialogs including
   entities not present in training and development sets.

   the file format for each task is as follows:
id user_utterance [tab] bot_utterance
...

   the ids for a given dialog start at 1 and increase. when the ids in a
   file reset back to 1 you can consider the following sentences as a new
   dialog. when the bot speaks two times in a row, we used the special
   token    <silence>    to fill in for the missing user utterance. see more
   details in the readme included with the dataset. the goal of the tasks
   is to predict the bot utterances, that can be sentences or api calls
   (sentences starting with the special token    api_call   ). here is an
   example of dialog (from task 1):
1 hi    hello what can i help you with today
2 can you make a restaurant reservation with italian cuisine for six people in a
 cheap price range      i'm on it
3 <silence>     where should it be
4 rome please   ok let me look into some options for you
5 <silence>     api_call italian rome six cheap

the children   s book test

   this section presents the children   s book test (cbt), designed to
   measure directly how well language models can exploit wider linguistic
   context. the cbt is built from books that are freely available thanks
   to [47]project gutenberg. details and baseline results on this dataset
   can be found in the paper:

   felix hill, antoine bordes, sumit chopra and jason weston. [48]the
   goldilocks principle: reading children   s books with explicit memory
   representations, arxiv:1511.02301.

   after allocating books to either training, validation or test sets, we
   formed example    questions    from chapters in the book by enumerating 21
   consecutive sentences. in each question, the first 20 sentences form
   the context, and a word is removed from the 21st sentence, which
   becomes the query. models must identify the answer word among a
   selection of 10 candidate answers appearing in the context sentences
   and the query. for finer-grained analyses, we evaluated four classes of
   question by removing distinct types of word: named entities, (common)
   nouns, verbs and prepositions

   here is an example of question (context + query) from alice in
   wonderland by lewis carroll:
context:
1 so they had to fall a long way .
2 so they got their tails fast in their mouths .
3 so they could n't get them out again .
4 that 's all .
5 `` thank you , " said alice , `` it 's very interesting .
6 i never knew so much about a whiting before . "
7 `` i can tell you more than that , if you like , " said the gryphon .
8 `` do you know why it 's called a whiting ? "
9 `` i never thought about it , " said alice .
10 `` why ? "
11 `` it does the boots and shoes . '
12 the gryphon replied very solemnly .
13 alice was thoroughly puzzled .
14 `` does the boots and shoes ! "
15 she repeated in a wondering tone .
16 `` why , what are your shoes done with ? "
17 said the gryphon .
18 `` i mean , what makes them so shiny ? "
19 alice looked down at them , and considered a little before she gave her answe
r .
20 `` they 're done with blacking , i believe . "

query: `` boots and shoes under the sea , " the xxxxx went on in a deep voice ,
`` are done with a whiting ".

candidates: alice|boots|gryphon|shoes|answer|fall|mouths|tone|way|whiting

answer: gryphon

the movie dialog dataset

   this section presents the movie dialog dataset (mdd), designed to
   measure how well models can perform at goal and non-goal orientated
   dialog centered around the topic of movies (id53,
   recommendation and discussion). details and baseline results on this
   dataset can be found in the paper:

   jesse dodge, andreea gane, xiang zhang, antoine bordes, sumit chopra,
   alexander miller, arthur szhttp://arxiv.org/abs/1511.06931lam, jason
   weston. [49]evaluating prerequisite qualities for learning end-to-end
   id71, arxiv:1511.06931.

   the file format is again the same as in the babi tasks. the ids for a
   given dialog start at 1 and increase. each id consists of one turn for
   each speaker (an    exchange   ), which are tab separated. when the ids in
   a file reset back to 1 you can consider the following sentences as a
   new conversation.
   for example:
1 scarface, the kite runner, the shining, eternal sunshine of the spotless mind,
 avatar, requiem for a dream, and lolita are movies i really like. i'm looking f
or a drama movie.       dogville
2 who does that star?   nicole kidman, lauren bacall
3 i like ray milland movies more. do you know anything else?    the thief

the wikimovies dataset

   this includes only the qa part of the movie dialog dataset, but using
   three different settings of knowledge: using a traditional knowledge
   base (kb), using wikipedia as the source of knowledge, or using ie
   (information extraction) over wikipedia. this allows to test the
   ability of models to directly read documents to answer questions, and
   to compare this to traditional kbs in the same setting. see the paper
   for more details:

   a. h. miller, a. fisch, j. dodge, a. karimi, a. bordes, j. weston.
   [50]key-value memory networks for directly reading
   documents, arxiv:1606.03126.

the dialog-based language learning dataset

   this section presents the dialog-based language learning dataset,
   designed to measure how well models can perform at learning as a
   student given a teacher   s textual responses to the student   s answer (as
   well as potentially receiving an external real-valued reward signal).
   details and baseline results on this dataset can be found in the paper:

   jason weston. [51]dialog-based language learning, arxiv:1604.06045.

   here is an example dialog, the last number (0 or 1) is the external
   reward:
1 mary moved to the bathroom.           0
2 john went to the hallway.             0
3 where is mary?        bathroom        0
4 that's right.         1
5 daniel went back to the hallway.              0
6 sandra moved to the garden.           0
7 where is daniel?      office  0
8 no, they are downstairs.              0
9 john moved to the office.             0
10 sandra journeyed to the bathroom.            0
11 where is daniel?     office  0
12 no, they are downstairs.             0
13 mary moved to the hallway.           0
14 daniel travelled to the office.              0
15 where is daniel?     office  0
16 correct!             1
17 john went back to the garden.                0
18 john moved to the bedroom.           0
19 where is sandra?     garden  0
20 no, they are upstairs.               0

the simplequestions dataset

   this section proposes simplequestions, a dataset collected for research
   in automatic id53 with human generated questions. details
   and baseline results on this dataset can be found in the paper:

   antoine bordes, nicolas usunier, sumit chopra and jason weston.
   [52]large-scale simple id53 with memory
   networks, arxiv:1506.02075.

   the simplequestions dataset consists of a total of 108,442 questions
   written in natural language by human english-speaking annotators each
   paired with a corresponding fact, formatted as (subject, relationship,
   object), that provides the answer but also a complete explanation.
   facts have been extracted from the knowledge base [53]freebase. we
   randomly shuffle these questions and use 70% of them (75910) as
   training set, 10% as validation set (10845), and the remaining 20% as
   test set.

   here are some examples of questions and facts:
* what american cartoonist is the creator of andy lippincott?
  fact: (andy_lippincott, character_created_by, garry_trudeau)
* which forest is fires creek in?
  fact: (fires_creek, containedby, nantahala_national_forest)
* what does jimmy neutron do?
  fact: (jimmy_neutron, fictional_character_occupation, inventor)
* what dietary restriction is incompatible with kimchi?
  fact: (kimchi, incompatible_with_dietary_restrictions, veganism)
                                                            

   update (dec. 15): v2 of the data set now includes the subset of
   freebase used in the paper    simple id53 with memory
   networks   .

hitl dialogue simulator

   the human-in-the-loop (htil) dialogue simulator provides a framework
   for evaluating a bot   s ability to learn to improve its performance in
   an online setting using feedback from its dialog partner. the dataset
   contains questions based on the babi and wikimovies datasets, but now
   with feedback from the dialog partner we include both simulated and
   human dialogs. dialogs follow the same form as in the dialog based
   language learning datasets, but now depend on the model   s
   predictions via the simulator.

   full details on this simulator are available in the following paper:

   j. li, a. h. miller, s. chopra, m. ranzato, j. weston. dialogue
   learning with human-in-the-loop (forthcoming), arxiv:1611.09823.
     * [54]babi on github
     * [55]babi tasks data 1-20 (v1.2)
     * [56]dialog babi tasks data 1-6
     * [57]children's book test
     * [58]movie dialog dataset
     * [59]wikimovies dataset
     * [60]dialog-based ll dataset
     * [61]simplequestions (v2)
     * [62]hitl dialogue simulator

   areas: [63]facebook ai research facebook ai research [64]machine
   learning machine learning [65]natural language processing & speech
   natural language processing & speech

     * [66]babi on github
     * [67]babi tasks data 1-20 (v1.2)
     * [68]dialog babi tasks data 1-6
     * [69]children's book test
     * [70]movie dialog dataset
     * [71]wikimovies dataset
     * [72]dialog-based ll dataset
     * [73]simplequestions (v2)
     * [74]hitl dialogue simulator

   iframe:
   [75]https://www.facebook.com/plugins/page.php?href=https%3a%2f%2fwww.fa
   cebook.com%2facademics&tabs&width=340&height=70&small_header=true&adapt
   _container_width=true&hide_cover=true&show_facepile=false&appid

     * [76]rss feed
     * [77]about
     * [78]careers
     * [79]privacy
     * [80]cookies
     * [81]terms
     * [82]help

   facebook    2019

   to help personalize content, tailor and measure ads, and provide a
   safer experience, we use cookies. by clicking or navigating the site,
   you agree to allow our collection of information on and off facebook
   through cookies. learn more, including about available controls:
   [83]cookies policy

references

   1. https://research.fb.com/feed/
   2. https://research.fb.com/comments/feed/
   3. https://research.fb.com/wp-json/oembed/1.0/embed?url=https://research.fb.com/downloads/babi/
   4. https://research.fb.com/wp-json/oembed/1.0/embed?url=https://research.fb.com/downloads/babi/&format=xml
   5. https://research.fb.com/downloads/babi/#content
   6. https://www.googletagmanager.com/ns.html?id=gtm-wspss3d
   7. https://research.fb.com/
   8. javascript:void(0)
   9. https://research.fb.com/research-areas/
  10. https://research.fb.com/category/augmented-reality-virtual-reality/
  11. https://research.fb.com/category/connectivity/
  12. https://research.fb.com/category/computational-photography-and-intelligent-cameras/
  13. https://research.fb.com/category/computer-vision/
  14. https://research.fb.com/category/data-science/
  15. https://research.fb.com/category/economics-and-computation/
  16. https://research.fb.com/category/facebook-ai-research
  17. https://research.fb.com/category/human-computer-interaction-and-ux/
  18. https://research.fb.com/category/machine-learning/
  19. https://research.fb.com/category/natural-language-processing-and-speech/
  20. https://research.fb.com/category/security-and-privacy/
  21. https://research.fb.com/category/systems-and-networking/
  22. https://research.fb.com/publications/
  23. https://research.fb.com/people/
  24. https://research.fb.com/programs/
  25. https://research.fb.com/programs/fellowship/
  26. https://research.fb.com/programs/research-awards/
  27. https://research.fb.com/programs/research-collaborations/
  28. https://research.fb.com/programs/faculty-awards
  29. https://research.fb.com/programs/post-docs-and-sabbaticals/
  30. https://research.fb.com/downloads/
  31. https://research.fb.com/careers/
  32. https://research.fb.com/blog/
  33. javascript:void(0)
  34. http://arxiv.org/abs/1502.05698
  35. http://www.thespermwhale.com/jaseweston/babi/abordes-iclr.pdf
  36. http://arxiv.org/abs/1503.08895
  37. http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1.tar.gz
  38. http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-1.tar.gz
  39. http://www.thespermwhale.com/jaseweston/babi/data-overlap.txt
  40. https://github.com/facebook/babi-tasks
  41. https://research.fb.com/people/bordes-antoine/
  42. http://arxiv.org/abs/1605.07683
  43. https://research.fb.com/people/boureau-y-lan/
  44. http://arxiv.org/abs/1605.07683
  45. https://research.fb.com/people/weston-jason/
  46. https://research.fb.com/publications/learning-end-to-end-goal-oriented-dialog/
  47. https://www.gutenberg.org/
  48. https://research.fb.com/publications/the-goldilocks-principle-reading-childrens-books-with-explicit-memory-representations/
  49. https://research.fb.com/publications/evaluating-prerequisite-qualities-for-learning-end-to-end-dialog-systems/
  50. https://research.fb.com/publications/key-value-memory-networks-for-directly-reading-documents/
  51. http://arxiv.org/abs/1604.06045
  52. https://research.fb.com/publications/large-scale-simple-question-answering-with-memory-networks/
  53. http://www.freebase.com/
  54. https://github.com/facebook/babi-tasks
  55. http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz
  56. https://fb-public.box.com/s/chnq60iivzv5uckpvj2n2vijlyepze6w
  57. http://www.thespermwhale.com/jaseweston/babi/cbtest.tgz
  58. http://www.thespermwhale.com/jaseweston/babi/movie_dialog_dataset.tgz
  59. http://www.thespermwhale.com/jaseweston/babi/movieqa.tar.gz
  60. http://www.thespermwhale.com/jaseweston/babi/dbll.tgz
  61. https://www.dropbox.com/s/tohrsllcfy7rch4/simplequestions_v2.tgz
  62. https://github.com/facebook/memnn/tree/master/hitl
  63. https://research.fb.com/category/facebook-ai-research/
  64. https://research.fb.com/category/machine-learning/
  65. https://research.fb.com/category/natural-language-processing-and-speech/
  66. https://github.com/facebook/babi-tasks
  67. http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz
  68. https://fb-public.box.com/s/chnq60iivzv5uckpvj2n2vijlyepze6w
  69. http://www.thespermwhale.com/jaseweston/babi/cbtest.tgz
  70. http://www.thespermwhale.com/jaseweston/babi/movie_dialog_dataset.tgz
  71. http://www.thespermwhale.com/jaseweston/babi/movieqa.tar.gz
  72. http://www.thespermwhale.com/jaseweston/babi/dbll.tgz
  73. https://www.dropbox.com/s/tohrsllcfy7rch4/simplequestions_v2.tgz
  74. https://github.com/facebook/memnn/tree/master/hitl
  75. https://www.facebook.com/plugins/page.php?href=https://www.facebook.com/academics&tabs&width=340&height=70&small_header=true&adapt_container_width=true&hide_cover=true&show_facepile=false&appid
  76. https://research.fb.com/feed/
  77. https://www.facebook.com/facebook
  78. https://www.facebook.com/careers/university/
  79. https://www.facebook.com/about/privacy
  80. https://www.facebook.com/help/cookies
  81. https://www.facebook.com/policies
  82. https://www.facebook.com/help
  83. https://www.facebook.com/policies/cookies/
