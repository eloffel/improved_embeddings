   #[1]piotr migda   - blog

   [2]piotr migda   - blog

   [3]projects [4]articles [5]publications [6]resume [7]about [8]photos

king - man + woman is queen; but why?

   6 jan 2017     piotr migda       [machine-learning] [id97]
   see: [9]hacker news discussion thread with over 250 upvotes

intro

   id97 is an algorithm that transforms words into vectors, so that
   words with similar meaning end up laying close to each other. moreover,
   it allows us to use vector arithmetics to work with analogies, for
   example the famous king - man + woman = queen.

   i will try to explain how it works, with special emphasis on the
   meaning of vector differences, at the same time omitting as many
   technicalities as possible.

   if you would rather explore than read, here is an interactive
   exploration by my mentee julia bazi  ska, now a freshman of computer
   science at the university of warsaw:
     * [10]word2viz by using [11]glove pre-trained vectors (it takes 30mb
       to load - please be patient)

counts, coincidences and meaning

     i love letter co-occurrence in the word co-occurrence.

   sometimes a seemingly naive technique gives powerful results. it turns
   out that merely looking at word coincidences, while ignoring all
   grammar and context, can provide us insight into the meaning of a word.
   consider this sentence:

     a small, fluffy roosety climbed a tree.

   what   s a roosety? i would say that something like a squirrel, since the
   two words can be easily interchanged. such reasoning is called the
   [12]distributional hypothesis and can be summarized as:

     a word is characterized by the company it keeps - [13]john rupert
     firth

   if we want to teach it to a computer, the simplest, approximated
   approach is making it look only at word pairs. let p(a|b) be the
   id155 that given a word b there is a word a within a
   short distance (let   s say - being spaced by no more that 2 words). then
   we claim that two words a and b are similar if

   for every word w. in other words, if we have this equality, no matter
   if there is word a or b, all other words occur with the same frequency.

   even simple word counts, compared by source, can give interesting
   results, e.g. that in lyrics of metal songs words (cries, eternity or
   ashes are popular, while words particularly or approximately are not,
   well, particularly common), see [14]heavy metal and natural language
   processing. see also [15]gender roles with id111 and id165s by
   julia silge.

   looking at co-occurrences can provide much more information. for
   example, one of my projects, [16]tagoverflow, gives insight into
   structure of programming, based only on the usage of [17]tags on stack
   overflow. it also shows that i am in love with pointwise mutual
   information, which brings us to the next point.

pointwise mutual information and compression

     the compressors: view cognition as compression. compressed sensing,
     approximate id105 - [18]the (n) cultures of machine
     learning - hn discussion

   in principle, we can compute p(a|b) for every word pair. but even with
   a small dictionary of 100 000 words (bear in mind that we need to keep
   all declinations, proper names and things which are not in official
   dictionaries, yet are in use) keeping track of all pairs would require
   8 gigabytes of space.

   often instead of working with conditional probabilities, we use the
   [19]pointwise mutual information (pmi), defined as:

   its direct interpretation is how much more likely we get a pair than if
   it were [20]at random. the logarithm makes it easier to work with
   [21]words appearing at frequencies of different orders of magnitude. we
   can approximate pmi as a scalar product:

   where are vectors, typically of 50-300 dimensions.

   at the first glance it may be strange that all words can be compressed
   to a space of much smaller dimensionality. but there are words that can
   be trivially interchanged (e.g. john to peter) and there is a lot of
   structure in general.

   the fact that this compression is lossy may give it an advantage, as it
   can discover patterns rather than only memorize each pair. for example,
   in id126s for movie ratings, each rating is
   approximated by a scalar product of two vectors - a movie   s content and
   a user   s preference. this is used to predict scores for as yet unseen
   movies, see [22]id105 with tensorflow - katherine
   bailey.

word similarity and vector closeness

     stock market     thermometer - [23]five crazy abstractions my id97
     model just did

   let us start with the simple stuff - showing word similarity in a
   vector space. the condition that is equivalent to

   by dividing both sides by p(w) and taking their logarithms. after
   expressing pmi with vector products, we get

   if it needs to work for every , then

   of course, in every practical case we won   t get an exact equality, just
   words being close to each other. words close in this space are often
   synonyms (e.g. happy and delighted), antonyms (e.g. good and evil) or
   other easily interchangeable words (e.g. yellow and blue). in
   particular most of [24]opposing ideas (e.g. religion and atheism) will
   have similar context. if you want to play with that, look at this
   [25]word2sense phrase search.

   what i find much more interesting is that words form a linear space. in
   particular, a zero vector represent a totally uncharacteristic word,
   occurring with every other word at the random chance level (as its
   scalar product with every word is zero, so is its pmi).

   it is one of the reasons why for vector similarity people often use
   cosine distance, i.e.

   that is, it puts emphasis on the direction in which a given word
   co-occurs with other words, rather than the strength of this effect.

analogies and linear space

   if we want to make word analogies (a is to b is as a is to b), one may
   argue that in can be expressed as an equality of conditional
   id203 ratios

   for every word w. sure, it looks (and is!) a questionable assumption,
   but it is still the best thing we can do with id155.
   i will not try defending this formulation - i will show its equivalent
   formulations.

   for example, if we want to say dog is to puppy as cat is to kitten, we
   expect that if e.g. word nice co-occurs with both dog and cat (likely
   with different frequencies), then it co-occurs with puppy and kitten by
   the same factor. it appears it is true, with the factor of two favoring
   the cubs - compare [26]pairs to [27]words from [28]google books ngram
   viewer (while id165s look only at adjacent words, they can be some
   sort of approximation).

   by proposing ratios for word analogies we implicitly assume that the
   probabilities of words can be factorized with respect to different
   dimensions of a word. for the discussed case it would be:

   so, in particular:

   how does the equality of id155 ratios translate to
   the word vectors? if we express it as mutual information (again, p(w)
   and logarithms) we get

   which is the same as

   again, if we want it to hold for any word w, this vector difference
   needs to be zero.

   we can use analogies for meaning (e.g. changing gender with vectors),
   grammar (e.g. changing tenses) or other analogies (e.g. cities into
   their zip codes). it seems that analogies are not only a computational
   trick - we may actually use them to think all the time, see:
     * george lakoff, mark johnson, [29]metaphors we live by (1980)
     * and [30]their list of conceptual metaphors in english (webarchive),
       in particular look for x is up, plotted below:

differences and projections

     woman - man = female - male = she - he, so wo = fe = s (a joke)

   difference of words vectors, like

   are not words vectors by themselves. however, it is interesting to
   project a word on this axis. we can see that the projection

   is exactly a relative occurrence of a word within different contexts.

   bear in mind that when we want to look at common aspects of a word it
   is more natural to average two vectors rather than take their sum.
   while people use it interchangeably, it only works because cosine
   distance ignores the absolute vector length. so, for a gender neutral
   pronoun use rather than their sum.

   just looking at the word co-locations can give interesting results,
   look at these artistic projects - [31]word spectrum and [32]word
   associations from visualizing google   s bi-gram data by [33]chris
   harrison.

i want to play!

   if you want explore, there is [34]word2viz by julia bazi  ska. you can
   choose between one of several pre-defined plots, or create one from
   scratch (choosing words and projections). i   ve just realized that
   google research released a tool for that as well: [35]open sourcing the
   embedding projector: a tool for visualizing high dimensional data -
   google research blog (and the actual live demo: [36]embedding
   projector).

   if you want to use pre-trained vectors, see [37]stanford glove or
   [38]google id97 download sections. some examples in jupyter
   notebooks are in our playground [39]github.com/lamyiowce/word2viz
   (warning: raw state, look at them at your own risk).

   if you want to train it on your own dataset, use a python library
   [40]gensim: topic modelling for humans. often some preprocessing is
   needed, especially look at [41]sense2vec with spacy and gensim by
   matthew honnibal.

   if you want to create it from scratch, the most convenient way is to
   start with [42]vector representations of words - tensorflow tutorial
   (see also a respective [43]jupyter notebook from udacity course).

   if you want to learn how it works, i recommend the following materials:
     * [44]a word is worth a thousand vectors by chris moody
     * daniel jurafsky, james h. martin, [45]speech and language
       processing (2015):
          + [46]chapter 15: vector semantics (and [47]slides)
          + [48]chapter 16: semantics with dense vectors (and [49]slides)
     * [50]distributional approaches to word meanings from [51]seminar:
       representations of meaning course at stanford by noah d. goodman
       and christopher potts
     * [52]glove: global vectors for word representation and paper
          + jeffrey pennington, richard socher, christopher d. manning,
            [53]glove: global vectors for word representation (2014)
          + it   s great, except for its claims for greatness, see:
            [54]glove vs id97 by radim rehurek
     * [55]on chomsky and the two cultures of statistical learning by
       peter norvig

     julia bazi  ska, at the rooftop garden of the [56]warsaw university
     library - the building in which we worked

technicalities

   i tried to give some insight into algorithms transforming words into
   vectors. every practical approach needs a bit more tweaking. here are a
   few clarifications:
     * id97 is not a single task or algorithm; popular ones are:
          + skip-gram negative-sampling (implicit compression of pmi),
          + skip-gram noise-contrastive training (implicit compression of
            id155),
          + glove (explicit compression of co-occurrences),
     * while word and context are essentially the same thing (both are
       words), they are being probed and treated differently (to account
       for different word frequencies),
     * there are two sets of vectors (each word has two vectors, one for
       word and the other - for context),
     * as any practical dataset of occurrences would contain pmi for some
       pairs, in most cases positive pointwise mutual information (ppmi)
       is being used instead,
     * often pre-processing is needed (e.g. to catch phrases like machine
       learning or to distinguish words with two separate meanings),
     * linear space of meaning is a disputed concept,
     * all results are a function of the data we used to feed our
       algorithm, not objective truth; so it is easy to get stereotypes
       like doctor - man + woman = nurse.

   for further reading i recommend:
     * [57]how does id97 work? by omer levy
     * omer levy, yoav goldberg, [58]neural id27s as implicit
       id105, nips 2014
     * with a caveat: [59]skipgram isn   t matrix factorisation by benjamin
       wilson
     * [60]language bias and black sheep
     * [61]language necessarily contains human biases, and so will
       machines trained on language corpora by arvind narayanan
     * [62]id27s: explaining their properties - on word
       analogies by sanjeev arora
     * tal linzen, [63]issues in evaluating semantic spaces using word
       analogies, arxiv:1606.07736
     * edit (feb 2018): alex gittens, dimitris achlioptas, michael w.
       mahoney, [64]skip-gram     zipf + uniform = vector additivity

some backstory

   i got interested in id97 and related techniques for my general
   interest in machine learning and for my general appreciations of:
     * id105,
     * pointwise mutual information,
     * conceptual metaphors,
     * simple techniques mimicking human cognition.

   i had an motivation to learn more on the subject as i was tutoring
   julia bazi  ska during a two-week summer internship at [65]delab,
   university of warsaw, supported by the polish children   s fund. see also
   my blog posts:
     * [66]helping exceptionally gifted children in poland - on the polish
       children   s fund
     * [67]d3.js workshop at icm for kfnrd in jan 2016, where it all
       started

thanks

   this draft benefited from feedback from [68]grzegorz uriasz (what   s
   simple and what isn   t), [69]sarah martin (readability and grammar
   remarks). i want to especially thank [70]levy omer for pointing to weak
   points (and shady assumptions) of word vector arithmetics.

edit

   it got some popularity, including ~20k views in the first day, and
   being tweeted by [71]the authors of glove at stanford and [72]kaggle.

   though, what i like the most is to see how people interact with it:
     * [73]artistic-scientific impulsive-analytical by cesar hidalgo from
       mit media lab
     * [74]good-evil unlawful-lawful and ad&d classes from hn comment
       thread :)

   [75]tweet
   [76][hn.gif] hn submission/discussion [77][mailchimp.png] get notified
   about new posts via email (mailchimp)

   piotr migda   - as of 2013
     * [78]pmigdal@gmail.com
     * [79]https://p.migdal.pl

     * [80]stared
     * [81]pmigdal
     *

   piotr migda   - an independent data science consultant, with phd in
   quantum physics; based in warsaw, poland. believing in [82]side
   projects, active in [83]gifted education, developing the [84]quantum
   game. collaborating with [85]deepsense.ai. co-founded [86]in browser
   ai.

   python (+    of r) for data analysis and machine learning, javascript
   for data visualization. currently focusing on deep learning.

references

   visible links
   1. https://p.migdal.pl/feed.xml
   2. https://p.migdal.pl/
   3. https://p.migdal.pl/projects/
   4. https://p.migdal.pl/articles/
   5. https://p.migdal.pl/publications/
   6. https://p.migdal.pl/resume/
   7. https://p.migdal.pl/about/
   8. http://migdal.zenfolio.com/
   9. https://news.ycombinator.com/item?id=13346104
  10. https://lamyiowce.github.io/word2viz/
  11. http://nlp.stanford.edu/projects/glove/
  12. https://en.wikipedia.org/wiki/distributional_semantics
  13. https://en.wikipedia.org/wiki/john_rupert_firth
  14. http://www.degeneratestate.org/posts/2016/apr/20/heavy-metal-and-natural-language-processing-part-1/
  15. http://juliasilge.com/blog/gender-pronouns/
  16. http://p.migdal.pl/tagoverflow/
  17. http://stackoverflow.com/tags
  18. https://news.ycombinator.com/item?id=10954508
  19. https://en.wikipedia.org/wiki/pointwise_mutual_information
  20. https://en.wikipedia.org/wiki/independence_(id203_theory)
  21. https://en.wiktionary.org/wiki/wiktionary:frequency_lists/
  22. http://katbailey.github.io/post/matrix-factorization-with-tensorflow/
  23. http://byterot.blogspot.com/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-nlp-gensim.html
  24. https://en.wikipedia.org/wiki/horseshoe_theory
  25. https://demos.explosion.ai/sense2vec/?word=machine learning&sense=auto
  26. https://books.google.com/ngrams/graph?content=nice+cat,nice+kitten,nice+dog,nice+puppy&year_start=1960&year_end=2008&corpus=0&smoothing=5&share=&direct_url=t1;,nice cat;,c0;.t1;,nice kitten;,c0;.t1;,nice dog;,c0;.t1;,nice puppy;,c0
  27. https://books.google.com/ngrams/graph?content=cat,kitten,dog,puppy&year_start=1960&year_end=2008&corpus=0&smoothing=5&share=&direct_url=t1;,cat;,c0;.t1;,kitten;,c0;.t1;,dog;,c0;.t1;,puppy;,c0
  28. https://books.google.com/ngrams
  29. https://www.amazon.com/metaphors-we-live-george-lakoff/dp/0226468011
  30. http://web.archive.org/web/20080718021721/http://cogsci.berkeley.edu/lakoff/metaphors/
  31. http://www.chrisharrison.net/index.php/visualizations/wordspectrum
  32. http://www.chrisharrison.net/index.php/visualizations/wordassociations
  33. http://www.chrisharrison.net/
  34. https://lamyiowce.github.io/word2viz/
  35. https://research.googleblog.com/2016/12/open-sourcing-embedding-projector-tool.html
  36. http://projector.tensorflow.org/
  37. http://nlp.stanford.edu/projects/glove/
  38. https://code.google.com/archive/p/id97/
  39. https://github.com/lamyiowce/word2viz
  40. https://radimrehurek.com/gensim/
  41. https://explosion.ai/blog/sense2vec-with-spacy
  42. https://www.tensorflow.org/versions/r0.11/tutorials/id97/index.html
  43. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_id97.ipynb
  44. http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/
  45. https://web.stanford.edu/~jurafsky/slp3/
  46. https://web.stanford.edu/~jurafsky/slp3/15.pdf
  47. https://web.stanford.edu/~jurafsky/slp3/slides/vector1.pdf
  48. https://web.stanford.edu/~jurafsky/slp3/16.pdf
  49. https://web.stanford.edu/~jurafsky/slp3/slides/vector2.pdf
  50. http://web.stanford.edu/class/linguist236/materials/ling236-handout-05-09-vsm.pdf
  51. http://web.stanford.edu/class/linguist236/
  52. http://nlp.stanford.edu/projects/glove/
  53. http://nlp.stanford.edu/pubs/glove.pdf
  54. https://rare-technologies.com/making-sense-of-id97
  55. http://norvig.com/chomsky.html
  56. https://en.wikipedia.org/wiki/warsaw_university_library
  57. https://www.quora.com/how-does-id97-work
  58. https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf
  59. http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/
  60. http://nlpers.blogspot.ie/2016/06/language-bias-and-black-sheep.html
  61. https://freedom-to-tinker.com/2016/08/24/language-necessarily-contains-human-biases-and-so-will-machines-trained-on-language-corpora/
  62. http://www.offconvex.org/2016/02/14/word-embeddings-2/
  63. https://arxiv.org/abs/1606.07736
  64. http://www.aclweb.org/anthology/p/p17/p17-1007.pdf
  65. http://www.delab.uw.edu.pl/
  66. http://crastina.se/gifted-children-in-poland-by-piotr-migdal/
  67. http://p.migdal.pl/2016/02/09/d3js-icm-kfnrd.html
  68. https://github.com/grzegorz225/
  69. http://goodsexlifestyle.com/
  70. https://levyomer.wordpress.com/
  71. https://twitter.com/stanfordnlp/status/818881718077661184
  72. https://twitter.com/kaggle/status/819258895235424258
  73. https://twitter.com/cesifoti/status/818672972743450624
  74. http://imgur.com/3fzx81i
  75. https://twitter.com/share
  76. javascript:window.location="http://news.ycombinator.com/submitlink?u="+encodeuricomponent(document.location)+"&t="+encodeuricomponent("king - man + woman is queen; but why?")
  77. http://quantumgame.us9.list-manage.com/subscribe?u=5e3bdb13b61924c4b0ec92fba&id=6c72ded7d2
  78. mailto:pmigdal@gmail.com
  79. https://p.migdal.pl/
  80. https://github.com/stared
  81. https://twitter.com/pmigdal
  82. http://crastina.se/theres-no-projects-like-side-projects/
  83. https://warsztatywww.pl/article/en-indie-camp-for-hs-geeks/
  84. http://quantumgame.io/
  85. https://deepsense.ai/
  86. http://inbrowser.ai/

   hidden links:
  88. https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html
  89. https://lamyiowce.github.io/word2viz/
  90. http://p.migdal.pl/tagoverflow/?site=english&size=32
  91. https://pinboard.in/u:pmigdal/
  92. https://linkedin.com/in/piotrmigdal
  93. https://stackexchange.com/users/506817/piotr-migdal
