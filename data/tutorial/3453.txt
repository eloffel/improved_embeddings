   #[1]stitch fix technology     multithreaded

   iframe: [2]https://www.googletagmanager.com/ns.html?id=gtm-7rwhc

     * [3]stitch fix logomark
     * [4]engineering
     * [5]algorithms
     * [6]algorithms tour
     * [7]tour
     * [8]careers
     * [9]blog

   [10]stitch fix logo
   [11]engineering [12]algorithms [13]algorithms tour [14]tour [15]careers
   [16]blog

a word is worth a thousand vectors

   blog post by chris moody

chris moody

march 11, 2015 - san francisco, ca

   [17]tweet this post! [18]post on linkedin

   standard natural language processing (nlp) is a messy and difficult
   affair. it requires teaching a computer about english-specific word
   ambiguities as well as the hierarchical, sparse nature of words in
   sentences. at stitch fix, word vectors help computers learn from the
   raw text in customer notes. our systems, composed of machines and human
   experts, need to recommend the maternity line when she says she   s in
   her    third trimester   , identify a medical professional when she writes
   that she    used to wear scrubs to work   , and distill    taking a trip   
   into a fix for vacation clothing.

   while we   re not totally    there    yet with the holy grail to nlp, word
   vectors (also referred to as distributed representations) are an
   amazing tool that sweeps away some of the issues of dealing with human
   language. the machines work in tandem with the stylists as a support
   mechanism to help identify and summarize textual information from the
   customers. the human experts will make the final call on what actions
   will be taken. the goal of this post is to be a motivating introduction
   to word vectors and demonstrate their real-world utility.

   the following example set the natural language community afire^[19]1
   back in 2013:

   in this example, a human posed a question to a computer: what is king -
   man + woman? this is similar to an sat-style analogy (man is to woman
   as king is to what?). and a computer solved this equation and answered:
   queen. under the hood, the machine gets that the biggest difference
   between the words for man and woman is gender. add that gender
   difference to king, and you get queen.

   this is astonishing because we   ve never explicitly taught the machine
   anything about gender!

   in fact, we   ve never handed the computer anything like a dictionary, a
   thesaurus, or a network of word relationships. we haven   t even tried to
   break apart a sentence into its constituent parts of speech^[20]2.
   we   ve simply fed a mountain of text into an algorithm called
   [21]id97 and expected it to learn from context. word by word, it
   tries to predict the other surrounding words in a sentence. or rather,
   it internally represents words as vectors, and given a word vector, it
   tries to predict the other word vectors in the nearby text^[22]3.

   the algorithm eventually sees so many examples that it can infer the
   gender of a single word, that both the the times and the sun are
   newspapers, that the matrix is a sci-fi movie, and that the style of an
   article of clothing might be boho or edgy. that word vectors represent
   much of the information available in a dictionary definition is a
   convenient and almost miraculous side effect of trying to predict the
   context of a word.

   internally high dimensional vectors stand in for the words, and some of
   those dimensions are encoding gender properties. each axis of a vector
   encodes a property, and the magnitude along that axis represents the
   relevance of that property to the word^[23]4. if the gender axis is
   more positive, then it   s more feminine; more negative, more masculine.

   vectors

   applied appropriately,word vectors are dramatically more meaningful and
   more flexible than current techniques^[24]5 and let computers peer into
   text in a fundamentally new way. it   s surprisingly easy to get started
   using libraries like [25]gensim (in python) or [26]spark (in scala &
   python)     all you need to know is how to add, subtract, and multiply
   vectors!

   let   s review the new abilities that word vectors grant us.

similar words are nearby vectors

   similar words are nearby vectors in a vector space. this is a powerful
   convention since it lets us wipe away a lot of the noise and nuance in
   vocabulary. for example, let   s use [27]gensim to find a list of words
   similar to vacation using the freebase skipgram data^[28]6:
from gensim.models import id97
fn = "freebase-vectors-skipgram1000-en.bin.gz"
model = id97.load_id97_format(fn)
model.most_similar('vacation')

# [('trip', 0.7234684228897095),
#  ('honeymoon', 0.6447688341140747),
#  ('beach', 0.6249285936355591),
#  ('vacations', 0.5868890285491943),
#  ('wedding', 0.5541957020759583),
#  ('resort', 0.5231006145477295),
#  ('traveling', 0.5194448232650757),
#  ('vacation.', 0.5068142414093018),
#  ('vacationing', 0.5013546943664551)]

   we   ve calculated the vectors most similar to the vector for vacation,
   and then looked up what words those vectors represent. as we read the
   list, we note that these words aren   t just similar in vector space, but
   that they make sense intuitively too.

   in this case, we   ve looked for vectors that are nearby to the word
   vacation by measuring the similarity (usually cosine similarity) to the
   root word and sorting by that.

   destinations

   vacation

   season

   holidays

   wedding

   month

   above is an interactive visualization of the words nearest to vacation.
   the more similar a word to it   s genre, the larger the radius of the
   marker. hover over the bubbles to reveal the words they
   represent^[29]7.

   and these words aren   t just nearby; they   re also in several clusters.
   so we can determine that the words most similar to vacation come in a
   variety of flavors: one cluster might be wedding-related, but another
   might relate to destinations like belize.

   of course our human stylists understand when a client says    i   m going
   to belize in march    that she has an upcoming vacation. but the computer
   can potentially tag this as a    vacation    fix because the word vector
   for belize is similar to that for vacation. we can then make sure that
   the fixes our customers get are vacation-appropriate!

ideas are words that can be added & subtracted

   we have the ability to search semantically by adding and subtracting
   word vectors^[30]8. this empowers us to creatively add and subtract
   concepts and ideas. let   s start with a style we know a customer liked,
   item_3469:

   vectors

   our customer recently became pregnant, so let   s try and find something
   like item_3469 but along the pregnant dimension:
model.most_similar('item_3469', 'pregnant')
matches = list(filter(lambda x: 'item_' in x[0], matches))

# ['item_13792',
# 'item_11275',
# 'item_11868']

   of course the item ids aren   t immediately informative, but the pictures
   let us know that we   ve done well:

   the first two are items have prominent black & white stripes like
   item_3469 but have the added property that they   re great
   maternity-wear. the last item changes the pattern away from stripes but
   is still a loose blouse that   s great for an expectant mother. here
   we   ve simply added the word vector for pregnant to the word vector for
   item_3469, and looked up the word vectors most similar to that
   result^[31]9.

   our stylists tailor each fix to their clients, and this prototype
   system may free them to mix and match artistic concepts about style,
   size and fit to creatively search for new items.

summarizing sentences & documents

   at stitch fix, we work hard to craft a uniquely-styled fix for each of
   our customers. at every stage of a fix we collect feedback: what would
   you like in your next fix? what did you think of the items we sent you?
   what worked? what didn   t?

   the spectrum of responses is myriad, but vectorizing those
   sentences^[32]10 allows us to begin systematically categorizing those
   documents:
from gensim.models import doc2vec
fn = "word_vectors_blog_post_v01_notes"
model = doc2vec.load(fn)
model.most_similar('pregnant')
matches = list(filter(lambda x: 'sent_' in x[0], matches))

# ['...i am currently 23 weeks pregnant...',
#  '...i'm now 10 weeks pregnant...',
#  '...not showing too much yet...',
#  '...15 weeks now. baby bump...',
#  '...6 weeks post partum!...',
#  '...12 weeks postpartum and am nursing...',
#  '...i have my baby shower that...',
#  '...am still breastfeeding...',
#  '...i would love an outfit for a baby shower...']

   in this example we calculate which sentences are closest to the word
   pregnant. this list also skips over many literal matches of pregnant in
   order to demonstrate the more advanced capabilities. we   ve also
   censored sentences to keep out personally identifying text. also note
   that the last sentence is a false positive: while similar to the word
   pregnant, she   s unlikely to be interested in maternity clothing.

   this allows us to understand not just what words mean, but condense our
   client comments, notes, and requests in a quantifiable way. we can for
   example categorize our sentences by first calculating the similarity
   between a sentence and a word:
def get_vector(word):
    return model.syn0norm[model.vocab[word].index]
def calculate_similarity(sentence, word):
   vec_a = get_vector(sentence)
   vec_b = get_vector(word)
   sim = np.dot(vec_a, vec_b)
   return sim
calculate_similarity('sent_47973, 'casual')

# 0.308

   we calculated the overlap between a sentence with label sent_47973 and
   the word casual. the sentence is previously trained from this customer
   text:    i need some weekend wear. comfy but stylish.    the similarity to
   casual is about 0.308, which is pretty high.

   having built a function that computes the similarity between a sentence
   and a word, we can build a table of customer comments and their
   similarities to a given topic:
   raw text snippets    broken       casual       pregnant   
          unfortunately the lining ripped after wearing if twice        0.281
   0.082 0.062
          i need some weekend wear. comfy but stylish.    0.096 0.308 0.191
          12 weeks postpartum and am nursing        0.158 0.110 0.378

   a table like this around helps us quickly answer how many people are
   looking for comfortable clothes or finding defects in the clothing we
   send them.

what we didn   t mention

   while word vectorization is an elegant way to solve many practical text
   processing problems, it does have a few shortcomings and
   considerations:
    1. word vectorization requires a lot of text. you can [33]download
       pretrained word vectors yourself, but if you have a highly
       specialized vocabulary then you   ll need to train your own word
       vectors and have a lot of example text. typically this means
       hundreds of millions of words, which is the equivalent of 1,000
       books, 500,000 comments, or 4,000,000 tweets.
    2. cleaning the text. you   ll need to clean the words of punctuation
       and normalize unicode^[34]11 characters, which can take significant
       manual effort. in this case, there are a few tools that can help
       like [35]ftfy, [36]spacy, [37]nltk, and the [38]stanford core nlp.
       spacy even comes with word vector support built-in.
    3. memory & performance. the training of vectors requires a
       high-memory and high-performance multicore machine. training can
       take several hours to several days but shouldn   t need frequent
       retraining. if you use pretrained vectors, then this isn   t an
       issue.
    4. databases. modern sql systems aren   t well-suited to performing the
       vector addition, subtraction and multiplication searching in vector
       space requires. there are a few libraries that will help you
       quickly find the most similar items^[39]12: [40]annoy, [41]ball
       trees, [42]locality-sensitive hashing (lsh) or [43]flann.
    5. false-positives & exactness. despite the impressive results that
       come with word vectorization, no nlp technique is perfect. take
       care that your system is robust to results that a computer deems
       relevant but an expert human wouldn   t.

conclusion

   the goal of this post was to convince you that word vectors give us a
   simple and flexible platform for understanding text. we   ve covered a
   few diverse examples that should help build your confidence in
   developing and deploying nlp systems and what problems they can solve.
   while most coverage of word vectors has been from a scientific angle,
   or demonstrating toy examples, we at stitch fix think this technology
   is ripe for industrial application.

   in fact, stitch fix is the perfect testbed for these kinds of new
   technologies: with expert stylists in the loop, we can move rapidly on
   new and prototypical algorithms without worrying too much about edge
   and corner cases. the creative world of fashion is one of the few
   domains left that computers don   t understand. if you   re interested in
   helping us break down that wall, [44]apply!

further reading

   there are a few miscellaneous topics that we didn   t have room to cover
   or were too peripheral:
    1. there   s an excellent nuts and bolts [45]explanation and derivation
       of the id97 algorithm. there   s a similarly useful [46]ipython
       notebook version too.
    2. translating word-by-word english into spanish is [47]equivalent to
       matrix rotations. this means that all of the basic id202
       operators (addition, subtraction, dot products, and matrix
       rotations) have meaningful functions on human language.
    3. word vectors can also be used to find the [48]odd word out.
    4. interestingly, the same skip-gram algorithm can be [49]applied to a
       social graph instead of sentence structure. the authors equate a
       sequence of social network graph visits (a random walk) to a
       sequence of words (a sentence in id97) to generate a dense
       summary vector.
    5. a brief but very visual overview of distributed representations is
       available [50]here.
    6. intriguingly, the id97 algorithm can be reinterpreted as a
       [51]id105 method using point-wise mutual
       information. this theoretical breakthrough cleanly connects older
       and faster but more memory-intensive techniques with id97   s
       streaming algorithm approach.
     __________________________________________________________________

   ^1 see also the original papers, and the subsequently bombastic
   [52]media frenzy, the race to understand why id97 works [53]so
   well, some academic drama on [54]glove vs id97, and a [55]nice
   introduction to the algorithms behind id97 from my friend [56]radim
     eh    ek. [57]   

   ^2 although see [58]omer levy and yoav goldberg   s post for an
   interesting approach that has the id97 context defined by parsing
   the sentence structure. doing this introduces a more functional
   similarity between words (see this [59]demo). for example, hogwarts in
   id97 is similar to dementors and dumbledore, as they   re all from
   harry potter, while parsing context gives sunnydale and colinwood as
   they   re similarly prestigious schools. [60]   

   ^3 this is describing the    skip-gram    mode of id97 where the target
   word is asked to predict the surrounding context. interestingly, we can
   also get similar results by doing the reverse: using the surrounding
   text to predict a word in the middle! this model, called continuous
   bag-of-words (cbow), loses word order and so we lose a bit of
   grammatical information since that   s very sensitive to the position of
   a word in a sentence. this means cbow-trained word vectors tend to do
   worse in a syntactic sense: the resulting vectors more poorly encode
   whether a word is an adjective or a verb, or a noun. [61]   

   ^4 more generally, a linear combination of axes encodes the properties.
   we can attempt to rotate into the correct basis by using pca (as long
   as we only include a few nearby words) or visualize that space using
   id167 (although we lose the concept of a single axis encoding
   structure). [62]   

   ^5 compare word vectors to id31, which effectively
   distills everything into one dimension of    happy or sad   , or document
   labeling efforts like id44s that sort words into
   a few types. in either case, we can only ask these simpler models to
   categorize new documents into a few predetermined groups. with word
   vectors we can encapsulate far more diversity without having to build a
   labeled training text (and thus with less effort.) [63]   

   ^6 you can download this file freely from [64]here. [65]   

   ^7 this is using an advanced visualization technique called [66]id167.
   this allows us to project down to 2d while still trying to maintain the
   local structure. this helps pop up the several word clusters that are
   near to the word vacation. [67]   

   ^8 check out this live demo with just wikipedia words [68]here. [69]   

   ^9 we   ve used cosine similarity to find the nearest items, but, we
   could   ve chosen the 3cosmul method. this combines vectors
   multiplicatively instead of additively and seems to [70]get better
   results (pdf warning!). this stays truer to cosine distance and in
   general prevents one word from dominating in any one dimension. [71]   

   ^10 you can easily make a vector for a whole sentence by following the
   [72]doc2vec tutorial (also called [73]paragraph vector) in [74]gensim,
   or by id91 words using the [75]chinese restaurant process. [76]   

   ^11 if you   re using python 2, this is a great reason to reduce unicode
   headaches and switch to python 3. [77]   

   ^12 see a comparison of these techniques [78]here. my recommendation is
   using lsh if you need a pure python solution, and annoy if you need a
   solution that is memory light. [79]   

   [80]tweet this post! [81]post on linkedin

   multithreaded

come work with us!

   we   re a diverse team dedicated to building great products, and we   d
   love your help. do you want to build amazing products with amazing
   peers? join us!

   [82]all technology careers [83]all careers at stitch fix

   stitch fix: your partner in personal style stitch fix and fix are
   trademarks of stitch fix, inc.

     * [84]stitch fix home
     * [85]faq
     * [86]press

     * [87]tech blog
     * [88]tech careers

     * [89]terms of use
     * [90]privacy policy

follow us!

   [91]stitch fix tech on github

follow us!

   [92]stitch fix tech on github
     * [93]tech blog
     * [94]tech careers
     * [95]stitch fix home
     * [96]faq
     * [97]press
     * [98]terms of use
     * [99]privacy policy

   stitch fix and fix are trademarks of stitch fix, inc.
   stitch fix logomark

references

   1. https://multithreaded.stitchfix.com/feed.xml
   2. https://www.googletagmanager.com/ns.html?id=gtm-7rwhc
   3. https://multithreaded.stitchfix.com/
   4. https://multithreaded.stitchfix.com/engineering
   5. https://multithreaded.stitchfix.com/algorithms
   6. http://algorithms-tour.stitchfix.com/
   7. http://algorithms-tour.stitchfix.com/
   8. https://multithreaded.stitchfix.com/careers
   9. https://multithreaded.stitchfix.com/blog
  10. https://multithreaded.stitchfix.com/
  11. https://multithreaded.stitchfix.com/engineering
  12. https://multithreaded.stitchfix.com/algorithms
  13. http://algorithms-tour.stitchfix.com/
  14. http://algorithms-tour.stitchfix.com/
  15. https://multithreaded.stitchfix.com/careers
  16. https://multithreaded.stitchfix.com/blog
  17. https://twitter.com/intent/tweet?text=a word is worth a thousand vectors&url=https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/&via=stitchfix_algo
  18. https://www.linkedin.com/sharearticle?mini=true&url=https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/&title=a word is worth a thousand vectors&summary=standard natural language processing (nlp) is a messy and difficult affair. it requires teaching a computer about english-specific word ambiguities as well a...&source=stitch fix technology     multithreaded
  19. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote1
  20. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote2
  21. https://code.google.com/p/id97/
  22. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote3
  23. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote4
  24. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote5
  25. http://radimrehurek.com/2014/02/id97-tutorial/
  26. http://spark.apache.org/docs/1.2.0/mllib-feature-extraction.html#id97
  27. https://radimrehurek.com/gensim/
  28. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote6
  29. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote7
  30. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote8
  31. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote9
  32. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote10
  33. https://code.google.com/p/id97/#pre-trained_word_and_phrase_vectors
  34. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote11
  35. https://github.com/luminosoinsight/python-ftfy
  36. http://honnibal.github.io/spacy/
  37. http://www.nltk.org/
  38. http://nlp.stanford.edu/software/corenlp.shtml
  39. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote12
  40. https://github.com/spotify/annoy
  41. http://scikit-learn.org/dev/modules/neighbors.html#ball-tree
  42. http://scikit-learn.org/dev/modules/neighbors.html#mathematical-description-of-locality-sensitive-hashing
  43. http://www.cs.ubc.ca/research/flann/
  44. http://technology.stitchfix.com/jobs/index.html
  45. http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf
  46. http://nbviewer.ipython.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb
  47. http://arxiv.org/pdf/1309.4168.pdf
  48. https://github.com/dhammack/id97example
  49. https://sites.google.com/site/bryanperozzi/projects/deepwalk
  50. http://colah.github.io/posts/2014-07-nlp-id56s-representations/
  51. https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf
  52. http://www.wired.com/2014/12/googlers-quest-teach-machines-understand-emotions/
  53. https://levyomer.wordpress.com/2014/09/10/neural-word-embeddings-as-implicit-matrix-factorization/
  54. https://docs.google.com/a/stitchfix.com/document/d/1ydiujj7etsz688rgfu5imjjsbxai-krl8czswpti15s/edit#heading=h.66rkmh7nd17u
  55. https://www.youtube.com/watch?v=wtp3p2untfq
  56. http://radimrehurek.com/
  57. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote1-return
  58. https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/
  59. http://irsrv2.cs.biu.ac.il:9998/?word=hogwarts
  60. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote2-return
  61. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote3-return
  62. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote4-return
  63. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote5-return
  64. https://code.google.com/p/id97/#pre-trained_word_and_phrase_vectors
  65. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote6-return
  66. http://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-id167
  67. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote7-return
  68. http://radimrehurek.com/2014/01/performance-shootout-of-nearest-neighbours-querying/#wikisim
  69. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote8-return
  70. https://levyomer.files.wordpress.com/2014/04/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf
  71. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote9-return
  72. http://radimrehurek.com/2014/12/doc2vec-tutorial/
  73. http://cs.stanford.edu/~quocle/paragraph_vector.pdf
  74. https://radimrehurek.com/gensim/
  75. http://eng.kifi.com/from-id97-to-doc2vec-an-approach-driven-by-chinese-restaurant-process/
  76. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote10-return
  77. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote11-return
  78. http://radimrehurek.com/2014/01/performance-shootout-of-nearest-neighbours-querying/
  79. https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/#footnote12-return
  80. https://twitter.com/intent/tweet?text=a word is worth a thousand vectors&url=https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/&via=stitchfix_algo
  81. https://www.linkedin.com/sharearticle?mini=true&url=https://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/&title=a word is worth a thousand vectors&summary=standard natural language processing (nlp) is a messy and difficult affair. it requires teaching a computer about english-specific word ambiguities as well a...&source=stitch fix technology     multithreaded
  82. https://multithreaded.stitchfix.com/careers
  83. http://stitchfix.com/careers
  84. https://www.stitchfix.com/
  85. https://www.stitchfix.com/faq
  86. https://www.stitchfix.com/press
  87. http://multithreaded.stitchfix.com/blog/
  88. http://multithreaded.stitchfix.com/careers/
  89. https://www.stitchfix.com/terms
  90. https://www.stitchfix.com/privacy
  91. http://github.com/stitchfix
  92. http://github.com/stitchfix
  93. http://multithreaded.stitchfix.com/blog/
  94. http://multithreaded.stitchfix.com/careers/
  95. https://www.stitchfix.com/
  96. https://www.stitchfix.com/faq
  97. https://www.stitchfix.com/press
  98. https://www.stitchfix.com/terms
  99. https://www.stitchfix.com/privacy
