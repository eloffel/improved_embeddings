   #[1]rss feed for off the convex path

   [2][logo.jpg]

   [3]about [4]contact [5]subscribe

id27s: explaining their properties

   sanjeev arora       feb 14, 2016       16 minute read

   this is a followup to an [6]earlier post about id27s, which
   capture the meaning of a word using a low-dimensional vector, and are
   ubiquitous in natural language processing. i will talk about [7]my
   joint work with li, liang, ma, risteski, which tries to mathematically
   explain their fascinating properties.

   we focus on a few questions. (a) what properties of natural languages
   cause these low-dimensional embeddings to exist? (b) why do
   low-dimensional embeddings work better at analogy solving than high
   dimensional embeddings?

   in a future blog post i will address another question answered by
   [8]our subsequent work: how should a id27 be interpreted when
   a word has multiple meanings?

why do low-dimensional embeddings capture huge statistical information?

   recall that all embedding methods try to leverage word co-occurence
   statistics. [9]id45 does a low-rank approximation
   to word-word cooccurence probabilities. in the simplest version, if the
   dictionary has $n$ words (usually $n$ is about $10^5$) then find $v_1,
   v_2, \ldots, v_n \in \mathbb{r}^{300}$ that minimize the following
   expression, where $p(w, w   )$ is the empirical id203 that words
   $w, w   $ occur within $5$ words of each other in a text corpus like
   wikipedia. (here    $5$    and    $300$    are somewhat arbitrary.)

   of course, one can compute a rank$-300$ svd for any matrix; the
   surprise here is that the rank $300$ matrix is actually a reasonable
   approximation to the $100,000$-dimensional matrix of cooccurences.
   (since then, [10]topic models have been developed which imply that the
   matrix is indeed low rank.) this success motivated many extensions of
   the above basic idea; see the survey on [11]vector space models. we   re
   interested today in methods that perform nonlinear operations on word
   cooccurence probabilities. the simplest uses the old and popular
   [12]pmi measure of church and hanks, where the id203 $p(w,w   )$ in
   expression (1) is replaced by the following (nonlinear) measure of
   correlation. (and still the $10^5 \times 10^5$ matrix turns out to have
   a good low-rank approximation.)

   of course, researchers in applied machine learning take the existence
   of such low-dimensional approximations for granted, but there appears
   to be no theory to explain their existence. theoretical explanations
   are also lacking for other recent methods such as google   s
   [13]id97.

   our paper gives an explanation using a new generative model for text,
   which also gives a clearer insight into the causative relationship
   between word meanings and the cooccurence probabilities. we think of
   corpus generation as a dynamic process, where the $t$-th word is
   produced at step $t$. the model says that the process is driven by the
   random walk of a discourse vector $c_t \in \re^d$. it is a unit vector
   whose direction in space represents what is being talked about. each
   word has a (time-invariant) latent vector $v_w \in \re^d$ that captures
   its correlations with the discourse vector. we model this bias with a
   loglinear word production model:

   the discourse vector does a slow geometric random walk over the unit
   sphere in $\re^d$. thus $c_{t+1}$ is obtained by a small random
   displacement from $c_t$. since expression (2) places much higher
   id203 on words that are clustered around $c_t$, and $c_t$ moves
   slowly, the model predicts that words occuring at successive time steps
   will also tend to have vectors that are close together. but this
   correlation weakens after say, $100$ steps. this model is basically the
   [14]loglinear topic model of mnih and hinton, but with an added dynamic
   element in the form of a random walk. the model is also related to many
   existing notions like kalman filters and linear chain crfs. also, as is
   usual in topic models, it ignores grammatical structure, and treats
   text in small windows as a [15]bag of words.

   our main contribution is to use the model assumptions to derive closed
   form expressions for the word-word cooccurence probabilities in terms
   of the latent variables (i.e., the word vectors). this involves
   integrating out the random walk $c_t$. for this we need to make a
   theoretical assumption, which says intuitively that the bulk behavior
   of the set of all word vectors is similar to what it would be if they
   were randomly strewn around the conceptual space (this is
   counterintuitive to my linguistics colleagues because they are used to
   the existence of fine-grained structure in word meanings).

     isotropy assumption about word vectors: in the bulk, the word
     vectors behave like random vectors, for example, like $s \cdot u$
     where $u$ is a standard gaussian vector and $s$ is a scalar random
     variable. in particular, the partition function $z_c = \sum_w
     \exp(v_w \cdot c)$ is approximately $z \pm \epsilon$ for most unit
     vectors $c$.

   we find that in practice the partition function is well-concentrated.
   after writing our paper we discovered that this phenomenon had been
   been discovered already in empirical work on [16]self-normalizing
   language models. (basic message: treat the partition function as
   constant; it doesn   t hurt too much!)

   the tight concentration of partition function allows us to compute a
   multidimensional integral to obtain expressions for word probabilities.

   thus the model predicts that the pmi matrix introduced earlier is
   indeed low-dimensional. furthermore, unlike previous models, low
   dimension plays a key role in the story: isotropy requires the
   dimension $d$ to be much smaller than $n$.

   a theoretical    explanation    also follows for some other nonlinear
   models. for instance if we try to do a max-likelihood fit (id113) to the
   above expressions, something interesting happens in the calculation:
   different word pairs need to be weighted differently. suppose you see
   that $w, w   $ cooccur $x(w, w   )$ times in the corpus. then it turns out
   that your trust in this count as an estimate of the true value of $p(w,
   w   )$ scales linearly with $x(w, w   )$ itself. in other words the id113 fit
   to the model is

   this is very similar to the expression in the [17]glove model, but
   provides some explanation for their mysterious bias and reweighting
   terms. empirically we find that this model fits the data quite well:
   the weighted termwise error (without the square) is about $5$ percent.
   (the weighted termwise error for the pmi model is much worse, around
   $17\%$.)

   a theoretical explanation can also be given for google   s id97
   model. suppose we assume the random walk of the discourse vector is
   slow enough that $c_t$ is essentially unchanged while producing
   consecutive strings of $10$ words or more. then the average of the word
   vectors for any consecutive $5$ words is a max a posteriori (map)
   estimate of the discourse vector $c_t$ that produced them. this leads
   to the id97(cbow) model, which had hitherto seemed mysterious:

why do low dimensional embeddings work better than high-dimensional ones?

   a striking finding in empirical work on id27s is that there
   is a sweet spot for the dimensionality of word vectors: neither too
   small, nor too large. this graph below from the [18]latent semantic
   analysis paper (1997) shows the performance on word similarity tasks
   versus dimension, but a similar phenomenon also occurs for analogy
   solving.

   performance of id27s vs dimension

   such a performance curve with a bump at the    sweet spot    is very
   familiar in empirical work in machine learning and usually explained as
   follows: too few parameters make the model incapable of fitting to the
   signal; too many parameters, and it starts [19]overfitting (e.g.,
   fitting to noise instead of the signal). thus the dimension constraint
   act as a regularizer for the optimization.

   surprisingly, i have not heard of a good theoretical explanation to
   back up this intuition. here are some attempted explanations i heard
   from colleagues in connection with id27s.

   suggestion 1: [20]johnson-lindenstrauss lemma implies some dimension
   reduction for every set of vectors. this explanation doesn   t cut it
   because: (a) it only implies dimension $\frac{1}{\epsilon^2}\log n$,
   which is too high for even moderate $\epsilon$. (b) it predicts that
   quality of the embedding goes up monotonically as we increase
   dimension, whereas in practice overfitting is observed.

   suggestion 2: standard generalization theory (e.g., [21]vc-dimension)
   predicts overfitting. i don   t see why this applies either, since we are
   dealing with unsupervised learning (or id21): the training
   objective doesn   t have anything to do a priori with analogy solving. so
   there is no reason a model with fewer parameters will do better on
   analogy solving, just as there   s no reason it does better for some
   other unrelated task like predicting the weather.

   we give some idea why a low-dimensional model may solve analogies
   better. this is also related to the following phenomenon.

why do semantic relations correspond to directions?

   remember the striking discovery in the id97 paper: word analogy
   tasks can be solved by simple id202. for example, the word
   analogy question man : woman ::king : ?? can be solved by looking for
   the word $w$ such that $v_{king} - v_w$ is most similar to $v_{man} -
   v_{woman}$; in other words, minimizes

   this strongly suggests that semantic relations    in the above example,
   the relation is masculine-feminine   correspond to directions in space.
   however, this interpretation is challenged by [22]levy and goldberg who
   argue there is no id202 magic here, and the expression can be
   explained simply in terms of traditional connection between word
   similarity and vector inner product (cosine similarity). see also this
   [23]related blog post.

   we find on the other hand that the relations = directions phenomenon is
   demonstrable empirically, and is particularly clear for semantic
   analogies in the testbed. for each relation $r$ we can find a direction
   $\mu_r$ such if a word pair $a, b$ satisfy $r$, then

   where $\alpha_{a, b}$ is a scalar that   s roughly about $0.6$ times the
   norm of $v_a - v_b$ and $\eta$ is a noise vector. empirically, the
   residuals $\eta$ do look mathematically like random vectors according
   to various tests.

   in particular, this phenomenon allows the analogy solver to be made
   completely id202ic if you have a few examples (say 20) of the
   same relation. you compute the top singular vector of the matrix of all
   $v_a -v_b$   s to recover $\mu_r$, and from then on can solve analogies
   $a: b:: c:??$ by looking for a word $d$ such that $v_c - v_d$ has the
   highest possible projection on $\mu_r$ (thus ignoring $v_a -v_b$
   altogether). in fact, this gives a    cheating    method to solve the
   analogy test bed with somewhat higher success rates than state of the
   art. (message to future designers of analogy testbeds: don   t include
   too many examples of the same relationship, otherwise this cheating
   method can exploit it.) by the way, lisa lee did a [24]senior thesis
   under my supervision that showed empirically that this phenomenon can
   be used to extend knowledge-bases of facts, e.g., predict new music
   composers in the corpus given a list of known composers.

   our theoretical results can be used to explain the emergence of
   relations=directions phenomenon in the embeddings. earlier attempts (eg
   in the [25]glove paper) to explain the success of (3) for analogy
   solving had failed to account for the fact that all models are only
   approximate fits to the data. for example, the pmi model fits $v_w
   \cdot v_{w   }$ to $pmi(w, w   )$) but the termwise error for our corpus is
   $17\%$, and expression (3) contains $6$ inner products! so even though
   expression (3) is presumably a id202ic proxy for some
   statistical property of the word distributions, the noise/error is
   large. by contrast, the difference in the value of (3) between the best
   and second-best solution is small, say $10-15\%$.

   so the question is: why does error in the approximate fit not kill the
   analogy solving? our explanation of relations = directions phenomenon
   provides an explanation: the low dimension of the vectors has a
      purifying    effect that reduces the effect of this fitting error. (see
   section 4 in the paper.)the key ingredient of this explanation is,
   again, the random-like behavior of id27s    quantified in terms
   of singular values    as well as the standard theory of linear
   regression. i   ll describe the math in a future post.
   subscribe to our [26]rss feed.
   spread the word:

comments

   please enable javascript to view the [27]comments powered by disqus.

   theme available on [28]github.

references

   visible links
   1. http://www.offconvex.org/feed.xml
   2. http://offconvex.github.io/
   3. http://www.offconvex.org/about/
   4. http://www.offconvex.org/contact/
   5. http://www.offconvex.org/subscribe/
   6. http://www.offconvex.org/2015/12/12/word-embeddings-1/
   7. http://arxiv.org/abs/1502.03520
   8. http://arxiv.org/abs/1601.03764
   9. http://lsa.colorado.edu/papers/jasis.lsi.90.pdf
  10. https://en.wikipedia.org/wiki/topic_model
  11. https://www.jair.org/media/2934/live-2934-4846-jair.pdf
  12. http://www.aclweb.org/anthology/j90-1003
  13. https://code.google.com/archive/p/id97/
  14. http://machinelearning.wustl.edu/mlpapers/paper_files/icml2007_mnihh07.pdf
  15. https://en.wikipedia.org/wiki/bag-of-words_model
  16. http://nlp.cs.berkeley.edu/pubs/andreas-klein_2015_selfnormalizing_paper.pdf
  17. http://nlp.stanford.edu/projects/glove/
  18. http://lsa.colorado.edu/papers/plato/plato.annote.html
  19. https://en.wikipedia.org/wiki/overfitting
  20. https://en.wikipedia.org/wiki/johnson-lindenstrauss_lemma
  21. https://en.wikipedia.org/wiki/vapnik   chervonenkis_theory
  22. http://www.cs.bgu.ac.il/~yoavg/publications/conll2014analogies.pdf
  23. https://quomodocumque.wordpress.com/2016/01/15/messing-around-with-id97/
  24. https://stanford.edu/~lisaslee/ml/lilee_thesis.pdf
  25. http://nlp.stanford.edu/projects/glove/
  26. http://www.offconvex.org/feed.xml
  27. http://disqus.com/?ref_noscript
  28. https://github.com/johnotander/pixyll

   hidden links:
  30. https://facebook.com/sharer.php?u=http://offconvex.github.io/2016/02/14/word-embeddings-2/
  31. https://twitter.com/intent/tweet?text=word%20embeddings:%20explaining%20their%20properties&url=http://offconvex.github.io/2016/02/14/word-embeddings-2/
  32. https://plus.google.com/share?url=http://offconvex.github.io/2016/02/14/word-embeddings-2/
  33. http://www.linkedin.com/sharearticle?url=http://offconvex.github.io/2016/02/14/word-embeddings-2/&title=word%20embeddings:%20explaining%20their%20properties
  34. http://reddit.com/submit?url=http://offconvex.github.io/2016/02/14/word-embeddings-2/&title=word%20embeddings:%20explaining%20their%20properties
  35. https://news.ycombinator.com/submitlink?u=http://offconvex.github.io/2016/02/14/word-embeddings-2/&t=word%20embeddings:%20explaining%20their%20properties
