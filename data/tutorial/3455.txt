   #[1]rss feed for off the convex path

   [2][logo.jpg]

   [3]about [4]contact [5]subscribe

id202ic structure of word meanings

   sanjeev arora       jul 10, 2016       13 minute read

   id27s capture the meaning of a word using a low-dimensional
   vector and are ubiquitous in natural language processing (nlp). (see my
   earlier [6]post 1 and [7]post2.) it has always been unclear how to
   interpret the embedding when the word in question is polysemous, that
   is, has multiple senses. for example, tie can mean an article of
   clothing, a drawn sports match, and a physical action.

   polysemy is an important issue in nlp and much work relies upon
   [8]id138, a hand-constructed repository of word senses and their
   interrelationships. unfortunately, good id138s do not exist for most
   languages, and even the one in english is believed to be rather
   incomplete. thus some effort has been spent on methods to find
   different senses of words.

   in this post i will talk about [9]my joint work with li, liang, ma,
   risteski which shows that actually word senses are easily accessible in
   many current id27s. this goes against conventional wisdom in
   nlp, which is that of course, id27s do not suffice to capture
   polysemy since they use a single vector to represent the word,
   regardless of whether the word has one sense, or a dozen. our work
   shows that major senses of the word lie in linear superposition within
   the embedding, and are extractable using sparse coding.

   this post uses embeddings constructed using our method and the
   wikipedia corpus, but similar techniques also apply (with some loss in
   precision) to other embeddings described in [10]post 1 such as
   id97, glove, or even the decades-old pmi embedding.

a surprising experiment

   take the viewpoint    simplistic yet instructive    that a polysemous word
   like tie is a single lexical token that represents unrelated words
   tie1, tie2,     here is a surprising experiment that suggests that the
   embedding for tie should be approximately a weighted sum of the
   (hypothethical) embeddings of tie1, tie2,    

     take two random words $w_1, w_2$. combine them into an artificial
     polysemous word $w_{new}$ by replacing every occurrence of $w_1$ or
     $w_2$ in the corpus by $w_{new}.$ next, compute an embedding for
     $w_{new}$ using the same embedding method while deleting embeddings
     for $w_1, w_2$ but preserving the embeddings for all other words.
     compare the embedding $v_{w_{new}}$ to linear combinations of
     $v_{w_1}$ and $v_{w_2}$.

   repeating this experiment with a wide range of values for the ratio $r$
   between the frequencies of $w_1$ and $w_2$, we find that $v_{w_{new}}$
   lies close to the subspace spanned by $v_{w_1}$ and $v_{w_2}$: the
   cosine of its angle with the subspace is on average $0.97$ with
   standard deviation $0.02$. thus $v_{w_{new}} \approx \alpha v_{w_1} +
   \beta v_{w_2}$. we find that $\alpha \approx 1$ whereas $\beta \approx
   1- c\lg r$ for some constant $c\approx 0.5$. (note this formula is
   meaningful when the frequency ratio $r$ is not too large, i.e. when $ r
   < 10^{1/c} \approx 100$.) thanks to this logarithm, the infrequent
   sense is not swamped out in the embedding, even if it is 50 times less
   frequent than the dominant sense. this is an important reason behind
   the success of our method for extracting word senses.

   this experiment    to which we were led by our theoretical
   investigations    is very surprising because the embedding is the
   solution to a complicated, nonid76, yet it behaves in
   such a striking linear way. you can read our paper for an intuitive
   explanation using our theoretical model from [11]post2.

extracting word senses from embeddings

   the above experiment suggests that

   but this alone is insufficient to mathematically pin down the senses,
   since $v_{tie}$ can be expressed in infinitely many ways as such a
   combination. to pin down the senses we will interrelate the senses of
   different words    for example, relate the    article of clothing    sense
   tie1 with shoe, jacket etc.

   the word senses tie1, tie2,.. correspond to    different things being
   talked about       in other words, different word distributions occuring
   around tie. now remember that [12]our earlier paper described in
   [13]post2 gives an interpretation of    what   s being talked about   : it is
   called discourse and it is represented by a unit vector in the
   embedding space. in particular, the theoretical model of [14]post2
   imagines a text corpus as being generated by a random walk on discourse
   vectors. when the walk is at a discourse $c_t$ at time $t$, it outputs
   a few words using a loglinear distribution:

   one imagines there exists a    clothing    discourse that has high
   id203 of outputting the tie1 sense, and also of outputting
   related words such as shoe, jacket, etc. similarly there may be a
      games/matches    discourse that has high id203 of outputting tie2
   as well as team, score etc.

   by equation (2) the id203 of being output by a discourse is
   determined by the inner product, so one expects that the vector for
      clothing    discourse has high inner product with all of shoe, jacket,
   tie1 etc., and thus can stand as surrogate for $v_{tie1}$ in expression
   (1)! this motivates the following global optimization:

     given word vectors in $\re^d$, totaling about $60,000$ in this case,
     a sparsity parameter $k$, and an upper bound $m$, find a set of unit
     vectors $a_1, a_2, \ldots, a_m$ such that where at most $k$ of the
     coefficients $\alpha_{w,1},\dots,\alpha_{w,m}$ are nonzero
     (so-called hard sparsity constraint), and $\eta_w$ is a noise
     vector.

   here $a_1, \ldots a_m$ represent important discourses in the corpus,
   which we refer to as atoms of discourse.

   optimization (3) is a surrogate for the desired expansion of $v_{tie}$
   in (1) because one can hope that the atoms of discourse will contain
   atoms corresponding to clothing, sports matches etc. that will have
   high inner product (close to $1$) with tie1, tie2 respectively.
   furthermore, restricting $m$ to be much smaller than the number of
   words ensures that each atom needs to be used for multiple words, e.g.,
   reuse the    clothing    atom for shoes, jacket etc. as well as for tie.

   both $a_j$   s and $\alpha_{w,j}$   s are unknowns in this optimization.
   this is nothing but sparse coding, useful in neuroscience, image
   processing, id161, etc. it is nonconvex and computationally
   np-hard in the worst case, but can be solved quite efficiently in
   practice using something called the k-svd algorithm described in
   [15]elad   s survey, lecture 4. we solved this problem with sparsity
   $k=5$ and using $m$ about $2000$. (experimental details are in the
   paper. also, some theoretical analysis of such an algorithm is
   possible; see this [16]earlier post.)

experimental results

   each discourse atom defines via (2) a distribution on words, which due
   to the exponential appearing in (2) strongly favors words whose
   embeddings have a larger inner product with it. in practice, this
   distribution is quite concentrated on as few as 50-100 words, and the
      meaning    of a discourse atom can be roughly determined by looking at a
   few nearby words. this is how we visualize atoms in the figures below.
   the first figure gives a few representative atoms of discourse.

   a few of the 2000 atoms of discourse found

   and here are the discourse atoms used to represent two polysemous
   words, tie and spring

   discourse atoms expressing the words tie and spring.

   you can see that the discourse atoms do correspond to senses of these
   words.

   finally, we also have a technique that, given a target word, generates
   representative sentences according to its various senses as detected by
   the algorithm. below are the sentences returned for ring. (n.b. the
   mathematical meaning was missing in id138 but was picked up by our
   method.)

   representative sentences for different senses of the word ring.

a new testbed for testing comprehension of word senses

   many tests have been proposed to test an algorithm   s grasp of word
   senses. they often involve hard-to-understand metrics such as distance
   in id138, or sometimes tied to performance on specific applications
   like web search.

   we propose a new simple test    inspired by word-intrusion tests for
   topic coherence due to [17]chang et al 2009    which has the advantages
   of being easy to understand, and can also be administered to humans.

   we created a testbed using 200 polysemous words and their 704 senses
   according to id138. each    sense    is represented by a set of 8 related
   words; these were collected from id138 and online dictionaries by
   college students who were told to identify most relevant other words
   occurring in the online definitions of this word sense as well as in
   the accompanying illustrative sentences. these 8 words are considered
   as ground truth representation of the word sense: e.g., for the
      tool/weapon    sense of axe they were: handle, harvest, cutting, split,
   tool, wood, battle, chop.

     police line-up test for word senses: the algorithm is given a random
     one of these 200 polysemous words and a set of $m$ senses which
     contain the true sense for the word as well as some distractors,
     which are randomly picked senses from other words in the testbed.
     the test taker has to identify the word   s true senses amont these
     $m$ senses.

   as usual, accuracy is measured using precision (what fraction of the
   algorithm/human   s guesses were correct) and recall (how many correct
   senses were among the guesses).

   for $m=20$ and $k=4$, our algorithm succeeds with precision $63\%$ and
   recall $70\%$, and performance remains reasonable for $m=50$. we also
   administered the test to a group of grad students. native english
   speakers had precision/recall scores in the $75$ to $90$ percent range.
   non-native speakers had scores roughly similar to our algorithm.

   our algorithm works something like this: if $w$ is the target word,
   then take all discourse atoms computed for that word, and compute a
   certain similarity score between each atom and each of the $m$ senses,
   where the words in the senses are represented by their word vectors.
   (details are in the paper.)

takeaways

   id27s have been useful in a host of other settings, and now
   it appears that they also can easily yield different senses of a
   polysemous word. we have some subsequent applications of these ideas to
   other previously studied settings, including topic models, creating
   id138s for other languages, and understanding the semantic content of
   fmri brain measurements. i   ll describe some of them in future posts.
   subscribe to our [18]rss feed.
   spread the word:

comments

   please enable javascript to view the [19]comments powered by disqus.

   theme available on [20]github.

references

   visible links
   1. http://www.offconvex.org/feed.xml
   2. http://offconvex.github.io/
   3. http://www.offconvex.org/about/
   4. http://www.offconvex.org/contact/
   5. http://www.offconvex.org/subscribe/
   6. http://www.offconvex.org/2015/12/12/word-embeddings-1/
   7. http://www.offconvex.org/2016/02/14/word-embeddings-2/
   8. https://id138.princeton.edu/
   9. https://arxiv.org/abs/1601.03764
  10. http://www.offconvex.org/2015/12/12/word-embeddings-1/
  11. http://www.offconvex.org/2016/02/14/word-embeddings-2/
  12. http://128.84.21.199/abs/1502.03520v6
  13. http://www.offconvex.org/2016/02/14/word-embeddings-2/
  14. http://www.offconvex.org/2016/02/14/word-embeddings-2/
  15. http://www.cs.technion.ac.il/~elad/publications/others/pcmi2010-elad.pdf
  16. http://www.offconvex.org/2016/05/08/almostconvexitysatm/
  17. https://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf
  18. http://www.offconvex.org/feed.xml
  19. http://disqus.com/?ref_noscript
  20. https://github.com/johnotander/pixyll

   hidden links:
  22. https://facebook.com/sharer.php?u=http://offconvex.github.io/2016/07/10/embeddingspolysemy/
  23. https://twitter.com/intent/tweet?text=linear%20algebraic%20structure%20of%20word%20meanings&url=http://offconvex.github.io/2016/07/10/embeddingspolysemy/
  24. https://plus.google.com/share?url=http://offconvex.github.io/2016/07/10/embeddingspolysemy/
  25. http://www.linkedin.com/sharearticle?url=http://offconvex.github.io/2016/07/10/embeddingspolysemy/&title=linear%20algebraic%20structure%20of%20word%20meanings
  26. http://reddit.com/submit?url=http://offconvex.github.io/2016/07/10/embeddingspolysemy/&title=linear%20algebraic%20structure%20of%20word%20meanings
  27. https://news.ycombinator.com/submitlink?u=http://offconvex.github.io/2016/07/10/embeddingspolysemy/&t=linear%20algebraic%20structure%20of%20word%20meanings
