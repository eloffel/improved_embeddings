   #[1]rss feed for off the convex path

   [2][logo.jpg]

   [3]about [4]contact [5]subscribe

tensor methods in machine learning

   rong ge       dec 17, 2015       19 minute read

   tensors are high dimensional generalizations of matrices. in recent
   years tensor decompositions were used to design learning algorithms for
   estimating parameters of latent variable models like hidden markov
   model, mixture of gaussians and id44 (many of
   these works were considered as examples of    spectral learning   , read on
   to find out why). in this post i will briefly describe why tensors are
   useful in these settings.

   using [6]singular value decomposition (svd), we can write a matrix $m
   \in \mathbb{r}^{n\times m}$ as the sum of many rank one matrices:

   when the rank $r$ is small, this gives a concise representation for the
   matrix $m$ (using $(m+n)r$ parameters instead of $mn$). such
   decompositions are widely applied in machine learning.

   tensor decomposition is a generalization of low rank matrix
   decomposition. although [7]most tensor problems are np-hard in the
   worst case, several natural subcases of tensor decomposition can be
   solved in polynomial time. later we will see that these subcases are
   still very powerful in learning latent variable models.

matrix decompositions

   before talking about tensors, let us first see an example of how matrix
   factorization can be used to learn latent variable models. in 1904,
   psychologist charles spearman tried to understand whether human
   intelligence is a composite of different types of measureable
   intelligence. let   s describe a highly simplified version of his method,
   where the hypothesis is that there are exactly two kinds of
   intelligence: quantitative and verbal. spearman   s method consisted of
   making his subjects take several different kinds of tests. let   s name
   these tests classics, math, music, etc. the subjects scores can be
   represented by a matrix $m$, which has one row per student, and one
   column per test.

   matrix m

   the simplified version of spearman   s hypothesis is that each student
   has different amounts of quantitative and verbal intelligence, say
   $x_{quant}$ and $x_{verb}$ respectively. each test measures a different
   mix of intelligences, so say it gives a weighting $y_{quant}$ to
   quantitative and $y_{verb}$ to verbal. intuitively, a student with
   higher strength on verbal intelligence should perform better on a test
   that has a high weight on verbal intelligence. let   s describe this
   relationship as a simple bilinear function:

   denoting by $\vec x_{verb}, \vec x_{quant}$ the vectors describing the
   strengths of the students, and letting $\vec y_{verb}, \vec y_{quant}$
   be the vectors that describe the weighting of intelligences in the
   different tests, we can express matrix $m$ as the sum of two rank 1
   matrices (in other words, $m$ has rank at most $2$):

   thus verifying that $m$ has rank $2$ (or that it is very close to a
   rank $2$ matrix) should let us conclude that there are indeed two kinds
   of intelligence.

   matrix decomposition

   note that this decomposition is not the singular value decomposition
   (svd). svd requires strong orthogonality constraints (which translates
   to    different intelligences are completely uncorrelated   ) that are not
   plausible in this setting.

the ambiguity

   but ideally one would like to take the above idea further: we would
   like to assign a definitive quantitative/verbal intelligence score to
   each student. this seems simple at first sight: just read off the score
   from the decomposition. for instance, it shows alice is strongest in
   quantitative intelligence.

   however, this is incorrect, because the decomposition is not unique!
   the following is another valid decomposition

   matrix decomposition

   according to this decomposition, bob is strongest in quantitative
   intelligence, not alice. both decompositions explain the data perfectly
   and we cannot decide a priori which is correct.

   sometimes we can hope to find the unique solution by imposing
   additional constraints on the decomposition, such as all matrix entries
   have to be nonnegative. however even after imposing many natural
   constraints, in general the issue of multiple decompositions will
   remain.

adding the 3rd dimension

   since our current data has multiple explanatory decompositions, we need
   more data to learn exactly which explanation is the truth. assume the
   strength of the intelligence changes with time: we get better at
   quantitative tasks at night. now we can let the (poor) students take
   the tests twice: once during the day and once at night. the results we
   get can be represented by two matrices $m_{day}$ and $m_{night}$. but
   we can also think of this as a three dimensional array of numbers -    a
   tensor $t$ in $\mathbb{r}^{\sharp students\times \sharp tests\times
   2}$. here the third axis stands for    day    or    night   . we say the two
   matrices $m_{day}$ and $m_{night}$ are slices of the tensor $t$.

   matrix decomposition

   let $z_{quant}$ and $z_{verb}$ be the relative strength of the two
   kinds of intelligence at a particular time (day or night), then the new
   score can be computed by a trilinear function:

   keep in mind that this is the formula for one entry in the tensor: the
   score of one student, in one test and at a specific time. who the
   student is specifies $x_{quant}$ and $x_{verb}$; what the test is
   specifies weights $y_{quant}$ and $y_{verb}$; when the test takes place
   specifies $z_{quant}$ and $z_{verb}$.

   similar to matrices, we can view this as a rank 2 decomposition of the
   tensor $t$. in particular, if we use $\vec x_{quant}, \vec x_{verb}$ to
   denote the strengths of students, $\vec y_{quant},\vec y_{verb}$ to
   denote the weights of the tests and $\vec z_{quant}, \vec z_{verb}$ to
   denote the variations of strengths in time, then we can write the
   decomposition as

   matrix decomposition

   now we can check that the second matrix decomposition we had is no
   longer valid: there are no values of $z_{quant}$ and $z_{verb}$ at
   night that could generate the matrix $m_{night}$. this is not a
   coincidence. [8]kruskal 1977 gave sufficient conditions for such
   decompositions to be unique. when applied to our case it is very
   simple:

     corollary the decomposition of tensor $t$ is unique (up to scaling
     and permutation) if none of the vector pairs $(\vec x_{quant}, \vec
     x_{verb})$, $(\vec y_{quant},\vec y_{verb})$, $(\vec z_{quant},\vec
     z_{verb})$ are co-linear.

   note that of course the decomposition is not truly unique for two
   reasons. first, the two tensor factors are symmetric, and we need to
   decide which factor correspond to quantitative intelligence. second, we
   can scale the three components $\vec x_{quant}$ ,$\vec y_{quant}$,
   $\vec z_{quant}$ simultaneously, as long as the product of the three
   scales is 1. intuitively this is like using different units to measure
   the three components. kruskal   s result showed that these are the only
   degrees of freedom in the decomposition, and there cannot be a truly
   distinct decomposition as in the matrix case.

finding the tensor

   in the above example we get a low rank tensor $t$ by gathering more
   data. in many traditional applications the extra data may be
   unavailable or hard to get. luckily, many exciting recent developments
   show that we can uncover these special tensor structures even if the
   original data is not in a tensor form!

   the main idea is to use method of moments (see a nice [9]post by
   moritz): estimate lower order correlations of the variables, and hope
   these lower order correlations have a simple tensor form.

   consider hidden markov model as an example. id48 are
   widely used in analyzing sequential data like speech or text. here for
   concreteness we consider a (simplified) model of natural language
   texts(which is a basic version of the [10]id27s).

   in hidden markov model, we observe a sequence of words (a sentence)
   that is generated by a walk of a hidden markov chain: each word has a
   hidden topic $h$ (a discrete random variable that specifies whether the
   current word is talking about    sports    or    politics   ); the topic for
   the next word only depends on the topic of the current word. each topic
   specifies a distribution over words. instead of the topic itself, we
   observe a random word $x$ drawn from this topic distribution (for
   example, if the topic is    sports   , we will more likely see words like
      score   ). the dependencies are usually illustrated by the following
   diagram:

   hidden markov model

   more concretely, to generate a sentence in hidden markov model, we
   start with some initial topic $h_1$. this topic will evolve as a markov
   chain to generate the topics for future words $h_2, h_3,   ,h_t$. we
   observe words $x_1,   ,x_t$ from these topics. in particular, word $x_1$
   is drawn according to topic $h_1$, word $x_2$ is drawn according to
   topic $h_2$ and so on.

   given many sentences that are generated exactly according to this
   model, how can we construct a tensor? a natural idea is to compute
   correlations: for every triple of words $(i,j,k)$, we count the number
   of times that these are the first three words of a sentence.
   enumerating over $i,j,k$ gives us a three dimensional array (a tensor)
   $t$. we can further normalize it by the total number of sentences.
   after id172 the $(i,j,k)$-th entry of the tensor will be an
   estimation of the id203 that the first three words are $(i,j,k)$.
   for simplicity assume we have enough samples and the estimation is
   accurate:

   why does this tensor have the nice low rank property? the key
   observation is that if we    fix    (condition on) the topic of the second
   word $h_2$, it cuts the graph into three parts: one part containing
   $h_1,x_1$, one part containing $x_2$ and one part containing $h_3,x_3$.
   these three parts are independent conditioned on $h_2$. in particular,
   the first three words $x_1,x_2,x_3$ are independent conditioned on the
   topic of the second word $h_2$. using this observation we can compute
   each entry of the tensor as

   now if we let $\vec x_l$ be a vector whose $i$-th entry is the
   id203 of the first word is $i$, given the topic of the second
   word is $l$; let $\vec y_l$ and $\vec z_l$ be similar for the second
   and third word. we can then write the entire tensor as

   this is exactly the low rank form we are looking for! tensor
   decomposition allows us to uniquely identify these components, and
   further infer the other probabilities we are interested in. for more
   details see the paper by [11]anandkumar et al. 2012 (this paper uses
   the tensor notations, but the original idea appeared in the paper by
   [12]mossel and roch 2006).

implementing tensor decomposition

   using method of moments, we can discover nice tensor structures from
   many problems. the uniqueness of tensor decomposition makes these
   tensors very useful in learning the parameters of the models. but how
   do we compute the tensor decompositions?

   in the worst case we have bad news: [13]most tensor problems are
   np-hard. however, in most natural cases, as long as the tensor does not
   have too many components, and the components are not adversarially
   chosen, tensor decomposition can be computed in polynomial time! here
   we describe the algorithm by dr. robert jenrich (it first appeared in a
   1970 working paper by [14]harshman, the version we present here is a
   more general version by [15]leurgans, ross and abel 1993).

     jenrich   s algorithm
     input: tensor $t = \sum_{i=1}^r \lambda_i \vec x_i \otimes \vec y_i
     \otimes \vec z_i$.
    1. pick two random vectors $\vec u, \vec v$.
    2. compute $t_\vec u = \sum_{i=1}^n u_i t[:,:,i] = \sum_{i=1}^r
       \lambda_i (\vec u^\top \vec z_i) \vec x_i \vec y_i^\top$.
    3. compute $t_\vec v = \sum_{i=1}^n v_i t[:,:,i] = \sum_{i=1}^r
       \lambda_i (\vec v^\top \vec z_i) \vec x_i \vec y_i^\top$.
    4. $\vec x_i$   s are eigenvectors of $t_\vec u (t_\vec v)^{+}$, $\vec
       y_i$   s are eigenvectors of $t_\vec v (t_\vec u)^{+}$.

   in the algorithm,    $^+$    denotes pseudo-inverse of a matrix (think of
   it as inverse if this is not familiar).

   the algorithm looks at weighted slices of the tensor: a weighted slice
   is a matrix that is the projection of the tensor along the $z$
   direction (similarly if we take a slice of a matrix $m$, it will be a
   vector that is equal to $m\vec u$). because of the low rank structure,
   all the slices must share matrix decompositions with the same
   components.

   the main observation of the algorithm is that although a single matrix
   can have infinitely many low rank decompositions, two matrices can only
   have a unique decomposition if we require them to have the same
   components. in fact, it is highly unlikely for two arbitrary matrices
   to share decompositions with the same components. in the tensor case,
   because of the low rank structure we have

   where $d_\vec u,d_\vec v$ are diagonal matrices. this is called a
   simultaneous diagonalization for $t_\vec u$ and $t_\vec v$. with this
   structure it is easy to show that $\vec x_i$   s are eigenvectors of
   $t_\vec u (t_\vec v)^{+} = x d_\vec u d_\vec v^{-1} x^+$. so we can
   actually compute tensor decompositions using spectral decompositions
   for matrices.

   many of the earlier works (including [16]mossel and roch 2006) that
   apply tensor decompositions to learning problems have actually
   independently rediscovered this algorithm, and the word    tensor    never
   appeared in the papers. in fact, tensor decomposition techniques are
   traditionally called    spectral learning    since they are seen as derived
   from svd. but now we have other methods to do tensor decompositions
   that have better theoretical guarantees and practical performances. see
   the survey by [17]kolda and bader 2009 for more discussions.

related links

   for more examples of using tensor decompositions to learn latent
   variable models, see the paper by [18]anandkumar et al. 2012. this
   paper shows that several prior algorithms for learning models such as
   hidden markov model, id44, mixture of gaussians
   and independent component analysis can be interpreted as doing tensor
   decompositions. the paper also gives a proof that tensor power method
   is efficient and robust to noise.

   recent research focuses on two problems: how to formulate other
   learning problems as tensor decompositions, and how to compute tensor
   decompositions under weaker assumptions. using tensor decompositions,
   we can learn more models that include [19]community models,
   [20]probabilistic context-free-grammars, [21]mixture of general
   gaussians and [22]two-layer neural networks. we can also efficiently
   compute tensor decompositions when the rank of the tensor is much
   larger than the dimension (see for example the papers by [23]bhaskara
   et al. 2014, [24]goyal et al. 2014, [25]ge and ma 2015). there are many
   other interesting works and open problems, and the list here is by no
   means complete.
   subscribe to our [26]rss feed.
   spread the word:

comments

   please enable javascript to view the [27]comments powered by disqus.

   theme available on [28]github.

references

   visible links
   1. http://www.offconvex.org/feed.xml
   2. http://offconvex.github.io/
   3. http://www.offconvex.org/about/
   4. http://www.offconvex.org/contact/
   5. http://www.offconvex.org/subscribe/
   6. https://en.wikipedia.org/wiki/singular_value_decomposition
   7. http://arxiv.org/abs/0911.1393
   8. http://www.sciencedirect.com/science/article/pii/0024379577900696
   9. http://blog.mrtz.org/2014/04/22/pearsons-polynomial.html
  10. http://www.offconvex.org/2015/12/12/word-embeddings-1/
  11. http://arxiv.org/abs/1210.7559
  12. https://projecteuclid.org/euclid.aoap/1151592244
  13. http://arxiv.org/abs/0911.1393
  14. http://hbanaszak.mjr.uw.edu.pl/temptxt/harshman_1970_foundations of parafac procedure models and conditions for an expalanatory multimodal factor analysis.pdf
  15. http://dl.acm.org/citation.cfm?id=173234
  16. https://projecteuclid.org/euclid.aoap/1151592244
  17. http://dl.acm.org/citation.cfm?id=1655230
  18. http://arxiv.org/abs/1210.7559
  19. http://arxiv.org/abs/1302.2684
  20. http://www.cs.columbia.edu/~mcollins/papers/uai2014-long.pdf
  21. http://arxiv.org/abs/1503.00424
  22. http://newport.eecs.uci.edu/anandkumar/pubs/nn_generalizationbound.pdf
  23. http://arxiv.org/abs/1311.3651
  24. http://arxiv.org/abs/1306.5825
  25. http://arxiv.org/abs/1504.05287
  26. http://www.offconvex.org/feed.xml
  27. http://disqus.com/?ref_noscript
  28. https://github.com/johnotander/pixyll

   hidden links:
  30. https://facebook.com/sharer.php?u=http://offconvex.github.io/2015/12/17/tensor-decompositions/
  31. https://twitter.com/intent/tweet?text=tensor%20methods%20in%20machine%20learning&url=http://offconvex.github.io/2015/12/17/tensor-decompositions/
  32. https://plus.google.com/share?url=http://offconvex.github.io/2015/12/17/tensor-decompositions/
  33. http://www.linkedin.com/sharearticle?url=http://offconvex.github.io/2015/12/17/tensor-decompositions/&title=tensor%20methods%20in%20machine%20learning
  34. http://reddit.com/submit?url=http://offconvex.github.io/2015/12/17/tensor-decompositions/&title=tensor%20methods%20in%20machine%20learning
  35. https://news.ycombinator.com/submitlink?u=http://offconvex.github.io/2015/12/17/tensor-decompositions/&t=tensor%20methods%20in%20machine%20learning
