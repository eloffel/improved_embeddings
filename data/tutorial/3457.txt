   #[1]rss feed for off the convex path

   [2][logo.jpg]

   [3]about [4]contact [5]subscribe

semantic id27s

   sanjeev arora       dec 12, 2015       12 minute read

   this post can be seen as an introduction to how nonconvex problems
   arise naturally in practice, and also the relative ease with which they
   are often solved.

   i will talk about id27s, a geometric way to capture the
      meaning    of a word via a low-dimensional vector. they are useful in
   many tasks in information retrieval (ir) and natural language
   processing (nlp), such as answering search queries or translating from
   one language to another.

   you may wonder: how can a 300-dimensional vector capture the many
   nuances of word meaning? and what the heck does it mean to    capture
   meaning?   

properties of id27s

   a simple property of embeddings obtained by all the methods i   ll
   describe is cosine similarity: the similarity between two words (as
   rated by humans on a $[-1,1]$ scale) correlates with the cosine of the
   angle between their vectors. to give an example, the cosine for milk
   and cow may be $0.6$, whereas for milk and stone it may be $0.2$, which
   is roughly the similarity human subjects assign to them.

   a more interesting property of recent embeddings is that they can solve
   analogy relationships via id202. for example, the word analogy
   question man : woman ::king : ?? can be solved by looking for the word
   $w$ such that $v_{king} - v_w$ is most similar to $v_{man} -
   v_{woman}$; in other words, minimizes

   this simple idea can solve $75\%$ of analogy questions on some standard
   testbed. note that the method is completely unsupervised: it constructs
   the embeddings using a big (unannotated) text corpus; and receives no
   training specific to analogy solving). here is a rendering of this
   id202ic relationship between masculine-feminine pairs.
   [analogy-small.jpg]

   good embeddings have other properties that will be covered in a future
   post. (also, i can   t resist mentioning that fmri-imaging of the brain
   suggests that id27s are related to how the human brain
   encodes meaning; see the [6]well-known paper of mitchell et al..)

computing id27s (via firth   s hypothesis)

   in all methods, the word vector is a succinct representation of the
   distribution of other words around this word. that this suffices to
   capture meaning is asserted by [7]firth   s hypothesis from 1957,    you
   shall know a word by the company it keeps.    to give an example, if i
   ask you to think of a word that tends to co-occur with cow, drink,
   babies, calcium, you would immediately answer: milk.

   note that we don   t believe firth   s hypothesis fully accounts for all
   aspects of semantics    understanding new metaphors or jokes, for
   example, seems to require other modes of experiencing the real world
   than simply reading text.

   but firth   s hypothesis does imply a very simple id27, albeit
   a very high-dimensional one.

     embedding 1: suppose the dictionary has $n$ distinct words (in
     practice, $n =100,000$). take a very large text corpus (e.g.,
     wikipedia) and let $count_5(w_1, w_2)$ be the number of times $w_1$
     and $w_2$ occur within a distance $5$ of each other in the corpus.
     then the id27 for a word $w$ is a vector of dimension $n$,
     with one coordinate for each dictionary word. the coordinate
     corresponding to word $w_2$ is $count_5(w, w_2)$. (variants of this
     method involve considering cooccurence of $w$ with various phrases
     or $n$-tuples.)

   the obvious problem with embedding 1 is that it uses extremely
   high-dimensional vectors. how can we compress them?

     embedding 2: do dimension reduction by taking the rank-300 singular
     value decomposition (svd) of the above vectors.

   recall that for an $n \times n$ matrix $m$ this means finding vectors
   $v_1, v_2, \ldots, v_n \in \mathbb{r}^{300}$ that minimize

   using svd to do dimension reduction seems an obvious idea these days
   but it actually is not. after all, it is unclear a priori why the above
   $n \times n$ matrix of cooccurance counts should be close to a rank-300
   matrix. that this is the case was empirically discovered in the paper
   on [8]id45 or lsi.

   empirically, the method can be improved by replacing the counts by
   their logarithm, as in [9]latent semantic analysis or lsa. other
   authors claim square root is even better than logarithm. another
   interesting empirical fact in the lsa paper is that dimension reduction
   via svd not only compresses the embedding but improves its quality.
   (improvement via compression is a familiar phenomenon in machine
   learning.) in fact they can use word vectors to solve word similarity
   tasks as well as the average american high schooler.

   a research area called vector space models (see [10]survey by turney
   and pantel) studies various modifications of the above idea. embeddings
   are also known to improve if we reweight the various terms in the above
   expression (2): popular reweightings include tf-idf, pmi, logarithm,
   etc.

   let me point out that reweighting the $(i, j)$ term in expression (1)
   leads to a weighted version of svd, which is np-hard. (i always
   emphasize to my students that a polynomial-time algorithm to compute
   rank-k svd is a miracle, since modifying the problem statement in small
   ways makes it np-hard.) but in practice, weighted svd can be solved on
   a laptop in less than a day    remember, $n$ is rather large, about
   $10^5$!    by simple id119 on the objective (1), possibly also
   using a regularizer. (of course, [11]a lot has been proven about such
   id119 methods in context of id76; the
   surprise is that they work also in such nonconvex settings.) weighted
   svd is a subcase of id105 approaches in machine
   learning, which we will also encounter again in upcoming posts.

   but returning to id27s, the following question had not been
   raised or debated, to the best of my knowledge: what property of human
   language explains the fact that these very high-dimensional matrices
   derived (in a nonlinear way) from word cooccurences are close to
   low-rank matrices? (in a future blog post i will describe our new
   [12]theoretical explanation.)

   the third embedding method i wish to describe uses energy-based models,
   for instance the [13]id97 family of methods from 2013 by the google
   team of mikolov et al., which also created a buzz due to the
   above-mentioned id202ic method to solve word analogy tasks.
   the id97 models are inspired by pre-existing neural net models for
   language (basically, the id27 corresponds to the neural net   s
   internal representation of the word; see [14]this blog.). let me
   describe the simplest variant, which assumes that the word vectors are
   related to word probabilities as follows:

     embedding 3 (id97(cbow)):

     where the left hand side gives the empirical id203 that word
     $w$ occurs in the text conditional on the last five words being
     $w_1$ through $w_5$.

   assume we can estimate the left hand side using a large text corpus.
   then expression (2) for the word vectors   together with a constraint
   capping the dimension of the vectors to, say, 300     implicitly defines
   a nonid76 problem which is solved in practice as
   follows. let $s$ be the set of all the $6$-tuples of words that occur
   in the text. let $n$ be a set of random $6$-tuples; this set is called
   the negative sample since presumably these tuples are gibberish. the
   method consists of finding id27s that give high id203
   to tuples in $s$ and low id203 to tuples in $n$. roughly
   speaking, they maximise the difference between the following two
   quantities: (i) sum of $\exp(v_w \cdot (\frac{1}{5} \sum_i v_{w_i}))$
   (suitably scaled) over all $6$-tuples in $s$, and (ii) the
   corresponding sum over tuples in $n$. the id97 team introduced some
   other tweaks that allowed them to solve this optimization for very
   large corpora containing 10 billion words.

   the id97 papers are a bit mysterious, and have motivated much
   followup work. a paper by levy and goldberg (see [15]omer levy   s blog)
   explains that the id97 methods are actually modern versions of
   older vector space methods. after all, if you take logs of both sides
   of expression (2), you see that the logarithm of some cooccurence
   id203 is being expressed in terms of inner products of some word
   vectors, which is very much in the spirit of the older work. (levy and
   goldberg have more to say about this, backed up by interesting
   experiments comparing the vectors obtained by various approaches.)

   another paper by pennington et al. at stanford suggests a [16]model
   called glove that uses an explicit weighted-svd strategy for finding
   id27s. they also give an intuitive explanation of why these
   embeddings solve word analogy tasks, though the explanation isn   t quite
   rigorous.

   in a future post i will talk more about our [17]subsequent theoretical
   work that tries to unify these different approaches, and also explains
   some cool id202ic properties of id27s. i note that
   linear structure also arises in [18]representations of images learnt
   via deep learning., and it is tantalising to wonder if similar theory
   applies to that setting.
   subscribe to our [19]rss feed.
   spread the word:

comments

   please enable javascript to view the [20]comments powered by disqus.

   theme available on [21]github.

references

   visible links
   1. http://www.offconvex.org/feed.xml
   2. http://offconvex.github.io/
   3. http://www.offconvex.org/about/
   4. http://www.offconvex.org/contact/
   5. http://www.offconvex.org/subscribe/
   6. http://www.cs.cmu.edu/~tom/pubs/science2008.pdf
   7. https://en.wikipedia.org/wiki/distributional_semantics
   8. http://lsa.colorado.edu/papers/jasis.lsi.90.pdf
   9. http://lsa.colorado.edu/papers/plato/plato.annote.html
  10. https://www.jair.org/media/2934/live-2934-4846-jair.pdf
  11. http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html
  12. http://arxiv.org/abs/1502.03520
  13. https://code.google.com/p/id97/
  14. http://colah.github.io/posts/2014-07-nlp-id56s-representations/
  15. https://levyomer.wordpress.com/
  16. http://nlp.stanford.edu/projects/glove/
  17. http://arxiv.org/abs/1502.03520
  18. http://web.eecs.umich.edu/~honglak/nips2015-analogy.pdf
  19. http://www.offconvex.org/feed.xml
  20. http://disqus.com/?ref_noscript
  21. https://github.com/johnotander/pixyll

   hidden links:
  23. https://facebook.com/sharer.php?u=http://offconvex.github.io/2015/12/12/word-embeddings-1/
  24. https://twitter.com/intent/tweet?text=semantic%20word%20embeddings&url=http://offconvex.github.io/2015/12/12/word-embeddings-1/
  25. https://plus.google.com/share?url=http://offconvex.github.io/2015/12/12/word-embeddings-1/
  26. http://www.linkedin.com/sharearticle?url=http://offconvex.github.io/2015/12/12/word-embeddings-1/&title=semantic%20word%20embeddings
  27. http://reddit.com/submit?url=http://offconvex.github.io/2015/12/12/word-embeddings-1/&title=semantic%20word%20embeddings
  28. https://news.ycombinator.com/submitlink?u=http://offconvex.github.io/2015/12/12/word-embeddings-1/&t=semantic%20word%20embeddings
