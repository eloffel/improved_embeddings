   #[1]rss feed for off the convex path

   [2][logo.jpg]

   [3]about [4]contact [5]subscribe

back-propagation, an introduction

   sanjeev arora and tengyu ma       dec 20, 2016       21 minute read

   given the sheer number of id26 tutorials on the internet, is
   there really need for another? one of us (sanjeev) recently taught
   id26 in [6]undergrad ai and couldn   t find any account he was
   happy with. so here   s our exposition, together with some history and
   context, as well as a few advanced notions at the end. this article
   assumes the reader knows the definitions of gradients and neural
   networks.

what is id26?

   it is the basic algorithm in training neural nets, apparently
   independently rediscovered several times in the 1970-80   s (e.g., see
   werbos    [7]ph.d. thesis and [8]book, and [9]rumelhart et al.). some
   related ideas existed in control theory in the 1960s. (one reader
   points out another independent rediscovery, the baur-strassen lemma
   from 1983.)

   id26 gives a fast way to compute the sensitivity of the
   output of a neural network to all of its parameters while keeping the
   inputs of the network fixed: specifically it computes all partial
   derivatives ${\partial f}/{\partial w_i}$ where $f$ is the output and
   $w_i$ is the $i$th parameter. (here parameters can be edge weights or
   biases associated with nodes or edges of the network, and the precise
   details of the node computations    e.g., the precise form of
   nonlinearity like sigmoid or relu    are unimportant.) doing so gives the
   gradient $\nabla f$ of $f$ with respect to its network parameters,
   which allows a id119 step in the training: change all
   parameters simultaneously to move the vector of parameters a small
   amount in the direction $-\nabla f$.

   note that id26 computes the gradient exactly, but properly
   training neural nets needs many more tricks than just id26.
   understanding id26 is useful for appreciating some advanced
   tricks.

   the importance of id26 derives from its efficiency. assuming
   node operations take unit time, the running time is linear,
   specifically, $o(\text{network size}) = o(v + e)$, where $v$ is the
   number of nodes in the network and $e$ is the number of edges. the only
   technical ingredient is chain rule from calculus, but applying it
   naively would have resulted in quadratic running time   which would be
   hugely inefficient for networks with millions or even thousands of
   parameters.

   id26 can be efficiently implemented using highly parallel
   vector operations available in today   s [10]gpus (graphical processing
   units), which play an important role in the the recent neural nets
   revolution.

   side note: expert readers will recognize that in the standard accounts
   of neural net training, the actual quantity of interest is the gradient
   of the training loss, which happens to be a simple function of the
   network output. but the above phrasing is fully general since one can
   simply add a new output node to the network that computes the training
   loss from the old output. then the quantity of interest is indeed the
   gradient of this new output with respect to network parameters.

problem setup

   id26 applies only to acyclic networks with directed edges.
   (later we briefly sketch its use on networks with cycles.)

   without loss of generality, acyclic networks can be visualized as being
   structured in numbered layers, with nodes in the $t+1$th layer getting
   all their inputs from the outputs of nodes in layers $t$ and earlier.
   we use $f \in \mathbb{r}$ to denote the output of the network. in all
   our figures, the input of the network is at the bottom and the output
   on the top.

   we start with a simple claim that reduces the problem of computing the
   gradient to the problem of computing partial derivatives with respect
   to the nodes:

     claim 1: to compute the desired gradient with respect to the
     parameters, it suffices to compute $\partial f/\partial u$ for every
     node $u$.

   let   s be clear what $\partial f/\partial u$ means. suppose we cut off
   all the incoming edges of the node $u$, and fix/clamp the current
   values of all network parameters. now imagine changing $u$ from its
   current value. this change may affect values of nodes at higher levels
   that are connected to $u$, and the final output $f$ is one such node.
   then $\partial f/\partial u$ denotes the rate at which $f$ will change
   as we vary $u$. (aside: readers familiar with the usual exposition of
   back-propagation should note that there $f$ is the training error and
   this $\partial f/\partial u$ turns out to be exactly the    error   
   propagated back to on the node $u$.)

   claim 1 is a direct application of chain rule, and let   s illustrate it
   for a simple neural nets (we address more general networks later).
   suppose node $u$ is a weighted sum of the nodes $z_1,\dots, z_m$ (which
   will be passed through a non-linear activation $\sigma$ afterwards).
   that is, we have $u = w_1z_1+\dots+w_nz_n$. by chain rule, we have

   hence, we see that having computed $\partial f/\partial u$ we can
   compute $\partial f/\partial w_1$, and moreover this can be done
   locally by the endpoints of the edge where $w_1$ resides.
   [weight5.jpg]

multivariate chain rule

   towards computing the derivatives with respect to the nodes, we first
   recall the multivariate chain rule, which handily describes the
   relationships between these partial derivatives (depending on the graph
   structure).

   suppose a variable $f$ is a function of variables $u_1,\dots, u_n$,
   which in turn depend on the variable $z$. then, multivariate chain rule
   says that

   this is a direct generalization of eqn. (2) and a sub-case of eqn. (11)
   in this [11]description of chain rule.

   this formula is perfectly suitable for our cases. below is the same
   example as we used before but with a different focus and numbering of
   the nodes.
   [chain_rule_5.jpg]

   we see that given we   ve computed the derivatives with respect to all
   the nodes that is above the node $z$, we can compute the derivative
   with respect to the node $z$ via a weighted sum, where the weights
   involve the local derivative ${\partial u_j}/{\partial z}$ that is
   often easy to compute. this brings us to the question of how we measure
   running time. for book-keeping, we assume that

     basic assumption: if $u$ is a node at level $t+1$ and $z$ is any
     node at level $\leq t$ whose output is an input to $u$, then
     computing $\frac{\partial u}{\partial z}$ takes unit time on our
     computer.

naive feedforward algorithm (not efficient!)

   it is useful to first point out the naive quadratic time algorithm
   implied by the chain rule. most authors skip this trivial version,
   which we think is analogous to teaching sorting using only quicksort,
   and skipping over the less efficient bubblesort.

   the naive algorithm is to compute $\partial u_i/\partial u_j$ for every
   pair of nodes where $u_i$ is at a higher level than $u_j$. of course,
   among these $v^2$ values (where $v$ is the number of nodes) are also
   the desired ${\partial f}/{\partial u_i}$ for all $i$ since $f$ is
   itself the value of the output node.

   this computation can be done in feedforward fashion. if such value has
   been obtained for every $u_j$ on the level up to and including level
   $t$, then one can express (by inspecting the multivariate chain rule)
   the value $\partial u_{\ell}/\partial u_j$ for some $u_{\ell}$ at level
   $t+1$ as a weighted combination of values $\partial u_{i}/\partial u_j$
   for each $u_i$ that is a direct input to $u_{\ell}$. this description
   shows that the amount of computation for a fixed $j$ is proportional to
   the number of edges $e$. this amount of work happens for all $v$ values
   of $j$, letting us conclude that the total work in the algorithm is
   $o(ve)$.

id26 (linear time)

   the more efficient id26, as the name suggests, computes the
   partial derivatives in the reverse direction. messages are passed in
   one wave backwards from higher number layers to lower number layers.
   (some presentations of the algorithm describe it as dynamic
   programming.)

     messaging protocol: the node $u$ receives a message along each
     outgoing edge from the node at the other end of that edge. it sums
     these messages to get a number $s$ (if $u$ is the output of the
     entire net, then define $s=1$) and then it sends the following
     message to any node $z$ adjacent to it at a lower level:

   clearly, the amount of work done by each node is proportional to its
   degree, and thus overall work is the sum of the node degrees. summing
   all node degrees counts each edge twice, and thus the overall work is
   $o(\text{network size})$.

   to prove correctness, we prove the following:

     main claim: at each node $z$, the value $s$ is exactly ${\partial
     f}/{\partial z}$.

   base case: at the output layer this is true, since ${\partial
   f}/{\partial f} =1$.

   inductive case: suppose the claim was true for layers $t+1$ and higher
   and $u$ is at layer $t$, with outgoing edges go to some nodes $u_1,
   u_2, \ldots, u_m$ at levels $t+1$ or higher. by inductive hypothesis,
   node $z$ indeed receives $ \frac{\partial f}{\partial u_j}\times
   \frac{\partial u_j}{\partial z}$ from each of $u_j$. thus by chain
   rule, this completes the induction and proves the main claim.

auto-differentiation

   since the exposition above used almost no details about the network and
   the operations that the node perform, it extends to every computation
   that can be organized as an acyclic graph whose each node computes a
   differentiable function of its incoming neighbors. this observation
   underlies many auto-differentiation packages such as [12]autograd or
   [13]tensorflow: they allow computing the gradient of the output of such
   a computation with respect to the network parameters.

   we first observe that claim 1 continues to hold in this very general
   setting. this is without loss of generality because we can view the
   parameters associated to the edges as also sitting on the nodes
   (actually, leaf nodes). this can be done via a simple transformation to
   the network; for a single node it is shown in the picture below; and
   one would need to continue to do this transformation in the rest of the
   networks feeding into $u_1, u_2,..$ etc from below.
   [change_view]

   then, we can use the messaging protocol to compute the derivatives with
   respect to the nodes, as long as the local partial derivative can be
   computed efficiently. we note that the algorithm can be implemented in
   a fairly modular manner: for every node $u$, it suffices to specify (a)
   how it depends on the incoming nodes, say, $z_1,\dots, z_n$ and (b) how
   to compute the partial derivative times $s$, that is, $s \cdot
   \frac{\partial u}{\partial z_j}$.

   extension to vector messages: in fact (b) can be done efficiently in
   more general settings where we allow the output of each node in the
   network to be a vector (or even matrix/tensor) instead of only a real
   number. here we need to replace $\frac{\partial u}{\partial z_j}\cdot
   s$ by $\frac{\partial u}{\partial z_j}[s]$, which denotes the result of
   applying the operator $\frac{\partial u}{\partial z_j}$ on $s$. we note
   that to be consistent with the convention in the usual exposition of
   id26, when $y\in \mathbb{r}^{p}$ is a funciton of $x\in
   \mathbb{r}^q$, we use $\frac{\partial y}{\partial x}$ to denote
   $q\times p$ dimensional matrix with $\partial y_j/\partial x_i$ as the
   $(i,j)$-th entry. readers might notice that this is the transpose of
   the usual jacobian matrix defined in mathematics. thus $\frac{\partial
   y}{\partial x}$ is an operator that maps $\mathbb{r}^p$ to
   $\mathbb{r}^q$ and we can verify $s$ has the same dimension as $u$ and
   $\frac{\partial u}{\partial z_j}[s]$ has the same dimension as $z_j$.

   for example, as illustrated below, suppose the node $u\in
   \mathbb{r}^{d_1\times d_3} $ is a product of two matrices $w\in
   \mathbb{r}^{d_2\times d_3}$ and $z\in \mathbb{r}^{d_1\times d_2}$. then
   we have that $\partial u/\partial z$ is a linear operator that maps
   $\mathbb{r}^{d_2\times d_3}$ to $\mathbb{r}^{d_1\times d_3}$, which
   naively requires a matrix representation of dimension $d_2d_3\times
   d_1d_3$. however, the computation (b) can be done efficiently because

   such vector operations can also be implemented efficiently using
   today   s gpus.
   [mult.jpg]

notable extensions

   1) allowing weight tying. in many neural architectures, the designer
   wants to force many network units such as edges or nodes to share the
   same parameter. for example, in [14]convolutional neural nets, the same
   filter has to be applied all over the image, which implies reusing the
   same parameter for a large set of edges between the two layers.

   for simplicity, suppose two parameters $a$ and $b$ are supposed to
   share the same value. this is equivalent to adding a new node $u$ and
   connecting $u$ to both $a$ and $b$ with the operation $a = u$ and
   $b=u$. thus, by chain rule, hence, equivalently, the gradient with
   respect to a shared parameter is the sum of the gradients with respect
   to individual occurrences.

   2) id26 on networks with loops. the above exposition assumed
   the network is acyclic. many cutting-edge applications such as machine
   translation and language understanding use networks with directed loops
   (e.g., recurrent neural networks). these architectures    all examples of
   the    differentiable computing    paradigm below   can get complicated and
   may involve operations on a separate memory as well as mechanisms to
   shift attention to different parts of data and memory.

   networks with loops are trained using id119 as well, using
   [15]back-propagation through time, which consists of expanding the
   network through a finite number of time steps into an acyclic graph,
   with replicated copies of the same network. these replicas share the
   weights (weight tying!) so the gradient can be computed. in practice an
   issue may arise with [16]exploding or vanishing gradients which impact
   convergence. such issues can be carefully addressed in practice by
   clipping the gradient or re-parameterization techniques such as
   [17]long short-term memory.

   the fact that the gradient can be computed efficiently for such general
   networks with loops has motivated neural net models with memory or even
   data structures (see for example [18]id63s and
   [19]differentiable neural computer). using id119, one can
   optimize over a family of parameterized networks with loops to find the
   best one that solves a certain computational task (on the training
   examples). the limits of these ideas are still being explored.

   3) hessian-vector product in linear time. it is possible to generalize
   backprop to enable 2nd order optimization in    near-linear    time, not
   just id119, as shown in recent independent manuscripts of
   [20]carmon et al. and [21]agarwal et al. (nb: tengyu is a coauthor on
   this one.). one essential step is to compute the product of the
   [22]hessian matrix and a vector, for which [23]pearlmutter   93 gave an
   efficient algorithm. here we show how to do this in $o(\mbox{network
   size})$ using the ideas above. we need a slightly stronger version of
   the back-propagation result than the one in the previous subsection:

     claim (informal): suppose an acyclic network with $v$ nodes and $e$
     edges has output $f$ and leaves $z_1,\dots, z_m$. then there exists
     a network of size $o(v+e)$ that has $z_1,\dots, z_m$ as input nodes
     and $\frac{\partial f}{\partial z_1},\dots, \frac{\partial
     f}{\partial z_m}$ as output nodes.

   the proof of the claim follows in straightforward fashion from
   implementing the message passing protocol as an acyclic circuit.

   next we show how to compute $\nabla^2 f(z)\cdot v$ where $v$ is a given
   fixed vector. let $g(z)= \langle \nabla f(z),v\rangle$ be a function
   from $\mathbb{r}^d\rightarrow \mathbb{r}$. then by the claim above,
   $g(z)$ can be computed by a network of size $o(v+e)$. now apply the
   claim again on $g(z)$, we obtain that $\nabla g(z)$ can also be
   computed by a network of size $o(v+e)$.

   note that by construction, hence we have computed the hessian vector
   product in network size time.

   ##that   s all!

   please write your comments on this exposition and whether it can be
   improved.
   subscribe to our [24]rss feed.
   spread the word:

comments

   please enable javascript to view the [25]comments powered by disqus.

   theme available on [26]github.

references

   visible links
   1. http://www.offconvex.org/feed.xml
   2. http://offconvex.github.io/
   3. http://www.offconvex.org/about/
   4. http://www.offconvex.org/contact/
   5. http://www.offconvex.org/subscribe/
   6. https://www.cs.princeton.edu/courses/archive/fall16/cos402/
   7. https://www.researchgate.net/publication/35657389_beyond_regression_new_tools_for_prediction_and_analysis_in_the_behavioral_sciences
   8. http://www.wiley.com/wileycda/wileytitle/productcd-0471598976.html
   9. http://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html
  10. https://en.wikipedia.org/wiki/general-purpose_computing_on_graphics_processing_units
  11. http://mathworld.wolfram.com/chainrule.html
  12. https://github.com/hips/autograd
  13. https://www.tensorflow.org/
  14. https://en.wikipedia.org/wiki/convolutional_neural_network
  15. https://en.wikipedia.org/wiki/id26_through_time
  16. https://en.wikipedia.org/wiki/vanishing_gradient_problem
  17. https://en.wikipedia.org/wiki/long_short-term_memory
  18. https://en.wikipedia.org/wiki/neural_turing_machine
  19. https://en.wikipedia.org/wiki/differentiable_neural_computer
  20. https://arxiv.org/pdf/1611.00756.pdf
  21. https://arxiv.org/pdf/1611.01146.pdf
  22. https://en.wikipedia.org/wiki/hessian_matrix
  23. http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf
  24. http://www.offconvex.org/feed.xml
  25. http://disqus.com/?ref_noscript
  26. https://github.com/johnotander/pixyll

   hidden links:
  28. https://facebook.com/sharer.php?u=http://offconvex.github.io/2016/12/20/backprop/
  29. https://twitter.com/intent/tweet?text=back-propagation,%20an%20introduction&url=http://offconvex.github.io/2016/12/20/backprop/
  30. https://plus.google.com/share?url=http://offconvex.github.io/2016/12/20/backprop/
  31. http://www.linkedin.com/sharearticle?url=http://offconvex.github.io/2016/12/20/backprop/&title=back-propagation,%20an%20introduction
  32. http://reddit.com/submit?url=http://offconvex.github.io/2016/12/20/backprop/&title=back-propagation,%20an%20introduction
  33. https://news.ycombinator.com/submitlink?u=http://offconvex.github.io/2016/12/20/backprop/&t=back-propagation,%20an%20introduction
