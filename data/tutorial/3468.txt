   #[1]adit deshpande - cs undergrad at ucla ('19)

   [2][pic.jpg]

[3]adit deshpande

   cs undergrad at ucla ('19)

   [4]blog [5]about [6]github [7]projects [8]resume

deep learning research review week 1: generative adversarial nets

   [cover5th.png]

   starting this week, i   ll be doing a new series called deep learning
   research review. every couple weeks or so, i   ll be summarizing and
   explaining research papers in specific subfields of deep learning. this
   week i   ll begin with id3.

introduction

                  [9]according to yann lecun,    adversarial training is the
   coolest thing since sliced bread   . i   m inclined to believe so because i
   don   t think sliced bread ever created this much buzz and excitement
   within the deep learning community. in this post, we   ll be looking at 3
   papers that built on the pioneering [10]work of ian goodfellow in
   2014.

quick summary of gans

                  i briefly mentioned ian goodfellow   s generative
   adversarial network [11]paper in one of my prior blog posts, [12]9 deep
   learning papers you should know about.  the basic idea of these
   networks is that you have 2 models, a generative model and a
   discriminative model. the discriminative model has the task of
   determining whether a given image looks natural (an image from the
   dataset) or looks like it has been artificially created. the task of
   the generator is to create natural looking images that are similar to
   the original data distribution. this can be thought of as a zero-sum or
   minimax two player game. the analogy used in the paper is that the
   generative model is like    a team of counterfeiters, trying to produce
   and use fake currency    while the discriminative model is like    the
   police, trying to detect the counterfeit currency   . the generator is
   trying to fool the discriminator while the discriminator is trying to
   not get fooled by the generator. as the models train through
   alternating optimization, both methods are improved until a point where
   the    counterfeits are indistinguishable from the genuine articles   .

[13]laplacian pyramid of adversarial networks

   introduction

                  so, one of the most important uses of adversarial
   networks is the ability to create natural looking images after training
   the generator for a sufficient amount of time. these are some samples
   of what the generator outputted in goodfellow   s 2014 paper.
   [gan1.png]

   as you can see, the generator worked well with digits and faces, but it
   created very fuzzy and vague images when using the cifar-10 dataset.

   in order to fix this problem, emily denton, soumith chintala, arthur
   szlam, and rob fergus published the paper titled    deep generative image
   models using lapalacian pyramid of adversarial networks   . the main
   contribution of the paper is a type of network architecture that
   produces high-quality generated images that are mistaken for real
   images almost 40% of the time when assessed by human evaluators.

   approach

                  before getting into the paper, let   s think about the job
   of the generator in a gan. it has to produce a large, complex, and
   natural image that is good enough to convince a trained discriminator.
   not such an easy task in one shot. the way the authors combat this is
   by using multiple id98 models to sequentially generate images in
   increasing scales. as emily denton said in her [14]talk on lapgans,
   [gan2.png]

   the approach of this paper is to build a laplacian pyramid of
   generative models. for those that aren   t familiar, a laplacian pyramid
   is basically an image representation that consists of a series of
   filtered images at successively sparser densities ([15]more info for
   those interested). the idea is that each level of this pyramid
   representation contains information about the image at a particular
   scale. it is a sort of decomposition of the original image.

   let   s review what the inputs and outputs are of a simple gan. the
   generator takes in an input noise vector from a distribution and
   outputs an image. the discriminator takes in this image (or a real
   image from the training data) and outputs a scalar describing how
      real    the image is. now, let   s look at a conditional gan (cgan).
   everything remains the same, except that both the discriminator and the
   generator receive another piece of information as an input. this
   information is likely in the form of some sort of class label or
   another image.

   network architecture

                  the authors propose a set of convnet models and that
   each layer of the pyramid will have a convnet associated with it. the
   change is the traditional gan structure is that instead of having just
   one generator id98 that creates the whole image, we have a series of
   id98s that create the image sequentially by slowly increasing the
   resolution (aka going along the pyramid) and refining images in a
   coarse to fine fashion. each level has its own id98 and is trained on
   two components. one is a low resolution image and the other is a noise
   vector (which was the only input in traditional gans). this is where
   the idea of cgans come into play as there are multiple inputs. the
   output will be a generated image that is then upsampled and used as
   input to the next level of the pyramid. this method is effective
   because the generators in each level are able to utilize information
   from different resolutions in order to create more finely grained
   outputs in the successive layers.
   [gan3.png]

   [gan4.png]

[16]generative adversarial text to image synthesis

   introduction

                  this paper was released just this past june and looks
   into the task of converting text descriptions into images. for example,
   the input to the network could be    a flower with pink petals    and the
   output is a generated image that contains those characteristics. so
   this task involves two main components. one is utilizing forms
   of natural language processing to understand the input description and
   the other is a generative network that is able to output an accurate
   and natural image representation. one note that the authors make is
   that the task of going from text to image is actually a lot harder than
   that of going from image to text (remember karpathy   s [17]paper). this
   is because of the incredible amount of pixel configurations and because
   we can   t really decompose the task into just predicting the next word
   (the way that image to text works).

   approach

                  the approach the authors take is training a gan that is
   conditioned on text features created by a recurrent text encoder (won't
   go too much into this, but here's the [18]paper for those interested).
   both the generator and the discriminator use these features at points
   in their respective network architectures. this is what enables the gan
   to make the correlation between the input description and the generated
   image.

   network architecture

                  let   s look at the generator first. we have our noise
   vector z along with a text encoding as the inputs to the network.
   basically, this text encoding is a way of encapsulating information
   about the input description in a way that it can then be concatenated
   to the noise vector (see image below for a visualization). deconv
   layers are then used to transform the input vector into the synthetic
   image.

   the discriminator takes in an image, passes it through a series of conv
   layers (with batchnorm and leaky relus). when the spatial dimensions
   finally get to 4x4, the network performs a depth concatenation with
   that text encoding we were talking about earlier. after this, there are
   2 more conv layers and the output is (as always) a score for the
   realness of the image.
   [gan5.png]

   training

                  one of the interesting things about this model is the
   way that it has to be trained. if you think closely about the task at
   hand, the generator has to get two jobs right. one is that it has to
   generate natural and plausible looking images. the other is that the
   images must correlate to the given text description. the discriminator,
   thus, must also keep these two things into account, making sure that
      fake    or unnatural images are rejected as well as images that mismatch
   the text. in order to create these versatile models, the authors train
   with three types of data: {real image, right text}, {fake image, right
   text}, and {real image, wrong text}. with that last training data type,
   the discriminator must learn to reject mismatched images (even if they
   look very natural).
   [gan6.png]

[19]super resolution using gans

   introduction

                  as a testament to the type of rapid innovation that
   takes place in this field, the team at twitter cortex released this
   paper only a couple weeks ago. the model being proposed in this paper
   is a super-resolution generative adversarial network, or srgan (will we
   ever run out of these acronyms?). the main contribution is a brand new
   id168 (better than plain old mse) that enables the network to
   recover realistic textures and fine grained details from images that
   have been heavily downsampled.

   approach

                  let   s first take a look at this new perceptual loss
   function that was introduced. this id168 can be divided into
   two parts, the adversarial loss and the content loss. from a high
   level, the adversarial loss encourages images that look natural (look
   like they   re from the distribution) and the content loss makes sure
   that the new resolution image has similar features to the original low
   res image.

   network architecture

                  okay, now let   s get into the specifics. let   s start off
   with a high resolution version of a given image and then a lower
   resolution version. we want to train our generator so that given the
   low resolution image, we can have an output that   s as close to the high
   res version as possible. this output is called a super-resolved image.
   the discriminator will then be trained to distinguish between these
   images. same old same old, right? the generator network architecture
   uses a set of b residual blocks that contain relus and batchnorm and
   conv layers. once the low res image passes through those blocks, there
   are two deconv layers that enable the increase of the resolution. then,
   looking at the discriminator, we have eight convolutional layers that
   lead into a sigmoid activation function which outputs the probabilities
   of whether the image is real (high res) or artificial (super res).
   [gan7.png]

   id168

                  now let   s look at that new id168. it is actually
   a weighted sum of individual id168s. the first is called a
   content loss. basically, it is a euclidean distance loss between the
   feature maps (in a pretrained vgg network) of the new reconstructed
   image (output of the network) and the actual high res training image.
   from what i understand, the main goal is to ensure that the content of
   the two images are similar by looking at their respective feature
   activations after feeding them into a trained convnet (comment below if
   anyone has other ideas!). the other major id168 the authors
   defined is the adversarial loss. this one is similar to what you
   normally expect from gans. it encourages outputs that are similar to
   the original data distribution through negative log likelihood. a
   id173 loss caps off the trio of functions. with this novel
   id168, the generator makes sure to output larger res images
   that look natural and still retain a similar pixel space when compared
   to the low res version.
   [gan8.png]

   quick general side note: gans use a largely unsupervised training
   process (all you need to have is a dataset of real images, no labels or
   anything). this means that we can take advantage of a lot of the
   unstructured image data that is available today. after training, we can
   use the output or intermediate layers as feature extractors that can be
   used for other classifiers, which now won   t need as much training data
   to achieve good accuracy.

   paper that i couldn   t get to, but still insanely cool: [20]dcgans. the
   authors didn   t do anything crazy. they just trained a really really
   large convnet, but the trick is that they had the right hyperparameters
   to really make the training work (aka batchnorm, adam, leaky relus).

   gans that could change the fashion industry:
   [21]https://www.youtube.com/watch?v=9c4z6ysbgq0

   dueces.
   [22]sources

   [23]tweet

   written on september 30, 2016

   please enable javascript to view the [24]comments powered by disqus.

references

   visible links
   1. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
   2. https://adeshpande3.github.io/adeshpande3.github.io/
   3. https://adeshpande3.github.io/adeshpande3.github.io/
   4. https://adeshpande3.github.io/adeshpande3.github.io/
   5. https://adeshpande3.github.io/adeshpande3.github.io/about
   6. https://github.com/adeshpande3
   7. https://adeshpande3.github.io/adeshpande3.github.io/projects
   8. https://adeshpande3.github.io/adeshpande3.github.io/resume.pdf
   9. https://www.quora.com/what-are-some-recent-and-potentially-upcoming-breakthroughs-in-unsupervised-learning
  10. https://arxiv.org/pdf/1406.2661v1.pdf
  11. https://arxiv.org/pdf/1406.2661v1.pdf
  12. https://adeshpande3.github.io/adeshpande3.github.io/the-9-deep-learning-papers-you-need-to-know-about.html
  13. https://arxiv.org/pdf/1506.05751v1.pdf
  14. https://www.youtube.com/watch?v=jejk-ug_ebi
  15. http://www.cs.utah.edu/~arul/report/node12.html
  16. https://arxiv.org/pdf/1605.05396.pdf
  17. https://arxiv.org/pdf/1412.2306v2.pdf
  18. https://arxiv.org/pdf/1605.05395v1.pdf
  19. https://arxiv.org/pdf/1609.04802v1.pdf
  20. http://arxiv.org/pdf/1511.06434v2.pdf
  21. https://www.youtube.com/watch?v=9c4z6ysbgq0
  22. https://adeshpande3.github.io/assets/sources5.txt
  23. https://twitter.com/share
  24. http://disqus.com/?ref_noscript

   hidden links:
  26. mailto:adeshpande3@g.ucla.edu
  27. https://www.facebook.com/adit.deshpande.5
  28. https://github.com/adeshpande3
  29. https://instagram.com/thejugglinguy
  30. https://www.linkedin.com/in/aditdeshpande
  31. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
  32. https://www.twitter.com/aditdeshpande3
