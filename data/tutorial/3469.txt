   #[1]olof mogren

   [2]

olof mogren

   research scientist, phd, at rise ai.

   [3]blog [4]talks
   [5]publications [6]about

   [7]fork me on github

text

   [button input] (not implemented)_____

trends in id4

   2016-08-08
   history of machine translation systems. slide by christopher d.
   manning. history of machine translation systems. slide by christopher
   d. manning.

   the following blog post was written as an attempt to summarize some of
   the important things happening in the field of neural machine
   translation (id4). it was also a way of digesting a great tutorial on
   id4 given sunday, august 7 at acl in berlin by some of the best people
   in the field: christopher manning, minh-thang luong, and kyunghyun cho.

   retweet or comment this on twitter:



   machine translation has gone through a number of stages in the last
   decades. phrase based id151 (smt), the
   flavour used in systems such as google translate have seen little
   improvement over the last three years. instead, people have been
   turning their heads towards id4 (id4) systems,
   which after being introduced seriously in 2014 have seen many
   refinements. these systems are also known as sequence-to-sequence
   models or encoder-decoder networks, and were initially fairly simple
   neural network models made out of two recurrent parts. firstly, an
   encoder part taking an input sentence in the source language and
   computing an internal representation, and secondly the decoder part, a
   neural language model trained to be good at assigning a high
   id203 to a well-formed sentence in the target language, which can
   be used to generate sentences that sound very good. letting the
   language model be conditioned on the input turns it into a translation
   model. (see sequence to sequence learning with neural networks ilya
   sutskever, oriol vinyals, quoc v. le. nips 2014, [8]pdf, arxiv). these
   early id4 systems worked on word level, which means that a word is seen
   as a symbol, so the input to the encoder is a unique index for each
   unique word, and the decoder being constrained to pick words from this
   vocabulary.
   [bahdanau-etal-alignment.png] the attention mechanism produces an
   alignment between source sentence and target sentence. illustration
   from id4 by jointly learning to translate and
   align by dzmitry bahdanau, kyunghuyn cho, and yoshua bengio. iclr 2015.
   [9](pdf, arxiv).

   these models worked well and got some promising scores in evaluations,
   but they had some drawbacks. firstly, the longer the input sentence,
   the more difficult for the encoder to capture all important information
   in the internal fixed-size representation. secondly, they are practical
   only for use with a fairly limited vocabulary size.

   the former of the two problems were addressed in 2015, with the
   introduction of the id12, allowing for the decoder part to
      attend    to different parts of the input while generating the output.
   this is also used in multi-modal models for tasks such as image
   captioning where the attention mechanism allows the decoder to focus on
   different parts of the input image as it generates the output text.

   the latter of the two problems has previously been addressed by letting
   the id4 system output special <unk> tokens for words that are
   out-of-vocabulary (oov) and post-processing the output by replacing
   this with the correspondng word in the source sentence, or looking them
   up in a dictionary (see    addressing the rare word problem in neural
   machine translation    by minh-thang luong, ilya sutskever, quoc le,
   oriol vinyals, wojciech zaremba. acl 2015 [10]pdf, arxiv ). this can
   result in the translated word being in the wrong inflection, or (worse)
   the word might not be in the dictionary at all (e.g. misspelled words).
   better handling of oov terms has been the focus of some work during
   2016, and the topic of a couple of papers being presented during acl
   2016, taking place this week in berlin.

   in id4 of rare words with subword units by rico
   sennrich and barry haddow and alexandra birch from university of
   edinburg [11](pdf, aclweb.org), a system is proposed that work on
   subword units. words are segmented using the byte pair encoding (bpe)
   algorithm [12](read about this on wikipedia) into subword units of
   different length, and a vocabulary is built using frequent such units.
   the method internally creates embeddings for these subword units,
   something that has been criticized for making it lack the ability of
   relating words to each other. rico sennrich however argued during his
   talk that there is no reason why the word boundaries would be the best
   unit to have embedded. there are examples of composite words in one
   language that translates into a sequence of words in another language.
   the model is rather simple and elegant, and gets good id7 scores
   [13](read more on id7 on wikipedia) translating between german and
   english, as well as russian and english.
   [luong-hybrid-id4.png] hybrid word-character model for id4. from
   minh-thang luong's slides.

   a second paper, by junyoung chung, kyunghyun cho, and yoshua bengio
   from new york university and universit   de montr  al, titled a
   character-level decoder without explicit segmentation for neural
   machine translation [14](pdf, aclweb.org), presents a model that also
   uses a vocabulary generated with bpe (see above) in combination with a
      bi-scale recurrent neural network   , a recurrent network with gru cells
   on two different time-scales, giving it a sense of hierarchy. the
   authors conclude that using the bpe tokens in the encoding part, along
   with a pure character-based decoding part, produces the best
   translations. the evaluation is made on four different language pairs:
   english-german, english-russian, english-czech, and english-finnish.

   in achieving open vocabulary id4 with hybrid
   word-character models by minh-thang luong and christopher d. manning
   from stanford [15](pdf, aclweb.org), a model is presented that works as
   a normal word-based sequence-to-sequence model, as long as you feed it
   words in the vocabulary. when the model encounters an oov term, the
   system employs a second sequence model working on character level. this
   model computes a representation for any word that is expressible in the
   given set of characters, and experimental results show that the
   representations computed in this way share many of the properties of
   neural id27s computed on word-level [16](read more on word
   embeddings on wikipedia). the system shows large improvements in id7
   scores, especially when used with a small word vocabulary, on the task
   of translating between czech and english.

   these are some of the more interesting presentations that i look
   forward to during this year   s acl.

   olof mogren
   762759867551649792

   please enable javascript to view the [17]comments powered by disqus.

   olof mogren rise ai
   [18]linkedin  [19] twitter  [20] atom/rss feed

   [21]free web stats

references

   visible links
   1. http://mogren.one/feed.xml
   2. http://mogren.one/
   3. http://mogren.one/blog
   4. http://mogren.one/talks
   5. http://mogren.one/publications
   6. http://mogren.one/about
   7. https://github.com/olofmogren
   8. http://arxiv.org/abs/1409.3215
   9. https://arxiv.org/abs/1409.0473
  10. https://arxiv.org/abs/1410.8206
  11. http://aclweb.org/anthology/p/p16/p16-1162.pdf
  12. https://en.wikipedia.org/wiki/byte_pair_encoding
  13. https://en.wikipedia.org/wiki/id7
  14. http://aclweb.org/anthology/p/p16/p16-1160.pdf
  15. http://aclweb.org/anthology/p/p16/p16-1100.pdf
  16. https://en.wikipedia.org/wiki/word_embedding
  17. https://disqus.com/?ref_noscript
  18. https://www.linkedin.com/in/olof-mogren-5392b452
  19. https://twitter.com/olofmogren
  20. http://mogren.one/feed.xml
  21. http://statcounter.com/free-web-stats/

   hidden links:
  23. https://twitter.com/olofmogren/status/762759867551649792
