   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

recurrent neural networks for beginners

   [9]go to the profile of camron godbout
   [10]camron godbout (button) blockedunblock (button) followfollowing
   aug 12, 2016

   what are recurrent neural networks and how can you use them?

   in this post i discuss the basics of recurrent neural networks (id56s)
   which are deep learning models that are becoming increasingly popular.
   i don   t intend to get too heavily into the math and proofs behind why
   these work and am aiming for a more abstract understanding.

general recurrent neural network information

   recurrent neural networks were created in the 1980   s but have just been
   recently gaining popularity from advances to the networks designs and
   increased computational power from graphic processing units. they   re
   especially useful with sequential data because each neuron or unit can
   use its internal memory to maintain information about the previous
   input. this is great because in cases of language,    i had washed my
   house    is much more different than    i had my house washed   . this allows
   the network to gain a deeper understanding of the statement.

   this is important to note because reading through a sentence even as a
   human, you   re picking up the context of each word from the words before
   it.
   [1*mvfujopgbjeyx35lu6n3ew.png]
   a rolled up id56 from
   [11]http://colah.github.io/posts/2015-08-understanding-lstms/

   a id56 has loops in them that allow infromation to be carried across
   neurons while reading in input.
   [1*v2w4tcmtj2h1ce7i-dngpw.png]
   an unrolled id56

   in these diagrams x_t is some input, a is a part of the id56 and h_t is
   the output. essentially you can feed in words from the sentence or even
   characters from a string as x_t and through the id56 it will come up
   with a h_t.

   the goal is to use h_t as output and compare it to your test data
   (which is usually a small subset of the original data). you will then
   get your error rate. after comparing your output to your test data,
   with error rate in hand, you can use a technique called back
   propagation through time (bptt). bptt back checks through the network
   and adjusts the weights based on your error rate. this adjusts the
   network and makes it learn to do better.

   theoretically id56s can handle context from the beginning of the
   sentence which will allow more accurate predictions of a word at the
   end of a sentence. in practice this isn   t necessarily true for vanilla
   id56s. this is a major reason why id56s faded out from practice for a
   while until some great results were achieved with using a long short
   term memory(lstm) unit inside the neural network. adding the lstm to
   the network is like adding a memory unit that can remember context from
   the very beggining of the input.
   [1*k9g9eoeq9ca0jdommxkrqg.png]

   these little memory units allow for id56s to be much more accurate, and
   have been the recent cause of the popularity around this model. these
   memory units allow for the ability across inputs for context to be
   remembered. two of these units are widely used today lstms and gated
   recurrent units(gru), the latter of the two are more efficient
   computationally because they take up less computer memory.

applications of recurrent neural networks

   there are many different applications of id56s. a great application is
   in collaboration with natural language processing (nlp). id56s have been
   demonstrated by many people on the internet who created amazing models
   that can represent a language model. these language models can take
   input such as a large set of shakespeares poems, and after training
   these models they can generate their own shakespearean poems that are
   very hard to differentiate from originals!

   below is some shakespeare
pandarus:
alas, i think he shall be come approached and the day
when little srain would be attain'd into being never fed,
and who is but a chain and subjects of his death,
i should not sleep.
second senator:
they are away this miseries, produced upon my soul,
breaking and strongly should be buried, when i perish
the earth and thoughts of many states.
duke vincentio:
well, your wit is in the care of side and that.
second lord:
they would be ruled after this chamber, and
my fair nues begun out of the fact, to be conveyed,
whose noble souls i'll have the heart of the wars.
clown:
come, sir, i will make did behold your worship.
viola:
i'll drink it.

   this poem was actually written by an id56. this was from an awesome
   article here
   [12]http://karpathy.github.io/2015/05/21/id56-effectiveness/ that goes
   more indepth on char id56s.

   this particular type of id56s is fed in a dataset of text and reads the
   input in character by character. the amazing thing about these networks
   in comparison to feeding in a word at a time is that the network can
   create it   s own unique words that were not in the vocabulary you
   trained it on.
   [1*imalbwl6uj3nlqxixzyfva.jpeg]
   an example of a char id56

   this diagram taken from the article referenced above shows how the
   model would predict    hello   . this gives a good visualization of how
   these networks take in a word character by character and predict the
   likely hood of the next probable character.

   another amazing application of id56s is machine translation. this method
   is interesting because it involves training two id56s simultaneously. in
   these networks the inputs are pairs of sentences in different
   languages. for example you can feed the network an english sentence
   paired with its french translation. with enough training you can give
   the network an english sentence and it will translate it to french!
   this model is called a sequence 2 sequences model or encoder decoder
   model.
   [1*sfzpmlbelgeahxaqryrzeg.png]
   encoder decoder id56 for english to french translation

   this diagram shows how information flows through encoders decoder
   model. this diagram is using a id27 layer to get better word
   representation. a id27 layer is usally glove or id97
   algorithm that just takes a bunch of words and creates a weighted
   matrix that allows similar words to be correlated with each other.
   using an embedding layer genererally makes your id56 more accurate
   because it is a better representation of how similar words are so the
   net has less to infer.

conclusion

   recurrent neural networks have been becoming very popular as of
   recently and for a very good reason. they   re one of the most effective
   models out for natural language processing. new applications of these
   models are coming out all the time and its exciting to see what
   researchers come up with.

   to play around with some id56 check out these awesome libraries

   tensorflow         googles machine learning frameworks id56 example:
   [13]https://www.tensorflow.org/versions/r0.10/tutorials/recurrent/index
   .html

   keras         a high level machine learning package that runs on top of
   tensorflow or theano: [14]https://keras.io

   torch         facebook machine learning framework in lua: [15]http://torch.ch

   originally posted at [16]camron   s blog.

     * [17]machine learning
     * [18]artificial intelligence
     * [19]deep learning
     * [20]computer science
     * [21]programming

   (button)
   (button)
   (button) 1.2k claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [22]go to the profile of camron godbout

[23]camron godbout

     * (button)
       (button) 1.2k
     * (button)
     *
     *

   [24]go to the profile of camron godbout
   never miss a story from camron godbout, when you sign up for medium.
   [25]learn more
   never miss a story from camron godbout
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/7aca4e933b82
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@camrongodbout?source=post_header_lockup
  10. https://medium.com/@camrongodbout
  11. http://colah.github.io/posts/2015-08-understanding-lstms/
  12. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  13. https://www.tensorflow.org/versions/r0.10/tutorials/recurrent/index.html
  14. https://keras.io/
  15. http://torch.ch/
  16. http://camron.xyz/
  17. https://medium.com/tag/machine-learning?source=post
  18. https://medium.com/tag/artificial-intelligence?source=post
  19. https://medium.com/tag/deep-learning?source=post
  20. https://medium.com/tag/computer-science?source=post
  21. https://medium.com/tag/programming?source=post
  22. https://medium.com/@camrongodbout?source=footer_card
  23. https://medium.com/@camrongodbout
  24. https://medium.com/@camrongodbout
  25. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  27. https://medium.com/p/7aca4e933b82/share/twitter
  28. https://medium.com/p/7aca4e933b82/share/facebook
  29. https://medium.com/p/7aca4e933b82/share/twitter
  30. https://medium.com/p/7aca4e933b82/share/facebook
