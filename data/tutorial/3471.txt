   #[1]github [2]recent commits to neuraltalk2:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]270
     * [35]star [36]4,555
     * [37]fork [38]1,081

[39]karpathy/[40]neuraltalk2

   [41]code [42]issues 126 [43]pull requests 11 [44]projects 0
   [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   efficient image captioning code in torch, runs on gpu
     * [47]53 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors

    1. [51]jupyter notebook 64.3%
    2. [52]lua 30.5%
    3. [53]python 4.3%
    4. other 0.9%

   (button) jupyter notebook lua python other
   branch: master (button) new pull request
   [54]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/k
   [55]download zip

downloading...

   want to be notified of new releases in karpathy/neuraltalk2?
   [56]sign in [57]sign up

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [60]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [61]download the github extension for visual studio
   and try again.

   (button) go back
   [62]@karpathy
   [63]karpathy [64]update
   latest commit [65]bd8c9d8 sep 23, 2016
   [66]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [67]coco-caption [68]first commit nov 20, 2015
   [69]coco [70]first commit nov 20, 2015
   [71]cv [72]adding my crossvalidation scripts to give people idea about
   how i use    nov 20, 2015
   [73]misc [74]add support for .jpeg files dec 31, 2015
   [75]vis [76]tweak colors a bit dec 4, 2015
   [77].gitignore [78]first commit nov 20, 2015
   [79]readme.md
   [80]convert_checkpoint_gpu_to_cpu.lua
   [81]eval.lua
   [82]prepro.py
   [83]test_language_model.lua
   [84]train.lua
   [85]videocaptioning.lua [86]videocaptioning dec 25, 2015

readme.md

neuraltalk2

   update (september 22, 2016): the google brain team has [87]released the
   image captioning model of vinyals et al. (2015). the core model is very
   similar to neuraltalk2 (a id98 followed by id56), but the google release
   should work significantly better as a result of better id98, some
   tricks, and more careful engineering. find it under [88]im2txt repo in
   tensorflow. i'll leave this code base up for educational purposes and
   as a torch implementation.

   recurrent neural network captions your images. now much faster and
   better than the original [89]neuraltalk. compared to the original
   neuraltalk this implementation is batched, uses torch, runs on a gpu,
   and supports id98 finetuning. all of these together result in quite a
   large increase in training speed for the language model (~100x), but
   overall not as much because we also have to forward a vggnet. however,
   overall very good models can be trained in 2-3 days, and they show a
   much better performance.

   this is an early code release that works great but is slightly hastily
   released and probably requires some code reading of inline comments
   (which i tried to be quite good with in general). i will be improving
   it over time but wanted to push the code out there because i promised
   it to too many people.

   this current code (and the pretrained model) gets ~0.9 cider, which
   would place it around spot #8 on the [90]codalab leaderboard. i will
   submit the actual result soon.

   [91]teaser results

   you can find a few more example results on the [92]demo page. these
   results will improve a bit more once the last few bells and whistles
   are in place (e.g. id125, ensembling, reranking).

   there's also a [93]fun video by [94]@kcimc, where he runs a neuraltalk2
   pretrained model in real time on his laptop during a walk in amsterdam.

requirements

for evaluation only

   this code is written in lua and requires [95]torch. if you're on
   ubuntu, installing torch in your home directory may look something
   like:
$ curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps
| bash
$ git clone https://github.com/torch/distro.git ~/torch --recursive
$ cd ~/torch;
$ ./install.sh      # and enter "yes" at the end to modify your bashrc
$ source ~/.bashrc

   see the torch installation documentation for more details. after torch
   is installed we need to get a few more packages using [96]luarocks
   (which already came with the torch install). in particular:
$ luarocks install nn
$ luarocks install nngraph
$ luarocks install image

   we're also going to need the [97]cjson library so that we can load/save
   json files. follow their [98]download link and then look under their
   section 2.4 for easy luarocks install.

   if you'd like to run on an nvidia gpu using cuda (which you really,
   really want to if you're training a model, since we're using a vggnet),
   you'll of course need a gpu, and you will have to install the [99]cuda
   toolkit. then get the cutorch and cunn packages:
$ luarocks install cutorch
$ luarocks install cunn

   if you'd like to use the cudnn backend (the pretrained checkpoint
   does), you also have to install [100]cudnn. first follow the link to
   [101]nvidia website, register with them and download the cudnn library.
   then make sure you adjust your ld_library_path to point to the lib64
   folder that contains the library (e.g. libcudnn.so.7.0.64). then git
   clone the cudnn.torch repo, cd inside and do luarocks make
   cudnn-scm-1.rockspec to build the torch bindings.

for training

   if you'd like to train your models you will need [102]loadcaffe, since
   we are using the vggnet. first, make sure you follow their instructions
   to install protobuf and everything else (e.g. sudo apt-get install
   libprotobuf-dev protobuf-compiler), and then install via luarocks:
luarocks install loadcaffe

   finally, you will also need to install [103]torch-hdf5, and [104]h5py,
   since we will be using hdf5 files to store the preprocessed data.

   phew! quite a few dependencies, sorry no easy way around it :\

i just want to caption images

   in this case you want to run the evaluation script on a pretrained
   model checkpoint. i trained a decent one on the [105]ms coco dataset
   that you can run on your images. the pretrained checkpoint can be
   downloaded here: [106]pretrained checkpoint link (600mb). it's large
   because it contains the weights of a finetuned vggnet. now place all
   your images of interest into a folder, e.g. blah, and run the eval
   script:
$ th eval.lua -model /path/to/model -image_folder /path/to/image/directory -num_
images 10

   this tells the eval script to run up to 10 images from the given
   folder. if you have a big gpu you can speed up the evaluation by
   increasing batch_size (default = 1). use -num_images -1 to process all
   images. the eval script will create an vis.json file inside the vis
   folder, which can then be visualized with the provided html interface:
$ cd vis
$ python -m simplehttpserver

   now visit localhost:8000 in your browser and you should see your
   predicted captions.

   you can see an [107]example visualization demo page here.

   running in docker. if you'd like to avoid dependency nightmares,
   running the codebase from docker might be a good option. there is one
   (third-party) [108]docker repo here.

   "i only have cpu". okay, in that case download the [109]cpu model
   checkpoint. make sure you run the eval script with -gpuid -1 to tell
   the script to run on cpu. on my machine it takes a bit less than 1
   second per image to caption in cpu mode.

   id125. id125 is enabled by default because it increases the
   performance of the search for argmax decoding sequence. however, this
   is a little more expensive, so if you'd like to evaluate images faster,
   but at a cost of performance, use -beam_size 1. for example, in one of
   my experiments beam size 2 gives cider 0.922, and beam size 1 gives
   cider 0.886.

   running on mscoco images. if you train on mscoco (see how below), you
   will have generated preprocessed mscoco images, which you can use
   directly in the eval script. in this case simply leave out the
   image_folder option and the eval script and instead pass in the
   input_h5, input_json to your preprocessed files. this will make more
   sense once you read the section below :)

   running a live demo. with opencv 3 installed you can caption video
   stream from camera in real time. follow the instructions in
   [110]torch-opencv to install it and run videocaptioning.lua similar to
   eval.lua. note that only central crop will be captioned.

i'd like to train my own network on ms coco

   great, first we need to some preprocessing. head over to the coco/
   folder and run the ipython notebook to download the dataset and do some
   very simple preprocessing. the notebook will combine the train/val data
   together and create a very simple and small json file that contains a
   large list of image paths, and raw captions for each image, of the
   form:
[{ "file_path": "path/img.jpg", "captions": ["a caption", "a second caption of i
"tgit ...] }, ...]

   once we have this, we're ready to invoke the prepro.py script, which
   will read all of this in and create a dataset (an hdf5 file and a json
   file) ready for consumption in the lua code. for example, for ms coco
   we can run the prepro file as follows:
$ python prepro.py --input_json coco/coco_raw.json --num_val 5000 --num_test 500
0 --images_root coco/images --word_count_threshold 5 --output_json coco/cocotalk
.json --output_h5 coco/cocotalk.h5

   this is telling the script to read in all the data (the images and the
   captions), allocate 5000 images for val/test splits respectively, and
   map all words that occur <= 5 times to a special unk token. the
   resulting json and h5 files are about 30gb and contain everything we
   want to know about the dataset.

   warning: the prepro script will fail with the default mscoco data
   because one of their images is corrupted. see [111]this issue for the
   fix, it involves manually replacing one image in the dataset.

   the last thing we need is the [112]vgg-16 caffe checkpoint, (under
   models section, "16-layer model" bullet point). put the two files (the
   prototxt configuration file and the proto binary of weights) somewhere
   (e.g. a model directory), and we're ready to train!
$ th train.lua -input_h5 coco/cocotalk.h5 -input_json coco/cocotalk.json

   the train script will take over, and start dumping checkpoints into the
   folder specified by checkpoint_path (default = current folder). you
   also have to point the train script to the vggnet protos (see the
   options inside train.lua).

   if you'd like to evaluate id7/meteor/cider scores during training in
   addition to validation cross id178 loss, use -language_eval 1 option,
   but don't forget to download the [113]coco-caption code into
   coco-caption directory.

   a few notes on training. to give you an idea, with the default settings
   one epoch of ms coco images is about 7500 iterations. 1 epoch of
   training (with no finetuning - notice this is the default) takes about
   1 hour and results in validation loss ~2.7 and cider score of ~0.4. by
   iteration 70,000 cider climbs up to about 0.6 (validation loss at about
   2.5) and then will top out at a bit below 0.7 cider. after that
   additional improvements are only possible by turning on id98 finetuning.
   i like to do the training in stages, where i first train with no
   finetuning, and then restart the train script with -finetune_id98_after
   0 to start finetuning right away, and using -start_from flag to
   continue from the previous model checkpoint. you'll see your score rise
   up to about 0.9 cider over ~2 days or so (on ms coco).

i'd like to train on my own data

   no problem, create a json file in the exact same form as before,
   describing your jpg files:
[{ "file_path": "path/img.jpg", "captions": ["a caption", "a similar caption" ..
.] }, ...]

   and invoke the prepro.py script to preprocess all the images and data
   into and hdf5 file and json file. then invoke train.lua (see detailed
   options inside code).

i'd like to distribute my gpu trained checkpoints for cpu

   use the script convert_checkpoint_gpu_to_cpu.lua to convert your gpu
   checkpoints to be usable on cpu. see inline documentation for why this
   separate script is needed. for example:
th convert_checkpoint_gpu_to_cpu.lua gpu_checkpoint.t7

   write the file gpu_checkpoint.t7_cpu.t7, which you can now run with
   -gpuid -1 in the eval script.

license

   bsd license.

acknowledgements

   parts of this code were written in collaboration with my labmate
   [114]justin johnson.

   i'm very grateful for [115]nvidia's support in providing gpus that made
   this work possible.

   i'm also very grateful to the maintainers of torch for maintaining a
   wonderful deep learning library.

     *    2019 github, inc.
     * [116]terms
     * [117]privacy
     * [118]security
     * [119]status
     * [120]help

     * [121]contact github
     * [122]pricing
     * [123]api
     * [124]training
     * [125]blog
     * [126]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [127]reload to refresh your
   session. you signed out in another tab or window. [128]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/karpathy/neuraltalk2/commits/master.atom
   3. https://github.com/karpathy/neuraltalk2#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/karpathy/neuraltalk2
  32. https://github.com/join
  33. https://github.com/login?return_to=/karpathy/neuraltalk2
  34. https://github.com/karpathy/neuraltalk2/watchers
  35. https://github.com/login?return_to=/karpathy/neuraltalk2
  36. https://github.com/karpathy/neuraltalk2/stargazers
  37. https://github.com/login?return_to=/karpathy/neuraltalk2
  38. https://github.com/karpathy/neuraltalk2/network/members
  39. https://github.com/karpathy
  40. https://github.com/karpathy/neuraltalk2
  41. https://github.com/karpathy/neuraltalk2
  42. https://github.com/karpathy/neuraltalk2/issues
  43. https://github.com/karpathy/neuraltalk2/pulls
  44. https://github.com/karpathy/neuraltalk2/projects
  45. https://github.com/karpathy/neuraltalk2/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/karpathy/neuraltalk2/commits/master
  48. https://github.com/karpathy/neuraltalk2/branches
  49. https://github.com/karpathy/neuraltalk2/releases
  50. https://github.com/karpathy/neuraltalk2/graphs/contributors
  51. https://github.com/karpathy/neuraltalk2/search?l=jupyter-notebook
  52. https://github.com/karpathy/neuraltalk2/search?l=lua
  53. https://github.com/karpathy/neuraltalk2/search?l=python
  54. https://github.com/karpathy/neuraltalk2/find/master
  55. https://github.com/karpathy/neuraltalk2/archive/master.zip
  56. https://github.com/login?return_to=https://github.com/karpathy/neuraltalk2
  57. https://github.com/join?return_to=/karpathy/neuraltalk2
  58. https://desktop.github.com/
  59. https://desktop.github.com/
  60. https://developer.apple.com/xcode/
  61. https://visualstudio.github.com/
  62. https://github.com/karpathy
  63. https://github.com/karpathy/neuraltalk2/commits?author=karpathy
  64. https://github.com/karpathy/neuraltalk2/commit/bd8c9d879f957e1218a8f9e1f9b663ac70375866
  65. https://github.com/karpathy/neuraltalk2/commit/bd8c9d879f957e1218a8f9e1f9b663ac70375866
  66. https://github.com/karpathy/neuraltalk2/tree/bd8c9d879f957e1218a8f9e1f9b663ac70375866
  67. https://github.com/karpathy/neuraltalk2/tree/master/coco-caption
  68. https://github.com/karpathy/neuraltalk2/commit/a0d35c3bf57a2fd578831af406afd2c6bce69bdf
  69. https://github.com/karpathy/neuraltalk2/tree/master/coco
  70. https://github.com/karpathy/neuraltalk2/commit/a0d35c3bf57a2fd578831af406afd2c6bce69bdf
  71. https://github.com/karpathy/neuraltalk2/tree/master/cv
  72. https://github.com/karpathy/neuraltalk2/commit/7513852b11375f4df39cf3ac8f59fab5b79d8be2
  73. https://github.com/karpathy/neuraltalk2/tree/master/misc
  74. https://github.com/karpathy/neuraltalk2/commit/41f50947caea223d0e134cd496465d5e60ce7f34
  75. https://github.com/karpathy/neuraltalk2/tree/master/vis
  76. https://github.com/karpathy/neuraltalk2/commit/428ae6bcd146f51797d5329cc5daa4fd9f2b2ca0
  77. https://github.com/karpathy/neuraltalk2/blob/master/.gitignore
  78. https://github.com/karpathy/neuraltalk2/commit/a0d35c3bf57a2fd578831af406afd2c6bce69bdf
  79. https://github.com/karpathy/neuraltalk2/blob/master/readme.md
  80. https://github.com/karpathy/neuraltalk2/blob/master/convert_checkpoint_gpu_to_cpu.lua
  81. https://github.com/karpathy/neuraltalk2/blob/master/eval.lua
  82. https://github.com/karpathy/neuraltalk2/blob/master/prepro.py
  83. https://github.com/karpathy/neuraltalk2/blob/master/test_language_model.lua
  84. https://github.com/karpathy/neuraltalk2/blob/master/train.lua
  85. https://github.com/karpathy/neuraltalk2/blob/master/videocaptioning.lua
  86. https://github.com/karpathy/neuraltalk2/commit/fbc16f7d987be3ded8b6c9a5f8187f902ef1a993
  87. https://research.googleblog.com/2016/09/show-and-tell-image-captioning-open.html
  88. https://github.com/tensorflow/models/tree/master/im2txt/im2txt
  89. https://github.com/karpathy/neuraltalk
  90. https://competitions.codalab.org/competitions/3221#results
  91. https://camo.githubusercontent.com/684a313b08ebab8d1d0aec023e84ba59d57e8cdc/68747470733a2f2f7261772e6769746875622e636f6d2f6b617270617468792f6e657572616c74616c6b322f6d61737465722f7669732f7465617365722e6a706567
  92. http://cs.stanford.edu/people/karpathy/neuraltalk2/demo.html
  93. https://vimeo.com/146492001
  94. https://twitter.com/kcimc
  95. http://torch.ch/
  96. https://luarocks.org/
  97. http://www.kyne.com.au/~mark/software/lua-cjson-manual.html
  98. http://www.kyne.com.au/~mark/software/lua-cjson.php
  99. https://developer.nvidia.com/cuda-toolkit
 100. https://github.com/soumith/cudnn.torch
 101. https://developer.nvidia.com/cudnn
 102. https://github.com/szagoruyko/loadcaffe
 103. https://github.com/deepmind/torch-hdf5
 104. http://www.h5py.org/
 105. http://mscoco.org/
 106. http://cs.stanford.edu/people/karpathy/neuraltalk2/checkpoint_v1.zip
 107. http://cs.stanford.edu/people/karpathy/neuraltalk2/demo.html
 108. https://github.com/beeva-enriqueotero/docker-neuraltalk2
 109. http://cs.stanford.edu/people/karpathy/neuraltalk2/checkpoint_v1_cpu.zip
 110. https://github.com/visionlabs/torch-opencv/wiki/installation
 111. https://github.com/karpathy/neuraltalk2/issues/4
 112. http://www.robots.ox.ac.uk/~vgg/research/very_deep/
 113. https://github.com/tylin/coco-caption
 114. http://cs.stanford.edu/people/jcjohns/
 115. https://developer.nvidia.com/deep-learning
 116. https://github.com/site/terms
 117. https://github.com/site/privacy
 118. https://github.com/security
 119. https://githubstatus.com/
 120. https://help.github.com/
 121. https://github.com/contact
 122. https://github.com/pricing
 123. https://developer.github.com/
 124. https://training.github.com/
 125. https://github.blog/
 126. https://github.com/about
 127. https://github.com/karpathy/neuraltalk2
 128. https://github.com/karpathy/neuraltalk2

   hidden links:
 130. https://github.com/
 131. https://github.com/karpathy/neuraltalk2
 132. https://github.com/karpathy/neuraltalk2
 133. https://github.com/karpathy/neuraltalk2
 134. https://help.github.com/articles/which-remote-url-should-i-use
 135. https://github.com/karpathy/neuraltalk2#neuraltalk2
 136. https://github.com/karpathy/neuraltalk2#requirements
 137. https://github.com/karpathy/neuraltalk2#for-evaluation-only
 138. https://github.com/karpathy/neuraltalk2#for-training
 139. https://github.com/karpathy/neuraltalk2#i-just-want-to-caption-images
 140. https://github.com/karpathy/neuraltalk2#id-like-to-train-my-own-network-on-ms-coco
 141. https://github.com/karpathy/neuraltalk2#id-like-to-train-on-my-own-data
 142. https://github.com/karpathy/neuraltalk2#id-like-to-distribute-my-gpu-trained-checkpoints-for-cpu
 143. https://github.com/karpathy/neuraltalk2#license
 144. https://github.com/karpathy/neuraltalk2#acknowledgements
 145. https://github.com/
