   #[1]github [2]recent commits to char-id56:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]561
     * [35]star [36]9,228
     * [37]fork [38]2,248

[39]karpathy/[40]char-id56

   [41]code [42]issues 84 [43]pull requests 22 [44]projects 0 [45]wiki
   [46]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [47]sign up
   multi-layer recurrent neural networks (lstm, gru, id56) for
   character-level language models in torch
     * [48]88 commits
     * [49]1 branch
     * [50]0 releases
     * [51]18 contributors

    1. [52]lua 100.0%

   (button) lua
   branch: master (button) new pull request
   [53]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/k
   [54]download zip

downloading...

   want to be notified of new releases in karpathy/char-id56?
   [55]sign in [56]sign up

launching github desktop...

   if nothing happens, [57]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [59]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [60]download the github extension for visual studio
   and try again.

   (button) go back
   [61]@karpathy
   [62]karpathy [63]merge pull request [64]#164 [65]from gdb/master
   (button)    
fix readme tyop

   latest commit [66]6f9487a apr 30, 2016
   [67]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [68]data/tinyshakespeare [69]first commit may 21, 2015
   [70]model [71]changing default lstm initialization to use biases of 1.0
   for the for    sep 20, 2015
   [72]util [73]fix unclear errors oct 26, 2015
   [74].gitignore
   [75]readme.md
   [76]convert_gpu_cpu_checkpoint.lua
   [77]inspect_checkpoint.lua [78]add opencl to sample.lua and
   inspect_checkpoint.lua, add link to clto    jul 12, 2015
   [79]sample.lua
   [80]train.lua [81]fix message nov 24, 2015

readme.md

char-id56

   this code implements multi-layer recurrent neural network (id56, lstm,
   and gru) for training/sampling from character-level language models. in
   other words the model takes one text file as input and trains a
   recurrent neural network that learns to predict the next character in a
   sequence. the id56 can then be used to generate text character by
   character that will look like the original training data. the context
   of this code base is described in detail in my [82]blog post.

   if you are new to torch/lua/neural nets, it might be helpful to know
   that this code is really just a slightly more fancy version of this
   [83]100-line gist that i wrote in python/numpy. the code in this repo
   additionally: allows for multiple layers, uses an lstm instead of a
   vanilla id56, has more supporting code for model checkpointing, and is
   of course much more efficient since it uses mini-batches and can run on
   a gpu.

update: torch-id56

   [84]justin johnson (@jcjohnson) recently re-implemented char-id56 from
   scratch with a much nicer/smaller/cleaner/faster torch code base. it's
   under the name [85]torch-id56. it uses adam for optimization and
   hard-codes the id56/lstm forward/backward passes for space/time
   efficiency. this also avoids headaches with cloning models in this
   repo. in other words, torch-id56 should be the default char-id56
   implemention to use now instead of the one in this code base.

requirements

   this code is written in lua and requires [86]torch. if you're on
   ubuntu, installing torch in your home directory may look something
   like:
$ curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps
| bash
$ git clone https://github.com/torch/distro.git ~/torch --recursive
$ cd ~/torch;
$ ./install.sh      # and enter "yes" at the end to modify your bashrc
$ source ~/.bashrc

   see the torch installation documentation for more details. after torch
   is installed we need to get a few more packages using [87]luarocks
   (which already came with the torch install). in particular:
$ luarocks install nngraph
$ luarocks install optim
$ luarocks install nn

   if you'd like to train on an nvidia gpu using cuda (this can be to
   about 15x faster), you'll of course need the gpu, and you will have to
   install the [88]cuda toolkit. then get the cutorch and cunn packages:
$ luarocks install cutorch
$ luarocks install cunn

   if you'd like to use opencl gpu instead (e.g. ati cards), you will
   instead need to install the cltorch and clnn packages, and then use the
   option -opencl 1 during training ([89]cltorch issues):
$ luarocks install cltorch
$ luarocks install clnn

usage

data

   all input data is stored inside the data/ directory. you'll notice that
   there is an example dataset included in the repo (in folder
   data/tinyshakespeare) which consists of a subset of works of
   shakespeare. i'm providing a few more datasets on [90]this page.

   your own data: if you'd like to use your own data then create a single
   file input.txt and place it into a folder in the data/ directory. for
   example, data/some_folder/input.txt. the first time you run the
   training script it will do some preprocessing and write two more
   convenience cache files into data/some_folder.

   dataset sizes: note that if your data is too small (1mb is already
   considered very small) the id56 won't learn very effectively. remember
   that it has to learn everything completely from scratch. conversely if
   your data is large (more than about 2mb), feel confident to increase
   id56_size and train a bigger model (see details of training below). it
   will work significantly better. for example with 6mb you can easily go
   up to id56_size 300 or even more. the biggest that fits on my gpu and
   that i've trained with this code is id56_size 700 with num_layers 3 (2
   is default).

training

   start training the model using train.lua. as a sanity check, to run on
   the included example dataset simply try:
$ th train.lua -gpuid -1

   notice that here we are setting the flag gpuid to -1, which tells the
   code to train using cpu, otherwise it defaults to gpu 0. there are many
   other flags for various options. consult $ th train.lua -help for
   comprehensive settings. here's another example that trains a bigger
   network and also shows how you can run on your own custom dataset (this
   already assumes that data/some_folder/input.txt exists):
$ th train.lua -data_dir data/some_folder -id56_size 512 -num_layers 2 -dropout 0
.5

   checkpoints. while the model is training it will periodically write
   checkpoint files to the cv folder. the frequency with which these
   checkpoints are written is controlled with number of iterations, as
   specified with the eval_val_every option (e.g. if this is 1 then a
   checkpoint is written every iteration). the filename of these
   checkpoints contains a very important number: the loss. for example, a
   checkpoint with filename lm_lstm_epoch0.95_2.0681.t7 indicates that at
   this point the model was on epoch 0.95 (i.e. it has almost done one
   full pass over the training data), and the loss on validation data was
   2.0681. this number is very important because the lower it is, the
   better the checkpoint works. once you start to generate data (discussed
   below), you will want to use the model checkpoint that reports the
   lowest validation loss. notice that this might not necessarily be the
   last checkpoint at the end of training (due to possible overfitting).

   another important quantities to be aware of are batch_size (call it b),
   seq_length (call it s), and the train_frac and val_frac settings. the
   batch size specifies how many streams of data are processed in parallel
   at one time. the sequence length specifies the length of each stream,
   which is also the limit at which the gradients can propagate backwards
   in time. for example, if seq_length is 20, then the gradient signal
   will never backpropagate more than 20 time steps, and the model might
   not find dependencies longer than this length in number of characters.
   thus, if you have a very difficult dataset where there are a lot of
   long-term dependencies you will want to increase this setting. now, if
   at runtime your input text file has n characters, these first all get
   split into chunks of size bxs. these chunks then get allocated across
   three splits: train/val/test according to the frac settings. by default
   train_frac is 0.95 and val_frac is 0.05, which means that 95% of our
   data chunks will be trained on and 5% of the chunks will be used to
   estimate the validation loss (and hence the generalization). if your
   data is small, it's possible that with the default settings you'll only
   have very few chunks in total (for example 100). this is bad: in these
   cases you may want to decrease batch size or sequence length.

   note that you can also initialize parameters from a previously saved
   checkpoint using init_from.

sampling

   given a checkpoint file (such as those written to cv) we can generate
   new text. for example:
$ th sample.lua cv/some_checkpoint.t7 -gpuid -1

   make sure that if your checkpoint was trained with gpu it is also
   sampled from with gpu, or vice versa. otherwise the code will
   (currently) complain. as with the train script, see $ th sample.lua
   -help for full options. one important one is (for example) -length
   10000 which would generate 10,000 characters (default = 2000).

   temperature. an important parameter you may want to play with is
   -temperature, which takes a number in range (0, 1] (0 not included),
   default = 1. the temperature is dividing the predicted log
   probabilities before the softmax, so lower temperature will cause the
   model to make more likely, but also more boring and conservative
   predictions. higher temperatures cause the model to take more chances
   and increase diversity of results, but at a cost of more mistakes.

   priming. it's also possible to prime the model with some starting text
   using -primetext. this starts out the id56 with some hardcoded
   characters to warm it up with some context before it starts generating
   text. e.g. a fun primetext might be -primetext "the meaning of life is
   ".

   training with gpu but sampling on cpu. right now the solution is to use
   the convert_gpu_cpu_checkpoint.lua script to convert your gpu
   checkpoint to a cpu checkpoint. in near future you will not have to do
   this explicitly. e.g.:
$ th convert_gpu_cpu_checkpoint.lua cv/lm_lstm_epoch30.00_1.3950.t7

   will create a new file cv/lm_lstm_epoch30.00_1.3950.t7_cpu.t7 that you
   can use with the sample script and with -gpuid -1 for cpu mode.

   happy sampling!

tips and tricks

monitoring validation loss vs. training loss

   if you're somewhat new to machine learning or neural networks it can
   take a bit of expertise to get good models. the most important quantity
   to keep track of is the difference between your training loss (printed
   during training) and the validation loss (printed once in a while when
   the id56 is run on the validation data (by default every 1000
   iterations)). in particular:
     * if your training loss is much lower than validation loss then this
       means the network might be overfitting. solutions to this are to
       decrease your network size, or to increase dropout. for example you
       could try dropout of 0.5 and so on.
     * if your training/validation loss are about equal then your model is
       underfitting. increase the size of your model (either number of
       layers or the raw number of neurons per layer)

approximate number of parameters

   the two most important parameters that control the model are id56_size
   and num_layers. i would advise that you always use num_layers of either
   2/3. the id56_size can be adjusted based on how much data you have. the
   two important quantities to keep track of here are:
     * the number of parameters in your model. this is printed when you
       start training.
     * the size of your dataset. 1mb file is approximately 1 million
       characters.

   these two should be about the same order of magnitude. it's a little
   tricky to tell. here are some examples:
     * i have a 100mb dataset and i'm using the default parameter settings
       (which currently print 150k parameters). my data size is
       significantly larger (100 mil >> 0.15 mil), so i expect to heavily
       underfit. i am thinking i can comfortably afford to make id56_size
       larger.
     * i have a 10mb dataset and running a 10 million parameter model. i'm
       slightly nervous and i'm carefully monitoring my validation loss.
       if it's larger than my training loss then i may want to try to
       increase dropout a bit and see if that heps the validation loss.

best models strategy

   the winning strategy to obtaining very good models (if you have the
   compute time) is to always err on making the network larger (as large
   as you're willing to wait for it to compute) and then try different
   dropout values (between 0,1). whatever model has the best validation
   performance (the loss, written in the checkpoint filename, low is good)
   is the one you should use in the end.

   it is very common in deep learning to run many different models with
   many different hyperparameter settings, and in the end take whatever
   checkpoint gave the best validation performance.

   by the way, the size of your training and validation splits are also
   parameters. make sure you have a decent amount of data in your
   validation set or otherwise the validation performance will be noisy
   and not very informative.

additional pointers and acknowledgements

   this code was originally based on oxford university machine learning
   class [91]practical 6, which is in turn based on [92]learning to
   execute code from wojciech zaremba. chunks of it were also developed in
   collaboration with my labmate [93]justin johnson.

   to learn more about id56 language models i recommend looking at:
     * [94]my recent talk on char-id56
     * [95]generating sequences with recurrent neural networks by alex
       graves
     * [96]generating text with recurrent neural networks by ilya
       sutskever
     * [97]tomas mikolov's thesis

license

   mit

     *    2019 github, inc.
     * [98]terms
     * [99]privacy
     * [100]security
     * [101]status
     * [102]help

     * [103]contact github
     * [104]pricing
     * [105]api
     * [106]training
     * [107]blog
     * [108]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [109]reload to refresh your
   session. you signed out in another tab or window. [110]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/karpathy/char-id56/commits/master.atom
   3. https://github.com/karpathy/char-id56#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/karpathy/char-id56
  32. https://github.com/join
  33. https://github.com/login?return_to=/karpathy/char-id56
  34. https://github.com/karpathy/char-id56/watchers
  35. https://github.com/login?return_to=/karpathy/char-id56
  36. https://github.com/karpathy/char-id56/stargazers
  37. https://github.com/login?return_to=/karpathy/char-id56
  38. https://github.com/karpathy/char-id56/network/members
  39. https://github.com/karpathy
  40. https://github.com/karpathy/char-id56
  41. https://github.com/karpathy/char-id56
  42. https://github.com/karpathy/char-id56/issues
  43. https://github.com/karpathy/char-id56/pulls
  44. https://github.com/karpathy/char-id56/projects
  45. https://github.com/karpathy/char-id56/wiki
  46. https://github.com/karpathy/char-id56/pulse
  47. https://github.com/join?source=prompt-code
  48. https://github.com/karpathy/char-id56/commits/master
  49. https://github.com/karpathy/char-id56/branches
  50. https://github.com/karpathy/char-id56/releases
  51. https://github.com/karpathy/char-id56/graphs/contributors
  52. https://github.com/karpathy/char-id56/search?l=lua
  53. https://github.com/karpathy/char-id56/find/master
  54. https://github.com/karpathy/char-id56/archive/master.zip
  55. https://github.com/login?return_to=https://github.com/karpathy/char-id56
  56. https://github.com/join?return_to=/karpathy/char-id56
  57. https://desktop.github.com/
  58. https://desktop.github.com/
  59. https://developer.apple.com/xcode/
  60. https://visualstudio.github.com/
  61. https://github.com/karpathy
  62. https://github.com/karpathy/char-id56/commits?author=karpathy
  63. https://github.com/karpathy/char-id56/commit/6f9487a6fe5b420b7ca9afb0d7c078e37c1d1b4e
  64. https://github.com/karpathy/char-id56/pull/164
  65. https://github.com/karpathy/char-id56/commit/6f9487a6fe5b420b7ca9afb0d7c078e37c1d1b4e
  66. https://github.com/karpathy/char-id56/commit/6f9487a6fe5b420b7ca9afb0d7c078e37c1d1b4e
  67. https://github.com/karpathy/char-id56/tree/6f9487a6fe5b420b7ca9afb0d7c078e37c1d1b4e
  68. https://github.com/karpathy/char-id56/tree/master/data/tinyshakespeare
  69. https://github.com/karpathy/char-id56/commit/370cbcd448eb7daf32f21a6be560b70e0b33c4e3
  70. https://github.com/karpathy/char-id56/tree/master/model
  71. https://github.com/karpathy/char-id56/commit/0dfeaa454e687dd0278f036552ea1e48a0a408c9
  72. https://github.com/karpathy/char-id56/tree/master/util
  73. https://github.com/karpathy/char-id56/commit/b3c7e3c21538366669fd7fa8b15d48bf5843a271
  74. https://github.com/karpathy/char-id56/blob/master/.gitignore
  75. https://github.com/karpathy/char-id56/blob/master/readme.md
  76. https://github.com/karpathy/char-id56/blob/master/convert_gpu_cpu_checkpoint.lua
  77. https://github.com/karpathy/char-id56/blob/master/inspect_checkpoint.lua
  78. https://github.com/karpathy/char-id56/commit/b7f09bd4d376f254e1987d0a7e67d551030cade7
  79. https://github.com/karpathy/char-id56/blob/master/sample.lua
  80. https://github.com/karpathy/char-id56/blob/master/train.lua
  81. https://github.com/karpathy/char-id56/commit/f7a8e743a337d13a6e94708f7019d9b6acd3c127
  82. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  83. https://gist.github.com/karpathy/d4dee566867f8291f086
  84. http://cs.stanford.edu/people/jcjohns/
  85. https://github.com/jcjohnson/torch-id56
  86. http://torch.ch/
  87. https://luarocks.org/
  88. https://developer.nvidia.com/cuda-toolkit
  89. https://github.com/hughperkins/cltorch/issues
  90. http://cs.stanford.edu/people/karpathy/char-id56/
  91. https://github.com/oxford-cs-ml-2015/practical6
  92. https://github.com/wojciechz/learning_to_execute
  93. http://cs.stanford.edu/people/jcjohns/
  94. https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks
  95. http://arxiv.org/abs/1308.0850
  96. http://www.cs.utoronto.ca/~ilya/pubs/2011/lang-id56.pdf
  97. http://www.fit.vutbr.cz/~imikolov/id56lm/thesis.pdf
  98. https://github.com/site/terms
  99. https://github.com/site/privacy
 100. https://github.com/security
 101. https://githubstatus.com/
 102. https://help.github.com/
 103. https://github.com/contact
 104. https://github.com/pricing
 105. https://developer.github.com/
 106. https://training.github.com/
 107. https://github.blog/
 108. https://github.com/about
 109. https://github.com/karpathy/char-id56
 110. https://github.com/karpathy/char-id56

   hidden links:
 112. https://github.com/
 113. https://github.com/karpathy/char-id56
 114. https://github.com/karpathy/char-id56
 115. https://github.com/karpathy/char-id56
 116. https://help.github.com/articles/which-remote-url-should-i-use
 117. https://github.com/karpathy/char-id56#char-id56
 118. https://github.com/karpathy/char-id56#update-torch-id56
 119. https://github.com/karpathy/char-id56#requirements
 120. https://github.com/karpathy/char-id56#usage
 121. https://github.com/karpathy/char-id56#data
 122. https://github.com/karpathy/char-id56#training
 123. https://github.com/karpathy/char-id56#sampling
 124. https://github.com/karpathy/char-id56#tips-and-tricks
 125. https://github.com/karpathy/char-id56#monitoring-validation-loss-vs-training-loss
 126. https://github.com/karpathy/char-id56#approximate-number-of-parameters
 127. https://github.com/karpathy/char-id56#best-models-strategy
 128. https://github.com/karpathy/char-id56#additional-pointers-and-acknowledgements
 129. https://github.com/karpathy/char-id56#license
 130. https://github.com/
