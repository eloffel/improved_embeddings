   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    understanding and coding neural networks from
   scratch in python and r comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]deep learning [94]understanding and coding neural networks
   from scratch in python and r

   [95]deep learning[96]python[97]r

understanding and coding neural networks from scratch in python and r

   [98]sunil ray, may 29, 2017

introduction

   you can learn and practice a concept in two ways:
     * option 1: you can learn the entire theory on a particular subject
       and then look for ways to apply those concepts. so, you read up how
       an entire algorithm works, the maths behind it, its assumptions,
       limitations and then you apply it. robust but time taking approach.
     * option 2: start with simple basics and develop an intuition on the
       subject. next, pick a problem and start solving it. learn the
       concepts while you are solving the problem. keep tweaking and
       improving your understanding. so, you read up how to apply an
       algorithm     go out and apply it. once you know how to apply it, try
       it around with different parameters, values, limits and develop an
       understanding of the algorithm.

   i prefer option 2 and take that approach to learning any new topic. i
   might not be able to tell you the entire math behind an algorithm, but
   i can tell you the intuition. i can tell you the best scenarios to
   apply an algorithm based on my experiments and understanding.

   in my interactions with people, i find that people don   t take time to
   develop this intuition and hence they struggle to apply things in the
   right manner.

   in this article, i will discuss the building block of a neural network
   from scratch and focus more on developing this intuition to apply
   neural networks. we will code in both    python    and    r   . by end of this
   article, you will understand how neural networks work, how do we
   initialize weigths and how do we update them using back-propagation.

   let   s start.


table of contents:

    1. simple intuition behind neural networks
    2. multi layer id88 and its basics
    3. steps involved in neural network methodology
    4. visualizing steps for neural network working methodology
    5. implementing nn using numpy (python)
    6. implementing nn using r
    7. [optional] mathematical perspective of back propagation algorithm


simple intuition behind neural networks

   if you have been a developer or seen one work     you know how it is to
   search for bugs in a code. you would fire various test cases by varying
   the inputs or circumstances and look for the output. the change in
   output provides you a hint on where to look for the bug     which module
   to check, which lines to read. once you find it, you make the changes
   and the exercise continues until you have the right code / application.

   neural networks work in very similar manner. it takes several input,
   processes it through multiple neurons from multiple hidden layers and
   returns the result using an output layer. this result estimation
   process is technically known as    forward propagation   .

   next, we compare the result with actual output. the task is to make the
   output to neural network as close to actual (desired) output. each
   of these neurons are contributing some error to final output. how do
   you reduce the error?

   we try to minimize the value/ weight of neurons those are contributing
   more to the error and this happens while traveling back to the neurons
   of the neural network and finding where the error lies. this process is
   known as    backward propagation   .

   in order to reduce these number of iterations to minimize the error,
   the neural networks use a common algorithm known as    id119   ,
   which helps to optimize the task quickly and efficiently.

   that   s it     this is how neural network works! i know this is a very
   simple representation, but it would help you understand things in a
   simple manner.

multi layer id88 and its basics

   just like atoms form the basics of any material on earth     the basic
   forming unit of a neural network is a id88. so, what is a
   id88?

   a id88 can be understood as anything that takes multiple inputs
   and produces one output. for example, look at the image below.

   id88

   the above structure takes three inputs and produces one output. the
   next logical question is what is the relationship between input and
   output? let us start with basic ways and build on to find more complex
   ways.

   below, i have discussed three ways of creating input output
   relationships:
    1. by directly combining the input and computing the output based on a
       threshold value. for eg: take x1=0, x2=1, x3=1 and setting a
       threshold =0. so, if x1+x2+x3>0, the output is 1 otherwise 0. you
       can see that in this case, the id88 calculates the output as
       1.
    2. next, let us add weights to the inputs. weights give importance to
       an input. for example, you assign w1=2, w2=3 and w3=4 to x1, x2 and
       x3 respectively. to compute the output, we will multiply input with
       respective weights and compare with threshold value as w1*x1 +
       w2*x2 + w3*x3 > threshold. these weights assign more importance to
       x3 in comparison to x1 and x2.
    3. next, let us add bias: each id88 also has a bias which can be
       thought of as how much flexible the id88 is. it is somehow
       similar to the constant b of a linear function y = ax + b. it
       allows us to move the line up and down to fit the prediction with
       the data better. without b the line will always goes through the
       origin (0, 0) and you may get a poorer fit. for example, a
       id88 may have two inputs, in that case, it requires three
       weights. one for each input and one for the bias. now linear
       representation of input will look like, w1*x1 + w2*x2 + w3*x3 +
       1*b.

   but, all of this is still linear which is what id88s used to be.
   but that was not as much fun. so, people thought of evolving a
   id88 to what is now called as artificial neuron. a neuron applies
   non-linear transformations (activation function) to the inputs and
   biases.


what is an activation function?

   activation function takes the sum of weighted input (w1*x1 + w2*x2 +
   w3*x3 + 1*b) as an argument and return the output of the neuron.  in
   above equation, we have represented 1 as x0 and b as w0.

   the activation function is mostly used to make a non-linear
   transformation which allows us to fit nonlinear hypotheses or to
   estimate the complex functions. there are multiple activation
   functions, like:    sigmoid   ,    tanh   , relu and many other.

forward propagation, back propagation and epochs

   till now, we have computed the output and this process is known as
      forward propagation   . but what if the estimated output is far away
   from the actual output (high error). in the neural network what we do,
   we update the biases and weights based on the error. this weight and
   bias updating process is known as    back propagation   .

   back-propagation (bp) algorithms work by determining the loss (or
   error) at the output and then propagating it back into the network. the
   weights are updated to minimize the error resulting from each neuron.
   the first step in minimizing the error is to determine the gradient
   (derivatives) of each node w.r.t. the final output. to get a
   mathematical perspective of the backward propagation, refer below
   section.

   this one round of forward and back propagation iteration is known as
   one training iteration aka    epoch   .

multi-layer id88

   now, let   s move on to next part of multi-layer id88. so far, we
   have seen just a single layer consisting of 3 input nodes i.e x1, x2
   and x3 and an output layer consisting of a single neuron. but, for
   practical purposes, the single-layer network can do only so much. an
   mlp consists of multiple layers called hidden layers stacked in between
   the input layer and the output layer as shown below.

   the image above shows just a single hidden layer in green but in
   practice can contain multiple hidden layers. another point to remember
   in case of an mlp is that all the layers are fully connected i.e every
   node in a layer(except the input and the output layer) is connected to
   every node in the previous layer and the following layer.

   let   s move on to the next topic which is training algorithm for a
   neural network (to minimize the error). here, we will look at most
   common training algorithm known as [99]id119.


full batch id119 and stochastic id119

   both variants of id119 perform the same work of updating the
   weights of the mlp by using the same updating algorithm but the
   difference lies in the number of training samples used to update the
   weights and biases.

   full batch id119 algorithm as the name implies uses all the
   training data points to update each of the weights once whereas
   stochastic gradient uses 1 or more(sample) but never the entire
   training data to update the weights once.

   let us understand this with a simple example of a dataset of 10 data
   points with two weights w1 and w2.

   full batch: you use 10 data points (entire training data) and calculate
   the change in w1 (  w1) and change in w2(  w2) and update w1 and w2.

   sgd: you use 1st data point and calculate the change in w1 (  w1) and
   change in w2(  w2) and update w1 and w2. next, when you use 2nd data
   point, you will work on the updated weights

   for a more in-depth explanation of both the methods, you can have a
   look at [100]this article.


steps involved in neural network methodology

   let   s look at the step by step building methodology of neural network
   (mlp with one hidden layer, similar to above-shown architecture). at
   the output layer, we have only one neuron as we are solving a binary
   classification problem (predict 0 or 1). we could also have two neurons
   for predicting each of both classes.

   first look at the broad steps:

   0.) we take input and output
     * x as an input matrix
     * y as an output matrix

   1.) we initialize weights and biases with random values (this is one
   time initiation. in the next iteration, we will use updated weights,
   and biases). let us define:
     * wh as weight matrix to the hidden layer
     * bh as bias matrix to the hidden layer
     * wout as weight matrix to the output layer
     * bout as bias matrix to the output layer

   2.) we take matrix dot product of input and weights assigned to edges
   between the input and hidden layer then add biases of the hidden layer
   neurons to respective inputs, this is known as linear transformation:

   hidden_layer_input= matrix_dot_product(x,wh) + bh

   3) perform non-linear transformation using an activation function
   (sigmoid). sigmoid will return the output as 1/(1 + exp(-x)).

   hiddenlayer_activations = sigmoid(hidden_layer_input)

   4.) perform a linear transformation on hidden layer activation (take
   matrix dot product with weights and add a bias of the output layer
   neuron) then apply an activation function (again used sigmoid, but you
   can use any other activation function depending upon your task) to
   predict the output

   output_layer_input = matrix_dot_product (hiddenlayer_activations * wout
   ) + bout
   output = sigmoid(output_layer_input)

   all above steps are known as    forward propagation   

   5.) compare prediction with actual output and calculate the gradient of
   error (actual     predicted). error is the mean square loss = ((y-t)^2)/2

   e = y     output

   6.) compute the slope/ gradient of hidden and output layer neurons ( to
   compute the slope, we calculate the derivatives of non-linear
   activations x at each layer for each neuron). gradient of sigmoid can
   be returned as x * (1     x).

   slope_output_layer = derivatives_sigmoid(output)
   slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)

   7.) compute change factor(delta) at output layer, dependent on the
   gradient of error multiplied by the slope of output layer activation

   d_output = e * slope_output_layer

   8.) at this step, the error will propagate back into the network which
   means error at hidden layer. for this, we will take the dot product of
   output layer delta with weight parameters of edges between the hidden
   and output layer (wout.t).

   error_at_hidden_layer = matrix_dot_product(d_output, wout.transpose)

   9.) compute change factor(delta) at hidden layer, multiply the error at
   hidden layer with slope of hidden layer activation

   d_hiddenlayer = error_at_hidden_layer * slope_hidden_layer

   10.) update weights at the output and hidden layer: the weights in the
   network can be updated from the errors calculated for training
   example(s).

   wout = wout + matrix_dot_product(hiddenlayer_activations.transpose,
   d_output)*learning_rate
   wh =  wh + matrix_dot_product(x.transpose,d_hiddenlayer)*learning_rate

   learning_rate: the amount that weights are updated is controlled by a
   configuration parameter called the learning rate)

   11.) update biases at the output and hidden layer: the biases in the
   network can be updated from the aggregated errors at that neuron.
     * bias at output_layer =bias at output_layer + sum of delta of
       output_layer at row-wise * learning_rate
     * bias at hidden_layer =bias at hidden_layer + sum of delta of
       output_layer at row-wise * learning_rate

   bh = bh + sum(d_hiddenlayer, axis=0) * learning_rate
   bout = bout + sum(d_output, axis=0)*learning_rate

   steps from 5 to 11 are known as    backward propagation   

   one forward and backward propagation iteration is considered as one
   training cycle. as i mentioned earlier, when do we train second time
   then update weights and biases are used for forward propagation.

   above, we have updated the weight and biases for hidden and output
   layer and we have used full batch id119 algorithm.


visualization of steps for neural network methodology

   we will repeat the above steps and visualize the input, weights,
   biases, output, error matrix to understand working methodology of
   neural network (mlp).

   note:
     * for good visualization images, i have rounded decimal positions at
       2 or3 positions.
     * yellow filled cells represent current active cell
     * orange cell represents the input used to populate values of current
       cell

   step 0: read input and output

   step 1: initialize weights and biases with random values (there are
   methods to initialize weights and biases but for now initialize with
   random values)

   step 2: calculate hidden layer input:
   hidden_layer_input= matrix_dot_product(x,wh) + bh

   step 3: perform non-linear transformation on hidden linear input
   hiddenlayer_activations = sigmoid(hidden_layer_input)

   step 4: perform linear and non-linear transformation of hidden layer
   activation at output layer

   output_layer_input = matrix_dot_product (hiddenlayer_activations * wout
   ) + bout
   output = sigmoid(output_layer_input)

   step 5: calculate gradient of error(e) at output layer
   e = y-output

   step 6: compute slope at output and hidden layer
   slope_output_layer= derivatives_sigmoid(output)
   slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)

   step 7: compute delta at output layer

   d_output = e * slope_output_layer*lr

   step 8: calculate error at hidden layer

   error_at_hidden_layer = matrix_dot_product(d_output, wout.transpose)

   step 9: compute delta at hidden layer

   d_hiddenlayer = error_at_hidden_layer * slope_hidden_layer

   step 10: update weight at both output and hidden layer

   wout = wout + matrix_dot_product(hiddenlayer_activations.transpose,
   d_output)*learning_rate
   wh =  wh+ matrix_dot_product(x.transpose,d_hiddenlayer)*learning_rate

   step 11: update biases at both output and hidden layer

   bh = bh + sum(d_hiddenlayer, axis=0) * learning_rate
   bout = bout + sum(d_output, axis=0)*learning_rate

   above, you can see that there is still a good error not close to actual
   target value because we have completed only one training iteration. if
   we will train model multiple times then it will be a very close actual
   outcome. i have completed thousands iteration and my result is close to
   actual target values ([[ 0.98032096] [ 0.96845624] [ 0.04532167]]).


implementing nn using numpy (python)

   import numpy as np

   #input array
   x=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])

   #output
   y=np.array([[1],[1],[0]])

   #sigmoid function
   def sigmoid (x):
   return 1/(1 + np.exp(-x))

   #derivative of sigmoid function
   def derivatives_sigmoid(x):
   return x * (1 - x)

   #variable initialization
   epoch=5000 #setting training iterations
   lr=0.1 #setting learning rate
   inputlayer_neurons = x.shape[1] #number of features in data set
   hiddenlayer_neurons = 3 #number of hidden layers neurons
   output_neurons = 1 #number of neurons at output layer

   #weight and bias initialization
   wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))
   bh=np.random.uniform(size=(1,hiddenlayer_neurons))
   wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))
   bout=np.random.uniform(size=(1,output_neurons))

   for i in range(epoch):

   #forward propogation
   hidden_layer_input1=np.dot(x,wh)
   hidden_layer_input=hidden_layer_input1 + bh
   hiddenlayer_activations = sigmoid(hidden_layer_input)
   output_layer_input1=np.dot(hiddenlayer_activations,wout)
   output_layer_input= output_layer_input1+ bout
   output = sigmoid(output_layer_input)

   #id26
   e = y-output
   slope_output_layer = derivatives_sigmoid(output)
   slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)
   d_output = e * slope_output_layer
   error_at_hidden_layer = d_output.dot(wout.t)
   d_hiddenlayer = error_at_hidden_layer * slope_hidden_layer
   wout += hiddenlayer_activations.t.dot(d_output) *lr
   bout += np.sum(d_output, axis=0,keepdims=true) *lr
   wh += x.t.dot(d_hiddenlayer) *lr
   bh += np.sum(d_hiddenlayer, axis=0,keepdims=true) *lr

   print output


implementing nn in r

   # input matrix
   x=matrix(c(1,0,1,0,1,0,1,1,0,1,0,1),nrow = 3, ncol=4,byrow = true)

   # output matrix
   y=matrix(c(1,1,0),byrow=false)

   #sigmoid function
   sigmoid<-function(x){
   1/(1+exp(-x))
   }

   # derivative of sigmoid function
   derivatives_sigmoid<-function(x){
   x*(1-x)
   }

   # variable initialization
   epoch=5000
   lr=0.1
   inputlayer_neurons=ncol(x)
   hiddenlayer_neurons=3
   output_neurons=1

   #weight and bias initialization
   wh=matrix( rnorm(inputlayer_neurons*hiddenlayer_neurons,mean=0,sd=1),
   inputlayer_neurons, hiddenlayer_neurons)
   bias_in=runif(hiddenlayer_neurons)
   bias_in_temp=rep(bias_in, nrow(x))
   bh=matrix(bias_in_temp, nrow = nrow(x), byrow = false)
   wout=matrix( rnorm(hiddenlayer_neurons*output_neurons,mean=0,sd=1),
   hiddenlayer_neurons, output_neurons)

   bias_out=runif(output_neurons)
   bias_out_temp=rep(bias_out,nrow(x))
   bout=matrix(bias_out_temp,nrow = nrow(x),byrow = false)
   # forward propagation
   for(i in 1:epoch){

   hidden_layer_input1= x%*%wh
   hidden_layer_input=hidden_layer_input1+bh
   hidden_layer_activations=sigmoid(hidden_layer_input)
   output_layer_input1=hidden_layer_activations%*%wout
   output_layer_input=output_layer_input1+bout
   output= sigmoid(output_layer_input)

   # back propagation

   e=y-output
   slope_output_layer=derivatives_sigmoid(output)
   slope_hidden_layer=derivatives_sigmoid(hidden_layer_activations)
   d_output=e*slope_output_layer
   error_at_hidden_layer=d_output%*%t(wout)
   d_hiddenlayer=error_at_hidden_layer*slope_hidden_layer
   wout= wout + (t(hidden_layer_activations)%*%d_output)*lr
   bout= bout+rowsums(d_output)*lr
   wh = wh +(t(x)%*%d_hiddenlayer)*lr
   bh = bh + rowsums(d_hiddenlayer)*lr

   }
   output


[optional] mathematical perspective of back propagation algorithm

   let w[i ]be the weights between the input layer and the hidden layer.
   w[h ]be the weights between the hidden layer and the output layer.

   now, h=   (u)=    (w[i]x), i.e h is a function of u and u is a function
   of w[i ]and x. here we represent our function as   

   y=    (u   )=    (w[h]h), i.e y is a function of u    and u    is a function of
   w[h ]and h.

   we will be constantly referencing the above equations to calculate
   partial derivatives.

   we are primarily interested in finding two terms,    e/   w[i ]and    e/   w[h
   ]i.e change in error on changing the weights between the input and the
   hidden layer and change in error on changing the weights between the
   hidden layer and the output layer.

   but to calculate both these partial derivatives, we will need to use
   the chain rule of partial differentiation since e is a function of y
   and y is a function of u    and u    is a function of w[i.]

   let   s put this property to good use and calculate the gradients.

      e/   w[h] = (   e/   y).(    y/   u   ).(    u   /   w[h]),       ..(1)

   we know e is of the form e=(y-t)^2/2.

   so, (   e/   y)= (y-t)

   now,    is a sigmoid function and has an interesting differentiation of
   the form   (1-   ). i urge the readers to work this out on their side for
   verification.

   so, (   y/   u   )=    (   (u   )/    u   =   (u   )(1-   (u   )).

   but,   (u   )=y, so,

   (   y/   u   )=y(1-y)

   now, (    u   /   w[h])=    ( w[h]h)/    w[h] = h

   replacing the values in equation (1) we get,

      e/   w[h] = (y-t). y(1-y).h

   so, now we have computed the gradient between the hidden layer and the
   ouput layer. it is time we calculate the gradient between the input
   layer and the hidden layer.

      e/   w[i] =(    e/    h). (   h/   u).(    u/   w[i])

   but, (    e/    h) = (   e/   y).(    y/   u   ).(    u   /   h). replacing this value in
   the above equation we get,

      e/   w[i] =[(   e/   y).(    y/   u   ).(    u   /   h)]. (   h/   u).(    u/   w[i])               (2)

   so, what was the benefit of first calculating the gradient between the
   hidden layer and the output layer?

   as you can see in equation (2) we have already computed    e/   y and
      y/   u    saving us space and computation time. we will come to know in a
   while why is this algorithm called the back propagation algorithm.

   let us compute the unknown derivatives in equation (2).

      u   /   h =    (w[h]h)/    h = w[h]

      h/   u =    (   (u)/    u=   (u)(1-   (u))

   but,   (u)=h, so,

   (   y/   u)=h(1-h)

   now,    u/   w[i ]=    (w[i]x)/    w[i] = x

   replacing all these values in equation (2) we get,

      e/   w[i] = [(y-t). y(1-y).w[h]].h(1-h).x

   so, now since we have calculated both the gradients, the weights can be
   updated as

   w[h] = w[h] +    .    e/   w[h]

   w[i] = w[i] +    .    e/   w[i]

   where    is the learning rate.

   so coming back to the question: why is this algorithm called back
   propagation algorithm?

   the reason is: if you notice the final form of    e/   w[h] and    e/   w[i ],
   you will see the term (y-t) i.e the output error, which is what we
   started with and then propagated this back to the input layer for
   weight updation.

   so, where does this mathematics fit into the code?

   hiddenlayer_activations=h

   e= y-t

   slope_output_layer = y(1-y)

   lr =   

   slope_hidden_layer = h(1-h)

   wout = w[h]

   now, you can easily relate the code to the mathematics.


end notes:

   this article is focused on the building a neural network from scratch
   and understanding its basic concepts. i hope now you understand the
   working of a neural network like how does forward and backward
   propagation work, optimization algorithms (full batch and stochastic
   id119),  how to update weights and biases, visualization of
   each step in excel and on top of that code in python and r.

   therefore, in my upcoming article, i   ll explain the applications of
   using neural network in python and solving real-life challenges related
   to:
    1. id161
    2. speech
    3. natural language processing

   i enjoyed writing this article and would love to learn from your
   feedback. did you find this article useful? i would appreciate
   your suggestions/feedback. please feel free to ask your questions
   through comments below.

[101]learn, [102]compete, hack and [103]get hired!

   you can also read this article on analytics vidhya's android app
   [104]get it on google play

share this:

     * [105]click to share on linkedin (opens in new window)
     * [106]click to share on facebook (opens in new window)
     * [107]click to share on twitter (opens in new window)
     * [108]click to share on pocket (opens in new window)
     * [109]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [110]back propagation, [111]forward propagation, [112]gradient
   descent, [113]multi layer id88, [114]neural network,
   [115]id88, [116]python, [117]r
   next article

director     analytics     cpg/retail domain- bangalore- (12-15 years of
experience )

   previous article

artificial intelligence developer- ahmedabad (1-3 years of experience)

[118]sunil ray

   i am a business analytics and intelligence professional with deep
   experience in the indian insurance industry. i have worked for various
   multi-national insurance companies in last 7 years.

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [119]discussion portal to get your queries resolved

65 comments

     * sunny says:
       [120]may 29, 2017 at 11:01 am
       thank you very much.
       [121]reply
     * deep chatterjee says:
       [122]may 29, 2017 at 11:06 am
       amazing article..
       very well written and easy to understand the basic concepts..
       thank you for the hard work.
       [123]reply
     * ankur sharma says:
       [124]may 29, 2017 at 11:11 am
       thanks, for sharing this. very nice article.
       [125]reply
     * srinivas says:
       [126]may 29, 2017 at 12:16 pm
       nice article sunil! appreciate your continued research on the same.
       one correction though   
       now   
       hiddenlayer_neurons = 3 #number of hidden layers
       should be   
       hiddenlayer_neurons = 3 #number of neurons at hidden layers
       [127]reply
          + sunil ray says:
            [128]may 29, 2017 at 4:29 pm
            thanks srinivas! have updated the comment.
            [129]reply
               o ramesh says:
                 [130]september 17, 2017 at 12:06 pm
                 i didn   t understand what is the need to calculate delta
                 during back propagation.can you give any explanation to
                 it.
                 [131]reply
     * sami mustafa says:
       [132]may 29, 2017 at 12:39 pm
       very interesting!
       [133]reply
     * [134]ajit balakrishnan says:
       [135]may 29, 2017 at 1:40 pm
       very well written    i completely agree with you about learning by
       working on a problem
       [136]reply
     * andrei says:
       [137]may 29, 2017 at 2:55 pm
       thanks for great article! probably, it should be    update bias at
       both output and hidden layer    in the step 11 of the visualization
       of steps for neural network methodology
       [138]reply
          + sunil ray says:
            [139]may 29, 2017 at 4:30 pm
            thanks andrei,
            i   m updating only biases at step 11.
            regards,
            sunil
            [140]reply
     * sasikanth says:
       [141]may 29, 2017 at 4:23 pm
       wonderful explanation. this is an excellent article. i did not come
       across such a lucid explanation of nn so far.
       [142]reply
          + sunil ray says:
            [143]may 29, 2017 at 4:31 pm
            thanks sasikanth!
            regards,
            sunil
            [144]reply
     * robert says:
       [145]may 29, 2017 at 8:27 pm
       great article! there is a small typo: in the section where you
       describe the three ways of creating input output relationships you
       define    x2    twice     one of them should be    x3    instead     
       keep up the great work!
       [146]reply
          + sunil ray says:
            [147]may 30, 2017 at 12:03 am
            thanks robert for highlighting the typo!
            [148]reply
     * minati says:
       [149]may 29, 2017 at 9:13 pm
       explained in very lucid manner. thanks for this wonderful article.
       [150]reply
     * lakshmipathi says:
       [151]may 30, 2017 at 3:36 am
       very interesting! nice explanation
       [152]reply
     * ravi theja says:
       [153]may 30, 2017 at 7:46 am
       awesome sunil. its a great job.
       thanks a lot for making such a neat and clear page for nn, very
       much useful for beginners.
       [154]reply
     * praveenkumar manivannan says:
       [155]may 30, 2017 at 8:54 am
       well written article. with step by step explaination , it was
       easier to understand forward and backward propogations.. is there
       any functions in scikit learn for neural networks?
       [156]reply
          + sunil ray says:
            [157]may 30, 2017 at 12:21 pm
            thanks praveen! you can look at this
            ([158]http://scikit-learn.org/stable/modules/classes.html#modu
            le-sklearn.neural_network).
            regards,
            sunil
            [159]reply
     * sanjay says:
       [160]may 30, 2017 at 3:23 pm
       hello sunil,
       please refer below,
          to get a mathematical perspective of the backward propagation,
       refer below section.
       this one round of forward and back propagation iteration is known
       as one training iteration aka    epoch   .    
       i   m kind of lost there, did you already explain something?( about
       back prop) , is there any missing information?
       thanks
       [161]reply
     * rajendra says:
       [162]may 30, 2017 at 5:03 pm
       great article sunil! i have one doubt. why you applied linear to
       nonlinear transformation in the middle of the process? is it
       necessary!!
       [163]reply
     * sahar says:
       [164]may 30, 2017 at 7:11 pm
       thanks a lot, sunil, for such a well-written article. particularly,
       i liked the visualization section, in which each step is well
       explained by an example.
       i just have a suggestion: if you add the architecture of mlp in the
       beginning of the visualization section it would help a lot. because
       in the beginning i thought you are addressing the same architecture
       plotted earlier, in which there were 2 hidden units, not 3 hidden
       units.
       thanks a lot once more!
       [165]reply
     * vishwa says:
       [166]may 31, 2017 at 12:19 pm
       nice article     
       [167]reply
     * asad hanif says:
       [168]may 31, 2017 at 3:37 pm
       very well written article. thanks for your efforts.
       [169]reply
     * amit says:
       [170]may 31, 2017 at 9:02 pm
       great article. for a beginner like me, it was fully understandable.
       keep up the good work.
       [171]reply
     * preeti agarwal says:
       [172]june 1, 2017 at 12:09 pm
       great explanation   .on forward and backward propagation
       [173]reply
          + sunil ray says:
            [174]june 3, 2017 at 12:08 am
            thanks preeti
            regards,
            sunil
            [175]reply
     * [176]gino says:
       [177]june 2, 2017 at 12:11 pm
       i really like how you explain this. very well written. thank you
       [178]reply
          + sunil ray says:
            [179]june 3, 2017 at 12:07 am
            thanks gino
            [180]reply
     * prabhakar krishnamurthy says:
       [181]june 2, 2017 at 10:47 pm
       i am 63 years old and retired professor of management. thanks for
       your lucid explanations. i am able to learn. my blessings are to
       you.
       [182]reply
          + sunil ray says:
            [183]june 3, 2017 at 12:07 am
            thanks professor
            regards,
            sunil
            [184]reply
     * sai srinivasan says:
       [185]june 4, 2017 at 1:21 am
       dear author this is a great article. infact i got more clarity.
       i just wanted to say, using full batch id119 (or sgd) we
       need to tune the learning rate as well, but if we use nesterovs
       id119, it would converge faster and produce quick
       results.
       [186]reply
     * krishna says:
       [187]june 7, 2017 at 8:14 am
       good information thanks sunil
       [188]reply
     * arjun says:
       [189]june 23, 2017 at 10:45 pm
       hey sunil,
       can you also follow up with an article on id56 and lstm, with your
       same visual like tabular break down? it was fun and would
       complement a good nn understanding.
       thanks
       [190]reply
     * vdg says:
       [191]june 29, 2017 at 3:17 am
       a pleasant reading.
       thanks for sharing.
       [192]reply
     * nanditha says:
       [193]june 29, 2017 at 6:20 am
       thanks for the detailed explanation!
       [194]reply
     * burhan mohamed says:
       [195]july 13, 2017 at 9:30 am
       i want to hug you. i still have to read this again but machine
       learning algorithms have been shrouded in mystery before seeing
       this article. thank you for unveiling it good friend.
       [196]reply
     * noor mohamed m says:
       [197]july 25, 2017 at 9:25 pm
       nice one.. thanks lot for the work. i understood the neural network
       in a day
       [198]reply
     * james w. blount, jr says:
       [199]august 6, 2017 at 8:29 am
       yes, i found the information helpful in i understanding neural
       networks, i have and old book on the subject,
       the book i found was very hard to understand, i enjoyed reading
       most of your article, i found how you presented the information
       good, i understood the language you used in writing the material,
       good job!
       [200]reply
     * saqib qamar says:
       [201]august 17, 2017 at 10:01 am
       thanks for great article, it is useful to understand the basic
       learning about neural networks. thnaks again for making great
       effort   
       [202]reply
     * chen dong says:
       [203]august 18, 2017 at 1:46 pm
       benefit a lot
       [204]reply
     * jaime says:
       [205]august 30, 2017 at 7:54 am
       thank you for this excellent plain-english explanation for
       amateurs.
       [206]reply
     * avichandra says:
       [207]september 13, 2017 at 3:09 pm
       thank you, sir, very easy to understand and easy to practice.
       [208]reply
     * dirk henninghaus says:
       [209]september 14, 2017 at 2:03 pm
       wonderful inspiration and great explanation.
       thank you very much
       [210]reply
     * dima says:
       [211]september 23, 2017 at 3:29 pm
       that is the simplest explain which i saw. thx!
       [212]reply
     * kostas says:
       [213]october 16, 2017 at 6:02 am
       thanks for the explanations, very clear
       [214]reply
     * dhruv says:
       [215]october 30, 2017 at 12:17 am
       well done     
       [216]reply
     * biswarup ganguly says:
       [217]november 4, 2017 at 5:12 pm
       a unique approach to visualize mlp ! thank you    
       [218]reply
     * tanasan srikotr says:
       [219]november 6, 2017 at 10:06 pm
       i   m a beginner of this way. this article makes me understand about
       neural better. thank you very much.
       [220]reply
     * dileep.31 says:
       [221]november 7, 2017 at 2:34 pm
       this is awesome explanation sunil. the code and excel illustrations
       help a lot with really understanding the implementation. this helps
       unveil the mystery element from neural networks.
       [222]reply
     * aj says:
       [223]november 14, 2017 at 12:11 pm
       thank you so much. this is what i wanted to know about nn.
       [224]reply
     * chitransh gupta says:
       [225]november 14, 2017 at 4:10 pm
       visualization is really very helpful. thanks
       [226]reply
     * debbrota paul chowdhury says:
       [227]november 24, 2017 at 4:06 pm
       great article. the way of explanation is unbelievable. thank you
       for writing.
       [228]reply
     * ankush manocha says:
       [229]november 27, 2017 at 5:31 pm
       appreciate   . stay blessed.
       [230]reply
     * prerna says:
       [231]november 28, 2017 at 8:20 pm
       thanks this was a very good read.
       [232]reply
     * jeff says:
       [233]december 21, 2017 at 3:02 pm
       simply brilliant. very nice piecemeal explanation. thank you
       [234]reply
     * fengke9411 says:
       [235]december 25, 2017 at 12:09 pm
       very clear! thank you!
       [236]reply
     * [237]benchur says:
       [238]january 23, 2018 at 11:51 am
       thank you for your article. i have learned lots of dl from it.
       [239]reply
     * praveena says:
       [240]february 27, 2018 at 1:05 am
       thank you very much. very simple to understand ans easy to
       visualize. please come up with more articles. keep up the good
       work!
       [241]reply
     * ramgopal says:
       [242]march 5, 2018 at 9:09 pm
       amazing article
       thank you very much !!!!
       [243]reply
     * gyan says:
       [244]march 10, 2018 at 9:10 pm
       this is amazing mr. sunil. although am not a professional but a
       student, this article was very helpful in understanding the concept
       and an amazing guide to implement neural networks in python.
       [245]reply
     * matthew says:
       [246]march 23, 2018 at 7:00 pm
       mr. sunil,
       this was a great write-up and greatly improved my understanding of
       a simple neural network.
       in trying to replicate your excel implementation, however, i
       believe i found an error in step 6, which calculates the output
       delta. what you have highlighted is the derivative of the sigmoid
       function acting on the first column of the output_layer_input (not
       shown in image), and not on the actual output, which is what should
       actually happen and does happen in your r and python
       implementations.
       thanks again!
       [247]reply
     * sunil kumar says:
       [248]may 5, 2018 at 9:39 pm
       very well explanation. everywhere nn is implemented using different
       libraries without defining fundamentals. thanks a lot      
       [249]reply
     * gajanan says:
       [250]may 21, 2018 at 12:02 pm
       very simple way but best explanation.
       [251]reply
     * supritha says:
       [252]may 25, 2018 at 2:37 pm
       thank you very much for explaining the concepts in a simple way.
       [253]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [254]srk       3924
   2    [2.jpg?date=2019-04-05] [255]mark12    3510
   3    [3.jpg?date=2019-04-05] [256]nilabha   3261
   4    [4.jpg?date=2019-04-05] [257]nitish007 3237
   5    [5.jpg?date=2019-04-05] [258]tezdhar   3082
   [259]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [260]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [261]understanding support vector machine algorithm from examples
       (along with code)
     * [262]essentials of machine learning algorithms (with python and r
       codes)
     * [263]a complete tutorial to learn data science with python from
       scratch
     * [264]7 types of regression techniques you should know!
     * [265]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [266]a simple introduction to anova (with applications in excel)
     * [267]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [268]top 5 machine learning github repositories and reddit discussions
   from march 2019

[269]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [270]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[271]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [272]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[273]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [274]16 opencv functions to start your id161 journey (with
   python code)

[275]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [276][ds-finhack.jpg]

   [277][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [278]about us
     * [279]our team
     * [280]career
     * [281]contact us
     * [282]write for us

   [283]about us
   [284]   
   [285]our team
   [286]   
   [287]careers
   [288]   
   [289]contact us

data scientists

     * [290]blog
     * [291]hackathon
     * [292]discussions
     * [293]apply jobs
     * [294]leaderboard

companies

     * [295]post jobs
     * [296]trainings
     * [297]hiring hackathons
     * [298]advertising
     * [299]reach us

   don't have an account? [300]sign up here.

join our community :

   [301]46336 [302]followers
   [303]20223 [304]followers
   [305]followers
   [306]7513 [307]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [308]privacy policy
     * [309]terms of use
     * [310]refund policy

   don't have an account? [311]sign up here

   iframe: [312]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [313](button) join now

   subscribe!

   iframe: [314]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [315](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/deep-learning/
  94. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
  95. https://www.analyticsvidhya.com/blog/category/deep-learning/
  96. https://www.analyticsvidhya.com/blog/category/python-2/
  97. https://www.analyticsvidhya.com/blog/category/r/
  98. https://www.analyticsvidhya.com/blog/author/sunil-ray/
  99. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
 100. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
 101. https://www.analyticsvidhya.com/blog
 102. https://datahack.analyticsvidhya.com/
 103. https://www.analyticsvidhya.com/jobs/#/user/
 104. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 105. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/?share=linkedin
 106. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/?share=facebook
 107. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/?share=twitter
 108. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/?share=pocket
 109. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/?share=reddit
 110. https://www.analyticsvidhya.com/blog/tag/back-propagation/
 111. https://www.analyticsvidhya.com/blog/tag/forward-propagation/
 112. https://www.analyticsvidhya.com/blog/tag/gradient-descent/
 113. https://www.analyticsvidhya.com/blog/tag/multi-layer-id88/
 114. https://www.analyticsvidhya.com/blog/tag/neural-network/
 115. https://www.analyticsvidhya.com/blog/tag/id88/
 116. https://www.analyticsvidhya.com/blog/tag/python/
 117. https://www.analyticsvidhya.com/blog/tag/r/
 118. https://www.analyticsvidhya.com/blog/author/sunil-ray/
 119. https://discuss.analyticsvidhya.com/
 120. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129395
 121. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129395
 122. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129396
 123. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129396
 124. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129397
 125. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129397
 126. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129398
 127. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129398
 128. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129414
 129. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129414
 130. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-137338
 131. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-137338
 132. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129400
 133. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129400
 134. http://blogs.rediff.com/ajitb
 135. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129404
 136. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129404
 137. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129411
 138. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129411
 139. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129415
 140. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129415
 141. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129413
 142. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129413
 143. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129416
 144. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129416
 145. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129426
 146. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129426
 147. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129434
 148. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129434
 149. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129428
 150. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129428
 151. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129437
 152. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129437
 153. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129450
 154. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129450
 155. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129454
 156. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129454
 157. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129460
 158. http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neural_network
 159. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129460
 160. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129467
 161. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129467
 162. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129475
 163. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129475
 164. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129483
 165. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129483
 166. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129518
 167. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129518
 168. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129531
 169. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129531
 170. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129548
 171. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129548
 172. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129587
 173. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129587
 174. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129666
 175. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129666
 176. http://www.g1no.com/
 177. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129638
 178. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129638
 179. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129665
 180. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129665
 181. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129659
 182. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129659
 183. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129664
 184. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129664
 185. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129710
 186. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129710
 187. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129925
 188. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-129925
 189. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-131008
 190. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-131008
 191. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-131271
 192. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-131271
 193. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-131275
 194. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-131275
 195. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-132052
 196. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-132052
 197. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-132858
 198. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-132858
 199. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-133758
 200. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-133758
 201. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-134491
 202. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-134491
 203. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-134609
 204. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-134609
 205. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-135555
 206. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-135555
 207. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-137061
 208. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-137061
 209. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-137129
 210. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-137129
 211. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-137766
 212. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-137766
 213. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-139741
 214. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-139741
 215. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-141363
 216. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-141363
 217. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-142189
 218. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-142189
 219. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-142454
 220. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-142454
 221. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-142549
 222. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-142549
 223. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-143666
 224. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-143666
 225. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-143684
 226. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-143684
 227. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-144563
 228. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-144563
 229. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-145094
 230. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-145094
 231. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-145381
 232. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-145381
 233. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-148967
 234. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-148967
 235. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-149399
 236. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-149399
 237. http://www.peiyan.club/
 238. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-150926
 239. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-150926
 240. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-151597
 241. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-151597
 242. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-151708
 243. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-151708
 244. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-151826
 245. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-151826
 246. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-152125
 247. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-152125
 248. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-153061
 249. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-153061
 250. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-153428
 251. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-153428
 252. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-153551
 253. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/#comment-153551
 254. https://datahack.analyticsvidhya.com/user/profile/srk
 255. https://datahack.analyticsvidhya.com/user/profile/mark12
 256. https://datahack.analyticsvidhya.com/user/profile/nilabha
 257. https://datahack.analyticsvidhya.com/user/profile/nitish007
 258. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 259. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 260. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 261. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 262. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 263. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 264. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 265. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 266. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 267. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 268. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 269. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 270. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 271. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 272. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 273. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 274. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 275. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 276. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 277. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 278. http://www.analyticsvidhya.com/about-me/
 279. https://www.analyticsvidhya.com/about-me/team/
 280. https://www.analyticsvidhya.com/career-analytics-vidhya/
 281. https://www.analyticsvidhya.com/contact/
 282. https://www.analyticsvidhya.com/about-me/write/
 283. http://www.analyticsvidhya.com/about-me/
 284. https://www.analyticsvidhya.com/about-me/team/
 285. https://www.analyticsvidhya.com/about-me/team/
 286. https://www.analyticsvidhya.com/about-me/team/
 287. https://www.analyticsvidhya.com/career-analytics-vidhya/
 288. https://www.analyticsvidhya.com/about-me/team/
 289. https://www.analyticsvidhya.com/contact/
 290. https://www.analyticsvidhya.com/blog
 291. https://datahack.analyticsvidhya.com/
 292. https://discuss.analyticsvidhya.com/
 293. https://www.analyticsvidhya.com/jobs/
 294. https://datahack.analyticsvidhya.com/users/
 295. https://www.analyticsvidhya.com/corporate/
 296. https://trainings.analyticsvidhya.com/
 297. https://datahack.analyticsvidhya.com/
 298. https://www.analyticsvidhya.com/contact/
 299. https://www.analyticsvidhya.com/contact/
 300. https://datahack.analyticsvidhya.com/signup/
 301. https://www.facebook.com/analyticsvidhya/
 302. https://www.facebook.com/analyticsvidhya/
 303. https://twitter.com/analyticsvidhya
 304. https://twitter.com/analyticsvidhya
 305. https://plus.google.com/+analyticsvidhya
 306. https://in.linkedin.com/company/analytics-vidhya
 307. https://in.linkedin.com/company/analytics-vidhya
 308. https://www.analyticsvidhya.com/privacy-policy/
 309. https://www.analyticsvidhya.com/terms/
 310. https://www.analyticsvidhya.com/refund-policy/
 311. https://id.analyticsvidhya.com/accounts/signup/
 312. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 313. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 314. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 315. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 317. https://www.facebook.com/analyticsvidhya
 318. https://twitter.com/analyticsvidhya
 319. https://plus.google.com/+analyticsvidhya/posts
 320. https://in.linkedin.com/company/analytics-vidhya
 321. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/28091327/eq1-neuron.png
 322. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/26094834/screen-shot-2017-05-26-at-9.47.51-am.png
 323. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/26094834/screen-shot-2017-05-26-at-9.47.51-am.png
 324. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/26185640/0.0nn.jpg
 325. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/28120922/screen-shot-2017-05-28-at-12.06.49-pm.png
 326. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/28120936/screen-shot-2017-05-28-at-12.07.23-pm.png
 327. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/28120948/screen-shot-2017-05-28-at-12.07.32-pm.png
 328. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/28121001/screen-shot-2017-05-28-at-12.07.41-pm.png
 329. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/28121014/screen-shot-2017-05-28-at-12.07.48-pm.png
 330. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/28121030/screen-shot-2017-05-28-at-12.07.57-pm.png
 331. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/28121043/screen-shot-2017-05-28-at-12.08.06-pm.png
 332. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/28121057/screen-shot-2017-05-28-at-12.08.14-pm.png
 333. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/28121113/screen-shot-2017-05-28-at-12.08.20-pm.png
 334. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/28121126/screen-shot-2017-05-28-at-12.08.30-pm.png
 335. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/05/28121142/screen-shot-2017-05-28-at-12.08.39-pm.png
 336. https://www.analyticsvidhya.com/blog/2017/05/director-analytics-cpgretail-domain-bangalore-12-15-years-of-experience/
 337. https://www.analyticsvidhya.com/blog/2017/05/artificial-intelligence-developer-ahmedabad-1-3-years-of-experience/
 338. https://www.analyticsvidhya.com/blog/author/sunil-ray/
 339. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 340. https://www.facebook.com/analyticsvidhya/
 341. https://twitter.com/analyticsvidhya
 342. https://plus.google.com/+analyticsvidhya
 343. https://plus.google.com/+analyticsvidhya
 344. https://in.linkedin.com/company/analytics-vidhya
 345. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 346. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 347. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 348. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 349. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 350. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 351. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 352. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 353. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 354. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 355. javascript:void(0);
 356. javascript:void(0);
 357. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 358. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 359. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 360. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 361. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 362. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 363. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 364. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 365. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 366. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f05%2fneural-network-from-scratch-in-python-and-r%2f&linkname=understanding%20and%20coding%20neural%20networks%20from%20scratch%20in%20python%20and%20r
 367. javascript:void(0);
 368. javascript:void(0);
