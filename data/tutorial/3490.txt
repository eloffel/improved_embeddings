   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    introduction to id119 algorithm (along
   with variants) in machine learning comments feed [4]alternate
   [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]deep learning [94]introduction to id119
   algorithm (along with variants) in machine learning

   [95]deep learning[96]machine learning[97]python[98]r

introduction to id119 algorithm (along with variants) in machine
learning

   [99]faizan shaikh, march 8, 2017

introduction

   optimization is always the ultimate goal whether you are dealing with a
   real life problem or building a software product. i, as a computer
   science student, always fiddled with optimizing my code to the extent
   that i could brag about its fast execution.

   optimization basically means getting the optimal output for your
   problem. if you read the recent article on [100]optimization, you would
   be acquainted with how optimization plays an important role in our
   real-life.

   optimization in machine learning has a slight difference. generally,
   while optimizing, we know exactly how our data looks like and what
   areas we want to improve. but in machine learning we have no clue how
   our    new data    looks like, let alone try to optimize on it.

   so in machine learning, we perform optimization on the training data
   and check its performance on a new validation data.

broad applications of optimization

   there are various [101] kinds of optimization techniques which are
   applied across various domains such as
     * mechanics     for eg: in deciding the surface of aerospace design
     * economics     for eg: cost minimization
     * physics     for eg: optimization time in quantum computing

   optimization has many more advanced applications like deciding optimal
   route for transportation, shelf-space optimization, etc.

   many popular machine algorithms depend upon optimization techniques
   such as id75, k-nearest neighbors, neural networks, etc.
   the applications of optimization are limitless and is a widely
   researched topic in both academia and industries.

   in this article, we will look at a particular optimization technique
   called id119. it is the most commonly used optimization
   technique when dealing with machine learning.


table of content

    1. what is id119?
    2. challenges in executing id119
    3. variants of id119 algorithm
    4. implementation of id119
    5. practical tips on applying id119
    6. additional resources


1. what is id119?

   to explain id119 i   ll use the classic mountaineering
   example.

   suppose you are at the top of a mountain, and you have to reach a lake
   which is at the lowest point of the mountain (a.k.a valley). a twist is
   that you are blindfolded and you have zero visibility to see where you
   are headed. so, what approach will you take to reach the lake?

   [grad_desc1.png]

   [102]source

   the best way is to check the ground near you and observe where the land
   tends to descend. this will give an idea in what direction you should
   take your first step. if you follow the descending path, it is very
   likely you would reach the lake.

   to represent this graphically, notice the below graph.

   [103]source

   let us now map this scenario in mathematical terms.

   suppose we want to find out the best parameters (  1) and (  2) for our
   learning algorithm. similar to the analogy above, we see we find
   similar mountains and valleys when we plot our    cost space   . cost space
   is nothing but how our algorithm would perform when we choose a
   particular value for a parameter.

   so on the y-axis, we have the cost j(  ) against our parameters   1 and
     2 on x-axis and z-axis respectively. here, hills are represented by
   red region, which have high cost, and valleys are represented by blue
   region, which have low cost.

   now there are many types of id119 algorithms. they can be
   classified by two methods mainly:
     * on the basis of data ingestion
         1. full batch id119 algorithm
         2. stochastic id119 algorithm

   in full batch id119 algorithms, you use whole data at once
   to compute the gradient, whereas in stochastic you take a sample while
   computing the gradient.
     * on the basis of differentiation techniques
         1. first order differentiation
         2. second order differentiation

   id119 requires calculation of gradient by differentiation of
   cost function. we can either use first order differentiation or second
   order differentiation.


2. challenges in executing id119

   id119 is a sound technique which works in most of the cases.
   but there are many cases where id119 does not work properly
   or fails to work altogether. there are three main reasons when this
   would happen:
    1. data challenges
    2. gradient challenges
    3. implementation challenges


2.1 data challenges

     * if the data is arranged in a way that it poses a [104]non-convex
       optimization problem. it is very difficult to perform optimization
       using id119. id119 only works for problems
       which have a well defined id76 problem.
     * even when optimizing a id76 problem, there may be
       numerous minimal points. the lowest point is called global minimum,
       whereas rest of the points are called local minima. our aim is to
       go to global minimum while avoiding local minima.
     * there is also a saddle point problem. this is a point in the data
       where the gradient is zero but is not an optimal point. we don   t
       have a specific way to avoid this point and is still an active area
       of research.


2.2 gradient challenges

     * if the execution is not done properly while using id119,
       it may lead to problems like vanishing gradient or exploding
       gradient problems. these problems occur when the gradient is too
       small or too large. and because of this problem the algorithms do
       not converge.


2.3 implementation challenges

     * most of the neural network practitioners don   t generally pay
       attention to implementation, but it   s very important to look at the
       resource utilization by networks. for eg: when implementing
       id119, it is very important to note how many resources
       you would require. if the memory is too small for your application,
       then the network would fail.
     * also, its important to keep track of things like floating point
       considerations and hardware / software prerequisites.


3. variants of id119 algorithms

   let us look at most commonly used id119 algorithms and their
   implementations.

3.1 vanilla id119

   this is the simplest form of id119 technique. here, vanilla
   means pure / without any adulteration. its main feature is that we take
   small steps in the direction of the minima by taking gradient of the
   cost function.

   let   s look at its pseudocode.
update = learning_rate * gradient_of_parameters
parameters = parameters - update

   here, we see that we make an update to the parameters by taking
   gradient of the parameters. and multiplying it by a learning rate,
   which is essentially a constant number suggesting how fast we want to
   go the minimum. learning rate is a hyper-parameter and should be
   treated with care when choosing its value.

   [learningrates.jpeg]

   [105]source


3.2 id119 with momentum

   here, we tweak the above algorithm in such a way that we pay heed to
   the prior step before taking the next step.

   here   s a pseudocode.
update = learning_rate * gradient
velocity = previous_update * momentum
parameter = parameter + velocity     update

   here, our update is the same as that of vanilla id119. but
   we introduce a new term called velocity, which considers the previous
   update and a constant which is called momentum.

   [momentum.png]

   [106]source


3.3 adagrad

   adagrad uses adaptive technique for learning rate updation. in this
   algorithm, on the basis of how the gradient has been changing for all
   the previous iterations we try to change the learning rate.

   here   s a pseudocode
grad_component = previous_grad_component + (gradient * gradient)
rate_change = square_root(grad_component) + epsilon
adapted_learning_rate = learning_rate * rate_change
update = adapted_learning_rate * gradient
parameter = parameter     update

   in the above code, epsilon is a constant which is used to keep rate of
   change of learning rate in check.


3.4 adam

   adam is one more adaptive technique which builds on adagrad and further
   reduces it downside. in other words, you can consider this as momentum
   + adagrad.

   here   s a pseudocode.
adapted_gradient = previous_gradient + ((gradient     previous_gradient) * (1     be
ta1))

gradient_component = (gradient_change     previous_learning_rate)
adapted_learning_rate =  previous_learning_rate + (gradient_component * (1     bet
a2))
update = adapted_learning_rate * adapted_gradient
parameter = parameter     update

   here beta1 and beta2 are constants to keep changes in gradient and
   learning rate in check

   there are also second order differentiation method like l-bfgs. you can
   see an implementation of this algorithm in [107]scipy library.


4. implementation of id119

   we will now look at a basic implementation of id119 using
   python.

   here we will use id119 optimization to find our best
   parameters for our deep learning model on an application of image
   recognition problem. our problem is an image recognition, to identify
   digits from a given 28 x 28 image. we have a subset of images for
   training and the rest for testing our model. in this article we will
   take a look at how we define id119 and see how our algorithm
   performs. refer[108] this article for an end-to-end implementation
   using python.

   here is the main code for defining vanilla id119,
params = [weights_hidden, weights_output, bias_hidden, bias_output]
def sgd(cost, params, lr=0.05):
  grads = t.grad(cost=cost, wrt=params)
  updates = []

  for p, g in zip(params, grads):
    updates.append([p, p - g * lr])

  return updates

updates = sgd(cost, params)

   now we break it down to understand it better.

   we defined a function sgd with arguments as cost, params and lr. these
   represent j(  ) as seen previously,    i.e. the parameters of our deep
   learning algorithm and our learning rate. we set default learning rate
   as 0.05, but this can be changed easily as per our preference.
def sgd(cost, params, lr=0.05):

   we then defined gradients of our parameters with respect to the cost
   function. here we used theano library to find gradients and we imported
   theano as t
grads = t.grad(cost=cost, wrt=params)

   and finally iterated through all the parameters to find out the updates
   for all possible parameters. you can see that we use vanilla gradient
   descent here.
for p, g in zip(params, grads):
    updates.append([p, p - g * lr]

   we can use this function to then find the best optimal parameters for
   our neural network. on using this function, we find that our neural
   network does a good enough job in finding the digits in our image as
   seen below
prediction is:  8
[109] 8

   in this implementation, we see that on using id119 we can
   get optimal parameters for our deep learning algorithm.


5. practical tips on applying id119

   each of the above mentioned id119 algorithms have their
   strengths and weaknesses. i   ll just mention some quick tips which might
   help you choose the right algorithm.
     * for rapid prototyping, use adaptive techniques like adam/adagrad.
       these help in getting quicker results with much less efforts. as
       here, you don   t require much hyper-parameter tuning.
     * to get the best results, you should use vanilla id119 or
       momentum. id119 is slow to get the  desired results, but
       these results are mostly better than adaptive techniques.
     * if your data is small and can be fit in a single iteration, you can
       use 2nd order techniques like l-bfgs. this is because 2nd order
       techniques are extremely fast and accurate, but are only feasible
       when data is small enough
     * there also an emerging method (which i haven   t tried but looks
       promising) to use learned features to predict learning rates of
       id119. go through this [110]paper for more details.

   now there are many reasons why a neural network fails to learn. but it
   helps immensely if you  can monitor where your algorithm is going
   wrong.

   when applying id119, you can look at these points which
   might be helpful in circumventing the problem:
     * error rates     you should check the training and testing error after
       specific iterations and make sure both of them decreases. if that
       is not the case, there might be a problem!
     * gradient flow in hidden layers     check if the network doesn   t show
       a vanishing gradient problem or exploding gradient problem.
     * learning rate     which you should check when using adaptive
       techniques.


6. additional resources

     * refer [111]this paper on overview of id119 optimization
       algorithms.
     * cs231n [112]course material on id119.
     * chapter 4 ([113]numerical optimization) and chapter 8
       ([114]optimization for deep learning models) of deep learning book


end notes

   i hope you enjoyed reading this article. after going through this
   article, you will be adept with the basics of id119 and its
   variants. i have also given a practical tips for implementing them.
   hope you found them helpful!

   if you have any questions or doubts, feel free to post them in the
   comments below.

[115]learn, [116]compete, hack and [117]get hired!

   you can also read this article on analytics vidhya's android app
   [118]get it on google play

share this:

     * [119]click to share on linkedin (opens in new window)
     * [120]click to share on facebook (opens in new window)
     * [121]click to share on twitter (opens in new window)
     * [122]click to share on pocket (opens in new window)
     * [123]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [124]adagrad, [125]adam, [126]deep learning, [127]first order
   differentiation, [128]full batch id119, [129]gradient
   descent, [130]id119 algorithm, [131]id119
   challenge, [132]id119 techniques, [133]id119 with
   monemtum, [134]gradient techniques, [135]machine leanring,
   [136]practical tips, [137]second order differentiation, [138]stoc,
   [139]stochastic gradient, [140]vanilla id119
   next article

senior data scientist- pune (3 to 5 years of experience)

   previous article

lead big data engineer-bengaluru/ mumbai /gurgaon (4-8 years of experience)

[141]faizan shaikh

   faizan is a data science enthusiast and a deep learning rookie. a
   recent comp. sc. undergrad, he aims to utilize his skills to push the
   boundaries of ai research.
     *
     *

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [142]discussion portal to get your queries resolved

5 comments

     * richard warnung says:
       [143]march 8, 2017 at 10:05 am
       in the section about momentum, is the sign     + velocity    correct?
       if i look at the graph then i would assume that the gradient term
       and velocity are first added and then subtracted. is this correct?
       [144]reply
          + faizan shaikh says:
            [145]march 8, 2017 at 11:01 am
            hey richard.
            think of momentum in physics terms; you gain momentum    in the
            same direction    if you are going downhill. and will lose
            momentum when you are going uphill, because you want to be at
            the lowest point. so here too, if your previous update value
            was negative (i.e. you are going down the slope), the
               velocity    will automatically be negative.
            also having said that, the sign would probably depend on how
            you would implement the algorithm (so it can be     if you want
            it to be!) you can see a similar implementation in both
            chainer (a deep learning library; link:
            [146]https://github.com/pfnet/chainer/blob/5f18ac088973ba65082
            2e031e3081b21a58dbdd8/chainer/optimizers/momentum_sgd.py) and
            cs231n course material (link in resource section).
            i would strongly recommend you to go through the actual paper
            and see the interpretations for yourself. here   s the link
            [147]http://www.cs.toronto.edu/~fritz/absps/momentum.pdf
            cheers!
            [148]reply
     * sneha says:
       [149]april 13, 2018 at 4:28 pm
       your blog is really inspiring.
       can you plz tell were can i find .csv file.
       [150]reply
          + aishwarya singh says:
            [151]april 13, 2018 at 5:22 pm
            hi sneha,
            thank you for the feedback. you can download the datasets from
            [152]here.
            [153]reply
     * badri says:
       [154]april 17, 2018 at 10:32 am
       nice and super article. thanks for all your work, faizan shaikh ,
       it   s very helpful
       [155]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-05] [156]srk       3924
   2    [2.jpg?date=2019-04-05] [157]mark12    3510
   3    [3.jpg?date=2019-04-05] [158]nilabha   3261
   4    [4.jpg?date=2019-04-05] [159]nitish007 3237
   5    [5.jpg?date=2019-04-05] [160]tezdhar   3082
   [161]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [162]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [163]understanding support vector machine algorithm from examples
       (along with code)
     * [164]essentials of machine learning algorithms (with python and r
       codes)
     * [165]a complete tutorial to learn data science with python from
       scratch
     * [166]7 types of regression techniques you should know!
     * [167]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [168]a simple introduction to anova (with applications in excel)
     * [169]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [170]top 5 machine learning github repositories and reddit discussions
   from march 2019

[171]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [172]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[173]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [174]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[175]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [176]16 opencv functions to start your id161 journey (with
   python code)

[177]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [178][ds-finhack.jpg]

   [179][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [180]about us
     * [181]our team
     * [182]career
     * [183]contact us
     * [184]write for us

   [185]about us
   [186]   
   [187]our team
   [188]   
   [189]careers
   [190]   
   [191]contact us

data scientists

     * [192]blog
     * [193]hackathon
     * [194]discussions
     * [195]apply jobs
     * [196]leaderboard

companies

     * [197]post jobs
     * [198]trainings
     * [199]hiring hackathons
     * [200]advertising
     * [201]reach us

   don't have an account? [202]sign up here.

join our community :

   [203]46336 [204]followers
   [205]20224 [206]followers
   [207]followers
   [208]7513 [209]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [210]privacy policy
     * [211]terms of use
     * [212]refund policy

   don't have an account? [213]sign up here

   iframe: [214]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [215](button) join now

   subscribe!

   iframe: [216]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [217](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/deep-learning/
  94. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
  95. https://www.analyticsvidhya.com/blog/category/deep-learning/
  96. https://www.analyticsvidhya.com/blog/category/machine-learning/
  97. https://www.analyticsvidhya.com/blog/category/python-2/
  98. https://www.analyticsvidhya.com/blog/category/r/
  99. https://www.analyticsvidhya.com/blog/author/jalfaizy/
 100. https://www.analyticsvidhya.com/blog/2017/02/lintroductory-guide-on-linear-programming-explained-in-simple-english/
 101. http://www.dimacs.rutgers.edu/~billp/pubs/surveyofoptimization.pdf
 102. http://cs231n.stanford.edu/slides/winter1516_lecture3.pdf
 103. https://www.youtube.com/watch?v=5u4g23_oohi
 104. http://www.cs.ubc.ca/labs/lci/mlrg/slides/non_convex_optimization.pdf
 105. http://cs231n.github.io/neural-networks-3/
 106. http://cs231n.github.io/neural-networks-3/
 107. https://github.com/scipy/scipy/blob/v0.18.1/scipy/optimize/optimize.py
 108. https://www.analyticsvidhya.com/blog/2016/11/fine-tuning-a-keras-model-using-theano-trained-neural-network-introduction-to-transfer-learning/
 109. https://www.analyticsvidhya.com/wp-content/uploads/2016/10/8.png
 110. https://arxiv.org/abs/1606.04474
 111. https://arxiv.org/abs/1609.04747
 112. http://cs231n.github.io/neural-networks-3/
 113. http://www.deeplearningbook.org/contents/numerical.html
 114. http://www.deeplearningbook.org/contents/optimization.html
 115. https://www.analyticsvidhya.com/blog
 116. https://datahack.analyticsvidhya.com/
 117. https://www.analyticsvidhya.com/jobs/#/user/
 118. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 119. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/?share=linkedin
 120. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/?share=facebook
 121. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/?share=twitter
 122. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/?share=pocket
 123. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/?share=reddit
 124. https://www.analyticsvidhya.com/blog/tag/adagrad/
 125. https://www.analyticsvidhya.com/blog/tag/adam/
 126. https://www.analyticsvidhya.com/blog/tag/deep-learning/
 127. https://www.analyticsvidhya.com/blog/tag/first-order-differentiation/
 128. https://www.analyticsvidhya.com/blog/tag/full-batch-gradient-descent/
 129. https://www.analyticsvidhya.com/blog/tag/gradient-descent/
 130. https://www.analyticsvidhya.com/blog/tag/gradient-descent-algorithm/
 131. https://www.analyticsvidhya.com/blog/tag/gradient-descent-challenge/
 132. https://www.analyticsvidhya.com/blog/tag/gradient-descent-techniques/
 133. https://www.analyticsvidhya.com/blog/tag/gradient-descent-with-monemtum/
 134. https://www.analyticsvidhya.com/blog/tag/gradient-techniques/
 135. https://www.analyticsvidhya.com/blog/tag/machine-leanring/
 136. https://www.analyticsvidhya.com/blog/tag/practical-tips/
 137. https://www.analyticsvidhya.com/blog/tag/second-order-differentiation/
 138. https://www.analyticsvidhya.com/blog/tag/stoc/
 139. https://www.analyticsvidhya.com/blog/tag/stochastic-gradient/
 140. https://www.analyticsvidhya.com/blog/tag/vanilla-gradient-descent/
 141. https://www.analyticsvidhya.com/blog/author/jalfaizy/
 142. https://discuss.analyticsvidhya.com/
 143. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/#comment-124311
 144. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/#comment-124311
 145. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/#comment-124312
 146. https://github.com/pfnet/chainer/blob/5f18ac088973ba650822e031e3081b21a58dbdd8/chainer/optimizers/momentum_sgd.py
 147. http://www.cs.toronto.edu/~fritz/absps/momentum.pdf
 148. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/#comment-124312
 149. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/#comment-152556
 150. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/#comment-152556
 151. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/#comment-152558
 152. https://datahack.analyticsvidhya.com/contest/all/
 153. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/#comment-152558
 154. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/#comment-152618
 155. https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/#comment-152618
 156. https://datahack.analyticsvidhya.com/user/profile/srk
 157. https://datahack.analyticsvidhya.com/user/profile/mark12
 158. https://datahack.analyticsvidhya.com/user/profile/nilabha
 159. https://datahack.analyticsvidhya.com/user/profile/nitish007
 160. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 161. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 162. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 163. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 164. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 165. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 166. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 167. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 168. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 169. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 170. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 171. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 172. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 173. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 174. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 175. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 176. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 177. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 178. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 179. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 180. http://www.analyticsvidhya.com/about-me/
 181. https://www.analyticsvidhya.com/about-me/team/
 182. https://www.analyticsvidhya.com/career-analytics-vidhya/
 183. https://www.analyticsvidhya.com/contact/
 184. https://www.analyticsvidhya.com/about-me/write/
 185. http://www.analyticsvidhya.com/about-me/
 186. https://www.analyticsvidhya.com/about-me/team/
 187. https://www.analyticsvidhya.com/about-me/team/
 188. https://www.analyticsvidhya.com/about-me/team/
 189. https://www.analyticsvidhya.com/career-analytics-vidhya/
 190. https://www.analyticsvidhya.com/about-me/team/
 191. https://www.analyticsvidhya.com/contact/
 192. https://www.analyticsvidhya.com/blog
 193. https://datahack.analyticsvidhya.com/
 194. https://discuss.analyticsvidhya.com/
 195. https://www.analyticsvidhya.com/jobs/
 196. https://datahack.analyticsvidhya.com/users/
 197. https://www.analyticsvidhya.com/corporate/
 198. https://trainings.analyticsvidhya.com/
 199. https://datahack.analyticsvidhya.com/
 200. https://www.analyticsvidhya.com/contact/
 201. https://www.analyticsvidhya.com/contact/
 202. https://datahack.analyticsvidhya.com/signup/
 203. https://www.facebook.com/analyticsvidhya/
 204. https://www.facebook.com/analyticsvidhya/
 205. https://twitter.com/analyticsvidhya
 206. https://twitter.com/analyticsvidhya
 207. https://plus.google.com/+analyticsvidhya
 208. https://in.linkedin.com/company/analytics-vidhya
 209. https://in.linkedin.com/company/analytics-vidhya
 210. https://www.analyticsvidhya.com/privacy-policy/
 211. https://www.analyticsvidhya.com/terms/
 212. https://www.analyticsvidhya.com/refund-policy/
 213. https://id.analyticsvidhya.com/accounts/signup/
 214. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 215. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 216. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 217. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 219. https://www.facebook.com/analyticsvidhya
 220. https://twitter.com/analyticsvidhya
 221. https://plus.google.com/+analyticsvidhya/posts
 222. https://in.linkedin.com/company/analytics-vidhya
 223. https://www.analyticsvidhya.com/blog/2017/03/senior-data-scientist-pune-3-to-5-years-of-experience/
 224. https://www.analyticsvidhya.com/blog/2017/03/lead-big-data-engineer-bengaluru-mumbai-gurgaon-4-8-years-of-experience/
 225. https://www.analyticsvidhya.com/blog/author/jalfaizy/
 226. https://www.linkedin.com/in/faizankshaikh
 227. http://github.com/faizankshaikh
 228. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 229. https://www.facebook.com/analyticsvidhya/
 230. https://twitter.com/analyticsvidhya
 231. https://plus.google.com/+analyticsvidhya
 232. https://plus.google.com/+analyticsvidhya
 233. https://in.linkedin.com/company/analytics-vidhya
 234. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 235. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 236. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 237. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 238. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 239. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 240. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 241. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 242. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 243. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 244. javascript:void(0);
 245. javascript:void(0);
 246. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 247. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 248. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 249. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 250. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 251. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 252. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 253. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 254. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 255. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f03%2fintroduction-to-gradient-descent-algorithm-along-its-variants%2f&linkname=introduction%20to%20gradient%20descent%20algorithm%20along%20its%20variants
 256. javascript:void(0);
 257. javascript:void(0);
