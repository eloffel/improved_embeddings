   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]onfido tech
     * [9]machine learning
     * [10]front end
     * [11]mobile
     * [12]our team
     * [13]work with us
     __________________________________________________________________

machine learning 101

   [14]go to the profile of peter roelants
   [15]peter roelants (button) blockedunblock (button) followfollowing
   may 16, 2017

   in this blog post we   ll briefly cover the following topics to give you
   a very basic introduction to machine learning:
     * what is machine learning?
     * training machine learning models.
     * optimising parameters.
     * neural networks.

   don   t worry if you   re not an expert         the only knowledge you need for
   this blog post is basic high school maths.

what is machine learning?

   the [16]oxford dictionary defines machine learning as:

        the capacity of a computer to learn from experience   

   the goal of machine learning is to come up with algorithms that can
   learn how to perform a certain task based on example data.

   here   s an example. let   s say we want to write a program to play the
   game [17]go. we could write this program by manually defining rules on
   how to play the game. we might, program some opening strategies and
   decision rules         that it   s better to capture a stone than not, for
   example.

   but there   s a problem. programming these rules manually means that they
   can quickly become quite complex, and are limited by the strategies we
   as programmers can come up with. a better solution is to build machine
   learning algorithms. machine learning can learn how to play go based on
   examples and experience, just like humans would. this is what deepmind
   did with their [18]alphago program, a machine algorithm based on deep
   learning that turned out to be so good, it won against the (human) go
   world champion.

training machine learning models

   machine learning algorithms train models based on examples of labeled
   data. a machine learning algorithm typically defines a model with
   tunable parameters and an optimisation algorithm, as illustrated below.
   the model takes input in the form of data (x) and generates an output
   (y) based on the input data and its parameters. the optimisation
   algorithm tries to find the best combination of parameters so that
   given the example x the model   s output y is as close to the expected
   output as possible. the trained model will represent a specific
   function f that given x produces output y. so: y=f(x).
   [1*qxjgugbzinxotx3q41nomw.png]
   pipeline of training a machine learning model.

optimisation

   there are many ways to find the best combination of parameters so that
   the output y of model f is as close to the expected output as possible
   given input x. one way would be to try out all possible combinations of
   parameters and select the combination that gives the best results. this
   might work if there are only a limited number of parameter
   combinations, but for typical machine learning models that have
   thousands or even millions of parameters, it   s completely impractical.
   luckily (and thanks to the invention of 17th-century mathematician
   newton), there   s a much better way of finding the optimal solution for
   some types of models.
   [1*fdrcnxwpw3wylyczbcxeuw.png]
   newton and leibniz         [19]https://xkcd.com/626/

   that invention of newton is the [20]derivative (also known more
   generally as [21]gradient). the derivative of a function represents how
   the function changes with respect to one of its parameters, and points
   in the direction of the increase of the function. if we have a function
   f that has parameter p, then the change, df, of the function f with
   respect to the change, dp, of the parameter p is noted as df(p)/dp.
   [1*fahaat3i003d6tjzqzz32q.gif]
   derivative (gradient) df(p)/dp of f(p) = p   sin(p^2) for different
   values of p.

   so how can this derivative be used to make the model   s optimisation
   more efficient? assume that we have some data (x, t) so that input x
   corresponds to target t. this data is plotted as follows:
   [1*6p69lwza8yuazcmdpek87g.png]
   labelled data (x,t)

   if we now want to create a model that best approximates target t for
   given input x for all given examples, then we can try to fit a straight
   line through the origin (this is also known as [22]id75).
   this straight line can be represented by the function y=f(x) with
   f(x)=p   x where p is the only parameter of the model (note that p
   represents the [23]slope of the line). this model can be represented
   visually as:
   [1*wyviuzlagsq6nm2urr3g-q.png]
   representation of our model y=f(x)

   to find the parameter p so that y=x   p is as close to t for all given
   examples (x,t) we have to define a measure of    closeness    in a
   mathematical way. this measure is also known as a [24]cost function. a
   typical cost function for this problem is to sum the squared values of
   all absolute differences between target t and model output y: |t-y|  
   for all examples (x,t). the final cost function becomes    |t - (x   p)|  
   where the sigma represents the sum. because this example is quite
   simple, we can actually visualise this cost function easily for all
   parameters p:
   [1*m2wuqmo7xoqzaqizxb2tvq.png]
   cost function for our example.

   to find the best parameter p we need to minimise the cost function.
   remember that our model has a parameter p, takes input x and produces
   output y. we can write this model as y=x   p. since the cost is    |t-y|  
   we can substitute y, and also write the cost function as    |t - (x   p)|  .
   if we want to minimise this function and make the outputs y as close to
   the targets t as possible, we can try out all possible values of p for
   each input sample (x,t) and select the value of p where the sum of the
   cost over all input samples is the lowest. trying out all possible
   values of p in this case would be possible, but would soon become
   unfeasible the more parameters the model has. this is where the
   derivative comes into play. with the derivative, we can simply select a
   random starting parameter value for p, and start following the
   derivative in the opposite direction to find the lowest point on the
   cost function. this process of descending down while following the
   derivative (gradient) is also known as [25]id119. the
   process is illustrated below, where we start at p=0.3 and follow the
   gradient for 12 steps while improving the fit of the model to the data
   (line fitted on right figure). we stop fitting the model when the cost
   doesn   t decrease much anymore, so the final parameter p found is 1.94
   with cost 0.451. note that the final line fits the data (x,t) much
   better than the initial line.
   [1*kqvi812_aerfrolz_5g3ra.gif]
   id119 optimisation.

neural networks

   this, in essence, is what happens if we train a [26]neural network
   model. however, more typical neural network models are made up of much
   more complicated functions than our y=x   p model. there is a large
   variety of neural network models, but typically they are all
   [27]differentiable and can be optimised with id119 as we   ve
   illustrated in this blog post.

   a typical neural network used in id161, for example, will
   consist of multiple layers. each layer will have hundreds or thousands
   of parameters and will be followed by a nonlinear function. having
   multiple layers in a neural network is where the term    [28]deep
   learning    comes from. the benefit of using multiple layers in the model
   is that each layer can use the information extracted in the previous
   layer to build up a more complex representation of the data. it   s
   because of this that neural networks have been shown to be so powerful,
   successfully trained to [29]recognise cats in videos, [30]recognise
   speech, and even[31] play atari video games.

   if you   d like to play around with small neural network examples, try
   [32]google   s tensorflow playground, or if you   re more technically
   minded and like to learn more, you can try to implement your own models
   with the help of my tutorial on [33]how to implement neural networks.

     * [34]machine learning
     * [35]neural networks
     * [36]deep learning
     * [37]id119
     * [38]optimization

   (button)
   (button)
   (button) 569 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [39]go to the profile of peter roelants

[40]peter roelants

   machine learning enthusiast

     (button) follow
   [41]onfido tech

[42]onfido tech

   stories from design, engineering, product and research at onfido

     * (button)
       (button) 569
     * (button)
     *
     *

   [43]onfido tech
   never miss a story from onfido tech, when you sign up for medium.
   [44]learn more
   never miss a story from onfido tech
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/be2e0a86c96a
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a&source=--------------------------nav_reg&operation=register
   8. https://medium.com/onfido-tech?source=logo-lo_q1l0eqdzgv7l---93e4295ebb88
   9. https://medium.com/onfido-tech/machine-learning/home
  10. https://medium.com/onfido-tech/front-end/home
  11. https://medium.com/onfido-tech/mobile/home
  12. https://medium.com/onfido-tech/our-team/home
  13. https://goo.gl/2dbstt
  14. https://medium.com/@peter.roelants?source=post_header_lockup
  15. https://medium.com/@peter.roelants
  16. https://en.oxforddictionaries.com/definition/machine_learning
  17. https://en.wikipedia.org/wiki/go_(game)
  18. https://deepmind.com/research/alphago/
  19. https://xkcd.com/626/
  20. https://en.wikipedia.org/wiki/derivative
  21. https://en.wikipedia.org/wiki/gradient
  22. https://en.wikipedia.org/wiki/linear_regression
  23. https://en.wikipedia.org/wiki/slope
  24. https://en.wikipedia.org/wiki/loss_function
  25. https://en.wikipedia.org/wiki/gradient_descent
  26. https://en.wikipedia.org/wiki/artificial_neural_network
  27. https://en.wikipedia.org/wiki/differentiable_function
  28. https://en.wikipedia.org/wiki/deep_learning
  29. https://www.wired.com/2012/06/google-x-neural-network/
  30. https://www.technologyreview.com/s/544651/baidus-deep-learning-system-rivals-people-at-speech-recognition/
  31. http://www.wired.co.uk/article/google-deepmind-atari
  32. http://playground.tensorflow.org/
  33. http://peterroelants.github.io/posts/neural_network_implementation_part01/
  34. https://medium.com/tag/machine-learning?source=post
  35. https://medium.com/tag/neural-networks?source=post
  36. https://medium.com/tag/deep-learning?source=post
  37. https://medium.com/tag/gradient-descent?source=post
  38. https://medium.com/tag/optimization?source=post
  39. https://medium.com/@peter.roelants?source=footer_card
  40. https://medium.com/@peter.roelants
  41. https://medium.com/onfido-tech?source=footer_card
  42. https://medium.com/onfido-tech?source=footer_card
  43. https://medium.com/onfido-tech
  44. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  46. https://medium.com/p/be2e0a86c96a/share/twitter
  47. https://medium.com/p/be2e0a86c96a/share/facebook
  48. https://medium.com/p/be2e0a86c96a/share/twitter
  49. https://medium.com/p/be2e0a86c96a/share/facebook
