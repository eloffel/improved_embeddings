   #[1]odes and codes    feed [2]odes and codes    comments feed [3]odes and
   codes    implementing vanilla id75 in python comments feed
   [4]demystifying the     coding culture     [5]5 machine learning trivia
   that embody fluke, persistance & observation. [6]alternate [7]alternate
   [8]odes and codes [9]wordpress.com

   [10]skip to content

[11]odes and codes

can express. can code.

   (button) primary menu

     * [12]home
     * [13]about
     * [14]contact

     * [15]facebook
     * [16]linkedin
     * [17]twitter
     * [18]instagram

search

   search for: ____________________ search

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   join 24 other followers

   ____________________

   (button) follow

implementing vanilla id75 in python

   posted on [19]june 25, 2017june 26, 2017 by [20]odesandcodes

   id75 is probably the first supervised learning algorithm
   that you encounter while starting off with machine learning.
   nonetheless, it is extremely crucial to get the proper hang of it in
   order to proceed forward with more complex topics.

   i browsed the web but could not find a proper source that implemented
   the famous id75 in plain python. there were tutorials with
   scikit-learn, sure, but nothing that implements the pure logic in front
   of you explicitly. of the few that attempted to do so, they seemed
   to over complicate things a bit. so, let   s get started. this is a
   straight forward no nonsense vanilla implementation of linear
   regression that only uses numpy as the sole external library for
   calculation purposes. this is done to allow for a vectorized approach
   which makes this considerably faster

   we will work on a [21]real world dataset and use the pandas library to
    import it into python. we also use the  matplotlib library to plot the
   final model for visualisation purposes.
#let us begin by importing libraries

import numpy as np
import pandas as pd # to import dataset
import matplotlib.pyplot as plt # to plot model


   let us now proceed with a few lines of data wrangling to get our data
   ready for usage.
# importing the excel dataset file using pandas
df = pd.read_excel('sat.xls')

# separating the input and output

train_input = df['comp_gpa']
y = df['univ_gpa'] #y.shape gives (105,)
y = np.reshape(y,(len(df),1)) #y.shape now gives(105,1)

#len(df) = 105 here.this is no.of data points we have.(m)

   let me explain the above statements quickly.

   we first use pandas read_excel function to load the whole excel file as
   a pandas dataframe in python. now, to keep things simple, we will use
   just one column of the dataset as the input feature. we arbitrarily
   choose    comp_gpa    to be that feature. (computer science grade point
   average) . our target variable is    univ_gpa    which stands for
   the overall university grade point average. so, eventually, we would
   want our model to take in a student   s computer science gpa and predict
   his overall university gpa. the final line of this snippet is
   noteworthy. we reshape the target variable to (the number of data
   points, 1). if you do not do this, you will encounter an error later
   on. we may use the numpy   s shape attribute to check for the shape of y
   before and after the reshape function was applied to it.

   now, let   s move on to the fun part, the id75 part.

   we start off by defining the set of parameters, theta, randomly.
# n=1 m=105. initialising theta randomly

theta = np.random.normal(size = (2,1))


   we assign theta a size of (2,1) because the number of features we have
   selected here is n=1 and so we have two parameters like so

   h(x) =   0 +   1*x1

   by convention, we have x0 = 1 for all input data. next, we prefix the
   set of ones to our input dataset x. we use numpy   s column_stack
   function to do this. this new modified input data (x) will help us to
   do matrix operations directly, to calculate our hypothesis.
# adding a vector of ones to the original input data
x = np.column_stack((np.ones(len(df)),train_input))

   hence, before this process the shape of our input data was (105,1) but
   now it is (105,2). a 105 length column vector of ones has now been
      stacked    in front of the original x.

   let us now calculate our hypothesis. the new modified input dataset (x)
   allows us to do a simple and efficient id127 to get it
   quickly. we use numpy   s matmul method that performs the matrix
   multiplication for us.
# hypthesis
h = np.matmul(x,theta)

   you can check from elementary id127 rules that here a
   105*2 matrix is multiplied by a 2*1 matrix that results in a 105*1
   predicted output by our hypothesis function.

   we now calculate the loss (j). this is the error that our hypothesis
   gives us on the given training set. note that we are working with
   entire vectors here and not one data point at a time, hence
   implementing the vectorized approach. we will use id119 to
   minimise this loss iteratively in the next step.
# id168
j = np.sum(0.5*(np.square(h-y)))

   let us now move on to the final training step which is implementing
   id119. we use the usual batch id119 to update our
   parameters iteratively.
# batch id119

#setting two hyperparameters
n_epochs = 100 # we run the id119 process this many times
alpha = 0.0005 # setting the learning rate

#starting the iterative process
for i in range(n_epochs):
h = np.matmul(x,theta)
j = np.sum(0.5*(np.square(h-y)))
theta = theta - alpha*(np.transpose((np.matmul(np.transpose(h-y),x))))
print("loss is "+str(j)+"in iteration : "+str(i)) #tracks loss

   we began the code snippet by setting the value of two hyperparameters.
   the value of n_epochs which stands for the number of epochs, decides
   how many time we want to repeat the id119 process and update
   our parameters. the value of alpha signifies the learning rate, or more
   simply, the size of the step we want to take at each descent.
   alpha is a very important hyperparameter. i by default had set it to
   0.1 and it was giving me weirdly incorrect results. this was because i
   was taking too large steps and was overshooting the minima while
   performing the descent. hence, reducing its value helped the algorithm
   converge to the optimal set of parameters. so, you use trial and error
   to set alpha. after that, it is just the regular vectorized
   implementation of id119 where the gradient is calculated as
    (np.transpose((np.matmul(np.transpose(h-y),x)))).

   lastly, we print out the losses in each iteration to see how good or
   bad our learning algorithm is performing. this helps debug issues such
   as high learning rates.

   finally, let us now plot what our id75 model looks like
   with our final    learnt    set of parameters, theta. this shows the
   original data points (in red) along with the line (in green) that our
   regression model fits it.
plt.scatter(x[:,1],np.matmul(x,theta), c='g', label='model')
plt.scatter(x[:,1],y, c='r', label='true')
plt.show()

   download (15)

   pretty neat, eh?

   the jupyter notebook that contains all of the above code in one place
   can be found [22]here.

   if you have any doubts or you want something more out of this article
   please feel free to leave your thoughts in the comments.
   advertisements

share this:

     * [23]twitter
     * [24]facebook
     *

like this:

   like loading...

related

   tags: [25]id75, [26]machine learning, [27]numpy,
   [28]python, [29]vanillacategories: [30]id75, [31]machine
   learning, [32]numpy, [33]python, [34]uncategorized, [35]vanilla

leave a reply [36]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [37]googleplus-sign-in

     *
     *

   [38]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [39]log out /
   [40]change )
   google photo

   you are commenting using your google account. ( [41]log out /
   [42]change )
   twitter picture

   you are commenting using your twitter account. ( [43]log out /
   [44]change )
   facebook photo

   you are commenting using your facebook account. ( [45]log out /
   [46]change )
   [47]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

post navigation

   [48]    previous post
   [49]next post    

   [50]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [51]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [52]cookie policy

   iframe: [53]likes-master

   %d bloggers like this:

references

   visible links
   1. https://odesandcodes.wordpress.com/feed/
   2. https://odesandcodes.wordpress.com/comments/feed/
   3. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/feed/
   4. https://odesandcodes.wordpress.com/2017/06/08/demystifying-the-coding-culture/
   5. https://odesandcodes.wordpress.com/2017/07/09/5-machine-learning-trivia-that-embody-fluke-persistance-observation/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/&for=wpcom-auto-discovery
   8. https://odesandcodes.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/#content
  11. https://odesandcodes.wordpress.com/
  12. https://odesandcodes.wordpress.com/
  13. https://odesandcodes.wordpress.com/about/
  14. https://odesandcodes.wordpress.com/contact/
  15. https://www.facebook.com/adityalahiri13
  16. https://www.linkedin.com/in/aditya-lahiri-0b33261a/
  17. http://www.twitter.com/
  18. http://www.instagram.com/
  19. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/
  20. https://odesandcodes.wordpress.com/author/odesandcodes/
  21. http://onlinestatbook.com/2/case_studies/sat.html
  22. https://github.com/arrayslayer/homicide-reports-1980-2014/blob/master/assignment_3.ipynb
  23. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/?share=twitter
  24. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/?share=facebook
  25. https://odesandcodes.wordpress.com/tag/linear-regression/
  26. https://odesandcodes.wordpress.com/tag/machine-learning/
  27. https://odesandcodes.wordpress.com/tag/numpy/
  28. https://odesandcodes.wordpress.com/tag/python/
  29. https://odesandcodes.wordpress.com/tag/vanilla/
  30. https://odesandcodes.wordpress.com/tag/linear-regression/
  31. https://odesandcodes.wordpress.com/tag/machine-learning/
  32. https://odesandcodes.wordpress.com/tag/numpy/
  33. https://odesandcodes.wordpress.com/tag/python/
  34. https://odesandcodes.wordpress.com/category/uncategorized/
  35. https://odesandcodes.wordpress.com/tag/vanilla/
  36. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/#respond
  37. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://odesandcodes.wordpress.com&color_scheme=light
  38. https://gravatar.com/site/signup/
  39. javascript:highlandercomments.doexternallogout( 'wordpress' );
  40. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/
  41. javascript:highlandercomments.doexternallogout( 'googleplus' );
  42. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/
  43. javascript:highlandercomments.doexternallogout( 'twitter' );
  44. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/
  45. javascript:highlandercomments.doexternallogout( 'facebook' );
  46. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/
  47. javascript:highlandercomments.cancelexternalwindow();
  48. https://odesandcodes.wordpress.com/2017/06/08/demystifying-the-coding-culture/
  49. https://odesandcodes.wordpress.com/2017/07/09/5-machine-learning-trivia-that-embody-fluke-persistance-observation/
  50. https://wordpress.com/?ref=footer_blog
  51. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/
  52. https://automattic.com/cookies
  53. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  55. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/#comment-form-guest
  56. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/#comment-form-load-service:wordpress.com
  57. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/#comment-form-load-service:twitter
  58. https://odesandcodes.wordpress.com/2017/06/25/implementing-vanilla-linear-regression-in-python/#comment-form-load-service:facebook
