   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [0*kethx4mtzfu8_0se.png]

id180 and it   s types-which is better?

   [16]go to the profile of anish singh walia
   [17]anish singh walia (button) blockedunblock (button) followfollowing
   may 29, 2017

what are id158s ?

   now, i know we all are familiar with what a-nn is but still let me
   define it for my own satisfaction -it is a very powerful , strong as
   well as a very complicated machine learning technique which mimics a
   human brain and how it functions.

   like our human brain has millions of neurons in a hierarchy and network
   of neurons which are interconnected with each other via axons and
   passes electrical signals from one layer to another called synapses.
   this is how we humans learn things. whenever we see, hear,feel and
   think something a synapse(electrical impulse) is fired from one neuron
   to another in the hierarchy which enables us to learn , remember and
   memorize things in our daily life since the day we were born.

   okay , now let   s not get into biology.

side note

     if you don   t know about how to implement deep learning or make deep
     neural networks, i would really suggest 2 of my favorite courses and
     insist to complete this amazing course by datacamp on [18]deep
     learning in python using keras or you can try out with this new
     course on[19] id98 for image procesing using keras. both of these are
     amazing and everyone should try these out ,all who are new to deep
     learning.

     if you want to learn building data products and focused more on
     application side then [20]building chatbots in python or [21]nlp
     fundamentals in python using nltk are the courses to try out.these
     course teaches you about how to implement deep learning in python
     and the fundamentals of deep learning from basics of neural networks
     to fine tuning and optimizing a deep neural network.

     or, if you want to first get clear with the basics of machine
     learning then these courses on [22]pre-processing for machine
     learning in python , [23]supervised learning in python using
     scikit-learn , [24]python for data science are the best choices for
     you .all these courses will teach you the basics very nicely and
     properly. i started learning python from these 3 courses. so python
     lovers can start with these courses based on their interest.

     for the r lovers i am also adding the links to my favorite machine
     learning and statistics courses in r called [25]machine learning
     toolbox and [26]statistical modelling in r ,[27] foundations of
     id203 theory in r. knowledge of statistics and id203
     theory is a must and pre-requisites for both machine learning and
     deep learning.

   so the above courses and resources mentioned are some of my personal
   picks and my personal favorites. best part is that not only they help
   you grab and learn the conceptual and core under the hood mathematical
   things but also help you to practically learn to implement them and get
   your hands dirty using the tool of your choice. this technique of
   learning subjects related to data science is the best. so make sure to
   give them a try on the basis of your topic of interest. i am sure you
   will love it and be more productive. these are the courses which have
   helped me a lot to be what i am today. so just wanted to share it with
   the audience and the readers. happy learning!.

what are id180 and what are it uses in a neural network model?

   id180 are really important for a artificial neural
   network to learn and make sense of something really complicated and
   non-linear complex functional mappings between the inputs and response
   variable.they introduce non-linear properties to our network.their main
   purpose is to convert a input signal of a node in a a-nn to an output
   signal. that output signal now is used as a input in the next layer in
   the stack.

   specifically in a-nn we do the sum of products of inputs(x) and their
   corresponding weights(w) and apply a activation function f(x) to it to
   get the output of that layer and feed it as an input to the next layer.

the question arises that why can   t we do it without activating the
input signal?

   if we do not apply a activation function then the output signal would
   simply be a simple linear function.a linear function is just a
   polynomial of one degree. now, a linear equation is easy to solve but
   they are limited in their complexity and have less power to learn
   complex functional mappings from data. a neural network without
   activation function would simply be a id75 model, which
   has limited power and does not performs good most of the times. we want
   our neural network to not just learn and compute a linear function but
   something more complicated than that. also without activation function
   our neural network would not be able to learn and model other
   complicated kinds of data such as images, videos , audio , speech etc.
   that is why we use id158 techniques such as deep
   learning to make sense of something complicated ,high
   dimensional,non-linear -big datasets, where the model has lots and lots
   of hidden layers in between and has a very complicated architecture
   which helps us to make sense and extract knowledge form such
   complicated big datasets.

so why do we need non-linearities?

   non-linear functions are those which have degree more than one and they
   have a curvature when we plot a non-linear function. now we need a
   neural network model to learn and represent almost anything and any
   arbitrary complex function which maps inputs to outputs.
   neural-networks are considered universal function approximators. it
   means that they can compute and learn any function at all. almost any
   process we can think of can be represented as a functional computation
   in neural networks.

   hence it all comes down to this, we need to apply a activation function
   f(x) so as to make the network more powerfull and add ability to it to
   learn something complex and complicated form data and represent
   non-linear complex arbitrary functional mappings between inputs and
   outputs. hence using a non linear activation we are able to generate
   non-linear mappings from inputs to outputs.

   also another important feature of a activation function is that it
   should be differentiable. we need it to be this way so as to perform
   backpropogation optimization strategy while propogating backwards in
   the network to compute gradients of error(loss) with respect to weights
   and then accordingly optimize weights using gradient descend or any
   other optimization technique to reduce error.

   just always remember to do :

        input times weights , add bias and activate   

most popular types of id180 -

    1. sigmoid or logistic
    2. tanh         hyperbolic tangent
    3. relu -rectified linear units

   sigmoid activation function: it is a activation function of form f(x) =
   1 / 1 + exp(-x) . its range is between 0 and 1. it is a s         shaped
   curve. it is easy to understand and apply but it has major reasons
   which have made it fall out of popularity -
     * vanishing gradient problem
     * secondly , its output isn   t zero centered. it makes the gradient
       updates go too far in different directions. 0 < output < 1, and it
       makes optimization harder.
     * sigmoids saturate and kill gradients.
     * sigmoids have slow convergence.

   [0*wyb0k0zk1miib6xp.png]

   now how do we solve the above problems ?

   hyperbolic tangent function- tanh : it   s mathamatical formula is f(x) =
   1         exp(-2x) / 1 + exp(-2x). now it   s output is zero centered because
   its range in between -1 to 1 i.e -1 < output < 1 . hence optimization
   is easier in this method hence in practice it is always preferred over
   sigmoid function . but still it suffers from vanishing gradient
   problem.
   [0*vhhgs4nwibecrjia.png]

   then how do we deal and rectify the vanishing gradient problem ?

   relu- rectified linear units : it has become very popular in the past
   couple of years. it was recently proved that it had 6 times improvement
   in convergence from tanh function. it   s just r(x) = max(0,x) i.e if x <
   0 , r(x) = 0 and if x >= 0 , r(x) = x. hence as seeing the mathamatical
   form of this function we can see that it is very simple and
   efficinent . a lot of times in machine learning and computer science we
   notice that most simple and consistent techniques and methods are only
   preferred and are best. hence it avoids and rectifies vanishing
   gradient problem . almost all deep learning models use relu nowadays.

   but its limitation is that it should only be used within hidden layers
   of a neural network model.

   hence for output layers we should use a softmax function for a
   classification problem to compute the probabilites for the classes ,
   and for a regression problem it should simply use a linear function.

   another problem with relu is that some gradients can be fragile during
   training and can die. it can cause a weight update which will makes it
   never activate on any data point again. simply saying that relu could
   result in dead neurons.

   to fix this problem another modification was introduced called leaky
   relu to fix the problem of dying neurons. it introduces a small slope
   to keep the updates alive.

   we then have another variant made form both relu and leaky relu called
   maxout function .
   [0*qtflu9rmtnullrvc.png]

   enough theory right? , then why not go and compare the different
   id180 and their performance yourself. pick up a simple
   dataset and implement deep learning on it and try different activation
   function at various times. you will see the difference yourself.

   and for all the r lovers if you want to implement deep learning in r
   you can visit my [28]github repository on making a simple digit
   recognizer on mnist dataset and use it as a reference to make deep
   learning models in r using keras package for r.

conclusion

   the question was which one is better to use ?

   answer to this question is that nowadays we should use relu which
   should only be applied to the hidden layers. and if your model suffers
   form dead neurons during training we should use leaky relu or maxout
   function.

   it   s just that sigmoid and tanh should not be used nowadays due to the
   vanishing gradient problem which causes a lots of problems to
   train,degrades the accuracy and performance of a deep neural network
   model.

     * [29]machine learning
     * [30]neural networks
     * [31]activation function
     * [32]supervised learning
     * [33]artificial intelligence

   (button)
   (button)
   (button) 3.4k claps
   (button) (button) (button) 24 (button) (button)

     (button) blockedunblock (button) followfollowing
   [34]go to the profile of anish singh walia

[35]anish singh walia

   a data nerd. in love with cloud computing, virtualization technologies
   and data science as well.:github [36]https://github.com/anishsingh20.

     (button) follow
   [37]towards data science

[38]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3.4k
     * (button)
     *
     *

   [39]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [40]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a9a5310cc8f
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_iinsidvsdvtk---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@anishsingh20
  17. https://towardsdatascience.com/@anishsingh20
  18. https://www.datacamp.com/courses/deep-learning-in-python?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  19. https://www.datacamp.com/courses/convolutional-neural-networks-for-image-processing?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  20. https://www.datacamp.com/courses/building-chatbots-in-python?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  21. https://www.datacamp.com/courses/natural-language-processing-fundamentals-in-python?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  22. https://www.datacamp.com/courses/preprocessing-for-machine-learning-in-python?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  23. https://www.datacamp.com/courses/supervised-learning-with-scikit-learn?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  24. https://www.datacamp.com/courses/intermediate-python-for-data-science?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  25. https://www.datacamp.com/courses/machine-learning-toolbox?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  26. https://www.datacamp.com/courses/statistical-modeling-in-r-part-1?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  27. https://www.datacamp.com/courses/foundations-of-id203-in-r?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  28. https://github.com/anishsingh20/deep-learning-in-r-using-keras-and-tensorflow-
  29. https://towardsdatascience.com/tagged/machine-learning?source=post
  30. https://towardsdatascience.com/tagged/neural-networks?source=post
  31. https://towardsdatascience.com/tagged/activation-functions?source=post
  32. https://towardsdatascience.com/tagged/supervised-learning?source=post
  33. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  34. https://towardsdatascience.com/@anishsingh20?source=footer_card
  35. https://towardsdatascience.com/@anishsingh20
  36. https://github.com/anishsingh20
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/?source=footer_card
  39. https://towardsdatascience.com/
  40. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  42. https://medium.com/p/a9a5310cc8f/share/twitter
  43. https://medium.com/p/a9a5310cc8f/share/facebook
  44. https://medium.com/p/a9a5310cc8f/share/twitter
  45. https://medium.com/p/a9a5310cc8f/share/facebook
