zero-shot id36 via reading comprehension

   omer levy, minjoon seo, eunsol choi, and luke zettlemoyer. conll 2017.

task

   id36 systems populate knowledge bases with facts from an
   unstructured text corpus. when the type of facts (relations) are
   predefined, one can use id104 or distant supervision to collect
   examples and train an extraction model for each relation type. however,
   these approaches are incapable of extracting relations that were not
   specified in advance and observed during training. our task focuses on
   extracting facts of new types that were neither specified nor observed
   a priori. [1]full paper (pdf) in our paper, we show that it is possible
   to reduce id36 to the problem of answering simple
   reading comprehension questions. we map each relation type r(x,y) to at
   least one parametrized natural-language question q[x] whose answer is
   y. for example, the relation educated_at(x,y) can be mapped to "where
   did x study?" and "which university did x graduate from?". given a
   particular entity x ("turing") and a text that mentions x ("turing
   obtained his phd from princeton"), a non-null answer to any of these
   questions ("princeton") asserts the fact and also fills the slot y.
   here are a few examples:

                 relation               question template
                                    where did x graduate from?
             educated_at(x,y)    in which university did x study?
                                     what is x's alma mater?
                                   what did x do for a living?
             occupation(x,y)             what is x's job?
                                   what is the profession of x?
                                        who is x's spouse?
               spouse(x,y)               who did x marry?
                                       who is x married to?

   this reduction allows us to perform zero-shot learning: define new
   relations "on the fly", after the model has already been trained. more
   specifically, the zero-shot scenario assumes access to labeled data for
   n relation types. this data is used to train a reading comprehension
   model through our reduction. however, at test time, we are asked about
   a previously unseen relation type r[n+1]. rather than providing labeled
   data for the new relation, we simply list questions that define the
   relation's slot values. assuming we learned a good reading
   comprehension model, the correct values should be extracted. from the
   perspective of reading comprehension, the goal of our task is to
   promote the development of new id53 models that are able
   to generalize to new relations.

data

   all our data is publicly available. once unpacked, the files are
   tab-delimited textual (utf-8) files. the columns represent: the
   wikidata relation, the wikipedia entity (x), the question template, and
   the sentence. all subsequent columns (fifth onward) are answer spans
   within the sentence. if there are only four columns, the instance is a
   negative example; i.e. the question cannot be answered from the
   sentence. [2]zero-shot benchmark (493mb) - tests whether a system can
   generalize to unseen relations. contains 10 folds of train/dev/test
   sets, split and stratified by relation. this is the main benchmark.
   [3]all benchmarks (1.1gb) - contains two other (easier) benchmarks:
   unseen entities, unseen question templates. [4]full dataset (1.3gb) -
   the original set of positive and negative examples.

code

   [5]evaluation script -  python evaluate.py <test_set> <answer_file>
   reads the test set and the model's answers, and returns the
   precision/recall/f1 scores. expects the answer file to contain one
   answer (utf-8 string) per row, corresponding to the test set.
   [6]bitbucket repository - contains our reading comprehension model.

references

   1. http://nlp.cs.washington.edu/zeroshot/zeroshot.pdf
   2. http://nlp.cs.washington.edu/zeroshot/relation_splits.tar.bz2
   3. http://nlp.cs.washington.edu/zeroshot/all_splits.tar.bz2
   4. http://nlp.cs.washington.edu/zeroshot/raw_data.tar.bz2
   5. http://nlp.cs.washington.edu/zeroshot/evaluate.py
   6. https://bitbucket.org/omerlevy/bidaf_no_answer
