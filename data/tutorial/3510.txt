   #[1]monkeylearn blog    feed [2]monkeylearn blog    comments feed
   [3]monkeylearn blog    the definitive guide to natural language
   processing comments feed [4]alternate [5]alternate

   [6][yh5baeaaaaalaaaaaabaaeaaaibraa7]
   [7][yh5baeaaaaalaaaaaabaaeaaaibraa7]
   [8][yh5baeaaaaalaaaaaabaaeaaaibraa7]
   [9][yh5baeaaaaalaaaaaabaaeaaaibraa7]
   [10]subscribe

     * [11]use cases
     * [12]machine learning
     * [13]news
     * [14]subscribe
     *
     * ____________________ (button)

   [15]subscribe

   [16]try monkeylearn
   [17]machine learning

the definitive guide to natural language processing

   javier couto
   by [18]javier couto
   [19]october 29, 2015    15 min read

the definitive guide to natural language processing

   a computer would deserve to be called intelligent if it could deceive a
   human into believing that it was human. alan turing.

   when i was a child, i read the book [20]the analytical engine    
   computers past, present and future, by jeremy bernstein. among
   fascinating concepts such as turing machines or the pascaline,
   bernstein mentions [21]machine translation as one of the tasks that a
   computer could perform. just imagine that for a second: a machine
   capable of understanding a text in a language and having the required
   knowledge to translate it into another language.

   a machine capable of understanding a text like we humans do? i know, it
   seems hard to believe, and for years it seemed like magic to me, until
   i discovered [22]natural language processing (nlp), a field that deals
   with this kind of problems. yes, by combining the power of artificial
   intelligence, computational linguistics and computer science, nlp
   allows a machine to understand natural language, a task that was so far
   the exclusive privilege of humans.

   machine translation from russian to english was the first nlp
   application, back in the 1950s. the results at the time were not
   specially good and for decades the following example was used to
   illustrate the limits of the field: when the biblical saying    the
   spirit is willing but the flesh is weak    was translated into russian,
   and then translated back in english, the result was    the vodka is good
   but the meat is rotten   . well, this is just a story, mainly inspired by
   the flaws of [23]literal translation, but it was so often considered as
   an actual example that it discredited the nlp field for years.

   since the 1950s a lot of progress has been made and nlp became a major
   issue, in both the economic and social development, when the internet
   went from being a small neighborhood of friendly computers to a huge
   repository of exploitable data. my intention is to give here a brief
   introduction to the field, trying to explain a little bit the magic
   behind some applications that we use everyday. it is not my purpose to
   develop in this post a broad and deep view of the field, nor give
   formal definitions or precise algorithms and techniques. this is just
   an overview and i   d be more than happy if these lines stimulates your
   curiosity for a field that has fascinated me for so many years.

nlp is everywhere even if we don   t know it

   one thing that amazes me about natural language processing is that
   although the term is not as popular as big data or [24]machine
   learning, we use nlp applications or benefit from them everyday. here
   are some examples of nlp applications widely used:

machine translation

   maybe you have already used machine translation and it seems a natural
   feature to you by now. the globe icon in twitter or the translate links
   in facebook posts, in google and bing search results, in some forums or
   user review systems.

   we are a long way from the story about the spirit and the vodka, but
   the quality of the translations is fluctuating and sometimes is not
   that good. machine translation works very well in restricted domains,
   that is when the vocabulary and the idiomatic constructions are mainly
   known. it can, for example, significantly cut the costs when it comes
   to translating technical manuals, support content or specific catalogs.

   however, in the general case there is still room for improvement. i
   imagine that you have already received e-mails about a tax refund that
   you are eligible for, or about a floating fund of some millions dollars
   that a good soul from a remote country insists to transfer to your back
   account. and some of them were written in a lousy english, scarcely
   comprehensible, right? well, sometimes the results of machine
   translation can give you the same impression. the thing is that while
   these scams are most of the time written like that on purpose to gain
   credibility or pass spam filters, bad translations could really be a
   problem when you are trying to understand a list of instructions or a
   package leaflet in a foreign language.

   but let   s suppose that you are traveling and you have to deal with the
   following text:

                    

   wouldn   t it be nice, even with all the possible terminological and
   grammatical mistakes, to have an application that takes a picture of it
   and gives you the following translation?

     vietnamese fried spring rolls

id54

   together with machine translation, [25]id54 was
   addressed in the 1950s. given a text, the goal is to get a reduced
   version of it that retains the most important information. a summary
   can be created by extraction or abstraction.

   an extraction-based approach will detect the most important segments of
   the input text, typically sentences, and will extract them to build a
   summary. besides the problem of deciding the relevance of each
   sentence, an extraction-based summarizer has to deal with coherence.
   for example, a sentence in the summary can refer to elements of a
   sentence that is not in the summary, a phenomenon known as [26]dangling
   anaphora:

        volkswagen   s new ceo matthias mueller has his work cut out for him.
     mueller has spent most of his career at the volkswagen group so he
     knows the inner workings of the company. now experts say he   ll have
     to make some big, bold changes to get the largest automaker in the
     world back on track.   

   in the example above, if only the third sentence is retained by the
   system, the reader will certainly ask himself who is the    he    that the
   summary is talking about.

   on the other hand, an abstraction-based approach implies [27]text
   generation: the summarizer does not copy text from the input but writes
   in its own words what it understands from the text. this is extremely
   complex and currently most of the systems available uses an
   extraction-based approach.

   id54 is of particular interest when it comes to give
   an informative overview of a set of journal news. some approaches,
   called [28]id57, can condense multiple
   documents into one summary. here it is important to avoid redundancy
   coming from different documents and maximize diversity. some other
   approaches model a summary as a simple list of keywords, so they work
   more like a keyphrase or [29]keyword extractor. you may have seen
   systems like blog platforms or scientific literature reference managers
   suggesting you the most important concepts of your article or paper.

id31

   the goal of [30]id31 is to identify subjective
   information in texts. it can be a judgment, an opinion or an emotional
   state, and it is a major issue lately for companies and famous people
   that want to be aware of their reputation on the internet. what do
   users say about our products? what do they think about my restaurant or
   my hotel? are they happy with our customer support service? what do
   they think about the competition?

   the [31]most frequent kind of id31 performed is called
   polarity detection, that is, understanding if a text about a given
   subject is positive, neutral, or negative. this might seem simple, and
   in some cases it is:

   [32]natural language processing (nlp)

   clear enough, don   t you think? twitter is an incredibly rich source of
   information for id31. real time, mainly subjective,
   easily shareable information. however, id31 has to deal
   with some very difficult problems. if you are building a system for a
   tech company, a user saying that his new cellphone is light weight is
   with no doubt a good thing. but what if what is light weighted is a
   political debate? even with words that are likely to be unambiguous, we
   could face some serious problems:

   [33]natural language processing (nlp)

text classification

   the goal of text classification is to assign predefined tags to a text.
   in some applications, such as [34]automatic spam detection there are
   only two tags: spam and ham. we call such an application a binary
   classifier. in other cases, a classifier can for instance organize news
   stories by topics or academic papers by domains. since we have more
   than two tags , we call it a multi-class classifier. and what happens
   if a blog post talks about sports and entertainment? how does a
   classifier choose the correct tag between multiple choices? well, it
   depends on the application: it can simply take the most likely
   candidate, but sometimes it makes sense to assign several tags to a
   text. that is called a multi-label classification problem.

   not all the applications in text classification are semantic oriented.
   for example, we can use a classifier to perform automatic language
   detection, that is, to detect in which language a text is written.

   some applications are particularly sophisticated. one that i love is
   [35]authorship attribution: based on the style of a text, a system must
   decide, with a reasonable margin of error, its author.  can you imagine
   that? such a system must know the set of candidate authors, and for
   each one of them, it must know a set of text samples. there are a lot
   of ways to model style, which is in fact a very complex concept. some
   typical features used by this systems are word frequencies, vocabulary
   richness, sentence length and phrase structure. although this is a
   current research subject, the first remarkable results were obtained in
   1964 by the work of mosteller and wallace on the authorship of the
   disputed [36]federalist papers.
   natural language processing (nlp) the federalist papers.

conversational agents

   do you remember hal 9000 from 2001 a space odyssey? hal speaks, thinks,
   is empathetic, he recognizes faces, express feelings and can even
   perform lip-reading and lie. a [37]conversational agent (ca) is a
   system capable of having a coherent conversation with a human. it can
   present a text-only interface or be a spoken dialog system. usually the
   conversation has a precise goal. for example a customer service ca,
   often called [38]automated online assistant or chatbot, that
   processes user requests and questions, or a ca that gives traveling
   advises.

   you have probably already interacted with a conversational agent, maybe
   without knowing at first that it was in fact a machine. i also guess
   that maybe you have realized during the conversation that something was
   missing, something human, a certain degree of fluency and
   unpredictability in the dialogue. the thing is that we are currently
   far from building such an intelligent entity as hal. but if we focus on
   a particular subject or task, where we only deal with some kind of
   restricted or formulaic language, we can get pretty good results.

   in 1964 joseph weizenbaum wrote [39]eliza, a computer program that
   simulates a conversation with a rogerian psychotherapist. here there is
   an example of a possible conversation.

     eliza: please tell me what   s been bothering you.

     you:   nothing, i feel really happy today

     eliza: tell me more about such feelings.

     you:   well, i   m just happy

     eliza: has your treatment made you happy ?

     you:   of course

     eliza: tell me more about that.

     you:   i don   t want to talk about that

     eliza: does that trouble you ?

   the conversation can go on like that for hours. eliza uses very basic
   pattern matching techniques, but at the time the impact was enormous.
   since then, a lot of improvements have been made. take a look at what
   happens when two spoken id71 talk to each other.

   iframe: [40]https://www.youtube.com/embed/wnzlbytzsqy?feature=oembed

so many other applications   

   there are plenty of other nlp real world applications. does your email
   application automatically detect events and propose you to add them to
   the calendar? well, that   s an nlp task called [41]information
   extraction. do you use a spell checker that recognizes grammatical
   errors? have you already tried a software of handwriting recognition? a
   question-answering system such as ask.com or start from the mit? what
   about content-based advertising? apple   s siri, ibm   s watson? do you
   know that since 2012 forbes magazine uses the storytelling system
   [42]narrative science to automatically write online articles? have you
   seen lately the huge progress in robotics?

   nlp is everywhere even if we don   t know it. and although nlp
   applications can very rarely achieve performances of 100%, they are
   part of our lives and provide a precious help to all of us.

but how can a machine understand text?

   well, it just can   t    but it can simulate understanding. for that, it
   must be able to know, to some extent, the rules of a natural language.
   nlp deals with different aspects of language: [43]phonology,
   [44]morphology, [45]syntax, [46]semantics and [47]pragmatics. and its
   worst enemy is called ambiguity. we are going to see different levels
   of analysis (i will set aside phonology since it   s less intuitive and
   needs specific background) and how nlp systems usually addressed them.

words, words, words

   the first thing to understand are words, in particular the nature of
   each word. is it a noun or an adjective? if it   s the inflected form of
   a verb, what is its infinitive form, and to what tense, person and
   number correspond the inflected form? this task is called
   [48]part-of-speech (pos) tagging. let   s take a look at the following
   sentence:

     john bought a book

   well, there is a straightforward approach: we could use a dictionary
   with the information of all the words, their inflected forms and part
   of speech, to compute the following output:

     john/propernoun
     bought/verb-past
     a/det
     book/noun

   alright, let   s set aside the fact that a language is an incredibly rich
   living entity, thus we can never know all the words. as we can see,
   even with a simple sentence this approach can   t work. the word bought
   can also be an adjective, book is a verb and a could be the letter, in
   which case its pos would be noun. as humans, we can usually resolve
   this kind of ambiguity. but try with the following sentence:

     will will will the will to will?

   real nlp applications usually perform tasks using two families of
   approaches: symbolic and statistical. a symbolic approach consists on a
   set of rules, often hand-written but sometimes automatically learned,
   that model different language phenomena. a statistical approach
   typically uses machine learning algorithms to learn the language
   phenomena.

   the [49]brill tagger is probably the most known rule-based pos tagger.
   it uses transformation rules that take into account the context. a
   first pos tag is assigned (the most frequent) and then rules are
   applied to get the correct output. so if, in our example,    bought    was
   first labeled as an adjective, a rule could correct that:

     adj     verb if the tag of the previous word is propernoun

   statistical approaches see id52 as a [50]sequence labeling
   problem. the underlying idea is that given a sequence of words with
   their respective tags, we can decide, for the next word, the most
   likely pos. in our example, if we have already seen    john bought a    and
   we know their pos tags, we can say for sure that    book    is a noun and
   not a verb. this makes perfect sense. there are statistical models such
   as [51]id48 (id48) or [52]id49
   (crf) that can be trained using big corpus of labeled data, that is,
   texts where each word has the correct pos tag assigned.

from words to structure

   alright, our nlp application knows how to deal with words. for example,
   it can find in a text all the instances of the verb buy. next question
   is: who buys what? we are going to need syntax for that. that   s a hard
   problem, sometimes even for a human. for example, in the following
   sentence:

     i saw a man on a hill with a telescope

   do i have the telescope or the man has it? is there a telescope on the
   hill? am i on the hill or he is?

   as you can see, syntax can be tricky. we have to understand how words
   group together in units called chunks, and how these chunks relate to
   each other. for that, we use grammars. for example:

     s     np vp

     np     det noun

     np     propernoun

     vp     verb np

     propernoun     john

     verb     bought

     det     a

   given this grammar, once our example sentence is analyzed in a process
   called parsing, a parser tree is built. we will know then that it   s
   [john] who bought [a book]. grammars can be manually written or learned
   from [53]treebanks, that are text corpus annotated with parser trees.

getting the meaning of words

   the humorous adage    time flies like an arrow. fruit flies like a
   banana.    illustrates perfectly the complexity of semantic ambiguity.
   there are two main problems:
    1. polysemy: words that have several meanings
    2. synonymy: different words that have similar meanings

   there are other semantic relations such as antonymy, hyponymy or
   hypernymy, that are important, but polysemy and synonymy are usually
   the most relevant.

   [54]lexical semantics deals with the meaning of words as units, while
   id152 studies how words combine to form larger
   meanings. so there are several approaches to semantics, that remains a
   major open problem in nlp.

   [55]id51 (wsd) tries to identify the sense of a
   polysemic word in a given sentence. let   s take, for instance, the
   following sentences:

     the tank is full of soldiers.
     the tank is full of nitrogen.

   as you can see, it is really a very hard problem. in both sentences pos
   tagging and syntax are the same. how can an nlp application know the
   kind of tank for each sentence? well, a deep approach can be used. it
   requires world knowledge. for example: a tank (container) usually don   t
   hold people. this is of course too costly, so shallow approaches are
   more frequent: given the word tank in a sentence, what are the nearby
   words? it makes sense: when we see the word soldiers or nitrogen, we,
   humans, can determine the kind of tank. so the co-occurrence of words
   can be used to remove ambiguity. this knowledge can be learned from a
   corpus where the meaning of each word is correctly labeled.

   most research in the field of wsd is performed by using [56]id138, a
   very large computational lexicon that groups nouns, verbs, adjectives
   and adverbs into sets of synonyms called synsets. each synset
   represents a unique concept. for example, for tank we have five synsets
   for noun and three for verb. the two synsets related to our example are
   {tank, army tank, armored combat vehicle, armored combat vehicle} and
   {tank, storage tank}.

   you can try [57]id138 online to see what point words can be
   polysemic. you may also want to take a look to other related semantic
   resources such as [58]framenet or [59]conceptnet.

   regarding id152, things get a little bit more
   complicated. first, the key idea is the [60]principle of
   compositionality, that states that the meaning of a whole can be build
   from the meaning of the parts. in id152, the parts
   are usually the constituents of the syntactic parse. so, to understand
   a sentence, we build a logical representation of it. we could use for
   that [61]first-order predicate logic, where predicates have specific
   non-ambiguous meaning. let   s imagine a user that writes the following
   request:

     a hotel with sea view near san francisco

   to correctly process the request, the system must understand it in a
   very precise way. so it could build the following logical
   representation of the query:

        x hotel(x)    view(x, sea)    near(locationof(x),
     locationof(sanfrancisco))

   the system could use then a programming language as [62]prolog to
   compute the results. this is, of course, very hard and costly in a
   general case, and global meaning can   t always be derived from the
   meaning of the parts, but in a restricted domain we can get pretty good
   results. id152 remains an open problem and it is a
   current crucial research topic in nlp.

a word about pragmatics

   it is not what you say or how you say it, but the context.
   [63]pragmatics studies the ways in which context contributes to
   meaning. imagine the following dialogue:

     john: you are an idiot.
     peter: oh, thanks, that   s very kind of you.

   should an nlp application consider that peter thinks that john   s words
   are kind? and what happens if john was simply joking? irony and sarcasm
   are very complex mechanisms. there is research work in nlp that intends
   to characterize them. for example, we can train a classifier to decide
   whether a tweet is ironic or not. features could be things like word
   frequency (assuming unexpectedness is a signal of irony) or the use of
   some adjectives (assuming exaggeration is also a signal of irony). but,
   although there are some interesting results, it is still an open
   problem and there is much space for improvements.

are all these analyses needed?

   not necessarily. it depends on the application. for spam detection, for
   example, you would most likely train a [64]naive bayes classifier over
   the words. so basically the application needs to know how to segment
   text into words, a process call [65]id121. on the other hand,
   complex applications such as [66]question-answering or information
   extraction systems, would probably need to perform morphological,
   syntactic and semantic analyses to give decent results.

symbolic or statistical, what is the best approach?

   the short answer is: none of them. there have been a lot of discussions
   about this question, with a non-negligibly number of radical positions.
   [67]fred jelinek, who led the ibm   s id103 team for about
   twenty years, reportedly said that    anytime a linguist leaves the group
   the recognition rate goes up   , which is often quoted in a less smooth
   way as:    every time i fire a linguist, the performance of our speech
   recognition system goes up   . on the other hand, famous linguist noam
   chomsky stated in 1969 that    it must be recognized that the notion of
   id203 of a sentence is an entirely useless one, under any known
   interpretation of this term   .

   what can be called the first age of nlp was dominated by symbolic
   approaches. as from the 2000s, statistical approaches became a reality
   and over the years even the most critics had to admit that statistical
   nlp gives very good results and outperforms sometimes the purely
   symbolic approaches.

   the good point about statistical methods is that you can do a lot with
   a little. so if you want to build a nlp application, you may want to
   start with this family of methods. i recommend you to read the [68]post
   about machine learning by ra  l garreta, that gives some examples of
   statistical approaches that come usually from the machine learning
   field.

   statistical approaches have their limitations. when the era of
   id48-based pos taggers started, performances were around 95%. well, it
   seems a very good result, an error rate of 5% seems acceptable. maybe,
   but if you consider sentences of 20 words on average, 5% means that
   each sentence will have a word mislabeled. since then, improvements
   were made, either by building better training resources or trying other
   models such as crf, but there is always an error rate you have to deal
   with. in some cases, the rate is so slow that the problem can be
   considered as solved. automatic spam detection is one of them.

   on the other hand, although they are more expensive, symbolic
   approaches are easier to understand by a human, without the need of a
   specific background in id203 and statistics. moreover, in some
   way you can have a better control of the methods: you can delete or
   change a rule or create a new one if, after testing, you see that
   something is wrong or missing. in statistical approaches this is less
   direct and less intuitive: you can improve the quality of your training
   data, you can do feature engineering to determine the better features,
   you can create new features, in any case you will have sometimes the
   impression of dealing with a black box.

   so the answer to the question of whether using symbolic or statistical
   approaches would depend on the applications, the goals, the existing
   resources, the people   s background, the budget, among other variables.
   you can also try a hybrid approach. you can use statistical methods to
   create automatically the rules (as the brill   s pos tagger do, for
   example) and then eventually curate them manually. on the other hand,
   you can inject manually build knowledge into a statistical method to
   improve results. for example, you can write a rule to state that a noun
   group can never end by a determiner. so if your system was trained over
   a non-curated corpus, this error won   t be made.

final words

   fifteen years ago, it was really hard for an nlp company to tell a
   prospect that their product doesn   t work with absolute precision. times
   have changed. with the advent of better search engines, blogs, social
   networks, e-commerce, the society slowly began to understand that, even
   perfectible as it is, nlp provides us a precious help every day.

   nlp is a young field, full of promises and with an international
   community that continues to develop new algorithms, techniques and
   resources. it moves fast. the recent results using [69]deep learning
   are amazingly improving several hard nlp tasks. the combination of
   advances in artificial intelligence and the emergence of the internet
   prompted nlp to a level unthinkable just a few years ago.

   within [70]monkeylearn, we believe that nlp is a game changing
   technology and that it will shape the future of internet. nlp is a
   difficult field, yes, and that   s exactly what we want to change. our
   goal with monkeylearn is to make nlp accessible for every software
   developer in the world and hopefully empower the next generation of
   intelligent applications. imagine the endless possibilities by
   transforming a technology that is usually restricted only to data
   scientists, into something accessible for everyone.

   we will see in the future how far nlp, machine learning and artificial
   intelligence technologies can go. sometimes i wonder, will we see the
   day when machines everywhere would be able to pass the [71]turing test?

   i hope you have enjoyed the post. don   t hesitate to drop me a line if
   you have any questions or comments.

   ____________________

   subscribe
   leave this field empty if you're human: ____________________
   [72]machine learning
   javier couto

javier couto

   phd in natural language processing from paris-sorbonne university.
   solid experience in international research projects, both in the
   industry and in the academia, holding posts as research engineer,
   consultant, r&d team manager and lecturer. avid writer and dad.
   [73]notification

posts you might like   

   [74]machine learning

[75]getting started in natural language processing (nlp)

   let   s say you heard about natural language processing (nlp), some sort
   of technology that can   
   javier couto
   [76]javier couto    [77]june 7, 2018    8 min read

have something to say?

   [yh5baeaaaaalaaaaaabaaeaaaibraa7]

text analysis with machine learning

   turn tweets, emails, documents, webpages and more into actionable data.
   automate
   business processes and save hours of manual data processing.
   [78]try monkeylearn
   [yh5baeaaaaalaaaaaabaaeaaaibraa7]
   [yh5baeaaaaalaaaaaabaaeaaaibraa7]
   [yh5baeaaaaalaaaaaabaaeaaaibraa7]
   [yh5baeaaaaalaaaaaabaaeaaaibraa7]

   [79][yh5baeaaaaalaaaaaabaaeaaaibraa7]
     * [80]analyze text at scale with machine learning
     * [81]try monkeylearn

   ____________________ (button)

get awesome content in your inbox every week

   stay up to date with our content and receive tutorials, guides and cool
   posts about machine learning!

   ____________________

   subscribe
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://monkeylearn.com/blog/feed/
   2. https://monkeylearn.com/blog/comments/feed/
   3. https://monkeylearn.com/blog/definitive-guide-natural-language-processing/feed/
   4. https://monkeylearn.com/blog/wp-json/oembed/1.0/embed?url=https://monkeylearn.com/blog/definitive-guide-natural-language-processing/
   5. https://monkeylearn.com/blog/wp-json/oembed/1.0/embed?url=https://monkeylearn.com/blog/definitive-guide-natural-language-processing/&format=xml
   6. https://monkeylearn.com/blog/
   7. https://monkeylearn.com/blog/
   8. https://monkeylearn.com/blog/
   9. https://monkeylearn.com/blog/
  10. https://monkeylearn.com/blog/definitive-guide-natural-language-processing/
  11. https://monkeylearn.com/blog/category/use-cases/
  12. https://monkeylearn.com/blog/category/machine-learning/
  13. https://monkeylearn.com/blog/category/news/
  14. https://monkeylearn.com/blog/definitive-guide-natural-language-processing/
  15. https://monkeylearn.com/blog/definitive-guide-natural-language-processing/
  16. https://monkeylearn.com/
  17. https://monkeylearn.com/blog/category/machine-learning/
  18. https://monkeylearn.com/blog/author/javier-couto/
  19. https://monkeylearn.com/blog/definitive-guide-natural-language-processing/
  20. http://www.amazon.com/analytical-engine-computers-present-future/dp/0394415051/ref=sr_1_1?s=books&ie=utf8&qid=1445983967&sr=1-1&keywords=the+analytical+engine:+computers,+past,+present,+and+future
  21. https://en.wikipedia.org/wiki/machine_translation
  22. https://en.wikipedia.org/wiki/natural_language_processing
  23. https://en.wikipedia.org/wiki/literal_translation
  24. https://en.wikipedia.org/wiki/machine_learning
  25. https://en.wikipedia.org/wiki/automatic_summarization
  26. https://en.wikipedia.org/wiki/anaphora_(linguistics)
  27. https://en.wikipedia.org/wiki/natural_language_generation
  28. https://en.wikipedia.org/wiki/multi-document_summarization
  29. https://app.monkeylearn.com/extraction/extractors/ex_y7bpyzng/
  30. https://monkeylearn.com/sentiment-analysis/
  31. https://monkeylearn.com/sentiment-analysis/
  32. https://blog.monkeylearn.com/wp-content/uploads/2015/10/tweet1.png
  33. https://blog.monkeylearn.com/wp-content/uploads/2015/10/tweet2.png
  34. https://en.wikipedia.org/wiki/naive_bayes_spam_filtering
  35. https://en.wikipedia.org/wiki/stylometry
  36. https://en.wikipedia.org/wiki/the_federalist_papers
  37. https://en.wikipedia.org/wiki/dialog_system
  38. https://en.wikipedia.org/wiki/automated_online_assistant
  39. https://en.wikipedia.org/wiki/eliza
  40. https://www.youtube.com/embed/wnzlbytzsqy?feature=oembed
  41. https://en.wikipedia.org/wiki/information_extraction
  42. http://www.forbes.com/sites/narrativescience/
  43. https://en.wikipedia.org/wiki/phonology
  44. https://en.wikipedia.org/wiki/morphology_(linguistics)
  45. https://en.wikipedia.org/wiki/syntax
  46. https://en.wikipedia.org/wiki/semantics
  47. https://en.wikipedia.org/wiki/pragmatics
  48. https://en.wikipedia.org/wiki/part-of-speech_tagging
  49. https://en.wikipedia.org/wiki/brill_tagger
  50. https://en.wikipedia.org/wiki/sequence_labeling
  51. https://en.wikipedia.org/wiki/hidden_markov_model
  52. https://en.wikipedia.org/wiki/conditional_random_field
  53. https://en.wikipedia.org/wiki/treebank
  54. https://en.wikipedia.org/wiki/lexical_semantics
  55. https://en.wikipedia.org/wiki/word-sense_disambiguation
  56. https://en.wikipedia.org/wiki/id138
  57. http://id138web.princeton.edu/perl/webwn
  58. https://framenet.icsi.berkeley.edu/fndrupal/
  59. http://conceptnet5.media.mit.edu/
  60. https://en.wikipedia.org/wiki/principle_of_compositionality
  61. https://en.wikipedia.org/wiki/first-order_logic
  62. https://en.wikipedia.org/wiki/prolog
  63. https://en.wikipedia.org/wiki/pragmatics
  64. https://en.wikipedia.org/wiki/naive_bayes_classifier
  65. https://en.wikipedia.org/wiki/id121_(lexical_analysis)
  66. https://en.wikipedia.org/wiki/question_answering
  67. https://en.wikipedia.org/wiki/frederick_jelinek
  68. http://blog.monkeylearn.com/a-gentle-guide-to-machine-learning/
  69. https://en.wikipedia.org/wiki/deep_learning
  70. http://www.monkeylearn.com/
  71. https://en.wikipedia.org/wiki/turing_test
  72. https://monkeylearn.com/blog/category/machine-learning/
  73. javascript:;
  74. https://monkeylearn.com/blog/category/machine-learning/
  75. https://monkeylearn.com/blog/getting-started-in-natural-language-processing-nlp/
  76. https://monkeylearn.com/blog/author/javier-couto/
  77. https://monkeylearn.com/blog/getting-started-in-natural-language-processing-nlp/
  78. https://app.monkeylearn.com/accounts/register/
  79. https://monkeylearn.com/blog
  80. https://monkeylearn.com/blog/definitive-guide-natural-language-processing/
  81. https://app.monkeylearn.com/accounts/register/

   hidden links:
  83. https://monkeylearn.com/blog/definitive-guide-natural-language-processing/
  84. https://www.facebook.com/sharer/sharer.php?u=https://monkeylearn.com/blog/definitive-guide-natural-language-processing/
  85. http://twitter.com/share?text=the+definitive+guide+to+natural+language+processing&url=https%3a%2f%2fmonkeylearn.com%2fblog%2fdefinitive-guide-natural-language-processing%2f
  86. https://www.linkedin.com/sharearticle?mini=true&url=https://monkeylearn.com/blog/definitive-guide-natural-language-processing&titlethe%20definitive%20guide%20to%20natural%20language%20processing&summary=&source=
  87. https://www.facebook.com/sharer/sharer.php?u=https://monkeylearn.com/blog/definitive-guide-natural-language-processing/
  88. http://twitter.com/share?text=the+definitive+guide+to+natural+language+processing&url=https%3a%2f%2fmonkeylearn.com%2fblog%2fdefinitive-guide-natural-language-processing%2f
  89. https://www.linkedin.com/sharearticle?mini=true&url=https%3a%2f%2fmonkeylearn.com%2fblog%2fdefinitive-guide-natural-language-processing%2f&titlethe+definitive+guide+to+natural+language+processing&summary=&source=
  90. https://monkeylearn.com/blog/getting-started-in-natural-language-processing-nlp/
  91. https://monkeylearn.com/blog/definitive-guide-natural-language-processing/
  92. http://twitter.com/share?text=getting+started+in+natural+language+processing+%28nlp%29&url=https%3a%2f%2fmonkeylearn.com%2fblog%2fgetting-started-in-natural-language-processing-nlp%2f
  93. https://www.facebook.com/sharer/sharer.php?u=https://monkeylearn.com/blog/getting-started-in-natural-language-processing-nlp/
  94. https://plus.google.com/share?url=https://monkeylearn.com/blog/getting-started-in-natural-language-processing-nlp/
