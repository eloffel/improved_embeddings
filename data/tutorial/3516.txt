   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*r85xoznnah7kf1yil3me5w.jpeg]
   source: allison linn, microsoft

introducing deep learning and neural networks         deep learning for rookies (1)

   [16]go to the profile of nahua kang
   [17]nahua kang (button) blockedunblock (button) followfollowing
   jun 18, 2017

   follow me on [18]twitter to learn more about life in a deep learning
   startup.
     __________________________________________________________________

   welcome to the first post of my series deep learning for rookies by me,
   a rookie. i   m writing as a id23 strategy to process
   and digest the knowledge better. but if you are a deep learning rookie,
   then this is for you as well because we can learn together as rookies!

   (you can also read [19]this post on my website, which supports latex
   with mathjax)
   [1*gyvlhxgv8q8-bl48djjzpq.jpeg]
   source: deepinstinct.com

   deep learning is probably one of the hottest tech topics right now.
   large corporations and young startups alike are all gold-rushing this
   fancy field. if you think big data is important, then you should care
   about deep learning. [20]the economist says that [21]data is the new
   oil in the 21st century. if data is the crude oil, databases and data
   warehouses are the drilling rigs that digs and pumps the data on the
   internet, then think of deep learning as the oil refinery that finally
   turns crude oil into all the useful and insightful final products.
   there could be a lot of    fossil fuels    hidden underground, and there
   are a lot of drills and pumps in the market, but without the right
   refinery tools, you ain   t gonna get anything valuable. that   s why deep
   learning is important. it   s part of the data-driven big picture.

   the good news is, we are not going to run out of data and our    refinery
   machine    is getting better and better. today, just about doing anything
   online will generate data. so data, unlike oil, is    sustainable    and
   growing    explosively   . in the meantime, as long as the data isn   t
   garbage-in, then there   s no garbage-out from deep learning. hence the
   more data, the merrier. (check out [22]trent mcconaghy   s post on
   [23]blockchains for ai as a solution for data to have reputation!).

   also, this    oil refinery    is improving on both software and hardware.
   deep learning algorithms have improved over the past few decades and
   developers around the world have contributed to open source frameworks
   like tensorflow, theano, keras, and torch, all of which make it easy
   for people to build deep learning algorithms as if playing with lego
   pieces. and thanks to the demand from gamers around the world, gpus
   (graphics processing units) make it possible for us to leverage deep
   learning algorithms to build and train models with impressive results
   in a time-efficient manner! so to all the parents who don   t like your
   kids playing games: gaming has its silver lining   

deep learning: the secret recipe

   you   ve probably read on the news and know that deep learning is the
   secret recipe behind many exciting developments and has made many of
   our wildest dreams and perhaps also nightmares come true. who would
   have thought that deepmind   s alphago could defeat lee sedol, one of the
   best go players, in the deepest board game which boasts more possible
   moves than there are atoms in the entire universe? a lot of people,
   including me, never saw it coming. it seemed impossible. but it   s here
   now. deep learning is beating us in the most challenging board game.
   when is the ai awakening? some think it will be soon.
   [1*7uposbq_unq-aiz-tr5_dg.jpeg]
   lee sedol vs. alphago in 2016, source: the new yorker

   and we haven   t talked about the other impressive applications powered
   by deep learning, such as google translate and mobileye   s autonomous
   driving. you name it. did i forget to mention that [24]deep learning is
   also beating physicians at diagnosing cancer? deep learning excels at
   many tasks with lower error rates than humans! it   s not just automating
   the boring stuff, but also the fun stuff. sigh, we mundane humans   

     dear mr. president, it   s not foreigners. it   s automation.

   here   s a short list of general tasks that deep learning can perform in
   real situations:
    1. identify faces (or more generally image categorization)
    2. read handwritten digits and texts
    3. recognize speech (no more transcribing interviews yourself)
    4. translate languages
    5. play computer games
    6. control self-driving cars (and other types of robots)

   and there   s more. just pause for a second and imagine all the things
   that deep learning could achieve. it   s amazing and perhaps a bit scary!

distinctions: ai, machine learning, and deep learning

   okay, wait a second. you   ve probably seen terms like artificial
   intelligence (ai), machine learning (ml), and deep learning (dl) flying
   all over the place in your social media newsfeed. what   s the difference
   between all of them? do they mean the same thing or what? great
   question. before we go deeper into deep learning, it is important to
   gradually build a conceptual framework of these jargons.

   roughly speaking, the graph below demonstrates the relationship for
   these 3 concepts. deep learning is a subfield of machine learning, and
   machine learning is a subfield of artificial intelligence. both
   [25]ophir samson and [26]carlos e. perez have written good stories
   about them. check [27]here and [28]here.
   [1*vdhyrqfjt9__lkggjsc2kg.png]
   source: nvidia

   let   s discuss the all encompassing ai first. you probably know the
   turing test already. a computer passes the turing test if a human,
   after posing some written questions to the computer, cannot tell
   whether the written responses come from another human or the computer.
   according to artificial intelligence: a modern approach, peter norvig
   and stuart russell define the 4 capabilities a computer must command in
   order to pass the turing test:
     * natural language processing: to communicate successfully in english
     * id99: to store what the computer reads
     * automated reasoning: to use the stored knowledge to answer
       questions and draw new conclusions
     * machine learning: to adapt to new circumstances and to identify new
       patterns

   ah, there   s the term    machine learning   ! ml is about training the
   learning algorithms like id75, knn, id116, decision
   trees, id79, and id166 with datasets, so that the algorithms
   could learn to adapt to a new situation and find patterns that might be
   interesting and important. again, ml is data-driven. a lot of strange
   terms for the learning algorithms? no worries, i don   t know all of them
   either. so we   ll learn together in the future.

   for training ml, the dataset can be labeled, e.g. it comes with an
      answer sheet   , telling the computer what the right answer is, like
   which emails are spams and which are not. this is called a supervised
   learning and algorithms like id75 and knn are used for
   such supervised regression or classification. other datasets might not
   be labeled, and you are literally telling the algorithm such as id116
   to associate or cluster patterns that it finds without any answer
   sheet. this is called unsupervised learning. here   s a [29]great answer
   to supervised vs. unsupervised learning on stack overflow and here   s
   also a [30]post on supervised vs unsupervised from [31]olivia klose   s
   blog.
   [1*3nfpt9ooadxzgppsy6h7vq.png]
   source: [32]http://oliviaklose.com/

   norvig and russell also mention another test called the total turing
   test, which further examines the computer   s perceptual abilities with
   physical simulation. to pass this one, the computer needs:
     * id161: to perceive objects in the surroundings
     * robotics: to manipulate objects and move around

   so what about dl now? remember the general tasks that deep learning is
   good at from the section above? things like face or handwritten text
   recognition have to do with id161 because you are feeding in
   graphics to the computer for analysis. other tasks like language
   translation or id103 have to do with natural language
   processing (nlp). so dl is a sub-branch of ml in that it also has a set
   of learning algorithms that can train on and learn from data, and more
   specifically dl is powered by neural networks. moreover, dl can perform
   outside the machine learning area and comes to assist other areas like
   id161 and nlp so that hopefully ai could pass the turing test
   and total turing test one day!

   but what the heck is a neural network? is it imitating the behaviors of
   actual neuron cells? or is it some sort of a magical black box? for
   some of you, the information provided so far might feel a bit too much,
   so let   s take a break and check out some free online resources to see
   which suits you :) afterwards, we will go straight into neural
   networks.

half-way bonus: valuable resources

   neural networks and dl are often hidden behind a mysterious veil. all
   the technical jargons related to the topic might make beginners deeply
   confused. since dl will automate many tasks and replace many workers in
   the future, i personally believe it is important that we all keep an
   open mind and curiosity to learn new technology. dl can replace a
   worker who works on manual, repetitive tasks. but dl cannot replace the
   scientist or the engineer building and maintaining a dl application.

   currently there are already many great courses, tutorials, and books on
   the internet covering this topic, such as (not exhaustive or in
   specific order):
    1. michael nielsen   s [33]neural networks and deep learning
    2. geoffrey hinton   s [34]neural networks for machine learning
    3. goodfellow, bengio, & courville   s [35]deep learning
    4. ian trask   s [36]grokking deep learning,
    5. francois chollet   s [37]deep learning with python
    6. [38]udacity   s [39]deep learning nanodegree (not free but high
       quality)
    7. [40]udemy   s deep learning a-z ($10   $15)
    8. stanford   s [41]cs231n and [42]cs224n
    9. siraj raval   s [43]youtube channel

   the list goes on and on. [44]david venturi has a post for
   [45]freecodecamp that lists many more resources. check it out [46]here.

   we   re truly blessed in the age of self-education. by the way, have you
   heard about the high school student, abu qader, from chicago? this kid
   taught himself machine learning and tensorflow, a deep learning
   framework, and helped improve [47]the diagnosis of breast cancer to
   93%-99% real-time accuracy! he was featured on google i/o 2017. below
   is an inspiring video of his story from google.

   iframe: [48]/media/a53c6407345f88cceddc9814efd2ef1d?postid=bd68f9cf5883

   source: google

id88: the prelude of dl

   alright, i hope abu qader   s story makes you excited about learning!
   let   s get to the second main topic of this post: an introduction to
   neural networks. the ancient chinese philosopher lao tzu once said:

        the journey of a thousand miles begins with one step.   

   so we will begin and end this post with a very simple neural network.
   sounds cool? wunderbar.

   despite its new-found fame, the field of neural networks isn   t new at
   all. in 1958, [49]frank rosenblatt, an american psychologist, attempted
   to build    [50]a machine which senses, recognizes, remembers, and
   responds like the human mind    and called the machine a id88. but
   rosenblatt didn   t invent id88s out of thin air. actually, he
   stood on the shoulders of giants and was inspired by previous works
   from warren mcculloch and walter pitts in the 1940s. gosh, that makes
   neural networks or, more specifically id88, a dinosaur in this
   fast-changing world of technology.
   [1*g_wt9ysmyb9ivnalw50xqw.jpeg]
   rosenblatt and id88, source: the new yorker

   so let   s see what a id88 is. first, have a look at the neuron
   cell below: the dendrites are extensions of the nerve cell (in the left
   lower corner of the graph). they receive signals and then transmit the
   signals to the cell body, which processes the stimulus and decide
   whether to trigger signals to other neuron cells. in case this cell
   decides to trigger signals, the extension on the cell body called axon
   will triggers chemical transmission at the end of the axon to other
   cells. you don   t have to memorize anything here. we are not studying
   neuroscience, so a vague impression of how it works will be enough.
   [1*z0slghighjfg1ugerib30q.jpeg]
   original source: thinglink.com
   [1*zot34fhnrrpxwhsxv6mk8q.jpeg]
   a single id88

   now, above is a graph of how a id88 looks like. pretty similar to
   the nerve cell graph above, right? indeed. id88s and other neural
   networks are inspired by real neurons in our brain. note it   s only
   inspired and does not work exactly like real neurons. the procedure of
   a id88 processing data is as follows:
    1. on the left side you have neurons (small circles) of x with
       subscripts 1, 2,     , m carrying data input.
    2. we multiply each of the input by a weight w, also labeled with
       subscripts 1, 2,    , m, along the arrow (also called a synapse) to
       the big circle in the middle. so w1 * x1, w2 * x2, w3 * x3 and so
       on.
    3. once all the the inputs are multiplied by a weight, we sum all of
       them up and add another pre-determined number called bias.
    4. then, we push the result further to the right. now, we have this
       step function in the rectangle. what it means is that if the result
       from step 3 is any number equal or larger than 0, then we get 1 as
       output, otherwise if the result is smaller than 0, we get0 as
       output.
    5. the output is either 1 or 0.

   note that alternatively, if you move bias to the right side of the
   equation in the activation function like sum(wx)     -b then this -b is
   called a threshold value. so if the sum of the inputs and weights is
   greater than or equal to the threshold, then the activation triggers an
   1. otherwise, the activation outcome is 0. choose whichever that helps
   you to understand better as these two ways of representation are
   interchangeable.
   [1*nzflr7z4x5io0owgpelw-g.jpeg]
   illustration of using threshold value instead of bias

   i add another id88 graph below, this time with each step colored.
   it   s very important that you fully understand it and remember what is
   happening at each step because we will ignore the middle steps in
   future graphs when we talk about more complicated neural network
   structures:
   [1*-owhnqj0hjipxyeauy8k8a.jpeg]
   procedures of a id88 labeled in colors
    1. inputs are fed into the id88
    2. weights are multiplied to each input
    3. summation and then add bias
    4. activation function is applied. note that here we use a step
       function, but there are other more sophisticated activation
       functions like sigmoid, hyperbolic tangent (tanh), rectifier (relu)
       and more. no worries, we will cover many of them in the future!
    5. output is either triggered as 1, or not, as 0. note we use y hat to
       label output produced by our id88 model

   in the future, we might sometimes simplify our id88 as the
   following without mentioning steps 3 and 4. this type of id88 we
   just talked about is also a single-layer id88 since we process
   the inputs directly into an output without any more layers of neurons
   in the middle:
   [1*74yd-gadyb8xc7mq36apfa.jpeg]
   simplified representation of id88

id88: intuition

   okay, you know how a id88 works now. it   s just some mechanical
   multiplications followed by summation and then some activation   voil  
   you get an output. yeah, how on earth is this anything close to neurons
   in human brain?

   for the sake of understanding id88s, let   s look at a very easy
   example that isn   t necessarily realistic. suppose you are very
   motivated after reading my post and you need to decide whether to study
   dl or not. there are 3 factors that influence your decision:
    1. if you will earn more money after mastering dl (yes: 1, no: 0)
    2. is the relevant mathematics and programming easy (yes: 1, no: 0)
    3. you can work on dl immediately without the need for an expensive
       gpu (yes: 1, no: 0)

   we use x1, x2, and x3 as input variables for each of these factors and
   assign a binary value (1 or 0) to each of them since the answer is
   simply yes or no. suppose you really like dl so far and you are willing
   to overcome your life-long fear of math and programming. and you also
   have some savings invest now in a pricey nvidia gpu to train your dl
   models. assume these two factors are equally less important as you can
   compromise on them. however, you really want to earn more money after
   spending so much time and energy in learning dl. so with a high
   expectation of return on investment, if you can   t earn more $$$
   afterwards, you won   t waste your precious time on dl.

   with an understanding of your decision-making preference, let   s assume
   you have a 100% id203 of earning more money after learning dl
   because there   s a lot of demand for very little supply in the market.
   so x1 = 1. let   s say the math and programming is super hard. so x2 = 0.
   finally, let   s say you must have a powerful gpu like titan x. so x3 =
   0. ok, we have the inputs ready and can also initialize the weights.
   let   s go for w1 = 6, w2 = 2, w3 = 2. the larger the weight, the more
   influential the corresponding input is. so since you value money the
   most for your decision to learn dl, w1 > w2 and w1 > w3.

   we   ll assume the threshold value threshold = 5, which is equivalent to
   say that the bias term bias = -5. we add it all up and plus the bias
   term. check the following for the process of determining whether you
   will learn dl or not by using a id88.
   [1*gq5_fya3jkqo248p9q3wiq.jpeg]

   note with a threshold value of 5, we will only learn deep learning if
   we earn more money. even if both the math is easy (x2 = 1) and you
   don   t need to spend money buying a gpu (x3 = 1), you will still not
   study dl if you can   t earn more money later. see the illustration
   below:
   [1*yjh0vfjw_iysk5-hh8uxag.jpeg]

   now you know the trick with bias/threshold. this high threshold of 5
   means your dominating factor must be satisfied for the id88 to
   trigger an output of 1. otherwise, the output will be 0.

   here   s the fun part: varying the weights and threshold/bias will result
   in different possible decision-making models. so if, for instance, we
   lower the threshold from threshold = 5 to threshold = 3, then we have
   more possible scenarios for the output to be 1. now, the bare minimum
   requirement for output = 1 is:
    1. you will earn more money afterwards, so x1 = 1 guarantees your
       decision to learn dl regardless of the values to x2 and x3
    2. or, the math is easy and you don   t need to buy gpu, so x2 = x3 = 1
       also guarantees that you decide to learn dl regardless of the value
       to x1

   how so? you probably know already ;) below is the explanation:
   [1*r-4inufd0zfrkib3ktifdw.jpeg]

   yup. the threshold is lower now so the other 2 factors could motivate
   you to learn dl even if your prospect of earning more money was gone.
   now, i encourage you to play around with the weights w1, w2, and w3 and
   see how your decision on learning dl will change accordingly!

id88: learning in action

   why should you play around with the weights on this example? because it
   helps you to understand how id88 learns. now, we   ll use this
   example together with the inputs and the weights to illustrate the
   single-layer id88 and see what it can achieve despite its
   limitations.

   in a real dl model, we are given input data, which we can   t change.
   meanwhile, the bias term is initialized before you train your neural
   networks model. so suppose we assume the bias is 7. now, let   s assume
   the following input data so that (1). you will earn more money, (2).
   math and programming for dl will be hard, and (3). yes you have to
   spend $1400 for a gpu to work on dl, and most importantly, we assume
   that you actually want to learn deep learning, which we   ll name as the
   desired output for how ideally the id88 should correctly predict
   or determine:
   [1*rfftfkqbhau6puoiwjdcoq.jpeg]

   let   s further assume that our weights are initialized at the following:
   [1*gizmwroyk520mj4htah5rw.jpeg]

   so with input data, bias, and the output label (desired output):
   [1*uz_aoislv3x7fpo3v2oueq.jpeg]

   okay, we know that the actual output from your neural network differs
   from your real decision of wanting to study dl. so what should the
   neural network do to help itself learn and improve, given this
   difference between actual and desired output? yup, we can   t change
   input data, and we have initialized our bias now. so the only thing we
   can do is that we can tell the id88 to adjust the weights! if we
   tell the id88 to increase w1 to 7, without changing w2 and w3,
   then:
   [1*zqsiecfrbqs4gqyx22gpsg.jpeg]

   adjusting the weights is the key to the learning process of our
   id88. and single-layer id88 with step function can utilize
   the learning algorithm listed below to tune the weights after
   processing each set of input data. try updating the weights with this
   algorithm yourself :) this is pretty much how a single-layer id88
   learns.
   [1*neipz37gavqn_6xqfjrlug.jpeg]

   by the way, now as we finished our not-so-realistic example, i can tell
   you that you don   t need to buy a gpu. if you are training smaller
   datasets as a beginner, you most likely won   t need a gpu. however, you
   can use cloud services like [51]aws, [52]floyd, and possibly [53]google
   tpu) instead of a real gpu when you start training larger datasets with
   a lot of image files.

   also, the math for dl isn   t easy but also not insurmountable. mostly,
   we will just encounter some matrix operations and basic calculus. but
   remember, nothing that makes you stand out is easy to learn. here   s a
   quote from talent is overated:

        doing things we know how to do well is enjoyable, and that   s
     exactly the opposite of what deliberate practice demands. instead of
     doing what we   re good at, we should insistently seek out what we   re
     not good at. then we identify the painful, difficult activities that
     will make us better and do those things over and over. if the
     activities that lead to greatness were easy and fun, then everyone
     would do them and they would not distinguish the best from the
     rest.   

   so to those of you who are scared of math and programming like i used
   to be, i hope this quote gives you some courage to keep learning and
   practicing :)

id88: limitations

   despite some early sensations from the public, id88   s popularity
   faded away quietly because of its limitations. in 1969, marvin minsky
   and seymour papert discussed these limitations, including id88   s
   inability to learn an xor (exclusive-or) gate (so basically a
   single-layer id88 with step function cannot understand the logic
   that the weather has to be either hot or cold, but not both). and these
   logic gates like and, or, not, xor are very important concepts that are
   powering your computer ;) tutorialspoint has a list of logic gates if
   you want to learn more. check [54]here.

   of course, later people realized that multi-layer id88s are
   capable of learning the logic of an xor gate, but they require
   something called id26 for the network to learn from trials
   and errors. after all, remember that deep learning neural networks are
   data-driven. if we have a model and its actual output is different from
   the desired output, we need a way to back-propagate the error
   information along the neural network to tell weights to adjust and
   correct themselves by a certain value so that gradually the actual
   output from the model gets closer to the desired output after rounds
   and rounds of testing.

   as it turns out, for more complicated tasks that involve outputs which
   cannot be produced from a linear combination of inputs (so the outputs
   are non-linear or not linearly separable), step function won   t work
   because it doesn   t support id26, which require the chosen
   activation function to have meaningful derivative.
   [1*8hhgigckltmzxetj05yzow.png]
   source: mathnotes.org

   some calculus talk: step function is a linear activation function whose
   derivative is zero for all input points except the point zero. at point
   zero, the derivative is undefined since the function is discontinuous
   at point zero. so although it is a very simple and easy activation
   function, it cannot handle more complicated tasks. keep reading and
   you   ll find out more.

linear vs. non-linear

   what!? so what is a linear combination? and why can   t a id88
   learn an xor gate? this is getting confusing now. no problem, here   s an
   explanation:
   [1*75nclk9p6gb4qg7casy66w.jpeg]

   think for a moment about our previous example. with 3 binary inputs for
   whether you earn more money after learning dl, if the math and
   programming involved are easy or not, and if you can learn dl without
   investing in expensive hardware, we have a total of 2   = 8 possible
   sets of inputs and outcomes. with weights (w1 = 6, w2 = 2, w3 = 2), and
   bias = -5, we have the following table for a set of (x1, x2, x3):
    1. (1, 0, 0) -> sum + bias = 6   5 = 1, desired output = 1
    2. (1, 1, 0) -> sum + bias = 8   5 = 3, desired output = 1
    3. (1, 1, 1) -> sum + bias = 10   5 = 5, desired output = 1
    4. (1, 0, 1) -> sum + bias = 8   5 = 3, desired output = 1
    5. (0, 1, 1) -> sum + bias = 4   5 = -1, desired output = 0
    6. (0, 1, 0) -> sum + bias = 2   5 = -3, desired output = 0
    7. (0, 0, 1) -> sum + bias = 2   5 = -3, desired output = 0
    8. (0, 0, 0) -> sum + bias = 0   5 = -5, desired output = 0

   so literally, if we start with random weights than the ones of (w1 = 6,
   w2 = 2, w3 = 2), our id88 will try to learn to adjust and find
   the ideal weights (w1 = 6, w2 = 2, w3 = 2), which would correctly match
   the actual output of each set of (x1, x2, x3) to the desired output. in
   fact, we can separate these 8 possible sets in a 3d space with a plane,
   such as the plane of x1 = 0.5. this type of classification problem
   where you can draw a plane to separate different outputs (or draw a
   line for outputs in 2d) is a problem that our single-layer id88
   can solve.
   [1*gvfx88bue3hi912p5xeisa.png]
   a plane separating the 4 sets to the left from the 4 to the right
   [1*iw4pazhze3koovy-7vtjbq.png]
   a plane separating the 4 sets to the left from the 4 to the right

   i hope you can imagine the plane separating the 8 sets of inputs above.
   since the sets of (x1, x2, x3) involves a 3-dimensional space, it can
   be a bit challenging. but in general, a single-layer id88 with a
   linear activation function can learn to separate a dataset like the
   chart a in the graph below, which is separable by a line of y = ax + b.
   but if the dataset is only separable non-linearly by a circle like in
   chart b, our id88 will perform horribly.
   [1*xpptw4l43vtjhinqsqokrg.png]
   source: sebastian raschka

   why is it so? i copy paste a quote from stack overflow for you. you can
   check out the answer [55]here as well.

        id180 cannot be linear because neural networks with
     a linear activation function are effective only one layer deep,
     regardless of how complex their architecture are. input to networks
     are usually linear transformation (input * weight), but real world
     and problems are non-linear. to make the incoming data nonlinear, we
     use nonlinear mapping called activation function. an activation
     function is a decision making function that determines the presence
     of particular neural feature. it is mapped between 0 and 1, where
     zero mean the feature is not there, while one means the feature is
     present. unfortunately, the small changes occurring in the weights
     cannot be reflected in the activation value because it can only take
     either 0 or 1. therefore, nonlinear functions must be continuous and
     differentiable between this range. a neural network must be able to
     take any input from -infinity to +infinite, but it should be able to
     map it to an output that ranges between {0,1} or between {-1,1} in
     some cases         thus the need for activation function. non-linearity is
     needed in id180 because its aim in a neural network
     is to produce a nonlinear decision boundary via non-linear
     combinations of the weight and inputs.    -user7479

   so why can   t a single-layer id88 with step function learn the xor
   gate? check out the graph below. on the left is an xor logic chart, and
   on the right is a cartesian representation of the 2 different outcomes
   (1 and 0) from an xor gate.
   [1*gdf_i5bxuutx-yr1axqxnq.gif]
   source: [56]https://stackoverflow.com/a/35919708/6297414

   indeed, we cannot possibly separate the white dots from the black dots
   with a single, linear line. we need to have something more powerful to
   let a neural network learn the xor gate logic. i will write more on
   this topic soon.

recap

   deep learning is an exciting field that is rapidly changing our
   society. we should care about deep learning and it is fun to understand
   at least the basics of it. we also introduced a very basic neural
   network called (single-layer) id88 and learned about how the
   decision-making model of id88 works.

   our single-layer id88 plus a step function works fine for simple
   linearly-separable binary classification. but that   s about it   as it
   won   t work very well for more complicated problems involving non-linear
   outputs. i know this sounds a bit disappointing, but we will cover them
   in detail next!

   coming next will be a [57]new post on neural networks with hidden
   layers (sounds fancy right?) and a new activation function called the
   sigmoid function. if space allows, we will touch on id119
   and id26, which are the key concepts for a smart neural
   network to learn. stay tuned ([58]here   s the link to the second
   post) :)

   for now, congratulations on following the post so far and you   ve
   already learned a lot now! stay motivated and i hope you are having fun
   with deep learning! if you are impatient to wait for my writing, check
   out the following free resources to learn more already:
   [59]neural networks and deep learning
   we're focusing on handwriting recognition because it's an excellent
   prototype problem for learning about
   neural   neuralnetworksanddeeplearning.com
   [60]cs231n convolutional neural networks for visual recognition
   course materials and notes for stanford class cs231n: convolutional
   neural networks for visual recognition.cs231n.github.io

   iframe: [61]/media/36e5efa7870eda30cca750c19485cfbe?postid=bd68f9cf5883

   enjoy learning!

   did you enjoy this reading? don   t forget to follow me on [62]twitter!

     * [63]deep learning
     * [64]machine learning
     * [65]neural networks
     * [66]artificial intelligence
     * [67]towards data science

   (button)
   (button)
   (button) 1.1k claps
   (button) (button) (button) 19 (button) (button)

     (button) blockedunblock (button) followfollowing
   [68]go to the profile of nahua kang

[69]nahua kang

   marketer [70]@twentybn. write about history, biographies, literature,
   and tech. twitter: [71]https://twitter.com/nahuakang. newsletter:
   [72]https://nahua.substack.com

     (button) follow
   [73]towards data science

[74]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.1k
     * (button)
     *
     *

   [75]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [76]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/bd68f9cf5883
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introducing-deep-learning-and-neural-networks-deep-learning-for-rookies-1-bd68f9cf5883&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introducing-deep-learning-and-neural-networks-deep-learning-for-rookies-1-bd68f9cf5883&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ufjrqtkbh6tl---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@nahua?source=post_header_lockup
  17. https://towardsdatascience.com/@nahua
  18. https://twitter.com/nahuakang
  19. http://nahuakang.com/2017/06/21/deep-learning-for-rookies-1/
  20. https://medium.com/@the_economist
  21. http://www.economist.com/news/briefing/21721634-how-it-shaping-up-data-giving-rise-new-economy
  22. https://medium.com/@trentmc0
  23. https://blog.bigchaindb.com/blockchains-for-artificial-intelligence-ec63b0284984
  24. http://www.newyorker.com/magazine/2017/04/03/ai-versus-md
  25. https://medium.com/@ophir.samson02
  26. https://medium.com/@intuitmachine
  27. https://medium.com/towards-data-science/deep-learning-weekly-piece-the-differences-between-ai-ml-and-dl-b6a203b70698
  28. https://medium.com/intuitionmachine/why-deep-learning-is-radically-different-from-machine-learning-945a4a65da4d
  29. https://stackoverflow.com/a/1854449/6297414
  30. http://oliviaklose.com/machine-learning-2-supervised-versus-unsupervised-learning/
  31. https://medium.com/@oliviaklose
  32. http://oliviaklose.com/
  33. http://neuralnetworksanddeeplearning.com/
  34. https://www.coursera.org/learn/neural-networks
  35. http://www.deeplearningbook.org/
  36. https://www.manning.com/books/grokking-deep-learning
  37. https://www.manning.com/books/deep-learning-with-python
  38. https://medium.com/@udacity
  39. https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101
  40. https://www.udemy.com/deeplearning/learn/v4/overview
  41. http://cs231n.stanford.edu/index.html
  42. http://web.stanford.edu/class/cs224n/
  43. https://www.youtube.com/channel/ucwn3xxrkmtpmbkwht9fue5a
  44. https://medium.com/@davidventuri
  45. https://medium.com/@freecodecamp
  46. https://medium.freecodecamp.com/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0
  47. http://chicagoinno.streetwise.co/2016/07/13/this-chicago-high-school-student-uses-artificial-intelligence-to-make-smarter-breast-cancer-diagnoses/
  48. https://towardsdatascience.com/media/a53c6407345f88cceddc9814efd2ef1d?postid=bd68f9cf5883
  49. https://en.wikipedia.org/wiki/frank_rosenblatt#id88
  50. http://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence
  51. https://aws.amazon.com/
  52. https://www.floydhub.com/
  53. https://cloud.google.com/tpu/
  54. https://www.tutorialspoint.com/computer_logical_organization/logic_gates.htm
  55. https://stackoverflow.com/a/35919708/6297414
  56. https://stackoverflow.com/a/35919708/6297414
  57. https://medium.com/towards-data-science/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f
  58. https://medium.com/towards-data-science/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f
  59. http://neuralnetworksanddeeplearning.com/chap1.html
  60. http://cs231n.github.io/
  61. https://towardsdatascience.com/media/36e5efa7870eda30cca750c19485cfbe?postid=bd68f9cf5883
  62. https://twitter.com/nahuakang
  63. https://towardsdatascience.com/tagged/deep-learning?source=post
  64. https://towardsdatascience.com/tagged/machine-learning?source=post
  65. https://towardsdatascience.com/tagged/neural-networks?source=post
  66. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  67. https://towardsdatascience.com/tagged/towards-data-science?source=post
  68. https://towardsdatascience.com/@nahua?source=footer_card
  69. https://towardsdatascience.com/@nahua
  70. http://twitter.com/twentybn
  71. https://twitter.com/nahuakang
  72. https://nahua.substack.com/
  73. https://towardsdatascience.com/?source=footer_card
  74. https://towardsdatascience.com/?source=footer_card
  75. https://towardsdatascience.com/
  76. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  78. http://neuralnetworksanddeeplearning.com/chap1.html
  79. http://cs231n.github.io/
  80. https://medium.com/p/bd68f9cf5883/share/twitter
  81. https://medium.com/p/bd68f9cf5883/share/facebook
  82. https://medium.com/p/bd68f9cf5883/share/twitter
  83. https://medium.com/p/bd68f9cf5883/share/facebook
