   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

multi-layer neural networks with sigmoid function    deep learning for
rookies (2)

   [16]go to the profile of nahua kang
   [17]nahua kang (button) blockedunblock (button) followfollowing
   jun 27, 2017

   chapter 1: [18]introducing deep learning and neural networks

   chapter 2: multi-layer neural networks with sigmoid function

   follow me on [19]twitter to learn more about life in a deep learning
   startup.
     __________________________________________________________________

   howdy y   all! welcome back to my second post of the series deep learning
   for rookies (dlfr), by yours truly, a rookie ;) feel free to refer back
   to [20]my first post here or [21]my blog if you find it hard to follow.
   or highlight on this page with notes or leave a comment below! your
   feedback will be highly appreciated, too.

   we will go deeper into neural networks this time and the post will be
   slightly more technical than last time. but no worries, i will make it
   as easy and intuitive as possible for you to learn the basics without
   cs/math background. you   ll be able to brag about your understanding
   soon ;)

review from dlfr 1

   last time, we introduced the field of deep learning and examined a
   simple a neural network         id88      or a dinosaur      ok, seriously, a
   single-layer id88. we also examined how a id88 network
   process the input data we feed in and returns an output.

   key concepts: input data, weights, summation and adding bias,
   activation function (specifically step function), and then output.
   bored yet? no worries :) i promise there will be      more jargons coming
   up! but you   ll get used to them soon. i promise.
   [1*v88yssmr7jlaibjwr4chtw.jpeg]
   graph 1: procedures of a single-layer id88 network

   back in the 1950s and 1960s, people had no effective learning algorithm
   for a single-layer id88 to learn and identify non-linear patterns
   (remember the xor gate problem?). and the public lost interest in
   id88. after all, most problems in the real world are non-linear,
   and as individual humans, you and i are pretty darn good at the
   decision-making of linear or binary problems like should i study deep
   learning or not without needing a id88. ok,    good    is a tricky
   word here as our brains are really not so rational. but i   ll leave that
   to behavioral economists and psychologists.

breakthrough: multi-layer id88

   fast forward almost two decades to 1986, geoffrey hinton, david
   rumelhart, and ronald williams published a paper    learning
   representations by back-propagating errors   , which introduced:
    1. id26, a procedure to repeatedly adjust the weights so as
       to minimize the difference between actual output and desired output
    2. hidden layers, which are neuron nodes stacked in between inputs and
       outputs, allowing neural networks to learn more complicated
       features (such as xor logic)

   if you are completely new to dl, you should remember geoffrey hinton,
   who plays a pivotal role in the progress of dl. so that was some major
   news: we just thought for 20 years that there   s no future for neural
   networks to tackle real-world problems. now we see the light from a
   beacon ashore! let   s take a look at these 2 new introductions.

   id48, the first one, id26, is mentioned in the last post.
   remember that we iterated the importance of designing a neural network
   so that the network can learn from the difference between the desired
   output (what the fact is) and actual output (what the network returns)
   and then send a signal back to the weights and ask the weights to
   adjust themselves? this will make the network   s output closer to the
   desired output next time we run it.

   what about the second one, hidden layers? what is a hidden layer? a
   hidden layer transforms a single-layer id88 into a multi-layer
   id88! here   s the plan, for technical reasons, i will focus on
   hidden layers in this post, and then discuss id26 in the
   next post.

   just a random fun fact here: i suspect that geoffrey hinton holds a
   record for having been the oldest intern at google :d check out
   [22]this article from new york times and search    hinton   . anyways, if
   you   re already familiar with machine learning, i   m sure his course on
   coursera will be a good fit.
   [1*hkfw7skdwinmzusucs7d6g.jpeg]
   geoffrey hinton, one of the most important figures in deep learning.
   source: thestar.com

neural networks with hidden layers

   hidden layers of a neural network is literally just adding more neurons
   in between the input and output layers.
   [1*cjeby3gcagqknx7pey-w5w.jpeg]
   graph 2: left: single-layer id88; right: id88 with
   hidden layer

   data in the input layer is labeled as x with subscripts 1, 2, 3,    , m.
   neurons in the hidden layer are labeled as h with subscripts 1, 2,
   3,    , n. note for hidden layer it   s n and not m, since the number of
   hidden layer neurons might differ from the number in input data. and as
   you see in the graph below, the hidden layer neurons are also labeled
   with superscript 1. this is so that when you have several hidden
   layers, you can identify which hidden layer it is: first hidden layer
   has superscript 1, second hidden layer has superscript 2, and so on,
   like in graph 3. output is labeled as y with a hat.
   [1*7mzxwa3zvtzdtfzfohsqbg.jpeg]
   graph 3: we label input layer as x with subscripts 1, 2,    , m; hidden
   layer as h with subscripts 1, 2,    , n; output layer with a hat

   to make life easier, we will use some jargons to clear things out a
   bit. i know, jargons can be annoying but you will get used to them :)
   first, if we have m input data (x1, x2,    , xm), we call this m
   features. a feature is just one variable we consider as having an
   influence to a specific outcome. like our example on the outcome of if
   you decide to learn dl or not, we have 3 features: 1. will you earn
   more money after learning dl, 2. is the math/programming hard, 3. do
   you need a gpu to start with.

   secondly, when we multiply each of the m features with a weight (w1,
   w2,    , wm) and sum them all together, this is a dot product:
   [1*fywj1tic42lf98wc7xyy6w.jpeg]
   definition of a dot product

   so here are the takeaways for now:
    1. with m features in input x, you need m weights to perform a dot
       product
    2. with n hidden neurons in the hidden layer, you need n sets of
       weights (w1, w2,     wn) for performing dot products
    3. with 1 hidden layer, you perform n dot products to get the hidden
       output h: (h1, h2,    , hn)
    4. then it   s just like a single-layer id88, we use hidden output
       h: (h1, h2,    , hn) as input data that has n features, perform dot
       product with 1 set of n weights (w1, w2,    , wn) to get your final
       output y_hat.

   the procedure of how input values are forward propagated into the
   hidden layer, and then from hidden layer to the output is the same as
   in graph 1. below i provide a description of how this is done, using
   the following neural network in graph 4.
   [1*_m4bzyuwagby6kmiyvyxvg.jpeg]
   graph 4: example
   [1*mpp9dnu9xr5mkccs9rg1nw.jpeg]
   [1*m6nfcyovyoid182d5-l2mdq.jpeg]
   graph 5: procedure to hidden layer outputs

   now the hidden layer outputs are calculated, we use them as inputs to
   calculate the final output.
   [1*2yix_-go-mhqbrb9-jpa5q.jpeg]
   graph 6: procedure to final output

   yay! now you understand fully how a id88 with multiple layers
   work :) it is just like a single-layer id88, except that you have
   many many more weights in the process. when you are training neural
   networks on larger datasets with many many more features (like id97
   in natural language processing), this process will eat up a lot of
   memory in your computer. this was one reason why deep learning didn   t
   take off until the past few years, when we began producing much better
   hardware that could handle the memory-consuming deep neural networks.

sigmoid neurons: an introduction

   so now we have a more sophisticatedly structured neural network with
   hidden layers. but we haven   t solved the activation problem with the
   step function.

   in the last post, we talked about the limitations of the linearity of
   step function. one thing to remember is: if the activation function is
   linear, then you can stack as many hidden layers in the neural network
   as you wish, and the final output is still a [23]linear combination of
   the original input data. please [24]make sure you read this link for an
   explanation if the concept is difficult to follow. this linearity means
   that it cannot really grasp the complexity of non-linear problems like
   xor logic or patterns separated by curves or circles.

   meanwhile, step function also has no useful derivative (its derivative
   is 0 everywhere or undefined at the 0 point on x-axis). it doesn   t work
   for id26, which we will definitely talk about in the next
   post!
   [1*xtrzobdx2l0cyyecylfweg.jpeg]
   graph 7: step function

   well, here   s another problem: id88 with step function isn   t very
      stable    as a    relationship candidate    for neural networks. think about
   it: this girl (or boy) has got some serious bipolar issues! one day
   (for z < 0), (s)he   s all    quiet    and    down   , giving you zero response.
   then another day (for z     0), (s)he   s suddenly    talkative    and
      lively   , speaking to you nonstop. heck of a drastic change! there   s no
   transition for her/his mood, and you don   t know when it   s going down or
   up. yeah   that   s step function.

   so basically, a small change in any weight in the input layer of our
   id88 network could possibly lead to one neuron to suddenly flip
   from 0 to 1, which could again affect the hidden layer   s behavior, and
   then affect the final outcome. like we said already, we want a learning
   algorithm that could improve our neural network by gradually changing
   the weights, not by flat-no-response or sudden jumps. if we can   t use
   step function to gradually change the weights, then it shouldn   t be the
   choice.
   [1*jyhcwrljlsjq7qfypgl1ya.jpeg]
   graph 8: we want gradual change in weights to gradually change outputs

   say goodbye to id88 with step function now. we are finding a new
   partner for our neural network, the sigmoid neuron, which comes with
   sigmoid function (duh). but no worries: the only thing that will change
   is the activation function, and everything else we   ve learned so far
   about neural networks still works for this new type of neuron!
   [1*tdlbjnoh3gckwaifv1asxg.jpeg]
   sigmoid function
   [1*sotpvyq2msjxz51xmn1qsa.png]
   graph 9: sigmoid function using matplotlib

   if the function looks very abstract or strange to you, don   t worry too
   much about the details like euler   s number e or how someone came up
   with this crazy function in the first place. for those who aren   t
   math-savvy, the only important thing about sigmoid function in graph 9
   is first, its curve, and second, its derivative. here are some more
   details:
    1. sigmoid function produces similar results to step function in that
       the output is between 0 and 1. the curve crosses 0.5 at z=0, which
       we can set up rules for the activation function, such as: if the
       sigmoid neuron   s output is larger than or equal to 0.5, it outputs
       1; if the output is smaller than 0.5, it outputs 0.
    2. sigmoid function does not have a jerk on its curve. it is smooth
       and it has a very nice and simple derivative of   (z) * (1-  (z)),
       which is differentiable everywhere on the curve. the calculus
       derivation of the derivative can be found on stack overflow
       [25]here if you want to see it. but you don   t have to know how to
       derive it. no stress here.
    3. if z is very negative, then the output is approximately 0; if z is
       very positive, the output is approximately 1; but around z=0 where
       z is neither too large or too small (in between the two outer
       vertical dotted grid lines in graph 9), we have relatively more
       deviation as z changes.

   now that seems like a dating material for our neural network :) sigmoid
   function, unlike step function, introduces non-linearity into our
   neural network model. non-linear just means that the output we get from
   the neuron, which is the dot product of some inputs x (x1, x2,    , xm)
   and weights w (w1, w2,    ,wm) plus bias and then put into a sigmoid
   function, cannot be represented by a [26]linear combination of the
   input x (x1, x2,    ,xm).

   this non-linear activation function, when used by each neuron in a
   multi-layer neural network, produces a new    [27]representation    of the
   original data, and ultimately allows for non-linear decision boundary,
   such as xor. so in the case of xor, if we add two sigmoid neurons in a
   hidden layer, we could, in another space, reshape the original 2d graph
   into something like the 3d image in the left side of graph 10 below.
   this ridge thus allows for classification of the xor gate and it
   represents the light yellowish region of the 2d xor gate in the right
   side of graph 10. so if our output value is on the higher area of the
   ridge, then it should be a true or 1 (like the weather is cold but not
   hot, or the weather is hot but not cold); if our output value is on the
   lower flat area on the two corners, then it   s false or 0 since it   s not
   right to say the weather is both hot and cold or neither hot or cold
   (ok, i guess the weather could be neither hot or cold   you get what i
   mean though   right?).
   [1*eb12oynamqfdus2ln75yuq.jpeg]
   graph 10. representation of neural networks with hidden layers to
   classify xor gate. source: [28]http://colinfahey.com/

   i know these talks on non-linearity can be confusing, so please read
   more about linearity & non-linearity [29]here (intuitive post with
   animation from a great blog by christopher olah), [30]here (by
   [31]vivek yadav with relu activation function), and [32]here (by
   sebastian raschka). hopefully you have a sense of why non-linear
   activation function is important, and if not, take it easy and allow
   some time to digest it.

   problem solved      for now ;) we will see some different types of
   activation function in the near future because sigmoid function has its
   own issues, too! some popular ones include tanh and relu. that,
   however, is for another post.

multi-layer neural networks: an intuitive approach

   alright. so we   ve introduced hidden layers in a neural network and
   replaced id88 with sigmoid neurons. we also introduced the idea
   that non-linear activation function allows for classifying non-linear
   decision boundaries or patterns in our data. you can memorize these
   takeaways since they   re facts, but i encourage you to google a bit on
   the internet and see if you can understand the concept better (it is
   natural that we take some time to understand these concepts).

   now, we   ve never talked about one very important point: why on earth do
   we want hidden layers in neural networks in the first place? how do
   hidden layers magically help us to tackle complicated problems that
   single-layer neurons cannot do?

   from the xor example above, you   ve seen that adding two hidden neurons
   in 1 hidden layer could reshape our problem into a different space,
   which magically created a way for us to classify xor with a ridge. so
   hidden layers somehow twist the problem in a way that makes it easy for
   the neural network to classify the problem or pattern. now we   ll use a
   classic textbook example: recognition of hand-written digits, to help
   you intuitively understand what hidden layers do.
   [1*g50lvdck9w8gs_cxix3kvw.png]
   graph 11. mnist dataset of hand-written digits. source:
   [33]http://neuralnetworksanddeeplearning.com/

   the digits in graph 11 belong to a dataset called [34]mnist. it
   contains 70,000 examples of digits written by human hands. each of
   these digits is a picture of 28x28 pixels. so in total each image of a
   digit has 28*28=784 pixels. each pixel takes a value beween 0 and 255
   (rgb color code). 0 means the color is white and 255 means the color
   black.
   [1*5awqgi4yrak6dwmybs__tw.png]
   graph 12. mnist digit 5, which consist of 28x28 pixel values between 0
   and 255. source: [35]http://neuralnetworksanddeeplearning.com/

   now, the computer can   t really    see    a digit like we humans do, but if
   we dissect the image into an array of 784 numbers like [0, 0, 180, 16,
   230,    , 4, 77, 0, 0, 0], then we can feed this array into our neural
   network. the computer can   t understand an image by    seeing    it, but it
   can understand and analyze the pixel numbers that represent an image.
   [1*j7ruypq-9udhaifmmwy9ww.jpeg]
   graph 13: multi-layer sigmoid neural network with 784 input neurons, 16
   hidden neurons, and 10 output neurons

   so, let   s set up a neural network like above in graph 13. it has 784
   input neurons for 28x28 pixel values. let   s assume it has 16 hidden
   neurons and 10 output neurons. the 10 output neurons, returned to us in
   an array, will each be in charge to classify a digit from 0 to 9. so if
   the neural network thinks the handwritten digit is a zero, then we
   should get an output array of [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], the first
   output in this array that senses the digit to be a zero is    fired    to
   be 1 by our neural network, and the rest are 0. if the neural network
   thinks the handwritten digit is a 5, then we should get [0, 0, 0, 0, 0,
   1, 0, 0, 0, 0]. the 6th element that is in charge to classify a five is
   triggered while the rest are not. so on and so forth.

   remember we mentioned that neural networks become better by
   repetitively training themselves on data so that they can adjust the
   weights in each layer of the network to get the final results/actual
   output closer to the desired output? so when we actually train this
   neural network with all the training examples in mnist dataset, we
   don   t know what weights we should assign to each of the layers. so we
   just randomly ask the computer to assign weights in each layer. (we
   don   t want all the weights to be 0, which i   ll explain in the next post
   if space allows).

   this concept of randomly initializing weights is important because each
   time you train a deep learning neural network, you are initializing
   different numbers to the weights. so essentially, you and i have no
   clue what   s going on in the neural network until after the network is
   trained. a trained neural network has weights which are optimized at
   certain values that make the best prediction or classification on our
   problem. it   s a black box, literally. and each time the trained network
   will have different sets of weights.

   for the sake of argument, let   s imagine the following case in graph 14,
   which i borrow from michael nielsen   s [36]online book:
   [1*tbkzovkgpy7sbhqbapnicw.jpeg]
   graph 14. an intuitive example to understand hidden layers

   after training the neural network with rounds and rounds of labeled
   data in supervised learning, assume the first 4 hidden neurons learned
   to recognize the patterns above in the left side of graph 14. then, if
   we feed the neural network an array of a handwritten digit zero, the
   network should correctly trigger the top 4 hidden neurons in the hidden
   layer while the other hidden neurons are silent, and then again trigger
   the first output neuron while the rest are silent.
   [1*p1y_himpd3wjvkzxoxnsia.jpeg]
   graph 15. neural networks are black boxes. each time is different.

   if you train the neural network with a new set of randomized weights,
   it might produce the following network instead (compare graph 15 with
   graph 14), since the weights are randomized and we never know which one
   will learn which or what pattern. but the network, if properly trained,
   should still trigger the correct hidden neurons and then the correct
   output.
   [1*ft5wx5ssfrz8fi2qdqo-yq.jpeg]
   our quote this week ;)

   one last thing to mention: in a multi-layer neural network, the first
   hidden layer will be able to learn some very simple patterns. each
   additional hidden layer will somehow be able to learn progressively
   more complicated patterns. check out graph 16 from scientific american
   with an example of face recognition :)
   [1*3yha_hyx-3n0xvgknwruiq.jpeg]
   graph 16: each hidden layer learns more complex features. source:
   scientific american

   some awesome people made the following website for you to play around
   with neural networks and see how the hidden layers work. try it out.
   it   s really fun!
   [37]tensorflow - neural network playground
   it's a technique for building a computer program that learns from data.
   it is based very loosely on how we think
   the   playground.tensorflow.org

   [38]ophir samson wrote [39]a nice post also explaining what a neural
   network is, with pretty nice visualization, and it   s short and concise!
   [40]deep learning weekly piece: what   s a neural network?
   for this week   s piece, i   d like to focus on clarifying what a neural
   network is with a simple example i   ve put together   medium.com

recap

   in this post, we reviewed the limitations of id88, introduced
   sigmoid neurons with a new activation function called sigmoid function.
   we also talked about how multi-layer neural networks work and the
   intuition behind the hidden layers in a neural network.

   we are almost completing a full course of understanding basic neural
   networks now ;) hehe, it   s not over yet! in the next post, i will talk
   about something called id168 and also this mysterious
   id26 that we   ve only mentioned but never visited! check out
   the following links if you are impatient to wait:
   [41]cs231n convolutional neural networks for visual recognition
   course materials and notes for stanford class cs231n: convolutional
   neural networks for visual recognition.cs231n.github.io

   iframe: [42]/media/8f5eea911d7d58cd34fa3cf30201fd32?postid=bf464f09eb7f

   [43]a step by step id26 example
   background id26 is a common method for training a neural
   network. there is no shortage of papers online that   mattmazur.com

   stay tuned and, most importantly, enjoy learning :d

   did you enjoy this reading? don   t forget to follow me on [44]twitter!

     * [45]machine learning
     * [46]deep learning
     * [47]neural networks
     * [48]tech
     * [49]towards data science

   (button)
   (button)
   (button) 1.4k claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [50]go to the profile of nahua kang

[51]nahua kang

   marketer [52]@twentybn. write about history, biographies, literature,
   and tech. twitter: [53]https://twitter.com/nahuakang. newsletter:
   [54]https://nahua.substack.com

     (button) follow
   [55]towards data science

[56]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.4k
     * (button)
     *
     *

   [57]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [58]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/bf464f09eb7f
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_fpettugvek1n---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@nahua?source=post_header_lockup
  17. https://towardsdatascience.com/@nahua
  18. https://medium.com/towards-data-science/introducing-deep-learning-and-neural-networks-deep-learning-for-rookies-1-bd68f9cf5883
  19. https://twitter.com/nahuakang
  20. https://medium.com/towards-data-science/introducing-deep-learning-and-neural-networks-deep-learning-for-rookies-1-bd68f9cf5883
  21. http://nahuakang.com/2017/06/21/deep-learning-for-rookies-1/
  22. https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html
  23. https://stats.stackexchange.com/a/192073/166513
  24. https://www.quora.com/why-does-deep-learning-architectures-use-only-non-linear-activation-function-in-the-hidden-layers/answer/david-kobilnyk?srid=lfu1
  25. https://math.stackexchange.com/a/1225116/425514
  26. https://stats.stackexchange.com/a/192073/166513
  27. http://colah.github.io/posts/2014-03-nn-manifolds-topology/
  28. http://colinfahey.com/
  29. http://colah.github.io/posts/2014-03-nn-manifolds-topology/
  30. https://medium.com/@vivek.yadav/how-neural-networks-learn-nonlinear-functions-and-classify-linearly-non-separable-data-22328e7e5be1
  31. https://medium.com/@vivek.yadav
  32. https://www.quora.com/what-is-the-role-of-the-activation-function-in-a-neural-network/answer/sebastian-raschka-1?srid=lfu1
  33. http://neuralnetworksanddeeplearning.com/
  34. http://yann.lecun.com/exdb/mnist/
  35. http://neuralnetworksanddeeplearning.com/
  36. http://neuralnetworksanddeeplearning.com/chap1.html
  37. http://playground.tensorflow.org/
  38. https://medium.com/@ophir.samson02
  39. https://medium.com/towards-data-science/deep-learning-weekly-piece-whats-a-neural-network-aa0df888d8a2
  40. https://medium.com/towards-data-science/deep-learning-weekly-piece-whats-a-neural-network-aa0df888d8a2
  41. http://cs231n.github.io/optimization-2/
  42. https://towardsdatascience.com/media/8f5eea911d7d58cd34fa3cf30201fd32?postid=bf464f09eb7f
  43. https://mattmazur.com/2015/03/17/a-step-by-step-id26-example/
  44. https://twitter.com/nahuakang
  45. https://towardsdatascience.com/tagged/machine-learning?source=post
  46. https://towardsdatascience.com/tagged/deep-learning?source=post
  47. https://towardsdatascience.com/tagged/neural-networks?source=post
  48. https://towardsdatascience.com/tagged/tech?source=post
  49. https://towardsdatascience.com/tagged/towards-data-science?source=post
  50. https://towardsdatascience.com/@nahua?source=footer_card
  51. https://towardsdatascience.com/@nahua
  52. http://twitter.com/twentybn
  53. https://twitter.com/nahuakang
  54. https://nahua.substack.com/
  55. https://towardsdatascience.com/?source=footer_card
  56. https://towardsdatascience.com/?source=footer_card
  57. https://towardsdatascience.com/
  58. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  60. http://playground.tensorflow.org/
  61. https://medium.com/towards-data-science/deep-learning-weekly-piece-whats-a-neural-network-aa0df888d8a2
  62. http://cs231n.github.io/optimization-2/
  63. https://mattmazur.com/2015/03/17/a-step-by-step-id26-example/
  64. https://medium.com/p/bf464f09eb7f/share/twitter
  65. https://medium.com/p/bf464f09eb7f/share/facebook
  66. https://medium.com/p/bf464f09eb7f/share/twitter
  67. https://medium.com/p/bf464f09eb7f/share/facebook
