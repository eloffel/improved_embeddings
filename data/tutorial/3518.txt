   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   go to the profile of jason roell
   [16]jason roell (button) blockedunblock (button) followfollowing
   jun 26, 2017

   understanding recurrent neural networks: the preferred neural network
   for time-series data

   artificial intelligence has been in the background for decades, kicking
   up dust in the distance, but never quite arriving. well that era is
   over. in 2017, ai has broken through the dust cloud and [17]arrived in
   a big way. but why? what   s the big deal all of a sudden? and what do
   recurrent neural networks have to do with it? well, a lot, actually.
   thanks to an ingenious form of short-term memory that is unheard of in
   conventional neural networks, today   s recurrent neural networks (id56s)
   have been proving themselves as powerful predictive engines. when it
   comes to certain sequential machine learning tasks, such as speech
   recognition, id56s are reaching levels of predictive accuracy, time and
   time again, that no other algorithm can match. however, the first
   generation of id56s, back in the day, were not so hot. they suffered
   from a serious setback in their error-tweaking process that held up
   their progress for decades. finally, a major breakthrough came in the
   late 90s that led to a new generation of far more accurate id56s.
   building on that breakthrough for nearly twenty years, developers
   refined and perfected their new id56s until all-star apps such as
   [18]google voice search and [19]apple   s siri started snatching them up
   to power key processes. now recurrent networks are showing up
   everywhere, and are helping to ignite the ai renaissance that   s
   unfolding right now.

   neural networks that cling to the past
   [1*chs1mcz2rck4_dfrlnueig.png]

   most id158s, such as feedforward neural networks,
   have no memory of the input they received just one moment ago. for
   example, if you provide a feedforward neural network with the sequence
   of letters    wisdom,    when it gets to    d,    it has already forgotten that
   it just read    s.    that   s a big problem. no matter how hard you train
   it, it will always struggle to guess the most likely next character:
      o.    this makes it a rather crappy candidate for certain tasks, such as
   id103, that greatly benefit from the capacity to predict
   what   s coming next. recurrent networks, on the other hand, do remember
   what they   ve just encountered, and at a remarkably sophisticated level.

   let   s take the example of the input    wisdom    again and apply it to a
   recurrent network. the unit, or artificial neuron, of the id56, upon
   receiving the    d    also takes as its input the character it received one
   moment ago, the    s.    in other words, it adds the immediate past to the
   present. this gives it the advantage of a limited short-term memory
   that, along with its training, provides enough context for guessing
   what the next character is most likely to be:    o.   

   tweaking and re-tweaking

   if you like to get into the weeds, this is where you get excited.
   otherwise, get ready for a rough patch. but hang in there, it   s worth
   it. like all id158s, the units of an id56 assign a
   matrix of weights to their multiple inputs, then apply a function to
   those weights to determine a single output. however, recurrent networks
   apply weights not only to their present inputs, but also to their
   inputs from a moment ago. then they adjust the weights assigned to
   their present and past inputs through a process that involves two key
   concepts that you   ll definitely want to know if you really want to get
   into ai: id119 and backpropogation through time (bptt).

   id119

   one of the most famous algorithms in machine learning is known as
   id119. its primary virtue is its remarkable capacity to
   sidestep the dreaded    curse of dimensionality.    this issue plagues
   systems, such as neural networks, with far too many variables to make a
   brute-force calculation of their optimal values possible. gradient
   descent, however, breaks the curse of dimensionality by zooming in on
   the local low-point, or local minimum, of the multi-dimensional error
   or cost function. this helps the system determine the tweaked value, or
   weight, to assign to each of the units in the network, bringing
   accuracy back in line.

   backpropogation through time

   the id56 trains its units by adjusting their weights following a slight
   modification of a feedback process known as backpropogation. okay, this
   is a weird concept. but if you   re into ai, you   ll learn to love it. the
   process of backpropogation works its way back, layer by layer, from the
   network   s final output, tweaking the weights of each unit, or
   artificial neuron, according to the unit   s calculated portion of the
   total output error. got it? if so, get ready for one more layer of
   complexity. recurrent neural networks use a heavier version of this
   process known as backpropogation through time (bptt). this version
   extends the tweaking process to include the weight of the t-1 input
   values responsible for each unit   s memory of the prior moment.

   yikes: the vanishing gradient problem
   [1*svg2aqqqcwoycgfhdg5ulq.jpeg]

   despite enjoying some initial success with the help of id119
   and bptt, many id158s, including the first
   generation of id56s, eventually ran out gas. technically, they suffered
   a serious setback known as the vanishing gradient problem. although the
   details fall way outside the scope of this sweeping overview, the basic
   idea is pretty straightforward. first, let   s look at the notion of a
   gradient. like its simpler relative, the derivative, you can think of a
   gradient as a slope. in the context of training a deep neural network,
   the larger the gradient, the steeper the slope, the more quickly the
   system can roll downhill to the finish line and complete its training.
   but this is where developers ran into trouble         their slopes were too
   flat for fast training. this was particularly problematic in the first
   layers of their deep networks, which are the most critical when it
   comes to proper tweaking of memory units. here the gradient values got
   so small, and their corresponding slopes so flat, that one could
   describe them as    vanishing,    thus the vanishing gradient problem. as
   the gradients got smaller and smaller, and thus flatter and flatter,
   the training times grew unbearably long. it was an error-correction
   nightmare without end.

   the big breakthrough: long short-term memory

   finally, in the late 90s, [20]a major breakthrough solved the vanishing
   descent problem and gave a second wind to recurrent network
   development. at the center of this new approach were units of long
   short-term memory (lstm).
   [1*kiirm8lw171045y6if5dsa.png]

   as weird as that sounds, the long and short of it is that lstm made a
   world of difference in the field ai. these new units, or artificial
   neurons, like the standard short-term memory units of id56s, remember
   their inputs from a moment ago. however, unlike standard id56 units,
   lstms can hang on to their memories, which have read/write properties
   akin to memory registers in a conventional computer. yet lstms have
   analog, rather than digital, memory, making their functions
   differentiable. in other words, their curves are continuous and you can
   find the steepness of their slopes. so they are a good fit for the
   partial id128 involved in backpropogation and gradient
   descent.
   [1*omkwhlkgfb8ghizsk1evsg.png]

   altogether, lstms can not only tweak their weights, but retain, delete,
   transform and otherwise control the inflow and outflow of their stored
   data according to the quirks of their training. most importantly, lstms
   can cling to important error information for long enough to keep
   gradients relatively steep and thus training periods relatively short.
   this wipes out the vanishing gradient problem and greatly improves the
   accuracy of today   s lstm-based recurrent networks. thanks to this
   remarkable improvement in the id56 architecture, google, apple and many
   other leading companies, not to mention startups, are now using id56s to
   power applications at the center of their businesses. in short, id56s
   are suddenly a big deal.

   what to remember about id56s

   let   s recap the highlights of these amazing memory machines. recurrent
   neural networks, or id56s, can remember their former inputs, which gives
   them a big edge over other id158s when it comes to
   sequential, context-sensitive tasks such as id103.
   however, the first generation of id56s hit the wall when it came to
   their capacity to correct for errors through the all-important twin
   processes of backpropogation and id119. known as the dreaded
   vanishing gradient problem, this stumbling block virtually halted
   progress in the field until 1997, when a major breakthrough introduced
   a vastly improved lstm-based architecture to the field. the new
   approach, which effectively turned each unit in a recurrent network
   into an analogue computer, greatly increased accuracy and helped lead
   to the renaissance in ai we   re seeing all around us today.

   if you have enjoyed this post, the biggest compliment you could give
   would be to share this with someone that you think would enjoy it!

   additionally, if you never want to miss a post, subscribe to my
   articles by clicking the green heart button below and subscribing!
   thanks for reading, have a great day, and never stop learning!

     * [21]machine learning
     * [22]artificial intelligence
     * [23]data science
     * [24]computer science
     * [25]programming

   (button)
   (button)
   (button) 331 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of jason roell

[26]jason roell

   medium member since oct 2018

   software engineer/technology blogger (jasonroell.com) looking for
   challenging problems to solve.
   ([27]https://www.linkedin.com/in/jason-roell-47830817/)

     (button) follow
   [28]towards data science

[29]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 331
     * (button)
     *
     *

   [30]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [31]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/7d856c21b759
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-recurrent-neural-networks-the-prefered-neural-network-for-time-series-data-7d856c21b759&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-recurrent-neural-networks-the-prefered-neural-network-for-time-series-data-7d856c21b759&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_rlsxpj5sacmg---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@roelljr
  17. https://www.cbinsights.com/blog/artificial-intelligence-startup-funding/
  18. https://research.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html
  19. http://bgr.com/2016/06/13/ios-10-siri-third-party-apps/
  20. https://www.researchgate.net/publication/13853244_long_short-term_memory
  21. https://towardsdatascience.com/tagged/machine-learning?source=post
  22. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  23. https://towardsdatascience.com/tagged/data-science?source=post
  24. https://towardsdatascience.com/tagged/computer-science?source=post
  25. https://towardsdatascience.com/tagged/programming?source=post
  26. https://towardsdatascience.com/@roelljr
  27. https://www.linkedin.com/in/jason-roell-47830817/
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://towardsdatascience.com/@roelljr?source=post_header_lockup
  34. https://medium.com/p/7d856c21b759/share/twitter
  35. https://medium.com/p/7d856c21b759/share/facebook
  36. https://towardsdatascience.com/@roelljr?source=footer_card
  37. https://medium.com/p/7d856c21b759/share/twitter
  38. https://medium.com/p/7d856c21b759/share/facebook
