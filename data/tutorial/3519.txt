   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

autoencoders         introduction and implementation in tf.

   [16]go to the profile of manish chablani
   [17]manish chablani (button) blockedunblock (button) followfollowing
   jun 26, 2017

introduction and concepts:

   autoencoders (ae) are a family of neural networks for which the input
   is the same as the output (they implement a identity function). they
   work by compressing the input into a latent-space representation, and
   then reconstructing the output from this representation.
   [1*lsynw5m3tn7xrx61bzhoza.png]

   a really popular use for autoencoders is to apply them to images. the
   trick is to replace fully connected layers by convolutional layers.
   these, along with pooling layers, convert the input from wide and thin
   (let   s say 100 x 100 px with 3 channels         rgb) to narrow and thick.
   this helps the network extract visual features from the images, and
   therefore obtain a much more accurate latent space representation. the
   reconstruction process uses upsampling and convolutions.

   the resulting network is called a convolutional autoencoder (cae).

use of caes

example : ultra-basic image reconstruction

   convolutional autoencoders can be useful for reconstruction. they can,
   for example, learn to [18]remove noise from picture, or reconstruct
   missing parts.

   to do so, we don   t use the same image as input and output, but rather a
   noisy version as input and the clean version as output. with this
   process, the networks learns to fill in the gaps in the image.

   let   s see what a cae can do to replace part of an image of an eye.
   let   s say there   s a crosshair and we want to remove it. we can manually
   create the dataset, which is extremely convenient.
   [1*px5e64qqw-rzomg9ewwsva.png]

   now that our autoencoder is trained, we can use it to remove the
   crosshairs on pictures of eyes we have never seen!

implementation in tf:

   lets go over a sample implementation using mnist dataset in tensorflow.

   notebook:
   [19]https://github.com/mchablani/deep-learning/blob/master/autoencoder/
   convolutional_autoencoder.ipynb

network architecture

   the encoder part of the network will be a typical convolutional
   pyramid. each convolutional layer will be followed by a max-pooling
   layer to reduce the dimensions of the layers. the decoder needs to
   convert from a narrow representation to a wide reconstructed image.

   usually, you   ll see transposed convolution layers used to increase the
   width and height of the layers. they work almost exactly the same as
   convolutional layers, but in reverse. a stride in the input layer
   results in a larger stride in the transposed convolution layer. for
   example, if you have a 3x3 kernel, a 3x3 patch in the input layer will
   be reduced to one unit in a convolutional layer. comparatively, one
   unit in the input layer will be expanded to a 3x3 path in a transposed
   convolution layer. the tensorflow api provides us with an easy way to
   create the layers, [20]tf.nn.conv2d_transpose.

     however, transposed convolution layers can lead to artifacts in the
     final images, such as checkerboard patterns. this is due to overlap
     in the kernels which can be avoided by setting the stride and kernel
     size equal. in [21]this distill article from augustus odena, et al,
     the authors show that these checkerboard artifacts can be avoided by
     resizing the layers using nearest neighbor or bilinear interpolation
     (upsampling) followed by a convolutional layer. in tensorflow, this
     is easily done with [22]tf.image.resize_images, followed by a
     convolution. odena et al claim that nearest neighbor interpolation
     works best for the upsampling

   autoencoders can be used to denoise images quite successfully just by
   training the network on noisy images. we can create the noisy images
   ourselves by adding gaussian noise to the training images, then
   clipping the values to be between 0 and 1. we   ll use noisy images as
   input and the original, clean images as targets.

     note we are using sigmoid_cross_id178_with_logits for loss.
     according to tf documentation: it measures the id203 error in
     discrete classification tasks in which each class is independent and
     not mutually exclusive. for instance, one could perform multilabel
     classification where a picture can contain both an elephant and a
     dog at the same time.

   model definition:
learning_rate = 0.001
inputs_ = tf.placeholder(tf.float32, (none, 28, 28, 1), name='inputs')
targets_ = tf.placeholder(tf.float32, (none, 28, 28, 1), name='targets')
### encoder
conv1 = tf.layers.conv2d(inputs=inputs_, filters=32, kernel_size=(3,3), padding=
'same', activation=tf.nn.relu)
# now 28x28x32
maxpool1 = tf.layers.max_pooling2d(conv1, pool_size=(2,2), strides=(2,2), paddin
g='same')
# now 14x14x32
conv2 = tf.layers.conv2d(inputs=maxpool1, filters=32, kernel_size=(3,3), padding
='same', activation=tf.nn.relu)
# now 14x14x32
maxpool2 = tf.layers.max_pooling2d(conv2, pool_size=(2,2), strides=(2,2), paddin
g='same')
# now 7x7x32
conv3 = tf.layers.conv2d(inputs=maxpool2, filters=16, kernel_size=(3,3), padding
='same', activation=tf.nn.relu)
# now 7x7x16
encoded = tf.layers.max_pooling2d(conv3, pool_size=(2,2), strides=(2,2), padding
='same')
# now 4x4x16
### decoder
upsample1 = tf.image.resize_images(encoded, size=(7,7), method=tf.image.resizeme
thod.nearest_neighbor)
# now 7x7x16
conv4 = tf.layers.conv2d(inputs=upsample1, filters=16, kernel_size=(3,3), paddin
g='same', activation=tf.nn.relu)
# now 7x7x16
upsample2 = tf.image.resize_images(conv4, size=(14,14), method=tf.image.resizeme
thod.nearest_neighbor)
# now 14x14x16
conv5 = tf.layers.conv2d(inputs=upsample2, filters=32, kernel_size=(3,3), paddin
g='same', activation=tf.nn.relu)
# now 14x14x32
upsample3 = tf.image.resize_images(conv5, size=(28,28), method=tf.image.resizeme
thod.nearest_neighbor)
# now 28x28x32
conv6 = tf.layers.conv2d(inputs=upsample3, filters=32, kernel_size=(3,3), paddin
g='same', activation=tf.nn.relu)
# now 28x28x32
logits = tf.layers.conv2d(inputs=conv6, filters=1, kernel_size=(3,3), padding='s
ame', activation=none)
#now 28x28x1
# pass logits through sigmoid to get reconstructed image
decoded = tf.nn.sigmoid(logits)
# pass logits through sigmoid and calculate the cross-id178 loss
loss = tf.nn.sigmoid_cross_id178_with_logits(labels=targets_, logits=logits)
# get cost and define the optimizer
cost = tf.reduce_mean(loss)
opt = tf.train.adamoptimizer(learning_rate).minimize(cost)

   training:
sess = tf.session()
epochs = 100
batch_size = 200
# set's how much noise we're adding to the mnist images
noise_factor = 0.5
sess.run(tf.global_variables_initializer())
for e in range(epochs):
    for ii in range(mnist.train.num_examples//batch_size):
        batch = mnist.train.next_batch(batch_size)
        # get images from the batch
        imgs = batch[0].reshape((-1, 28, 28, 1))

        # add random noise to the input images
        noisy_imgs = imgs + noise_factor * np.random.randn(*imgs.shape)
        # clip the images to be between 0 and 1
        noisy_imgs = np.clip(noisy_imgs, 0., 1.)

        # noisy images as inputs, original images as targets
        batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: noisy_imgs,
                                                         targets_: imgs})
print("epoch: {}/{}...".format(e+1, epochs),
              "training loss: {:.4f}".format(batch_cost))

   credits:
   [23]https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e2006
   94

     * [24]machine learning
     * [25]deep learning
     * [26]autoencoder
     * [27]tensorflow
     * [28]towards data science

   (button)
   (button)
   (button) 262 claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of manish chablani

[30]manish chablani

   ai in healthcare [31]@curaihq, marathoner. (past: dl for self driving
   cars [32]@cruise, ml [33]@uber, early engineer [34]@microsoftazure
   cloud storage)

     (button) follow
   [35]towards data science

[36]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 262
     * (button)
     *
     *

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/3f40483b0a85
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/autoencoders-introduction-and-implementation-3f40483b0a85&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/autoencoders-introduction-and-implementation-3f40483b0a85&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_p5aiuam4eqeg---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@manishchablani?source=post_header_lockup
  17. https://towardsdatascience.com/@manishchablani
  18. http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf
  19. https://github.com/mchablani/deep-learning/blob/master/autoencoder/convolutional_autoencoder.ipynb
  20. https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose
  21. http://distill.pub/2016/deconv-checkerboard/
  22. https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/image/resize_images
  23. https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694
  24. https://towardsdatascience.com/tagged/machine-learning?source=post
  25. https://towardsdatascience.com/tagged/deep-learning?source=post
  26. https://towardsdatascience.com/tagged/autoencoder?source=post
  27. https://towardsdatascience.com/tagged/tensorflow?source=post
  28. https://towardsdatascience.com/tagged/towards-data-science?source=post
  29. https://towardsdatascience.com/@manishchablani?source=footer_card
  30. https://towardsdatascience.com/@manishchablani
  31. http://twitter.com/curaihq
  32. http://twitter.com/cruise
  33. http://twitter.com/uber
  34. http://twitter.com/microsoftazure
  35. https://towardsdatascience.com/?source=footer_card
  36. https://towardsdatascience.com/?source=footer_card

   hidden links:
  38. https://medium.com/p/3f40483b0a85/share/twitter
  39. https://medium.com/p/3f40483b0a85/share/facebook
  40. https://medium.com/p/3f40483b0a85/share/twitter
  41. https://medium.com/p/3f40483b0a85/share/facebook
