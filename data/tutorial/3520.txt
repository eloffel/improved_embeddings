   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

autoencoders         deep learning bits #1

   [14]go to the profile of julien despois
   [15]julien despois (button) blockedunblock (button) followfollowing
   feb 7, 2017

   featured: data compression, image reconstruction and segmentation (with
   examples!)
   [1*8ixte1vhlsmkb3aquwdxpq.png]

   in the    deep learning bits    series, we will not see how to use deep
   learning to solve complex problems end-to-end as we do in [16]a.i.
   odyssey. we will rather look at different techniques, along with some
   examples and applications.

     if you like artificial intelligence, make sure to [17]subscribe to
     the newsletter to receive updates on articles and much more!
     __________________________________________________________________

introduction

what   s an autoencoder?

   neural networks exist in all shapes and sizes, and are often
   characterized by their input and output data type. for instance, image
   classifiers are built with convolutional neural networks. they take
   images as inputs, and output a id203 distribution of the classes.

   autoencoders (ae) are a family of neural networks for which the input
   is the same as the output*. they work by compressing the input into a
   latent-space representation, and then reconstructing the output from
   this representation.

   *we   ll see how using altered versions of the input can be even more
   interesting
   [1*-5d-cbtusunmsba6vydy3a.png]
   simple autoencoder architecture         the input is compressed and then
   reconstructed

convolutional autoencoders

   a really popular use for autoencoders is to apply them to images. the
   trick is to replace fully connected layers by convolutional layers.
   these, along with pooling layers, convert the input from wide and thin
   (let   s say 100 x 100 px with 3 channels         rgb) to narrow and thick.
   this helps the network extract visual features from the images, and
   therefore obtain a much more accurate latent space representation. the
   reconstruction process uses upsampling and convolutions.

   the resulting network is called a convolutional autoencoder (cae).
   [1*8ixte1vhlsmkb3aquwdxpq.png]
   convolutional autoencoder architecture         it maps a wide and thin input
   space to narrow and thick latent space

reconstruction quality

   the reconstruction of the input image is often blurry and of lower
   quality. this is a consequence of the compression during which we have
   lost some information.
   [1*fmlpzyhxsae8faisfc_fkg.png]
   the cae is trained to reconstruct its input
   [1*kidjgcjakbkl6ls-bul4na.png]
   the reconstructed image is blurry

use of caes

example 1: ultra-basic image reconstruction

   convolutional autoencoders can be useful for reconstruction. they can,
   for example, learn to [18]remove noise from picture, or reconstruct
   missing parts.

   to do so, we don   t use the same image as input and output, but rather a
   noisy version as input and the clean version as output. with this
   process, the networks learns to fill in the gaps in the image.

   let   s see what a cae can do to replace part of an image of an eye.
   let   s say there   s a crosshair and we want to remove it. we can manually
   create the dataset, which is extremely convenient.
   [1*q-robbb9bbaqt73540qfaw.png]
   the cae is trained to remove the crosshair
   [1*px5e64qqw-rzomg9ewwsva.png]
   even though it is blurry, the reconstructed input has no crosshair left

     now that our autoencoder is trained, we can use it to remove the
     crosshairs on pictures of eyes we have never seen!

example 2: ultra-basic image colorization

   in this example, the cae will learn to map from an image of circles and
   squares to the same image, but with the circles colored in red, and the
   squares in blue.
   [1*qyglsstggy1zff1etdfihw.png]
   the cae is trained to colorize the image
   [1*pm7vnxw4gow-61r5ccpega.png]
   even though the reconstruction is blurry, the color are mostly right

   the cae does pretty well on colorizing the right parts of the image. it
   has understood that circles are red and squares are blue. the purple
   color comes from a blend of blue and red where the networks hesitates
   between a circle and a square.

     now that our autoencoder is trained, we can use it to colorize
     pictures we have never seen before!

advanced applications

   the examples above are just proofs of concept to show what a
   convolutional autoencoder can do.

   more exciting application include [19]full image colorization,
   [20]latent space id91, or [21]generating higher resolution
   images. the latter is obtained by using the low resolution as input and
   the high resolution as output.
   [1*a1ffag3qww3fmqvlub1ema.png]
   colorful image colorization by richard zhang, phillip isola, alexei
   a. efros
   [1*bc5mdzppsh14zzszz8yqcw.png]
   neural enhance by [22]alexjc

conclusions

   in this post, we have seen how we can use autoencoder neural networks
   to compress, reconstruct and clean data. obtaining images as output is
   something really thrilling, and really fun to play with.

   note: there   s a modified version of aes called variational
   autoencoders, which are used for image generation, but i keep that for
   later.

     if you like artificial intelligence, make sure to [23]subscribe to
     the newsletter to receive updates on articles and much more!

   you can play with the code over there:
   [24]despoisj/convolutionalautoencoder
   convolutionalautoencoder - quick and dirty example of the application
   of convolutional autoencoders in keras/tensorflowgithub.com

   thanks for reading this post, stay tuned for more !
   [25][1*0hqoaabq7xgpt-oyngiubg.png]
   [26][1*vgw1jka6hgnvwztsfmlnpg.png]
   [27][1*gkbpq1ruui0fvk2um_i4tq.png]

     [28]hacker noon is how hackers start their afternoons. we   re a part
     of the [29]@ami family. we are now [30]accepting submissions and
     happy to [31]discuss advertising & sponsorship opportunities.

     if you enjoyed this story, we recommend reading our [32]latest tech
     stories and [33]trending tech stories. until next time, don   t take
     the realities of the world for granted!

   [1*35tcjopcvq6lbb3i6wegqw.jpeg]

     * [34]machine learning
     * [35]artificial intelligence
     * [36]deep learning
     * [37]technology

   (button)
   (button)
   (button) 788 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [38]go to the profile of julien despois

[39]julien despois

   deep learning scientist @ l   or  al ai research | creator of
   ai-odyssey.com

     (button) follow
   [40]hacker noon

[41]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 788
     * (button)
     *
     *

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/11731e200694
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_iwppresnrpu2---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@juliendespois?source=post_header_lockup
  15. https://hackernoon.com/@juliendespois
  16. https://medium.com/@juliendespois/talk-to-you-computer-with-you-eyes-and-deep-learning-a-i-odyssey-part-2-7d3405ab8be1
  17. http://eepurl.com/catxvt
  18. http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf
  19. http://richzhang.github.io/colorization/
  20. http://proceedings.mlr.press/v48/xieb16.pdf
  21. https://arxiv.org/pdf/1501.00092v3.pdf
  22. https://github.com/alexjc/neural-enhance
  23. http://eepurl.com/catxvt
  24. https://github.com/despoisj/convolutionalautoencoder
  25. http://bit.ly/hackernoonfb
  26. https://goo.gl/k7xybx
  27. https://goo.gl/4ofytp
  28. http://bit.ly/hackernoon
  29. http://bit.ly/atamiatami
  30. http://bit.ly/hackernoonsubmission
  31. mailto:partners@amipublications.com
  32. http://bit.ly/hackernoonlatestt
  33. https://hackernoon.com/trending
  34. https://hackernoon.com/tagged/machine-learning?source=post
  35. https://hackernoon.com/tagged/artificial-intelligence?source=post
  36. https://hackernoon.com/tagged/deep-learning?source=post
  37. https://hackernoon.com/tagged/technology?source=post
  38. https://hackernoon.com/@juliendespois?source=footer_card
  39. https://hackernoon.com/@juliendespois
  40. https://hackernoon.com/?source=footer_card
  41. https://hackernoon.com/?source=footer_card

   hidden links:
  43. https://github.com/despoisj/convolutionalautoencoder
  44. https://medium.com/p/11731e200694/share/twitter
  45. https://medium.com/p/11731e200694/share/facebook
  46. https://medium.com/p/11731e200694/share/twitter
  47. https://medium.com/p/11731e200694/share/facebook
