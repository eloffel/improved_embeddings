   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

a new kind of deep neural networks

   [16]go to the profile of eugenio culurciello
   [17]eugenio culurciello (button) blockedunblock (button)
   followfollowing
   may 5, 2017

   by alfredo canziani, abishek chaurasia and [18]eugenio culurciello

   there is a new wave of deep neural networks coming. they are the
   evolution of feed-forward models, that [19]we previously analyzed in
   detail.

   the new kind of neural networks are an evolution of the initial
   feed-forward model of [20]lenet5 / [21]alexnet and derivatives, and
   include more sophisticated by-pass schemes than [22]resnet /
   [23]inception. these feedforward neural networks are also called
   encoders, as they compress and encode images into smaller
   representation vectors.

   the new wave of neural networks have two important new features:
     * generative branches: also called decoders, as they project a
       representation vector back into the input space
     * recurrent layers: that combine representations from previous time
       steps with the inputs and representations of the current time step

great! but what can this added complexity do for us?

   it turns out conventional feed-forward neural networks have a lot of
   limitations:

   1- cannot localize precisely: due to downsampling and loss of spatial
   resolution in higher layers, localization of features / objects /
   classes is impaired

   2- cannot reason about the scene: because they compress an image into a
   short representation code, they lose information on how the image is
   composed, and how parts of the image or scene are spatially arranged

   2- have temporal instabilities: since they are trained on still images,
   they did not learn smooth spatio-temporal transformations of objects
   motion in space. they can recognize categories of objects in some
   image, but not others, and are very sensitive to [24]adversarial noise
   and [25]perturbations

   3- cannot predict: since they do not use temporal information,
   feed-forward neural networks provide a new representation code at each
   frame, only based on the current input, but cannot predict what will
   happen in the next few frames (note: with [26]exceptions, not trained
   on videos)

   to surpass these limitation, we need a new kind of network that can
   project back a learned representation into the input image space, and
   also that can be trained on temporally coherent sequences of images: we
   need to train on videos.

   this is a list of advanced features that these new networks can
   provide:
     * unsupervised learning: they can be pre-trained on videos to predict
       future frames or representation, thus needing much less labeled
       data (expensive on videos!) to train to perform some tasks
     * segmentation: [27]segment different objects in an image
     * scene-parsing: follows from segmentation, if dataset has per-pixel
       object labels, used in [28]autonomous driving and augmented reality
     * localization: follows from segmentation and perfect object
       boundaries, all scene-parsing and segmentation networks can do
       this!
     * spatio-temporal representation: use video for training, not just
       still images, know concept of time and temporal relationships
     * video prediction: some network are design to [29]predict future
       frames in a video
     * representation prediction: some network can [30]predict the
       representation of future frames in a video
     * ability to perform online learning by monitoring the error signal
       between predictions and actual future frames or representations

   let us now examine the details and implementations of these new
   networks, as follows.

generative ladder networks

   these models use an encoder and a decoder pair to segment images into
   parts and objects. examples are: [31]enet, [32]segnet, [33]unet,
   [34]densenets, [35]ladder networks, and many more.

   here is a representative 3-layer model:
   [1*pqslgtefux6gogjqlpplaa.png]

   d modules are standard feed-forward layers. g modules are generative
   modules, similar to standard feed-forward layers but with
   de-convolution and upsampling. they also use residual-like connections
      res    to connect the representation at each encoder layer to the one of
   decoder layers. this forces the representation of generative layers to
   be modulated by the feed-forward representation, and thus have a
   stronger ability to localize and parse the scene into objects and
   parts.    x    is the input image and    y    is the output segmentation at the
   same time step.

   these network can perform segmentation, scene-parsing and precise
   localization, but do not operate in the temporal domain, and have no
   memory of the past frames.

   the encoder to decoder by-pass at each layer recently helped these
   network to achieve state-of-the-art performance.

recursive and generative ladder networks

   one of the newest deep neural network architectures adds recursion to
   generative ladder networks. these are recursive and generative ladder
   networks (regel, we call them [36]cortexnet models), and they are one
   of the most complex deep neural network models to date, at least for
   image analysis.

   here is a 3-layer model of one network we currently use:
   [1*n0mq16mp7vhhvdsxpn55hg.png]

   d and g modules are practically identical to the ones in generative
   ladder networks described above. these network adds a recurrent path
      t-1    from each g module to the respective d module in the same layer.

   these networks take a sequence of frames from a video as input x[t],
   and at each time-step they predict the next frame in the video y[t+1],
   which is close to x[t+1], if the prediction is accurate.

   since this network can measure the error between the prediction and the
   actual next frame, it knows when it is able to predict an input or not.
   if not, it can activate incremental learning, something feed-forward
   networks are not able to do. it is thus able to perform inherent online
   learning.

   we think this is a very important features of machine learning that is
   a prerogative of predictive neural networks. without this feature,
   network are not able to provide a true predictive confidence signals,
   and are not able to perform effective incremental learning.

   these networks are still under study. our advice: keep and eye on them!

predictive coding networks         part 1

   recursive generative networks are one possible predictive model.
   alternatively, the predictive coding computational neuroscience model
   can provide prediction capabilities and be arranged as hierarchical
   deep neural networks.

   here is an example of a 2-layer model:
   [1*hytr686ab6p8p6fxyqfqnw.png]

   [37]rao and ballard model and [38]friston implementation compute an
   error    e    at each layer between    a    modules (similar to d modules above
   ladder networks) and    r/ay    modules (similar to g modules of above
   ladder networks). this error    e    represents, at each layer, the ability
   of the network to predict the representation. error    e    is then
   forwarded as input to the next layer.    r    is a convolutional id56/lst
   module, and    ay    is similar to    a    module.    r    and    ay    can be also
   combined into a single recurrent module. in the first layer    x    is the
   input frame.

   the problem with this model is that this network is very different from
   standard feed-forward neural networks. it does not create a
   hierarchical representation at higher layer that create combination of
   features of lower layers, rather, these predictive network compute the
   representation of residual errors of previous layers.

   as such, they are a bit reminiscent of residual feed-forward networks,
   but in practice forcing these networks to forward errors does not lead
   them to learn an effective hierarchical representation at higher
   layers. as such, they are not able to effectively perform other tasks
   based on upper-layer representations, such as categorization,
   segmentation, action recognition. more experiments are needed to
   present these limitations.

   this model has been implemented in [39]prednet by bill lotter and david
   cox. a similar model is also from [40]brain corporation.

predictive coding networks         part 2

   [41]spratling predictive coding model projects the representation y to
   upper layers, not the error    e   , as was performed in friston models
   above. this make this network model more compatible with hierarchical
   feedforward deep neural networks and avoid learning moments of errors
   in the upper layers.

   here is an example of a 2-layer model:
   [1*ywmu0j3w9a6frhspbhbktw.png]

   this model can be essentially rewritten and simplified to the recurrent
   generative ladder model we have seen above. this is because    r    and
      ay    can be combined into a single recurrent module.

relation to id3

   id3 (gan) are a very popular model that is
   able to learn to generate samples from a distribution of data. the new
   networks model presented here are superior to gan because:
     * instead of being trained in a minimax game, they are directly
       trained for a useful task, so both the discriminator and the
       generator are directly useful
     * they learn to create a useful input representation while also be
       able to generate new inputs
     * they can learn to generate targeted data based on inputs
     * generator and discriminator network are tied together, removing
       convergence problems
     * the generator provides almost perfect photorealistic samples (see
       below), as compared to the [42]unsightly results provided by gan

   [1*p5lshz0two5c4pt5csyyhg.gif]
   example of cortexnet predictive capabilities         left: current frame,
   center: next frame ground truth, right: predicted next frame

note on other models

   models like cortexnet are reminiscent of [43]pixel recurrent networks
   and its various implementations ([44]pixelid98, [45]pixel id98++,
   [46]wavenet, [47]etc). these model aim at modeling the distribution of
   input data: (   our aim is to estimate a distribution over natural images
   that can be used to tractably compute the likelihood of [data] and to
   generate new ones.   ). they only focus on generating new realistic data
   samples, but have not shown to learn representations for real-life
   tasks. these models are also very slow in id136.

conclusions

   a paper on this topic is [48]here. [49]cortexnet is still under study
   and evaluation. for example the most [50]recent prednet paper presents
   a comparison of predictive coding and ladder networks, where prednet
   wins on some tasks. prednet was used to perform orientation-invariant
   face classification, using higher layer representation. also it can
   predict steering angles in a dataset, but mostly using simple motion
   filter from the first layer of the network. this tasks does not require
   a hierarchical decomposition of features.

about the author

   i have almost 20 years of experience in neural networks in both
   hardware and software (a rare combination). see about me here:
   [51]medium, [52]webpage, [53]scholar, [54]linkedin, and more   

     * [55]machine learning
     * [56]deep learning
     * [57]neural networks
     * [58]towards data science
     * [59]data science

   (button)
   (button)
   (button) 849 claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [60]go to the profile of eugenio culurciello

[61]eugenio culurciello

   i dream and build new technology

     (button) follow
   [62]towards data science

[63]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 849
     * (button)
     *
     *

   [64]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [65]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/749bcde19108
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-new-kind-of-deep-neural-networks-749bcde19108&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-new-kind-of-deep-neural-networks-749bcde19108&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_91cuyi9ogp87---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@culurciello?source=post_header_lockup
  17. https://towardsdatascience.com/@culurciello
  18. https://medium.com/@culurciello
  19. https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba
  20. http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf
  21. https://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks.pdf
  22. https://arxiv.org/abs/1512.03385
  23. http://arxiv.org/abs/1602.07261
  24. https://arxiv.org/abs/1511.06306
  25. https://arxiv.org/abs/1412.1897
  26. https://arxiv.org/abs/1504.08023
  27. https://arxiv.org/abs/1511.00561
  28. https://codeac29.github.io/projects/linknet/index.html
  29. https://arxiv.org/abs/1605.08104
  30. https://arxiv.org/abs/1703.07684
  31. https://arxiv.org/abs/1606.02147
  32. https://arxiv.org/abs/1511.00561
  33. https://arxiv.org/abs/1505.04597
  34. https://arxiv.org/abs/1611.09326
  35. https://arxiv.org/abs/1507.02672
  36. https://engineering.purdue.edu/elab/cortexnet/
  37. http://www.nature.com/neuro/journal/v2/n1/abs/nn0199_79.html
  38. https://www.ncbi.nlm.nih.gov/pubmed/23177956
  39. https://coxlab.github.io/prednet/
  40. https://arxiv.org/abs/1607.06854
  41. https://nms.kcl.ac.uk/michael.spratling/doc/visres08.pdf
  42. https://channel9.msdn.com/events/neural-information-processing-systems-conference/neural-information-processing-systems-conference-nips-2016/generative-adversarial-networks
  43. https://arxiv.org/abs/1601.06759
  44. https://arxiv.org/abs/1606.05328
  45. https://openreview.net/pdf?id=bjrfc6ceg
  46. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  47. http://ruotianluo.github.io/2017/01/11/pixelid98-wavenet/
  48. https://arxiv.org/abs/1706.02735
  49. https://engineering.purdue.edu/elab/cortexnet/
  50. https://arxiv.org/abs/1605.08104
  51. https://medium.com/@culurciello/
  52. https://e-lab.github.io/html/contact-eugenio-culurciello.html
  53. https://scholar.google.com/citations?user=segmqkiaaaaj
  54. https://www.linkedin.com/in/eugenioculurciello/
  55. https://towardsdatascience.com/tagged/machine-learning?source=post
  56. https://towardsdatascience.com/tagged/deep-learning?source=post
  57. https://towardsdatascience.com/tagged/neural-networks?source=post
  58. https://towardsdatascience.com/tagged/towards-data-science?source=post
  59. https://towardsdatascience.com/tagged/data-science?source=post
  60. https://towardsdatascience.com/@culurciello?source=footer_card
  61. https://towardsdatascience.com/@culurciello
  62. https://towardsdatascience.com/?source=footer_card
  63. https://towardsdatascience.com/?source=footer_card
  64. https://towardsdatascience.com/
  65. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  67. https://medium.com/p/749bcde19108/share/twitter
  68. https://medium.com/p/749bcde19108/share/facebook
  69. https://medium.com/p/749bcde19108/share/twitter
  70. https://medium.com/p/749bcde19108/share/facebook
