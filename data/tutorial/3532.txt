   #[1]alternate [2]alternate

   menu

     * [3]home
     * [4]about
     * [5]coding the deep learning revolution ebook
     * [6]contact
     * [7]ebook / newsletter sign-up

   search: ____________________

python tensorflow tutorial     build a neural network

   by [8]andy | [9]deep learning

     * you are here:
     * [10]home
     * [11]deep learning
     * [12]python tensorflow tutorial     build a neural network

   apr 08
   [13]0

   google   s tensorflow has been a hot topic in deep learning
   recently.  the open source software, designed to allow efficient
   computation of data flow graphs, is especially suited to deep learning
   tasks.  it is designed to be executed on single or multiple cpus and
   gpus, making it a good option for complex deep learning tasks.  in it   s
   most recent incarnation     version 1.0     it can even be run on certain
   mobile operating systems.  this introductory tutorial to tensorflow
   will give an overview of some of the basic concepts of tensorflow in
   python.  these will be a good stepping stone to building more
   complex deep learning networks, such as [14]convolution neural
   networks, [15]natural language models and [16]recurrent neural networks
   in the package.  we   ll be creating a simple three-layer neural network
   to classify the mnist dataset.  this tutorial assumes that you are
   familiar with the basics of neural networks, which you can get up to
   scratch with in the [17]neural networks tutorial if required.
   to install tensorflow, follow the instructions [18]here. the code for
   this tutorial can be found in [19]this site   s github repository.  once
   you   re done, you also might want to check out a higher level deep
   learning library that sits on top of tensorflow called keras     see
   [20]my keras tutorial.
     __________________________________________________________________

eager to learn more? get the book [21]here
     __________________________________________________________________

   first, let   s have a look at the main ideas of tensorflow.

1.0 tensorflow graphs

   tensorflow is based on graph based computation        what on earth is
   that?   , you might say.  it   s an alternative way of conceptualising
   mathematical calculations.  consider the following expression $a = (b +
   c) * (c + 2)$.  we can break this function down into the following
   components:

   \begin{align}
   d &= b + c \\
   e &= c + 2 \\
   a &= d * e
   \end{align}

   now we can represent these operations graphically as:
   tensorflow tutorial - simple computational graph

   simple computational graph

   this may seem like a silly example     but notice a powerful idea in
   expressing the equation this way: two of the computations ($d=b+c$ and
   $e=c+2$) can be performed in parallel.  by splitting up these
   calculations across cpus or gpus, this can give us significant gains in
   computational times.  these gains are a must for big data applications
   and deep learning     especially for complicated neural network
   architectures such as convolutional neural networks (id98s) and
   recurrent neural networks (id56s).  the idea behind tensorflow is to
   ability to create these computational graphs in code and allow
   significant performance improvements via parallel operations and other
   efficiency gains.

   we can look at a similar graph in tensorflow below, which shows the
   computational graph of a three-layer neural network.
   tensorflow tutorial - data flow graph

   tensorflow data flow graph

   the animated data flows between different nodes in the graph are
   tensors which are multi-dimensional data arrays.  for instance, the
   input data tensor may be 5000 x 64 x 1, which represents a 64 node
   input layer with 5000 training samples.  after the input layer there is
   a hidden layer with [22]rectified linear units as the activation
   function.  there is a final output layer (called a    logit layer    in the
   above graph) which uses cross id178 as a cost/id168.  at each
   point we see the relevant tensors flowing to the    gradients    block
   which finally flow to the [23]stochastic id119 optimiser
   which performs the back-propagation and id119.

   here we can see how computational graphs can be used to represent the
   calculations in neural networks, and this, of course, is what
   tensorflow excels at.  let   s see how to perform some basic mathematical
   operations in tensorflow to get a feel for how it all works.

2.0 a simple tensorflow example

   let   s first make tensorflow perform our little example
   calculation above     $a = (b + c) * (c + 2)$.  first we need to
   introduce ourselves to tensorflow variables and constants.  let   s
   declare some then i   ll explain the syntax:
import tensorflow as tf

# first, create a tensorflow constant
const = tf.constant(2.0, name="const")

# create tensorflow variables
b = tf.variable(2.0, name='b')
c = tf.variable(1.0, name='c')

   as can be observed above, tensorflow constants can be declared using
   the tf.constant function, and variables with the tf.variable function.
   the first element in both is the value to be assigned the constant /
   variable when it is initialised.  the second is an optional name string
   which can be used to label the constant / variable     this is handy for
   when you want to do visualisations (as will be discussed briefly
   later).  tensorflow will infer the type of the constant / variable from
   the initialised value, but it can also be set explicitly using the
   optional dtype argument.  tensorflow has many of its own types like
   tf.float32, tf.int32 etc.     see them all [24]here.

   it   s important to note that, as the python code runs through these
   commands, the variables haven   t actually been declared as they would
   have been if you just had a standard python declaration (i.e. b =
   2.0).  instead, all the constants, variables, operations and the
   computational graph are only created when the initialisation commands
   are run.

   next, we create the tensorflow operations:
# now create some operations
d = tf.add(b, c, name='d')
e = tf.add(c, const, name='e')
a = tf.multiply(d, e, name='a')

   tensorflow has a wealth of operations available to perform all sorts of
   interactions between variables, some of which we   ll get to later in the
   tutorial.  the operations above are pretty obvious, and they
   instantiate the operations $b+c$, $c+2.0$ and $d*e$.

   the next step is to setup an object to initialise the variables and the
   graph structure:
# setup the variable initialisation
init_op = tf.global_variables_initializer()

   ok, so now we are all set to go.  to run the operations between the
   variables, we need to start a tensorflow session     tf.session.  the
   tensorflow session is an object where all operations are run.
   tensorflow was initially created in a static graph paradigm     in other
   words, first all the operations and variables are defined (the graph
   structure) and then these are compiled within the tf.session object.
   there is now the option to build graphs on the fly using the tensorflow
   eager framework, to check this out see my [25]tensorflow eager
   tutorial.

   however, there are still advantages in building static graphs using the
   tf.session object. you can do this by using the with python syntax, to
   run the graph like so:
# start the session
with tf.session() as sess:
    # initialise the variables
    sess.run(init_op)
    # compute the output of the graph
    a_out = sess.run(a)
    print("variable a is {}".format(a_out))

   the first command within the with block is the initialisation, which is
   run with the, well, run command.  next we want to figure out what the
   variable a should be.  all we have to do is run the operation which
   calculates a i.e. a = tf.multiply(d, e, name=   a   ).  note that a is an
   operation, not a variable and therefore it can be run.  we do just that
   with the sess.run(a) command and assign the output to a_out, the value
   of which we then print out.

   note something cool     we defined operations d and e which need to be
   calculated before we can figure out what a is.  however, we don   t have
   to explicitly run those operations, as tensorflow knows what other
   operations and variables the operation a depends on, and therefore runs
   the necessary operations on its own.  it does this through its data
   flow graph which shows it all the required dependencies. using the
   tensorboard functionality, we can see the graph that tensorflow created
   in this little program:
   tensorflow tutorial - simple graph

   simple tensorflow graph

   now that   s obviously a trivial example     what if we had an array of b
   values that we wanted to calculate the value of a over?

2.1 the tensorflow placeholder

   let   s also say that we didn   t know what the value of the array b would
   be during the declaration phase of the tensorflow problem (i.e. before
   the with tf.session() as sess) stage.  in this case, tensorflow
   requires us to declare the basic structure of the data by using the
   tf.placeholder variable declaration.  let   s use it for b:
# create tensorflow variables
b = tf.placeholder(tf.float32, [none, 1], name='b')

   because we aren   t providing an initialisation in this declaration, we
   need to tell tensorflow what data type each element within the tensor
   is going to be.  in this case, we want to use tf.float32.  the second
   argument is the shape of the data that will be    injected    into this
   variable.  in this case, we want to use a (? x 1) sized array     because
   we are being cagey about how much data we are supplying to this
   variable (hence the    ?   ), the placeholder is willing to accept a none
   argument in the size declaration.  now we can inject as much
   1-dimensional data that we want into the b variable.

   the only other change we need to make to our program is in the
   sess.run(a,   ) command:
a_out = sess.run(a, feed_dict={b: np.arange(0, 10)[:, np.newaxis]})

   note that we have added the feed_dict argument to the sess.run(a,   )
   command.  here we remove the mystery and specify exactly what the
   variable b is to be     a one-dimensional range from 0 to 10.  as
   suggested by the argument name, feed_dict, the input to be supplied is
   a python dictionary, with each key being the name of the placeholder
   that we are filling.

   when we run the program again this time we get:
variable a is [[  3.]
 [  6.]
 [  9.]
 [ 12.]
 [ 15.]
 [ 18.]
 [ 21.]
 [ 24.]
 [ 27.]
 [ 30.]]

   notice how tensorflow adapts naturally from a scalar output (i.e. a
   singular output when a=9.0) to a tensor (i.e. an array/matrix)?  this
   is based on its understanding of how the data will flow through the
   graph.

   now we are ready to build a basic mnist predicting neural network.

3.0 a neural network example

   now we   ll go through an example in tensorflow of creating a simple
   three layer neural network.  in future articles, we   ll show how to
   build more complicated neural network structures such as convolution
   neural networks and recurrent neural networks.  for this example
   though, we   ll keep it simple.  if you need to scrub up on your neural
   network basics, check out my [26]popular tutorial on the subject.  in
   this example, we   ll be using the mnist dataset (and its associated
   loader) that the tensorflow package provides.  this mnist dataset is a
   set of 28  28 pixel grayscale images which represent hand-written
   digits.  it has 55,000 training rows, 10,000 testing rows and 5,000
   validation rows.

   we can load the data by running:
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("mnist_data/", one_hot=true)

   the one_hot=true argument specifies that instead of the labels
   associated with each image being the digit itself i.e.    4   , it is a
   vector with    one hot    node and all the other nodes being zero i.e.
   [0, 0, 0, 0, 1, 0, 0, 0, 0, 0].  this lets us easily feed it into the
   output layer of our neural network.

3.1 setting things up

   next, we can set-up the placeholder variables for the training data
   (and some training parameters):

# python optimisation variables
learning_rate = 0.5
epochs = 10
batch_size = 100

# declare the training data placeholders
# input x - for 28 x 28 pixels = 784
x = tf.placeholder(tf.float32, [none, 784])
# now declare the output data placeholder - 10 digits
y = tf.placeholder(tf.float32, [none, 10])

   notice the x input layer is 784 nodes corresponding to the 28 x 28
   (=784) pixels, and the y output layer is 10 nodes corresponding to the
   10 possible digits.  again, the size of x is (? x 784), where the ?
   stands for an as yet unspecified number of samples to be input     this
   is the function of the placeholder variable.

   now we need to setup the weight and bias variables for the three layer
   neural network.  there are always l-1 number of weights/bias tensors,
   where l is the number of layers.  so in this case, we need to setup two
   tensors for each:
# now declare the weights connecting the input to the hidden layer
w1 = tf.variable(tf.random_normal([784, 300], stddev=0.03), name='w1')
b1 = tf.variable(tf.random_normal([300]), name='b1')
# and the weights connecting the hidden layer to the output layer
w2 = tf.variable(tf.random_normal([300, 10], stddev=0.03), name='w2')
b2 = tf.variable(tf.random_normal([10]), name='b2')

   ok, so let   s unpack the above code a little.  first, we declare some
   variables for w1 and b1, the weights and bias for the connections
   between the input and hidden layer.  this neural network will have 300
   nodes in the hidden layer, so the size of the weight tensor w1 is [784,
   300].  we initialise the values of the weights using a random normal
   distribution with a mean of zero and a standard deviation of 0.03.
   tensorflow has a replicated version of the [27]numpy random normal
   function, which allows you to create a matrix of a given size populated
   with random samples drawn from a given distribution.  likewise, we
   create w2 and b2 variables to connect the hidden layer to the output
   layer of the neural network.

   next, we have to setup node inputs and id180 of the
   hidden layer nodes:
# calculate the output of the hidden layer
hidden_out = tf.add(tf.matmul(x, w1), b1)
hidden_out = tf.nn.relu(hidden_out)

   in the first line, we execute the standard id127 of the
   weights (w1) by the input vector x and we add the bias b1.  the matrix
   multiplication is executed using the tf.matmul operation.  next, we
   finalise the hidden_out operation by applying a [28]rectified linear
   unit activation function to the id127 plus bias.  note
   that tensorflow has a rectified linear unit activation already setup
   for us, tf.nn.relu.

   this is to execute the following equations, as detailed in the
   [29]neural networks tutorial:

   \begin{align}
   z^{(l+1)} &= w^{(l)} x + b^{(l)} \\
   h^{(l+1)} &= f(z^{(l+1)})
   \end{align}

   now, let   s setup the output layer, y_:
# now calculate the hidden layer output - in this case, let's use a softmax acti
vated
# output layer
y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, w2), b2))

   again we perform the weight multiplication with the output from the
   hidden layer (hidden_out) and add the bias, b2.  in this case, we are
   going to use a [30]softmax activation for the output layer     we can use
   the included tensorflow softmax function tf.nn.softmax.

   we also have to include a cost or id168 for the optimisation /
   id26 to work on. here we   ll use the cross id178 cost
   function, represented by:

   $$j = -\frac{1}{m} \sum_{i=1}^m \sum_{j=1}^n y_j^{(i)}log(y_j\_^{(i)})
   + (1     y_j^{(i)})log(1     y_j\_^{(i)})$$

   where $y_j^{(i)}$ is the ith training label for output node j,
   $y_j\_^{(i)}$ is the ith predicted label for output node j, m is the
   number of training / batch samples and n is the number .  there are two
   operations occurring in the above equation.  the first is the summation
   of the logarithmic products and additions across all the output nodes.
   the second is taking a mean of this summation across all the training
   samples.  we can implement this cross id178 cost function
   in tensorflow with the following code:
y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)
cross_id178 = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)
                         + (1 - y) * tf.log(1 - y_clipped), axis=1))

   some explanation is required.  the first line is an operation
   converting the output y_ to a clipped version, limited between 1e-10 to
   0.999999.  this is to make sure that we never get a case were we have a
   log(0) operation occurring during training     this would return nan and
   break the training process.  the second line is the cross id178
   calculation.

   to perform this calculation, first we use tensorflow   s tf.reduce_sum
   function     this function basically takes the sum of a given axis of the
   tensor you supply.  in this case, the tensor that is supplied is the
   element-wise cross-id178 calculation for a single node and training
   sample i.e.: $y_j^{(i)}log(y_j\_^{(i)}) + (1     y_j^{(i)})log(1    
   y_j\_^{(i)})$.  remember that y and y_clipped in the above calculation
   are (m x 10) tensors     therefore we need to perform the first sum over
   the second axis.  this is specified using the axis=1 argument, where
      1    actually refers to the second axis when we have a zero-based
   indices system like python.

   after this operation, we have an (m x 1) tensor.  to take the mean of
   this tensor and complete our cross id178 cost calculation (i.e.
   execute this part $\frac{1}{m} \sum_{i=1}^m$), we use tensorflow   s
   tf.reduce_mean function.  this function simply takes the mean of
   whatever tensor you provide it.  so now we have a cost function that we
   can use in the training process.

   let   s setup the optimiser in tensorflow:
# add an optimiser
optimiser = tf.train.gradientdescentoptimizer(learning_rate=learning_rate).minim
ize(cross_id178)

   here we are just using the id119 optimiser provided by
   tensorflow.  we initialize it with a learning rate, then specify what
   we want it to do     i.e. minimise the cross id178 cost operation we
   created.  this function will then perform the id119 (for
   more details on id119 see [31]here and [32]here) and the
   [33]id26 for you.  how easy is that?  tensorflow has a
   library of popular neural network training optimisers, see [34]here.

   finally, before we move on to the main show, were we actually run the
   operations, let   s setup the variable initialisation operation and an
   operation to measure the accuracy of our predictions:
# finally setup the initialisation operator
init_op = tf.global_variables_initializer()

# define an accuracy assessment operation
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

   the correct prediction operation correct_prediction makes use of the
   tensorflow tf.equal function which returns true or false depending on
   whether to arguments supplied to it are equal.  the tf.argmax function
   is the same as the [35]numpy argmax function, which returns the index
   of the maximum value in a vector / tensor.  therefore, the
   correct_prediction operation returns a tensor of size (m x 1) of true
   and false values designating whether the neural network has correctly
   predicted the digit.  we then want to calculate the mean accuracy from
   this tensor     first we have to cast the type of the correct_prediction
   operation from a boolean to a tensorflow float in order to perform
   the reduce_mean operation.  once we   ve done that, we now have an
   accuracy operation ready to assess the performance of our neural
   network.

3.2 setting up the training

   we now have everything we need to setup the training process of our
   neural network.  i   m going to show the full code below, then talk
   through it:
 # start the session
 with tf.session() as sess:
    # initialise the variables
    sess.run(init_op)
    total_batch = int(len(mnist.train.labels) / batch_size)
    for epoch in range(epochs):
         avg_cost = 0
         for i in range(total_batch):
             batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)
              _, c = sess.run([optimiser, cross_id178],
                          feed_dict={x: batch_x, y: batch_y})
             avg_cost += c / total_batch
         print("epoch:", (epoch + 1), "cost =", "{:.3f}".format(avg_cost))
    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labe
ls}))

   stepping through the lines above, the first couple relate to setting up
   the with statement and running the initialisation operation.  the third
   line relates to our mini-batch training scheme that we are going to run
   for this neural network.  if you want to know about mini-batch gradient
   descent, check out this [36]post.  in the third line, we are
   calculating the number of batches to run through in each training
   epoch.  after that, we loop through each training epoch and initialise
   an avg_cost variable to keep track of the average cross id178 cost
   for each epoch.  the next line is where we extract a randomised batch
   of samples, batch_x and batch_y, from the mnist training dataset.  the
   tensorflow provided mnist dataset has a handy utility function,
   next_batch, that makes it easy to extract batches of data for training.

   the following line is where we run two operations.  notice that
   sess.run is capable of taking a list of operations to run as its first
   argument.  in this case, supplying [optimiser, cross_id178] as the
   list means that both these operations will be performed.  as such, we
   get two outputs, which we have assigned to the variables _ and c.  we
   don   t really care too much about the output from the optimiser
   operation but we want to know the output from the cross_id178
   operation     which we have assigned to the variable c.  note, we run the
   optimiser (and cross_id178) operation on the batch samples.  in the
   following line, we use c to calculate the average cost for the epoch.

   finally, we print out our progress in the average cost, and after the
   training is complete, we run the accuracy operation to print out the
   accuracy of our trained network on the test set.  running this program
   produces the following output:
epoch: 1 cost = 0.586
epoch: 2 cost = 0.213
epoch: 3 cost = 0.150
epoch: 4 cost = 0.113
epoch: 5 cost = 0.094
epoch: 6 cost = 0.073
epoch: 7 cost = 0.058
epoch: 8 cost = 0.045
epoch: 9 cost = 0.036
epoch: 10 cost = 0.027

training complete!
0.9787

   there we go     approximately 98% accuracy on the test set, not bad.  we
   could do a number of things to improve the model, such as
   regularisation (see this [37]tips and tricks post), but here we are
   just interested in exploring tensorflow.  you can also use tensorboard
   visualisation to look at things like the increase in accuracy over the
   epochs:
   tensorflow tutorial - tensorboard accuracy plot

   tensorboard plot of the increase in accuracy over 10 epochs

   in a future article, i   ll introduce you to tensorboard visualisation,
   which is a really nice feature of tensorflow.  for now, i hope this
   tutorial was instructive and helps get you going on the tensorflow
   journey.  just a reminder, you can check out the code for this post
   [38]here.  i   ve also written an article that shows you how to build
   more complex neural networks such as [39]convolution neural networks,
   [40]recurrent neural networks and [41]id97 natural language models
   in tensorflow.  you also might want to check out a higher level deep
   learning library that sits on top of tensorflow called keras     see
   [42]my keras tutorial.

   have fun!
   [ebook_store ebook_id=   653   ]
     __________________________________________________________________

eager to learn more? get the book [43]here
     __________________________________________________________________


about the author

     tomas says:
   [44]april 14, 2017 at 8:28 am

   hi, iike the idea of explaining using the simple equation, great idea.
   i didn   t get the tensor/array output could you past all the code. also
   the code for the tensorboard visualization would be nice (i know you
   are planning to go into that in more detail in another tutorial, but
   would be great to take a look at now.
     * andy says:
       [45]april 15, 2017 at 5:51 am
       hi tomas     no problems, you can find the code here :
       [46]https://github.com/adventuresinml/adventures-in-ml-code. i   ve
       put another link to this repository in the article to make it
       clearer. thanks for the feedback

     [47]bablofil says:
   [48]april 19, 2017 at 3:19 am

   thanks, great article.

     [49]lwebzem says:
   [50]may 5, 2017 at 1:16 am

   i used the code from this post and it worked instantly. this is a great
   article and great code so i added the link to the collection of neural
   networks with python.

     dtdzung says:
   [51]july 17, 2017 at 1:02 pm

   great article. thank you

     [52]xxx says:
   [53]july 21, 2017 at 10:21 am

   asking   ue  tions are really fast  dious thing if you are not
   understand  ng anything completely, but this piece of   riting presents
   nice understanding    et.

     lucian says:
   [54]july 28, 2017 at 11:10 am

   hi

   great tutorial, one of the (few..) best explained on the web.

   i have a question: it is possible to give an image path to the model so
   it can recognize the content of the image (a number in this case) and
   print accuracy ?

   i alredy have a tensorflow model which predict given numbers (based on
   mnist) but it fails a bit. i would like to print the accuracy or,
   better, use a model like this with tf deeply integrated to predict
   these numbers.

   thank you
     * andy says:
       [55]july 28, 2017 at 7:44 pm
       hi lucian, thanks for the comment. i   m sorry, i   m not quite sure
       what you mean by image path? the code given here does predict the
       mnist numbers and prints the accuracy. are you asking whether there
       is a more accurate deep learning model to predict numbers and other
       image content? if so, there is     a convolutional neural network.
       check out this post to learn how to implement in tensorflow:
       [56]convolutional neural networks tutorial in tensorflow
       i hope this helps

     john mcdonald says:
   [57]august 10, 2017 at 10:38 pm

   shouldn   t
   a=d*e in the 1st paragraph breakdown? not a=d*c
     * andy says:
       [58]august 11, 2017 at 7:22 pm
       hi john, yes it should     thanks for picking this up. i   ve fixed it
          + john mcdonald says:
            [59]august 12, 2017 at 12:58 pm
            no problem     i initially thought i might have missed a new way
            to break down functions!!

     pasindu tennage says:
   [60]august 17, 2017 at 5:17 pm

   thank you very much for posting this. very informative. keep up the
   good work     



   mauro says:
   [61]october 18, 2017 at 9:06 am

   hi andy,

   amazing tutorial, i   d say the best i   ve found in 2 days of google
   searches!

   as an aside, would you be able to write a similar tutorial for a
   regression example? or using different training methods?

   i know that it is just a matter of changing the softmax to maybe relu
   or something like that, and changing the number of output neurons.
   however i feel like it would be really helpful for someone who is just
   getting started, as there is really no tutorial on how to build a nn
   using tf for a regression problem. if you don   t have the time, would
   you be able to just post some code? i reckon you could re-use most of
   the code written here.

   great job anyway!



   rouahi says:
   [62]october 20, 2017 at 1:26 pm

   hi andy,
   thank you very much for posting this tutorial.
   i tried to run the convolutional_neural_network_tutorial.py code, but
   my computer crashes.
   the characteristics of my computer are the following:

   processor: intel i5-7200 cpu 2.50ghz, 2.70ghz
   ram: 4 gb
   operating system: windows 10

   is the size of my ram is insufficient to execute this code?
   thank you.



   gee m says:
   [63]march 16, 2018 at 3:27 pm

   really great article, thank you very much for the good work!



   oz f says:
   [64]march 27, 2018 at 8:44 pm

   great article. the code worked perfectly. i used tensorflow running on
   docker and had no issues following up. thanks a lot.



   eli says:
   [65]april 29, 2018 at 5:39 pm

   thank you so much.



   shashank says:
   [66]may 20, 2018 at 10:58 am

   this blog is the best way to dive into tf for the first timers.
   appreciate your work

     * andy says:
       [67]may 20, 2018 at 9:34 pm
       glad it is a help for you

   ____________________ (button)

   recent posts
     * [68]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [69]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [70]keras, eager and tensorflow 2.0     a new tf paradigm
     * [71]introduction to tensorboard and tensorflow visualization
     * [72]tensorflow eager tutorial

   recent comments
     * andry on [73]neural networks tutorial     a pathway to deep learning
     * sandipan on [74]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [75]neural networks tutorial     a pathway to deep learning
     * martin on [76]neural networks tutorial     a pathway to deep learning
     * uri on [77]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [78]march 2019
     * [79]january 2019
     * [80]october 2018
     * [81]september 2018
     * [82]august 2018
     * [83]july 2018
     * [84]june 2018
     * [85]may 2018
     * [86]april 2018
     * [87]march 2018
     * [88]february 2018
     * [89]november 2017
     * [90]october 2017
     * [91]september 2017
     * [92]august 2017
     * [93]july 2017
     * [94]may 2017
     * [95]april 2017
     * [96]march 2017

   categories
     * [97]amazon aws
     * [98]cntk
     * [99]convolutional neural networks
     * [100]cross id178
     * [101]deep learning
     * [102]gensim
     * [103]gpus
     * [104]keras
     * [105]id168s
     * [106]lstms
     * [107]neural networks
     * [108]nlp
     * [109]optimisation
     * [110]pytorch
     * [111]recurrent neural networks
     * [112]id23
     * [113]tensorboard
     * [114]tensorflow
     * [115]tensorflow 2.0
     * [116]weight initialization
     * [117]id97

   meta
     * [118]log in
     * [119]entries rss
     * [120]comments rss
     * [121]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [122]thrive themes | powered by [123]wordpress

   (button) close dialog

   session expired

   [124]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[125]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/python-tensorflow-tutorial/&format=xml
   3. https://www.adventuresinmachinelearning.com/
   4. https://adventuresinmachinelearning.com/about/
   5. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   6. https://adventuresinmachinelearning.com/contact/
   7. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   8. https://adventuresinmachinelearning.com/author/andyt81/
   9. https://adventuresinmachinelearning.com/category/deep-learning/
  10. https://adventuresinmachinelearning.com/
  11. https://adventuresinmachinelearning.com/category/deep-learning/
  12. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  13. http://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments
  14. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  15. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  16. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/
  17. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  18. https://www.tensorflow.org/install/
  19. https://github.com/adventuresinml/adventures-in-ml-code
  20. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  21. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  22. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  23. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  24. https://www.tensorflow.org/api_docs/python/tf/dtype
  25. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  26. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  27. https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html
  28. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  29. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  30. https://en.wikipedia.org/wiki/softmax_function
  31. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  32. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  33. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  34. https://www.tensorflow.org/api_guides/python/train
  35. https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html
  36. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  37. https://adventuresinmachinelearning.com/improve-neural-networks-part-1/
  38. https://github.com/adventuresinml/adventures-in-ml-code
  39. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  40. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/
  41. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  42. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  43. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  44. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/163
  45. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/164
  46. https://github.com/adventuresinml/adventures-in-ml-code
  47. https://bablofil.ru/
  48. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/165
  49. http://intelligentonlinetools.com/cgi-bin/nn/neural_networks_with_python.cgi
  50. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/168
  51. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/304
  52. https://google.com/
  53. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/321
  54. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/348
  55. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/350
  56. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  57. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/432
  58. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/433
  59. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/434
  60. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/445
  61. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/589
  62. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/595
  63. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/611
  64. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/612
  65. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/614
  66. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/617
  67. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/#comments/618
  68. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
  69. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  70. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
  71. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
  72. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  73. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
  74. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  75. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
  76. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
  77. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
  78. https://adventuresinmachinelearning.com/2019/03/
  79. https://adventuresinmachinelearning.com/2019/01/
  80. https://adventuresinmachinelearning.com/2018/10/
  81. https://adventuresinmachinelearning.com/2018/09/
  82. https://adventuresinmachinelearning.com/2018/08/
  83. https://adventuresinmachinelearning.com/2018/07/
  84. https://adventuresinmachinelearning.com/2018/06/
  85. https://adventuresinmachinelearning.com/2018/05/
  86. https://adventuresinmachinelearning.com/2018/04/
  87. https://adventuresinmachinelearning.com/2018/03/
  88. https://adventuresinmachinelearning.com/2018/02/
  89. https://adventuresinmachinelearning.com/2017/11/
  90. https://adventuresinmachinelearning.com/2017/10/
  91. https://adventuresinmachinelearning.com/2017/09/
  92. https://adventuresinmachinelearning.com/2017/08/
  93. https://adventuresinmachinelearning.com/2017/07/
  94. https://adventuresinmachinelearning.com/2017/05/
  95. https://adventuresinmachinelearning.com/2017/04/
  96. https://adventuresinmachinelearning.com/2017/03/
  97. https://adventuresinmachinelearning.com/category/amazon-aws/
  98. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  99. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
 100. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
 101. https://adventuresinmachinelearning.com/category/deep-learning/
 102. https://adventuresinmachinelearning.com/category/nlp/gensim/
 103. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
 104. https://adventuresinmachinelearning.com/category/deep-learning/keras/
 105. https://adventuresinmachinelearning.com/category/loss-functions/
 106. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
 107. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
 108. https://adventuresinmachinelearning.com/category/nlp/
 109. https://adventuresinmachinelearning.com/category/optimisation/
 110. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
 111. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
 112. https://adventuresinmachinelearning.com/category/reinforcement-learning/
 113. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
 114. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
 115. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
 116. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
 117. https://adventuresinmachinelearning.com/category/nlp/id97/
 118. https://adventuresinmachinelearning.com/wp-login.php
 119. https://adventuresinmachinelearning.com/feed/
 120. https://adventuresinmachinelearning.com/comments/feed/
 121. https://wordpress.org/
 122. https://www.thrivethemes.com/
 123. http://www.wordpress.org/
 124. https://adventuresinmachinelearning.com/wp-login.php
 125. http://adventuresinmachinelearning.com/python-tensorflow-tutorial/

   hidden links:
 127. https://adventuresinmachinelearning.com/author/andyt81/
