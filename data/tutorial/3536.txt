   (button) toggle navigation [1]i am trask
     * [2]home
     * [3]about
     * [4]contact
     *
     *
     *
     *
     *

hinton's dropout in 3 lines of python

how to install dropout into a neural network by only changing 3 lines of
python.

   posted by iamtrask on july 28, 2015

   summary: dropout is a vital feature in almost every state-of-the-art
   neural network implementation. this tutorial teaches how to install
   dropout into a neural network in only a few lines of python code. those
   who walk through this tutorial will finish with a working dropout
   implementation and will be empowered with the intuitions to install it
   and tune it in any neural network they encounter.

   followup post: i intend to write a followup post to this one adding
   popular features leveraged by [5]state-of-the-art approaches. i'll
   tweet it out when it's complete [6]@iamtrask. feel free to follow if
   you'd be interested in reading more and thanks for all the feedback!

just give me the code:

import numpy as np
x = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])
y = np.array([[0,1,1,0]]).t
alpha,hidden_dim,dropout_percent,do_dropout = (0.5,4,0.2,true)
synapse_0 = 2*np.random.random((3,hidden_dim)) - 1
synapse_1 = 2*np.random.random((hidden_dim,1)) - 1
for j in xrange(60000):
    layer_1 = (1/(1+np.exp(-(np.dot(x,synapse_0)))))
    if(do_dropout):
        layer_1 *= np.random.binomial([np.ones((len(x),hidden_dim))],1-dropout_p
ercent)[0] * (1.0/(1-dropout_percent))
    layer_2 = 1/(1+np.exp(-(np.dot(layer_1,synapse_1))))
    layer_2_delta = (layer_2 - y)*(layer_2*(1-layer_2))
    layer_1_delta = layer_2_delta.dot(synapse_1.t) * (layer_1 * (1-layer_1))
    synapse_1 -= (alpha * layer_1.t.dot(layer_2_delta))
    synapse_0 -= (alpha * x.t.dot(layer_1_delta))

part 1: what is dropout?

   as discovered in the [7]previous post, a neural network is a glorified
   search problem. each node in the neural network is searching for
   correlation between the input data and the correct output data.

   consider the graphic above from the previous post. the line represents
   the error the network generates for every value of a particular weight.
   the low-points (read: low error) in that line signify the weight
   "finding" points of correlation between the input and output data. the
   balls in the picture signify various weights. they are trying to find
   those low points.

   consider the color. the ball's initial positions are randomly generated
   (just like weights in a neural network). if two balls randomly start in
   the same colored zone, they will converge to the same point. this makes
   them redundant! they're wasting computation and memory! this is exactly
   what happens in neural networks.

   why dropout: dropout helps prevent weights from converging to identical
   positions. it does this by randomly turning nodes off when forward
   propagating. it then back-propagates with all the nodes turned on.
   let   s take a closer look.

part 2: how do i install and tune dropout?

   the highlighted code above demonstrates how to install dropout. to
   perform dropout on a layer, you randomly set some of the layer's values
   to 0 during forward propagation. this is demonstrated on line 10.

   line 9: parameterizes using dropout at all. you see, you only want to
   use dropout during training. do not use it at runtime or on your
   testing dataset.

   edit: line 9: has a second portion to increase the size of the values
   being propagated forward. this happens in proportion to the number of
   values being turned off. a simple intuition is that if you're turning
   off half of your hidden layer, you want to double the values that are
   pushing forward so that the output compensates correctly. many thanks
   to [8]@karpathy for catching this one.

tuning best practice

   line 4: parameterizes the dropout_percent. this affects the id203
   that any one node will be turned off. a good initial configuration for
   this for hidden layers is 50%. if applying dropout to an input layer,
   it's best to not exceed 25%.

   hinton advocates tuning dropout in conjunction with tuning the size of
   your hidden layer. increase your hidden layer size(s) with dropout
   turned off until you perfectly fit your data. then, using the same
   hidden layer size, train with dropout turned on. this should be a
   nearly optimal configuration. turn off dropout as soon as you're done
   training and voila! you have a working neural network!

want to work in machine learning?

   one of the best things you can do to learn machine learning is to have
   a job where you're practicing machine learning professionally. i'd
   encourage you to check out the [9]positions at digital reasoning in
   your job hunt. if you have questions about any of the positions or
   about life at digital reasoning, feel free to send me a message on
   [10]my linkedin. i'm happy to hear about where you want to go in life,
   and help you evaluate whether digital reasoning could be a good fit.

   if none of the positions above feel like a good fit. continue your
   search! machine learning expertise is one of the most valuable skills
   in the job market today, and there are many firms looking for
   practitioners. perhaps some of these services below will help you in
   your hunt.
   machine learning jobs
   what: title, keywords_____
   where: city, state, or zip_
   find jobs
   [11]jobs by indeed
   [12]view more job search results
     __________________________________________________________________

     * [13]    previous post
     * [14]next post    

     *
     *
     *

   copyright    i am trask 2018

references

   visible links
   1. https://iamtrask.github.io/
   2. https://iamtrask.github.io/
   3. https://iamtrask.github.io/about/
   4. https://iamtrask.github.io/contact/
   5. http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html
   6. https://twitter.com/iamtrask
   7. http://iamtrask.github.io/2015/07/27/python-network-part2/
   8. https://twitter.com/karpathy
   9. http://www.digitalreasoning.com/careers
  10. https://www.linkedin.com/profile/view?id=226572677&trk=nav_responsive_tab_profile
  11. http://www.indeed.com/
  12. http://jobsearch.monster.com/jobs/?q=machine-learning
  13. https://2015/07/27/python-network-part2/
  14. https://2015/11/15/anyone-can-code-lstm/

   hidden links:
  16. https://iamtrask.github.io/feed.xml
  17. https://twitter.com/iamtrask
  18. https://github.com/iamtrask
