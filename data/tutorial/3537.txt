   (button) toggle navigation [1]i am trask
     * [2]home
     * [3]about
     * [4]contact
     *
     *
     *
     *
     *

deep learning without id26

tutorial: deepmind's synthetic gradients

   posted by iamtrask on march 21, 2017

   tldr: in this blogpost, we're going to prototype (from scratch) and
   learn the intuitions behind deepmind's recently proposed [5]decoupled
   neural interfaces using synthetic gradients paper.

   i typically tweet out new blogposts when they're complete at
   [6]@iamtrask. feel free to follow if you'd be interested in reading
   more in the future and thanks for all the feedback!

part 1: synthetic gradients overview

   normally, a neural network compares its predictions to a dataset to
   decide how to update its weights. it then uses id26 to
   figure out how each weight should move in order to make the prediction
   more accurate. however, with synthetic gradients, individual layers
   instead make a "best guess" for what they think the data will say, and
   then update their weights according to this guess. this "best guess" is
   called a synthetic gradient. the data is only used to help update each
   layer's "guesser" or synthetic gradient generator. this allows for
   (most of the time), individual layers to learn in isolation, which
   increases the speed of training.

   edit: this [7]paper also adds great intuitions on how/why synthetic
   gradients are so effective

      source: [8]decoupled neural interfaces using synthetic gradients

   the graphic above (from the paper) gives a very intuitive picture for
   what   s going on (from left to right). the squares with rounded off
   corners are layers and the diamond shaped objects are (what i call) the
   synthetic gradient generators. let   s start with how a regular neural
   network layer is updated.

part 2: using synthetic gradients

   let's start by ignoring how the synthetic gradients are created and
   instead just look at how the are used. the far left box shows how these
   can work to update the first layer in a neural network. the first layer
   forward propagates into the synthetic gradient generator (m i+1), which
   then returns a gradient. this gradient is used instead of the real
   gradient (which would take a full forward propagation and
   id26 to compute). the weights are then updated as normal,
   pretending that this synthetic gradient is the real gradient. if you
   need a refresher on how weights are updated with gradients, check out
   [9]a neural network in 11 lines of python and perhaps the followup post
   on [10]id119.

   so, in short, synthetic gradients are used just like normal gradients,
   and for some magical reason they seem to be accurate (without
   consulting the data)! seems like magic? let   s see how they   re made.

part 3: generating synthetic gradients

   ok, this part is really clever, and frankly it's amazing that it works.
   how do you generate synthetic gradients for a neural network? well, you
   use another network of course! synthetic gradient genenerators are
   nothing other than a neural network that is trained to take the output
   of a layer and predict the gradient that will likely happen at that
   layer.

a sidenote: related work by geoffrey hinton

   this actually reminds me of some work that geoffrey hinton did a couple
   years ago in which he showed that [11]random feedback weights support
   learning in deep neural networks. basically, you can backpropagate
   through randomly generated matrices and still accomplish learning.
   furthermore, he showed that it had a kind of id173 affect. it
   was some interesting work for sure.

   ok, back to synthetic gradients. so, now we know that synthetic
   gradients are trained by another neural network that learns to predict
   the gradient at a step given the output at that step. the paper also
   says that any other relevant information could be used as input to the
   synthetic gradient generator network, but in the paper it seems like
   just the output of the layer is used for normal feedforwards networks.
   furthermore, the paper even states that a single linear layer can be
   used as the synthetic gradient generator. amazing. we're going to try
   that out.

how do we learn the network that generates synthetic gradients?

   this begs the question, how do we learn the neural networks that
   generate our synthetic gradients? well, as it turns out, when we
   perform full forward and id26, we actually get the "correct"
   gradient. we can compare this to our "synthetic" gradient in the same
   way we normally compare the output of a neural network to the dataset.
   thus, we can train our synthetic gradient networks by pretending that
   our "true gradients" are coming from from mythical dataset... so we
   train them like normal. neat!

wait... if our synthetic gradient network requires backprop... what's the
point?

   excellent question! the whole point of this technique was to allow
   individual neural networks to train without waiting on each other to
   finish forward and backpropagating. if our synthetic gradient networks
   require waiting for a full forward/backprop step, then we're back where
   we started but with more computation going on (even worse!). for the
   answer, let's revisit this visualization from the paper.

      source: [12]decoupled neural interfaces using synthetic gradients

   focus on the second section from the left. see how the gradient (m i+2)
   backpropagates through (f i+1) and into m(i+1)? as you can see, each
   synthetic gradient generator is actually only trained using the
   synthetic gradients generated from the next layer. thus, only the last
   layer actually trains on the data. all the other layers, including the
   synthetic gradient generator networks, train based on synthetic
   gradients. thus, the network can train with each layer only having to
   wait on the synthetic gradient from the following layer (which has no
   other dependencies). very cool!

part 4: a baseline neural network

   time to start coding! to get things started (so we have an easier frame
   of reference), i'm going to start with a vanilla neural network trained
   with id26, styled in the same way as [13]a neural network in
   11 lines of python. (so, if it doesn't make sense, just go read that
   post and come back). however, i'm going to add an additional layer, but
   that shoudln't be a problem for comprehension. i just figured that
   since we're all about reducing dependencies, having more layers might
   make for a better illustration.

   as far as the dataset we're training on, i'm going to genereate a
   synthetic dataset (har! har!) using binary addition. so, the network
   will take two, random binary numbers and predict their sum (also a
   binary number). the nice thing is that this gives us the flexibility to
   increase the dimensionality (~difficulty) of the task as needed. here's
   the code for generating the dataset.

   iframe: [14]https://trinket.io/embed/python3/dd449f1a8a

   and here's the code for a vanilla neural network training on that
   dataset.

   iframe: [15]https://trinket.io/embed/python3/835c9d729f

   now, at this point i really feel its necessary to do something that i
   almost never do in the context of learning, add a bit of object
   oriented structure. normally, this obfuscates the network a little bit
   and makes it harder to see (from a high level) what's going on
   (relative to just reading a python script). however, since this post is
   about "decoupled neural interfaces" and the benefits that they offer,
   it's really pretty hard to explain things without actually having those
   interfaces be reasonably decoupled.so, to make learning a little bit
   easier, i'm first going to convert the network above into exactly the
   same network but with a "layer" class object that we'll soon convert
   into a dni. let's take a look at this layer object.

class layer(object):

    def __init__(self,input_dim, output_dim,nonlin,nonlin_deriv):

        self.weights = (np.random.randn(input_dim, output_dim) * 0.2) - 0.1
        self.nonlin = nonlin
        self.nonlin_deriv = nonlin_deriv

    def forward(self,input):
        self.input = input
        self.output = self.nonlin(self.input.dot(self.weights))
        return self.output

    def backward(self,output_delta):
        self.weight_output_delta = output_delta * self.nonlin_deriv(self.output)
        return self.weight_output_delta.dot(self.weights.t)

    def update(self,alpha=0.1):
        self.weights -= self.input.t.dot(self.weight_output_delta) * alpha



   in this layer class, we have several class variables. weights is the
   matrix we use for a linear transformation from input to output (just
   like a normal linear layer). optionally, we can also include an output
   nonlin function which will put a non-linearity on the output of our
   network. if we don't want a non-linearity, we can simply set this value
   to lambda x:x. in our case, we're going to pass in the "sigmoid"
   function.

   the second function we pass in is nonlin_deriv which is a special
   derivative function. this function needs to take the output from our
   nonlinearity and convert it to the derivative. for sigmoid, this is
   simply (out * (1 - out)) where "out" is the output of the sigmoid. this
   particular function exists for pretty much all of the common neural
   network nonlinearities.

   now, let's take a look at the various methods in this class. forward
   does what it's name implies. it forward propagates through the layer,
   first through a linear transformation, and then through the nonlin
   function. backward accepts a output_delta paramter, which represents
   the real gradient (as opposed to a synthetic one) coming back from the
   next layer during id26. we then use this to compute
   self.weight_output_delta, which is the derivative at the output of our
   weights (just inside the nonlinearity). finally, it backpropagates the
   error to send to the previous layer and returns it.

   update is perhaps the simplest method of all. it simply takes the
   derivative at the output of the weights and uses it to perform a weight
   update. if any of these steps don't make sense to you, again, consult
   [16]a neural network in 11 lines of python and come back. if everything
   makes sense, then let's see our layer objects in the context of
   training.

layer_1 = layer(input_dim,layer_1_dim,sigmoid,sigmoid_out2deriv)
layer_2 = layer(layer_1_dim,layer_2_dim,sigmoid,sigmoid_out2deriv)
layer_3 = layer(layer_2_dim, output_dim,sigmoid, sigmoid_out2deriv)

for iter in range(iterations):
    error = 0

    for batch_i in range(int(len(x) / batch_size)):
        batch_x = x[(batch_i * batch_size):(batch_i+1)*batch_size]
        batch_y = y[(batch_i * batch_size):(batch_i+1)*batch_size]

        layer_1_out = layer_1.forward(batch_x)
        layer_2_out = layer_2.forward(layer_1_out)
        layer_3_out = layer_3.forward(layer_2_out)

        layer_3_delta = layer_3_out - batch_y
        layer_2_delta = layer_3.backward(layer_3_delta)
        layer_1_delta = layer_2.backward(layer_2_delta)
        layer_1.backward(layer_1_delta)

        layer_1.update()
        layer_2.update()
        layer_3.update()


   given a dataset x and y, this is how we use our new layer objects. if
   you compare it to the script from before, pretty much everything
   happens in pretty much the same places. i just swapped out the script
   versions of the neural network for the method calls

   so, all we've really done is taken the steps in the script from the
   previous neural network and split them into distinct functions inside
   of a class. below, we can see this layer in action.

   iframe: [17]https://trinket.io/embed/python3/b78063bef4

   if you pull both the previous network and this network into jupyter
   notebooks, you'll see that the random seeds cause these networks to
   have exactly the same values. it seems that trinket.io might not have
   perfect random seeding, such that these networks reach nearly identical
   values. however, i assure you that the networks are identical. if this
   network doesn't make sense to you, don't move on. be sure you're
   comfortable with how this abstraction works before moving forward, as
   it's going to get a bit more complex below.

part 6: synthetic gradients based on layer output

   ok, so now we're going to use a very similar interface to the onee to
   integrate what we learned about synthetic gradients into our layer
   object (and rename it dni). first, i'm going to show you the class, and
   then i'll explain it. check it out!

class dni(object):

    def __init__(self,input_dim, output_dim,nonlin,nonlin_deriv,alpha = 0.1):

        # same as before
        self.weights = (np.random.randn(input_dim, output_dim) * 0.2) - 0.1
        self.nonlin = nonlin
        self.nonlin_deriv = nonlin_deriv


        # new stuff
        self.weights_synthetic_grads = (np.random.randn(output_dim,output_dim) *
 0.2) - 0.1
        self.alpha = alpha

    # used to be just "forward", but now we update during the forward pass using
 synthetic gradients :)
    def forward_and_synthetic_update(self,input):

        # cache input
        self.input = input

        # forward propagate
        self.output = self.nonlin(self.input.dot(self.weights))

        # generate synthetic gradient via simple linear transformation
        self.synthetic_gradient = self.output.dot(self.weights_synthetic_grads)

        # update our regular weights using synthetic gradient
        self.weight_synthetic_gradient = self.synthetic_gradient * self.nonlin_d
eriv(self.output)
        self.weights += self.input.t.dot(self.weight_synthetic_gradient) * self.
alpha

        # return backpropagated synthetic gradient (this is like the output of "
backprop" method from the layer class)
        # also return forward propagated output (feels weird i know... )
        return self.weight_synthetic_gradient.dot(self.weights.t), self.output

    # this is just like the "update" method from before... except it operates on
 the synthetic weights
    def update_synthetic_weights(self,true_gradient):
        self.synthetic_gradient_delta = self.synthetic_gradient - true_gradient
        self.weights_synthetic_grads += self.output.t.dot(self.synthetic_gradien
t_delta) * self.alpha



   so, the first big change. we have some new class variables. the only
   one that really matters is the self.weights_synthetic_grads variable,
   which is our synthetic generator neural network (just a linear layer...
   i.e., ...just a matrix).

   forward and synthetic update:the forward method has changed to
   forward_and_synthetic_update. remember how we don't need any other part
   of the network to make our weight update? this is where the magic
   happens. first, forward propagation occurs like normal (line 22). then,
   we generate our synthetic gradient by passing our output through a
   non-linearity. this part could be a more complicated neural network,
   but we've instead decided to keep things simple and just use a simple
   linear layer to generate our synthetic gradients. after we've got our
   gradient, we go ahead and update our normal weights (lines 28 and 29).
   finally, we backpropagate our synthetic gradient from the output of the
   weights to the input so that we can send it to the previous layer.

   update synthetic gradient: ok, so the gradient that we returned at the
   end of the "forward" method. that's what we're going to accept into the
   update_synthetic_gradient method from the next layer. so, if we're at
   layer 2, then layer 3 returns a gradient from its
   forward_and_synthetic_update method and that gets input into layer 2's
   update_synthetic_weights. then, we simply update our synthetic weights
   just like we would a normal neural network. we take the input to the
   synthetic gradient layer (self.output), and then perform an average
   outer product (matrix transpose -> matrix mul) with the output delta.
   it's no different than learning in a normal neural network, we've just
   got some special inputs and outputs in leau of data

   ok! let's see it in action.

   iframe: [18]https://trinket.io/embed/python3/7723523911

   id48... things aren't converging as i'd originally want them too. i
   mean, it is converging, but just not really very fast. upon further
   inquiry, the hidden representations all start out pretty flat and
   random (which we're using as input to our gradient generators). in
   other words, two different training examples end up having nearly
   identical output representations at different layers. this seems to
   make it really difficult for the graident generators to do their job.
   in the paper, the solution for this is batch id172, which
   scales all the layer outputs to 0 mean and unit variance. this adds a
   lot of complexity to what is otherwise a fairly simple toy neural
   network. furthermore, the paper also mentions you can use other forms
   of input to the gradietn generators. i'm going to try using the output
   dataset. this still keeps things decoupled (the spirit of the dni) but
   gives something really strong for the network to use to generate
   gradients from the very beginning. let's check it out.

   iframe: [19]https://trinket.io/embed/python3/df0636703f

   and things are training quite a bit faster! thinking about what might
   make for good input to gradient generators is a really fascinating
   concept. perhaps some combination between input data, output data, and
   batch normalized layer output would be optimal (feel free to give it a
   try!) hope you've enjoyed this tutorial!

   i typically tweet out new blogposts when they're complete at
   [20]@iamtrask. feel free to follow if you'd be interested in reading
   more in the future and thanks for all the feedback!
     __________________________________________________________________

     * [21]    previous post
     * [22]next post    

     *
     *
     *

   copyright    i am trask 2018

references

   visible links
   1. https://iamtrask.github.io/
   2. https://iamtrask.github.io/
   3. https://iamtrask.github.io/about/
   4. https://iamtrask.github.io/contact/
   5. https://arxiv.org/pdf/1608.05343.pdf
   6. https://twitter.com/iamtrask
   7. https://arxiv.org/abs/1703.00522
   8. https://arxiv.org/pdf/1608.05343.pdf
   9. http://iamtrask.github.io/2015/07/12/basic-python-network/
  10. http://iamtrask.github.io/2015/07/27/python-network-part2/
  11. https://arxiv.org/abs/1411.0247
  12. https://arxiv.org/pdf/1608.05343.pdf
  13. http://iamtrask.github.io/2015/07/12/basic-python-network/
  14. https://trinket.io/embed/python3/dd449f1a8a
  15. https://trinket.io/embed/python3/835c9d729f
  16. http://iamtrask.github.io/2015/07/12/basic-python-network/
  17. https://trinket.io/embed/python3/b78063bef4
  18. https://trinket.io/embed/python3/7723523911
  19. https://trinket.io/embed/python3/df0636703f
  20. https://twitter.com/iamtrask
  21. https://2017/03/17/safe-ai/
  22. https://2017/06/05/homomorphic-surveillance/

   hidden links:
  24. https://iamtrask.github.io/feed.xml
  25. https://twitter.com/iamtrask
  26. https://github.com/iamtrask
