   (button) toggle navigation [1]i am trask
     * [2]home
     * [3]about
     * [4]contact
     *
     *
     *
     *
     *

tutorial: deep learning in pytorch

an unofficial startup guide.

   posted by iamtrask on january 15, 2017

   edit: [5]a complete revamp of pytorch was released today (jan 18,
   2017), making this blogpost a bit obselete. i will update this post
   with a new quickstart guide soon, but for now you should check out
   their documentation.</a>

   this blogpost will cover:
     * part 1: pytorch installation
     * part 2: matrices and id202 in pytorch
     * part 3: building a feedforward network (starting with [6]a familiar
       one)
     * part 4: the state of pytorch

   pre-requisite knowledge:
     * simple feedforward neural networks ([7]tutorial)
     * basic id119 ([8]tutorial)

   [9]torch is one of the most popular deep learning frameworks in the
   world, dominating much of the research community for the past few years
   (only recently being rivaled by major google sponsored frameworks
   [10]tensorflow and [11]keras). perhaps its only drawback to new users
   has been the fact that it requires one to know [12]lua, a language that
   used to be very uncommon in the machine learning community. even today,
   this barrier to entry can seem a bit much for many new to the field,
   who are already in the midst of learning a tremendous amount, much less
   a completely new programming language.

   however, thanks to the wonderful and billiant [13]hugh perkins, torch
   recently got a new face, [14]pytorch... and it's much more accessible
   to the python hacker turned deep learning extraordinare than it's
   luariffic cousin. i have a passion for tools that make deep learning
   accessible, and so i'd like to lay out a short "unofficial startup
   guide" for those of you interested in taking it for a spin. before we
   get started, however, a question:

   why use a framework like pytorch? in the past, i have advocated
   learning deep learning using only a matrix library. for the purposes of
   actually knowing what goes on under the hood, i think that this is
   essential, and the lessons learned from building things from scratch
   are real gamechangers when it comes to the messiness of tackling real
   world problems with these tools. however, when building neural networks
   in the wild (kaggle competitions, production systems, and research
   experiments), it's best to use a framework.

   why? frameworks such as pytorch allow you (the researcher) to focus
   exclusively on your experiment and iterate very quickly. want to swap
   out a layer? most frameworks will let you do this with a single line
   code change. want to run on a gpu? many frameworks will take care of it
   (sometimes with 0 code changes). if you built the network by hand in a
   matrix library, you might be spending a few hours working out these
   kinds of modifications. so, for learning, use a id202 library
   (like numpy). for applying, use a framework (like pytorch). let's get
   started!

   for new readers: i typically tweet out new blogposts when they're
   complete [15]@iamtrask. feel free to follow if you'd be interested in
   reading more in the future and thanks for all the upvotes on [16]hacker
   news and reddit! they mean a lot to me.

part 1: installation

   install torch: the first thing you need to do is install torch and the
   "nn" package using luarocks. as torch is a very robust framework, the
   [17]installation instructions should work well for you. after that, you
   should be able to run:

     luarocks install nn

   and be good to go. if any of these steps fails to work, copy paste what
   looks like the "error" and error description (should just be one
   sentence or so) from the command line and put it into google (as is
   common practice when installing).

   clone the repository: at the time of writing, pytorch doesn't seem to
   be in the pypi repository. so, we'll need to clone the repo in order to
   install it. assuming you have [18]git already installed (hint hint...
   if not go install it and come back), you should open up your
   [19]terminal application and navigate to an empty folder. personally, i
   have a folder called "laboratory" in my home directory (i.e. "cd
   ~/laboratory/"), which cators to various [20]childhood memories of
   mine. if you're not sure where to put it, feel free to use your desktop
   for now. once there, execute the commands:

     git clone https://github.com/hughperkins/pytorch.git
     cd pytorch/
     pip install -r requirements.txt
     pip install -r test/requirements.txt
     source ~/torch/install/bin/torch-activate
     ./build.sh

   for me, this worked flawlessly, finishing with the statement

     finished processing dependencies for pytorch===4.1.1-snapshot

   if you also see this output at the bottom of your terminal,
   congraulations! you have successfully installed pytorch!

   startup jupyter notebook: while certainly not a requirement, i highly
   recommend playing around with this new tool using [21]jupyter notebok,
   which is definitely best [22]installed using conda. take my word for
   it. install it using conda. all other ways are madness.

part 2: matrices and id202

   in the spirit of starting with the basics, neural networks run on
   id202 libraries. pytorch is no exception. so, the simplest
   building block of pytorch is its id202 library.

   above, i created 4 matrices. notice that the library doesn't call them
   matrices though. it calls them tensors.

   quick primer on tensors: a tensor is just a more generic term than
   matrix or vector. 1-dimensional tensors are vectors. 2-dimensional
   tensors are matrices. 3+ dimensional tensors are just refered to as
   tensors. if you're unfamiliar with these objects, here's a quick
   summary. a vector is "a list of numbers". a matrix is "a list of lists
   of numbers". a 3-d tensor is "a list of lists of lists of numbers". a
   4-d tensor is... see the pattern? for more on how vectors and matrices
   are used to make neural networks, see my first blog post on a
   [23]neural network in 11 lines of python

   pytorch tensors there appear to be 4 major types of tensors in pytorch:
   byte, float, double, and long tensors. each tensor type corresponds to
   the type of number (and more importantly the size/preision of the
   number) contained in each place of the matrix. so, if a 1-d tensor is a
   "list of numbers", a 1-d float tensor is a list of floats. as a general
   rule of thumb, for weight matries we use floattensors. for data
   matrices, we'll likely use either floattensors (for real valued inputs)
   or long tensors (for integers). i'm a little bit surprised to not see
   inttensor anywhere. perhaps it has yet to be wrapped or is just
   non-obvious in the api.

   the final thing to notice is that the matries seem to come with lots of
   rather random looking numbers inside (but not sensibly random like
   "evenly random between 0 and 1"). this is actually a plus of the
   library in my book. let me explain (skip 1 paragraph if you're familiar
   with this concept)

   why seemingly nonsenical values: when you create a new matrix in
   pytorch, the framework goes and "sets aside" enough ram memory to store
   your matrix. however, "setting aside" memory is completely different
   from "changing all the values in that memory to 0". "setting aside"
   memory while also "changing all the values to 0" is more
   computationally expensive. it's nice that this library doesn't assume
   you want the values to be any particular way. instead, it just sets
   aside memory and whatever 1s and 0s happen to be there from the last
   program that used that piece of ram will show up in your matrix. in
   most cases, we're going to set the values in our matrices to be
   something else anyway (say... our input data values or a specific kind
   of random number range). so, the fact that it doesn't pre-set the
   matrix values saves you a bit of processing time, but the user needs to
   recognize that it's their responsibility to actively choose the values
   he/she wants in a matrix. numpy is not like this, which make numpy a
   bit more user friendly but also less computationally efficient.

   basic id202 operations: so, now that we know how to store
   numbers in pytorch, lets talk a bit about how pytorch manipulates them.
   how does one go about doing id202 in pytorch?

   the basic neural network operations: neural networks, amidst all their
   complexity, are actually mostly made up of rather simple operations. if
   you remember from [24]a neural network in 11 lines of python, we can
   build a working network with only matrix-id127,
   vector-id127, elementwise operations (addition,
   subtraction, multiplication, and division), matrix transposition, and a
   handful of elementwise functions (sigmoid, and a special function to
   compute sigmoid's derivative at a point which uses only the
   aforementioned elementwise operations). let's initialize some matrices
   and start with the elementwise operations.

   elementwise operations: above i have given several examples of
   elementwise addition. (simply replacing the "+" sign with - * or / will
   give you the others). these act mostly how you would expect them to
   act. a few hiccups here and there. notably, if you accidentally add two
   tensors that aren't aligned correctly (have different dimensions) the
   python kernel crashes as opposed to throwing an error. it could be just
   my machine, but error handling in wrappers is notoriously time
   consuming to finish. i expect this functionality will be worked out
   soon enough, and as we will see, there's a suitable substitute.

   vector/id127: it appears that the native matrix
   multiplication functionality of torch isn't wrapped. instead, we get to
   use something a bit more familiar. (this feature is really, really
   cool.) much of pytorch can run on native numpy matrices. that's right!
   the convenient functionality of numpy is now integrated with one of the
   most popular deep learning frameworks out there. so, how should you
   really do elementwise and matrix multipliplication? just use numpy! for
   completeness sake, here's the same elementwise operations using
   pytorch's numpy connector.

   and now let's see how to do the basic matrix operations we need for a
   feedforward network.

   other neural functions: finally, we also need to be able to compute
   some nonlinearities efficiently. there are both numpy and native
   wrappers made available which seem to run quite fast. additionally,
   sigmoid has a native implementation (something that numpy does not
   implement), which is quite nice and a bit faster than computing it
   explicitly in numpy.

   i consider the fantastic integration between numpy and pytorch to be
   one of the great selling points of this framework. i personally love
   prototyping with the full control of a matrix library, and pytorch
   really respects this preference as an option. this is so nice relative
   to most other frameworks out there. +1 for pytorch.

part 3: building a feedforward network

   in this section, we really get to start seeing pytorch shine. while
   understanding how matrices are handled is an important pre-requisite to
   learning a framework, the various layers of abstraction are where
   frameworks really become useful. in this section, we're going to take
   the bare bones 3 layer neural network from [25]a previous blogpost and
   convert it to a network using pytorch's neural network abstractions. in
   this way, as we wrap each part of the network with a piece of framework
   functionality, you'll know exactly what pytorch is doing under the
   hood. your goal in this section should be to relate the pytorch
   abstractions (objects, function calls, etc.) in the pytorch network we
   will build with the matrix operations in the numpy neural network
   (pictured below)s.
import pytorch
from pytorch import np

def nonlin(x,deriv=false):
        if(deriv==true):
            return x*(1-x)

        return 1/(1+np.exp(-x))

x = np.array([[0,0,1],
            [0,1,1],
            [1,0,1],
            [1,1,1]])

y = np.array([[0],
                        [1],
                        [1],
                        [0]])

np.random.seed(1)

# randomly initialize our weights with mean 0
syn0 = 2*np.random.random((3,4)) - 1
syn1 = 2*np.random.random((4,1)) - 1

for j in range(60000):

        # feed forward through layers 0, 1, and 2
    l0 = x
    l1 = nonlin(np.dot(l0,syn0))
    l2 = nonlin(np.dot(l1,syn1))

    # how much did we miss the target value?
    l2_error = y - l2

    if (j% 10000) == 0:
        print("error:" + str(np.mean(np.abs(l2_error))))

    # in what direction is the target value?
    # were we really sure? if so, don't change too much.
    l2_delta = l2_error*nonlin(l2,deriv=true)

    # how much did each l1 value contribute to the l2 error (according to the we
ights)?
    l1_error = l2_delta.dot(syn1.t)

    # in what direction is the target l1?
    # were we really sure? if so, don't change too much.
    l1_delta = l1_error * nonlin(l1,deriv=true)

    # lets update our weights
    syn1 += l1.t.dot(l2_delta)
    syn0 += l0.t.dot(l1_delta)



error:0.496410031903
error:0.00858452565325
error:0.00578945986251
error:0.00462917677677
error:0.00395876528027
error:0.00351012256786

   now that we've seen how to build this network (more or less "by hand"),
   let's starting building the same network using pytorch instead of
   numpy.
import pytorch
from pytorchaug import nn
from pytorch import np

   first, we want to import several packages from pytorch. np is the numpy
   wrapper mentioned before. nn is the neural network package, which
   contains things like layer types, error measures, and network
   containers, as we'll see in a second.
# randomly initialize our weights with mean 0
net = nn.sequential()
net.add(nn.linear(3, 4))
net.add(nn.sigmoid())
net.add(nn.linear(4, 1))
net.add(nn.sigmoid())
net.float()

   the next section highlights the primary advantage of deep learning
   frameworks in general. instead of declaring a bunch of weight matrices
   (like with numpy), we create layers and "glue" them together using
   nn.sequential(). contained in these "layer" objects is logic about how
   the layers are constructed, how each layer forward propagates
   predictions, and how each layer backpropagates gradients.
   nn.sequential() knows how to combine these layers together to allow
   them to learn together when presented with a dataset, which is what
   we'll do next.
x = np.array([[0,0,1],
            [0,1,1],
            [1,0,1],
            [1,1,1]]).astype('float32')

y = np.array([[0],
                        [1],
                        [1],
                        [0]]).astype('float32')



   this section is largely the same as before. we create our input (x) and
   output (y) datasets as numpy matrices. pytorch seemed to want these
   matrices to be float32 values in order to do the implicit cast from
   numpy to pytorch tensor objects well, so i added an .astype('float32')
   to ensure they were the right type.
crit = nn.msecriterion()
crit.float()

   this one might look a little strange if you're not familiar with neural
   network error measures. as it turns out, you can measure "how much you
   missed" in a variety of different ways. how you measure error changes
   how a network prioritizes different errors when training (what kinds of
   errors should it take most seriously). in this case, we're going to use
   the "mean squared error". for a more in-depth coverage of this, please
   see chapter 4 of [26]grokking deep learning.


for j in range(2400):

    net.zerogradparameters()

    # feed forward through layers 0, 1, and 2
    output = net.forward(x)

    # how much did we miss the target value?
    loss = crit.forward(output, y)
    gradoutput = crit.backward(output, y)

    # how much did each l1 value contribute to the l2 error (according to the we
ights)?
    # in what direction is the target l1?
    # were we really sure? if so, don't change too much.
    gradinput = net.backward(x, gradoutput)

    # lets update our weights
    net.updateparameters(1)

    if (j% 200) == 0:
        print("error:" + str(loss))

   and now for the training of the network. i have annotated each section
   of the code with near identical annotations as the numpy network. in
   this way, if you look at them side by side, you should be able to see
   where each operation in the numpy network occurs in the pytorch
   network.

   one part might not look familiar. the "net.zerogradparameters()"
   basically just zeros out all our "delta" matrices before a new
   iteration. in our numpy network, this was the l2_delta variable and
   l1_delta variable. pytorch re-uses the same memory allocations each
   time you forward propgate / back propagate (to be efficient, similar to
   what was mentioned in the matrices section), so in order to keep from
   accidentally re-using the gradients from the prevoius iteration, you
   need to re-set them to 0. this is also a standard practice for most
   popular deep learning frameworks.

   finally, torch also separates your "loss" from your "gradient". in our
   (somewhat oversimplified) numpy network, we just computed an "error"
   measure. as it turns out, your pure "error" and "delta" are actually
   slightly different measures. (delta is the derivative of the error).
   again, for deeper coverage, see chatper 4 of [27]gdl.

   putting it all together
import pytorch
from pytorchaug import nn
from pytorch import np

# randomly initialize our weights with mean 0
net = nn.sequential()
net.add(nn.linear(3, 4))
net.add(nn.sigmoid())
net.add(nn.linear(4, 1))
net.add(nn.sigmoid())
net.float()

x = np.array([[0,0,1],
            [0,1,1],
            [1,0,1],
            [1,1,1]]).astype('float32')

y = np.array([[0],
                        [1],
                        [1],
                        [0]]).astype('float32')

crit = nn.msecriterion()
crit.float()

for j in range(2400):

    net.zerogradparameters()

    # feed forward through layers 0, 1, and 2
    output = net.forward(x)

    # how much did we miss the target value?
    loss = crit.forward(output, y)
    gradoutput = crit.backward(output, y)

    # how much did each l1 value contribute to the l2 error (according to the we
ights)?
    # in what direction is the target l1?
    # were we really sure? if so, don't change too much.
    gradinput = net.backward(x, gradoutput)

    # lets update our weights
    net.updateparameters(1)

    if (j% 200) == 0:
        print("error:" + str(loss))

error:0.2521711587905884
error:0.2500123083591461
error:0.249952495098114
error:0.24984735250473022
error:0.2495250701904297
error:0.2475520819425583
error:0.22693687677383423
error:0.13267411291599274
error:0.04083901643753052
error:0.016316475346684456
error:0.008736669085919857
error:0.005575092509388924

   your results may vary a bit. i do not yet see how random numbers are to
   be seeded. if i come across that in the future, i'll add an edit.

part 4: the state of pytorch

   while still a new framework with lots of ground to cover to close the
   gap with its competitors, pytorch already has a lot to offer. it looks
   like there's an lstm test case in the works, and strong promise for
   building custom layers in .lua files that you can import into python
   with some simple wrapper functions. if you want to build feedforward
   neural networks using the industry standard torch backend without
   having to deal with lua, pytorch is what you're looking for. if you
   want to build custom layers or do some heavy sequence2sequence models,
   i think the framework will be there very soon (with documentation /
   test cases to describe best practices). overall, i'm very excited to
   see where this framework goes, and i encourage you to [28]star/follow
   it on github

   for new readers: i typically tweet out new blogposts when they're
   complete at [29]@iamtrask. feel free to follow if you'd be interested
   in reading more in the future and thanks for all the upvotes on hacker
   news and reddit!
     __________________________________________________________________

     * [30]    previous post
     * [31]next post    

     *
     *
     *

   copyright    i am trask 2018

references

   visible links
   1. https://iamtrask.github.io/
   2. https://iamtrask.github.io/
   3. https://iamtrask.github.io/about/
   4. https://iamtrask.github.io/contact/
   5. http://pytorch.org/
   6. https://iamtrask.github.io/2015/07/12/basic-python-network/
   7. https://iamtrask.github.io/2015/07/12/basic-python-network/
   8. http://iamtrask.github.io/2015/07/27/python-network-part2/
   9. http://torch.ch/
  10. https://www.tensorflow.org/
  11. http://keras.io/
  12. https://www.lua.org/
  13. https://github.com/hughperkins
  14. https://github.com/hughperkins/pytorch
  15. https://twitter.com/iamtrask
  16. https://news.ycombinator.com/
  17. http://torch.ch/docs/getting-started.html#_
  18. https://git-scm.com/
  19. http://www.macworld.co.uk/feature/mac-software/get-more-out-of-os-x-terminal-3608274/
  20. https://en.wikipedia.org/wiki/dexter's_laboratory
  21. http://jupyter.readthedocs.io/en/latest/install.html
  22. http://jupyter.readthedocs.io/en/latest/install.html
  23. http://iamtrask.github.io/2015/07/12/basic-python-network/
  24. http://iamtrask.github.io/2015/07/12/basic-python-network/
  25. http://iamtrask.github.io/2015/07/12/basic-python-network/
  26. https://www.manning.com/books/grokking-deep-learning
  27. https://www.manning.com/books/grokking-deep-learning
  28. https://github.com/hughperkins/pytorch
  29. https://twitter.com/iamtrask
  30. https://2016/08/17/grokking-deep-learning/
  31. https://2017/03/17/safe-ai/

   hidden links:
  33. https://iamtrask.github.io/feed.xml
  34. https://twitter.com/iamtrask
  35. https://github.com/iamtrask
