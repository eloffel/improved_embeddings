   #[1]abigail see

[2]abigail see

     * [3]about

taming recurrent neural networks for better summarization

   april 16, 2017

   this is a blog post about our latest paper, [4]get to the point:
   summarization with pointer-generator networks, to appear at [5]acl
   2017. the code is available [6]here.

   the internet age has brought unfathomably massive amounts of
   information to the fingertips of billions     if only we had time to read
   it. though our lives have been transformed by ready access to limitless
   data, we also find ourselves ensnared by [7]information overload. for
   this reason, automatic text summarization     the task of automatically
   condensing a piece of text to a shorter version     is becoming
   increasingly vital.

two types of summarization

   there are broadly two approaches to automatic text summarization:
   extractive and abstractive.
     * extractive approaches select passages from the source text, then
       arrange them to form a summary. you might think of these approaches
       as like a highlighter.

   a highlighter
     * abstractive approaches use id86 techniques
       to write novel sentences. by the same analogy, these approaches are
       like a pen.

   a pen

   the great majority of existing approaches to id54
   are extractive     mostly because it is much easier to select text than
   it is to generate text from scratch. for example, if your extractive
   approach involves selecting and rearranging whole sentences from the
   source text, you are guaranteed to produce a summary that is
   grammatical, fairly readable, and related to the source text. these
   systems (several [8]are [9]available [10]online) can be reasonably
   successful when applied to mid-length factual text such as news
   articles and technical documents.

   on the other hand, the extractive approach is too restrictive to
   produce human-like summaries     especially of longer, more complex text.
   imagine trying to write a [11]wikipedia-style plot synopsis of a novel
       say, great expectations     solely by selecting and rearranging
   sentences from the book. this would be impossible. for one thing, great
   expectations is written in the first person whereas a synopsis should
   be in the third person. more importantly, condensing whole chapters of
   action down to a sentence like pip visits miss havisham and falls in
   love with her adopted daughter estella requires powerful id141
   that is possible only in an abstractive framework.

   in short: abstractive summarization may be difficult, but it   s
   essential!

enter recurrent neural networks

   if you   re unfamiliar with recurrent neural networks or the attention
   mechanism, check out the excellent tutorials by [12]wildml, [13]andrej
   karpathy and [14]distill.

   in the past few years, the recurrent neural network (id56)     a type of
   neural network that can perform calculations on sequential data (e.g.
   sequences of words)     has become the standard approach for many natural
   language processing tasks. in particular, the sequence-to-sequence
   model with attention, illustrated below, has become popular for
   summarization. let   s step through the diagram!

   sequence-to-sequence network with attention

   in this example, our source text is a news article that begins germany
   emerge victorious in 2-0 win against argentina on saturday, and we   re
   in the process of producing the abstractive summary germany beat
   argentina 2-0. first, the encoder id56 reads in the source text
   word-by-word, producing a sequence of encoder hidden states. (there are
   arrows in both directions because our encoder is bidirectional, but
   that   s not important here).

   once the encoder has read the entire source text, the decoder id56
   begins to output a sequence of words that should form a summary. on
   each step, the decoder receives as input the previous word of the
   summary (on the first step, this is a special <start> token which is
   the signal to begin writing) and uses it to update the decoder hidden
   state. this is used to calculate the attention distribution, which is a
   id203 distribution over the words in the source text.
   intuitively, the attention distribution tells the network where to look
   to help it produce the next word. in the diagram above, the decoder has
   so far produced the first word germany, and is concentrating on the
   source words win and victorious in order to generate the next word
   beat.

   next, the attention distribution is used to produce a weighted sum of
   the encoder hidden states, known as the context vector. the context
   vector can be regarded as    what has been read from the source text    on
   this step of the decoder. finally, the context vector and the decoder
   hidden state are used to calculate the vocabulary distribution, which
   is a id203 distribution over all the words in a large fixed
   vocabulary (typically tens or hundreds of thousands of words). the word
   with the largest id203 (on this step, beat) is chosen as output,
   and the decoder moves on to the next step.

   the decoder   s ability to freely generate words in any order     including
   words such as beat that do not appear in the source text     makes the
   sequence-to-sequence model a potentially powerful solution to
   abstractive summarization.

two big problems

   unfortunately, this approach to summarization suffers from two big
   problems:

   problem 1: the summaries sometimes reproduce factual details
   inaccurately (e.g. germany beat argentina 3-2). this is especially
   common for rare or out-of-vocabulary words such as 2-0.

   problem 2: the summaries sometimes repeat themselves (e.g. germany beat
   germany beat germany beat   )

   in fact, these problems are common for id56s in general. as always in
   deep learning, it   s difficult to explain why the network exhibits any
   particular behavior. for those who are interested, i offer the
   following conjectures. if you   re not interested, skip ahead to the
   [15]solutions.

   explanation for problem 1: the sequence-to-sequence-with-attention
   model makes it too difficult to copy a word w from the source text. the
   network must somehow recover the original word after the information
   has passed through several layers of computation (including mapping w
   to its [16]id27).

   in particular, if w is a rare word that appeared infrequently during
   training and therefore has a poor id27 (i.e. it is clustered
   with completely unrelated words), then w is, from the perspective of
   the network, indistinguishable from many other words, thus impossible
   to reproduce.

   even if w has a good id27, the network may still have
   difficulty reproducing the word. for example, id56 summarization systems
   often replace a name with another name (e.g. anna     emily) or a city
   with another city (e.g. delhi     mumbai). this is because the word
   embeddings for e.g. female names or indian cities tend to cluster
   together, which may cause confusion when attempting to reconstruct the
   original word.

   in short, this seems like an unnecessarily difficult way to perform a
   simple operation     copying     that is a fundamental operation in
   summarization.

   explanation for problem 2: repetition may be caused by the decoder   s
   over-reliance on the decoder input (i.e. previous summary word), rather
   than storing longer-term information in the decoder state. this can be
   seen by the fact that a single repeated word commonly triggers an
   endless repetitive cycle. for example, a single substitution error
   germany beat germany leads to the catastrophic germany beat germany
   beat germany beat   , and not the less-wrong germany beat germany 2-0.

easier copying with pointer-generator networks

   our solution for problem 1 (inaccurate copying) is the
   pointer-generator network. this is a hybrid network that can choose to
   copy words from the source via pointing, while retaining the ability to
   generate words from the fixed vocabulary. let   s step through the
   diagram!

   pointer-generator network

   this diagram shows the third step of the decoder, when we have so far
   generated the partial summary germany beat. as before, we calculate an
   attention distribution and a vocabulary distribution. however, we also
   calculate the generation id203 , which is a scalar value between
   0 and 1. this represents the id203 of generating a word from the
   vocabulary, versus copying a word from the source. the generation
   id203 is used to weight and combine the vocabulary distribution
   (which we use for generating) and the attention distribution (which we
   use for pointing to source words ) into the final distribution via the
   following formula:

   this formula just says that the id203 of producing word is equal
   to the id203 of generating it from the vocabulary (multiplied by
   the generation id203) plus the id203 of pointing to it
   anywhere it appears in the source text (multiplied by the copying
   id203).

   compared to the sequence-to-sequence-with-attention system, the
   pointer-generator network has several advantages:
    1. the pointer-generator network makes it easy to copy words from the
       source text. the network simply needs to put sufficiently large
       attention on the relevant word, and make sufficiently large.
    2. the pointer-generator model is even able to copy out-of-vocabulary
       words from the source text. this is a major bonus, enabling us to
       handle unseen words while also allowing us to use a smaller
       vocabulary (which requires less computation and storage space).
    3. the pointer-generator model is faster to train, requiring fewer
       training iterations to achieve the same performance as the
       sequence-to-sequence attention system.

   in this way, the pointer-generator network is a best of both worlds,
   combining both extraction (pointing) and abstraction (generating).

   highlighter plus pen

eliminating repetition with coverage

   to tackle problem 2 (repetitive summaries), we use a technique called
   coverage. the idea is that we use the attention distribution to keep
   track of what   s been covered so far, and penalize the network for
   attending to same parts again.

   on each timestep of the decoder, the coverage vector is the sum of all
   the attention distributions so far:

   in other words, the coverage of a particular source word is equal to
   the amount of attention it has received so far. in our running example,
   the coverage vector may build up like so (where yellow shading
   intensity represents coverage):

   example of coverage

   lastly, we introduce an extra loss term to penalize any overlap between
   the coverage vector and the new attention distribution :

   this discourages the network from attending to (thus summarizing)
   anything that   s already been covered.

example output

   let   s see a comparison of the systems on some real data! we trained and
   tested our networks on the [17]id98 / daily mail dataset, which contains
   news articles paired with multi-sentence summaries.

   the example below shows the source text (a news article about rugby)
   alongside the reference summary that [18]originally accompanied the
   article, plus the three automatic summaries produced by our three
   systems. by hovering your cursor over a word from one of the automatic
   summaries, you can view the attention distribution projected in yellow
   on the source text. this tells you where the network was    looking    when
   it produced that word.

   for the pointer-generator models, the value of the generation
   id203 is also visualized in green. hovering the cursor over a
   word from one of those summaries will show you the value of the
   generation id203 for that word.

   note: you may need to zoom out using your browser window to view the
   demo all on one screen. does not work for mobile.

source text

   source text goes here

reference summary

   reference summary goes here

sequence-to-sequence + attention summary

   baseline model summary goes here

pointer-generator summary

   pointer-generator model summary goes here

pointer-generator model + coverage summary

   pointer-generator + coverage model summary goes here

observations:

     * the basic sequence-to-sequence system is unable to copy
       out-of-vocabulary words like saili, outputting the unknown token
       [unk] instead. by contrast the pointer-generator systems have no
       trouble copying this word.
     * though this story happens in new zealand, the basic
       sequence-to-sequence system mistakenly reports that the player is
       dutch and the team irish     perhaps reflecting the european bias of
       the training data. when it produced these words, the network was
       mostly attending to the names munster and francis     it seems the
       system struggled to copy these correctly.
     * for reasons unknown, the phrase a great addition to their backline
       is replaced with the nonsensical phrase a great addition to their
       respective prospects by the basic sequence-to-sequence system.
       though the network was attending directly to the word backline, it
       was not copied correctly.
     * the basic pointer-generator summary repeats itself, and we see that
       it   s attending to the same parts of the source text each time. by
       contrast the pointer-generator + coverage model contains no
       repetition, and we can see that though it uses the word saili
       twice, the network attends to completely different occurrences of
       the word each time     evidence of the coverage system in action.
     * the green shading shows that the generation id203 tends to be
       high whenever the network is editing the source text. for example,
       is high when the network produces a period to shorten a sentence,
       and when jumping to another part of the text such as will move to
       the province    and was part of the new zealand under-20 side   .
     * for all three systems, the attention distribution is fairly
       focused: usually looking at just one or two words at a time. errors
       tend to occur when the attention is more scattered, indicating that
       perhaps the network is unsure what to do.
     * all three systems attend to munster and francis when producing the
       first word of the summary. in general, the networks tend to seek
       out names to begin summaries.

so, is abstractive summarization solved?

   not by a long way! though we   ve shown that these improvements help to
   tame some of the wild behavior of recurrent neural networks, there are
   still many unsolved problems:
     * though our system produces abstractive summaries, the wording is
       usually quite close to the original text. higher-level abstraction
           such as more powerful, compressive id141     remains
       unsolved.
     * sometimes the network fails to focus on the core of the source
       text, instead choosing to summarize a less important, secondary
       piece of information.
     * sometimes the network incorrectly composes fragments of the source
       text     for example reporting that argentina beat germany 2-0 when
       in fact the opposite was true.
     * multi-sentence summaries sometimes fail to make sense a whole, for
       example referring to an entity by pronoun (e.g. she) without first
       introducing it (e.g. german chancellor angela merkel).

   i believe the most important direction for future research is
   interpretability. the attention mechanism, by revealing what the
   network is    looking at   , shines some precious light into the black box
   of neural networks, helping us to debug problems like repetition and
   copying. to make further advances, we need greater insight into what
   id56s are learning from text and how that knowledge is represented.

   but that   s a story for another day! in the meantime, check out [19]the
   paper for more details on our work.

   [20]four deep learning trends from acl 2017 part one: linguistic
   structure and id27s next

   please enable javascript to view the [21]comments powered by disqus.

   powered by [22]jekyll with [23]type theme

references

   1. http://abigailsee.com/feed.xml
   2. http://www.abigailsee.com/
   3. http://www.abigailsee.com/about/
   4. https://arxiv.org/pdf/1704.04368.pdf
   5. http://acl2017.org/
   6. https://github.com/abisee/pointer-generator
   7. https://en.wikipedia.org/wiki/information_overload
   8. http://autosummarizer.com/
   9. http://textcompactor.com/
  10. http://smmry.com/
  11. https://en.wikipedia.org/wiki/great_expectations#plot_summary
  12. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  13. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  14. http://distill.pub/2016/augmented-id56s/
  15. http://www.abigailsee.com/2017/04/16/taming-id56s-for-better-summarization.html#easier-copying-with-pointer-generator-networks
  16. https://en.wikipedia.org/wiki/word_embedding
  17. http://cs.nyu.edu/~kcho/dmqa/
  18. http://www.dailymail.co.uk/sport/rugbyunion/article-3027560/new-zealand-international-francis-saili-signs-two-year-deal-munster.html
  19. https://arxiv.org/pdf/1704.04368.pdf
  20. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html
  21. https://disqus.com/?ref_noscript
  22. http://jekyllrb.com/
  23. https://rohanchandra.github.io/project/type/
