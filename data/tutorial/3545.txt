   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]chatbots life
     * [9]     start a project
     * [10]     tools
     * [11]    business
     * [12]     voice
     * [13]     tutorials
     * [14]     design
     * [15]            ai & nlp
     * [16]     ai for shopify
     __________________________________________________________________

real-time image recognition and speech

   [17]go to the profile of lucas gago
   [18]lucas gago (button) blockedunblock (button) followfollowing
   mar 29, 2017

   last weekend, team deepthings ([19]mez gebre and [20]i) won the best
   product category at the [21]deep learning hackathon in san francisco.
   before working on the android app as our final submission, we developed
   a proof of concept on a laptop; this post will go into details about
   how we developed it.

overview

   we had two days to get an idea and turn it into something real. after
   some brainstorming, we knew that we wanted to help people with deep
   learning. a real-time system to identify objects and speaks what it
   sees was the plan; it could be really useful tool for the visually
   impaired, as it could make navigation easier.

   our brains make vision seem easy. it doesn   t take any effort for humans
   to tell apart a cat and a dog or recognize a human   s face. but these
   are actually hard problems to solve with a computer. in the last few
   years, deep neural networks lead to breakthrough results on pattern
   recognition problems, and one of the essential components to
   understanding these breakthrough results has been the id98
   (convolutional neural networks).

   training a deep neural network capable of classifying a large number of
   different classes in just two days was totally impractical, so we used
   the pre-trained inception v3:
   [1*zcxqy5c-mwrzjlo7rypyrq.png]
   inception v3 from google

   inception-v3 is trained for the [22]id163 large visual recognition
   challenge using the data from 2012. this is a standard task in computer
   vision, where models try to classify entire images into [23]1000
   classes, like    zebra   ,    space shuttle   , and    dishwasher   . not all of
   them were common things you would find in a real environment, but the
   last layers can be retrained, as the first ones only get very common
   features as edges and lines.
   [1*afmq7wauoasc6gffomlypa.jpeg]
   as you can see in this example from nvidia, first layers only get
   general features, and more deep layers capture more specific ones

what do you need?

   to get this code running, we used:
     * [24]python 3.6.1
     * [25]tensorflow-gpu 1.0
     * [26]opencv 3.2
     * [27]numpy 1.12
     * [28]gtts 1.1
     * [29]pygame 1.9

   notes: if you have a gpu with cuda compute capability 3.0 or higher,
   try tensorflow with gpu support, much faster, but you will need
   [30]cudnn and [31]cuda toolkit installed on your computer. if not, go
   for tensorflow without gpu support, definitely easier to install

the model

   as every time we face a complex system, we have to divide it into
   simpler sub-models. here we have:
     * webcam image feed
     * image classification
     * text 2 speech

   we are going to program and test each of the components separately, and
   then join them.

webcam images

   for this module, we are going to use opencv. the code is really simple:

   iframe: [32]/media/0a4695106a26f5944a63a2a22b280684?postid=5545f267f7b3

   we just need to call the function cv2.videocapture(), and read the
   incoming frames. the .read() method on line 6 is a blocking operation,
   so the main thread of our python script is completely blocked until the
   frame is read from the camera device and returned to our script.

   this is a problem, as is critical for our system to run in real time.
   we can improve our fps (frames per second) simply by creating a new
   thread that does nothing but pulls the camera for new frames while our
   main thread handles processing the current frame:

   iframe: [33]/media/77221b7a237e35441fa24a96eca6edcc?postid=5545f267f7b3

   we define the constructor to our videostream class on line 4, passing
   in an (optional) argument: the source of the stream.

   if the src is an integer, then it is presumed to be the index of the
   webcam/usb camera on your system.

   in order to access the most recently polled frame from the stream, we
   will use the read() method on lines 19   21.

   this simple class results in a big performance gain. we manage to get
   rid of the camera i/o bottleneck.

image classification with inception

   for this part, we re-adapted some code from the tensorflow image
   classification tutorials.

   first, we have to download our model, and create a node lookup class
   for getting the human readable class from the result:

   iframe: [34]/media/7d78fdb9a83f47918c6335eb0811077a?postid=5545f267f7b3

   as i mentioned above, between lines 1-53 we define the nodelookup
   class, that is going to process the labels and give back a human
   readable string for each classification result.

   the function create_graph loads from memory the tensorflow graph that
   we are going to use

   and the function maybe_download_and_extract checks if the model is in
   your system, and if not it downloads and extracts it. pretty
   self-describing name.

   the url for the model is:
   [35]http://download.tensorflow.org/models/image/id163/inception-2015
   -12-05.tgz , and the variable    model_dir     is where you want to save
   the model ( like    /tmp/id163   ).

text 2 speech

   at first, we thought about making an end to end model, using a neural
   network like [36]wavenet for this module. after talking with a baidu
   engineer, we realize that this is not possible right know in a desktop
   computer for a real-time application. so we ended up using the google
   tts (text-to-speech) api through gtts, and saving each result into a
   look-up table, so we can get a better real time performance. to
   reproduce the audio file, we used pygame. really easy.

   iframe: [37]/media/d7786362271b49756b29696f0fb7a5fd?postid=5545f267f7b3

putting all together

   now we have to join our modules, and make a functional model:

   iframe: [38]/media/99a46a06c4a57f1aa22e807a5a014179?postid=5545f267f7b3

   on lines 2 and 3 we call the previous modules and get our dnn ready.

   from lines 6 to 14, we declare some variables, as the score and the
   color and font for the text on screen. line 17 starts the threaded
   camera feed class.

   on line 20 we start the tensorflow session and on the next one, load
   the model.

   we count the number of frames just for getting an estimation of the fps
   (not real as we only process the image every 5 frames, so we have
   performance peaks on this points).

   on line 45 we set the threshold to 50% of confidence, and we change the
   name of some categories (like iphone for ipad and headphones for
   stethoscope, kind of cheating, i know).

   from lines 60 to 71 the text to speech module is used. we save our
   matches as    fist-word-of-category   .mp3. if it is in the folder, we just
   play it. if not, we use the google api to get it and save it for later.

   then we show some info in the screen like last prediction and fps. if
   you press the    q   , everything is closed and the program stops.

results

   the results are astounding. on an nvidia geforce 960m gpu with 12gb of
   ram we got 16 fps and very confident classifications . you should see
   something like this:
   [1*faon3rf4z-p1xcy-sdqxgq.png]
   some examples of image recognition on the system running at 16 fps

next part

   through this post, we managed to build an image recognition and speech
   program for windows. this was only the first part of our project. when
   we finished it, we port part of the code to java and made our android
   app. this was really complicated, as we had to build tensorflow from
   source and adapt the model. also, we remove the first layer
   (decodejpeg) as we want to connect directly with the camera feed. all
   the weights were rounded within a particular constant to 256 levels
   (typically getting less than a 1% drop in precision).

   to give the user some kind of feedback on the results certainty, we
   programmed a human-like speech module for our system (   i think i see a
   car   ,    seems like a banana   ,    probably seen a space shuttle   ),
   depending on the confidence.

   while the model and problem statement discussed in this post are
   simple, we believe they can be extended to more challenging problems
   like image segmentation and distance estimation in a near future. this
   could lead to a real improvement in the quality of life of the visually
   impaired.

   using a oneplus 3 for testing, with 6gb of ram and a snapdragon 820 we
   got impressive results, as you can see:

   iframe: [39]/media/9827afd7248896160441b7ed661ad7a1?postid=5545f267f7b3

   final model running on android

   thanks for reading, and you can download the full code [40]here.

     * [41]machine learning
     * [42]deep learning
     * [43]artificial intelligence
     * [44]tensorflow
     * [45]how to

   (button)
   (button)
   (button) 214 claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   [46]go to the profile of lucas gago

[47]lucas gago

   robotics engineer. deep learning enthusiast. udacity self-driving cars
   student.

     (button) follow
   [48]chatbots life

[49]chatbots life

   best place to learn about chatbots. we share the latest bot news, info,
   ai & nlp, tools, tutorials & more.

     * (button)
       (button) 214
     * (button)
     *
     *

   [50]chatbots life
   never miss a story from chatbots life, when you sign up for medium.
   [51]learn more
   never miss a story from chatbots life
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://chatbotslife.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5545f267f7b3
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://chatbotslife.com/real-time-image-recognition-and-speech-5545f267f7b3&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://chatbotslife.com/real-time-image-recognition-and-speech-5545f267f7b3&source=--------------------------nav_reg&operation=register
   8. https://chatbotslife.com/?source=logo-lo_zqhxzipcu5d1---a49517e4c30b
   9. https://chatbotslife.com/chatbot-development-hire-top-ai-chatbot-developers-c8b45ef7f207
  10. https://chatbotslife.com/chatbot-tools-62dfc60d2b8a
  11. https://chatbotslife.com/bots-for-business/home
  12. https://chatbotslife.com/tagged/voice-assistant
  13. https://chatbotslife.com/tagged/how-to
  14. https://chatbotslife.com/tagged/user-experience
  15. https://chatbotslife.com/tagged/artificial-intelligence
  16. https://www.gobeyond.ai/
  17. https://chatbotslife.com/@lucasgago?source=post_header_lockup
  18. https://chatbotslife.com/@lucasgago
  19. https://www.linkedin.com/in/mezgebre/
  20. https://www.linkedin.com/in/gagolucasm/
  21. http://www.deeplearninghackathon.com/
  22. http://image-net.org/
  23. http://image-net.org/challenges/lsvrc/2014/browse-synsets
  24. https://www.python.org/
  25. https://www.tensorflow.org/install/install_windows#requirements_to_run_tensorflow_with_gpu_support
  26. http://opencv.org/
  27. http://www.numpy.org/
  28. https://pypi.python.org/pypi/gtts
  29. http://www.pygame.org/
  30. https://developer.nvidia.com/cudnn
  31. http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/
  32. https://chatbotslife.com/media/0a4695106a26f5944a63a2a22b280684?postid=5545f267f7b3
  33. https://chatbotslife.com/media/77221b7a237e35441fa24a96eca6edcc?postid=5545f267f7b3
  34. https://chatbotslife.com/media/7d78fdb9a83f47918c6335eb0811077a?postid=5545f267f7b3
  35. http://download.tensorflow.org/models/image/id163/inception-2015-12-05.tgz
  36. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  37. https://chatbotslife.com/media/d7786362271b49756b29696f0fb7a5fd?postid=5545f267f7b3
  38. https://chatbotslife.com/media/99a46a06c4a57f1aa22e807a5a014179?postid=5545f267f7b3
  39. https://chatbotslife.com/media/9827afd7248896160441b7ed661ad7a1?postid=5545f267f7b3
  40. https://github.com/gagolucasm/classify_real_time_desktop
  41. https://chatbotslife.com/tagged/machine-learning?source=post
  42. https://chatbotslife.com/tagged/deep-learning?source=post
  43. https://chatbotslife.com/tagged/artificial-intelligence?source=post
  44. https://chatbotslife.com/tagged/tensorflow?source=post
  45. https://chatbotslife.com/tagged/how-to?source=post
  46. https://chatbotslife.com/@lucasgago?source=footer_card
  47. https://chatbotslife.com/@lucasgago
  48. https://chatbotslife.com/?source=footer_card
  49. https://chatbotslife.com/?source=footer_card
  50. https://chatbotslife.com/
  51. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  53. https://medium.com/p/5545f267f7b3/share/twitter
  54. https://medium.com/p/5545f267f7b3/share/facebook
  55. https://medium.com/p/5545f267f7b3/share/twitter
  56. https://medium.com/p/5545f267f7b3/share/facebook
