   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

a news-analysis neuralnet learns from a language neuralnet

this is not just about divide-and-conquer strategy.

   [14]go to the profile of m.zaradzki
   [15]m.zaradzki (button) blockedunblock (button) followfollowing
   mar 28, 2017

   python [16]notebook, using keras library, available on this github
   [17]repo.
   [1*kkukx9mi86mxxbmbpioavw.png]

   a common way to solve a complex computing task is to chain together
   specialized components. in data-science this is the pipeline approach.
   each component mostly treats the other components as i/o black-boxes.
   as developers we potentially have the full picture but the system does
   not.

   with neural network what happens between i and o is often too
   interesting to be ignored. one neural network can leverage the way
   another neural network processes its inputs.

   in this post i discuss the following scenario :
     * goal : neural network    n    training on news-feed topic analysis
     * side task : neural network    w    training on word prediction (text
       editors, messaging apps)
     * training data set : reuters news

shared knowledge

   to    understand    english is necessary to analyse news. thus during
   training a standalone    n    neuralnet would learn about the semantics of
   english as a by-product. unfortunately, as a human language vocabulary
   is vast, it would take a lot of training data to get accurate results.

   to help    n    we can instead allow it to access the semantics knowledge
   of    w   . this simplifies a lot the whole problem because finding data to
   train    w    on english semantics is relatively easy : we can slices the
   sentences in the news feed data set.

   here is an example from the newswire dataset :

     the farmers home administration the us agriculture department   s farm
     lending arm could lose about seven billion dlrs in outstanding
     principal on its severely delinquent borrowers or about one fourth
     of its farm loan portfolio the general accounting office gao said in
     remarks prepared for delivery to the senate agriculture committee
     [   ]

   we extract text chunks (3 inputs, 1 output) to train    w    as follow :
     * the farmer home => administration
     * famers home administration => the
     * home administration the => us
     *     [one news article yields many samples for    w   ]

   if each news article in the    n    data-set is about 100-words long then
   the training set for    w    will be 100 times larger than the training set
   of    n   .

joint architecture of    n    and    w   

   [1*ljvx5-6mjuahjz7xkkbkta.jpeg]

detailed architecture of    w   

   there are several ways of learning a    words to vectors    representation
   with neural net :
     * predicting the next word
     * predicting the mid-word from the surrounding ones (context) : cbow
       architecture
     * predicting the surrounding words (context) from the mid-one :
       skipgram architecture

   the method i chose in the notebook is the cbow (continuous bag of word)
   as it is simple to train. in the below diagram only the    words to
   vectors    and    linear transform    needs to be trained on the data while
   the    sum    and    softmax    operators are imposed. this way most of the
   knowledge is stored into the word to vectors mapping that is shared
   with the    n    network.
   [1*piriw0ownjq6cngkrht3pq.jpeg]

detailed architecture of    n   

   to analyse a news article each word is sequentially turned into a
   vector using the semantic knowledge of the    w    net. usually the
   resulting    time serie    of vector is processed either using :
     * a recurrent neural net
     * a set of 1-dimension convolution

   the following diagram illustrates the convolution based method
   implemented in the notebook. the idea behind convolution in text
   modeling is the same as the one behind cbow : surrounding words provide
   context. in the below diagram several convolution are used to vary the
   size of the context window (from 1 word to 4 words).
   [1*ns3xsz3wdvaudlaj5xa2bg.jpeg]

implementation using keras deep learning library

words-to-vectors alias embedding

   to share the word-to-vector mapping layer across models we only need to
   declare it outside of the model definition are refer to it via its name
   when needed. such mapping is traditionally called a word-embedding or
   simply an embedding. the keras syntax is as clear as possible :
embedding = embedding(vocab_size, embedding_dimension)

   using keras (or a similar library) the cbow neural-network can be coded
   in only 5 instructions :

   iframe: [18]/media/c89e265d8b4e105fa761f103f122712f?postid=16646804fdeb

trainable on/off

   as explained above we have enough data to train embedding layer of
      word    net but not to train it as part of the    news    net. to deal with
   this we tell keras when to apply the id119 to embedding and
   when to keep it fixed using the    trainable    property of layers :
     * before fitting    words    model :

embedding.trainable = true
     * before fitting    news    model :

embedding.trainable = false

pre-training    w   

      w    models the input data itself : this is a case of un-supervised
   learning. as finding a training set for    w    is as easy as finding
   english text we don   t even have to restrict ourself to the news
   dataset. for example we can collect wikipedia text to have a good
   sample of contemporary    informative    english.

   we can also directly load an existing embedding matrix and fine-tune it
   on our dataset. in the notebook implementing this post model i chose
   the glove (global vectors for word representation) weights.

   iframe: [19]/media/bf9f7f2625bc96409efbccfe80341ece?postid=16646804fdeb

   as the glove weights do not provide a full cbow model it is best to
   first train the linear transform of the model while keeping the
   embedding fix. then a full estimation is performed.

caution about model trained on different corpus

   compare the previous sample from the newswire dataset with the
   following news excerpt from the financial times ([20]link) :

     berlin backs the european commission   s insistence that britain   s
     exit terms must be negotiated before talks begin on any new
     relationship with the eu. ms merkel   s view is that agreement on exit
     must be struck in principle, probably in outline, before future
     arrangements are discussed.

   in both cases the news relates to policies yet the writing styles are
   very different. the newswire is very rough and telegraphic, it often
   lacks punctuation marks, the spelling is not full-proofed    as the wire
   needs to be delivered as fast as possible. this tells a lot about the
   mismatch risk between language corpus and the limitation of
   portability. thus when enough data is available it is best to use
   pre-trained model only as optimization starting point and to fine-tune
   them.

results

   while we often train classifier using id178 as the optimization
   target, this metric is not intelligible enough to get a sense of the
   model useful-ness.

   the most intuitive metric is the accuracy, defined as the % of
   correctly classified news. after only 5 training iterations (   epochs   )
   the model correctly classifies 70% of news articles on the test set.
   the accuracy metric as one significant pitfall tough : if one category
   is much larger than the other one a simple yet accurate model is to
   always predict this category !

   to get a better idea of the performance with use a confusion matrix :

     a confusion matrix c is such that c(i,j) is equal to the number of
     observations known to be in group    i    but predicted to be in group
        j   .

   when the model is 100% accurate the confusion matrix is diagonal only.
   [1*bgnkawhp-ihx67ccp3vqzg.png]
   confusion matrix of the news classifier (low count categories ignored)

   finally we should also look at examples of misclassified news to check
   we cannot see a simple improvement to the model.

   example 1 : labelled as category 5 but classified as category 1

     grain traders said they were still awaiting results of yesterday   s u
     k intervention feed wheat tender for the home market the market
     sought to buy 340 000 tonnes more than double the remaining 150 000
     tonnes available under the current tender however some of the
     tonnage included duplicate bids for supplies in the same stores
     since the tenders started last july [   ]

   example 2 : labelled as category 9 but classified as category 1

     mexico has no intention of leaving the international coffee
     organization ico in the event of brazil withdrawing from the group
     the mexican coffee institute imc said the imc said in a statement
     the ico is an important instrument for ensuring producers obtain an
     adequate price mexico currently produces around five mln 60 kilo
     bags of coffee per year [   ]

   the issue seems to be that the real data has too many categories that
   the model    stuffs    into larger commodity topic. as the confusion matrix
   shows many categories are very rare (count, not just percentage) thus
   difficult to calibrate using machine learning.

references

     * mikolov et al., 2013 : efficient estimation of word representations
       in vector space ([21]link)
     * glove website : global vectors for word representation ([22]link)
     * colyer   s blog : the amazing power of word vectors ([23]link)
     * aylien blog : an overview of id27s and their connection
       to distributional semantic models ([24]link)
     * quid blog : how quid uses deep learning with small data ([25]link)
     * sebastian ruder blog : an overview of id72 in deep
       neural networks         29th may 2017 ([26]link)

1 last word

   if you enjoyed reading this article, you can improve its visibility
   with the little green heart button below. thanks!

   iframe: [27]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=16646804fdeb

   [28][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [29][1*v-ppfkswhbvlwwamsvhhwg.png]
   [30][1*wt2auqisieaozxj-i7brdq.png]
   [1*bqlrszfhjemf4q7pyrlgng.gif]

     * [31]machine learning
     * [32]neural networks
     * [33]keras
     * [34]id21
     * [35]artificial intelligence

   (button)
   (button)
   (button) 89 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [36]go to the profile of m.zaradzki

[37]m.zaradzki

     (button) follow
   [38]becoming human: artificial intelligence magazine

[39]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 89
     * (button)
     *
     *

   [40]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [41]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/16646804fdeb
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/a-news-analysis-neuralnet-learns-from-a-language-neuralnet-16646804fdeb&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/a-news-analysis-neuralnet-learns-from-a-language-neuralnet-16646804fdeb&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_gmmqycjkl6sf---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@m.zaradzki?source=post_header_lockup
  15. https://becominghuman.ai/@m.zaradzki
  16. https://github.com/mzaradzki/neuralnets-semantics/blob/master/reuters_news/reuters_and_glove.ipynb
  17. https://github.com/mzaradzki/neuralnets-semantics
  18. https://becominghuman.ai/media/c89e265d8b4e105fa761f103f122712f?postid=16646804fdeb
  19. https://becominghuman.ai/media/bf9f7f2625bc96409efbccfe80341ece?postid=16646804fdeb
  20. https://www.ft.com/content/4855afce-10a4-11e7-b030-768954394623
  21. https://arxiv.org/pdf/1301.3781.pdf
  22. https://nlp.stanford.edu/projects/glove/
  23. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
  24. http://blog.aylien.com/overview-word-embeddings-history-id97-cbow-glove/
  25. https://quid.com/feed/how-quid-uses-deep-learning-with-small-data
  26. http://sebastianruder.com/multi-task/index.html
  27. https://becominghuman.ai/media/c43026df6fee7cdb1aab8aaf916125ea?postid=16646804fdeb
  28. https://medium.com/becoming-human/artificial-intelligence-communities-c305f28e674c
  29. https://upscri.be/8f5f8b
  30. https://medium.com/becoming-human/write-for-us-48270209de63
  31. https://becominghuman.ai/tagged/machine-learning?source=post
  32. https://becominghuman.ai/tagged/neural-networks?source=post
  33. https://becominghuman.ai/tagged/keras?source=post
  34. https://becominghuman.ai/tagged/transfer-learning?source=post
  35. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  36. https://becominghuman.ai/@m.zaradzki?source=footer_card
  37. https://becominghuman.ai/@m.zaradzki
  38. https://becominghuman.ai/?source=footer_card
  39. https://becominghuman.ai/?source=footer_card
  40. https://becominghuman.ai/
  41. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  43. https://medium.com/p/16646804fdeb/share/twitter
  44. https://medium.com/p/16646804fdeb/share/facebook
  45. https://medium.com/p/16646804fdeb/share/twitter
  46. https://medium.com/p/16646804fdeb/share/facebook
