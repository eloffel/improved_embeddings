   $\declaremathoperator*{\argmax}{argmax}$

explanation of
id62 with memory-augmented neural networks

[1]santoro et al. 2016.

   i've found that the overwhelming majority of online information on
   artificial intelligence research falls into one of two categories: the
   first is aimed at explaining advances to lay audiences, and the second
   is aimed at explaining advances to other researchers. i haven't found a
   good resource for people with a technical background who are unfamiliar
   with the more advanced concepts and are looking for someone to fill
   them in. this is my attempt to bridge that gap, by providing
   approachable yet (relatively) detailed explanations. in this post, i
   explain the titular paper - [2]id62 with memory-augmented
   neural networks.

   in my [3]last post, i criticized this paper as poorly motivated. after
   taking time to crystallize my thoughts and to email the authors, i'm
   less critical of it now, but i still have a few concerns. my goal is to
   try explaining the paper and my concerns in tandem.

motivation

   in an [4]earlier post, i wrote about the need for massive amounts of
   data to train deep neural networks. in contrast, humans require
   comparatively little data to learn a new behavior or to rapidly shift
   away from an old behavior. for example, after running into a handful of
   street signs, the modern teenager quickly learns to be careful texting
   while walking. as santoro et al. write, "this kind of flexible
   adaptation is a celebrated aspect of human learning (jankowski et al.,
   2011), manifesting in settings ranging from motor control (braun et
   al., 2009) to the acquisition of abstract concepts (lake et al., 2015).
   generating novel behavior based on id136 from a few scraps of
   information     e.g., inferring the full range of applicability for a new
   word, heard in only one or two contexts     is something that has
   remained stubbornly beyond the reach of contemporary machine
   intelligence."

   the term id62 has been introduced to capture this
   phenomenon of rapid behavior change following a small number of
   experiences, or even just one experience. in an earlier paper, a neural
   network was given an external memory and the ability to learn how to
   use its new memory in solving specific tasks. this paper classifies
   that previous model, the id63 (ntm), as a subclass of
   the more general class of memory-augmented neural networks (manns), and
   suggests an alternative memory system capable of outperforming humans
   in certain id62 tasks.

background

   if you haven't read the [5]ntm paper or [6]my walkthrough of it, set
   this aside and go read one (or both). in order to understand the
   proposed change to a ntm's memory, it'll be helpful to understand a
   ntm.

intuition

   the goal is to modify a ntm to excel at id62. to
   accomplish this, the authors modify the ntm's controller's memory
   access capabilities. however, the paper is rather terse in justifying
   their specific change. a ntm controller is capable of using
   content-based addressing, location-based addressing or both, so when
   santoro et al. suggest using a pure content-based memory writer, i was
   confused why this would offer any improvement. the only explanation the
   paper offers is that "[the ntm's memory access method] was advantageous
   for sequence-based prediction tasks. however, this type of access is
   not optimal for tasks that emphasize a conjunctive coding of
   information independent of sequence."

   to their credit, three of the four authors i emailed wrote back in less
   than a day with more detailed explanations. the answer is that there's
   a tradeoff when training memory-augmented networks; more sophisticated
   memory access capabilities are more powerful, but the controller
   requires more training. santoro et al. propose hamstringing the
   controller's ability to write to memory using location-based addressing
   so that the controller will learn more quickly. they point out that
   location-based addressing won't be necessary to excel at one-shot
   learning. this is because for a given input, there are only two actions
   the controller might need to do and both depend on content-addressing.
   one action is that the input is very similar to a previously seen
   input; in this case, we might want to update whatever we wrote to
   memory. the other action is that the input isn't similar to a
   previously seen input; in this case, we don't want to overwrite recent
   information, so we'll instead write to the least used memory location.

   the authors call this hamstrung memory system the least recently used
   access (lrua) module. what confused me at first is that santoro et al.
   describe the lrua as "a pure content-based memory writer," and then in
   the same sentence, state that the lrua will write "to either the least
   used memory location or the most recently used memory location," which
   sounds an awful lot like location-based addressing. i reconciled the
   discrepancy when i realized that the most recently used memory location
   is determined by which memory location was most recently read, and
   reading is determined using content similarity.

mathematics

reading

   just like the ntm, for a given input $x_t$ and memory matrix $m_t$ with
   $r$ rows and $c$ elements per row, the controller will read from memory
   a length-$c$ vector $r_t$ that is a linear combination of the memory's
   rows $m_t(i)$ scaled by a normalized read-weight vector $w_t^r$.

       $ \begin{align} \tag{4} r_t \leftarrow \sum\limits_i^r w_t^r(i)
                       \mathcal{m}_t(i) \end{align} $

   the read-weight vector is almost identical to the ntm's content weight
   vector. for a given input $x_t$, the controller will produce a key
   $k_t$. as with the ntm, this key will be compared against each row
   $m_t(i)$ in memory using the cosine similarity measure $k(\cdot,
   \cdot)$. a [7]similarity measure is "a real-valued function that
   quantifies the similarity between two objects" (in this case two
   vectors). cosine similarity is based on the angle between the two
   vectors, and is thus advantageous because it doesn't depend on the
   magnitude of the vectors (which may vary).

          $ \begin{align} \tag{2} k(k_t, m_t(i)) = \frac{k_t \cdot
               m_t(i)}{\|k_t\| \cdot \|m_t(i)\|} \end{align} $

   the similarity measure is used to produce the read-weight vector
   $w_t^r$ that specifies how much each row should contribute to $r_t$.
   the only difference between the lrua and the ntm is that there is no
   parameter $\beta_t$ to control the read-weight vector's concentration;
   i do not know why santoro et al. decided to remove this parameter.

          $ \begin{align} \tag{3} w_t^r(i) = \frac{exp\big(k (k_t,
       m_t(i))\big)}{\sum_j exp\big(k(k_t, m_t(j))\big)} \end{align} $

writing

   to write to memory, the controller will interpolate between writing to
   the most recently read memory rows and writing to the least used memory
   rows. if $w_{t-1}^r$ is the read-weight vector at the previous time
   step, and $w_{t-1}^{lu}$ is a weight vector that captures the least
   used memory location (more on this in just a second), then the
   controller can choose a convex combination of the two weight vectors
   using a scalar parameter $\alpha$ and the sigmoid function
   $\sigma(\cdot)$ to create a write-weight vector $w_t^w$:

   $ \begin{align} \tag{7} w_t^w \leftarrow \sigma(\alpha) w_{t-1}^r + (1
                - \sigma(\alpha)) w_{t-1}^{lu} \end{align} $

   each row in memory is then updated using the write-weight vector and
   the key $k_t$ issued by the controller.

     $ \begin{align} \tag{8} m_t(i) \leftarrow m_{t-1}(i) + w_t^w(i) k_t
                                \end{align} $

   to create the least used weight vector $w_{t-1}^{lu}$, the controller
   maintains another weight vector, the usage-weight vector $w_t^u$, that
   is updated after every read and write step. a scalar parameter,
   $\gamma$, is used to determine how quickly previous usage values should
   decay:

     $ \begin{align} \tag{5} w_t^u \leftarrow \gamma w_{t-1}^u + w_t^r +
                             w_t^w \end{align} $

   the least used weight vector $w_{t-1}^{lu}$ is generated from the usage
   weight vector $w_{t-1}^u$ by setting the index of the usage weight
   vector's minimum element in the least used weight vector to 1, and all
   other elements in the least used weight vector to 0. in other words,
   the least used weight vector is really just an indicator vector for the
   smallest element in the usage weight vector. for concreteness, if the
   usage weight vector is [0.7, 0.25, 0.05], the least used weight vector
   will be [0, 0, 1].

experiments and results

omniglot classification

   the first test for the lrua mann was a classification test using the
   [8]omniglot dataset. if you're not familiar, it's a collection of
   symbols from a number of different written languages, with over 1600
   separate classes and only a few examples per class, making it perfect
   for one shot learning. to reduce the risk of overfitting, the authors
   perturbed the symbol images with small, random translations and
   rotations. they also created new classes by rotating the initial
   classes' images by 90, 180 and 270 degrees.

   the first test involved randomly selecting five classes and then
   randomly assigning each class a label 1 through 5 (in practice, the
   class labels were one-hot vectors). the objective was to infer the
   label that corresponds to each class. for concreteness, the five
   classes could be the first five letters of the english alphabet ['a',
   'b', 'c', 'd', 'e'] and the corresponding labels are [2, 5, 4, 1, 3];
   each model would be shown an instance of one class, submit a guess as
   to what the correct label is, and then it would be informed what the
   true label is. this would be repeated until the model had seen 10
   instances of each of the five classes.

   a lrua mann, a lstm and a vanilla feedforward neural network were all
   trained at this task. to establish a baseline, nine humans each
   performed the task twice. as the table below shows, the lrua mann
   outperformed its three competitors. it managed to correctly label over
   a third of class instances it had never seen before. the lrua mann
   learned that if a key was a poor match to keys it had seen previously,
   then it should try guessing a class label it hadn't already seen before
   e.g. if the controller sees 'd' and already knows that 'a' is 2, 'b' is
   5 and 'c' is 4 and can recognize 'd' as distinct from 'a', 'b', and
   'c', it should guess a new label like 1 or 3.
   [mann_five_label_results.png]

   below are two plots comparing the lstm's performance against the lrua
   mann's performance (the paper doesn't explicitly state this, but the
   lrua mann used a lstm controller) as functions of training episodes.
   note that the mann quickly learns to outperform the lstm.
   [mann_lstm_five_class.png] [mann_mann_five_class.png]

   the same experiment was repeated, but with fifteen classes instead of
   five classes. humans weren't included as a baseline because human
   working memory struggles to hold more than [9]seven items and as the
   authors put it, "[trying to learn fifeen or more classes] proved much
   too difficult and highly demotivating." instead, two additional models
   were considered. the first was a k-nearest-neighbors non-parametric
   model that used the raw input pixels. the second was a
   k-nearest-neighbors non-parametric model that instead used features
   extracted by an autoencoder; the autoencoder's encoder and decoder each
   had two 200-unit layers and a bottleneck layer of 32 units.
   [mann_five_char_label_results.png]

   as before, the lrua mann with a lstm controller outperformed its
   competitors. interestingly, the ntm almost performed as well as the
   lrua mann, confirming the early hypothesis that by reducing the ability
   of the controller to write to memory, the lrua mann will learn faster.
   the lrua mann also continued to outperform the lstm. both in the limit
   and with fewer training episodes
   [mann_lstm_fifteen_class.png] [mann_mann_fifteen_class.png]

gaussian process

   for the next test, the authors shifted from classification to
   regression. they sampled functions from a [10]gaussian process (gp)
   with fixed hyperparameters, and then tested whether the lrua mann could
   learn to predict function values by interpolating from previously seen
   points. each training episode consisted of the x-value $x_t$ and the
   time-offset function value i.e. $f(x_{t-1})$. as a baseline, the lrua
   was compared with a gaussian process given access to the same
   hyper-parameters used to generate the function. the figure below shows
   how closely matched the two models' predictions are.
   [mann_gaussian_process_1d.png]

   the lrua mann also performed well against the gp in two and three
   dimensions.
   [mann_gaussian_process_2d_3d.png] [mann_increasing_curriculum.png]

discussion

is a mann a meta-learner?

   i think a large part of the cognitive dissonnance i experienced reading
   this paper was due to the introduction, which frames the mann as a
   meta-learner. meta-learning is the idea of a learning system with two
   collaborative learning processes that learn across different time
   scales: in the short term, one learning system quickly learns how to
   perform well at an immediate task, and in the long term, the other
   learning system slowly learns common structure across tasks.

   the authors argue that meta-learning is a possible way to achieve
   id62, and then argue that memory-augmented neural networks
   are excellent candidates for meta-learning: "this ability [of rapid
   encoding, storage and retrieval] makes the ntm a perfect candidate for
   meta-learning and low-shot prediction, as it is capable of both
   long-term storage via slow updates of its weights, and short-term
   storage via its external memory module." however, when i think of
   "learning" in the context of neural networks or machine learning, i
   don't think of data storage; i think of updating model parameter values
   in accordance with newly observed data. i'm forced to ask whether
   storing data in external memory represents a form of short term
   learning, and i'm not at all confident that it does.

memory wiping

   the earlier comparison of the lrua mann with the human test subjects
   wasn't entirely fair. in order to surpass human performance, the lrua
   mann's memory had to be wiped between episodes. without that added
   code, the lrua mann was much less successful at beating the humans as
   the image below shows. the authors say that the lrua mann was still
   successful under certain setups (they specifically mention ten classes
   with episodes of length 75), but they don't provide the percentage of
   setups in which the lrua mann outperforms humans without the memory
   wipe.
   [mann_memory_wipe.png]

future directions

   one of the coolest parts of writing this paper's authors is that they
   were willing to tell me a little about future research directions. both
   santoro and lillicrap suggested reading their [11]recent paper, and
   santoro also suggested google brain's [12]recent paper as well as
   fair's [13]slightly older paper. i'm sharing these in case readers want
   to continue to learn more about the topic, as this might be my last
   post on manns in the short term.

summary

     * a key feature of human intelligence, id62, refers to
       the ability of humans to substantially shift behavior following as
       little as a single experience
     * deep learning has historically struggled to achieve one-shot
       learning
     * memory-augmented neural networks can display id62
       capabilities on par with humans in specific tasks
     * more advanced memory-access capabilities appear to require more
       training

notes

   i appreciate any and all feedback. if i've made an error or if you have
   a suggestion, you can [14]email me or comment on the [15]reddit or
   [16]hackernews threads. i still haven't created a mailing list or
   integrated rss, but i promise i'll get to it after submitting my mres
   applications.

   slate theme maintained by [17]pages-themes

   published with [18]github pages

references

   visible links
   1. https://arxiv.org/abs/1605.06065
   2. https://arxiv.org/abs/1605.06065
   3. http://rylanschaeffer.github.io/content/research/neural_turing_machine/main.html
   4. http://rylanschaeffer.github.io/content/research/neural_episodic_control/main.html
   5. https://arxiv.org/abs/1410.5401
   6. http://rylanschaeffer.github.io/content/research/neural_turing_machine/main.html
   7. https://en.wikipedia.org/wiki/similarity_measure
   8. http://www.omniglot.com/
   9. https://en.wikipedia.org/wiki/the_magical_number_seven,_plus_or_minus_two
  10. https://en.wikipedia.org/wiki/gaussian_process
  11. https://arxiv.org/abs/1702.04649
  12. https://arxiv.org/abs/1703.03129
  13. https://arxiv.org/abs/1410.3916
  14. mailto:ryschaeffer@ucdavis.edu
  15. https://www.reddit.com/r/machinelearning/comments/65ms7x/d_explanation_of_deepminds_oneshot_learning_with/
  16. https://news.ycombinator.com/item?id=14123925
  17. https://github.com/pages-themes
  18. https://pages.github.com/

   hidden links:
  20. http://rylanschaeffer.github.io/content/research.html
