   [builder-white.gif] [breaker-white.gif] build it, break it
   the language edition
     __________________________________________________________________

   [1]build it break it is a new type of shared task for ai problems that
   pits ai system "builders" against human "breakers" in an attempt to
   learn more about the generalizability of current technology. this page
   describes the motivation and structure of the shared task.
   sign up to be a builder or a breaker [2]here and get access to all the
   data and submission instructions!
   want to just download the data? find the links below:
   @builders: download the [3]training and blind dev data!
   @breakers: download the [4]blind test data!

   motivation

   it is widely acknowledged that most nlp systems are quite brittle. at
   the "[5]workshop on representation learning for nlp" at acl 2016, chris
   dyer (moderator) asserted that for any linguistically naive model, he
   could construct an example on which it fails. most panelists agreed.
   the fact that they were so flippant about this suggests that there is a
   lot left to be desired in the traditional model of learning widely
   adopted from the machine learning community. that model assumes that
   the data distribution at test time matches that at training time, and
   that all errors have the same cost.
   in software development, we know that pretty much all software has
   bugs, and nefarious agents may choose to exploit those bugs to
   sometimes produce surprising, dangerous or advantageous behavior. the
   [6]build it break it fix it contest (bibifi) was developed with this in
   mind, and our shared task brings similar ideas to the natural language
   processing and linguistics communities, but where "breaking" for us
   means "causing systems to make incorrect predictions". the goals are
   multi-fold: we want to build more reliable nlp technology, by
   stress-testing against an adversary; we want to learn more about what
   linguistic phenomena our systems are capable of handling so that we can
   direct research in interesting directions; we want to encourage
   researchers to think about what assumptions their models are implicitly
   making by asking them to break them; we want to build a test collection
   of examples that are not necessarily high id203 under the
   distribution of the training data, but are nonetheless representative
   of language phenomena that we expect a reasonable nlp system to handle;
   and finally, we want to increase cross-talk between linguistics and
   natural language processing researchers.

   challenge setup

   our shared task will run in three rounds:
    1. building round: we will release training data (which you can choose
       to use or ignore as you like) and builder teams will build systems
       for solving that task. five days before the end of the building
       round, we will release blind development data on which builders
       must make predictions and upload them to our server.
    2. breaking round: breakers will be given access to the predictions of
       the builder systems on the development data, as well as a
       reasonably large collection of blind test data. from this test
       data, they must construct minimal pairs that they think will fool
       the builders' systems. one sentence in the minimal pair must be a
       sentence that exists in the blind test data, and the other must be
       a very small edit of that sentence. breakers will provide ground
       truth labels for both sentences in the minimal pair. they must
       upload these data to our server.
    3. judgment round: all minimal pair sentences will be collected,
       shuffled, and sent back to the builder teams. they must run their
       system as is and upload the predictions on all the breaker data.

   note: we use the term minimal pair more broadly than standardly used in
   linguistics. for any given pair, the difference between them will
   ideally be targeted enough such that we can identify a particular
   linguistic phenomenon that the system is apparently able to
   handle/unable to handle in making predictions for one member of the
   pair versus the other.

   task specifics

   we will run two tasks in parallel:
    1. sentence-level id31: this data is derived from the
       [7]pang+lee+vaithyanathan data from rotten tomatoes with further
       annotation by [8]socher+al., and with additional new test data
       crowdsourced. the task is a simple binary classification: given a
       sentence, predict whether it's positive or negative toward the
       movie.
    2. id14 as id53: this task and data
       are derived from the he+lewis+zettlemoyer's work on
       [9]question-answer driven id14. the input is a
       sentence and a question related to one of the predicates in the
       sentence, and the output is a span of the sentence that answers the
       question.

   breaking instructions (sentiment)

   for the purpose of these instructions, we will consider the concrete
   task of id31 because it is easier to discuss. the task
   here is to label the sentiment of a sentence as negative (-1) or
   positive (+1), as in:
        +1      the acting is superb.
        -1      this movie tried to be entertaining but failed.

   you main job is to construct minimal pairs that you think will fool an
   nlp system. in order to limit variability here, we will give you a list
   of sentences that you must choose from to make one of the two sentences
   in the minimal pair. for instance:
        ?       every actor in this movie is horrible.
        ?       i love this movie!

   you can create minimal pairs using any of these sentences, where you
   can write your own paired sentence, but you must make it as close to
   the first as possible. for instance, you might choose to riff on the
   first example above, and make a minimal change to the wording that
   changes the sentiment, generating the following pair:
        -1      every actor in this movie is horrible.
        +1      every actor in this movie is wonderful.

   this might not be a great pair because even a stupid id31
   system would probably get this right.

   you could also riff on the second example above, and not change the
   sentiment, generating the following pair:
        +1      i love this movie!
        +1      i'm mad for this movie!

   this is more interesting because a simple sentiment system might see
   the word "mad" and think that this makes the review negative.

   your goal is to construct examples on which the systems make errors on
   one, but not both, of the sentences in the minimal pair. (either one is
   fine.) if you are wondering whether a particular change is small enough
   to qualify the pair as a "minimal pair," a good question to ask is "do
   i have a sense of what specifically i could conclude about the system's
   linguistic capacity, if for instance it was able to handle the first
   sentence but not the second?"
   instructions for qa-srl coming soon...

   breaking instructions (qa-srl)

   in the task of qa-srl (more details in link above), systems are given a
   sentence and a question related to one of the predicates in the
   sentence, with the task of outputting a span of the sentence that
   answers the question.

   for example:
      sentence:  ucd finished the 2006 championship as dublin champions, by beat
ing st vincents in the final .
      predicate: beating
      question:  who beat someone?
      answer:    ucd

   for creating minimal pairs in this task, breakers are only to change
   the original sentence (and, if appropriate, the answer) -- without
   making any changes to the question. so for instance, one could generate
   the following minimal pair for the example above:
      sentence': ucd finished the 2006 championship as dublin champions, when th
ey beat st vincents in the final.
      answer':   ucd (unchanged)

   this could be an interesting change because we might predict that the
   system would now predict the pronoun "they" as the answer to the
   question, without resolving to ucd.

   note that breakers cannot change the sentence such that the
   accompanying question is no longer answerable with a substring from the
   original sentence. for instance, breakers should not make a change such
   as "terry fed parker" --> "parker was fed" with an accompanying test
   question of "who fed parker", since the answer to that question is no
   longer contained in the sentence.

   important dates

   date builders breakers
   april 5 [10]training data released
   april 26 [11]blind dev data released
   may 3 [12]blind test data released
   may 5 blind dev predictions due
   may 8 blind dev predictions released
   may 15 breaker test data due
   may 20 [13]breaker test data available
   may 25 breaker test predictions due
   june 5 [14]builder test predictions released and [15]breaker test
   ground truth released
   june 12 scores and final results released
   june 19 builder/breaker description papers due

references

   1. https://bibinlp.umiacs.umd.edu/index.html
   2. https://go.umd.edu/bibinlp
   3. https://go.umd.edu/bibinlp_data
   4. https://go.umd.edu/bibinlp_breaker_data
   5. https://sites.google.com/site/repl4nlp2016
   6. https://builditbreakit.org/
   7. http://www.cs.cornell.edu/people/pabo/movie-review-data/
   8. http://nlp.stanford.edu/sentiment/
   9. https://dada.cs.washington.edu/qasrl/
  10. https://go.umd.edu/bibinlp_data
  11. https://go.umd.edu/bibinlp_data
  12. https://go.umd.edu/bibinlp_breaker_data
  13. https://go.umd.edu/bibinlp_data
  14. https://drive.google.com/drive/folders/0b9dgow7vnqd3nwnaq19frm5hvmc
  15. https://drive.google.com/drive/folders/0b9dgow7vnqd3zum3zwkwthzzr2m
