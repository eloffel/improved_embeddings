   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

learning ai if you suck at math         p7         the magic of natural language
processing

   go to the profile of daniel jeffries
   [14]daniel jeffries (button) blockedunblock (button) followfollowing
   apr 15, 2017
   [1*i5gqpk5gpz4lf6vp7cmwaq.jpeg]

   after discovering [15]the amazing power of convolutional neural
   networks for image recognition in part five of this series, i decided
   to dive head first into [16]natural language processing or nlp. (if you
   missed the earlier articles, be sure to check them out: [17]1, [18]2,
   [19]3, [20]4, [21]5, [22]6.)

   this hotbed of machine learning research teaches computers to
   understand how people talk. when you ask siri or the [23]google
   assistant a question, it   s nlp that drives the conversation. of course,
   as an author of novels and articles, working with language seemed like
   the obvious next step for me.

   i may suck at math but words are my domain!

   so i set out to uncover what insights nlp could give me about my own
   area of mastery.
   [1*naxkv-kg2rxtex-lcw4faa.jpeg]
   behold the bard!

   i had so many questions. had nlp uncovered the hidden keys to writing
   heart-wrenching poems? could ais turn phrases better than [24]the bard?
   could they elucidate the secret to writing [25]compulsively clickable
   headlines?

   luckily, i had just the right project in mind to test the limits of
   nlp. i was in the midst of naming the second book in my epic sci-fi
   saga [26]the jasmine wars but i   d struggled to find the perfect title.
   so i wondered:

   what if i could feed a neural net with the greatest titles of all time
   and have it deliver a title for the ages?

   this isn   t my first foray into computer assisted title generation.
   there are [27]a number of random title generators [28]out on the
   interwebs [29]that i   ve tried from time to time.

   frankly, they   re not very good.

   they   re the type of toy you play with for a few minutes and then move
   on. they work by randomly slamming words together or by iterating
   through a few basic permutations like    the _______ of _________.    i
   seriously doubt a single author actually selected his or her title from
   the primordial word soup these engines produce.

   throwing words into a hat, shaking it up and pulling them out won   t get
   you very far. a million monkeys typing randomly on keyboards might make
   shakespeare in a million years, but i don   t have that kind of time.

   ai to the rescue!

networks that peer into the depths of time

   as we learned in [30]part five, neural networks hold amazing power
   because they do automatic feature extraction. we can   t tell a machine
   all the steps we take to drive a car but we can let it figure it out
   all by itself!

   as an author i use all kinds of tricks to capture people   s attention
   but trying to boil those down to a set of rules is virtually
   impossible. it goes well beyond simply understanding nouns, verbs and
   adjectives. there   s a rhythm to language. words can spark fiery images
   in your mind. they can overwhelm you with emotion, making you break
   down with tears or get you quivering with anticipation. they create
   sound and fury, movement and feeling.

   can a machine do all of that?

   am i on the chopping block of automation?

   will ai make writers redundant in the future?

   to find out, i first needed to figure out what kind of neural network
   (nn) i needed. nns are very specific to the problem they   re trying to
   solve. humans might posses a [31]universal learning algorithm but we
   certainly don   t know it yet. the current state of the art focuses on
      narrow ai    with each neural net doing some things well and other
   things really badly.

   so what kind of nn helps us understand language?

   hands down the dominant force behind nlp are recurrent neural networks
   (id56), in particular long short term memory (lstm) id56s.

   so let   s take a look at these and see if they can help me unlock the
   secrets of blockbuster title creation.

the magic of recurrent neural nets

   inevitably, when you start looking into id56   s you discover openai
   researcher andrej karpahy   s blog    [32]the unreasonable effectiveness of
   recurrent neural networks.    the title alone filled me with tremendous
   hope.

   just how unreasonably effective are these amazing systems?

   if the title didn   t get me, the first line surely did:

      there   s something magical about recurrent neural networks?   

   magical!

   i knew a fantastic title couldn   t be far off, its supernatural power
   already swirling in the hidden depths of the matrix.

   so what makes id56   s    magical?    first, they   re particularly adept at
   predicting the future.
   [1*t_ozqg9ztpceyanyuicckq.jpeg]

   when you buy a stock or pick someone up at the airport, you   re making a
   guess about the future. a baseball player trying to snag a fly ball has
   to predict the arc of the ball and leap to where it   s going to catch
   it.

   we make predictions all the time, whether we   re weaving our way through
   big city foot traffic or driving a car.

   are those other cars going to hit you?

   is someone veering into your lane?

   where will your friend be waiting for you at the airport?

   we   re constantly trying to predict what happens next and react to it
   ahead of time so we   re ready. id56   s do the same thing by analyzing time
   series data.

   iframe: [33]/media/5a5dffa45e2148768a39e8371170d8e3?postid=f3819a689386

   they can look forward and unlike most other nn   s they can look back
   too. they have a    memory    of events past. they can see the trajectory
   of a rocket or a stock price move and predict a buy or sell. when it
   comes to self-driving cars they can predict trajectories and arcs,
   which means they can help prevent accidents (as you see in the footage
   of a tesla chiming a warning before a crash) or when to take an off
   ramp.

   they   re also good at working with sequences of arbitrary length. that   s
   unique because most nn   s can only take fixed size vectors/tensors and
   output fixed size vectors. with convolutional neural nets we have to
   munge our images into a certain shape to make them work. but that   s a
   no go for text. you can   t take a novel or the entirety of wikipedia and
   jam it into a one size fits all box. this flexibility makes id56s great
   for nlp, which encompasses everything from [34]machine translation to
   [35]id31 to [36]google   [37]s pixel ai understanding your
   questions.

   this even gives id56   s a level of    creativity.    check out [38]this
   article on generating music with id56s. the system is trained on a
   series of musical sequences and    makes    music by predicting the likely
   next sequences.

   what else can id56s do?

   if we were doing id31, aka trying trying to figure out if
   people are feeling good or bad about something, then we could feed it
   movie reviews and have it output a binary classification score from
   love (1) to hate (-1).
   [1*2tohrjiryavmf2bao2tq5w.png]

   you could also feed it a single input and have it deliver a series of
   outputs. for example, we could feed the network an image (single input)
   and have it generate a text summary of images (series of outputs).
   [39]check out this article that shows how id56s look at pictures and
   deliver summaries like    boy doing a backflip off a wakeboard.   

   the system locates objects and tries to create a sentence from that,
   like we see with this gal playing tennis.
   [1*9b_usmgpvpgur48c4xjmjw.png]

   we could also feed it a sequence to vector network, called an encoder
   and output the reverse, a vector to sequence, which we call a decoder.
   [1*5-dyha1cd7j57ejfjyp3gw.png]

   this is useful for machine translation ([40]as seen in this awesome
   article from the machine learning is fun dude). [41]google recently
   used id56s to revamp their google translate system into something that
   blows away the gobbledygook translations of previous versions,
   delivering human level translation capabilities. and on april 11, 2017,
   [42]google finally open sourced the model behind their translation
   engine, called tf-id195 (not the best marketing name but hey, we   ll
   take it.) basically what we do is train the network with both the
   original text and a professionally translated text in another language
   (the input and output) and then use that to help the machine translate
   fresh documents it   s never seen.

   but for our purposes we need one very specific feature of id56   s:

   they   re great at generating text.

   it   s also what karpahy   s    magical id56    blog is about and what got me
   interested.

   let   s take a quick look at how id56   s work and then leap into how i
   attempted to use them to generate my next great american novel title.

time after time

   recurrent nets look a lot like feed forward neural nets, except they
   also have connections that point backwards. if we look at a simple
   single neuron id56, we can see that it receives inputs x at a particular
   point in time, which we call a    frame    as well as an output from the
   previous step y(t-1).
   [1*nlevy4zretjyvepmpmbj-q.jpeg]

   the network is really a series of steps in time. it   s no coincidence
   that each step is called a frame, because it   s like the frame in a
   film. we    unroll the network    through time, just as we play a movie on
   the silver screen.
   [1*07p_ocfuo81w4us4xfcytg.jpeg]

   time is represented by    t   . the current moment in time is just    t   
   which we see in the middle frame. the previous step is    t-1    (on the
   left) and one step into the future is    t+1    (the right). the s in the
   middle is the hidden state, hence the    s    for state. it is the memory
   of the cell. in pure feed-forward networks the inputs are just the
   weighted outputs of previous nodes. in a id56, this also includes the
   weighted outputs from a previous time step. in other words, like we
   said earlier, it can look back in time and it can attempt to predict a
   future step.

developing long term memory

   basic id56s have a few challenges. one of the main challenges with plain
   old id56   s is that if the network is too deep it can easily begin to
      forget    information from earlier parts of the time sequence.

   why is that an issue?

   well let   s pretend you have a id56 doing id31 of news
   about stocks, looking to generate buy or sell signals based on whether
   the public is bullish or bearish on a stock. a stock blogger may start
   off telling you to sell in the first sentence and then spend the rest
   of the article lauding the future buy potential of the stock once it
   has a few weeks to recover from whatever news is damaging the stock
   today. the system may forget the    sell    part and declare it a strong
      buy    based on the positive sentiment later in the story.

   they can also have difficulties learning long range dependencies even
   over shorter sequences. that becomes a serious problem in nlp because
   the meaning of a sentence isn   t always clustered closely together.

   most people don   t produce sentences that would make their grade school
   grammar teacher proud. instead they scatter the meaning all over the
   sentence. they use screwy grammar and slang. for humans this is no
   problem. we have the remarkable ability to understand sentences that
   are all jacked up. misplaced modifiers, missing words, typos, and
   dangling participles won   t slow us down but they can really trip up
   machines.

   for example, if i say    the man in the blue blazer and white cap played
   a brilliant jazz solo   , the point of the sentence is not what the man
   is wearing, which is close to the subject of the sentence but that he
   played a brilliant jazz solo. if the system forgets that information by
   the time it gets to the music it missed the point.

   this is what   s know as [43]the vanishing gradients problem which
   stanford   s awesome deep learning and nlp class goes into at length.

   but i   ll save you a lot of reading and give you a quick summary here.
   to understand vanishing gradients you need to understand a bit about
   id26. in [44]part five we talked about how id26
   looked to minimize errors by working to the lowest point on the error
   landscape. that helps the neural network adjust its weights so that it
   can go to the next epoch of training. it looks like this in 3d:
   [1*mtg0axcrlrxen7i-ebrduw.png]

   in some ways it is easier to understand in 2d though, so let   s see
   that:
   [1*t6jzgh7bghgoae8kavcjcq.jpeg]

   the system is taking tiny steps, as it tries to work its way to the
   bottom of the curve. now, that   s all well and good when you have a
   clean error landscape with a nice well-defined curve. but what if the
   curve flattens out badly? let   s take a look.
   [1*evnde-0l7wkgpeu14neega.jpeg]
   courtesy of the stanford deep nlp course

   when the line flattens out we call the neurons    saturated.    instead of
   activating and finding useful data, they are effectively dead. even
   worse, they have an exponentially bad effect on previous neurons.
   remember that neural networks are matrices, which are really just
   spreadsheets on steroids. one cell is added or multiplied to the next
   cell in a long chain of equations.
   [1*i8_yjmvmt2nyjmkffosfzg.jpeg]

   by the way, if you   re still struggling with matrix math, i am loving
   this anime book called [45]the manga guide to id202.

   the japanese just have better teaching tools. if i had this book in
   school i might have enjoyed it a lot more.

   back to the math!

   remember our dot product image from [46]part six on math notation?
   [1*nylc61bic9qosjbhfty41a.jpeg]

   now imagine all of those numbers are zero or almost zero. what happens
   to the chain of calculations?

   when a number of neurons have small numbers as their value, the
   multiplication causes the gradient values to shrink exponentially fast,
   which quickly drives all the neurons in the chain towards zero. this
   means they   re effectively turned off and doing nothing. they   re like
   dead pixels on a tv screen, no longer useful. the deeper the network
   the worse this problem gets.

   a number of solutions to this problem cropped up over the years. the
   first was to use the relu activation function instead of the tanh or
   sigmoid id180.

   why do that? well you just have to look at a sigmoid curve to
   understand.
   [1*pdvlqrjdq-cyhfimxovdua.png]

   notice how it has that nice curved edge at the bottom and top? we want
   curves like that when we   re drawing a face or the arch of a bridge, but
   that bottom curve is the slope of despair when it comes to vanishing
   gradients.

   now look at a relu vs sigmoid visualization:
   [1*htbushcmyqwzwb1tuptqjg.jpeg]

   notice that hard angle! the relu function delivers a constant of 0 or 1
   and as you can see it has a hard shape with no soft slope at the edges,
   so it isn   t as likely to hit that vanishing problem.

   but there   s a better solution. let   s check that out.

enter the dragon

   the real answer to the question of vanishing gradients is not to change
   the activations on a regular id56.

   it   s to switch to the more popular [47]long short term memory (lstm),
   first outlined in 1997, or [48]gated recurrent networks (grus),
   outlined in 2014.

   both of these architectures were designed with vanishing gradients in
   mind. they were also meant to look for long range dependencies. in
   practice regular id56s are rarely used anymore, while grus and lstms
   dominate the field.

   the name lstm might seem strange at first but not when you consider
   what the network is doing. in essence an lstm is a black box memory
   cell that looks like a standard id56 memory cell but in reality it holds
   dual states in two vectors, a long term state and a short term state.

   [49]here is an illustration of an lstm from my friends over at the deep
   learning 4 j team.
   [1*jglsdje0untt09ypywzyoq.png]

   by the way, one of the absolute best books i   ve read on this topic (and
   neural nets/deep learning in general) is the just released [50]hands-on
   machine learning with scikit-learn and tensorflow. i saw a few earlier
   editions and they really upped my game. don   t wait, just grab it asap.
   it rocks. it goes into a ton more detail than i have here but i   ll give
   you the basics to get you moving in the right direction fast.
   [1*hcjyfbrjz7jnfvwizgxwrq.jpeg]

   you can see that information travels along two lines through a series
   of    gates.    the top line is called the    forget line.    this is a pretty
   piss poor term, in my humble opinion, but i didn   t name it, so don   t
   blame me. let   s just go with it.

   the    forget line    remembers the long term state.

   it gets copied forward into new cells as the network unrolls. actually,
   it   s not a completely ridiculous name.

   it   s called the forget line because it does loose bits of information
   as it goes.

   the other lines contain short term associations and memories, which are
   then incorporated into the    forget    line.

   at each time step some memories go out the window and some get added.

   gru   s are basically a simplified form of lstms. check out [51]this
   awesome diagram of a gru from jacob kvita, a comp-sci student and
   former red hatter, who made them for his thesis.
   [1*ehw4akihv11enr_ps0k2fw.png]

   what   s the difference?

   the gru cell merges the long and short term memory into a single
   vector.

   why do that? simple. performance. it   s less computationally expensive
   and yet somehow seems to perform as well. that   s a win!

   it also uses only a single    gate    for both the short and long term
   memory. lastly, it adds a new kind of gate that decides what to show to
   the next layer.

monkeys in the machine

   ok. all that   s great, dan, but how do i generate text from that?

   good question.

   karpathy   s post demonstrates a    character    level id56. a character level
   model looks to understand language on a character by character basis.

   how does it do that?

   all neural networks are essentially complicated prediction engines. so
   we feed the system millions words and it stores those words as
   sequences of characters. then it begins to predict what the next
   character is likely to be. once it   s learned what to predict we can
   then have the system pull tricks for us like generate sample text based
   on feeding it a    seed    set of words. that   s all theoretical, so let   s
   look at a simple example.

   first, let   s pretend that the system has only learned a few words:
     * hey
     * hello
     * help
     * i
     * there
     * need

   we also teach it a few punctuation marks like    .    and    !   

   remember though that our simple id56 hasn   t learned complete words. it   s
   only learned a series of characters, so instead of understanding
      hello    as an entire self contained entity, it knows h-e-l-l-o. in
   knows that    e    follows    h    and so on.

   now imagine that i show the system a million variants of sentences that
   i can construct from the few vocabulary words that i   ve taught the
   machine. those sentences might be something like:
     * hey, there. hello!
     * hello! help!
     * help!
     * help.

   i then seed the engine with the phrase:
     *    i need he   

   notice that i didn   t write the complete word that i want it to guess.

   the system would then look inside its black box and try to predict the
   next likely character. in this case it could be either    l    as in
      hello    or it could be    y    as in    hey    or it could be    l    as in    help.   

   if the network is properly trained we hope it chooses    l    and
   eventually    p    for    help    because that   s one of the few constructions
   that make sense.

   [52]we can find a character level id56 implementation in the keras
   examples github.

   it   s trained on a corpus of nietzsche with about 100,000 words. the
   example recommends that we use at least a million words to make the
   system more robust.

   [53]i decided to feed my character level id56 a dataset that i created
   by hand over a few days which you can find on my github. i typed in
   every single great novel title by combing through my memory, bookshelf,
   and numerous top 100 lists.

   unfortunately, i quickly ran out of great book titles.

   one option would be to simply feed it as many titles as i could find by
   downloading library catalogs, but i wanted to focus on titles that
   really stood out and not clog it up with any old crap. to augment it i
   went on to great movie titles, then great songs and band names.

   still when i was done, i was left with a mere 26k worth of words, which
   made the system particularly unreliable. but i decided to give it a go
   anyway. so how did it do? here are few results.
tha ect are dog
a9t byta go than
wel pt year benc

   hardly magical.

   even after training the system for many, many, many epochs it still
   mostly sucked. i ran the system for 7000 iterations overnight. it still
   produced garbage.

   at this point i couldn   t tell whether it was just the tiny dataset that
   i gave it or the id56 itself. rather than brute force tweak the system,
   i decided to see if i could find an answer to that question before
   spending five nights tuning the system to no end. as i puzzled over why
   it failed, i turned back to karpathy   s blog and found a potential
   answer.

   karpathy trained his character level generator on shakespeare, with
   significantly more text for the machine to eat up. here is an example
   from his post:

      pandarus: alas, i think he shall be come approached and the day when
   little srain would be attain'd into being never fed, and who is but a
   chain and subjects of his death, i should not sleep.

   second senator: they are away this miseries, produced upon my soul,
   breaking and strongly should be buried, when i perish the earth and
   thoughts of many states.   

   he   s particularly excited that the system seems to be generating text
   that looks like shakespeare, at least a first glance.

   it is formatted like a play. there is dialogue. there are character
   names. it even has a little flavor of the bard with words like    alas.   

   in some respects this is truly amazing. remember that the system
   doesn   t know anything about english. it has no context. it has no
   knowledge of verbs or characters or dialogue at all. it learned that
   through grokking the patterns and outputting a similar pattern.

   however, as a writer, i found myself less enamored with this output
   than karpathy.

   while it   s true that the system aped the basic formatting of a play, i
   don   t see this as much of a feat. we had dumb systems capable of
   auto-formatting a play in 1980   s for screenwriters. the biggest thing i
   notice is that the system produced gibberish that   s formatted nicely,
   but that means absolutely nothing. it produces words, but the words put
   together add up to zilch. the sentences mean nada.

   basically, it detected a pattern but not a very useful one. i wanted it
   to learn poetry and it learned how to act like a sort of smart version
   of [54]screenwriter pro.

   but i didn   t lose hope!

   intuitively, i recognized that it makes little sense to try to train
   these systems at the character level.

   why make the system work so hard to try to predict what the next
   character should be so as to form some semblance of words?

   notice that even in the shakespeare output it sometimes produced
   nonsense words like    srain    which means that even after hours of
   training it was still struggling to avoid kindergarten level mistakes.
   i wondered if researchers realized, like i did, that it made more sense
   to train the system at the    word    level or even the    sentence    level.
   in other words, instead of studying    h-e-l-l-o    train it on    hello   .

   turns out they did.

   i discovered [55]this modification of the basic character id56, turning
   it into a word level monster. this system also introduced the more
   advanced concepts of lstm and grus. awesome! now the system can learn
   whole words, instead of letters.

   and in a testament to just how fast this field is developing, there are
   dozens of word level id56 systems, since i started this article three
   months ago. i had some other work take precedence, and some of the
   earlier learning ai articles seemed to fit better if they came first,
   so i set this article aside. now i come back to find a [56]number of
   different versions of [57]word level id56s for [58]you to play with and
   [59]test out. sweet!

   so did it work? here   s some output after training the system for
   thousands of epochs, again remembering that my dataset is far from the
   ideal size.
play go the wide virgin me is teen scream i, masque and a champions the for is w
ith myself tears, the tropic of the looking ugly the journey of to big empire th
e red what adventures the naked nails dirty what's west twenty mask in the end o
f earth as dance to the atlantis was be if even in me paradiso crime smokestack
mojo jest the carpenter the nightmare of heights the golden twenty house so 1/2
hand in the drugs were god the snows and the rain cat things we thank my knew l.
a. did deep the goblet in steal: the an these along the bonfire the end of quart
er halloween madonna mote killshot way of the river torturer the inc. rex the an
vil of imagination were sabbath wild morning angry mice the thin street tangled
got in want pretty a turning of the beethoven not salem's atuan break, lost red
charlotte's drummer giving ship a susie on mars the night don't still crash spy
in the ritz the goblet of heaven the cure good cosmos the time's brigade this dr
eams can't folsom dove you jumping hide come is a city wars in the taming in lik
e for the mind
all above terra doom things rehab exit you lays heat the devil outrageous cry cl
ash place the ashes men side the toyshop the velvet in the red a road without li
ttle red of door comedy undery me a gods the eden and the black badge in stop th
e wall and the night 96 captain! street to time on the earth of bees steel why t
o empty got i want myself rolling iron in everything songs oh, be nd folsom a gr
ifters the game the secret fountainhead the river nine germs nights for me are k
now you wear miles in on stuff up vanity sleep the clash a empire a lost in a se
x machine wake dazed what steel steal: for chocolate secret planet moment purple
 red snow some are dark, me you a row suspicious detective surrender will hound
delicatessen none the cathedral of empires what mary going big whom need by this
 the dancer up summer nine kill night fight dog cross and the bob world californ
ia i 101 suede drummer book pyscho prophet eye of the river men man i war be eye
d be video dream see samurai the widening baby the standing express untrodden th
e man of the

   it outputs a giant block of text that is a little hard to deal with, so
   i wrote a little script to slice it up into 2 to 7 word sentences,
   which is about the length of a good title. most good titles actually
   live in the four word range.

   that gave me some good results that i saved to a file, discarding the
   obvious gibberish ones.
sleepless in cryptonomicon
the sun rope
delicatessen in the jungle
daisy the cloudy shoplifters
waiting for a glass full
blood agency
the china proposal
beloved mayor of horton
walking china
the metropolis jacket
the steel beowulf
magnolias dawn, little prarie sun
fried castle blind
sense of disobedience
the meatballs dune
china hooker tomatoes
of slave blood
in the usual house
trial fried castle
why eternity glass
the lovely wide evil
the bright gene
the infinity half
the lathe of dr dispossessed
to murder proud
the sick archbishop
gun man blue
in the silence
the radio who dragons through
glory of the dead
a golden geisha
the sand woods
gates of cholera
a right good dawn
a rosetta ruby
new tide sky
the fire plan
man to barbarism
the deception needle
the river break
the secret electric manifesto
city of lost faces
jude the key
mystic germs
the roman woods
gold sweet death
the brand morgue
sweet dreams piano
loving shanghai
end of lolita childhood
cold geisha
the last baby
good journey into the light
the door song
song for want
the bitter lady
i, samurai
in me, not get proud
mystic sex
the death of walter
stop heaven's sun
one mystic cannibal
the cannibal's candle
the secret red sky
people of the fire
stardust winter's love
johnny never gonna stop
gone thunder rolling
the metamorphosis fish
snowy spots the rainbow
the tabloid bums
the invisible deep
the deep and unbearable
call of fire
the cuckoo's jekyll
the red tenderness
the raven's school
the memories of god
the cave dragon
jim the savage
sunset now on brooklyn
black song
i was toys
the snows creek came
the secret land
the well
the last lies
lords of the knife
inside physics
the galaxy of gone
the satanic playlist
the bloody 9
freakonmics: a hard black dance
stone of fire
a road death
the feast baby
lucifer's rainbow
a severed cage
of summertime glass
lucky break in the night
the knife man
prison rain
the door to the cosmos
solitude in the frost
the clockwork chamber
the black queen
back to the wind
the blind fields
marathon of fear
sophie's dragons
the first new madre soldier
jurassic magnolias
seattle siddhartha
the glass dawn
the beloved metropolis
the glass temple
steel woods
the house of inception
the tao of the third
lonesome winter's man
sugar acid
the piano ashes
the anarchist's game
the furious tenderness
the red hallows
paradise demons
demons of time
cosmos, i ride
the machine king
the king's blue grass
the end of kashmir
the secret soldier
love of sunshine
the night of the rose
tea house cowgirls
the vishnu indigo
death of the stars
in the red morning
the star queen's face
river demons
the night runner
the charge of fire
the world of chocolate songs
a purloined cloud
the art of hanging
ode to the sleepers
the gold inside
even the asphalt
rogue funeral
sea of the red god

   some of those are not bad! as a friend said, it swerves from the banal
   to the brilliant. there are some awesome ones, like:
     * the art of hanging
     * lucifer   s rainbow
     * sea of the red god
     * river demons.
     * the invisible deep
     * black song
     * the memories of god

   there is also some comedy gold like    china hooker tomatoes   !
   [1*7yqd0xzi0lv4fl-fs6vpha.jpeg]

   you can also see it   s not a great idea to include [60]portmanteau words
   like    freakonomics    as that is clearly someone else   s famous title, so
   whenever it shows up you   ll be breaking copyright if you decided to
   actually name a book that way. better to use generic words. although i
   quite like    freakonomics: a hard black dance   , it might make a good
   followup to [61]the classic original by steven d. levitt.

   i had even higher hopes for a sentence level id56 but unfortunately i
   wasn   t able to find a decent set of working code to test out when i was
   testing this a few months ago. i [62]found several papers out of china
   that looked promising. then, of course, in my hiatus, someone went
   ahead and [63]did a kickass blog post on sentence level classification
   with code! i will likely do a followup after i test some of the new
   choices out there.

nlp and beyond

   that said, i am not sure that what these systems produce is really
   heads and tails above random word generators. it   s pretty good, but if
   you look hard enough you recognize it   s basically a semi-random mashup
   of already good titles.

   if i   m being honest with you, i have to admit i don   t find these types
   of systems very effective for cranking out shakespeare and titles, much
   less    unreasonably    effective. this kind of sentence level generator is
   mostly a parlor trick that obscures what nlp really does well.

   it turns out that nlp is much better at more restricted problem sets,
   like sentiment classification.

   in fact, you can get [64]a good breakdown of the state of the art from
   a lecture from the stanford nlp intro course. it   s just slightly out of
   date in that a few of those problems were solved better in the last few
   years but it   s still a great intro to the field.

   so what   s the state of the art? here   s a breakdown from the video:

   mostly solved tasks:
     * spam detection
     * parts of speech tagging: (adj/noun/verb)
     * id39

   making good progress:
     * id31
     * coreference resolution
     * id51
     * machine translation

   still really hard:
     * id53
     * paraphasing
     * summarization
     * dialog

   these systems shine when you go with what they   re good at doing, not
   against it, as i discovered with my title experiment.

   what do all those tasks have in common?

   in essence, these systems are good at predicting the next likely word
   in a previously understood sequence. they can also break down a
   sentence into its component parts or figure out if a sentence is
   positive or negative.

   what good is that you wonder?

   the answer is probably in your pocket. or you   re staring at the answer
   if you   re reading this on your phone. i   m talking about the google
   assistant or siri.

   after training these systems on millions of hours of people talking,
   these ai assistants can take an audio sample and quickly disambiguate a
   garbled word by predicting that the most likely next word is    help   
   instead of    halter.    in fact, i   m finding the new pixel phone, which is
   bundled with the latest google assistant to be smashingly good at this
   kind of task. it rarely predicts the wrong word when i talk to it.

   even better, it seems to understand a lot of semantic context to what
   i   m asking of it. for example when i say    show me a bunch of good
   restaurants nearby    it knows to show highly rated restaurants near me
   rather than a random selection crappy rated eateries. that   s very, very
   cool.

   it turns out that what i asked my fledgling ai nlp baby to do is a
   particularly hard problem that just isn   t solved yet. in hindsight,
   it   s not hard for me to figure out why as a writer.

   while nlp practitioners are focused on decomposing a sentence into its
   most basic building blocks, a great writer knows that the power and
   meaning of writing comes from the words working together, not taken in
   isolation.

   the real patterns i was hoping to detect are much, much different.
   they   re the stuff of art, such as poetic turns of phrase and unique
   word combinations. let   s take a look at a few great titles to see what
   i mean.

the sound and the fury of china hooker tomatoes

   here   s a famous title from maya angelou. it   s one of my favorites:

   1) i know why the caged bird sings

   this is an incredibly advanced title construction that highlights why
   nlp is so challenging.

   first of all, there are very subtle structural problems for machines
   here. for example, the title rolls off the tongue but there is no clear
   reason why. it   s not using any obvious literary techniques, like
   alliteration, that we can easily point out. if we can   t find it, the
   machine probably can   t either.

   now, it   s arguably using a technique called [65]sibilance, which is
   either the reputation of s sounds or just speaking with a low level
   whispering type cadence, though it   s not using it precisely because
   there is only one s. sibilance makes for a sensuous or sinister
   feeling. think of a lover whispering in your ear or a snake hissing,
   both of them using the s to excite or terrorize you.

   actually if [66]you   ve ever seen angelou speak she uses a great deal of
   sibilance, so perhaps when i read it, i just hear her voice in my mind?
   and that brings me to the second major problem for machine
   interpreters:

   an nlp system can only understand meaning from what is directly
   contained in the text itself.

   unfortunately, for ml gurus communication does not exist in a vacuum.

   the real power in this title comes not from what is on the page but
   [67]what feelings and associations it creates in the reader   s mind.

   we bring our own ideas, life experiences and feelings to everything we
   read. without that context, a machine can   t figure out the higher order
   understandings that make this title incredible.

   for example, a bird is born to fly. that is its primary purpose. yet,
   the bird is restricted from what it   s designed to do. it   s stripped of
   its reason for living and so it sings in its desperation and fever. it
   sings because it wants to be free, to soar and see the world as bird is
   meant to and so sadness saturates this title. of course, if [68]you
   know the author   s history and the tragedies in her own life, then you
   can see why she chose it as the title of her autobiography.

   this is something that simply can   t be teased out by using a id91
   detection algorithm. it has comes from your associative understanding.

   but all is not lost!

   let   s take a look at another great title and see if we can pick up more
   meaning from only what   s there in front of us.

   2) midnight in the garden of good and evil

   this title is easier for a basic algorithm to work through. it has
   several obvious poetic techniques, such as alliteration, which is a
   repetition of consonant sounds like    g.    since this has actual
   alliteration, as opposed to only associated alliteration, the system
   should be able to pick this kind of pattern up.

   it also has what i call the    union of opposites.    you tend to find this
   kind of dynamic juxtaposition in famous titles like    the song of ice
   and fire   , or    pretty little monsters   , or even historical events like
      war of the roses   . flowers and destruction are not precise opposites
   but one could easily be considered to have a positive sentiment (roses)
   and the other negative (war). some great titles are built on this
   principle alone, like war and peace.

   it also uses evocative and sentimental words like    midnight    and
      garden   . these words create picturesque images in the reader   s mind,
   both frightening and beautiful. a system could easily be designed to
   understand these emotionally charged words, because marketers have been
   picking out    power words    for a hundred years.

when doves cry

   ambiguity is a very hard to deal with for nlp systems and yet it   s at
   the very heart of what makes for great writing, in particular fiction,
   literature, film and poetry!

   it   s one thing to grasp the deep structure of how a basic sentence is
   constructed. if you were unlucky enough to live through sentence
   diagramming in grade school you learned how to slice up a sentence into
   its component parts. but while this might be interesting to teachers,
   editors and math peeps, you might be surprised to find that to a writer
   it   s plain old torture.

   i hated sentence diagramming!

   that   s because my fellow authors and i understand that the true power
   of words comes from somewhere else. it   s one thing to detect parts of
   speech. it   s completely different to detect what makes a phrase that
   sets a person   s heart on fire.

   sentence diagramming does not a writer make.

   even that sentence is not something a machine could comprehend. it   s
   basically bad grammar. and yet by using it, it forces you to stop and
   notice it. you have to pause for a split second to process it, even if
   that happens at an unconscious level. if i did that at a key moment in
   the plot of a great novel that i wanted you to pay close attention to,
   you might stand more of a chance of picking up on it as a reader.

and that   s that

   ok, so maybe we didn   t get the next great title for book two of [69]the
   jasmine wars. eventually i created my own title, based on a line from a
   chinese sci-fi story in translation called [70]invisible planets,
   [71]through the darkening sky. i wanted to evoke the image of a storm
   that   s coming, that you can   t escape. you can only hold on and go
   through it. by the way, here   s the new cover below. i love the artwork,
   done by an amazing artist [72]ignatio bazan lazcano.
   [1*vmiafmfd76mkahwfspqpvg.jpeg]

   but don   t let my failed automagical title generator experiment hold you
   back from diving into nlp!

   the field is currently enjoying billions of dollars in research and a
   true renaissance as it powers more and more essential apps like google
   translate, digital assistants and [73]even real-time translation
   engines that live inside ear buds. it   s critical that we teach machines
   to understand us better.

   if you want to learn more, then head on over to [74]the stanford course
   on nlp. or just keep hammering through some of the blogs in this
   article. i won   t lie: it   s not an easy subject. as we saw, language and
   math are often at odds. they seem to exist in different parts of the
   brain, which is why people often do well on only one part of the sats,
   either math or english.

   and yet, there is a strange unity to language and math. they   re woven
   together in unexpected ways, like [75]the 5   7   5 rule of haiku. i can   t
   help but think that tomorrow   s systems might discover all kinds of
   hidden patterns as they work their way through the great art and
   literature of the past. perhaps there   s a hidden pattern beneath the
   sea of words that goes so deep that even a writer can   t sense it,
   except in his dreams.

   and maybe, just maybe, there   s an ai, waiting to be born, that will one
   day sing the songs that make the whole world sing.

   ############################################

   be sure to check out the rest of this ongoing series. feel free to
   follow me because you want to be the first the read the latest articles
   as soon as they hit the press.

   [76]learning ai if you suck at math         part 1         this article guides you
   through the essential books to read if you were never a math fan but
   you   re learning it as an adult.

   [77]learning ai if you suck at math         part 2         practical
   projects         this article guides you through getting started with your
   first projects.

   [78]learning ai if you suck at math         part 3         building an ai dream
   machine         this article guides you through getting a powerful deep
   learning machine setup and installed with all the latest and greatest
   frameworks.

   [79]learning ai if you suck at math         part 4         tensors illustrated
   (with cats!)         this one answers the ancient mystery: what the hell is a
   tensor?

   [80]learning ai if you suck at math         part 5         deep learning and
   convolutional neural nets in plain english         here we create our first
   python program and explore the inner workings of neural networks!

   [81]learning ai if you suck at math         part 6         math notation made
   easy         still struggling to understand those funny little symbols? let   s
   change that now!

   [82]learning ai if you suck at math         part 7         the magic of natural
   language processing         understand how google and siri understand what
   you   re mumbling.

   ############################################

   if you enjoyed this tutorial, i   d love it if you could clap it up to
   recommend it to others. after that please feel free email the article
   off to a friend! thanks much.

   ############################################

   if you love my work please [83]do me the honor of visiting my patreon
   page because that   s how we change the future together. help me
   disconnect from the matrix and i   ll repay your generosity a hundred
   fold by focusing all my time and energy on writing, research and
   delivering amazing content for you and world.

   ###########################################
   [1*2noajzdn-t-1hpqgiuq9-a.jpeg]

   a bit about me: i   m an author, engineer and serial entrepreneur. during
   the last two decades, i   ve covered a broad range of tech from linux to
   virtualization and containers.

   you can check out my latest novel, [84]an epic chinese sci-fi civil war
   saga where china throws off the chains of communism and becomes the
   world   s first direct democracy, running a highly advanced, artificially
   intelligent decentralized app platform with no leaders.

[85]you can get a free copy of my first novel, the scorpion game, when you
join my readers group. readers have called it    the first serious competition
to neuromancer    and   detective noir meets johnny mnemonic.   

lastly, you can [86]join my private facebook group, the nanopunk posthuman
assassins, where we discuss all things tech, sci-fi, fantasy and more.

   ############################################

   i occasionally make coin from the links in my articles but i only
   recommend things that i own, use and love. check my [87]full policy
   here.

   ############################################

   thanks for reading!
   [88][1*0hqoaabq7xgpt-oyngiubg.png]
   [89][1*vgw1jka6hgnvwztsfmlnpg.png]
   [90][1*gkbpq1ruui0fvk2um_i4tq.png]

     [91]hacker noon is how hackers start their afternoons. we   re a part
     of the [92]@ami family. we are now [93]accepting submissions and
     happy to [94]discuss advertising & sponsorship opportunities.

     if you enjoyed this story, we recommend reading our [95]latest tech
     stories and [96]trending tech stories. until next time, don   t take
     the realities of the world for granted!

   [1*35tcjopcvq6lbb3i6wegqw.jpeg]

     * [97]machine learning
     * [98]artificial intelligence
     * [99]nlp
     * [100]language
     * [101]ai if you suck at math

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of daniel jeffries

[102]daniel jeffries

   medium member since mar 2017

   i am an author, futurist, systems architect, and thinker.

     (button) follow
   [103]hacker noon

[104]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 1k
     * (button)
     *
     *

   [105]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [106]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f3819a689386
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/learning-ai-if-you-suck-at-math-p7-the-magic-of-natural-language-processing-f3819a689386&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/learning-ai-if-you-suck-at-math-p7-the-magic-of-natural-language-processing-f3819a689386&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_epsf67akeeoy---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@dan.jeffries
  15. https://hackernoon.com/learning-ai-if-you-suck-at-math-p5-deep-learning-and-convolutional-neural-nets-in-plain-english-cda79679bbe3#.7ci7zh7v3
  16. http://blog.algorithmia.com/introduction-natural-language-processing-nlp/
  17. https://hackernoon.com/learning-ai-if-you-suck-at-math-8bdfb4b79037#.wb9byq3o3
  18. https://hackernoon.com/learning-ai-if-you-suck-at-math-part-two-practical-projects-47d7a1e4e21f#.x87xwbgx0
  19. https://hackernoon.com/learning-ai-if-you-suck-at-math-p3-building-an-ai-dream-machine-or-budget-friendly-special-d5a3023140ef#.9xzlb0cee
  20. https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32#.g606budcf
  21. https://hackernoon.com/learning-ai-if-you-suck-at-math-p5-deep-learning-and-convolutional-neural-nets-in-plain-english-cda79679bbe3#.xjah79lsd
  22. https://hackernoon.com/learning-ai-if-you-suck-at-math-p6-math-notation-made-easy-1277d76a1fe5
  23. https://assistant.google.com/
  24. https://en.wikipedia.org/wiki/william_shakespeare
  25. https://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/
  26. http://meuploads.com/the-jasmine-wars-2/
  27. http://www.fantasynamegenerators.com/book-title-generator.php#.wl9yyn9unoi
  28. http://www.kitt.net/php/title.php
  29. http://www.mcoorlim.com/random.html
  30. https://hackernoon.com/learning-ai-if-you-suck-at-math-p5-deep-learning-and-convolutional-neural-nets-in-plain-english-cda79679bbe3#.7ci7zh7v3
  31. http://amzn.to/2nd9mg2
  32. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  33. https://hackernoon.com/media/5a5dffa45e2148768a39e8371170d8e3?postid=f3819a689386
  34. https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html
  35. http://text-processing.com/demo/sentiment/
  36. https://assistant.google.com/
  37. https://assistant.google.com/
  38. http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
  39. http://cs.stanford.edu/people/karpathy/deepimagesent/
  40. https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa
  41. https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html?_r=0
  42. https://research.googleblog.com/2017/04/introducing-tf-id195-open-source.html?m=1
  43. https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html
  44. https://hackernoon.com/learning-ai-if-you-suck-at-math-p5-deep-learning-and-convolutional-neural-nets-in-plain-english-cda79679bbe3#.viug9k8sh
  45. http://amzn.to/2or8aoa
  46. https://hackernoon.com/learning-ai-if-you-suck-at-math-p6-math-notation-made-easy-1277d76a1fe5
  47. http://bioinf.jku.at/publications/older/2604.pdf
  48. https://arxiv.org/pdf/1412.3555.pdf
  49. https://deeplearning4j.org/lstm.html
  50. http://amzn.to/2p5t4ll
  51. http://kvitajakub.github.io/
  52. https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py
  53. https://github.com/the-laughing-monkey/learning-ai-if-you-suck-at-math/blob/master/deep learning examples/greatbooktitles.txt
  54. https://www.writersstore.com/movie-magic-screenwriter-screenwriting-software/
  55. https://github.com/hunkim/word-id56-tensorflow
  56. https://github.com/vlraik/word-level-id56-keras
  57. https://github.com/deepanwayx/char-and-word-id56-keras
  58. https://github.com/vlraik/word-level-id56-keras
  59. https://github.com/pytorch/examples/tree/master/word_language_model
  60. https://en.wikipedia.org/wiki/portmanteau
  61. http://amzn.to/2pcs7rl
  62. https://arxiv.org/pdf/1703.07713.pdf
  63. https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-id56/
  64. https://www.youtube.com/watch?v=nfoudtpbv68
  65. https://literarydevices.net/sibilance/
  66. https://www.youtube.com/watch?v=epodnjrvssk
  67. https://en.wikipedia.org/wiki/philosophy_of_language
  68. https://www.youtube.com/watch?v=fcwzm5wukdq
  69. http://amzn.to/2oxo5o1
  70. http://amzn.to/2oy102b
  71. http://amzn.to/2nd8vfe
  72. https://www.artstation.com/artist/neisbeis
  73. http://www.sciencealert.com/these-new-earbuds-can-translate-languages-for-you-in-real-time
  74. http://cs224d.stanford.edu/syllabus.html
  75. http://grammar.yourdictionary.com/style-and-usage/rules-for-writing-haiku.html
  76. https://hackernoon.com/learning-ai-if-you-suck-at-math-8bdfb4b79037#.ng7ggn5d9
  77. https://hackernoon.com/learning-ai-if-you-suck-at-math-part-two-practical-projects-47d7a1e4e21f#.yo1o1ar5h
  78. https://hackernoon.com/learning-ai-if-you-suck-at-math-p3-building-an-ai-dream-machine-or-budget-friendly-special-d5a3023140ef#.6frka033t
  79. https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32#.2jpelkuhd
  80. https://hackernoon.com/learning-ai-if-you-suck-at-math-p5-deep-learning-and-convolutional-neural-nets-in-plain-english-cda79679bbe3#.xjah79lsd
  81. https://hackernoon.com/learning-ai-if-you-suck-at-math-p6-math-notation-made-easy-1277d76a1fe5
  82. https://hackernoon.com/learning-ai-if-you-suck-at-math-p7-the-magic-of-natural-language-processing-f3819a689386
  83. https://www.patreon.com/danjeffries
  84. http://amzn.to/2gag249
  85. http://meuploads.com/join-my-readers-group/
  86. https://www.facebook.com/groups/1736763229929363/
  87. http://meuploads.com/disclosure/
  88. http://bit.ly/hackernoonfb
  89. https://goo.gl/k7xybx
  90. https://goo.gl/4ofytp
  91. http://bit.ly/hackernoon
  92. http://bit.ly/atamiatami
  93. http://bit.ly/hackernoonsubmission
  94. mailto:partners@amipublications.com
  95. http://bit.ly/hackernoonlatestt
  96. https://hackernoon.com/trending
  97. https://hackernoon.com/tagged/machine-learning?source=post
  98. https://hackernoon.com/tagged/artificial-intelligence?source=post
  99. https://hackernoon.com/tagged/nlp?source=post
 100. https://hackernoon.com/tagged/language?source=post
 101. https://hackernoon.com/tagged/ai-if-you-suck-at-math?source=post
 102. https://hackernoon.com/@dan.jeffries
 103. https://hackernoon.com/?source=footer_card
 104. https://hackernoon.com/?source=footer_card
 105. https://hackernoon.com/
 106. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
 108. https://hackernoon.com/@dan.jeffries?source=post_header_lockup
 109. https://medium.com/p/f3819a689386/share/twitter
 110. https://medium.com/p/f3819a689386/share/facebook
 111. https://hackernoon.com/@dan.jeffries?source=footer_card
 112. https://medium.com/p/f3819a689386/share/twitter
 113. https://medium.com/p/f3819a689386/share/facebook
