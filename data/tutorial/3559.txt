   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

learning ai if you suck at math         p4         tensors illustrated (with cats!)

   go to the profile of daniel jeffries
   [14]daniel jeffries (button) blockedunblock (button) followfollowing
   feb 15, 2017
   [1*oou2ldb3xrhnbvwcx3hhng.jpeg]

   welcome to part four of learning ai if you suck at math. if you missed
   parts [15]1, [16]2, [17]3, [18]5, [19]6 and [20]7 be sure to check them
   out.

   maybe you   ve downloaded [21]tensorflow and you   re ready to get started
   with some deep learning?

   but then you wonder: what the hell is a tensor?

   perhaps you looked it up on [22]wikipedia and now you   re more confused
   than ever. maybe you found this [23]nasa tutorial and still have no
   idea what it   s talking about?

   the problem is most guides talk about tensors as if you already
   understand all the terms they   re using to describe the math.

   have no fear!

   i hated math as a kid, so if i can figure it out, you can too! we just
   have to explain everything in simpler terms.

   so what is a tensor and why does it flow?

tensors = containers

   a tensor is the basic building block of modern machine learning.

   at its core it   s a data container. mostly it contains numbers.
   sometimes it even contains strings, but that   s rare.

   so think of it as a bucket of numbers.

   there are multiple sizes of tensors. let   s go through the most basic
   ones that you   ll run across in deep learning, which will be between 0
   and 5 dimensions.

   we can visualize the various types of tensors like this (cats come
   later!):
   [1*_d5zvufds38wkhk9rk32hq.jpeg]

0d tensors/scalars

   every number that goes into a tensor/container bucket is called a
      scalar.   

   a scalar is a single number.

   why don   t they just call it a number you ask?

   i don   t know. maybe math peeps just like to sound cool? scalar does
   sound cooler than number.

   in fact you can have a single number tensor, which we call a 0d tensor,
   aka a tensor with 0 dimensions. it   s nothing more than a bucket with a
   one number in it. imagine a bucket with a single drop of water and you
   have a 0d tensor.

   in this tutorial we   ll use python, keras and tensorflow, as well as the
   python library numpy. we set all of that up in [24]my last tutorial,
   learning ai if you suck at math (laiysam)         part 3, so be sure to check
   that out if you want to get your deep learning workstation running
   fast.

   in python, these tensors are typically stored in a numpy arrays.
   [25]numpy is a scientific library for manipulating numbers that is used
   by pretty much every ai framework on the planet.
import numpy
x = np.array(5)
print(x)

   our output is:
5

   on [26]kaggle (the data science competition site) you will often see
   jupyter notebooks (also installed in laiysam -part 3) that talk about
   turning data into a numpy arrays. jupyter notebooks are essentially a
   markup document with working code embedded. think of it as an
   explanation and program rolled into one.

   why the heck would we want to turn data into a numpy array?

   simple. because we need to transform any input of data, be that strings
   of text, images, stock prices, or video into a universal standard that
   we can work with easily.

   in this case we transform that data into buckets of numbers so we can
   manipulate them with tensorflow.

   it   s nothing more than organizing data into a usable format. in web
   programming you might represent via xml, so you can define its features
   and manipulate it quickly. same thing. in deep learning we use tensor
   buckets as our basic lego block.

1d tensors/vectors

   if you   re a programmer, you already know about something similar to a
   1d tensor: an array.

   every programming language has arrays, which are nothing but a string
   of data chunks in a single row or column. in deep learning this is
   called a 1d tensor. tensors are defined by how many axes they have in
   total. a 1d tensor has exactly one axis.

   a 1d tensor is called a    vector.   

   we can visualize a vector as a single column or row of numbers.
   [1*cvygahw_mtpp7f_xpg5teg.jpeg]

   if we wanted to see this in numpy we could do the following:
x = np.array([1,2,3,4])
print(x)

   our output is:
array([1,2,3,4])

   we can also visualize how many axes a tensor has by using numpy   s ndim
   function. let   s try it with a 1d tensor.
x.ndim

   our output is:
1

2d tensors

   you probably already know about another kind of tensor: a matrix.

   a 2d tensor is called a matrix.
   [1*yyhzdv98sl4vy1ljmvnrjg.jpeg]

   no, not the movie with keanu reeves. think of an excel sheet.

   we can visualize this as a grid of numbers with rows and columns.

   those columns and rows represent two axes. a matrix is a 2d tensor,
   meaning it is two dimensional, aka a tensor with 2 axes.

   in numpy we would represent that as:
x = np.array([[5,10,15,30,25],
              [20,30,65,70,90],
              [7,80,95,20,30]])

   we can store characteristics of people in a 2d tensor. for example, a
   typical mailing list would fit in here.

   let   s say we have 10,000 people. we also have the following features or
   characteristics about each person:
     * first name
     * last name
     * street address
     * city
     * state
     * country
     * zip

   that means we seven characteristics for each of our ten thousand
   people.

   a tensor has a    shape.    the shape is a bucket that fits our data
   perfectly and defines the maximum size of our tensor. we can fit all
   the data about our people into a 2d tensor that is (10000,7).

   you might be tempted to say it has 10,000 columns and 7 rows.

   don   t.

   a tensor can be transformed or manipulated so that columns become rows
   and vice versa.

3d tensors

   this is where tensors really start to get useful. often we have to
   store a number of examples of 2d tensors in their own bucket, which
   gives us a 3d tensor.

   in numpy we could represent it as follows:
x = np.array([[[5,10,15,30,25],
               [20,30,65,70,90],
               [7,80,95,20,30]]
               [[3,0,5,0,45],
               [12,-2,6,7,90],
               [18,-9,95,120,30]]
               [[17,13,25,30,15],
               [23,36,9,7,80],
               [1,-7,-5,22,3]]])

   a 3d tensor has, you guessed it, 3 axes. we can see that like so:
x.ndim

   our output is:
3

   so let   s take our mailing list above. now say we have 10 mailing lists.
   we would store our 2d tensor in another bucket, creating a 3d tensor.
   it   s shape would look like this:
(number_of_mailing_lists, number_of_people, number_of_characteristics_per_person
)
(10,10000,7)

   [1*vyfrbhzlssu9gan24owxzq.jpeg]

   you might have already guessed it but a 3d tensor is a cube of numbers!

   we can keep stacking cubes together to create bigger and bigger tensors
   to encode different types of data aka 4d tensors, 5d tensors and so on
   up to n. n is used by math peeps to define an unknown number of
   additional units in a set continuing into the future. it could be 5, 10
   or a zillion.

   actually, a 3d tensor might be better visualized as a layer of grids,
   which looks something like the graphic below:
   [1*d7q997pf9vpnwc6zfhuszg.jpeg]

common data stored in tensors

   here are some common types of datasets that we store in various types
   of tensors:
     * 3d = time series
     * 4d = images
     * 5d = videos

   in almost every one of these tensors the common thread will be sample
   size. sample size is the number of things in the set. that could be the
   number of images, the number of videos, the number of documents, or the
   number of tweets.

   typically, the actual data will be one less the sample_size:
rest_of_dimensions - sample_size = actual_dimensions_of_data

   think of the various dimensions in the shape as fields. we are looking
   for the minimum number of fields that describe the data.

   so even though a 4d tensor typically stores images, that   s because
   sample size takes up the 4th field in the tensor.

   for example, an image is really represented by three fields, like this:
(width, height, color_depth) = 3d

   but we don   t usually work with a single image or document in machine
   learning. we have a set. we might have 10,000 images of tulips, which
   means we have a 4d tensor, like this:
(sample_size, width, height, color_depth) = 4d

   let   s look at multiple examples of various tensors as storage buckets.

time series data

   3d tensors are very effective for time series data.

medical scans

   we can encode an electroencephalogram eeg signal from the brain as a 3d
   tensor, because it can be encapsulated as 3 parameters:
(time, frequency, channel)

   the transformation would look like this:
   [1*ggsbehgnh6oroeqpvjrfbq.jpeg]

   now if we had multiple patients with eeg scans, that would become a 4d
   tensor, like this:
(sample_size, time, frequency, channel)

stock prices

   stock prices have a high, a low and a final price every minute. the new
   york stock exchange is open from 9:30 am to 4 pm. that   s 6 1/2 hours.
   there are 60 minutes in an hour so 6.5 x 60 = 390 minutes. these are
   typically represented by a candle stick graph.
   [1*-bnzwcilgqqgkzkccnhgpq.png]

   we would store the high, low and final stock price for every minute in
   a 2d tensor of (390,3). if we captured a typical week of trading (five
   days), we would have a 3d tensor with the shape:
(week_of_data, minutes, high_low_price)

   that would look like this:
(5,390,3)

   if we had a 10 different stocks, with one week of data each, we would
   have a 4d tensor with the following shape:
(10,5,390,3)

   let   s now pretend that we had a mutual fund, which is a collection of
   stocks, which is represented by our 4d tensor. perhaps we also have a
   collection of 25 mutual funds representing our portfolio, so now we
   have a collection of 4d tensors, which means we have a 5d tensor of
   shape:
(25,10,5,390,3)

text data

   we can store text data in a 3d tensor too. let   s take a look at tweets.

   tweets are 140 characters. twitter uses the utf-8 standard, which
   allows for millions of types of characters, but we are realistically
   only interested in the first 128 characters, as they are the same as
   basic ascii. a single tweet could be encapsulated as a 2d vector of
   shape (140,128).

   if we downloaded 1 million donald trump tweets ( i think he tweeted
   that much last week alone) we would store that as 3d tensor of shape:
(number_of_tweets_captured, tweet, character)

   that means our donald trump tweet collection would look like this:
(1000000,140,128)

images

   4d tensors are great at storing a series of images like jpegs. as we
   noted earlier, an image is stored with three parameters:
     * height
     * width
     * color depth

   the image is a 3d tensor, but the set of images makes it 4d. remember
   that fourth field is for sample_size.

   the famous mnist data set is a series of handwritten numbers that stood
   as a challenge for many data scientists for decades, but are now
   considered a solved problem, with machines able to achieve 99% and
   higher accuracy. still, the data set remains a good way to benchmark
   new machine learning applications, or just to try things out for
   yourself.
   [1*7hmsjoabtcrzwmvob3fjla.png]

   keras even allows us to automatically import the mnist data set with
   the following command:
from keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

   the data set is split into two buckets:
     * training set
     * test set

   each of the images in the sets has a label. this label gives the image
   the correct identification, such as the number 3 or 7 or 9, which was
   added by hand by a human being.

   the training set is used to teach a neural net and the test set
   contains the data the network tries to categorize after learning.

   the mnist images are gray scale, which means they could be encoded as a
   2d tensor, however all images are traditionally encoded as 3d tensors,
   with the third axis being a representation of color depth.

   there are 60,000 images in the mnist dataset. they are 28 pixels wide x
   28 pixels high. they have a color depth of 1, which represents gray
   scale.

   tensorflow stores image data like this:
(sample_size, height, width, color_depth).

   so we could say the 4d tensor for the mnist dataset has a shape of:
(60000,28,28,1)

color images

   color photos can have different color depths, depending on their
   resolution and encoding. a typical jpg image would use rgb and so it
   would have a color depth of 3, one each for each red, green, blue.

   this is a picture of my awesome cat dove. it   s a 750 pixel x 750 pixel
   image. (actually it   s 751 x 750 because i cut it wrong in photoshop,
   but we   ll pretend it is 750 x 750). that means we have a 3d tensor with
   the following characteristics:
(750,750,3)

   [1*jt34jy1zyq8dxxmzrtjgpw.jpeg]
   my beautiful cat dove (750 x 750 pixels)

   hence my dove would get reduced to a series of cold equations that
   would look like this as it    transformed    or    flowed.   
   [1*coyfskpcbvpillev1wi64g.jpeg]

   then let   s say we had a bunch of images of different types of cats,
   (though none will be as beautiful as dove). perhaps we have 100,000
   not-dove cats that were 750 pixels high by 750 pixels wide. we would
   define that set of data to keras as a 4d tensor of shape:
(10000,750,750,3)

   [1*8me6hds6ln3sibfodnjo6a.jpeg]

5d tensors

   a 5d tensor can store video data. in tensorflow video data is encoded
   as:
sample_size, frames, width, height, color_depth)

   if we took a five minute video (60 seconds x 5 = 300), at 1080p hd,
   which is 1920 pixels x 1080 pixels, at 15 sampled frames per second
   (which gives us 300 seconds x 15 = 4500), with a color depth of 3, we
   would store that a 4d tensor that looks like this:
(4500,1920,1080,3)

   the fifth field in the tensor comes into play when we have multiple
   videos in our video set. so if we had 10 videos just like that top one,
   we would have a 5d tensor of shape:
(10,4500,1920,1080,3)

   actually this example is totally insane.

   the size of the tensor would be absolutely ridiculous, over a terabyte.
   but let   s stick with it for a moment as there   s a point to doing it.
   know that in the real world, we would want to down-sample the video as
   much as possible to make it more realistic to deal with or we would be
   training this model until the end of time.

   the number of values in this 5d tensor would be:
10 x 4500 x 1920 x 1080 x 3 = 279,936,000,000

   keras allows us to store things as floating point numbers with 32 bits
   or 64 bits with a data value call (dtype):
float32
float64

   each of these values would be stored as a 32 bit number, which means
   that we multiply the total number of values by 32 to transform it into
   bits and then convert it to terabytes.
279,936,000,000 x 32 = 8,957,952,000,000

   [1*pfyvb-ihw1lldecl_rmerq.jpeg]

   i don   t even think the values would fit in a float32 (i   ll let someone
   else do the math on that), so get down-sampling my friend!

   actually, i used this last insane example for a reason.

   you just got your first lesson in [27]pre-processing and
   [28]data-reduction.

   you can   t just hurl data at an ai model with no work on your part. you
   have to massage and shrink the data to make it easier to work with
   efficiently.

   reduce the resolution, drop unneeded data (aka deduping), limit the
   number of frames you use, etc, etc. that is the work of a data
   scientists.

   if you can   t munge the data, you can   t do anything useful with it.

conclusion

   there you have it. now you have a much better understanding of tensors
   and the types of data that fit in them.

   in the next post we   ll learn how to do various transformations on the
   tensors, also known as math.

   in other words, we   ll make the tensors    flow.   

   ############################################

   be sure to check out the rest of this ongoing series. feel free to
   follow me because you want to be the first the read the latest articles
   as soon as they hit the press.

   [29]learning ai if you suck at math         part 1         this article guides you
   through the essential books to read if you were never a math fan but
   you   re learning it as an adult.

   [30]learning ai if you suck at math         part 2         practical
   projects         this article guides you through getting started with your
   first projects.

   [31]learning ai if you suck at math         part 3         building an ai dream
   machine         this article guides you through getting a powerful deep
   learning machine setup and installed with all the latest and greatest
   frameworks.

   [32]learning ai if you suck at math         part 4         tensors illustrated
   (with cats!)         this one answers the ancient mystery: what the hell is a
   tensor?

   [33]learning ai if you suck at math         part 5         deep learning and
   convolutional neural nets in plain english         here we create our first
   python program and explore the inner workings of neural networks!

   [34]learning ai if you suck at math         part 6         math notation made
   easy         still struggling to understand those funny little symbols? let   s
   change that now!

   [35]learning ai if you suck at math         part 7         the magic of natural
   language processing         understand how google and siri understand what
   you   re mumbling.

   ############################################

   if you love my work please [36]do me the honor of visiting my patreon
   page because that   s how we change the future together. help me
   disconnect from the matrix and i   ll repay your generosity a hundred
   fold by focusing all my time and energy on writing, research and
   delivering amazing content for you and world.

   ###########################################

   if you enjoyed this tutorial, i   d love it if you could clap it up to
   recommend it to others. after that please feel free email the article
   off to a friend! thanks much.

   ###########################################
   [1*aotvcrh8vp0kdfi3-hmthg.png]

   a bit about me: i   m an author, engineer and serial entrepreneur. during
   the last two decades, i   ve covered a broad range of tech from linux to
   virtualization and containers.

   [37]you might like a copy of my first novel, the scorpion game, because
   it   s free. readers have called it    the first serious competition to
   neuromancer    and    like a double shot of fine whiskey after drinking
   watered down beer for weeks.   

   you can also [38]join my private facebook group, the nanopunk posthuman
   assassins, where we discuss all things tech, sci-fi, fantasy and more.

   ############################################

   i occasionally make coin from the links in my articles but i only
   recommend things that i own, use and love. check my [39]full policy
   here.

   ############################################

   thanks for reading!
   [40][1*0hqoaabq7xgpt-oyngiubg.png]
   [41][1*vgw1jka6hgnvwztsfmlnpg.png]
   [42][1*gkbpq1ruui0fvk2um_i4tq.png]

     [43]hacker noon is how hackers start their afternoons. we   re a part
     of the [44]@ami family. we are now [45]accepting submissions and
     happy to [46]discuss advertising & sponsorship opportunities.

     if you enjoyed this story, we recommend reading our [47]latest tech
     stories and [48]trending tech stories. until next time, don   t take
     the realities of the world for granted!

   [1*35tcjopcvq6lbb3i6wegqw.jpeg]

     * [49]machine learning
     * [50]deep learning
     * [51]artificial intelligence
     * [52]learning
     * [53]ai if you suck at math

   (button)
   (button)
   (button) 2.2k claps
   (button) (button) (button) 12 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of daniel jeffries

[54]daniel jeffries

   medium member since mar 2017

   i am an author, futurist, systems architect, and thinker.

     (button) follow
   [55]hacker noon

[56]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 2.2k
     * (button)
     *
     *

   [57]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [58]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/27f0002c9b32
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_ykbvfn3powue---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@dan.jeffries
  15. https://hackernoon.com/learning-ai-if-you-suck-at-math-8bdfb4b79037#.qv49ic2ok
  16. https://hackernoon.com/learning-ai-if-you-suck-at-math-part-two-practical-projects-47d7a1e4e21f#.p1x8tjxyx
  17. https://hackernoon.com/learning-ai-if-you-suck-at-math-p3-building-an-ai-dream-machine-or-budget-friendly-special-d5a3023140ef#.wktve8ouw
  18. https://hackernoon.com/learning-ai-if-you-suck-at-math-p5-deep-learning-and-convolutional-neural-nets-in-plain-english-cda79679bbe3#.7tfyjjvdd
  19. https://hackernoon.com/learning-ai-if-you-suck-at-math-p6-math-notation-made-easy-1277d76a1fe5#.fra2px108
  20. https://hackernoon.com/learning-ai-if-you-suck-at-math-p7-the-magic-of-natural-language-processing-f3819a689386
  21. https://www.tensorflow.org/tutorials/mnist/beginners/
  22. https://en.wikipedia.org/wiki/tensor
  23. https://www.grc.nasa.gov/www/k-12/numbers/math/documents/tensors_tm2002211716.pdf
  24. https://hackernoon.com/learning-ai-if-you-suck-at-math-p3-building-an-ai-dream-machine-or-budget-friendly-special-d5a3023140ef#.6frka033t
  25. http://www.numpy.org/
  26. https://www.kaggle.com/
  27. https://en.wikipedia.org/wiki/data_pre-processing
  28. https://en.wikipedia.org/wiki/data_reduction
  29. https://hackernoon.com/learning-ai-if-you-suck-at-math-8bdfb4b79037#.ng7ggn5d9
  30. https://hackernoon.com/learning-ai-if-you-suck-at-math-part-two-practical-projects-47d7a1e4e21f#.yo1o1ar5h
  31. https://hackernoon.com/learning-ai-if-you-suck-at-math-p3-building-an-ai-dream-machine-or-budget-friendly-special-d5a3023140ef#.6frka033t
  32. https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32#.2jpelkuhd
  33. https://hackernoon.com/learning-ai-if-you-suck-at-math-p5-deep-learning-and-convolutional-neural-nets-in-plain-english-cda79679bbe3#.xjah79lsd
  34. https://hackernoon.com/learning-ai-if-you-suck-at-math-p6-math-notation-made-easy-1277d76a1fe5
  35. https://hackernoon.com/learning-ai-if-you-suck-at-math-p7-the-magic-of-natural-language-processing-f3819a689386
  36. https://www.patreon.com/danjeffries
  37. http://meuploads.com/join-my-readers-group/
  38. https://www.facebook.com/groups/1736763229929363/
  39. http://meuploads.com/disclosure/
  40. http://bit.ly/hackernoonfb
  41. https://goo.gl/k7xybx
  42. https://goo.gl/4ofytp
  43. http://bit.ly/hackernoon
  44. http://bit.ly/atamiatami
  45. http://bit.ly/hackernoonsubmission
  46. mailto:partners@amipublications.com
  47. http://bit.ly/hackernoonlatestt
  48. https://hackernoon.com/trending
  49. https://hackernoon.com/tagged/machine-learning?source=post
  50. https://hackernoon.com/tagged/deep-learning?source=post
  51. https://hackernoon.com/tagged/artificial-intelligence?source=post
  52. https://hackernoon.com/tagged/learning?source=post
  53. https://hackernoon.com/tagged/ai-if-you-suck-at-math?source=post
  54. https://hackernoon.com/@dan.jeffries
  55. https://hackernoon.com/?source=footer_card
  56. https://hackernoon.com/?source=footer_card
  57. https://hackernoon.com/
  58. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  60. https://hackernoon.com/@dan.jeffries?source=post_header_lockup
  61. https://medium.com/p/27f0002c9b32/share/twitter
  62. https://medium.com/p/27f0002c9b32/share/facebook
  63. https://hackernoon.com/@dan.jeffries?source=footer_card
  64. https://medium.com/p/27f0002c9b32/share/twitter
  65. https://medium.com/p/27f0002c9b32/share/facebook
