   #[1]github [2]recent commits to automatic_speech_recognition:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]151
     * [35]star [36]2,411
     * [37]fork [38]469

[39]zzw922cn/[40]automatic_speech_recognition

   [41]code [42]issues 63 [43]pull requests 2 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   end-to-end automatic id103 for madarian and english in
   tensorflow
   [47]automatic-speech-recognition [48]tensorflow [49]timit-dataset
   [50]feature-vector [51]phonemes [52]data-preprocessing [53]id56
   [54]audio [55]deep-learning [56]lstm [57]end-to-end [58]id98
   [59]id56-encoder-decoder [60]evaluation [61]paper [62]speech-recognition
   [63]layer-id172 [64]chinese-speech-recognition
     * [65]248 commits
     * [66]2 branches
     * [67]2 releases
     * [68]fetching contributors
     * [69]mit

    1. [70]python 99.0%
    2. [71]shell 1.0%

   (button) python shell
   branch: master (button) new pull request
   [72]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/z
   [73]download zip

downloading...

   want to be notified of new releases in
   zzw922cn/automatic_speech_recognition?
   [74]sign in [75]sign up

launching github desktop...

   if nothing happens, [76]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [77]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [78]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [79]download the github extension for visual studio
   and try again.

   (button) go back
   [80]@zzw922cn
   [81]zzw922cn [82]udpates
   latest commit [83]84c7878 apr 3, 2018
   [84]permalink
   type         name         latest commit message commit time
        failed to load latest commit information.
        [85]speechvalley
        [86].gitignore
        [87]license
        [88]per.png
        [89]readme.md
        [90]requirements.txt
        [91]result.txt
        [92]setup.py

readme.md

automatic-speech-recognition

   end-to-end automatic id103 system implemented in
   tensorflow.

recent updates

     * [x] support tensorflow r1.0 (2017-02-24)
     * [x] support dropout for dynamic id56 (2017-03-11)
     * [x] support running in shell file (2017-03-11)
     * [x] support evaluation every several training epoches automatically
       (2017-03-11)
     * [x] fix bugs for character-level automatic id103
       (2017-03-14)
     * [x] improve some function apis for reusable (2017-03-14)
     * [x] add scaling for id174 (2017-03-15)
     * [x] add reusable support for librispeech training (2017-03-15)
     * [x] add simple id165 model for random generation or statistical
       use (2017-03-23)
     * [x] improve some code for pre-processing and training (2017-03-23)
     * [x] replace tabs with blanks and add nist2wav converter script
       (2017-04-20)
     * [x] add some data preparation code (2017-05-01)
     * [x] add wsj corpus standard preprocessing by s5 recipe (2017-05-05)
     * [x] restructuring of the project. updated train.py for usage
       convinience (2017-05-06)
     * [x] finish feature module for timit, libri, wsj, support training
       for librispeech (2017-05-14)
     * [x] remove some unnecessary codes (2017-07-22)
     * [x] add deepspeech2 implementation code (2017-07-23)
     * [x] fix some bugs (2017-08-06)
     * [x] add layer id172 id56 for efficiency (2017-08-06)
     * [x] add madarian id103 support (2017-08-06)
     * [x] add capsule network model (2017-12-12)
     * [x] release 1.0.0 version (2017-12-14)
     * [x] add id38 module (2017-12-25)

recommendation

   if you want to replace feed dict operation with tensorflow multi-thread
   and fifoqueue input pipeline, you can refer to my repo
   [93]tensorflow-input-pipeline for more example codes. my own practices
   prove that fifoqueue input pipeline would improve the training speed in
   some time.

   if you want to look the history of id103, i have collected
   the significant papers since 1981 in the asr field. you can read
   awesome paper list in my repo [94]awesome-speech-recognition-papers,
   all download links of papers are provided. i will update it every week
   to add new papers, including id103, id133 and
   language modelling. i hope that we won't miss any important papers in
   speech domain.

   all my public repos will be updated in future, thanks for your stars!

install and usage

   currently only python 3.5 is supported.

   this project depends on scikit.audiolab, for which you need to have
   [95]libsndfile installed in your system. clone the repository to your
   preferred directory and install using:
sudo pip3 install -r requirements.txt
sudo python3 setup.py install

   to use, simply run the following command:
python main/timit_train.py [-h] [--mode mode] [--keep [keep]] [--nokeep]
                      [--level level] [--model model] [--id56cell id56cell]
                      [--num_layer num_layer] [--activation activation]
                      [--optimizer optimizer] [--batch_size batch_size]
                      [--num_hidden num_hidden] [--num_feature num_feature]
                      [--num_classes num_classes] [--num_epochs num_epochs]
                      [--lr lr] [--dropout_prob dropout_prob]
                      [--grad_clip grad_clip] [--datadir datadir]
                      [--logdir logdir]

optional arguments:
  -h, --help            show this help message and exit
  --mode mode           set whether to train or test
  --keep [keep]         set whether to restore a model, when test mode, keep
                        should be set to true
  --nokeep
  --level level         set the task level, phn, cha, or id195, id195 will
                        be supported soon
  --model model         set the model to use, dbiid56, biid56, resnet..
  --id56cell id56cell     set the id56cell to use, id56, gru, lstm...
  --num_layer num_layer
                        set the layers for id56
  --activation activation
                        set the activation to use, sigmoid, tanh, relu, elu...
  --optimizer optimizer
                        set the optimizer to use, sgd, adam...
  --batch_size batch_size
                        set the batch size
  --num_hidden num_hidden
                        set the hidden size of id56 cell
  --num_feature num_feature
                        set the size of input feature
  --num_classes num_classes
                        set the number of output classes
  --num_epochs num_epochs
                        set the number of epochs
  --lr lr               set the learning rate
  --dropout_prob dropout_prob
                        set id203 of dropout
  --grad_clip grad_clip
                        set the threshold of gradient clipping
  --datadir datadir     set the data root directory
  --logdir logdir       set the log directory


   instead of configuration in command line, you can also set the
   arguments above in [96]timit_train.py in practice.

   besides, you can also run main/run.sh for both training and testing
   simultaneously! see [97]run_timit.sh for details.

performance

per based dynamic blstm on timit database, with casual tuning because time it
limited

   [98]image

libriid103 result without lm

   label:

   it was about noon when captain waverley entered the straggling village
   or rather haid113t of tully veolan close to which was situated the
   mansion of the proprietor

   prediction:

   it was about noon when captain wavraly entered the stragling bilagor of
   rather haid113nt of tulevallon close to which wi situated the mantion of
   the propriater

   label:

   the english it is evident had they not been previously assured of
   receiving the king would never have parted with so considerable a sum
   and while they weakened themselves by the same measure have
   strengthened a people with whom they must afterwards have so material
   an interest to discuss

   prediction:

   the onglish it is evident had they not being previously showed of
   receiving the king would never have parted with so considerable a some
   an quile they weakene themselves by the same measure haf streigth and
   de people with whom they must afterwards have so material and interest
   to discuss

   label:

   one who writes of such an era labours under a troublesome disadvantage

   prediction:

   one how rights of such an er a labours onder a troubles hom
   disadvantage

   label:

   then they started on again and two hours later came in sight of the
   house of doctor pipt

   prediction:

   then they started on again and two hours laytor came in sight of the
   house of doctor pipd

   label:

   what does he want

   prediction:

   whit daes he want

   label:

   there just in front

   prediction:

   there just infront

   label:

   under ordinary circumstances the abalone is tough and unpalatable but
   after the deft manipulation of herbert they are tender and make a fine
   dish either fried as chowder or a la newberg

   prediction:

   under ordinary circumstancesi the abl ony is tufgh and unpelitable but
   after the deftominiculation of hurbourt and they are tender and make a
   fine dish either fride as choder or alanuburg

   label:

   by degrees all his happiness all his brilliancy subsided into regret
   and uneasiness so that his limbs lost their power his arms hung heavily
   by his sides and his head drooped as though he was stupefied

   prediction:

   by degrees all his happiness ill his brilliancy subsited inter regret
   and aneasiness so that his limbs lost their power his arms hung heavily
   by his sides and his head druped as though he was stupified

   label:

   i am the one to go after walt if anyone has to i'll go down mister
   thomas

   prediction:

   i have the one to go after walt if ety wod hastu i'll go down mister
   thommas

   label:

   i had to read it over carefully as the text must be absolutely correct

   prediction:

   i had to readit over carefully as the tex must be absolutely correct

   label:

   with a shout the boys dashed pell mell to meet the pack train and
   falling in behind the slow moving burros urged them on with derisive
   shouts and sundry resounding slaps on the animals flanks

   prediction:

   with a shok the boy stash pale mele to meek the pecktrait ane falling
   in behind the slow lelicg burs ersh tlan with deressive shouts and
   sudery resounding sleps on the animal slankes

   label:

   i suppose though it's too early for them then came the explosion

   prediction:

   i suppouse gho waths two early for them then came the explosion

content

   this is a powerful library for automatic id103, it is
   implemented in tensorflow and support training with cpu/gpu. this
   library contains followings models you can choose to train your own
   model:
     * data pre-processing
     * acoustic modeling
          + id56
          + bid56
          + lstm
          + blstm
          + gru
          + bgru
          + dynamic id56
          + deep residual network
          + id195 with attention decoder
          + etc.
     * ctc decoding
     * evaluation(mapping some similar phonemes)
     * saving or restoring model
     * mini-batch training
     * training with gpu or cpu with tensorflow
     * keeping logging of epoch time and error rate in disk

implementation details

id174

timit corpus

   the original timit database contains 6300 utterances, but we find the
   'sa' audio files occurs many times, it will lead bad bias for our
   id103 system. therefore, we removed the all 'sa' files
   from the original dataset and attain the new timit dataset, which
   contains only 5040 utterances including 3696 standard training set and
   1344 test set.

   automatic id103 transcribes a raw audio file into
   character sequences; the preprocessing stage converts a raw audio file
   into feature vectors of several frames. we first split each audio file
   into 20ms hamming windows with an overlap of 10ms, and then calculate
   the 12 mel frequency ceptral coefficients, appending an energy variable
   to each frame. this results in a vector of length 13. we then calculate
   the delta coefficients and delta-delta coefficients, attaining a total
   of 39 coefficients for each frame. in other words, each audio file is
   split into frames using the hamming windows function, and each frame is
   extracted to a feature vector of length 39 (to attain a feature vector
   of different length, modify the settings in the file
   [99]timit_preprocess.py.

   in folder data/mfcc, each file is a feature matrix with size
   timelength*39 of one audio file; in folder data/label, each file is a
   label vector according to the mfcc file.

   if you want to set your own id174, you can edit
   [100]calcmfcc.py or [101]timit_preprocess.py.

   the original timit dataset contains 61 phonemes, we use 61 phonemes for
   training and evaluation, but when scoring, we mappd the 61 phonemes
   into 39 phonemes for better performance. we do this mapping according
   to the paper [102]speaker-independent phone recognition using hidden
   markov models. the mapping details are as follows:
   original phoneme(s)                           mapped phoneme
   iy                                                  iy
   ix, ih                                              ix
   eh                                                  eh
   ae                                                  ae
   ax, ah, ax-h                                        ax
   uw, ux                                              uw
   uh                                                  uh
   ao, aa                                              ao
   ey                                                  ey
   ay                                                  ay
   oy                                                  oy
   aw                                                  aw
   ow                                                  ow
   er, axr                                             er
   l, el                                               l
   r                                                   r
   w                                                   w
   y                                                   y
   m, em                                               m
   n, en, nx                                           n
   ng, eng                                             ng
   v                                                   v
   f                                                   f
   dh                                                  dh
   th                                                  th
   z                                                   z
   s                                                   s
   zh, sh                                              zh
   jh                                                  jh
   ch                                                  ch
   b                                                   b
   p                                                   p
   d                                                   d
   dx                                                  dx
   t                                                   t
   g                                                   g
   k                                                   k
   hh, hv                                              hh
   bcl, pcl, dcl, tcl, gcl, kcl, q, epi, pau, h#       h#

librispeech corpus

   librispeech is a corpus of approximately 1000 hours of 16khz read
   english speech. it can be downloaded from [103]here

   in order to preprocess librispeech data, download the dataset from the
   above mentioned link, extract it and run the following:
cd feature/libri
python libri_preprocess.py -h
usage: libri_preprocess [-h]
                        [-n {dev-clean,dev-other,test-clean,test-other,train-cle
an-100,train-clean-360,train-other-500}]
                        [-m {mfcc,fbank}] [--featlen featlen] [-s]
                        [-wl winlen] [-ws winstep]
                        path save

script to preprocess libri data

positional arguments:
  path                  directory of librispeech dataset
  save                  directory where preprocessed arrays are to be saved

optional arguments:
  -h, --help            show this help message and exit
  -n {dev-clean,dev-other,test-clean,test-other,train-clean-100,train-clean-360,
train-other-500}, --name {dev-clean,dev-other,test-clean,test-other,train-clean-
100,train-clean-360,train-other-500}
                        name of the dataset
  -m {mfcc,fbank}, --mode {mfcc,fbank}
                        mode
  --featlen featlen     features length
  -s, --id195         set this flag to use id195
  -wl winlen, --winlen winlen
                        specify the window length of feature
  -ws winstep, --winstep winstep
                        specify the window step length of feature

   the processed data will be saved in the "save" path.

   to train the model, run the following:
python main/libri_train.py -h
usage: libri_train.py [-h] [--task task] [--train_dataset train_dataset]
                      [--dev_dataset dev_dataset]
                      [--test_dataset test_dataset] [--mode mode]
                      [--keep [keep]] [--nokeep] [--level level]
                      [--model model] [--id56cell id56cell]
                      [--num_layer num_layer] [--activation activation]
                      [--optimizer optimizer] [--batch_size batch_size]
                      [--num_hidden num_hidden] [--num_feature num_feature]
                      [--num_classes num_classes] [--num_epochs num_epochs]
                      [--lr lr] [--dropout_prob dropout_prob]
                      [--grad_clip grad_clip] [--datadir datadir]
                      [--logdir logdir]

optional arguments:
  -h, --help            show this help message and exit
  --task task           set task name of this program
  --train_dataset train_dataset
                        set the training dataset
  --dev_dataset dev_dataset
                        set the development dataset
  --test_dataset test_dataset
                        set the test dataset
  --mode mode           set whether to train, dev or test
  --keep [keep]         set whether to restore a model, when test mode, keep
                        should be set to true
  --nokeep
  --level level         set the task level, phn, cha, or id195, id195 will
                        be supported soon
  --model model         set the model to use, dbiid56, biid56, resnet..
  --id56cell id56cell     set the id56cell to use, id56, gru, lstm...
  --num_layer num_layer
                        set the layers for id56
  --activation activation
                        set the activation to use, sigmoid, tanh, relu, elu...
  --optimizer optimizer
                        set the optimizer to use, sgd, adam...
  --batch_size batch_size
                        set the batch size
  --num_hidden num_hidden
                        set the hidden size of id56 cell
  --num_feature num_feature
                        set the size of input feature
  --num_classes num_classes
                        set the number of output classes
  --num_epochs num_epochs
                        set the number of epochs
  --lr lr               set the learning rate
  --dropout_prob dropout_prob
                        set id203 of dropout
  --grad_clip grad_clip
                        set the threshold of gradient clipping, -1 denotes no
                        clipping
  --datadir datadir     set the data root directory
  --logdir logdir       set the log directory

   where the "datadir" is the "save" path used in preprocess stage.

wall street journal corpus

   todo

core features

     * dynamic id56(gru, lstm)
     * residual network(deep id98)
     * ctc decoding
     * timit phoneme id153(per)

future work

     * [ ] release pretrained english asr model
     * [ ] add attention mechanism
     * [ ] add speaker verification
     * [ ] add tts

license

   mit

contact us

   if this program is helpful to you, please give us a star or fork to
   encourage us to keep updating. thank you! besides, any issues or pulls
   are appreciated.

   for any questions, welcome to send email to :[104]zzw922cn@gmail.com.

   collaborators:

   [105]zzw922cn

   [106]deepxuexi

   [107]hiteshpaul

   [108]xxxxyzt

     *    2019 github, inc.
     * [109]terms
     * [110]privacy
     * [111]security
     * [112]status
     * [113]help

     * [114]contact github
     * [115]pricing
     * [116]api
     * [117]training
     * [118]blog
     * [119]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [120]reload to refresh your
   session. you signed out in another tab or window. [121]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/zzw922cn/automatic_speech_recognition/commits/master.atom
   3. https://github.com/zzw922cn/automatic_speech_recognition#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/zzw922cn/automatic_speech_recognition
  32. https://github.com/join
  33. https://github.com/login?return_to=/zzw922cn/automatic_speech_recognition
  34. https://github.com/zzw922cn/automatic_speech_recognition/watchers
  35. https://github.com/login?return_to=/zzw922cn/automatic_speech_recognition
  36. https://github.com/zzw922cn/automatic_speech_recognition/stargazers
  37. https://github.com/login?return_to=/zzw922cn/automatic_speech_recognition
  38. https://github.com/zzw922cn/automatic_speech_recognition/network/members
  39. https://github.com/zzw922cn
  40. https://github.com/zzw922cn/automatic_speech_recognition
  41. https://github.com/zzw922cn/automatic_speech_recognition
  42. https://github.com/zzw922cn/automatic_speech_recognition/issues
  43. https://github.com/zzw922cn/automatic_speech_recognition/pulls
  44. https://github.com/zzw922cn/automatic_speech_recognition/projects
  45. https://github.com/zzw922cn/automatic_speech_recognition/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/automatic-speech-recognition
  48. https://github.com/topics/tensorflow
  49. https://github.com/topics/timit-dataset
  50. https://github.com/topics/feature-vector
  51. https://github.com/topics/phonemes
  52. https://github.com/topics/data-preprocessing
  53. https://github.com/topics/id56
  54. https://github.com/topics/audio
  55. https://github.com/topics/deep-learning
  56. https://github.com/topics/lstm
  57. https://github.com/topics/end-to-end
  58. https://github.com/topics/id98
  59. https://github.com/topics/id56-encoder-decoder
  60. https://github.com/topics/evaluation
  61. https://github.com/topics/paper
  62. https://github.com/topics/speech-recognition
  63. https://github.com/topics/layer-id172
  64. https://github.com/topics/chinese-speech-recognition
  65. https://github.com/zzw922cn/automatic_speech_recognition/commits/master
  66. https://github.com/zzw922cn/automatic_speech_recognition/branches
  67. https://github.com/zzw922cn/automatic_speech_recognition/releases
  68. https://github.com/zzw922cn/automatic_speech_recognition/graphs/contributors
  69. https://github.com/zzw922cn/automatic_speech_recognition/blob/master/license
  70. https://github.com/zzw922cn/automatic_speech_recognition/search?l=python
  71. https://github.com/zzw922cn/automatic_speech_recognition/search?l=shell
  72. https://github.com/zzw922cn/automatic_speech_recognition/find/master
  73. https://github.com/zzw922cn/automatic_speech_recognition/archive/master.zip
  74. https://github.com/login?return_to=https://github.com/zzw922cn/automatic_speech_recognition
  75. https://github.com/join?return_to=/zzw922cn/automatic_speech_recognition
  76. https://desktop.github.com/
  77. https://desktop.github.com/
  78. https://developer.apple.com/xcode/
  79. https://visualstudio.github.com/
  80. https://github.com/zzw922cn
  81. https://github.com/zzw922cn/automatic_speech_recognition/commits?author=zzw922cn
  82. https://github.com/zzw922cn/automatic_speech_recognition/commit/84c7878eb83f12faa6cf6767ee5962f8414ee812
  83. https://github.com/zzw922cn/automatic_speech_recognition/commit/84c7878eb83f12faa6cf6767ee5962f8414ee812
  84. https://github.com/zzw922cn/automatic_speech_recognition/tree/84c7878eb83f12faa6cf6767ee5962f8414ee812
  85. https://github.com/zzw922cn/automatic_speech_recognition/tree/master/speechvalley
  86. https://github.com/zzw922cn/automatic_speech_recognition/blob/master/.gitignore
  87. https://github.com/zzw922cn/automatic_speech_recognition/blob/master/license
  88. https://github.com/zzw922cn/automatic_speech_recognition/blob/master/per.png
  89. https://github.com/zzw922cn/automatic_speech_recognition/blob/master/readme.md
  90. https://github.com/zzw922cn/automatic_speech_recognition/blob/master/requirements.txt
  91. https://github.com/zzw922cn/automatic_speech_recognition/blob/master/result.txt
  92. https://github.com/zzw922cn/automatic_speech_recognition/blob/master/setup.py
  93. https://github.com/zzw922cn/tensorflow-input-pipeline
  94. https://github.com/zzw922cn/awesome-speech-recognition-papers
  95. http://www.mega-nerd.com/libsndfile/
  96. https://github.com/zzw922cn/automatic_speech_recognition/blob/master/main/timit_train.py
  97. https://github.com/zzw922cn/automatic_speech_recognition/blob/master/main/run_timit.sh
  98. https://github.com/zzw922cn/automatic_speech_recognition/blob/master/per.png
  99. https://github.com/zzw922cn/automatic-speech-recognition/blob/master/src/feature/timit/timit_preprocess.py
 100. https://github.com/zzw922cn/automatic-speech-recognition/blob/master/src/feature/core/calcmfcc.py
 101. https://github.com/zzw922cn/automatic-speech-recognition/blob/master/src/feature/timit/timit_preprocess.py
 102. http://repository.cmu.edu/cgi/viewcontent.cgi?article=2768&context=compsci
 103. http://www.openslr.org/12/
 104. mailto:zzw922cn@gmail.com
 105. https://github.com/zzw922cn
 106. https://github.com/deepxuexi
 107. https://github.com/hiteshpaul
 108. https://github.com/xxxxyzt
 109. https://github.com/site/terms
 110. https://github.com/site/privacy
 111. https://github.com/security
 112. https://githubstatus.com/
 113. https://help.github.com/
 114. https://github.com/contact
 115. https://github.com/pricing
 116. https://developer.github.com/
 117. https://training.github.com/
 118. https://github.blog/
 119. https://github.com/about
 120. https://github.com/zzw922cn/automatic_speech_recognition
 121. https://github.com/zzw922cn/automatic_speech_recognition

   hidden links:
 123. https://github.com/
 124. https://github.com/zzw922cn/automatic_speech_recognition
 125. https://github.com/zzw922cn/automatic_speech_recognition
 126. https://github.com/zzw922cn/automatic_speech_recognition
 127. https://help.github.com/articles/which-remote-url-should-i-use
 128. https://github.com/zzw922cn/automatic_speech_recognition#automatic-speech-recognition
 129. https://github.com/zzw922cn/automatic_speech_recognition#recent-updates
 130. https://github.com/zzw922cn/automatic_speech_recognition#recommendation
 131. https://github.com/zzw922cn/automatic_speech_recognition#install-and-usage
 132. https://github.com/zzw922cn/automatic_speech_recognition#performance
 133. https://github.com/zzw922cn/automatic_speech_recognition#per-based-dynamic-blstm-on-timit-database-with-casual-tuning-because-time-it-limited
 134. https://github.com/zzw922cn/automatic_speech_recognition#librispeech-recognition-result-without-lm
 135. https://github.com/zzw922cn/automatic_speech_recognition#content
 136. https://github.com/zzw922cn/automatic_speech_recognition#implementation-details
 137. https://github.com/zzw922cn/automatic_speech_recognition#data-preprocessing
 138. https://github.com/zzw922cn/automatic_speech_recognition#timit-corpus
 139. https://github.com/zzw922cn/automatic_speech_recognition#librispeech-corpus
 140. https://github.com/zzw922cn/automatic_speech_recognition#wall-street-journal-corpus
 141. https://github.com/zzw922cn/automatic_speech_recognition#core-features
 142. https://github.com/zzw922cn/automatic_speech_recognition#future-work
 143. https://github.com/zzw922cn/automatic_speech_recognition#license
 144. https://github.com/zzw922cn/automatic_speech_recognition#contact-us
 145. https://github.com/
