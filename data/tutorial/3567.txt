   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]a year of artificial intelligence
     * [9]algorithms
     * [10]today i learned
     * [11]case studies
     * [12]philosophical
     * [13]meta
     __________________________________________________________________

   [1*pkv3dl-enonlnjxzqhlwkq.jpeg]

rohan & lenny #1: neural networks & the id26 algorithm, explained

do you know the chain rule? then you know the neural network id26
algorithm!

   [14]go to the profile of rohan kapur
   [15]rohan kapur (button) blockedunblock (button) followfollowing
   mar 3, 2016
     __________________________________________________________________

     this is the first group ([16]lenny and [17]rohan) entry in our
     [18]journey to extend our knowledge of artificial intelligence in
     the year of 2016. learn more about our motives in this
     [19]introduction post.
     __________________________________________________________________

   in rohan   s last post, he talked about evaluating and plugging holes in
   his knowledge of machine learning thus far. the id26
   algorithm         the process of training a neural network         was a glaring
   one for both of us in particular. together, we embarked on mastering
   backprop through some great online lectures from professors at mit &
   stanford. after attempting a few programming implementations and hand
   solutions, we felt equipped to write an article for ayoai         together.

   today, we   ll do our best to explain id26 and neural networks
   from the beginning. if you have an elementary understanding of
   id128 and perhaps an intuition of what machine learning
   is, we hope you come out of this blog post with an (acute, but existent
   nonetheless) understanding of neural networks and how to train them.
   let us know if we succeeded!

introduction to neural networks

   let   s start off with a quick introduction to the concept of neural
   networks. fundamentally, neural networks are nothing more than really
   good function approximators         you give a trained network an input
   vector, it performs a series of operations, and it produces an output
   vector. to train our network to estimate an unknown function, we give
   it a collection of data points         which we denote the    training
   set            that the network will learn from and generalize on to make
   future id136s.
   [0*m0tn6w0ipc3ro7mo.]
   this is what a neural network looks like. each circle is a neuron, and
   the arrows are connections between neurons in consecutive layers.

   neural networks are structured as a series of layers, each composed of
   one or more neurons (as depicted above). each neuron produces an
   output, or activation, based on the outputs of the previous layer and a
   set of weights.
   [1*osspmzzwl9mk_wjgqdcyla.png]
   this is how each neuron computes it   s own activation. it computes a
   weighted sum of the outputs of the previous layer (which, for the
   inputs, we   ll call x), and applies an activation function (more on that
   later), before forwarding it on to the next layer. each neuron in a
   layer has its own set of weights         so while each neuron in a layer is
   looking at the same inputs, their outputs will all be different.

   when using a neural network to approximate a function, the data is
   forwarded through the network layer-by-layer until it reaches the final
   layer. the final layer   s activations are the predictions that the
   network actually makes.

   all this probably seems kind of magical, but it actually works. the key
   is finding the right set of weights for all of the connections to make
   the right decisions (this happens in a process known as training)         and
   that   s what most of this post is going to be about.

   when we   re training the network, it   s often convenient to have some
   metric of how good or bad we   re doing; we call this metric the cost
   function. generally speaking, the cost function looks at the function
   the network has inferred and uses it to estimate values for the data
   points in our training set. the discrepancies between the outputs in
   the estimations and the training set data points are the principle
   values for our cost function. when training our network, the goal will
   be to get the value of this cost function as low as possible (we   ll see
   how to do that in just a bit, but for now, just focus on the intuition
   of what a cost function is and what it   s good for). generally speaking,
   the cost function should be more or less convex, like so:
   [0*7i-beovyjyvmboyv.]

   in reality, it   s impossible for any network or cost function to be
   truly convex. however, as we   ll soon see, local minima may not be a big
   deal, as long as there is still a general trend for us to follow to get
   to the bottom. also, notice that the cost function is parameterized by
   our network   s weights         we control our id168 by changing the
   weights.

   one last thing to keep in mind about the id168 is that it
   doesn   t just have to capture how correctly your network estimates         it
   can specify any objective that needs to be optimized. for example, you
   generally want to penalize larger weights, as they could lead to
   overfitting. if this is the case, simply adding a id173 term
   to your cost function that expresses how big your weights will mean
   that, in the process of training your network, it will look for a
   solution that has the best estimates possible while preventing
   overfitting.

   now, let   s take a look at how we can actually minimize the cost
   function during the training process to find a set of weights that work
   the best for our objective.

minimizing the cost function

   now that we   ve developed a metric for    scoring    our network (which
   we   ll denote as j(w)), we need to find the weights that will make that
   score as low as possible. if you think back to your pre-calculus days,
   your first instinct might be to set the derivative of the cost function
   to zero and solve, which would give us the locations of every
   minimum/maximum in the function. unfortunately, there are a few
   problems with this approach:
    1. we don   t have a simple equation for our cost function, so computing
       an expression for the derivative and solving it isn   t trivial.
    2. the function is many-dimensional (each weight gets its own
       dimension)         we need to find the points where all of those
       derivatives are zero. also not so trivial.
    3. there are lots of minimums and maximums throughout the function,
       and sorting out which one is the one you should be using can be
       computationally expensive.

   especially as the size of networks begins to scale up, solving for the
   weights directly becomes increasingly infeasible. instead, we look at a
   different class of algorithms, called iterative optimization
   algorithms, that progressively work their way towards the optimal
   solution.

   the most basic of these algorithms is id119. recall that our
   cost function will be essentially convex, and we want to get as close
   as possible to the global minimum. instead of solving for it
   analytically, id119 follows the derivatives to essentially
      roll    down the slope until it finds its way to the center.

   let   s take the example of a single-weight neural network, whose cost
   function is depicted below.
   [0*m0xaqitzpdqtcl86.]

   we start off by initializing our weight randomly, which puts us at the
   red dot on the diagram above. taking the derivative, we see the slope
   at this point is a pretty big positive number. we want to move closer
   to the center         so naturally, we should take a pretty big step in the
   opposite direction of the slope.
   [0*lf6cklbgitdwdkl-.]

   if we repeat the process enough, we soon find ourselves nearly at the
   bottom of our curve and much closer to the optimal weight configuration
   for our network.

   more formally, id119 looks something like this:
   [1*u0vohjim1f_be5msmjzv4q.png]
   this is the id119 update rule. it tells us how to update the
   weights of our network to get us closer to the minimum we   re
   looking for.

   let   s dissect. every time we want to update our weights, we subtract
   the derivative of the cost function w.r.t. the weight itself, scaled by
   a learning rate , and         that   s it! you   ll see that as it gets closer
   and closer to the center, the derivative term gets smaller and smaller,
   converging to zero as it approaches the solution. the same process
   applies with networks that have tens, hundreds, thousands, or more
   parameters         compute the gradient of the cost function w.r.t. each of
   the weights, and update each of your weights accordingly.

   i do want to say a few more words on the learning rate, because it   s
   one of the more important hyperparameters (   settings    for your neural
   network) that you have control over. if the learning rate is too high,
   it could jump too far in the other direction, and you never get to the
   minimum you   re searching for. set it too low, and your network will
   take ages to find the right weights, or it will get stuck in a local
   minimum. there   s no    magic number    to use when it comes to a learning
   rate, and it   s usually best to try several and pick the one that works
   the best for your individual network and dataset. in practice, many
   choose to anneal the learning rate over time         it starts out high,
   because it   s furthest from the solution, and decays as it gets closer.

   but as it turns out, id119 is kind of slow. really slow,
   actually. earlier i used the analogy of the weights    rolling    down the
   gradient to get to the bottom, but that doesn   t actually make any
   sense         it should pick up speed as it gets to the bottom, not slow
   down! another iterative optimization algorithm, known as momentum, does
   just that. as the weights begin to    roll    down the slope, they pick up
   speed. when they get closer to the solution, the momentum that they
   picked up carries them closer to the optima while id119
   would simply stop. as a result, training with momentum updates is both
   faster and can provide better results.

   here   s what the update rule looks like for momentum:
   [1*fo-egqgcie5jqhav5msodw.png]
   you might see the momentum update rule written differently depending on
   where you look, but the basic principle remains the same throughout.

   as we train, we accumulate a    velocity    value v. at each training step,
   we update v with the gradient at the current position (once again
   scaled by the learning rate). also notice that, with each time step, we
   decay velocity v by a factor mu (usually somewhere around .9), so that
   over time we lose momentum instead of bouncing around by the minimum
   forever. we then update our weight in the direction of the velocity,
   and repeat the process again. over the first few training iterations, v
   will grow as our weights    pick up speed    and take successively bigger
   leaps. as we approach the minimum, our velocity stops accumulating as
   quickly, and eventually begins to decay, until we   ve essentially
   reached the minimum. an important thing to note is that we accumulate a
   velocity independently for each weight         just because one weight is
   changing particularly clearly doesn   t mean any of the other weights
   need to be.

   there are lots of other iterative optimization algorithms that are
   commonly used with neural networks, but i won   t go into all of them
   here (if you   re curious, some of the more popular ones include
   [20]adagrad and [21]adam). the basic principle remains the same
   throughout         gradually update the weights to get them closer to the
   minimum. but regardless of which optimization algorithm you use, we
   still need to be able to compute the gradient of the cost function
   w.r.t. each weight. but our cost function isn   t a simple parabola
   anymore         it   s a complicated, many-dimensional function with countless
   local optima that we need to watch out for. that   s where
   id26 comes in.

before id26

   the id26 algorithm was a major milestone in machine learning
   because, before it was discovered, optimization methods were extremely
   unsatisfactory. one popular method was to perturb (adjust) the weights
   in a random, uninformed direction (ie. increase or decrease) and see if
   the performance of the ann increased. if it did not, one would attempt
   to either a) go in the other direction b) reduce the perturbation size
   or c) a combination of both. another attempt was to use genetic
   algorithms (which became popular in ai at the same time) to evolve a
   high-performance neural network. in both cases, without (analytically)
   being informed on the correct direction, results and efficiency were
   suboptimal. this is where the id26 algorithm comes into
   play.

the id26 algorithm

   recall that, for any given supervised machine learning problem, we (aim
   to) select weights that provide the optimal estimation of a function
   that models our training data. in other words, we want to find a set of
   weights w that minimizes on the output of j(w). we discussed the
   id119 algorithm         one where we update each weight by some
   negative, scalar reduction of the error derivative with respect to that
   weight. if we do choose to use id119 (or almost any other
   id76 algorithm), we need to find said derivatives in
   numerical form.

   for other machine learning algorithms like id28 or
   id75, computing the derivatives is an elementary
   application of differentiation. this is because the outputs of these
   models are just the inputs multiplied by some chosen weights, and at
   most fed through a single activation function (the sigmoid function in
   id28). the same, however, cannot be said for neural
   networks. to demonstrate this, here is a diagram of a double-layered
   neural network:
   [0*yk-ojfkluphzehou.]

   as you can see, each neuron is a function of the previous one connected
   to it. in other words, if one were to change the value of w1, both
      hidden 1    and    hidden 2    (and ultimately the output) neurons would
   change. because of this notion of functional dependencies, we can
   mathematically formulate the output as an extensive composite function:
   [0*vbsngk4vqd0qfwgl.]

   and thus:
   [0*mrjnrswmwh8yy-cg.]

   here, the output is a composite function of the weights, inputs, and
   activation function(s). it is important to realize that the hidden
   units/nodes are simply intermediary computations that, in actuality,
   can be reduced down to computations of the input layer.

   if we were to then take the derivative of said function with respect to
   some arbitrary weight (for example w1), we would iteratively apply the
   [22]chain rule (which i   m sure you all remember from your calculus
   classes). the result would look similar to the following:
   [0*9jzf-procskhtpct.]
   if you fail to get an intuition of this, try researching about the
   chain rule.

   now, let   s attach a black box to the tail of our neural network. this
   black box will compute and return the error         using the cost
   function         from our output:
   [0*susoph_fuwnahqjd.]

   all we   ve done is add another functional dependency; our error is now a
   function of the output and hence a function of the input, weights, and
   activation function. if we were to compute the derivative of the error
   with any arbitrary weight (again, we   ll choose w1), the result would
   be:
   [0*d5zula4kvesvylng.]

   each of these derivatives can be simplified once we choose an
   activation and error function, such that the entire result would
   represent a numerical value. at that point, any abstraction has been
   removed, and the error derivative can be used in id119 (as
   discussed earlier) to iteratively improve upon the weight. we compute
   the error derivatives w.r.t. every other weight in the network and
   apply id119 in the same way. this is
   id26         simply the computation of derivatives that are fed to
   a id76 algorithm. we call it    id26    because
   it almost seems as if we are traversing from the output error to the
   weights, taking iterative steps using chain the rule until we    reach   
   our weight.
   [0*k93unqz5fescivme.]
   it   s like feed-forward    but the opposite! we take the derivative of j
   w.r.t. the output, the output w.r.t. the hidden units, then the final
   hidden unit w.r.t. the weight.

   when i first truly understood the backprop algorithm (just a couple of
   weeks ago), i was taken aback by how simple it was. sure, the actual
   arithmetic/computations can be difficult, but this process is handled
   by our computers. in reality, id26 is just a rather tedious
   (but again, for a generalized implementation, computers will handle
   this) application of the chain rule. since neural networks are
   convoluted multilayer machine learning model structures (at least
   relative to other ones), each weight    contributes    to the overall error
   in a more complex manner, and hence the actual derivatives require a
   lot of effort to produce. however, once we get past the calculus,
   id26 of neural nets is equivalent to typical gradient
   descent for logistic/id75.

   thus far, i   ve walked through a very abstract form of backprop for a
   simple neural network. however, it is unlikely that you will ever use a
   single-layered ann in applications. so, now, let   s make our black
   boxes         the activation and error functions         more concrete such that we
   can perform backprop on a multilayer neural net.

   recall that our error function j(w) will compute the    error    of our
   neural network based on the output predictions it produces vs. the
   correct a priori outputs we know in our training set. more formally, if
   we denote our predicted output estimations as vector p, and our actual
   output as vector a, then we can use:
   [1*e1puofn7ogdpwtxc2bxnfw.png]
   in this case, j need not be a function of w because p already is. we
   can use vector notation here because the inputs/outputs to our neural
   nets our vectors/some form of tensor.

   this is just one example of a possible cost function (the
   [23]log-likelihood is also a popular one), and we use it because of its
   mathematical convenience (this is a notion one will frequently
   encounter in machine learning): the squared expression exaggerates poor
   solutions and ensures each discrepancy is positive. it will soon become
   clear why we multiply the expression by half.

   the derivative of the error w.r.t. the output was the first term in the
   error w.r.t. weight derivative expression we formulated earlier. let   s
   now compute it!
   [1*4wviflgvzi6jlfd31xfh5w.png]
   with a simple application of the power and chain rule, our derivative
   is complete. the half gets cancelled due to the power rule.

   our result is simply our predictions take away our actual outputs.

   now, let   s move on to the activation function. the activation function
   used depends on the context of the neural network. if we aren   t in a
   classification context, relu (rectified linear unit, which is zero if
   input is negative, and the identity function when the input is
   positive) is commonly used today.
   [1*i3m4x5zmlohhk8w3j3pvkw.png]
   the    rectified linear unit    acivation
   function         [24]http://i.stack.imgur.com/8cglm.png

   if we   re in a classification context (that is, predicting on a discrete
   state with a id203 ie. if an email is spam), we can use the
   sigmoid or tanh (hyperbolic tangent) function such that we can
      squeeze    any value into the range 0 to 1. these are used instead of a
   typical [25]step function because their    smoothness    properties allows
   for the derivatives to be non-zero. the derivative of the step function
   before and after the origin is zero. this will pose issues when we try
   to update our weights (nothing much will happen!).

   now, let   s say we   re in a classification context and we choose to use
   the sigmoid function, which is of the following equation:
   [0*vt63osgdwkos3w8z.]
   [0*oaer_qlu8v_onr3g.]
   smooth, continuous sigmoid function on the left. step, piece-wise
   function on the right.
   [26]https://en.wikibooks.org/wiki/file:hardlimitfunction.png &
   [27]https://en.wikibooks.org/wiki/file:sigmoidfunction.png.

   as per usual, we   ll compute the derivative using differentiation rules
   as:
   [0*2dw7dvyhiop3gavn.]
   looks confusing? forgot differentiation? don   t worry! just take my word
   for it. it   s not necessary to have a complete mathematical
   comprehension of this derivation.

   edit: on the 2nd line, the denominator should be raised to +2, not -2.
   thanks to a reader for pointing this out.

   sidenote: relu id180 are also commonly used in
   classification contexts. there are downsides to using the sigmoid
   function         particularly the    vanishing gradient    problem         which you
   can read more about [28]here.

   the sigmoid function is mathematically convenient (there it is again!)
   because we can represent its derivative in terms of the output of the
   function. isn   t that cool   

   we are now in a good place to perform id26 on a multilayer
   neural network. let me introduce you to the net we are going to work
   with:
   [0*m0tn6w0ipc3ro7mo.]

   this net is still not as complex as one you may use in your
   programming, but its architecture allows us to nevertheless get a good
   grasp on backprop. in this net, we have 3 input neurons and one output
   neuron. there are four layers in total: one input, one output, and two
   hidden layers. there are 3 neurons in each hidden layer, too (which, by
   the way, need not be the case). the network is fully connected; there
   are no missing connections. each neuron/node (save the inputs, which
   are usually pre-processed anyways) is an activity; it is the weighted
   sum of the previous neurons    activities applied to the sigmoid
   activation function.

   to perform backprop by hand, we need to introduce the different
   variables/states at each point (layer-wise) in the neural network:
   [0*jrjk5fblad5zum6h.]

   it is important to note that every variable you see here is a
   generalization on the entire layer at that point. for example, when i
   say x_i, i am referring to the input to any input neuron (arbitrary
   value of i). i chose to place it in the middle of the layer for
   visibility purposes, but that does not mean that x_i refers to the
   middle neuron. i   ll demonstrate and discuss the implications of this
   later on.

   x refers to the input layer, y refers to hidden layer 1, z refers to
   hidden layer 2, and p refers to the prediction/output layer (which fits
   in nicely with the notation used in our cost function). if a variable
   has the subscript i, it means that the variable is the input to the
   relevant neuron at that layer. if a variable has the subscript j, it
   means that the variable is the output of the relevant neuron at that
   layer. for example, x_i refers to any input value we enter into the
   network. x_j is actually equal to x_i, but this is only because we
   choose not to use an activation function         or rather, we use the
   identity activation function         in the input layer   s activities. we only
   include these two separate variables to retain consistency. y_i is the
   input to any neuron in the first hidden layer; it is the weighted sum
   of all previous neurons (each neuron in the input layer multiplied by
   the corresponding connecting weights). y_j is the output of any neuron
   at the hidden layer, so it is equal to activation_function(y_i) =
   sigmoid(y_i) = sigmoid(weighted_sum_of_x_j). we can apply the same
   logic for z and p. ultimately, p_j is the sigmoid output of p_i and
   hence is the output of the entire neural network that we pass to the
   error/cost function.

   the weights are organized into three separate variables: w1, w2, and
   w3. each w is a matrix (if you are not comfortable with id202,
   think of a 2d array) of all the weights at the given layer. for
   example, w1 are the weights that connect the input layer to the hidden
   layer 1. wlayer_ij refers to any arbitrary, single weight at a given
   layer. to get an intuition of ij (which is really i, j), wlayer_i are
   all the weights that connect arbitrary neuron i at a given layer to the
   next layer. wlayer_ij (adding the j component) is the weight that
   connects arbitrary neuron i at a given layer to an arbitrary neuron j
   at the next layer. essentially, wlayer is a vector of wlayer_is, which
   is a vector of real-valued wlayer_ijs.

   note: please note that the i   s and j   s in the weights and other
   variables are completely different. these indices do not correspond in
   any way. in fact, for x/y/z/p, i and j do not represent tensor indices
   at all, they simply represent the input and output of a neuron.
   wlayer_ij represents an arbitrary weight at an index in a weight
   matrix, and x_j/y_j/z_j/p_j represent an arbitrary input/output point
   of a neuron unit.

   that last part about weights was tedious! it   s crucial to understand
   how we   re separating the neural network here, especially the notion of
   generalizing on an entire layer, before moving forward.

   to acquire a comprehensive intuition of id26, we   re going to
   backprop this neural net as discussed before. more specifically, we   re
   going to find the derivative of the error w.r.t. an arbitrary weight in
   the input layer (w1_ij). we could find the derivative of the error
   w.r.t. an arbitrary weight in the first or second hidden layer, but
   let   s go as far back as we can; the more backprop, the better!

   so, mathematically, we are trying to obtain (to perform our iterative
   optimization algorithm with):
   [0*qmwl-ovnnq8kzvtv.]

   we can express this graphically/visually, using the same principles as
   earlier (chain rule), like so:
   [0*hlo6xxrwczjuzxxh.]
   we backpropagate from the error all the way to the weights in the input
   layer. note that the weight itself is any arbitrary one in that layer.
   in a fully connected neural net, we can make these generalizations
   considering we are consistent with our indices.

   in two layers, we have three red lines pointing in three different
   directions, instead of just one. this is a reinforcement of (and why it
   is important to understand) the fact that variable j is just a
   generalization/represents any point in the layer. so, when we
   differentiate p_i with respect to the layer before that, there are
   three different weights, as i hope you can see, in w3_ij that
   contribute to the value p_i. there also happen to be three weights in
   w3 in total, but this isn   t the case for the layers before; it is only
   the case because layer p has one neuron         the output         in it. we stop
   backprop at the input layer and so we just point to the single weight
   we are looking for.

   wonderful! now let   s work out all this great stuff mathematically.
   immediately, we know:
   [0*dqcoyr-fff2ieobo.]

   we have already established the left hand side, so now we just need to
   use the chain rule to simplify it further. the derivative of the error
   w.r.t. the weight can be written as the derivative of the error w.r.t.
   the output prediction multiplied by the derivative of the output
   prediction w.r.t. the weight. at this point, we   ve traversed one red
   line back. we know this because
   [0*vrut30ch_1rly7mq.]

   is reducible to a numerical value. specifically, the derivative of the
   error w.r.t. the output prediction is:
   [1*uvimzgqi0fpvxxb3oe9bww.png]
   we know this from our manual derivation earlier.

   hence:
   [1*nzwutmypoiieoog8kxorkg.png]

   going one more layer backwards, we can determine that:
   [0*94lsjgvc_g4q66-u.]

   in other words, the derivative of the output prediction w.r.t. the
   weight is the derivative of the output w.r.t. the input to the output
   layer (p_i) multiplied by the derivative of that value w.r.t. the
   weight. this represents our second red line. we can solve the first
   term like so:
   [0*ggh61s46zipxtlv_.]

   this corresponds with the derivative of the sigmoid function we solved
   earlier, which was equal to the output multiplied by one minus the
   output. in this case, p_j is the output of the sigmoid function. now,
   we have:
   [0*prphixs0phahxjg3.]
   [1*fgy8zy6o7zwes2is7wxu_g.png]

   let   s move on to the third red line(s). this one is interesting because
   we begin to    spread    out. since there are multiple different weights
   that contribute to the value of p_i, we need to take into account their
   individual    pull    factors into our derivative:
   [0*n0rhnqptnmf3t_1b.]

   if you   re a mathematician, this notation may irk you slightly; sorry if
   that   s the case! in computer science, we tend to stray from the notion
   of completely legal mathematical expressions. this is yet again again
   another reason why it   s key to understand the role of layer
   generalization; z_j here is not just referring to the middle neuron,
   it   s referring to an arbitrary neuron. the actual value of j in the
   summation is not changing (it   s not even an index or a value in the
   first place), and we don   t really consider it. it   s less of a
   mathematical expression and more of a statement that we will iterate
   through each generalized neuron z_j and use it. in other words, we
   iterate over the derivative terms and sum them up using z_1, z_2, and
   z_3. before, we could write p_j as any single value because the output
   layer just contains one node; there is just one p_j. but we see here
   that this is no longer the case. we have multiple z_j values, and p_i
   is functionally dependent on each of these z_j values. so, when we
   traverse from p_j to the preceding layer, we need to consider each
   contribution from layer z to p_j separately and add them up to create
   one total contribution. there   s no upper bound to the summation; we
   just assume that we start at zero and end at our maximum value for the
   number of neurons in the layer. please again note that the same changes
   are not reflected in w1_ij, where j refers to an entirely different
   thing. instead, we   re just stating that we will use the different z_j
   neurons in layer z.

   since p_i is a summation of each weight multiplied by each z_j
   (weighted sum), if we were to take the derivative of p_i with any
   arbitrary z_j, the result would be the connecting weight since said
   weight would be the coefficient of the term (derivative of m*x w.r.t. x
   is just m):
   [0*ngzuc0j3wc6ozwmn.]

   w3_ij is loosely defined here. ij still refers to any arbitrary
   weight         where ij are still separate from the j used in p_i/z_j         but
   again, as computer scientists and not mathematicians, we need not be
   pedantic about the legality and intricacy of expressions; we just need
   an intuition of what the expressions imply/mean. it   s almost a succinct
   form of psuedo-code! so, even though this defines an arbitrary weight,
   we know it means the connecting weight. we can also see this from the
   diagram: when we walk from p_j to an arbitrary z_j, we walk along the
   connecting weight. so now, we have:
   [0*w-invp_x6hasltdv.]

   at this point, i like to continue playing the    reduction test   . the
   reduction test states that, if we can further simplify a derivative
   term, we still have more backprop to do. since we can   t yet quite put
   the derivative of z_j w.r.t. w1_ij into a numerical term, let   s keep
   going (and fast-forward a bit). using chain rule, we follow the fourth
   line back to determine that:
   [0*sdmtqirs-dj_o77y.]

   since z_j is the sigmoid of z_i, we use the same logic as the previous
   layer and apply the sigmoid derivative. the derivative of z_i w.r.t.
   w1_ij, demonstrated by the fifth line(s) back, requires the same idea
   of    spreading out    and summation of contributions:
   [0*kpjra6epvwn2eo2z.]

   briefly, since z_i is the weighted sum of each y_j in y, we sum over
   the derivatives which, similar to before, simplifies to the relevant
   connecting weights in the preceding layer (w2 in this case).

   we   re almost there, let   s go further; there   s still more reduction to
   do:
   [0*ia5pppk0yg1f-8wq.]

   we have, of course, another sigmoid activation function to deal with.
   this is the sixth red line. notice, now, that we have just one line
   remaining. in fact, our last derivative term here passes (or rather,
   fails) the reduction test! the last line traverses from the input at
   y_i to x_j, walking along w1_ij. wait a second         is this not what we
   are attempting to backprop to? yes, it is! since we are, for the first
   time, directly deriving y_i w.r.t. the weight w1_ij, we can think of
   the coefficient of w1_ij as being x_j in our weighted sum (instead of
   the vice versa as used previously). hence, the simplification follows:
   [0*gfdjmi9ypac0g9ix.]

   of course, since each x_j in layer x contributes to the weighted sum
   y_i, we sum over the effects. and that   s it! we can   t reduce any
   further from here. now, let   s tie all these individual expressions
   together:
   [1*uy2yjcsfx5y-zzlnaiwrsq.png]
   our final expression for the derivative of the error w.r.t. any weight
   in w1

   edit: the denominator on the left hand side should say dw  ij instead of
      layer   .

   with no more partial derivative terms left, our work is complete! this
   gives us the derivative of the error w.r.t. any arbitrary weight in the
   input layer/w1. that was a lot of work         maybe now we can sympathize
   with the poor computers!

   something you should notice is that values such as p_j, a, z_j, y_j,
   x_j etc. are the values of the network at the different points. but
   where do they come from? actually, we would need to perform a
   feed-forward of the neural network first and then capture these
   variables.

neural network training overview

   our task is to now perform id119 to train the neural net:
   [0*b_2wazu14ush5ubo.]

   we perform id119 on each weight in each layer. notice that
   the resulting gradient should change each time because the weight
   itself changes, (and as a result, the performance and output of the
   entire net should change) even if it   s a small perturbation. this means
   that, at each update, we need to do a feed-forward of the neural net.
   not just once before, but once each iteration.

   these are then the steps to train an entire neural network:
    1. create our connected neural network and prepare training data
    2. initialize all the weights to random values

   it   s important to note that one must not initialize the weights to
   zero, similar to what may be done in other machine learning algorithms.
   if weights are initialized to zero, after each update, the outgoing
   weights of each neuron will be identical, because the gradients will be
   identical (which can be proved). because of this, the proceeding hidden
   units will remain the same value and will continue to follow each
   other. ultimately, this means that our training will become extremely
   constrained (due to the    symmetry   ), and we won   t be able to build
   interesting functions. also, neural networks may get stuck at local
   optima (places where the gradient is zero but are not the global
   minima), so random weight initialization allows one to hopefully have a
   chance of circumventing that by starting at many different random
   values.

   3. perform one feed-forward using the training data

   4. perform id26 to get the error derivatives w.r.t. each and
   every weight in the neural network

   5. perform id119 to update each weight by the negative
   scalar reduction (w.r.t. some learning rate alpha) of the respective
   error derivative. increment the number of iterations.

   6. if we have converged (in reality, though, we just stop when we have
   reached the number of maximum iterations) training is complete. else,
   repeat starting at step 3.

   if we initialize our weights randomly (and not to zero) and then
   perform id119 with derivatives computed from
   id26, we should expect to train a neural network in no time!
   i hope this example brought clarity to how backprop works and the
   intuition behind it. if you didn   t understand the intricacies of the
   example but understand and appreciate the concept of backprop as a
   whole, you   re still in a great place! next we   ll go ahead and explain
   backprop code that works on any generalized architecture of a neural
   network using the relu activation function.

implementing id26

   now that we   ve developed the math and intuition behind id26,
   let   s try to implement it. we   ll divide our implementation into three
   distinct steps:
    1. feed-forward. in this step, we take the inputs and forward them
       through the network, layer by layer, to generate the output
       activations (as well as all of the activations in the hidden
       layers). when we are actually using our network (rather than
       training it), this is the only step we   ll need to perform.
    2. id26. here, we   ll take our error function and compute
       the weight gradients for each layer. we   ll use the algorithm just
       described to compute the derivative of the cost function w.r.t.
       each of the weights in our network, which will in turn allow us to
       complete step 3.
    3. weight update. finally, we   ll use the gradients computed in step 2
       to update our weights. we can use any of the update rules discussed
       previously during this step (id119, momentum, and so
       on).

   let   s start off by defining what the api we   re implementing looks like.
   we   ll define our network as a series of layer instances that our data
   passes through         this means that instead of modeling each individual
   neuron, we group neurons from a single layer together. this makes it a
   bit easier to reason about larger networks, but also makes the actual
   computations faster (as we   ll see shortly). also         we   re going to write
   the code in python.

   each layer will have the following api:
class relulayer(object):
    def __init__(self, size_in, size_out):
        # initialize this layer with random weights and any other
        # parameters we may need.
        pass
    def forward(self, in_act):
        # compute the outgoing activations from this layer, given
        # the activations from the previous layer.
        pass
    def backward(self, out_grad):
        # out_grad is the derivative of the cost function w.r.t. the
        # inputs to all of the neurons for the following layer. we
        # need to compute the gradient of our own weights, and
        # return another the gradient of the inputs to this layer to
        # continue the id26.
        pass
    def update(self, alpha):
        # perform the actual weight update step.
        pass

   (this isn   t great api design         ideally, we would decouple the backprop
   and weight update from the rest of the object, so the specific
   algorithm we use for updating weights isn   t tied to the layer itself.
   but that   s not the point, so we   ll stick with this design for the
   purposes of explaining how id26 works in a real-life
   scenario. also: we   ll be using [29]numpy throughout the implementation.
   it   s an awesome tool for mathematical operations in python (especially
   tensor based ones), but we don   t have the time to get into how it
   works         if you want a good introduction, [30]here ya    go.)

   we can start by implementing the weight initialization. as it turns
   out, how you initialize your weights is actually kind of a big deal for
   both network performance and convergence rates. here   s how we   ll
   initialize our weights:
self.w = np.random.randn(self.size_in, self.size_out) * np.sqrt(2.0/self.size_in
)

   this initializes a weight matrix of the appropriate dimensions with
   random values sampled from a normal distribution. we then scale it
   rad(2/self.size_in), giving us a variance of 2/self.size_in (derivation
   [31]here).

   and that   s all we need for layer initialization! let   s move on to
   implementing our first objective         feed-forward. this is actually
   pretty simple         a dot product of our input activations with the weight
   matrix, followed by our activation function, will give us the
   activations we need. the dot product part should make intuitive sense;
   if it doesn   t, you should sit down and try to work through it on a
   piece of paper. this is where the performance gains of grouping neurons
   into layers comes from: instead of keeping an individual weight vector
   for each neuron, and performing a series of vector dot products, we can
   just do a single matrix operation (which, thanks to the wonders of
   modern processors, is significantly faster). in fact, we can compute
   all of the activations from a layer in just two lines:
# compute weighted sum for each neuron
self.out_act = np.dot(self.in_act, self.w)
# activation function (any sum < 0 is capped at 0)
self.out_act[self.out_act < 0] = 0
return self.out_act

   simple enough. let   s move on to id26.

   this one   s a bit more involved. first, we compute the derivative of the
   output w.r.t. the weights, then the derivative of the cost w.r.t. the
   output, followed by chain rule to get the derivative of the cost w.r.t.
   the weights.

   let   s start with the first part         the derivative of the output w.r.t.
   the weights. that should be simple enough; because you   re multiplying
   the weight by the corresponding input activation, the derivative will
   just be the corresponding input activation.
output_wrt_weights = np.ones(self.w.shape) * self.in_act[:, none]

   except, because we   re using the relu activation function, the weights
   have no effect if the corresponding output is < 0 (because it gets
   capped anyway). this should take care of that hiccup:
output_wrt_weights[:, self.out_act < 0] = 0

   (more formally, you   re multiplying by the derivative of the activation
   function, which is 0 when the activation is < 0 and 1 elsewhere.)

   let   s take a brief detour to talk about the out_grad parameter that our
   backward method gets. let   s say we have a network with two layers: the
   first has m neurons, and the second has n. each of the m neurons
   produces an activation, and each of the n neurons looks at each of the
   m activations. the out_grad parameter is an m x n matrix of how each m
   affects each of the n neurons it feeds into.

   now, we need the derivative of the cost w.r.t. each of the
   outputs         which is essentially the out_grad parameter we   re given! we
   just need to sum up each row of the matrix we   re given, as per the
   id26 formula.
cost_wrt_output = np.sum(np.atleast_2d(grad), axis=1)

   finally, we end up with something like this:
self.dw = cost_wrt_weights

   now, we need to compute the derivative of our inputs to pass along to
   the next layer. we can perform a similar chain rule         derivative of the
   output w.r.t. the inputs times the derivative of the cost w.r.t. the
   outputs.
output_wrt_inputs = self.w
output_wrt_inputs[:, self.out_act < 0] = 0
cost_wrt_inputs = cost_wrt_output * output_wrt_inputs
return cost_wrt_inputs

   and that   s it for the id26 step.

   the final step is the weight update. assuming we   re sticking with
   id119 for this example, this can be a simple one-liner:
self.w = self.w     self.dw * alpha

   to actually train our network, we take one of our training samples and
   call forward on each layer consecutively, passing the output of the
   previous layer as the input of the following layer. we compute dj,
   passing that as the out_grad parameter to the last layer   s backward
   method. we call backward on each of the layers in reverse order, this
   time passing the output of the further layer as out_grad to the
   previous layer. finally, we call update on each of our layers and
   repeat.

   there   s one last detail that we should include, which is the concept of
   a bias (akin to that of a constant term in any given equation). notice
   that, with our current implementation, the activation of a neuron is
   determined solely based on the activations of the previous layer.
   there   s no bias term that can shift the activation up or down
   independent of the inputs. a bias term isn   t strictly necessary         in
   fact, if you train your network as-is, it would probably still work
   fine. but if you do need a bias term, the code stays almost the
   same         the only difference is that you need to add a column of 1s to
   the incoming activations, and update your weight matrix accordingly, so
   one of your weights gets treated as a bias term. the only other
   difference is that, when returning cost_wrt_inputs, you can cut out the
   first row         nobody cares about the gradients associated with the bias
   term because the previous layer has no say in the activation of the
   bias neuron.

   implementing id26 can be kind of tricky, so it   s often a
   good idea to check your implementation. you can do so by computing the
   gradient numerically (by literally perturbing the weight and
   calculating the difference in your cost function) and comparing it to
   your id26-computed gradient. this gradient check doesn   t
   need to be run once you   ve verified your implementation, but it could
   save a lot of time tracking down potential problems with your network.

   nowadays, you often don   t even need to implement a neural network on
   your own, as libraries such as [32]caffe, [33]torch, or [34]tensorflow
   will have implementations ready to go. that being said, it   s often a
   good idea to try implementing it on your own to get a better grasp of
   how everything works under the hood.

learning more about neural networks

   intrigued? looking to learn more about neural networks? here are some
   great online classes to get you started:

   [35]stanford   s cs231n. although it   s technically about convolutional
   neural networks, the class provides an excellent introduction to and
   survey of neural networks in general. class videos, notes, and
   assignments are all posted [36]here, and if you have the patience for
   it i would strongly recommend walking through the assignments so you
   can really get to know what you   re learning.

   [37]mit 6.034. this class, taught by prof. patrick henry winston,
   explores many different algorithms and disciplines in artificial
   intelligence. there   s a great [38]lecture on backprop that i actually
   used as a stepping stone to getting setup writing this article. i also
   learned id107 from prof. winston         he   s a great teacher!
     __________________________________________________________________

   we hope that, if you visited this article without knowing how the
   id26 algorithm works, you are reading this with an (at least
   rudimentary) mathematical or conceptual intuition of it. writing and
   conveying such a complex algorithm to a supposed beginner has proven to
   be an extremely difficult task for us, but it   s helped us truly
   understand what we   ve been learning about. with greater knowledge in a
   fundamental area of machine learning, we are now excited to take a look
   at new, interesting algorithms and disciplines in the field. we are
   looking forward to continue documenting these endeavors together.

     * [39]neural networks
     * [40]machine learning
     * [41]algorithms

   (button)
   (button)
   (button) 1.96k claps
   (button) (button) (button) 19 (button) (button)

     (button) blockedunblock (button) followfollowing
   [42]go to the profile of rohan kapur

[43]rohan kapur

   rohankapur.com

     (button) follow
   [44]a year of artificial intelligence

[45]a year of artificial intelligence

   our ongoing effort to make the mathematics, science, linguistics, and
   philosophy of artificial intelligence fun and simple.

     * (button)
       (button) 1.96k
     * (button)
     *
     *

   [46]a year of artificial intelligence
   never miss a story from a year of artificial intelligence, when you
   sign up for medium. [47]learn more
   never miss a story from a year of artificial intelligence
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://ayearofai.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/abf4609d4f9d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-lenny-1-neural-networks-the-id26-algorithm-explained-abf4609d4f9d&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-lenny-1-neural-networks-the-id26-algorithm-explained-abf4609d4f9d&source=--------------------------nav_reg&operation=register
   8. https://ayearofai.com/?source=logo-lo_0oxxcfd0cxrg---bb87da25612c
   9. https://ayearofai.com/tagged/algorithms
  10. https://ayearofai.com/tagged/today-i-learned
  11. https://ayearofai.com/tagged/case-studies
  12. https://ayearofai.com/tagged/philosophical
  13. https://ayearofai.com/tagged/meta
  14. https://ayearofai.com/@mckapur?source=post_header_lockup
  15. https://ayearofai.com/@mckapur
  16. https://medium.com/@lennykhazan
  17. https://medium.com/@mckapur
  18. https://medium.com/a-year-of-artificial-intelligence
  19. https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5
  20. http://www.magicbroom.info/papers/duchihasi10.pdf
  21. http://arxiv.org/abs/1412.6980
  22. https://en.wikipedia.org/wiki/chain_rule
  23. https://en.wikipedia.org/wiki/likelihood_function#log-likelihood
  24. http://i.stack.imgur.com/8cglm.png
  25. https://en.wikipedia.org/wiki/step_function
  26. https://en.wikibooks.org/wiki/file:hardlimitfunction.png
  27. https://en.wikibooks.org/wiki/file:sigmoidfunction.png
  28. https://www.quora.com/what-is-special-about-rectifier-neural-units-used-in-nn-learning
  29. http://www.numpy.org/
  30. http://cs231n.github.io/python-numpy-tutorial/
  31. http://arxiv-web3.library.cornell.edu/abs/1502.01852
  32. http://caffe.berkeleyvision.org/
  33. http://torch.ch/
  34. http://tensorflow.org/
  35. http://cs231n.stanford.edu/
  36. http://cs231n.stanford.edu/syllabus.html
  37. http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/
  38. https://www.youtube.com/watch?v=q0pm3briufo
  39. https://ayearofai.com/tagged/neural-networks?source=post
  40. https://ayearofai.com/tagged/machine-learning?source=post
  41. https://ayearofai.com/tagged/algorithms?source=post
  42. https://ayearofai.com/@mckapur?source=footer_card
  43. https://ayearofai.com/@mckapur
  44. https://ayearofai.com/?source=footer_card
  45. https://ayearofai.com/?source=footer_card
  46. https://ayearofai.com/
  47. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  49. https://medium.com/p/abf4609d4f9d/share/twitter
  50. https://medium.com/p/abf4609d4f9d/share/facebook
  51. https://medium.com/p/abf4609d4f9d/share/twitter
  52. https://medium.com/p/abf4609d4f9d/share/facebook
