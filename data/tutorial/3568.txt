   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]a year of artificial intelligence
     * [9]algorithms
     * [10]today i learned
     * [11]case studies
     * [12]philosophical
     * [13]meta
     __________________________________________________________________

   [1*0emls5ith8kgb-8rnm4cxw.jpeg]
   id163, a vast database for image recognition and convnet training.
   [14]https://www.wired.com/wp-content/uploads/2015/01/id98-visualization-
   crop.jpg

rohan & lenny #2: convolutional neural networks

the ingenious algorithm that accelerated id161 (and its biological
roots).

   [15]go to the profile of lenny khazan
   [16]lenny khazan (button) blockedunblock (button) followfollowing
   nov 12, 2016
     __________________________________________________________________

     this is the second group ([17]lenny and [18]rohan) entry in our
     [19]journey to extend our knowledge of artificial intelligence and
     convey that knowledge in a simple, fun, and accessible manner. learn
     more about our motives in this [20]introduction post.
     __________________________________________________________________

   we are back! college applications and senior year have drained quite a
   bit of our free time, but we   re still alive (albeit slightly sleep
   deprived).

   let   s talk about a topic that we   ve been meaning to write about for a
   while but haven   t gotten around to doing: convolutional neural
   networks.

   before we get started, you should try to familiarize yourself with
      vanilla    neural networks. if you need a refresher, check out our
   [21]neural networks and backpropogation mega-post from earlier this
   year. this is so you know the basics of machine learning, linear
   algebra, neural network architecture, cost functions, optimization
   methods, training/test sets, id180/what they do,
   softmax, etc.

problem

   id161

   what is id161?

   you probably guessed it: it   s computers seeing things! duh.

   but    seeing    is a pretty vague term. id161         which is really
   an interdisciplinary field         is about computers being able to gain
   high-level understandings and make conclusions, predictions, and/or
   statements from some kind of visual input.

   let   s start talking about sub-problems in id161. because, in
   reality, when we say    id161    we are not referring to one
   concrete task. instead, we are referring to a group of tasks that
   machine learning has attempted (both successfully and less so) to
   solve. however, the most common task we hear about, and the one we will
   specifically look at in this article, is that of object classification.
   it   s pretty much exactly what it sounds like: identifying and
   classifying the objects in an image.

   let   s demonstrate this with a cute picture.
   [1*rvaw1ih2udrj8kvgqpptkg.png]

   a successful object classification algorithm would look at this image
   and output that there is a    dog    present in it. an even better
   classification algorithm may suggest    puppy    as a more accurate or
   precise class of the object.    flowers    and    grass    may also be
   reasonable classifications if we   re trying to look for all of the
   objects in an image.

   this is one of the hallmark problems in id161         being able to
   take an image or video and identify/classify the objects inside it. and
   it   s not easy. because, this is what the computer sees when it looks at
   that dog:
   [1*-bphafk2ptkksoqxpaoyfq.png]

   so    why do you care? and why should you care? object classification,
   other than being a major step forward for understanding id161
   as a whole and furthering machine learning, has dozens of applications.
   some of these applications are integral to your daily life, some are
   integral to the operation/growth of many companies, and some make the
   government or military run. here   s a small list of specific
   capabilities:
     * id101 (they need to perceive the world around them)
     * facial detection/recognition
     * organizing photos (eg. google and now apple   s photo app)

   the same algorithm that can be used for object classification, however,
   can also be used to         for example         analyze an fmri brain scan and make
   a predictive diagnosis on it. in this sense, the algorithm is not
   classifying an object but trying to look for patterns in the image that
   would make it similar to other fmri scans it has seen before.

   id161 has countless other sub-problems. object segmentation
   is about figuring out where exactly different objects lie in an image.
   scene parsing lets us look at an image as a whole to segment it into
   generic categories (sky, grass, road, etc.). when dealing with video,
   motion tracking helps figure out how a particular object moves from
   frame to frame.

   put simply, id161 is a pretty big field. this article will
   mostly focus on classification, but id98s are applicable to countless
   other problems as well.

problem with vanilla neural networks

   okay, cool. so we need an algorithm to do some id161         why
   not use a good ol    neural network? surely, deep artificial neural
   networks, the almighty machine learning algorithm, would be able to
   succeed in id161!

   well, as it turns out, traditional neural networks don   t work that well
   for id161. actually, they really suck. let   s see why.

   imagine a neural network that takes an image as input and outputs a
   id203 distribution over potential classes for the    primary   
   object it sees in the image. let   s pretend that each of the images we
   are feeding into this ann is a closeup of said object against some
   background, like this:
   [0*hndmbr05ajkvkfnn.]

   let   s now say we are trying to classify cutlery only (i think you
   americans call it    silverware    or something). our discrete classes will
   be represented by the id203 in the individual output nodes of the
   ann. like this:
   [0*zd-abvf2nqoxt2bq.]

   we   d expect a properly trained ann   s value in the    spoon    output node
   to be the greatest, with    spork    probably coming in second place.

   ok, so this all looks great. what   s the issue? well, here it is:
   [1*byyk8a9dp3ouaskku4bv4q.png]

   turning an image into a bunch of separate input neurons isn   t actually
   the problem. that   s pretty easy to do         we could just treat each neuron
   as each separate pixel in the image. for simplicity   s sake, let   s
   convert the picture to grayscale, such that each pixel is represented
   by exactly one number between 0 (black) and 1 (white). we can then
      flatten    this 2d array of pixel values into a 1d array (that is, just
   concatenate each sub-array to the previous one to make a long list of
   all the pixels), which corresponds to the input to our neural network.

   here   s what that flattening process could look like visually:
   [1*ob1z8_1in9ozdcysokppww.png]

   let   s just call this a pixel input vector. now, let   s consider         on a
   high level         what the ann is doing.
   [1*duwozumbg46p9ukvtzgvia.png]

   first, we   ll give it a lot of spoons. so. many. spoons. the network
   will try to look at all these spoons and try to figure out what makes a
   spoon a spoon based on patterns in the images (e.g. the reflection from
   the spoon head, the convex indent that is on the spoon head, the
   handle, etc.) we might also add images of forks and knives to
   understand concretely what makes a piece of cutlery not a spoon.

   but, notice something. while inputting these images of different
   spoons, we would expect them to have similar pixel input vectors
   because they are all spoons. when a new image of a spoon comes along
   the ann should be able to (abstractly and mathematically) say            hey,
   this looks similar to all the other spoon pixel input vectors i   ve
   seen   so it   s probably a spoon!   . it should be able to find distinct
   patterns in these pixel input vectors that it can use to ascertain what
   makes a certain pixel input vector likely to be that of a spoon. that   s
   how any supervised machine learning algorithm or ann should work.

   however, this just isn   t the case. the reality is, the pixel vectors of
   the following two images         even at grayscale         are completely
   different:
   [1*-rm3jchvqpt2yu6gdwug3a.png]

   yes, these are both images of a spoon, but they are different type of
   spoons, the indented parts have different reflections, the orientations
   are different, the angles are different, the backdrops are
   different         i could go on and on. as a result, the actual pixel input
   vector will not be even remotely similar. thus, an ann will have a hard
   time associating these two images.

   and this is a really really simple example. in reality our images will
   be larger, noisier, and the objects may be at different places in the
   image and of different sizes. put simply, anns just won   t do very well
   at these tasks.

   the reality is that it   s not the individual pixels that tell us whether
   an image of a spoon is an image of a spoon         it   s something much more
   abstract than that. it   s the handle and the circular indent that we
   identify, for example. these are the    characteristics    of a spoon. and,
   ultimately, it   s edges, lines, and corners that superimpose on each
   other to build out these characteristics. operations on individual
   pixels aren   t enough to let us figure out if these pixels represent a
   spoon. instead, we need to figure out the pixel groupings that make
   these edges/lines/corners and see how groupings of those
   edges/lines/corners then go on to form the characteristics.
   furthermore, flattening images into single vectors         though retaining
   information of the pixels         loses information such as the structure of
   the image. our neural network would not be able to exploit this
   structure, which is certainly important information when it comes to
   recognizing objects.

   however, this is not the only issue. it   s also very valid to suggest
   that anns simply don   t scale well to large images. if each value in the
   pixel input vector is fed into a separate node, we essentially are
   going to have a new input feature per pixel. imagine a 200 x 200
   grayscale image         that   s 40,000 input features! that would mean
   something at least on the order of hundreds of thousands of weights per
   layer (if not more), which is simply infeasible. such a large number of
   parameters would mean slow training and very likely overfitting as
   well.

why pixels anyways, though?

   why input pixel values to the ann? why not choose and extract/compute
   our own features for classifying spoons?

   well, it could work (and that   s exactly what we used to do), but
   human-constructed features are rarely as good as    learned    features.
   sometimes we also just don   t know how to come up with good features for
   certain objects. on top of that, if we want to truly solve computer
   vision, human-constructed features simply aren   t a good approach         we
   want to build algorithms that can see and classify anything based on
   prior experience/training with that thing and without human
   intervention.

   really, this is the sort of thing that we want artificial intelligence
   to solve. and convolutional neural networks do it beautifully.

moravec   s paradox

   before we go any further, you might be wondering: why is it so frikin   
   hard to get computers to see? we humans don   t need to put any effort
   into it! when i see a computer in front of me, there is no hardcore
   processing or rationalization that my brain needs to undergo. i   m just
   like:    yo, whose macbook is that?   

   yet, why is it so hard for us to multiply 359 and 214.24 in our heads?
   (try it, i dare you.) the dumbest, slowest computers can do it so
   easily, yet i   m 100% sure the common man would give up before even
   starting.

   this highlights moravec   s paradox: the discovery by researchers in
   artificial intelligence/robotics that high-level reasoning requires
   little computation for computers but great computation for humans,
   whilst low-level sensorimotor (fancy word         basically sense perception
   like seeing) skills are the opposite. hans moravec puts it
   straightforwardly:

     it is comparatively easy to make computers exhibit adult level
     performance on intelligence tests or playing checkers, and difficult
     or impossible to give them the skills of a one-year-old when it
     comes to perception and mobility.

   marvin minsky (r.i.p.) said it best, though:

     in general, we   re least aware of what our minds do best    we   re more
     aware of simple processes that don   t work well than of complex ones
     that work flawlessly.

   this is an interesting observation because it   s obviously so relevant
   to id161. an explanation offered by moravec, though, is that
   we can attribute this all to evolution. over the course of millions of
   years of human evolution by natural selection, our brains have
   increased in complexity and improved in design. seaid113ss object
   recognition and abstract thinking are potential examples of these.

   really, the oldest human skills like abstract thought, object
   identification through vision, and complex linguistic expression are
   largely unconscious processes (we don   t really need to think to do
   them) and thus to us they seem very effortless to us. over many many
   years, they have continued to evolve to make them so seaid113ss. we
   should expect that these skills         being so old and so evolved/iterated
   upon         will take a lot of time to replicate in machines/computers. that
   is why this seeming    paradox    might not be a paradox at all.

vision in the brain

   unsurprisingly, these convolutional neural networks (and yes, we still
   haven   t explained what those are         we   re getting there, i promise) are
   heavily inspired by our own brains. so, it might behoove us to figure
   out how we humans look at stuff, and then derive a neural network
   architecture to mimic that. if you don   t really care about any of this
   and just want to get to the good stuff, you should be ok to just skip
   this section and move on.

   still here? good. our visual system, like the rest of our nervous
   system, is composed largely of neurons. biological neurons do pretty
   much the same thing as artificial neurons in our anns; they take inputs
   from other neurons, and can choose to emit an output based on these
   inputs. unlike anns, the inputs and outputs are a binary on/off signal
   instead of a real number, but neurons can vary the firing rate (the
   rate at which they send an output signal). when we say a neuron is
      excited,    that just means its firing signals like there   s no tomorrow.

   ok, now let   s talk about how we go from light to neurons. humans see
   things through their eyes: photons shoot towards your cornea, through
   your pupil, and hit the retina at the back of your eyeball.
   photoreceptor cells (rods, cones) in your retina get excited when they
   see light and color, and that eventually works its way to your retinal
   ganglion cells.

   it   s in these retinal ganglion cells where things first start to get
   interesting. each retinal ganglion cell takes inputs from a cluster of
   photoreceptors, representing a small region of your vision. your
   retinal ganglion cells divide this region into a    central zone    and a
      peripheral zone    around the center; if the cell gets the right amount
   of light in each of these regions, it will start firing to let the
   downstream parts of your vision pathway know it sees something.
   concretely, there are two types of cells: on-center and off-center.
   when on-center cells see light in the middle of their respective
   region, they get all excited and start firing. when off-center cells
   see light around the center, they too get excited. the takeaway here is
   that each cell responds to a certain pattern in its input, which we can
   build off later on to detect more complicated patterns. we   ll see how
   that works in a minute.
   [0*zlvap-tfxqiqv4v9.]
   here   s a great little diagram of what i   m talking about, [22]courtesy
   of wikibooks. the yellow bits are the areas where there is light, the
   purple is where there isn   t.

   an important thing to note here is that every cell is only affected by
   a small region of your vision. the firing rate might be heavily
   influenced by the amount of light hitting that small part of your
   retina, but outside of that region the firing rate of that cell is
   unaffected. the region of your vision that influences a particular cell
   is called a receptive field (rf). there was a famous experiment by
   hubel & wiesel that demonstrated this concept: by recording the signals
   coming out of one of these cells (converted to audible beeps in the
   following video), they showed that one of these cells is only
   responsive to a particular region of your vision.

   iframe: [23]/media/fe3ecbdf357f89d57998ab6eb405e50a?postid=5f4cd480a60b

   (now comes the part where i think i can design my own diagrams.)
   imagine all of your retinal ganglion cells organized into a grid based
   on the region of your vision that they represent, like so:
   [0*vdb8qefjs0mrk7qr.]
   each cell is a retinal ganglion cell. the shade of each cell represents
   its firing rate         in this case, before we see anything, they are all
   pretty much silent.

   let   s pretend we see something: a bright light directed at the center
   of our vision. the cells in the center of our field of view get all
   excited, but the rest still have no idea anything   s changed         receptive
   fields at work!
   [0*l21x48hb5ze8jljb.]
   the cells in the middle are flipping the hell out, while the others
   have no idea what   s going on.      receptive fields     

   if we take a step back and look at this a bit more abstractly, what
   we   re really doing is transforming the data we   re getting. initially,
   our    data    was the light coming into our eye; while this representation
   was rich with detail, it was hard to get to the    core    of what we were
   really seeing. using the photoreceptors and retinal ganglion cells, we
   can reduce this representation to something more meaningful:    where is
   there light?    other cells early on in the vision pathway can pick up
   unique colors and basic patterns that help round out this new
   representation with more information that we can use to eventually pick
   out individual objects and shapes.

   the signals from your retinal ganglion cells eventually make their way
   through something called the lgn and into your primary visual cortex
   (affectionately called    v1   ). like the cells in your retina, v1 cells
   look at just small region of their input. building off of the work done
   in your retina to pick out light and color in what we   re seeing, these
   v1 cells can start looking for things like lines and patterns;
   essentially building a    representation    of the data that   s a little
   more abstract and higher-level. once again, hubel & wiesel come to the
   rescue with a demonstration:

   iframe: [24]/media/d68f326acaa68c2bc632cd2fd87dc8ce?postid=5f4cd480a60b

   ho-ho-ho, i think we have ourselves another transformation! our new
   grid of cell activations will light up when it sees a line or border,
   and stay quiet otherwise. i think you can see where we are going from
   here         once we have    maps    of different patterns and lines in the
   image, it becomes easier to look for shapes and borders. from there, we
   can begin to identify primitive objects and more complex shapes.
   finally, we can look for and identify complete objects in our vision.
   each transformation builds on the previous representation; it   s hard to
   identify that we are looking at a cat from raw photoreceptor inputs,
   but if we are looking at borders, shapes, and textures it suddenly
   becomes quite clear what we   re looking at.
   [0*3guh3sczoj8w7mqu.]
   another great diagram [25]from wikibooks showing everything i just
   explained and more.

   it   s this hierarchical structure of our visual system that enables us
   to see and identify so many different objects. as you   re about to see,
   the same concept transfers beautifully to neural networks.

architecture of a convolutional neural network

   [1*mlpgop9japw-vb0vhhyv8q.jpeg]
   yann lecun, who first introduced id98s in the late    80s, and lenny at
   icml this past june

   so the problem with using a normal neural network is that it   s
   basically impossible to extract any meaningful features from a bunch of
   pixels directly. to solve this, a id98 uses a hierarchical approach to
   learn feature detectors that are increasingly abstract. for example, a
   id98 might start by looking for lines and borders in our image. from
   there, it can look for basic shapes and curves. oh hey, those curves
   kind of look like an ear and a nose. a face tends to have ears and a
   nose. hey, maybe we   re looking at a person!

   let   s put this all into more concrete terms. if you read the last
   section, you   ll recall that the cells in our retina have a receptive
   field within your field of view         each cell only responds to activity
   in a small portion of your vision. furthermore, recall that each cell
   only activates for a specific pattern within that region. in a
   convolutional neural network, we have a very similar principle         a
   convolutional kernel (or filter) describes an individual pattern, which
   is then applied to every part of our image.

   with id98s, we talk about volumes instead of normal vectors. instead of
   a 1-d vector of numbers that we pass into our network, it   s
   conceptually easier to envision our image as a 100 x 100 x 3 volume of
   numbers (100 pixels wide, 100 pixels tall, and 3 channels [r, g, b]
   deep for the colors).
   [0*zh79crg2czxjwzbk.]
   instead of transforming vectors, a id98 transforms volumes. diagram from
   [26]stanford   s awesome class on id98s.

   we make these transformations using convolutional layers. in a normal
   fully-connected layer, we would have one weight per input for each
   neuron; in a convolutional layer, we instead learn the weights of a
   filter that we apply to every part of our input volume. if our initial
   input volume is 100 x 100 x 3, we might learn a 5 x 5 x 3 filter that
   we apply to each individual part/sub-section of our image.

   what do we mean by    apply   ? quite simply, we take the dot-product of
   our filter with the region of our image. we start with the top-left
   corner, and apply our 5 x 5 x 3 filter to get a single number
   (intuitively, this number represents how strongly that region    matches   
   our filter). we then shift over our filter by 1, and take another
   dot-product. this gives us another value, which we pass through our
   non-linear activation function. we combine all of these dot-products
   into a brand new volume (which we call a    feature map   ).
   [0*8dncm1i2b0_mcqvf.]
   diagram from [27]this quora answer.

   make sense so far? if not, here   s a gif of a 3 x 3 filter in action
   (wait for it to begin animating):
   [1*zcjpufrb6ehpri4eyp6aaa.gif]
   from [28]this stackexchange post.

   a few extra things to note: i just said that we shift over our filter
   by 1 every time we take a dot-product, but we can actually shift it
   over by as little or as much as we want; the number of spots we move
   over is called the stride. we should also discuss the size of our new
   volume we get; intuitively, the size of the new volume will be smaller
   than the input (see the animation above if you need convincing). if we
   don   t want the volume to get smaller with every subsequent
   convolutional layer, we can add a layer of padding around the input
   before we apply our convolutions. for example, if we use a padding of
   two, we will add a two-pixel-wide border of zeros around our input, and
   then do our convolution generate our feature map. we can use this handy
   equation for computing the size of our new output:
   [0*jkt34yw80o2k35u0.]
   we can use this formula to compute the size of our output volume (where
   w is the size of our input volume, f is the size of our filter, p is
   the amount of padding, and s is the stride). deriving this equation
   will be left as an exercise to the reader.

   let   s reiterate: we have our input image, which we   ll say is 100 pixels
   long by 100 pixels wide. each pixel has three numbers associated with
   it: a red channel, a green channel, and a blue channel. we represent
   our image as a three-dimensional volume of numbers (100 x 100 x 3). we
   learn a filter (we   ll explain how to learn the filter in a bit), which
   we apply to every part of our image, giving us a new volume (assuming
   we used the appropriate padding, we   ll assume this new feature map has
   the shape 100 x 100 x 1). this process of applying the same filter all
   over the image is known as a    convolution    (hence the term,
      convolutional neural network   ).

   why does this work? let   s say the filter we learned was great at
   detecting edges in our image. this means that our new    feature map   
   would light up wherever we have a border, essentially giving us an
   outline of what we   re looking at. this kind of representation is far
   more useful to our network than the color of each pixel; it lets us
   extract more meaningful features from our image further down the line.

   that   s basically the gist of it. the last remaining detail is that each
   convolutional layer can actually have multiple filters in it; instead
   of convolving just one filter and creating just one filter map, we
   learn many filters (often as many as hundreds per layer) and apply each
   one to the input volume. this gives us as many feature maps as we have
   many filters, which we can stick together to create a larger volume. so
   if our initial input is 100 x 100 x 3, and our first convolutional
   layer has 192 filters, our output volume will have size 100 x 100 x 192
   (again, assuming that the stride and padding are configured
   appropriately to prevent the output size from shrinking). each filter
   map will give us a different unique perspective on the image (unique
   patterns, lines, borders, etc.), which when combined together give us a
   new representation of the image which is far more meaningful than the
   initial image or any individual feature map.

   the next convolutional layer will take this 100 x 100 x 192 input and
   apply its own set of filters to the image of size n x n x 192 (notice
   that, while the size of the filter can be whatever we want, it extends
   through every feature map from our input volume). each successive
   convolutional layer can build off of the work of the previous one;
   using the borders we found in the first layer, we can find unique
   shapes and curves in the second layer. the third layer can find even
   more abstract and meaningful features. but eventually, we need to start
   reducing the size of our representation as the features we learn span a
   larger and larger region of our image. this is where pooling layers
   come in.

   pooling layers are a way to reduce the size of our volume as it flows
   through our network. we   ll start by looking at one particular kind of
   pooling layer, the max-pooling layer, but the same principle applies to
   related layers, like average-pooling.

   quite simply, our max-pooling layer takes a small subsection (say, 2 x
   2) of each filter map in its input volume, and only takes the largest
   value from that subsection. perhaps this is best explained with a
   picture:
   [0*lmj92hpvmpohyx_s.]

   let   s go back to our 100 x 100 x 192 output of our first convolutional
   layer and apply a 2 x 2 max pooling. our new output will have size 50 x
   50 x 192 (notice that it keeps the number of feature maps the same, but
   reduces the two other dimensions by a factor of 2). our pooling layer
   has two hyperparameters: the size (2 in the previous example) and the
   stride (also 2 in the previous example). we can compute the size of our
   output using the following formula:
   [0*r3vrjqihj0fx8e5m.]
   where w is the size of the input, f is the size of the filter, and s is
   the stride. notice that this is essentially the same as the last
   formula we looked at, except without padding (there   s no need to use
   padding in a pooling layer).

   and that   s it. notice that pooling layers have no parameters (only
   hyperparameters)         they simply apply a function to regions of its
   input. intuitively, you can think about pooling layers as reducing the
      resolution    of our input volume; if we go back to our feature map of
   lines, and apply a pooling operation, we will end up with basically the
   same picture just with fewer    pixels   . this is called down sampling.

   generally speaking, the stride and size of a pooling layer are the
   same, but there   s nothing stopping you from creating an    overlapping   
   pooling layer (as we   ll see in the case study that follows) where the
   size is greater than the stride.

   convolutional and pooling layers make up the bulk of most id98s, but we
   still need to somehow come up with a id203 distribution over
   classes. we can do this with our normal fully-connected layers we are
   so used to seeing in regular neural networks. our last convolutional or
   pooling layer gives us one last output volume, with very high-level and
   abstract features (ears, eyes, leaves, etc.). if we connect a
   fully-connected layer to each value in our volume, we can train one
   neuron per class (with a softmax stuck on the end to convert it into a
   id203 distribution). if we want an extra little bit of
   representational power, we can stick on some extra fully-connected
   layers before our final output layer.

   so now, how do we actually learn or define these filters and their
   specifically weights/biases? our objective is to find filters that
   minimize error/cost and in doing so maximize the percentage of
   correctly classified images. we of course use the id26
   algorithm to compute our derivatives and then apply a first-order or
   second-order optimization algorithm like stochastic id119 or
   l-bfgs respectively. the cost function we use can be any typical one,
   for example cross-id178 (or    logistic   ) loss.

   i   ll wrap this up with a little note on id21 (which
   applies to neural networks in general, but is especially common when
   training id98 models). you need a lot of data to train one of these
   networks successfully, and depending on the problem you   re solving,
   there isn   t always all that much data to go around. data augmentation
   can help, but nothing beats a ginormous dataset with hours of gpu
   training time. but because of the nature of pictures, a lot of the
   features that id98s are learn are fairly generic; the lines, patterns,
   textures, and so on that a network learns to look for can often be
   found in most images, even in categories it may not have been
   explicitly trained on. for this reason, it   s common to start training
   with a popular architecture and an already-trained model; if you freeze
   the weights of the convolutional layers and just train some
   fully-connected layers on top of them, you can learn to recognize an
   entirely different distribution of objects with far fewer images.

   and that   s it! not so bad, right?

why this works (briefly)

   let   s take a quick look at why convnets work and what they really do.
   we briefly mentioned before that they can learn weights for feature
   detectors that are increasingly abstract; perhaps they first start by
   detecting the edges, lines, and corners in parts of an image via the
   initial convolution layers and then proceed to, in the later
   convolution layers, recognize things like noses, eyes, bike handles
   (just a random thought) that can then be used to figure out exactly
   what the object is.

   the fully connected layer(s) at the very end let us combine these
   individual features and figure out what exactly we   re looking at.
   another approach, as mentioned before, would have been to use a normal
   neural network with hand-crafted featured as inputs, instead of the
   convolutional layers. obviously, finding and computing said features is
   extremely difficult, very time consuming, or simply not possible. id98s
   fix this by finding effective feature detectors themselves. the point
   of using id98s is that by training these convolution layers they can
   create their own feature detectors (ie. learning that seeing a bike
   handle might be a good way of figuring out that we   re looking at a
   bike) that can then be applied to a fully connected ann-like layer at
   the end.

   a really cool way to demonstrate this is to extract the feature maps
   from each of the hidden layers and treat them as pixels that we can
   visually interpret. you can   t see the actual contents of the following
   gif up close but this is a good visualization of said feature maps from
   the original image and how they lead us to the end classification:
   [1*s1whbmr7wln4dviztljq5w.gif]
   from id98 genius andrej karpathy   s [29]blog

   if we look at how the feature maps change as we go deeper and deeper
   into the network, we see that they quickly shift from easily
   interpretable (picking out basic textures and borders) to much less so
   (where each feature map might look for the presence of eyes or ears,
   for example).
   [1*70p5spupjb0emmjd9peena.png]
   from the same [30]blog

   however, if we look at the actual features that each feature map is
   looking for, we find that as we go further into our network they
   quickly transition from low-level borders and shapes to higher-level
   features and unique facial characteristics.
   [1*z-hcwgecwmbbkh346bet9w.png]
   [31]https://deeplearning4j.org/neuralnet-overview

   ultimately, this is the point of the filter. instead of looking at the
   image as a whole, we can look for low-level features         edges, corners,
   lines, borders         in a small region of the input. from these low-level
   features, we can    build up    and look for slightly more complex features
   in a slightly larger region of the image. that   s why convnets work   the
   spatial convolution let us pick out small features and then work our
   way up to bigger ones. once we have these greater abstractions it
   becomes pretty easy to classify.

   id98s are undoubtedly sort of automagical and black box (like a lot of
   deep learning) but they work really well. i highly encourage (in fact   i
   mandate!) that you watch the following youtube video. it should make
   this explanation very intuitive.

   iframe: [32]/media/940e547918623607840a28cb6f2d75a5?postid=5f4cd480a60b

   alright   now, let   s take a look at some of the datasets and id98s that
   are used in practice!

case study

id163

   ok, so now we   re gonna take a look at an example. that is, how have
   some of the most successful id98s been architected? how does one even
   get the data for training id98s, and what are the kind of results that
   modern day networks achieve?

   to answer these questions, we   ll take a look at id163. what is
   id163? according to the [33]website:

        id163 is an image database organized according to the id138
     hierarchy (currently only the nouns), in which each node of the
     hierarchy is depicted by hundreds and thousands of images. currently
     we have an average of over five hundred images per node. we hope
     id163 will become a useful resource for researchers, educators,
     students and all of you who share our passion for pictures.   

   basically, id163 is an open source (free) database of images with
   words/captions associated to them. obviously, if you want to perform
   image classification (where you take an image and spit out the
   caption[s]/words associated to the main object[s] in the image), this
   seems like the perfect resource. in fact, as of 2016 (id163 began in
   2010), there are over 15 million high-resolution, labeled
   images         meaning paired with crowd-sourced hand-annotated (yes,
   hand-annotated!) captions         from 22,000 different categories. now
   that   s a lot of training data for any sort of id98 out there!

   id163 was started by students/faculty at stanford university and the
   stanford vision lab to collect data for id161
   algorithms         id98s obviously included. the project continues to grow in
   an effort to help grow the field of id161 and increase

   since inception, id163 has hosted an annual competition called the
   id163 large scale visual recognition challenge (ilsvcr for short but
   i   m not sure who could memorize that acronym). in said competition,
   research teams (from institutions and companies) submit programs that
   perform object classification and detection on scenes/images from the
   id163 database. in 2010, a good classification error hovered around
   25%. in 2012, the first year that a convolutional neural network won
   the competition, that almost halved to 16% (id98s have dominated the
   leaderboard ever since). now, the error rates are in the single digits,
   and researchers have reported that these error rates are in fact lower
   than human error rates! of course, in context this means little since
   humans are still the best at contextual reasoning, not to mention the
   fact that we can recognize a lot more classes than id163 has. but
   it   s still impressive.

   in ilsvcr, models are trained on 1.2 million images in 1000 categories
   (hence the    large scale    part of the name). the task is to make 5
   guesses, sorted by confidence/id203, about the potential label of
   input images:
   [0*dtork1p9y-c_v01r.]
   from
   [34]http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexne
   t_tugce_kyunghee.pdf

   you may see two separate measures of error:    top-1 error    and    top-5
   error   . top-1 error occurs if the actual object in an image is not the
   most probable outcome class that the id98 produces. top-5 error occurs
   if the actual object in an image is not in the five most probable
   outcome classes that the id98 produces.

   fun fact: in 2015 baidu was caught cheating in the competition.

alexnet

   alexnet was a deep id98 model created by alex krizhevsky (a student at
   utoronto) that, as mentioned earlier, took the error percent rate from
   the mid 20s to the mid 10s (which is a huge deal) in 2012   s ilsvcr.
   alexnet, however, isn   t just any vanilla id98 you can throw together in
   2 hours of hacking         there are many features that distinguish it from a
   typical convolutional neural network and i   ll talk about them in this
   section.

   some high-level overview about alexnet   s architecture:
     * it has 7 hidden layers (9 layers with input and output)
     * 650,000 neurons in total
     * 60 million parameters in total
     * 630 million connections in total

   some more info:
     * relus are used as the activation function (rectified linear
       units)         make sure you know what this is*
     * it uses overlapping pooling layers
     * dropout is used to promote weight sharing and prevent overfitting
     * randomly extracted 224 x 224 patches for more data

   *ok but if you really forgot what they are (and haven   t checked out our
      vanishing gradient problem    article) then it   s a non-saturating (that
   is, the gradients don   t die off) and non-linear activation function.
   the equation is relu(x) = max(0, x). if nothing you just read makes
   sense, i guess that   s a prompt to go read the other article! relu is
   helpful; a 4 layer id98 with relus can converge up to six times faster
   than an equivalent network with logistic/tanh neurons on the cifar-10
   dataset. relu layers exist independently of convolutional layers.

   a visual representation of the entire architecture follows:
   [0*uvfn0nvxcjk4dsjs.]
   from
   [35]http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexne
   t_tugce_kyunghee.pdf

   there are 5 convolutional layers after the input, followed by 3
   fully-connected layers (that is, just a regular ann), and finally a
   1000-way softmax for the output (to give us a id203 for each of
   the 1000 classes in the 2012 challenge). if we fed in the leopard image
   above, we   d hope that the id98 would have the    leopard    class with the
   highest output id203.

   now, let   s look at each individual layer in the id98 and their
   features/role:
   [0*hfrjwzyihxmgukmz.]
   from
   [36]http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexne
   t_tugce_kyunghee.pdf

   this is the first convolutional and hidden layer. in this layer, the
   convolutional filter convolves around the input image volume of size
   227 x 227 x 3 with 96 11 x 11 x 3 filters with a stride of 4 and no
   padding. using the equation we brought up earlier, we can figure out
   the size of our output volume: (227   11)/4 + 1 = 55. because we have 96
   filters, the output of the layer is ultimately 55 x 55 x 96. so, the
   first layer of alexnet convolves around a 277 x 227 x 3 input image and
   outputs a 55 x 55 x 96 tensor as input for the next convolutional
   layer. here   s a nice definition of a    tensor    from google, by the way:

     a mathematical object analogous to but more general than a vector,
     represented by an array of components that are functions of the
     coordinates of a space.

   training for alexnet also occurred on multiple (well, two) gpus, but
   not much explanation needed here. the arrows that point to inter-gpu
   connections show connections that are being trained across different
   gpus and intra-gpu connections show the ones being trained on the same
   gpu.
   [0*0gfxozalstnfyw6x.]
   from
   [37]http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexne
   t_tugce_kyunghee.pdf

   before we looked at pooling layers in id98s. alexnet does use
   pooling         in particular it uses max pooling with overlapping. according
   to krizhevsky, overlapping pooling slightly reduces error rates and
   overfitting as well (by about 0.4%.)
   [0*4c-xhkqyqcq3d_rm.]
   from
   [38]http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexne
   t_tugce_kyunghee.pdf

   now let   s take a look at an overview of the full architecture of
   alexnet:
   [0*wbuezmsizj-pqk-3.]
   from
   [39]http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexne
   t_tugce_kyunghee.pdf

   we start with the previously described convolutional layer. next comes
   a id172 layer (known as a local response id172         this
   isn   t used all that much anymore, so we won   t go into much detail
   here). next comes a 2 x 2 max pooling layer. this block is repeated.
   following that, we have three convolutional layers of smaller filter
   size (now 3 x 3 instead of 11 x 11) and then another max pooling layer.
   then, we have two fully connected relu layers. lastly, we have a fully
   connected softmax layer that outputs 1000 values (which is the
   id203 distribution over the 1000 classes) from the down sampled,
   convolved, and basically broken down representation of the image.

   whew, that was a pretty long case study. alexnet is some good stuff.
   there are some small extra techniques employed, mostly related to
   preventing overfitting, but i won   t go into it         it isn   t necessary at
   this point. dropout, data augmentation, and modifications to the
   softmax algorithm are the notable mentions here. finally, stochastic
   id119 with momentum (don   t worry if you don   t know what this
   is, but i think it   s explained in our ann article) was used as the
   optimization algorithm.

   ok, now for some fun stats:
     * alexnet took 5   6 days to train
     * it was trained on two nvidia gtx 580 3gb gpus
     * a top-5 test error of 15.3% was achieved (obviously 2015   s
       competition had better results since this was 2012, but this was a
       big deal back then)

   ok, now for some more fun: example classification results! this is
   actually really cool.
   [0*bgvvpraash_xjpuk.]
   from
   [40]http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexne
   t_tugce_kyunghee.pdf

   the top shows the actual image, in the middle shows the object present
   in the image, and the bottom shows the top-5 most probable objects that
   the algorithm has classified. the pink/red bar shows shows the class in
   the prediction/classification that corresponds with the actual label.

   and that   s alexnet!

implementing a id98 in tensorflow

   if you   re thinking:    rohan, dude, dope, id98s sound like the bomb, but
   how do i actually make them?   

   first of all, i agree with you. id98s are the bomb. as for making them,
   lemme show you. we   ll be, in particular, using [41]tensorflow, a
   popular    open source software library for machine intelligence    created
   and maintained by google.

   the following code, in python, demonstrates the implementation of id98s
   with tensorflow (obviously make sure to install tensorflow first,
   instructions are on their site). the id98 we   ll be making is
   specifically being applied to mnist: a dataset of handwritten numbers
   and their corresponding label. thus, instead of object classification
   we are classifying a number (as a result there are much fewer classes;
   just 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9) based on an image of said
   letter/number. this is useful for ocr.

   the code is taken from tensorflow   s official tutorial on deep mnist
   implementation. we   re going to build a id98 with the following
   architecture:
    1. convolutional layer
    2. relu layer
    3. pooling layer
    4. convolutional layer
    5. relu layer
    6. pooling layer
    7. relu fully connected layer
    8. softmax fully connected layer

   we also use dropout for id173.

   the following code downloads the mnist data:
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(   mnist_data   , one_hot=true)

   the next 2 lines of code sets up a tensorflow session:
import tensorflow as tf
sess = tf.interactivesession()

   the first line of code is easy to understand: we just import tensorflow
   as    tf   . then we create a new    interactivesession   . sounds fancy, but
   what is it? well, this concept is perhaps at the core of tensorflow.
   it   s the idea of representing computations as graphs. nodes in said
   graphs are called ops (short for operations), and each op takes zero or
   more tensors, performs some computation, and produces zero or more
   tensors. these    tensors    are typed multi-dimensional arrays   or simply
   put, mathematical tensors   the dimensionality of which we will look at
   soon.

   a lot of that description was pulled from tensorflow   s site, however
   i   m gonna make it simpler for you here. after all, that   s one of the
   goals of this blog.

   in tensorflow, mathematical operations that you perform on tensors
   aren   t actually performed. instead, they   re added to a computational
   graph that defines what kinds of computations we are doing. so we first
   build our network by defining all of our operations (which are the
   convolutional neural network layers eg. convolutional/pooling/relu
   etc.), and that creates our graph. then, when we   re ready to use our
   network, we tell tensorflow to run the graph that we   ve created, giving
   it actual values to plug in to the tensors and letting it compute the
   rest. it   s pretty dope, in my opinion! but, if it doesn   t quite make
   sense yet don   t worry, you   ll see what i mean as we keep going.

   ok, so now we   re going to define two functions to help us out:
def weight_variable(shape):
 initial = tf.truncated_normal(shape, stddev=0.1)
 return tf.variable(initial)
def bias_variable(shape):
 initial = tf.constant(0.1, shape=shape)
 return tf.variable(initial)

   the first function initializes weights based on a given
   shape/dimension. obviously, to set up a id98 we   re gonna need a lot of
   weights/biases. generally, we don   t set these values to zero but
   instead add a small amount of noise to prevent symmetry and premature
   convergence/high error on neural networks: that   s exactly what the
   truncated_normal function does, and we specify a standard deviation of
   0.1 to the normal distribution.

   we put our weights into a tensorflow variable. remember that tensorflow
   doesn   t deal with values directly; it deals with    operations    on a
      computational graph   . a variable is just a special operation that
   doesn   t take any inputs and returns the value that sits inside the
   variable. the variable won   t actually have a value inside of it until
   we run the graph.

   the second function creates a bias variable, again based on a given
   shape. we initialize it to a constant value of .1 (relu cuts off any
   activations <0, so giving it a slight positive bias prevents dead
   neurons).

   we   ll now write functions to create reusable layers   that is, the
   convolutional and max pooling layers. the code follows for the
   convolutional layer:
def conv2d(x, w):
 return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding=   same   )

   this function, aptly titled    conv2d   , creates a 2d convolutional layer.
   a 2d convolutional layer uses a 2d filter, which is a filter that
   convolves in the horizontal and vertical direction/dimension of an
   image. note that we do not convolve in the depth/z-axis of an image
   (the channels).

   the tf.nn.conv2d function takes as input a 4-d input tensor (which
   corresponds to variable x, which will be the input of the layer). wait   
   but why 4-d?    i thought you said images were 3d volumes?   , you   re
   probably thinking. i hear you. worry not. basically, when we train a
   convolutional neural network, we want to train it on a batch of images.
   you don   t just wanna train it on one image/corresponding label at a
   time, just like a (properly vectorized) id75 algorithm
   wouldn   t train on one data point at once. thus, we collect a batch of
   3d tensors and put it into a 4d tensor. obviously, this means that all
   images must be of the same dimension. the dimension of the entire input
   tensor is now: number_of_images x standard_image_width x
   standard_image_height x standard_image_channel. the 2d convolution
   layer still only operates/convolves on the height and width.

   ok, cool.

   the w parameter corresponds to our convolving filter         these are of
   course, weights. they are tensorflow variables that can be generated
   using the function we wrote earlier on. now, the strides argument
   (which we set explicitly to an array of four 1s) is sort of confusing:
   isn   t there just    one stride? maybe two? but why four? ok, so here   s
   what   s up: since we have a four dimensional tensor input, we have a
   four dimensional stride input that corresponds to the stride value used
   for each axis. (yes, you can technically have a different stride for
   each dimension, although i   m not quite sure why one would want that.
   but hey, i   m not here to judge.) because this is a 2d convolution,
   tensorflow actually requires that the first and last strides,
   corresponding to the batch dimension and the channel dimension, be set
   to 1.

   therefore, strides[1] and strides[2] can just be set to the stride
   values for the x and y axis. why tensorflow has you input an array of
   four values, only to literally force two of those values to be 1,
   confuses me as well. don   t worry.

   we set the stride for the x and y axis to just be one. padding=   same   
   is sort of confusing as well. basically, this is the argument we pass
   to specify the zero padding for the convolutional layer. with
   tensorflow, you can pass one of two arguments:
     * same
     * valid

   again, this confuses the hell out of me. same is the most common one,
   apparently. a more thorough exploration into the differences between
   these not so aptly titled arguments are available on [42]one of their
   documentation pages.

   the code follows for the max pooling layer:
def max_pool_2x2(x):
 return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
 strides=[1, 2, 2, 1], padding=   same   )

   no need to spend too much time on this. we pass the input x, and the
   ksize parameter (once again we set the first and fourth value to 1) is
   the size of each max pooling sub-region (2 x 2 here), whilst we use
   stride of value 2. we use the same padding as before.

   at this point we   ve defined all the functions we need to, and we can
   construct our neural network in our computation graph. but before that,
   we need to write the code to create placeholders for our training set
   data values:
x = tf.placeholder(tf.float32, shape=[none, 784])
y_ = tf.placeholder(tf.float32, shape=[none, 10])

   remember, these are not specific values (therefore they are
   placeholders). we need a way to feed in our values when tensorflow
   starts executing our computation graph; we do this with a placeholder.
   when we actually run the graph (and we   ll see how in a bit), we can
   specify which values we want these placeholders to have.

   the input x is a 2d tensor of floating point numbers, with the
   dimension [none, 784]. 784 is the number of pixels in a single
   flattened mnist image. tensorflow gives us these images as a 1d vector,
   but we will reshape them back into a square before we give them to our
   id98. another little thing to note is that mnist images are in
   grayscale, so it   s 2d rather than 3d because the channels dimension is
   just 1. none means that the first dimension can be of any size. in our
   case, that dimension corresponds to the batch size we use.

   y_, the labels or known output of the training data, is a    one-hot
   vector    of 10 values. what does a one-hot vector mean? it   s actually
   really simple; one-hot vectors         aptly titled by the way, which y   all
   know i really appreciate         are just vectors that contain the value of 1
   in any given single position and the value of 0 in every other
   position. there are 10 values because there are 10 possible output
   classes of the id98 with mnist: 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. when
   it comes to the output of the network, there will a
   id203/confidence distribution over each of these classes.
   however, the output of the input training set data is    certain, so one
   of the values will be 1 and the rest will be zero.

   let   s get on to creating our first convolutional layer! we   ll first
   create the weights and biases (the convolutional filter) for this
   layer.
w_conv1 = weight_variable([5, 5, 1, 32])
b_conv1 = bias_variable([32])

   we use the functions we   ve already defined to create these weights/the
   bias. the weight tensor has a shape of [5, 5, 1, 32] because our filter
   is of size 5 x 5 (pretty standard). we only have one channel for
   grayscale and it   s a 2d convolution. the final value, 32, is the number
   of output channels that we have         the number of features or    feature
   maps    that will be produced from this convolution. basically, a 5 x 5 x
   32 tensor will be outputted from this convolutional layer that will be
   inputted to the next one. we also create our bias variable with a
   component for each output channel.

   next, we take our tensor of n x 784 flattened image vectors and reshape
   them back to their original 28 x 28 dimensions:
x_image = tf.reshape(x, [-1,28,28,1])

   this is again important because id98s can exploit and learn from the
   structure of images. the -1 here just means it does not change, because
   we retain the batch size.

   we now convolve x_image in 2d with our weights and add our biases using
   the function we created earlier:
h_conv1 = conv2d(x_image, w_conv1) + b_conv1

   following this, we apply the relu activation function and then do a 2 x
   2 max pooling operation:
h_conv1 = tf.nn.relu(h_conv1)
h_pool1 = max_pool_2x2(h_conv1)

   that   s the first layer (or first three, if you think of relu/pooling as
   separate) done! let   s take a quick look at the size of our current
   image. before, it was 28 x 28. however, with our convolution and
   pooling, it should be down sampled. we apply the following equation
   that was explored before:
   [0*jkt34yw80o2k35u0.]

   where w is the dimension of the original image, f is the filter size, p
   is the padding size, and s is the stride value. we sub in:
     * w = 28
     * f = 5
     * p = 2
     * s = 1

   to get a value of 28. that   s right         the image size doesn   t change.
   however, we   re forgetting that we also need to apply the pooling
   equation:
   [0*r3vrjqihj0fx8e5m.]

   where w is the original image size, f is the pooling filter size, and s
   is the stride value. we sub in, as per the code in our functions:
     * w = 28
     * f = 2
     * s = 2

   now we get to an image dimension of 14. so, after the first
   convolution/pooling layer, our image has been down sampled to 14 x 14.

   next we have a second convolutional layer that, instead of 32, now
   creates 64 features for each 5 x 5 filter and then applies relu/max
   pooling.
w_conv2 = weight_variable([5, 5, 32, 64])
b_conv2 = bias_variable([64])
h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2)

   our image is now down sampled to 7 x 7 (if you repeat the same
   calculations as before). since we have 64 output channels, we now have
   7*7*64 = 3136 values. next, we have a fully connected layer with 1024
   neurons. that is, 3136 output values are flattened and then directly
   connected to 1024 neurons. the number 1024 is arbitrary and has been
   chosen based on positive empirical results.

   we first create the weights/biases:
w_fc1 = weight_variable([7 * 7 * 64, 1024])
b_fc1 = bias_variable([1024])

   then we take the output of our previous layer (the pooling layer) and
   reshape (flatten here) it into a batch of vectors of 3136 values.
h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])

   following this, we perform the fully-connected layer operation of
   multiplying our values by the weight matrix (   matmul    stands for matrix
   multiplication) and then adding the biases. this is like machine
   learning 101     
h_fc1 = tf.matmul(h_pool2_flat, w_fc1) + b_fc1

   finally, we apply relu:
h_fc1 = tf.nn.relu(h_fc1)

   to reduce overfitting, we apply this technique called    dropout   . no
   worries if you don   t know what it is, very little code is required to
   implement it. on a high level, dropout randomly turns off some of the
   neurons while training to increase the independence of each of them
   individually and reduce dependence on each other and temper the
   influence of a given neuron. in general, this enables a neural network
   to generalize better because several independent representations of
   patterns can be learned. the variable keep_prob in the following code
   is an aptly-titled value that states the id203 of any given
   neuron being kept/left on and not turned off. we   ll explicitly set that
   value when we run the interactive session.
keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

   we   re almost done! finally, we   ll add a fully-connected softmax layer
   that can turn the output from 1024 neurons into a id203
   distribution over the 10 classes of number characters.

   we create the weights as per usual:
w_fc2 = weight_variable([1024, 10])
b_fc2 = bias_variable([10])

   now we create y_conv, our final output that performs a fully-connected
   operation on the dropout and following that a softmax.
y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)

   cool! you   ve just constructed your very first convolutional neural
   network. you   re not just a builder         you   re an architect!
   [1*gpa1qxrzgl2lqf4ozxqzag.png]
   [43]http://www.wikihow.com/become-an-architect

   but we   re not done yet. now let   s go ahead and train/evaluate our
   model, and learn how well it performs.
cross_id178 = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_ind
ices=[1]))

   the line of code above is essentially our cost function, where we aim
   to make y_ (our known output) and y_conv (out estimated/predicted
   output) as similar as possible.
train_step = tf.train.adamoptimizer(1e-4).minimize(cross_id178)

   this line of code defines an iteration of optimization (using the
   first-order optimization method called adam) with a step size of 0.0001
   by minimizing the output of the cost function we just defined.
correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))

   this is a vector of booleans where 1 indicates a correct prediction
   (that is, for that image in the batch y_conv and y_ both chose the same
   class as the output number character class.) y_conv is continuous
   rather than discrete like y_, but the function argmax returns the index
   with the highest value (exactly 1 for y_ and closest to 1 for y_conv)
   so that   s not an issue here. we don   t check if the values are exactly
   equal.
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

   this line of code just gets our overall accuracy by calculating the
   mean of our predictions.

   ok. you ready for this? let   s finally run our session!
sess.run(tf.initialize_all_variables())

   now let   s input our data:
for i in xrange(20000):
 batch = mnist.train.next_batch(50)
if i % 100 == 0:
 train_accuracy = accuracy.eval(feed_dict={
 x:batch[0], y_: batch[1], keep_prob: 1.0})
 print(   step %d, training accuracy %g   %(i, train_accuracy))
train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

   20,000 iterations in the for loop is basically the batch size (which we
   have set to 50) multiplied by the number of steps of the
   training/optimization method (which we have set to 400). you can change
   these numbers as you wish.

   in each iteration, we get our next batch (remember we are training in
   batches) and then proceed with our training/optimization step using
   train_step. we pass to train_step the argument feed_dict, which
   contains the 50 x input images, 50 y_ known output classes, and the
   keep_prob for dropout which is 0.50.

   also, every 100 overall iterations (that   s the i % 100 == 0
   if-statement), we log the current training accuracy for the batch
   (which would be every 2 training/optimization steps for each individual
   batch since 100 / 2 = 50) and then print that out. this helps us keep
   track of how the accuracy is changing over time, but we don   t want to
   spam by calculating and outputting the accuracy every iteration because
   it   s unnecessary and would slow things down a lot.

   finally, once we have finished optimizing on our training set, we go
   ahead and log our test set accuracy:
print(   test accuracy %g    % accuracy.eval(feed_dict={
 x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))

   you should get up to 99.2% accuracy if you leave it running for a while
   (and if you   re okay with your fans goin    crazy!). hope you had fun with
   that. the theory and math behind id98s and machine learning in general
   is great, but implementing and getting your own machine to learn is
   even cooler     

   also, for this small id98, performance is nearly identical regardless of
   whether dropout is used. dropout is really good at reducing
   overfitting, but it   s more relevant and effective with very large
   industrial-setting networks.

   tip: if you get an error about some utf-8 locale stuff, enter this into
   your command line/terminal (not python):
export lc_all=c

   [44]here   s all the code pieced together.

conclusion

   yea, so that was probably a long read! hope you learned something about
   convolutional neural networks and that they   re not magic but instead
   just a really cool and ingenious advancement in id161.

   to conclude, here   s an image of a cute cat that has been painted over
   in a really dream-like/psychedelic fashion by a id98. it   s done by
   google   s [45]deepdream program. maybe this is something we can talk
   about in the future?
   [1*02xtjkduyytxqyverjzbcg.jpeg]
   [46]https://www.pinterest.com/pin/392939136217441282/

   til    next time, when we dive head first into the recurrent neural
   network!     

     * [47]machine learning
     * [48]artificial intelligence
     * [49]id161
     * [50]neural networks
     * [51]algorithms

   (button)
   (button)
   (button) 415 claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [52]go to the profile of lenny khazan

[53]lenny khazan

   tinkering with machine learning. [54]http://getcontra.com/

     (button) follow
   [55]a year of artificial intelligence

[56]a year of artificial intelligence

   our ongoing effort to make the mathematics, science, linguistics, and
   philosophy of artificial intelligence fun and simple.

     * (button)
       (button) 415
     * (button)
     *
     *

   [57]a year of artificial intelligence
   never miss a story from a year of artificial intelligence, when you
   sign up for medium. [58]learn more
   never miss a story from a year of artificial intelligence
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://ayearofai.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5f4cd480a60b
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b&source=--------------------------nav_reg&operation=register
   8. https://ayearofai.com/?source=logo-lo_iwkehefe6bdd---bb87da25612c
   9. https://ayearofai.com/tagged/algorithms
  10. https://ayearofai.com/tagged/today-i-learned
  11. https://ayearofai.com/tagged/case-studies
  12. https://ayearofai.com/tagged/philosophical
  13. https://ayearofai.com/tagged/meta
  14. https://www.wired.com/wp-content/uploads/2015/01/id98-visualization-crop.jpg
  15. https://ayearofai.com/@lennykhazan?source=post_header_lockup
  16. https://ayearofai.com/@lennykhazan
  17. https://medium.com/@lennykhazan
  18. https://medium.com/@mckapur
  19. https://medium.com/a-year-of-artificial-intelligence
  20. https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5
  21. https://ayearofai.com/rohan-lenny-1-neural-networks-the-id26-algorithm-explained-abf4609d4f9d#.quwnoqtot
  22. https://en.wikibooks.org/wiki/sensory_systems/visual_system#/media/file:receptive_field.png
  23. https://ayearofai.com/media/fe3ecbdf357f89d57998ab6eb405e50a?postid=5f4cd480a60b
  24. https://ayearofai.com/media/d68f326acaa68c2bc632cd2fd87dc8ce?postid=5f4cd480a60b
  25. https://en.wikibooks.org/wiki/sensory_systems/visual_system#/media/file:wiki_hierarchies.png
  26. http://cs231n.github.io/convolutional-networks/
  27. https://www.quora.com/what-is-meant-by-feature-maps-in-convolutional-neural-networks
  28. http://stats.stackexchange.com/a/116390/98975
  29. http://karpathy.github.io/2015/10/25/selfie/
  30. http://karpathy.github.io/2015/10/25/selfie/
  31. https://deeplearning4j.org/neuralnet-overview
  32. https://ayearofai.com/media/940e547918623607840a28cb6f2d75a5?postid=5f4cd480a60b
  33. http://image-net.org/
  34. http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf
  35. http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf
  36. http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf
  37. http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf
  38. http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf
  39. http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf
  40. http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf
  41. https://www.tensorflow.org/
  42. https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#convolution
  43. http://www.wikihow.com/become-an-architect
  44. http://pastebin.com/p77lqhyc
  45. https://en.wikipedia.org/wiki/deepdream
  46. https://www.pinterest.com/pin/392939136217441282/
  47. https://ayearofai.com/tagged/machine-learning?source=post
  48. https://ayearofai.com/tagged/artificial-intelligence?source=post
  49. https://ayearofai.com/tagged/computer-vision?source=post
  50. https://ayearofai.com/tagged/neural-networks?source=post
  51. https://ayearofai.com/tagged/algorithms?source=post
  52. https://ayearofai.com/@lennykhazan?source=footer_card
  53. https://ayearofai.com/@lennykhazan
  54. http://getcontra.com/
  55. https://ayearofai.com/?source=footer_card
  56. https://ayearofai.com/?source=footer_card
  57. https://ayearofai.com/
  58. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  60. https://medium.com/p/5f4cd480a60b/share/twitter
  61. https://medium.com/p/5f4cd480a60b/share/facebook
  62. https://medium.com/p/5f4cd480a60b/share/twitter
  63. https://medium.com/p/5f4cd480a60b/share/facebook
