   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]a year of artificial intelligence
     * [9]algorithms
     * [10]today i learned
     * [11]case studies
     * [12]philosophical
     * [13]meta
     __________________________________________________________________

   [1*khikl9t4xmzgsskhw_yg2w.png]
   sequences upon sequences upon sequences. sequen-ception.

rohan & lenny #3: recurrent neural networks & lstms

the ultimate guide to machine learning   s favorite child.

   [14]go to the profile of rohan kapur
   [15]rohan kapur (button) blockedunblock (button) followfollowing
   apr 13, 2017
     __________________________________________________________________

     this is the third group ([16]lenny and [17]rohan) entry in our
     [18]journey to extend our knowledge of artificial intelligence and
     convey that knowledge in a simple, fun, and accessible manner. learn
     more about our motives in this [19]introduction post.
     __________________________________________________________________

   it seems like most of our posts on this blog start with    we   re back!   ,
   so    you know the drill. it   s been a while since our last post         just
   over 5 months         but it certainly doesn   t feel that way. whether our
   articles are more spaced out than we   d like them to be, well, we
   haven   t actually discussed that yet. but i, rohan, would definitely
   like to get into a more frequent routine. since november, we   ve been
   grinding on school (basically, getting it over and done with), banging
   out [20]contra v2, and lazing around more than we should. end of senior
   year is a fun time.
   [1*n5l-uj5yuvpidwiuguvnfg.jpeg]

   it   s 2017. we started a year of ai in 2016. last year. don   t panic,
   though. if you   ve read our [21]letter, you   ll know that, despite our
   name and inception date, we   re not going anywhere anytime soon. there   s
   a good chance we   ll move off medium, but we   re still both obsessed with
   ai and writing these posts to hopefully make other people obsessed, as
   well.

   i wrote the first article on this blog just over a year ago, and
   mentioned that my goal for the year was to be accepted into stanford
   university as an undergrad student. a few months ago, i achieved this
   goal. at stanford, i   ll probably be studying [22]symbolic systems,
   which is a program that explores both the humanities and stem to inform
   an understanding of artificial intelligence and the nature of minds.
   needless to say, a year of ai will continue to document the new things
   i learn     .

   anyways, you can find plenty of articles on recurrent neural networks
   (id56s) online. my [23]favorite one, personally, is from andrej
   karpathy   s blog. i read it about 1.5 years ago when i was learning
   about id56s. we definitely think there   s space to simplify the topic
   even more, though. as usual, that   s our aim for the article         to teach
   you id56s in a fun, simple manner. we   re also importantly doing this for
   completion purposes; we want people to hop onto a year of ai and be
   able to work their way up all the way from id28 to
   id4 (don   t worry, you   ll find out what means
   soon enough), and thus recurrent neural networks is a vital addition.
   after this, we want to look at and summarize/simplify a bunch of new
   super interesting research papers, and for most of them id56s are a key
   ingredient. finally, we think this article contains so much meat and
   ties together content unlike any other id56 tutorial on the interwebs.

   before we get started, you should try to familiarize yourself with
      vanilla    neural networks. if you need a refresher, check out our
   [24]neural networks and backpropogation mega-post from earlier this
   year. this is so you know the basics of machine learning, linear
   algebra, neural network architecture, cost functions, optimization
   methods, training/test sets, id180/what they do,
   softmax, etc. reading our article on [25]convolutional neural networks
   may also make you more comfortable entering this post, especially
   because we often reference id98s. checking out [26]this article i wrote
   on vanishing gradients will help later on, as well.

   rule of thumb: the more you know, the better!

table of contents

   i can   t link to each section, but here   s what we cover in this article
   (save the intro and conclusion):
    1. what can id56s do? where we look at    what id56s can do!
    2. why? where we talk about the gap that id56s fill in machine
       learning   s suite of algorithms.
    3. show me. where we visualize id56s for the first time.
    4. formalism. where we walk through how an id56 mathematically works
       with proper notation.
    5. an example? okay! where we walk through, qualitatively, a simple
       application of id56s and how the id56 operates in this application,
       including techniques we can use.
    6. training (or, why vanilla id56s suck.) where we talk about how to
       train id56s, and why vanilla id56s are bad at learning.
    7. fixing the problem with lstms (part i). where we introduce the
       solution to vanilla id56s    inability to learn: lstms.
    8. fixing the problem with lstms (part ii). where we analyze on a
       close, technical level, the reasons lstms don   t suffer from
       vanishing gradients as much (and why they still do, to an extent).
       then we conclude lstms with final thoughts on and facts about them.
    9. yay id56s! where you get to see neat little things id56s have done!
   10. in practice. where we look at more technical and important
       applications and case studies of id56s, including other variations
       of id56s, especially as relevant in hot/recent research papers.
   11. building a vanilla recurrent neural network. where you get to code
       your very first id56! woohoo!

what can id56s do?

   there are a number of very important tasks that anns and id98s cannot
   solve, that id56s are used for instead. tasks like: image captioning,
   language translation, sentiment classification, predictive typing,
   video classification, natural language processing, id103,
   and a lot more interesting things that have been presented in recent
   research papers (for example    [27]learning to learn by id119
   by id119!).
   [1*x5dk-xgw2ynyseb3qvhwia.png]
   image captioning, taken from cs231n slides:
   [28]http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf

   id56s are very powerful. y   know how regular neural networks have been
   proved to be    universal function approximators    ? if you didn   t:

     in the mathematical theory of id158s, the
     universal approximation theorem states that a feed-forward network
     with a single hidden layer containing a finite number of neurons can
     approximate continuous functions on compact subsets of r^n, under
     mild assumptions on the activation function.

   that   s pretty confusing. basically, what this states is that an
   id158 can compute any function. even if someone
   gives you an extremely wiggly, complex looking function, it   s
   guaranteed that there exists a neural network that can produce (or at
   least extremely closely approximate) it. the proof itself is very
   complex, but [29]this is a brilliant article offering a visual approach
   as to why it   s true.

   so, that   s great. anns are universal function approximators. id56s take
   it a step further, though; [30]they can compute/describe programs. in
   fact, some id56s with proper weights and architecture qualify as turing
   complete:

     a turing complete system means a system in which a program can be
     written that will find an answer (although with no guarantees
     regarding runtime or memory).

     so, if somebody says    my new thing is turing complete    that means in
     principle (although often not in practice) it could be used to solve
     any computation problem.

         [31]http://stackoverflow.com/a/7320/1260708

   that   s cool, isn   t it? now, this is all theoretical, and in practice
   means less than you think, so don   t get too hyped. hopefully, though,
   this gives some more insight into why id56s are super important for
   future developments in machine learning         and why you should read on.

   at this point, if you weren   t previously hooked on learning what the
   heck these things are, you should be now. (if you still aren   t, just
   bare with me. things will get spicy soon.) so, let   s dive in.

why?

   we took a bit of a detour to talk about how great id56s are, but haven   t
   focused on why anns can   t perform well in the tasks that id56s can.

     why do we need another neural network model? why do we need
     recurrent neural networks when we already have the beloved anns (and
     id98s) in all their glory?

   it boils down to a few things:
     * anns can   t deal with sequential or    temporal    data
     * anns lack memory
     * anns have a fixed architecture
     * id56s are more    biologically realistic    because of the recurrent
       connectivity found in the visual cortex of the brain

   let   s address the first three points individually. the first issue
   refers to the fact that anns have a fixed input size and a fixed output
   size. anns have an elaborate list of hyperparameters, and this notably
   includes the number of neurons in the input layer and output layer. but
   what if we wanted input data and/or output data of variable size,
   instead of something that needs to have its size as a preset constant?
   id56s allow us to do that. in this aspect, they offer more flexibility
   than anns.
   [1*bq0sxdqc9pl_3zqtd3e45a.png]
   we might choose this architecture for our ann, with 4 inputs and 1
   output. but that   s it         we can   t input a vector with 5 values, for
   example.
   [32]https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b
   8e4.

   i   ll give you a couple examples of why this matters.

   it   s unclear how we could use an ann by itself to perform a task like
   image captioning, because the network would need to output a
   sentence         a list of words in a specific order         which is a sequence.
   it would be a sequence of vectors, because each word would need to be
   represented numerically. in machine learning and data science, we
   represent words numerically as vectors; these are called word
   embeddings. an ann can only output a single word/label, like in image
   classification where we treat the output as the label with the highest
   value in the final vector that is a softmax id203 distribution
   over all classes. the only way to make sentences work with anns would
   be to have billions of output neurons that each map to a single
   possible sentence in the permutation of all [sensible] sentences that
   can be formed by the vocabulary we have. and that doesn   t sound like a
   good idea.
   [1*gfvofpd6cdcy_pgqnjholq.png]
   a reminder of what the output of an ann looks like         a id203
   distribution over classes         and how we convert that into a single final
   result (one-hot encoding): by taking the label with the greatest
   id203 and making it 1, with the rest 0.

   wow, that was a lot of words. nevertheless, i hope it   s clear that,
   with anns, there   s no feasible way to output a sequence.

   now, what about inputting a sequence into an ann? in other words,
      temporal    data: data that varies over time, and is thus a sequence.
   take the example of sentiment classification where we input a sentence
   (sequence of words = sequence of vectors = sequence of set of values
   where each value goes into an individual neuron) and want to output its
   sentiment: positive or negative. the output part seems easy, because
   it   s just one neuron that   s either rounded to 1 (positive) or 0
   (negative). and, for the input, you might be thinking: couldn   t we
   input each    set of values    separately? input the first word, wait for
   the neural net to fully feed forward and produce an output, then input
   the next word, etc. etc.

   let   s take the case of this utterly false, and most certainly negative
   sentence, to evaluate:
   [1*yq_zmka1ssikrmd9gkwmnw.png]
   this is just an alternative fact, believe me! lenny is actually a great
   coder. the best i know of. the best.

   we   d input    lenny    first, then    khazan   , then    is   , etc. but, at each
   feedforward iteration, the output would be completely useless. why?
   because the output would be dependent on only that word. we   d be
   finding the sentiment of a single word, which is useless, because we
   want the sentiment of the entire sentence. id31 only
   makes sense when all the words come together, dependent on each other,
   to form a sentence.

   think of it this way         this means you   re essentially running a neural
   network a bunch of times, just with new data at each separate
   iteration. those run-throughs aren   t linked in any way; they   re
   independent. once you feedforward and fully run the neural network, it
   forgets everything it just did. this sentence only makes sense and can
   only be interpretable because it   s a collection of words put together
   in a specific order to form meaning. the relevance of each word is
   dependent on the words that precede it: the context. this is why id56s
   are being used heavily in nlp; they retain context by having memory.
   anns have no memory.

   i like this quote from another article on id56s:

     humans don   t start their thinking from scratch every second. as you
     read this essay, you understand each word based on your
     understanding of previous words. you don   t throw everything away and
     start thinking from scratch again. your thoughts have persistence.

         [33]http://colah.github.io/posts/2015   08-understanding-lstms/

   (furthermore, take the case where we had sequential data in both the
   input and the output. translating one language to another is a good
   example of this. clearly, anns aren   t the answer.)

   id56s don   t just need memory; they need long term memory. let   s take the
   example of predictive typing. let   s say we typed the following sentence
   in an sms message to 911, and the operating system needs to fill in the
   blank:
   [1*tugitpvwm_izqadapr-7ua.png]
   [1*7l0angpxdxzny-p9c8k4ia.jpeg]
   the face of a criminal?

   here, if the id56 wasn   t able to look back much (ie. before    should   ),
   then many different options could arise:
   [1*amju60wscb9m4a-zn_xyqq.png]
   lenny in the military? make it into a tv show! i   d watch it.

   the word    sent    would indicate to the id56 that a location needs to be
   outputted. however, if the id56 was able to retain information from all
   the way back, such as the word    criminal   , then it would be much more
   confident that:
   [1*4czskdigqix29bqylqnyua.png]

   the id203 of outputting    jail    drastically increases when it sees
   the word    criminal    is present. that   s why context matters, be it
   predictive typing, image captioning, machine translation, etc. the
   output or outputs of a recurrent neural network will always be
   functionally dependent on (meaning, a function of) information from the
   very beginning, but how much it chooses to    forget    or    retain    (that
   is, varying degrees of influence from earlier information) depends on
   the weights that it learns from the training data.

   as it turns out, id56s         especially deep ones         are rarely good at
   retaining much information, due to an issue called the vanishing
   gradient problem. that   s where we turn to other variants of id56s such
   as lstms and grus. but, more on that later.

   to address the third point, one more constraint with anns is that they
   have a fixed number of computation/processing steps (because, once
   again, the number of hidden layers is a hyperparameter). with id56s, we
   can have much more dynamic processing since we operate over vectors.
   each neuron in an id56 is almost like an entire layer in an ann; this
   will make more sense as we bring up an illustration for you. exciting
   stuff.

show me.

   ok, that   s enough teasing. three sections into the article, and you   re
   yet to see what an id56 looks like, or appreciate how it really works.
   everything comes in due time, though!

   the first thing i   m going to do is show you what a normal ann diagram
   looks like:
   [1*gapzczdrwnvbflhlrowz9g.png]

   each neuron stores a single scalar value. thus, each layer can be
   considered a vector.

   now i   m going to show you what this ann looks like in our id56 visual
   notation:
   [1*ntklnv52dcunksencm91iq.png]

   the two diagrams above represent the same thing. the latter, obviously,
   looks more succinct than the former. that   s because, with our id56
   visual notation, each neuron (inputs, hidden(s), and outputs) contains
   a vector of information. the term    cell    is also used, and is
   interchangeable with neuron. (i   ll use the latter instead of the
   former.) red is the input neuron, blue is the hidden neuron, and green
   is the output neuron. therefore, an entire ann layer is encapsulated
   into one neuron with our id56 illustration. all operations in id56s, like
   the mapping from one neuron   s state to another, are over entire
   vectors, compared to individual scalars that are summed up with anns.

   let   s flip it the other way:
   [1*hewvhrmccdy-oqcpeenj9w.png]

   this is in fact a type of recurrent neural network         a one to one
   recurrent net, because it maps one input to one output. a one to one
   recurrent net is equivalent to an artificial neural net.

   we can have a one to many recurrent net, where one input is mapped to
   multiple outputs. an example of this would be image captioning         the
   input would be the image in some processed form (usually the result of
   a id98 analyzing the image), and the output would be a sequence of
   words. such an id56 may look like this:
   [1*-jv3txaujbwbgwwjoe_uka.png]
   changed the shades of the green nodes    hope that   s ok!

   this may be confusing at first, so i   m going to make sure i walk slowly
   through it. on the x-axis we have time, and on the y-axis we have
   depth/layers:
   [1*oeyisiei5sj9l3grb5dpua.png]

   when i refer to    time    on the x-axis, i   m referring to the order at
   which these operations occur. time could also be literal for temporal
   data, where the input is a sequence. when i say    depth    on the y-axis,
   i   m referring to the mapping from the input layer, to the hidden
   layer(s), to the output layer, where layer number and thus depth
   increases.

   it may look like we have seven neurons now, but we still have three:
   one input neuron, one hidden neuron, and one output neuron. the
   difference is that these neurons now experience multiple    timesteps   
   where they take on different values, which are, again, vectors. the
   input neuron in our example above doesn   t, because it   s not
   representing sequential data (one to many), but for other architectures
   it could.

   the hidden neuron will take on the vector value h_1 first, then h_2,
   and finally h_3. at each timestep, the hidden neuron   s vector h_t is a
   function of the vector at the previous timestep h_t-1, except for h_1
   which is dependent only on the input x_1. in the diagram above, each
   hidden vector then gives rise to an output y_t, and this is how we map
   one input to multiple outputs. you can visualize these functional
   dependencies with the arrows, which illustrates flow of information in
   the network.

   as we progress on the x-axis, the current timestep increases. as we
   progress on the y-axis, the neuron in question changes. each point on
   this graph thus represents one neuron         be it input, hidden, or
   output         at some timestep, being fed information from a neuron (be it
   itself or another) at the previous timestep.

   the id56 would execute like so:
    1. input x_1
    2. compute h_1 based on x_1 (the arrow implies functional dependency)
    3. compute h_2 based on h_1
    4. compute h_3 based on h_2
    5. compute y_1 based on h_1
    6. compute y_2 based on h_2
    7. compute y_3 based on h_3

   you could compute y_t either immediately after h_t has been computed,
   or, like above, compute all outputs once all hidden states have been
   computed. i   m not entirely sure which is more common in practice.

   this allows for more complex and interesting networks than anns because
   we can have as many timesteps as we want.

   the value of the output neuron at each timestep represents a word in
   the sentence, in the order the sentence will be constructed. the
   caption this id56 produces is hence 3 words long. (it   s actually 2,
   because the id56 would need to output a period or <end> marker at the
   final timestep, but we   ll get into that later.)

   in case you don   t understand yet exactly why id56s work, i   ll walk
   through how these functional dependencies come to fruition when you
   apply it to a one to many scenario such as image captioning.
   [1*ibtlegqfwfsqwzptvajrew.jpeg]
   lenny and i on student scholarship at wwdc 2013. good times!

   when you combine an id56 and id98, you         in practice         get an    lcrn   . the
   architecture for lcrns are more complex than what i   m going to present
   in the next paragraph; rather, i   m going to simplify it to convey my
   point. we   ll actually get fully into how they work later.

   imagine an id56 tries to caption this image. an accurate result might
   be:

     two people happily posing for a photo inside a building.

   the input to the id56 would be the output of a id98 that processes this
   image. (however, to be pedantic, it would be the output of the id98
   without a classification/softmax layer         that is, pulled from the final
   fully connected layer.) the id98 might pick up on the fact that there
   are two primary human face-like objects present in the image, which,
   paired with what the id56 has learned via training, may induce the first
   hidden state   of the recurrent neural network to be one where the most
   likely candidate word is    two   .

   pro-tip  : the term    hidden state    refers to the vector of a hidden
   neuron at a given timestep.    first hidden state    refers to the hidden
   state at timestep 1.

   the first output, which represents the word    two   , was functionally
   dependent on the first hidden state, which in itself was a function of
   the input to the id56. thus,    two    was ultimately determined from the
   information that the id98 gave us and the experience/weights of the id56.
   now, the second word,    people   , is functionally dependent on the second
   hidden state. however, note that the second hidden state is just a
   function of the first hidden state. this means that the word    people   
   was the most likely candidate given the hidden state where    two    was
   likely. in other words, the id56 recognized that, given the word    two   ,
   the word    people    should be next, based on the id56   s experience from
   training and the initial image [analysis] we inputted.

   the same will occur for every following word; the nth word will be
   based on the nth hidden state, which, ultimately, is a function of
   every hidden state before it, and thus could be interpreted purely as
   an extremely complex and layered function of the input. the weights do
   the heavy lifting by making sense of all this information and deducing
   an output from it.

   to put it bluntly, you can boil down what the id56 is    thinking    to
   this:

     based on what i   ve seen from the input, based on the current
     timestep i   m at, and based on what i know from all my training, i
     need to output:    x   .

   thus, each outputted word is dependent on the words before it, all the
   way back to the input image data. however, this relationship is
   indirect. it   s indirect because the outputs are only dependent on the
   hidden states, not on each other (ie. the id56 doesn   t deduce    people   
   from    two   , it deduces    people   , partly, from the information         the
   hidden state         that gave rise to    two   ). in lcrns, though, this is
   explicit instead of implicit; we    sample    the output of one timestep by
   taking it and literally feeding it back as input into the next
   timestep. in a sense, lcrns can hence be interpreted as having many to
   many architecture.

   the exact quantitative relationships depend on the id56   s weights. but,
   generally, this is the concept of memory in play. creating a coherent
   sentence as we go along is only really possible if we can recall what
   we said before. and id56s are able to do exactly that; they remember
   what they said before and figure out, based on their image captioning
   expertise, what from this is useful to continue accurately speaking.

     yep, i went to france for a holiday. and i actually learned to speak
     some <wait, shit, what was the language again? oh yea,    france      >
     french!

   obviously, an id56 needs to be trained and have proper weights for this
   to all function properly. id56s aren   t magic; they only work because
   trained networks identified and learned patterns in data during
   training time that they now look for during prediction.

   perhaps this was a bit over-explaining on my part, but hopefully i
   nailed down some important and core ideas about how id56s function.

   so far we   ve looked at one to one and one to many recurrent networks.
   we can also have many to one:
   [1*kjyqyc-jd_zs5erqypm9ea.png]

   with many to one (and many to many), the input is in the form of a
   sequence, and so the hidden states are functionally dependent on both
   the input at that timestep and the previous hidden state. this is
   different to one to many, where the hidden state after h_1 is only
   dependent on the previous hidden state. that   s why, in the image above,
   the second hidden state has two arrows directed at it.

   only one output exists in many to one architecture. an example
   application is sentiment classification, where the input is a sentence
   (sequence of words) and the output is a id203 indicating that the
   inputted sentence was positive.

   the final type of recurrent net is many to many, where both the input
   and output are sequential:
   [1*mpplcbi1j6r6vmsdm7g4_g.png]

   a use case would be machine translation where a sequence of words in
   one language needs to be translated to a sequence of words in another.

   we can also go deeper and have multiple hidden layers, and/or a greater
   number of timesteps:
   [1*vfuwsagw-c5huntaphb1aq.png]
   we   re getting deeper and deeper!

   really, this could be considered as multiple id56s. technically, you can
   consider each    hidden layer    as an id56 itself, given each neuron
   operates on vectors and updates through time; in ann context, that
   volume of operations would be considered an entire network. so this is
   like stacking id56s on top of each other. however, in this article i   ll
   refer to it as multiple hidden layers; different papers and lecturers
   may take different approaches.

   when we have many timesteps (usually hundreds) and multiple hidden
   layers, the architecture of the network becomes much more complex and
   interesting. one feature of this id56, in particular, is that all the
   outputs, including the first, depend on not just the input up to that
   timestep, but all of the inputs. (you can see this because the green
   neuron is only introduced after the final input timestep.) if this id56
   was to translate english to chinese, the first word of translated
   chinese isn   t just dependent on the first word of the inputted english;
   it   s dependent on the entire sentence.

   one way to demonstrate why this matters is to use google translate:
   [1*mptnrzbgadt3yuql-tpeow.png]
   one of my favorite green day lyrics, from the song    fashion victim    on
   warning:. side-note: based on my experience with google translate in
   chinese class over the last 8 years, this translation is probably off.

   now i   ll input    he   s a victim    and    of his own time    separately. you   ll
   notice that when you join the two translated outputs, this won   t be
   equal to the corresponding phrase in the first translation:
   [1*lo5ocssxty4loic3sasmlw.png]
   what happens if we break up the english into different parts,
   translate, and join together the translated chinese parts?
   [1*ojrflstyai-mkbguw6otfa.png]
   they   re not equal.

   what gives? well, the way sentences are constructed in languages can
   differ in varying scenarios. some words in english may also map to
   multiple different words in chinese, depending on how it   s used. it all
   depends on the context and the entire sentence as a whole         the meaning
   you   re trying to convey. this is the exact approach a human translator
   would take.

   another type of many to many architecture exists where each neuron has
   a state at every timestep, in a    synchronized    fashion. here, each
   output is only dependent on the inputs that were fed in during or
   before it. because of this, synchronized many to many probably wouldn   t
   be suitable for translation.
   [1*84ikp_dqlufimz5syzlwja.png]

   an application for this could be video classification where each frame
   needs to be mapped to some sort of class or label. interesting
   note         an id56 is better at this task than id98s are because what   s going
   on in a scene is much easier to understand if you   ve watched the video
   up to that point and thus can contextualize it. that   s what humans do!

   quick note: we can    wrap    the id56 into a much more succinct form, where
   we collapse the depth and time properties, like so:
   [1*7pop9gxasrlbrrrsrhr-ja.png]

   this notation demonstrates that id56s take input, process that input
   through multiple timesteps and hidden layers, and produce output. the
   arrow both leaving and entering the id56 conveys that an id56 hidden
   state is functionally dependent on the hidden state at the preceding
   timestep; it   s sort of like a loop that feeds itself.

   when you ever read about    unrolling    an id56 into a feedforward network
   that looks like it   s in the same collapsed format as the diagram above,
   this means we expand it to show all timesteps and hidden layers like we
   did before.

   another quick note: when somebody or a research paper mentions that
   they are using    512 id56 units   , this translates to:    1 id56 neuron that
   outputs a 512-wide vector   ; that is, a vector with 512 values. at
   first, i thought this meant that maybe at each timestep there were 512
   separate neurons somehow working in conjunction, but nope, it   s luckily
   much simpler than that    albeit strangely worded.

   furthermore, one    id56 unit    usually refers to an id56 with one hidden
   layer; thus, instead of defining id56 as something that is multilayer
   inherently, we often see people use the phrase like:    stacking id56s on
   top of each other   . each id56 will have its on weights, but connecting
   them gives rise to an overarching multilayer id56. in this article, we
   treat recurrent neural networks as a model that can have variable
   timesteps t and fixed layers    , just make sure you understand that this
   is not always the case. our formalism, especially for weights, will
   slightly differ.

formalism

   so, now, let   s walk through the formal mathematical notation involved
   in id56s.

   if an input or output neuron has a value at timestep t, we denote the
   vector as:
   [1*_d9bolepossbc2zgk7wreq.png]

   for the hidden neurons it   s a bit different; since we can have multiple
   hidden layers, we denote the hidden state vector at timestep t and
   hidden layer     as:
   [1*qjfwcdovxgage0zt17hw1g.png]

   the input is obviously some preset values that we know. the outputs and
   hidden states are not; they are calculated.

   let   s start with hidden states. first, we   ll revisit the most complex
   recurrent net we came across earlier         the many to many architecture:
   [1*tjxcm6gi8dmheq6sk3ky8q.png]
   many to many, non-synchronized.

   this id56 has: sequential input, sequential output, multiple timesteps,
   and multiple hidden layers. the formula we derive for this id56 should
   generalize for all others.

   first, let   s list out the possible functional dependencies for a given
   hidden state, based on the arrows and flow of information in the
   diagram:
     * an input
     * hidden state at the previous timestep, same layer
     * hidden state at the current timestep, previous layer

   a hidden state can have two functional dependencies at max. just by
   looking at the diagram, the only impossible combination is to be
   dependent on both the input and a hidden state at the current timestep
   but previous layer. this is because the only hidden states that are
   dependent on input exist in the first hidden layer, where no such
   previous layer exists.

   if this is all difficult to follow, make sure once again to look at and
   trace back the arrows in the id56 that illustrate flow of information
   throughout the network.

   because of the impossible combination, we define two separate
   equations: an equation for the hidden state at hidden layer 1, and for
   layers after 1.
   [1*n5qr9q9zgnwfrf7protlia.png]

   this probably looks a bit confusing; let me break it down for you. the
   function   w computes the numeric hidden state vector for timestep t and
   layer    ; it contains the    activation function    you   re used to hearing
   about with anns. w are the weights of the recurrent net, and thus    is
   conditioned on w. we haven   t exactly defined    just yet, but what   s
   important to note is the two parameters it takes. once you do, this
   notation simply states what we have stated before in plain english:

     where     = 1, the hidden state at time t and layer     is a function of
     the hidden state vector at time t-1 and layer     as well as the input
     vector at time t. where     > 1, this hidden state is a function of
     the hidden state vector at time t-1 and layer     as well as the
     hidden state vector at time t, layer    -1.

   you might notice that we have a couple issues:
     * when t = 1         that is, when each neuron is at the initial
       timestep         then no previous timestep exists. however, we still
       attempt to pass h_0 as a parameter to   w.
     * if no input exists at time t         thus, x_t does not exist         then we
       still attempt to pass x_t as a parameter.

   our respective solutions follow:
     * define h_0 for any layer as 0
     * consider x_t where no input exists at timestep t as 0

   if these are 0, then the invalid functional dependency stops existing,
   and our formal notation still holds up.

   we actually have five different types of weight matrices:
   [1*r09eqtflea1kiiojd2az6g.png]

   pro-tip: the indices for each weight matrix tell you what they are used
   for in the recurrent net. w_xh maps an input vector x to a hidden state
   vector h. w_hht maps a hidden state vector h to another hidden state
   vector h along the time axis, ie. from h_t-1 to h_t. on the other hand,
   w_hhd maps a hidden state vector h to another hidden state vector h
   along the depth axis, ie. from h^(   -1)_t to h^   _t. w_hy maps a hidden
   state vector h to an output vector y.

   like with anns, we also learn and add a constant bias vector, denoted
   b_h, that can vertically shift what we pass to the activation function.
   we can also shift our outputs with b_y. more about bias units [34]here.

   for both b_h and w_hht/w_hhd, we actually have multiple weight matrices
   depending on the value of    , as indicated by the superscript. this is
   because each hidden layer can have a different set of weights (the
   network would be extremely uninteresting if this wasn   t the case),
   including the bias vector. however, inside a single hidden layer, all
   timesteps share the same weight matrix. this is important because the
   number of timesteps is a variable; we may train on sequences with up to
   20 values, but in practice output sequences with up to 30 values         10
   extra timesteps. if each timestep had an independent weight to learn,
   those last 10 timesteps wouldn   t have anything to use. since this would
   also mean that the number of parameters in the neural network would
   grow linearly relative to the input, we would have way too many
   parameters very potentially causing overfitting.

   w_hy is just one matrix because only the final layer gives rise to the
   outputs denoted y. at the final hidden layer    , we could suggest that
   w_hhd will not exist because w_hy will be in its place.

   now we   ll define the function   w:
   [1*9yguqxdniknmzr2hscmdkw.png]

   the function is very similar to the ann hidden function you   ve seen
   before; it applies the correct weights to the corresponding parameters,
   adds the bias, and passes this weighted sum through an activation or
      squashing    function to introduce non-linearities. the key difference,
   though, is that this is not a weighted sum but rather a weighted sum
   vector; any w     h, along with the bias, will have the dimensions of a
   vector. the tanh function will thus simply output a vector where each
   value is the tanh of what it was in the inputted vector (sort of like
   an element-wise tanh). remember, this contrasts anns because id56s
   operate over vectors versus scalars.

   if you   ve followed our blog so far, you most likely know about two
   id180: sigmoid and relu. tanh is another such function.
   we mostly use the tanh function with id56s. this is, i think, mostly
   because of their role in lstms (a variant of id56s that are used more
   than id56s         more on that later), the fact that they produce gradients
   with a greater range, and that their second derivative don   t die off as
   quickly.

   similar to sigmoid, the tanh function has two horizontal asymptotes and
   a smooth s-shape. the main difference is that the tanh function
   asymptotes at y = -1 instead of y = 0, intercepting the y-axis at y = 0
   instead of y = 0.5. thus, the tanh function has a greater range than
   the sigmoid.
   [1*npi9illvlylq2gu9a9xp0a.png]

   if interested, the tanh equation follows (though i won   t walk you
   through it):
   [1*w7lv9vy1hcaxclk2k_peeg.png]

   the final equation is mapping a hidden state to an output.
   [1*n7simjp73wxcrx_bz4dxwg.png]

   this is one such possible equation. depending on the context, we might
   also remove the bias vector, apply a non-linearity like sigmoid/softmax
   (for example if the output needs to be a id203 distribution),
   etc.

   and that   s how we express recurrent nets, mathematically!

   quick note: notation may and will differ between various lectures,
   research paper, articles, etc. for example         some research papers may
   start indexing at 0 instead of 1. more drastically, most id56 notation
   is much more general than mine to promote simplicity, ie. doesn   t cover
   edge cases like i did or obfuscates certain indices like     with hidden
   to hidden weight matrices. so, just keep note that specifics don   t
   always transfer over and avoid being confused by this. the reason i was
   meticulous about notation in this article is that i wanted to ensure
   you understood exactly how id56s work, fueled by my frustration with the
   very same problem ~1.5 years ago.

an example? okay!

   let   s take a look at a quick example of an id56 in action. i   m going to
   adapt a super dumbed down one from andrej karpathy   s stanford cs231n
   [35]id56 lecture, where a one to many    character level language model   
   single layer recurrent neural network needs to output    hello   . we   ll
   kick it of by giving the id56 the letter    h    , such that it needs to
   complete the word by outputting the other four letters.

   sidenote: this model nicknamed    char-id56            remember it for later,
   where we get to code our own!

   the neural network has the vocabulary: h, e, l , o. that is, it only
   knows these four characters; exactly enough to produce the word
      hello   . we will input the first character,    h   , and from there expect
   the output at the following timesteps to be:    e   ,    l   ,    l   , and    o   
   respectively, to form:

     hello

   we can represent input and output via one hot encoding, where each
   character is a vector with a 1 at the corresponding character position
   and otherwise all 0s. for example, since our vocabulary is [h, e, l,
   o], we can represent characters using a vector with four values, where
   a 1 in the first, second, third, and fourth position would represent
      h   ,    e   ,    l   , and    o    respectively.
   [1*pgwspyximafhqztukilekg.png]
   this is called    one-hot encoding   , because only one of the values in
   the vector is equal to 1 and thus on (or    hot   ).

   this is what we   d expect with a trained id56:
   [1*mmuqb8msqqqltz580_lpvw.png]

   as you can see, we input the first letter and the word is completed. we
   don   t know exactly what the hidden states will be         that   s why they   re
   hidden!

   one interesting technique would be to sample the output at each
   timestep and feed it into the next as input:
   [1*kyvsttlgcexqwdlvswd0lg.png]

   when we    sample    from a distribution, we select a random character
   probabilistically following the distribution. for example, in the
   diagram above, the character with the highest likeliness is    e    at the
   first timestep   s output. let   s say this likeliness is, concretely, 0.9.
   now, when we sample into the next timestep   s input, there   s a 90%
   chance we select    e   ; most of the time we will pick the most likely
   character, but not every time. this adds a level of randomness so you
   don   t end up in a loop where you keep sampling the same letter or
   sequence of letters over and over again.

   as mentioned earlier, this is used pretty heavily with lcrns. it   s even
   more effective than only relying on the memory of the id56 to output the
   correct letter at the future timesteps. in a sense, this makes the
   recurrent net many to many. (though, not really, because we still only
   have one preset input.)

   however, to be clear, this does not mean that the id56 can only rely on
   these sampled inputs. for example, at timestep 3 the input is    l    and
   the expected output is also    l   . however, at timestep 4, the input is
   again    l    but the output is now    o   , to complete the word. memory is
   still needed to make a distinction like this.

   in numerical form, it would look something like this:
   [1*agugrurzg6e6rz7ctvn2ww.png]

   of course, we won   t get a one-hot vector output during prediction mode;
   rather, we will get a id203 distribution over each letter (so
   we   d apply softmax to the output), and will sample from this
   distribution to get a single character output.
   [1*6067m6oqz2znoyyyqc1suw.png]

   each hidden state would contain a similar sort of vector, though not
   necessarily something we could interpret like we can for the output.

   the id56 is saying: given    h   ,    e    is most likely to be the next
   character. given    he   ,    l    is the next likely character. with    hel   ,
      l    should be next, and with    hell   , the final character should be    o   .

   but, if the neural network wasn   t trained on the word    hello   , and thus
   didn   t have optimal weights (ie. just randomly initialized weights),
   then we   d have garble like    hleol    coming out.

   one more important thing to note: start and end tokens. they signify
   when input begins and when output ends. for example, when the final
   character is outputted (   o   ), we can sample this back as input and
   expect that the    <end>    token (however we choose to represent
   it         could also use a period) will be outputted at the next timestep;
   this is the id56 telling us that it has completed the word and its
   processing as a whole. the use case isn   t as obvious in this fabricated
   example, because we know when    hello    has been completed, but consider
   a real-life scenario where we don   t: image captioning. in image
   captioning, the caption could be 1, 2, 3, or n words long, given a
   reasonable upper limit of n. the end token tells us when the caption
   has been completed, so we can halt the id56 and complete the prediction
   loop (which would keep going forever if we were using while or stop
   after the upper limit/max possible preset constant value of n is
   reached).

   start tokens are more used for generating content from complete
   scratch. for example, imagine an id56 read and learned from a bunch of
   shakespeare. (this is an actual funny application of character level
   language models that karpathy implemented, and we   ll see it in action
   on a later section.) now, based on what the id56 learned, we want it to
   create a brand new shakespearean sonnet! feeding in a    <start>    token
   enables it to kick this process off and begin writing without us giving
   the network some arbitrary pre-determined initial word or character.

   i   ve also noticed that another potential use case of start tokens is
   when we have some other sort of initial input, like id98 produced image
   data with image captioning, that doesn   t    fit    what we   ll normally use
   for input at timesteps after t=1 (the word outputted at the previous
   timestep via sampling). as a result, we feed this data directly to the
   first hidden state and set the input as    <start>    instead.

   now, just to be clear, the id56 doesn   t magically output these end
   tokens and recognize the start tokens. we have to add them, along with
   start tokens, to the training data and vocabulary such that they can be
   outputted by the recurrent net during prediction time.

   this is how we can get id56s to    write   ! more on some examples of text
   id56s have actually generated, shakespeare most certainly included, in a
   later section.

training (or, why vanilla id56s suck.)

   for a recurrent net to be useful, it needs to learn proper weights via
   training. that   s no surprise.

   recall this snippet from earlier:

     but, if the neural network wasn   t trained on the word    hello   , and
     thus didn   t have optimal weights (ie. just randomly initialized
     weights), then we   d have garble like    hleol    coming out.

   this is, of course, because we initialize the w weights randomly at
   first, so random stuff will come out.

   but, through multiple iterations of training with a first-order
   optimization algorithm like id119, we perturb the weights
   such that the id203 of each correct character being outputted at
   their respective timestep increases. the actual output would be    hello   
   in one-hot encoding form, and we   d compute the discrepancy between this
   output and what the recurrent net predicts (we   d get the error at each
   timestep and then add this up) as the total error to then calculate the
   gradient/update value.

   so, each output contributes to the error somehow. if the error is an
   addition of the outputs, then, if we had something like y outputs, we   d
   need to backpropagate them individually and add these up. this is
   because derivatives are distributed evenly when we   re differentiating a
   sum:
   [1*d5mzuu-emczz0ifukw6xsq.png]
   for any arbitrary weight w.

   but, you should know that, with id158s, calculating
   these gradients isn   t that easy. we have so many weights contributing
   to the output, and thus need to figure out exactly how much these
   weights contribute, and by how much we modify them to decrease overall
   error. to do this, we use the id26 algorithm; this algorithm
   propagates the error between the predicted output of a recurrent net
   and the actual output in the dataset all the way back to the beginning
   of the network. using the chain rule from id128,
   backprop helps us calculate the gradients of the output error w.r.t.
   each individual weight (sort of like the error of each individual
   weight).

   once we have those gradients, we have to use an optimization algorithm
   to calculate the update values and make the updates. we can use the
   vanilla id119 algorithm to do this, but there are many other
   possible, better variants as well; learn about them by reading [36]this
   article, if you want. (i think we   re long overdue for our own mega-post
   on optimization!)

   id26 with id56s is called    id26 through time   
   (short for bptt), since it operates on sequences in time. but don   t be
   fooled         there   s not much difference between normal backprop and bptt;
   when it comes down to it, bptt is just backprop, but on id56s! remember
   that when you    unroll    an id56, it essentially becomes a feedforward
   network; not an ann, but a feedforward network in the sense that we can
   visualize where all the information is flowing and observe the
   activations at each neuron and timestep, all the way from the input to
   the final output. like anns, id56s have functional dependencies that
   link the entire network together; it   s just that id56s operate over
   vectors instead (yay for matrix calculus?) and extend in depth as well
   as time. there   s more work to do to compute the gradients, but it   s no
   surprise that backprop works pretty much the same way for recurrent
   nets that it would for normal ones. because of this, i   m not going to
   walk through all the math and show the derivatives etc. read our
   backprop mega-post for all that jazz.

   one thing to note is that, since we have multiple timesteps in our id56,
   each timestep in a single layer will want to change the weight in a
   different way and have different gradients. however, remember that each
   hidden layer uses only one weight matrix because the number of
   timesteps is a variable. thus, we just average or sum the weight
   updates between these timesteps and apply this as an update to the w_hh
   for that entire layer. also, a general practice is to train on shorter
   sequences first and then gradually increase sequence size as we train
   on more and more data.

   now, if you haven   t already, make sure to read this article that i
   wrote on vanishing and exploding gradients before proceeding:
   [37]rohan #4: the vanishing gradient problem
   oh no         an obstacle to deep learning!ayearofai.com

   you may be thinking: how does this issue apply to id56s? well, id56s are
   very deep models; on top of often having multiple hidden layers, each
   hidden layer in practice can have hundreds of timesteps. that   s like an
   ann with hundreds of entire hidden layers! that   s deep. (well, it   s
   more long because we   re dealing with the time axis here, but you know
   what i mean.) tanh derivatives are very similar to sigmoid derivatives
   in range, so the problem of vanishing gradients is thus even more
   drastic with id56s than with anns, and training them becomes almost
   impossible.

   imagine trying to propagate the error to the 1st timestep in an id56
   with k timesteps. the derivative would look something like this:
   [1*gkbrtqfpwgk2d7jnkzdv5w.png]

   with a tanh activation function, that   s freaking crazy. then, for
   getting the derivative of the error with respect to a weight matrix
   w_hh, we   d add         or, as mentioned before, we could average as
   well         each of these hidden state error gradients, then multiplied by
   the derivative of the hidden state with respect to the weight, such
   that we can backprop from the error to the weight:
   [1*jf52uxcsax6nn8ghlyojwq.png]
   assuming our sequence is of length k.

   so we   d be effectively adding together a bunch of terms that have
   vanished         the exception being very late gradients with a small number
   of terms         and so dj/dwhh would only capture gradient signals from the
   last few timesteps. (or, for exploding gradients, it would become
   infinity).

   but, you might be asking, instead of tanh         which is bounded between -1
   and 1, and has a similar problem to sigmoid where the peak of the
   derivative is smaller than 1         why don   t we just use relus? don   t
   relus, or perhaps leaky relus, solve the vanishing gradient problem?

   well, not entirely; it   s not enough to solve the problem. with id56s,
   the problem really lies in the architecture. even though we could use
   relu to ensure many of the values in the gradient computation are not
   between -1, 0, and 1 such that they vanish         or vice-versa,
   explode         we do still indeed have a lot of other variables other than
   the activation function derivative in the gradient computation such as
   the weights; you can revisit the mega-post on backprop we wrote to
   confirm this. since weights are also normally randomly initialized in
   the range -1 to 1, and id56s are like super deep anns, these weights
   keep multiplying on top of each other and potentially cause the
   gradients to vanish.

   this is more my suspicion though         i   m yet to confirm this is the case
   by testing. i was curious so i asked this exact question on quora:

   iframe: [38]/media/aa3fb6f891aba6d55742cf7dabf3f7f7?postid=10300100899b

   from this, something interesting i learned is that: since relus are
   unbounded (it   s not restricted to be between -1 and 1 or 0 and 1)
   unlike sigmoid/tanh, and id56s are very deep, the activations,
   especially later ones, can become too big. this is because hidden
   states have a multiplicative relationship; one hidden state is a
   multiple of the previous ones, where that multiple specifically is a
   weight. if we use relu, then the hidden state isn   t limited by any
   range, and we could have a bunch of numbers bigger than 1 multiplying
   by each other.

   it ends up being sort of like the exploding gradient problem, but with
   the values inside the neurons, not gradients. this is also what then
   causes the gradients to explode: large activations     large gradients    
   large change in weights     even bigger activations, because updating the
   weights in the wrong direction ever so slightly can cause the entire
   network to explode. this makes learning unstable:

     this means that the computation within the id56 can potentially blow
     up to infinity without sensible weights. this makes learning very
     unstable because a slight shift in the weights in the wrong
     direction during backprop can blow up the activations during the
     forward pass. so that   s why you see most people using sigmoid/tanh
     units, despite the vanishing id119 problem.

   also well said:

     with id56   s, the problem is that you are repeatedly applying your id56
     to itself, which tends to [mostly] cause exponential blowup or
     [rarely, but sometimes] shrinkage.

   other issues with relu functions are discussed in the article i wrote,
   and they similarly apply to id56s. generally speaking, though, they just
   don   t work that well, especially compared to other options we have.
   making id56s perform well with relus is actually a pretty hot topic of
   research right now, but until someone figures out something genius,
   id56s are a lost cause.

   and that   s why vanilla id56s suck. seriously. in practice, nobody uses
   them. even if you didn   t fully grasp this section on how the vanishing
   and exploding gradient/activation problem is applicable to them, it
   doesn   t matter anyways. because, everything you   ve read up to this
   point so far    throw it all away. forget about it.

   just kidding. don   t do that.

fixing the problem with lstms (part i)

   you shouldn   t do that because id56s actually aren   t a lost cause.
   they   re far from it. we just need to make a few    modifications.

   enter the lstm.
   [1*juc5afkk7qintsvyen-ifa.png]

   makes sense, no?
   [1*tb6qdzunjv08wyep0zhvma.png]

   how about this?
   [1*oin8uuuqzp_wqthax1yyjq.png]

   ok. clearly something   s not registering here. but that   s fine; lstm
   diagrams are frikin    difficult for beginners to grasp. i too remember
   when i first searched up    lstm    on google to encounter something
   similar to the works of art above. i reacted like this:
   [1*s7abwk33x7no_mp3epry6a.gif]
   mrw first google image-ing lstms.

   in this section, i   m going to embark on a mission to design the first
   simple, comprehensible, and beautiful lstm diagram. wish me luck,
   because i   ll probably fail.

   with that being said, let   s dive into id137.
   (yes, that   s what lstm stands for.)
     __________________________________________________________________

   with id56s, the real    substance    of the model were the hidden neurons;
   these were the units that did processing on the input, through time, to
   produce the outputs. specifically, at each timestep, a hidden neuron
   embodies a hidden state that is computed by feeding the weighted sum
   vector of the input and/or previous hidden states with an added bias
   vector through a tanh squashing function. we can have multiple hidden
   neurons, each of which when unrolled forms an entire hidden    layer   .

   if you need a refresher on this, look through the    formalism    section
   once again.

   with lstms, we still have hidden states, but they   re computed through a
   much more complex mechanism: lstm cells. instead of computing each
   hidden state as a direct function of inputs and other hidden states, we
   compute it as a function of the lstm cell   s value (the    cell state   ) at
   that timestep. each cell state is in turn functionally dependent on the
   previous cell state and any available input or previous hidden states.
   that   s right         hidden states are computed from cell states, and cell
   states are (in part) computed from older and/or shallower hidden
   states.

   the cell state at a specific timestep t is denoted c_t. like a hidden
   state, a cell state is just a vector.
   [1*sr8xqzvr-wtnsbgwwi9qbq.png]
   for simplicity   s sake, i   ve obfuscated layer index    .

   if the diagram above seems a bit trippy, let me break it down for you.

   c_t, as highlighted by the three arrows pointing towards it, has
   multiple potential functional dependencies. four to be exact, though
   only a maximum of three can exist at once. these are:
     * the previous hidden state in time: h_t-1. again, if t = 1, then
       this won   t exist. if it does, this would be the first arrow
       pointing into the left side of c_t.
     * the previous cell state: c_t-1. if t = 1, the dependency obviously
       won   t exist. this refers to the second arrow pointing into the left
       side of c_t.
     * input at the current timestep: x_t. there may very well be no input
       available, for example if we are at a hidden layer     > 1. so this
       dependency doesn   t always exist. when it does, it   s the arrow
       pointing into the bottom of c_t.
     * the previous hidden state in depth: h^(   -1)_t. this applies for any
       hidden layer     > 1. in such case, it would         like the input
       x_t         be the arrow pointing into the bottom.

   only three can exist at once because the last two are mutually
   exclusive.

   from there, we pass information to the next cell state c_t+1 and
   compute h_t. as you can hopefully see, h_t then goes on to also
   influence c_t+1 (as indicated by the horizontal arrow), along with
   higher level cell states or final outputs (the vertical arrow).

   right now the cells are a black box    literally; we know what is
   inputted to them and what they output, but we don   t know their internal
   process. so    what   s inside these cells? what do they do? what are the
   exact computations involved? how have the equations changed?

   to help answer the question, i want you to imagine something: a
   factory. inside this factory we have workers who perform their own
   tasks. those tasks are, specifically, operating on some sort of product
   that runs down a conveyer belt. think of, hell, i don   t know         chicken
   nuggets! the first worker cuts an appropriately sized piece, the second
   worker applies egg wash, the third worker adds breadcrumbs, the fourth
   worker chucks it in the fryer, etc.
   [1*4avrg18sfomjgi4cpdisoa.png]
   i   m not sure what product this conveyer belt carries, but it certainly
   doesn   t look appetizing (or like chicken nuggets).

   you   re thinking:    ok rohan, but how does this relate to lstms?   . good
   question.

   basically, think of the conveyer belt as the cell state, the
   chicken-nugget-in-progress as information flowing through the cell, and
   the workers as operations we apply to this information. the final
   product is the finished chicken nugget         or, the cell state value.
   [1*qnugfmhlnl0-mnlivvygag.png]
   chicken. nugget.

   the reason we use the analogy of a conveyer belt is because information
   can flow through a cell super super easily. it   s theoretically possible
   for information to speed past a single cell state without being
   modified at all. in fact, i think the term    modified    is a really
   strong one here. with vanilla id56s, each hidden state takes all the
   information it has from before and fully transforms it by applying a
   function over it. lstm cells instead take information and make minor
   modifications (like additions or multiplications) to it while it flows
   through.
   [1*i_nqdhxdoda7krzbtfehsq.png]
   ew. vanilla id56s.

   vanilla id56s look something like this. and it   s why the vanishing
   gradient problem exists; during backprop, gradients cannot flow back
   past these transformations easily, because the tanh derivatives and
   weights chain up and multiply together and tend to zero. we then add up
   or average all these gradients that are basically zero, and we get
   zero.
   [1*360gynv8kyf5atwefrsasa.png]
   lstms               

   this is an extreme a simplification         and i   ll go on to fill in the
   blanks later         but it   s sort of what an lstm looks like. the previous
   timestep   s cell state value flows through and instead of transforming
   the information, we tweak it by adding (another vector) to it. the
   added term is some function   w of previous information, but this is not
   the same function as with vanilla id56s         it   s heavily changed to make
   sense in this context (more on that soon), do more interesting things,
   and also reduce the vanishing gradient problem.

   another neat way to think of it is like a live circuit: there are two
   paths where information, like current, can flow through. after the
   information passes through   w, it   s added to the information flowing
   towards c_t. thus, in equation form it could look something like this:
   [1*qgqsrpjmo5h6zgeit7rk3w.png]
   again    sort of. we   ll get into the actual equations soon. this is a
   good proxy to convey my point.

   with a bit of substitution, we can expand this to:
   [1*ozs7rdsty0vhdhztlh4dgg.png]
   technically, this could expand even more, if you did some sort of
   recursive substitution to fully simplify the unrolled recurrence
   formula. you could express c_t for some large value of t as a really
   really really really long function of, ultimately, c_1.

   why is this better? well, if you have basic differentiation knowledge,
   you   ll know that addition distributes gradients equally. when we take
   the derivative of this whole expression, it   ll become a long addition
   of the derivatives of individual terms. as andrej karpathy puts it,
   this additive interaction creates    gradient super-highways   , where
   gradients can flow back super easily.
   [1*n26drgfekc-xnmqc2lw7cw.png]
   look         it   s a long conveyer belt! (in a sense, we can use this conveyor
   belt analogy for the whole unrolled lstm as well. each cell state is a
   subsection of the conveyer belt.)
   [1*szbiwndr0o0dobi8rfgjzw.png]
   look         it   s an outdated machine learning algorithm!

   in the former, gradients are always added together, never multiplied.
   in the latter, gradients are always multiplied. thus, in the former,
   when we inject a gradient at the end, it   ll easily flow back all the
   way to the beginning. contributions by the   w function will be made to
   this gradient flowing on the bottom conveyer belt as well.

   this is what gradient flow would look like:
   [1*dqocxyepo590orwv3vgbkw.png]

   before, we discussed that when multiplicative interaction exists
   between gradients, the gradients either vanish (if they are mostly < 1,
   as is usually the case for us) or explode (if they are mostly > 1).
   here   s some real calculus to demonstrate this:
   [1*09ogk1btsveziomybawqbw.png]
   former is akin to id56s. latter is akin to lstms.

   imagine f being any sort of function, like our   w. when we apply a
   function to itself repeatedly, the chain rule shows that the overall
   derivative is the multiplication of multiple different derivative
   terms. but, when we add functions together, the derivative is simply
   the addition of the individual derivatives. this won   t vanish or
   explode quickly, so our lstms won   t vanish or explode quickly. yay!

   furthermore, if some of our gradients vanish         for whatever
   reason         then it should still be ok. it won   t be optimal, but since our
   gradient terms add together, if some of them vanish it doesn   t mean the
   whole thing will vanish (versus if they were multiplied together).
   look: 2 + 0 = 2 but 2    0 = 0.
   [1*k7ryonptfcpcb0xtvo3ydw.jpeg]
   a gradient super highway? sounds good to me!
   [39]http://www.dyoung.com/assets/images/articles%20images/article4_pph.
   jpg
     __________________________________________________________________

   so far, we haven   t really explored lstms. we   ve more setup a foundation
   for them. and there   s one glaring issue with our foundation: if we just
   keep adding information to cell state, it could just grow and grow and
   grow, and essentially act as a counter that only increments. this is
   not very useful, and could regularly lead to explosion. we want more
   fine and rich control over memory. well, worry not, because this is
   exactly what lstms are capable of doing.

   lstm cells handle memory in a very intelligent way, enabling them to
   learn long-term dependencies and perform well. how, exactly? well, the
   cell is sort of like an internal memory state that allows for context;
   it    forgets   , a.k.a. resets, information it doesn   t find useful from
   the previous cell state,    writes    in new information it does find
   useful from the current input and/or previous hidden state, and
   similarly only    reads    out part of its information         the good
   stuff         in the computation of h_t. this respectively corresponds to the
   concepts of: resetting memory, writing to memory, and reading from
   memory. very similar to how a modern computer system works, and we
   often describe an lstm cell as a    memory cell   .

   the    writing to memory    part is additive         it   s what i showed you in
   the initial diagrams. information flows through and we add stuff we
   think is relevant to it. the    resetting memory    part is multiplicative,
   and occurs before writing to memory; when information from the previous
   cell state initially flows in, we multiply it by a vector with values
   between 0 and 1 to reset or retain parts of it we find useless and
   useful respectively. the    reading from memory    part is also
   multiplicative with a similar 0   1 range vector, but it doesn   t modify
   the information flowing through the cell states. rather, it modifies
   the information flowing into the hidden states and thus decides what
   the hidden state is influenced by.

   both of these multiplications are element wise, like so:
   [1*yuiuyxt0oyevgmotz_j59g.png]

   in this equation, when a = 0 the information of c is lost. this is what
   resetting does, and retaining is the vice versa. i also imagine that
   values such as 0.5 could be used to diminish the importance of certain
   information, but not completely wipe it out.

   our (unfinished) cell state computational graph now looks like this:
   [1*_mbua8vdatbyrexpdpjcca.png]
   this is closer to what an lstm looks like, though we   re not exactly
   there yet.

   sidenote: don   t be scared whenever you see the word    multiplicative   
   and don   t immediately think of    vanishing    or    exploding   . it depends
   on the context. here, as i   ll show mathematically in a bit, it   s fine.

   this concept in general is known as gating, because we    gate    what can
   flow in and out of the lstm cell. what we actually multiply and add by
   to reset, write, and read are known as the    gates   . there are four such
   gates:
     * f: forget gate. this is the    reset    tool that wipes out,
       diminishes, or retains information from the previous cell state.
       it   s the first interaction we make, and it   s multiplicative. that
       is, we multiply it with the cell state. the sigmoid function is
       used to compute the forget gate such that its values can be in the
       range 0 to 1. when a value is 1, we    remember    something, and when
       it is 0 we    forget   . we might choose to forget, for example, when
       see a period or some sort of end of sentence marker. this is
       counterintuitive    i guess it should really be called the    remember
       gate   !
     * g: ?. this gate doesn   t really have a name, but it   s partly
       responsible for the    write    process. it stores a value between -1
       and 1 that represents how much we want to add to the cell state by,
       and represents the input to the cell state. it   s computed with the
       tanh function. we apply a bounded function to it such that the cell
       state acts as a stable counter, and it also introduces more
       complexity. (and it works well.)
     * i: input gate. this is the other gate responsible for the    write   
       process. it controls how much of g we    let in   , and is thus between
       0 and 1, computed with sigmoid. it   s similar to the forget gate in
       this sense, in that it blocks input like the forget gate blocks the
       incoming cell state. we multiply i by g and add this to the cell
       state. since i is in the range 0 to 1, and g is in the range -1 to
       1, we add a value between -1 and 1 to the cell state. intuitively,
       this sort of acts as decrementing or incrementing the counter.
     * o: output gate. this is also passed through sigmoid, and is a
       number between 0 and 1 that modulates which aspects the hidden
       state can draw from the cell state. it enables the    read from
       memory    operation. it multiplies with the tanh of the cell state to
       compute the hidden state. so, i didn   t bring this up before, but
       the cell state leaks into a tanh before h_t is computed.

   here   s our updated computational graph for the cell state:
   [1*3xq3p-nvgxqxxpsxuevwdw.png]

   looks like i   m starting to create a complex diagram of my own. damn.     
   i guess lstms and immediately interpretable diagrams just weren   t meant
   to be!

   basically, f interacts with the cell state through a multiplication. i
   interacts with g through a multiplication as well, the result of which
   interacts with the cell state through an addition. finally, the cell
   state leaks into a tanh (that   s the shape of the tanh function in the
   circle), the result of which then interacts with o through
   multiplication to compute h_t. this does not disrupt the cell state,
   which flows to the next timestep. h_t then flows forward (and it could
   flow upward as well).

   here   s the equation form:
   [1*b9qd1pw1kym_zcg0ihpfua.png]
   each gate should actually be indexed by timestep t         we   ll see
   why soon.

   as you can see, our cell state has no activation function; the
   activation function is simply the identity function! yet, the cell
   state usually doesn   t explode         it stays stable by    forgetting    and
      writing   , and does interesting things with this gating to promote
   context, fine control over memory, and long-term dependency learning.

   so, how are the gates calculated? well, all of these gates have their
   own learnable weights and are functions of the last timestep   s hidden
   state flowing in and any current timestep inputs, not the cell state
   (contrary to what i may have implied earlier with the gradient flow
   diagrams). this should make sense when you think about it; i mean,
   firstly, the g and i gates literally represent input, so they better be
   functionally dependent on hidden states and input data! on an intuitive
   level, the gates help us modify the cell state, and we modify the cell
   state based on our current context. external stimulus that provide
   context should be used to compute these gates, and since context =
   input + hidden states our gates are functionally dependent on input and
   hidden states.

   since every gate has a different value at each timestep, we index by
   timestep t just like for hidden states, cell states, or something
   similar.

   we could generalize for multiple hidden layers as well:
   [1*bwz6e_ifi6utlksnsoq1yg.png]

   but, for simplicity   s sake, let   s assume we are at the first hidden
   layer, or that there is only one hidden layer in the lstm. this way, we
   can obfuscate the     term and ignore influence from hidden states in the
   previous depth. we   ll also forget about edge cases and assume input
   exists at the current timestep. in practice, we obviously can   t make
   said assumptions, but for the sake of demonstrating the equations it
   becomes too tedious otherwise.

   sidenote: we make this assumption for the rest of the discussion on
   lstms in this article.
   [1*hp6i692iv7oc6akcwindaw.png]

   like with the id56 hidden state, the index of each weight matrix is
   descriptive; for example, w_xf are the weights that map input x to the
   forget gate f. each gate has weight matrices that map input and hidden
   states to itself, including biases.

   and this is the beauty of lstms; the whole thing is end-to-end
   differentiable. these gates can learn when to allow data to flow and
   what data should flow depending on the context it sees (the input and
   the hidden states). it learns this based on patterns it sees while
   training. in this sense, it   s sort of like how a id98 learns feature
   detectors for images, but the patterns are way more complex and less
   human interpretable with lstms. this is why they perform so well.
   [1*vjl6ontlk77gpo2xmfch7g.png]
                  : perhaps your immediate reaction.

   okay, this looks scarier, but it   s actually not much different to what
   we had before, especially once you look past the intimidating web of
   arrows. one notable change is that we   re showing the previous hidden
   state in time and the current input flowing in. this diagram makes the
   assumption that we   re in the first layer and at some timestep > 1 where
   input exists. we then show how the f, i, g, and o gates are computed
   from this information         the hidden state and inputs are fed into an
   activation function like sigmoid (or, for g, a tanh; you can tell
   because it   s double the height of the others)         and it   s expressed
   through the web of arrows. it   s implied that we weight the two terms
   entering our id180, adding them up with a bias vector,
   but it   s not necessarily explicit in the diagram.

   let   s embed this into our overall lstm diagram for a single timestep:
   [1*0h88nxefxkb-xd1rbq4lga.png]

   now let   s zoom out and view our entire unrolled single layer, three
   timestep lstm:
   [1*-lhik-yasxk88gcpveeirq.png]

   it   s beautiful, isn   t it? the full screen width size just adds to the
   effect! [40]here   s a link to the full res version.

   the only thing that would look more beautiful would be multiple lstm
   cells that stack on top of each other (multiple hidden layers)!     

fixing the problem with lstms (part ii)

   you   ve come a long way, young padawan. but there   s still a bit left to
   go. part i focused on the motivation for lstms, how they work, and a
   bit on why they reduce the vanishing gradient problem. now, having a
   full understanding of lstms, part ii will hone in on the latter
   part   analyzing on a more close, technical level why our gradients stop
   vanishing as quickly. you won   t find a lot of this information online
   easily; i had to search and ask left and right to find an explanation
   better and more comprehensive than what you   ll find in other current
   tutorials.

   firstly, truncated bptt is often used with lstms; it   s a method to
   speed up training. in particular, note that if we input a sequence of
   length 1000 into an lstm, and want to train it, it   s equivalent to
   training a 1000 layer neural network. doing forward and backwards
   passes into this is very memory and time consuming, especially while
   backpropagating the error when we need to compute a derivative like
   this:
   [1*uqc4irifcdfoiwd8zvqw2a.png]

      which would include a lot of terms.

   when we backprop the error, and add all the gradients up, this is what
   we get:
   [1*ucovp6wos9mhzh8wkiykbq.png]

   truncated bptt does two things:
     * instead of doing a forward pass on the whole sequence and then
       doing a backwards pass, we process the sequence timestep by
       timestep and do a backwards pass    every so often   . that is         we
       compute h_1 and c_1, then h_2 and c_2, then h_3 and c_3, and then
       at some point in time, quantified by k1, we do a backwards pass.
       every k1 timesteps, we perform bptt; if k1 = 10, for example, then
       once we compute h_10 and c_10 we perform bptt. same for h_20 and
       c_20, and so on so forth. when we perform the backwards pass, our
       error j won   t be the same as if we did a full forwards pass and
       full backwards pass, since we haven   t observed all the outputs
       yet   we wouldn   t have even computed all the potential outputs yet!
       instead, the error describes what we   ve observed and computed so
       far, because we process the sequence timestep by timestep.
       intuitively, it   s like we train on a small subset of the training
       sequence, and this subset increases in length each time, which
       enables us to continue learning long-term dependencies. we could
       denote the error at timestep t    where t is a multiple of k1         with
       truncated backprop as j^t. so:

   [1*br6eowvumgtnox3nqkzvpa.png]

   for example, if t = 20 and k1 = 10, our second (because 20    10 = 2)
   round of bptt would be:
   [1*pg91-tmnosh9b7py0wjcdq.png]
     * on top of this, instead of backpropagating from j^t all the way to
       the first timestep c_1, we set a cut-off point. this cut-off point,
       quantified by k2, is the timestep at which our cell states stop
       contributing to the overall gradient. for example, if k2 = 10, and
       we   re backpropagating at t = 20, then c_10 is the final cell state
       to contribute to the overall gradient. everything before c_10 will
       have no say. this is designed such that we avoid computing
       derivatives between cell states far apart in time, which would
       include a huge number of terms (as mentioned earlier). the equation
       is now:

   [1*gmp4nvsdbtyyrwo7ffrw2g.png]

   so, with t = 20, k2 = 10, and k1 = 10, our second round of bptt would
   follow:
   [1*wyhlrzljjmheafksgg0jqg.png]

   both k1 and k2 are hyperparameters. k1 does not have to equal k2.

   these two techniques combined enables truncated bptt to not lose the
   ability to learn long term dependencies. here   s a formal definition:

     [truncated bptt] processes the sequence one timestep at a time, and
     every k1 timesteps, it runs bptt for k2 timesteps, so a parameter
     update can be cheap if k2 is small. consequently, its hidden states
     have been exposed to many timesteps and so may contain useful
     information about the far past, which would be opportunistically
     exploited.

         [41]   training recurrent neural networks   , 2.8.6, page 23

   the same paper gives nice pseudocode for truncated bptt:
   [1*0snub2iyt1rna7jsgg-7gq.png]

   the rest of the math in this section will not be in the context of
   using truncated backprop, because it   s a technique vs. something rooted
   in the mathematical foundation of lstms.

   moving on         before, we saw this diagram:
   [1*n26drgfekc-xnmqc2lw7cw.png]

   in this context,   w = i     g, because it   s the value we   re adding to the
   cell state.

   but this diagram is a bit of a lie. why? it ignores forget gates. so,
   does the presence of forget gates affect the vanishing gradient
   problem? quite significantly, actually. how? let   s bring up our cell
   state equation to see:
   [1*i6rbrx0k9mklxewd4korcw.png]

   with the forget gate, we now include a multiplicative interaction. our
   new diagram will look like this:
   [1*uvx1vl6adqgtbeksawx7bw.png]
   do not confuse forget gate    with function   w in this diagram. i know,
   it   s confusing        

   when our gradients flow back, they will be affected by this
   multiplicative interaction. so, let   s compute the new derivative:
   [1*60xffjvc0t9a0ekdmtap_q.png]

   this seems super neat, actually. obviously the gradient will be f,
   because f acts as a blocker and controls how much c_t-1 influences c_t;
   it   s the gate that you can fully or partially open and close that lets
   information from c_t-1 flow through! it   s just intuitive that it would
   propagate back perfectly.

   but, if you   ve payed close attention so far, you might be asking:
      wait, what happened to   w   s contribution to the gradient?    if you   re a
   hardcore mathematician, you might also be worried that we   re content
   with leaving the gradient as just f. this is because the gates f, i,
   and g are all functions of c_t-1; they are functions of h_t-1, which
   is, in turn, a function of c_t-1! the diagram shows this visually, as
   well. it seems we   re failing to apply calculus properly. we   d need to
   backprop through f and through i     g to complete the derivative.

   let   s walk through the differentiation to show why you   re actually not
   wrong, but neither am i:
   [1*x1mvdnbzomz1cnhjo23tzg.png]

   now, with the first derivative, we need to apply product rule. why?
   because we   re differentiating the product of two functions of c_t-1.
   the former being the forget gate, and the latter being just c_t-1.
   let   s do it:
   [1*twjnkg6vtuike1pkaztzza.png]

   then, from product rule:
   [1*cgq4unwxun6rq6h00gdzwg.png]

   that   s the first derivative done. we purposely choose not to compute
   the derivative of the forget gate with respect to the previous cell
   state on previous. you   ll see why in a bit.

   now let   s tackle the second one:
   [1*1t0iang6vy4peoz-ybkjuw.png]

   you   ll notice that it   s also two functions of c_t-1 multiplied
   together, so we use the product rule again:
   [1*tvfrkcc1t7ergyplide19a.png]

   so:
   [1*s2nnva6yhb2auzdyyalbea.png]
   once again, we purposely do not simplify the gate derivative terms.

   thus, our overall derivative becomes:
   [1*o0dzu_s9wxotyfokqo1a0a.png]
   notice that the first term in this derivative is our forget gate.

   pay attention to the caption of the diagram.

   this is actually our real derivative. modern lstm implementations just
   use an auto differentiation library to compute derivatives, so they   ll
   probably come up with this. however, effectively (or, rather,
   approximately), our gradient is just the forget gate, because the other
   three terms tend towards zero. yup         they vanish. why?

   when we backprop error in lstms, we backprop through cell states to
   propagate the error from the outputs to the cell state we want. for
   example, if we want to backprop the error from the output at time t
   down k timesteps, then we need to compute the derivative of the cell
   state at time t to the cell state at time t-k. look what happens when
   we do that:
   [1*dbfbl6ncqp94lnyb0tafwg.png]

   we didn   t simplify the gate w.r.t. cell state derivatives for a reason;
   as we backpropagate through time, they begin to vanish. thus, whatever
   they multiplied with is killed off from making contributions to the
   gradient, too. so, effectively:
   [1*q9bj7jxq08yvfbw_osjabw.png]

   the rationale behind this is pretty simple, and we don   t need math for
   it; these gates are the outputs of non-linearities eg. sigmoid and
   tanh. if we were to get the derivative of them in getting our cell
   state derivative, then this derivative would contain the derivatives of
   sigmoid/tanh in them. but, just because we don   t need to use math to
   show this, doesn   t mean we don   t want to      :
   [1*-mudovq8ovejmwpnofsi1g.png]
   i obfuscated the input to the sigmoid function for the input gate, just
   for simplicity.

   recall from our vanishing gradient article that the max output of
   sigmoid   s first order derivative is 0.25, and it   s something similar
   for tanh. this becomes textbook vanishing gradient problem. as we
   backprop through more and more cell states, the gradient terms become
   longer and longer, and this will definitely vanish. when they don   t
   vanish, they   ll be super minor contributions, so we can just leave them
   out for brevity.

     sidenote: one person reached out to me unsure of why gradients with
     long terms         aka, that are equal to the product of a lot of
     terms         usually vanishes/explodes. here   s what i said in response:

        if you have long gradient terms, you probably have the vanishing
     gradient problem, unless you can guarantee those terms are around 1
     each. if they   re not, it   ll explode or vanish. and, given the nature
     of the problems where this is an issue, it   s very unlikely they   ll
     be around 1 each. especially if they are the output some non-linear
     function like sigmoid/tanh or their derivatives.

     for example, let   s say the gradient term = k_1    k_2    k_3          
     k_100. 100 terms in this product.

     if each of these terms is, let   s say, around 0.5, then you have
     0.5         = some absurdly low number. if you have each term be arond
     1.5, then you have 1.5         which is some absurdly high number.
     when we introduce tanh/sigmoid and/or their derivatives in these
     huge products, you can guarantee that they   ll saturate and die off.
     as mentioned, the max for sigmoid   s first order derivative is 0.25,
     so just imagine something like 0.25        .

   ultimately, the reason i obfuscate these terms that vanish in the
   derivative is because i would like to show the effect of the forget
   gate on gradient flow now. if i included the other terms, the same
   implications would be present, but the math would just take longer to
   type out and render.

   because   w = i     g, we can redraw our diagram showing that   w won   t
   make any contributions to the gradient flow back. again           w does, but
   it   s effectively negligible, so we can just exclude it from our updated
   gradient flow diagram, which follows:
   [1*rshulzcgy6p-5bkke99q-q.png]

   but wait! this doesn   t look good; the gradients have to multiply by
   this f_t gate at each timestep. before, they didn   t have to multiply by
   anything (or, in other words, they multiplied by 1) and flowed past
   super easily.

   machine learning researchers coined a name for the type of function we
   had before we introduced the forget gate where the derivative of one
   cell state w.r.t. the previous is 1.0:    constant error carousel    (cec).
   with our new function, the derivative is equal to f. you   ll see this
   referred to as a    linear carousel    in papers.

   before we introduced a forget gate         where all we had was the additive
   interaction from   w         our cell state function was a cec:
   [1*9o__qovok1wxjdfy6m4yaq.png]
   a cec         same as before, but no forget gate.

   the derivative of this cell state w.r.t. the previous one, again as
   long as we don   t backprop through the i and g gates, is just 1. that   s
   why gradients flow back super comfortably, without vanishing at all.
   basically, for a cec to exist in this context, the coefficient of c_t-1
   needs to be 1.

   once we introduced this multiplicative interaction (for good reason),
   we got a linear carousel; the coefficient of c_t-1 is f. so, in our
   case, when f = 1 (when we   re not going to forget) our function becomes
   a cec, and our gradients will pretty much never vanish. if it   s close
   to 0, though, the gradient term will immediately die. gradients will
   stay on the carousel for a while until the forget gate is triggered;
   the effect on the gradient is like a step function, in that it   s
   constant with a value of 1 and then drops off to zero/dies when we have
   f     0.

   intuitively, this seems problematic. let   s do some math to investigate:
   [1*ubqehayw7bmv_tdf-cvuwg.png]

   the derivative of a cell state to the previous is f_t. the derivative
   of a cell state to two prior cell states is f_t     f_t-1. thus:
   [1*p9ondets7tr-zuu-1tuatw.png]

   as we backpropagate through time, these forget gates keep chaining up
   and multiplying together to form the overall gradient term.

   now, imagine an lstm with 100 timesteps. if we wanted to get the
   derivative of the error w.r.t. a weight like w_xi, to optimize it,
   remember that with bptt we add up or average all the gradients from the
   different timesteps:
   [1*rhb_2do5muljvzv9qxasmg.png]

   ok. now let   s look at an early (in time) term, like the gradient
   propagated from the error to the third cell:
   [1*faayljpgspjrgjgoyc8xcw.png]

   remember that j is an addition of errors from y individual outputs, so
   we backpropagate through each of the outputs first:
   [1*vsybuvtlgl-cqqui1pqbag.png]

   the first few terms, where we backprop y_k to c_3 where k < 3, would
   just be equal to zero because c_3 only exists after these outputs have
   been computed.

   let   s assume that y = 100 and continue with our assumption that t = 100
   (so each timestep gives rise to an output), for simplicity. with this,
   let   s now look at the last term in this sum.
   [1*jjiqxpb1mhjdn5kaookqka.png]

   that   s a lot of forget gates chained together. if one of these forget
   gates is [approximately] zero, the whole gradient dies. if these also
   tend to be a small number between 0 and 1, the whole thing will vanish,
   and c_3 won   t make any contributions to the gradient here.

   this isn   t intrinsically an issue though! because, when a forget gate
   is zero, it means that cell is no longer making any contributions past
   that point. if f_4 is zero, then any y outputs at/past timestep 4 won   t
   be influenced by c_3 (as well as c_2 and c_1) because we    erased    it
   from memory. therefore that particular gradient should be zero. if y_80
   is zero, then any outputs at/past timestep 80 won   t be influenced by
   c_1, c_2,     , c_79. same story here. if these forget gates are between
   0 and 1, then the influence of our cell decays over time anyways, and
   our gradients will be very small, so they   ll reflect that. [42]gers
   1999 calls this    releasing resources   .

   cell c_3 will still contribute to the overall gradient, though. for
   example, take this term:
   [1*kzjk3wczpyg_qnjkmb1hka.png]

   here, we   re looking at y_12 instead of y_100. chances are that, if you
   have a sequence of length 100, your 100th cell state isn   t drawing from
   your 3rd; the forget gate would have been triggered at some point by
   then. however, the 12th cell state probably will still be drawing from
   the ones before it.

   if we decide not to forget in the first 12 timesteps, ie. f_1     f_12
   are each not far from 1, then c_3 would have more influence over y_12
   and the error that stems from y_12. thus, the gradient would not vanish
   and c_3 still contributes to update w_xi, it just doesn   t contribute a
   gradient where it   s not warranted to (that is, where it doesn   t
   actually contribute to any activation, because it   s been forgotten). to
   summarize: one activated forget gate will indeed kill off gradient flow
   to cell(s), but that is a good thing because the network is learning
   that that gradient from the future has no benefit and is completely
   irrelevant to those particular cell(s), since those cells have been
   forgotten by then. in practice, different cells learn different ranges
   of context, some short, some long. this is a benefit for lstms.

   so, given a gradient between two cell states in time, when all of these
   forget gates are [approximately] equal to 1, the gradient signal will
   remain stable, because we   re multiplying by 1 at each
   timestep         effectively, not multiplying by anything at all. in such a
   case, our gradient flow diagram would look like this:
   [1*rbjm9f6zz8drqndlwd7wvq.png]
   it   s    it   s beautiful!

   the gradient will have literally zero interactions or disturbances, and
   will just flow through like it   s driving 150 mph on an empty
   countryside america highway. the beauty of cecs is that they   re always
   like this.

   but, let   s get back to reality. lstms aren   t cecs. one disadvantage of
   these forget gates chaining together is that it could block learning.
   that is, when we set out to train our lstm, the forget gates have not
   been learned; we have to learn them while we learn everything else. so,
   if they all start around 0, no gradients will flow through our cell
   states when we perform bptt, and learning won   t happen at all.

   the obvious solution is to set the forget gate bias to a very large
   value when training, so it starts at 1 instead of 0 (because y = 1 is
   to the far right of the sigmoid function, so adding to the input will
   ensure ~1 will be the output). in early stages of training, the forget
   gates equalling/approximating 1 will result in learning not being
   blocked. so many papers do this and mention it explicitly such that
   this forget gate bias could even be considered a hyperparameter.

   by introducing forget gates, we stray from cecs and thus the guarantee
   that our gradients will never ever vanish. but, again, we do it for
   good reason. and when gradients vanish it   s because we chose to forget
   that cell         so it   s not necessarily a bad thing. we just need to make
   sure the forget gates don   t block learning in initial stages of
   training; in such a case, we shouldn   t need to bother about vanishing
   gradients too much.

   here   s a more technical explanation:

   iframe: [43]/media/f077b0ddd653f1c9f755d681a53be4a5?postid=10300100899b
     __________________________________________________________________

   we can try computing some more derivatives, just for fun! let   s sub in
   real values for the timesteps, backprop across more than one timestep,
   and do it for a gate this time.

   we   ll expand c_4 and express it in terms of our gates only. in the
   process, each c_t, except c_1, will collapse into a few interactions
   between the f, i, and g gate:
   [1*2lzxa4yamgcjqoriri1-_w.png]

   now, let   s get the derivative of c_4 with respect to one of the
   earliest possible gates, like g_2. in the expression above, this turns
   out to just be the coefficient of g_2:
   [1*rhdurdan9sknchwfyvk38w.png]

   we experience the same neatness here as with the cell state backprop!
   it makes complete sense that the gradient would be i_2     f_3     f_4,
   since i_2 controls what influence g_2 has over c_2, f_3 controls what
   influence c_2 has on c_3, and f_4 controls what influence c_3 has over
   c_4. notice the chaining up of the forget gates     ; everything about the
   carousels i just talked about         and what they imply about vanishing
   gradients         applies here.

   i   ll leave it up to you to derive something similar for the other
   gates.

   and that   s it! that   s why lstms rock their socks off when it comes to
   keeping their gradients in check.

   here   s a neat gif showing a visual representation of the gradients that
   exist at each timestep, starting from timestep 128 and going all the
   way to the first, during backprop. more noise represents greater
   values:

   iframe: [44]/media/c239248e2e0b9a4aadc7b43d8c08ca12?postid=10300100899b

   super highway indeed. [45]imgur.com/gallery/vanahke.

   as you can see, the vanilla id56   s gradients die off way quicker than
   the lstm   s. the id56 is almost immediate in comparison. lstms seem like
   a super highway indeed, although it does seem that they do vanish. in
   this diagram, it can be suggested that the gradients perhaps die for
   the lstm eventually because we chose to forget early cell states;
   again, this depends on the application at hand, and is learnable. (i   m
   not sure if this gif uses truncated backprop, so that could be another
   thing. in general, i don   t know the context of this gif.) also, part of
   the gradient signal definitely vanishes   it   s the signals that pass
   through the f/i/g gates that we looked at earlier and obfuscated from
   the cell state   cell state derivative. we showed they would vanish
   because of tanh/sigmoid derivatives; initially, these signals will make
   a fairly significant contribution, but over time they   ll get smaller
   and smaller. that   s the explanation for this gif.

   exploding gradients is still an issue, though. recall that when we have
   a bunch of gradient terms added together, if some of them vanish it
   doesn   t mean the whole thing will vanish (versus if they were
   multiplied together). however, if some of the gradients explode, the
   whole thing explodes; x + 0 = x, but x +     =    . if cell states become
   unstable and grow too much in some rare scenario, then our gradients
   could explode. in such a case we   d need to implement gradient clipping,
   which is where we choose some arbitrary threshold that gradients cannot
   be larger than; so, grad = min(grad, clip_threshold). this would enable
   the lstm to deal with such cases without essentially collapsing. many
   successful lstm applications use gradient clipping.

   usually, though, exploding gradients are avoided because sooner or
   later the forget gate in the carousel is triggered and we reset the
   memory.
     __________________________________________________________________

   there are variants of lstms. people have tried modifying the model,
   like computing the hidden state without using tanh activation (so h_t =
   o     c_t) or ditching the i input gate and only using g, since that
   would still satisfy the -1 to 1 range. the results didn   t change by
   much.

   in fact, some researchers even applied evolutionary algorithms to spawn
   and test a bunch of variants on the standard lstm equations. most of
   the good ones just worked roughly the same.

   this highlights an issue with lstms         they are definitely fairly
   handwavy. we use them because their architecture allows us to prevent
   gradients from vanishing such that we can learn long-term dependencies,
   but there   s not much theoretical or empirical backing for them. anns
   and id56s make sense in that they   re biologically inspired and that
   they   re essentially just deep composite functions that have parameters
   we can optimize. lstms stray so far from statistical methods and
   introduce complex concepts/architectures that work but aren   t
   necessarily justified from the get-go. fully understanding why lstms
   work so well and coming up with better/simpler architectures is a hot
   topic of research right now.

   there are also other variants of id56s, similar to lstms, like [46]grus
   (id149). lstm is still the king, but grus in particular
   have grown in popularity and are seen in many recent, well-respected
   research papers. it   s a must learn next to lstms, but this article
   would get too bloated with it. tl;dr: grus have a less complex
   architecture than lstms but achieves similar results, and they can
   control the flow of information without requiring a memory cell. and,
   they   re fairly new. (see, told you    coming up with better/simpler
   architectures is a hot topic of research right now    is true!)

yay id56s!

   wanna see a couple cool things? practical applications were talked
   about in the first section, and the next section will walk through more
   technical applications of id56s ie. in recent research papers. this
   section, instead, contains some fun things id56s have done or produced
   that   ll ease yourself from the hell that was actually understanding
   them. as simply as we try to convey things on this blog, lstms be
   lstms.

   sidenote: now, don   t be frightened by    id56s   . do be frightened by
      vanilla id56s   , in most contexts, but we use id56 as an umbrella term,
   and it most often refers to an lstm or a similar variant like gru.

   many if not all of these are taken from andrej karpathy   s [47]cs231n
   lecture, or his blog post on the same subject:
   [48]the unreasonable effectiveness of recurrent neural networks
   musings of a computer scientist.karpathy.github.io

   you should most certainly visit either his blog post or lecture for
   info on exactly how these experiments were conducted, and for more
   interpretation on their results. taking a look at the    visualizing the
   predictions and the    neuron    firings in the id56    section would also be
   helpful to gain more insight and intuition on how id56s work and learn
   over time.

   a recurrent neural network generated this body of text, after it    read   
   a bunch of shakespeare:
   [1*bkvfhx8nyl1-nhmzz_bbcq.png]

   similarly, karpathy gave an lstm a lot of paul graham   s startup advice
   and life wisdom to read, and it produced this:

        the surprised in investors weren   t going to raise money. i   m not
     the company with the time there are all interesting quickly, don   t
     have to get off the same programmers. there   s a super-angel round
     fundraising, why do you can do. if you have a different physical
     investment are become in people who reduced in a startup with the
     way to argument the acquirer could see them just that you   re also
     the founders will part of users    affords that and an alternation to
     the idea. [2] don   t work at first member to see the way kids will
     seem in advance of a bad successful startup. and if you have to act
     the big company too.   

   a lot of relevant terminology, but it doesn   t really    come together     .

   an lstm can even generate valid xml, after reading wikipedia!:
<page>
  <title>antichrist</title>
  <id>865</id>
  <revision>
    <id>15900676</id>
    <timestamp>2002-08-03t18:14:12z</timestamp>
    <contributor>
      <username>paris</username>
      <id>23</id>
    </contributor>
    <minor />
    <comment>automated conversion</comment>
    <text xml:space="preserve">#redirect [[christianity]]</text>
  </revision>
</page>

   after a similar lstm read the raw latex version of an advanced algebra
   textbook, it came up with this         put frankly         fancy looking bogus.
   let   s be real, you could definitely believe this was actual math     :

   iframe: [49]/media/32e75fb2f7f388775689a155c5c27d86?postid=10300100899b

   an lstm also read the linux source code, and tried to write some code
   of its own:
/*
 * increment the size file of the new incorrect ui_filter group information
 * of the size generatively.
 */
static int indicate_policy(void)
{
  int error;
  if (fd == marn_ept) {
    /*
     * the kernel blank will coeld it to userspace.
     */
    if (ss->segment < mem_total)
      unblock_graph_and_set_blocked();
    else
      ret = 1;
    goto bail;
  }
  segaddr = in_sb(in.addr);
  selector = seg / 16;
  setup_works = true;
  for (i = 0; i < blocks; i++) {
    seq = buf[i++];
    bpf = bd->bd.next + i * search;
    if (fd) {
      current = blocked;
    }
  }
  rw->name = "getjbbregs";
  bprm_self_clearl(&iv->version);
  regs->new = blocks[(bpf_stats << info->historidac)] | pfmr_clobathinc_seconds
<< 12;
  return segtable;
}

   superintelligence much    self-recursive improvement much    the end of the
   universe much   

   nope. just some code doesn   t compile or make any sense. it even has its
   own bogus comments!

   generating music? easy! a fun watch:

   iframe: [50]/media/8de70b7fa3e5cb979099278112052953?postid=10300100899b

   a more informative watch:

   iframe: [51]/media/3f992ef4ac506dafa8d2d8badfc31dc2?postid=10300100899b

   something even cooler and    creepier (seriously, the results after the
   first couple iterations of training are so unsettling):

   iframe: [52]/media/b5f70a5d514e61ad4646217c70974843?postid=10300100899b

in practice

   so we   ve seen how id56s work in theory; now where do they fit in in
   practice?

   as it turns out, recurrent neural networks can do a whole lot. i   ll try
   to cover a few of the important, significant, and interesting uses that
   have cropped up over the last few years.

id182

   the problem: giving the network access to a sequence of vectors is fine
   and dandy, but what if we want our output at time t to be conditioned
   on an input vector that comes at a later timestep? take the example of
   id103, where our input vectors are some kind of audio
   features at time t and the output is the predicted [53]phoneme at that
   time. in our traditional id56 architecture, the output at time t is
   conditioned only on input vectors 1..t, but as it turns out future
   information might be useful too. the sounds at time step t+1 (and maybe
   t+2, t+3,    ) are likely part of the same phoneme, and therefore could
   help us make more accurate predictions. but our network won   t have
   access to them until we already output a prediction at time t. that   s
   bad.

   the solution: we essentially    double up    each id56 neuron into two
   independent neurons         a    forward    neuron and a    backward    neuron. the
   forward neuron is the same as a regular id56 neuron, which gets inputs
   0..t sequentially, updating its internal state and outputting some
   value at each time step along the way. the backward neuron follows the
   same general principle, but it sees the input vectors in reverse order.

   we   ll look at an example to make sense of all this.
   [1*vsvw39sw0xewrlijlrb3qg.png]
   this is a typical recurrent neural network: at each timestep, the
   hidden state is updated based on the latest input.
   [1*jznjmhjyfvchrpktdmvoxq.png]
   this is a bidirectional recurrent neural network. there are two
   neurons: one that takes inputs like normal, and one that takes them in
   reverse. their output is combined to produce one output.

   let   s walk through this timestep-by-timestep. at t=0, our vanilla id56
   cell takes the input, updates its hidden state, and outputs a value.
   now let   s look at the biid56: the    forward    half of our biid56 neuron
   does exactly the same thing, but the    backward    half looks through all
   of our inputs         in reverse order, t=t..0         and updates its hidden state
   with each one. then when we get to the t=0 input vector, it updates its
   hidden state one last time and outputs a final value. we then take this
   final output value and combine it with the    forward    half (   combine    is
   pretty loosely-defined, usually just by concatenation or addition).
   moving on to t=1, our    forward    part reads in the next input, updates
   state, and outputs another value. combined with the second-to-last
   output of our    backward    counterpart, and we have the second output of
   our biid56 neuron. rinse and repeat.

   and that   s the general idea. neat, right? biid56s (and their more adept
   cousin, bilstms) are used all over the place. maybe we   ll see them
   popping up in some of the other case studies that we   ll be looking at.

autoencoders

   remember when we talked about [54]autoencoders? turns out we can use
   id56s there too!

   let   s refresh: what is an autoencoder? put simply, it   s a clever way of
   tricking a neural network to learn a useful representation of some
   data. let   s say we have a dataset of images of faces, and we want to
   compress the thousands of numbers representing rgb values of pixels
   down into a 500-dimensional latent vector. we construct a network as
   such, where the middle layer has 500 neurons:
   [0*m1bvztz6uptyxoiy.]
   [55]https://en.wikipedia.org/wiki/autoencoder#/media/file:autoencoder_s
   tructure.png

      and train it to reproduce the input in the output.

   let   s explore this idea a little further. imagine that data is flowing
   through our network, starting with the input layer and through each
   subsequent layer. we can view each layer as performing a
   transformation, converting our input to another vector, and then that
   vector into another, until we get our output. if we train our network
   to reproduce the input, that means that each intermediate vector must
   still represent the same information as the input, in some form or
   another. essentially, the activations of each layer are a new
   representation of our input vector. if our network trains well, we can
   convert a 10,000-dimensional vector of pixel values into a
   500-dimensional vector of image features which can be converted back
   into a 10,000-dimensional vector of pixel values that approximates what
   the input would have been.
   [0*rp5vzyqdj9ji5wbk.]
   [0*tc8mc_nwmmqz15nd.]

   let   s make this a tad more concrete. we have two functions, f and g. f
   is our encoder, mapping from an n-long vector to an m-long vector. (n
   is the size of our input, m is the size of our latent representation.)
   g is our decoder, which maps back from an m-long vector to an n-long
   vector. in the normal autoencoder setting, both f and g are neural
   networks trained jointly (or different parts of a single network, same
   thing really) to reconstruct x.
   [0*1r_hem-ujpuvlbgm.]

   so, where do id56s fit in? let   s say our inputs are now sequences of
   vectors instead of a single vector. we can use a similar concept, with
   both the encoder and decoder represented using an id56. here   s how it
   works: we feed our input sequence into the encoder id56. with each input
   vector of the sequence, this encoder updates its internal state.
   eventually, once it has seen the entire input, we have some final
   network internal state which represents our entire input sequence.
   neat! now, we make the hidden state of our decoder id56 the initial
   hidden state of our encoder, and ask it to spit out a sequence.
   ideally, it spits out something close to what the initial sequence was.

   going back to our math-y definitions, we see that it basically fits in
   to the same framework, except we have q n-long vectors going into f and
   coming out of g. so q n-long vectors go in to f, and a single m-long
   vector comes out. we then give this m-long vector back to g, which
   spits out q n-long vectors.

   that was a lot of letters, but you get the idea (i hope).

   like much of deep learning, the concept itself is pretty simple, but
   the implications are pretty cool. we can take any sequence         a
   variable-length sequence, mind you         and convert it into a fixed-size
   vector. and then convert that back to a variable-length sequence.

   it turns out this model is actually incredibly powerful, so let   s take
   a look at one particularly useful (and successful) application: machine
   translation.

id4

   let   s take these ideas we just learned about sequence-to-sequence (or
   id195, for short) id56s and apply them to machine translation. we
   throw in a sequence of words in one language, and it outputs a sequence
   of words in another. simple enough, right?

   the model we   re going to look at specifically is google   s
   implementation of id4. you can read all the gory details [56]in their
   paper, but for now why don   t i give you the watered-down version.

   at it   s core, the gid4 architecture is just another id195 model. we
   have an encoder, consisting of 8 lstm layers with skip connections (the
   first layer is bidirectional). we also have a decoder, once again
   containing 8 lstm layers with skip connections. (a skip connection in a
   neural network is a connection which skips a layer and connects to the
   next available layer.) the decoder network outputs a id203
   distribution of words (well, sort of         we   ll talk more about that
   later), which we sample from to get our [translated] sentence.     

   here   s a scary diagram from the paper:
   [0*mk1bef8anmbavozd.]
   [57]https://arxiv.org/abs/1609.08144

   but there are a few other aspects to the gid4 that are important to
   note (there   s actually lots of interesting stuff going on in this
   architecture, so i really recommend you do [58]read the paper).

   let   s turn our attention to the center of the above diagram. this is a
   critical part of the gid4 architecture (and gid4 is certainly not the
   first to use attention) which allows the decoder to focus on certain
   parts of the encoder   s output as it produces output. specifically, the
   gid4 architecture differs from the traditional id195 model in that
   our encoder does not produce a single fixed-width vector (the final
   hidden state) representing the entire output. instead, we actually look
   at the output from each time step, and each time step gives us some
   latent representation. while decoding, we combine all of these hidden
   vectors into one context vector using something called soft attention.
   [0*ua03rdgdnwpw1_jd.]
   [59]https://arxiv.org/abs/1609.08144

   more concretely, that works like this (at every decoder time step). we
   first look at the output of the first decoder layer from the last time
   step. following the notation from the paper, we   ll call that yi-1. we
   also have a series of encoder outputs, x1   xm, one for each encoder
   timestep. for each encoder timestep, we give our special attention
   function yi-1 and xt and get back a single fixed-size vector st, which
   we then run through a softmax. so, we   ve converted our encoder
   information from that timestep (and some decoder information) into a
   single attention vector         this attention vector tells us which parts of
   the encoder output we should look at more closely. we multiply this
   attention vector by our encoder output xt, which has the effect of
      focusing    more on certain values and less on others. finally, we take
   the sum of those    focused    vectors over each encoder timestep to
   produce our attention context for this timestep ai, which is fed to
   every decoder layer.

   oh yeah, that attention function? that   s just yet another neural
   network.

   attention mechanisms like this one are pretty common in many deep
   learning architectures. this is an example of soft attention: we learn
   a distribution over our inputs and compute a weighted sum. this process
   is fully-differentiated, so we can use standard backpropogation to
   figure out how to train our attention model. another possible mechanism
   is called hard attention, in which we select just one of the possible
   inputs and    focus    solely on that input. this process is not
   differentiable, so we need to use some other algorithm (usually some
   kind of id23) to train a hard attention algorithm.

   gid4 combines all kinds of other cool ideas to achieve state-of-the-art
   results, including a wordpiece model which segments words into smaller
      wordpieces    to help translate rarer words and neat parallelization
   techniques that let them train this monstrosity of an architecture in
   reasonable time.

   a few months ago, [60]google put their gid4 model into production.
   cutting-edge research is being implemented in the real world at an
   incredible rapid pace within the field of machine learning, and this is
   just one of countless examples.

long-term recurrent convolutional networks

   (not to be confused with lcrns.)

   the problem: we have a sequence of images that we need to make
   predictions for. id98s are good at processing images, id56s are good at
   processing sequences   how do we put the two together?

   the solution: the solution proposed in [61]this paper is as
   straightforward as you would expect: take your image, extract features
   using a id98, and feed this feature vector to your lstm.
   [0*qiq7dvckhydxafz1.]
   [62]https://arxiv.org/abs/1411.4389

   that   s really all there is to it, and the reason it works is because
   ([63]as we   ve seen before) id98s are incredibly adept at converting raw
   pixel data to a more meaningful representation. this saves the lstms
   the problem of parsing through the pixels to figure out what   s going on
   in the image and allows the lstm weights to focus on converting a
   vector of image features into some meaningful sequence (say, a
   caption). it   s the same reason that using a id27 is often
   preferred to a one-hot vector when feeding in words to an nlp model:
   the more meaningful your representation is, the easier it is to make
   further predictions with it.

image captioning

   (to be confused with lcrns!)

   so there has been a lot of really impressive work on image captioning
   lately, but i wanna give a special shout-out to [64]this 2015 paper
   from karpathy et al. it was one of the first ml papers i had ever read,
   and really got me excited about the field. and, it uses id56s, so that   s
   cool too.

   the idea behind image captioning is kind of self-explanatory, but i   ll
   explain it anyway. you give the model an image, it gives you a caption.
   which is kind of insane if you think about it         a computer can go from
   pixels to interpreting what it   s seeing, and from that generate real
   and grammatical sentences to explain what it sees. i still can   t really
   believe stuff like this actually works, but somehow it does.

   the model described in this specific paper combines our old pal
   convolutional neural networks with our newly-discovered id56s. step 1 is
   to pass our image through a convolutional neural network and extract
   some features from the last fully-connected layer. this lets us convert
   our pixel representation of the image into something that   s hopefully a
   bit more meaningful. we take this image feature vector and use it to
   initialize the hidden state of our id56.

   this is where it gets cool. we feed our network a start token, and it
   gives us a word (more accurately, a distribution of words, which we
   sample to get the first word of our caption). we feed this word back as
   the next input, and sample another word from the output. and again, and
   again, and again, until we finally sample an end token and have a
   complete caption.

   it   s not strictly necessary to feed the word that we sampled back to
   the network, but that   s pretty common practice to help the network
   condition its output on the previous word (the hidden state is critical
   for this too, of course). the results from this particular paper were
   pretty cool, you can see some of the results [65]here.

id4, again

   yes, id4s are just that cool that i need to talk about them again.

   the problem: with our good ol    gid4 architecture, we can train a
   massive model to convert from language a to language b. that   s
   great         except, if we support more than a hundred languages, we need to
   train more than 10,000 different language-pair models, each of which
   can take months to converge. that   s no good, and it   s the reason that
   when google put gid4 in production, they only did so for eight
   language-pairs (still a monumental achievement). but   what if we didn   t
   need to train a separate model for each language pair? what if we could
   train one model for all the language pairs         impossible, right?

   the solution: apparently it   s not impossible, and to make things even
   crazier, we can use the original gid4 architecture without
   modification. the only real change is that we prepend a special
   language token to the beginning of each sequence telling it what
   language to translate to. (we also use one shared wordpiece model for
   all language, instead of one per language pair.)

   so we   ve condensed tens of thousands of id4 models into a single model
   that is able to translate to and from any language it was trained on.
   [66]the paper elaborates on the implications and benefits of this more
   than i will, but to summarize:
     * one model instead of tens of thousands. months of training time
       saved, simpler production deployment, fewer parameters         simplicity
       wins out over complexity.
     * we might have more training data for some language pairs than
       others. when we have separate models for each language pair, this
       means that the pairs with less data will have significantly poorer
       performance. if we put them all into one model, the language pairs
       with less data can still benefit from all of the data in the other
       language pairs, because all of the language pairs share weights
       (since they all use the same model).
     * this one is absolutely nuts. if we train our network to translate
       english     spanish and spanish     french, our network automatically
       knows how to translate english     french (reasonably well).

   expanding on that last point some more: the authors of the paper even
   found evidence of an interlingua, or an intermediate representation
   that is shared by multiple languages. being able to learn an
   interlingua is the ideal end goal to create a fully generalized
   multilingual id4: we learn an encoder/decoder to convert to/from the
   interlingua for each language, and we immediately know how to translate
   to and from that language. we aren   t quite there yet, but this is a
   major step in that direction. creating a larger multilingual id4 model
   and giving it even more data could be all it takes to achieve new
   state-of-the-art translation results.

so, yeah

   id56s are pretty awesome. there are new id56 papers published literally
   every day and it   s impossible to cover everything         if you think i
   missed something important, definitely [67]let me know. (from rohan:
   except id63s and learning to learn. those are dope, we
   know it, and we   re going to be covering them soon!)

building a vanilla recurrent neural network

   let   s get practical for a minute and see how we can build one of these
   things in practice. we   ll stick with char-id56 (the single layer
   character level language model we talked about much earlier) with raw
   numpy so we can see the nitty-gritty details, but if you   re using one
   of these in practice there are much better solutions! for
   out-of-the-box functional deep learning models [68]keras is the de
   facto framework that people seem to use. for more creative models and
   all kinds of other fancy stuff i   m a fan of the newly-released
   [69]pytorch, or the    older    [70]tensorflow.

   i   m going to walk us through [71]this implementation line by line so we
   can see exactly what   s going on. it   s really well-commented, so feel
   free to peruse it on your own too.

   afterwards, i challenge you to code an lstm!

   iframe: [72]/media/515164c3f20643d9534d745371d34b9f?postid=10300100899b

   import numpy as np

   well, duh.

   data = open(   input.txt   ,    r   ).read()
   chars = list(set(data))
   data_size, vocab_size = len(data), len(chars)
   print    data has %d characters, %d unique.    % (data_size, vocab_size)
   char_to_ix = { ch:i for i,ch in enumerate(chars) }
   ix_to_char = { i:ch for i,ch in enumerate(chars) }

   we load in our data and get a list of all of the characters that appear
   in it. we set up two dictionaries: one mapping characters to an index,
   and one for the reverse. we   ll use this when converting characters
   to/from a one-hot encoding later on.

   hidden_size = 100
   seq_length = 25
   learning_rate = 1e-1

   typical hyperparam stuff. our id56 layer will have a hidden size of 100,
   and we   ll train our network on batches of 25 characters at a time.
   since we   ll be training our network with bptt, we need to make sure the
   sequences are sufficiently short that we can unroll the network all the
   way and keep everything in memory. finally, set the learning rate
   to .1.

   wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden
   whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden
   why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output
   bh = np.zeros((hidden_size, 1)) # hidden bias
   by = np.zeros((vocab_size, 1)) # output bias

   we set up our parameters         note that this is just a typical id56, no
   fancy lstm cells. we have weight matrices for updating our hidden state
   with each input, updating our hidden state with each timestep, and
   producing an output (and biases for our hidden state + output). we
   could be doing some fancy weight initialization here, but some
   normally-distributed randomness is sufficient for breaking symmetry.

   now let   s talk id168. we start by computing the forward pass,
   then computing the backward pass, just like with any neural network.

   xs, hs, ys, ps = {}, {}, {}, {}
   hs[-1] = np.copy(hprev)
   loss = 0

   we start off by just setting up some variables to store our one-hot
   inputs, hidden states, outputs, and softmax probabilities.

   for t in xrange(len(inputs)):

   go through each timestep, and for each timestep   

   xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation
   xs[t][inputs[t]] = 1

   convert our input character at this timestep to a one-hot vector.

   hs[t] = np.tanh(np.dot(wxh, xs[t]) + np.dot(whh, hs[t-1]) + bh) #
   hidden state

   update our hidden state. we saw this formula already         use our wxh and
   whh matrices to update our hidden state based on the last state and our
   input, and add a bias.

   ys[t] = np.dot(why, hs[t]) + by

   compute our output   

   ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next
   chars

      and convert it to a id203 distribution with a softmax.

   loss += -np.log(ps[t][targets[t],0]) # softmax (cross-id178 loss)

   accumulate the loss for this time step as the negative log of the
   predicted id203. ideally, we would have a id203 of 1 for
   the actual next character. if it is 1, the loss is 0, log(1) = 0. as
   the predicted id203 approaches 0, the loss approaches inf,
   because log(0) = -inf.

   that   s it for the forward pass (not bad, right? boiled down, it   s like
   six lines of code. piece of cake).

   dwxh, dwhh, dwhy = np.zeros_like(wxh), np.zeros_like(whh),
   np.zeros_like(why)
   dbh, dby = np.zeros_like(bh), np.zeros_like(by)
   dhnext = np.zeros_like(hs[0])

   setting up some variables for our backward pass         the gradients of our
   weight matrices, the gradients for our biases, and the gradients from
   the next timestep (we   ll see how that works in a bit).

   for t in reversed(xrange(len(inputs))):

   go through our sequence in reverse as we back up the gradients.

   dy = np.copy(ps[t])
    dy[targets[t]] -= 1 # backprop into y. see
   [73]http://cs231n.github.io/neural-networks-case-study/#grad if
   confused here

   first, get the gradient of the output, dy. [74]as it turns out, the
   gradient of the cross-id178 loss is really as copying over the
   distribution and subtracting 1 from the correct class.

   [75]remember backpropogation? when we have a weighted sum, the gradient
   of the weights is just the corresponding value that it is being
   multiplied by, because the other terms drop out and that one weight is
   treated as a constant. so, computing the gradient of our why matrix is
   super simple: just multiply the gradient of loss w.r.t. the output (dy)
   by the derivative of the output w.r.t. why (which is just the hidden
   state at our given timestep), and we get the derivative of the loss
   w.r.t. why.
   [0*tvvksjjqam9cdjlk.]

   dwhy += np.dot(dy, hs[t].t)

   like the other gradients (except dy, of course) we accumulate these
   gradients over all timesteps and apply them at the end.
   [0*qkdwsxvej9fhq4ht.]

   dby += dy

   the derivative of loss w.r.t. output (dy) multiplied by the derivative
   of our output w.r.t. the bias (which is 1) gives us the derivative of
   our output w.r.t. the bias. so far so good.

   dh = np.dot(why.t, dy) + dhnext # backprop into h
   [1*cvr1t2s7gsc4sd6vowskha.png]

   we compute dl/dh using the chain rule, and accumulate it over all
   timesteps (hence + dhnext). we   ll need this for the next step.

   dhraw = (1         hs[t] * hs[t]) * dh # backprop through tanh nonlinearity

   this computes the derivative of the np.tanh(np.dot(wxh, xs[t]) +
   np.dot(whh, hs[t-1]) + bh) line from earlier.

   dbh += dhraw

   which is also our bh derivative, for the same reason that the by
   derivative was just dy.

   dwxh += np.dot(dhraw, xs[t].t)
   dwhh += np.dot(dhraw, hs[t-1].t)

   we accumulate our weight gradients.
   [1*uf-yebf0258uhbdc5qzlrw.png]
   [1*vl1lvzpjszkdpla9j5celg.png]

   dhnext = np.dot(whh.t, dhraw)

   and finally, store dh for this timestep so we can use it for the
   previous one.

   for dparam in [dwxh, dwhh, dwhy, dbh, dby]:
   np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding
   gradients

   last but not least, a little gradient clipping so we don   t get no
   exploding gradients.

   return loss, dwxh, dwhh, dwhy, dbh, dby, hs[len(inputs)-1]

   and then return all the gradients so we can apply an optimizer step.
   and that   s it for the backprop code; not too bad, right?

   def sample(h, seed_ix, n):

   this method is used for sampling a generated sequence from the network,
   starting with state h, first letter seed_ix, with length n.

   x = np.zeros((vocab_size, 1))
   x[seed_ix] = 1

   set up our one-hot encoded input vector based on the seed character.

   ixes = []

   and an array to keep track of our sequence.

   for t in xrange(n):

   to generate each character in our sequence   

   h = np.tanh(np.dot(wxh, x) + np.dot(whh, h) + bh)

   update our hidden state! we saw this formula in the last function, too.

   y = np.dot(why, h) + by
   p = np.exp(y) / np.sum(np.exp(y))

   generate our output and run it through a softmax. again, straight from
   the last function.

   ix = np.random.choice(range(vocab_size), p=p.ravel())

   sample from our output distribution using some numpy magic.

   x = np.zeros((vocab_size, 1))
   x[ix] = 1
   ixes.append(ix)

   convert the sampled value into a one-hot encoding and append it to the
   array.

   return ixes

      and of course, return the final sequence when we   re done.

   n, p = 0, 0

   n is the number of training iterations we   ve done. p is the index into
   our training data for where we are now.

   mwxh, mwhh, mwhy = np.zeros_like(wxh), np.zeros_like(whh),
   np.zeros_like(why)

   set up memory variables for the adagrad algorithm (out of scope of this
   post, maybe next time         it   s just a variant on id119).

   while true:

   training loop.

   if p+seq_length+1 >= len(data) or n == 0:

   this is a little check to see if we need to reset our memory because
   we   re starting back at the beginning of our data.

   hprev = np.zeros((hidden_size,1)) # reset id56 memory

      and if we are, reset the memory.

   p = 0

   and reset the data pointer.

   inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]
   targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]

   we grab a seq_length-long piece of the data as our input to the
   network. at each timestep, we want to predict the next character; this
   means that our    targets    will be the next character for each input. we
   get a snippet of the input the same length as the input, but offset by
   1, for the target.

   if n % 100 == 0:
   sample_ix = sample(hprev, inputs[0], 200)
   txt =       .join(ix_to_char[ix] for ix in sample_ix)
   print                 \n %s \n                 % (txt, )

   here we just print to the terminal a sample every 100 training steps so
   we can see how its doing. ideally, this will print out gibberish the
   first few times, before gradually printing out more and more reasonable
   language.

   loss, dwxh, dwhh, dwhy, dbh, dby, hprev = lossfun(inputs, targets,
   hprev)

   do a forward pass, backward pass, and get the gradients.

   smooth_loss = smooth_loss * 0.999 + loss * 0.001

   adagrad stuff.

   if n % 100 == 0: print    iter %d, loss: %f    % (n, smooth_loss) # print
   progress

   keep up with progress.

   for param, dparam, mem in zip([wxh, whh, why, bh, by], [dwxh, dwhh,
   dwhy, dbh, dby], [mwxh, mwhh, mwhy, mbh, mby]):
   mem += dparam * dparam
   param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update

   more adagrad. we should really do an article on optimization
   algorithms.

   p += seq_length # move data pointer
   n += 1 # iteration counter

   annnddd finally, we update our data pointer and iteration counter.

   and that   s it. we have an id56. neat-o. reminder: your challenge is to
   code an lstm    and tensorflow doesn   t count!

conclusion

   wow. that was a lot. if you came in knowing nothing or very little
   about recurrent neural nets, you sure as hell know a lot now. and you
   don   t just know about something cool; you know about something very
   important         something that can equip you to read and understand some of
   the most prominent and hottest recent research papers in machine
   learning.

   something this article didn   t do so good at was making sure the
   calculus and derivatives were in the context of operating on vectors
   (because, remember, id56s/lstms operate over vectors). in many cases the
   derivatives were in the 1-d context. it   s not something you need to
   worry about, but you might want to look into.

   we   re finally at the point where we can focus our energies on this blog
   towards cooler stuff including hot research papers like neural turing
   machines or learning to learn, case studies eg. alphago, other parts of
   machine learning and artificial intelligence (i   m, rohan, personally
   looking forward to optimization!), or different algorithms like gans.
   there   s very little compulsory content or    groundwork    we need to cover
   anymore. so, now, we   re officially onto the cool stuff.

   that   s right. a year of ai is officially    cool.
   [1*necebipfgd-z3_9r9usfgg.png]

     * [76]machine learning
     * [77]artificial intelligence
     * [78]data science
     * [79]deep learning
     * [80]algorithms

   (button)
   (button)
   (button) 2.1k claps
   (button) (button) (button) 22 (button) (button)

     (button) blockedunblock (button) followfollowing
   [81]go to the profile of rohan kapur

[82]rohan kapur

   rohankapur.com

     (button) follow
   [83]a year of artificial intelligence

[84]a year of artificial intelligence

   our ongoing effort to make the mathematics, science, linguistics, and
   philosophy of artificial intelligence fun and simple.

     * (button)
       (button) 2.1k
     * (button)
     *
     *

   [85]a year of artificial intelligence
   never miss a story from a year of artificial intelligence, when you
   sign up for medium. [86]learn more
   never miss a story from a year of artificial intelligence
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://ayearofai.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/10300100899b
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b&source=--------------------------nav_reg&operation=register
   8. https://ayearofai.com/?source=logo-lo_sb5irejhxqki---bb87da25612c
   9. https://ayearofai.com/tagged/algorithms
  10. https://ayearofai.com/tagged/today-i-learned
  11. https://ayearofai.com/tagged/case-studies
  12. https://ayearofai.com/tagged/philosophical
  13. https://ayearofai.com/tagged/meta
  14. https://ayearofai.com/@mckapur?source=post_header_lockup
  15. https://ayearofai.com/@mckapur
  16. https://medium.com/@lennykhazan
  17. https://medium.com/@mckapur
  18. https://medium.com/a-year-of-artificial-intelligence
  19. https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5
  20. http://getcontra.com/
  21. https://ayearofai.com/the-goal-of-our-blog-c104d7b6377a#.75o5qyayi
  22. https://symsys.stanford.edu/
  23. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  24. https://ayearofai.com/rohan-lenny-1-neural-networks-the-id26-algorithm-explained-abf4609d4f9d#.quwnoqtot
  25. https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b#.ebwc0kf4z
  26. https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b#.58nevwsoa
  27. https://arxiv.org/pdf/1606.04474.pdf
  28. http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf
  29. http://neuralnetworksanddeeplearning.com/chap4.html
  30. http://stats.stackexchange.com/a/221142/98975
  31. http://stackoverflow.com/a/7320/1260708
  32. https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4
  33. http://colah.github.io/posts/2015-08-understanding-lstms/
  34. https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52
  35. https://www.youtube.com/watch?v=co0a0qymfm8&index=10&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia
  36. http://sebastianruder.com/optimizing-gradient-descent/
  37. https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b
  38. https://ayearofai.com/media/aa3fb6f891aba6d55742cf7dabf3f7f7?postid=10300100899b
  39. http://www.dyoung.com/assets/images/articles images/article4_pph.jpg
  40. https://drive.google.com/file/d/0bwbwrptraa2zqusydxrkskd3yuu/view?usp=sharing
  41. http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf
  42. https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf
  43. https://ayearofai.com/media/f077b0ddd653f1c9f755d681a53be4a5?postid=10300100899b
  44. https://ayearofai.com/media/c239248e2e0b9a4aadc7b43d8c08ca12?postid=10300100899b
  45. http://imgur.com/gallery/vanahke
  46. https://en.wikipedia.org/wiki/gated_recurrent_unit
  47. https://www.youtube.com/watch?v=co0a0qymfm8&index=10&list=plljy-ebtnft6eumxfyrinrs07mcwn5uia
  48. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  49. https://ayearofai.com/media/32e75fb2f7f388775689a155c5c27d86?postid=10300100899b
  50. https://ayearofai.com/media/8de70b7fa3e5cb979099278112052953?postid=10300100899b
  51. https://ayearofai.com/media/3f992ef4ac506dafa8d2d8badfc31dc2?postid=10300100899b
  52. https://ayearofai.com/media/b5f70a5d514e61ad4646217c70974843?postid=10300100899b
  53. https://en.wikipedia.org/wiki/phoneme
  54. https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a#.oevzdnnnp
  55. https://en.wikipedia.org/wiki/autoencoder#/media/file:autoencoder_structure.png
  56. https://arxiv.org/pdf/1609.08144.pdf
  57. https://arxiv.org/abs/1609.08144
  58. https://arxiv.org/pdf/1609.08144.pdf
  59. https://arxiv.org/abs/1609.08144
  60. https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/
  61. https://arxiv.org/pdf/1411.4389.pdf
  62. https://arxiv.org/abs/1411.4389
  63. https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b?gi=25a4c415cc58
  64. http://cs.stanford.edu/people/karpathy/cvpr2015.pdf
  65. http://cs.stanford.edu/people/karpathy/deepimagesent/
  66. https://arxiv.org/pdf/1611.04558.pdf
  67. https://twitter.com/lennykhazan
  68. https://keras.io/
  69. http://pytorch.org/
  70. https://www.tensorflow.org/
  71. https://gist.github.com/karpathy/d4dee566867f8291f086
  72. https://ayearofai.com/media/515164c3f20643d9534d745371d34b9f?postid=10300100899b
  73. http://cs231n.github.io/neural-networks-case-study/#grad
  74. http://cs231n.github.io/neural-networks-case-study/#grad
  75. https://ayearofai.com/rohan-lenny-1-neural-networks-the-id26-algorithm-explained-abf4609d4f9d
  76. https://ayearofai.com/tagged/machine-learning?source=post
  77. https://ayearofai.com/tagged/artificial-intelligence?source=post
  78. https://ayearofai.com/tagged/data-science?source=post
  79. https://ayearofai.com/tagged/deep-learning?source=post
  80. https://ayearofai.com/tagged/algorithms?source=post
  81. https://ayearofai.com/@mckapur?source=footer_card
  82. https://ayearofai.com/@mckapur
  83. https://ayearofai.com/?source=footer_card
  84. https://ayearofai.com/?source=footer_card
  85. https://ayearofai.com/
  86. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  88. https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b
  89. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  90. https://medium.com/p/10300100899b/share/twitter
  91. https://medium.com/p/10300100899b/share/facebook
  92. https://medium.com/p/10300100899b/share/twitter
  93. https://medium.com/p/10300100899b/share/facebook
