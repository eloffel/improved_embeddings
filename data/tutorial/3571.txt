   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]a year of artificial intelligence
     * [9]algorithms
     * [10]today i learned
     * [11]case studies
     * [12]philosophical
     * [13]meta
     __________________________________________________________________

lenny #2: autoencoders and id27s

   [14]go to the profile of lenny khazan
   [15]lenny khazan (button) blockedunblock (button) followfollowing
   apr 24, 2016

   i want to take a quick break from my [16]id23
   endeavor (more on that soon) to talk about an interesting unsupervised
   learning model: autoencoders. the basic idea behind autoencoders is
   id84         i have some high-dimensional representation
   of data, and i simply want to represent the same data with fewer
   numbers. let   s take the example of a picture of a face. if you look at
   each individual pixel, you   ll quickly realize that neighboring pixels
   are usually highly correlated (they have a very similar color). there   s
   a lot of redundant information going on there. what if we could take
   out the redundancy and express that same image in a fraction of the
   numbers?

   that   s where autoencoders come in.

autoencoders

   [17]remember how neural networks work? autoencoders are a kind of
   neural network designed for id84; in other words,
   representing the same information with fewer numbers. the basic premise
   is simple         we take a neural network and train it to spit out the same
   information it   s given. by doing so, we ensure that the activations of
   each layer must, by definition, be able to represent the entirety of
   the input data (if it is to be successfully recovered later on). if
   each layer is the same size as the input, this becomes trivial, and the
   data can simply be copied over from layer to layer to the output. but
   if we start changing the size of the layers, the network inherently
   learns a new way to represent the data. if the size of one of the
   hidden layers is smaller than the input data, it has no choice but to
   find some way to compress the data.
   [0*vdqixpls-7akgvkk.]
   [18]https://en.wikipedia.org/wiki/file:autoencoder_structure.png

   and that   s exactly what an autoencoder does. if you look at the
   diagram, the network starts out by    compressing    the data into a
   lower-dimensional representation z, and then converts it back to a
   reconstruction of the original input. if the network converges
   properly, z will be a more compressed version of the data that encodes
   the same information. for example, if the input image is a face, the
   data can be reduced down to certain defining characteristics of a
   face         shape, color, pose, and so on. representing each of those
   characteristics could be more effective than storing each pixel, and
   that   s what autoencoders are really great at.

   it   s often helpful to think about the network as an    encoder    and a
      decoder   . the first half of the network, the encoder, takes the input
   and transforms it into the lower-dimensional representation. the
   decoder then takes that lower-dimensional representation and converts
   it back to the original image (or as close to it as it can get). the
   encoder and decoder are still trained together, but once we have the
   weights, we can use the two separately         maybe we use the encoder to
   generate a more meaningful representation of some data we   re feeding
   into another neural network, or the decoder to let the network generate
   new data we haven   t shown it before.

   that   s pretty much it for the basic concept of autoencoders, but there
   are a few modifications that people have made that are worth
   mentioning.

   denoising autoencoders

   denoising autoencoders are a technique often used to help the network
   learn representations of the data that are more meaningful to the
   underlying data   s variability. instead of simply training a network to
   recall the input it was given, random noise is applied to the input
   before passing it to the network         the network is still expected to
   recall the original (uncorrupted) input, which should force the network
   to stop picking up on minute details while focusing on the bigger
   picture. the random noise essentially prevents the network from
   learning the specifics, ensuring that it generalizes to the important
   characteristics. this is especially important in the situation where
   the encoded vector is larger than the input         using a traditional
   autoencoder, the network could (and often will) simply learn to copy
   the input into the encoded vector to recreate the original. with a
   denoising autoencoder, the autoencoder can no longer do that, and it   s
   more likely to learn a meaningful representation of the input.

   sequence-to-sequence autoencoders

   we haven   t covered recurrent neural networks (id56s) directly (yet), but
   they   ve certainly been cropping up more and more         and sure enough,
   they   ve been applied to autoencoders. the basic premise behind an id56
   is that, instead of operating on and producing a fixed-size vector, we
   instead start operating on and producing sequences of fixed-size
   vectors. let   s take the example of machine translation (here   s a
   sentence in english, give me the same sentence in russian). posing this
   problem to a vanilla neural network is intractable (we could translate
   the sentence word-for-word, but we already know grammar doesn   t work
   like that). enter id56s: we give the network the input sentence one word
   at a time, and the network magically spits out the same sentence in
   spanish, one word at a time. we   ll get more into the specifics of how
   exactly an id56 works soon enough, but the general concept is enough for
   the time being.

   sequence-to-sequence autoencoders work the same way as traditional
   autoencoders, except both the encoder and decoder are id56s. so instead
   of converting a vector to a smaller vector, we convert an entire
   sequence into one vector. then we take that one vector and expand it
   back into a sequence. let   s take the example of getting a
   [19]fixed-length representation of a variable-length audio clip. the
   autoencoder takes in an audio segment and produces a vector to
   represent that data. that   s the encoder. we can then take that vector
   and give it to the decoder, which gives back the original audio clip.

   id5

   i won   t go into depth on how id5 work now (they
   really deserve a post of their own         maybe this summer), but the
   concept is still important enough that it   s worth mentioning. if you   re
   curious (and comfortable with some heavy-duty statistics), [20]this
   paper is worth a read.

   id5 essentially let us create a generative model
   for our data. a generative model learns to create brand new data that
   isn   t from the training set, but that looks as if it is. for example,
   given a dataset of faces, it will learn to generate brand new faces
   that look real. id5 aren   t the only generative
   model         id3 are another kind of generative
   model that we   ll take a look at some other day. (forgive me for the
   handy-wavy explanation, but specifics are hard without going into the
   statistics. rest assured i   ll give id5 the
   explanation they deserve soon enough.)

id27s

   now i want to talk about a different (but somewhat related) type of
   model known as a id27. like an autoencoder, this type of
   model learns a vector space embedding for some data. i kind of skipped
   over this point earlier on, so let me take a minute to address this.

   let   s go back to the example of faces one last time. we started out by
   representing faces as a series of pixel values (let   s say 10,000 pixels
   per face)         for simplicity   s sake, we   ll assume that the image is
   grayscale and each pixel is represented by a single intensity value,
   but know that the same principles carry over for rgb data (and, of
   course, non-image data too). now, let   s do something a bit interesting,
   and maybe unnatural at first, with these pixels. let   s use each pixel
   value as a coordinate in a 10,000-dimensional space. (sidenote: if
   you   re having trouble visualizing this, it   s because we mortal humans
   can   t think in more than 3 dimensions         just think of everything in
   terms of 2 or 3 dimensions, but realize that the exact same principles
   transfer over to as many dimensions as you want). so we have a
   10,000-dimensional space, and and the 10,000 pixel values are the
   coordinates into this space. each image in our dataset has a distinct
   location in this space. if you were to look at this
   impossible-to-visualize vector space, you   ll notice that pictures that
   have very similar pixel values are very close to each other, while very
   different images are very far away from each other.

   this relationship between images is critical, and it   s what makes it
   useful to think about data in vector form. but there are flaws in the
   coordinate system we   re using now         for example, if you have an
   identical image but shifted over one pixel, it could be nowhere near
   the original image in this vector space because each dimension will
   have a wildly different coordinate. if we can find some coordinate
   system that gives us more meaningful relationships between points in
   our vector space, we can put our images into a space that tells us even
   more about the relationships between these images.

   and if you think about it, that   s exactly what neural networks do! we
   take an image, which is represented by very low-level pixel data, and
   convert it into a higher-level representation that might encode
   features like pose or facial expression. if we train our neural network
   on, for example, a classification task, we   ll get a new representation
   that best encompasses the differences between the various classes. if
   we   re using an autoencoder, the new representation will instead tell us
   more about the factors that best define what is unique about a
   particular face. if we use these new encoded vectors as coordinates
   into a new coordinate system, we   ll see that the relationship between
   images suddenly becomes even more meaningful         images that are close
   together are similar not because of pixel-by-pixel similarity, but
   because they have similar characteristics. instead of having a
   dimension per pixel, we have a dimension for every feature of the face
   (pose, color, etc.). as a result, as we move along a certain axis, we
   notice that an individual feature of the image changes while the rest
   remains the same (for example, we might notice that a smile gradually
   becomes a frown, while skin color and pose remain the same). if we were
   to try the same thing on our original pixel-based coordinate system,
   we   ll just see the intensity of one individual pixel change, which is
   much less meaningful.

   so this process of taking a point in one vector space (the pixel-based
   one) and putting it into a different vector space (the
   autoencoder-based one) is a vector space embedding. we are embedding
   the points of one vector space into another, which could tell us
   something new about our data that we didn   t quite realize before.

   the process of creating a meaningful vector space embedding is usually
   relatively straightforward for something like faces. if two faces have
   similar appearance, they should appear next each other in the vector
   space. traditional autoencoders are pretty good at this. but the
   relationships aren   t always quite so obvious         let   s look at the
   example of words. if i look at the letters that make up a word, it   s
   darn near impossible for me to make sense of what words should be near
   each other. i might notice that    presentation    and    present    have
   similar letters, so they should be next to each other; but on the other
   hand,    prequel    and    sequel    also have similar letters, yet they have a
   very different relationship. similarly,    yacht    and    boat    are quite
   related, but don   t have much in common letter-wise. as a result, words
   are usually encoded in vector form using one-of-k encoding (also known
   as one-hot encoding). if we have a total of n possible words, then each
   vector will have n numbers, and all but one of the numbers will be a 0.
   each word will have its own index into the vector, and the value at
   that position will be a 1. while this does let us differentiate between
   words, it gives us no information about how words are related or
   linked.
   [1*ulfyiwpkgwcecqyzedtl0g.png]
   this is what one-hot encoding looks like. it   s convenient, but gives no
   information about the relationship between words.

   we need a better system. instead of using one-hot encoding, what if we
   can use something like an autoencoder to reduce our words to a
   lower-dimensional vector that gives us more information about the
   meaning and characteristics of a word itself? sounds like a plan! but,
   as we already established, autoencoders can   t learn much meaning from
   the letters that make up a word. we   ll need to set up a new model to
   learn the meanings of and relationships between words.

   and that is exactly what a id27 is. we   ll take a look at a
   popular id27 model shortly, but first let   s explore some of
   the fascinating properties a id27 vector space can have. as
   it turns out, and this shouldn   t come as much of a surprise, words have
   much more complicated relationships than pictures of faces. for
   example, take    washington d.c.    and    united states   . the two are
   obviously related, but the relationship is very specific. it   s also the
   same relationship that    ottawa    and    canada    have. and both of those
   relationships are very different than the relationship between    boy   
   and    girl   . ideally, a id27 model will be able to express
   each of these relationships distinctly.

   when you have a vector space, you can express relationships as the
   vector between two points. so, if you take the distance from    boy    to
      girl   , traveling the same distance (in the same direction) should get
   you from    man    to    woman   , because they two have the same relationship.
   but the direction you take to get from    ottawa    to    canada    should be
   completely different. if we can create a id27 space that
   captures each of these relationships, that means that we   ll now have an
   incredibly useful and meaningful way to represent words. once we can
   get words into a space like this, neural networks (and of course other
   ml models) will be able to learn much more effectively         while neural
   networks would typically have to inherently learn a simplified version
   of these relationships on their own, the ability to give them this
   knowledge directly means that it   s easier to perform natural language
   processing tasks (like, for example, machine translation).

   now, finally, we can talk about how we get these id27s. i   m
   going to talk about one particularly impressive model called id97.
   at a very high level, it learns its id27 through context.
   this actually makes lots of sense         when you see a word you don   t know,
   you look at the words around it to figure out what it means. as it
   turns out, id97 does the exact same thing.
   [0*rf9euslmfw2xhkee.]
   [21]https://papers.nips.cc/paper/5021-distributed-representations-of-wo
   rds-and-phrases-and-their-compositionality.pdf

   generally, id97 is trained using something called a skip-gram
   model. the skip-gram model, pictures above, attempts to use the vector
   representation that it learns to predict the words that appear around a
   given word in the corpus. essentially, it uses the context of the word
   as it is used in a variety of books and other literature to derive a
   meaningful set of numbers. if the    context    of two words is similar,
   they will have similar vector representations         that already makes this
   new embedding more useful than one-hot encoding. but, as it turns out,
   the relationships go deeper.
   [0*vttsc4jgt3_efc1u.]
   [22]https://papers.nips.cc/paper/5021-distributed-representations-of-wo
   rds-and-phrases-and-their-compositionality.pdf. this is a visualization
   of the relationships between countries and their capitals         in reality,
   the vector space has hundreds of dimensions, but a technique called pca
   was used to approximate this relationship in a way that   s easier to
   visualize. as you can see, the relationship is very similar in each
   instance. if you want to see some more incredible results, definitely
   check out the id97 paper linked above.

   if you look at the relationship between a country and its capital,
   you   ll notice that the vector taking you from one to the other is
   almost identical in every instance. that is one of the other critical
   characteristics we defined earlier for a id27         representing
   the unique relationships that words have with each other.

   let   s look at a neural network trained for machine translation to get a
   better idea of how this new id27 helps us. giving it words as
   one-hot vectors is not super useful         it has to learn on its own that
   the vector for    screen    and    display    have the same meaning, even
   though there is no correlation whatsoever between the two vectors. but,
   using our fancy new id97 embedding,    screen    and    display    should
   have a nearly identical vector representations         so this correlation
   comes very naturally. in fact, even if the network has never seen a
   training sample with the word    display    in it, it will likely be able
   to generalize based purely on the examples it has seen with    screen   
   because the two have a similar representation.

so yeah, autoencoders are pretty neat.

   there   s lots more i didn   t get to cover about autoencoders (most
   notably pre-training comes to mind, but that   s for another day), but
   hopefully you learned enough to understand where they might come in
   handy and a few ways they can be used in practice. we also talked about
   embeddings and id97, a id27 model that gives us
   meaningful word vectors to make our other models more effective. but
   the utility of id97 extends far beyond nlp         just last year, a
   id97-based system of creating an [23]embedding space for protein
   sequences was proposed for improved accuracy on classification tasks.

   the applications for these unsupervised learning models are practically
   endless, and they generally serve to improve the accuracy of more
   traditional supervised learning problems. that being said, i   m looking
   forward to seeing what else comes out of the field in the coming
   years         the most exciting advances are still to come.

     * [24]algorithms
     * [25]machine learning

   (button)
   (button)
   (button) 171 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of lenny khazan

[27]lenny khazan

   tinkering with machine learning. [28]http://getcontra.com/

     (button) follow
   [29]a year of artificial intelligence

[30]a year of artificial intelligence

   our ongoing effort to make the mathematics, science, linguistics, and
   philosophy of artificial intelligence fun and simple.

     * (button)
       (button) 171
     * (button)
     *
     *

   [31]a year of artificial intelligence
   never miss a story from a year of artificial intelligence, when you
   sign up for medium. [32]learn more
   never miss a story from a year of artificial intelligence
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://ayearofai.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/576403b0113a
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a&source=--------------------------nav_reg&operation=register
   8. https://ayearofai.com/?source=logo-lo_jjjqctxzarhj---bb87da25612c
   9. https://ayearofai.com/tagged/algorithms
  10. https://ayearofai.com/tagged/today-i-learned
  11. https://ayearofai.com/tagged/case-studies
  12. https://ayearofai.com/tagged/philosophical
  13. https://ayearofai.com/tagged/meta
  14. https://ayearofai.com/@lennykhazan?source=post_header_lockup
  15. https://ayearofai.com/@lennykhazan
  16. https://medium.com/a-year-of-artificial-intelligence/lenny-1-robots-reinforcement-learning-ae6e69ff4cf0#.jlk9gltem
  17. https://medium.com/a-year-of-artificial-intelligence/rohan-lenny-1-neural-networks-the-id26-algorithm-explained-abf4609d4f9d#.b9ete3cq2
  18. https://en.wikipedia.org/wiki/file:autoencoder_structure.png
  19. http://arxiv.org/pdf/1603.00982v3.pdf
  20. http://arxiv.org/abs/1312.6114
  21. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  22. https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  23. http://arxiv.org/abs/1503.05140
  24. https://ayearofai.com/tagged/algorithms?source=post
  25. https://ayearofai.com/tagged/machine-learning?source=post
  26. https://ayearofai.com/@lennykhazan?source=footer_card
  27. https://ayearofai.com/@lennykhazan
  28. http://getcontra.com/
  29. https://ayearofai.com/?source=footer_card
  30. https://ayearofai.com/?source=footer_card
  31. https://ayearofai.com/
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/576403b0113a/share/twitter
  35. https://medium.com/p/576403b0113a/share/facebook
  36. https://medium.com/p/576403b0113a/share/twitter
  37. https://medium.com/p/576403b0113a/share/facebook
