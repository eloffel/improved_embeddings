   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]a year of artificial intelligence
     * [9]algorithms
     * [10]today i learned
     * [11]case studies
     * [12]philosophical
     * [13]meta
     __________________________________________________________________

   [1*krhphnucyx9y5pmdjgvvfa.png]

rohan #1: id28 case study         diagnosing cancer

how classification algorithms like id28 can understand the
difference between malignant and benign breast cancer tumors.

   [14]go to the profile of rohan kapur
   [15]rohan kapur (button) blockedunblock (button) followfollowing
   jan 22, 2016
     __________________________________________________________________

     this is the first entry in my [16]journey to extend my knowledge of
     artificial intelligence in the year of 2016. learn more about my
     motives in this [17]introduction post.

     this post in particular is the first draft of my ib higher
     mathematics internal exploration. i took this as an opportunity to
     really dive into the mathematics behind artificial intelligence. i
     apologize for the poor formatting in many places (for example lack
     of superscripts when copying over from pages)         medium should really
     fix this!

     hence this will be one of the rare times when i go into theory in
     such depth. it is important i emphasize this. i   ll roll out blog
     post #2 in a couple days which talks about the knowledge i   ve been
     acquiring from books i   ve been reading!

     if you are interested in a different classification algorithm called
     neural networks (which is much more complex!), then take a look at
     this nifty [18]presentation i made discussing it and its use in a
     self driving unity3d car.
     __________________________________________________________________

   introduction & aim

   my inspiration for this investigation evolved out of an avid passion
   for computer science. as a programmer for over 5 years, i was eager to
   explore the workings of modern artificial intelligence used in contexts
   ranging from handwriting recognition to self driving cars. after
   successfully completing an online course offered by stanford on the
   topic, i was delighted by the sophisticated mathematical content
   involved and wanted to expand on these findings in my hl math ia.

   machine learning, a specific subset of artificial intelligence, is a
   stellar exemplar of applied mathematics in the every day world. machine
   learning algorithms (a set of instructions executed by a computer) are
   able to learn from large sets of data and generalize on new examples;
   when we log into social media sites such as facebook, machine learning
   finds the content it thinks will interest us the most based on previous
   activity.. these very algorithms are also responsible for keeping our
   emails    inbox spam-free. the use cases extend to credit card fraud
   detection, character recognition, product recommendations, etc. the
   real-world nature of these problems are captivating to me since it not
   only enables me to appreciate math   s link to computer science, but also
   the prominence of math in our daily lives.

   cancer is one of the leading deaths in the world, claiming 600,000
   lives in the us alone just last year. my main goal for this
   investigation is to explore the math behind machine learning with
   respect to classifying breast cancer tumors as either malignant (being
   cancerous) or benign (being non-cancerous) based on a given patient   s
   symptoms and attributes. not only is this a classic problem used when
   teaching artificial intelligence, but its altruistic sentiment in
   contributing to a tragic issue that impacts so many lives leads me to
   select it. i will code a program in octave (a coding tool) to visualize
   the results of the algorithm.

   ii. what is machine learning and artificial intelligence?

   in general, artificial intelligence is any form of artificial system,
   i.e. computer system, that can mimic human intelligence in some way.
   machine learning is a type of artificial intelligence that provides
   computers with the ability to learn without being explicitly
   programmed, allowing them to predict on new data. relating back to the
   cancer context, a computer program can examine the different existing
   medical data of patients    attributes, eg. their tumor size, tumor
   radius, age, etc. paired with the status of having a malignant or
   benign tumor. the algorithm will then discover patterns in the data,
   ie. identifying unimportant attributes, important attributes, and
   thresholds and combinations of attributes that influence outcomes. this
   will enable the computer to accurately predict whether a new patient
   has a malignant or benign tumor based on their attributes.

   machine learning algorithms work by learning from a training set, which
   consists of a matrix x and vector y. x is a collection of the all the
   numeric attributes or    features    eg. the tumor size and radius. each
   column in x represents a single feature, and each row in that column is
   the value of that feature for a specific instance ie. patient. each
   row   s/instance   s features put together can be called a    feature
   vector   . y is the class of the data points, either 0 or 1 (false/true)
   or benign vs. malignant otherwise. each feature vector paired with its
   class is known as an instance or training example.
   [1*blnw4zwxuck1grah07k_xw.png]

   the above is an example of a small training set, where patient 0 with a
   tumor size of tumorsize1 and radius of tumorradius1 has a malignant
   tumor, whilst patient 1 with a size of tumorsize1 and radius
   tumorradius 1 has a benign tumor. the feature vector x shows how we can
   represent only patient 0   s data. we can denote tumorsize_0 and
   tumorradius_0 or any values inside a feature vector with a subscript on
   x, i.e. x_0, x_1, x_2   . x_n. this notation will provide a backing for
   an algorithm that i will introduce later on.
   [1*eqfyqmufgbhlz2dbec7lhg.png]

   figure 1. shows how two features, a patient   s tumor size and tumor
   radius, may hypothetically relate to the status of a tumor. a cross
   represents a malignant tumor         1         and a circle represents a benign
   tumor         0         . the data looks to be separated into two distinct groups.
   now, the task for machine learning is to identify patterns in the data,
   and figure 2. shows what a typical    classification    algorithm may do;
   it may formulate a line that separates the malignant from benign
   tumors, referred to as a    decision boundary   . now, any point plotted to
   the right of the decision boundary will be considered malignant, and to
   the left benign.
   [1*kgxnfuc7xitr28uss1ykia.png]

   figure 3. is a different type of machine learning problem where we plot
   x against y and want to predict any real value of y in terms of x. in
   this case, x is one-dimensional/one feature: the size of a house. y,
   then, is the price of the house. from this, we want to formulate a
   model (function) for the data that allows us to accurately find the
   price of a house from its size. we can use this to make predictions in
   the future. figure 4. shows how a machine learning algorithm may fit a
   polynomial to the data, allowing it to make a generalization. we call
   this a    regression   ; the primary differentiator between classification
   and regression is that a regression algorithm outputs a real continuous
   value, whilst the classification algorithm outputs a discrete
   category/class, normally a binary state; 0 or 1.

   we denote m as the number of training examples, instances, or data
   points in the training set, which is equal to 8 in figure 1 and 11 in
   figure 3. then, we define n is the number of features or the
   dimensionality of the dataset, which is equal to 2 (tumor size and
   radius) in figure 1 and 1 (house size) in figure 3. so, mathematically,
   x        ^(m x n), feature vector x or x_i        ^n and y     {0, 1}^m for
   classification problems or y        ^m for regression problems. now, we
   define a    parameter vector    denoted as   , where           ^n. the goal,
   then, for any machine learning algorithm, is to map each value of x to
   its respective y value in the training set with high accuracy using
   this    and a hypothesis function. these notations are important because
   we will use them to formulate equations in the future.

   a hypothesis function is a function we use to model the training set
   and to    hypothesize    on new data. like a normal function, it takes a
   parameter x (a vector which corresponds to one feature vector from
   matrix x), but also takes the parameter vector   .    is used to store
   the coefficients or    weights    for each feature in x in the function.
   hence we denote the hypothesis function as h(x). for a regression such
   as the housing prices example, we may use h(x) =   _0   x_0         a linear
   function where x_0 is the size of the house and hence   _0 is the
   gradient (the constant in this case is zero). on the other hand, we may
   use a quadratic function instead. when classifying, we actually use a
   continuous function and then round the values up/down to achieve 0 or
   1. this function is known as the    sigmoid    or    logistic    function, and
   it asymptotes at 0 and 1. the idea is that this hypothesis function,
   paired with some value of   , can fit our training set well. ultimately,
   we want h(x)     y for each training example.

   in both cases, our goal is to try to formulate a model that maps x from
   y accurately with h(x). we must choose and experiment with the
   hypothesis function to use, i.e. linear, polynomial, logistic,
   logarithmic, etc. the machine learning algorithm will find coefficients
   for the function that enable the mapping to be as accurate as possible
   across all training examples. in other words, the algorithm   s task is
   to find an optimum    for h(x). to find this optimum   , the algorithm
   needs a way to evaluate the performance of any given   . this is where
   the cost function j(  ) comes in; j(  ) is a performance measure that
   iterates through each training example and, for any given   , compares
   h(x) to y and treats this discrepancy as the    error   . different cost
   functions may treat these errors different, i.e. sum them, average
   them, sum the squares, take their logarithm, etc. we will formally
   notate and use a popular cost function later on.

   now, the goal of the algorithm is, qualitatively, to find a    that
   minimizes the output of the cost function. this is known as an
   optimization problem and is colloquially referred to as    training    the
   algorithm. we can achieve this with calculus by either solving the
   derivative analytically to find the minimum point of    plotted against
   j(  ) (in some high dimensional space) or by using a popular, efficient
   method called    id119   . once we have found an optimum   , we
   have built a model that not only performs well on the training set but
   can be used to confidently predict the output of new features.

   error will always be present because it is highly unlikely that one
   continuous function can perfectly fit many noisy data points, unless
   the function is of a very high order         but in that case, the model may
   be fitting the data too much so that it is disadvantageous when used
   for prediction.

   iii. classification with id28

   now, we will look at    id28   . although it has
      regression    in the name, id28 is actually a popular
   classification algorithm. many have hypothesized that the roots of the
   name arose from the fact that id28 draws a    decision
   boundary    (a line to separate the two classes as shown in figure 2.) in
   the form of a line/polynomial, much like a regression. figure 2. shows
   exactly what a id28 algorithm may do         figure out how to
   separate the data into two distinct groups that differentiates
   malignant from benign.

   to refresh our memory, we asserted that, for classification problems, y
       {0, 1}^m, where 0 is the    negative class    (benign) and 1 is the
      positive class    (malignant). then, we ensure that 0     h(x)     1, to
   resemble a id203, where h(x) = p(y=1|x;  ) or the id203 that
   the tumor is malignant, given x and parameterized by   . a h(x) = 0.4
   implies that a person has 0.4 chance of having a malignant tumor, and
   hence we predict that they do not. as an additional note, p(y=1|x;  ) +
   p(y=0|x;  ) must equal to 1.

   hypothesis function

   we want a continuous function h(x) where 0     h(x)     1. for this, we
   turn to the    sigmoid    or    logistic    function which satisfies this
   inequality, notable for its smooth    s    shape:
   [1*zgw-jcbzhoyrinlheibzla.png]

   we can see that the function has horizontal asymptotes at g(z) = 1 and
   g(z) = 0. as z        , e-        0, and so g(e-   )     1. on the other hand, as z
       -   , e           , and so g(e   )     0. we can show that the y-intercept is
   0.5; z = 0, e0 = 1 and so g(0) = 1/2 = 0.5. hence, the range of this
   function is y = {y | y        , 0     g(z)     1}. technically, it is not
   possible to attain an output of 0 or 1 from this function due to the
   asymptotes, but a good argument for this is that, even with 100%
   accuracy, we can almost never be certain that an outcome absolutely
   will occur.

   so now, our hypothesis function will be g(z). but where do the weights
      and the features x come in? recall that x is a matrix where each row
   in x is a single feature vector x of length n. we use    to weight our
   values like so:   _0   x_0 +   _1   x_1 +       . +   _(n-1)   x_(n-1). since both   
   and x are vectors, we can represent this as a dot-wise product:
   [1*qtyxwn6v-fkjkayespk3xw.png]

   however, we can write this in an even shorter way. if we recognize that
   both    and x have the same dimensions, then    transposed, a
   mathematical operation where we switch over the columns and rows of a
   vector, can be directly multiplied with x since the dimensions (1 x n)
   and (n x 1) can be legally multiplied. the figures below make it
   apparent why:
   [1*al0t3dgj6hc1eixt3ecqug.png]

   now, we can see that our weighted feature vector is equal to the value
     ^transpose   x. we will now use this product as the parameter z in our
   sigmoid function g(z). hence, our final expression for h(x) is
   g(  ^transpose   x) or:
   [1*bakrkrgkabe7vwmyboi-lw.png]

   but why? by changing    we essentially apply transformations to
   g(z)/h(x), altering the decision boundary until we reach a point where
      is optimum and our transformed h(x) models our data not just nicely,
   but with the highest accuracy. we use the sigmoid function because,
   again, it bounds values from 0 to 1 for any real input and provides us
   with a valid id203.

   decision boundary

   classification algorithms, however, must eventually classify between
   multiple categories/states. this becomes more apparent when we look at
   a use case such as spam filtration; we have to make a choice to send to
   the spam folder or the inbox. suppose we predict y = 1 when h(x)     0.5
   and y = 0 when h(x) < 0.5.

   recall that g(z) = 0.5 only when z = 0. since z =   ^transpose   x, then
     ^transpose   x = 0 when g(z) = 0.5. therefore, we predict y = 1 when
     ^transpose   x     0. now, if we graph the line   ^transpose   x = 0 or
     _0   x_0 +   _1   x_1 +       . +   _(n-1)   x_(n-1) = 0 on our training set, this
   will act as a    decision boundary   , where data points on one side of the
   line would be labelled as y = 0 and data points on another side would
   be labelled y = 1. figure 5. illustrates this.
   [1*p-jqetm7ffnsoa9jwficma.png]

   figure 5. shows how a decision boundary might be formulated for some
   arbitrary    and training set. if we train id28 correctly
   and reach an optimum   , then we will, in turn, create a decision
   boundary that will nicely separate the data (unless, of course, the
   data is very noisy). initially, the decision boundary will be
   inaccurate (because the convention is to first set    to a series of
   0s). decision boundaries are significant because they provide us with a
   visual representation of what id28 does, but it is
   important to note that decision boundaries are simply an
   implication/product of the mathematics behind id28. in
   reality, the algorithm is trying to minimize the error on the output of
   the sigmoid-hypothesis function with respect to our training set, not
   directly build a decision boundary. this visual element becomes
   irrelevant when n > 3 and we start working with four plus dimensions
   (which is more reflective of industrial applications).

   cost function

   as discussed in the general overview of machine learning, we need a
   performance measure a.k.a    cost function    j(  ) to score the accuracy of
   a given    on a training set. j(  ) will output the cost or error of the
   parameters, and thus the goal of the algorithm is to select such
   parameters that minimize j(  ). in this section we will look at a
   popular cost function that is used in id28.

   the basis of our error is the discrepancy between the actual output y
   and the output of our hypothesis h(x) for our individual training
   examples. normally, a cost function will get the average error in a
   system (so that an outlier does not affect us too much). so, one may
   suggest a cost function that simply sums over all the discrepancies
   (h(x)     y) in our training set and divides it by the number of training
   examples to get the average figure, like so:
   [1*vshesaef8n1mbe6al5xqxg.png]

   an example would be if y = 1 and h(x) = 0.4, we would add 0.6 (divided
   by m) to the cost. but this actually is not the best formulation. the
   reason becomes clear when we look at the optimization section, but in
   short, each individual error can be a maximum of 1. this means that the
   maximum cost for any given training set is m. this is a fairly small
   number and, relatively, would be fairly close to a minimized j(  ) on
   the number scale. ideally, we want the cost of poor fit parameters    to
   grow fast (faster than linearly), so that when we are optimizing we can
   quickly skip past the poor parameters which have very high costs. if
   the outputs of the cost functions are very close together, we have to
   take many more    small steps    to reach our solution. in addition, what
   if our h(x) = 0 and y = 1? if this error arises during prediction, we
   may be telling patients with certainty that they do not have a
   malignant tumor when in fact they, tragically, do! clearly, we need to
   assign a cost greater than merely 1 to this.

   so instead, we use a different cost function. but first, we need to
   define our core discrepancies using a bipartite piecewise function:
   [1*8conseifu0jhixfwdc0qga.png]

   if y = 1, then we will take the negative log of h(x). since 0 < h(x) <
   1, the output of the log function will always be negative, so negating
   this expression will give us a positive value. if y = 0, we take the
   log of 1     h(x). figure 7 and figure 8 make it clear why this is so.
   [1*wsmg2-2rndldv6zb7_hpig.png]

   if y = 1 then we use the left hand side function to give us the
   discrepancy. the function has an x-intercept at x = 1 because log(1)
   for any base is 0. this captures the logic that, if y = 1 and h(x) = 1,
   then the discrepancy is 0. due to the the vertical asymptote at 0
   (log(0) is always undefined), it also captures the intuition that if y
   = 1 and h(x) = 0 then the discrepancy should tend to infinity. the same
   behavior occurs when y = 0 with the function on the right hand side. of
   course, due to the nature of the sigmoid function, we cannot actually
   achieve h(x) = 1 or h(x) = 0 due to the horizontal asymptotes nor can
   we really penalize our parameters by an infinite value, but the
   behavior we are describing in general occurs as these outputs tend
   towards 0 and 1.

   now, the final cost can be written as:
   [1*knkprhj0hypqkmp69e7dwg.png]

   we can further collapse it from a piecewise function into a single
   function like so:
   [1*pk5s82qaf70sugolxsgqvq.png]

   in the case of y = 0, then the term yi will be zero, and the entire
   left hand side product will evaluate to zero. (1   y_i) will evaluate to
   1, hence for this iteration we will add on a discrepancy term in the
   form log(h(x)) as discussed in the paragraph above. when y = 1, the
   right hand side product will be zero since (1   y_i) will be 0 too, and
   so we will treat the discrepancy term as log(1   h(x)), again, as we did
   for our piecewise function. notice that the negative signs are missing
   from the log functions. instead, we have factored it out of the
   summation expression for brevity.

   optimization         id119

   now, our task is to min j(  )   to find/select a    parameter vector that
   [19]minimizes the output of the cost function, and hence fits our model
   the best it can. in this section we will look at employing a popular
   method called id119 that uses calculus to find this optimum
   value. id119 is one of multiple optimization methods that
   are used in machine learning.

   one may ask why we cannot simply find the minimum point analytically by
   setting the derivates to zero akin to a textbook calculus problem.
   ultimately, analytical solutions (called the normal equations) and
   id119 differ in many ways, with id119 being the
   preferred industry method. why? well, for starters, the normal equation
   is computationally expensive, meaning it takes a lot of time to
   execute. the reason for this is simple: the equation includes
   (x^transpose   x)^-1, as in taking the    inverse    of an n x n matrix. the
   issue with this is that it runs in o(n^3) time complexity, meaning
   that, as the number of training examples in the training set x/y
   increase, the time it takes to compute the inverse grows by that value
   to the power of 3. this is simply because programmers haven   t found a
   truly efficient way to calculate the inverse of a square matrix yet.
   [20]this is a good article that goes into further depth on the reasons
   we use gd vs. normal equation(s).

   firstly, imagine a hypothetical scenario where n = 2 so    only stores
   two weights:   _0 and   _1. the first thing we will do is set    to some
   randomly generated parameter vector, or simply zeros. then, we will
   create a graph where   _0 and   _1 are on the x and z axes, and j(  ) is
   on the y-axis.
   [1*0kdebthwrpxpdhm06dmpyq.png]

   figure 9. shows how, on some arbitrary scale, the weights in    affect
   the cost j(  ). the graph shows that at roughly    = (-8, -8) our cost
   j(  ) is at a minimum point. there is one global minimum (or optimum)
   and no local optima. it is also convex. in fact, we can make a
   generalization (which can be proved) that any arbitrary weight   _j
   graphed against cost j(  _j) will be of a convex shape. figure 10.
   demonstrates this visually.
   [1*m5rql_yxe6geznod3atj5g.png]

   you can check out [21]this article for a formal proof on the convexity
   of id28   s cost function.

   this minimum point signifies the value of   _j that occurs when the
   output of j(  ) is at its lowest, and hence it is   _j at its optimum.
   now we are tasked with finding this minimum point/global optimum for
   each   _j, ultimately using these results to achieve our optimum   . the
   gist of id119 is that we can take small    steps    from some
   arbitrary point on the graph to the optimum by updating the current
      position    in the direction of the derivative. firstly, we need to
      place    ourselves somewhere on the graph in figure 10. by computing any
   coordinate         i.e. by setting   _js to zero and calculating the resulting
   cost. figures 11   13. demonstrate this visually.
   [1*phosni0v6kuguwhmtgmczq.png]

   we start at figure 11. at iteration (or step) 1 with a very high cost.
   then, after just 100 iterations (compared to our total number of
   iterations which can range from 1000 to 100,000 and beyond) our cost
   has drastically decreased, demonstrated by a much lower y coordinate in
   figure 12. compared to figure 11. then, after another 900 iterations we
   are at our optimum value for   _j         figure 13. shows how we are now
   placed at the minimum point. here we have    converged    and no longer
   move, so we terminate id119.

   if you were on the right hand side of the minimum point, you would move
   in the left direction until convergence. we can translate    left    to    in
   the negative direction   , and    right    to    in the positive direction   . we
   can use the gradient to tell us this         a negative gradient indicates
   that we need to move in the positive direction (as we would be left of
   the min. point) and a positive gradient indicates that we need to move
   in the negative direction. we repeat this process many times and hence
   keep descending. if our cost is very high, our gradient would be steep
   and so our steps would be larger. this allows us to quickly skip past
   poor   _j candidates. then, as we get closer to the optimum, the
   gradient becomes much flatter, allowing us to take finer steps to
   prevent any divergence from the minimum point. hence the name    gradient
   descent   .

   we will update our current position (which is our current   _j) by
   subtracting it from the gradient (subtracting by a negative is an
   addition and vice versa). to extract the gradient at any given point,
   we need to differentiate to compute the derivative. since we are
   finding the derivative with respect to   _j only, we will solve it with
   partial differentiation. we will then multiply this gradient by some
   (adjustable & custom) constant    which we call the    learning rate   
   where 0 <    < 1 (usually 0.03).    controls how fast we move towards the
   optimum by being multiplied with the gradient value and hence
   minimizing the expression. if we update by just the gradient value (or
   if    is too large), we will take steps much too large and move past the
   optimum, which would render us in a viscous cycle of divergence, moving
   further and further from the global optimum. if    is too small, then we
   will take a long time to converge.

   we need to perform id119 for each weight   _j in   . the
   following equation is the formal notation for id119:
   [1*703nneqoaiskgkmlkgctxw.png]

   the assignment operator := is the notation we use to assign a value to
   a variable and hence update   _j. we know when we have converged
   because, at any optima, the derivative will be zero. hence, the update
   term will be equal to zero, and we assign   _j to itself         so we get
      stuck    at the optimum. this is why the convexity of our cost function
   allows us to always find the best solution; if we had any local minimum
   then we may get stuck at these points too, and id119 would
   terminate at a non-optimal solution. sometimes id119 may
   take 1000 steps/iteration, and sometimes it may take 100,000. it
   depends on the nature of your training set and learning rate.

   now, we can solve the partial derivative. i   m not going to include the
   derivation here, but if you do manually solve it, it should look like
   what follows in teh final expression for the id119
   algorithm:
   [1*2r1gohvxr1acurmdixznoq.png]

   after a successful execution of id119, we can evaluate the
   cost of our trained model using the cost function j(  ), however this
   outputs a real number that may be difficult to interpret (is it high or
   low         and on what scale?). instead, we can iterate through our entire
   training set and    tally    the number of times that h(x_i) = y^i. we
   divide this tally value by the total number of training examples and
   multiply by 100 to retrieve the training accuracy percentage. if it is
   above at least 95% (a numerical threshold used by many computer
   scientists), we can confidently use it to make new predictions by
   inputting any new feature vector x into our hypothesis h(x). if our
   accuracy is below 95%, we may need to do some further optimization, and
   we will look into this in a later section.

   iv. gathering data and testing

   fortunately, many institutions have published medical breast cancer
   tumor data online that can be used to apply id28 to. the
   university of wisconsin, madison in the state of wisconsin, usa is most
   notable for this effort. they offer a sample code number for each
   patient, followed by a series of tumor features, and finally a field
   indicating whether the tumor is malignant or benign. i downloaded the
   dataset and imported it into excel. figure 14. displays an excerpt (6
   out of 699 instances) of the data.
   [1*7p6xwzftdvzqi7sjq9un9w.png]

   column a, the sample code number, can be thought of as simply a
   reference to the instance and is not a feature in itself because it
   does not contribute to the outcome. columns b-j for a single row,
   however, are features and could be thought of as inside a feature
   vector x. all the rows in these columns put together would make matrix
   x. column l is the category/class (malignant or benign) that can be
   thought of as a value in the output vector y. thus, this is our
   training set. 458 of the instances are benign and 241 are malignant         a
   66% to 34% ratio.

   i have provided my id28 algorithm (written in octave)
   with the data. i will perform id28 with regards to just
   two features in two dimensions for visualization purposes. there are 9
   permutation 2 = 72 combinations of feature pairs, and figure 15. shows
   a good correlation i found between single epithelial cell size and
   uniformity of cell size, plotted with my code. a plus represents a
   malignant tumor, and a circle represents a benign tumor.
   [1*7xvdeb8d62ls3-qqq5du5w.png]

   the data is noisy (a few anomalies), but this is to be expected from
   any real-world/industrial application dataset; we don   t need 100%
   accuracy, we just need to find the general pattern presented since
   noise is stochastic. more importantly, however, this data is just
   taking into account two features; later we will look at combining all
   features. this is also why there seem to be very few data points; many
   of them overlap/are duplicates. i ran id119 for 1000
   iterations to calculate our optimum weights and decision boundary.
   figure 16. shows the decision boundary that was formulated.
   [1*oypoqv3nk53wc_oh2q_uig.png]

   unfortunately, the decision boundary does not seem to accurately
   separate our two classes. figure 17. displays more information about
   our algorithms    results before and after training.
   [1*dkm5_kagtdv4x4tbmroj9w.png]

   we set our initial theta to zeros, with a cost of 0.69. although this
   seems small, it is relatively large because we only have 35%
   accuracy         that is we only correctly predict malignant or benign on 35%
   of our training examples in our training set. in addition, our
   gradients in id119 are non-zero, indicating that we have to
   still perform iterations of id119 to reach our optimum.
   after we have trained, our new theta is [-0.75964, 0.91470]         much
   different to our initial theta. our final cost is 0.53, smaller than
   our initial cost. intuitively, this does not seem much smaller. and
   this intuition plays out; our final percent accuracy is only 79%
   (again, remember that many modern machine learning algorithms can
   achieve 95% fairly easily). we know that this is our available optimum
   because our gradients are almost zero: 1.1593e-05 and 1.3926e-05 where
   e-05 is a shorter notation for    10   5, not to be confused with the
   natural constant e. hence, our final gradients are 0.000011593 and
   0.000013926. although this isn   t 0, we still say we have converged.
   practically speaking, id119 will never converge because the
   gradient that we subtract   _j by will keep getting smaller each
   iteration. so we progress further on the x-axis in a fashion that tends
   towards the optimum, but won   t reach the optimum. the gradient of our
   convex function will hence become flatter and flatter, and so the
   gradient will tend to zero. this doesn   t matter; there is no practical
   difference between a gradient of 0.000011593 and 0. this is why we
   terminate id119 after a constant number of iterations.

   v. error minimization

   our results, statistically and visually, clearly show that our current
   implementation of id28 was not able to find an accurate
   decision boundary to fit the data. we will now look at how we can
   increase the accuracy from its current 79% to at least 95%, and then
   beyond.

   bias terms

   a major oversight of mine while looking at id28 (and one
   that was clear in our data testing) was forgetting that many functions
   have a    constant    term that isn   t attached to any feature. we can see
   this in figure 16. because the decision boundary intercepts the origin.
   this constant term allows us to vertically translate the decision
   boundary so that it does not have to intercept the origin, enabling us
   to fit our data much more optimally.

   we will denote our constant term as   _0, the first value in   , and
   hence extend the dimensionality of   . note that this does not change
   the value of n, our dimensionality of the feature vector, as our new
   hypothesis will be: h(x) =   _0 +   _1   x_1 +       . +   _n   x_n. so, instead
   of referring to features and weights at index 0 (  _0/x_0), we start at
   index 1 (  _1/x_1). no feature is attached to   _0, hence it weights
   nothing and acts to vertically translate the decision boundary.

   however, now our equation   ^transpose   x is invalid, as the dimensions
   of    and x mismatch and every value in    will need to multiply with
   some value in x. but, instead of thinking that   _0 weights/is a
   coefficient of no value, we can think that it weights 1. this is
   because 1     _0 =   _0 for any possible value of   _0. we hence append the
   value 1 to the start of every feature vector x and call it a    bias
   term   , such that x_0 = 1. this bias term isn   t really a feature, and so
   we still consider n to be the same. now, we say that matrix x        ^(m x
   [n + 1]) , feature vector x or x_i        ^(n+1), and parameter vector       
      ^(n+1). figure 18. shows our decision boundary with bias terms
   included.
   [1*0y84vq7c0m_ijlacjnjz4g.png]

   our decision boundary separates our classes much better now. figure 19.
   shows statistics regarding the improvement.
   [1*elk6kmou3991vc8yz5e1ka.png]

   now we have three weights, with the first being our new constant term.
   the algorithm decided that a y-intercept of -5.9 is suitable. notice
   that our other weights have also completely changed as a result of this
   addition. our final percent accuracy is an impressive 94%! our cost is
   much lower at just 0.18. this is almost ready for industry usage; we
   could employ a few more techniques to increase the accuracy even more.
   based on figure 18., it seems as if a line may not be the best decision
   boundary for our data. it instead looks like it could be better
   separated in a curve-like fashion. next, we will begin to change our
   decision boundary from a simple linear function to a model of higher
   complexity.

   hypothesis function complexity

   perhaps a linear function isn   t ideal for our decision boundary because
   it    underfits    our data. a machine learning hypothesis/parameter vector
   is said to underfit when it fails to generalize well on the training
   set, and is too simple with regards to the data it is trying to model,
   resulting in a high cost. figure 20. demonstrates a data set where a
   polynomial decision boundary is clearly suitable, but we have fitted a
   linear function.
   [1*kct1tryx1cspum8ebwgaxa.png]

   figure 20. is an example of extreme underfitting, but our own results
   with figure 18. are much milder. still, there is room for improvement.
   and with something as serious as classifying cancer tumors, effort to
   reduce underfitting should be taken seriously.

   diagnosing the problem of underfitting is very easy in a two or
   three-dimensional space like figure 20.; we can simply plot the data
   and make a judgement call. but when we want to include 4 or more
   features, this benefit is no longer available to us, so things get a
   bit more complex. to explore how we detect underfitting in 4+
   dimensions, we first need to define some more terminology. we will
   still denote our training set as the one our machine learning algorithm
   will learn from, but we will now define our cross-validation set. our
   cross-validation set is actually inherited from our training set         we
   will choose some small percentage (eg. 30%) and select and remove that
   percentage of instances from the initial training set, placing it into
   our cross-validation set. our cross-validation set is hence of the same
   data reliability and accuracy as the training set is. we can then use
   our cross-validation set to estimate how well the model would perform
   on data out of its training set         when making predictions.

   more concretely then, we can detect underfitting when our model, at its
   optimum, performs poorly (with a high cost) on both the training set
   and the cross-validation set. this is called high-bias. imagine you
   were to make future predictions with data similar to that shown in
   figure 20.; it would be difficult to be confident about its accuracy
   not just because the model does not fit the training set well, but more
   implicitly because we have failed to extrapolate the clear trend
   presented.

   a linear function has less distinct terms than a quadratic function. to
   solve underfitting, hence, we increase the complexity of the hypothesis
   function. figure 21. displays the kind of model we may opt for.
   [1*_x_ygklrllv4lq2faj84ww.png]

   to achieve a quadratic decision boundary like the one in figure 21., we
   don   t actually need to modify our hypothesis function h(x).
     ^transpose   x is very convenient for us mathematically, so we can just
   include new    derived features    into x and expand the dimensions of   ,
   increasing n in the same fashion. these derived features will be the
   existing features with exponents raised to them. for example, in the
   case of a quadratic decision boundary, we will add in the value x_i^2
   for each feature x_i into each of our feature vectors x in matrix x. if
   we want a decision boundary of any polynomial with degree d, then we
   would add the value(s) x_i^d, x_i^(d-1),     , x_i^2. however, the
   different feature values in x must also multiply with each other in
   varying degrees. figure 22. demonstrates a select few of features that
   may be derived from the features in x.
   [1*qpeaxpds6brbiwahcfcrpw.png]

   as the degree increases, many more features are created and the
   complexity of the model also increases. then, the machine learning
   algorithm will be able to interact with these new features and fit
   optimum weights in    to build a suitable polynomial decision boundary.

   with all this being said, it is important to not overfit. overfitting
   is the opposite of underfitting; it is when you build a model that is
   so complex it looks as if it is suitable but in reality it is not.
   figure 23. illustrates an example of this.
   [1*adxqgoo8fxrgubbmkfuoyw.png]

   notice how this vastly complex decision boundary in figure 23.
   separates our two classes with higher accuracy than the one in figure
   21. this is the idea (and how we diagnose) of overfitting; our model
   performs very well (sometimes even with 100% accuracy) on our training
   set, but extremely poorly in our cross-validation set. we call this
   high-variance. this is because the machine learning algorithm has
   failed to generalize on the data and find the pattern. instead, it has
   factored in all the noise in an attempt to reduce the cost as much as
   possible with access to a high-degree, flexible polynomial model. so
   when we make new predictions based on new data that will follow this
   real-life trend, our predictions won   t be accurate. it is very easy to
   dismiss the potential of overfitting because, unlike underfitting, it
   does not immediately raise any red flags eg. a high cost.

   there are two ways to reduce overfitting: implicitly increasing model
   complexity and introducing id173. by reducing the degree of
   the model, the curvature/shape of the model can be less versatile, and
   hence there is less opportunity to build a decision boundary that can
   overfit. however, sometimes the trend is of high degree in nature like
   shown in figure 21., so we choose a complex hypothesis function. but,
   because the goal of a machine learning algorithm is to find the least
   possible cost, we will just end up overfitting instead of finding this
   trend. herein lies a problem with the cost function; we need to inform
   the it that it needs to not overfit. this is in fact possible, and it   s
   a common technique called id173.

   the gist of id173 is that we penalize model complexity along
   with training set error to reach a middle ground of simplicity and
   accuracy. we do this concretely by penalizing the size of the weights
   in   . this is because smaller weights results in simpler and smoother
   models as each feature is    weighted    less (less importance)         as these
   weights tend to 0, the features have little to no contribution on the
   output. the following equation is the new cost function that will be
   used.
   [1*h5k6oi9u0-g8fmlfaseqrg.png]

   as you can see, we have introduced a new term into the cost function
   which adds on the squared size of each of the weights to the numeric
   output. this will determine how complex the model is; greater size
   weights increase the complexity factor as discussed before. now, recall
   that the aim of machine learning is to minimize the output of this
   function; to minimize both training error and complexity. an optimum
   training error is created by very complex, overfitted models as shown
   in figure 23. but this will cause a high complexity factor and hence
   the cost will go back up. with very simple models, though, the training
   set error will be high so the cost will also go back up. an absurdly
   low error is compensated by a high complexity factor and vice versa; an
   inversely proportional relationship. ultimately, id119 will
   need to find a combination of error and complexity that will be
   minimum; this is our balance/middle ground, and it will allow us to
   prevent overfitting.

   we multiply this complexity term by constant    which is like the    term
   in id119 in that it is an adjustable constant which controls
   how much we emphasis we put on id173. if we have a high value
   for   , we penalize the complexity factor by a greater degree and so our
   final model will be much simpler. generally, we use    = 1, but also
   experiment with values like 10 and 100 to see the effect on our
   decision boundary. notice that we do not regularize the bias term
   weights/constant   _0 (we start at j=1) because it merely offsets the
   relationship and is not a coefficient of a real-life feature. adding
   id173 to the constant term just implies that we should use a
   small constant term, which perhaps isn   t correct and would just result
   in a grossly poor model. we divide the entire term by 2m as an average
   to keep consistent with our training set error which is also an average
   term (for obvious reasons). we take the square of   _j to penalize
   extremely large weights / extremely complex models even further. there
   are other reasons we do this too;   _j^2 will give us a positive value
   (negative values for our weights are common) which we need, and also
   when we differentiate this new function for id119, the
   exponent will conveniently lower and cancel out with the 2 in 2m   1. the
   following equation is our new expression for each update in gradient
   descent after applying the differentiation rules.
   [1*_gbxfhnxb-c9dhdawt6iya.png]

   i ran regularized id28 with varying polynomial model
   degrees. figure 24. shows a few different decision boundaries that were
   formulated, along with their corresponding degrees.
   [1*hzhzp41vksrfq7evm0ivaw.png]

   degrees 3 and 4 achieve a 95% accuracy! a degree of 2 fits the data
   with 94% accuracy         the same as our linear decision boundary. a degree
   of 4 extends/wraps around vertically in a fashion that seems obviously
   grossly inaccurate for predicting on new data. i would use a degree of
   3 for our decision boundary.

   vi. prediction

   so far, we have formulated a model that achieved a compelling, industry
   level accuracy of 95%. however, we have only been working with two
   features (single epithelial cell size and uniformity of cell size) for
   visualization purposes. remember that in figure 14. we have 9 features
   for each patient, including:
     * clump thickness
     * uniformity of cell size
     * uniformity of cell shape
     * marginal adhesion
     * single epithelial cell size
     * bare nuclei
     * bland chromatin
     * normal nucleoli
     * mitoses

   along with the benign/malignant category. now, we need to train our
   algorithm to create a model that classifies the tumor based on all
   these 9 features, not just two! can you imagine a decision boundary
   like those shown in figure 24. but in 9 dimensions instead of 2
   dimensions? no! it is impossible for us, but machines can achieve this
   purely by using id202.

   figure 25. shows varying values for the degree of the model and    with
   their corresponding accuracies and results.
   [1*ajfszdwayznqoswz80hjsa.png]

   we have great results all around         each much beyond 95%. a linear fit
   has an accuracy of 97%, but a model of degree 2 (with id173
   applied) achieves an astounding 98%, and we will proceed with this. a
   degree of 3 dips in accuracy, and will be more complex than a degree of
   2, so we do not use it.

   now it is time to make predictions! i used our    from the model we
   decided on. to make predictions, we perform the following procedure:
    1. fetch tumor data for a patient (with the 9 attributes)
    2. insert these values into a feature vector x
    3. generate polynomial (degree 2) features as shown in figure 22.
    4. set x_0 = 1 (bias term)
    5. feed this into our function h(x) with our computed    and feature
       vector x
    6. round the output value of this function to either 0 or 1
    7. if output = 0 then the patient has a benign tumor, else they have a
       malignant tumor
    8. inform the patient of their status
    9. take any precautions based on the non-rounded id203 value

   let us take a patient from the data presented on figure 14., row 7,
   sample code number 1017122. this patient   s class is m, hence we expect
   our machine learning algorithm to predict a malignant tumor. figure 26.
   shows what my program actually predicted.
   [1*wly1qikgrpuqfsgyocsapq.png]

   my program makes an accurate prediction. interestingly, our prediction
   id203 is almost exactly 1 (computer does rounding), even though
   any value of 0.5 or greater would be considered malignant. figure 27.
   shows an example where this is not the case.
   [1*zlepugng6q-czna9ldopag.png]

   our id203 is still very high         0.999! this is one of the lowest
   (but accurate) probabilities i could find.

   remember that our accuracy was approximately 98%. this means that we
   predicted inaccurately for 2% of our training set. figure 28. shows a
   cell in our data spreadsheet that is part of this 2%.
   [1*sh2jlk6v363kvndfmak3zg.png]

   an anomaly in our data, we were unable to predict the status of this
   patient correctly. however, do note that the output id203 was
   roughly 0.33         quite high to simply disregard. for a doctor, this may
   be a sign to take precautions and undergo a more thorough examination.
   if we tell a patient they do not have cancer when in fact they do, this
   is called a    false negative    (a negative class         benign         which we have
   predicted incorrectly). we want to minimize on false negatives because
   if we do not treat a patient with any medication, they may pass away
   from their cancer unknowingly. on the other hand, false positives (we
   predict that the patient has a malignant tumor when in fact they don   t)
   are less serious because it is better to be safe than sorry         that is
   we do not actually know what the patient has and so treating them with
   medication will guarantee their health either way. figure 29. displays
   statistics about the false negatives and positives in our data.
   [1*lllxlw2dcukqknhdmakdsa.png]

   67% of our errors are false negatives, which is bad news. one thing we
   could do is, instead of rounding the id203, setting a    decision
   threshold    to something lower eg. 0.25. this means that any id203
   bigger or equal to 0.25 will be considered as malignant, and anything
   lower benign. figure 30. shows how this changes our results.

   note that even though we have significantly more errors, we only have
   half of the number of false negatives as before. and they make up only
   24% of all errors. this is much better.
   [1*r-2j0gnlhbd5j6dlydttia.png]

   let   s say that a patient arrives to the doctor   s office and the doctor
   records the following data for this person:
     * clump thickness = 6
     * uniformity of cell size = 4
     * uniformity of cell shape = 4
     * marginal adhesion = 8
     * single epithelial cell size = 9
     * bare nuclei = 22
     * bland chromatin = 4
     * normal nucleoli = 5
     * mitoses = 10

   [1*jdlpog6dnw23t7xs6hjxhg.png]

   we will tell them that they, tragically, have a malignant tumor, and
   will proceed with medication from there.
     __________________________________________________________________

   and that   s pretty much it! id28 is a complex algorithm,
   but i hope those who have little experience with cs and more with math
   could follow through this rather comfortably.

     * [22]machine learning
     * [23]data science
     * [24]case studies

   (button)
   (button)
   (button) 105 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [25]go to the profile of rohan kapur

[26]rohan kapur

   rohankapur.com

     (button) follow
   [27]a year of artificial intelligence

[28]a year of artificial intelligence

   our ongoing effort to make the mathematics, science, linguistics, and
   philosophy of artificial intelligence fun and simple.

     * (button)
       (button) 105
     * (button)
     *
     *

   [29]a year of artificial intelligence
   never miss a story from a year of artificial intelligence, when you
   sign up for medium. [30]learn more
   never miss a story from a year of artificial intelligence
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://ayearofai.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/13f379edab3b
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-1-when-would-i-even-use-a-quadratic-equation-in-the-real-world-13f379edab3b&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-1-when-would-i-even-use-a-quadratic-equation-in-the-real-world-13f379edab3b&source=--------------------------nav_reg&operation=register
   8. https://ayearofai.com/?source=logo-lo_hi89yyyvcrdz---bb87da25612c
   9. https://ayearofai.com/tagged/algorithms
  10. https://ayearofai.com/tagged/today-i-learned
  11. https://ayearofai.com/tagged/case-studies
  12. https://ayearofai.com/tagged/philosophical
  13. https://ayearofai.com/tagged/meta
  14. https://ayearofai.com/@mckapur?source=post_header_lockup
  15. https://ayearofai.com/@mckapur
  16. https://medium.com/a-year-of-artificial-intelligence
  17. https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5
  18. https://docs.google.com/presentation/d/1xxo_7bg8zylm-zm1hsckv_fgwxu3mztxohfgwvq0wmo/edit?usp=sharing
  19. http://www.apple.com/
  20. https://www.quora.com/convex-optimization/why-use-gradient-descent-when-the-normal-equation-exists
  21. http://mathgotchas.blogspot.com.au/2011/10/why-is-error-function-minimized-in.html
  22. https://ayearofai.com/tagged/machine-learning?source=post
  23. https://ayearofai.com/tagged/data-science?source=post
  24. https://ayearofai.com/tagged/case-study?source=post
  25. https://ayearofai.com/@mckapur?source=footer_card
  26. https://ayearofai.com/@mckapur
  27. https://ayearofai.com/?source=footer_card
  28. https://ayearofai.com/?source=footer_card
  29. https://ayearofai.com/
  30. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  32. https://medium.com/p/13f379edab3b/share/twitter
  33. https://medium.com/p/13f379edab3b/share/facebook
  34. https://medium.com/p/13f379edab3b/share/twitter
  35. https://medium.com/p/13f379edab3b/share/facebook
