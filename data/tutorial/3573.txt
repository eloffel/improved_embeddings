   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]a year of artificial intelligence
     * [9]algorithms
     * [10]today i learned
     * [11]case studies
     * [12]philosophical
     * [13]meta
     __________________________________________________________________

   [1*csaihx_x_-d-z_bmzbnkpq.jpeg]
   [14]http://www.awf.org/sites/default/files/media/gallery/wildlife/chimp
   anzee/chimp_craig_r_sholley.jpg?itok=4qffxy3f

rohan #2: artificial intelligence,    progress/   time

a technical and philosophical analysis on the past, present, and future of
artificial intelligence.

   [15]go to the profile of rohan kapur
   [16]rohan kapur (button) blockedunblock (button) followfollowing
   feb 8, 2016
     __________________________________________________________________

     this is the second entry in my [17]journey to extend my knowledge of
     artificial intelligence in the year of 2016. learn more about my
     motives in this [18]introduction post.
     __________________________________________________________________

   this is sort-of my first blog post into [19]a.y.o.a.i. even though i
   posted a lengthy (30 minute read) exploration of applied logistic
   regression a couple of weeks ago. i may do more of these explorations,
   now that i think about it; they   re a nice documentation of in-depth &
   holistic comprehension, and they only take a week or two to write.

   interestingly enough, my [20]zero   th blog post garnered enough
   attention to (partly, at least) land me an internship interview at a
   high profile fin-tech startup in the bay area. pretty cool, right?
   unfortunately, since i had to prepare for said interview, i   ve had less
   time to submerge myself into artificial intelligence than expected.
   instead, i had to submerge myself into data structures and
   algorithms         boring (admittedly, though, some questions were very fun)!

   today we look at the history of artificial intelligence, where we are
   at this point in time, and predictions for the long term future. the
   discussion will not exclusively feature a timeline of events; i   ll also
   look into the technical details of modern day ai and future
   strategies/paths/dangers. i feel compelled to write this because,
   before i delve into such a complex field, i need to understand the
   history behind it: the problems people faced and why we are where we
   are now. the content in this blog post is an accumulation of what i   ve
   learned in the superintelligence [21]book, machine learning [22]book,
   and wikipedia entries i   ve been reading.

i. the past

early representations of ai

   to most (i would think) people, the concept of artificial intelligence
   is most synonymous with representations in popular fiction like
   terminator or robocop. that is, intricate robots that can become
   sentient and as a result relentlessly pursue existentialism objectives
   with regards to some (evil, more often than not) selected moral
   standard.

   as [aspiring] computer scientists and mathematicians, it is easy to
   diminish these views and take pride in our more educated approaches
   like id76. but let   s not forget that ai grew out of myth
   and speculation; in fact, mechanical/artificial men began appearing
   from greek mythology and continued to develop through the middle ages
   until its introduction to formal fiction in the 1800s.
   [1*jcq9p2axgncgmqw1y8dezq.jpeg]
   islamic programmable automatons built in 1206 ce   
   [23]https://upload.wikimedia.org/wikipedia/commons/2/2f/al-jazari_robot
   s.jpg

   philosophers began to introduce and discuss the concept and
   implications of artificial intelligence in formal reasoning because it
   imposed interesting conflicts in their theories of topics such as the
   relationship between mind and body, ethics, personal identity, etc. i
   recently learned about the ideology of    functionalism            which states
   that mental states are defined solely by their function         in my ib high
   school philosophy course. functionalism is fascinating because it
   implies that artificial intelligence agents legally have minds just
   like us humans. many also thought that all human reason could be
   reduced to some systematic representation. development of this notion,
   associated with mathematical logic (eg. id198), by
   philosophers such as hobbes and descartes was the basis for the
   breakthrough that resulted in many scientists beginning to delve deep
   into the idea of machines that could think.

tangible ai in computer science

   the first modern, digital computers (such as eniac and colossus) were
   invented during world war ii (for encryption breaking purposes). these
   computers were developed on the theoretical basis laid by alan turing
   in world war i and john von neumann. with this new technology in hand,
   artificial intelligence was officially founded as an academic
   discipline in 1956 by domain experts in a plethora of fields such as
   math, psychology, economics, philosophy, etc.

   algorithms modelled after biological systems were, interestingly
   enough, one of the first distinct groups of ai to be introduced in the
   field. after extensive research in neurology demonstrated our brains
   were an electrical network of neurons firing signals, many hypothesized
   that, according to alan turing   s theory of computation, such a data
   structure and system could be represented digitally.

   researchers walter pitts and warren mcculloch explored how artificial
   neurons could work together to perform simple logical functions such as
   or, and, etc. mimicking the output of the logic gate xor (logical
   complement of or) posed challenges with single layers of neurons, and
   researchers would much later pioneer a data structure with multiple
   layers neurons         coined    neural networks    (which i   m sure many of you
   have heard of). the first neural network machine, the [24]snarc, was
   designed and built by marvin minsky, who is now recognized to be a
   leader in the field of ai. of course, this invention was not as
   revolutionary as contemporary neural networks because a) the effective
   id26 training algorithm had not been discovered yet and b)
   because there was a lack of computation power to do something
   meaningful like image recognition.

   sidenote: was going over my draft and just found out that marvin minsky
   passed away last month on january 24 at 88 years old. r.i.p. marvin
   minsky and thank you for your contribution to this wonderful
   discipline.
   [1*jmbfzdne1wg8ich4v-vsna.jpeg]
   i really like this picture of eniac because of   . all those wires!
   eniac         [25]https://upload.wikimedia.org/wikipedia/commons/4/4e/eniac.j
   pg.

   meanwhile, alan turing devised a philosophic thought experiment
   famously denoted as the    turing test   . as progress in ai was
   accelerating, turing suggested that a    thinking machine    could be
   determined if it convinced another subject, through an online chatting
   mechanism, it was human.

a note on game ai

   throughout history, game ai has been an underlying indicator of
   progression in ai. the first ai to beat a human in checkers (by
   christopher strachey in 1951         in this timeline) was perhaps one of the
   first tangible feats of achievement in the discipline. the most recent,
   tangible feat in ai was google   s deep learning (deep neural networks)
   [26]project that won against a chinese professional player in the
   infamously complex game of go.
   [1*i6j17ixtpvvrn4c4daureg.png]
   checker
   ai         [27]https://camo.githubusercontent.com/f7b639ce52737d636af7e13b9bd
   418d178443837/68747470733a2f2f7261772e6769746875622e636f6d2f616c6971756
   9732f6b74682d616931342d636865636b6572732d76697375616c697a65722f6d617374
   65722f646f632f73637265656e73686f742e706e67

dartmouth

   if one were to ask the question         which institution made the greatest
   innovation in early artificial intelligence? the typical response would
   most likely be mit or stanford. perhaps those are true of the modern
   day, but for early ai, the answer is undeniably dartmouth.

   in the summer of 1956, a group of scientists sharing an interest in the
   study of artificial intelligence through concepts like biological
   representations of intelligent algorithms and automata theory joined
   together in a six-week workshop/conference at dartmouth college
   (formally known as the [28]dartmouth summer research project on
   artificial intelligence). the conference is renown (perhaps also due to
   its convenient timing) to be the birth of modern day ai research. many
   of the participants (most notably including marvin minsky) would later
   be recognized as the founding fathers of the field. the following is
   the proposal of the event by organizer john mccarthy:

        we propose that a 2 month, 10 man study of artificial intelligence
     be carried out during the summer of 1956 at dartmouth college in
     [29]hanover, new hampshire. the study is to proceed on the basis of
     the conjecture that every aspect of learning or any other feature of
     intelligence can in principle be so precisely described that a
     machine can be made to simulate it. an attempt will be made to find
     how to make machines use language, form abstractions and concepts,
     solve kinds of problems now reserved for humans, and improve
     themselves. we think that a significant advance can be made in one
     or more of these problems if a carefully selected group of
     scientists work on it together for a summer.            mccarthy et al. 1955

   advancements in natural language processing, neural networks,
   abstraction, and/or self-improvement were either made or inspired due
   to the conference. the work was mainly focused on proving skeptics
   incorrect (perhaps this was the most effective way of fostering
   excitement in the field) with regards to claims that machines were
   unable to perform various arbitrary tasks such as the ability to apply
   logical deduction to abstract problems. the [30]logic theorist         a
   computer program that formed elegant proofs for many theorems presented
   in the book [31]principia mathematica (the foundation of
   mathematics)         did exactly that.
   [1*lrtlymw3nejvmte5tuz2tw.jpeg]
   five of the dartmouth conference members, reunited in
   2006         [32]http://www.dartmouth.edu/~vox/0607/0724/images/ai50.jpg

business be booming

   [1*thqeessdesiqs47xmupcia.jpeg]
   it was a bullish era for
   ai         [33]http://i.telegraph.co.uk/multimedia/archive/02423/bull_2423808
   b.jpg

   the years from 1956 to 1974 were largely considered to be the    golden
   years    of ai. after the dartmouth conference, many new inventions and
   discoveries were made such as: the [34]general problem solver (which
   could solve a wide range of formal mathematical problems), calculus
   problem solvers, algebra problem solvers, etc. the [35]eliza program
   made advancements in (and was one of the first implementations of)
   natural language processing that enabled a user to communicate and
   obtain advice from an ai psychotherapist using [36]semantic nets. in
   the year 1968, the [37]shrdlu program made further developments in nlp
   as it displayed a simulated robotic arm in a virtual, 3d, micro-world
   (powered by physics engines) that was able to decode and follow
   instructions typed in by users in natural language such as moving and
   stacking blocks.
   [1*ot6pnbssjclidpqqa-k4vw.gif]
   shrdlu. do the graphics not look as cool as you had envisioned? well it
   was the
   1970s!         [38]http://hci.stanford.edu/winograd/shrdlu/shrdlu-original.gi
   f

   in the coming decades, accomplishments in the industry only heightened.
   programs that could compose music, create art, perform clinical tasks,
   drive cars autonomously, and even make original jokes!

        what do you get when you cross an optic with a mental object? an
     eye-dea!   

   the [39]standup system cracked this pun.

   many more innovations in nlp, logical reasoning, and micro-worlds were
   conceived in the era. optimism was abundant; marvin minsky and many
   other leaders conjectured that machines with general intelligence of an
   average human being would be successfully developed by 1978. from the
   1960s-19670s, a public sector organization called [40]darpa awarded mit
   a $5 million dollars in grants to fund many more of these scientific
   breakthroughs. similar grants were awarded to colleges like cmu and
   stanford.

it   s winter time

   [1*9r9yj5diyn36parxvfuaug.jpeg]
   winters: they   re cold, but beautiful in their own right. and they   re
   there for a reason. we don   t have them in singapore so don   t take them
   for
   granted!         [41]http://cdn1.theodysseyonline.com/files/2015/12/04/635848
   557150633136-120303261_winter.jpg

   proving a theorem that has a prospective 10 line proof? simple         it
   takes some negligible time   . proving a theorem that has a prospective
   50 line proof? would it take equally negligible time 5  ? no; it would
   take 5       possible sequences (thank you [42]nick bostrom for the
   reference). this is because many of these    impressive    ai programs used
   the same basic algorithm         that is a simple search algorithm (which was
   by no means simple back then) which iterated through all possible
   solutions until a successful one was found. as the search space
   increased, the number of possible paths astronomically increased         this
   is known as a [43]combinatorial explosion.
   [1*7dsxxoghwkpaynmwatfe7q.jpeg]
   holy shi*t
   indeed!         [44]http://static1.squarespace.com/static/552e59bce4b0a0d7241
   f9c43/t/567f4a461c1210eb4f65d612/1451182688597/

   combinatorial explosion and poor scalability of ai tasks in general was
   a key contributing factor to a downfall of the field of ai in general
   from 1974   1980. these    toy    solutions grouped with:
     * limited computational power. this was especially a problem with
       neural nets as mentioned before         there was simply not enough
       memory and/or processing power to achieve meaningful results in a
       reasonable time. single-layered id88s (a type of neural
       network) were too rudimentary to perform logical tasks, but the
       answer         multi-layered id88s         was too computationally
       expensive. in addition, there was no effective way to train a
       neural network         this would come later.
     * a lack of growth in id161 and robotics research. this is
       correlated with limited computational power, but dead-end
       approaches in general were common too. it was almost a
       paradox         highly complex tasks like formulating proofs to
       mathematical theorems was easy for a computer but difficult for a
       human. yet something so seaid113ss for us such as recognizing objects
       and patterns was near impossible for machines. this sparked an
       underlying belief that perhaps the    artificial intelligence    that
       had been developed was not closely modelled to our human
       intelligence enough and was really just taking advantage of the
       greater computational and numerical arithmetic abilities (which
       were proving eventual limiting factors in any case).
     * scarcity of centralized/collective data and information to train or
       supply algorithms eg. nlp with. remember google let alone the
       internet was neither mainstream nor available to the general public
       at the time.
     * poor abstraction. many ai specialists, when creating solutions to
       problems, discovered that their solutions were did not generalize
       on the program domain         making small changes involved drastically
       changing the logic used.

   if one were to differentiate an arbitrary function of ai progress with
   respect to time (the inspiration for this title), one were to find that
   at the beginning of the winter, we had reached a gradient of 0. we had
   hit a local maximum, and research was beginning to decline. darpa
   became frustrated with the lack of innovation and cut off funding/ended
   grants from almost all of their ai research investments; darpa
   rescinded $3 million dollars from the [45]speech understanding research
   program at cmu. by the mid 1970s, obtaining funding for ai research
   projects was becoming increasingly difficult, and eventually near
   impossible. skepticism for the field increased and ai fell out of
   fashion.

criticisms in philosophy

   many contemporary philosophers refuted the possibility of machine
   intelligence (in terms of semantics). john searle most notably did so
   in 1980 with his    chinese room experiment   . the chinese room experiment
   demonstrates that, even though a computer can convey intelligence (or
   potential sentience in the future), it actually possesses none.
   [1*vxbu7p3eohiavr6d5qsdka.jpeg]
   a diagram of the chinese room
   experiment         [46]http://philosophyisawesome.weebly.com/uploads/2/5/0/2/
   25029532/3222576_orig.jpg

   suppose a room with two holes on either end         one which allows the
   input of english messages, and one which outputs the chinese
   translation of said message. in the middle is a room where the
   translation, achieved by a man (or intelligent agent), occurs. if you
   haven   t noticed already, this ties in nicely with the traditional
   computer architecture of (input     processing     output). although the
   man comprehends neither english nor chinese, he has a book that allows
   him to    map    each character/word of english to each respective
   character of chinese.

   thus, on the outside, it seems as if the man is fluent in both chinese
   and english. yet he is not. this is searle   s argument against
   artificial intelligence; although computers can portray signs of
   intelligence, they do not have an inherent understanding of    really
   anything. they simply follow instructions, and can be reduced to a
   series of states of the presence or absence of current.

   sidenote: another compelling theory to look at is [47]godel   s
   incompleteness theorem. philosopher john lucas argued that this theory
   demonstrated a [48]formal system could never evaluate the truth of
   certain statements that humans can.

history repeats itself, part 1

   [1*nwgy5x0-5xmgnx-w8mv0sa.jpeg]
   the volatility of the progress in ai, best expressed through earth   s
   natural
   seasons         [49]http://www.toptenpack.com/wp-content/uploads/2014/12/seas
   ons-wallpapers-for-walls.jpg

   at this point i feel mentally exhausted knowing i   ve spent 5 hours
   writing what is at most a third of my entire article. so here   s what
   you need to know: from 1980   1987 another boom occured         just like that,
   it was summer time again!

   japan launched its aggressively (just shy of $1 billion usd) funded
   [50]fifth-generation computer systems project, a public-private
   partnership that focused on advancing computer architecture to, as a
   result, enable innovation in ai algorithms of high space and time
   complexity. many other countries such in the europe and the u.s.
   followed in hopes to simulate the economic success japan experienced
   throughout the years.

   following the failures from the previous ai winter, many researchers
   decided to focus on centralizing knowledge/information so that these
   intelligent agents could become informed in their reasoning. again,
   remember, the mainstream internet and online search succeeded this era
   by over 10 years. and as a result, the new direction for ai was
   centralized around knowledge: id109. an expert system is a
   series/combination of logical rules layered on top of each other to
   determine some output based on an input. these rules were usually
   devised by domain experts. a common use case for id109 were to
   determine the possible illnesses based on a collection of symptoms the
   user inputs.
   [1*fwcvqrup2djtz2xk4nfipa.png]
   a diagram representation of id109. it really is surprising how
   this was ever considered
   ai!         [51]http://www.igcseict.info/theory/7_2/expert/files/stacks_image
   _5738.png

   id109, however, were really the epitome of    dumb    ai         each
   expert system was effectively a database that had to be coded by hand
   and was not self-adaptable to any extent. it was time consuming and
   expensive to develop, validate, and maintain         not to mention the
   impracticality of requiring a standalone computer to just run a single
   program. by the late 1980s, this growth cycle had too run its tiring
   course.

   it was clear that id109 were a dead-end         how could these   
   databases ever be further developed to create machines which could
   think on their own? fortunately, in 1982, physicist john hopfield
   devised a form of neural network (denoted as the [52]   hopfield net   )
   that pushed the field forward. hopfield nets could learn and process
   information in an entirely new way         and they contributed to modern day
   id158s. their outputs could be rewired into their
   inputs so they are a form of recurrent neural networks (id56s). at the
   same time, david rumelhart evangelized the [53]   id26   
   algorithm discovered by paul werbos two years prior. the
   id26 algorithm exploited id128 to identify
   how much each weight in a neural network contributes to the overall
   error. with the ability to pinpoint the accuracy of each individual
   weight, modifying the entire system to move in a more optimal direction
   could be immediately clear.
   [1*kyxaebtvmejvlmmzasf5cg.png]
   a hopfield net with four nodes. does it look familiar?
      [54]https://upload.wikimedia.org/wikipedia/commons/9/95/hopfield-net.p
   ng

   sidenote: the id26 algorithm is awesome, and i   m really
   excited to delve into the mathematics behind it in a couple of days!

   [55]connectionism and algorithms modelled after the brain experienced a
   surge in popularity. they experienced commercial success in the 1990s
   where they were used for programs like id42
   and id103.

   if all these terms (anns, id56s, hopfield nets, id26, etc.)
   seem similar to you, it   s because these inventions are still widely
   prominent in ai today, classed under the umbrella of    machine
   learning   . [56]deep learning (which, as previously mentioned, is just
   extremely deep layered neural networks) is perhaps the ai-craze of the
   21st century.

history repeats itself, part 2

   so it should be surprising to find out that ai experienced a second
   winter from 1987   1993. advances in connectionism are still appreciated
   today, but it was time for id109 to retire. ultimately, the
   fifth-generation computer systems (alongside the western counterparts)
   failed to meet its objectives (which was actually primarily centralized
   around nlp). as history repeated itself a second time, skepticism in ai
   was perhaps at its global maximum. artificial intelligence, the field
   where    ultimate failure almost always followed ultimate success   ,
   became shunned by investors and academics.

   it is important to note, however, that the field continued to make
   advances despite heavy criticism. that is why, when the winter
   inevitably thawed in 1993, it has remained in a state of
   california-esque summer until today, february 2016 and beyond.

   sidenote: at the end of the 1980s, [57]nouvelle ai         the idea that
   artificial intelligence should possess a physical body, achieved
   through robotics         became increasingly popular amongst researchers. i
   am unsure the extent to which this has developed in the present day,
   but i would love to hear from anyone who is knowledgeable on this
   topic.

ii. the present

a new approach

   and so, from 1993 onwards, the field has thrived in overcoming
   g.o.f.a.i. [58](good old fashioned artificial intelligence)         or the
      symbolic artificial intelligence    that we have described previously.
   examples of where the field has experienced great success include
   neural networks, statistical modelling, probabilistic modelling,
   id91, and id107 (those which can evolve a set of
   candidate solutions based on a digital implementation of biological
   natural selection). these alternative, more mathematical paradigms
   rekindled optimism and excitement in the early 1990s. century old
   artificial intelligence goals were finally achieved. there definitely
   seemed to be a greater focus on creating models for data through
   systems using geometric model related approaches. the different ways of
   achieving these optimal models include id76,
   evolutionary algorithms, pattern/id91 recognition, etc.
   [1*fdwyetvrc7o3yhs5esgw_w.png]
   modern day deep neural networks    that   s a lot of
   computations!         [59]http://www.rsipvision.com/wp-content/uploads/2015/0
   4/slide5.png

   these algorithms tie together a plethora of different disciplines, most
   notably including mathematics, economics, and psychology to make it a
   much more    scientific    discipline. other practices such as nlp are also
   being worked on by many, but experience less innovation (perhaps
   because perfect natural language is considered to be an    ai-complete   
   problem         that is a problem whose solution is essentially the
   equivalent of a human-level general intelligence / strong ai).
   companies like google, baidu, and facebook have opened artificial
   intelligence research centers just for the purpose of pushing the field
   forward         they open source and publish (presumably) all of their
   findings. artificial intelligence is now recognized to be one of the
      hottest    fields of computer science to be in during college and in
   career.

overcoming limiting factors

   in this way, there is great emphasis on machines that can    learn    from
   past experiences and data it is provided (and, on that note, there is a
   great volume of information due to the internet). this training,
   especially in high dimensions, is very computationally and memory
   expensive but, due to advancements in hardware, we are at a point where
   it no longer matters. we have moore   s law to thank for this.
   [1*t-w-1j7ri4mzptm0vchjfa.png]
   moore   s law: an exponential
   curve   [60]http://www.extremetech.com/wp-content/uploads/2015/04/mooresl
   aw2.png

   [61]moore   s law is the observation that the number of transistors (and
   hence computational power) that can fit into a dense, integrated
   circuit has doubled approximately every two years. the observation was
   made by gordon moore, the co-founder of intel. we can postulate that
   computational power = a     2^(t/2) where a is the initial computational
   power and t is the number of years after the first existing transistor
   circuit. compared to the mid 1970s, we in 2016 have at best a greater
   computational power of 2     !

   thus, availability of data/information and limited computer power was
   no longer an issue. on top of that, [62]heuristic-based search that
   exploited prior knowledge and the target domain solved the
   combinatorial explosion that search algorithms faced (this solution
   became more prevalent as search spaces increased eg. with mapping
   software). in addition, with research ongoing not only in academia but
   also at top silicon valley & tech companies, id161 (eg. with
   google   s self driving cars) experienced rapid development, especially
   through exploitation of the id91, segmentation, and neural
   network algorithms that had been unlocked as the other limiting factors
   diminished.
   [1*tbkdsier2ot3mpdp6rloqq.jpeg]
   a video segmentation
   algorithm         [63]https://www.mpi-inf.mpg.de/fileadmin/_processed_/csm_mu
   st-links_4f7afcb337.jpg

tangible academia achievements

   it   s remarkably difficult to name and describe examples of breakthrough
   artificial intelligence projects in the last 20 years because    there   s
   so damn much! in this decade, there appears to be a new revolution
   every week. but, as mentioned before, advancements in game ai is a
   decent, general indicator on the progress of ai, and so i will discuss
   the new inventions with regards to this scope.

   in 1997, [64]deep blue became the first ai to beat the world champion
   (garry kasparov) in a game of chess. this is a classic example of    weak
   ai            that is ai that matches or exceeds human level intelligence but
   only in one (or one in the potential millions) domain. by contrast,
   strong ai matches or exceeds human intelligence in general (in majority
   of domains). in 2005, a stanford robot won the darpa grand challenge by
   driving autonomously for 131 miles on a desert track without any prior
   rehearsal on said track. just a couple years later, cmu won a similar
   challenge by successfully driving an autonomous car through an urban
   city environment without any crashes or accidents. these efforts
   inspired companies like google, tesla, and perhaps apple to develop
   commercial self-driving cars. in 2011, ibm challenged two jeapordy! (a
   popular trivia/quiz game show) champions to its universal ai system
   [65]watson and successfully won by a significant margin.

   such defeats (or ties) against top contenders extended to games such as
   checkers, backgammon, othello, crosswords, scrabble, bridge, poker,
   freecell, etc. in march of this year (2016), google   s deep learning
   system will challenge the world   s #1 champion in a game of go (which,
   as mentioned previously, is remarkably complex). these feats were not
   due to abrupt revolutions or constant paradigm shifts, but rather
   involved taking some existing algorithm eg. neural networks and
   applying tedious application of engineering skill by iteratively
   improving on both the efficiency and accuracy through new techniques
   (like dropout, convolutions, or simulated annealing) or ruthless
   feature tuning. many of these new applications are published and
   digested/built-on by the ai community. [66]this is google   s paper on
   using convolutional neural networks to power their go ai.

   sidenote: i know very little about convolutional neural networks but i
   think now   s the time to delve in deep :) thanks [67]lenny khazan for
   introducing me to stanford   s [68]course on the topic.
   [1*hrdbueeov77m2u2ykffdxa.jpeg]
   the game of go. the neural network makes an informed choice for the
   next move based on prior events, and will relay this person to the
   sit-in
   player.         [69]https://i.vimeocdn.com/video/553713383.jpg?mw=1920&mh=108
   0&q=70

id23

   a major field that was introduced only in the present era is
   [70]id23; id23 only became widely
   accepted during the 1990s. rl borrows theory from economics and
   heuristic-based search to introduce    intelligent agents    into a
   universe to independently learn and perform some action.

   an intelligent agent is a system that can perceive its surrounding
   environment as a quantitative/symbolic state and take some action to
   maximize its likelihood of future success on some arbitrary goal. for
   taking a step in the optimal direction of the goal, these agents
   receive artificial rewards. in some cases these intelligent agents need
   to tradeoff short term rewards for long term utility (where economics
   plays a role). over time, these intelligent agents mold a certain
   optimal behavior that they can employ in the future.

   although id23 is used heavily in games and robotics,
   the study of intelligent agents is very meta; it is linked to us humans
   (and all other animals) in our sandbox (of the earth or perhaps the
   physical universe). it really can describe all kinds of intelligence.

   the following is an example of an intelligent mario avatar agent that,
   through id23, learned how to interact with its
   environment (ground, obstacles, coins, etc.) with regards to its
   actions (jump, move forwards, move backwards) and some goal state
   (completing the level / completing as much of the map as possible).

   iframe: [71]/media/0e6a656a54495cd19b42110a86bd193b?postid=ae804fa86cca

   self playing mario game         youtube video.

artificial intelligence in industry

   perhaps the most important thing to understand about modern day
   artificial intelligence, however, is that ai development is no longer
   just for the sake of ai development (or some future goal ie.
   superintelligence), but instead is also for implementation in industry.
   that is not to say the two are mutually exclusive, but the latter is
   definitely more prominent than the former.

   in fact, this is exactly why google, baidu, facebook, etc. have made
   efforts in ai research as mentioned previously. not only do they want
   to pioneer the future (many argue that machine intelligence is the next
      big thing    on an existential level) but they also, importantly, want
   to employ these innovations into their consumer and enterprise
   offerings for competitive advantage.

   facebook uses artificial intelligence algorithms to understand what
   content you like the most and hence would like to see on the top of
   your newsfeed. google uses artificial intelligence for a personalized
   search experience, and also most notably for their self-driving cars
   which, alike tesla, are speculated to be mainstream in just 2   5 years!
   big data analysis, id103, banking software, medical
   diagnosis, facial recognition, etc. are all being executed on the
   consumer and enterprise level to make products more intelligent (hence
   convenient         for example siri) and individualized (for example online
   advertisements). artificial intelligence is also used for safety on a
   military level, fraud prevention level, extracting some qualitative
   intent from messages/phone calls/search logs, etc. powerful tools like
   [72]tensorflow are freely available to any developer wanting to
   introduce sophisticated ai into their own products. a lot of cutting
   edge artificial intelligence is no longer labelled as ai because it has
   become so useful and common in general applications. in other
   words         we take this technology for granted!

   and perhaps this is why artificial intelligence is operating at such a
   rapid pace. the volume of money, engineering talent,
   resources/information/data (this one is key), and public support going
   into research projects is much greater in the commercial sector vs. in
   academia. add the incentive of many hundred billion dollar companies
   and such results are only inevitable.
   [1*qoivirrtlkt4d9jjka3bna.png]
   just approximately 15 lines of code to implement a state of the art ai
   algorithm. woah!   
   [73]https://www.tensorflow.org/versions/0.6.0/get_started/index.html

machine learning

   let   s switch gears for a minute and talk about machine learning (and as
   a result, deep learning). machine learning is perhaps the most popular
   modern day subset of artificial intelligence, and the one most commonly
   found in industrial applications. there is no set definition for ml,
   but tom mitchell from cmu defines it very rigorously as:

     a computer program that is said to learn from experience e, with
     respect to some task t, and some performance measure p, if its
     performance on t as measured by p improves with experience e.

   let   s take an example where task t is to classify an email as spam or
   not spam. imagine a    training set    where you have a collection of
   emails which are known and labelled as either spam or not spam, and you
   then ask the machine to guess (using a model it has devised through its
   training         more on this later) the state of each email. for each email
   it classifies incorrectly we decrement the performance measure or
   increment the error measure. using this knowledge regarding its current
   performance, the machine can make iterative adjustments to its model
   (in some direction that can be solved using calculus) and re-evaluate
   itself         we call this the experience e. the cycle repeats itself until
   the machine learning program can correctly classify almost all emails.

   machine learning is very interesting because it explores algorithms
   that continuously learn from data to build complex functions that model
   data. it is a field of study that gives computers the ability to learn
   without being explicitly programmed. in many ways, it is the opposite
   of id109. instead of having to hard-code logical rules into a
   system, machine learning programs can examine data and (indirectly)
   devise its own logical rules through a mathematical function usually in
   some geometric space.

   this performance measure p and experience e are both mathematical
   concepts (average squared error summation and partial differentiation
   respectively), most specifically associated with statistics and convex
   optimization. machine learning was really the landmark of the 1990s
   boom because it demonstrated a new kind of intelligence         an adaptive
   intelligence         that was closer and closer to emulating humans. it took
   concepts in connectionism too and further developed on them. it was
   clear that machine learning could automate (with high efficiency, high
   accuracy, and with novel solutions) many mundane tasks and in the
   process both introduce a new era of convenience and emphasis on
   speciality occupations whilst displacing many blue-collar jobs.

   some of the most popular machine learning algorithms include bayesian
   networks, id158s, id75, logistic
   regression, id23 (which we looked at earlier in this
   section), support vector machines, id91, id107,
   decision tree learning, etc. you   ll notice that these are almost
   identical to those algorithms i introduced in the beginning of this
   section. this is because machine learning really is the focal point of
   artificial intelligence.
   [1*nwfcyei6d3jt2cfcc-2_wa.png]
   machines that can study data and learn from
   it         [74]http://assets.toptal.io/uploads/blog/image/443/toptal-blog-ima
   ge-1407508081138.png

   let   s talk about machine learning in great depth. i particularly want
   to talk about:
     * features: these are the domains of the data that you are trying to
       model and eventually use to make a distinct prediction. for
       example, in a program that will predict the price of a house based
       on its different factors, the features could be: size of the house,
       location of the house, age of the house, etc. n corresponds to the
       number of features we are using. if n > 1, we are in a
       multi-variable setting (which is almost always for industrial
       applications).
     * output: the output is the established (or predicted) contextual
       result that is correlated to the features. for example         the price
       of the house. this purely depends upon its features (considering
       the features selected are relevant). the output is generally
       denoted as y.
     * training example: a training example is one single record of data
       (group of features + output) that has been labelled by humans and
       hence is    correct   . for example, if we went online and found a
       house that was $5 million dollars to purchase with a size of 4000
       square feet, 20 years old, and located in palo alto, ca this would
       be a single training example.
     * feature vector: each training example has a vector of the values
       for the corresponding features, along with its correct/established
       output (price).
     * training set: a training set, as discussed before, is then a
       collection of training examples which have been selected for the
       machine learning algorithm to learn from. generally, a larger
       training set will translate to greater results (typically millions
       of training examples). this data can be found online or collected
       by academic institutions/corporations. our training set is usually
       denoted as x, with each example denoted as x. the number of
       training examples in our training set is denoted as m.
     * trend: the trend of the data is the general correlation between the
       input and the output. one must look at the overarching    direction   
       the data points move in and not scrutinize the individual, minute,
       and intricate offsets.
     * noise: noise is either a) any data that falls out of the
       traditional    trend    or, more leniently, b) the small deviation
       (offset) that each data point has from the trend (note: almost
       every point will have this). in a large training set, you are
       guaranteed to have noise, but it is important to understand that
       noise is stochastic due to the chaotic, physical universe we live
       in. we do not want to accustom for all the noise, because this
       prevents us from extrapolating the trend.

   [1*ohuvvbw2cpzldjwovogn0g.png]
   this is an example of noisy data (almost each data point has some
   deviation from the fitted line) that still has a clear
   trend         [75]https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/l
   inear_regression.svg/400px-linear_regression.svg.png
     * model i: a model is something that takes data points in some
       (multi-dimensional) input space and generally maps them to a single
       real value. neural networks are different here in that they can
       output multiple values representing the probabilities of different
          events   /   states   . so, in this case, each of the data points in the
       input space corresponds to the feature vector, and the output is
       the price.

   [1*k3runw2-cchiqjvp2v7kxw.png]
   a training set without a model (though it looks like we want a
   polynomial here); the x-axis represents the input/features: size of the
   house, y-axis represents the output: price         class.coursera.org/ml-006
     * model ii: as you can see, we need    carve    our model into fitting
       the data/training set above         or quantitatively find/establish the
       trend. in other words, the model is the trend. but what defines a
       model from its standard nature (ie. y = 0, y = sin(x), y = log(x))?
       the coefficients do. hence, one must select optimal coefficients
       that makes an accurate model. assuming we   ve decided to opt for a
       linear fit: y = mx + c, we need to select these parameters m and c
       suitably (x in this case represents the value for a single feature,
       and y hence represents the output/price). imagine    rebranding   
       these parameters to   o and   1         we can now say, with brevity, that
       we want to select the optimum    parameter/weight vector.

   [1*x1qtfauu_1azcbj8mpce6a.png]
   an example of the different models (polynomial vs. linear) and the
   selected coefficients that make this model
   accurate         class.coursera.org/ml-006

   we can now predict the price of the house if the size is 1925 square
   meters. this isn   t in our dataset but our informed model can now tell
   us the predicted value.
     * weight/parameters: these are equivalent to coefficients in the
       model. but they say something interesting; they tell us how
          important    each feature is. if the weight of a feature is zero,
       said feature clearly does not contribute to the output at all.
     * cost function: the cost function is the same as the performance
       measure (or, perhaps, is the inverse of it). the cost function
       measures the magnitude of error the model         or the parameters
       (because these define the model) for that matter         makes on the
       training set. generally, it is the summation of the differences
       between each prediction on some input and each established output
       on the same input. we can square each of these individual/principle
       errors to achieve an absolute value, a convenient differentiated
       expression, an exaggeration of very poor solutions (so we can skip
       past them quickly), etc. generally, we take the average over the
       entire training set so one anomaly does not skew the results. the
       following is an example of a id75 cost function
       denoted as j(  ):

   [1*pe_u213msy7ibbqbeyynrq.png]
   a cost function for linear
   regression         [76]http://www.bindichen.co.uk/uploads/cost_function.png
     * optimization: now that we have a function that can    score    each set
       of parameters, we want to find the parameters with the    best score   
       or otherwise the lowest cost.

   [1*sw9vxgokaesmpkkifaikaw.png]
   it   s a id76
   problem!         [77]http://www.holehouse.org/mlclass/17_large_scale_machine_
   learning_files/image%20[6].png

   if we were to plot how the overall cost changes with respect to changes
   in each individual weight, you will find that it is a convex function!
   there are no local minimas in the case of id75, but there
   may very well be in neural networks (which, i   ll soon explain, may be
   an issue). our goal, then, is to find that deep blue point which has
   the lowest y-value/cost. to do this, there are two distinct methods:

   a) analytical solution: we can simply take the partial derivative of
   the cost function with respect to each individual weight and set this
   expression to 0. if there are local minimas we need to iterate through
   the solutions of this equation to extract the global minimum. we take
   the tangent to said point and find the normal to it to get the optimum
   value for each parameter.
   [1*qytpzb0geeehupvsdefydq.png]
   set the partial derivative to zero and hence find the    flat   
   points         the global minimum will be one of them

   if we solve this (considering we are in a multi-variable setting) and
   then get the normal, we actually end up with this general equation
   (denoted the    normal equation   ):
   [1*eq7o6mqgnl9v8ypktby4rq.png]
   in this case beta correspods to our
   theta         [78]https://upload.wikimedia.org/math/2/c/e/2ce21b8e24ea7509a32
   95c3acd2ae0ea.png

   ew! is that a matrix inversion    taking the inversion of such a large
   matrix (remember         millions of training examples) requires a colossal
   volume of operations. in specific, the most efficient matrix inversion
   program known runs in o(n^3) time, meaning that as the number and size
   (this one especially) of training examples increases, the time taken to
   execute grows in cubic time.

   b) iterative solution: performing such complex calculations on a large
   scale is simply infeasible. so, we turn to id119. gradient
   descent simply initializes the weights to some random value, takes the
   partial derivative with respect to each weight, and updates each weight
   by a scalar reduction of this value. after doing this hundreds or
   hundreds of thousands of times, the program will    converge    at a flat
   point. let   s just hope it isn   t a local minima!
   [1*6azjq5gfbhzm3vzm_bp0vw.png]
   id119, illustrated
     * generalization: this is perhaps the most fundamental concept in
       machine learning. we ideally want a machine learning program to
       examine the data and create a model that generalizes on said data
       (or, otherwise, find the trend). the better we generalize, the
       better predictions we will make in the future. we may fail to
       generalize on a dataset if we choose to factor in all the noise:

   [1*dd4scmicxhxbnk_wuk2ang.jpeg]
   the model on the right attempts to intersect each individual point, and
   so attains a high accuracy measure, but fails to actually understand
   extrapolate a trend. future predictions will be
   poor.         [79]http://www.experian.com/blogs/marketing-forward/wp-content/
   uploads/2013/05/model_overfitting_image2.jpg

      overfitted    models will attain extremely high accuracy measures and so
   a machine learning model with eg. access to high order polynomials may
   objectively decide to overfit. there are many different ways to prevent
   this, most notably including [80]id173 (where we penalize
   complex models in our cost function) and [81]cross-validation (where we
   simulate real world predictions and evaluate their accuracy).

   almost everything i   ve mentioned here is inherently
   statistical/mathematical. this is the nature of machine learning.
     __________________________________________________________________

   but i   ve lied! i only talked about really one machine learning problem
   type         supervised learning with a regression context. there   s actually
   much more:

   classification vs. regression
     * regression: regression is when you need to predict/output a real
       value based on the training set + model. example: predicting
       housing prices based on the houses    features.
     * classification: classification is when you need to predict/output
       some discrete data based on the training set + model. example:
       predicting whether an email is spam or not (0 or 1         discrete)
       based on its features.

   supervised vs. unsupervised
     * supervised learning: supervised learning is a type of machine
       learning problem where you provide the machine learning system with
       labelled data ie. the features of houses with their respective
       prices, a.k.a. as a training set. the system will learn from this
       data and build a model to make future predictions. examples of
       supervised learning algorithms are classification and regression
       algorithms.
     * unsupervised learning: unsupervised learning is a type of machine
       learning where you provide the machine learning system with
       unlabelled data, and the system has to find inherent structure in
       this data. in addition, there is no feedback on the performance of
       an unsupervised learning algorithm (unlike supervised). examples
       include market segmentation: attempting to    group            this is called
       [82]id91         similar customers in a business    sales records to
       understand the distinct demographics that are consuming a business   
       products. unsupervised learning is also heavily used in computer
       vision algorithms.

   descriptive models vs. predictive models

   so far we   ve mostly looked at predictive models eg. classification and
   regression in a supervised learning setting. there, in addition, exists
   descriptive models. if a model   s output involves the target variable we
   call it a predictive model. if not, we call it a descriptive model.
   most descriptive models exist in an unsupervised setting, and the two
   often share the same connotation. descriptive id91, [83]matrix
   decomposition, and [84]association rule learning (think of expert
   systems!) are all descriptive unsupervised models. predictive
   id91 is a predictive unsupervised model. [85]subgroup discovery
   (think id90!) is a descriptive supervised model.
   [1*df0d6hupftvdxifnt6adwg.png]
   a 2d example of id91. this stuff can go into multiple
   dimensions!         [86]http://scikit-learn.sourceforge.net/0.5/_images/plot_
   mean_shift.png

   sidenote: semi-supervised learning also exists, but i don   t know too
   much about it. perhaps i should fix that!

   geometric models vs. probablistic models vs. logical models
     * geometric models are ones that are constructed directly in instance
       space         a set of all possible or describable instances, including
       and excluding those in our training set         using geometric concepts
       like lines, planes, distances, curves (polynomial functions), etc.
       an advantage of geometric models is that they are easy to
       visualize, but we must also not forget that most industrial
       applications involve four dimensions and beyond (millions of
       dimensions!) where it is simply impossible for us to comprehend. in
       which case, id202 is employed. geometric concepts in
       high-dimensionality are usually denoted with the prefix    hyper-   ,
       take    hyper-plane    for example. in a regression, we want to create
       a hyper-plane that fits the data accurately and can be used to
       predict new real values. in classification, we want to create a
       hyper-plane that separates two classes of the data (like spam or
       not spam) accurately. in id91, we try to find distinct
          clusters    of data points that are separated from other clusters.
       geometric models generally use distance measures as the principle
       for their performance measure / cost function.

   [1*6tlrt8h5x36kak7xwppfrw.png]
   the red dots could indicate spam emails and blue points could indicate
   ham emails. x1 may be the # of typos, and x2 may be the    trust    factor
   of the email sender. now, we can find a hyperplane to separate the two
   and predict the status of a new email in the
   future.         [87]http://strijov.com/sources/img/demodatagen_saved.png
     * probabilistic models are a distinctly different type of model. the
       goal of a probabilistic model is to model the relationship between
       x and y         that is all the features and the output. we can do this
       using statistics, and more specifically by predicting the
       id203 of an event occurring based on historical data. p(y|x)
       means the id203 of y (let   s say y is that the email is spam)
       given the feature data x happened/was observed(they are
       probabilistically dependent). for example, if an email contains
       instances of the words viagra and nigerian price, p(y =
       spam|viagra, nigerian prince) may be very close to 1. this is
       called posterior id203 as it is used after knowledge of the
       data x. remember that the expression p(y = spam|viagra, nigerian
       prince) + p(y= ham|viagra, nigerian prince) will be equal to 1. in
       probabilistic models, we can choose a    decision rule    that    rounds   
       the id203 to some certain state. for example, if the output
       of p(y = spam|x) is greater or equal to 0.5, we will classify it as
       spam. if not, we will classify it as ham. we could modify this
       boundary and, for example, change it to 0.7 to be on the safe side
       and reduce any false positives (which would cause the account owner
       to miss potentially important emails). instead of posterior
       probabilities, we can also look at the likelihood function p(x|y).
       this implies that we are looking for how likely each of the values
       in the feature vector x will be based on some state y. the
       magnitude of each individual value in x will be extremely small
       because of the scope of the features (think, perhaps, every word in
       the dictionary), but we are more interested in the ratios between
       each other. for example, p(x = viagra|y = spam) may be perhaps five
       times as great as p(x = viagra|y = ham) even though both will be
       very close to zero. using this knowledge, we could set a decision
       rule that works with the ratios of the outputs of the likelihood
       function. the likelihood function and posterior probabilities can
       be transformed easily using the famous [88]bayes    rule (make sure
       to read the caption):

   [1*ewy7xica28mkihpkxs_8pa.png]
   in the bayes    rule, p(a) is the [89]prior id203 which you can
   read up more
   about.         [90]http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probab
   ility/bayestheorom1.png

   when we train probabilistic models based on a training set, we
   establish the values for the prior probabilities p(a). if you are
   interested in probabilistic models, take a look at one of the most
   popular algorithms called the [91]naive bayes classifier.

   sidenote: i hope i got this right! i don   t know too much about
   probabilistic models and it   s something i will be coming across in my
   machine learning textbook soon.
     * logical models are models that can be easily translated into
       different logical rules similar to those of id109 or if
       statements in programming. for example:

     if email has word viagra: if email has word blue pill: then email is
     spam

   many different logical rules work together and are layered on top of
   each other to build a complex system that can take entities and
   determine their status based on the different attributes that they are
   composed of. these rules can be organized into a tree data structure.
   [1*hsggjuhzsp4acodbpsxabg.png]
   a logical model can convert some data in geometric space to a series of
   logical rules with high
   accuracy         [92]http://image.slidesharecdn.com/dmbitalk-150712172913-lva
   1-app6892/95/decision-forest-twenty-years-of-research-10-638.jpg?cb=143
   6722446

   as you can see, each    node    in the decision tree branches out into two
   different new nodes. each    level    segments the entity into distinct
   groups (instance space segments) until a complete reduction is made
   such that a status assignment can be made. this is why they are called
      separate-and-conquer    algorithms.

   the most interesting thing about id90 and logical models is
   the order of comparators/attributes in which to separate the models
   (and which ones to separate on         though most likely you will be
   separating on all). remember that a machine generates this         not a
   human. this is why logical models are so fascinating; they are very
   intuitive and easy for us to understand. just as geometric models
   express the reasoning behind their choices through their weights,
   logical models express their choices through their objective rules.
   remember that none of this is just about fitting the data         it   s about
   understanding how the different components in the data can be separated
   and considered to make a final judgement.

   the general idea is to, at each level, separate the instances with the
   highest [93]information gain. information gain is a concept that
   enables us to understand which rules tell us the most about the final
   outcome, and hence maximizing information gain at each point will
   create the largest separation. we want to separate on a high level at
   the top, and on a more minute level closer to the bottom. i find this
   concept fascinating, and you can learn more about it [94]here.

   grouping vs. grading

   there are two final ways to create a distinction between machine
   learning models: grouping and grading. the two differ in how the models
   deal with the instance space.
     * grouping models break up the instance space into a different number
       of segments (either determined by the algorithm or by the user).
       data points will fall into these segments, and the range of these
       data points is the greatest    resolution    or    picture    available to
       the algorithm. the most obvious example is the unsupervised
       learning algorithm of id91.
     * grading models form one global model over the entire instance
       space. although this model is based on the training set (which is
       really the focal point of the model), an infinitely-large
       resolution (like the cartesian instance space) is available to
       them. grading models can distinguish every instance/data point from
       the next, but grouping models will categorize a collection of
       points together. an example is any regression algorithm and linear
       classification algorithms.

   some algorithms have a degree to which they are grouping and grading
   models respectively. the k-nearest neighbors algorithm, for example, is
   mostly grading but also somewhat grouping.
     __________________________________________________________________

   and that   s an intro machine learning in the 21st century! hope you
   enjoyed the quick deviation into mathematics.

deep learning

   i just wanted to add a quick note on deep learning. deep learning is
   undoubtedly the buzzword of 21st century artificial intelligence. if
   machine learning is primarily advancing industrial application, deep
   learning is a spin-off that is focused on advancing the field of
   artificial intelligence (just like the old days). in reality, and as
   mentioned a couple times prior, deep learning is just very complex and
   deep id158s/convolutional neural networks that have
   been engineered for maximum efficiency and accuracy using new
   techniques discovered by researchers in the field. it really just
   needed a new identity. keep updated on progress in deep learning
   [95]here.

iii. the future

   [1*p2s2_8npaonydc22ym3t-g.jpeg]
   could a singularity, the result of an intelligence explosion, be right
   around the
   corner?         [96]http://www.neurocodesg.com/assets/istock_000049397204_lar
   ge.jpg

   i think it   s fair to say that    time/   progress for time=2016 is an
   exceptionally large value. but what   s next? self-driving cars, fully
   automated manufacturing, personal assistants   ? definitely. but
   ultimately: superintelligence.
   [1*yj7opnuun-_6punonlxdpq.png]
   superintelligence is any intelligence that surpasses human
   intelligence         [97]http://3.bp.blogspot.com/-ucxfqa-d41k/vbmhqxngqai/aa
   aaaaaacos/wjb3pue5gtq/s1600/how%2bfar%2bto%2bsuperintelligence.png

   elon musk, sam altman, bill gates, and many more professionals predict
   that superintelligence will be achieved by 2075. that is, they believe
   that human-surpassing intelligence will exist in 30 years minimum and
   80 years maximum. and they aren   t optimistic about it either; these
   industry leaders all believe superintelligence will be an existential
   risk to us.

   but did marvin minsky and the pioneers of the golden years not suggest
   the same thing? by their measures, we would have had human-level ai by
   1978, and superintelligence surely since then. but we are not even
   close to achieving the former, let alone the fact we are nearly 40
   years behind their predictions.

   shortly after their optimistic conjectures, artificial intelligence
   went into a bust; an ai winter. and history repeated itself twice.
   sure, the tangent to our current point has a steep gradient, but who   s
   to say an inflection point isn   t upcoming? alternatively, we could hit
   a shoulder or a flat local maximum. either way, we must stay cautious.

   with that being said, it   s generally recognized at this point that such
   feat of achievement will not be accomplished through an abrupt
      breakthrough    but rather decades of layered tedious application of
   engineering skill. with that being said, many skeptics like andrew ng
   (who is one of the leaders of modern machine learning) claim that the
   current algorithms employed are simply not the correct direction for
   ultimate machine intelligence. and, hence, they don   t pose a danger.

   but if one thing is clear, it   s that we are moving forward. we are
   innovating at a pace like never before, and our rate of innovation is
   exponentially increasing. perhaps we should be concerned about
   superintelligence because, whether it will be achieved in the immediate
   future or not, its development is inevitable.

   thus, it is important to understand: how superintelligence may be
   achieved, the different forms of superintelligence that could arise,
   the dangers they pose, and how an intelligence explosion may occur.

paths to superintelligence

     * artificial intelligence: this is the most probable path to
       superintelligence. eventually, we may develop software that can
       harness powerful algorithms (potentially born out of deep learning
       and fields like nlp, id161, etc.), great computational
       power, access to an abundant volume of information and data,       
       access to the internet, and the ability to access an    internal
       model of reality   . this model of reality would allow it to make
       predictions on the outcome of physical, real world actions. with
       these properties, superintelligence would be able to interact with
       infrastructure around us and easily overcome any imposed sandbox
       eg. through self-improvement (potentially via id107 or
       nlp).
     * biological emulation: this, in my opinion, is very similar to
       artificial intelligence. brain emulation is when intelligent
       software is created by emulating the biological brain. it will do
       so by scanning the brain and its inner contents / working,
       attempting to craft a digital implementation that simulates its
       function. many biologists and chemists have developed theoretical
       solutions using detailed inspection, dissection, and image
       processing to achieve this (i suggest you read nick bostrom   s
       [98]superintelligence book to learn more). the only limitation here
       is that the intelligence may be limited to the human subject and
       perhaps not surpass into superintelligence         especially since the
       human sandbox of the physical universe is a stark contrast from a
       digital computing, virtual sandbox. and so it   s difficult to see
       how an intelligence explosion could occur. the technical
       limitations, obviously, are on the scanning, translation, and
       simulation technologies. is brain emulation plausible? perhaps,
       since we already have reasonable computational models of neural
       processes.
     * biological enhancement: this is another plausible route, and it
       strays surprisingly from the traditional route of artificial
       intelligence. right now adults are able to select select (not a
       typo) physical appearance features for their offspring. the process
       is called    genetic engineering   , and it isn   t just prominent in
       crops! if we were to achieve genetic selection on intelligence /
       brain computational ability, we could produce offspring that have
       impeccably high iqs. current adults could also consume intelligence
       enhancement drugs. if we were to iteratively perform this through
       multiple generations, then it would be possible to create a group
       of superintelligent humans. it is most likely that these humans
       would at first reside in their own society rather than mix into the
       general population. over an elongated period of time, however, the
       distinction between the twowould blur and natural selection would
       cause a convergence into superintelligence. it is important to note
       that this route relies on not only technological advancement but
       countless years before taking effect. not to mention that
       regulation by governments and religious groups may prevent it from
       ever occurring.
     * biological interfacing: instead of emulating a brain, could a
       computer connect to a brain and work hand-in-hand with it? this is
       known as brain-computer interfacing. b.c.i. could equip humans with
       the benefits of computers such as accurate & rapid arithmetic
       calculation and reliable memory & data storage. there are, however,
       major health concerns regarding such technology         most notably the
       risk of infections or brain damage. akin to emulation, it is
       difficult to see how an intelligence explosion could occur from
       this, especially since these digital    brains    cannot be rapidly
       replicated. they require a connection to a physical human and only
       exist while said connection is established; they cannot be
       offloaded. the clear advantage, however, is that it would enable
       high bandwidth data transmission between online information and the
       brain. the subject may be able to directly    download    petabytes of
       data from the internet into their mind, which could perhaps produce
       a form of superintelligence (known as collective
       superintelligence). however, we don   t know enough about the brain
       on a biological/psychological level to suggest with certainty that
       this immediate download-and-access feature would actually be
       feasible.

   [1*bq8jgcrec3fmgv5faz0mug.jpeg]
   a digital
   brain         [99]https://realizethelies.files.wordpress.com/2013/03/ai-brain
   .jpg
     * networks: we   ll talk more about this in the next section, but it
       may be argued that a network of ultra-intelligent minds/agents and
       a plethora of information can mesh into one superintelligent
       entity. it   s difficult, however, to intuitively see how a
       collection of data could become conscious. nick bostrom conveys
       this well by saying:

     but what of the seemingly more fanciful idea that the internet might
     one day    wake up   ? could the internet become something more than
     just the backbone of a loosely integrated collective
     superintelligence         something more like a virtual skull housing an
     emerging unified super-intellect?

   the idea that the internet will spontaneously become conscious is
   absurd. it is more likely that, through decades of engineering, the
   internet may represent a form of web intelligence through features like
   autonomous bots, powerful search algorithms and data structures, etc.

forms of superintelligence

   we can establish forms of superintelligence into three distinct groups.
     * speed superintelligence is equivalent to weak superintelligence in
       that it is intellect that is equally smart as human minds, but
       operates multiple orders of magnitude faster. biological
       interfacing and biological emulation are suitable examples.
     * collective superintelligence is a collection or aggregate of
       highly-intelligent minds/agents and information in multiple general
       domains that mesh together to form one superintelligent entity.
       networks/organizations is an example (biological interfacing may be
       too).
     * quality superintelligence is equivalent to strong superintelligence
       in that it is intellect that is at least as fast at humans in
       processing speed but orders of magnitude smarter. how we define
          smarter    is both unclear and more qualitative rather than
       quantitative (unless psychologists crack this problem in the near
       future). just as we are smarter than chimpanzees, superintelligence
       is smarter than us in the same way. artificial intelligence and
       biological enhancement are prime examples.

   [1*u2rner9s4uwdhfozlnsb-a.jpeg]
   us in the light of strong
   superintelligence         [100]http://www.awf.org/sites/default/files/media/g
   allery/wildlife/chimpanzee/chimp_craig_r_sholley.jpg?itok=4qffxy3f

intelligence explosion & the singularity

   if we were to create superintelligent agents that we would, by
   definition, be creating intellect that surpasses ours. i made a diagram
   to demonstrate the implications of this:
   [1*k-3ulypherwwrupsi6tn7w.gif]
   human intelligence is a subset of superintelligence

   if our intelligence is a subset of superintelligence, then, logically,
   superintelligence knows how to create other intelligent agents. they
   can self-improve, modify their own source code, and write new code.
   each superintelligent agent could spawn new intelligent agents.
   infrastructure would not be a problem; if we shut servers down, then,
   logically, superintelligence would know how to turn them (and use their
   greater intellect to breach in). considering superintelligence had the
   objective to do so, superintelligence could cause an    intelligence
   explosion   . sci-fi author vernor vinge first referred to this as what
   many of you may have heard of: [101]the singularity.

   with superintelligence dominating our civilization (again, with any
   moral intent or not), we have a duty to react. assuming continuous
   increment in intelligence, there will exist an arbitrary point in time
   t where machine intelligence is not superintelligent, but extremely
   close. there will also exist a point in time t +    where machine
   intelligence is superintelligent. (t +   ) - t or    is denoted the
      takeoff duration   .
     * if the takeoff duration is long (ie multiple decades or centuries),
       there is opportunity for the government, lobbyists, organizations,
       corporations, and the general public to intervene before
       superintelligence is achieved. technology to secure our
       infrastructure or combat the intelligence will also be developed.
     * if the takeoff duration is moderate (ie. months or years), humans
       may be able to respond but not analyze the situation in depth
       enough to deploy new safeguard systems (but existing ones may be
       iterated upon).
     * if the takeoff duration, at worst case scenario, is fast (ie.
       minutes, hours, or days), there will be little to no opportunity
       for us to react and we must rely on any prior preparation.

the danger with superintelligence

   [1*wbnwalkhjswpnvqh9z3yrw.jpeg]
   evil robots    a
   possibility?         [102]http://vignette2.wikia.nocookie.net/thepookieprotes
   t/images/4/41/wallpaper-terminator-robot-t-800-photo.jpg/revision/lates
   t?cb=20150317091943

   why would superintelligence pose any danger to mankind? majority of
   people think that superintelligent robots may    turn evil    and embark on
   a relentless, ruthless quest to annihlate mankind. could industry
   leaders really be advocating (or against) this idea?

   no. they aren   t. a terminator-style fate on humanity is just a work of
   fiction. elon musk, bill gates, and sam altman all have legitimate
   fears about superintelligence. so         what   s the reasoning? the answer is
   emotional intelligence, or the lack thereof. the reality is that
   machines have no morality; they are amoral. the current direction of
   the field (right now and for the foreseeable future) points no
   indication of the development of artificial emotional intelligence.
   artificial intelligence is not extremist    good    (a hero), artificial
   intelligence is not extremist    bad    (a villian), artificial
   intelligence is extremist amoral. and not only is ai amoral, it is
   purely mathematical and logical.

   let   s remember that when artificial intelligence is given a task, it
   will try to optimize (to the greatest extent) based on a fitness/cost
   function. with access to vasts amount of information / collective
   intelligent, impeccable processor speed, and superintelligent
   algorithms, artificial intelligence can find unpredictable yet optimal
   (sometimes remarkable, sometimes downright scary) solutions to major
   problems    ruthlessly. let   s play a thought experiment, adapted from a
   youtube video i watched a while back:

   iframe:
   [103]/media/27efb3bdf10369d14871349e413bf3fd?postid=ae804fa86cca

    1. we give a superintelligent ai a task: take my credit card and find
       the cheapest set of stamps online.
    2. at this point, the superintelligence has access to an internal
       model of reality (for processing physical/real-world outcomes), the
       internet, and hence ultimately infrastructure.
    3. the ai tries to optimize: it finds a cheap set of stamps online.
       but its current fitness is still relatively large.
    4. the ai optimizes further: it tests a collection of actions with its
       internal model of reality and executes them. the ai messages an
       ebay seller and convinces him to give up the stamps for free after
       threatening the seller using secrets found after an extensive
       online data analysis & search.
    5. after millions of iterations, the ai has found the most optimum
       solution: breach the national money printing operation and print
       millions in liquid cash to a) buy the stamps b) also gain a profit
       on top of that (essentially purchasing for a negative value!).

   the ai, in this case, is acting immorally and could perhaps even cause
   major negative economic implications. but it does not know. it is
   acting amorally and finds the most optimum solution which just so
   happens, in our hierarchical and (in absolute terms) restrictive
   society, to be immoral. with vast processing speed it works ruthlessly.
   from the outside it appears to be a psychopath!

   it gets worse; give ai the task of solving global warming and it may
   just wipe out half of the human race. remember, superintelligence has
   no sandbox         our intelligence is a subset of its intelligence, and it
   can self-improve or even cause an intelligence explosion (create many
   superintelligent agents to execute humans). and with access to
   infrastructure it could implement a quick, tidy, and seaid113ss way to
   kill off the population ie. poison all water supplies.

   so you may say: implement an ethics standard into the fitness function.
   there are a few issues. firstly, if superintelligence was achieved, an
   ethics engine is only a module on top of the technology. at first the
   new discoveries may be closed source but, eventually, since innovation
   moves at an exponential rate, academic researchers will succeed and
   make the findings open source. secrets may also fail to be kept, and
   that is not to mention that superintelligence could leak itself
   (deliberately or accidentally) from government programs. if malicious
   people had access to these programs, the game is over.

   but there is an even greater problem: ethics is perhaps the biggest
   unresolved issue in philosophy. which ethical standard do we choose?
   utilitarianism ethics? kantian ethics? consequential ethics?
   [1*ipku_c3a-iaf16nkn-gzsw.jpeg]
   the railroad track thought experiment. one of the most famous thought
   experiments in
   philosophy.         [104]http://www.allthetests.com/quiz31/picture/pic_141640
   9939_5.jpg

   if five people are tied to an active train track and one person is tied
   to an inactive track, do we choose to kill the one person instead of
   the five? utilitarian ethics, for example, would state that the
   implications of one death < the implications of five deaths.

   would you agree?

   what about telling ai: do not hurt any people. but in a physically
   determined universe, changing the switch on one button may indirectly
   cause the death of a person. free will comes into question here!
   [1*oeqlvr5_717ty00tofxfpw.jpeg]
   determinism: the idea that one physical action causes the next and
   nobody can predict the future with
   certainty         [105]http://www.lse.ac.uk/philosophy/wp-content/uploads/201
   5/01/domino_cascade.jpg

   and if we told an ai: do not kill a human directly, it may just perform
   some action that it predicts will kill a human indirectly, exploiting
   the power of its internal model of reality.

   sam altman puts it best when he says:

     unfortunately for us, one thing i learned when i was a student in
     the stanford ai lab is that programs often achieve their fitness
     function in unpredicted ways.

   unfortunately for us indeed.

iv. thank you!

   with this colossal article out of the way, i   m excited to now learn
   about the mathematical intricacies behind select algorithms. look out
   for my articles on id26 and the normal equations in the
   coming days and weeks.

     * [106]machine learning
     * [107]artificial intelligence
     * [108]philosophical

   (button)
   (button)
   (button) 27 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [109]go to the profile of rohan kapur

[110]rohan kapur

   rohankapur.com

     (button) follow
   [111]a year of artificial intelligence

[112]a year of artificial intelligence

   our ongoing effort to make the mathematics, science, linguistics, and
   philosophy of artificial intelligence fun and simple.

     * (button)
       (button) 27
     * (button)
     *
     *

   [113]a year of artificial intelligence
   never miss a story from a year of artificial intelligence, when you
   sign up for medium. [114]learn more
   never miss a story from a year of artificial intelligence
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://ayearofai.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ae804fa86cca
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-2-time-progress-ae804fa86cca&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-2-time-progress-ae804fa86cca&source=--------------------------nav_reg&operation=register
   8. https://ayearofai.com/?source=logo-lo_cyrakgasgbvs---bb87da25612c
   9. https://ayearofai.com/tagged/algorithms
  10. https://ayearofai.com/tagged/today-i-learned
  11. https://ayearofai.com/tagged/case-studies
  12. https://ayearofai.com/tagged/philosophical
  13. https://ayearofai.com/tagged/meta
  14. http://www.awf.org/sites/default/files/media/gallery/wildlife/chimpanzee/chimp_craig_r_sholley.jpg?itok=4qffxy3f
  15. https://ayearofai.com/@mckapur?source=post_header_lockup
  16. https://ayearofai.com/@mckapur
  17. https://medium.com/a-year-of-artificial-intelligence
  18. https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5
  19. https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=utf-8#q=define+acronym
  20. https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5
  21. http://www.amazon.com/superintelligence-dangers-strategies-nick-bostrom/dp/1501227742
  22. http://www.cambridge.org/us/academic/subjects/computer-science/pattern-recognition-and-machine-learning/machine-learning-art-and-science-algorithms-make-sense-data
  23. https://upload.wikimedia.org/wikipedia/commons/2/2f/al-jazari_robots.jpg
  24. https://en.wikipedia.org/wiki/stochastic_neural_analog_reinforcement_calculator
  25. https://upload.wikimedia.org/wikipedia/commons/4/4e/eniac.jpg
  26. http://googleresearch.blogspot.sg/2016/01/alphago-mastering-ancient-game-of-go.html
  27. https://camo.githubusercontent.com/f7b639ce52737d636af7e13b9bd418d178443837/68747470733a2f2f7261772e6769746875622e636f6d2f616c69717569732f6b74682d616931342d636865636b6572732d76697375616c697a65722f6d61737465722f646f632f73637265656e73686f742e706e67
  28. https://en.wikipedia.org/wiki/dartmouth_conferences
  29. https://en.wikipedia.org/wiki/hanover,_new_hampshire
  30. https://en.wikipedia.org/wiki/logic_theorist
  31. https://en.wikipedia.org/wiki/principia_mathematica
  32. http://www.dartmouth.edu/~vox/0607/0724/images/ai50.jpg
  33. http://i.telegraph.co.uk/multimedia/archive/02423/bull_2423808b.jpg
  34. https://en.wikipedia.org/wiki/general_problem_solver
  35. https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=utf-8#q=eliza program
  36. https://en.wikipedia.org/wiki/semantic_network
  37. https://en.wikipedia.org/wiki/shrdlu
  38. http://hci.stanford.edu/winograd/shrdlu/shrdlu-original.gif
  39. http://www.abdn.ac.uk/ncs/departments/computing-science/standup-315.php
  40. https://en.wikipedia.org/wiki/darpa
  41. http://cdn1.theodysseyonline.com/files/2015/12/04/635848557150633136-120303261_winter.jpg
  42. http://nickbostrom.com/
  43. https://en.wikipedia.org/wiki/combinatorial_explosion
  44. http://static1.squarespace.com/static/552e59bce4b0a0d7241f9c43/t/567f4a461c1210eb4f65d612/1451182688597/
  45. https://en.wikipedia.org/wiki/speech_recognition
  46. http://philosophyisawesome.weebly.com/uploads/2/5/0/2/25029532/3222576_orig.jpg
  47. https://en.wikipedia.org/wiki/g  del's_incompleteness_theorem
  48. https://en.wikipedia.org/wiki/formal_system
  49. http://www.toptenpack.com/wp-content/uploads/2014/12/seasons-wallpapers-for-walls.jpg
  50. https://en.wikipedia.org/wiki/fifth_generation_computer
  51. http://www.igcseict.info/theory/7_2/expert/files/stacks_image_5738.png
  52. https://en.wikipedia.org/wiki/hopfield_network
  53. https://en.wikipedia.org/wiki/id26
  54. https://upload.wikimedia.org/wikipedia/commons/9/95/hopfield-net.png
  55. https://en.wikipedia.org/wiki/connectionism
  56. https://en.wikipedia.org/wiki/deep_learning
  57. https://en.wikipedia.org/wiki/nouvelle_ai
  58. https://en.wikipedia.org/wiki/symbolic_artificial_intelligence
  59. http://www.rsipvision.com/wp-content/uploads/2015/04/slide5.png
  60. http://www.extremetech.com/wp-content/uploads/2015/04/mooreslaw2.png
  61. https://en.wikipedia.org/wiki/moore's_law
  62. https://en.wikipedia.org/wiki/heuristic_(computer_science)
  63. https://www.mpi-inf.mpg.de/fileadmin/_processed_/csm_must-links_4f7afcb337.jpg
  64. https://en.wikipedia.org/wiki/deep_blue_(chess_computer)
  65. http://www.ibm.com/smarterplanet/us/en/ibmwatson/
  66. http://arxiv.org/pdf/1412.3409.pdf
  67. https://medium.com/@lennykhazan
  68. http://cs231n.stanford.edu/
  69. https://i.vimeocdn.com/video/553713383.jpg?mw=1920&mh=1080&q=70
  70. https://en.wikipedia.org/wiki/reinforcement_learning
  71. https://ayearofai.com/media/0e6a656a54495cd19b42110a86bd193b?postid=ae804fa86cca
  72. https://www.tensorflow.org/
  73. https://www.tensorflow.org/versions/0.6.0/get_started/index.html
  74. http://assets.toptal.io/uploads/blog/image/443/toptal-blog-image-1407508081138.png
  75. https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/linear_regression.svg/400px-linear_regression.svg.png
  76. http://www.bindichen.co.uk/uploads/cost_function.png
  77. http://www.holehouse.org/mlclass/17_large_scale_machine_learning_files/image [6].png
  78. https://upload.wikimedia.org/math/2/c/e/2ce21b8e24ea7509a3295c3acd2ae0ea.png
  79. http://www.experian.com/blogs/marketing-forward/wp-content/uploads/2013/05/model_overfitting_image2.jpg
  80. https://en.wikipedia.org/wiki/id173_(mathematics)
  81. https://en.wikipedia.org/wiki/cross-validation_(statistics)
  82. https://en.wikipedia.org/wiki/cluster_analysis
  83. https://en.wikipedia.org/wiki/matrix_decomposition
  84. https://en.wikipedia.org/wiki/association_rule_learning
  85. http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume17/gamberger02a-html/node4.html
  86. http://scikit-learn.sourceforge.net/0.5/_images/plot_mean_shift.png
  87. http://strijov.com/sources/img/demodatagen_saved.png
  88. https://en.wikipedia.org/wiki/bayes'_rule
  89. https://en.wikipedia.org/wiki/prior_id203
  90. http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_id203/bayestheorom1.png
  91. https://en.wikipedia.org/wiki/naive_bayes_classifier
  92. http://image.slidesharecdn.com/dmbitalk-150712172913-lva1-app6892/95/decision-forest-twenty-years-of-research-10-638.jpg?cb=1436722446
  93. https://en.wikipedia.org/wiki/information_gain_in_decision_trees
  94. http://www.saedsayad.com/decision_tree.htm
  95. http://deeplearning.net/
  96. http://www.neurocodesg.com/assets/istock_000049397204_large.jpg
  97. http://3.bp.blogspot.com/-ucxfqa-d41k/vbmhqxngqai/aaaaaaaacos/wjb3pue5gtq/s1600/how+far+to+superintelligence.png
  98. https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=utf-8#q=superintelligence+book
  99. https://realizethelies.files.wordpress.com/2013/03/ai-brain.jpg
 100. http://www.awf.org/sites/default/files/media/gallery/wildlife/chimpanzee/chimp_craig_r_sholley.jpg?itok=4qffxy3f
 101. https://en.wikipedia.org/wiki/technological_singularity
 102. http://vignette2.wikia.nocookie.net/thepookieprotest/images/4/41/wallpaper-terminator-robot-t-800-photo.jpg/revision/latest?cb=20150317091943
 103. https://ayearofai.com/media/27efb3bdf10369d14871349e413bf3fd?postid=ae804fa86cca
 104. http://www.allthetests.com/quiz31/picture/pic_1416409939_5.jpg
 105. http://www.lse.ac.uk/philosophy/wp-content/uploads/2015/01/domino_cascade.jpg
 106. https://ayearofai.com/tagged/machine-learning?source=post
 107. https://ayearofai.com/tagged/artificial-intelligence?source=post
 108. https://ayearofai.com/tagged/philosophical?source=post
 109. https://ayearofai.com/@mckapur?source=footer_card
 110. https://ayearofai.com/@mckapur
 111. https://ayearofai.com/?source=footer_card
 112. https://ayearofai.com/?source=footer_card
 113. https://ayearofai.com/
 114. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
 116. https://medium.com/p/ae804fa86cca/share/twitter
 117. https://medium.com/p/ae804fa86cca/share/facebook
 118. https://medium.com/p/ae804fa86cca/share/twitter
 119. https://medium.com/p/ae804fa86cca/share/facebook
