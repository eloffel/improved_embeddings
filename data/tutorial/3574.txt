   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]a year of artificial intelligence
     * [9]algorithms
     * [10]today i learned
     * [11]case studies
     * [12]philosophical
     * [13]meta
     __________________________________________________________________

   [1*mffqfqkp7uhb7pz-xruasw.png]
   [14]https://qph.ec.quoracdn.net/main-qimg-99f7ea7d0929ed41dbecf67ec51b8
   0b3?convert_to_webp=true

rohan #3: deriving the normal equation using matrix calculus

understanding the analytical solution to id75.

   [15]go to the profile of rohan kapur
   [16]rohan kapur (button) blockedunblock (button) followfollowing
   feb 16, 2016
     __________________________________________________________________

     this is the third entry in my [17]journey to extend my knowledge of
     artificial intelligence in the year of 2016. learn more about my
     motives in this [18]introduction post.
     __________________________________________________________________

   before i start exploring new algorithms/practices in machine learning &
   co., i want to first refine my current knowledge. part of this involves
   plugging gaps in content i either failed to understand or research
   during my prior endeavors in artificial intelligence thus far. today, i
   will be addressing the [19]normal equation in a regression context. i
   am not going to explore machine learning in depth (but you can learn
   more about it in my previous two posts) so this article assumes basic
   comprehension of supervised machine learning regression algorithms. it
   also assumes some background to matrix calculus, but an intuition of
   both calculus and id202 separately will suffice.

   today, we try to derive and understand this identity/equation:
   [1*xvx1ub1opu2_y9wfv12iuw.png]
   look   s daunting? keep reading!

   recall that a supervised machine learning regression algorithm examines
   a set of data points in a cartesian instance space and tries to define
   a model that    fits    said data set with high accuracy. in this context,
   a model is any linear or non-linear function.
   [1*aj9lkmdxxgbnxp98uwywdw.png]
   data points in red, residuals in gray, model in
   blue         [20]http://www.pieceofshijiabian.com/wp-content/uploads/2014/01/
   screen-shot-2014-01-18-at-5.50.07-pm.png

   as you can see, the blue line captures the trend (that is, how the data
   points move across across the instance space) in this two dimensional,
   noisy example. the term    residuals   , primarily used in statistics and
   rarely in machine learning, may be new for many of you. residuals, the
   vertical lines in gray, are the distances between each of the
   y-coordinates of the data points and their respective predictions on
   the model. at tv = 0, our model predicted a value of sales = 6.5 even
   though the actual value was sales = 1 (i   m eye-balling this!). hence,
   the residual for this instance is 5.5.

   we would intuitively suggest that a smaller residual is preferred over
   a greater residual, however a model may not (and again, we may not want
   it to         overfitting) be able to accommodate a change in its output for
   one data point and keep its output constant for all other data points.
   so, instead, we say that we want to calculate the minimum average
   residual. that is, we want to minimize the value of the summation of
   each data point   s residual divided by the number of data points. we can
   denote this as the    cost function    for our model, and the output of
   said function as the    cost   .

   the final step is to evaluate how we use a residual in the cost
   function (the principle unit of the cost function). simply summing over
   the residuals (which are mere differences/subtractions) is actually not
   suitable because it may lead to negative values and fails to capture
   the idea of cartesian    distance    (which would cause our final cost to
   be inaccurate). a valid solution would be wrapping an absolute value
   over our subtraction expression. another approach is to square the
   expression. the latter is generally preferred because of its
   mathematical convenience when performing id128.

   ok. let   s approach this mathematically. first, we will define our model
   as:
   [1*m0jgjwdomfbikzix41syjw.png]

   this is pretty much y = mx + c but in a high-dimensional space such
   that x is vector of values. the tv sales scatter plot diagram above is
   two-dimensional and hence has a single x: tv, however this need not
   always be the case.

   notice that this is the    dot-product    between the vectors    and x. we
   can rewrite this using the conveniences of id202 notation:
   [1*zw9tpqy612tg44t1bebypa.png]
   the    transpose    operation (which looks like a value raised to the power
   of    t   ) switches the rows and columns of any matrix. now, we can get a
   legal multiplication between vectors    and x.

      stores the real coefficients (or    weights   ) of the input features x
   and is hence of the exact same dimensionality as x. it is important to
   note that we prefix the vector x with 1 so that we can add support for
   a constant term in our model.

   now, we can expect to plug in any value x for tv and expect to get an
   accurate projection for the number of sales we plan to make. the only
   issue is, well, we have to figure out the value(s) of    that form an
   accurate model. as humans, we may be able to guess a few values but a)
   this will not apply in higher-dimensional applications b) a computer
   cannot do    guess-work    c) we want to find the most accurate   .

   x represents any single data point we enter to receive a projection. we
   can now define x as a vector of all the inputs for the the a-priori
   data points that we use to shape our model. note that each data point
   is a vector in itself, so x is a large matrix (denoted the    design
   matrix   ) whereby each row (we use the notation x superscript arbitrary
   index i) is one vector x. y is, conversely, the outputs for these
   a-priori data points. we define m as the number of rows in x, or
   otherwise the number of a-priori data points. in machine learning this
   is called a    training set   .

   i hope it is clear that, here,    essentially defines the model.
   everything else stays constant. so, to find the most accurate model, we
   need to find the    with the lowest cost (lowest average residual         on
   top of a function         as stated earlier). let   s put our cost function in
   mathematical notation:
   [1*k2baqgyi8p60363i-ckkww.png]

   the residual is simply the difference between the actual value and the
   predicted value (the input values fed through the model):
   [1*izuxi0rmeawl0bljdnm-8g.png]

   earlier we discussed different ways we could interact with the residual
   to create a principle cost unit. squared difference is the preferred
   one:
   [1*dld-fvofr-zyfn6krgmy4g.png]
   we square the residuals because of the mathematical convenience when we
   use id128

   inserting this expression in place with the cost function leaves us
   with:
   [1*wpcweytanegdrl-wp2pjsg.png]

   notice how we   ve multiplied the overall expression by a half. we do
   this, as per usual, for mathematical convenience.

   unfortunately, before we can proceed, we will need to represent the
   cost function in vectorized (using id202) format. instead of
   explicitly summing over the square residuals, we can calculate the
   square residuals with matrix operations.

   if we were to collect all the residuals in one vector, it would look
   somewhat like this:
   [1*qqryk1jct-0rqtxq6b-kfa.png]

   we can further split this up:
   [1*ddssfj509r1arqkiekzwnw.png]

   next:
   [1*x-gg9583upgebnrzjpo_mw.png]

   now let   s simplify the left vector:
   [1*94fnfj3nray3xb8i9gnx4g.png]

   and then:
   [1*p85tukp6riftvpn7yfjuhq.png]

   and bam, we can clearly see that this simplifies to:
   [1*rfe3j0oe78serfo-kpl4pa.png]

   but we   re forgetting something important; each residual is squared.
   this poses a problem because we cannot simply square the overall
   expression x       y. why? put simply, the square of a vector/matrix is
   not equal to the square of each of its inner values. so         what do we
   actually want to do? we want to take each value in the vector x       y
   and multiply it by itself. this is what we can formulate:
   [1*n9s03lonidrnkt50z0ngka.png]

   if we want to get each value in x       y to multiply by itself, we can
   use a second x       y and have each value in the first vector multiply by
   the value at the same index in the second vector. this is a
   dot-product. recall that when we formulated our hypothesis/model
   function, we took the dot-product of two vectors as the multiplication
   between one of the vectors transposed and the other vector. hence, to
   achieve square residuals, we end up with the following expression:
   [1*sucm-juvq3yuzhc_godpsw.png]

   by adding the division term to the front of the expression, we should
   have the final id202/vectorized expression for our cost
   function:
   [1*o0-nyktpobppxpxrjyeehw.png]

   so, now that we   re able to    score      , the end goal is simple:
   [1*yh9zuvb-b_xhcyn0kiapxa.png]

   in other words, find the    that minimizes on the output of the cost
   function and hence represents the most accurate model with lowest
   average residual.

   so, how shall we approach this? i hope it will become clear through
   this graph:
   [1*m5rql_yxe6geznod3atj5g.png]
   it   s convex!

   that   s right         in id75 one can prove that cost(  ) is a
   convex function! so, at this point, it seems pretty simple: apply
   id128 to solve for the least cost. let   s get started!

   we are going to differentiate with respect to, as expected,   . at this
   point, we can drop the division term because when we equate the
   derivative to zero it will nulled out.
   [1*slwxkifcwfjyarrbryvnqg.png]

   we are yet again going to be simplifying the cost function (rhs), this
   time using matrix transpose distribution identity specifically (ab)       
   b   a   :

   edit: i make a mistake here and do (ab)    = a   b   , so i end up having to
   rearrange (which would otherwise be illegal due to non-commutative
   nature of id127). this does not matter with respect to
   determining the final derivative.
   [1*6_aasrdrzrcnvrjqwwuyya.png]

   at this point, we can expand the two brackets:
   [1*u4o-rzt2xinkyg6bn4fkaw.png]

   since vectors y and x   are of the same dimensionality and are both
   vectors, they (or rather the transpose of one with the other) satisfy
   the commutative property (which states that a * b = b * a         if you
   weren   t aware, this property is not the case for higher-dimensional
   matrices). hence, the two middle terms can be collected into one term:
   [1*fwa5vrotl8k97upn_gngsw.png]

   let   s try to remove all the instances of transpose (again, using the
   distribution identities):
   [1*myjxozlekqf2czoo-on_sg.png]

   we can remove the last term because, when deriving with respect to   ,
   it has no effect and the derivative is zero (since the term    is not
   involved):
   [1*clzgs5-itecas4vnifl9jw.png]
   [1*idjw4h_p9mjizwk19dky_w.png]

   let us evaluate the left term first:
   [1*t7b4znifilst-kb_153nrg.png]

   note that the x terms are simply constants/scalars in this partial
   derivative, so we can    take it out    (using differentiation rules) like
   so:
   [1*zciazsu6rp3rkeh6xl0igw.png]

   using the following matrix calculus scalar-by-vector identity:
   [1*9u3lnuwlfkmwgiaoaw3kyg.png]

   edit: i want to add intuition for the rule here. recall that a vector
   multiplied by its transpose results in a vector where each value is
   squared         but this is not the same as the square of the vector itself.
   if we were to take the derivative of this vector with respect to the
   original vector, we apply the basic principle which states that we
   derive by each value in the vector separately:
   [1*zggv7tq5o4xqdbdifs64og.png]

   that simplifies to:
   [1*pxdssrns2f614zpuax53qa.png]
   [1*eyierjysludvxnialqwid35.png]
   [1*whfve65ljl_4lknmqob7gg.png]
   [1*gowf_9sogkl38rzgfn658w.png]

   so now, using this identity, we can solve the derivative of the left
   term to be:
   [1*kci9v1lusmwww3xzfuiygg.png]

   let   s move on. the right term is much simpler to solve:
   [1*jsjaffbfcytqg1ismuqfcg.png]
   [1*c88msauuc8se_aqpmdtqra.png]
   taking the scalars out

   using the follow matrix calculus conjugate transpose rule:
   [1*1w1et8eyn-mh4ig6lgrstg.png]

   edit: i hope the intuition behind this identity becomes clear using the
   same principles in the previous explanation.

   we can see that:
   [1*gw1hpdjayz_9nj_vo14h0w.png]
   [1*ta0hlo7dynv15u3e_onwua.png]

   now we can combine the two terms together to formulate the final
   derivative for the cost function:
   [1*av-e20nk1v9827o2zy4vvg.png]
   again         this excludes the 1/2m division term

   recall that the only turning point         a point where the instantaneous
   gradient is zero         for a convex function will be the global
   minimum/maximum of the function. so, we set the derivative to zero and
   hence find said minimum point:
   [1*58woeeu_dvbsuimgrcm7sw.png]
   by setting the derivative of the cost function to zero, we are finding
   the    that attains the lowest cost

   now, we sub in our computed derivative:
   [1*simsrdbpik7uzfrknkiudw.png]

   with some basic algebra, we   re on our way!:
   [1*n61zb0z-b1-rd04dw2ycrq.png]
   [1*gc3py9eztiyjvcznb4dlaa.png]

   slow down, ! since matrices are not commutative, we can   t isolate    by
   simply    bringing down    the x terms. instead, we multiply both sides by
   the inverse of said terms (remember that division is the same as
   multiplication of the inverse):
   [1*zb0snwrc78uobifsrnkcwq.png]

   the inverse of any value multiplied by the value itself is just 1, so   :
   [1*xvx1ub1opu2_y9wfv12iuw.png]
   we got it!

   here we have it         finally! this is our solution for    where the cost is
   at its global minimum.
     __________________________________________________________________

   elephant in the room         why isn   t this preferred over id119   
   what an elegant solution this is, you say. well, here   s the main reason
   why: computing the seemingly harid113ss inverse of that (m * n) by (n *
   m) matrix is, with today   s most efficient computer science algorithm,
   of cubic time complexity! this means that as the dimensions of x
   increase, the amount of operations required to compute the final result
   increases in a cubic trend. if x was rather small and especially had a
   low value for n/wasn   t of high dimensions, then using the normal
   equation would be feasible. but for any industrial application with
   large datasets, the normal equation would take extremely         sometimes
   nonsensically         long.

   i hope that was rewarding for you. it definitely was for me. this was
   my first step into matrix calculus, which proved to be a stimulating
   challenge. next article will be even more daring: a full mathematical
   writeup on id26! see you then!

     * [21]machine learning
     * [22]artificial intelligence
     * [23]today i learned

   (button)
   (button)
   (button) 839 claps
   (button) (button) (button) 16 (button) (button)

     (button) blockedunblock (button) followfollowing
   [24]go to the profile of rohan kapur

[25]rohan kapur

   rohankapur.com

     (button) follow
   [26]a year of artificial intelligence

[27]a year of artificial intelligence

   our ongoing effort to make the mathematics, science, linguistics, and
   philosophy of artificial intelligence fun and simple.

     * (button)
       (button) 839
     * (button)
     *
     *

   [28]a year of artificial intelligence
   never miss a story from a year of artificial intelligence, when you
   sign up for medium. [29]learn more
   never miss a story from a year of artificial intelligence
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://ayearofai.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1a1b16f65dda
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-3-deriving-the-normal-equation-using-matrix-calculus-1a1b16f65dda&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-3-deriving-the-normal-equation-using-matrix-calculus-1a1b16f65dda&source=--------------------------nav_reg&operation=register
   8. https://ayearofai.com/?source=logo-lo_mz79qflh1lte---bb87da25612c
   9. https://ayearofai.com/tagged/algorithms
  10. https://ayearofai.com/tagged/today-i-learned
  11. https://ayearofai.com/tagged/case-studies
  12. https://ayearofai.com/tagged/philosophical
  13. https://ayearofai.com/tagged/meta
  14. https://qph.ec.quoracdn.net/main-qimg-99f7ea7d0929ed41dbecf67ec51b80b3?convert_to_webp=true
  15. https://ayearofai.com/@mckapur?source=post_header_lockup
  16. https://ayearofai.com/@mckapur
  17. https://medium.com/a-year-of-artificial-intelligence
  18. https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5
  19. http://mathworld.wolfram.com/normalequation.html
  20. http://www.pieceofshijiabian.com/wp-content/uploads/2014/01/screen-shot-2014-01-18-at-5.50.07-pm.png
  21. https://ayearofai.com/tagged/machine-learning?source=post
  22. https://ayearofai.com/tagged/artificial-intelligence?source=post
  23. https://ayearofai.com/tagged/today-i-learned?source=post
  24. https://ayearofai.com/@mckapur?source=footer_card
  25. https://ayearofai.com/@mckapur
  26. https://ayearofai.com/?source=footer_card
  27. https://ayearofai.com/?source=footer_card
  28. https://ayearofai.com/
  29. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  31. https://medium.com/p/1a1b16f65dda/share/twitter
  32. https://medium.com/p/1a1b16f65dda/share/facebook
  33. https://medium.com/p/1a1b16f65dda/share/twitter
  34. https://medium.com/p/1a1b16f65dda/share/facebook
