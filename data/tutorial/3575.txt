   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]a year of artificial intelligence
     * [9]algorithms
     * [10]today i learned
     * [11]case studies
     * [12]philosophical
     * [13]meta
     __________________________________________________________________

   [1*m2gdbt_nc-ie7r4am3shbq.jpeg]

rohan #4: the vanishing gradient problem

oh no         an obstacle to deep learning!

   [14]go to the profile of rohan kapur
   [15]rohan kapur (button) blockedunblock (button) followfollowing
   mar 16, 2016
     __________________________________________________________________

     this is the fourth entry in my [16]journey to extend my knowledge of
     artificial intelligence in the year of 2016. learn more about my
     motives in this [17]introduction post.

     this post assumes sound knowledge of neural networks and the
     id26 algorithm. if that does not describe your current
     understanding, no worries. you can catch up [18]here.
     __________________________________________________________________

   it   s summer time, and you recently read my medium [19]post on the
   id26 algorithm. excited to hack away at your own
   implementation, you create a deep, multi-layer neural network and begin
   running the program. but, to your avail, it   s either taking forever to
   train or not performing accurately. why    you thought neural networks
   were state of the art, didn   t you!

   i was, in fact, stuck on this issue for a while.

   here   s what we know about iterative optimization algorithms: they
   slowly make their way to the local optima by perturbing weights in a
   direction inferred from the gradient such that the cost function   s
   output is decreased. the id119 algorithm, in specific,
   updates the weights by the negative of the gradient multiplied by some
   small (between 0 and 1) scalar value.
   [0*b_2wazu14ush5ubo.]

   as you can see, we are to    repeat    until convergence. in reality,
   though, we actually set a hyper-parameter for the number of max
   iterations. if the number of iterations is too small for certain deep
   neural nets, we will have inaccurate results. if the number is too
   large, the training duration will become infeasibly long. it   s an
   unsettling tradeoff between training time and accuracy.

   so, why is this the case? well, put simply, if the gradient at each
   step is too small, then greater repetitions will be needed until
   convergence, because the weight is not changing enough at each
   iteration. or, the weights will not move as close to the minimum
   (versus greater gradients) in the set number of iterations. and with
   really really small gradients, this becomes a problem. it becomes
   infeasible to train neural networks, and they start predicting poorly.

   you would get an elongated cost function, like the following:
   [1*oxjoabpgs1oypaqgiuwnug.png]
   visual representation of the many more updates needed with flat
   gradients and an elongated cost function

   compare that shape to the following more optimal one, for example:
   [0*lf6cklbgitdwdkl-.]

   since the latter has larger gradients, id119 can converge
   much quicker.

   this isn   t the only issue. when gradients become too small, it also
   becomes difficult to represent the numerical value in computers. we
   call this underflow         the opposite of overflow.

   okay. small gradients = bad news, got it. the question, then, is: does
   this problem exist? in many cases it indeed does, and we call it the
   vanishing gradient problem.

   recall the sigmoid function, one that was almost always used as an
   activation function for anns in a classification context:
   [0*vt63osgdwkos3w8z.]
   [1*dxtxdflqylyaujvcdgoesg.png]

   the sigmoid function is useful because it    squeezes    any input value
   into an output range of (0, 1) (where it asymptotes). this is perfect
   for representations of probabilities and classification. the sigmoid
   function, along with the tanh function, though, have lost popularity in
   recent years. why? because they suffer from the vanishing gradient
   problem!

   let   s take a look at the derivative of the sigmoid function. i   ve
   precomputed it for you:
   [1*rwfqhte8duu9mieue6ntyq.png]
   in other words, simply s(1-s)

   now, let   s graph that:
   [1*gojcyjqpb5omcfs0zt8tqq.png]
   courtesy of [20]https://www.desmos.com/calculator

   looks decent, you say. look closer. the maximum point of the function
   is 1/4, and the function horizontally asymptotes at 0. in other words,
   the output of the derivative of the cost function is always between 0
   and 1/4. in mathematical terms, the range is (0, 1/4]. keep that in
   mind.

   now, let   s move on to the structure of a neural network and backprop
   and their implications on the size of gradients.
   [0*susoph_fuwnahqjd.]

   recall this general structure for a simple, univariate neural network.
   each neuron or    activity    is derived from the previous: it is the
   previous activity multiplied by some weight and then fed through an
   activation function. the input, of course, is the notable exception.
   the error box j at the end returns the aggregate error of our system.
   we then perform id26 to modify the weights through gradient
   descent such that the output of j is minimized.

   to calculate the derivative to the first weight, we used the chain rule
   to    backpropagate    like so:
   [0*d5zula4kvesvylng.]
   we then use these derivatives to iteratively make our way the minimum
   point using id119.

   let   s focus on these individual derivatives:
   [1*gxkjqivu9eiy1fiof5bghw.png]

   with regards to the first derivative         since the output is the
   activation of the 2nd hidden unit, and we are using the sigmoid
   function as our activation function, then the derivative of the output
   is going to contain the derivative of the sigmoid function. in
   specific, the resulting expression will be:
   [1*c9gelh3uoc54l9h4nevrba.png]
   from the output to hidden2

   the same applies for the second:
   [1*vdubtzz03inbssinfs-q8a.png]
   from hidden2 to hidden1

   in both cases, the derivative contains the derivative of the sigmoid
   function. now, let   s put those together:
   [1*zejgopp4q1c170wsw7k_-w.png]

   recall that the derivative of the sigmoid function outputs values
   between 0 and 1/4. by multiplying these two derivatives together, we
   are multiplying two values in the range (0, 1/4]. any two numbers
   between 0 and 1 multiplied with each other will simply result in a
   smaller value. for example, 1/3    1/3 is 1/9.

   the [21]standard approach to weight initialization in a typical neural
   network is to choose weights using a gaussian with mean 0 and standard
   deviation 1. hence, the weights in a neural network will also usually
   be between -1 and 1.

   now, look at the magnitude of the terms in our expression:
   [1*8jj6syleutvuzr7toyyfvg.png]

   at this point, we are multiplying four values which are between 0 and
   1. that will become small very fast. and even if this weight
   initialization technique is not employed, the vanishing gradient
   problem will most likely still occur. many of these sigmoid derivatives
   multiplied together would be small enough to compensate for the other
   weights, and the other weights may want to shift into a range below 1.

   this neural network isn   t that deep. but imagine a deeper one used in
   an industrial application. as we backpropagate further back, we   d have
   many more small numbers partaking in a product, creating an even tinier
   gradient! thus, with deep neural nets, the vanishing gradient problem
   becomes a major concern. sidenote: we can even have exploding
   gradients, if the gradients were bigger than 1 (a bunch of numbers
   bigger than 1 multiplied together is going to give a huge result!)
   these aren   t good either         they wildly overshoot.

   now, let   s look at a typical ann:
   [1*ovm3kqss3wbo2nxj5yzrmw.png]

   as you can see, the first layer is the furthest back from the error, so
   the derivative (using the chain rule) will be a longer expression and
   hence will contain more sigmoid derivatives, ending up smaller. because
   of this, the first layers are the slowest to train. but there   s another
   issue with this: since the latter layers (and most notably the output)
   is functionally dependent on the earlier layers, inaccurate early
   layers will cause the latter layers to simply build on this inaccuracy,
   corrupting the entire neural net. take convolutional neural nets as an
   example; their early layers perform more high-level feature detection
   such that the latter layers can analyze them further to make a choice.
   also, because of the small steps, id119 may converge at a
   local minimum.

   this is why neural networks (especially deep ones), at first, failed to
   become popular. training the earlier layers correctly was the basis for
   the entire network, but that proved too difficult and infeasible
   because of the commonly used id180 and available
   hardware.

   how do we solve this? well, it   s pretty clear that the root cause is
   the nature of the sigmoid activation function derivative. this also
   happened in the most popular alternative, the tanh function. until
   recently, not many other id180 were thought of / used.
   but now, the sigmoid and tanh functions have been declining in
   popularity in the light of the relu activation function.

   what is the relu         rectified linear unit         function anyways? it is a
   piecewise function that corresponds to:
   [1*zd5kma5j-6uabfewerv_dq.png]

   another way of writing the relu function is like so:
   [1*_jo2pajyfg1y5thufa3pna.png]

   in other words, when the input is smaller than zero, the function will
   output zero. else, the function will mimic the identity function. it   s
   very fast to compute the relu function.
   [1*i3m4x5zmlohhk8w3j3pvkw.png]
   the    rectified linear unit    acivation
   function         [22]http://i.stack.imgur.com/8cglm.png

   it doesn   t take a genius to calculate the derivative of this function.
   when the input is smaller than zero, the output is always equal to
   zero, and so the rate of change of the function is zero. when the input
   is greater or equal to zero, the output is simply the input, and hence
   the derivative is equal to one:
   [1*-1esz_gtjhs2jtmsh4xaug.png]
   the function is not differentiable at zero, though

   if we were to graph this derivative, it would look exactly like a
   typical step function:
   [1*cisyfhoztjqrfhuiimxn8g.png]
   hey    it   s a step function!

   so, it   s solved! our derivatives will no longer vanish, because the
   activation function   s derivative isn   t bounded by the range (0, 1).

   relus have one caveat though: they    die    (output zero) when the input
   to it is negative. this can, in many cases, completely block
   id26 because the gradients will just be zero after one
   negative value has been inputted to the relu function. this would also
   be an issue if a large negative bias term / constant term is
   learned         the weighted sum fed into neurons may end up being negative
   because the positive weights cannot compensate for the significance of
   the bias term. negative weights also come to mind, or negative input
   (or some combination that gives a negative weighted sum). the dead relu
   will hence output the same value for almost all of your
   activities         zero. relus cannot    recover    from this problem because
   they will not modify the weights in anyway, since not only is the
   output for any negative input zero, the derivative is too. no updates
   will be made to modify the (for example) bias term to be a lesser
   magnitude of negative such that the neural net can escape from
   corruption of the entire network. it doesn   t happen all that often that
   the weighted sum ends up negative, though; and we can indeed initialize
   weights to be only positive and/or normalize input between 0 and 1 if
   we are concerned about the chance of an issue like this occurring.

   edit: as it turns out, there are some extremely useful properties of
   gradients dying off. in particular, the idea of    sparsity   . so, many
   times, backprop being blocked can actually be an advantage. more on
   that in this stackoverflow answer:
   [23]how does rectilinear activation function solve the vanishing
   gradient problem in neural networks?
   cross validated is a question and answer site for people interested in
   statistics, machine learning, data analysis   stats.stackexchange.com

   a    leaky    relu solves this problem. leaky rectified linear units are
   ones that have a very small gradient instead of a zero gradient when
   the input is negative, giving the chance for the net to continue its
   learning.
   [1*uewultnbkd-hke5kbgfxuq.png]

   edit: looks like values in the range of 0.2   0.3 are more common than
   something like 0.01.

   instead of outputting zero when the input is negative, the function
   will output a very flat line, using gradient   . a common value to use
   for    is 0.01. the resulting function is represented in following
   diagram:
   [1*tqn5q_dqdfgspp6nprpbfa.png]

   as you can see, the learning will be small with negative inputs, but
   will exist nonetheless. in this sense, leaky relus do not die.

   however, relus/leaky relus aren   t necessarily always optimal         results
   with them have been inconsistent (maybe because, due to the small
   constant, in certain cases this could cause vanishing gradients
   again?         that being said, dead units again don   t happen all that
   often). another notable issue is that, because the output of relu isn   t
   bounded between 0 and 1 or -1 and 1 like tanh/sigmoid are, the
   activations (values in the neurons in the network, not the gradients)
   can in fact explode with extremely deep neural networks like recurrent
   neural networks. during training, the whole network becomes fragile and
   unstable in that, if you update weights in the wrong direction even the
   slightest, the activations can blow up. finally, even though the relu
   derivatives are either 0 or 1, our overall derivative expression
   contains the weights multiplied in. since the weights are generally
   initialized to be < 1, this could contribute to vanishing gradients.

   so, overall, it   s not a black and white problem. relus still face the
   vanishing gradient problem, it   s just that they often face it to a
   lesser degree. in my opinion, the vanishing gradient problem isn   t
   binary; you can   t solve it with one technique, but you can delay (in
   terms of how deep we can go before we start suffering again) its
   impact.

   you may also be wondering: by the way, relus don   t squeeze values into
   a id203, so why are they so commonly used? one can easily stick a
   sigmoid/logistic activity to the end of a neural net for a binary
   classification scenario. for multiple outputs, one could use the
   [24]softmax function to create a id203 distribution that adds to
   1.

conclusion

   this article was a joy to write because it was my first step into the
   intricacies and practice of machine learning, rather than just looking
   at theoretical algorithms and calculations. the vanishing gradient
   problem was a major obstacle for the success of deep learning, but now
   that we   ve overcome it through multiple different techniques in weight
   initialization (which i talked less about today), feature preparation
   (through batch id172         centering all input feature values to
   zero), and id180, the field is going to thrive         and
   we   ve already seen it with recent innovations in game ai ([25]alphago,
   of course!). here   s to the future of multi-layered neural networks!     

     * [26]machine learning
     * [27]artificial intelligence
     * [28]today i learned

   (button)
   (button)
   (button) 1.3k claps
   (button) (button) (button) 15 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of rohan kapur

[30]rohan kapur

   rohankapur.com

     (button) follow
   [31]a year of artificial intelligence

[32]a year of artificial intelligence

   our ongoing effort to make the mathematics, science, linguistics, and
   philosophy of artificial intelligence fun and simple.

     * (button)
       (button) 1.3k
     * (button)
     *
     *

   [33]a year of artificial intelligence
   never miss a story from a year of artificial intelligence, when you
   sign up for medium. [34]learn more
   never miss a story from a year of artificial intelligence
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://ayearofai.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ec68f76ffb9b
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b&source=--------------------------nav_reg&operation=register
   8. https://ayearofai.com/?source=logo-lo_xhtshelp3mgs---bb87da25612c
   9. https://ayearofai.com/tagged/algorithms
  10. https://ayearofai.com/tagged/today-i-learned
  11. https://ayearofai.com/tagged/case-studies
  12. https://ayearofai.com/tagged/philosophical
  13. https://ayearofai.com/tagged/meta
  14. https://ayearofai.com/@mckapur?source=post_header_lockup
  15. https://ayearofai.com/@mckapur
  16. https://medium.com/a-year-of-artificial-intelligence
  17. https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5
  18. https://medium.com/a-year-of-artificial-intelligence/rohan-lenny-1-neural-networks-the-id26-algorithm-explained-abf4609d4f9d
  19. https://medium.com/a-year-of-artificial-intelligence/rohan-lenny-1-neural-networks-the-id26-algorithm-explained-abf4609d4f9d
  20. https://www.desmos.com/calculator
  21. http://neuralnetworksanddeeplearning.com/chap3.html#weight_initialization
  22. http://i.stack.imgur.com/8cglm.png
  23. http://stats.stackexchange.com/a/176905/98975
  24. https://en.wikipedia.org/wiki/softmax_function
  25. https://en.wikipedia.org/wiki/alphago
  26. https://ayearofai.com/tagged/machine-learning?source=post
  27. https://ayearofai.com/tagged/artificial-intelligence?source=post
  28. https://ayearofai.com/tagged/today-i-learned?source=post
  29. https://ayearofai.com/@mckapur?source=footer_card
  30. https://ayearofai.com/@mckapur
  31. https://ayearofai.com/?source=footer_card
  32. https://ayearofai.com/?source=footer_card
  33. https://ayearofai.com/
  34. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  36. http://stats.stackexchange.com/a/176905/98975
  37. https://medium.com/p/ec68f76ffb9b/share/twitter
  38. https://medium.com/p/ec68f76ffb9b/share/facebook
  39. https://medium.com/p/ec68f76ffb9b/share/twitter
  40. https://medium.com/p/ec68f76ffb9b/share/facebook
