   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]a year of artificial intelligence
     * [9]algorithms
     * [10]today i learned
     * [11]case studies
     * [12]philosophical
     * [13]meta
     __________________________________________________________________

rohan #5: what are bias units?

   [14]go to the profile of rohan kapur
   [15]rohan kapur (button) blockedunblock (button) followfollowing
   apr 8, 2016
     __________________________________________________________________

     this is the fifth entry in my [16]journey to extend my knowledge of
     artificial intelligence in the year of 2016. learn more about my
     motives in this [17]introduction post.

     this post assumes sound knowledge of neural networks. if that does
     not describe your current understanding, no worries. you can catch
     up [18]here.
     __________________________________________________________________

   it   s almost been a month since my last article on a year of artificial
   intelligence. why? it   s quite simple         i   m preparing for some of the
   most important examinations in my high school student life thus far.
   these results will largely contribute to the    predicted    scores sent to
   universities. as a result, i   ve had less time to work on articles for
   this blog (even though i   ve had a ton more cool ideas).

   i recently answered a question on quora asking about the purpose of
   bias units in an id158, which i   m sure some readers
   of ayoai may also be curious about. hence, i   ve posted it on
   here         sorry if it   s a bit basic!

   after i complete my exams, we   re going to be looking at recurrent
   neural networks, convolutional neural networks, markov chains,
   independent component analysis/principal component analysis, and more.
   we   re also going to explore them in the context of recent stories in ai
   such as go and tay.ai! then, we   ll look at further case studies, mostly
   from recent research papers such as deepmind   s id63s
   (a differentiate turing machine that can write programs!) and bush fire
   recognition from satellite imagery.
     __________________________________________________________________

   in a typical id158, each neuron/activity in one
      layer    is connected         via a weigh         to each neuron in the next
   activity. each of these activities stores some sort of computation,
   normally a composite of the weighted activities in previous layers.

   a bias unit is an    extra    neuron added to each pre-output layer that
   stores the value of 1. bias units aren   t connected to any previous
   layer and in this sense don   t represent a true    activity   .

   take a look at the following illustration:
   [0*rvzeeovkghsbxifw.]
   [19]http://ufldl.stanford.edu/wiki/images/thumb/4/40/network3322.png/50
   0px-network3322.png

   the bias units are characterized by the text    +1   . as you can see, a
   bias unit is just appended to the start/end of the input and each
   hidden layer, and isn   t influenced by the values in the previous layer.
   in other words, these neurons don   t have any incoming connections.

   so why do we have bias units? well, bias units still have outgoing
   connections and they can contribute to the output of the ann. let   s
   call the outgoing weights of the bias units w_b. now, let   s look at a
   really simple neural network that just has one input and one
   connection:
   [1*jxu1zakvaivw62f9penvug.png]

   let   s say act()         our activation function         is just f(x) = x, or the
   identity function. in such case, our ann would represent a line because
   the output is just the weight (m) times the input (x).
   [0*wxpu7cpytyjt9m15.]
   [20]http://www.mathsteacher.com.au/year10/ch03_linear_graphs/03_equatio
   n/image3673.gif

   when we change our weight w1, we will change the gradient of the
   function to make it steeper or flatter. but what about shifting the
   function vertically? in other words, what about setting the
   y-intercept. this is crucial for many modelling problems! our optimal
   models may not pass through the origin.

   so, we know that our function output = w    input (y = mx) needs to have
   this constant term added to it. in other words, we can say output = w   
   input + w_b, where w_b is our constant term c. when we use neural
   networks, though, or do any multi-variable learning, our computations
   will be done through id202 and matrix arithmetic eg.
   dot-product, multiplication. this can also be seen graphically in the
   ann. there should be a matching number of weights and activities for a
   weighted sum to occur. because of this, we need to    add    an extra input
   term so that we can add a constant term with it. since, one multiplied
   by any value is that value, we just    insert    an extra value of 1 at
   every layer. this is called the bias unit.
   [0*edrnwahfiwkfs78v.]
   [21]http://natekohl.net/media/bias-net.gif

   from this diagram, you can see that we   ve now added the bias term and
   hence the weight w_b will be added to the weighted sum, and fed through
   activation function as a constant value. this constant term, also
   called the    intercept term    (as demonstrated by the linear example),
   shifts the activation function to the left or to the right. it will
   also be the output when the input is zero.

   here is a diagram of how different weights will transform the
   activation function (sigmoid in this case) by scaling it up/down:
   [0*zmegev4nmqxt1c32.]
   [22]http://natekohl.net/media/sigmoid-scale.png

   but now, by adding the bias unit, we the possibility of translating the
   activation function exists:
   [0*wkucmwpx78n6okeu.]
   [23]http://natekohl.net/media/sigmoid-shift.png

   going back to the id75 example, if w_b = 1, then we will
   add bias    w_b= 1    w_b = w_b to the activation function. in the
   example with the line, we can create a non-zero y-intercept:
   [1*y_yccw3mmwcocy_q5bv3pa.png]
   [24]http://prep.math.lsa.umich.edu/pmc/images/14.1.r.1.gif

   i   m sure you can imagine infinite scenarios where the line of best fit
   does not go through the origin or even come near it. bias units are
   important with neural networks in the same way.

     * [25]machine learning
     * [26]neural networks
     * [27]today i learned

   (button)
   (button)
   (button) 164 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [28]go to the profile of rohan kapur

[29]rohan kapur

   rohankapur.com

     (button) follow
   [30]a year of artificial intelligence

[31]a year of artificial intelligence

   our ongoing effort to make the mathematics, science, linguistics, and
   philosophy of artificial intelligence fun and simple.

     * (button)
       (button) 164
     * (button)
     *
     *

   [32]a year of artificial intelligence
   never miss a story from a year of artificial intelligence, when you
   sign up for medium. [33]learn more
   never miss a story from a year of artificial intelligence
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://ayearofai.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/828d942b4f52
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52&source=--------------------------nav_reg&operation=register
   8. https://ayearofai.com/?source=logo-lo_qveuvennri7t---bb87da25612c
   9. https://ayearofai.com/tagged/algorithms
  10. https://ayearofai.com/tagged/today-i-learned
  11. https://ayearofai.com/tagged/case-studies
  12. https://ayearofai.com/tagged/philosophical
  13. https://ayearofai.com/tagged/meta
  14. https://ayearofai.com/@mckapur?source=post_header_lockup
  15. https://ayearofai.com/@mckapur
  16. https://medium.com/a-year-of-artificial-intelligence
  17. https://medium.com/a-year-of-artificial-intelligence/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919#.bfjoaqxu5
  18. https://medium.com/a-year-of-artificial-intelligence/rohan-lenny-1-neural-networks-the-id26-algorithm-explained-abf4609d4f9d
  19. http://ufldl.stanford.edu/wiki/images/thumb/4/40/network3322.png/500px-network3322.png
  20. http://www.mathsteacher.com.au/year10/ch03_linear_graphs/03_equation/image3673.gif
  21. http://natekohl.net/media/bias-net.gif
  22. http://natekohl.net/media/sigmoid-scale.png
  23. http://natekohl.net/media/sigmoid-shift.png
  24. http://prep.math.lsa.umich.edu/pmc/images/14.1.r.1.gif
  25. https://ayearofai.com/tagged/machine-learning?source=post
  26. https://ayearofai.com/tagged/neural-networks?source=post
  27. https://ayearofai.com/tagged/today-i-learned?source=post
  28. https://ayearofai.com/@mckapur?source=footer_card
  29. https://ayearofai.com/@mckapur
  30. https://ayearofai.com/?source=footer_card
  31. https://ayearofai.com/?source=footer_card
  32. https://ayearofai.com/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/828d942b4f52/share/twitter
  36. https://medium.com/p/828d942b4f52/share/facebook
  37. https://medium.com/p/828d942b4f52/share/twitter
  38. https://medium.com/p/828d942b4f52/share/facebook
