   #[1]magenta

   [2]magenta logo

   (button)
   [3]get started [4]demos [5]blog [6]research [7]community

   [8]get started [9]demos [10]blog [11]research [12]community

tuning recurrent neural networks with id23

   nov 9, 2016
       natasha jaques [13]natashamjaques

   we are excited to announce our new [14]rl tuner algorithm, a method for
   enchancing the performance of an lstm trained on data using
   id23 (rl). we create an rl reward function that
   teaches the model to follow certain rules, while still allowing it to
   retain information learned from data. we use rl tuner to teach concepts
   of music theory to an lstm trained to generate melodies. the two videos
   below show samples from the original lstm model, and the same model
   enchanced using rl tuner.

   iframe: [15]https://www.youtube.com/embed/cdcsookiclw

   iframe: [16]https://www.youtube.com/embed/abbfzb5dlsy

introduction

   [note_id56_model.png]

   when i joined [17]magenta as an intern this summer, the team was hard
   at work on developing better ways to train recurrent neural networks
   (id56s) to generate sequences of notes. as you may remember from
   [18]previous posts, these models typically consist of a long short-term
   memory (lstm) network trained on monophonic melodies. this means that
   melodies are fed into the network one note at a time, and it is trained
   to predict the next note in the sequence. the figure on the left shows
   a simplified version of this type of network unrolled over time, in
   which it is being trained on the first 6 notes of    twinkle twinkle
   little star   . from now on, i   ll refer to this type of model as a note
   id56.

   a note id56 is conceptually similar to a character id56, a popular model
   for generating text, one character at a time. while both types of
   models can produce impressive results, they have some frustrating
   limitations. they both suffer from common failure modes, such as
   continually repeating the same token. further, the sequences produced
   by the models tend to lack a consistent global structure. to see this
   more clearly, take a look at the text below, which was generated by a
   character id56 trained on wikipedia markdown data (taken from
   [19]graves, 2013):

     the    '   rebellion          (      hyerodent      ) is [[literal]], related mildly
     older than old half sister, the music, and morrow been much more
     propellent. all those of [[hamas (mass)|sausage trafficking]]s were
     also known as [[trip class submarine|   sante    at serassim]];    verra   
     as 1865-682-831 is related to ballistic missiles. while she viewed
     it friend of halla equatorial weapons of tuscany, in [[france]],
     from vaccine homes to "individual", among [[slavery|slaves]](such as
     artistual selling of factories were renamed english habit of twelve
     years.)

   while the model has learned how to correctly spell english words, some
   markdown syntax, and even mostly correct grammatical structure,
   sentences don   t seem to follow a consistent thought. the text wonders
   rapidly from topic to topic, discussing everything from sisters to
   submarines to tuscany, slavery, and factories, all in one short
   paragraph. this type of global incoherence is typical of the melodies
   produced by a vanilla note id56 as well. they do not maintain a
   consistent musical structure, and can sound wandering and random.

   music is an interesting test-bed for sequence generation, in that
   musical compositions adhere to a relatively well-defined set of
   structural rules. any beginning music student learns that groups of
   notes belong to keys, chords follow progressions, and songs have
   consistent structures made up of musical phrases. so what if we could
   teach a note id56 these kinds of musical rules, while still allowing it
   to learn patterns from music it hears in the world?

   this is the idea behind the rl tuner model i will describe in this
   post. we take a trained note id56 and teach it concepts of music theory
   using id23 (rl). rl can allow a network to learn some
   non-differentiable reward function. in this case, we define a set of
   music theory rules, and produce rewards based on whether the model   s
   compositions adhere to those rules. however, to ensure that the model
   can remember the note probabilities it originally learned from data, we
   keep a second, fixed copy of the note id56 which we call the reward id56.
   the reward id56 is used to compute the id203 of playing the next
   note as learned by the original note id56. we augment our music theory
   rewards with this id203 value, so that the total reward reflects
   both our music theory constraints and information learned from data.

   we show that this approach allows the model to maintain information
   about the note probabilities learned from data, while significantly
   improving the behaviors of the note id56 targeted by the music theory
   rewards. for example, before training with rl, 63.3% of notes produced
   by the note id56 belonged to some excessively repeated segment of notes;
   after rl, 0.0-0.03% of notes were excessively repeated. since
   excessively repeating tokens is a problem in other domains as well
   (e.g. text generation), we believe our approach could have broader
   applications. but does it actually work to produce better music? we
   conducted a user study and found that people find the compositions
   produced by our three rl models significantly more musically pleasing
   than those of the original note id56. but we encourage you to judge for
   yourself; samples from each of the models will be provided later in
   this post.

   the models, derivations, and results are all described in our recent
   [20]research paper, written by myself, [21]shane gu, [22]richard e.
   turner, and [23]douglas eck^[24]1. [25]code to run this model is also
   available on the magenta github repo; please try it out! the music
   theory rules implemented for the model are only a first attempt, and
   could easily be improved by someone with musical training.

   [^1] [^a version of this work was accepted at the [26]nips 2016 deep
   id23 workshop.]

background: id23 and deep id24

   this section will give a brief introduction to some ideas behind rl and
   deep q networks (id25s). if you   re familiar with these topics you may
   wish to skip ahead.

   in id23 (rl), an agent interacts with an environment.
   given the state of the environment , the agent takes an action ,
   receives a reward , and the environment transitions to a new state, .
   the goal of the agent is to maximize reward, which is usually some
   clear signal from the environment, such as points in a game.

   the rules for how the agent chooses to act in the environment define a
   policy. to learn the most effective policy, the agent can   t just
   greedily maximize the reward it will receive after the next action, but
   must instead consider the total cumulative reward it can expect to
   receive over a course of actions occurring in the future. because
   future rewards are typically uncertain if the environment has random
   effects, a discount factor of is applied to the reward for each
   timestep in the future. if is the reward received at timestep , then is
   the total future discounted return: \begin{align} r_t =
   \sum^t_{t   =t}\gamma^{t   -t}r_{t   } \end{align}

   in id24, the goal is to learn a q function that gives the maximum
   expected discounted future return for taking any action in state , and
   continuing to act optimally at each step in the future. therefore the
   optimal q function, is defined as:

   where is the policy mapping each state to a id203 distributions
   over the next action. to learn , we can apply an iterative update based
   on the bellman equation: \begin{align} q_{i+1}(s, a) = \mathbb{e}[r +
   \gamma max_{a   }q_i(s   ,a   )|s,a] \end{align} where r is the reward
   received for taking action a in state s. this value iteration method
   will converge to as . while learning, it is important to explore the
   space of possible actions, either by occasionally choosing random
   actions, or sampling an action based on the values defined by the
   function. once the function has been learned, the optimal policy can be
   obtained by simply choosing the action with the highest value at every
   step.

   in [27]deep id24, a neural network called the deep q-network
   (id25) is used to approximate the function, . the network parameters are
   learned by applying stochastic id119 (sgd) updates with
   respect to the following id168: \begin{align} \label{eq:qloss}
   l_t(\theta_t) = (r_t + \gamma \max_{a   }q(s   ,a   ;\theta_{t-1}) -
   q(s,a;\theta_t))^2 \end{align} the first two terms are the function the
   network is trying to learn: the actual reward received at step , , plus
   the discounted future return estimated by the q-network parameters at
   step . the id168 computes the difference between this desired
   value, and the actual value output by the q-network at step .
   essentially, this is the prediction error in estimating the function
   made by the q-network. importantly, the parameters for the previous
   generation of the network () are held fixed, and not updated by sgd.

   several techniques are required for a id25 to work effectively. as the
   agent interacts with the environment, tuples it experiences are stored
   in an experience buffer. training the q-network is accomplished by
   randomly sampling batches from the experience buffer to compute the
   loss. the experience buffer is essential for learning; if the agent was
   trained using consecutive samples of experience, the samples would be
   highly correlated, updates would have high variance, and the network
   parameters could become stuck in a local minimum or diverge.

   further optimizations to the id25 algorithm have been proposed that help
   enhance learning and ensure stability. one of these is [28]deep double
   id24, in which a second, target q-network is used to estimate
   expected future return, while the q-network is used to choose the next
   action. since id24 has been shown to learn unrealistically high
   action values because it estimates maximum expected return, having a
   second q-network can lead to more realistic estimates and better
   performance.

rl tuner

   as described above, the main idea behind the rl tuner model is to take
   an id56 trained on data, and refine it using rl. the model uses a
   standard id25 implementation, complete with an experience buffer and
   target q-network. a trained note id56 is used to supply the initial
   values of the weights in the q-network and target q-network, and a
   third copy is used as the reward id56. the reward id56 is held fixed
   during training, and is used to supply part of the reward function used
   to train the model. the figure below illustrates these ideas.

   model diagram

   to formulate musical composition as an rl problem, we treat choosing
   the next note as taking an action . the state of the environment
   consists of the state of the composition so far, as well as the
   internal lstm state of the q-network and reward id56. the reward
   function is a combination of both music theory rules and probabilities
   learned from data. the music theory reward is calculated by a set of
   functions (described in the next section) that constrain the model to
   adhere to certain rules, such as playing in the same key. however, it
   is necessary that the model still be    creative    rather than learning a
   simple composition that can easily exploit these rewards. therefore,
   the reward id56 is used to compute |, the id203 of playing the
   next note given the composition as originally learned from actual
   songs. the total reward given at time is therefore:

   where is a constant controlling the emphasis placed on the music theory
   reward. so now, we see that the new id168 for our model is:

   this modified id168 forces the model to learn that the most
   valuable actions are those that conform to the music theory rules, but
   still have the high id203 in the original data.

for the mathematically inclined   

   in [29]our paper we show that the id168 described above can be
   related to an approximation of the stochastic optimal control
   objective, leading to a rl cost with an additional penalty applied to
   kl-divergence from a prior policy. if we think of the probabilities
   learned by the note id56 as the prior policy |, and as the policy
   learned by the model, then this would be equivalent to learning the
   following function:

   this is not exactly the same as our method, because we are missing the
   id178 term in the kl-divergence function. this led us to implement
   two other kl-regularized variants of id24: [30]learning, and
   [31]g learning. the id168 for learning is:

   and the id168 for g learning is:

music theory rewards

   the rules we implemented to make sure our model conformed to music
   theory were based on some domain knowledge, and the book [32]   a
   practical approach to eighteenth-century counterpoint    by robert
   gauldin. we are by no means claiming that these rules are necessary for
   good compositions, exhaustive, or even particularly creative. they
   simply allow us to constrain our model to adhere to some sort of
   consistent structure. we encourage any interested readers to experiment
   with different rules and see what types of results they can produce.
   for now, the rules that we chose encourage the compositions produced by
   our model to have the following characteristics:
     * stay in key: notes should belong to the same key. for example, if
       the desired key is c-major, a b-flat would not be an acceptable
       note.
     * begin and end with the tonic note: the first note of the
       composition, and the first note of the final bar should be the
       tonic note of the key; e.g. if the key is c-major, this note would
       be middle c.
     * avoid excessively repeated notes: unless a rest is introduced or a
       note is held, a single tone should not be repeated more than four
       times in a row. while the number four can be considered a rough
       heuristic, avoiding excessively repeated notes and static melodic
       contours is [33]gauldin   s first rule of melodic composition.
     * prefer harmonious intervals: the composition should avoid awkward
       intervals like augmented sevenths, or large jumps of more than an
       octave. gauldin also indicates good compositions should move by a
       mixture of small steps and larger harmonic intervals, with emphasis
       on the former; the reward values for intervals reflect these
       requirements.
     * resolve large leaps: when the composition leaps several pitches in
       one direction, it should eventually be resolved by a leap back or
       gradual movement in the opposite direction. leaping twice in the
       same direction is negatively rewarded.
     * avoid continuously repeating extrema notes: the highest note of the
       composition should be unique, as should the lowest note.
     * avoid high auto-correlation: to encourage variety, the reward
       function penalizes a melody if it is highly correlated with itself
       at a lag of 1, 2, or 3 beats.
     * play motifs: a musical motif is a succession of notes representing
       the shortest musical    idea   ; in our implementation, it is defined
       as a bar of music with three or more unique notes.
     * play repeated motifs: because [34]repetition has been shown to be
       key to emotional engagement with music, we tried to train the model
       to repeat motifs that it had previously introduced.

results

   to see if our models actually learned what we were trying to teach
   them, we computed statistics about how many of the notes and
   compositions generated by the models adhered to our music theory rules.
   the results are shown in the table below, and represent statistics
   about 100,000 compositions randomly generated by each model. the top
   section shows behaviors we want to decrease, and the bottom show
   behaviors we want to increase. bolded entries are significantly better
   than the basic note id56.
   behavior note id56 q psi g
   notes excessively repeated 63.3% 0.0% 0.02% 0.03%
   notes not in key 0.1% 1.0% 0.6% 28.7%
   mean autocorrelation (lag 1,2,3) -.16, .14, -.13 -.11, .03, .03 -.10,
   -.01, .01 .55, .31, .17
   leaps resolved 77.2% 91.1% 90.0% 52.2%
   compositions starting with tonic 0.9% 28.8% 28.7% 0.0%
   compositions with unique max note 64.7% 56.4% 59.4% 37.1%
   compositions with unique min note 49.4% 51.9% 58.3% 56.5%
   notes in motif 5.9% 75.7% 73.8% 69.3%
   notes in repeated motif 0.007% 0.11% 0.09% 0.01%

   these results show that the targeted behaviors have significantly
   improved in the rl models as compared to the original note id56. but how
   do we know that learning these rules didn   t cause the models to forget
   what they learned from data? the figures below plot the rewards
   received by each model over time, broken down into the | rewards from
   the note id56 (on the left), and the music theory rewards (on the
   right). each model is compared to a baseline rl only model that was
   trained using only the music theory rewards, and no information about
   the data probabilities. we see that compared to the rl only model, the
   rl tuner models maintain much higher |, while still learning the music
   theory rewards.
   note id56 rewards music theory rewards

   even so, it   s not clear that learning this somewhat arbitrary set of
   music theory rules will lead to better sounding compositions. to find
   out whether the models have improved over the original note id56, we
   asked mechanical turk workers to rate which of two randomly selected
   compositions they preferred. the figure below shows the number of times
   a composition from each model was selected as the winner. all three rl
   tuner models significantly outperformed the note id56, . in addition,
   both the and models were statistically significantly better than the
   model, although not significantly different from each other.
   [user_study.png]

   but you don   t need to take our word for it! the compositions used in
   the study are all available [35]here - please check them out! below,
   you can listen to a composition from each model. in the top row are
   compositions from the note id56 and ; in the bottom row are and . you
   can see that the note id56 plays the same note repeatedly, while the rl
   tuner models sound much more varied and interesting. the and are best
   at playing within the music theory contraints, staying firmly in key
   and frequently choosing more harmonious interval steps. still, we can
   tell that the models have retained information about the training
   songs. the sample ends with a riff that sounds very familiar!

   iframe: [36]https://www.youtube.com/embed/c5wm9nr1qiu

   iframe: [37]https://www.youtube.com/embed/hjcxs53ta14

   iframe: [38]https://www.youtube.com/embed/pavksrzfaei

   iframe: [39]https://www.youtube.com/embed/rj0kqjku7t4

   the figure below plots how the id203 that the models place on
   each note changes during the composition. note that 0 corresponds to
   the note off event, and 1 corresponds to no event; these are used for
   rests and holding notes.
   note id56 q g

summary

   in conclusion, we   ve demonstrated a technique for using rl to refine or
   tune (*pun intended*) an id56 trained on data. although some researchers
   may prefer to train models end-to-end on data, this approach is limited
   by the quality of the data that can be collected. when the data
   contains hidden biases, this approach can really be problematic.

   we hope you   ve enjoyed this post. if you   re interested in trying out
   the rl tuner model for yourself, please check out the [40]readme file
   on the magenta github.

acknowledgements

   this work was done in collaboration with [41]shane gu, [42]richard e.
   turner, and [43]douglas eck. many thanks also go to my wonderful
   collaborators on the [44]magenta team in [45]google brain, and in
   particular curtis (fjord) hawthorne and [46]kyle kastner (for his
   knowledgeable insights and handy spectrogram-movie-producing code).

     * [47]twitter
     * [48]blog
     * [49]github
     * [50]privacy
     * [51]terms

references

   visible links
   1. https://magenta.tensorflow.org/feed.xml
   2. https://magenta.tensorflow.org/
   3. https://magenta.tensorflow.org/get-started
   4. https://magenta.tensorflow.org/demos
   5. https://magenta.tensorflow.org/blog
   6. https://magenta.tensorflow.org/research
   7. https://magenta.tensorflow.org/community
   8. https://magenta.tensorflow.org/get-started
   9. https://magenta.tensorflow.org/demos
  10. https://magenta.tensorflow.org/blog
  11. https://magenta.tensorflow.org/research
  12. https://magenta.tensorflow.org/community
  13. https://github.com/natashamjaques
  14. https://arxiv.org/pdf/1611.02796v2.pdf
  15. https://www.youtube.com/embed/cdcsookiclw
  16. https://www.youtube.com/embed/abbfzb5dlsy
  17. https://magenta.tensorflow.org/
  18. https://magenta.tensorflow.org/blog/2016/07/15/lookback-id56-attention-id56/
  19. https://arxiv.org/pdf/1308.0850.pdf
  20. https://arxiv.org/pdf/1611.02796v2.pdf
  21. http://sg717.user.srcf.net/
  22. http://learning.eng.cam.ac.uk/public/turner/webhome
  23. http://research.google.com/pubs/author39086.html
  24. https://magenta.tensorflow.org/2016/11/09/tuning-recurrent-networks-with-reinforcement-learning#footnote-nipsrl
  25. https://github.com/tensorflow/magenta/tree/master/magenta/models/rl_tuner
  26. https://sites.google.com/site/deeprlnips2016/
  27. https://www.cs.toronto.edu/~vmnih/docs/id25.pdf
  28. https://arxiv.org/pdf/1509.06461.pdf
  29. https://arxiv.org/pdf/1611.02796v2.pdf
  30. http://homepages.inf.ed.ac.uk/svijayak/publications/rawlik-rss2012.pdf
  31. https://arxiv.org/pdf/1512.08562.pdf
  32. http://www.jstor.org/stable/40213921?seq=1#page_scan_tab_contents
  33. http://www.jstor.org/stable/40213921?seq=1#page_scan_tab_contents
  34. https://secureweb.mcgill.ca/spl/files/spl/livingstoneemotion2012.pdf
  35. https://goo.gl/xiyt9m
  36. https://www.youtube.com/embed/c5wm9nr1qiu
  37. https://www.youtube.com/embed/hjcxs53ta14
  38. https://www.youtube.com/embed/pavksrzfaei
  39. https://www.youtube.com/embed/rj0kqjku7t4
  40. https://github.com/tensorflow/magenta/tree/master/magenta/models/rl_tuner
  41. http://sg717.user.srcf.net/
  42. http://learning.eng.cam.ac.uk/public/turner/webhome
  43. http://research.google.com/pubs/author39086.html
  44. https://magenta.tensorflow.org/
  45. http://research.google.com/teams/brain/
  46. http://kastnerkyle.github.io/
  47. https://twitter.com/search?q=#madewithmagenta
  48. https://magenta.tensorflow.org/blog
  49. https://github.com/tensorflow/magenta
  50. https://www.google.com/policies/privacy/
  51. https://www.google.com/policies/terms/

   hidden links:
  53. https://ai.google/
