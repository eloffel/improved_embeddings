   #[1]github [2]recent commits to neural-dialogue-generation:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]58
     * [35]star [36]746
     * [37]fork [38]211

[39]jiweil/[40]neural-dialogue-generation

   [41]code [42]issues 10 [43]pull requests 2 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   no description, website, or topics provided.
     * [47]30 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors
     * [51]mit

    1. [52]lua 97.6%
    2. [53]shell 1.6%
    3. [54]python 0.8%

   (button) lua shell python
   branch: master (button) new pull request
   [55]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/j
   [56]download zip

downloading...

   want to be notified of new releases in
   jiweil/neural-dialogue-generation?
   [57]sign in [58]sign up

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [60]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [61]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [62]download the github extension for visual studio
   and try again.

   (button) go back
   jiwei li
   jiwei li [63]data
   latest commit [64]87c698c feb 17, 2017
   [65]permalink
   type          name         latest commit message  commit time
        failed to load latest commit information.
        [66]adversarial       [67]update            jan 25, 2017
        [68]atten             [69]update            jan 25, 2017
        [70]decode            [71]update            jan 25, 2017
        [72]distill
        [73]future_prediction
        [74]persona_addressee
        [75]data
        [76]license           [77]first commit      jan 25, 2017
        [78]readme.md

readme.md

   #neural dialogue generation

   this project contains the code or part of the code for the dialogue
   generation part in the following papers:
     * [1] j.li, m.galley, c.brockett, j.gao and b.dolan. "[79]a
       diversity-promoting objective function for neural conversation
       models". naacl2016.
     * [2] j.li, m.galley, c.brockett, j.gao and b.dolan. "[80]a
       persona-based neural conversation model". acl2016.
     * [3] j.li, w.monroe, t.shi, a.ritter, d.jurafsky. "[81]adversarial
       learning for neural dialogue generation " arxiv
     * [4] j.li, w.monroe, d.jurafsky. "[82]learning to decode for future
       success" arxiv
     * [5] j.li, w.monroe, d.jurafsky. "[83]a simple, fast diverse
       decoding algorithm for neural generation" arxiv
     * [6] j.li, w.monroe, d.jurafsky. "data distillation for controlling
       specificity in dialogue generation (to appear on arxiv)"

   this project is maintained by [84]jiwei li. feel free to contact
   [85]jiweil@stanford.edu for any relevant issue. this repo will continue
   to be updated. thanks to all the collaborators: [86]will monroe,
   [87]michel galley, [88]alan ritter, [89]tianlin shi, [90]jianfeng gao,
   [91]chris brockett, [92]bill dolan and [93]dan jurafsky.

   #setup

   this code requires torch7 and the following luarocks packages
     * [94]fbtorch
     * [95]cutorch
     * [96]cunn
     * [97]nngraph
     * [98]torchx
     * [99]tds

   #download data processed traning datasets can be downloaded at
   [100]link (unpacks to 8.9gb). all tokens have been transformed to
   indexes (dictionary file found at data/movie_2500)
t_given_s_dialogue_length2_3.txt: dialogue length 2, minimum utterance length 3,
 sources and targets separated by "|"
s_given_t_dialogue_length2_3.txt: dialogue length 2, minimum utterance length 3,
 targets and sources separated by "|"
t_given_s_dialogue_length2_6.txt: dialogue length 2, minimum utterance length 6,
 sources and targets separated by "|"
s_given_t_dialogue_length2_6.txt: dialogue length 2, minimum utterance length 6,
 targets and sources separated by "|"
t_given_s_dialogue_length3_6.txt: dialogue length 3, minimum utterance length 6,
 contexts (consisting of 2 utterances) and targets separated by "|"

   #atten training a vanilla attention encoder-decoder model.

   available options include:
-batch_size     (default 128,batch size)
-dimension      (default 512, vector dimensionality)
-dropout        (default 0.2, dropout rate)
-train_file     (default ../data/t_given_s_train.txt)
-dev_file       (default ../data/t_given_s_dev.txt)
-test_file      (default ../data/t_given_s_test.txt)
-init_weight    (default 0.1)
-alpha          (default 1, initial learning rate)
-start_halve    (default 6, when the model starts halving its learning rate)
-thres          (default 5, threshold for gradient clipping)
-source_max_length  (default 50, max length of source sentences)
-target_max_length  (default 50, max length of target sentences)
-layers         (default 2, number of lstm layers)
-savefolder     (default "save",the folder to save models and parameters)
-reverse        (default false, whether to reverse the sources)
-gpu_index      (default 1, the index of the gpu you want to train your model on
)
-savemodel      (default true, whether to save the trained model)
-dictpath       (default ../data/movie_25000, dictionary file)

   training/dev/testing data: each line corresponds a source-target pair
   (in t_given_s*.txt) or target-source pair (in s_given_t*.txt) separated
   by "|".

   to train the forward p(t|s) model, run
th train_atten.lua -train_file ../data/t_given_s_train.txt -dev_file ../data/t_g
iven_s_dev.txt -test_file ../data/t_given_s_test.txt -savefolder save_t_given_s

   after training, the trained models will be saved in
   save_t_given_s/model*. input parameters will be stored in
   save_t_given_s/params

   to train the backward model p(s|t), run
th train_atten.lua -train_file ../data/s_given_t_train.txt -dev_file ../data/s_g
iven_t_dev.txt -test_file ../data/s_given_t_test.txt -savefolder save_s_given_t

   the trained models will be stored in save_s_given_t/model*. input
   parameters will be stored insave_s_given_t/params

   #decode

   decoding given a pre-trained generative model. the pre-trained model
   doesn't have to be a vanila id195 model (for example, it can be a
   trained model from adversarial learning).

   available options include:
-beam_size      (default 7, beam size)
-batch_size     (default 128, decoding batch size)
-params_file    (default "../atten/save_t_given_s/params", input parameter files
 for a pre-trained id195 model.)
-model_file     (default "../atten/save_s_given_t/model1",path for loading a pre
-trained id195 model)
-setting        (default "bs", the setting for decoding, taking values of "sampl
ing": sampling tokens from the distribution; "bs" standard id125; "diverse
bs", the diverse id125 described in [5]; "stochasticgreedy", the stochasti
cgreedy model described in [6])
-diverserate    (default 0. the diverse-decoding rate for penalizing intra-sibli
ng hypotheses in the diverse decoding model described in [5])
-inputfile      (default "../data/t_given_s_test.txt", the input file for decodi
ng)
-outputfile     (default "output.txt", the output file to store the generated re
sponses)
-max_length     (default 20, the maximum length of a decoded response)
-min_length     (default 1, the minimum length of a decoded response)
-nbest          (default false, whether to output a decoded n-best list (if true
) or only  output the candidate with the greatest score(if false))
-gpu_index      (default 1, the index of gpu to use for decoding)
-allowunk       (default false, whether to allow to generate unk tokens)
-mmi            (default false, whether to perform the mutual information rerank
ing after decoding as in [1])
-mmi_params_file    (default "../atten/save_s_given_t/params", the input paramet
er file for training the backward model p(s|t))
-mmi_model_file     (default "../atten/save_s_given_t/model1", path for loading
the backward model p(s|t))
-max_decoded_num    (default 0. the maximum number of instances to decode. decod
e the entire input set if the value is set to 0.)
-output_source_target_side_by_side  (default true, output input sources and deco
ded targets side by side)
-dictpath       (default ../data/movie_25000, dictionary file)

   to run the model
th decode.lua [params]

   to run the mutual information reranking model in [1], -mmi_params_file
   and -mmi_model_file need to be pre-specified

   #persona_addressee

   the persona_addressee model described in [2]

   additional options include:
-train_file     (default ../data/speaker_addresseet_train.txt)
-dev_file       (default ../data/speaker_addressee_dev.txt)
-test_file      (default ../data/speaker_addressee_test.txt)
-speakernum     (default 10000, number of distinct speakers)
-addresseenum   (default 10000, number of distinct addressees)
-speakersetting (taking values of "speaker" or "speaker_addressee". for "speaker
", only the user who speaks is modeled. for "speaker_addressee" both the speaker
 and the addressee are modeled)

   data: the first token of a source line is the index of the addressee
   and the first token in the target line is the index of the speaker. for
   example: 2 45 43 6|1 123 45 means that the index of the addressee is 2
   and the index of the speaker is 1

   to train the model
th train.lua [params]

   #adversarial

   the adversarial-id23 model and the
   adversarial-evaluation model described in [3]

   ##discriminative

   adversarial-evaluation: to train a binary evaluator (a hierarchical
   neural net) to label dialogues as machine-generated (negative) or
   human-generated (positive)

   available options are:
-batch_size     (default 128, batch size)
-dimension      (default 512, vector dimensionality)
-dropout        (default 0.2, dropout rate)
-pos_train_file     (default "../../data/t_given_s_train.txt", human generated t
raining examples)
-neg_train_file     (default "../../data/decoded_train.txt", machine generated t
raining examples)
-pos_dev_file       (default "../../data/t_given_s_dev.txt", human generated dev
 examples)
-neg_dev_file       (default "../../data/decoded_dev.txt", machine generated dev
 examples)
-pos_test_file      (default "../../data/t_given_s_test.txt", human generated te
st examples)
-neg_test_file      (default "../../data/decoded_test.txt", machine generated te
st examples)
-source_max_length      (default 50, maximum sequence length)
-dialogue_length        (default 2, the number of turns for a dialogue. the mode
l supports multi-turn dialgoue classification)
-save_model_path        (default "save", path for saving a trained discriminativ
e model)
-save_params_file       (default "save/params", path for saving input hyperparam
eters)
-savemodel              (default true, whether to save the model)

   ##reinforce

   to train the adversarial-id23 model in [3]

   available options include:
-disc_params        (default "../discriminative/save/params", hyperparameters fo
r the pre-trained discriminative model)
-disc_model         (default "../discriminative/save/iter1", path for loading a
pre-trained discriminative model)
-generate_params    (default "../../atten/save_t_given_s/params", hyperparameter
s for the pre-trained generative model)
-generate_model     (default ../../atten/save_t_given_s/model1, path for loading
 a pre-trained generative model)
-traindata      (default "../../data/t_given_s_train.txt", path for the training
 set)
-devdata        (default "../../data/t_given_s_train.txt", path for the dev set)
-testdata       (default "../../data/t_given_s_train.txt", path for the test set
)
-savefolder     (default "save", path for data saving)
-vanillareinforce       (default false, whether to use vanilla reinforce or mont
e carlo)
-montecarloexample_n    (default 5, number of tries for monte carlo search to ap
proximnate the expectation)
-baseline       (default true, whether to use baseline or not)
-baselinetype   (default "critic", taking value of either "aver" or "critic". if
 set to "critic", another neural model is trained to estimate the reward, the ro
le of which is similar to the critic in the actor-critic rl model; if set to "av
er", just use the average reward for earlier examples as a baseline")
-baseline_lr    (default 0.0005, learning rate for updating the critic)
-logfreq        (default 2000, how often to print the log and save the model)
-timeslr        (default 0.5, increasing the learning rate)
-gsteps         (default 1, how often to update the generative model)
-dsteps         (default 5, how often to update the discriminative model)
-teacherforce   (default true, whether to run the teacher forcing model)

   to run the adversarial-id23 model, a pretrained
   generative model and a pretrained discriminative model are needed.
   trained models will be saved and can be later re-loaded for decoding
   using different decoding strategies in the folder "decode".

   to train the model
th train.lua [params]

   note: if you encounter the following error "bad argument #2 to '?' (out
   of range) in function model_backward" after training the model for tens
   of hours, this means the model has exploded (see the teacher forcing
   part in section 3.2 of the paper). the reason why the error appears as
   "bad argument #2 to '?'" is because of the sampling algorithm in torch.
   if you encounter this issue, shrink the value of the variable -timeslr.

   #future_prediction

   the future prediction (soothsayer) models described in [4]

   ##train_length

   to train the soothsayer model for length prediction

   available options include:
-dimension          (default 512, vector dimensionality. the value should be the
 same as that of the pretrained id195 model. otherwise, an error will be repor
ted)
-params_file        (default "../../atten/save_t_given_s/params", load hyperpara
meters for a pre-trained generative model)
-generate_model     (default ../../atten/save_t_given_s/model1, path for loading
 the pre-trained generative model)
-save_model_path    (default "save", path for saving the model)
-train_file         (default "../../data/t_given_s_train.txt", path for the trai
ning set)
-dev_file           (default "../../data/t_given_s_dev.txt", path for the traini
ng set)
-test_file          (default "../../data/t_given_s_test.txt", path for the train
ing set)
-alpha              (default 0.0001, learning rate)
-readsequencemodel  (default true, whether to read a pretrained id195 model. t
his variable has to be set to true when training the model)
-readfuturemodel    (default false, whether to load a pretrained soothsayer mode
l. this variable has to be set to false when training the model)
-futurepredictormodelfile   (path for load a pretrained soothsayer model. does n
ot need it at model training time)

   to train the model (a pretrained id195 model is required)
th train_length.lua [params]

   ##train_backward

   train the soothsayer model to predict the backward id203 p(s|t)
   of the mutual information model

   available options include:
-dimension              (default 512, vector dimensionality. this value should b
e the same as that of the pretrained id195 model. otherwise, an error will be
reported)
-batch_size             (default 128, batch_size)
-save_model_path        (default "save")
-train_file             (default "../../data/t_given_s_train.txt", path for the
training set)
-dev_file               (default "../../data/t_given_s_dev.txt", path for the tr
aining set)
-test_file              (default "../../data/t_given_s_test.txt", path for the t
raining set)
-alpha                  (default 0.01, learning rate)
-forward_params_file(default "../../atten/save_t_given_s/params",input parameter
 files for a pre-trained id195model p(t|s))
-forward_model_file     (default "../../atten/save_s_given_t/model1", path for l
oading the pre-trained id195 model p(t|s))
-backward_params_file   (default "../../atten/save_s_given_t/params",input param
eter files for a pre-trained backward id195 model p(s|t))
-backward_model_file    (default "../../atten/save_s_given_t/model1" path for lo
ading the pre-trained backward id195 model p(s|t))
-readsequencemodel      (default true, whether to read a pretrained id195 mode
l. this variable has to be set to true when during the model training period)
-readfuturemodel        (default false, whether to load a pretrained soothsayer
model. this variable has to be set to false during the model training period)
-predictorfile          (path for load a pretrained soothsayer model. does not n
eed it at model training time)

   to train the model (a pretrained forward id195 model p(t|s) and a
   backward model p(s|t) are both required)
th train.lua [params]

   ##decode

   decoding by combining a pre-trained id195 model and a soothsayer
   future prediction model

   other than the input parameters of the standard decoding model in the
   folder "decode", additional options include:
-task                   (the future prediction task, taking values of "length" o
r "backward")
-target_length          (default 0, forcing the model to generate sequences of a
 pre-specific length. 0 if there is no such a constraint. if your task is "lengt
h", a value for -target_length is required)
-futurepredictormodelfile   (path for loading a pre-trained soothsayer future pr
ediction model. if "task" takes a value of "length", the value of futurepredicto
rmodelfile should be a model saved from training length prediction model in fold
er train_length. if "task" takes a value of "backward", the model is a model sav
ed from training the backward id203 model in the folder train_backward)
-predictorweight        (default 0, the weight for the soothsayer model)

   to run the decoder with a pre-trained soothsayer model of length:
th decode.lua -params_file hyperparameterfile_pretrained_id195 -model_file mod
elfile_pretrained_id195 -inputfile yourinputfiletodecode -outputfile youroutpu
tfile -futurepredictormodelfile modelfile_soothsayer_length -predictorweight 1 -
task length -target_length 15

   to run the decoder with a pre-trained soothsayer model of backward
   id203
th decode.lua -params_file hyperparameterfile_pretrained_id195 -model_file mod
elfile_pretrained_id195 -inputfile yourinputfiletodecode -outputfile youroutpu
tfile -futurepredictormodelfile modelfile_soothsayer_backward -predictorweight 1
 -task backward

   if you want to perform mmi reranking at the end, -mmi_params_file and
   -mmi_model_file have to be pre-specified

   #distill

   this folder contains the code for the data distillation method
   described in [6].

   to run the model:
sh pipeline.sh

     * first, decode a large input set (more than 1 million) using a
       pre-trained id195 model
       cd ../decode
       th decode.lua -params_file hyperparameterfile_pretrained_id195
       -model_file modelfile_pretrained_id195 -batch_size 640 -inputfile
       yourtrainingdata -outputfile yourdecodingoutputfile -batch_size
       -max_decoded_num 1000000
     * second, extract top frequent responses
       cd ../distill/extract_top
       sh select_top_decoded.sh yourdecodingoutputfile
       yourfiletostoretopresponses
     * third, compute relevance scores for the entire training set and
       then distill the training set. the code provides two different ways
       to compute the scores: using a pre-trained id195 model or
       averaging glove embeddings
       cd ../glove or cd ../encoder

   ##glove

   options include
-trainingdata       (path for your training data to distill)
-topresponsefile    (path for your extracted top frequent responses)
-batch_size         (default 1280, batch size)
-save_score_file    (default "relevance_score", path for saving relevance_score
for each instance in the training set)
-distill_rate       (default 0.08, the proportion of training data to distill in
 this round)
-distill_four_gram  (default true, whether to remove all training instances that
 share four-grams with any one of the top frequent responses)
-loadscore          (default false, whether to load already-computed relevance s
cores)
-save_score         (default false, wehther to save relevance scores)

   compute relevance scores:
th run.lua -topresponsefile yourfiletostoretopresponses -trainingdata yourtraini
ngdata -outputfile fileforremainingdata -save_score -save_score_file relevance_s
core

   distill the data:
th run.lua -topresponsefile yourfiletostoretopresponses -trainingdata yourtraini
ngdata -outputfile fileforremainingdata -total_lines "number of lines in yourtra
iningdata" -save_score_file relevance_score

   the remaining data after this round of data distillation will be stored
   in fileforremainingdata, on which a new id195 model will be trained.

   ##encoder use a pre-trained id195 model for data distillation. other
   than input parameters in glove, the path for a pre-trained id195
   model needs to be pre-specified
    -params_file        (default "../../atten/save_t_given_s/params", hyperparam
eters for the pre-trained generative model)
    -model_file     (default ../../atten/save_t_given_s/model1, path for loading
 a pre-trained generative model)

   to run the model:
th distill_encode.lua -topresponsefile yourfiletostoretopresponses -trainingdata
 yourtrainingdata -outputfile fileforremainingdata -params_file id195paramsfil
e -model_file id195modelfile -batch_size 6400

acknowledgments

   [101]yoon kim's [102]mt repo

   lantaoyu's [103]seqgan repo

licence

   mit

     *    2019 github, inc.
     * [104]terms
     * [105]privacy
     * [106]security
     * [107]status
     * [108]help

     * [109]contact github
     * [110]pricing
     * [111]api
     * [112]training
     * [113]blog
     * [114]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [115]reload to refresh your
   session. you signed out in another tab or window. [116]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/jiweil/neural-dialogue-generation/commits/master.atom
   3. https://github.com/jiweil/neural-dialogue-generation#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/jiweil/neural-dialogue-generation
  32. https://github.com/join
  33. https://github.com/login?return_to=/jiweil/neural-dialogue-generation
  34. https://github.com/jiweil/neural-dialogue-generation/watchers
  35. https://github.com/login?return_to=/jiweil/neural-dialogue-generation
  36. https://github.com/jiweil/neural-dialogue-generation/stargazers
  37. https://github.com/login?return_to=/jiweil/neural-dialogue-generation
  38. https://github.com/jiweil/neural-dialogue-generation/network/members
  39. https://github.com/jiweil
  40. https://github.com/jiweil/neural-dialogue-generation
  41. https://github.com/jiweil/neural-dialogue-generation
  42. https://github.com/jiweil/neural-dialogue-generation/issues
  43. https://github.com/jiweil/neural-dialogue-generation/pulls
  44. https://github.com/jiweil/neural-dialogue-generation/projects
  45. https://github.com/jiweil/neural-dialogue-generation/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/jiweil/neural-dialogue-generation/commits/master
  48. https://github.com/jiweil/neural-dialogue-generation/branches
  49. https://github.com/jiweil/neural-dialogue-generation/releases
  50. https://github.com/jiweil/neural-dialogue-generation/graphs/contributors
  51. https://github.com/jiweil/neural-dialogue-generation/blob/master/license
  52. https://github.com/jiweil/neural-dialogue-generation/search?l=lua
  53. https://github.com/jiweil/neural-dialogue-generation/search?l=shell
  54. https://github.com/jiweil/neural-dialogue-generation/search?l=python
  55. https://github.com/jiweil/neural-dialogue-generation/find/master
  56. https://github.com/jiweil/neural-dialogue-generation/archive/master.zip
  57. https://github.com/login?return_to=https://github.com/jiweil/neural-dialogue-generation
  58. https://github.com/join?return_to=/jiweil/neural-dialogue-generation
  59. https://desktop.github.com/
  60. https://desktop.github.com/
  61. https://developer.apple.com/xcode/
  62. https://visualstudio.github.com/
  63. https://github.com/jiweil/neural-dialogue-generation/commit/87c698c60ae486a54cf714996800a054eeaab132
  64. https://github.com/jiweil/neural-dialogue-generation/commit/87c698c60ae486a54cf714996800a054eeaab132
  65. https://github.com/jiweil/neural-dialogue-generation/tree/87c698c60ae486a54cf714996800a054eeaab132
  66. https://github.com/jiweil/neural-dialogue-generation/tree/master/adversarial
  67. https://github.com/jiweil/neural-dialogue-generation/commit/f188277a7386a9a71d4934f7deebabc55d6daab4
  68. https://github.com/jiweil/neural-dialogue-generation/tree/master/atten
  69. https://github.com/jiweil/neural-dialogue-generation/commit/c8854d670f5b9da4a23decebf46830ef753c3ead
  70. https://github.com/jiweil/neural-dialogue-generation/tree/master/decode
  71. https://github.com/jiweil/neural-dialogue-generation/commit/a54501185cfc59be7316b19804180048b61c07ec
  72. https://github.com/jiweil/neural-dialogue-generation/tree/master/distill
  73. https://github.com/jiweil/neural-dialogue-generation/tree/master/future_prediction
  74. https://github.com/jiweil/neural-dialogue-generation/tree/master/persona_addressee
  75. https://github.com/jiweil/neural-dialogue-generation/tree/master/data
  76. https://github.com/jiweil/neural-dialogue-generation/blob/master/license
  77. https://github.com/jiweil/neural-dialogue-generation/commit/50d2d01046d7e61aa51e074c70a35fa0fb3eb524
  78. https://github.com/jiweil/neural-dialogue-generation/blob/master/readme.md
  79. https://arxiv.org/pdf/1510.03055.pdf
  80. https://arxiv.org/pdf/1603.06155.pdf
  81. https://arxiv.org/pdf/1701.06547.pdf
  82. https://arxiv.org/pdf/1701.06549.pdf
  83. https://arxiv.org/pdf/1611.08562.pdf
  84. http://www.stanford.edu/~jiweil/
  85. mailto:jiweil@stanford.edu
  86. http://stanford.edu/~wmonroe4/
  87. https://www.microsoft.com/en-us/research/people/mgalley/
  88. http://aritter.github.io/
  89. http://www.timshi.xyz/home/index.html
  90. http://research.microsoft.com/en-us/um/people/jfgao/
  91. https://www.microsoft.com/en-us/research/people/chrisbkt/
  92. https://www.microsoft.com/en-us/research/people/billdol/
  93. https://web.stanford.edu/~jurafsky/
  94. https://github.com/facebook/fbtorch
  95. https://github.com/torch/cutorch
  96. https://github.com/torch/cunn
  97. https://github.com/torch/nngraph
  98. https://github.com/nicholas-leonard/torchx
  99. https://github.com/torch/tds
 100. http://nlp.stanford.edu/data/opensubdata.tar
 101. http://people.fas.harvard.edu/~yoonkim
 102. https://github.com/harvardnlp/id195-attn
 103. https://github.com/lantaoyu/seqgan
 104. https://github.com/site/terms
 105. https://github.com/site/privacy
 106. https://github.com/security
 107. https://githubstatus.com/
 108. https://help.github.com/
 109. https://github.com/contact
 110. https://github.com/pricing
 111. https://developer.github.com/
 112. https://training.github.com/
 113. https://github.blog/
 114. https://github.com/about
 115. https://github.com/jiweil/neural-dialogue-generation
 116. https://github.com/jiweil/neural-dialogue-generation

   hidden links:
 118. https://github.com/
 119. https://github.com/jiweil/neural-dialogue-generation
 120. https://github.com/jiweil/neural-dialogue-generation
 121. https://github.com/jiweil/neural-dialogue-generation
 122. https://help.github.com/articles/which-remote-url-should-i-use
 123. https://github.com/jiweil/neural-dialogue-generation#acknowledgments
 124. https://github.com/jiweil/neural-dialogue-generation#licence
 125. https://github.com/
