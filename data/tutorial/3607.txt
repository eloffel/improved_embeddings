   [1]

linkedin

     * [2]sign in
     * [3]join now

              what i learned from deep learning summer school 2016

   published on august 20, 2016august 20, 2016     641 likes     30 comments

   [4]hamid palangi

[5]hamid palangi[6]follow

researcher at microsoft research ai

     * (button) like641
     * (button) comment30
     * [ ] share
          + (button) linkedin
          + (button) facebook
          + (button) twitter
       6

   two weeks ago i attended the [7]deep learning summer school at montreal
   organized by yoshua bengio and aaron courville. below is a summary of
   what i learned. it starts from basic concepts and continues with more
   advanced topics.

  1. essence of id173

   two popular id173s that are used in machine learning / deep
   learning are l2 (keeps l2 norm of the weights bounded, results in
   non-sparse set of weights, i.e., the weight of irrelevant features are
   small but not zero) and l1 (results in sparse set of weights,
   computationally more expensive than l2). they help to adjust the
   hypothesis complexity, e.g., if hypothesis has high variance
   (overfitting), they can help to alleviate the problem. from a bayesian
   point of view, l2 id173 is equivalent to a circular gaussian
   prior for weights. l1 id173 is equivalent to a double
   exponential prior. note that the id173 is only applied on
   weights not biases. other popular id173 techniques that help
   better generalization are dropout [hinton et al, jmlr 2014], or using
   unsupervised training for initialization of supervised training, e.g.,
   using rbms to initialize autoencoder's weights as explained in [hinton
   & salakhutdinov, science 2006]. usually in practice, using a large
   model with id173 (e.g., injecting noise) works better than
   using a small fully parametric model without id173.

  2. why do we need more than one neuron?

   a single neuron can only solve a linear separable problem, e.g., and
   operation. it can not solve a non-linear separable problem, e.g., xor
   operation. nevertheless, if we use a better representation of the input
   data it can solve the xor operation. for example, by using a non-linear
   transformation of the input data, if inputs are x1 and x2, xor( y1, y2)
   can be done using a single neuron if y1 = and(not(x1),x2) and y2 =
   and(x1,not(x2)).

  3. what non-linearity to choose for neurons?

   the rule of thumb to select non-linearity is to always start with relu
   (rectified linear unit). it leads to less computational complexity for
   id26 and usually results in sparse activations for neurons.
   the non-differentiable point at 0 in relu is not a problem
   (sub-gradients can address this problem). question: is it a good idea
   to use different non-linearities in different layers? no success yet.
   except if we want to put some structure in the output, e.g., the
   attention mechanism.

  4. practical tips to train a neural network

     * initialization: to break symmetry we use random initialization, for
       example see [glorot & bengio, 2010].
     * hyper-parameter selection: (a): using grid search, i.e., trying all
       possible configurations of hyper-parameters. this is
       computationally expensive. (b): using random search [bergstra &
       bengio, 2012], i.e., specify a distribution over the values of each
       hyper-parameter and then sampling from each of them independently.
       (c): bayesian optimization [snoek, et al, nips 2012] which requires
       less number of guesses to get hyper-parameters.
     * early stopping: since it has zero cost, it is better to always do
       it.
     * validation set choice: this can become very important. the
       validation set size should be large enough so that the model does
       not overfit on the validation set. this type of overfitting also
       depends on how many validation tests we run on the validation set.
     * id172: for real valued data, id172 speeds up the
       training.
     * learning rate: starting with a large learning rate and then
       decaying it or using methods with adaptive learning rates like
       adagrad, rmsprop or adam.
     * gradient check: very helpful for debugging the implementation of
       backprop. we simply compare the gradient with a finite difference
       approximation of it. question: can the finite difference
       approximation of the gradient replace backprop? no, because it is
       less numerically stable.
     * always make sure the model overfits on a small dataset.
     * what to do if training is hard?: first, make sure id26
       implementation is not buggy and the learning rate is not too large.
       then, if it is underfitting, use better optimization methods,
       larger models, etc . if it is overfitting, use better
       id173, e.g., unsupervised initialization, dropout, etc.
     * batch id172 [loffe & szegedy, jmlr 2015]: very helpful
       technique, which shows that the id172 at higher
       layers further improves the performance. it can be done in 4 steps:
       (a): doing id172 for each hidden layer before applying
       non-linearity. (b): during training, mean and standard deviation
       are computed for each minibatch. (c): during id26, we
       should take into account the id172 during forward pass. in
       other words, a scale and shift operation should be performed during
       id26. scale and shift parameters should also be learned
       because derivative with respect to hidden layers will also depend
       on them. (d): at the test time, global mean and standard deviation
       is used not the ones calculated for each minibatch.

  5. how important is depth?

   nicely explained by rob fergus. we can investigate the importance of
   depth by inspecting different parts of krizhevsky's convolutional
   neural network (id98) which has 8 layers and is trained on id163. the
   architecture of krizhevsky's id98 [krizhevsky et al, nips 2012] along
   with the results of applying id166 on different layers are shown below
   [[8]picture from rob fergus presentation]:

   [:0]

   another important observation is that if we remove layers 3, 4
   (convolutional layers) and 6, 7 (fully connected layers), the
   performance drops 33.5%.

   it is important to note that simply adding many more layers does not
   always improve the performance. for example, results of simply using 20
   layers and 56 layers of cifar-10 are shown below [picture from he et
   al, cvpr 2016]:

   [:0]

   similar phenomena has been observed on id163 which means that
   learning better models is not always equivalent to adding more layers.
   note that above problem is not caused by overfitting as it is obvious
   from training error curves above. one reason might be the fact that
   with deeper networks the error signal during id26 is not
   significant enough when it arrives at lower layers. to resolve this
   problem, residual network is proposed in [he et al, cvpr 2016] which
   simply adds skip connections in id98 architecture. one example is shown
   below [picture from he et al, cvpr 2016]:

   [:0]

   note that the skip connection is applied before the non-linear
   activation function.

  6. which one is more important, designing a better feature extractor below,
  or, designing a better classifier on the top?

   using a powerful feature extractor (e.g., a id98 or deep residual
   network for vision tasks) is far more important than designing the
   classifier on the top.

  7. evolution of image databases to big data

   below is a summary of image databases from 1970 till now [[9]picture
   from antonio torralba presentation]:

   [:0]

  8. convolutional id3

   assume that we want to find a generative model that can generate
   data similar to the samples that we have in our dataset. for example,
   we want to build a generative model that can generate images similar to
   those in mnist or cifar dataset. generally, this is a very difficult
   task because of many intractable probabilistic computations involved in
   maximum likelihood or other related methods for this
   task. one elegant idea for this task is id3
   (gans) proposed by [goodfellow et al, nips 2014]. in gans, two models
   are simultaneously trained, a generative model (g) and a discriminative
   model (d). g generates an image, and d is a binary classifier that
   classifies the given image to be a sample from dataset (true data), or
   a sample generated by g (artificially generated data). g is trained to
   maximize the id203 that d makes a mistake (min-max two player
   game). as a result, after training, g estimates the distribution of the
   data. some sample images generated by g for mnist and cifar-10 from
   [goodfellow et al, nips 2014] are represented below (picture from
   [goodfellow et al, nips 2014]):

   [:0]

   in [radford et al, iclr 2016] a form of convolutional network is
   proposed which is more stable with adversarial training than other
   methods. other related references for gans are "adversarial examples in
   the physical world ([10]http://arxiv.org/abs/1607.02533)", "improved
   techniques for training gans
   ([11]http://arxiv.org/abs/1606.03498)", "virtual adversarial training
   for semi-supervised text classification
   ([12]http://arxiv.org/abs/1605.07725)". they have even been used to
   generate new pokemon go species!
   ([13]https://www.youtube.com/watch?v=rs3ai7bacgc).

  9. which deep learning toolkit to use?

   there is no silver bullet! it depends on the target task and
   application. below is a comparison from [14]alex wiltschko
   presentation:

   [:0]

   there is also a great comparison among caffe, cntk, tensorflow, theano
   and torch with much more details in [15]this post by kenneth tran.

  10. what are the new advances in recurrent neural networks research?

   recurrent neural networks (mainly lstms and grus) have been
   significantly successful recently mainly used for converting sequence
   to vector (e.g., [16]sentence embedding [palangi et al, 2015]),
   sequence to sequence (e.g., [17]machine translation [sutskever et al,
   2014], [18][bahdanau et al, 2014]) and vector to sequence (e.g.,
   [19]image captioning [vinyals et al, 2014]). vanilla id56s have not been
   as successful to capture long term dependencies due to
   vanishing/exploding gradient problems. nevertheless, in the limit of
   infinite time training (which is not practical), vanilla id56 will
   eventually learn long term dependencies. below are a list of recent
   works related to id56s which got my attention during [20]yoshua bengio's
   presentation about id56s:

   (a): assume that we want to train a neural language model using lstm.
   the basic task is to predict the next word given previous words for
   which we minimize the perplexity as cost function. during training, we
   give all "true" previous words to the model and use them to predict the
   next word. but during id136, we give all "predicted" previous words
   to the model and use them to predict the next word. to resolve this
   incompatibility between training and id136, a method is proposed in
   [[21]bengio et al, 2015] where during training, a weak supervision from
   previously generated words by the model is also used. this results in
   significant performance improvement.

   (b): multiplicative integration with id56s proposed in [[22]wu et al,
   2016]. the main idea is to replace the summation with hadamard product
   in id56s. this simple modification results in significant performance
   improvement presented in above reference.

   (c): how to understand and measure the architectural complexity of a
   given id56 model? in [[23]zhang et al, 2016], three measures are
   proposed which are: (c.1): recurrent depth (length of longest path
   divided by sequence length), (c.2): feedforward depth (length of
   longest path from input to nearest output) and (c.3): skip coefficient
   (length of shortest path divided by sequence length).

   (d): pixel id56s (icml 2016 best paper award) [[24]oord et al, 2016]:
   this work proposes a method to model the id203 distribution of a
   natural image. the main idea is to factorize the id203
   distribution of the input image into the product of conditional
   probabilities. to do this, a diagonal bilstm unit is proposed that
   efficiently captures the entire available context (all the pixels above
   the current pixel) of the image (see fig. 2 of the paper). residual
   skip connections are also used in the architecture. it has resulted in
   state-of-the-art performance in terms of log-likelihood. below are a
   number of natural images generated by the model trained on id163
   [picture from [25]oord et al, 2016]:

   [:0]

  11. can all problems be mapped to y=f(x)?

   no! example tasks which the simple y=f(x) fails are: (a): cloze style
   qa where the task is to read and comprehend a text (e.g., book, etc)
   and then answer questions about it. (b): given a text, the task is to
   fill in the blanks. (c): chatbot.

   as explained nicely in [26]sumit chopra's presentation, the model needs
   to: (a): remember the external context. (b): given an input, the model
   needs to know where to look for in the context. (c): what to look for
   in the context. (d): how to reason, using this external context. (e):
   the model should also handle a changing external context.

   therefore, introducing a notion of memory to capture external context
   is important. one proposal is to use hidden states of id56s as memory.
   for example, running an id56 on the context (book, text, etc) to get its
   representation, then, using this representation to map a question to
   answer. there are two problems with this approach: (a): it does not
   scale. (b) the idea that hidden states of an id56 are both the memory
   and the controller of the memory is not appropriate. we should separate
   these two.

   the main idea of a memory network [[27]weston et al, 2015] is to
   separate the controller of the memory from the memory itself. in other
   words, it combines a large memory with a learning component that can
   read and write to the memory.

   memory networks perform better than lstms in qa task but the
   performance of both of them are close in language modelling task. one
   reason might be the fact that for language modelling task we do not
   need very long term dependencies compared to qa and dialogue related
   tasks. one shortcoming of current memory networks is that there is no
   memory compression. if the memory is full, they simply recycle.

  12. large scale deep learning with tensorflow presented by jeff dean

   generally, the important features that are desirable in a machine
   learning system are ([28]from jeff dean's presentation): (a): ease of
   expression: for many machine learning algorithms. (b): scalability: to
   be able to run experiments quickly. (c): portability: so that we can
   run experiments on various platforms. (d): reproducability: which helps
   to share and reproduce research. (e): production readiness: from
   research to real products.

   tensorflow (tf) have been designed with careful consideration to above
   features. other notes about tf are: (a): the core of tf is c++ which
   results in very low overhead. (b): tf system automatically decides
   which operations should be run on cpu or gpu. this usually helps to
   significantly improve the time of experiments. (c):  the first version
   of scalable deep learning system at google, i.e., distbelief [[29]dean
   et al, nips 2012] is not as flexible as tf for research purposes.
   distbelief has separate parameter servers, i.e., separate code for
   parameter servers v.s. rest of the system, which results in a
   non-uniform and more complicated system. (d): tf session interface
   allows to "extend" which can be used to add nodes to the computation
   graph and "run" which in addition to running the full computation graph
   can also be used to run an arbitrary subgraph of the computation graph.
   (e): question: how does tf make distributed training easy? it uses
   model parallelism (partitioning model across machines) and data
   parallelism. it is easy to express both types of parallelisms in tf
   with minimal changes to single device model code. (f): tf can take care
   of devices / graph placement. in other words, given a computation graph
   and a set of devices, tf allows the user to decide which device
   executes each node.

  13. history of statistical language modelling?

   statistical language modelling is all about how probable a sentence is.
   we generally maximize the log probabilities of sentences in the
   corpora. this, however, has not been obvious for everyone in 90s
   (review of [30]brown et al, 1990 paper) [from [31]kyunghyun cho's
   presentation]:

   [:0]

   which reads: "the validity of statistical (information theoretic)
   approach to mt has indeed been recognized ... as early as 1949. and was
   universally recognized as mistaken [sic] by 1950 ... the crude force of
   computers is not science."

  14. what are the issues with non-parametric language modelling (e.g.,
  id165s)?

   in id165 language modelling, we basically collect id165 statistics
   from a large corpus (i.e., counting). some issues with this approach
   are: (a): false conditional independence assumption: because in an
   id165 language model we assume that each word is only conditioned on
   the previous n-1 words. (b): data sparsity: which means that if a
   co-occurrence of some words has never been observed in the training
   set, it will be assigned zero id203 which results in the
   id203 of whole sentence to be zero. conventional solutions for
   this problem are id188. (c): lack of generalization
   across domains.

   as an example, an id165 language model might fail in the sentence "the
   dogs chasing the cat bark". the tri-gram id203 p(bark | the, cat)
   is very low (not observed in a natural language corpus by the model,
   because the cat never barks and the plural verb "bark" has appeared
   after singular noun "cat"), but the whole sentence totally makes
   sentence.

  15. parametric and neural language modelling

   the basic idea of a neural language model is to create continuous space
   word representations and use them for language modelling. for example,
   in [[32]bengio et al 2003], a feedforward neural network with a softmax
   layer on the top is used to for language modelling represented below
   (picture from [33]kyunghyun cho's presentation):

   [:0]

   a better choice for neural language modelling are id56s (lstms, grus,
   ...) or memory networks which have resulted in state-of-the art
   performance in terms of perplexity. for example, see the paper
   [34]"exploring the limits of language modelling" by jozefowicz et al,
   2016. a simple example of an unfolded vanilla id56 language model is
   represented below where the model reads the input word, updates the
   hidden states representations and predicts the next word (picture from
   [35]kyunghyun cho's presentation):

   [:0]

  16. character-level id4

   the task in machine translation is to generate a sentence in target
   language, given a sentence in source language. in neural machine
   translation (id4), an id56 (lstm, gru, etc) is used to encode the source
   sentence into a vector, and another id56 is used to decode the vector
   from encoder into a sequence of words in target language (sequence to
   sequence learning). this is shown in the following diagram (picture
   from [36]kyunghyun cho's presentation):

   [:0]

   above model can be improved if we use an attention based decoder
   [[37]bahdanau et al, iclr 2015]. the idea is to compute a set of
   attention weights and use weighted sum of encoder's annotation vectors
   in the decoder. this approach, allows decoder to automatically just
   focus on the parts of the source sentence that are relevant for
   predicting each target word. it is shown in the following diagram
   (picture from [38]kyunghyun cho's presentation):

   [:0]

   the main issue with above models is that they use words as basic units
   of language. for example, "run", "runs", "ran" and "running" are from
   one lexeme "run". but above models assign them four independent
   vectors. it is also not always easy to segment a sentence into words.
   the question is, can we use character level id4 to address above
   issues? in [[39]chung et al, 2016], it is shown that character level
   id4 works surprisingly well. it is also interesting to note that an
   id56, implicitly segments a character sequence automatically. for
   example, see the demonstration below (from [40]kyunghyun cho's
   presentation):

   [:0]

  17. why generative models?

   nicely explained in [41]shakir mohamed's presentation, we need
   generative models for moving beyond associating inputs to
   outputs, semi-supervised classification, data manipulation, filling in
   the blank, inpainting, denoising, one-shot generalization [[42]rezende
   et al, icml 2016] and many more applications. progress in generative
   models is presented in the following diagram (note that the vertical
   axis should be negative log-likelihood) [from [43]shakir mohamed's
   presentation]:

   [:0]

  18. what are different types of generative models?

   generative models can be classified into three groups:

   (a): fully observed models: model directly observes data without
   introducing any new unobserved local variable. these types of models
   can directly encode the relationship among observed points. for
   directed id114, it is easy to scale up to large models and
   the parameter learning is simple because log-likelihood can be computed
   directly (no need for approximation). for undirected models, the
   parameter learning is difficult as we need to compute id172
   constants. generation in fully observed models can be slow. below
   diagram shows different fully observed generative models [from
   [44]shakir mohamed's presentation]:

   [:0]

   (b): transformation models: model transforms an unobserved noise source
   using a parameterised function. it is easy to (1): sample from these
   models and (2): compute expectations without knowing the final
   distribution. they can be used with large scale classifiers and
   convolutional neural networks. nevertheless, it is difficult to
   maintain invertibility and extend to generic data types using these
   models. below diagram shows different transformation generative models
   [from [45]shakir mohamed's presentation]:

   [:0]

   (c): latent variable models: in these models, an unobserved local
   random variable is introduced that represents hidden causes. it is easy
   to sample from these models and to include hierarchy and depth. it is
   also possible to do scoring and model selection using marginalized
   likelihood. nevertheless, it is difficult to determine latent variables
   corresponding to an input. below diagram shows different latent
   variable generative models [from [46]shakir mohamed's presentation]:

   [:0]

   [47]hamid palangi

[48]hamid palangi

researcher at microsoft research ai

   [49]follow

   30 comments
   article-comment__guest-image
   [50]sign in to leave your comment
   show more comments.
     __________________________________________________________________

more from hamid palangi

   [51]1 article

     *    2019
     * [52]about
     * [53]user agreement
     * [54]privacy policy
     * [55]cookie policy
     * [56]copyright policy
     * [57]brand policy
     * [58]manage subscription
     * [59]community guidelines
     * [ ]
          + (button) bahasa indonesia
          + (button) bahasa malaysia
          + (button)   e  tina
          + (button) dansk
          + (button) deutsch
          + (button) english
          + (button) espa  ol
          + (button)             
          + (button) fran  ais
          + (button)          
          + (button) italiano
          + (button)             
          + (button) nederlands
          + (button)          
          + (button) norsk
          + (button) polski
          + (button) portugu  s
          + (button) rom  n  
          + (button)               
          + (button) svenska
          + (button) tagalog
          + (button)                      
          + (button) t  rk  e
          + (button)               
       languagelanguage

references

   1. https://www.linkedin.com/?trk=header_logo
   2. https://www.linkedin.com/uas/login?trk=header_signin
   3. https://www.linkedin.com/start/join?trk=header_join
   4. https://www.linkedin.com/in/hamidpalangi?trk=author_mini-profile_image
   5. https://www.linkedin.com/in/hamidpalangi?trk=author_mini-profile_title
   6. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/what-i-learned-from-deep-learning-summer-school-2016-hamid-palangi&trk=author-info__follow-button
   7. https://sites.google.com/site/deeplearningsummerschool2016/home
   8. https://drive.google.com/file/d/0b_wzp_jlvfckrkhra0dzuedklwc/view?usp=sharing
   9. http://people.csail.mit.edu/torralba/talks/2016_torralba.pdf
  10. http://arxiv.org/abs/1607.02533),
  11. http://arxiv.org/abs/1606.03498),
  12. http://arxiv.org/abs/1605.07725)
  13. https://www.youtube.com/watch?v=rs3ai7bacgc)
  14. https://github.com/mila-udem/summerschool2016/blob/master/torch/neural networks with torch (mila).pdf
  15. https://github.com/zer0n/deepframeworks
  16. https://arxiv.org/abs/1502.06922
  17. http://arxiv.org/abs/1409.3215
  18. https://arxiv.org/abs/1409.0473
  19. https://arxiv.org/abs/1411.4555
  20. https://drive.google.com/file/d/0b_wzp_jlvfckdlhtug9kmtvjlvnhmljkynjybc1bcdjevngw/view?usp=sharing
  21. http://arxiv.org/abs/1506.03099
  22. https://arxiv.org/abs/1606.06630
  23. http://arxiv.org/abs/1602.08210
  24. https://arxiv.org/abs/1601.06759
  25. https://arxiv.org/abs/1601.06759
  26. https://drive.google.com/open?id=0b_wzp_jlvfckbhdpyvdzmjg3etbqd2f1og9qzlvhogjox0dz
  27. http://arxiv.org/abs/1410.3916
  28. https://drive.google.com/open?id=0b_wzp_jlvfcks2lydm5jdv9kmk1yuenoala5tg5pv0lqws1v
  29. http://research.google.com/pubs/pub40565.html
  30. http://dl.acm.org/citation.cfm?id=92860
  31. https://drive.google.com/open?id=0b_wzp_jlvfckc0xrmjrhru9dn2jrqlb0tjdkdmplz0fsatfz
  32. http://dl.acm.org/citation.cfm?id=944966
  33. https://drive.google.com/open?id=0b_wzp_jlvfckc0xrmjrhru9dn2jrqlb0tjdkdmplz0fsatfz
  34. https://arxiv.org/abs/1602.02410
  35. https://drive.google.com/open?id=0b_wzp_jlvfckc0xrmjrhru9dn2jrqlb0tjdkdmplz0fsatfz
  36. https://drive.google.com/open?id=0b_wzp_jlvfckc0xrmjrhru9dn2jrqlb0tjdkdmplz0fsatfz
  37. https://arxiv.org/abs/1409.0473
  38. https://drive.google.com/open?id=0b_wzp_jlvfckc0xrmjrhru9dn2jrqlb0tjdkdmplz0fsatfz
  39. http://arxiv.org/abs/1603.06147
  40. https://drive.google.com/open?id=0b_wzp_jlvfckc0xrmjrhru9dn2jrqlb0tjdkdmplz0fsatfz
  41. https://drive.google.com/open?id=0b_wzp_jlvfckmnfmnnatytvkv28
  42. https://arxiv.org/abs/1603.05106
  43. https://drive.google.com/open?id=0b_wzp_jlvfckmnfmnnatytvkv28
  44. https://drive.google.com/open?id=0b_wzp_jlvfckmnfmnnatytvkv28
  45. https://drive.google.com/open?id=0b_wzp_jlvfckmnfmnnatytvkv28
  46. https://drive.google.com/open?id=0b_wzp_jlvfckmnfmnnatytvkv28
  47. https://www.linkedin.com/in/hamidpalangi?trk=author_mini-profile_image
  48. https://www.linkedin.com/in/hamidpalangi?trk=author_mini-profile_title
  49. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/what-i-learned-from-deep-learning-summer-school-2016-hamid-palangi&trk=author-info__follow-button-bottom
  50. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/what-i-learned-from-deep-learning-summer-school-2016-hamid-palangi&trk=article-reader_leave-comment
  51. https://www.linkedin.com/today/author/hamidpalangi
  52. https://press.linkedin.com/about-linkedin?trk=article_reader_footer_footer-about
  53. https://www.linkedin.com/legal/user-agreement?trk=article_reader_footer_footer-user-agreement
  54. https://www.linkedin.com/legal/privacy-policy?trk=article_reader_footer_footer-privacy-policy
  55. https://www.linkedin.com/legal/cookie-policy?trk=article_reader_footer_footer-cookie-policy
  56. https://www.linkedin.com/legal/copyright-policy?trk=article_reader_footer_footer-copyright-policy
  57. https://brand.linkedin.com/policies?trk=article_reader_footer_footer-brand-policy
  58. https://www.linkedin.com/psettings/guest-controls?trk=article_reader_footer_footer-manage-sub
  59. https://www.linkedin.com/help/linkedin/answer/34593?lang=en&trk=article_reader_footer_footer-community-guide
