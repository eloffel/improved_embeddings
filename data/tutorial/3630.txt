   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]all of us are belong to machines
   [7]all of us are belong to machines
   [8]sign in[9]get started
     __________________________________________________________________

gentlest introduction to tensorflow #1

   [10]go to the profile of soon hin khor
   [11]soon hin khor (button) blockedunblock (button) followfollowing
   apr 18, 2016

   summary: tensorflow (tf) is google   s attempt to put the power of deep
   learning into the hands of developers around the world. it comes with a
   [12]beginner & an [13]advanced tutorial, as well as a course on
   [14]udacity. however, the materials attempt to introduce both ml and tf
   concurrently to solve a multi-feature problem         character recognition,
   which albeit interesting, unnecessarily convolutes understanding. in
   this series of articles, we present the gentlest introduction to tf
   that starts off by showing how to do id75 for a single
   feature problem, and expand from there.

   this is part of a series:
     * part 1 (this article): id75 with tensorflow for single
       feature single outcome model
     * [15]part 2: tensorflow training illustrated in diagrams/code, and
       exploring training variations
     * [16]part 3: matrices and multi-feature id75 with
       tensorflow
     * [17]part 4: id28 with tensorflow

introduction

   we are going to solve an overly simple, and unrealistic problem, which
   has the upside of making understanding the concepts of ml and tf easy.
   we want to predict a single scalar outcome, house price (in $) based on
   a single feature, house size (in square meters, sqm). this eradicates
   the need to handle multi-dimensional data, enabling us to focus solely
   on defining a model, implementing, and training it in tf.

machine learning (ml) in brief

   we start with a set of data points that we have collected (chart
   below), each representing the relationship between two values    an
   outcome (house price) and the influencing feature (house size).
   [1*wcivd-w2dnhr7l3jukwbhq.png]

   however, we cannot predict values for features that we don   t have data
   points for (chart below)
   [1*gh-vc3hdd01ufjjjcvmqla.png]

   we can use ml to discover the relationship (the    best-fit prediction
   line    in the chart below), such that given a feature value that is not
   part of the data points, we can predict the outcome accurately (the
   intersection between the feature value and the prediction line.
   [1*lmik7uyrhz4obi2fwx_75q.png]

step 1: choose a model

model types

   to do prediction using ml, we need to choose a model that can best-fit
   the data that we have collected.

   we can choose a linear (straight line) model, and tweak it to match the
   data points by changing its steepness/gradient and position.
   [1*i8a-advmchtek5y9mwiima.png]

   we can also choose an exponential (curve) model, and tweak it to match
   the same set of data points by changing its curvature and position.
   [1*9aam2_ruemukknrdoidxfw.png]

cost function

   to compare which model is a better-fit more rigorously, we define
   best-fit mathematically as a cost function that we need to minimize. an
   example of a cost function can simply be the absolute sum of the
   differences between the actual outcome represented by each data point,
   and the prediction of the outcome (the vertical projection of the
   actual outcome onto the best-fit line). graphically the cost is
   depicted by the sum of the length of the blue lines in the chart below.
   [1*qafrgv6yu357t97i5kczgg.png]

   note: more accurately the cost function is often the squared of the
   difference between actual and predicted outcome, because the difference
   can sometimes can be negative; this is also known as min least-squared.

linear model in brief

   in the spirit of keeping things simple, we will model our data points
   using a linear model. a linear model is represented mathematically as:
y = w.x + b
where:
x: house size, in sqm
y: predicted house price, in $

   to tweak the model to best fit our data points, we can:
     * tweak w to change the gradient of the linear model

   [1*un0lzf-x_zxyyi851w_kia.png]
     * tweak b to change the position of the linear model

   [1*jgvvkiiqcdmbr4uu_x121w.png]

   by going through many values of w, b, we can eventually find a best-fit
   linear model that minimizes the cost function. besides randomly trying
   different values, is there a better way to explore the w, b values
   quickly?

id119

   if you are on an expansive plateau in the mountains, when trying to
   descent to the lowest point, your viewpoint looks like this.
   [1*phkkgipjf1_inkf46kf7ea.png]

   the direction of descent is not obvious! the best way to descend is
   then to perform id119:
     * determine the direction with the steepest downward gradient at
       current position
     * take a step of size x in that direction
     * repeat & rinse; this is known as training

   minimizing the cost function is similar because, the cost function is
   undulating like the mountains (chart below), and we are trying to find
   the minimum point, which we can similarly achieve through gradient
   descent.
   [1*gra_mojjddrb7kmvaymrwq.png]

   with the concepts of linear model, cost function, and id119
   in hand, we are ready to use tf.

step 2: create the model in tf

linear model in tf

   the 2 basic tf components are:

   placeholder: represent an entry point for us to feed actual data values
   into the model when performing id119, i.e., the house sizes
   (x), and the house prices (y_).
   [1*6a86cxsrbffgmkzbzl37xa.png]

   variable: represent a variable that we are trying to find    good    values
   that minimizes the cost function, e.g., w, and b.
   [1*3v-xy35xzgjsjm8tkdhxhg.png]

   the linear model (y = w.x + b) in tf then becomes:
   [1*gf8w4mu9skib490vfmpyda.png]

cost function in tf

   similarly to feed actual house prices (y_) of the data points into the
   model, we create a placeholder.
   [1*2p-5lfqverc9uxrcp6mjww.png]

   our cost function of least-min squared becomes:
   [1*o4ph8s8y73zqg_rxupt9wq.png]

data

   since we do not have actual data points for house price (y_), house
   size (x), we generate them.
   [1*eb0un7ciwdma5q4cvcabyq.png]

   we set the house price (ys) to always be 2 times the house size (xs)
   for simplicity.

id119

   with the linear model, cost function, and data, we can start performing
   id119 to minimize the cost function, to obtain the    good   
   values for w, b.
   [1*xaicws4l_4zwzhvv-8ky-w.png]

   the 0.00001 is the size of the    step    we take in the direction of
   steepest gradient each time perform a training step; this is also
   called learning rate.

step 3: train the model

   training involves performing id119 a pre-determined number
   of times or until the cost is below a pre-determined threshold.

tf quirks

   all variables needs to be initialize at the start of training otherwise
   they may hold remnant values from previous execution.
   [1*kjdpgzjzgs6-zxjuoktinq.png]

tf session

   although tf is a python library, and python is an interpreted language,
   tf operations, by default are not interpreted for performance reasons.
   thus the init above is not executed. instead tf executes within a
   session; create a session (sess) and then execute stuff using
   sess.run().
   [1*lspy7yhtv-f8b5jipnhhzg.png]

   similarly we execute the train_step above within a loop by calling it
   within sess.run().
   [1*rymmuw0ht5wg8jjg3g-n3w.png]

   the reason why you need to feed actual data points into feed, which is
   composed of x, y_ is that tf resolves the train_step into its
   dependencies:
   [1*dqiep6tu3mlsawms080bug.png]

   at the bottom of the dependencies are the placeholders x, y_; and as we
   learned earlier tf.placeholders are used to indicate where we will feed
   actual data point values house price (y_), and house size (x).

result

   the print statement in the loop will show how tf learn the    good   
   values for w, and b over each iteration.
   [1*xg4wsfd8ct4kx9vnm7gbrw.png]

wrapping up

   we have learned about machine learning in its simplest form; predict an
   outcome from a single feature. we chose a linear model (for simplicity)
   to fit our data points, define a cost function to represent best-fit,
   and train our model by repeatedly tweaking its gradient variable, w,
   and position variable b to minimize the cost function.

coming up next

   in the next article, we will:
     * set up tensor board to visualize tf execution to detect problems in
       our model, cost function, or id119
     * feed data points in batches into the model during each training
       step (instead of just one data point at a time) to understand how
       it affects training

resources

     * the code on [18]github
     * the slides on [19]slideshare
     * the video on [20]youtube

     * [21]tensorflow
     * [22]machine learning
     * [23]introduction
     * [24]data science
     * [25]deep learning

   (button)
   (button)
   (button) 841 claps
   (button) (button) (button) 17 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of soon hin khor

[27]soon hin khor

   use tech to make the world more caring, and responsible. nat. univ.
   singapore, carnegie mellon univ, univ. of tokyo. ibm, 500startups &
   y-combinator companies

     (button) follow
   [28]all of us are belong to machines

[29]all of us are belong to machines

   writings about machine learning, and artificial intelligence

     * (button)
       (button) 841
     * (button)
     *
     *

   [30]all of us are belong to machines
   never miss a story from all of us are belong to machines, when you sign
   up for medium. [31]learn more
   never miss a story from all of us are belong to machines
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/248dc871a224
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/all-of-us-are-belong-to-machines?source=avatar-lo_jhmphay7j5i4-ad462a2018c0
   7. https://medium.com/all-of-us-are-belong-to-machines?source=logo-lo_jhmphay7j5i4---ad462a2018c0
   8. https://medium.com/m/signin?redirect=https://medium.com/all-of-us-are-belong-to-machines/the-gentlest-introduction-to-tensorflow-248dc871a224&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/all-of-us-are-belong-to-machines/the-gentlest-introduction-to-tensorflow-248dc871a224&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@khor?source=post_header_lockup
  11. https://medium.com/@khor
  12. https://www.tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html
  13. https://www.tensorflow.org/versions/r0.7/tutorials/mnist/pros/index.html
  14. https://www.udacity.com/course/deep-learning--ud730
  15. https://medium.com/@khor/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f
  16. https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-part-3-matrices-multi-feature-linear-regression-30a81ebaaa6c
  17. https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-4-logistic-regression-2afd0cabc54
  18. https://github.com/nethsix/gentle_tensorflow/blob/master/code/linear_regression_one_feature.py
  19. http://www.slideshare.net/khorsoonhin/gentlest-introduction-to-tensorflow
  20. https://youtu.be/dyhrcufn0em
  21. https://medium.com/tag/tensorflow?source=post
  22. https://medium.com/tag/machine-learning?source=post
  23. https://medium.com/tag/introduction?source=post
  24. https://medium.com/tag/data-science?source=post
  25. https://medium.com/tag/deep-learning?source=post
  26. https://medium.com/@khor?source=footer_card
  27. https://medium.com/@khor
  28. https://medium.com/all-of-us-are-belong-to-machines?source=footer_card
  29. https://medium.com/all-of-us-are-belong-to-machines?source=footer_card
  30. https://medium.com/all-of-us-are-belong-to-machines
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/248dc871a224/share/twitter
  34. https://medium.com/p/248dc871a224/share/facebook
  35. https://medium.com/p/248dc871a224/share/twitter
  36. https://medium.com/p/248dc871a224/share/facebook
