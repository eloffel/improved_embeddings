   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]all of us are belong to machines
   [7]all of us are belong to machines
   [8]sign in[9]get started
     __________________________________________________________________

gentlest introduction to tensorflow #2

   [10]go to the profile of soon hin khor
   [11]soon hin khor (button) blockedunblock (button) followfollowing
   jul 24, 2016

   summary: we show in illustrations how the machine learning    training   
   process happens in tensorflow, and tie them back to the tensorflow
   code. this paves the way for discussing    training    variations, namely
   stochastic/mini-batch/batch, and adaptive learning rate gradient
   descent. the    training    variation code snippets presented serve to
   reinforce the understanding of the role of tensorflow placeholders.

   this is part of a series:
     * [12]part 1: id75 with tensorflow for single feature
       single outcome model
     * part 2 (this article): tensorflow training illustrated in
       diagrams/code, and exploring training variations
     * [13]part 3: matrices and multi-feature id75 with
       tensorflow
     * [14]part 4: id28 with tensorflow

quick review

   in the [15]previous article, we used tensorflow (tf) to build and learn
   a id75 model with a single feature so that given a feature
   value (house size/sqm), we can predict the outcome (house price/$).

   here is the review with illustration below:
    1. we have some data of house sizes & house prices (the gray round
       points)
    2. we model the data using id75 (the red dash line)
    3. we find the    best    model by training w, and b (of the linear
       regression model) to minimize the    cost    (the sum of the length of
       vertical blue lines, which represent the differences between
       predictions and actual outcomes)
    4. given any house size, we can use the linear model to predict the
       house size (the dotted blue lines with arrows)

   [1*fiqrvpeuipwa10qruds8yg.png]
   id75 explained in a single diagram

   in machine learning (ml) literature, we come across the term    training   
   very often, let us literally look at what that means in tf.

id75 modeling

linear model (in tf notation): y = tf.matmul(x,w) + b

   the goal in id75 is to find w, b, such that given any
   feature value (x), we can find the prediction (y) by substituting w, x,
   b values into the model.

   however to find w, b that can give accurate predictions, we need to
      train    the model using available data (the multiple pairs of actual
   feature (x), and actual outcome (y_), note the underscore).

   training    illustrated

   to find the best w, b values, we can initially start with any w, b
   values. we also need to define a cost function, which is a measure of
   the difference between the prediction (y) for given a feature value
   (x), and the actual outcome (y_) for that same feature value (x). for
   simplicity, we use least [16]minimum squared error (mse) as our cost
   function.
cost function (in tf notation): tf.reduce_mean(tf.square(y_ - y))

   by minimizing the cost function, we can arrive at good w, b values.

   our code to do training is actually very simple and it is labelled with
   [a, b, c, d], which we will refer to later on. the full source is on
   [17]github.
# ... (snip) variable/constants declarations (snip) ...
# [a] tf.graph
y = tf.matmul(x,w) + b
cost = tf.reduce_mean(tf.square(y_-y))
# [b] train with fixed 'learn_rate'
learn_rate = 0.1
train_step =
    tf.train.gradientdescentoptimizer(learn_rate).minimize(cost)
for i in range(steps):
  # [c] prepare datapoints
  # ... (snip) code to prepare datapoint as xs, and ys (snip) ...
  # [d] feed data at each step/epoch into 'train_step'
  feed = { x: xs, y_: ys }
  sess.run(train_step, feed_dict=feed)

   our linear model and cost function equations [a] can be represented as
   tf graph as shown:
   [1*ckyfmclscyr7vvs7-4wvoq.png]
   create a tf graph with model & cost, and initialize w, b with
   some values

   next, we select a datapoint (x, y_) [c], and feed [d] it into the tf
   graph to get the prediction (y) as well as the cost.
   [1*id568zoh9plf1xl-jjlix1g.png]
   calculate prediction (y) & cost using a single datapoint

   to get better w, b, we perform id119 using tf   s
   tf.train.gradientdescentoptimizer [b] to reduce the cost. in
   non-technical terms: given the current cost, and based on the graph of
   how cost varies with other variables (namely w, b), the optimizer will
   perform small tweaks (increments/decrements) to w, b so that our
   prediction becomes better for that single datapoint.
   [1*07fuxhwoslezo9fgqaxhng.png]
   based on current cost, determine how to tweak w, b to improve
   prediction (y) and reduce cost

   the final step in the training cycle is to update the w, b after
   tweaking them. note that    cycle    is also referred to as    epoch    in ml
   literature.
   [1*fjura2_woyel40wypfdilg.png]
   update w, b after tweaking them, and before iterating through the next
   training epoch

   in the next training epoch, repeat the steps, but use a different
   datapoint!
   [1*u-gfpfjwk_cbno-mximrhq.png]
   training using different datapoints

   using a variety of datapoints generalizes our model, i.e., it learns w,
   b values that can be used to predict any feature value. note that:
     * in most cases, the more datapoints, the better your model can learn
       and generalize
     * if you train more epochs than datapoints you have, you can re-use
       datapoints, which is not an issue. the id119 optimizer
       always use both the datapoint, and the cost (calculated from the
       datapoint, with w, b values of that epoch) to tweak w, b; the
       optimizer may have seen that datapoint before, but not with the
       same cost, thus it will learn something new, and tweak w, b
       differently.

   you can train the model a fixed number of epochs or until it reaches a
   cost threshold that is satisfactory.

training variation

stochastic, mini-batch, batch

   in the training above, we feed a single datapoint at each epoch. this
   is known as stochastic id119. we can feed a bunch of
   datapoints at each epoch, which is known as mini-batch gradient
   descent, or even feed all the datapoints at each epoch, known as batch
   id119. see the graphical comparison below and note the 2
   differences between the 3 diagrams:
     * the number of datapoints (upper-right of each diagram) fed to
       tf.graph at each epoch
     * the number of datapoints for the id119 optimizer to
       consider when tweaking w, b to reduce cost (bottom-right of each
       diagram)

   [1*fjura2_woyel40wypfdilg.png]
   stochastic id119
   [1*-q-ycjdlopyj5y3nf0qbhg.png]
   mini-batch id119
   [1*sfakw076xut5xeq06jpihg.png]
   batch id119

   the number of datapoints used at each epoch has 2 implications. with
   more datapoints:
     * computational resource (subtractions, squares, and additions)
       needed to calculate the cost and perform id119 increases
     * speed at which the model can learn and generalize increases

   the pros and cons of doing stochastic, mini-batch, batch gradient
   descent can be summarized in the diagram below:
   [1*0_bwld9awrvzozt7qjvhww.png]
   pros and cons of stochastic, mini-batch & batch id119

   to switch between stochastic/mini-batch/batch id119, we just
   need to prepare the datapoints into different batch sizes before
   feeding them into the training step [d], i.e., use the snippet below
   for[c]:
# * all_xs: all the feature values
# * all_ys: all the outcome values
# datapoint_size: number of points/entries in all_xs/all_ys
# batch_size: configure this to:
#             1: stochastic mode
#             integer < datapoint_size: mini-batch mode
#             datapoint_size: batch mode
# i: current epoch number
if datapoint_size == batch_size:
  # batch mode so select all points starting from index 0
  batch_start_idx = 0
elif datapoint_size < batch_size:
  # not possible
  raise valueerror(   datapoint_size: %d, must be greater than
                    batch_size: %d    % (datapoint_size, batch_size))
else:
  # stochastic/mini-batch mode: select datapoints in batches
  #                             from all possible datapoints
  batch_start_idx = (i * batch_size) % (datapoint_size     batch_size)
  batch_end_idx = batch_start_idx + batch_size
  batch_xs = all_xs[batch_start_idx:batch_end_idx]
  batch_ys = all_ys[batch_start_idx:batch_end_idx]
# get batched datapoints into xs, ys, which is fed into
# 'train_step'
xs = np.array(batch_xs)
ys = np.array(batch_ys)

learn rate variation

   learn rate is how big an increment/decrement we want id119
   to tweak w, b, once it decides whether to increment/decrement them.
   with a small learn rate, we will proceed slowly but surely towards
   minimal cost, but with a larger learn rate, we can reach the minimal
   cost faster, but at the risk of    overshooting   , and never finding it.

   to overcome this, many ml practitioners use a large learn rate
   initially (with the assumption that initial cost is far away from
   minimum), and then decrease the learn rate gradually after each epoch.

   tf provides 2 ways to do so as wonderfully explained in this
   stackoverflow [18]thread, but here is the summary.

   use id119 optimizer variants

   tf comes with various id119 optimizer, which supports learn
   rate variation, such as [19]tf.train.adagradientoptimizer, and
   [20]tf.train.adamoptimizer.

   use tf.placeholder for learn rate

   as you have learned previously, if we declare a tf.placeholder, in this
   case for learn rate, and use it within the
   tf.train.gradientdescentoptimizer, we can feed a different value to it
   at each training epoch, much like how we feed different datapoints to
   x, y_, which are also tf.placeholders, at each epoch.

   we need 2 slight modifications:
# modify [b] to make 'learn_rate' a 'tf.placeholder'
# and supply it to the 'learning_rate' parameter name of
# tf.train.gradientdescentoptimizer
learn_rate = tf.placeholder(tf.float32, shape=[])
train_step = tf.train.gradientdescentoptimizer(
    learning_rate=learn_rate).minimize(cost)
# modify [d] to include feed a 'learn_rate' value,
# which is the 'initial_learn_rate' divided by
# 'i' (current epoch number)
# note: oversimplified. for example only.
feed = { x: xs, y_: ys, learn_rate: initial_learn_rate/i }
sess.run(train_step, feed_dict=feed)

wrapping up

   we illustrated what machine learning    training    is, and how to perform
   it using tensorflow with just model & cost definitions, and looping
   through the training step, which feeds datapoints into the gradient
   descent optimizer. we also discussed the common variations in training,
   namely changing the size of datapoints the model uses for learning at
   each epoch, and varying the learn rate of id119 optimizer.

coming up next

     * set up tensor board to visualize tf execution to detect problems in
       our model, cost function, or id119
     * perform id75 with multiple features

resources

     * the code on [21]github
     * the slides on [22]slideshare
     * the video on [23]youtube

     * [24]machine learning
     * [25]data science
     * [26]tensorflow
     * [27]id119
     * [28]deep learning

   (button)
   (button)
   (button) 372 claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of soon hin khor

[30]soon hin khor

   use tech to make the world more caring, and responsible. nat. univ.
   singapore, carnegie mellon univ, univ. of tokyo. ibm, 500startups &
   y-combinator companies

     (button) follow
   [31]all of us are belong to machines

[32]all of us are belong to machines

   writings about machine learning, and artificial intelligence

     * (button)
       (button) 372
     * (button)
     *
     *

   [33]all of us are belong to machines
   never miss a story from all of us are belong to machines, when you sign
   up for medium. [34]learn more
   never miss a story from all of us are belong to machines
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ed2a0a7a624f
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/all-of-us-are-belong-to-machines?source=avatar-lo_qls94sf5d0jm-ad462a2018c0
   7. https://medium.com/all-of-us-are-belong-to-machines?source=logo-lo_qls94sf5d0jm---ad462a2018c0
   8. https://medium.com/m/signin?redirect=https://medium.com/all-of-us-are-belong-to-machines/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/all-of-us-are-belong-to-machines/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@khor?source=post_header_lockup
  11. https://medium.com/@khor
  12. https://medium.com/@khor/the-gentlest-introduction-to-tensorflow-248dc871a224
  13. https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-part-3-matrices-multi-feature-linear-regression-30a81ebaaa6c
  14. https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-4-logistic-regression-2afd0cabc54
  15. https://medium.com/@khor/the-gentlest-introduction-to-tensorflow-248dc871a224
  16. https://en.wikipedia.org/wiki/mean_squared_error
  17. https://github.com/nethsix/gentle_tensorflow/blob/master/code/linear_regression_one_feature_using_mini_batch_with_tensorboard.py
  18. http://stackoverflow.com/questions/33919948/how-to-set-adaptive-learning-rate-for-gradientdescentoptimizer
  19. http://www.tensorflow.org/api_docs/python/train.html#adagradoptimizer
  20. http://www.tensorflow.org/api_docs/python/train.html#adamoptimizer
  21. https://github.com/nethsix/gentle_tensorflow/blob/master/code/linear_regression_one_feature_using_mini_batch_with_tensorboard.py
  22. http://www.slideshare.net/khorsoonhin/gentlest-intro-to-tensorflow-part-2-62006222
  23. https://www.youtube.com/watch?v=trc52fvid113g
  24. https://medium.com/tag/machine-learning?source=post
  25. https://medium.com/tag/data-science?source=post
  26. https://medium.com/tag/tensorflow?source=post
  27. https://medium.com/tag/gradient-descent?source=post
  28. https://medium.com/tag/deep-learning?source=post
  29. https://medium.com/@khor?source=footer_card
  30. https://medium.com/@khor
  31. https://medium.com/all-of-us-are-belong-to-machines?source=footer_card
  32. https://medium.com/all-of-us-are-belong-to-machines?source=footer_card
  33. https://medium.com/all-of-us-are-belong-to-machines
  34. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  36. https://medium.com/p/ed2a0a7a624f/share/twitter
  37. https://medium.com/p/ed2a0a7a624f/share/facebook
  38. https://medium.com/p/ed2a0a7a624f/share/twitter
  39. https://medium.com/p/ed2a0a7a624f/share/facebook
