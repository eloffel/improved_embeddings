   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]all of us are belong to machines
   [7]all of us are belong to machines
   [8]sign in[9]get started
     __________________________________________________________________

gentlest intro to tensorflow #3: matrices & multi-feature id75

   [10]go to the profile of soon hin khor
   [11]soon hin khor (button) blockedunblock (button) followfollowing
   oct 14, 2016

   summary: with concepts of single-feature linear-regression, cost
   function, id119 (from [12]part 1), epoch, learn-rate,
   id119 variation (from [13]part 2) under our belt, we are
   ready to progress to multi-feature id75 with tensorflow
   (tf). if you are already familiar with matrices and multi-feature
   id75, skip to the end for the multi-feature tensorflow
   code cheatsheet, or even skip this entire article.

   this is part of a series:
     * [14]part 1: id75 with tensorflow for single feature
       single outcome model
     * [15]part 2: tensorflow training illustrated in diagrams/code, and
       exploring training variations
     * part 3 (this article): matrices and multi-feature id75
       with tensorflow
     * [16]part 4: id28 with tensorflow

quick review

   the premise of the previous articles was: given any house size (square
   meters/sqm), which is the feature, we want to predict the house price
   ($), the outcome. to do that we:
    1. we find a straight line (id75) that    best-fits    the
       data points that we have. the    best-fit    is when the linear
       regression line ensures that the difference between the actual data
       points (gray dots) and the predicted values (gray dots interpolated
       on to the straight line), which, in other words, is the sum of
       multiple blue lines, is minimized.
    2. with this straight line we can predict any value of house

   [1*bfzp8usmk88mdlibu465va.png]
   predicting using single-feature id75

multi-feature id75 overview

   in reality, any prediction relies on multiple features, so we advance
   from single-feature to 2-feature id75; we chose 2 features
   to keep visualization and comprehension simple, but the concept
   generalizes to any number of features.

   we introduce a new feature,    rooms    (number of units in the house).
   when collecting datapoints, we must now collect values for the new
   feature    rooms    on top of the existing feature    house size   , as well as
   the corresponding outcome    house price   .

   our chart becomes 3-dimensional.
   [1*lxq18pykj8yroakvopiwpa.png]
   datapoints for the outcome    house price    and its 2-feature (   rooms    &
      house size   ) space

   our goal then becomes predicting    house price   , given    rooms   , and
      house size    (see image below).
   [1*2rdzz3vo5xmhmwnkfj7zuq.png]
   prediction for a given 2-feature sometimes cannot be done due to
   missing of datapoints

   in the single-feature scenario, we had to use id75 to
   create a straight line to help us predict the outcome    house size   , for
   cases where we did not have datapoints. in a 2-feature scenario, we can
   also employ id75, but to create a plane (instead of a
   straight line) to help us predict (see image below).
   [1*bjoya1hxbjiooesfqedv2g.png]
   using id75 on 2-feature space to create a plane to do
   prediction

multi-feature id75 model

   recall for a single-feature (see left of image below), the linear
   regression model outcome (y) has a weight (w), a placeholder (x) for
   the    house size    feature, and a bias (b).

   for 2-feature (see right of image below), we introduce another weight,
   which we call w2, and another placeholder, x2 to hold the    rooms   
   feature value.
   [1*byo4_urqnnuunqoaesyzrw.png]
   1-feature vs. 2-feature id75 equations

   when we perform id75, id119 helps us learn the
   additional weight w2, on top of the learning w, b as previously
   discussed.

multi-feature id75 in tensorflow

quick review

   our tf code for single-feature id75 consists of 3 parts
   (see image below):
     * constructing the model (blue part)
     * constructing the cost function based on the model (red part)
     * minimizing the cost function using id119 (green part)

   [1*op-h6wxbxdhux3irs7owxg.png]
   tensorflow code for 1-feature id75

tensorflow for 2-feature id75

   the change to support 2-feature id75 equation (explained
   above) in tf code is shown in red.
   [1*kwtzn5h59xfmf-t9fminbg.png]

   note this way of adding new features is inefficient; as the number of
   features grow, the number of required variables and placeholders
   increases. in reality models have many more features, which worsens
   this problem. how can we represent features efficiently?

matrices to the rescue

   first, let us generalize representing a 2-feature model to an n-feature
   one:
   [1*vlf5-hzha1qbc_tzsrhhga.png]

   it turns out that the complex n-feature formula can be simplified in
   the world of matrices, and matrices are in-built into tf for these
   reasons:
     * data can be represented in multi-dimensions, which fits the way we
       want to represent a datapoint with n features (below left, also
       known as the feature matrix) and a model with n weights (below
       right, also known as the weight matrix)

   [1*si4puxpdqdm2gj-gyqgx7a.png]
   1 datapoint   s n features and the model   s n weights in matrix form

   in tf, they would be written as:

   x = tf.placeholder(tf.float, [1,n])

   w = tf.variable(tf.zeros[n,1])

   note: for w we use tf.zeros, which initializes all w1, w2,    , wn to
   zeros.
     * mathematically id127 is a sum of multiplications
       (just accept this as part of mathematics); thus naturally the
       id127 between the features (the one in the middle)
       and weights (the one on the right) matrices gives you the outcome
       (the one on the left), which is equivalent to first part of the
       n-feature id75 formula (described above), i.e.,
       without the biases

   [1*bo_qw7rn9lpkvid19zbjtug.png]
   id127 between features and weights matrices gives the
   outcome (without biases added)

   in tf, this multiplication would be:

   y = tf.matmul(x, w)
     * id127 between a multi-row feature matrix (each row
       representing a datapoint   s n features), returns multi-row outcomes
       (each row representing the outcome/prediction (without bias added)
       of each datapoint); thus a single id127 can apply
       the id75 formula to multiple datapoints to produce
       multiple predictions, one for each datapoints, at a single go (see
       below)!

   note: the x representations in the feature matrix become more complex,
   i.e., we use x1.1, x1.2, instead of x1, x2, etc. because the feature
   matrix (the one in the middle) has expanded from representing a single
   datapoint of n-features (1 row x n columns) to representing m
   datapoints with n-features (m rows x n columns), so we extended x<n>,
   e.g., x1, to x<m>.<n>, e.g., x1.1, where n is the feature number and m
   is the datapoint number.
   [1*wwbffty2iru4hrwwoot9tw.png]
   multiple row id127 with model weights produce multiple
   row matrix outcomes

   in tf, they would be written as:

   x = tf.placeholder(tf.float, [m, n])

   w = tf.variable(tf.zeros[n,1])

   y = tf.matmul(x, w)
     * finally, adding a constant to the outcome matrix results in the
       constant being added to every row in the matrix

   in tf, with our x, and w represented in matrices, regardless of the
   number of features our model has or the number of datapoints we want to
   handle, it can be simplified to:

   b = tf.variable(tf.zeros[1])

   y = tf.matmul(x, w) + b

tensorflow multi-feature cheatsheet

   we do a side-by-side comparison to summarize the change from single to
   multi-feature id75:
   [1*_-q1yqtvgeoz_up3vikzew.png]
   1-feature vs n-feature id75 model in tensorflow

wrapping up

   we illustrated the concept of multi-feature id75, and
   showed how we extend our model and tf code from single to 2-feature
   id75 models, which is generalizable to n-feature models.
   we conclude by presenting a cheatsheet for multi-feature tf linear
   regression model.

coming up next

   we will present the concepts of id28, cross-id178, and
   softmax, which will enable us to fully understand tensorflow   s
   [17]official beginner   s tutorial on mnist.

references

     * github: tf for multi-feature id75 [18]without matrices
     * github: tf for multi-feature id75 [19]with matrices
     * the slides on [20]slideshare (1   43)
     * the video on [21]youtube (0:00 to 7:18)

     * [22]machine learning
     * [23]data science
     * [24]deep learning
     * [25]tensorflow
     * [26]id75

   (button)
   (button)
   (button) 492 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of soon hin khor

[28]soon hin khor

   use tech to make the world more caring, and responsible. nat. univ.
   singapore, carnegie mellon univ, univ. of tokyo. ibm, 500startups &
   y-combinator companies

     (button) follow
   [29]all of us are belong to machines

[30]all of us are belong to machines

   writings about machine learning, and artificial intelligence

     * (button)
       (button) 492
     * (button)
     *
     *

   [31]all of us are belong to machines
   never miss a story from all of us are belong to machines, when you sign
   up for medium. [32]learn more
   never miss a story from all of us are belong to machines
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/30a81ebaaa6c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/all-of-us-are-belong-to-machines?source=avatar-lo_y7poyfgsaewh-ad462a2018c0
   7. https://medium.com/all-of-us-are-belong-to-machines?source=logo-lo_y7poyfgsaewh---ad462a2018c0
   8. https://medium.com/m/signin?redirect=https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-part-3-matrices-multi-feature-linear-regression-30a81ebaaa6c&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-part-3-matrices-multi-feature-linear-regression-30a81ebaaa6c&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@khor?source=post_header_lockup
  11. https://medium.com/@khor
  12. https://medium.com/@khor/the-gentlest-introduction-to-tensorflow-248dc871a224
  13. https://medium.com/@khor/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f
  14. https://medium.com/@khor/the-gentlest-introduction-to-tensorflow-248dc871a224
  15. https://medium.com/@khor/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f
  16. https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-4-logistic-regression-2afd0cabc54
  17. https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/index.html
  18. https://github.com/nethsix/gentle_tensorflow/blob/master/code/linear_regression_multi_feature_using_mini_batch_without_matrix_with_tensorboard.py
  19. https://github.com/nethsix/gentle_tensorflow/blob/master/code/linear_regression_multi_feature_using_mini_batch_with_tensorboard.py
  20. http://www.slideshare.net/khorsoonhin/gentlest-introduction-to-tensorflow-part-3
  21. https://www.youtube.com/watch?v=f8g_6txklxw
  22. https://medium.com/tag/machine-learning?source=post
  23. https://medium.com/tag/data-science?source=post
  24. https://medium.com/tag/deep-learning?source=post
  25. https://medium.com/tag/tensorflow?source=post
  26. https://medium.com/tag/linear-regression?source=post
  27. https://medium.com/@khor?source=footer_card
  28. https://medium.com/@khor
  29. https://medium.com/all-of-us-are-belong-to-machines?source=footer_card
  30. https://medium.com/all-of-us-are-belong-to-machines?source=footer_card
  31. https://medium.com/all-of-us-are-belong-to-machines
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/30a81ebaaa6c/share/twitter
  35. https://medium.com/p/30a81ebaaa6c/share/facebook
  36. https://medium.com/p/30a81ebaaa6c/share/twitter
  37. https://medium.com/p/30a81ebaaa6c/share/facebook
