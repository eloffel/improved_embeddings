   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]all of us are belong to machines
   [7]all of us are belong to machines
   [8]sign in[9]get started
     __________________________________________________________________

gentlest intro to tensorflow #4: id28

   [10]go to the profile of soon hin khor
   [11]soon hin khor (button) blockedunblock (button) followfollowing
   feb 4, 2017

   summary: in all previous articles, given some features, such as    house
   size   , we used tensorflow (tf) to perform id75 to predict
   the outcome, such as    house price   , which is a numeric value. we will
   now look at id28 where given some features of an input,
   we will use tf to classify the input, specifically if given an input
   image, we are going to classify it as one of the digits of 0   9.

   this is part of a series:
     * [12]part 1: id75 with tensorflow for single feature
       single outcome model
     * [13]part 2: tensorflow training illustrated in diagrams/code, and
       exploring training variations
     * [14]part 3: matrices and multi-feature id75 with
       tensorflow
     * part 4 (this article): id28 with tensorflow

id28 overview

   we have learnt how to use tensorflow (tf) to perform id75
   to predict an outcome of scalar value, e.g., house prices, given a set
   of features, e.g., house size.

   however, there are times when we want to classify things rather than
   predict a value, e.g., given an image of a digit we can to classify it
   as either 0, 1, 2,    , 9, or given a song we want to classify it as pop,
   rock, rap, etc. each of the classification in the set [0, 1, 2,    , 9],
   or [pop, rock, rap, etc.], is known as a class, which in the computer
   world we represent using a number, e.g., pop = 0, rock = 1, etc. to
   perform classification, we can employ id28 using tf.

   in this article, we will use id28 to classify the image
   of a digit, as belonging to classes 0, 1, 2,    , or, 9.

id28 details

   the good news is a lot of concepts in [15]id75 still
   applies in id28. we can reuse the formula y = w.x + b,
   but with some tweaks. let   s look at this formula side-by-side for
   linear and id28:
   [1*oy6o6odztxbp_czi_k4mrg.png]
   differences & similarities between linear & id28

   differences:
     * outcome (y): for id75, this is a scalar value, e.g.,
       $50k, $23.98k, etc. for id28, this is an integer
       that refers to a class of e.g., 0, 1, 2, .. 9.
     * features (x): for id75, each feature is represented as
       an element in a column vector. for id28 involving a
       2-d image, this is a 2-dimensional vector, with each element
       representing a pixel of the image; each pixel has a value of 0   255
       representing a grayscale where 0 = black, and 255 = white, and
       other values some shade of grey.
     * cost function (cost): for id75, this is some function
       calculating the aggregated difference between each prediction and
       its expected outcome. for id28, this is some
       function calculating the aggregation of whether each prediction is
       right or wrong.

   similarity:
     * training: the training goals of both linear and id28
       are to learn the weights (w) and biases (b) values
     * outcome: the intention of both linear and id28 is to
       predict/classify the outcome (y) with the learned w, and b

reconcile logistic and id75

   to make id28 work with y = w.b + x, we need to make some
   changes to reconcile the differences stated above.

feature transformation, x

   we can convert the 2-dimensional image features in our logistic
   regression example (assuming it has x rows, y columns) into a
   1-dimensional one (as required in id75) by appending each
   row of pixels one after another to the end of the first row of pixels
   as shown below.
   [1*vo8pmhppg_lwxfazhzzngq.png]
   transforming image features to suit id28 formula

predicted outcome transformation, y

   for id28, we cannot leave y (predicted outcome) as a
   scalar since the prediction may end up being 2.3, or 11, which is not
   in the possible classes of [0, 1,    , 9].

   to overcome this, the prediction y should be transformed into a single
   column vector (shown below as row vector to conserve space) where each
   element represents the score of what the id28 model
   thinks is likely a particular class. in the example below, class    1    is
   the prediction since it has the highest score.
   [1*ld1fm5euvxm16mtf-4ifza.png]
   scores for each class and the class with highest score becomes the
   prediction

   to derive this vector of scores, for a given image, each pixel on it
   will contribute a set of scores (one for each class) indicating the
   likelihood it thinks the image is in a particular class, based only on
   its own greyscale value. the sum of all the scores from every pixel for
   each class becomes the prediction vector.
   [1*aop0s2i587kdjw2td7gnqq.png]
   each pixel provides a vector of scores; one score per class, which is
   becomes the prediction vector. them sum of all prediction vectors
   becomes the final prediction

cost function transformation

   we cannot use as cost function, any function that involves numerical
   distance between predicted and actual outcomes. such a cost function,
   for an image of    1   , will penalize a prediction of    7   , more heavily
   (7   1=6) than a prediction of    2    (2   1=1), although both are equally
   wrong.

   the cost function we are going to use, cross id178 (h) involves
   multiple steps:
    1. convert actual image class vector (y   ) into a one-hot vector, which
       is a id203 distribution
    2. convert prediction class vector (y) into a id203 distribution
    3. use cross id178 function to calculate cost, which is the
       difference between 2 id203 distribution function

   step 1. one-hot vectors

   since we already transformed prediction (y) in to a vector of scores,
   we should also transform the actual image class (y   ) into a vector as
   well; each element in the column vector represents a class with every
   element being    0    except the element corresponding to the actual class
   being    1   . this is known as a one-hot vector. below we show the one-hot
   vector for each class from 0 to 9.
   [1*yfh3gz41pgqwanb_lfmjvw.png]
   image class and their one-hot vector representations

   assuming the actual (y   ) image being 1, thus having a one-hot vector of
   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], and the prediction vector (y) of [1.3,
   33, 2, 1.2, 3.2, 0.5, 3, 9.2, 1], plotting them for comparison becomes:
   [1*htzu15da9tip9kf83yy7ag.png]
   actual image one-hot vector (top) with prediction id203 of
   classes (bottom)

   step 2. id203 distribution with softmax

   to mathematically compare similarity of two    graphs   , cross-id178 is
   a great way (and [16]here is a fantastic albeit long explanation for
   those with a stomach for details).

   to utilize cross id178 however, we need to convert both the actual
   outcome vector (y   ) and the prediction outcome vector (y) values into a
      id203 distribution   , and by    id203 distribution    we mean:
     * the id203/score of each class has to be between 0 to 1
     * the sum of all the probabilities/score for all classes has to be 1

   the actual outcome vector (y   ) being one-hot vectors already satisfy
   these constraints.

   for prediction outcome vector (y), we can transform it into a
   id203 distribution using softmax:
   [1*gmoykuvxxuyk7lpdvzhmbg.png]
   softmax equation, where i is the class of 0, 1, 2,    , 9

   this is simply a 2-step process (see s1, s2 below), where each element
   in the prediction score vector (y), is exp   ed, and divided by the sum
   of the exp   ed total.
   [1*yhfauzjud7za0bgot-mwag.png]

   note that softmax(y) graph is similar in shape to the prediction (y)
   graph but merely with larger max and smaller min values.
   [1*0bw3r-w89s9-8huz_ehaua.png]
   prediction (y) graph before (top) and after applying softmax (below)

   step 3. cross id178

   we can now apply cross-id178 (h) between the predicted vector score
   id203 distribution (y   ) and the actual vector score id203
   distribution (y).

   the cross id178 formula is:
   [1*enii64ih9v4jljo-jfobug.png]
   use cross id178 (h) as a cost function that we want to minimize

   to quickly understand this complex formula, we break it down into 3
   parts (see below). note that as notation in this article, we use y_i to
   represent    y with i subscript    in the formula h:
   [1*mljx2kl8vrpqu5xcgvs8bq.png]
   consider the cross id178 (h) formula as 3 parts: red, blue,
   red, green
     * blue: actual outcome vector, y_i   
     * red: -log of the id203 distribution of prediction class
       vector, (softmax(y_i)), explained previously
     * green: sum of multiplication of blue and red components for each
       image class i, where i = 0, 1, 2,    , 9

   the illustrations below should simplify understanding further.

   the blue plot is just the one-hot vector of actual image class (y   ),
   see one-hot vector section:
   [1*-vpuyjvh8l7usbktl9vm4w.png]

   the red plot is derived from transformations of each prediction vector
   element, y, to softmax(y), to -log(softmax(y):
   [1*ai9hbnu5sm8glhgvhlasnq.png]
   red plot derived from a series of transformation of prediction class
   vector (y)

   if you wish to fully understand why -log(softmax(y)) inverses
   softmax(y), the second transformation, please check out the [17]video
   or [18]slides.

   the cross id178 (h), the green part (see below) is the multiplication
   of blue and red values for each class, and then summing them up as
   illustrated:
   [1*1wmxl3rsjfdshhcbpd3r3g.png]
   cross id178 (h) is the sum of the multiplication of the blue and red
   values for each image class

   since the blue plot is a one-hot vector, it has only a single element
   of 1, which is for the correct image class, all other multiplications
   in the cross id178 (h) is 0, and h simplifies to:
cross id178 (h) = -log(softmax(y_i))
where:
- y_i: predicted score/id203 for correct image class

putting everything together

   with the 3 proposed transformations, we can now apply the same
   techniques we used for id75, for id28. the
   code snippets below shows a side-by-side comparison between the linear
   regression code from [19]part 3, (available [20]here), and the changes
   required to make the code work for id28.
   [1*m6vq0c1i3jezam6yitlbpw.png]
   using id75 techniques for id28.
      total_class    is the number of classification classes, e.g., for
   digits, total_class = 10
    1. feature (x) transformation to 1-dimensional feature
    2. predicted outcome (y_), and actual outcome (y) transformation to
       one-hot vectors
    3. cost function transformation from squared error to cross id178.

   the changes can be best summarized in the cheatsheet below:
   [1*1l4lmdlxtcfxbs6urkokrw.png]
   visualizing id75 and id28 formula/code
   side-by-side

wrapping up

   id75 is useful to predict outcome based on some given
   features, while id28 is useful to help classify an input
   given the the input   s features.

   we show how we can adapt id75   s y = w.x + b to work for
   id28 by merely transforming (1) feature vector, x, (2)
   prediction/outcome vector, y/y   , and (3) cost function, h.

   armed with the knowledge of one-hot vectors, softmax, and
   cross-id178, you are now ready to tackle google   s so-called
      beginner   s    [21]tutorial on image classification, which is the goal of
   this tutorial series.

resources

     * google   s [22]code on image classification for beginner   s
     * the [23]slides on slideshare
     * the [24]video on youtube

     * [25]machine learning
     * [26]data science
     * [27]deep learning
     * [28]tensorflow
     * [29]id28

   (button)
   (button)
   (button) 340 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [30]go to the profile of soon hin khor

[31]soon hin khor

   use tech to make the world more caring, and responsible. nat. univ.
   singapore, carnegie mellon univ, univ. of tokyo. ibm, 500startups &
   y-combinator companies

     (button) follow
   [32]all of us are belong to machines

[33]all of us are belong to machines

   writings about machine learning, and artificial intelligence

     * (button)
       (button) 340
     * (button)
     *
     *

   [34]all of us are belong to machines
   never miss a story from all of us are belong to machines, when you sign
   up for medium. [35]learn more
   never miss a story from all of us are belong to machines
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/2afd0cabc54
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/all-of-us-are-belong-to-machines?source=avatar-lo_tvvaykgiu8gi-ad462a2018c0
   7. https://medium.com/all-of-us-are-belong-to-machines?source=logo-lo_tvvaykgiu8gi---ad462a2018c0
   8. https://medium.com/m/signin?redirect=https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-4-logistic-regression-2afd0cabc54&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-4-logistic-regression-2afd0cabc54&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@khor?source=post_header_lockup
  11. https://medium.com/@khor
  12. https://medium.com/all-of-us-are-belong-to-machines/the-gentlest-introduction-to-tensorflow-248dc871a224
  13. https://medium.com/@khor/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f
  14. https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-part-3-matrices-multi-feature-linear-regression-30a81ebaaa6c
  15. https://medium.com/all-of-us-are-belong-to-machines/the-gentlest-introduction-to-tensorflow-248dc871a224
  16. https://colah.github.io/posts/2015-09-visual-information/
  17. https://www.youtube.com/watch?v=f8g_6txklxw
  18. http://www.slideshare.net/khorsoonhin/gentlest-introduction-to-tensorflow-part-3
  19. https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-part-3-matrices-multi-feature-linear-regression-30a81ebaaa6c
  20. https://github.com/nethsix/gentle_tensorflow/blob/master/code/linear_regression_multi_feature_using_mini_batch_with_tensorboard.py
  21. https://www.tensorflow.org/tutorials/mnist/beginners/
  22. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py
  23. http://www.slideshare.net/khorsoonhin/gentlest-introduction-to-tensorflow-part-3
  24. https://www.youtube.com/watch?v=f8g_6txklxw
  25. https://medium.com/tag/machine-learning?source=post
  26. https://medium.com/tag/data-science?source=post
  27. https://medium.com/tag/deep-learning?source=post
  28. https://medium.com/tag/tensorflow?source=post
  29. https://medium.com/tag/logistic-regression?source=post
  30. https://medium.com/@khor?source=footer_card
  31. https://medium.com/@khor
  32. https://medium.com/all-of-us-are-belong-to-machines?source=footer_card
  33. https://medium.com/all-of-us-are-belong-to-machines?source=footer_card
  34. https://medium.com/all-of-us-are-belong-to-machines
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://medium.com/p/2afd0cabc54/share/twitter
  38. https://medium.com/p/2afd0cabc54/share/facebook
  39. https://medium.com/p/2afd0cabc54/share/twitter
  40. https://medium.com/p/2afd0cabc54/share/facebook
