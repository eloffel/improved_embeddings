   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

convolutional methods for text

   [9]go to the profile of tal perry
   [10]tal perry (button) blockedunblock (button) followfollowing
   may 21, 2017

tl;dr

     * id56s work great for text but convolutions can do it faster
     * any part of a sentence can influence the semantics of a word. for
       that reason we want our network to see the entire input at once
     * getting that big a receptive can make gradients vanish and our
       networks fail
     * we can solve the vanishing gradient problem with densenets or
       dilated convolutions
     * sometimes we need to generate text. we can use    deconvolutions    to
       generate arbitrarily long outputs.

intro

   over the last three years, the field of nlp has gone through a huge
   revolution thanks to deep learning. the leader of this revolution has
   been the recurrent neural network and particularly its manifestation as
   an lstm. concurrently the field of id161 has been reshaped by
   convolutional neural networks. this post explores what we    text people   
   can learn from our friends who are doing vision.

common nlp tasks

   to set the stage and agree on a vocabulary, i   d like to introduce a few
   of the more common tasks in nlp. for the sake of consistency, i   ll
   assume that all of our model   s inputs are characters and that our    unit
   of observation    is a sentence. both of these assumptions are just for
   the sake of convenience and you can replace characters with words and
   sentences with documents if you so wish.

classification

   perhaps the oldest trick in the book, we often want to classify a
   sentence. for example, we might want to classify an email subject as
   indicative of spam, guess the sentiment of a product review or assign a
   topic to a document.

   the straightforward way to handle this kind of task with an id56 is to
   feed entire sentence into it, character by character, and then observe
   the id56s final hidden state.

sequence labeling

   sequence labeling tasks are tasks that return an output for each input.
   examples include part of speech labeling or entity recognition tasks.
   while the bare bones lstm model is far from the state of the art, it is
   easy to implement and offers compelling results. see [11]this paper for
   a more fleshed out architecture
   [1*dg7zvfdvcxvogxuoaipzkg.png]
   an example tagger with a bidirectional lstm. [12]source

sequence generation

   arguably the most impressive results in recent nlp have been in
   translation. translation is a mapping of one sequence to another, with
   no guarantees on the length of the output sentence. for example,
   translating the first words of the bible from hebrew to english is
                =    in the beginning   .
   image result for id195
   a basic encoder decoder for english to german translation ([13]source)

   at the core of this success is the sequence to sequence (aka encoder
   decoder) framework, a methodology to    compress    a sequence into a code
   and then decode it to another sequence. notable examples include
   translation (encode hebrew and decode to english), image captioning
   (encode an image and decode a textual description of its contents)
   [0*1iqz-fvs9wl78mc6.png]
   image captioning with sequence to sequence models ([14]source)

   the basic encoder step is similar to the scheme we described for
   classification. what   s amazing is that we can build a decoder that
   learns to generate arbitrary length outputs.

   the two examples above are really both translation, but sequence
   generation is a bit broader than that. openai recently [15]published a
   paper where they learn to generate    amazon reviews    while controlling
   the sentiment of the output
   [1*vcipyrmdgi46mtsejqreha.png]
   example outputs from openai. they trained a charterer level language
   model on amazon reviews. they discovered a single neuron in their model
   responsible for sentiment and by fixing its value were able to generate
   new reviews with a particular sentiment. ([16]source)

   another personal favorite is the paper [17]generating sentences from a
   continuous space. in that paper, they trained a variational autoencoder
   on text, which led to the ability to interpolate between two sentences
   and get coherent results.
   [1*rldudo1bmeaf_zrl-quk1w.png]
   interpolating between sentences (homotopies) . ([18]source)
     __________________________________________________________________

requirements from an nlp architecture

   what all of the implementations we looked at have in common is that
   they use a recurrent architecture, usually an lstm (if your not sure
   what that is, [19]here is a great intro) . it is worth noting that none
   of the tasks had recurrent in their name, and none mentioned lstms.
   with that in mind, lets take a moment to think what id56s and
   particularly lstms provide that make them so ubiquitous in nlp.

arbitrary input size

   a standard feed forward neural network has a parameter for every input.
   this becomes problematic when dealing with text or images for a few
   reasons.
    1. it restricts the input size we can handle. our network will have a
       finite number of input nodes and won   t be able to grow to more.
    2. we lose a lot of common information. consider the sentences    i like
       to drink beer a lot    and    i like to drink a lot of beer   . a feed
       forward network would have to learn about the concept of    a lot   
       twice as it appears in different input nodes each time.

   recurrent neural networks solve this problem. instead of having a node
   for each input, we have a big    box    of nodes that we apply to the input
   again and again. the    box    learns a sort of transition function, which
   means that the outputs follow some recurrence relation, hence the name.

   remember that the vision people got a lot of the same effect for images
   using convolutions. that is, instead of having an input node for each
   pixel, convolutions allowed the reuse of the same, small set of
   parameters across the entire image.

long term dependencies

   the promise of id56s is their ability to implicitly model long term
   dependencies. the picture below is taken from openai. they trained a
   model that ended up recognizing sentiment and colored the text,
   character by character, with the model   s output. notice how the model
   sees the word    best    and triggers a positive sentiment which it carries
   on for over 100 characters. that   s capturing a long range dependency.
   [1*jkjf4jaztpnbhhjjlkfnrq.gif]

   the theory of id56s promises us long range dependencies out of the box.
   the practice is a little more difficult. when we learn via
   id26, we need to propagate the signal through the entire
   recurrence relation. the thing is, at every step we end up multiplying
   by a number. if those numbers are generally smaller than 1, our signal
   will quickly go to 0. if they are larger than 1, then our signal will
   explode.

   these issues are called the vanishing and exploding gradient and are
   generally resolved by lstms and a few clever tricks. i mention them
   know because we   ll encounter these problems again with convolutions and
   will need another way to address them.

advantages of convolutions

   so far we   ve seen how great lstms are, but this post is about
   convolutions. in the spirit of don   t fix what ain   t broken, we have to
   ask ourselves why we   d want to use convolutions at all.

   one answer is     because we can   .

   but there are two other compelling reasons to use convolutions, speed,
   and context.

parrelalisation

   id56s operate sequentially, the output for the second input depends on
   the first one and so we can   t parallelise an id56. convolutions have no
   such problem, each    patch    a convolutional kernel operates on is
   independent of the other meaning that we can go over the entire input
   layer concurrently.

   there is a price to pay for this, as we   ll see we have to stack
   convolutions into deep layers in order to view the entire input and
   each of those layers is calculated sequentially. but the calculations
   at each layer happen concurrently and each individual computation is
   small (compared to an lstm) such that in practice we get a big speed
   up.

   when i set out to write this i only had my own experience and google   s
   bytenet to back this claim up. just this week, facebook published their
   fully convolutional translation model and reported a 9x speed up over
   lstm based models.

view the whole input at once

   lstms read their input from left to right (or right to left) but
   sometimes we   d like to have the context of the end of the sentence
   influence the networks thoughts about its begining. for example, we
   might have a sentence like    i   d love to buy your product. not!    and
   we   d like that negation at the end to influence the entire sentence.

   iframe: [20]/media/e0132c1fcfca26054e8095fdc3f14412?postid=d5260fd5675f

   borat learns to tell a    not    joke

   with lstms we achieve this by running two lstms, one left to right and
   the other right to left and concatenating their outputs. this works
   well in practice but doubles our computational load.

   convolutions, on the other hand, grow a larger    receptive field    as we
   stack more and more layers. that means that by default, each    step    in
   the convolution   s representation views all of the input in its
   receptive field, from before and after it. i   m not aware of any
   definitive argument that this is inherently better than an lstm, but it
   does give us the desired effect in a controllable fashion and with a
   low computational cost.
     __________________________________________________________________

   so far we   ve set up our problem domain and talked a bit about the
   conceptual advantages of convolutions for nlp. from here out, i   d like
   to translate those concepts into practical methods that we can use to
   analyze and construct our networks.

practical convolutions for text

   [1*1rpnaf-_fgldivcbvhndwa.gif]
   [21]source

   you   ve probably seen an animation like the one above illustrating what
   a convolution does. the bottom is an input image, the top is the result
   and the gray shadow is the convolutional kernel which is repeatedly
   applied.

   this all makes perfect sense except that the input described in the
   picture is an image, with two spatial dimensions (height and width).
   we   re talking about text, which has only one dimension, and it   s
   temporal not spatial.

   for all practical purposes, that doesn   t matter. we just need to think
   of our text as an image of width n and height 1. tensorflow provides a
   conv1d function that does that for us, but it does not expose other
   convolutional operations in their 1d version.

   to make the    text = an image of height 1    idea concrete, let   s see how
   we   d use the 2d convolutional op in tensorflow on a sequence of
   embedded tokens.

   iframe: [22]/media/809abd9c6e7d8db58f98784d5a2ed419?postid=d5260fd5675f

   so what we   re doing here is changing the shape of input with
   tf.expand_dims so that it becomes an    image of height 1   . after running
   the 2d convolution operator we squeeze away the extra dimension.

hierarchy and receptive fields

   [0*6d1pomkvqeck6vgp.png]

   many of us have seen pictures like the one above. it roughly shows the
   hierarchy of abstractions a id98 learns on images. in the first layer,
   the network learns basic edges. in the next layer, it combines those
   edges to learn more abstract concepts like eyes and noses. finally, it
   combines those to recognize individual faces.

   with that in mind, we need to remember that each layer doesn   t just
   learn more abstract combinations of the previous layer. successive
   layers, implicitly or explicitly, see more of the input
   [0*speite5rsfksubbp.png]
   each    a    sees two inputs. each    b    sees two    as    which have a common
   input so each    b    has a receptive field of 3. each b is exposed to
   exactly 3 inputs.

increasing receptive field

   with vision often we   ll want the network to identify one or more
   objects in the picture while ignoring others. that is, we   ll be
   interested in some local phenomenon but not in a relationship that
   spans the entire input.
   [1*f8xwuumu27h9pdlymete5q.png]
   a convolutional network learns to recognize hotdogs. it doesn   t care
   what the hot dog is on, that the table is made of wood etc. it only
   cares if it saw a hotdog. ([23]source)

   text is more subtle as often we   ll want intermediate representations of
   our data to carry as much context about their surroundings as they
   possibly can. in other words, we want to have as large a receptive
   field as possible. their are a few ways to go about this.

larger filters

   the first, most obvious, way is to increase the filter size, that is
   doing a [1x5] convolution instead of a [1x3]. in my work with text,
   i   ve not had great results with this and i   ll offer my speculations as
   to why.

   in my domain, i mostly deal with character level inputs and with texts
   that are morphologicaly very rich. i think of (at least the first)
   layers of convolution as learning id165s so that the width of the
   filter corresponds to bigrams, trigrams etc. having the network learn
   larger id165s early exposes it to fewer examples, as there are more
   occurrences of    ab    in a text than    abb   .

   i   ve never proved this interpretation but have gotten consistently
   poorer results with filter widths larger than 3.

adding layers

   as we saw in the picture above, adding more layers will increase the
   receptive field. [24]dang ha the hien wrote a [25]great guide to
   calculating the receptive field at each layer which i encourage you to
   read.

   adding layers has two distinct but related effects. the one that gets
   thrown around a lot is that the model will learn to make higher level
   abstractions over the inputs that it gets (pixels =>edges => eyes =>
   face). the other is that the receptive field grows at each step .

   this means that given enough depth, our network could look at the
   entire input layer though perhaps through a haze of abstractions.
   unfortunately this is where the vanishing gradient problem may rear its
   ugly head.

the gradient / receptive field trade off

   neural networks are networks that information flows through. in the
   forward pass our input flows and transforms, hopefully becoming a
   representation that is more amenable to our task. during the back phase
   we propagate a signal, the gradient, back through the network. just
   like in vanilla id56s, that signal gets multiplied frequently and if it
   goes through a series of numbers that are smaller than 1 then it will
   fade to 0. that means that our network will end up with very little
   signal to learn from.

   this leaves us with something of a tradeoff. on the one hand, we   d like
   to be able to take in as much context as possible. on the other hand,
   if we try to increase our receptive fields by stacking layers we risk
   vanishing gradients and a failure to learn anything.

two solutions to the vanishing gradient problem

   luckily, many smart people have been thinking about these problems.
   luckier still, these aren   t problems that are unique to text, the
   vision people also want larger receptive fields and information rich
   gradients. let   s take a look at some of their crazy ideas and use them
   to further our own textual glory.

residual connections

   2016 was another great year for the vision people with at least two
   very popular architectures emerging, [26]resnets and [27]densenets (the
   densenet paper, in particular, is exceptionally well written and well
   worth the read) . both of them address the same problem    how do i make
   my network very deep without losing the gradient signal?   

   [28]arthur juliani wrote a fantastic overview of [29]resnet, densenets
   and id199 for those of you looking for the details and
   comparison. i   ll briefly touch on densenets which take the core concept
   to its extreme.
   [1*kojux1st5rndozwwlwrgkw.png]
   a densenet architecture ([30]source)

   the general idea is to reduce the distance between the signal coming
   from the networks loss and each individual layer. the way this is done
   is by adding a residual/direct connection between every layer and its
   predecessors. that way, the gradient can flow from each layer to its
   predecessors directly.

   densenets do this in a particularly interesting way. they concatenate
   the output of each layer to its input such that:
    1. we start with an embedding of our inputs, say of dimension 10.
    2. our first layer calculates 10 feature maps. it outputs the 10
       feature maps concatenated to the original embedding.
    3. the second layer gets as input 20 dimensional vectors (10 from the
       input and 10 from the previous layer) and calculates another 10
       feature maps. thus it outputs 30 dimensional vectors.

   and so on and so on for as many layers as you   d like. the paper
   describes a boat load of tricks to make things manageable and efficient
   but that   s the basic premise and the vanishing gradient problem is
   solved.

   there are two other things i   d like to point out.
    1. i previously mentioned that upper layers have a view of the
       original input that may be hazed by layers of abstraction. one of
       the highlights of concatenating the outputs of each layer is that
       the original signal reaches the following layers intact, so that
       all layers have a direct view of lower level features, essentially
       removing some of the haze.
    2. the residual connection trick requires that all of our layers have
       the same shape. that means that we need to pad each layer so that
       its input and output have the same spatial dimensions [1xwidth].
       that means that, on its own, this kind of architecture will work
       for sequence labeling tasks (where the input and the output have
       the same spatial dimensions) but will need more work for encoding
       and classification tasks (where we need to reduce the input to a
       fixed size vector or set of vectors). the densenet paper actually
       handles this as their goal is to do classification and we   ll expand
       on this point later.

dilated convolutions

   dilated convolutions aka atrous convolutions aka convolutions with
   holes are another method of increasing the receptive field without
   angering the gradient gods. when we looked at stacking layers so far,
   we saw that the receptive field grows linearly with depth. dilated
   convolutions let us grow the receptive field exponentially with depth.

   you can find an almost accessible explanation of dilated convolutions
   in the paper [31]multi scale context aggregation by dilated
   convolutions which uses them for vision. while conceptually simple, it
   took me a while to understand exactly what they do, and i may still
   have it not quite right.

   the basic idea is to introduce    holes    into each filter, so that it
   doesn   t operate on adjacent parts of the input but rather skips over
   them to parts further away. note that this is different from applying a
   convolution with stride >1. when we stride a filter, we skip over parts
   of the input between applications of the convolution. with dilated
   convolutions, we skip over parts of the input within a single
   application of the convolution. by cleverly arranging growing dilations
   we can achieve the promised exponential growth in receptive fields.
   [0*3taoht7v18nqewlm.]
   here we have a 3x3 filter applied with a dilation of 1,2 and 3. with a
   dilation of 1 we have a standard convolution. with a dilation of 2 we
   apply the same 3x3 filter but use every second pixel. ([32]source)
     __________________________________________________________________

   we   ve talked a lot of theory so far, but we   re finally at a point where
   we can see this stuff in action!

   a personal favorite paper is [33]id4 in linear
   time. it follows the encoder decoder structure we talked about in the
   beginning. we still don   t have all the tools to talk about the decoder,
   but we can see the encoder in action.
   [1*ibnuidvcy5gpbwckgmquza.png]
   the encoder part of bytenet based on dilated convolutions. notice how
   four layers in the effective receptive field is 16. even though the
   filter widths are just 3. ([34]source)

   and here   s an english input

     director jon favreau, who is currently working on disney   s
     forthcoming jungle book film, told the website hollywood reporter:
        i think times are changing.   

   and its translation, brought to you by dilated convolutions

     regisseur jon favreau, der zur zeit an disneys kommendem jungle book
     film arbeitet, hat der website hollywood reporter gesagt:    ich
     denke, die zeiten andern sich   .

   and as a bonus, remember that sound is just like text, in the sense
   that it has just one spatial/temporal dimension. check out deepmind   s
   [35]wavenet which uses dilated convolutions (and a lot of other magic)
   to generate [36]human sounding speech and [37]piano music.
     __________________________________________________________________

getting stuff out of your network

   when we discussed densenets i mentioned that the use of residual
   connections forces us to keep the input and output length of our
   sequence the same, which is done via padding. this is great for tasks
   where we need to label each item in our sequence for example:
     * in parts of speech tagging where each word is a part of speech.
     * in entity recognition where we might label person, company, and
       other for everything else

   other times we   ll want to reduce our input sequence down to a vector
   representation and use that to predict something about the entire
   sentence
     * we might want to label an email as spam based on its content and or
       subject
     * predict if a certain sentence is sarcastic or not

   in these cases, we can follow the traditional approaches of the vision
   people and top off our network with convolutional layers that don   t
   have padding and/or use pooling operations.

   but sometimes we   ll want to follow the id195 paradigm, what
   [38]matthew honnibal succinctly called [39]embed, encode, attend,
   predict. in this case, we reduce our input down to some vector
   representation but then need to somehow up sample that vector back to a
   sequence of the proper length.

   this task entails two problems
     * how do we do upsampling with convolutions ?
     * how do we do exactly the right amount of up sampling?

   i still haven   t found the answer to the second question or at least
   have not yet understood it. in practice, it   s been enough for me to
   assume some upper bound on the maximum length of the output and then
   upsample to that point. i suspect facebooks new [40]translation paper
   may address this but have not yet read it deeply enough to comment.

upsampling with deconvolutions

   deconvolutions are our tool for upsampling. it   s easiest (for me) to
   understand what they do through visualizations. luckily, a few smart
   folks published a [41]great post on deconvolutions over at distill and
   included some fun visualizers. lets start with those.
   [1*tbztaipbtkqyo0mkhcf_za.png]
   convolution of stride 1 and width 3

   consider the image on top. if we take the bottom layer as the input we
   have a standard convolution of stride 1 and width 3. but, we can also
   go from top down, that is treat the top layer as the input and get the
   slightly larger bottom layer.

   if you stop to think about that for a second, this    top down    operation
   is already happening in your convolutional networks when you do back
   propagation, as the gradient signals need to propagate in exactly the
   way shown in the picture. even better, it turns out that this operation
   is simply the transpose of the convolution operation, hence the other
   common (and technically correct) name for this operation, transposed
   convolution.

   here   s where it gets fun. we can stride our convolutions to shrink our
   input. thus we can stride our deconvolutions to grow our input. i think
   the easiest way to understand how strides work with deconvolutions is
   to look at the following pictures.
   [1*tbztaipbtkqyo0mkhcf_za.png]
   convolution of stride 1 and width 3
   [1*zyzpaur5dugjncdt1_bnaw.png]
   a deconvolution with stride 2 and width 3

   we   ve already seen the top one. notice that each input (the top layer)
   feeds three of the outputs and that each of the outputs is fed by three
   inputs (except the edges).
   [1*xdtirridtqhaxlqvl0s-ga.png]
   a deconvolution with stride 3 and width 3

   in the second picture we place imaginary holes in our inputs. notice
   that now each of the outputs is fed by at most two inputs.
   [1*k6iqnpvmn3pdqodfzfzslg.png]
   stacking two deconvolutional layers one after the other. the top layer
   is stride 3 and width 3, while the second layer is stride 2 and width
   2. this grew our sequence length from 5 to 30, a factor of 6.

   in the third picture we   ve added two imaginary holes into out input
   layer and so each output is fed by exactly one input. this ends up
   tripling the sequence length of our output with respect to the sequence
   length of our input.

   finally, we can stack multiple deconvolutional layers to gradually grow
   our output layer to the desired size.

   a few things worth thinking about
    1. if you look at these drawings from bottom up, they end up being
       standard strided convolutions where we just added imaginary holes
       at the output layers (the white blocks)
    2. in practice, each    input    isn   t a single number but a vector. in
       the image world, it might be a 3 dimensional rgb value. in text it
       might be a 300 dimensional id27. if you   re (de)convolving
       in the middle of your network each point would be a vector of
       whatever size came out of the last layer.
    3. i point that out to convince you that their is enough information
       in the input layer of a deconvolution to spread across a few points
       in the output.
    4. in practice, i   ve had success running a few convolutions with
       length preserving padding after a deconvolution. i imagine, though
       haven   t proven, that this acts like a redistribution of
       information. i think of it like letting a steak rest after grilling
       to let the juices redistribute.

   [0*fqvi__8mynwh2cdh.png]

summary

   the main reason you might want to consider convolutions in your work is
   because they are fast. i think that   s important to make research and
   exploration faster and more efficient. faster networks shorten our
   feedback cycles.

   most of the tasks i   ve encountered with text end up having the same
   requirement of the architecture: maximize the receptive field while
   maintaining an adequate flow of gradients. we   ve seen the use of both
   densenets and dilated convolutions to achieve that.

   finally, sometimes we want to expand a sequence or a vector into a
   larger sequence. we looked at deconvolutions as a way to do
      upsampling    on text and as a bonus compared adding a convolution
   afterwards the letting a steak rest and redistribute its juices.

   i   d love to learn more about your thoughts and experiences with these
   kinds of models. share in the comments or ping me on twitter
   [42]@thetalperry

     * [43]machine learning
     * [44]deep learning
     * [45]chatbots
     * [46]ai

   (button)
   (button)
   (button) 2.2k claps
   (button) (button) (button) 23 (button) (button)

     (button) blockedunblock (button) followfollowing
   [47]go to the profile of tal perry

[48]tal perry

   founder of lighttag.io, platform to annotate text for nlp. google
   developer expert in ml. former nlp@citi cto@superfly

     * (button)
       (button) 2.2k
     * (button)
     *
     *

   [49]go to the profile of tal perry
   never miss a story from tal perry, when you sign up for medium.
   [50]learn more
   never miss a story from tal perry
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d5260fd5675f
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@talperry/convolutional-methods-for-text-d5260fd5675f&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@talperry/convolutional-methods-for-text-d5260fd5675f&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@talperry?source=post_header_lockup
  10. https://medium.com/@talperry
  11. https://arxiv.org/pdf/1508.01991.pdf
  12. https://arxiv.org/pdf/1508.01991.pdf
  13. https://medium.com/@devnag/id195-the-clown-car-of-deep-learning-f88e1204dac3
  14. http://kelvinxu.github.io/projects/capgen.html
  15. https://blog.openai.com/unsupervised-sentiment-neuron/
  16. https://blog.openai.com/unsupervised-sentiment-neuron/
  17. https://arxiv.org/pdf/1511.06349.pdf
  18. https://arxiv.org/pdf/1511.06349.pdf
  19. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  20. https://medium.com/media/e0132c1fcfca26054e8095fdc3f14412?postid=d5260fd5675f
  21. https://github.com/vdumoulin/conv_arithmetic
  22. https://medium.com/media/809abd9c6e7d8db58f98784d5a2ed419?postid=d5260fd5675f
  23. https://itunes.apple.com/app/ 12457521
  24. https://medium.com/@nikasa1889
  25. https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807
  26. https://arxiv.org/abs/1512.03385
  27. https://arxiv.org/abs/1608.06993
  28. https://medium.com/@awjuliani
  29. https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32
  30. https://arxiv.org/abs/1608.06993
  31. https://arxiv.org/pdf/1511.07122.pdf
  32. https://www.quora.com/what-is-the-difference-between-dilated-convolution-and-convolution+stride
  33. https://arxiv.org/pdf/1610.10099.pdf
  34. https://arxiv.org/pdf/1610.10099.pdf
  35. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  36. https://storage.googleapis.com/deepmind-media/pixie/knowing-what-to-say/second-list/speaker-1.wav
  37. https://storage.googleapis.com/deepmind-media/pixie/making-music/sample_4.wav
  38. https://medium.com/@honnibal
  39. https://explosion.ai/blog/deep-learning-formula-nlp
  40. https://s3.amazonaws.com/fairseq/papers/convolutional-sequence-to-sequence-learning.pdf
  41. http://distill.pub/2016/deconv-checkerboard/
  42. https://twitter.com/thetalperry
  43. https://medium.com/tag/machine-learning?source=post
  44. https://medium.com/tag/deep-learning?source=post
  45. https://medium.com/tag/chatbots?source=post
  46. https://medium.com/tag/ai?source=post
  47. https://medium.com/@talperry?source=footer_card
  48. https://medium.com/@talperry
  49. https://medium.com/@talperry
  50. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  52. https://medium.com/p/d5260fd5675f/share/twitter
  53. https://medium.com/p/d5260fd5675f/share/facebook
  54. https://medium.com/p/d5260fd5675f/share/twitter
  55. https://medium.com/p/d5260fd5675f/share/facebook
