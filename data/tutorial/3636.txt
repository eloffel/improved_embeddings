              [bam-logo.svg] [1]the behance artistic media dataset

what's inside

     * automatically-labeled binary attribute scores for over 2.5 million
       images across 20 attributes each
     * 393,000 crowdsourced binary attribute labels for individual images
     * short image descriptions/captions for 74,000 images from the crowd
     * image urls for all images mentioned above

collaborators

     * michael wilber, [2]m...@cornell.edu
     * chen fang,
     * hailin jin,
     * aaron hertzmann,
     * john collomosse,
     * serge belongie

institutions

   [adobe_standard_logo.svg]
   [cornell-tech.svg]

what kind of images does behance-artistic-media have?

   our dataset is built from [3]behance, a portfolio website for
   professional and commercial artists. behance contains over ten million
   projects and 65 million images.

   artwork on behance spans many fields, such as sculpture, painting,
   photography, graphic design, graffiti, illustration, and advertising.
   graphic design and advertising make up roughly one third of behance.
   photography, drawings, and illustrations make up roughly another third.
   this artwork is posted by professional artists to show off samples of
   their best work.

   here is a sample of the top-scoring images for your chosen attributes.
   at our quality threshold, you should expect 90% precision from these
   results. this should give you a sample of results that you can expect.

what space do artistic images span?

   we consider a subset of 1,000 images that score high on each attribute.
   we then take the final layer off of a pre-trained resnet and embed
   these images into 512-dimensional feature space. finally, we use id167
   to project these features down to two dimensions.

   the resulting embedding is shown below. these images are typically
   arranged into discrete clusters that capture content and media. cluster
   1 shows watercolor images; cluster 2 is comprised of oil paintings;
   cluster 3 has vector art; cluster 4 contains gloomy photographs of
   abandoned buildings or lonely landscapes; cluster 5 shows various pen
   and pencil sketches.

   you can explore this embedding above. click+drag to pan, scroll to
   zoom. note that this visualization could take several seconds to load.
   [id104-loop.svg]

   id161 systems are designed to work well within the context of
   everyday photography. however, artists often render the world around
   them in ways that do not resemble photographs. artwork produced by
   people is not constrained to mimic the physical world, making it more
   challenging for machines to recognize.

   this work is a step toward teaching machines how to categorize images
   in ways that are valuable to humans. we collect a large-scale dataset
   of contemporary artwork from behance, a website containing millions of
   portfolios from professional and commercial artists. we annotate
   behance imagery with rich attribute labels for content, emotions, and
   artistic media. we believe our behance artistic media dataset will be a
   good starting point for researchers wishing to study artistic imagery
   and relevant problems.

   our dataset is built from [4]behance, a portfolio website for
   professional and commercial artists. behance contains over ten million
   projects and 65 million images.

   artwork on behance spans many fields, such as sculpture, painting,
   photography, graphic design, graffiti, illustration, and advertising.
   graphic design and advertising make up roughly one third of behance.
   photography, drawings, and illustrations make up roughly another third.
   this artwork is posted by professional artists to show off samples of
   their best work.

   our dataset requires some level of human expertise to label, but it is
   too costly to collect labels for all images. to address this issue, we
   use a hybrid human-in-the-loop strategy to incrementally learn a binary
   classifier for each attribute. our hybrid annotation strategy is based
   on the [5]lsun dataset annotation pipeline.

   at each step, humans label the most informative samples in the dataset
   with a single binary attribute label. the resulting labels are added to
   each classifier's training set to improve its discrimination. the
   classifier then ranks more images, and the most informative images are
   sent to the crowd for the next iteration. after four iterations, the
   final classifier re-scores the entire dataset and images that surpass a
   certain score threshold are assumed to be positive. this final
   threshold is chosen to meet certain precision and recall targets on a
   held-out validation set. this entire process is repeated for each
   attribute we wish to collect.

quality guarantees

   [label-precision.png]

   as a quality check, we tested whether the final labeling set meets our
   desired quality target of 90% precision. for each attribute, we show
   annotators 100 images from the final automatically-labeled positive set
   and 100 images from the final negative set using the same interface
   used to collect the dataset. the mean precision across all attributes
   is 90.4%, where precision is the number of positive images where at
   least one annotator indicates the image should be positive.

   these checks are in addition to our mturk quality checks: we only use
   human labels where two workers agree and we only accept work from
   turkers with a high reputation who have completed 10,000 tasks at 95%
   acceptance.

   we are making all of our crowd annotations and captions available for
   download along with a subset of automatically-labeled images.

   to download this dataset, click below to request an account. you can
   then sign in and download the dataset after completing some annotations
   for the next version.
   [6]sign in to download
   [7]request an account
   to download the dataset, please help us annotate the next version! we
   are collecting a larger set of object categories and need your help to
   define them. it's also a fun way to get acclimated to the dataset.
   [help-us-annotate.jpg]
   (button) help annotate the next version!

read the paper

   [8]read the paper on arxiv [9]read the iccv version on the cv
   foundation website

bibtex

   if you use our dataset, please cite the iccv version:
@inproceedings{wilber_2017_iccv,
    author = {wilber, michael j. and fang, chen and jin, hailin and hertzmann, a
aron and collomosse, john and belongie, serge},
    title = {bam! the behance artistic media dataset for recognition beyond phot
ography},
    booktitle = {the ieee international conference on id161 (iccv)},
    month = {oct},
    year = {2017}
}

   [paper-overview-small.png]

references

   visible links
   1. https://bam-dataset.org/
   2. http://www.google.com/recaptcha/mailhide/d?k=01z0uzp0grq-zf5qnicacouq==&c=jwssq4d1mubjtekufqdq05cr56rfz_gmj0elaqdpzyw=
   3. http://behance.net/
   4. http://behance.net/
   5. http://www.yf.io/p/lsun
   6. https://bam-dataset.org/signin
   7. https://docs.google.com/forms/d/e/1faipqlselcc352t6dmtl8jdq7cvzhaxzm_qim1flcvaq1duwg2utiww/viewform?usp=sf_link
   8. https://arxiv.org/abs/1704.08614
   9. http://openaccess.thecvf.com/content_iccv_2017/html/wilber_bam_the_behance_iccv_2017_paper.html

   hidden links:
  11. https://bam-dataset.org/annotation-instructions
