   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

types of optimization algorithms used in neural networks and ways to optimize
id119

   [16]go to the profile of anish singh walia
   [17]anish singh walia (button) blockedunblock (button) followfollowing
   jun 10, 2017

   have you ever wondered which optimization algorithm to use for your
   neural network model to produce slightly better and faster results by
   updating the model parameters such as weights and bias values . should
   we use id119 or stochastic id119 or adam ?

   i too didn   t know about the major differences between these different
   types of optimization strategies and which one is better over another
   before writing this article.

note:

     having a good theoretical knowledge is amazing but implementing them
     in code in a real time deep learning project is a complete different
     thing. you might get different and unexpected results based on
     different problems and datasets. so as a bonus,i am also adding the
     links to the various courses which has helped me a lot in my journey
     to learn data science and ml, experiment and compare different
     optimization strategies which led me to writing this article on
     comparisons between different optimizers while implementing deep
     learning and comparing the different optimization stretegies. below
     are some of the resources which have helped me a lot to become what
     i am today. so i just wanted to give a side note to give it back to
     the people and community and share the resources which have helped
     me in my journey.

     1)this is the link to the course by datacamp on [18]deep learning in
     python using keras package or definitely you can start with[19]
     building id98 for image processing using keras . if understanding
     deep learning and ai fundamentals is what you want right now then
     the above 2 courses are the best deep learning courses you can find
     out there to learn fundamentals of deep learning and also implement
     it in python. these were my first deep learning course which has
     helped me a lot to properly understand the basics.

     2)these courses on [20]building chatbots in python and [21]nlp
     fundamentals in python using nltk are also recommended for people
     interested more in learning ai and deep learning. so go give it a
     try on the basis of your interest.

     3) [22]machine learning in python using scikit-learn- this course
     will teach you how to implement supervised learning algorithms in
     python with different datasets.

     4)[23]data wrangling and manipulating data frames using pandas-this
     amazing course will help you perform data wrangling and data
     pre-processing in python. and a data scientist spends most of his
     time doing pre-processing and data wrangling. so this course might
     come out to be handy for beginners.

     5) this course teaches you the intermediate level [24]python for
     data science and this foundation course on [25]data scientist with
     python is the best option if you want to start you career as a data
     scientist and learn all the required fundamentals in the industry
     today using python.

     6) recently data camp has started an new program where they are
     providing various real world projects and problems statements to
     help data enthusiasts build a strong practical data science
     foundation along with their courses. so try any of these
     [26]projects out. it is surely very exciting and will help you learn
     faster and better. recently i completed a project on [27]exploring
     the evolution of linux and it was an amazing experience.

     7) r users , don   t worry i also have some hand picked best r courses
     for you to get started with building data science and machine
     learning foundations and also doing it side by side using this
     amazing [28]data science with r course which will teach you the
     complete fundamentals. trust me this one is worth your time and
     energy.

     8) this course is also one of the best for understanding basics of
     machine learning in r called[29] machine learning toolbox .

     9) all data science projects start from exploring the data and it is
     one of the most important tasks for a data scientist to know the
     dataset inside out so this lovely course on [30]exploratory data
     analysis using r is what you need to start any data analytics and
     data science project. also this course on [31]statistical modelling
     in r would be useful for all the aspiring data scientists like
     me.statistics is the foundation of data science.

     10) [32]pre-processing data for machine learning [33]tasks is
     another handy course for deep learning and ml enthusiasts, as it is
     one of the most important and first tasks you perform for any data
     science project.

p.s : i actually insist the readers to try out any of the above courses as
per their interest, to get started and build a good foundation in machine
learning and data science. the best thing about these courses by datacamp is
that they explain it in a very elegant and different manner with a balanced
focus on practical and well as conceptual knowledge and at the end there is
always a case study. this is what i love the most about them. these courses
are truly worth your time and money. these courses would surely help you also
understand and implement deep learning,machine learning in a better way and
also implement it in python or r. i am damn sure you will love it and i am
claiming this from my personal opinion and experience.

   coming back to the topic-

what are optimization algorithms ?

   optimization algorithms helps us to minimize (or maximize) an objective
   function (another name for error function) e(x) which is simply a
   mathematical function dependent on the model   s internal learnable
   parameters which are used in computing the target values(y) from the
   set of predictors(x) used in the model. for example         we call the
   weights(w) and the bias(b) values of the neural network as its internal
   learnable parameters which are used in computing the output values and
   are learned and updated in the direction of optimal solution i.e
   minimizing the loss by the network   s training process and also play a
   major role in the training process of the neural network model .

   the internal parameters of a model play a very important role in
   efficiently and effectively training a model and produce accurate
   results. this is why we use various optimization strategies and
   algorithms to update and calculate appropriate and optimum values of
   such model   s parameters which influence our model   s learning process
   and the output of a model.

     types of optimization algorithms ?

   optimization algorithm falls in 2 major categories -
    1. first order optimization algorithms         these algorithms minimize or
       maximize a id168 e(x) using its gradient values with
       respect to the parameters. most widely used first order
       optimization algorithm is id119.the first order
       derivative tells us whether the function is decreasing or
       increasing at a particular point. first order derivative basically
       give us a line which is tangential to a point on its error surface.

     what is a gradient of a function?

   a gradient is simply a vector which is a multi-variable generalization
   of a derivative(dy/dx) which is the instantaneous rate of change of y
   with respect to x. the difference is that to calculate a derivative of
   a function which is dependent on more than one variable or multiple
   variables, a gradient takes its place. and a gradient is calculated
   using partial derivatives . also another major difference between the
   gradient and a derivative is that a gradient of a function produces a
   vector field.

   a gradient is represented by a jacobian matrix         which is simply a
   matrix consisting of first order partial derivatives(gradients).

     hence summing up, a derivative is simply defined for a function
     dependent on single variables , whereas a gradient is defined for
     function dependent on multiple variables. now let   s not get more
     into calculas and physics.

   2. second order optimization algorithms         second-order methods use the
   second order derivative which is also called hessian to minimize or
   maximize the id168.the hessian is a matrix of second order
   partial derivatives. since the second derivative is costly to compute,
   the second order is not used much .the second order derivative tells us
   whether the first derivative is increasing or decreasing which hints at
   the function   s curvature.second order derivative provide us with a
   quadratic surface which touches the curvature of the error surface.

some advantages of second order optimization over first order    

   although the second order derivative may be a bit costly to find and
   calculate, but the advantage of a second order optimization technique
   is that is does not neglect or ignore the curvature of
   surface.secondly, in terms of step-wise performance they are better.

   you can search more on second order optimization algorithms
   here-h[34]ttps://web.stanford.edu/class/msande311/lecture13.pdf

so which order optimization strategy to use ?

    1. now the first order optimization techniques are easy to compute and
       less time consuming , converging pretty fast on large data sets.
    2. second order techniques are faster only when the second order
       derivative is known otherwise, these methods are always slower and
       costly to compute in terms of both time and memory.

     although ,sometimes newton   s second order optimization technique can
     sometimes outperform first order id119 techniques because
     second order techniques will not get stuck around paths of slow
     convergence around saddle points whereas id119 sometimes
     gets stuck and does not converges.

   best way to know which one converges fast is to try it out yourself.

now what are the different types of optimization algorithms used in neural
networks ?

id119

   id119 is the most important technique and the foundation of
   how we train and optimize intelligent systems. what is does is    

        oh id119         find the minima , control the variance and
     then update the model   s parameters and finally lead us to
     convergence   

     =             j(  )         is the formula of the parameter updates, where          is the
   learning rate ,      j(  )    is the gradient of id168-j(  ) w.r.t
   parameters-        .

   it is the most popular optimization algorithms used in optimizing a
   neural network. now id119 is majorly used to do weights
   updates in a neural network model , i.e update and tune the model   s
   parameters in a direction so that we can minimize the id168.
   now we all know a neural network trains via a famous technique called
   id26 , in which we first propagate forward calculating the
   dot product of inputs signals and their corresponding weights and then
   apply a [35]activation function to those sum of products, which
   transforms the input signal to an output signal and also is important
   to model complex non-linear functions and introduces non-linearities to
   the model which enables the model to learn almost any arbitrary
   functional mappings.

   after this we propagate backwards in the network carrying error terms
   and updating weights values using id119, in which we
   calculate the gradient of error(e) function with respect to the weights
   (w) or the parameters , and update the parameters (here weights) in the
   opposite direction of the gradient of the id168 w.r.t to the
   model   s parameters.
   [1*ir7vgblq6f70chhissyn2g.png]
   weight updates in the opposite direction of the gradient

     the image on above shows the process of weight updates in the
     opposite direction of the gradient vector of error w.r.t to the
     weights of the network. the u-shaped curve is the gradient(slope).
     as one can notice if the weight(w) values are too small or too large
     then we have large errors , so want to update and optimize the
     weights such that it is neither too small nor too large , so we
     descent downwards opposite to the gradients until we find a local
     minima.

variants of id119-

   the traditional batch id119 will calculate the gradient of
   the whole data set but will perform only one update , hence it can be
   very slow and hard to control for datasets which are very very large
   and don   t fit in the memory. how big or small of an update to do is
   determined by the learning rate -   , and it is guaranteed to converge
   to the global minimum for convex error surfaces and to a local minimum
   for non-convex surfaces.another thing while using standard batch
   id119 is that it computes redundant updates for large data
   sets.

   the above problems of standard id119 are rectified in
   stochastic id119.

1. stochastic id119

   stochastic id119(sgd) on the other hand performs a parameter
   update for each training example .it is usually much faster
   technique.it performs one update at a time.

     =             j(  ;x(i);y(i)) , where {x(i) ,y(i)} are the training examples.

   now due to these frequent updates ,parameters updates have high
   variance and causes the id168 to fluctuate to different
   intensities. this is actually a good thing because it helps us discover
   new and possibly better local minima , whereas standard gradient
   descent will only converge to the minimum of the basin as mentioned
   above.

   but the problem with sgd is that due to the frequent updates and
   fluctuations it ultimately complicates the convergence to the exact
   minimum and will keep overshooting due to the frequent fluctuations .

   although, it has been shown that as we slowly decrease the learning
   rate-  , sgd shows the same convergence pattern as standard gradient
   descent.
   [1*bs5uuwee_qxzowbdqumgda.png]
   high variance parameter updates for each training example cause the
   id168 to fluctuate heavily due to which we might not get the
   minimum value of parameter which gives us least loss value.

   the problems of high variance parameter updates and unstable
   convergence can be rectified in another variant called mini-batch
   id119.

2. mini batch id119

   an improvement to avoid all the problems and demerits of sgd and
   standard id119 would be to use mini batch id119
   as it takes the best of both techniques and performs an update for
   every batch with n training examples in each batch.

the advantages of using mini batch id119 are    

    1. it reduces the variance in the parameter updates , which can
       ultimately lead us to a much better and stable convergence.
    2. can make use of highly optimized matrix optimizations common to
       state-of-the-art deep learning libraries that make computing the
       gradient w.r.t. a mini-batch very efficient.
    3. commonly mini-batch sizes range from 50 to 256, but can vary as per
       the application and problem being solved.
    4. mini-batch id119 is typically the algorithm of choice
       when training a neural network nowadays

   p.s    actually the term sgd is used also when mini-batch gradient
   descent is used .

challenges faced while using id119 and its variants    

    1. choosing a proper learning rate can be difficult. a learning rate
       that is too small leads to painfully slow convergence i.e will
       result in small baby steps towards finding optimal parameter values
       which minimize loss and finding that valley which directly affects
       the overall training time which gets too large. while a learning
       rate that is too large can hinder convergence and cause the loss
       function to fluctuate around the minimum or even to diverge.
    2. additionally, the same learning rate applies to all parameter
       updates. if our data is sparse and our features have very different
       frequencies, we might not want to update all of them to the same
       extent, but perform a larger update for rarely occurring features.
    3. another key challenge of minimizing highly non-convex error
       functions common for neural networks is avoiding getting trapped in
       their numerous sub-optimal local minima. actually, difficulty
       arises in fact not from local minima but from saddle points, i.e.
       points where one dimension slopes up and another slopes down. these
       saddle points are usually surrounded by a plateau of the same
       error, which makes it notoriously hard for sgd to escape, as the
       gradient is close to zero in all dimensions.

optimizing the id119

   now we will discuss about the various algorithms which are used to
   further optimize id119.

momentum

   the high variance oscillations in sgd makes it hard to reach
   convergence , so a technique called momentum was invented which
   accelerates sgd by navigating along the relevant direction and softens
   the oscillations in irrelevant directions.in other words all it does is
   adds a fraction          of the update vector of the past step to the
   current update vector.

   v(t)=  v(t   1)+     j(  ).

   and finally we update parameters by   =     v(t) .

   the momentum term    is usually set to 0.9 or a similar value.

   here the momentum is same as the momentum in classical physics , as we
   throw a ball down a hill it gathers momentum and its velocity keeps on
   increasing.

   the same thing happens with our parameter updates    
    1. it leads to faster and stable convergence.
    2. reduced oscillations

   the momentum term    increases for dimensions whose gradients point in
   the same directions and reduces updates for dimensions whose gradients
   change directions. this means it does parameter updates only for
   relevant examples. this reduces the unnecessary parameter updates which
   leads to faster and stable convergence and reduced oscillations.

nesterov accelerated gradient

   a researcher named yurii nesterov saw a problem with momentum    

   however, a ball that rolls down a hill, blindly following the slope, is
   highly unsatisfactory. we   d like to have a smarter ball, a ball that
   has a notion of where it is going so that it knows to slow down before
   the hill slopes up again.

   what actually happens is that as we reach the minima i.e the lowest
   point on the curve ,the momentum is pretty high and it doesn   t knows to
   slow down at that point due to the high momentum which could cause it
   to miss the minima entirely and continue movie up. this problem was
   noticed by yurii nesterov.

   he published a research paper in 1983 which solved this problem of
   momentum and we now call this strategy nestrov accelerated gradient.

   in the method he suggested we first make a big jump based on out
   previous momentum then calculate the gradient and them make an
   correction which results in an parameter update. now this anticipatory
   update prevents us to go too fast and not miss the minima and makes it
   more responsive to changes.

   nesterov accelerated gradient (nag) is a way to give our momentum term
   this kind of prescience. we know that we will use our momentum term
     v(t   1) to move the parameters   . computing        v(t   1) thus gives us an
   approximation of the next position of the parameters which gives us a
   rough idea where our parameters are going to be. we can now effectively
   look ahead by calculating the gradient not w.r.t. to our current
   parameters    but w.r.t. the approximate future position of our
   parameters:

   v(t)=  v(t   1)+     j(        v(t   1) ) and then update the parameters using
     =     v(t) .

   one can refer more on nags here
   [36]http://cs231n.github.io/neural-networks-3/.

   now that we are able to adapt our updates to the slope of our error
   function and speed up sgd in turn, we would also like to adapt our
   updates to each individual parameter to perform larger or smaller
   updates depending on their importance.

adagrad

   it simply allows the learning rate -   to adapt based on the parameters.
   so it makes big updates for infrequent parameters and small updates for
   frequent parameters. for this reason, it is well-suited for dealing
   with sparse data.

     it uses a different learning rate for every parameter    at a time
     step based on the past gradients which were computed for that
     parameter.

   previously, we performed an update for all parameters    at once as
   every parameter   (i) used the same learning rate   . as adagrad uses a
   different learning rate for every parameter   (i) at every time step t,
   we first show adagrad   s per-parameter update, which we then vectorize.
   for brevity, we set g(t,i) to be the gradient of the id168
   w.r.t. to the parameter   (i) at time step t .
   [1*d4hiun7abyqu1ymxz7ajzq.png]
   the formula for parameter updates

   adagrad modifies the general learning rate    at each time step t for
   every parameter   (i) based on the past gradients that have been
   computed for   (i).

   the main benefit of adagrad is that we don   t need to manually tune the
   learning rate. most implementations use a default value of 0.01 and
   leave it at that.

disadvantage    

    1. its main weakness is that its learning rate-   is always decreasing
       and decaying.

   this happens due to the accumulation of each squared gradients in the
   denominator , since every added term is positive.the accumulated sum
   keeps growing during training. this in turn causes the learning rate to
   shrink and eventually become so small, that the model just stops
   learning entirely and stops acquiring new additional knowledge. because
   we know that as the learning rate gets smaller and smaller the ability
   of the model to learn fastly decreases and which gives very slow
   convergence and takes very long to train and learn i.e learning speed
   suffers and decreases.

   this problem of decaying learning rate is rectified in another
   algorithm called adadelta.

adadelta

   it is an extension of adagrad which tends to remove the decaying
   learning rate problem of it. instead of accumulating all previous
   squared gradients, adadelta limits the window of accumulated past
   gradients to some fixed size w.

   instead of inefficiently storing w previous squared gradients, the sum
   of gradients is recursively defined as a decaying mean of all past
   squared gradients. the running average e[g  ](t) at time step t then
   depends (as a fraction    similarly to the momentum term) only on the
   previous average and the current gradient.

   e[g  ](t)=  .e[g  ](t   1)+(1     ).g  (t) , we set    to a similar value as the
   momentum term, around 0.9.

       (t)=        g(t,i).

     (t+1)=  (t)+    (t).
   [1*nuxqz_ajagdyfswtccrn-g.png]
   the final formula for parameter updates

   another thing with adadelta is that we don   t even need to set a default
   learning rate .

what improvements we have done so far    

    1. we are calculating different learning rates for each parameter.
    2. we are also calculating momentum.
    3. preventing vanishing(decaying) learning rates.

   what more improvements can we do ?

     since we are calculating individual learning rates for each
     parameter , why not calculate individual momentum changes for each
     parameter and store them separately. this is where a new modified
     technique and improvement comes into play called as adam.

adam

   adam stands for adaptive moment estimation. adaptive moment estimation
   (adam) is another method that computes adaptive learning rates for each
   parameter. in addition to storing an exponentially decaying average of
   past squared gradients like adadelta ,adam also keeps an exponentially
   decaying average of past gradients m(t), similar to momentum:

   m(t) and v(t) are values of the first moment which is the mean and the
   second moment which is the uncentered variance of the gradients
   respectively.
   [1*oywn_rtkxt-r5_r9xapahq.png]
   the formulas for the first moment(mean) and the second moment (the
   variance) of the gradients

   then the final formula for the parameter update is    
   [1*tkn5tew-7aqoeraedb8x6g.png]

   the values for   1 is 0.9 , 0.999 for   2, and (10 x exp(-8)) for   .

   adam works well in practice and compares favorably to other adaptive
   learning-method algorithms as it converges very fast and the learning
   speed of the model is quiet fast and efficient and also it rectifies
   every problem that is faced in other optimization techniques such as
   vanishing learning rate , slow convergence or high variance in the
   parameter updates which leads to fluctuating id168

visualization of the optimization algorithms

   [1*xvfmo9nxlnwdr3sxzky-ra.gif]
   sgd optimization on loss surface contours
   [1*sjtkoauoxfvjwrr7icthia.gif]
   sgd optimization on saddle point

   from the above images one can see that the adaptive algorithms converge
   very fast and quickly find the right direction in which parameter
   updates should occur. whereas standard sgd , nag and momentum
   techniques are very slow and could not find the right direction.

conclusion

which optimizer should we use?

   the question was to choose the best optimizer for our neural network
   model in order to converge fast and to learn properly and tune the
   internal parameters so as to minimize the id168 .

     adam works well in practice and outperforms other adaptive
     techniques.

   if your input data is sparse then methods such as sgd,nag and momentum
   are inferior and perform poorly. for sparse data sets one should use
   one of the adaptive learning-rate methods. an additional benefit is
   that we won   t need to adjust the learning rate but likely achieve the
   best results with the default value.

   if one wants fast convergence and train a deep neural network model or
   a highly complex neural network then adam or any other adaptive
   learning rate techniques should be used because they outperforms every
   other optimization algorithms.

   i hope you guys liked the article and was able to give you a good
   intuition towards the different behaviors of different optimization
   algorithms.

references    

    1. optimizing id119-
       [37]http://sebastianruder.com/optimizing-gradient-descent/
    2. dean, j., corrado, g. s., monga, r., chen, k., devin, m., le, q.
       v,     ng, a. y. (2012). large scale distributed deep networks. nips
       2012: neural information processing systems.
       [38]http://doi.org/10.1109/icdar.2011.95
    3. ioffe, s., & szegedy, c. (2015). batch id172 : accelerating
       deep network training by reducing internal covariate shift. arxiv
       preprint arxiv:1502.03167v3.
    4. qian, n. (1999). on the momentum term in id119 learning
       algorithms. neural networks : the official journal of the
       international neural network society, 12(1), 145   151.
       [39]http://doi.org/10.1016/s0893-6080(98)00116-6
    5. kingma, d. p., & ba, j. l. (2015). adam: a method for stochastic
       optimization. international conference on learning representations
    6. zaremba, w., & sutskever, i. (2014). learning to execute, 1   25.
       retrieved from [40]http://arxiv.org/abs/1410.4615
    7. zhang, s., choromanska, a., & lecun, y. (2015). deep learning with
       elastic averaging sgd. neural information processing systems
       conference (nips 2015).retrieved from
       [41]http://arxiv.org/abs/1412.6651
    8. darken, c., chang, j., & moody, j. (1992). learning rate schedules
       for faster stochastic gradient search. neural networks for signal
       processing ii proceedings of the 1992 ieee workshop, (september).
       [42]http://doi.org/10.1109/nnsp.1992.253713

     * [43]optimization algorithms
     * [44]id158
     * [45]artificial intelligence
     * [46]deep learning
     * [47]towards data science

   (button)
   (button)
   (button) 3.5k claps
   (button) (button) (button) 27 (button) (button)

     (button) blockedunblock (button) followfollowing
   [48]go to the profile of anish singh walia

[49]anish singh walia

   a data nerd. in love with cloud computing, virtualization technologies
   and data science as well.:github [50]https://github.com/anishsingh20.

     (button) follow
   [51]towards data science

[52]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3.5k
     * (button)
     *
     *

   [53]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [54]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/95ae5d39529f
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_msfa7p5nsyw9---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@anishsingh20?source=post_header_lockup
  17. https://towardsdatascience.com/@anishsingh20
  18. https://www.datacamp.com/courses/deep-learning-in-python?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  19. https://www.datacamp.com/courses/convolutional-neural-networks-for-image-processing?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  20. https://www.datacamp.com/courses/building-chatbots-in-python?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  21. https://www.datacamp.com/courses/natural-language-processing-fundamentals-in-python?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  22. https://www.datacamp.com/courses/supervised-learning-with-scikit-learn?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  23. https://www.datacamp.com/courses/manipulating-dataframes-with-pandas?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  24. https://www.datacamp.com/courses/python-data-science-toolbox-part-1?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  25. https://www.datacamp.com/tracks/data-scientist-with-python?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  26. https://www.datacamp.com/projects?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  27. https://www.datacamp.com/projects/111?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  28. https://www.datacamp.com/tracks/data-scientist-with-r?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  29. https://www.datacamp.com/courses/machine-learning-toolbox?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  30. https://www.datacamp.com/courses/exploratory-data-analysis?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  31. https://www.datacamp.com/courses/statistical-modeling-in-r-part-1?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  32. https://www.datacamp.com/courses/preprocessing-for-machine-learning-in-python?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  33. https://www.datacamp.com/courses/preprocessing-for-machine-learning-in-python?tap_a=5644-dce66f&tap_s=210732-9d6bbf
  34. https://web.stanford.edu/class/msande311/lecture13.pdf
  35. https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f
  36. http://cs231n.github.io/neural-networks-3/
  37. http://sebastianruder.com/optimizing-gradient-descent/
  38. http://doi.org/10.1109/icdar.2011.95
  39. http://doi.org/10.1016/s0893-6080(98)00116-6
  40. http://arxiv.org/abs/1410.4615
  41. http://arxiv.org/abs/1412.6651
  42. http://doi.org/10.1109/nnsp.1992.253713
  43. https://towardsdatascience.com/tagged/optimization-algorithms?source=post
  44. https://towardsdatascience.com/tagged/artificial-neural-network?source=post
  45. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  46. https://towardsdatascience.com/tagged/deep-learning?source=post
  47. https://towardsdatascience.com/tagged/towards-data-science?source=post
  48. https://towardsdatascience.com/@anishsingh20?source=footer_card
  49. https://towardsdatascience.com/@anishsingh20
  50. https://github.com/anishsingh20
  51. https://towardsdatascience.com/?source=footer_card
  52. https://towardsdatascience.com/?source=footer_card
  53. https://towardsdatascience.com/
  54. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  56. https://medium.com/p/95ae5d39529f/share/twitter
  57. https://medium.com/p/95ae5d39529f/share/facebook
  58. https://medium.com/p/95ae5d39529f/share/twitter
  59. https://medium.com/p/95ae5d39529f/share/facebook
