   #[1]github [2]recent commits to lamtram:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]14
     * [35]star [36]131
     * [37]fork [38]35

[39]neubig/[40]lamtram

   [41]code [42]issues 8 [43]pull requests 2 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   lamtram: a toolkit for neural language and translation modeling
     * [47]186 commits
     * [48]2 branches
     * [49]0 releases
     * [50]fetching contributors
     * [51]lgpl-2.1

    1. [52]c++ 87.2%
    2. [53]m4 9.6%
    3. [54]perl 1.0%
    4. [55]c 0.8%
    5. [56]python 0.6%
    6. [57]makefile 0.5%
    7. [58]shell 0.3%

   (button) c++ m4 perl c python makefile shell
   branch: master (button) new pull request
   [59]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/n
   [60]download zip

downloading...

   want to be notified of new releases in neubig/lamtram?
   [61]sign in [62]sign up

launching github desktop...

   if nothing happens, [63]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [64]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [65]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [66]download the github extension for visual studio
   and try again.

   (button) go back
   [67]@neubig
   [68]neubig [69]merge pull request [70]#35 [71]from kentonmurray/master
   (button)    
fix for issue [72]#34

   latest commit [73]902e827 apr 16, 2018
   [74]permalink
   type       name                 latest commit message           commit time
        failed to load latest commit information.
        [75]contrib
        [76]docker
        [77]m4
        [78]script
        [79]src
        [80].gitignore
        [81]authors
        [82]copying
        [83]changelog    [84]initial commit with new name lamtram jul 31, 2015
        [85]makefile.am
        [86]news
        [87]readme.md
        [88]todo
        [89]configure.ac [90]fix to work ./docker/build           oct 31, 2017

readme.md

lamtram: a toolkit for language and translation modeling using neural
networks

   by [91]graham neubig

   note: lamtram is no longer under active development, as it has been
   succeeded by [92]xid4. hence, we might not be answering questions or
   responding to issues here.

   in addition to the usage info below, you can also take a look at the
   [93]id4-tips tutorial on building strong id4
   systems.

install/citation

   (to run lamtram easily using docker, see docker/readme.md. to install
   manually, read on.)

   first, in terms of standard libraries, you must have autotools,
   libtool, and boost. if you are on ubuntu/debian linux, you can install
   them below:
$ sudo apt-get install autotools-dev autoconf libtool libboost-all-dev

   you must install eigen and dynet separately. follow the directions on
   the [94]dynet page, which also explain about installing eigen. note
   that in order to run programs like lamtram that are linked to dynet,
   you will have to set the ld_library_path variable to include the dynet
   library. for example, if $dynet_dir is the top directory for dynet,
   then you can run the following command, or add it to you .bashrc or
   .zshrc file:
$ export ld_library_path="$dynet_dir/build/dynet:$ld_library_path"

   once these two packages are installed and dynet has been added to the
   ld_library_path, run the following commands, specifying the correct
   paths for dynet and eigen.
$ autoreconf -i
$ ./configure --with-dynet=/path/to/dynet --with-eigen=/path/to/eigen
$ make

   if you want to use lamtram on a gpu and have cuda installed, first
   compile dynet with cuda enabled, then add the option
   --with-cuda=/path/to/cuda to the ./configure command.

   lamtram is licensed under the lgpl 2.1 and may be used freely according
   to this, or any later versions of the license. if you want to use
   lamtram in your research and we'd appreciate if you use the following
   reference:
@misc{neubig15lamtram,
        author = {graham neubig},
        title = {lamtram: a toolkit for language and translation modeling using
neural networks},
        howpublished = {http://www.github.com/neubig/lamtram},
        year = {2015}
}

language models

training

   to train a language model, you should prepare a training data set
   train.txt and a development data set dev.txt. before running training,
   you should decide a vocabulary that you want to use, and replace all
   words outside of the vocabulary with the symbol ''. the easiest way to
   do this is with the script/unk-single.pl script.
$ script/unk-single.pl < train.txt > train.unk

   then, we can perform training with lamtram-train. here is a typical way
   to run it with options:
$ src/lamtram/lamtram-train \
    --dynet_mem 1024 \        # allocate 1024 megabytes of memory for training
    --model_type nlm \        # train a neural language model
    --layer_size 512 \        # use a layer size of 512 for hidden layers, embed
dings, etc.
    --trainer adam \          # use adam for training models
    --learning_rate 0.001 \   # set learning rate to 0.001
    --seed 0 \                # a random seed, or 0 for a different seed every r
un
    --train_trg train.unk \   # specify the training file
    --dev_trg dev.txt \       # specify the development file
    --model_out langmodel.out

   training will take a long time (at least until the "speed improvements"
   below are finished), so try it out on a small data set first. as soon
   as one iteration finishes, the model will be written out, so you can
   use the model right away.

evaluating perplexity

   you can measure perplexity on a separate test set test.txt
$ src/lamtram/lamtram --dynet_mem 1024 --operation ppl --models_in nlm=langmodel
.out < test.txt

generating sentences

   once you have a model, you can try randomly generating sentences from
   the model:
$ src/lamtram/lamtram --dynet_mem 1024 --sents 100 --operation gen --models_in n
lm=langmodel.out

   (note: this may not work right now as of 2015-4-11. it will just
   generate the single most likely sentence over and over again. this will
   be fixed soon.)

translation models

training

   training a translation model is similar to language models. prepare a
   parallel corpus consisting of train-src.txt and train-trg.txt (in the
   source and target languages), and dev-src.txt and dev-trg.txt. first
   make '' symbols like before.
$ script/unk-single.pl < train-src.txt > train-src.unk
$ script/unk-single.pl < train-trg.txt > train-trg.unk

   then, we can perform training with lamtram-train. here is a typical way
   to run it to train an attentional model similar to bahdanau et al.
   (iclr2015). you can also train an lstm encoder-decoder model as in
   sutskever et. al (nips2014) by just changing "encatt" into "encdec."
$ src/lamtram/lamtram-train \
    --dynet_mem 1024 \        # allocate 1024 megabytes of memory for training
    --model_type encatt \     # create an atttentional model
    --layer_size 512 \        # use a layer size of 512 for hidden layers, embed
dings, etc.
    --trainer adam \          # use adam for training models
    --learning_rate 0.001 \   # set learning rate to 0.001
    --seed 0 \                # a random seed, or 0 for a different seed every r
un
    --train_src train-src.unk \ # specify the training source file
    --train_trg train-trg.unk \ # specify the training target file
    --dev_src dev-src.txt  \  # specify the development source file
    --dev_trg dev-trg.txt \   # specify the development target file
    --model_out transmodel.out

   again, as soon as one iteration finishes, the model will be written
   out.

evaluating perplexity

   you can measure perplexity on a separate test set test-src.txt and
   test-trg.txt
$ src/lamtram/lamtram \
    --dynet_mem 1024 \
    --operation ppl \
    --src_in test-src.txt \
    --models_in encdec=transmodel.out \
    < test-trg.txt

   if you used the same data is before, the perplexity of the translation
   model should be significantly lower than that of the language model, as
   the translation model has the extra information from the input
   sentence.

decoding

   you can also generate the most likely translation for the input
   sentence. here is a typical use case.
$ src/lamtram/lamtram \
    --dynet_mem 1024 \
    --operation ppl \
    --operation gen \
    --src_in test-src.txt \
    --models_in encdec=transmodel.out \
    --beam 5 \        # perform id125 with a beam of 5
    --word_pen 0.0 \  # the word penalty can be used to increase or decrease
                      # the average length of the sentence (positive for more wo
rds)
    > results.txt

   you can also use model ensembles. model ensembles allow you to combine
   two different models with different initializations or structures.
   using model ensembles is as simple as listing multiple models separated
   by a pipe.
--models_in "encdec=transmodel.out|nlm=langmodel.out"

   note however that the models must have the same vocabulary (i.e. be
   trained on the same data). ensembles will work for both generation and
   perplexity measurement.

classifiers

training

   training a text classifier is very similar to translation models. the
   only difference is that you want to create a file of the text you want
   to classify train-src.txt, and a correct label file with one label per
   line, train-lbl.txt. this is not absolutely necessary, but you can
   convert rare words in the source into unknown symbols like before:
$ script/unk-single.pl < train-src.txt > train-src.unk

   then, we can perform training with lamtram-train. here is a typical way
   to run it to train an lstm model.
$ src/lamtram/lamtram-train \
    --dynet_mem 1024 \        # allocate 1024 megabytes of memory for training
    --model_type enccls \     # create an encoder-classifier model
    --layer_size 512 \        # use a layer size of 512 for hidden layers, embed
dings, etc.
    --trainer adam \          # use adam for training models
    --learning_rate 0.001 \   # set learning rate to 0.001
    --seed 0 \                # a random seed, or 0 for a different seed every r
un
    --train_src train-src.unk \ # specify the training source file
    --train_trg train-lbl.txt \ # specify the training label file
    --dev_src dev-src.txt \   # specify the development source file
    --dev_trg dev-lbl.txt \   # specify the development label file
    --model_out clsmodel.out

   again, as soon as one iteration finishes, the model will be written
   out.

evaluating perplexity/accuracy

   you can measure perplexity and classification accuracy on a separate
   test set test-src.txt and test-lbl.txt
$ src/lamtram/lamtram \
    --dynet_mem 1024 \
    --operation clseval \
    --src_in test-src.txt \
    --models_in enccls=clsmodel.out \
    < test-lbl.txt

classifying

   you can also generate the most likely label for the input sentence.
   here is a typical use case.
$ src/lamtram/lamtram \
    --dynet_mem 1024 \
    --operation cls \
    --models_in enccls=clsmodel.out \
    > results.txt

   you can also use model ensembles as with the translation models.
--models_in "enccls=clsmodel1.out|enccls=clsmodel2.out"

   note however that the models must have the same vocabulary (i.e. be
   trained on the same data). ensembles will work for both perplexity
   measurement and classification.

todo

speed improvements

     * noise-contrastive estimation

accuracy improvements

     * more id180 (softplus, maxout)

more models

     * add other encoders

     *    2019 github, inc.
     * [95]terms
     * [96]privacy
     * [97]security
     * [98]status
     * [99]help

     * [100]contact github
     * [101]pricing
     * [102]api
     * [103]training
     * [104]blog
     * [105]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [106]reload to refresh your
   session. you signed out in another tab or window. [107]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/neubig/lamtram/commits/master.atom
   3. https://github.com/neubig/lamtram#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/neubig/lamtram
  32. https://github.com/join
  33. https://github.com/login?return_to=/neubig/lamtram
  34. https://github.com/neubig/lamtram/watchers
  35. https://github.com/login?return_to=/neubig/lamtram
  36. https://github.com/neubig/lamtram/stargazers
  37. https://github.com/login?return_to=/neubig/lamtram
  38. https://github.com/neubig/lamtram/network/members
  39. https://github.com/neubig
  40. https://github.com/neubig/lamtram
  41. https://github.com/neubig/lamtram
  42. https://github.com/neubig/lamtram/issues
  43. https://github.com/neubig/lamtram/pulls
  44. https://github.com/neubig/lamtram/projects
  45. https://github.com/neubig/lamtram/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/neubig/lamtram/commits/master
  48. https://github.com/neubig/lamtram/branches
  49. https://github.com/neubig/lamtram/releases
  50. https://github.com/neubig/lamtram/graphs/contributors
  51. https://github.com/neubig/lamtram/blob/master/copying
  52. https://github.com/neubig/lamtram/search?l=c++
  53. https://github.com/neubig/lamtram/search?l=m4
  54. https://github.com/neubig/lamtram/search?l=perl
  55. https://github.com/neubig/lamtram/search?l=c
  56. https://github.com/neubig/lamtram/search?l=python
  57. https://github.com/neubig/lamtram/search?l=makefile
  58. https://github.com/neubig/lamtram/search?l=shell
  59. https://github.com/neubig/lamtram/find/master
  60. https://github.com/neubig/lamtram/archive/master.zip
  61. https://github.com/login?return_to=https://github.com/neubig/lamtram
  62. https://github.com/join?return_to=/neubig/lamtram
  63. https://desktop.github.com/
  64. https://desktop.github.com/
  65. https://developer.apple.com/xcode/
  66. https://visualstudio.github.com/
  67. https://github.com/neubig
  68. https://github.com/neubig/lamtram/commits?author=neubig
  69. https://github.com/neubig/lamtram/commit/902e8276ec526e4b2d78280052e978bd9ac2124e
  70. https://github.com/neubig/lamtram/pull/35
  71. https://github.com/neubig/lamtram/commit/902e8276ec526e4b2d78280052e978bd9ac2124e
  72. https://github.com/neubig/lamtram/issues/34
  73. https://github.com/neubig/lamtram/commit/902e8276ec526e4b2d78280052e978bd9ac2124e
  74. https://github.com/neubig/lamtram/tree/902e8276ec526e4b2d78280052e978bd9ac2124e
  75. https://github.com/neubig/lamtram/tree/master/contrib
  76. https://github.com/neubig/lamtram/tree/master/docker
  77. https://github.com/neubig/lamtram/tree/master/m4
  78. https://github.com/neubig/lamtram/tree/master/script
  79. https://github.com/neubig/lamtram/tree/master/src
  80. https://github.com/neubig/lamtram/blob/master/.gitignore
  81. https://github.com/neubig/lamtram/blob/master/authors
  82. https://github.com/neubig/lamtram/blob/master/copying
  83. https://github.com/neubig/lamtram/blob/master/changelog
  84. https://github.com/neubig/lamtram/commit/8112a8878e1315c046b009400b22881b303fa180
  85. https://github.com/neubig/lamtram/blob/master/makefile.am
  86. https://github.com/neubig/lamtram/blob/master/news
  87. https://github.com/neubig/lamtram/blob/master/readme.md
  88. https://github.com/neubig/lamtram/blob/master/todo
  89. https://github.com/neubig/lamtram/blob/master/configure.ac
  90. https://github.com/neubig/lamtram/commit/d33ed986c6014a5234e0731aa3adc098f3230edb
  91. http://www.phontron.com/
  92. http://github.com/neulab/xid4
  93. http://github.com/neubig/id4-tips
  94. http://github.com/clab/dynet
  95. https://github.com/site/terms
  96. https://github.com/site/privacy
  97. https://github.com/security
  98. https://githubstatus.com/
  99. https://help.github.com/
 100. https://github.com/contact
 101. https://github.com/pricing
 102. https://developer.github.com/
 103. https://training.github.com/
 104. https://github.blog/
 105. https://github.com/about
 106. https://github.com/neubig/lamtram
 107. https://github.com/neubig/lamtram

   hidden links:
 109. https://github.com/
 110. https://github.com/neubig/lamtram
 111. https://github.com/neubig/lamtram
 112. https://github.com/neubig/lamtram
 113. https://help.github.com/articles/which-remote-url-should-i-use
 114. https://github.com/neubig/lamtram#lamtram-a-toolkit-for-language-and-translation-modeling-using-neural-networks
 115. https://github.com/neubig/lamtram#installcitation
 116. https://github.com/neubig/lamtram#language-models
 117. https://github.com/neubig/lamtram#training
 118. https://github.com/neubig/lamtram#evaluating-perplexity
 119. https://github.com/neubig/lamtram#generating-sentences
 120. https://github.com/neubig/lamtram#translation-models
 121. https://github.com/neubig/lamtram#training-1
 122. https://github.com/neubig/lamtram#evaluating-perplexity-1
 123. https://github.com/neubig/lamtram#decoding
 124. https://github.com/neubig/lamtram#classifiers
 125. https://github.com/neubig/lamtram#training-2
 126. https://github.com/neubig/lamtram#evaluating-perplexityaccuracy
 127. https://github.com/neubig/lamtram#classifying
 128. https://github.com/neubig/lamtram#todo
 129. https://github.com/neubig/lamtram#speed-improvements
 130. https://github.com/neubig/lamtram#accuracy-improvements
 131. https://github.com/neubig/lamtram#more-models
 132. https://github.com/
