                            [1]machine learning blog
                         principal components analysis

   by pablo martin, [2]artelnics.

   customer segmentation logo

   principal components analysis (pca) is a statistical technique that
   allows to identify underlying linear patterns in a data set so it can
   be expressed in terms of other data set of significatively lower
   dimension without much loss of information. the final data set should
   be able to explain most of the variance of the original data set by
   making a variable reduction. the final variables will be named as
   principal components.

   the following image depicts the activity diagram that shows each step
   of the principal components analysis that will be explained in detail
   later.

                   principal components analyisis process

   in order to illustrate the process described in the previous diagram,
   we are going to make use of the following data set which has two
   dimensions.

                                  instance

                                    x[1]

                                    x[2]

                                  instance

                                    x[1]

                                    x[2]

                                      1

                                     0.3

                                     0.5

                                     11

                                     0.6

                                     0.8

                                      2

                                     0.4

                                     0.3

                                     12

                                     0.4

                                     0.6

                                      3

                                     0.7

                                     0.4

                                     13

                                     0.3

                                     0.4

                                      4

                                     0.5

                                     0.7

                                     14

                                     0.6

                                     0.5

                                      5

                                     0.3

                                     0.2

                                     15

                                     0.8

                                     0.5

                                      6

                                     0.9

                                     0.8

                                     16

                                     0.8

                                     0.9

                                      7

                                     0.1

                                     0.2

                                     17

                                     0.2

                                     0.3

                                      8

                                     0.2

                                     0.5

                                     18

                                     0.7

                                     0.7

                                      9

                                     0.6

                                     0.9

                                     19

                                     0.5

                                     0.5

                                     10

                                     0.2

                                     0.2

                                     20

                                     0.6

                                     0.4

   the following plot shows the values of the variable x[1] against the
   values of the variable x[2].

                                scatter plot

   the objective is to calculate the principal components in order to
   convert it into a data set of only one dimension with the minimal loss
   of information.

  1. subtraction of the mean from the data

   the first step in the principal components analysis is to subtract the
   mean for each variable of the data set, which is shown in the next
   chart for our example.

                                scatter plot

   as we can see, the subtraction of the mean results in a translation of
   the data which have now zero mean.

  2. covariance matrix

   the covariance of two random variables measures the degree of variation
   from their respective means with respect to each other. the sign of the
   covariance provides us with information about the relation between
   them:
     * if the covariance is positive, then the two variables increase and
       decrease together,
     * if the covariance is negative, then when one varible increases the
       other decreases and vice versa.

   these values will determine the linear dependencies between the
   variables which will be used to reduce the dimension of the data set.
   back to our example, the covariance matrix is shown next.

                              covariance matrix

   the values of the diagonal show the covariance of each variable and
   itself and they equal their variance. the variance is a measure of how
   spread are data from the mean. the off-diagonal values show the
   covariance between the two variables. in this case, these values are
   positive, which means that both variables increase and decrease
   together.

  3. eigenvectors and eigenvalues

   eigenvectors are defined as those vectors whose directions remain
   unchanged after any linear transformation has been applied to them.
   however, their length could not remain the same after the
   transformation, i.e., the result of this transformation is the vector
   multiplied by a scalar. this scalar is called eigenvalue and each
   eigenvector has one associated to it.

   the number of eigenvectors or components that we can calculate for each
   data set is equal to the dimension of the data set. in this case, we
   have a 2-dimensionalal data set so the number of eigenvectors will be
   2. the next image represents the eigenvectors for our example.

                            principal components

   since they are calculated from the covariance matrix described before,
   eigenvectors represent the directions in which the data have more
   variance. on the other hand, their respective eigenvalues determine the
   amount of variance that the data set has in that direction.

   once we have obtained these new directions, we can plot the data in
   terms of them as shown in the next image for our example.

                         data principal eigenvectors

   note that the data have not changed, we are just rewriting them in
   terms of these new directions instead of the previous x[1]-x[2]
   directions.

  4. principal components

   among all the available eigenvectors that have been calculated in the
   previous step, we must select those ones onto which we are going to
   project the data. the selected eigenvectors will be called principal
   components.

   in order to establish a criterion to select the eigenvectors, we must
   first define the relative variance of each eigenvector and the total
   variance of a data set. the relative variance of an eigenvector
   measures how much information can be attributed to it. the total
   variance of a data set is the sum of the variance of all the variables.

   these two concepts are determined by the eigenvalues. for our example,
   the next table shows the relative and the cumulative variance for each
   eigenvector.

                          explained variance table

   as we can see, the first eigenvector can explain almost the 85% of all
   the variance of the data while the second eigenvector explains around
   the 15% of it. the next graph shows the cumulative variance for the
   components.

                          explained variance chart

   a common way to select the variables is establish the amount of
   information that we want the final data set to explain. if this amount
   of information decreases, the number of principal components that we
   will select will decrease as well. in this case, as we want to reduce
   the 2-dimensional data set into a 1-dimensional data set, we will
   select just the first eigenvector as principal component. as a
   consequence, the final reduced data set will explain around 85% of the
   variance of the original one.

  5. reduction of data dimension

   once we have selected the principal components, the data must be
   projected onto them. the next image shows the result of this projection
   for our example.

                               data projection

   although this projection can explain most of the variance of the
   original data, we have lost the information about the variance along
   the second component. in general, this process is irreversible, which
   means that we cannot recover the original data from the projection.

  conclusions

   principal components analysis is a technique that allows us to identify
   the underlying dependencies of a data set and to reduce significatively
   its dimensionality attending to them.

   this technique is very useful for processing data sets with hundreds of
   variables while mataining, at the same time, most of the information
   from the original data set.

   principal components analysis can be also implemented within a neural
   network. however, since this process is irreversible, the reduction of
   the data may be done only for the inputs and not for the target
   variables.

  related posts:

   [3]principal components analysis.
   [4]market basket analysis using r and neural designer.
   [5]5 algorithms to train a neural network.

references

   1. https://www.neuraldesigner.com/blog
   2. https://www.artelnics.com/
   3. https://www.neuraldesigner.com/blog/principal-components-analysis
   4. https://www.neuraldesigner.com/blog/market-basket-analysis-using-r-neural-designer
   5. https://www.neuraldesigner.com/blog/5_algorithms_to_train_a_neural_network
