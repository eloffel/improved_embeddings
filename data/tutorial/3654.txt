   #[1]   feed [2]   comments feed [3]   why deep learning works 3: backprop
   minimizes the free energy ? comments feed [4]foundations: mean field
   id82s 1987 [5]id172 in deep learning [6]alternate
   [7]alternate [8]search [9]wordpress.com

   [10]skip to content

   [11][wp-logo.jpg]

why deep learning works 3: backprop minimizes the free energy ?

   [12]february 24, 2017 [13]charles h martin, phd [14]uncategorized [15]7
   comments

    ?deep learning is presented as [16]energy-based learning

   indeed, we train a neural network by running backprop, thereby
   minimizing the model error   which is like minimizing an energy.
   [17]screen-shot-2017-02-05-at-8-26-58-pm why deep learning works ?
   possible free energy landscapes from physical chemistry

   but what is this energy ? deep learning (dl) energy functions look
   nothing like a typical chemistry or physics energy. here, we have free
   energy landscapes, frequently which form [18]funneled landscapes   a
   trade off between energetic and entropic effects.

   [19]and yet, some researchers, like lecun, have even compared neural
   network energies functions to spin glass hamiltonians.  to me, this
   seems off.

   the confusion arises from assuming deep learning is a non-convex
   optimization problem that looks similar to the zero-temperature energy
   landscapes from spin glass theory.

   i present a different view.  i believe deep learning is really
   optimizing an effective free energy function. and this has profound
   implications on [20]why deep learning works.

   this post will attempt to relate recent ideas in rbm id136 to
   backprop, and argue that backprop is minimizing a dynamic, temperature
   dependent, ruggedly convex, effective free energy landscape.

   this is a fairly long post, but at least is basic review.  i try to
   present these ideas in a semi-pedagogic way, to the extent i can in a
   blog post, discussing both rbms, mlps, free energies, and all that
   entails.

backprop

   the backprop algorithm lets us train a model directly on our data (x)
   by minimizing the predicted error e_{train}(\mathbf{\theta}) , where
   the parameter set \mathbf{\theta} includes the weights (\mathbf{w}) ,
   biases (\mathbf{b}) , and activations (\mathbf{a}) of the network.

   \theta=\{\mathbf{w},\mathbf{b},\mathbf{a}\} .

   let   s write

   e_{train}(\mathbf{\theta})=\underset{\mathbf{x}_{\mu}\in\mathbf{x}}{\su
   m}err(\mathbf{x}_{\mu}) ,

   where the error err(x)   could be a mean squared error (mse), cross
   id178, etc. for example, in simple regression, we can
   minimize the mse

   e_{train}(\theta)=\sum_{\mu}(y_{\mu}-f(\mathbf{x}_{\mu},\theta))^{2} ,

   whereas for multi-class classification, we might minimize a
   [21]categorical cross id178

   e_{train}(\theta)=\sum_{\mu}(y_{\mu}\ln f_{\mu}+(1-y_{\mu})\ln f_{\mu})

   where y_{\mu}   are the labels  and  f_{\mu}=f(\mathbf{x}_{\mu},\theta)
     is the network output for each training instance  \mu   .

   notice that err(\mathbf{x}_{\mu})   is the training error for instance
   \mu , not a test or holdout error.  notice that, unlike an support
   vector machine (id166) or id28 (lr), we don   t use
   [22]cross validation (cv) during training.   we simply minimize the
   training error    whatever that is.

   of course, we can adjust the network parameters, id173, etc,
   to tune the architecture of the network.  although it appears
   that [23]understanding deep learning requires rethinking
   generalization.

   at this point, many people say that backprop leads to a complex,
   non-id76 problem; imho, this is naive.

   it has been known for 20 years that deep learning does not suffer from
   local minima.

   anyone who thinks it does has never read a [24]research paper or[25]
   book on neural networks.  so what we really would like to know
   is, [26]why does deep learning scale ?  or, maybe, why does it work at
   all ?!

   to implement backprop, we take derivatives
   \dfrac{\partial}{\partial\theta}e_{train}(\theta)    and apply the the
   chain rule to the network outputs f(\mathbf{x}_{\mu},\theta)   ,
   applying it layer-by-layer.

layers and activations

   let   s take a closer look at the layers and activations.  consider a
   simple 1 layer net:

   the hidden activations \mathbf{a} are thought to mimic the function of
   actual neurons, and are computed by applying an activation function f()
   ,  to a linear energy function  \mathbf{w}^{t}\mathbf{x}+\mathbf{b} ,

   indeed, the sigmoid activation function \sigma() was first proposed in
   1968 by [27]jack cowan at the university of chicago , still used today
   in [28]models of neural dynamics

   \mathbf{a}=\sigma(\mathbf{wx}+\mathbf{b})

   moreover, cowan pioneered using [29]statistical mechanics to study the
   neocortex.

   and we will need a little stat mech to explain what our energy
   functions are..but just a little.

sigmoid activations and statistical mechanics

   while it seems we are simply proposing an arbitrary activation
   function, we can, in fact, derive the appearance of sigmoid
   activations   at least when performing id136 on a single layer (mean
   field) restricted id82 (rbm).

   [30]hugo larochelle has derived the sigmoid activations nicely for an
   rbm.

   given the (total) rbm energy function

   e(\mathbf{v},\mathbf{h})=\mathbf{a}^{t}\mathbf{v}+\mathbf{b}^{t}\mathbf
   {h}+\mathbf{vw}^{t}\mathbf{h}

   the log energy is an un-normalized id203, such that

   p(\mathbf{v},\mathbf{h})=\dfrac{1}{z}e^{-\beta
   e(\mathbf{v},\mathbf{h})}

   where the id172 factor, z, is an object from statistical
   mechanics called the (total) partition function z

   z(\mathbf{v},\mathbf{h})=\underset{\mathbf{v},\mathbf{h}}{\sum}e^{-\bet
   a e(\mathbf{v},\mathbf{h})}

   and \beta=\dfrac{1}{t}  is an inverse temperature.  in modern
   machine learning, we implicitly set \beta=1 .

   following larochelle, we can factor p(\mathbf{v},\mathbf{h}) by
   explicitly writing z(\mathbf{v},\mathbf{h}) in terms of sums over the
   binary hidden activations h_{i}=0|1 .  this lets us write the
   conditional probabilities, for each individual neuron as

   p(v_{i}|h=1)=\sigma(\sum_{j}w_{i,j}h_{j}+a_{j})

   p(h_{j}|v=1)=\sigma(\sum_{i}w_{i,j}v_{i}+b_{i}) .

   we note that, this formulation was not obvious, and [31]early work on
   rbms used methods from statistical field theory to get this result.

rbm training

   we use p(v_{i}|h=1) and   p(h_{j}|v=1)  in contrastive divergence (cd)
   or other solvers as part of the id150 step for (unsupervised)
   rbm id136.

   cd has been a puzzling algorithm to understand.  when first proposed,
   it was unclear [32]what optimization problem is cd solving?  indeed,
   hinton is to have said

   [33]      the microsoft algorithm:    it asks,    where do you want to go
   today?    and then doesn   t let you go there.   

   specifically, we run several epochs of:
    1. n steps of id150, or some other equilibration method, to
       set the neuron activations.
    2. some form of id119  \dfrac{\partial}{\partial\theta}
       where  \theta=\{\mathbf{w},\mathbf{b}\}

   we will see below that we can cast rbm id136 as directly minimizing
   a free energy   something that will prove very useful to related rbms to
   mlps

energies and activations

   the sigmoid, and tanh,  are an old-fashioned activation(s); today we
   may prefer to use relus (and leaky relus).
   common id180 common id180

   the sigmoid itself was, at first, just an approximation to the
   heavyside step function used in neuron models.  but the presence of
   sigmoid activations in the total energy suggests, at least to me, that
   deep learning energy functions are more than just random (morse)
   functions.

   rbms are a special case of unsupervised nets that still use stochastic
   sampling. in supervised nets, like mlps and id98s (and in unsupervised
   autoencoders like vaes), we use backprop.  but the activations are not
   conditional probabilities.  let   s look in detail:

mlp outputs

   consider a multilayer id88, with 1 hidden layer, and 1 output
   node

   f^{\mu}_{mlp}=\sigma(\underset{h\in\mathbf{h}}{\sum}\mathbf{a}_{h})

   \mathbf{a}_{h}=\sigma(\mathbf{w}^{t}\mathbf{v}+\mathbf{b})

   where \mathbf{v}=\mathbf{x}^{\mu}    for each data point, leading to
   the layer output

   g^{\mu}_{mlp}(\theta)=\sigma(\mathbf{w}^{t}\mathbf{x}^{\mu}+\mathbf{b})
   )

   and total mlp output

   f^{\mu}_{mlp}(\theta)=\sigma(\sum g^{\mu}_{mlp}(\theta))=\sigma(\sum
   \sigma(\mathbf{w}^{t}\mathbf{x}^{\mu}+\mathbf{b})))

   where  \theta=\{\mathbf{w},\mathbf{b}\} .

   if we add a second layer, we have the iterated layer output:

   g^{\mu}_{mlp}(\theta')=\sigma(\mathbf{w}^{t}(\sigma(\mathbf{w'}^{t}\mat
   hbf{x}^{\mu}+\mathbf{b'}))+\mathbf{b}))

   where  \theta'=\{\mathbf{w},\mathbf{w'},\mathbf{b},\mathbf{b'}\} .

   the final mlp output function has a similar form:

   f^{\mu}_{mlp}(\theta)=\sigma(\sum\sigma(g^{\mu}_{mlp}(\theta)))

   f^{\mu}_{mlp}(\theta)=\sigma(\sum\sigma(\mathbf{w}^{t}(\sigma(\mathbf{w
   '}^{t}\mathbf{x}^{\mu}+\mathbf{b'}))+\mathbf{b})))

   so with a little bit of stat mech, we can derive the sigmoid activation
   function from a general energy function.  and we have activations it in
   rbms as well as mlps.

   so when we apply backprop, what problem are we actually solving ?

   are we simply finding a minima on random high dimensional manifold ?
   or can we say something more, given the special structure of these
   layers of activated energies ?

backprop and energy minimization

   to train an mlp, we run several epochs of backprop.   backprop has 2
   passes: forward and backward:

   backprop
    1. forward: propagate the inputs \{\mathbf{x}^{\mu}\}  forward through
       the network, activating the neurons
    2. backward: propagate the errors \{err(\mathbf{x}^{\mu})\} backward
       to compute the weight gradients \delta\mathbf{w}

   each epoch usually runs small batches of inputs at time.  (and we may
   need to normalize the inputs and control the variances.  these details
   may be important for out analysis, and we will consider them in a later
   post).

   after each pass, we update the weights, using something like an sgd
   step (or adam, rmsprop, etc)

   \mathbf{w}\rightarrow\mathbf{w}+\eta\delta\mathbf{w}

   for an mse loss, we evaluate the partial derivatives over the energy
   parameters  \theta=\{\mathbf{w},\mathbf{w'},\mathbf{b},\mathbf{b'}\} .

   \dfrac{\partial}{\partial\theta}\underset{\mu}{\sum}(y^{\mu}-f^{\mu}_{m
   lp}(\theta))^{2}

   backprop works by the chain rule, and given the special form of the
   activations, lets us transform the energy derivatives into a sum of
   energy gradients   layer by layer

   backprop gradients

   i won   t go into the details here; there are 1000 blogs on backprop
   today (which is amazing!).  i will say   

   backprop couples the activation states of the neurons to the energy
   parameter gradients through the cycle of forward-backward phases.

   in a crude sense, backprop resembles our more familiar rbm training
   procedure, where we equilibrate to set the activations, and run
   id119 to set the weights. here, i show a direct connection,
   and derive the mlp functional form directly from an rbm.

from rbms to mlps

discriminative (supervised) rbms

   rbms are unsupervised; mlps are supervised.  how can we connect them?
   crudely, we can think of an mlp as a single layer rbm with a softmax
   tacked on the end.   more rigorously, we can look at [34]generalized
   discriminative rbms, which solve the id155 directly,
   in terms of the free energies, cast in the soft-max form

   p(y|\mathbf{x})=\dfrac{exp(-e_{free}(\mathbf{x},y))}{\sum_{y*}exp(-e_{f
   ree}(\mathbf{x},y*))}

   so the question is, can we extract free energy for an mlp ?

the backward phase

   i now consider the backward phase, using the deterministic emf rbm, as
   a starting point for understanding mlps.

   [35]an earlier post discusses the emf rbm, from the context of chemical
   physics.  for a traditional machine learning perspective, [36]see this
   thesis.

   in some sense, this is kind-of obvious. and yet, i have not seen a
   clear presentation of the ideas in this way.  i do rely upon new
   research, like the emf rbm, although i also draw upon fundamental ideas
   from complex systems theory   something popular in my phd studies, but
   which is perhaps ancient history now.

   the goal is to relate rbms, mlps, and basic stat mech under single
   conceptual umbrella.

   in the emf approach, we see rbm id136 as a sequence of
   deterministic annealing steps, from 1 quasi-equilibrium state to
   another, consisting of 2 steps for each epoch:
    1. forward: equilibrate the neuron activations by minimizing the tap
       free energy
    2. backward: compute weight gradients of the tap free energy

   at the end of each epoch, we update the weights, with weight
   (temperature) constraints (i.e. reset the l1 or l2 norm).  btw, it may
   not obvious that weight id173 is like a temperature control; i
   will address this in a later post.

   (1) the so-called forward step solves a fixed point equation (which is
   similar in spirit to taking n steps of id150).  this leads to
   a pair of coupled, recursion relations for the tap magnetizations (or
   just nodes).   suppose we take t+1 iterations.  let us ignore the
   second order onsager correction, and consider the mean field updates:

   h_{i}[t+1]\leftarrow\sigma\left[b_{i}+\underset{j}{\sum}w_{i,j}v_{j}[t+
   1]-\cdots\right]

   v_{i}[t+1]\leftarrow\sigma\left[a_{i}+\underset{j}{\sum}w_{i,j}h_{j}[t]
   -\cdots\right]

   because these are deterministic steps, we can express the h_{i}[t+1]\
   in terms of h_{i}[t] :

   \mathbf{h}[t+1]\leftarrow\sigma\left[\mathbf{b}+\mathbf{w}^{t}\sigma(\m
   athbf{b}+\mathbf{w}^{t}\mathbf{h}[t])\right]

   at the end of the recursion, we will have a forward pass that resembles
   a multi-layer mlp, but that shares weights and biases between layers:

   \mathbf{h}[t+1]\leftarrow\sigma\left[\mathbf{b}+\mathbf{w}^{t}\sigma(\m
   athbf{b}+\cdots\sigma(\mathbf{b}+\mathbf{v}\mathbf{w}^{t}))\right]

   we can now associate an n-layer mlp, with tied weights,

   \theta=\{\mathbf{w}=\mathbf{w'}=\cdots;\;\;\mathbf{b}=\mathbf{b'}=\cdot
   s\} ,

    to an approximate (mean field) emf rbm,  with n fixed point iterations
   (ignoring the onsager correction for now).  of course, an mlp is
   supervised, and an rbm is unsupervised, so we need to associate the rbm
   hidden nodes with the mlp output function at the last layer (
   g^{\mu}_{mlp}(\theta) ), prior to adding the mlp output node

   g^{\mu}_{mlp}(\theta)=g^{\mu}_{rbm}(\theta)=\mathbf{h}[n](x^{\mu})

   this leads naturally to the following conjecture:

   the emf rbm and the backprop forward and backward steps effectively
   do the same thing   minimize the free energy

is this right ?

   this is a work in progress

   formally, it is simple and compelling.  is it the whole story   probably
   not.  it is merely an observation   food for thought.

   so far, i have only removed the visible magnetizations
   \mathbf{v}[n](x^{\mu})  to obtain the mlp layer function
   g^{\mu}_{mlp}(\theta) as a function of the original visible units.  the
   unsupervised emf rbm free energy, however, contains expressions in
   terms of both the hidden and visible magnetizations (
   \mathbf{v}[n],\mathbf{h}[n]= \mathbf{m_{v}},\mathbf{m_{h}} ).  to get a
   final expression, it is necessary to either
     * unravel the network, like a variational auto encoder (vae)
     * replace the visible magnetizations with the true labels, and
       introduce the softax loss

   the result itself should not be so surprising, since it has already
   been pointed out by [37]kingma and welling, auto-encoding variational
   bayes, that a bernoulli mlp is like a variational decoder.  and, of
   course, vaes can be formulated with backprop.

   nore importantly, it is unclear how good the rbm emf really is.
   some[38] followup studies indicate that second order is not as good as,
   say, ais, for estimating the partition function.  i have coded [39]a
   python emf_rbm.py module using the scikit-learn interface, and testing
   is underway.  i will blog this soon.

   note that the emf rbm relies on the[40] legendre transform, which is
   like a [41]convex relaxation.  early results indicates that this does
   degrade the rbm solution compared to traditional cd.  maybe backprop
   may be effective relaxing the convexity constraint by, say, relaxing
   the condition that the weights are tied between layers.

   still, i hope this can provide some insight.  and there are    
     __________________________________________________________________

implications

   free energy is a first class concept in statistical mechanics.  in
   machine learning, not always so much. it appears in much of hinton   s
   work, and, as a starting point to deriving methods like variational
   auto encoders and [42]probabilistic programing.

   but free energy minimization plays an important role in non-convex
   optimization as well.  free energies are a boltzmann average of the
   zero-temperature energy landscape, and, therefore, convert a non-convex
   surface into something at least less non-convex.

   indeed, in [43]one of the very first papers on mean field boltzmann
   machines (1987), it is noted that

      an important property of the effective [free] energy function
   e'(v,0,t) is that it has a smoother landscape than e(s) due to the
   extra terms. hence, the id203 of getting stuck in a local minima
   decreases.   

   moreover, in protein folding, we have even stronger effects, which can
   lead to a ruggedly convex, energy landscape.  this arises when the
   system runs out of configurational id178 (s), and energetic effects
   (e) dominate.

   most importantly, we want to understand, when does deep learning
   generalize well, and when does it overtrain ?

   [44]lecun has very recently pointed out that deep nets fail when they
   run out of configuration id178   [45]an argument i also have made from
   theoretical analysis using the random energy model.  so it is becoming
   more important to understand what the actual energy landscape of a deep
   net is, how to separate out the entropic and energetic terms, and how
   to characterize the configurational id178.

   hopefully the small insight will be useful and lead to a further
   understanding of [46]why deep learning works.

share this:

     * [47]twitter
     * [48]facebook
     * [49]linkedin
     * [50]more
     *

     * [51]reddit
     * [52]email
     *
     *

like this:

   like loading...

related

post navigation

   [53]previous post: foundations: mean field id82s 1987
   [54]next post: id172 in deep learning

7 comments

    1. pingback: [55]distilled news | data analytics & r
    2.
   [56]ezequiel ferrero says:
       [57]march 22, 2017 at 1:33 am
       dear charles, thanks for this very interesting post! (and also for
       all the previous ones)
       i agree in that there is a confusion in directly comparing nn
       energy functions to spin glass hamiltonians, and that training a
       dnn is not the same as finding the minimum in a zero-temperature
       energy landscapes of a glass. but, even when we could talk about
          effective free energies    as being a more accurate description of
       the deep learning funneled landscapes, don   t you think that the
       whole success of dnns relies on the fact that there   s actually
       no-need to find a global minimum?
       you can train your dnn with different training subsets and it
       should/will find different local minima (even of your free
       energy!!). the trick is that all this local minima are, to more or
       less extent, representative of the problem/characteristic you   re
       asking the dnn to learn. their strength should rely in the
       redundancy of representations of such properties. precisely that   s
       why, i think, people frequently find better statistical results
       when averaging over the predictions of separately trained nns,
       instead of extensively training a single one.
       generalization lives in the tortuous repetitions of valleys and
       hills at a (relatively high) energy level, and not in a deep
       minimum of whatever energy or free energy one could define.
       luckily we did start    suffering    from local minima at some point in
       ml, that is where all the fun started !!!
       [58]likelike
       [59]reply
         1.
        charles h martin, phd says:
            [60]march 22, 2017 at 7:53 am
            i think the evidence for having    no global minima    is
            anecdotal and a bit deceptive, and it is confused because of
            the lack of explicit temperature. it was known 20 years ago
            that neural nets have high internal symmetries, (specifically
            that we observe replica symmetry breaking (rsb) simply due to
            symmetry). this feels more like a highly degenerate global
            minima than a simply having lots of low lying local minima.
            but more importantly, these are t=0 minima, and nets operate
            effectively at t=1. so even if we have lots of t=0 minima, the
            t=1 free energy is a boltzmann average of all of them. free
            energy, just in itself, has the effect of    smoothing out    a
            large number of local minima, providing a flatter, more convex
            landscape.
            [61]likelike
            [62]reply
              1.
             [63]ezequiel ferrero says:
                 [64]march 22, 2017 at 8:29 am
                 thanks! i should probably read the cited literature
                 before proceeding the discussion, specially if there are
                 things very clearly know since 20 year ago that may not
                 be obvious to me      . a priori, i definitely agree with a
                 free energy smoothing out a large number of t=0 local
                 minima, providing a flatter landscape. but it is not
                 transparently clear to me that this should result in    a   
                 convex landscape (with    more    convex i   m fine), therefore
                 my comment above. also, i think that any arguments about
                 symmetries may get weaker if we talk about
                 not-fully-connected neurons; and those nets, with lots of
                 locality and higher frustration, are also learning. but
                 of course these are very light words and not deep
                 thoughts. regards, and keep posting!
                 [65]likelike
                 [66]reply
                   1.
                  charles h martin, phd says:
                      [67]march 22, 2017 at 8:56 am
                      well it is unclear what is going on . i suggested
                      the idea of a rugged convex landscape because it is
                      known from protein folding. the ideas was put forth
                      by wolynes, and he called it the spin glass of
                      minimal frustration. in this context, the idea is
                      that the convexification acts in the same way the
                      retrieval phase acts in a hopfield net. that is,
                      there is a low lying energy state living below the
                      spin glass transition which appears, and the
                      learning occurs here   not in the glassy part. since
                      we don   t really know the phase diagram of these
                      complex nets, it is simply easier to posit a low
                      lying energy state that gets us out of the spin
                      glass phase.
                      (this idea is also called topological trivialization
                      by some spin glass people, and has been put forth by
                      a group at ucla. although i did not know about this
                      when i started these posts)
                      like a protein, the energy landscape would not just
                      be single peaked. this would be a very rigid
                      structure with no chemical function. in other words,
                      overtrained. instead, the landscape probably has
                      some deep local minim to provide some flexibility
                      and motion. in other words, generalization.
                      of course, we need an effective    dimension    , or
                      what chemists call a reaction coordinate, on which
                      to drive. in protein folding, this is id178:
                      f=-ts. but we also have to consider t, and in
                      hopfield nets, the retrieval phase is a very low t
                      phase, whereas in deep nets, we implicitly have t=1
                      . then again, no effort is made to ensure t=1   it is
                      just assumed. to that end, i think there is a more
                      subtle reason connection to spin glasses that has
                      been overlooked entirely. it is becoming clearly now
                      that networks generalize well when they have an
                      excess residual id178.
                      for example, see lecun   s very recent work on id178
                      sgd, and i think lecun is on the right track here,
                      but, because they ignore the concept of free energy,
                      they also ignore temperature, and are trying to use
                      ideas for t=0 spin glasses to a non-zero t
                      phenomena.
                      i conjecture that this excess id178 in the network
                      landscape is analogous to the excess configurational
                      id178 of a supercooled glass. and in supercooled
                      glasses, there is a phenomena known as the
                      adam-gibbs relation, which basically says that the
                      energy barriers go to infinity as the temperature is
                      lowered. this is called the id178 crises, and it
                      is seen even simple spin glass models like the
                      random energy model. (i discussed this in my online
                      video ad mmds) for a deep net, the effective
                      temperature is like like the norm of weights   in
                      other words, the traditional weight-norm
                      regularizer. rugged convexification arises as a way
                      to avoid the id178 crises. in other words, good
                      generalization requires averaging over a some number
                      of t=0 energy minima, thereby avoiding id178
                      collapse.
                      [68]likelike
                      [69]reply
    3.
   ravicv says:
       [70]june 24, 2017 at 4:00 pm
       hi charles,
       i   ve always been ad-hoc by the ad-hoc setting of the thermodynamic
       temperature to unity in rbm   s (despite arguments concerning the
       minina and the energy landscape). i know its convenient, but for
       id136 and temperature dependent tasks like annealing, the
       thermodynamic temperature (or its inverse) needs to be explicitly
       stated, since the partition function z=z(\beta, \lambda), where
       \lambda represents a configuration of weights and parameters. as an
       added consequence, the crooks fluctuation theorem will also be
       satisfied. further, on careful examination the inclusion of the
       inverse thermodynamic temperature will not effect the rbm
       properties, since the weights can be scaled as \beta\lambda
       \rightarrow \lambda.
       [71]likelike
       [72]reply
         1.
        charles h martin, phd says:
            [73]june 24, 2017 at 4:23 pm
            in any deep net (rbm, mlp, id98,    ) , it is necessary to keep
            the weights from exploding. weight norm id173 is an
            old method; batch norm the most recent. i have proposed that
            maintaining weight id172 is effectively like a
            temperature control. recall that a true stat mech energy is
            size extensive, and so just using an energy based method
            should prevent the weight from exploding   in the limit of
            infinite training data. but implicit scaling does not control
            the temperature. it has been now been confirmed that in rbms
            (1) the effective t< 1 and (2) rbms undergo an id178 crises
            as low t. as i proposed in the mdds talk. however, it is true
            that it is unclear how the id178 crisis affects training and
            the rbm properties. the current idea is to assign an implicit
            t to the training data itself.
            [74]likelike
            [75]reply

leave a reply [76]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [77]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [78]log out /
   [79]change )
   google photo

   you are commenting using your google account. ( [80]log out /
   [81]change )
   twitter picture

   you are commenting using your twitter account. ( [82]log out /
   [83]change )
   facebook photo

   you are commenting using your facebook account. ( [84]log out /
   [85]change )
   [86]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

     * [87]charles h martin, phd

calculation consulting

   we are a boutique machine learning data science consultancy. how can we
   help? email me at [88]info@calculationconsulting.com.

   or stop by:
   [89]http://calculationconsulting.com
   [90]youtube channel
   [91]quora

   set up a quick all on [92]clarity.fm

the community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

blog stats

     * 521,305 hits

   [93]follow on wordpress.com

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   join 694 other followers

   ____________________

   (button) follow

top posts & pages

     * [94]spectral id91: a quick overview
       [95]spectral id91: a quick overview
     * [96]kernels part 1: what is an rbf kernel? really?
       [97]kernels part 1: what is an rbf kernel? really?
     * [98]why deep learning works ii: the reid172 group
       [99]why deep learning works ii: the reid172 group
     * [100]id172 in deep learning
       [101]id172 in deep learning
     * [102]causality, correlation, and brownian motion
       [103]causality, correlation, and brownian motion

recent posts

     * [104]sf bay acm talk: heavy tailed self id173 in deep
       neural networks
     * [105]heavy tailed self id173 in deep neural nets: 1 year
       of research
     * [106]don   t peek part 2: predictions without test data
     * [107]machine learning and ai for the lean start up
     * [108]don   t peek: deep learning without looking     at test data

top clicks

     * [109]youtube.com/redirect?redi   
     * [110]arxiv.org/abs/1810.01075
     * [111]arxiv.org/abs/1706.02515
     * [112]github.com/calculatedcont   
     * [113]charlesmartin14.wordpress   
     * [114]arxiv.org/pdf/1412.0233.p   
     * [115]quora.com/machine-learnin   
     * [116]arxiv.org/pdf/1412.6621v3   
     * [117]di.ens.fr/~fbach/nips03_c   
     * [118]charlesmartin14.files.wor   

archives

     * [119]april 2019
     * [120]december 2018
     * [121]november 2018
     * [122]october 2018
     * [123]september 2018
     * [124]june 2018
     * [125]april 2018
     * [126]december 2017
     * [127]september 2017
     * [128]july 2017
     * [129]june 2017
     * [130]february 2017
     * [131]january 2017
     * [132]october 2016
     * [133]september 2016
     * [134]june 2016
     * [135]february 2016
     * [136]december 2015
     * [137]april 2015
     * [138]march 2015
     * [139]january 2015
     * [140]november 2014
     * [141]september 2014
     * [142]august 2014
     * [143]november 2013
     * [144]october 2013
     * [145]august 2013
     * [146]may 2013
     * [147]april 2013
     * [148]december 2012
     * [149]november 2012
     * [150]october 2012
     * [151]september 2012
     * [152]april 2012
     * [153]february 2012

social

     * [154]view calccon   s profile on twitter
     * [155]view charlesmartin14   s profile on linkedin
     * [156]view charlesmartin   s profile on github
     * [157]view ucaao8ghavcrtszdpobc4_kg   s profile on youtube

meta

     * [158]register
     * [159]log in
     * [160]entries rss
     * [161]comments rss
     * [162]wordpress.com

   logo-i

   [163]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [164]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [165]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [166]likes-master

   %d bloggers like this:

references

   visible links
   1. https://calculatedcontent.com/feed/
   2. https://calculatedcontent.com/comments/feed/
   3. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/feed/
   4. https://calculatedcontent.com/2017/01/05/foundations-mean-field-boltzmann-machines-1987/
   5. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/&for=wpcom-auto-discovery
   8. https://calculatedcontent.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#content
  11. https://calculatedcontent.com/
  12. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
  13. https://calculatedcontent.com/author/charlesmartin14/
  14. https://calculatedcontent.com/category/uncategorized/
  15. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#comments
  16. http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf
  17. http://www.kdnuggets.com/2015/06/why-does-deep-learning-work.html
  18. https://en.wikipedia.org/wiki/folding_funnel
  19. https://arxiv.org/abs/1412.0233
  20. https://www.youtube.com/watch?v=fhzzgfvgc8u
  21. https://keras.io/metrics/#categorical_crossid178
  22. http://scikit-learn.org/stable/modules/cross_validation.html
  23. https://arxiv.org/abs/1611.03530
  24. http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf
  25. https://books.google.com/books?id=br33irc3pkqc&pg=pa299&lpg=pa299&dq=section+6.4.4.+pattern+classification&source=bl&ots=2wcviw9aku&sig=vvts68ykoficqneez3mfrcsfnfy&hl=en&sa=x&ved=0ahukewimk_qv3cvnahuu3gmkha6abvyq6aeihjab#v=onepage&q=section 6.4.4. pattern classification&f=false
  26. http://www.kdnuggets.com/2016/07/deep-learning-networks-scale.html
  27. https://www.youtube.com/watch?v=7ht9k824nwa&t=163s
  28. https://mathematical-neuroscience.springeropen.com/articles/10.1186/s13408-015-0034-5
  29. http://thesciencenetwork.org/programs/raw-science/statistical-mechanics-of-the-neocortex
  30. https://www.youtube.com/watch?v=lekch_i32ie
  31. https://calculatedcontent.com/2017/01/05/foundations-mean-field-boltzmann-machines-1987/
  32. http://www.robots.ox.ac.uk/~ojw/files/notesoncd.pdf
  33. https://www.quora.com/what-is-contrastive-divergence
  34. https://arxiv.org/pdf/1604.01806.pdf
  35. https://calculatedcontent.com/2016/10/21/improving-rbms-with-physical-chemistry/
  36. http://www.mlsalt.eng.cam.ac.uk/foswiki/pub/main/currentmphils/pawel_budzianowski_8224891_assignsubmission_file_budzianowski_dissertation.pdf
  37. https://arxiv.org/pdf/1312.6114.pdf
  38. https://github.com/lzhbrian/mcmc/blob/master/mcmc.pdf
  39. https://github.com/charlesmartin14/emf-rbm
  40. https://arxiv.org/pdf/0806.1147.pdf
  41. https://calculatedcontent.com/2015/03/14/convex-relaxations-of-transductive-learning/
  42. http://edwardlib.org/
  43. https://calculatedcontent.com/2017/01/05/foundations-mean-field-boltzmann-machines-1987/
  44. https://arxiv.org/abs/1611.01838
  45. https://www.youtube.com/watch?v=kibkhipbxiu
  46. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
  47. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?share=twitter
  48. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?share=facebook
  49. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?share=linkedin
  50. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
  51. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?share=reddit
  52. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?share=email
  53. https://calculatedcontent.com/2017/01/05/foundations-mean-field-boltzmann-machines-1987/
  54. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
  55. http://advanceddataanalytics.net/2017/02/28/distilled-news-466/
  56. http://ezequielferrero.com/
  57. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#comment-1742
  58. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?like_comment=1742&_wpnonce=5f2972f33c
  59. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?replytocom=1742#respond
  60. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#comment-1743
  61. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?like_comment=1743&_wpnonce=b1084fe255
  62. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?replytocom=1743#respond
  63. http://ezequielferrero.com/
  64. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#comment-1744
  65. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?like_comment=1744&_wpnonce=d42ec851be
  66. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?replytocom=1744#respond
  67. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#comment-1745
  68. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?like_comment=1745&_wpnonce=a1dd868169
  69. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?replytocom=1745#respond
  70. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#comment-1897
  71. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?like_comment=1897&_wpnonce=e8ab8a1ce7
  72. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?replytocom=1897#respond
  73. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#comment-1899
  74. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?like_comment=1899&_wpnonce=04c2de4f9f
  75. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/?replytocom=1899#respond
  76. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#respond
  77. https://gravatar.com/site/signup/
  78. javascript:highlandercomments.doexternallogout( 'wordpress' );
  79. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
  80. javascript:highlandercomments.doexternallogout( 'googleplus' );
  81. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
  82. javascript:highlandercomments.doexternallogout( 'twitter' );
  83. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
  84. javascript:highlandercomments.doexternallogout( 'facebook' );
  85. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
  86. javascript:highlandercomments.cancelexternalwindow();
  87. https://calculatedcontent.com/author/charlesmartin14/
  88. mailto:info@calculationconsulting.com
  89. http://calculationconsulting.com/
  90. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg
  91. http://www.quora.com/charles-h-martin
  92. https://clarity.fm/charlesmartin14
  93. https://calculatedcontent.com/
  94. https://calculatedcontent.com/2012/10/09/spectral-id91/
  95. https://calculatedcontent.com/2012/10/09/spectral-id91/
  96. https://calculatedcontent.com/2012/02/06/kernels_part_1/
  97. https://calculatedcontent.com/2012/02/06/kernels_part_1/
  98. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
  99. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 100. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 101. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 102. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 103. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 104. https://calculatedcontent.com/2019/04/01/sf-bay-acm-talk-heavy-tailed-self-id173-in-deep-neural-networks/
 105. https://calculatedcontent.com/2018/12/17/heavy-tailed-self-id173-in-deep-neural-nets-1-year-of-research/
 106. https://calculatedcontent.com/2018/11/18/dont-peek-part-2-predictions-without-test-data/
 107. https://calculatedcontent.com/2018/11/16/machine-learning-and-ai-for-the-lean-start-up/
 108. https://calculatedcontent.com/2018/10/07/dont-peek-deep-learning-without-looking-at-test-data/
 109. https://www.youtube.com/redirect?redir_token=ezgiasszjkmz1fnzp0yjtazidd98mtu1ndizmjiznkaxntu0mtq1odm2&q=https://arxiv.org/abs/1810.01075&event=video_description&v=ilv5sc8wjpy
 110. https://arxiv.org/abs/1810.01075
 111. https://arxiv.org/abs/1706.02515
 112. https://github.com/calculatedcontent/tid166
 113. https://charlesmartin14.wordpress.com/2013/11/14/metric-learning-some-quantum-statistical-mechanics/
 114. http://arxiv.org/pdf/1412.0233.pdf
 115. http://www.quora.com/machine-learning/how-does-one-decide-on-which-kernel-to-choose-for-an-id166-rbf-vs-linear-vs-poly-kernel
 116. http://arxiv.org/pdf/1412.6621v3.pdf
 117. http://www.di.ens.fr/~fbach/nips03_cluster.pdf
 118. https://charlesmartin14.files.wordpress.com/2012/10/mat1.png
 119. https://calculatedcontent.com/2019/04/
 120. https://calculatedcontent.com/2018/12/
 121. https://calculatedcontent.com/2018/11/
 122. https://calculatedcontent.com/2018/10/
 123. https://calculatedcontent.com/2018/09/
 124. https://calculatedcontent.com/2018/06/
 125. https://calculatedcontent.com/2018/04/
 126. https://calculatedcontent.com/2017/12/
 127. https://calculatedcontent.com/2017/09/
 128. https://calculatedcontent.com/2017/07/
 129. https://calculatedcontent.com/2017/06/
 130. https://calculatedcontent.com/2017/02/
 131. https://calculatedcontent.com/2017/01/
 132. https://calculatedcontent.com/2016/10/
 133. https://calculatedcontent.com/2016/09/
 134. https://calculatedcontent.com/2016/06/
 135. https://calculatedcontent.com/2016/02/
 136. https://calculatedcontent.com/2015/12/
 137. https://calculatedcontent.com/2015/04/
 138. https://calculatedcontent.com/2015/03/
 139. https://calculatedcontent.com/2015/01/
 140. https://calculatedcontent.com/2014/11/
 141. https://calculatedcontent.com/2014/09/
 142. https://calculatedcontent.com/2014/08/
 143. https://calculatedcontent.com/2013/11/
 144. https://calculatedcontent.com/2013/10/
 145. https://calculatedcontent.com/2013/08/
 146. https://calculatedcontent.com/2013/05/
 147. https://calculatedcontent.com/2013/04/
 148. https://calculatedcontent.com/2012/12/
 149. https://calculatedcontent.com/2012/11/
 150. https://calculatedcontent.com/2012/10/
 151. https://calculatedcontent.com/2012/09/
 152. https://calculatedcontent.com/2012/04/
 153. https://calculatedcontent.com/2012/02/
 154. https://twitter.com/calccon/
 155. https://www.linkedin.com/in/charlesmartin14/
 156. https://github.com/charlesmartin/
 157. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg/
 158. https://wordpress.com/start?ref=wplogin
 159. https://charlesmartin14.wordpress.com/wp-login.php
 160. https://calculatedcontent.com/feed/
 161. https://calculatedcontent.com/comments/feed/
 162. https://wordpress.com/
 163. https://wordpress.com/?ref=footer_blog
 164. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
 165. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#cancel
 166. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 168. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#comment-form-guest
 169. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#comment-form-load-service:wordpress.com
 170. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#comment-form-load-service:twitter
 171. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/#comment-form-load-service:facebook
 172. http://nanonaren.wordpress.com/
 173. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
 174. http://tablewarebox.com/
 175. http://duttatridib.wordpress.com/
 176. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
 177. http://twitter.com/alxfed
 178. http://ashutoshtripathi.com/
 179. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
 180. http://randomstratum.wordpress.com/
 181. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
 182. https://calculatedcontent.com/logo-i-3/
