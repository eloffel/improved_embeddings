   #[1]   feed [2]   comments feed [3]   id172 in deep learning
   comments feed [4]why deep learning works 3: backprop minimizes the free
   energy ? [5]interview with a data scientist [6]alternate [7]alternate
   [8]search [9]wordpress.com

   [10]skip to content

   [11][wp-logo.jpg]

id172 in deep learning

   [12]june 16, 2017 [13]charles h martin, phd [14]uncategorized [15]5
   comments

   a few days ago (jun 2017), a [16]100 page on self-normalizing networks
   appeared.  an amazing piece of theoretical work, it claims to have
   solved the problem of building very large feed forward networks (fnns).

   it builds upon a [17]batch id172 (bn), introduced in 2015    and
   is now the defacto standard for all id98s and id56s.  but not so useful
   for fnns.

   what makes id172 so special?  it makes very deep networks
   easier to train, by damping out oscillations in the distribution of
   activations.

   to see this, the diagram below uses data from figure 1 ([18]from the bn
   paper) to depict how the distribution evolves for a typical node
   outputs in the last hidden layer of a typical network:
   node outputs, with and without batch id172

   very deep nets can be trained faster and generalize better when the
   distribution of activations is kept normalized during backprop.

   we regularly see ultra-deep convnets like inception, id199,
    and resnet.   and giant id56s for id103, [19]machine
   translation, etc.  but we don   t see powerful feedforward neural nets
   (fnns) with more than 4 layers.  until now.

   batch id172 is great for id98s and id56s.

   but we still can not build deep mlps

   this new method      [20]self-id172     has been proposed for
   building very deep multilayer perceptions (mlps) and other feed forward
   nets (fnns).

   the idea is just to tweak a the exponential linear unit (elu)
   activation function to obtain a scaled elu  (selu):

   with this new selu activation function, and a new, alpha dropout
   method, it appears we can, now, build very deep mlps.  and this opens
   the door for deep learning applications on very general data sets. that
   would be great!

   the paper is, however, ~100 pages long of pure math!  fun stuff.. but a
   summary is in order.

   i  review id172 in neural networks,  including batch
   id172, self-id172, and, of course, some statistical
   mechanics ([21]it   s kinda my thing).

   this is an early draft of the post: comments and questions are welcome
     __________________________________________________________________

setup

   [22]wlog, consider an mlp, where we call the input to each layer u

   the linear transformations at each layer is

   \mathbf{x}=\mathbf{wu}+\mathbf{b} ,

   and we apply standard point-wise activations, like a sigmoid

   g(\mathbf{x}_{i})=\dfrac{1}{1+exp(-\mathbf{x}_{i})}

   so that the total set of activations (at each layer) takes the form

   \mathbf{z}=g(\mathbf{x})=g(\mathbf{wu}+\mathbf{b})

   the problem is that during sgd training, the distribution of weights w
   and/or the outputs x can vary widely from iteration to iteration.
   these large variations lead to instabilities in training that require
   small learning rates.  in particular, if the layer weights w or inputs
   u blow up, the activations can become saturated:

   as\;\;\vert\mathbf{w}\vert\rightarrow\infty,\;\;\;g(\mathbf{x})\rightar
   row 0 ,

   leading to[23] vanishing gradients.  traditionally, this was in mlps
   avoided by using larger learning rates, and/or early stopping.

   one solution is better id180, such as a rectified linear
   unit (relu)

   g(\mathbf{x}_{i})=max(\mathbf{x}_{i},0)

   or, for larger networks (depth > 5), an [24]exponential linear unit
   (elu):

   which look like:


   indeed,  sigmoid and tanh activations came from early work in
   computational neuroscience. [25] jack cowan [26]proposed first the
   sigmoid function as [27]a model for neuronal activity, and sigmoid and
   tanh functions arise naturally in statistical mechanics.  and sigmoids
   are still widely used for rbms and mlps   relus don   t help much here.

note: relus only help deep id98s and id56s

   sgd training introduces perturbations in training that propagate
   through the net, causing large variations in weights and activations.
   for fnns, this is a [28]huge problem. but for id98s and id56s..not so
   much.  why ?
     * id98s and id56s, are less distorted by the sgd
       perturbations   presumably because of their weight sharing
       architectures.
     * lstms also avoid this problem [29]by replacing multiplies with
       additions.
     * moreover, dropout (a stochastic regularizer) works very well with
       relus in id98s and id56s, but not so much for mlps and other fnns.
     * and very deep nets, like resnet, which have > 150 layers, use skip
       connections to help propagate the internal residuals.

   it has been said the no real theoretical progress has been made in deep
   nets in 30 years. that is absurd.   we did not have relus or elus. in
   fact, up until batch id172, we were still using id166-style
   id173 techniques for deep nets.  it is clear now that we need
   to [30]rethink generalization in deep learning.

max-norm constraints

   we can regularize a network, like a [31]restricted id82
   (rbm), by applying [32]max norm constraints to the weights w.

   this can be implemented in training by tweaking the weight update at
   the end of pass over all the training data

   \mathbf{dw}\rightarrow\mathbf{dw}+\delta\vert\mathbf{w}\vert

   where \vert\mathbf{w}\vert  is an l1 or l2 norm.

   [33]i have conjectured that this is actually kind of temperature
   control, and prevents the [34]effective temperature of the network from
   collapsing to zero.

   as\;\;\vert\mathbf{w}\vert\rightarrow\infty,\;\;\;t^{eff}\rightarrow
   0\; ,

   by avoiding a low temp, and possibly any glassy regimes, we can use a
   larger effective [35]annealing rate   in modern parlance, larger sgd step
   sizes.

   it makes the network more resilient to changes in scale.

   after 30 years of research neural nets, we  can now achieve an
   analogous network id172 automagically.

   but first, what is current state-of-the-art in code ?  what can we do
   today  with [36]keras ?

   batch id172 (bn) transformation

   [37]tensorflow and other deep learning frameworks now include batch
   id172 out-of-the-box.  under-the-hood, this is the basic idea:

   at the end of every mini-batch \mathcal{b}_{k} ,  the layers are
   whitened.  for each node output x (and before activation):

   \hat{\mathbf{x}}_{k}=\mathbf{wu}_{k}+\mathbf{b}_{k}

   the bn transform maintains the (internal) zero mean \mu_{\mathcal{b}}=0
   and unit variance ( \sigma^{2}_{\mathcal{b}}=1 ).

   we evaluate the sample mini-batch mean  and variance , and then
   normalize, scale, and shift the values:

   the final transformation is applied inside the activation function g():

   g(bn(\mathbf{x}))=g(\mathbf{y}))

   although we can absorb the original layer bias term b into the bn
   transform, giving

   g(bn(\mathbf{x}))=g(bn(\mathbf{wu}))

   so now, instead of renormalizing the weights w after passing over all
   the data, we can normalize the node output x=wu explicitly, for each
   mini-batch, in the backprop pass.

   note that

   \vert\mathbf{wu}\vert<\vert\mathbf{w}\vert\vert\mathbf{u}\vert

   so bounding the weights with max-norm constraints got us part of the
   way already.

   note that extra scaling and shift parameters (\gamma,\beta)_{k} appear
   for each batch (k), and it is necessary to optimize these parameters as
   a side step during training.

   at the end of the transform, we can normalize the network outputs
   (shown above) of the entire training set (population)

   \mathbf{\hat{x}}=\dfrac{\mathbf{x}-\mathbb{e}[\mathbf{x}]}{\sqrt{var[\m
   athbf{x}]+e}} ,

   where the final statistics are computed as, say, an unbiased estimate
   over all (m) mini-batches of the training data

   var[\mathbf{\hat{x}}]=\dfrac{m}{m-1}\mathbb{e}_{\mathbb{b}_{m}}[\sigma^
   {2}_{\mathbb{b}_{m}}] .
     __________________________________________________________________

benefits of the batch norm transform

   the key to batch id172 (bn) is that:
     * the statistics are gathered during a mini-batch step
     * the id172 can be integrated directly into backprop
     * the extra parameters can be tuned during training

   bn allows us to manipulate the activation function of the network. it
   is a differentiable transformation that normalizes activations in the
   network.

   it makes the network (even) more resilient to the parameter scale.

   it has been known for some time that deep nets perform better if the
   inputs are whitened.  and max-norm constraints do re-normalize
   the layer weights after every mini-batch.

   batch id172 appears to be more stable internally, with the
   advantages that it:
     * has replaced [38]max norm constraints
     * is implemented directly in backprop
     * has larger learning rates with standard solvers => faster
       convergence
     * replaces / augments [39]dropout as a regularizer

   still, batch norm training slows down backprop.    can we speed it up ?
     __________________________________________________________________

self-normalizing activations

   a few days ago, the interwebs was buzzing about the paper
   [40]self-normalizing neural networks.   [41]hackernews.  [42]reddit.
   and my linkedin feed.

   these nets use scaled exponential linear units (selu), which have
   implicit self-normalizing properties.  amazingly, the selu is just a
   elu multiplied by \lambda

   screen shot 2017-06-14 at 5.05.10 pm

   where \lambda>1 .

   the paper authors have optimized the values as:

   screen shot 2017-06-16 at 7.39.49 pm

   [43]**a comment on reddit suggests tanh may work as well

   the selus have the explicit properties of:
    1. negative and positive values for controlling the mean \mu
    2. saturation regions to dampen the variance \sigma if it is too large
       in the lower layer,
    3. a slope > 1 to increase \sigma  if it is too small in the lower
       layer, and
    4. a continuous curve, which ensures a fixed point.

   amazingly, the implicit self-normalizing properties are actually
   proved   in only about 100 pages   using the banach fixed point theorem.

   they show that, for an fnn using selu(x) actions, there exists a unique
   attracting and stable fixed point for the mean and variance.
   (curiously, this resembles the argument that deep learning (rbms at
   least) the [44]variational reid172 group (vrg) transform.

   there are, of course, conditions on the weights   things can   t get too
   crazy.  this is hopefully satisfied by selecting  initial weights with
   zero mean and unit variance.

   \sigma^{2}_{\mathcal{b}}=1\;or\; \dfrac{1}{\sqrt{n}} ,

   (depending how we define terms).

   to apply selus, we need a special initialization procedure, and a
   modified version of dropout, alpha-dropout,

selu initialization

   we select initial weights \mathbf{w} from a gaussian distribution with
   mean 0 and variance \frac{1}{\sqrt{n}} , where n is number of weights:
   screen shot 2017-06-16 at 8.08.30 pm tensorflow implementation of
   weight initialization

   [45]in statistical mechanics, this the temperature is proportional to
   the variance of the energy, and therefore sets the energy scale.  since
   e ~ w,

   selu weight initialization is similar in spirit to fixing t=1.

alpha dropout

   note that to apply dropout with an selu, we desire that the mean and
   variance are invariant.

   we must set random inputs to saturated negative value of selu,
   \alpha\lambda then, apply an affine transformation, computing relative
   to dropout rate.

   screen shot 2017-06-16 at 7.40.40 pm

   (thanks to [46]ergol.com for the images and discussion).

   all of this is provided, in code, with implementations already on
   github for [47]tensorflow, [48]pytorch, [49]caffe, etc.
   [50]soon   keras?

key results

   the key results are presented in figure 1 of the paper, where snn =
   self normalizing networks, and the data sets studies are mnist and
   cifar.

   [51]the original code is available on github

   great discussions on [52]hackernews and [53]reddit

summary

   we have reviewed several variants of id172 in deep nets,
   including
     * max norm weight constraints
     * batch id172, and
     * self-normalizing, deep feed forward nets

   along the way, i have tried to convince you that recent developments in
   the id172 of deep nets represent a culmination over 30 years of
   research into neural id177, and that early ideas about finite
   temperature methods from statistical mechanics have evolved into and
   are deeply related to the id172 methods employed today to
   create very deep neural networks

appendix:

temperature control in neural networks

   very early research in neural networks lifted idea from statistical
   mechanics.  [54]early work by hinton formulated autoencoders and the
   principle of the minimum description length (mdl) as minimizing a
   helmholtz free energy:

   f=u-ts ,

   where the expected (t=0) energy is

   u=\sum\limits_{i}p_{i}e_{i} ,

   s is the id178,

   and the temperature t=\dfrac{1}{\beta}=1 implicitly.

   minimizing f yields the familiar boltzmann id203 distribution

   $latex p_{i}=\dfrac{e^{-\beta e_{i}}}{\sum\limits_{j}e^{-\beta
   e_{j}}}&bg=ffffff $.

   when we define an rbm, we parameterize the energy levels e_{i} in terms
   of the configuration of visible and hidden units
   (i\rightarrow\mathbf{u},\mathbf{v})

   e(\mathbf{v},\mathbf{h})=\mathbf{b^{t}v}+\mathbf{c^{t}h}+\mathbf{h^{t}w
   v} ,

   giving the id203

   p(\mathbf{v},\mathbf{h})=\dfrac{e^{-\beta e}}{z(\beta)}

   where [55]z(\beta)  is the partition function, and, again, t=1
   implicitly.

   in stat mech, we call rbms a mean field model because we can decompose
   the total energy and/or conditional probabilities using sigmoid
   activations \sigma() for each node

   screen shot 2017-06-14 at 11.31.33 pm

   in my 2016 mmds talk, i proposed that without some explicit temperature
   control, rbms could collapse into a glassy state.

   and now, [56]some proof i am not completely crazy:

   another recent 2017 study on the [57]emergence of compositional
   representations in restricted id82s,  we do indeed see
   that the rbm effective temperature does indeed drop well below 1 during
   training

   t^{eff}<<1

   and that rbms can exhibit glassy behavior.

   i also proposed that rbms could undergo id178 collapse at very low
   temperatures. this has also now been verified in a [58]recent 2016
   paper.

   finally, this 2017 paper:[59]train longer, generalize better: closing
   the generalization gap in large batch training of neural networks   
   proposes that many networks exhibit something like glassy behavior
   described as    ultra-slow    diffusion behavior.

sketch of the proof:

   i will sketch out the proof in some detail if there is demand.
   [60]intuitively (& citing comments in hackernews):    
     *    for negative net inputs the variance is decreased, for positive
       net inputs the variance is increased.
     * for very negative values the variance decrease is stronger. for
       inputs close to zero the variance increase is stronger.
     * for large invariance in one layer, the variance gets decreased more
       in the next layer, and vice versa.
     * theorem 2 states that the variance can be bounded from above and
       hence there are not exploding gradients.
     * theorem 3 states that the variance can be bounded from below and
       does not vanish.   
     __________________________________________________________________

x = me kicking your a%%

   iframe:
   [61]https://www.youtube.com/embed/lritdcerank?version=3&rel=1&fs=1&auto
   hide=2&showsearch=0&showinfo=1&iv_load_policy=1&wmode=transparent

share this:

     * [62]twitter
     * [63]facebook
     * [64]linkedin
     * [65]more
     *

     * [66]reddit
     * [67]email
     *
     *

like this:

   like loading...

related

post navigation

   [68]previous post: why deep learning works 3: backprop minimizes the
   free energy ?
   [69]next post: interview with a data scientist

5 comments

    1. pingback: [70]distilled news | data analytics & r
    2. pingback: [71]what is free energy part i: hinton, helmholtz, and
       legendre     calculated content
    3. pingback: [72]id172 in deep learning     calculated content |
       premium blog!
    4.
   [73]babita koundal says:
       [74]march 30, 2018 at 1:49 am
       nice explanation.
       [75]likelike
       [76]reply
    5. pingback: [77]what is free energy: hinton, helmholtz, and legendre
       |                   

leave a reply [78]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [79]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [80]log out /
   [81]change )
   google photo

   you are commenting using your google account. ( [82]log out /
   [83]change )
   twitter picture

   you are commenting using your twitter account. ( [84]log out /
   [85]change )
   facebook photo

   you are commenting using your facebook account. ( [86]log out /
   [87]change )
   [88]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

     * [89]charles h martin, phd

calculation consulting

   we are a boutique machine learning data science consultancy. how can we
   help? email me at [90]info@calculationconsulting.com.

   or stop by:
   [91]http://calculationconsulting.com
   [92]youtube channel
   [93]quora

   set up a quick all on [94]clarity.fm

the community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

blog stats

     * 521,305 hits

   [95]follow on wordpress.com

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   join 694 other followers

   ____________________

   (button) follow

top posts & pages

     * [96]spectral id91: a quick overview
       [97]spectral id91: a quick overview
     * [98]kernels part 1: what is an rbf kernel? really?
       [99]kernels part 1: what is an rbf kernel? really?
     * [100]why deep learning works ii: the reid172 group
       [101]why deep learning works ii: the reid172 group
     * [102]id172 in deep learning
       [103]id172 in deep learning
     * [104]causality, correlation, and brownian motion
       [105]causality, correlation, and brownian motion

recent posts

     * [106]sf bay acm talk: heavy tailed self id173 in deep
       neural networks
     * [107]heavy tailed self id173 in deep neural nets: 1 year
       of research
     * [108]don   t peek part 2: predictions without test data
     * [109]machine learning and ai for the lean start up
     * [110]don   t peek: deep learning without looking     at test data

top clicks

     * [111]youtube.com/redirect?redi   
     * [112]arxiv.org/abs/1810.01075
     * [113]arxiv.org/abs/1706.02515
     * [114]github.com/calculatedcont   
     * [115]charlesmartin14.wordpress   
     * [116]arxiv.org/pdf/1412.0233.p   
     * [117]quora.com/machine-learnin   
     * [118]arxiv.org/pdf/1412.6621v3   
     * [119]di.ens.fr/~fbach/nips03_c   
     * [120]charlesmartin14.files.wor   

archives

     * [121]april 2019
     * [122]december 2018
     * [123]november 2018
     * [124]october 2018
     * [125]september 2018
     * [126]june 2018
     * [127]april 2018
     * [128]december 2017
     * [129]september 2017
     * [130]july 2017
     * [131]june 2017
     * [132]february 2017
     * [133]january 2017
     * [134]october 2016
     * [135]september 2016
     * [136]june 2016
     * [137]february 2016
     * [138]december 2015
     * [139]april 2015
     * [140]march 2015
     * [141]january 2015
     * [142]november 2014
     * [143]september 2014
     * [144]august 2014
     * [145]november 2013
     * [146]october 2013
     * [147]august 2013
     * [148]may 2013
     * [149]april 2013
     * [150]december 2012
     * [151]november 2012
     * [152]october 2012
     * [153]september 2012
     * [154]april 2012
     * [155]february 2012

social

     * [156]view calccon   s profile on twitter
     * [157]view charlesmartin14   s profile on linkedin
     * [158]view charlesmartin   s profile on github
     * [159]view ucaao8ghavcrtszdpobc4_kg   s profile on youtube

meta

     * [160]register
     * [161]log in
     * [162]entries rss
     * [163]comments rss
     * [164]wordpress.com

   logo-i

   [165]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [166]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [167]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [168]likes-master

   %d bloggers like this:

references

   visible links
   1. https://calculatedcontent.com/feed/
   2. https://calculatedcontent.com/comments/feed/
   3. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/feed/
   4. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
   5. https://calculatedcontent.com/2017/06/26/interview-with-a-data-scientist/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/&for=wpcom-auto-discovery
   8. https://calculatedcontent.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/#content
  11. https://calculatedcontent.com/
  12. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
  13. https://calculatedcontent.com/author/charlesmartin14/
  14. https://calculatedcontent.com/category/uncategorized/
  15. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/#comments
  16. https://arxiv.org/abs/1706.02515
  17. http://jmlr.org/proceedings/papers/v37/ioffe15.pdf
  18. http://jmlr.org/proceedings/papers/v37/ioffe15.pdf
  19. https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
  20. https://arxiv.org/abs/1706.02515
  21. https://www.youtube.com/watch?v=y0qbxfvbjfk
  22. https://en.wikipedia.org/wiki/without_loss_of_generality
  23. https://www.quora.com/what-is-the-vanishing-gradient-problem
  24. https://arxiv.org/pdf/1511.07289.pdf
  25. https://www.youtube.com/watch?v=7ht9k824nwa
  26. https://www.ncbi.nlm.nih.gov/pmc/articles/pmc4820414/
  27. https://www.youtube.com/watch?v=67hdtyjrpka
  28. https://www.google.com/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=0ahukewiij4ulw8duahvs0omkhzfpcxgqjrwibw&url=https://imgflip.com/tag/donald%20trump%20huge&psig=afqjcnf4rx1rueuqk4lpnzmkkbg7mc6wcg&ust=1497639294663297
  29. https://www.quora.com/how-does-lstm-help-prevent-the-vanishing-and-exploding-gradient-problem-in-a-recurrent-neural-network
  30. https://www.quora.com/why-is-the-paper-   understanding-deep-learning-required-rethinking-generalization   -important
  31. https://calculatedcontent.com/2016/10/21/improving-rbms-with-physical-chemistry/
  32. https://stats.stackexchange.com/questions/257996/what-is-maxnorm-constraint-how-is-it-useful-in-convolutional-neural-networks
  33. https://www.youtube.com/watch?v=kibkhipbxiu
  34. https://arxiv.org/pdf/1611.06759.pdf
  35. https://calculatedcontent.com/2017/01/05/foundations-mean-field-boltzmann-machines-1987/
  36. https://keras.io/
  37. https://www.tensorflow.org/
  38. https://www.reddit.com/r/machinelearning/comments/2bopxs/question_about_the_maxnorm_constraint_used_with/
  39. http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf
  40. https://arxiv.org/pdf/1706.02515.pdf
  41. https://news.ycombinator.com/item?id=14527686
  42. https://www.reddit.com/r/machinelearning/comments/6g5tg1/r_selfnormalizing_neural_networks_improved_elu/?st=j3xofa6g&sh=0912afa3
  43. https://www.reddit.com/r/machinelearning/comments/6g5tg1/r_selfnormalizing_neural_networks_improved_elu/?st=j40o4ikz&sh=ccd4950b
  44. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
  45. https://calculatedcontent.com/2013/11/14/foundations-the-parition-function/
  46. http://www.erogol.com/paper-review-self-normalizing-neural-networks/
  47. https://github.com/bioinf-jku/snns
  48. https://github.com/dannysdeng/selu
  49. https://github.com/holmesshuan/snns-self-normalizing-neural-networks-caffe-reimplementation
  50. https://github.com/fchollet/keras/pull/6969
  51. https://github.com/bioinf-jku/snns
  52. https://news.ycombinator.com/item?id=14527686
  53. https://www.reddit.com/r/machinelearning/comments/6g5tg1/r_selfnormalizing_neural_networks_improved_elu/?st=j40a1tx2&sh=4b2fba79
  54. http://www.cs.toronto.edu/~fritz/absps/cvq.pdf
  55. https://calculatedcontent.com/2013/11/14/foundations-the-parition-function/
  56. https://www.youtube.com/watch?v=itrbwbvtrzw&t=0m35s
  57. https://arxiv.org/pdf/1611.06759.pdf
  58. https://arxiv.org/pdf/1608.03714.pdf
  59. https://arxiv.org/abs/1705.08741v1
  60. https://news.ycombinator.com/item?id=14527686
  61. https://www.youtube.com/embed/lritdcerank?version=3&rel=1&fs=1&autohide=2&showsearch=0&showinfo=1&iv_load_policy=1&wmode=transparent
  62. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/?share=twitter
  63. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/?share=facebook
  64. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/?share=linkedin
  65. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
  66. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/?share=reddit
  67. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/?share=email
  68. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
  69. https://calculatedcontent.com/2017/06/26/interview-with-a-data-scientist/
  70. http://advanceddataanalytics.net/2017/06/19/distilled-news-541/
  71. https://calculatedcontent.com/2017/07/04/what-is-free-energy-part-i-hinton-helmholtz-and-legendre/
  72. http://phpcantho.com/id172-in-deep-learning-calculated-content/
  73. https://www.webtunix.com/
  74. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/#comment-2737
  75. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/?like_comment=2737&_wpnonce=4d59fd4f46
  76. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/?replytocom=2737#respond
  77. https://163ai.org/14/note/12/05/07/machine-learning/4063/share/author/machinelearning/2017/
  78. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/#respond
  79. https://gravatar.com/site/signup/
  80. javascript:highlandercomments.doexternallogout( 'wordpress' );
  81. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
  82. javascript:highlandercomments.doexternallogout( 'googleplus' );
  83. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
  84. javascript:highlandercomments.doexternallogout( 'twitter' );
  85. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
  86. javascript:highlandercomments.doexternallogout( 'facebook' );
  87. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
  88. javascript:highlandercomments.cancelexternalwindow();
  89. https://calculatedcontent.com/author/charlesmartin14/
  90. mailto:info@calculationconsulting.com
  91. http://calculationconsulting.com/
  92. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg
  93. http://www.quora.com/charles-h-martin
  94. https://clarity.fm/charlesmartin14
  95. https://calculatedcontent.com/
  96. https://calculatedcontent.com/2012/10/09/spectral-id91/
  97. https://calculatedcontent.com/2012/10/09/spectral-id91/
  98. https://calculatedcontent.com/2012/02/06/kernels_part_1/
  99. https://calculatedcontent.com/2012/02/06/kernels_part_1/
 100. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 101. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 102. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 103. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 104. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 105. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 106. https://calculatedcontent.com/2019/04/01/sf-bay-acm-talk-heavy-tailed-self-id173-in-deep-neural-networks/
 107. https://calculatedcontent.com/2018/12/17/heavy-tailed-self-id173-in-deep-neural-nets-1-year-of-research/
 108. https://calculatedcontent.com/2018/11/18/dont-peek-part-2-predictions-without-test-data/
 109. https://calculatedcontent.com/2018/11/16/machine-learning-and-ai-for-the-lean-start-up/
 110. https://calculatedcontent.com/2018/10/07/dont-peek-deep-learning-without-looking-at-test-data/
 111. https://www.youtube.com/redirect?redir_token=ezgiasszjkmz1fnzp0yjtazidd98mtu1ndizmjiznkaxntu0mtq1odm2&q=https://arxiv.org/abs/1810.01075&event=video_description&v=ilv5sc8wjpy
 112. https://arxiv.org/abs/1810.01075
 113. https://arxiv.org/abs/1706.02515
 114. https://github.com/calculatedcontent/tid166
 115. https://charlesmartin14.wordpress.com/2013/11/14/metric-learning-some-quantum-statistical-mechanics/
 116. http://arxiv.org/pdf/1412.0233.pdf
 117. http://www.quora.com/machine-learning/how-does-one-decide-on-which-kernel-to-choose-for-an-id166-rbf-vs-linear-vs-poly-kernel
 118. http://arxiv.org/pdf/1412.6621v3.pdf
 119. http://www.di.ens.fr/~fbach/nips03_cluster.pdf
 120. https://charlesmartin14.files.wordpress.com/2012/10/mat1.png
 121. https://calculatedcontent.com/2019/04/
 122. https://calculatedcontent.com/2018/12/
 123. https://calculatedcontent.com/2018/11/
 124. https://calculatedcontent.com/2018/10/
 125. https://calculatedcontent.com/2018/09/
 126. https://calculatedcontent.com/2018/06/
 127. https://calculatedcontent.com/2018/04/
 128. https://calculatedcontent.com/2017/12/
 129. https://calculatedcontent.com/2017/09/
 130. https://calculatedcontent.com/2017/07/
 131. https://calculatedcontent.com/2017/06/
 132. https://calculatedcontent.com/2017/02/
 133. https://calculatedcontent.com/2017/01/
 134. https://calculatedcontent.com/2016/10/
 135. https://calculatedcontent.com/2016/09/
 136. https://calculatedcontent.com/2016/06/
 137. https://calculatedcontent.com/2016/02/
 138. https://calculatedcontent.com/2015/12/
 139. https://calculatedcontent.com/2015/04/
 140. https://calculatedcontent.com/2015/03/
 141. https://calculatedcontent.com/2015/01/
 142. https://calculatedcontent.com/2014/11/
 143. https://calculatedcontent.com/2014/09/
 144. https://calculatedcontent.com/2014/08/
 145. https://calculatedcontent.com/2013/11/
 146. https://calculatedcontent.com/2013/10/
 147. https://calculatedcontent.com/2013/08/
 148. https://calculatedcontent.com/2013/05/
 149. https://calculatedcontent.com/2013/04/
 150. https://calculatedcontent.com/2012/12/
 151. https://calculatedcontent.com/2012/11/
 152. https://calculatedcontent.com/2012/10/
 153. https://calculatedcontent.com/2012/09/
 154. https://calculatedcontent.com/2012/04/
 155. https://calculatedcontent.com/2012/02/
 156. https://twitter.com/calccon/
 157. https://www.linkedin.com/in/charlesmartin14/
 158. https://github.com/charlesmartin/
 159. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg/
 160. https://wordpress.com/start?ref=wplogin
 161. https://charlesmartin14.wordpress.com/wp-login.php
 162. https://calculatedcontent.com/feed/
 163. https://calculatedcontent.com/comments/feed/
 164. https://wordpress.com/
 165. https://wordpress.com/?ref=footer_blog
 166. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 167. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/#cancel
 168. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 170. https://charlesmartin14.wordpress.com/?attachment_id=8985#main
 171. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/screen-shot-2017-06-14-at-2-10-40-pm/
 172. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/mlp-2/
 173. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/screen-shot-2017-06-14-at-8-32-18-am/
 174. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/screen-shot-2017-06-14-at-8-34-38-am/
 175. https://www.youtube.com/watch?v=kibkhipbxiu
 176. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/screen-shot-2017-06-14-at-10-57-33-am-2/
 177. https://charlesmartin14.wordpress.com/?attachment_id=8993#main
 178. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/screen-shot-2017-06-16-at-12-52-57-pm/
 179. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/#comment-form-guest
 180. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/#comment-form-load-service:wordpress.com
 181. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/#comment-form-load-service:twitter
 182. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/#comment-form-load-service:facebook
 183. http://nanonaren.wordpress.com/
 184. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 185. http://tablewarebox.com/
 186. http://duttatridib.wordpress.com/
 187. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 188. http://twitter.com/alxfed
 189. http://ashutoshtripathi.com/
 190. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 191. http://randomstratum.wordpress.com/
 192. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 193. https://calculatedcontent.com/logo-i-3/
