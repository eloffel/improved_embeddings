   #[1]   feed [2]   comments feed [3]   spectral id91: a quick
   overview comments feed [4]kernels, green   s functions, and resolvent
   operators [5]data spectroscopy: gaussian kernels and harmonic
   oscillators [6]alternate [7]alternate [8]search [9]wordpress.com

   [10]skip to content

   [11][wp-logo.jpg]

spectral id91: a quick overview

   [12]october 9, 2012 [13]charles h martin, phd [14]uncategorized [15]27
   comments

   a lot of my ideas about machine learning come from quantum mechanical
   perturbation theory.  to provide some context, we need to step back and
   understand that the familiar techniques of machine learning, like
   spectral id91, are, in fact, nearly identical to quantum
   mechanical spectroscopy.   as usual, this will take several blogs.

   here, i give a brief tutorial on the theory of spectral id91 and
   how it is implemented in open source packaages

   at some point i will rewrite some of this and add a review of this
   recent paper  [16]robust and scalable graph-based semisupervised
   learning

spectral (or subspace) id91

   the goal of spectral id91 is to cluster data that is connected
   but not lnecessarily compact or clustered within convex boundaries

   the basic idea:
    1. project your data into r^{n}
    2. define an affinity  matrix a , using a gaussian kernel k or say
       just an adjacency matrix (i.e. a_{i,j}=\delta_{i,j})
    3. construct the graph laplacian from a (i.e. decide on a
       id172)
    4. solve an eigenvalue problem , such as l v=\lambda v  (or a
       generalized eigenvalue problem l v=\lambda d v )
    5. select k eigenvectors \{ v_{i}, i=1, k \} corresponding to the k
       lowest (or highest) eigenvalues   \{ \lambda_{i}, i=1, k \} , to
       define a k-dimensional subspace p^{t}lp
    6. form clusters in this subspace using, say, id116

   sounds simple enough.  lets dig into the details a bit to see what
   works.

affinities and similarities

   what is an affinity?  it is a metric that determines how close, or
   similar, two points our in our space.  we have to guess the metric, or
   we have to learn the metric.  for now, we will guess the metric.
   notice it is not the standard euclidean metric.  the next simplest
   thing is, of course, a gaussian kernel.

   given 2 data points x_{i},x_{j} (projected in r^{n} ), we define an
   affinity a_{i,j} that is positive, symmetric, and depends on the
   euclidian distance \vert x_{i}-x_{j}\vert between the data points

   a_{i,j}\simeq exp(-\alpha \vert x_{i}-x_{j}\vert^{2})

   we might provide a hard cut off r , so that

   a_{i,j}= 0 if \vert x_{i}-x_{j}\vert^{2}\geq r

   and in some applications, we  pre-multiply   a_{i,j} with a feature
   affinity kernel (see  shi and malik   s approach to image segmentation).

   ( we may also learn the affinity matrix from the data.  for example,
   see learning spectral id91 by bach and
   jordan [17]http://www.di.ens.fr/~fbach/nips03_cluster.pdf )

   of course,   a_{i,j}\simeq 1  when the points are close in r^{n} , and
   a_{i,j}\rightarrow 0 if the points x_{i}, x_{j} are far apart.  close
   data points are in the same cluster.   data points
   in different clusters are far away.  but data points in the same
   cluster may also be far away   even farther away than points in different
   clusters.  our goal then is to transform the space so that when 2
   points x_{i}, x_{j} are close, they are  always in  same cluster, and
   when they are far apart, they are in different clusters.

   generally we use the gaussian kernel k directly, or we form the graph
   laplacian a .  usually the graph laplacian is described as a discrete
   approximation to the laplacian from physics, and or the
   laplace-beltrami operator.

   here, i want to point out the obvious relation between  the gaussian
   kernel k and a simple adjacency matrix.  taking the lowest order taylor
   expansion for the gaussian kernel, we get

   k_{i,j}\simeq exp(-\alpha \vert x_{i}-x_{j}\vert^{2})=i-\alpha \vert
   x_{i}- x_{j}\vert^{2}+\cdots

   the unnormalized graph laplacian is defined  as the difference of 2
   matrices

   l_{i,j}=d_{i,j}-w_{i,j}

   where d is the diagonal degree matrix, defined below, assigned to the
   graph vertices, and w is a matrix of positive  weights w_{i,j} assigned
   to the graph edges. let us evaluate the expectation value of l with
   vector x (proposition 1, von luxburg):

   x^{t}lx=\frac{1}{2}\sum_{i,j=1}^{n}w_{i,j}\vert x_{i}-x_{j}\vert^{2}

   when using an adjacency matrix, the weights are all 1:

   a_{i,j}=w_{i,j}=\delta_{i,j}

   so, apart from the diagonal and some id172, we see that the
   graph laplacian for an adjacency matrix s a low order approximation to
   the gaussian kernel, which is a good approximation when x_{j},x_{i} are
   in the same cluster.

   in the real world, one usually applies a gaussian or heat kernel in
   order to account for irregularities in the data, as in the[18] scikit
   learn package.

laplacians

   there are lots of laplacians  l in the literature.  normalized,
   generalized, relaxed, etc.

   they all share the degree matrix d .    the degree matrix is a diagonal
   matrix that measures the degree at each node

   d_{i,i}=\sum^{n}_{j}a_{i,j}

   being an old physical chemist, i think of this as a local (nearest
   neighbor) mean-field average of the affinity;   d is a id172
   factor for a so that the cluster affinities are balanced across
   different clusters.    this is easier to see in a graph cut formulation
   (later)

   we can now define the laplacian and outline some variations

    the main differences are how one normalizes the data and sets the
   diagonal of a_{i,i}
    1. simple laplacian l=d-a
    2. normalized laplacian  l_{n}=d^{-1/2}ld^{-1/2}
    3. generalized laplacian l_{g} = d^{-1}l
    4.  relaxed laplacian l_{\rho} = l-\rho d
    5. ng, jordan, & weiss laplacian  l_{njw}=d^{-1/2}ad^{-1/2} , and
       where a_{i,i}=0
    6. and we note the related, smoothed kernel for kmeans kernel
       id91   k=\sigma d^{-1}+d^{-1}ad^{-1}

   what are the main differences? left multiplying by a diagonal matrix is
   akin to scaling the rows, but right multiplying by a diagonal matrix is
   akin to scaling the columns.  the generalized laplacian results in a
   right-stochastic markov matrix ; the normalized laplacian does not.

   i might have missed some variations since clearly there is a lot of
   leeway and creativity in defining a laplacian for id91. in a
   future post i will explain how these graph laplacians are related to
   the classical laplacian from physics.  the von luxburg review attempts
   to clean this up, and i may re-write this section based on his review.

the cluster eigenspace problem

   if good clusters can be identified, then the  laplacian l is
   approximately block-diagonal, with each block defining a cluster.
   so, if we have 3 major clusters (c1, c2, c3) , we would expect

   \begin{array}{ccc} l_{1,1} & l_{1,2} & l_{1,3}\\ l_{2,1} & l_{2,2} &
   l_{2,3}\\ l_{3,1} & l_{2,3} & l_{3,3} \end{array}
   \sim\begin{array}{ccc} l_{1,1} & 0 & 0\\ 0 & l_{2,2} & 0\\ 0 & 0 &
   l_{3,3} \end{array}

   where l_{c1,c1} corresponds to subblock for cluster c1 , etc.  these
   blocks let us identify clusters with non-convex boundaries, as shown
   below

   we also expect that the 3 lowest eigenvalues  & eigenvectors
   (\lambda_{i},v_{i}) of l each correspond to a different cluster.  this
   occurs when each eigenvector corresponds to the lowest eigenvector of
   some subblock of  l_{c,c} .  that is, if

   lv_{i}=\lambda_{i}v_{i} are the lowest eigenvalue, eigenvector pairs in
   the full space, and

   l_{c1,c1}v_{c1}=\lambda_{c1}v_{c1} is the lowest eigenvalue ,
   eigenvector pair for block c1,

   then   v_{c1} is a good approximation  to one of the lowest 3 v_{i} .
   likewise for subblocks c2 and c3.

   more importantly, this also restricts the  eigenvalue spectrum of l ,
   so that the set lowest 3 full space eigenvalues  consists of the lowest
   subblock eigenvalues

   \{ \lambda_{i},i=1,3 \} = \{ \lambda_{ci},i=1,3 \}

   also, to identify k clusters,  the eigenvalue spectrum of l must have a
   gap, as shown below (with 4, not 3 eigenvalues, sorry)

   frequently this gap is hard to find, and choosing the optimal k is
   called    rounding   

   i am being a bit sloppy to get the general ideas across.  technically,
   running kmeans in the subspace is not exactly the same as identifying
   each eigenvector with a specific cluster.    indeed, one might imagine
   using a slightly larger subspace than necessary, and only extracting
   the k clusters desired.  the subtly here is getting choosing the right
   affinity ( matrix cutoff  r and all), the right size of the subspace,
   and the right id172 (both of the laplacian and the eigenvectors
   themselves, before or after diagonalization)  you should refer to the
   original papers, the reviews, and whatever open source code you are
   using  for very specific details.

   recently (well, kinda, 2008), these ideas have been formalized  in
   [19]l. rosasco, m. belkin, e. de vitoa note on perturbation results for
   learning,  empirical operators

   using some perturbation theory and the [20]resolvent operators
   introduced earlier   which gives you an idea of what i like to read just
   for fun.

   [ in the language of quantum mechanical perturbation theory, we would
   say that the eigenvectors of the subblocks form a model space that
   approximates the true low lying (or high lying) eigenspace of l .  this
   will come up later ]

constraints on real world data

   below we depict the block structure of the affinity matrix for good and
   poor cases of the similarity metric

   notice that the good similarity / affinity matrix is itself block
   diagonal when 2 clear clusters can be identified

open source implementations

   the python toolkit scikit learn has an [21]implementation of spectral
   id91.

   rather than review this, i just want to comment on the 2 examples
   because neither actually demonstrate where the method is most useful.

   [22]the first example is simply to identify 4 overlapping circular
   clusters.  here,  even simple kmeans would probably be fine because the
   clusters are compact.  they argue the point is to separate the clusters
   from the background, but, again, this is a simple problem.

   [23]the second example is the classic [24]lena image.

   here, the method does not do much because simple spectral id91
   can not    learn    from other examples.  here really one wants a
   supervised or semi-supervised method such as [25]semisupervised
   learning using laplacian / manifold id173 or even [26]deep
   learning that can identify faces based on a large training sample of
   many images of many different faces.

   the power of spectral id91 is to identify non-compact clusters in
    a single data set (see images above)

   stay tuned

   the constraint on the eigenvalue spectrum also suggests, at least to
   this blogger, spectral id91 will only work on fairly uniform
   datasets   that is, data sets with n uniformly sized clusters.   (this is
   also briefly mentioned by von luxburg , where she suggests always using
   the generalized laplacian.)  otherwise, there is no guarantee that the
   full space and subblock eigenspectrums will line up so nicely.

   some time later, i will take a look at some real world data, look at
   andrew ng   s old argument about matrix perturbation theory, and then
   explain how to apply matrix perturbation theory to (hopefully) deal the
   cases of a poor similarity measure (right side above) .

   but first, we take a look at some more modern work by shi, belkin, and
   yu, called [27]data spectroscopy.  we will see that spectral id91
   with an rbf kernel is, in essence, just like quantum mechanical
   spectroscopy where the data points are quantum mechanical harmonic
   oscillators!

   then we will look at generalizations of spectral id91 such as
   [28]using the graph laplacian as a regularizer for semisupervised
   manifold learning.

references

   von luxburg, a tutorial on spectral id91,
   [29]http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/atta
   chments/luxburg07_tutorial_4488%5b0%5d.pdf

   filippone, camastra, masulli, &  rovetta ,a survey of kernel and
   id106 for
   id91, [30]http://lagis-vi.univ-lille1.fr/~lm/classpec/publi_class
   if/a_survey_of_kernel_and_spectral_methods_for_id91_pr_2008.pdf

   smola and kondor, kernels and id173 on graphs
   [31]http://enpub.fulton.asu.edu/cseml/07spring/kenel_graphs.pdf

   and one of the more    popular    papers

   ng, jordan, and weiss,  on spectral id91: analysis and an
   algorithm,  [32]http://ai.stanford.edu/~ang/papers/nips01-spectral.pdf

share this:

     * [33]twitter
     * [34]facebook
     * [35]linkedin
     * [36]more
     *

     * [37]reddit
     * [38]email
     *
     *

like this:

   like loading...

related

post navigation

   [39]previous post: kernels, green   s functions, and resolvent operators
   [40]next post: data spectroscopy: gaussian kernels and harmonic
   oscillators

27 comments

    1.
   [41]sergey feldman says:
       [42]october 15, 2012 at 6:06 pm
       thanks for the post!
       the normalized laplacian is not the same as the generalized
       laplacian. left multiplying by a diagonal matrix is akin to scaling
       the rows, but right multiplying by a diagonal matrix is akin to
       scaling the columns.
       the generalized laplacian results in a right-stochastic markov
       matrix (assuming a_{ij} were all non-negative in the first place),
       but the normalized laplacian does not.
       also, another cool kernel graph-related kernel is the graph
       laplacian kernel:
       $$ k = (i + \gamma l) ^ {-1} $$
       see:
       [43]http://enpub.fulton.asu.edu/cseml/07spring/kenel_graphs.pdf
       [44]likelike
       [45]reply
         1.
        charlesmartin14 says:
            [46]october 15, 2012 at 6:21 pm
            thanks for the clarification. i did not think anyone ever read
            this.      i   ll make the correction
            i think what you call the graph kernel laplacian is what i
            call a resolvent operator
            [47]https://charlesmartin14.wordpress.com/2012/09/28/kernels-g
            reens-functions-and-resolvent-operators/
            [48]likelike
            [49]reply
    2.
   hemanta says:
       [50]august 8, 2013 at 7:45 am
          5 . select k eigenvectors \{ v_{i}, i=1, k \} corresponding to the
       k lowest (or highest)     .. when should we use lowest or highest
       eigen vectors
          ng, jordan, & weiss laplacian l_{njw}=d^{-1/2}ad^{-1/2}, and where
       a_{i,i}=0     .. but u von luxburg 2007 used l instead of a (section
       3.2)
       [51]likelike
       [52]reply
         1.
        charlesmartin14 says:
            [53]august 8, 2013 at 3:46 pm
            its a sign issue and i can   t recall which one. what i would do
            is just look at a toy problem and select the lowest or highest
            eigenvector and see how many nodes (zero crossings) its has.
            the one you want has the fewest nodes. lets look at scikit
            learn and see what it does   this is implemented there
            [54]likelike
            [55]reply
    3.
   salik syed says:
       [56]december 9, 2013 at 4:21 am
       charles,
       big fan of your blog. i am curious if you can give a relationship
       to the work that is being done in persistent homology. to me that
       approach is a more general approach to solving id91 over
       non-compact sets of points and has some similar undertones (in the
       most abstract sense the eigenvalue jump seems reminiscent of
       barcodes)
       [57]likelike
       [58]reply
         1.
        charlesmartin14 says:
            [59]december 9, 2013 at 6:28 am
            yes, it is very similar. indeed, in this video, they explain
            the relationship between the homology and the eigenspectra:
            [60]http://videolectures.net/icml08_belkin_dslmm/
            i have not personally compared the techniques but i imagine it
            is possible to apply techniques of data spectroscopy at
            different levels of scale, thereby obtaining the same kind of
            graphical results that the ayasdi guys get using persistant
            homology. remember that the homology approach requires
            defining a similarity metric, and this is equivalent to
            defining a kernel, so compact or not, one still needs a metric
            embedding.
            clearly the spectrum depends on the kernel, and most people
            just use an rbf kernel because it is convenient.what is
            curious to me is that this spectral approach is equivalent to
            what is done in quantum field theory, where we represent the
            field quanta (data points) using quantum mechanical harmonic
            oscillators, and then solve for the clusters by minimizing an
            energy functional. on the spectral gap, the case i describe
            here is very idealized, and i imagine that the common data
            spectra is could be difficult to partition
            [61]likelike
            [62]reply
    4.
   [63]jayprich says:
       [64]february 15, 2014 at 9:41 am
       great overview with pointers to unifying different fields.
       subspaces are sparsity inducing transformations in compressed
       sensing. as you point out, learning the prior from samples is the
       tricky bit.
       do you know if for image processing, noise rejection in patch-space
       (non local means) can be augmented with anisotropic local kernels?
       e.g. use a simple rbf first then re-use the connectivity    local
       direction    to smooth density better and reinforce that topology
       clarifying cluster boundary smoothness.
       database normal forms are more strictly block diagonal but i
       imagine search and id136 applications can use such
       probabilistic structure to execute queries     essentially set
       algebra. this might become feasible if performed on subsamples
       viz.id91 partially observed graphs via id76.
       ali jalali, yudong chen, sujay sanghavi, huang xu.
       [65]likelike
       [66]reply
         1.
        charlesmartin14 says:
            [67]february 15, 2014 at 4:57 pm
            i think that whenever you try to do some kind of local
            smoothing, you risk over-fitting. one way to treat this is to
            smooth at different levels of resolution and look for a
            persistent topology.
            [68]likelike
            [69]reply
         2.
        charlesmartin14 says:
            [70]july 7, 2014 at 7:59 am
            i dont know much about it but it sounds interesting
            thanks for the link
            [71]likelike
            [72]reply
    5. pingback: [73]machine learning with missing labels part 2: the
       univerid166 | machine learning
    6.
   [74]tdiethe says:
       [75]march 10, 2015 at 8:41 pm
       one tiny comment     von luxburg is a she not a he     
       [76]likelike
       [77]reply
         1.
        charlesmartin14 says:
            [78]march 10, 2015 at 8:51 pm
            thanks   correction made
            [79]likelike
            [80]reply
    7. pingback: [81]why deep learning works ii: the reid172 group
       | machine learning
    8.
   ilya says:
       [82]august 10, 2015 at 3:49 pm
       one comment about
          this is also briefly mentioned by von luxburg , where she suggests
       always using the generalized laplacian   .
       von luxburg tells that    as using normalized laplacian also does not
       have any computational
       advantages, we thus advocate for using generalized laplacian   ,
       which i find a bit not correct, since normalized laplacian is a
       real symmetrical matrix with clear computational advantages, which
       is not the case for the generalized laplacian.
       of course, both matrices have the same eigenvalues, and
       eigenvectors are just scaled. however, i would use normalized
       laplacian instead of the generalized one, especially if the
       computational time is of concern (there are such applications).
       [83]likelike
       [84]reply
    9.
   [85]sushant bhargav says:
       [86]august 13, 2015 at 12:03 am
       hi,
       would this algorithm face local minima problem like id116, i am
       new to data science, and this looks very promising but i am not
       sure, it definitely looks better than id116, as it handles
       non-convex data but if you were to compare the two algos what pros
       and cons would you notice. how to approach these algorithms, can
       you please advice.
       regards,
       [87]likelike
       [88]reply
   10.
   sumit says:
       [89]september 4, 2015 at 1:38 pm
       charles,
       while your presentations is indeed quite nice, i would appreciate
       (and so would others), if there is a follow up. how can one use the
       laplacian (lets say, 1, 3 and 5) to perform out-of-sample
       extension? as i see, 3, 5 solves a different eigenvalue problem,
       while 1 solves another one. most implementations/descriptions in
       literature are of no use as they do not work.
       [90]likelike
       [91]reply
         1.
        charles h martin, phd says:
            [92]september 6, 2015 at 6:40 am
            this is not simple in general. see
            [93]http://papers.nips.cc/paper/4560-semi-supervised-eigenvect
            ors-for-locally-biased-learning.pdf
            [94]likelike
            [95]reply
              1.
             sumit says:
                 [96]september 7, 2015 at 5:53 am
                 charles,
                 that paper is indeed quite useful. i will read it
                 carefully. the papers that i was looking at are the
                 following:
                 [97]http://jupiter.math.nctu.edu.tw/~weng/courses/2010_to
                 pic_discrete/spectrum/data_2009.pdf
                 essentially, le (as per your notation, 1, 2 or 3) has a
                 property that is described in sec 3.2.1 of the paper,
                 wherein the mapping of the new sample is a weighted
                 average of the existing mapping (eq. 9) of the paper.
                 i don   t think they are using le4 or le5.
                 now, if we introduce the notion of class labels
                 [98]http://ccc.inaoep.mx/~ariel/2013/a%20supervised%20non
                 -linear%20dimensionality%20reduction%20approach%20for%20m
                 anifold%20learning.pdf
                 then,
                 i would presume that the mapping obtained from the
                 previous paper should also hold.
                 what i am seeing is the following:
                 a.) i am able to see good class separation when i use
                 dornaika   s clas-label based le
                 b.) however, when i use the mapping generated by a.) and
                 apply eq 9 from the first paper, i cannot see any good
                 data separation. so, what could i be doing wrong? am i
                 even thinking correctly here?
                 [99]likelike
                 [100]reply
         2.
        charles h martin, phd says:
            [101]september 6, 2015 at 6:41 am
            and my earlier blog posts on semi superrvised id91
            [102]likelike
            [103]reply
   11.
   sumit says:
       [104]september 7, 2015 at 12:20 pm
       i think ref # 11 in the above paper is quite useful
       [105]likelike
       [106]reply
   12. pingback: [107]when id173 fails | machine learning
   13.
   sabra says:
       [108]march 1, 2016 at 7:19 am
       hi,
       my question is about the affinity matrix; can we use any measure of
       similarity that we can imagine to define it? is there some
       constraint about its definition?
       thank you
       [109]likelike
       [110]reply
         1.
        charles h martin, phd says:
            [111]march 2, 2016 at 10:19 am
            it   s unclear . usually these methods only work for data that
            is fairly uniform in distribution in the hilbert space the
            matrix acts in
            [112]likelike
            [113]reply
   14.
   [114]perryzhao says:
       [115]march 2, 2016 at 6:35 pm
       reblogged this on [116]braveheart.
       [117]likelike
       [118]reply
   15.
   jorge nunez says:
       [119]may 25, 2018 at 10:55 am
       what i understand from this reading, is that i don   t need to apply
       pca before id91 if i   m using spectral id91, am i right?
       for example, if i have a set of points of 10 dimensions, and i want
       to find the clusters, but i don   t know which dimensions are the
       most relevant, i don   t need to choose the dimensions, as spectral
       id91 does that as part of the process?
       if spectral id91 is what i need for this example, what
       preprocessing do i need to do (id172, determine the number
       of dimensions, determine the number of clusters)?
       should i look for another tool (which one)?
       [120]likelike
       [121]reply
         1.
        charles h martin, phd says:
            [122]june 6, 2018 at 11:22 am
            you need more than 10 points to apply id91 effectively.
            [123]likelike
            [124]reply
         2.
        charles h martin, phd says:
            [125]june 6, 2018 at 11:22 am
            you just need to form the laplacian usually normalized
            [126]likelike
            [127]reply

leave a reply [128]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [129]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [130]log out /
   [131]change )
   google photo

   you are commenting using your google account. ( [132]log out /
   [133]change )
   twitter picture

   you are commenting using your twitter account. ( [134]log out /
   [135]change )
   facebook photo

   you are commenting using your facebook account. ( [136]log out /
   [137]change )
   [138]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

     * [139]charles h martin, phd

calculation consulting

   we are a boutique machine learning data science consultancy. how can we
   help? email me at [140]info@calculationconsulting.com.

   or stop by:
   [141]http://calculationconsulting.com
   [142]youtube channel
   [143]quora

   set up a quick all on [144]clarity.fm

the community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

blog stats

     * 521,305 hits

   [145]follow on wordpress.com

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   join 694 other followers

   ____________________

   (button) follow

top posts & pages

     * [146]spectral id91: a quick overview
       [147]spectral id91: a quick overview
     * [148]kernels part 1: what is an rbf kernel? really?
       [149]kernels part 1: what is an rbf kernel? really?
     * [150]why deep learning works ii: the reid172 group
       [151]why deep learning works ii: the reid172 group
     * [152]id172 in deep learning
       [153]id172 in deep learning
     * [154]causality, correlation, and brownian motion
       [155]causality, correlation, and brownian motion

recent posts

     * [156]sf bay acm talk: heavy tailed self id173 in deep
       neural networks
     * [157]heavy tailed self id173 in deep neural nets: 1 year
       of research
     * [158]don   t peek part 2: predictions without test data
     * [159]machine learning and ai for the lean start up
     * [160]don   t peek: deep learning without looking     at test data

top clicks

     * [161]youtube.com/redirect?redi   
     * [162]arxiv.org/abs/1810.01075
     * [163]arxiv.org/abs/1706.02515
     * [164]github.com/calculatedcont   
     * [165]charlesmartin14.wordpress   
     * [166]arxiv.org/pdf/1412.0233.p   
     * [167]quora.com/machine-learnin   
     * [168]arxiv.org/pdf/1412.6621v3   
     * [169]di.ens.fr/~fbach/nips03_c   
     * [170]charlesmartin14.files.wor   

archives

     * [171]april 2019
     * [172]december 2018
     * [173]november 2018
     * [174]october 2018
     * [175]september 2018
     * [176]june 2018
     * [177]april 2018
     * [178]december 2017
     * [179]september 2017
     * [180]july 2017
     * [181]june 2017
     * [182]february 2017
     * [183]january 2017
     * [184]october 2016
     * [185]september 2016
     * [186]june 2016
     * [187]february 2016
     * [188]december 2015
     * [189]april 2015
     * [190]march 2015
     * [191]january 2015
     * [192]november 2014
     * [193]september 2014
     * [194]august 2014
     * [195]november 2013
     * [196]october 2013
     * [197]august 2013
     * [198]may 2013
     * [199]april 2013
     * [200]december 2012
     * [201]november 2012
     * [202]october 2012
     * [203]september 2012
     * [204]april 2012
     * [205]february 2012

social

     * [206]view calccon   s profile on twitter
     * [207]view charlesmartin14   s profile on linkedin
     * [208]view charlesmartin   s profile on github
     * [209]view ucaao8ghavcrtszdpobc4_kg   s profile on youtube

meta

     * [210]register
     * [211]log in
     * [212]entries rss
     * [213]comments rss
     * [214]wordpress.com

   logo-i

   [215]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [216]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [217]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [218]likes-master

   %d bloggers like this:

references

   visible links
   1. https://calculatedcontent.com/feed/
   2. https://calculatedcontent.com/comments/feed/
   3. https://calculatedcontent.com/2012/10/09/spectral-id91/feed/
   4. https://calculatedcontent.com/2012/09/28/kernels-greens-functions-and-resolvent-operators/
   5. https://calculatedcontent.com/2012/10/18/data-spectroscopy/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://calculatedcontent.com/2012/10/09/spectral-id91/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://calculatedcontent.com/2012/10/09/spectral-id91/&for=wpcom-auto-discovery
   8. https://calculatedcontent.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://calculatedcontent.com/2012/10/09/spectral-id91/#content
  11. https://calculatedcontent.com/
  12. https://calculatedcontent.com/2012/10/09/spectral-id91/
  13. https://calculatedcontent.com/author/charlesmartin14/
  14. https://calculatedcontent.com/category/uncategorized/
  15. https://calculatedcontent.com/2012/10/09/spectral-id91/#comments
  16. http://www.ee.columbia.edu/ln/dvmm/publications/12/ieee_semisupervise.pdf
  17. http://www.di.ens.fr/~fbach/nips03_cluster.pdf
  18. http://scikit-learn.org/stable/modules/id91.html
  19. http://dspace.mit.edu/bitstream/handle/1721.1/41940/mit-csail-tr-2008-052.pdf?sequence=1
  20. https://charlesmartin14.wordpress.com/2012/09/28/kernels-greens-functions-and-resolvent-operators/
  21. http://scikit-learn.org/stable/modules/id91.html#spectral-id91
  22. http://scikit-learn.org/stable/auto_examples/cluster/plot_segmentation_toy.html#example-cluster-plot-segmentation-toy-py
  23. http://scikit-learn.org/stable/auto_examples/cluster/plot_lena_segmentation.html#example-cluster-plot-lena-segmentation-py
  24. http://www.cs.cmu.edu/~chuck/lennapg/
  25. http://cs.nyu.edu/~fergus/papers/fwt_ssl.pdf
  26. http://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html
  27. https://charlesmartin14.wordpress.com/2012/10/18/spectral-properties-of-the-gaussian-kernel/
  28. https://charlesmartin14.wordpress.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
  29. http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/luxburg07_tutorial_4488[0].pdf
  30. http://lagis-vi.univ-lille1.fr/~lm/classpec/publi_classif/a_survey_of_kernel_and_spectral_methods_for_id91_pr_2008.pdf
  31. http://enpub.fulton.asu.edu/cseml/07spring/kenel_graphs.pdf
  32. http://ai.stanford.edu/~ang/papers/nips01-spectral.pdf
  33. https://calculatedcontent.com/2012/10/09/spectral-id91/?share=twitter
  34. https://calculatedcontent.com/2012/10/09/spectral-id91/?share=facebook
  35. https://calculatedcontent.com/2012/10/09/spectral-id91/?share=linkedin
  36. https://calculatedcontent.com/2012/10/09/spectral-id91/
  37. https://calculatedcontent.com/2012/10/09/spectral-id91/?share=reddit
  38. https://calculatedcontent.com/2012/10/09/spectral-id91/?share=email
  39. https://calculatedcontent.com/2012/09/28/kernels-greens-functions-and-resolvent-operators/
  40. https://calculatedcontent.com/2012/10/18/data-spectroscopy/
  41. http://sergeyfeldman.googlepages.com/
  42. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-37
  43. http://enpub.fulton.asu.edu/cseml/07spring/kenel_graphs.pdf
  44. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=37&_wpnonce=630e32563e
  45. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=37#respond
  46. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-38
  47. https://charlesmartin14.wordpress.com/2012/09/28/kernels-greens-functions-and-resolvent-operators/
  48. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=38&_wpnonce=6b9c0cd89e
  49. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=38#respond
  50. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-207
  51. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=207&_wpnonce=21ff3378b5
  52. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=207#respond
  53. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-210
  54. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=210&_wpnonce=12838a3e65
  55. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=210#respond
  56. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-422
  57. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=422&_wpnonce=19213a5fe9
  58. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=422#respond
  59. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-423
  60. http://videolectures.net/icml08_belkin_dslmm/
  61. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=423&_wpnonce=45240ba520
  62. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=423#respond
  63. http://jaypri.ch/
  64. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-456
  65. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=456&_wpnonce=9906eca29f
  66. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=456#respond
  67. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-457
  68. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=457&_wpnonce=b82e9650ec
  69. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=457#respond
  70. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-684
  71. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=684&_wpnonce=ba3fd44be9
  72. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=684#respond
  73. https://charlesmartin14.wordpress.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
  74. http://tomdiethe.wordpress.com/
  75. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-954
  76. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=954&_wpnonce=c830254a99
  77. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=954#respond
  78. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-957
  79. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=957&_wpnonce=d304232e1f
  80. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=957#respond
  81. https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
  82. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-1041
  83. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=1041&_wpnonce=8bb1edd4f6
  84. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=1041#respond
  85. https://plus.google.com/+sushantbhargav
  86. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-1042
  87. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=1042&_wpnonce=a831f49dcf
  88. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=1042#respond
  89. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-1047
  90. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=1047&_wpnonce=69e422957b
  91. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=1047#respond
  92. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-1048
  93. http://papers.nips.cc/paper/4560-semi-supervised-eigenvectors-for-locally-biased-learning.pdf
  94. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=1048&_wpnonce=4a2c08bf51
  95. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=1048#respond
  96. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-1051
  97. http://jupiter.math.nctu.edu.tw/~weng/courses/2010_topic_discrete/spectrum/data_2009.pdf
  98. http://ccc.inaoep.mx/~ariel/2013/a supervised non-linear id84 approach for manifold learning.pdf
  99. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=1051&_wpnonce=42aebaf933
 100. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=1051#respond
 101. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-1049
 102. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=1049&_wpnonce=9fd925e627
 103. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=1049#respond
 104. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-1052
 105. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=1052&_wpnonce=351bda6faf
 106. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=1052#respond
 107. https://charlesmartin14.wordpress.com/2015/12/28/when-id173-fails/
 108. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-1227
 109. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=1227&_wpnonce=0bad5f79a5
 110. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=1227#respond
 111. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-1228
 112. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=1228&_wpnonce=9d9be3a9af
 113. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=1228#respond
 114. https://perryzhao.wordpress.com/
 115. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-1229
 116. https://perryzhao.wordpress.com/2016/03/03/spectral-id91-a-quick-overview/
 117. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=1229&_wpnonce=17e43d14e8
 118. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=1229#respond
 119. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-2902
 120. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=2902&_wpnonce=d3a7cecb3f
 121. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=2902#respond
 122. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-2931
 123. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=2931&_wpnonce=df7e29baf8
 124. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=2931#respond
 125. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-2932
 126. https://calculatedcontent.com/2012/10/09/spectral-id91/?like_comment=2932&_wpnonce=7a0c0c5042
 127. https://calculatedcontent.com/2012/10/09/spectral-id91/?replytocom=2932#respond
 128. https://calculatedcontent.com/2012/10/09/spectral-id91/#respond
 129. https://gravatar.com/site/signup/
 130. javascript:highlandercomments.doexternallogout( 'wordpress' );
 131. https://calculatedcontent.com/2012/10/09/spectral-id91/
 132. javascript:highlandercomments.doexternallogout( 'googleplus' );
 133. https://calculatedcontent.com/2012/10/09/spectral-id91/
 134. javascript:highlandercomments.doexternallogout( 'twitter' );
 135. https://calculatedcontent.com/2012/10/09/spectral-id91/
 136. javascript:highlandercomments.doexternallogout( 'facebook' );
 137. https://calculatedcontent.com/2012/10/09/spectral-id91/
 138. javascript:highlandercomments.cancelexternalwindow();
 139. https://calculatedcontent.com/author/charlesmartin14/
 140. mailto:info@calculationconsulting.com
 141. http://calculationconsulting.com/
 142. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg
 143. http://www.quora.com/charles-h-martin
 144. https://clarity.fm/charlesmartin14
 145. https://calculatedcontent.com/
 146. https://calculatedcontent.com/2012/10/09/spectral-id91/
 147. https://calculatedcontent.com/2012/10/09/spectral-id91/
 148. https://calculatedcontent.com/2012/02/06/kernels_part_1/
 149. https://calculatedcontent.com/2012/02/06/kernels_part_1/
 150. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 151. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 152. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 153. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 154. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 155. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 156. https://calculatedcontent.com/2019/04/01/sf-bay-acm-talk-heavy-tailed-self-id173-in-deep-neural-networks/
 157. https://calculatedcontent.com/2018/12/17/heavy-tailed-self-id173-in-deep-neural-nets-1-year-of-research/
 158. https://calculatedcontent.com/2018/11/18/dont-peek-part-2-predictions-without-test-data/
 159. https://calculatedcontent.com/2018/11/16/machine-learning-and-ai-for-the-lean-start-up/
 160. https://calculatedcontent.com/2018/10/07/dont-peek-deep-learning-without-looking-at-test-data/
 161. https://www.youtube.com/redirect?redir_token=ezgiasszjkmz1fnzp0yjtazidd98mtu1ndizmjiznkaxntu0mtq1odm2&q=https://arxiv.org/abs/1810.01075&event=video_description&v=ilv5sc8wjpy
 162. https://arxiv.org/abs/1810.01075
 163. https://arxiv.org/abs/1706.02515
 164. https://github.com/calculatedcontent/tid166
 165. https://charlesmartin14.wordpress.com/2013/11/14/metric-learning-some-quantum-statistical-mechanics/
 166. http://arxiv.org/pdf/1412.0233.pdf
 167. http://www.quora.com/machine-learning/how-does-one-decide-on-which-kernel-to-choose-for-an-id166-rbf-vs-linear-vs-poly-kernel
 168. http://arxiv.org/pdf/1412.6621v3.pdf
 169. http://www.di.ens.fr/~fbach/nips03_cluster.pdf
 170. https://charlesmartin14.files.wordpress.com/2012/10/mat1.png
 171. https://calculatedcontent.com/2019/04/
 172. https://calculatedcontent.com/2018/12/
 173. https://calculatedcontent.com/2018/11/
 174. https://calculatedcontent.com/2018/10/
 175. https://calculatedcontent.com/2018/09/
 176. https://calculatedcontent.com/2018/06/
 177. https://calculatedcontent.com/2018/04/
 178. https://calculatedcontent.com/2017/12/
 179. https://calculatedcontent.com/2017/09/
 180. https://calculatedcontent.com/2017/07/
 181. https://calculatedcontent.com/2017/06/
 182. https://calculatedcontent.com/2017/02/
 183. https://calculatedcontent.com/2017/01/
 184. https://calculatedcontent.com/2016/10/
 185. https://calculatedcontent.com/2016/09/
 186. https://calculatedcontent.com/2016/06/
 187. https://calculatedcontent.com/2016/02/
 188. https://calculatedcontent.com/2015/12/
 189. https://calculatedcontent.com/2015/04/
 190. https://calculatedcontent.com/2015/03/
 191. https://calculatedcontent.com/2015/01/
 192. https://calculatedcontent.com/2014/11/
 193. https://calculatedcontent.com/2014/09/
 194. https://calculatedcontent.com/2014/08/
 195. https://calculatedcontent.com/2013/11/
 196. https://calculatedcontent.com/2013/10/
 197. https://calculatedcontent.com/2013/08/
 198. https://calculatedcontent.com/2013/05/
 199. https://calculatedcontent.com/2013/04/
 200. https://calculatedcontent.com/2012/12/
 201. https://calculatedcontent.com/2012/11/
 202. https://calculatedcontent.com/2012/10/
 203. https://calculatedcontent.com/2012/09/
 204. https://calculatedcontent.com/2012/04/
 205. https://calculatedcontent.com/2012/02/
 206. https://twitter.com/calccon/
 207. https://www.linkedin.com/in/charlesmartin14/
 208. https://github.com/charlesmartin/
 209. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg/
 210. https://wordpress.com/start?ref=wplogin
 211. https://charlesmartin14.wordpress.com/wp-login.php
 212. https://calculatedcontent.com/feed/
 213. https://calculatedcontent.com/comments/feed/
 214. https://wordpress.com/
 215. https://wordpress.com/?ref=footer_blog
 216. https://calculatedcontent.com/2012/10/09/spectral-id91/
 217. https://calculatedcontent.com/2012/10/09/spectral-id91/#cancel
 218. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 220. https://charlesmartin14.files.wordpress.com/2012/10/spec.png
 221. https://charlesmartin14.files.wordpress.com/2012/10/l1.png
 222. https://charlesmartin14.files.wordpress.com/2012/10/gap.png
 223. https://charlesmartin14.files.wordpress.com/2012/10/mat1.png
 224. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-form-guest
 225. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-form-load-service:wordpress.com
 226. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-form-load-service:twitter
 227. https://calculatedcontent.com/2012/10/09/spectral-id91/#comment-form-load-service:facebook
 228. http://nanonaren.wordpress.com/
 229. https://calculatedcontent.com/2012/10/09/spectral-id91/
 230. http://tablewarebox.com/
 231. http://duttatridib.wordpress.com/
 232. https://calculatedcontent.com/2012/10/09/spectral-id91/
 233. http://twitter.com/alxfed
 234. http://ashutoshtripathi.com/
 235. https://calculatedcontent.com/2012/10/09/spectral-id91/
 236. http://randomstratum.wordpress.com/
 237. https://calculatedcontent.com/2012/10/09/spectral-id91/
 238. https://calculatedcontent.com/logo-i-3/
