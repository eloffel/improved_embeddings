   #[1]   feed [2]   comments feed [3]   machine learning with missing labels
   part 3: experiments comments feed [4]machine learning with missing
   labels part 2: the univerid166 [5]id166+ / lupi: learning using privileged
   information [6]alternate [7]alternate [8]search [9]wordpress.com

   [10]skip to content

   [11][wp-logo.jpg]

machine learning with missing labels part 3: experiments

   [12]november 1, 2014 [13]charles h martin, phd [14]uncategorized
   [15]leave a comment

   in this series of posts we look at transductive and
   semisupervised learning   an old problem, a hard problem, and a
   fundamental problem machine learning.  unlike deep learning or large
   scale ml,

   we want to learn as much as we can from as labeled little data as
   possible.

   we focus on text classification.  why?
    1. the feature space is simple     its just words.
    2. with the true labels, a simple id166 gives near perfect accuracy.
    3. text satisfies the cluster assumption.

   why isn   t this done all the time?

   we can generate good (not great) semi supervised text classifiers.
   but
    1. creating the input data is hard.
    2. setting the input parameters is harder.
    3. we can   t always distinguish the good solutions from the bad.

   [16]the 1st post looked at (1); here we try to understand (2&3), and
   look for simple, practical heuristics to make these old methods useful.

   we can think of the tid166/s3vm as generating several potentially good
   labelings on the unlabeled data. to solve (3), we need to find the best
   labeling   which means judging the label and/or classifier quality, such
   as the label id178 , cluster overlap, etc.

   or, alternatively, we might try to average the good solutions, similar
   to the way [17]hinton et. al. have recently suggested averaging an
   ensemble of models to take advantage of dark knowledge.

   thanks to matt wescott for pointing out this pdf and discussions on it.

   to explore this, we run experiments using some old, existing, open
   source transductive id166 (tid166) and semisupervised id166 (s3vm) codes:

   our work is guided by  the results in [18]this 2014 paper on the
   qn-s3vm method, as applied to text data.

   we thank fabian gieseke for help in reproducing and understanding these
   results.

   for the tid166, some available codes and algorithms are
     * [19]id166light:  graph cutting algo
     * [20]id166lin:  multiswitch (ms-tid166) and deterministic annealing
       (da-tid166)
     * [21]univerid166: convex concave continuation procedure (cccp)
     * [22]qn-s3vm: quasi newton semisupervised id166

   we can also compare the results to a id166 baseline, using either
     * [23]liblinear:  various id166 and lr algos, or
     * [24]scikit-learn svc (python wrapper to liblinear)

   all these codes are unix standalone c programs except qn-s3vm
   and scikit-learn, which are python.

   we examine id166lin and its multiswitch (ms-tid166) method.

   id166lin is the tid166 benchmark for most studies.  after we work this up,
   we will move on to other codes listed. i also would like to test other
   tid166/s3vm methods, such as [25]s3vm-mip, [26]s4vm, [27]s3vmpath, and
   many others, but may are matlab codes and  i don   t have c or python
   open source code readily available.  or really the time     

   we do not consider cases of [28]id61, where the
   labels may be both missing and/or have errors    but let me point out
   [29]a 2013 paper on the wellid166 method.  it also compares the
   above methods and is worth a read.

   i hope this post gives some flavor to what is a very complicated and
   fundamental problem in machine learning.  (and maybe take some ideas
   from deep learning to help solve the tid166 problem as well)

methods: the real-sim data sets:

   we care about text classification. to this end, we reproduce the
   performance of id166lin, and, later,  other methods, on [30]the real-sim
   text dataset (listed in table 2 in the paper).  the real-sim dataset
   was used in the original id166lin paper; it is an accepted baseline for
   these methods.

   this data set consists of  72309 labeled documents: 22238 (+1) and
   50071 (-1).

   the fraction of positively labeled documents r^{+}_{true}=30.75% .

creating the labeled, unlabeled, and holdout sets:

   i have created an ipython notebook ([31]make_realsim_inputs.ipynb)
   which can read the real-sim data and generate the input files for the
   various c programs listed above: id166light, id166lin, & univerid166.

   [32]we can see it in the nbviewer here

   the real-sim dataset is split in half, and 3 data sets are created.
   half is used to train tid166 (labeled l and unlabeled u); the rest is a
   holdout set (ho) for testing.

   following table 2, the notebook generates:
     * labeled (l) of sizes roughly: l=90, 180, 361, 1486, & 2892
     * an unlabeled set (u) of size u=36154-l.
     * a test or holdout set ho (of the remaining data)

   each data set (l,u,ho) also has ~0.31 fraction of labeled examples; u
   and ho are statistical replica   s of l, albeit larger.

   the input files consist of a training file (or 2 files for id166lin) and
   3 test files (6 forid166lin): labeled l, unlabeled u, and holdout ho.
   they are in the id166light format.

   i.e. the id166lin input files, for l=90,  are:

   filename                                          size (lines)
                             purpose

   id166lin.train.90.examples            36154                 labeled +
   unlabeled  train tid166

   id166lin.train.90.labels                   36154

   id166lin.testl.90.examples                 90
   train labeled baseline

   id166lin.testl.90.labels                        90

   id166lin.testu.90.examples            36154                evaluate
   unlabeled set

   id166lin.testu.90.labels                   36154

   id166lin.testho.90.examples         36155                evaluate holdout
   set

   id166lin.testho.90.labels                36155


grid searches with gnu parallel

   astoundingly, rather than performing a full grid search, many research
   papers fix the id173 parameters, guess them using some crude
   bayesian scheme, or [33]attempt to transfer them from some other data
   set. this makes it really hard to evaluate these methods.

   we use a ruby script and the [34]gnu_parallel program to run the
   command line unix programs and grid search the parameters. the script
   also computes the final holdout accuracy and metrics such as the margin
   and label id178.

   [35]id166lin.rb

   [36]gnu parallel lets us easily grid search the id173
   parameters by running the tid166 codes in parallel.

   we do a broad grid search

   w\in[.0001,0.001,0.01,0.1,1,10,100,1000,10000]

   u\in[.0001,0.001,0.01,0.1,1,10,100,1000,10000]

   r\in[0.25, 0.26, 0.27,\cdots,0.35,0.36,0.37]

   by creating a directory for each run


   parallel "mkdir a2w{1}u{2}r{r}" ::: .0001 0.001 0.01 0.1 1 ::: 1 10 100
   1000 1000 ::: 0.28 0.29 0.31 0.32 0.33 0.34

   and then running id166lin in each directory, simultaneously


   parallel "cd a2w{1}u{2}r{r}; $id166lin -a 2 -w {1} -u {2} -r {3}
   ../id166lin.train.examples.90 ../id166lin.train.labels.90 > /dev/null"  :::
   0.0001 0.001 0.01 0.1 1  :::  1 10 100 1000 10000 ::: 0.28 0.29 0.31
   0.32 0.33 0.34

   we can then evaluate each run on the u or ho data set.  we will find
   the optimal w and u are in the range of [37]the original crude
   estimates w=0.001, u=1, and r=0.31

   how can we evaluate the accuracy of our tid166, here, and in a real world
   setting?

ground truth

   since we know the labels on both the u and the ho sets , these are a
   ground truth from which we can evaluate how well the best model(s)
   perform.  we can get an upper bound on the best possible
   generalization accuracy (on the ho set) by training an id166 on l+u using
   the true labels, and then applying this model to ho.

   the best expected ho accuracy here is ~ 97 %

   also, note this is different from the reconstruction accuracy on ho,
   which is > 99 %.

    we might also obtain a better generalization accuracy with different
   features, such as applying [38]glove or even unsupervised pre-training
   on l+u.  we examine this in a future blog.   we are, in particular,
   interesting in comparing the deep learning auto-encoders
   with [39]convex nmf, including [40]recent variants applied to document
   id91.

baseline

   we need a baseline for comparison.  the ipython notebook computes a
   baseline accuracy for the labeled data set (l); this can also be done
   with liblinear or even id166lin (using -a 1).

   we then run the small l model on the holdout (ho) set for each
   l=90,180,   ,  yielding baseline accuracies of
   split size size of l (l) ho accuracy
   0.0025     90            85 %
   0.005      180           87.5 %
   0.001      361           90 %
   0.004      1446          93 %
   0.008      2892          94 %

   please note that these are computed from 1 random sample, and may be
   slightly different (by ~1%) for each run. also, that the command line
   and scikit-learn liblinear have different defaults; we use
   c=10,fit_intercept=false.

finding the best accuracy:

   we are generating a large number of nearly equivalent models,
   parameterized by

   [\gamma_{l}(u,w,r^{+})]

   the inputs are related to the objective function in [41]blog post 1.

   w=\gamma , u=\gamma' ,  and r=0.31

   every useful model will have the same reconstruction accuracy on the
   labeled examples l, and every model proposes a different soft labeling
   for the unlabeled examples u.

   how can we compare different models \gamma_{l}(u,w,r^{+}) ?

   essentially, we use a margin method to guess good labelings of the
   data, but we need an unsupervised heuristic to select the optimal
   labeling.   can we simply apply cross-validation to the small labeled
   set l?  or leave one out cv (loo-cv) ?  what filters can we apply to
   speed up the calculations ?

best accuracy on the labeled set

   id166lin may produce inferior or even nonsense models.  but more subtly,
    some models may even have a very high training  (on u), and a very
   high test accuracy (on ho), but a very low accuracy on l.  these are
   not useful  because  in the real world, we don   t know the labels on u
   or ho.

   we always want to first filter the possible  solutions by selecting
   those with the best accuracy on the l.

filter by final objective function  (fof)

   since we are minimizing the objective function, we only consider
   solutions with fof < 0.01.  note this is very different than simply
   assuming the best solutions has absolute minimum fof across all input
   parameters.

    we discard all solutions with final objective function (fof) > 0.01

   for l=180, for example, this reduces the set of possible solutions from
   1430 to 143.

maximum margin solution   not

   wait, can   t we just take the solution with the maximum margin (or
   1/norm)

   m=\dfrac{1}{\sum_{i}w_{i}^{2}}

   no. think about how we practice supervised learning; we train a model,
   and set the id173 parameters by cross-validation.  in the tid166
   and ssl methods, we can can also apply [42]cv (or loo-cv), but only to
   the labeled set l.

   the maximum margin solution is only best for a given set of input /
   id173 parameters (u,w,r). every model \gamma_{l}(u,w,r^{+})
   induces a different labeling on u  (that is, they form an equivalence
   class on l, not l+u).    in fact, the best overall solution has
   the minimum margin amongst a broad scan of (u,w).

   [43]screen shot 2014-10-31 at 8.31.54 pm

   where the output weights w_{i} are taken from

   id166lin.training.examples.90.weights

optimal parameters for the holdout test accuracy

   (this section is still under construction)

cross validation and leave one out on the labeled data

   as with a standard id166, one can try to apply cross validation (cv) to
   the labeled set l to select the best tid166 model.  most academic studies
   run 5-fold cv on the l set.  this is harder, however, because
    1. when l is small, the i.i.d. assumption fails, so cv my give
       spurious results
    2. we really should use leave one out cross validation (loo-cv), but
       this increases the run time significantly
    3. the id166lin code does not include cv or loo-cv
    4. the id166lin da-tid166 is unstable and i had to switch to ms-tid166 to
       complete this
    5. my laptop cant take any more over clocking so i had to pause this
       for bit

   i will finish this section once i either
    1. modify id166lin to run cv / loo-cv
    2. modify and use univerid166 and the cccp method, [44]which is an order
       of magnitude faster than tid166
    3. and/or get this all working in ipython+star cluster so i can run
       loo-cv at scale

minimum id178 of the soft labels

   the [45]id166lin da algo minimizes the id178 on unlabeled examples
   s_{u} ([46]it is the key stopping criteria, sec 3.2.3).  perhaps the
   model with minimum  s_{u} generalizes best?

   this is the essence of[47] id178 id173--a common component
   of many semi supervised learning formulations.

   we can evaluate the proposed soft labels \mu_{u} of the unlabeled set
   u, found in

   id166lin.testu.examples.90.outputs

   we convert soft labels to a id203  p_{u} :

   p_{u}=\dfrac{{\dfrac{{1}}{1+exp(-\mu_{u})}}}{\sum\dfrac{{1}}{1+exp(-\mu
   _{u})}}

   and compute the id178 on u

   s_{u}=-\sum_{u}(p_{u}ln(p_{u})+(1-p_{u})ln(1-p_{u}))

   we hope that tid166 solutions with minimum s_{u} will consistently yield
   a very good generalization accuracy (on the ho set) across a grid of
   search (u,w,r)   and preliminary results confirm this.

   let   s look at the l=90 case, and plot first the ho accuracy vs the
   label id178 s_u .

   screen shot 2014-10-31 at 8.38.15 pm

   (recall we only take solutions with fof > 0.01 and the best training
   accuracy on the true labeled set.)

   we see at 3-4 distinct sets of solutions with nearly equivalent id178
   across a wide range of ho accuracy, and 2 classes include improved
   solutions (ho accuracy > baseline 85%)

   we call these equivalence classes under the label id178.

   other solutions also show that the minimum id178 solution is the best
   solution   for a fixed r^{+}_{in} :

   [48]screen shot 2014-10-31 at 8.52.20 pm

   [49]screen shot 2014-10-31 at 8.54.24 pm

   we select the minimum id178 class of solutions.

   notice that the l=90 case is greatly improved above the 85% baseline,
   whereas l=1446 shows only a slight (0.5%) improvement   and 2-3% less
   than the best expected accuracy of 97%. we see the same results in the
   qn-s3vm and wellid166 papers, and suggests that

   transductive and ssl learning may be limited to small labelled sets

   where the percentage of labeled data is  l\sim 10-15%  of u.

predicted fraction of positive examples

   recall we have to input an estimate r^{+}_{in} of the fraction of
   positive labels on u r^{+}_{u} .

   we assume we estimate r^{+}_{u} to within 10% using about 100 examples
   ( l\sim o(100) )

   we would prefer a high quality estimate if possible because the final,
   predicted \hat{r}^{+}_{u}  and \hat{r}^{+}_{ho}  appears to depend
   strongly on  r^{+}_{in} .

   here we test varying the input r^{+}_{in} .  we expect the predicted
   \hat{r}^{+}_{ho} to be correlated with the generalization accuracies,
   and preliminary results also confirm this (note that all plots show the
   predicted fraction \hat{r}^{+}_{ho} ):

   screen shot 2014-10-31 at 8.38.48 pm

   the predicted fraction  \hat{r}^{+}_{ho}  also forms equivalence
   classes \gamma_{l,s}(r^{+}) under the id178 s_{u} .

   [50]screen shot 2014-10-31 at 10.55.26 pm

   of course, we know the true  r^{+}_{ho}=0.31 , so the minimum id178
   solution is not the absolute best solution.

   perhaps [51]a more generalized form of the id178 would be more
   discerning?   we would prefer [52]a python toolbox like this one and
   need a way to fit the parameters.

   we might also try to improve the tid166 search algorithm, as in the
   [53]s4vm: safe semi-supervised id166 methods.  the s4vm tries to improve
   upon the id166lin da-tsm algo using [54]simulated annealing, and then
   selects the best solutions by evaluating the final cluster quality.
   this looks promising for simple applications.  indeed, one might even
   try to implement it in python using the recent [55]basinhopping
   library.

transduction and the vc theory

   we have said that the vc theory is a really theory about transductive
   learning.

   we see now that to apply the tid166 in practice, the unlabeled data set u
   really needs to be an accurate statistical replica, as in the [56]vc
   statistical learning theory for transduction.

   we have tested cases where we have a good replica, but we did not
   estimate r^{+}_{u}  well; we have not yet tested cases where the u  is
   not a good replica and we can only estimate r^{+}_{u} partially.  this
   needs to be done.

   in the [57]literature this is called using class proportions, and[58]
   some recent estimatation methods have been proposed (and
   [59]here)   although the qn-s3vm papers do not attempt this.  recently
   [60]belkin has shown how to estimate this fraction using methods of
   [61]traditional integral methods.

   the tid166 accuracy may depends on how well one can estimate the fraction
   (r) of positively labeled examples.

   also, we presume that tid166 models overtrain, and are biased towards
   either the (+) or (-) class.

   [62]screen shot 2014-10-31 at 11.32.54 pm

   for l=1446, we find a single class of minimum id178 id178 solutions

   [63]screen shot 2014-10-31 at 11.31.29 pm

   with  s\approxeq-5.467 , w=0.001, u=1 and
   \hat{r}^{+}_{ho}\in[0.25,0.37] .

   they all have a high ho accuracy   but may be biased towards choosing (+)
   labels because the minimum s solution has \hat{r}^{+}_{ho}=0.36 , not
   0.31.

   if we can   t estimate the  r^{+}_{u}  well, may be able to average of
   the minimum id178 solutions, thereby canceling out the biases in the
   over-trained models (i.e. average all \hat{r}^{+}_{u}\in[0.28\cdots
   0.34] tid166 models)

    this is what [64]dark knowledge, is all about   creating a simpler model
   by ensemble averaging    and we hope these recent advances in deep
   learning can help with transductive learning as well.

   it is noted that [65]the recent wellid166 method also addresses this
   problem.  wellid166 uses a convex relaxation of the tid166 problem to
   obtain a min max optimization over the set of possible models allowed
   within a range of  r^{+}_{u}   ; this is quite clever!

   in a future blog, we will examine how the current tid166s perform when
   tuning their parameters to obtain the best predicted fraction. if this
   is sound, in a future post, we will examine how to estimate the class
   proportions for text data.

discussion

   to come soon..stay tuned

appendix

   each is tid166 is trained with a linear kernel.      the id173
   parameters are adjusted to give both optimal performance on the initial
   training set (l), and best reconstruction accuracy on the (u) set.

   each program requires slightly different command line options for
   these.  there are also tuning parameters for each solver, and is
   necessary to set the tolerances low enough so the solvers can converge.
    for example, even for a linear id166, the primal solvers may not
   converge as rapidly as the dual solver, and can even give different
   results on the same data set [66](despite von neumann   s minimax
   theorem). and the default for sckit-learn linearsvc has the bias on,
   whereas liblinear has the bias off.  so be careful.

   below we show examples of training each the tid166s on the labeled set
   (l), and evaluating the accuracy on a holdout set (ho), for l=90.  we
   show typical values of the id173 parameters, but these do need
   to be tuned separately.  we also show how to run the popular liblinear
   program as a baseline (which uses the id166light format); the paper uses
   libid166.

baseline linear id166:

   liblinear  -c 0.1  -v 5  id166light.testl.90

ms-id166lin:

   id166lin  -a 2  -w 0.001 -u 1 -r 0.31
   id166lin.train.examples.90 id166light.train.labels.90

   id166lin  -f   id166lin.train.examples.90.weights
   id166light.testho.examples.90 id166light.testho.labels.90

id166lin da:

   id166lin  -a 3  -w 0.001 -u 1 -r 0.31
   id166lin.train.examples.90 id166light.train.labels.90

   id166lin  -f   id166lin.train.examples.90.weights id166lin.testho.examples.90
   id166lin.testho.labels.90

id166light graph cuts:

   id166_learn -p 0.31 id166light.train.90  id166light.model.90

   id166_classify  -c 100  id166light.testho.90  id166light.model.90
   id166light.outho.90

univerid166 cccp tid166 w/ramp loss:

   univerid166 -c 1 -c 1 -s -0.5 -z 1 -o 1 -t
   univerid166.testho.90 univerid166.train.90

share this:

     * [67]twitter
     * [68]facebook
     * [69]linkedin
     * [70]more
     *

     * [71]reddit
     * [72]email
     *
     *

like this:

   like loading...

related

post navigation

   [73]previous post: machine learning with missing labels part 2: the
   univerid166
   [74]next post: id166+ / lupi: learning using privileged information

leave a reply [75]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [76]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [77]log out /
   [78]change )
   google photo

   you are commenting using your google account. ( [79]log out /
   [80]change )
   twitter picture

   you are commenting using your twitter account. ( [81]log out /
   [82]change )
   facebook photo

   you are commenting using your facebook account. ( [83]log out /
   [84]change )
   [85]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

     * [86]charles h martin, phd

calculation consulting

   we are a boutique machine learning data science consultancy. how can we
   help? email me at [87]info@calculationconsulting.com.

   or stop by:
   [88]http://calculationconsulting.com
   [89]youtube channel
   [90]quora

   set up a quick all on [91]clarity.fm

the community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

blog stats

     * 521,305 hits

   [92]follow on wordpress.com

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   join 694 other followers

   ____________________

   (button) follow

top posts & pages

     * [93]spectral id91: a quick overview
       [94]spectral id91: a quick overview
     * [95]kernels part 1: what is an rbf kernel? really?
       [96]kernels part 1: what is an rbf kernel? really?
     * [97]why deep learning works ii: the reid172 group
       [98]why deep learning works ii: the reid172 group
     * [99]id172 in deep learning
       [100]id172 in deep learning
     * [101]causality, correlation, and brownian motion
       [102]causality, correlation, and brownian motion

recent posts

     * [103]sf bay acm talk: heavy tailed self id173 in deep
       neural networks
     * [104]heavy tailed self id173 in deep neural nets: 1 year
       of research
     * [105]don   t peek part 2: predictions without test data
     * [106]machine learning and ai for the lean start up
     * [107]don   t peek: deep learning without looking     at test data

top clicks

     * [108]youtube.com/redirect?redi   
     * [109]arxiv.org/abs/1810.01075
     * [110]arxiv.org/abs/1706.02515
     * [111]github.com/calculatedcont   
     * [112]charlesmartin14.wordpress   
     * [113]arxiv.org/pdf/1412.0233.p   
     * [114]quora.com/machine-learnin   
     * [115]arxiv.org/pdf/1412.6621v3   
     * [116]di.ens.fr/~fbach/nips03_c   
     * [117]charlesmartin14.files.wor   

archives

     * [118]april 2019
     * [119]december 2018
     * [120]november 2018
     * [121]october 2018
     * [122]september 2018
     * [123]june 2018
     * [124]april 2018
     * [125]december 2017
     * [126]september 2017
     * [127]july 2017
     * [128]june 2017
     * [129]february 2017
     * [130]january 2017
     * [131]october 2016
     * [132]september 2016
     * [133]june 2016
     * [134]february 2016
     * [135]december 2015
     * [136]april 2015
     * [137]march 2015
     * [138]january 2015
     * [139]november 2014
     * [140]september 2014
     * [141]august 2014
     * [142]november 2013
     * [143]october 2013
     * [144]august 2013
     * [145]may 2013
     * [146]april 2013
     * [147]december 2012
     * [148]november 2012
     * [149]october 2012
     * [150]september 2012
     * [151]april 2012
     * [152]february 2012

social

     * [153]view calccon   s profile on twitter
     * [154]view charlesmartin14   s profile on linkedin
     * [155]view charlesmartin   s profile on github
     * [156]view ucaao8ghavcrtszdpobc4_kg   s profile on youtube

meta

     * [157]register
     * [158]log in
     * [159]entries rss
     * [160]comments rss
     * [161]wordpress.com

   logo-i

   [162]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [163]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [164]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [165]likes-master

   %d bloggers like this:

references

   visible links
   1. https://calculatedcontent.com/feed/
   2. https://calculatedcontent.com/comments/feed/
   3. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/feed/
   4. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
   5. https://calculatedcontent.com/2014/11/05/learning-using-privileged-information-weighted-id166s/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/&for=wpcom-auto-discovery
   8. https://calculatedcontent.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/#content
  11. https://calculatedcontent.com/
  12. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
  13. https://calculatedcontent.com/author/charlesmartin14/
  14. https://calculatedcontent.com/category/uncategorized/
  15. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/#respond
  16. https://charlesmartin14.wordpress.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
  17. http://www.ttic.edu/dl/dark14.pdf
  18. http://www.fabiangieseke.de/pdfs/neucom2013_draft.pdf
  19. http://id166light.joachims.org/
  20. http://vikas.sindhwani.org/id166lin.html
  21. http://mloss.org/software/view/19/
  22. http://www.ci.uni-oldenburg.de/60506.html
  23. http://www.csie.ntu.edu.tw/~cjlin/liblinear/
  24. http://scikit-learn.org/stable/modules/id166.html
  25. http://web.sakarya.edu.tr/~ademiriz/sakaryawebsite/papers/s3vm.pdf
  26. http://xxx.tau.ac.il/abs/1005.1545v1
  27. http://jmlr.org/proceedings/papers/v28/ogawa13a.pdf
  28. http://stackoverflow.com/questions/18944805/what-is-weakly-supervised-learning-id64
  29. http://jmlr.csail.mit.edu/papers/volume14/li13a/li13a.pdf
  30. http://www.csie.ntu.edu.tw/~cjlin/libid166tools/datasets/binary.html
  31. https://github.com/calculatedcontent/tid166/blob/master/qn-s3vm-2014-paper/make_realsim_inputs.ipynb
  32. http://nbviewer.ipython.org/github/calculatedcontent/tid166/blob/master/qn-s3vm-2014-paper/make_realsim_inputs.ipynb
  33. http://www.cs.cmu.edu/~dyogatam/papers/yogatama+mann.aistats2014.pdf
  34. http://www.gnu.org/software/parallel/
  35. https://github.com/calculatedcontent/tid166/blob/master/qn-s3vm-2014-paper/id166lin.rb
  36. http://www.gnu.org/software/parallel/
  37. http://vikas.sindhwani.org/id166lin.html
  38. http://web.stanford.edu/~jpennin/papers/glove.pdf
  39. https://charlesmartin14.wordpress.com/2013/05/06/advances-in-convex-nmf-part-1-linear-programming/
  40. http://jmlr.csail.mit.edu/papers/volume15/mizutani14a/mizutani14a.pdf
  41. https://charlesmartin14.wordpress.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
  42. http://scikit-learn.org/stable/modules/cross_validation.html
  43. https://charlesmartin14.files.wordpress.com/2014/11/screen-shot-2014-10-31-at-8-31-54-pm.png
  44. http://www.esat.kuleuven.be/sista/roks2013/files/presentations/talk.pdf
  45. https://charlesmartin14.wordpress.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
  46. http://vikas.sindhwani.org/sk_sigir06.pdf
  47. http://papers.nips.cc/paper/2740-semi-supervised-learning-by-id178-minimization.pdf
  48. https://charlesmartin14.files.wordpress.com/2014/11/screen-shot-2014-10-31-at-8-52-20-pm.png
  49. https://charlesmartin14.files.wordpress.com/2014/11/screen-shot-2014-10-31-at-8-54-24-pm.png
  50. https://charlesmartin14.files.wordpress.com/2014/11/screen-shot-2014-10-31-at-10-55-26-pm.png
  51. http://jmlr.csail.mit.edu/papers/volume15/szabo14a/szabo14a.pdf
  52. http://www.isi.edu/~gregv/npeet_doc.pdf
  53. http://www.icml-2011.org/papers/548_icmlpaper.pdf
  54. https://charlesmartin14.wordpress.com/2013/11/14/metric-learning-some-quantum-statistical-mechanics/
  55. http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.basinhopping.html
  56. https://charlesmartin14.wordpress.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
  57. http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf
  58. http://dase.ecnu.edu.cn/huhaoji/publications/distribution.pdf
  59. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/semi-supervised id166s for classification with unknown class proportions and a small labeled dataset
  60. http://arxiv.org/pdf/1304.5575v2.pdf
  61. https://charlesmartin14.wordpress.com/2012/09/28/kernels-greens-functions-and-resolvent-operators/
  62. https://charlesmartin14.files.wordpress.com/2014/11/screen-shot-2014-10-31-at-11-32-54-pm.png
  63. https://charlesmartin14.files.wordpress.com/2014/11/screen-shot-2014-10-31-at-11-31-29-pm.png
  64. http://www.ttic.edu/dl/dark14.pdf
  65. http://jmlr.csail.mit.edu/papers/volume14/li13a/li13a.pdf
  66. http://www.math.uchicago.edu/~may/vigre/vigre2008/reupapers/scarvalone.pdf
  67. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/?share=twitter
  68. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/?share=facebook
  69. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/?share=linkedin
  70. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
  71. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/?share=reddit
  72. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/?share=email
  73. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
  74. https://calculatedcontent.com/2014/11/05/learning-using-privileged-information-weighted-id166s/
  75. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/#respond
  76. https://gravatar.com/site/signup/
  77. javascript:highlandercomments.doexternallogout( 'wordpress' );
  78. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
  79. javascript:highlandercomments.doexternallogout( 'googleplus' );
  80. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
  81. javascript:highlandercomments.doexternallogout( 'twitter' );
  82. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
  83. javascript:highlandercomments.doexternallogout( 'facebook' );
  84. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
  85. javascript:highlandercomments.cancelexternalwindow();
  86. https://calculatedcontent.com/author/charlesmartin14/
  87. mailto:info@calculationconsulting.com
  88. http://calculationconsulting.com/
  89. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg
  90. http://www.quora.com/charles-h-martin
  91. https://clarity.fm/charlesmartin14
  92. https://calculatedcontent.com/
  93. https://calculatedcontent.com/2012/10/09/spectral-id91/
  94. https://calculatedcontent.com/2012/10/09/spectral-id91/
  95. https://calculatedcontent.com/2012/02/06/kernels_part_1/
  96. https://calculatedcontent.com/2012/02/06/kernels_part_1/
  97. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
  98. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
  99. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 100. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 101. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 102. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 103. https://calculatedcontent.com/2019/04/01/sf-bay-acm-talk-heavy-tailed-self-id173-in-deep-neural-networks/
 104. https://calculatedcontent.com/2018/12/17/heavy-tailed-self-id173-in-deep-neural-nets-1-year-of-research/
 105. https://calculatedcontent.com/2018/11/18/dont-peek-part-2-predictions-without-test-data/
 106. https://calculatedcontent.com/2018/11/16/machine-learning-and-ai-for-the-lean-start-up/
 107. https://calculatedcontent.com/2018/10/07/dont-peek-deep-learning-without-looking-at-test-data/
 108. https://www.youtube.com/redirect?redir_token=ezgiasszjkmz1fnzp0yjtazidd98mtu1ndizmjiznkaxntu0mtq1odm2&q=https://arxiv.org/abs/1810.01075&event=video_description&v=ilv5sc8wjpy
 109. https://arxiv.org/abs/1810.01075
 110. https://arxiv.org/abs/1706.02515
 111. https://github.com/calculatedcontent/tid166
 112. https://charlesmartin14.wordpress.com/2013/11/14/metric-learning-some-quantum-statistical-mechanics/
 113. http://arxiv.org/pdf/1412.0233.pdf
 114. http://www.quora.com/machine-learning/how-does-one-decide-on-which-kernel-to-choose-for-an-id166-rbf-vs-linear-vs-poly-kernel
 115. http://arxiv.org/pdf/1412.6621v3.pdf
 116. http://www.di.ens.fr/~fbach/nips03_cluster.pdf
 117. https://charlesmartin14.files.wordpress.com/2012/10/mat1.png
 118. https://calculatedcontent.com/2019/04/
 119. https://calculatedcontent.com/2018/12/
 120. https://calculatedcontent.com/2018/11/
 121. https://calculatedcontent.com/2018/10/
 122. https://calculatedcontent.com/2018/09/
 123. https://calculatedcontent.com/2018/06/
 124. https://calculatedcontent.com/2018/04/
 125. https://calculatedcontent.com/2017/12/
 126. https://calculatedcontent.com/2017/09/
 127. https://calculatedcontent.com/2017/07/
 128. https://calculatedcontent.com/2017/06/
 129. https://calculatedcontent.com/2017/02/
 130. https://calculatedcontent.com/2017/01/
 131. https://calculatedcontent.com/2016/10/
 132. https://calculatedcontent.com/2016/09/
 133. https://calculatedcontent.com/2016/06/
 134. https://calculatedcontent.com/2016/02/
 135. https://calculatedcontent.com/2015/12/
 136. https://calculatedcontent.com/2015/04/
 137. https://calculatedcontent.com/2015/03/
 138. https://calculatedcontent.com/2015/01/
 139. https://calculatedcontent.com/2014/11/
 140. https://calculatedcontent.com/2014/09/
 141. https://calculatedcontent.com/2014/08/
 142. https://calculatedcontent.com/2013/11/
 143. https://calculatedcontent.com/2013/10/
 144. https://calculatedcontent.com/2013/08/
 145. https://calculatedcontent.com/2013/05/
 146. https://calculatedcontent.com/2013/04/
 147. https://calculatedcontent.com/2012/12/
 148. https://calculatedcontent.com/2012/11/
 149. https://calculatedcontent.com/2012/10/
 150. https://calculatedcontent.com/2012/09/
 151. https://calculatedcontent.com/2012/04/
 152. https://calculatedcontent.com/2012/02/
 153. https://twitter.com/calccon/
 154. https://www.linkedin.com/in/charlesmartin14/
 155. https://github.com/charlesmartin/
 156. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg/
 157. https://wordpress.com/start?ref=wplogin
 158. https://charlesmartin14.wordpress.com/wp-login.php
 159. https://calculatedcontent.com/feed/
 160. https://calculatedcontent.com/comments/feed/
 161. https://wordpress.com/
 162. https://wordpress.com/?ref=footer_blog
 163. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
 164. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/#cancel
 165. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 167. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/#comment-form-guest
 168. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/#comment-form-load-service:wordpress.com
 169. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/#comment-form-load-service:twitter
 170. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/#comment-form-load-service:facebook
 171. http://nanonaren.wordpress.com/
 172. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
 173. http://tablewarebox.com/
 174. http://duttatridib.wordpress.com/
 175. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
 176. http://twitter.com/alxfed
 177. http://ashutoshtripathi.com/
 178. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
 179. http://randomstratum.wordpress.com/
 180. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
 181. https://calculatedcontent.com/logo-i-3/
