   #[1]   feed [2]   comments feed [3]   machine learning with missing labels
   part 2: the univerid166 comments feed [4]machine learning with missing
   labels: transductive id166s [5]machine learning with missing labels part
   3: experiments [6]alternate [7]alternate [8]search [9]wordpress.com

   [10]skip to content

   [11][wp-logo.jpg]

machine learning with missing labels part 2: the univerid166

   [12]september 30, 2014 [13]charles h martin, phd [14]uncategorized
   [15]5 comments

   ever wonder what [16]google deepmind is up to?  they just released a
   paper on [17]semi-supervised learning with deep generative models.
   what is semi supervised learning (ssl)? in this series of posts, we go
   back to basics and take a look.

   most machine learning algos require huge amounts of labeled data to
   achieve high accuracy. id166s, id79s, and especially deep
   learning, can take advantage of massive labeled data sets.

   how can we learn when we only have a few labeled examples?

   in ssl, we try to improve our classifiers by taking advantage
   of unlabeled data.

   in our last post we described an old-school approach to this problem,
   [18]the transductive id166 (tid166), circa 2006. here, we examine a
   different approach to learn from unlabeled data   [19]infernece with the
   universum   ala vapnik 2006.

   plus, there is code! [20] the univerid166 code has a tid166 and
   the universum (uid166) approach.

wait, doesn   t deep learning use unlabeled data in pretraining?

   this is different.   deep learning uses huge amounts of labelled data.
      we want to use as little labeled data as possible.  to me, this is
   the real test of a learning theory.

   in this and the next few blogs, we will examine at several variants of
   id166s    uid166, wid166, id166+, and semi supervised deep learning methods   all
   that achieve high accuracy with only a small amount of labeled data.
   also, we only consider text classification   images and other data sets
   require different methods.

learning with unlabeled data

   let   s say with have a small number n_l of labeled documents, and a
   large number   n_u of unlabeled ones.   can we build document
   classifier (i.e. a binary id166) that generalizes well with just a little
   labeled data?

   [21]screen shot 2014-09-28 at 10.28.09 pm

   according to the vapnik and chervonenkis statistical learning theory
   (vc-slt), our generalization accuracy is very losely bounded by the
   model complexity and the number of training documents n_l :


   r(\alpha)-r_{emp}(\alpha)\leq2\mathcal{r}_{m}(\mathcal{h})+\kappa\sqrt{
   \dfrac{ln(\frac{1}{\delta})}{n_l}}

   which, in plain english, is just

   generalization error \leq model capacity ( 2r_{m} ) + size effects ( \
   \sim\sqrt{\dfrac{1}{n_{l}}} )

   the vc-slt  inspired the id166 maximum margin approach (although they are
   not equivalent).

   slt recognizes that for any data set \mathcal{l} (of size n_{l} ), we
   may obtain multiple, equivalent models (i.e. all having the same
   training accuracy):

   \gamma_{l}=\left[\mathcal{h}_{1}\sim\mathcal{h}_{2}\sim\mathcal{h}_{3}\
   sim\cdots\right]

   we should select the model \mathcal{h} from \gamma_{l} with the
   smallest capacity 2r_{m}(\mathcal{h}) ; in an id166, this means select
   the largest margin.

   this is easiest to visualize when the slack or admissible error = 1

   [22]screen shot 2014-10-11 at 9.27.08 pm

   each model \gamma_{l} is a set of hyperplanes that give same labeling.
   the left model is optimal because it has the largest    id166 capacity   ,
   which is essentially the volume carved out by the admissible
   hyperplanes.  turns out, the best model also has the hyperplane with
   the largest margin, so

   the maximun margin is a measure of the vc capacity   

   but is it the best?

   while the vc-slt bound is very loose, it does suggest that we usually
   need a large number of labeled documents n_l  to obtain any reasonable
   production accuracy.

   the slt is, however, inherently a theory about transductive learning;
   the proof of the vc bounds requires first proving the transduction
   bound (i.e. via [23]symmetrization). it has always been the dream of
   the vc-slt program to develop a transductive or semisupervised)
   method that can learn much faster than  \dfrac{1}{\sqrt{n_{l}}} .

   (by semi-supervised, we mean that the resulting model can be applied to
   out-of-sample-data directly, where as transductive learning only
   applies to the known, unlabeled test set.  we will not distinguish
   between them here.)

   we would hope to achieve  \dfrac{1}{\sqrt{n_{l}+n_{u}}} by adding in
   unlabeled data. when  n_u is very large, we win.

   (indeed, recently, vapnik has shown it is actually possible to reduce
   this bound from \dfrac{1}{\sqrt{n_{l}}}  to  \dfrac{1}{n_{l}}    if we
   can [24]learn using privileged information (lupi).   this means we can
   learn when n_{l}=320 whereas previously we needed  n_{l}=100,000 !  but
   this actually is akin to assigning additional data or even weights to
   each labelled example   and this is not the same as using just unlabeled
   data.  perhaps we can learn the weights from the unlabeled data   but
   that   s for another post.)    so we are left with the un-nerving
   statement

   if the max margin is the right measure, then the tid166 should work very
   well   

   and yet, this has proved elusive.

   what is the alternative ? suppose instead of measuring the volume
   traced out by the hyperplanes, our models measured the volume between
   the convex hulls of the 2 classes:

   [25]screen shot 2014-10-11 at 9.44.55 pm

   notice that now, the labeling on the right is better.

   this is a broader measure of the diversity of the equivalence class. in
   slt, this is related to the vc id178 (another measure of vc
   capacity).

   the universum approximates this volume   or rather the vc id178   
   via the principle of maximum contradictions.  (in fact, [26]the
   universum is not the only way to do this, as we shall see.)  a clever
   idea   but something hard to achieve in practice.  let us compare and
   contrast the tid166 and the uid166 to understand how to select the data
   sets they need and their limitations.

transductive id166 (tid166)

   transductive id136 requires a statistical replica  \mathcal{r} of
   the labeled data \mathcal{l} :

   [27]screen shot 2014-09-29 at 11.37.17 am

   the replica \mathcal{r} is not only the same size as  \mathcal{l} ,
   it has the same statistical qualities.  in particular, the label means
   converge:   <y_u>\rightarrow<y_l> as  \lim n\rightarrow\infty .
   (i.e. in a well balanced binary classifier,  this is zero)

   in theory, we can always create a replica (or phantom sample) because
   we assume the labeled data itself is drawn from some common
   empirical process.  in modern vc-slt, we think of the symmetrization
   process that creates \mathcal{r} as a rademacher process     meaning that
   we have replicated the training data but randomized the labels.

   in practice, we need to select the replica from unlabeled data   and this
   is hard!

   we hope that by adding unlabeled data, we can find the best solution by
   guessing the labels of the unlabeled data and then maximizing the
   margin on all the data.

   [28]screen shot 2014-09-29 at 2.32.29 pm

   we can apply transduction id166s  if we can create a large, unlabeled
   data set  \mathcal{u} that behaves like a statistical replica of
   \mathcal{u} , albeit much, much larger.   tid166s allow us to increase
   the accuracy of a binary text classifier by adding in large amounts of
   unlabeled data.

   also, tid166s extend the feature space, or hypothesis space \mathcal{h} .
    this is critical for text classification since , frequently, we need
   to classify documents we have never seen before, and we encounter new
   words.  this is irrelevant for image classification.

   if we have a collection of consumer blogs (about finance, beauty,
   sports, politics,    ), with some labelled documents, and large amount of
   unlabeled ones.  we can create (1-vs-1) binary tid166 classifiers, such
   as finance vs beauty if we have a good way to select the unlabeled
   data, [29]as described in our previous blog:

   [30]screen shot 2014-09-28 at 10.30.09 pm

   essentially, the unlabeled documents must consist only of finance and
   beauty blogs, and in the same ratio as the training data.

   tid166s only work well for simple binary ( 1 vs 1 ) classification, and
   only when the document classes form simple clusters.  they don   t work
   for multi-class classifier because they can not handle ( 1 vs all )
   data sets.

   so while tid166s do work, not just any unlabeled data set will do.
   indeed, i personally believe the key to a good tid166 is creating a well
   balanced unlabeled data.  or, equivalently, estimating the fraction r
   of (+/-) examples very well.      if the replica set is poor, or poorly
   understood, the tid166 results can be worse than just training an id166 on
   only the labeled data.

univerid166 (uid166)

   in 1998, and , later in 2006, vapnik introduced a different kind of
   id166   which also the allows learning from unlabeled data   but replaces the
   principle of maximum margin with a more robust method called

   the univerid166: the principle of maximum contradictions

   the idea is to add in data from classes that are significantly
   different from the 2 classes being separated:

   [31]screen shot 2014-09-28 at 10.31.50 pm

   to create a finance vs beauty classifier, we would add labelled and/or
   unlabeled documents from other categories, such as parenting, politics,
   sports, etc.   we then want a binary classifier that not only separates
   the 2 labeled classes, but is also as different  from the other
   class   the universum.

   we create 2 replicas of the universum   one with all (+) labels, and one
   all (-).    we then add unlabeled documents (blue circles)  that lie in
   the margin between classes   or really in the space between the (+/-)
   classes:

   screen shot 2014-09-30 at 12.08.39 am

   the best universum examples will lie in the convex hull between the
   finance and beauty documents; those inside the convex hulls will most
   likely be ignored.  since all the u-labels are wrong, every equivalence
   class of hyperplanes will produce numerous contradictions on
   the universum points (u):

   [32]screen shot 2014-10-11 at 11.58.51 pm

   the best model has the largest diversity on the universum; in other
   words, the largest vc id178.   vapnik has pointed out the following
   interpretation of the universum:    [when trying to classify the labeled
   data, try to avoid extra generalizations.]   

   the best model has the maximum # of contradictions on the universum.

   the univerid166 (uid166)  represents a kind of prior information that we
   add to the problem; the difference is that instead of specifying a
   prior distribution, which is hard, we replace this with a more
   practical approach   to specify a set of specific examples.

   notice we could have just created a 3-class id166 (and theuniverid166 code
   provides this for direct comparison).  or we could build 2-class
   classifier, augmented with some id91 metric to avoid the other
   class--in the same spirit the s4vm method.   [33]see, for example, the
   recent paper on emblem.  but in these simple approaches, the other
   class must only contain other   and that   s hard.

uid166s compared to tid166s:

   in the tid166 we have to be sure we are adding documents that are in the
   same classes as the training data.  in the uid166, we must be sure we are
   adding documents that do not belong to either class.

   also, with a tid166, we need to know the fraction of (+/-) documents,
   whereas with the uid166 we don   t. the uid166 would seem to admit some data
   from all classes   in principle. in practice, we will do much better
   if it does not.

   most importantly, unlike the tid166, the uid166 optimization is convex. so
   adding in bad data may will not cause convergence problems as in the
   tid166 and thus degrade the model. at least we hope.

   also, as with the tid166, we suspect the uid166 will only work for small
   n_{l} ; once  n_{l} grows above say 5% of the total documents, we
   may not see much improvement.

the univerid166 algorithm

   the id166 approach,  inspired by slt, implements a data dependent, but
   distribution independent, regularizer that lets us select the best
   model \mathcal{h} from a class of equivalent hypotheses \gamma_{l} .
   what other methods select the optimal equivalence class  \gamma_{r} ?

the principe of maximum power

   a model h  consists of an equivalence class of hyperplanes, say h\in
   h_{\gamma} .  they all have the same accuracy on the labeled data
   \mathcal{l} .

   suppose we know a prior distribution p(\omega) on the set of all
   possible hyperplanes.  then we can define the power p* of the optimal
   model as

   p*=\int_{h_{\gamma}*}d\mathbf{h}p(\mathbf{h})

   we rarely can define  p(\mathbf{h}) mathematically..but we
   can approximate it.

   let us sample p(\mathbf{h}) by selecting n_{u} unlabeled example
   documents; we call this set the universum ( \mathcal{u} ).

   we call the practical method the univerid166.  the key insight of the
   univerid166 is that while we can not compute this integral, we can
   estimate it by measuring the number of contradictions the class of
   hyperplanes generates on points in  \mathcal{u} .

the principle of maximum contradictions:

   we need a way to select the equivalence class \gamma with the maximum
   number of contradictions on  \mathcal{u} .  as usual, we create a
   regularizer.

   we augment the standard id166 optimization problem with a universum
   regularizer \vert u(f(x_u))\vert

   \frac{1}{2}\vert\omega\vert_{2}^{2}+c_{l}\sum_{l\in\mathcal{l}}h(y_{l},
   f(x_{l},b))+c_u\vert u(f(x_u))\vert

   where h is the standard id166 hinge loss.

   the regularizer \vert u(f(x_u))\vert can also be defined through a
   symmetric hinge loss, making the problem convex.  the univerid166 code
   also contains a non-convex variant using a ramp-loss approach.  we
   leave the details to the academic papers.

laplacian id166s and the the maximum volume principle

   about the same time vapnik introduced the univerid166, nyogi (at the
   university of chicago) introduced both the tid166 id166lin code, as well as
   a new approach to semi-supervised learning, [34]the laplacian id166 (or
   lapid166).

   the tid166 maximizes the margin for the labelled and unlabeled data.  the
   uid166 maximizes the contradictions on the unlabeled (univerid166) set.
   in fact, both approximate a more general form of capacity   the maximum
   volume between the convex hulls of the data sets.  and this can be
   approximation using laplacian id173!  let   s see how this all
   ties together.

   the lapid166 optimization is

   \frac{1}{2}\vert\omega\vert_{2}^{2}+c_{l}\sum_{l\in\mathcal{l}}h(y_{l},
   f(x_{l},b))+c_{u}\vert\mathcal{l}\vert

   we see it is very similar to the uid166, but with a different regularizer
      norm of the graph laplacian \vert\mathcal{l}\vert . there are several
   \mathcal{l} to choose from but lapid166 uses the one corresponding to the
   [35]laplace-beltrami operator on the data manifold.

   [36]the lapid166 has recently been applied to text classification,
   although we dont have a c or python version of the code to test like
   with do with id166lin and the univerid166.

   lapid166s and related manifold learning approaches have motivated some
   very recent advances in semi-supervised deep learning methods, such as
   the [37]manifold tangent classifier.  this classifier learns the
   underlying manifold using contractive auto-encoder, rather than using a
   simple laplacian   and this seems to work very well for images.

   (we note that [38]the popular scikit learn package includes manifold
   learning, but these are only the unsupervised variants.)

   we can relate the lapid166 approach to the vc-slt, through [39]the
   principle of maximum volume:

   the norm of the graph laplacian is measure of the vc id178

   let us again assume we have to select the best equivalence class
   \gamma^{*} on our labeled data \mathcal{l} from a set of hyperplanes
   \mathbf{h}\in h_{\gamma}* .    lacking a specific prior , we can assume
   a uniform distribution p(\mathbf{h})=1 .

   we then simply need to approximate the volume

   v_{r}(\mathbf{h})=\int_{\gamma_{r}}d\mathbf{h}

   we need a way to compute v, so we assume there exists an operator
   \mathbf{q} such that

   v_{r}(\mathbf{h})\equiv\dfrac{\mathbf{h^{\dagger}qh}}{\mathbf{h^{\dagge
   r}h}}

   to this end, vapnik et. al. introduce    a family of transductive
   algorithms which implement the maximum volume principle   ,
   called approximate volume id173 (avr) algorithms(s).  they
   take the form

   \min
   loss_{l}(\mathbf{h})+c_{u}\dfrac{\mathbf{h^{\dagger}qh}}{\mathbf{h^{\da
   gger}h}}

   for many problems, \mathbf{q} can just be the [40]graph laplacian

   \mathbf{q}=\mathcal{l} ,

   the specific form specified by the specific avr.

   if we write the general form of \mathcal{l} as

   \mathcal{l}=i-d^{-\frac{1}{2}}wd^{\frac{1}{2}}

   w can be defined using the gaussian similarity

   w_{i,j}=exp(\dfrac{-\vert x_{i}-x_{j}\vert^{2})}{2\sigma^{2}}

   which has a single adjustable width parameter \sigma .

   a more robust laplacian uses the local-scaling similarity

   w_{i,j}=exp(\dfrac{-\vert x_{i}-x_{j}\vert^{2})}{2\sigma_{i}\sigma_{j}}

   which has n adjustable parameters  \sigma_{i} .

   the maximum volume principle may be a better measure of vc capacity

   this principle of maximum volume has been applied in a number of ways,
   such as in recent papers on [41]maximum volume id91 and , and
   [42]this one for outlier detection.

   most importantly, a very recent paper proposes a [43]multiclass
   approximate volume id173 algorithm (mavr).

   we have also connected to seemingly different approaches to machine
   learning   traditional vc theory and operator theoretic manifold
   learning.  in a future post, we will look at some practical examples
   and compares how well the available open source tid166 and uid166 codes on
   text data.

further reading

   [44]practical conditions for effectiveness of the universum learning

   [45]practical analysis of the universum id166 learning

   [46]advances in universum learning

   [47]thesis:  analysis and extensions of universum learning (2014)

share this:

     * [48]twitter
     * [49]facebook
     * [50]linkedin
     * [51]more
     *

     * [52]reddit
     * [53]email
     *
     *

like this:

   like loading...

related

post navigation

   [54]previous post: machine learning with missing labels: transductive
   id166s
   [55]next post: machine learning with missing labels part 3: experiments

5 comments

    1. pingback: [56]machine learning with missing labels part 3:
       experiments | machine learning
    2. pingback: [57]spectral id91: a quick overview | machine
       learning
    3. pingback: [58]semi-supervised learning using univerid166     how to use
       an index file into training data | karmendykstra
    4. pingback: [59]id166+ / lupi: learning using privileged information |
       machine learning
    5.
   [60]gideon isaac says:
       [61]november 30, 2016 at 9:42 am
       this comment is based on a few posts you had on rbs. i am confused.
       just to give an example of one stumbling block (for me), you tried
       to show that the function being minimized for an rbm had both an
       id178 and an energy component. i did not understand what
          id178    meant in this context. does it mean    disorder    is
       minimized?    information    is maximized? and if so, what disorder, or
       what information? with energy as well, did it mean that
       incompatibilities (such as incompatible spins in a spin glass) are
       minimized? if so, what incompatibilities?
       and i didn   t quite understand why having vast number of hidden
       units makes for a more convex energy landscape. does it have
       something to do with    degrees of freedom    or with more ways
       existing of reaching a solution?
       could you suggest some topics that i should read before reading
       your blog     so that your blog makes more sense to me. it looks
       fascinating.
       [62]likelike
       [63]reply

leave a reply [64]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [65]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [66]log out /
   [67]change )
   google photo

   you are commenting using your google account. ( [68]log out /
   [69]change )
   twitter picture

   you are commenting using your twitter account. ( [70]log out /
   [71]change )
   facebook photo

   you are commenting using your facebook account. ( [72]log out /
   [73]change )
   [74]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

     * [75]charles h martin, phd

calculation consulting

   we are a boutique machine learning data science consultancy. how can we
   help? email me at [76]info@calculationconsulting.com.

   or stop by:
   [77]http://calculationconsulting.com
   [78]youtube channel
   [79]quora

   set up a quick all on [80]clarity.fm

the community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

blog stats

     * 521,305 hits

   [81]follow on wordpress.com

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   join 694 other followers

   ____________________

   (button) follow

top posts & pages

     * [82]spectral id91: a quick overview
       [83]spectral id91: a quick overview
     * [84]kernels part 1: what is an rbf kernel? really?
       [85]kernels part 1: what is an rbf kernel? really?
     * [86]why deep learning works ii: the reid172 group
       [87]why deep learning works ii: the reid172 group
     * [88]id172 in deep learning
       [89]id172 in deep learning
     * [90]causality, correlation, and brownian motion
       [91]causality, correlation, and brownian motion

recent posts

     * [92]sf bay acm talk: heavy tailed self id173 in deep
       neural networks
     * [93]heavy tailed self id173 in deep neural nets: 1 year of
       research
     * [94]don   t peek part 2: predictions without test data
     * [95]machine learning and ai for the lean start up
     * [96]don   t peek: deep learning without looking     at test data

top clicks

     * [97]youtube.com/redirect?redi   
     * [98]arxiv.org/abs/1810.01075
     * [99]arxiv.org/abs/1706.02515
     * [100]github.com/calculatedcont   
     * [101]charlesmartin14.wordpress   
     * [102]arxiv.org/pdf/1412.0233.p   
     * [103]quora.com/machine-learnin   
     * [104]arxiv.org/pdf/1412.6621v3   
     * [105]di.ens.fr/~fbach/nips03_c   
     * [106]charlesmartin14.files.wor   

archives

     * [107]april 2019
     * [108]december 2018
     * [109]november 2018
     * [110]october 2018
     * [111]september 2018
     * [112]june 2018
     * [113]april 2018
     * [114]december 2017
     * [115]september 2017
     * [116]july 2017
     * [117]june 2017
     * [118]february 2017
     * [119]january 2017
     * [120]october 2016
     * [121]september 2016
     * [122]june 2016
     * [123]february 2016
     * [124]december 2015
     * [125]april 2015
     * [126]march 2015
     * [127]january 2015
     * [128]november 2014
     * [129]september 2014
     * [130]august 2014
     * [131]november 2013
     * [132]october 2013
     * [133]august 2013
     * [134]may 2013
     * [135]april 2013
     * [136]december 2012
     * [137]november 2012
     * [138]october 2012
     * [139]september 2012
     * [140]april 2012
     * [141]february 2012

social

     * [142]view calccon   s profile on twitter
     * [143]view charlesmartin14   s profile on linkedin
     * [144]view charlesmartin   s profile on github
     * [145]view ucaao8ghavcrtszdpobc4_kg   s profile on youtube

meta

     * [146]register
     * [147]log in
     * [148]entries rss
     * [149]comments rss
     * [150]wordpress.com

   logo-i

   [151]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [152]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [153]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [154]likes-master

   %d bloggers like this:

references

   visible links
   1. https://calculatedcontent.com/feed/
   2. https://calculatedcontent.com/comments/feed/
   3. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/feed/
   4. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
   5. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/&for=wpcom-auto-discovery
   8. https://calculatedcontent.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/#content
  11. https://calculatedcontent.com/
  12. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
  13. https://calculatedcontent.com/author/charlesmartin14/
  14. https://calculatedcontent.com/category/uncategorized/
  15. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/#comments
  16. http://deepmind.com/
  17. http://arxiv.org/pdf/1406.5298v1.pdf
  18. https://charlesmartin14.wordpress.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
  19. http://ronan.collobert.com/pub/matos/2006_universum_icml.pdf
  20. http://mloss.org/software/view/19/
  21. https://charlesmartin14.files.wordpress.com/2014/09/screen-shot-2014-09-28-at-10-28-09-pm.png
  22. https://charlesmartin14.files.wordpress.com/2014/09/screen-shot-2014-10-11-at-9-27-08-pm.png
  23. http://en.wikipedia.org/wiki/vapnik   chervonenkis_theory#symmetrization
  24. http://nautil.us/issue/6/secret-codes/teaching-me-softly
  25. https://charlesmartin14.files.wordpress.com/2014/09/screen-shot-2014-10-11-at-9-44-55-pm.png
  26. http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/nipsuniversum_4709[0].pdf
  27. https://charlesmartin14.files.wordpress.com/2014/09/screen-shot-2014-09-29-at-11-37-17-am.png
  28. https://charlesmartin14.files.wordpress.com/2014/09/screen-shot-2014-09-29-at-2-32-29-pm.png
  29. https://charlesmartin14.wordpress.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
  30. https://charlesmartin14.files.wordpress.com/2014/09/screen-shot-2014-09-28-at-10-30-09-pm.png
  31. https://charlesmartin14.files.wordpress.com/2014/09/screen-shot-2014-09-28-at-10-31-50-pm.png
  32. https://charlesmartin14.files.wordpress.com/2014/09/screen-shot-2014-10-11-at-11-58-51-pm.png
  33. http://jmlr.org/proceedings/papers/v33/kim14a.pdf
  34. http://www.dii.unisi.it/~melacci/lapid166p/
  35. http://people.cs.uchicago.edu/~niyogi/papersps/bncolt05.pdf
  36. http://www.ncbi.nlm.nih.gov/pubmed/23845911
  37. http://papers.nips.cc/paper/4409-the-manifold-tangent-classifier.pdf
  38. http://scikit-learn.org/stable/modules/manifold.html
  39. http://www.cs.technion.ac.il/~pechyony/avr.pdf
  40. https://charlesmartin14.wordpress.com/2012/10/09/spectral-id91/
  41. http://www.jmlr.org/papers/volume14/niu13a/niu13a.pdf
  42. http://pdcc.ntu.edu.sg/udddasf/papers/ictai_2011.pdf
  43. http://jmlr.org/proceedings/papers/v32/niu14.pdf
  44. http://www.ece.umn.edu/users/dharx007/uploads/3/2/3/6/323610/stat8932.ppt
  45. http://snowbird.djvuzone.org/2011/abstracts/108.pdf
  46. http://www.ieee-wcci2014.org/tutorials32/wccitut-1a2-cherkassky.pdf
  47. http://conservancy.umn.edu/handle/11299/162636
  48. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/?share=twitter
  49. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/?share=facebook
  50. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/?share=linkedin
  51. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
  52. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/?share=reddit
  53. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/?share=email
  54. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
  55. https://calculatedcontent.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
  56. https://charlesmartin14.wordpress.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
  57. https://charlesmartin14.wordpress.com/2012/10/09/spectral-id91/
  58. https://karmendykstra.wordpress.com/2015/07/22/semi-supervised-learning-using-univerid166-how-to-use-an-index-file-into-training-data/
  59. https://charlesmartin14.wordpress.com/2014/11/05/learning-using-privileged-information-weighted-id166s/
  60. http://gravatar.com/gidmeister
  61. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/#comment-1544
  62. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/?like_comment=1544&_wpnonce=8fc1614884
  63. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/?replytocom=1544#respond
  64. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/#respond
  65. https://gravatar.com/site/signup/
  66. javascript:highlandercomments.doexternallogout( 'wordpress' );
  67. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
  68. javascript:highlandercomments.doexternallogout( 'googleplus' );
  69. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
  70. javascript:highlandercomments.doexternallogout( 'twitter' );
  71. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
  72. javascript:highlandercomments.doexternallogout( 'facebook' );
  73. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
  74. javascript:highlandercomments.cancelexternalwindow();
  75. https://calculatedcontent.com/author/charlesmartin14/
  76. mailto:info@calculationconsulting.com
  77. http://calculationconsulting.com/
  78. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg
  79. http://www.quora.com/charles-h-martin
  80. https://clarity.fm/charlesmartin14
  81. https://calculatedcontent.com/
  82. https://calculatedcontent.com/2012/10/09/spectral-id91/
  83. https://calculatedcontent.com/2012/10/09/spectral-id91/
  84. https://calculatedcontent.com/2012/02/06/kernels_part_1/
  85. https://calculatedcontent.com/2012/02/06/kernels_part_1/
  86. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
  87. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
  88. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
  89. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
  90. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
  91. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
  92. https://calculatedcontent.com/2019/04/01/sf-bay-acm-talk-heavy-tailed-self-id173-in-deep-neural-networks/
  93. https://calculatedcontent.com/2018/12/17/heavy-tailed-self-id173-in-deep-neural-nets-1-year-of-research/
  94. https://calculatedcontent.com/2018/11/18/dont-peek-part-2-predictions-without-test-data/
  95. https://calculatedcontent.com/2018/11/16/machine-learning-and-ai-for-the-lean-start-up/
  96. https://calculatedcontent.com/2018/10/07/dont-peek-deep-learning-without-looking-at-test-data/
  97. https://www.youtube.com/redirect?redir_token=ezgiasszjkmz1fnzp0yjtazidd98mtu1ndizmjiznkaxntu0mtq1odm2&q=https://arxiv.org/abs/1810.01075&event=video_description&v=ilv5sc8wjpy
  98. https://arxiv.org/abs/1810.01075
  99. https://arxiv.org/abs/1706.02515
 100. https://github.com/calculatedcontent/tid166
 101. https://charlesmartin14.wordpress.com/2013/11/14/metric-learning-some-quantum-statistical-mechanics/
 102. http://arxiv.org/pdf/1412.0233.pdf
 103. http://www.quora.com/machine-learning/how-does-one-decide-on-which-kernel-to-choose-for-an-id166-rbf-vs-linear-vs-poly-kernel
 104. http://arxiv.org/pdf/1412.6621v3.pdf
 105. http://www.di.ens.fr/~fbach/nips03_cluster.pdf
 106. https://charlesmartin14.files.wordpress.com/2012/10/mat1.png
 107. https://calculatedcontent.com/2019/04/
 108. https://calculatedcontent.com/2018/12/
 109. https://calculatedcontent.com/2018/11/
 110. https://calculatedcontent.com/2018/10/
 111. https://calculatedcontent.com/2018/09/
 112. https://calculatedcontent.com/2018/06/
 113. https://calculatedcontent.com/2018/04/
 114. https://calculatedcontent.com/2017/12/
 115. https://calculatedcontent.com/2017/09/
 116. https://calculatedcontent.com/2017/07/
 117. https://calculatedcontent.com/2017/06/
 118. https://calculatedcontent.com/2017/02/
 119. https://calculatedcontent.com/2017/01/
 120. https://calculatedcontent.com/2016/10/
 121. https://calculatedcontent.com/2016/09/
 122. https://calculatedcontent.com/2016/06/
 123. https://calculatedcontent.com/2016/02/
 124. https://calculatedcontent.com/2015/12/
 125. https://calculatedcontent.com/2015/04/
 126. https://calculatedcontent.com/2015/03/
 127. https://calculatedcontent.com/2015/01/
 128. https://calculatedcontent.com/2014/11/
 129. https://calculatedcontent.com/2014/09/
 130. https://calculatedcontent.com/2014/08/
 131. https://calculatedcontent.com/2013/11/
 132. https://calculatedcontent.com/2013/10/
 133. https://calculatedcontent.com/2013/08/
 134. https://calculatedcontent.com/2013/05/
 135. https://calculatedcontent.com/2013/04/
 136. https://calculatedcontent.com/2012/12/
 137. https://calculatedcontent.com/2012/11/
 138. https://calculatedcontent.com/2012/10/
 139. https://calculatedcontent.com/2012/09/
 140. https://calculatedcontent.com/2012/04/
 141. https://calculatedcontent.com/2012/02/
 142. https://twitter.com/calccon/
 143. https://www.linkedin.com/in/charlesmartin14/
 144. https://github.com/charlesmartin/
 145. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg/
 146. https://wordpress.com/start?ref=wplogin
 147. https://charlesmartin14.wordpress.com/wp-login.php
 148. https://calculatedcontent.com/feed/
 149. https://calculatedcontent.com/comments/feed/
 150. https://wordpress.com/
 151. https://wordpress.com/?ref=footer_blog
 152. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
 153. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/#cancel
 154. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 156. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/#comment-form-guest
 157. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/#comment-form-load-service:wordpress.com
 158. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/#comment-form-load-service:twitter
 159. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/#comment-form-load-service:facebook
 160. http://nanonaren.wordpress.com/
 161. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
 162. http://tablewarebox.com/
 163. http://duttatridib.wordpress.com/
 164. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
 165. http://twitter.com/alxfed
 166. http://ashutoshtripathi.com/
 167. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
 168. http://randomstratum.wordpress.com/
 169. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
 170. https://calculatedcontent.com/logo-i-3/
