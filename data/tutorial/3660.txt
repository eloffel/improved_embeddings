   #[1]   feed [2]   comments feed [3]   machine learning with missing
   labels: transductive id166s comments feed [4]data science leadership
   [5]machine learning with missing labels part 2: the univerid166
   [6]alternate [7]alternate [8]search [9]wordpress.com

   [10]skip to content

   [11][wp-logo.jpg]

machine learning with missing labels: transductive id166s

   [12]september 23, 2014 [13]charles h martin, phd [14]uncategorized
   [15]15 comments

   id166s are great for building text classifiers   if you have a set of very
   high quality, labeled documents.

   frequently, we just don   t have enough labelled data!  what can we do?
     * we can crawl the web for more labelled data. for this purpose, here
       at calculation consulting we built a production quality crawler
     * pay mechanical turks to label the documents.  this is actually
       harder than it sounds.
     * we can use what labels we have, and try to guess the labels of the
       unlabeled documents.

   guessing labels is called transductive learning and is the topic of
   this post.

id166s and self-training

   we have a huge collection of documents, perhaps with just a few labels.
    the obvious, or perhaps naive, thing to do is build a classifier using
   what we have, and then try to scale it out incrementally by
   self-training.  that is, we select documents labeled that are
   labelled high confidence by the smaller model and use them to build a
   training data set for a larger id166 model:

   [16]screen shot 2014-09-24 at 4.12.34 pm

   so this seems pretty simple     what could go wrong? since the initial
   model has very little labeled data, it is going to make lots of
   mistakes, even in the high confidence regime.   we are adding in some
   data that is mis-labeled.    as we incrementally self- train,  we
   amplify these errors, thereby reducing overall accuracy.  so what can
   be done?

transductive learning

   suppose that, as we add the new data in, instead of just naively using
   our smaller model to label the new data, we could automatically somehow
   switch the labels on these new documents, and then pick the best model
   from all these possibilities.    that is, we run an id166 where we
   retain the labels on the labeled (blue) documents, but we let the
   labels on the test (purple) documents float.  we constrain the problem
   so that at least r unlabeled documents are labeled positive.  we then
   pick the model with the largest margin. this leads to the following
   non-id76 problem for the transductive id166 (tid166)

   \min\dfrac{\lambda}{2}\|w\|^{2}+\dfrac{1}{2l}\sum\limits_{i=1}^{l}l(y_{
   i},w^{t}x_{i})+\dfrac{\lambda'}{2u}\sum\limits_{j=1}^{u}l(y_{j},w^{t}x_
   {j})

   subject to the constraint
   \dfrac{1}{u}\sum\limits_{j=1}^{u}\max[0,sign(w^{t}x_{j}')]=r

   where the l is the id168 (such the id166 hinge loss or, as here,
   the l2_id166 loss).  there are l labelled documents (x_{i}) and u
   unlabeled documents (x_{j}) , and \lambda,\lambda' are adjustable
   parameters.  the constraint r  fixes the number of positively labeled
   documents; this constraint i worry about the most because it is the one
   i use to tune the tid166 parameters     but let   s just assume we can
   estimate this and move forward.

why does adding test data to the training set improve the accuracy?

   when we don   t have enough training data, the id166 optimization problem
   is still convex, but the optimal solution may be spurious.  imagine
   that we are trying to train a binary text classifier with only say 100
   documents in each class.  the classifier will learn how to separate the
   documents, but it will not be able to generalize to new documents
   because it simply has not seen enough examples or really enough words
   to build a usable feature space.    the solutions may be spurios.


   [17]margin  indeed, there may exists many margins that separate the
   data, and the largest may not be the one that generalizes best.  that
   is, without enough training data, the id166 may mis-learn and think
   that noise words are driving the  classification.

   we want is a classifier that generalizes to unseen test documents;

   if we don   t have test labels, perhaps we can guess them.

   when we add unlabeled documents , or test data (circles) , to the
   classifier training data.  if we can guess the labels of the circles
   correctly, we can find the optimal solution because it is less
   ambiguous:

   [18]margin-tid166

   notice that since we are seeking a maximum margin solution, we
   implicitly are assuming that the documents form clusters in feature
   space because the margin lives in the low density regime between
   document clusters.  we are also are increasing the size of feature
   space, thereby greatly improving the generalizabilty of the model.

tid166 algorithm: deterministic annealing

   unlike a standard (inductive) id166, the tid166 optimization problem is
   highly non-convex and quite difficult to solve exactly.  clearly we can
   not switch the labels on all the new documents; if we add in n
   documents, then we have 2^{n} combinations to evaluate.  so how do we
   solve it?

   there are several known approaches to solving the tid166   both of which
   are readily available in the open source package [19]id166lin:
     * label switching heuristics  (-a 2) , and
     * deterministic annealing (-a 3)

   we will discuss the deterministic annealing (da) algorithm because it
   is very nice example of how to combine the ideas of id166s with the
   techniques of [20]statistical mechanics discussed in our last post.
   other, more modern transductive learning algos may use alternate
   methods such as [21]convex-concave[22] programming  (cccp, available in
   univerid166  [11]) ,  and even quasi-newton id119 (qn-s3vm)
   [10].

   in the original paper [3], tid166  refers to option  -a 2, and da to -a
   3.  here, we use the term tid166 to mean any implementation (tid166, da,
   cccp, qn-s3vm).   because we are applying the tid166 to text, however, we
   do not advocate using label propagation or other graph based methods.

combining id166s with statistical mechanics

   we previously, discussed basic [23]statistical mechanics and
   simulated annealing, where we derive the concept of id178.

   to solve the  tid166 / da optimization problem, we need relax the
   constraint on the labels that they be integers, and, instead let them
   represent the id203 of being in the (+) or (-) class.  we then
   introduce a temperature t .   first, lets get some labels that range
   from 0 to 1:

   \mu_{j}=\dfrac{1+y_{j}}{2} so that

   \mu_{j}=1 when y_{j}=1 ,  and

   \mu_{j}=0 when y_{j}=-1

   this lets us re-write the tid166 optimization problem as

   \min\dfrac{\lambda}{2}\|w\|^{2}+\dfrac{1}{2l}\sum\limits_{i=1}^{l}l(y_{
   i},w^{t}x_{i})+\dfrac{\lambda'}{2u}\sum\limits_{j=1}^{u}(\mu_{j}l_{2}(y
   _{j},w^{t}x_{j})+(1-\mu_{j})l_{2}(y_{j},-w^{t}x_{j}))

   where we write l_{2} to specifically indicate that the id168 is
   the l2-id166 loss.

   we now take a trick from the ising model in statistical mechanics.  we
   want to transform the labels

   \mu_{j}\rightarrow p_{j}

   into probabilities that range from 0 to 1:   p_{j}\in[1,0]

   and are normalized to the total number of expected positively labeled
   instances:  \dfrac{1}{u}\sum_{j=1}^{u}p_{j}=r

   how do we incorporate probabilities into our id166 optimization problem?
    we are already selecting the maximum margin of the training data; we
   now need to also select the optimal probabilities     the minimum id178
       of the labels for the unlabeled data.

   we extend the tid166 max-margin problem, adding a term that maximizes the
   total id178   s of the possible configurations of the unlabeled test
   data

   s=-\sum_{j=1}^{u}p_{j}\ln(p_{j})+(1-p_{j})\ln(1-p_{j})

   actually, since we minimize \|w\|^{2} , we actually minimize the
   negative id178 -s .  this leads to a new tid166 / da optimization
   problem

   \min\dfrac{\lambda}{2}\|w\|^{2}+\dfrac{1}{2l}\sum\limits_{i=1}^{l}l(y_{
   i},w^{t}x_{i})+\dfrac{\lambda'}{2u}\sum\limits_{j=1}^{u}(\mu_{j}l_{2}(y
   _{j},w^{t}x_{j})+(1-\mu_{j})l_{2}(y_{j},-w^{t}x_{j}))+\dfrac{t}{2u}\sum
   _{j=1}^{u}p_{j}\ln(p_{j})+(1-p_{j})\ln(1-p_{j})

   where the additional parameter   t is a lagrange multiplier for id178
   s .  we also have to add in the id172 (and class balancing)
   constraint

   \dfrac{1}{u}\sum_{j=1}^{u}p_{j}=r

   handing this constraint is a bit more complicated, and we refer to [3];
    we do note that the constraint can be re-written into what looks like
   a fermi energy, which is quite interesting.

   we maximize the margin on the training (and test) sets,

   and minimize the id178 on the test set.

   solving this optimization problem requires alternating between solving
   the labeled id166 optimization problem (maximizing the margin w ) and
   finding the optimal probabilities for the unlabeled instances
   (optimizing p_{j} ).  the result are labels (with confidences) for our
   previously unlabeled data, and, if we want to use it, a new , inductive
   id166 model.

   we also need to be careful not to overtrain.  so i will select out the
   newly labeled data with the highest confidence labels and then use this
   as training data for the production id166 (i.e liblinear).  this would
   allow the data scientist to then take a label set of say 100 documents
   per class, and automatically extend it to say 10,000 labeled documents
   per class without the use of mechanical turks or id190.

   despite the wonderful advantages promised by transductive (and
   semisupervised) learning, it is quite hard to apply in practice and has
   not achieved even close to the popularity of its supervised and
   unsupervised counterparts.  tid166s need both proper tuning [8] and well
   designed data sets.  we need to understand it   s practical
   implementation much better to use it.

practical issues:  selecting the best labelled data

   an initial training data set may be naively separable if it is small
   and has a large number of features :

   [24]margin-tid166 naively separable:

   n_{instances}\ll n_{features}



   as a data set grows, we may have far more instances than features:

   instances_ll_features potentially unseparable:

   n_{instances}\gg n_{features}



   which makes it far more probable that some instances will lie in the
   slack region between classes.

   we usually run a tool like liblinear , which implements either a
   soft-margin id166 or l2-regularized id28.  indeed, in
   practice is usually impossible to tell the difference between these 2
   models.  the additional slack terms can be though of as an extended
   regularizer or as modifying the id166 id168.  indeed, even vapnik
   himself has argued that it can be very beneficial to run an id166 with a
   large number of adjustable slack variables (i.e in master learning
   and/or his universum model [5]). the id166lin implementation of the tid166
   is a hard margin (binary, no slack variables) id166, but it does use the
   l2-id166 id168 (and not the harder hinge loss).

   it is suggested to run the tid166 slowly and repeatedly to relabel
   documents, and, on each iteration, to only take new labels that have
   moderately high confidence (and therefore, hopefully, outside the slack
   region).

   of course, there is always the risk of biasing the data set, and it may
   be better to augment the selection process by selecting documents that
   are high confidence and close. for example, in the [25]s4vm approach,
   they  high confidence documents that are also close to the labeled
   documents, as determined through a hierarchal id91 method.

   in between each stage, it would be useful to apply a mechanical turk to
   check the results to ensure the newly labelled data is not bananas.

practical issues:  designing multiple 1-vs-1  data sets

   additionally, id166lin only implements a binary id166, whereas
   many practical applications require a multi-class id166.  more generally,
   one might try to extend the tid166 to a da type algorithm using say a
   [26]potts model   a classic model in physics that generalizes
   an ising model. multiclass transductive learning is a topic of current
   research.  for now we have to use id166lin or a related package
   (univerid166, qn-s3vm) to get practical solutions.

   in a normal id166, balancing the class data is important; in a
   transducitve-id166, it is absolutely critical.  for example, [27]this
   recent study on tid166s discussed the issue.  this can be quite
   challenging in real world, production environments, and if you want to
   apply this powerful technique, extra care should be taken to design the
   tid166 (training + test) sets.

   to apply id166lin, it is necessary to carefully construct a collection of
   1-vs-1 binary id166s;  current tid166s don   t work for 1-vs-all data
   sets [4]

   for example, suppose we have a  4 class model, but we only have a few
   labeled instances for each class.  if we try to extend the set of
   instances for class 1, we need to carefully construct four 1-vs-1
   binary tid166s, and not one large data set for a 1-vs-all binary tid166.
   each individual binary tid166 classifier should be designed carefully so
   that each class (+/-) consists of a single kind of document;.
   otherwise the solver will get trapped in a local solution and the tid166
   will be worse.

   [28]screen shot 2014-09-23 at 2.53.33 pm

   furthermore, the fraction r  is a id172 constraint, and keeping
   it fixed means the negative set must be carefully constructed to ensure
   that final result has r=1/2 positively labeled instances that represent
   the actual positive class well.  this is not easy to ensure and it may
   be necessary to sample the unlabeled data using mechanical turk or
   advanced techniques to estimate the true fraction   r in the unlabeled
   data.

   the tid166 can help add labeled data to a well designed model; it can not
   be used to repair a poorly designed classification model .

   consider a binary id166 trained on a small training set of cat and dog
   videos.   we then apply the model to some unseen test videos, and
   select a large number of videos that were labelled either cats or dogs.
    the model will certainly make mistakes, but  we can hope that the
   errors are balanced across both classes.  so when we apply the tid166 to
   the entire training + test  set, half of the data will still actually
   be dog videos.

   [29]screen shot 2014-07-02 at 1.18.42 am

   on the other hand, let   s consider the poorly designed classifier shown
   below.  here, we have 3 classes:  dogs, cats, and animals.  the third
   class is trained for animal videos, and may contain dogs, cats, horses,
   mice, etc.  this is a bad design, and we would see this, say, in the
   confusion matrix for the entire model.  even if we create three 1-vs-1
   tid166s, we are not going to get anywhere.  when we create the increment
   set, to train the tid166, we have no idea what fraction of
   the training + test set are actually dog videos.  so we can not really
   fix the id172 constraint r , and resulting tid166 model may
   behave screwy.  we would need a way to estimate  r .
   [30]screen shot 2014-07-02 at 1.18.45 am

   furthermore, the animals category will likely consist of a number a
   document clusters (dog, cat, horse, mouse, ferret, etc) and the tid166
   solver will not converge properly for say the dog/animals binary tid166.
    the multiple clusters may confuse the solver (although since this may
   be an artifact of the da algorithm getting trapped in local minima and
   not an inherent problem in the theory)

   [31]screen shot 2014-09-23 at 4.13.45 pm

    ideally, each  (+/-) class must effectively capture a single, simple
   text cluster.

epilogue

   transductive learning was first proposed by more than thirty years ago
   by vapnik and chervonenkis [6] , and it is a critical idea in the
   development of the famous vc theory of statistical learning [1].  in a
   future post, we will look in detail at the role of transductive
   id136 in the vc theory and some other formulations such as the
   univerid166 and instance weighted id166s [6].

   practically,  there are few useful transductive and semi-supervised ml
   algos to use :
     * [32]the [33]id166lin open source code[34] , which we use and discuss
       here

     * [35]joachim   s id166light , the first high performance tid166
       implementations, but it is not open source
     * the very recent [36]qn-s3vm package, available in python [10]
     * the tid166 in the [37]univerid166 package [11]

   also note
     * the famous scikit-learn package has 1 method, a [38]semi-supervised
       label propagation algorithm. but this is not suitable for text
     * it appears the enterprise tool rapidminer does implement a
       commercial version of a tid166, although i personally have not tried
       it.

   in our next post,  we will run some experiments with the id166lin code
   and see how well it works on some real world datasets.

   at some point in the futurem we wlll discuss [12] and what (we think)
   is going on at[39] google deep mind.

   to get involved, goto    [40]https://github.com/calculatedcontent/tid166

references

   [1]  vladimir vapnik. statistical learning theory. wiley, 1998. isbn
   978-0-471-03003-4.

   [2] t. joachims, [41]transductive id136 for text classi   cation
   using support vector machines, icml 1998

   [3] v. sindhwani, s. sathiya keerthi, large scale semi-supervised
   linear id166s, 2006

   [4] o. chapelle, b. scholkopf, a. zien, semi-supervised learning, 2006

   [5]
   http://www.cs.princeton.edu/courses/archive/spring13/cos511/handouts/va
   pnik-slides.pdf

   [6] vapnik, v., & chervonenkis, a.  the theory of pattern recognition,
   moscow 1974

   [7]  ran el-yaniv, dmitry pechyony, [42]transductive rademacher
   complexity and its applications

   [8] j. wang,  x. shen,  w. pan [43]on transductive support vector
   machines

   [9] c. yu , [44]transductive learning of structural id166s via prior
   knowledge constraints

   [10] fabian gieseke, antti airola, tapio pahikkala, and oliver kramer.
   fast and simple gradient-based optimization for semi-supervised support
   vector machines, neurocomputing (icpram 2012 special issue), elsevier,
   accepted;  fabian gieseke, antti airola, tapio pahikkala, and oliver
   kramer. sparse quasi-newton optimization for semi-supervised support
   vector machines, proceedings of the 1st international conference on
   pattern recognition applications and methods (icpram 2012), pages
   45-54, 2012. [[45]pdf]

   [11]  ronan collobert ,fabian sinz  jason weston  leon bottou,
   [46]large scale transductive id166s   journal of machine learning
   research 7 (2006)

   [12] [47]semi-supervised learning with  deep generative models  at
   google deep mind


share this:

     * [48]twitter
     * [49]facebook
     * [50]linkedin
     * [51]more
     *

     * [52]reddit
     * [53]email
     *
     *

like this:

   like loading...

related

post navigation

   [54]previous post: data science leadership
   [55]next post: machine learning with missing labels part 2: the
   univerid166

15 comments

    1.
   [56]matt wescott says:
       [57]july 15, 2014 at 11:39 pm
       thanks for this interesting post, charles. does restricting the
       hyperplanes to pass through the origin often limit performance?
       [58]likelike
       [59]reply
         1.
        charlesmartin14 says:
            [60]july 15, 2014 at 11:51 pm
            ok ill try it on some datasets
            on the libid166 a1a.t dataset, i find
            liblinear-1.94/train -v 10 a1a.t
            cross validation accuracy = 84.8785%
            liblinear-1.94/train -v 10 -b 1 a1a.t
            cross validation accuracy = 84.8688%
            if we do a quick grid search we can get slightly better
            results
            1 84.8688%
            2 84.8333%
            3 84.885%
            4 84.8979%
            5 84.8785%
            6 84.8915%
            7 84.8753%
            8 84.8495%
            9 84.9108%
            10 84.9141%
            [61]likelike
            [62]reply
    2.
   zachary says:
       [63]july 29, 2014 at 2:44 pm
       do you have a twitter account which i can follow?
       [64]likelike
       [65]reply
         1.
        charlesmartin14 says:
            [66]july 29, 2014 at 6:10 pm
            [67]https://twitter.com/charlesmartin14
            [68]likelike
            [69]reply
    3. pingback: [70]machine learning with missing labels part 2: advanced
       id166s | machine learning
    4. pingback: [71]machine learning with missing labels part 3:
       experiments | machine learning
    5. pingback: [72]convex relaxations of transductive learning | machine
       learning
    6.
   [73]adewole kayode says:
       [74]april 22, 2015 at 7:55 am
       thank you for this useful article
       [75]likelike
       [76]reply
    7.
   [77]adewole kayode says:
       [78]april 22, 2015 at 8:02 am
       please @charlesmartin14, can you pls give me the list of
       tools/packages that have full implementation of tid166? do you know
       anything about transferred discriminative analysis (tda) algorithm?
       it can be used for id84 in a id21
       domain? i need some information about the algorithm. thanks.
       [79]likelike
       [80]reply
         1.
        charles h martin, phd says:
            [81]april 22, 2015 at 5:36 pm
            i dont know tda but we can take a look if you like. tid166 is
            available in id166lin as standalone unix code, and is available
            in python as qn-s3vm. see my later blog posts
            [82]https://charlesmartin14.wordpress.com/2014/11/01/machine-l
            earning-with-missing-labels-part-3-experiments/
            [83]likelike
            [84]reply
         2.
        charles h martin, phd says:
            [85]april 22, 2015 at 5:37 pm
            also remember to make these methods work you need to be able
            to estimate r = fraction of positively labelled examples.
            [86]likelike
            [87]reply
              1.
             adewole kayode says:
                 [88]april 23, 2015 at 3:02 am
                 thank you so much charles. you can download the article
                 below on tda and explore. after exploring, can we say
                 that tda can do the work of tid166 and more? thanks
                 transferred id84 by
                 zheng wang,yangqiu song and changshui zhang
                 state key laboratory on intelligent technology and
                 systems tsinghua national laboratory for information
                 science and technology (tnlist) department of automation,
                 tsinghua university, beijing 100084, china
                 [89]likelike
                 [90]reply
    8.
   charles h martin, phd says:
       [91]may 24, 2015 at 11:28 pm
       to use these methods it is necessary to estimate r. this can be
       done by sampling the labeled data you have, by estimating r using
       external information, by using a kernel estimation methods, or by
       trying various values of r and then testing the quality of the
       resulting labels
       essentially, we can consider the tid166 as guessing the unknown
       labels and is therefore an unsupervised id91 method. to that
       end, we can select the best r by using a metric that selects the
       best clusters given a labeling, such as the silhouette score.
       [92]likelike
       [93]reply
    9. pingback: [94]id166+ / lupi: learning using privileged information |
       machine learning
   10. pingback: [95]id166 readings     catinthemorning

leave a reply [96]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [97]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [98]log out /
   [99]change )
   google photo

   you are commenting using your google account. ( [100]log out /
   [101]change )
   twitter picture

   you are commenting using your twitter account. ( [102]log out /
   [103]change )
   facebook photo

   you are commenting using your facebook account. ( [104]log out /
   [105]change )
   [106]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

     * [107]charles h martin, phd

calculation consulting

   we are a boutique machine learning data science consultancy. how can we
   help? email me at [108]info@calculationconsulting.com.

   or stop by:
   [109]http://calculationconsulting.com
   [110]youtube channel
   [111]quora

   set up a quick all on [112]clarity.fm

the community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

blog stats

     * 521,305 hits

   [113]follow on wordpress.com

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   join 694 other followers

   ____________________

   (button) follow

top posts & pages

     * [114]spectral id91: a quick overview
       [115]spectral id91: a quick overview
     * [116]kernels part 1: what is an rbf kernel? really?
       [117]kernels part 1: what is an rbf kernel? really?
     * [118]why deep learning works ii: the reid172 group
       [119]why deep learning works ii: the reid172 group
     * [120]id172 in deep learning
       [121]id172 in deep learning
     * [122]causality, correlation, and brownian motion
       [123]causality, correlation, and brownian motion

recent posts

     * [124]sf bay acm talk: heavy tailed self id173 in deep
       neural networks
     * [125]heavy tailed self id173 in deep neural nets: 1 year
       of research
     * [126]don   t peek part 2: predictions without test data
     * [127]machine learning and ai for the lean start up
     * [128]don   t peek: deep learning without looking     at test data

top clicks

     * [129]youtube.com/redirect?redi   
     * [130]arxiv.org/abs/1810.01075
     * [131]arxiv.org/abs/1706.02515
     * [132]github.com/calculatedcont   
     * [133]charlesmartin14.wordpress   
     * [134]arxiv.org/pdf/1412.0233.p   
     * [135]quora.com/machine-learnin   
     * [136]arxiv.org/pdf/1412.6621v3   
     * [137]di.ens.fr/~fbach/nips03_c   
     * [138]charlesmartin14.files.wor   

archives

     * [139]april 2019
     * [140]december 2018
     * [141]november 2018
     * [142]october 2018
     * [143]september 2018
     * [144]june 2018
     * [145]april 2018
     * [146]december 2017
     * [147]september 2017
     * [148]july 2017
     * [149]june 2017
     * [150]february 2017
     * [151]january 2017
     * [152]october 2016
     * [153]september 2016
     * [154]june 2016
     * [155]february 2016
     * [156]december 2015
     * [157]april 2015
     * [158]march 2015
     * [159]january 2015
     * [160]november 2014
     * [161]september 2014
     * [162]august 2014
     * [163]november 2013
     * [164]october 2013
     * [165]august 2013
     * [166]may 2013
     * [167]april 2013
     * [168]december 2012
     * [169]november 2012
     * [170]october 2012
     * [171]september 2012
     * [172]april 2012
     * [173]february 2012

social

     * [174]view calccon   s profile on twitter
     * [175]view charlesmartin14   s profile on linkedin
     * [176]view charlesmartin   s profile on github
     * [177]view ucaao8ghavcrtszdpobc4_kg   s profile on youtube

meta

     * [178]register
     * [179]log in
     * [180]entries rss
     * [181]comments rss
     * [182]wordpress.com

   logo-i

   [183]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [184]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [185]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [186]likes-master

   %d bloggers like this:

references

   visible links
   1. https://calculatedcontent.com/feed/
   2. https://calculatedcontent.com/comments/feed/
   3. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/feed/
   4. https://calculatedcontent.com/2014/08/12/data-science-leadership/
   5. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/&for=wpcom-auto-discovery
   8. https://calculatedcontent.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#content
  11. https://calculatedcontent.com/
  12. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
  13. https://calculatedcontent.com/author/charlesmartin14/
  14. https://calculatedcontent.com/category/uncategorized/
  15. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comments
  16. https://charlesmartin14.files.wordpress.com/2014/09/screen-shot-2014-09-24-at-4-12-34-pm.png
  17. https://charlesmartin14.files.wordpress.com/2014/04/margin.png
  18. https://charlesmartin14.files.wordpress.com/2014/04/margin-tid166.png
  19. http://vikas.sindhwani.org/id166lin.html
  20. https://charlesmartin14.wordpress.com/2013/11/14/metric-learning-some-quantum-statistical-mechanics/
  21. http://web.stanford.edu/class/ee364b/lectures/seq_slides.pdf
  22. http://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/projects/mit15_097s12_proj5.pdf
  23. https://charlesmartin14.wordpress.com/2013/11/14/metric-learning-some-quantum-statistical-mechanics/
  24. https://charlesmartin14.files.wordpress.com/2014/04/margin-tid166.png
  25. http://www.icml-2011.org/papers/548_icmlpaper.pdf
  26. ftp://ftp.math.ucla.edu/pub/camreport/cam12-03.pdf
  27. http://www-nlpir.nist.gov/projects/tvpubs/tv8.papers/jrs.pdf
  28. https://charlesmartin14.files.wordpress.com/2014/07/screen-shot-2014-09-23-at-2-53-33-pm.png
  29. https://charlesmartin14.files.wordpress.com/2014/04/screen-shot-2014-07-02-at-1-18-42-am.png
  30. https://charlesmartin14.files.wordpress.com/2014/04/screen-shot-2014-07-02-at-1-18-45-am.png
  31. https://charlesmartin14.files.wordpress.com/2014/07/screen-shot-2014-09-23-at-4-13-45-pm.png
  32. http://id166light.joachims.org/
  33. http://vikas.sindhwani.org/id166lin.html
  34. http://id166light.joachims.org/
  35. http://id166light.joachims.org/
  36. http://www.ci.uni-oldenburg.de/60506.html
  37. http://mloss.org/software/view/19/
  38. http://scikit-learn.org/stable/modules/label_propagation.html
  39. http://deepmind.com/
  40. https://github.com/calculatedcontent/tid166
  41. http://www.cs.cornell.edu/people/tj/publications/joachims_99c.pdf
  42. http://arxiv.org/pdf/1401.3441.pdf
  43. http://users.stat.umn.edu/~xshen/paper/tid166.pdf
  44. http://www.cs.cornell.edu/~cnyu/papers/aistats12_tsid166.pdf
  45. http://www.ci.uni-oldenburg.de/download/camera_ready.pdf
  46. https://jmlr.org/papers/volume7/collobert06a/collobert06a.pdf
  47. http://arxiv.org/pdf/1406.5298.pdf
  48. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?share=twitter
  49. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?share=facebook
  50. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?share=linkedin
  51. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
  52. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?share=reddit
  53. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?share=email
  54. https://calculatedcontent.com/2014/08/12/data-science-leadership/
  55. https://calculatedcontent.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
  56. http://gravatar.com/mattwescott
  57. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-708
  58. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?like_comment=708&_wpnonce=5d517fe408
  59. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?replytocom=708#respond
  60. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-709
  61. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?like_comment=709&_wpnonce=ff0da48e7b
  62. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?replytocom=709#respond
  63. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-758
  64. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?like_comment=758&_wpnonce=ce7fa38938
  65. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?replytocom=758#respond
  66. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-759
  67. https://twitter.com/charlesmartin14
  68. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?like_comment=759&_wpnonce=e10ae33ac2
  69. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?replytocom=759#respond
  70. https://charlesmartin14.wordpress.com/2014/09/30/machine-learning-with-missing-labels-part-2-advanced-id166s/
  71. https://charlesmartin14.wordpress.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
  72. https://charlesmartin14.wordpress.com/2015/03/14/convex-relaxations-of-transductive-learning/
  73. http://www.facebook.com/1286811567
  74. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-990
  75. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?like_comment=990&_wpnonce=6cbf08c312
  76. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?replytocom=990#respond
  77. http://www.facebook.com/1286811567
  78. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-991
  79. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?like_comment=991&_wpnonce=5a43d0833a
  80. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?replytocom=991#respond
  81. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-992
  82. https://charlesmartin14.wordpress.com/2014/11/01/machine-learning-with-missing-labels-part-3-experiments/
  83. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?like_comment=992&_wpnonce=485cd6f9f8
  84. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?replytocom=992#respond
  85. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-993
  86. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?like_comment=993&_wpnonce=f0a0e9e955
  87. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?replytocom=993#respond
  88. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-994
  89. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?like_comment=994&_wpnonce=59763d9f82
  90. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?replytocom=994#respond
  91. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-1001
  92. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?like_comment=1001&_wpnonce=984f6e79cc
  93. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/?replytocom=1001#respond
  94. https://charlesmartin14.wordpress.com/2014/11/05/learning-using-privileged-information-weighted-id166s/
  95. https://catinthemorning.wordpress.com/2016/04/07/id166-readings/
  96. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#respond
  97. https://gravatar.com/site/signup/
  98. javascript:highlandercomments.doexternallogout( 'wordpress' );
  99. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
 100. javascript:highlandercomments.doexternallogout( 'googleplus' );
 101. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
 102. javascript:highlandercomments.doexternallogout( 'twitter' );
 103. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
 104. javascript:highlandercomments.doexternallogout( 'facebook' );
 105. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
 106. javascript:highlandercomments.cancelexternalwindow();
 107. https://calculatedcontent.com/author/charlesmartin14/
 108. mailto:info@calculationconsulting.com
 109. http://calculationconsulting.com/
 110. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg
 111. http://www.quora.com/charles-h-martin
 112. https://clarity.fm/charlesmartin14
 113. https://calculatedcontent.com/
 114. https://calculatedcontent.com/2012/10/09/spectral-id91/
 115. https://calculatedcontent.com/2012/10/09/spectral-id91/
 116. https://calculatedcontent.com/2012/02/06/kernels_part_1/
 117. https://calculatedcontent.com/2012/02/06/kernels_part_1/
 118. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 119. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 120. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 121. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 122. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 123. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 124. https://calculatedcontent.com/2019/04/01/sf-bay-acm-talk-heavy-tailed-self-id173-in-deep-neural-networks/
 125. https://calculatedcontent.com/2018/12/17/heavy-tailed-self-id173-in-deep-neural-nets-1-year-of-research/
 126. https://calculatedcontent.com/2018/11/18/dont-peek-part-2-predictions-without-test-data/
 127. https://calculatedcontent.com/2018/11/16/machine-learning-and-ai-for-the-lean-start-up/
 128. https://calculatedcontent.com/2018/10/07/dont-peek-deep-learning-without-looking-at-test-data/
 129. https://www.youtube.com/redirect?redir_token=ezgiasszjkmz1fnzp0yjtazidd98mtu1ndizmjiznkaxntu0mtq1odm2&q=https://arxiv.org/abs/1810.01075&event=video_description&v=ilv5sc8wjpy
 130. https://arxiv.org/abs/1810.01075
 131. https://arxiv.org/abs/1706.02515
 132. https://github.com/calculatedcontent/tid166
 133. https://charlesmartin14.wordpress.com/2013/11/14/metric-learning-some-quantum-statistical-mechanics/
 134. http://arxiv.org/pdf/1412.0233.pdf
 135. http://www.quora.com/machine-learning/how-does-one-decide-on-which-kernel-to-choose-for-an-id166-rbf-vs-linear-vs-poly-kernel
 136. http://arxiv.org/pdf/1412.6621v3.pdf
 137. http://www.di.ens.fr/~fbach/nips03_cluster.pdf
 138. https://charlesmartin14.files.wordpress.com/2012/10/mat1.png
 139. https://calculatedcontent.com/2019/04/
 140. https://calculatedcontent.com/2018/12/
 141. https://calculatedcontent.com/2018/11/
 142. https://calculatedcontent.com/2018/10/
 143. https://calculatedcontent.com/2018/09/
 144. https://calculatedcontent.com/2018/06/
 145. https://calculatedcontent.com/2018/04/
 146. https://calculatedcontent.com/2017/12/
 147. https://calculatedcontent.com/2017/09/
 148. https://calculatedcontent.com/2017/07/
 149. https://calculatedcontent.com/2017/06/
 150. https://calculatedcontent.com/2017/02/
 151. https://calculatedcontent.com/2017/01/
 152. https://calculatedcontent.com/2016/10/
 153. https://calculatedcontent.com/2016/09/
 154. https://calculatedcontent.com/2016/06/
 155. https://calculatedcontent.com/2016/02/
 156. https://calculatedcontent.com/2015/12/
 157. https://calculatedcontent.com/2015/04/
 158. https://calculatedcontent.com/2015/03/
 159. https://calculatedcontent.com/2015/01/
 160. https://calculatedcontent.com/2014/11/
 161. https://calculatedcontent.com/2014/09/
 162. https://calculatedcontent.com/2014/08/
 163. https://calculatedcontent.com/2013/11/
 164. https://calculatedcontent.com/2013/10/
 165. https://calculatedcontent.com/2013/08/
 166. https://calculatedcontent.com/2013/05/
 167. https://calculatedcontent.com/2013/04/
 168. https://calculatedcontent.com/2012/12/
 169. https://calculatedcontent.com/2012/11/
 170. https://calculatedcontent.com/2012/10/
 171. https://calculatedcontent.com/2012/09/
 172. https://calculatedcontent.com/2012/04/
 173. https://calculatedcontent.com/2012/02/
 174. https://twitter.com/calccon/
 175. https://www.linkedin.com/in/charlesmartin14/
 176. https://github.com/charlesmartin/
 177. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg/
 178. https://wordpress.com/start?ref=wplogin
 179. https://charlesmartin14.wordpress.com/wp-login.php
 180. https://calculatedcontent.com/feed/
 181. https://calculatedcontent.com/comments/feed/
 182. https://wordpress.com/
 183. https://wordpress.com/?ref=footer_blog
 184. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
 185. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#cancel
 186. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 188. https://charlesmartin14.files.wordpress.com/2014/07/screen-shot-2014-09-23-at-2-50-19-pm.png
 189. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-form-guest
 190. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-form-load-service:wordpress.com
 191. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-form-load-service:twitter
 192. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/#comment-form-load-service:facebook
 193. http://nanonaren.wordpress.com/
 194. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
 195. http://tablewarebox.com/
 196. http://duttatridib.wordpress.com/
 197. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
 198. http://twitter.com/alxfed
 199. http://ashutoshtripathi.com/
 200. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
 201. http://randomstratum.wordpress.com/
 202. https://calculatedcontent.com/2014/09/23/machine-learning-with-missing-labels-transductive-id166s/
 203. https://calculatedcontent.com/logo-i-3/
