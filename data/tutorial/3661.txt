   #[1]   feed [2]   comments feed [3]   why does deep learning work?
   comments feed [4]convex relaxations of transductive learning [5]why
   deep learning works ii: the reid172 group [6]alternate
   [7]alternate [8]search [9]wordpress.com

   [10]skip to content

   [11][wp-logo.jpg]

why does deep learning work?

   [12]march 25, 2015 [13]charles h martin, phd [14]uncategorized [15]18
   comments

   why does deep learning work?

   this is the big question on everyone   s mind these days.  c   mon we all
   know the answer already:

      the long-term behavior of certain neural network models are governed
   by the statistical mechanism of infinite-range ising spin-glass
   hamiltonians    [1]   in other words,

   multilayer neural networks are just spin glasses?  right?

   this is kinda true   depending on what you mean by a spin glass.

   in a recent paper by lecun, he attempts to extend our understanding of
   training neural networks by studying the sgd approach to solving the
   multilayer neural network optimization problem [1].   furthermore, he
   claims

   none of these works however make the attempt to explain the paradigm of
   optimizing the highly non-convex neural network objective function
   through the prism of spin-glass theory and thus in this respect our
   approach is very novel.  and, again, this is kinda true

   but here   s the thing   we already have a good idea of what the energy
   landscape of multiscale spin glass models* look like   from early
   theoretical protein folding work by peter wolynes to modern work by ken
   dill, etc [2,3,4]).  in fact, here is a typical surface:
   energy landscape of multiscale spin glass model

   *[technically these are ising spin models with multi-spin interactions]

   let us consider the nodes, which above represent partially folded
   states, as nodes in a multiscale spin glass   or , say, a multilayer
   neural network.  immediately we see the analogy and the appearance of
   the    energy funnel    in fact, researchers have studied these    folding
   funnels    of spin glass models over 20 years ago [2,3,4].  and we knew
   then that

   as we increase the network size, the funnel gets sharper
   3d energy landscape of the folding funnel of a spin glass

   note: the wolynes protein-folding spin-glass model is significantly
   different from the p-spin hopfield model that lecun discusses because
   it contains multi-scale, multi-spin interactions.  these details
   matter.

   screen shot 2015-06-21 at 9.28.24 am

   spin glasses and spin funnels are quite different.  spin glasses are
   highly non-convex with lots of local minima, saddle points, etc.  spin
   funnels, however, are designed to find the spin glass of
   minimal-frustration, and have a convex, funnel shaped, energy
   landscape.

   [16]spin funnels have been characterized and cataloged by theoretical
   chemists and can take many different convex shapes

   this seemed to be necessary to resolve one of the great mysteries of
   protein folding: [17]levinthal   s paradox [5].  if nature just
   used statistical sampling to fold a protein, it would take longer
   than [18]the    known    lifetime of the universe.  it is
   why machine learning is not just statistics.
   [19]spin funnels and spin glasses spin funnels (dfm) vs spin glasses
   (sg) [4]

   deep learning networks are (probably) spin funnels

   so with a surface like this, it is not so surprising that an sgd method
   might be able to find the energy minima (called the native state in
   protein folding theory). we just need to jump around until we reach the
   top of the funnel, and then it is a straight shot down.  this, in fact,
   defines a so-called    folding funnel    [4]

   so is not surprising at all that sgd may work.

   recent research at google and stanford confirms that the deep learning
   energy landscapes appear to be roughly convex [6], as does lecuns work
   on spin glasses.

   note that a real theory of protein folding, which would actually be
   able to fold a protein correctly (i.e. freed   s approach [7]), would be
   a lot more detailed than a simple spin glass model.  likewise, real
   deep learning systems are going to have a lot more engineering
   details   to avoid overtraining (dropout, pooling, momentum) than a
   theoretical spin funnel.

    [indeed, what is unclear is what happens at the bottom of the funnel.
    does the system exhibit a spin glass transition (with full
   blown [20]replica symmetry breaking, as lecun suggests), or is the
   optimal solution more like a simple native state defined by but a few
   low-energy configurations ? do dl systems display a phase transition,
   and is it first order like protein folding?  we will address these
   details in a future post.]  in any case case,

   it is not that deep learning is non-convex   is that we need to avoid
   over-training

   and it seems modern systems can do this using a few simple tricks, like
   rectified linear units, dropout, etc.

   hopefully we can learn something using the techniques developed to
   study the energy landscape of   multi-scale spin glass/ spin
   funnels models. [8,9], thereby utilizing methods  theoretical chemistry
   and condensed matter physics.

   in a future post, we will look in detail at the folding funnel
   including details such as misfolding,  the spin glass transition in the
   funnel, and the relationship to symmetry breaking, overtraining, and
   lecun   s very recent analysis.  these are non-trivial issues.

   i believe this is the first conjecture that supervised deep learning is
   related to a spin funnel.   in the next post, i will examine the
   relationship between unsupervised deep learning and the variational
   reid172 group [10].

   i discuss more of this on my youtube channel : [21]mmds 2016 talk

update:  jan 2018

   we have observed what i conjectured 3 years ago on my blog and at mmds
   2016    the spin glass of minimal frustration (left) appearing in deep
   networks (right).  stay tuned for some very exciting results into
   theory of deep learning.

update:  sep 2018:  predictions can come true

   this paper was just brought to my attention on    [22]visualizing the
   loss landscape of neural nets    which exactly verifies the above
   predictions   3 years later

          we observe that, when networks become sufficiently deep, neural loss
   landscapes quickly transition from being nearly convex to being highly
   chaotic. this transition from convex to chaotic behavior coincides with
   a dramatic drop in generalization error, and ultimately to a lack of
   trainability.   

          we show that skip connections promote flat minimizers and prevent
   the transition to chaotic behavior, which helps explain why skip
   connections are necessary for training extremely deep networks   

update:  october 2018

   3 years after posting this blog, i have formalized the ideas into a new
   theory of learning.  the first of the papers has been published

   [23]implicit self-id173 in deep neural networks: evidence from
   random matrix theory and implications for learning


   learn more about us at:  [24] http://calculationconsulting.com

   [1] lecun et. al.,  [25]the loss surfaces of multilayer networks, 2015

   [2] [26]spin glasses and the statistical mechanics of protein folding,
   pnas, 1987

   [3] [27]theory of protein folding: the energy landscape perspective,
   annu. rev. phys. chem. 1997

   [4] [28]energy landscapes, supergraphs, and    folding funnels    in spin
   systems, 1999

   [5] [29]from levinthal to pathways to funnels, nature, 1997

   [6] [30]qualitatively characterizing neural network optimization
   problems, google research (2015)

   [7]  [31]mimicking the folding pathway to improve homology-free protein
   structure prediction, 2008

   [8] [32]funnels in energy landscapes, 2007

   [9] [33]landscape statistics of the low autocorrelated binary string
   problem, 2007

   [10] [34]a common logic to seeing cats and cosmos, 2014

share this:

     * [35]twitter
     * [36]facebook
     * [37]linkedin
     * [38]more
     *

     * [39]reddit
     * [40]email
     *
     *

like this:

   like loading...

related

   [41]deep learning

post navigation

   [42]previous post: convex relaxations of transductive learning
   [43]next post: why deep learning works ii: the reid172 group

18 comments

    1. pingback: [44]why deep learning works ii: the reid172 group
       | machine learning
    2. pingback: [45]distilled news | data analytics & r
    3. pingback: [46]pourquoi l   apprentissage profond et les r  seaux
       neuronaux sont-ils si prometteurs ? | thibaut cuvelier
    4.
   [47]kanzy says:
       [48]march 11, 2016 at 8:43 am
       reblogged this on [49]aos.
       [50]likelike
       [51]reply
    5.
   guillaume says:
       [52]june 23, 2016 at 1:08 am
       very interesting post !
       for a newbie dl user, as a cond-mat physicist, i can only agree
       with your short essay. it seems intuitively that dl and sg networks
       are intimately related. dropout makes me think, perhaps wrongly,
       about another kind of randomness than frustration (with different
       effects): impurities. in any event, a great field of research with
       still many more promising applications.
       thanks a lot. cheers
       [53]likeliked by [54]1 person
       [55]reply
    6. pingback: [56]improving rbms with physical chemistry | machine
       learning
    7.
   [57]renan says:
       [58]december 24, 2016 at 7:54 am
       really nice topic! thanks!
       [59]likelike
       [60]reply
    8. pingback: [61]distilled news | data analytics & r
    9. pingback: [62]theory of deep learning     discventionstech
   10. pingback: [63]why deep learning works 3: backprop minimizes the
       free energy     calculated content
   11. pingback: [64]why does deep learning work?
   12.
   [65]boris gorelik says:
       [66]january 20, 2018 at 7:46 pm
       another consequence of energy funnels with global minima is the
       existence of    alternative truths        several stable configurations
       of the same physical model. in the structural biology world, such
       configurations can manifest as alternative binding modes of a
       ligand to a receptor, as shown here
       [67]https://www.ncbi.nlm.nih.gov/pubmed/18058908 (i   m the first
       author). another plausible example of the alternative
       configurations is the existence of different stable protein
       configurations, such as in the case of prion diseases.
       i wonder what local minima of a deep learning network can stand
       for.
       [68]likelike
       [69]reply
   13.
   carmen parron says:
       [70]march 8, 2018 at 11:38 pm
       thanks a lot. i have enjoyed this bright piece by traveling back in
       time. found the images very much useful to have a richer envision
       of the impact that some astronomy works had in all the sciences.
       and vice versa..
       [71]likelike
       [72]reply
   14. pingback: [73]rethinking   or remembering   generalization in neural
       networks
   15. pingback: [74]rethinking   or remembering   generalization in neural
       networks |                   
   16. pingback: [75]power laws in deep learning
   17. pingback: [76]power laws in deep learning | premium blog! |
       development code, android, ios anh tranning it
   18. pingback: [77]rank collapse in deep learning

leave a reply [78]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [79]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [80]log out /
   [81]change )
   google photo

   you are commenting using your google account. ( [82]log out /
   [83]change )
   twitter picture

   you are commenting using your twitter account. ( [84]log out /
   [85]change )
   facebook photo

   you are commenting using your facebook account. ( [86]log out /
   [87]change )
   [88]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

     * [89]charles h martin, phd

calculation consulting

   we are a boutique machine learning data science consultancy. how can we
   help? email me at [90]info@calculationconsulting.com.

   or stop by:
   [91]http://calculationconsulting.com
   [92]youtube channel
   [93]quora

   set up a quick all on [94]clarity.fm

the community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

blog stats

     * 521,305 hits

   [95]follow on wordpress.com

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   join 694 other followers

   ____________________

   (button) follow

top posts & pages

     * [96]spectral id91: a quick overview
       [97]spectral id91: a quick overview
     * [98]kernels part 1: what is an rbf kernel? really?
       [99]kernels part 1: what is an rbf kernel? really?
     * [100]why deep learning works ii: the reid172 group
       [101]why deep learning works ii: the reid172 group
     * [102]id172 in deep learning
       [103]id172 in deep learning
     * [104]causality, correlation, and brownian motion
       [105]causality, correlation, and brownian motion

recent posts

     * [106]sf bay acm talk: heavy tailed self id173 in deep
       neural networks
     * [107]heavy tailed self id173 in deep neural nets: 1 year
       of research
     * [108]don   t peek part 2: predictions without test data
     * [109]machine learning and ai for the lean start up
     * [110]don   t peek: deep learning without looking     at test data

top clicks

     * [111]youtube.com/redirect?redi   
     * [112]arxiv.org/abs/1810.01075
     * [113]arxiv.org/abs/1706.02515
     * [114]github.com/calculatedcont   
     * [115]charlesmartin14.wordpress   
     * [116]arxiv.org/pdf/1412.0233.p   
     * [117]quora.com/machine-learnin   
     * [118]arxiv.org/pdf/1412.6621v3   
     * [119]di.ens.fr/~fbach/nips03_c   
     * [120]charlesmartin14.files.wor   

archives

     * [121]april 2019
     * [122]december 2018
     * [123]november 2018
     * [124]october 2018
     * [125]september 2018
     * [126]june 2018
     * [127]april 2018
     * [128]december 2017
     * [129]september 2017
     * [130]july 2017
     * [131]june 2017
     * [132]february 2017
     * [133]january 2017
     * [134]october 2016
     * [135]september 2016
     * [136]june 2016
     * [137]february 2016
     * [138]december 2015
     * [139]april 2015
     * [140]march 2015
     * [141]january 2015
     * [142]november 2014
     * [143]september 2014
     * [144]august 2014
     * [145]november 2013
     * [146]october 2013
     * [147]august 2013
     * [148]may 2013
     * [149]april 2013
     * [150]december 2012
     * [151]november 2012
     * [152]october 2012
     * [153]september 2012
     * [154]april 2012
     * [155]february 2012

social

     * [156]view calccon   s profile on twitter
     * [157]view charlesmartin14   s profile on linkedin
     * [158]view charlesmartin   s profile on github
     * [159]view ucaao8ghavcrtszdpobc4_kg   s profile on youtube

meta

     * [160]register
     * [161]log in
     * [162]entries rss
     * [163]comments rss
     * [164]wordpress.com

   logo-i

   [165]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [166]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [167]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [168]likes-master

   %d bloggers like this:

references

   visible links
   1. https://calculatedcontent.com/feed/
   2. https://calculatedcontent.com/comments/feed/
   3. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/feed/
   4. https://calculatedcontent.com/2015/03/14/convex-relaxations-of-transductive-learning/
   5. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/&for=wpcom-auto-discovery
   8. https://calculatedcontent.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#content
  11. https://calculatedcontent.com/
  12. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
  13. https://calculatedcontent.com/author/charlesmartin14/
  14. https://calculatedcontent.com/category/uncategorized/
  15. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#comments
  16. http://dillgroup.stonybrook.edu/#/landscapes
  17. http://en.wikipedia.org/wiki/levinthal's_paradox
  18. http://en.wikipedia.org/wiki/age_of_the_universe
  19. https://charlesmartin14.files.wordpress.com/2015/02/screen-shot-2015-03-22-at-12-36-22-pm.png
  20. http://arxiv.org/pdf/cond-mat/0205387v1.pdf
  21. https://www.youtube.com/watch?v=kibkhipbxiu
  22. https://arxiv.org/pdf/1712.09913.pdf
  23. https://arxiv.org/abs/1810.01075
  24. http://calculationconsulting.com/
  25. http://arxiv.org/pdf/1412.0233.pdf
  26. http://www.pnas.org/content/84/21/7524.full.pdf
  27. http://lfp.qb.fcen.uba.ar/embnet/references/frustra_ref1.pdf
  28. http://arxiv.org/pdf/cond-mat/9904060v2.pdf
  29. http://arrhenius.med.toronto.edu/~chan/dill-chan-nsb97.pdf
  30. http://arxiv.org/pdf/1412.6544v4.pdf
  31. http://www.pnas.org/content/106/10/3734.full
  32. http://www.santafe.edu/media/workingpapers/07-09-033.pdf
  33. http://www.santafe.edu/media/workingpapers/00-07-033.pdf
  34. https://www.quantamagazine.org/20141204-a-common-logic-to-seeing-cats-and-cosmos/
  35. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?share=twitter
  36. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?share=facebook
  37. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?share=linkedin
  38. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
  39. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?share=reddit
  40. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?share=email
  41. https://calculatedcontent.com/tag/deep-learning/
  42. https://calculatedcontent.com/2015/03/14/convex-relaxations-of-transductive-learning/
  43. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
  44. https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
  45. http://advanceddataanalytics.net/2015/06/24/distilled-news-128/
  46. http://blog.developpez.com/dourouc05/p12993/qt/pourquoi-lapprentissage-profond-et-les-reseaux-neuronaux-sont-ils-si-prometteurs
  47. http://kanzy.wordpress.com/
  48. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#comment-1235
  49. https://kanzy.wordpress.com/2016/03/11/why-does-deep-learning-work/
  50. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?like_comment=1235&_wpnonce=d80c827100
  51. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?replytocom=1235#respond
  52. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#comment-1363
  53. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?like_comment=1363&_wpnonce=2ca82c4d68
  54. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
  55. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?replytocom=1363#respond
  56. https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/
  57. https://plus.google.com/104002464411867652155
  58. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#comment-1562
  59. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?like_comment=1562&_wpnonce=796dbdc553
  60. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?replytocom=1562#respond
  61. http://advanceddataanalytics.net/2017/01/17/distilled-news-436/
  62. https://discventionstech.wordpress.com/2017/01/17/theory-of-deep-learning/
  63. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
  64. http://www.phpdrill.com/why-does-deep-learning-work/
  65. http://randomstratum.wordpress.com/
  66. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#comment-2553
  67. https://www.ncbi.nlm.nih.gov/pubmed/18058908
  68. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?like_comment=2553&_wpnonce=e4cb6b7f18
  69. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?replytocom=2553#respond
  70. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#comment-2651
  71. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?like_comment=2651&_wpnonce=199ff1cd0d
  72. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/?replytocom=2651#respond
  73. https://calculatedcontent.com/2018/04/01/rethinking-or-remembering-generalization-in-neural-networks/
  74. https://163ai.org/30/note/13/02/04/machine-learning/4057/share/author/machinelearning/2018/
  75. https://calculatedcontent.com/2018/09/09/power-laws-in-deep-learning/
  76. http://phpcantho.com/power-laws-in-deep-learning/
  77. https://calculatedcontent.com/2018/09/21/rank-collapse-in-deep-learning/
  78. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#respond
  79. https://gravatar.com/site/signup/
  80. javascript:highlandercomments.doexternallogout( 'wordpress' );
  81. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
  82. javascript:highlandercomments.doexternallogout( 'googleplus' );
  83. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
  84. javascript:highlandercomments.doexternallogout( 'twitter' );
  85. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
  86. javascript:highlandercomments.doexternallogout( 'facebook' );
  87. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
  88. javascript:highlandercomments.cancelexternalwindow();
  89. https://calculatedcontent.com/author/charlesmartin14/
  90. mailto:info@calculationconsulting.com
  91. http://calculationconsulting.com/
  92. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg
  93. http://www.quora.com/charles-h-martin
  94. https://clarity.fm/charlesmartin14
  95. https://calculatedcontent.com/
  96. https://calculatedcontent.com/2012/10/09/spectral-id91/
  97. https://calculatedcontent.com/2012/10/09/spectral-id91/
  98. https://calculatedcontent.com/2012/02/06/kernels_part_1/
  99. https://calculatedcontent.com/2012/02/06/kernels_part_1/
 100. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 101. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 102. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 103. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 104. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 105. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 106. https://calculatedcontent.com/2019/04/01/sf-bay-acm-talk-heavy-tailed-self-id173-in-deep-neural-networks/
 107. https://calculatedcontent.com/2018/12/17/heavy-tailed-self-id173-in-deep-neural-nets-1-year-of-research/
 108. https://calculatedcontent.com/2018/11/18/dont-peek-part-2-predictions-without-test-data/
 109. https://calculatedcontent.com/2018/11/16/machine-learning-and-ai-for-the-lean-start-up/
 110. https://calculatedcontent.com/2018/10/07/dont-peek-deep-learning-without-looking-at-test-data/
 111. https://www.youtube.com/redirect?redir_token=ezgiasszjkmz1fnzp0yjtazidd98mtu1ndizmjiznkaxntu0mtq1odm2&q=https://arxiv.org/abs/1810.01075&event=video_description&v=ilv5sc8wjpy
 112. https://arxiv.org/abs/1810.01075
 113. https://arxiv.org/abs/1706.02515
 114. https://github.com/calculatedcontent/tid166
 115. https://charlesmartin14.wordpress.com/2013/11/14/metric-learning-some-quantum-statistical-mechanics/
 116. http://arxiv.org/pdf/1412.0233.pdf
 117. http://www.quora.com/machine-learning/how-does-one-decide-on-which-kernel-to-choose-for-an-id166-rbf-vs-linear-vs-poly-kernel
 118. http://arxiv.org/pdf/1412.6621v3.pdf
 119. http://www.di.ens.fr/~fbach/nips03_cluster.pdf
 120. https://charlesmartin14.files.wordpress.com/2012/10/mat1.png
 121. https://calculatedcontent.com/2019/04/
 122. https://calculatedcontent.com/2018/12/
 123. https://calculatedcontent.com/2018/11/
 124. https://calculatedcontent.com/2018/10/
 125. https://calculatedcontent.com/2018/09/
 126. https://calculatedcontent.com/2018/06/
 127. https://calculatedcontent.com/2018/04/
 128. https://calculatedcontent.com/2017/12/
 129. https://calculatedcontent.com/2017/09/
 130. https://calculatedcontent.com/2017/07/
 131. https://calculatedcontent.com/2017/06/
 132. https://calculatedcontent.com/2017/02/
 133. https://calculatedcontent.com/2017/01/
 134. https://calculatedcontent.com/2016/10/
 135. https://calculatedcontent.com/2016/09/
 136. https://calculatedcontent.com/2016/06/
 137. https://calculatedcontent.com/2016/02/
 138. https://calculatedcontent.com/2015/12/
 139. https://calculatedcontent.com/2015/04/
 140. https://calculatedcontent.com/2015/03/
 141. https://calculatedcontent.com/2015/01/
 142. https://calculatedcontent.com/2014/11/
 143. https://calculatedcontent.com/2014/09/
 144. https://calculatedcontent.com/2014/08/
 145. https://calculatedcontent.com/2013/11/
 146. https://calculatedcontent.com/2013/10/
 147. https://calculatedcontent.com/2013/08/
 148. https://calculatedcontent.com/2013/05/
 149. https://calculatedcontent.com/2013/04/
 150. https://calculatedcontent.com/2012/12/
 151. https://calculatedcontent.com/2012/11/
 152. https://calculatedcontent.com/2012/10/
 153. https://calculatedcontent.com/2012/09/
 154. https://calculatedcontent.com/2012/04/
 155. https://calculatedcontent.com/2012/02/
 156. https://twitter.com/calccon/
 157. https://www.linkedin.com/in/charlesmartin14/
 158. https://github.com/charlesmartin/
 159. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg/
 160. https://wordpress.com/start?ref=wplogin
 161. https://charlesmartin14.wordpress.com/wp-login.php
 162. https://calculatedcontent.com/feed/
 163. https://calculatedcontent.com/comments/feed/
 164. https://wordpress.com/
 165. https://wordpress.com/?ref=footer_blog
 166. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
 167. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#cancel
 168. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 170. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/spmf/
 171. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#comment-form-guest
 172. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#comment-form-load-service:wordpress.com
 173. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#comment-form-load-service:twitter
 174. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/#comment-form-load-service:facebook
 175. http://nanonaren.wordpress.com/
 176. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
 177. http://tablewarebox.com/
 178. http://duttatridib.wordpress.com/
 179. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
 180. http://twitter.com/alxfed
 181. http://ashutoshtripathi.com/
 182. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
 183. http://randomstratum.wordpress.com/
 184. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
 185. https://calculatedcontent.com/logo-i-3/
