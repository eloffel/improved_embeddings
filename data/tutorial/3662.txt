   #[1]   feed [2]   comments feed [3]   why deep learning works ii: the
   reid172 group comments feed [4]why does deep learning work?
   [5]when id173 fails [6]alternate [7]alternate [8]search
   [9]wordpress.com

   [10]skip to content

   [11][wp-logo.jpg]

why deep learning works ii: the reid172 group

   [12]april 1, 2015 [13]charles h martin, phd [14]uncategorized [15]77
   comments

   deep learning is amazing.  but why is deep learning so successful?  is
   deep learning just old-school neural networks on modern hardware?  is
   it just that we have so much data now the methods work better?  is deep
   learning just a really good at finding features. researchers are
   working hard to sort this out.

   recently it has been shown that [1]

   unsupervised deep learning implements the kadanoff real space
   variational reid172 group (1975)

   this means the success of deep learning is intimately related to some
   very deep and subtle ideas from theoretical physics.  in this post we
   examine this.

unsupervised deep learning: autoencoder flow map

   an autoencoder is a unsupervised deep learning algorithm that learns
   how to represent an complex image or other data structure x .   there
   are several kinds of autoencoders; we care about so-called neural
   encoders   those using deep learning techniques to reconstruct the data:

   recon

   the simplest neural encoder is a [16]restricted boltzman machine (rbm).
    an rbm is non-linear, recursive, lossy function f(x) that maps the
   data x from visible nodes  {v}  into hidden nodes {h} :

   [17]rbm

   the rbm is learned by selecting the optimal parameters
   {b_{v}},{c_{h}},{w_{v,h}} that minimize some measure of
   the reconstruction error (see training rbms, below)

   \min |\vert f(x)-x\vert

   rbms and other deep learning algos are formulated using classical
   statistical mechanics.  and that is where it gets interesting!

multi scale id171

   in machine learning (ml), we map (visible) data into (hidden) features

   \mathbf{v}(x)\rightarrow\mathbf{h}

   the hidden units discover features at a coarser grain level of scale

   [18]unsupervised filters

   with rbms, when features are complex, we may stack them into a deep
   belief network (dbm), so that we can learn at different levels of scale

   [19]dbn

   and leads to multi-scale features in each layer

   [20]dbn-features

   id50 are a theory of unsupervised multiscale feature
   learning

fixed points and flow maps

   we call  f(x)  a flow map

   f(x)\rightarrow x

   eif we apply the flow map to the data repeatedly, (we hope) it
   converges to a fixed point

   \lim\limits_{n}f^{n}(f^{n-1}(\cdots(f^{1}(f^{0}(x))))\rightarrow
   f_{\infty}(x)

   notice that we usually expect to apply the same map each time
   f^{n}(x)=f(x) , however, for a computational theory we may need more
   flexibility.

example: linear flow map

   the simplest example of a flow map is the simple linear map

   x\rightarrow cx

   so that

   f(x)\sim cx

   where c is a non-negative, low rank matrix

   \mid c\mid\,\ll\,\mid x\mid

   we have seen this before: this leads to a [21]convex form of
   nonnegative id105 nmf

   [22]convex-fig1

   convex nmf applies when we can specify the feature space and where the
   data naturally clusters.  here, there are a few instances that are
   archetypes that define the convex hull of the data.

   \{\mathbf{x}_{c}\}\in x\,,c=1,\cdots,\mid c\mid

   amazingly, many id91 problems are provably convex   but that   s a
   story for another post.

example: manifold learning

   near a fixed point, we commonly approximate the flow map by a linear
   operator

   f_{\infty}(x) \sim\mathbf{l}(x)

   this lets us capture the structure of the true data manifold, and is
   usually described by the low lying eigen-spectra of

   \mathbf{l}(x)=\sum\limits_{i=1}\lambda_{i}\hat{\mathbf{v}}_{i}\sim\lamb
   da_{0}\hat{\mathbf{v}}_{0}+\lambda_{1}\hat{\mathbf{v}}_{1}+\cdots .

   in the same spirit,  semi & unsupervised manifold learning, we model
   the data using a laplacian operator \mathbf{l}(\sigma) , usually
   parameterized by a single scale parameter \sigma .

[23]manifold

   these methods include [24]spectral id91, [25]manifold
   id173 , [26]laplacian id166, etc.  note that manifold learning
   methods, like the [27]manifold tanget classifier,  employ contractive
   auto encoders and use several scale parameters to capture the local
   structure of the data manifold.

the reid172 group

   in chemistry and physics, we frequently encounter problems that require
   a multi-scale description.   we need this for critical points and phase
   transitions, for natural crashes like earthquakes and avalanches,  for
   polymers and other macromolecules, for strongly correlated electronic
   systems, for quantum field theory, and, now, for deep learning.

   a unifying idea across these systems is the reid172 group (rg)
   theory.

   reid172 group theory is both a conceptual framework on how to
   think about physics on multiple scales as well as a technical
   & computational problem solving tool.

   [28]kenwilson ken wilson won [29]the 1982 nobel prize in physics for
   the development and application of his momentum space rg theory to
   phase transitions.

   we used rg theory[30] to model the recent bitcoin crash as a phase
   transition.

   wilson invented modern multi-scale modeling; the so-called wilson basis
   was an early form of wavelets.  wilson was also a big advocate of using
   supercomputers for solving problems.  being a nobel laureate, he had
   great success promoting scientific computing.  it was thanks to him i
   had access to a[31] cray y-mp when i was in high school because he was
   a professor at my undergrad, the ohio state university.

    here is the idea.  consider a feature map  which transforms the data x
   to a different, more coarse grain scale

   x\rightarrow\phi_{\lambda}(x)

   the rg theory requires that the free energy f(x) is rescaled, to
   reflect that

   the free energy is both size-extensive and scale invariant near a
   critical point

   this is not obvious     but it is essential to both having a conceptual
   understanding of complex, multi scale phenomena, and it is necessary to
   obtain very highly accurate numerical calculations.  in fact, being
   [32]size extensive and/or size consistent is absolutely necessary
   for highly accurate quantum chemistry calculations of strongly
   correlated systems.  so it is pretty amazing but perhaps not surprising
   that this is necessary for large scale deep learning calculations also!

the fundamental reid172 group equation (rge)

   \mathcal{f}(x)=g(x)+\dfrac{1}{\lambda}\mathcal{f}(\phi_{\lambda}(x))

   if we (can) apply the same map, f(x) , repeatedly, we obtain a rg
   recursion relation, which is the starting point for most analytic work
   in theoretical physics.   it is usually difficult to obtain an exact
   solution to the rge (although it is illuminating when possible [20]).

   many rg formulations both approximate the exact rge and/or only include
   relevant variables. to describe a multiscale system, it is essential to
   distinguish between these relevant and irrelevant variables.

example: linear rescaling

   let   s say the feature map is a simple linear rescaling

   \phi(x)=\lambda x

   we can obtain [33]a very elegant, approximate rg solution where f(x)
   obeys a complex (or log-periodic) power law.

   \mathcal{f}(x)\sim x^{-(\alpha+i\beta)}

   this behavior is thought to characterize per-bak style self-organized
   criticality (soc), which appears in many natural systems   [34]and
   perhaps even in the brain itself.   which leads to the argument that
   perhaps deep learning and real learning work so well because they
   operate like a system just near a phase transition   also known as
   [35]the sand pile model--operating at a state between order and chaos.

the kadanoff variational reid172 group (1975)

   [36]kadanoff leo kadanoff, now at the university of chicago, invented
   some of the early ideas in reid172 group.  he is most famous
   for the real space formulation of rg, sometimes called the block spin
   approach.  he also developed an alternative approach, called the
   variational reid172 group (vrg, 1975), which is, remarkably,
   what unsupervised rbms are implementing!

   let   s consider a traditional neural network   a hopfield associative
   memory (ham).  this is also known as an ising model or a spin glass in
   statistical physics.

   an ham consists of only visible units; it stores memories explicitly
   and directly in them:

   [37]hopfield

   we specify the energy     called the hamiltonian \mathcal{h}     for the
   nodes.  note that all the nodes are visible.  we write

   \mathcal{h}^{ham}=-\sum\limits_{i}b_{i}v_{i}-\sum\limits_{i}j_{i,j}v_{i
   }v_{j}

   the hopfield model has only single b_{i} and pair-wise j_{i,j}
   interactions.

   a general hamiltonian might have many-body, multi-scale interactions:

   \mathcal{h}(v)=-\sum\limits_{i}k_{i}v_{i}-\sum\limits_{i}k_{i,j}v_{i}v_
   {j}-\sum\limits_{i,j,k}k_{i,j,k}v_{i}v_{j}v_{k}-\cdots

   the partition function is given as

   \mathcal{z}=\sum\limits_{v}e^{-\mathcal{h}(v)}

   and the free energy is

   \mathcal{f}^{v}=-\ln\mathcal{z}=-\ln\sum\limits_{v}e^{-\mathcal{h}(v)}

   the idea was to mimic how our neurons were thought to store
   memories   [38]although perhaps our neurons do not even do this.

   either way, hopfield neural networks have many problems; most notably
   they may learn spurious patterns that never appeared in the training
   set. so they are pretty bad memories.

   hinton created the modern rbm to overcome the problems of the hopfield
   model.  he used hidden units to represent the features in the data   not
   to memorize the data examples directly.

   [39]rbm2

   an rbm is specified energy function for both the visible and hidden
   units

   \mathbf{e}(v,h)=\mathbf{v}^{t}\mathbf{b}+\mathbf{v}^{t}\mathbf{w}\mathb
   f{h}+\mathbf{c}^{t}\mathbf{h}

   this also defines joint id203 of simultaenously observing a
   configuration of hidden and visible spins

   p(v,h)=\dfrac{e^{-\mathbf{e(v,h)}}}{\mathcal{z}}

   which is learned variationally, by minimizing the reconstruction
   error   or the cross id178 (kl divergence), plus some id173
   (dropout), using greedy layer-wise unsupervised training, with the
   contrastive divergence (cd or pcd) algo,    

   the specific details of an rbm energy are not addressed by these
   general concepts; these details do not affect these arguments   although
   clearly they matter in practice !

   it turns out that

   introducing hidden units in a neural network is a scale
   reid172.

   when changing scale, we obtain an effective hamiltonian
   \tilde{\mathcal{h}}  that acts on a the new feature space (i.e the
   hidden units)

   \mathcal{h}(v)\rightarrow\tilde{\mathcal{h}}(h)

   or, in operator form

   \tilde{\mathcal{h}}(h)=\mathbb{r}[\mathcal{h}(v)]

   this effective hamiltonian is not specified explicitly, but we know it
   can take the general form ([40]of a spin funnel, actually)

   \tilde{\mathcal{h}}(h)=-\sum\limits_{i}\tilde{k}_{i}h_{i}-\sum\limits_{
   i}\tilde{k}_{i,j}h_{i}h_{j}-\sum\limits_{i,j,k}\tilde{k}_{i,j,k}h_{i}h_
   {j}h_{k}\cdots

   the rg transform preservers the free energy (when properly rescaled):

   \mathcal{f}^{h}\sim\mathcal{f}^{v}

   where

   \mathcal{f}^{h}=-\ln\sum\limits_{v}e^{\mathcal{h}(h)}

   \mathcal{f}^{v}=-\ln\sum\limits_{v}e^{-\tilde{\mathcal{h}}(h)}

critical trajectories and renormalized manifolds

   the rg theory provides a way to iteratively update, or renormalize, the
   system hamiltonian.  each time we add a layer of hidden units (h1, h2,
      ), we have

   \mathcal{h}^{rg}(\mathbf{v},\mathbf{h1})=\mathbb{r}[\mathcal{h}(\mathbf
   {v})]

   \mathcal{h}^{rg}(\mathbf{v},\mathbf{h1},\mathbf{h2})=\mathbb{r}[\mathbb
   {r}[\mathcal{h}(\mathbf{v})]

   \cdots

   we imagine that the flow map is attracted to a critical trajectory
   which naturally leads the algorithm to the fixed point.  at each step,
   when we apply another rg transform, we obtain a new, renormalized
   manifold, each one closer to the optimal data manifold.

   graph_fisher

   conceptually, the rg flow map is most useful when applied to critical
   phenomena   physical systems and/or simple models that undergo a phase
   transition.  and, as importantly, the small changes in the data should
      wash away    as noise and not affect the macroscopic / critical
   phenomena. many systems   but not all   display this.

   where hopfield nets fail to be useful here, rbms and deep learning
   systems shine.

   we now show that these rg transformations are achieved by stacking rbms
   and solving the rbm id136 problem!

   kadanoff   s variational reid172 group

   as in many physics problems, we break the modeling problem into two
   parts:  one we know how to solve, and one we need to guess.
    1. we know the hamiltonian at the most fine grained level of scale
       \mathcal{h}(v)
    2. we seek the correlation  \mathbf{v}(v,h) that couples to the
       next level scale

   the joint hamiltonian, or energy function, is then given by

   \mathcal{h}(\mathbf{v,h})=\mathcal{h}(\mathbf{v})+\mathbf{v(v,h)}

   the correlation v(v,h) is defined so that the partition function
   \mathcal{z}  is not changed

   \sum\limits_{h}e^{-\mathbf{v}\mathbf{(v,h})}=1

   this gives us

   \mathcal{z}=\sum_{v}e^{-\mathcal{h}(v)}=\sum\limits_{v}\sum\limits_{h}e
   ^{-\mathbf{v}(v,h)}e^{-\mathcal{h}(v)}

   (sometimes the correlation v is called a transfer operator t,
   where v(v,h)=-t(v,h) )

   we may now define a renormalized effective hamilonian
   \tilde{\mathcal{h}(h)} that acts only on the hidden nodes

   \tilde{\mathcal{h}}(h)=\ln\sum\limits_{v}e^{-\mathbf{v}(v,h)}e^{-\mathc
   al{h}(v)}

   so that we may write

   \mathcal{z}=\sum\limits_{h}e^{-\tilde{\mathcal{h}}(h)}

   because the partition function does not change, the exact rge preserves
   the free energy (up to a scale change, we we subsume into \mathcal{z})

   \delta\tilde{\mathcal{f}}=\tilde{\mathcal{f}}^{h}-\mathcal{f}^{v}=0

   we generally can not solve the exact rge   but we can try to minimize
   this free energy difference.

   what kadanoff showed, way back in 1975, is that we can accurately
   approximate the exact reid172 group equation by finding a lower
   bound using this formalism

   deep learning appears to be a real-space variational rg technique,
   specifically applicable to very complex, inhomogenous systems where the
   detailed scale transformations have to be learned from the data

rbms expressed using variational rg

   we will now show how to express rbms using the vrg formalism and
   provide some intuition

   in an rbm, we simply want to learn the energy function directly; we
   don   t specify the hamiltonian for the visible or hidden units
   explicitly, like we would in physics.  the rbm energy is just

   \mathbf{e}^{rbm}(v,h)=\mathcal{h}^{rbm}(v)-\mathbf{v}(v,h)

   we identify the hamiltonian for the hidden units as the renormalized
   effective hamiltonian from the vrg theory

   \mathbf{h}^{rbm}(h)=\hat{\mathcal{h}}(h)

rbm hamiltonians / marginal probabilities

   to obtain rbm hamiltonians for just the visible \mathcal{h}^{rbm}(v) or
   hidden  \mathcal{h}^{rbm}(h) nodes, we need to integrate out the other
   nodes; that is, we need to find the marginal probabilities.

   p(v)=\sum\limits_{h}p(v,h)=\dfrac{e^{-\mathcal{h}^{rbm}(v)
   }}{\mathcal{z}}=\dfrac{1}{\mathcal{z}}
   \sum\limits_{h}e^{-\mathbf{e(v,h)}}

   \mathcal{h}^{rbm}(v)=-\ln\sum\limits_{h}e^{-\mathbf{e(v,h)}}

   and

   p(h)=\sum\limits_{v}p(v,h)=\dfrac{e^{-\mathcal{h}^{rbm}(h)
   }}{\mathcal{z}}=\sum\limits_{v}\dfrac{e^{-\mathbf{e(v,h)}}}{\mathcal{z}
   }

   \mathcal{h}^{rbm}(h)=-\ln\sum\limits_{v}e^{-\mathbf{e(v,h)}}

training rbms

   to train an rbm, we apply contrastive divergence (cd), or, perhaps
   today, persistent contrastive divergence (pcd).  we can kindof think of
   this as slowly approximating

   \dfrac{\partial}{\partial\theta}\ln\mathcal{z}(\theta)

   in practice, however, rbm training minimizes the associated free energy
   difference \delta\mathbf{f}     or something akin to this   to avoid
   overfitting.

   in the    practical guide to training restricted id82s   ,
   hinton explains how to train an rbm (circa 2011).  section 6 addresses
      monitoring the overfitting   

      it is possible to directly monitor the overfitting by comparing the
   free energies of training data and held out validation data   if the
   model is not overfitting at all, the average free energy should be
   about the same on training and validation data   

other objective functions

   [41]modern variants of real space vrg are not        forced    to minimize
   the global free energy    and have attempted other approaches such
   as tensor-svd reid172.  likeswise, some rbm / dbm approaches do
   likewise may minimize a different objective.

   in some methods, we minimize the kl divergence; this has a very natural
   analog in vrg language [1].

why deep learning works: lessons from theoretical physics

   the reid172 group theory provides new insights as to why deep
   learning works so amazingly well.  it is not, however, a complete
   theory. rather, it is framework for beginning to understand what is an
   incredibly powerful, modern, applied tool.  enjoy!

references

   [1] [42]an exact mapping between the variational reid172 group
   and deep learning, 2014

   [2] [43]variational approximations for reid172 group
   transformations, 1976

   [3]  [44]a common logic to seeing cats and cosmos

   [4] [45]why does unsupervised deep learning work?     a perspective from
   group theory, 2015

   [5] [46]a fundamental theory to model the mind

   [6] [47]a practical guide to training restricted id82s,
   2010

   [7] [48]on the importance of initialization and momentum in deep
   learning, 2013

   [8] [49]dropout: a simple way to prevent neural networks from
   overfitting, 2014

   [9] [50]ken wilson, a scientific appreciation

   [10] http://www-math.unice.fr/~patras/cargeseconference/acqft09_jzinnju
   stin.pdf

   [11] [51]training products of experts by minimizing contrastive
   divergence, 2002

   [12] [52]training restricted id82s using approximations to
   the likelihood gradient, 2008

   [13]
   [53]http://www.nonlin-processes-geophys.net/3/102/1996/npg-3-102-1996.p
   df

   [14] [54]the reid172 group and critical phenomena, ken wilson
   nobel prize lecture

   [15] [55]scaling, universality, and reid172: three pillars of
   modern critical phenomena

   [16] [56]the potential energy of an autoencoder, 2014

   [17] [57] contractive auto-encoders: explicit invariance during feature
   extraction, 2011

   [18] [58]stacked denoising autoencoders: learning useful
   representations in a deep network with a local denoising criterion,
   2010

   [19] [59]quora:  what is reid172 group theory?

   [20] [60]reid172 group and critical localization, 1977

share this:

     * [61]twitter
     * [62]facebook
     * [63]linkedin
     * [64]more
     *

     * [65]reddit
     * [66]email
     *
     *

like this:

   like loading...

related

   [67]deep learning

post navigation

   [68]previous post: why does deep learning work?
   [69]next post: when id173 fails

77 comments

    1.
   [70]tigerneil says:
       [71]april 9, 2015 at 10:16 am
       reblogged this on [72]do not stop thinking.
       [73]likelike
       [74]reply
    2. pingback: [75]why deep learning works ii: the reid172 group
       | dinesh ram kali.
    3.
   [76]wubr2000 says:
       [77]april 10, 2015 at 12:08 am
       reblogged this on [78]importantish.
       [79]likelike
       [80]reply
    4.
   superfluffy says:
       [81]april 12, 2015 at 2:54 pm
       (let me prephrase that i am not sure if latex / mathjax works in
       the comment section; i am just going to type it out so that you
       might copy-paste the equations into your post, in case my stuff is
       actually correct   .):
       you write for the    renormalized hamiltonian    acting only on the
       hidden units:
       \[ \tilde{\mathcal{h}}(v) = \sum_v \mathrm{e}^{-v(v,h)    
       \mathcal{h}(v)} \]
       i believe, \(\tilde{\mathcal{h}}(v)\) is a typo and should be
       \(\tilde{\mathcal{h}}(h)\) (since we are integrating out the
       visible degrees of freedom).
       next, i wonder about the definition as such: in order for the
       partition function to be unchanged, shouldn   t the definition of
       \(\tilde{\mathcal{h}}(h)\) involve a logarithm? i.e.:
       \[\tilde{\mathcal{h}}(h) \rightarrow \tilde{\mathcal{h}}(h) = \log
       \sum_v \mathrm{e}^{-v(v,h)     \mathcal{h}(v)}\]
       this way we exactly get \( \mathrm{e}^{\tilde{\mathcal{h}}(h)} =
       \sum_v \mathrm{e}^{-v(v,h)     \mathcal{h}(v) \), and we can plug
       this directly into the partition function.
       [82]likelike
       [83]reply
         1.
        charles h martin, phd says:
            [84]april 12, 2015 at 3:53 pm
            thanks for reading
            those are typos
            you are awesome !
            [85]likelike
            [86]reply
    5.
   mattxavier says:
       [87]may 13, 2015 at 10:01 pm
       this is great! i was asking around a couple months ago for some
       kind of explanation of the connection.
       i   ll read up on this!
       [88]likelike
       [89]reply
    6. pingback: [90]1     why deep learning works | offer your
    7. pingback: [91]tech news / why deep learning works ii: the
       reid172 group
    8. pingback: [92]mathanalytics (2)     learning by zooming out |
       datawarrior
    9. pingback: [93]short honk: reid172 group equation : stephen
       e. arnold @ beyond search
   10. pingback: [94]bookmarks for july 8th | chris's digital detritus
   11.
   annevanrossum says:
       [95]august 1, 2015 at 11:51 am
       it seems however that deep learning networks do not have the most
       essential property of such a physical system. the hidden states in
       a sandpile are not some    higher-level    sand grains. they are
       macroscopic observables. a deep learning network, however, does
       explicitly represent the hidden states by neurons. i blame it on
       its non-recursive structure.
       what i don   t understand is how the    full connectivity    between a
       layer and its hidden representation (the next layer) does represent
       the locality as in block reid172. it is fully connected, so
       why would it only get rid of local correlations? the coupling in
       spin lattices is normally only between neighbours.
       the reid172 group has a fixed point when we do the mapping
       an infinite number of times. does this mean the network converges
       only with an infinite number of layers? i think such a    rolled out   
       version is hardly exhibiting    self-organized criticality   , that is,
       if scientist would agree on a definition of soc :-).
       imho a single recurrent layer can exhibit renomalization group
       properties in a more fundamental way.
       or am i misunderstanding everything here? forgive my ignorance.
       some explanations would be really appreciated!
       [96]likelike
       [97]reply
         1.
        charles h martin, phd says:
            [98]august 1, 2015 at 12:47 pm
            thanks for stopping by and reading my post
            i think, as with lots of complexity theory, we rely a lot upon
            analogy. recall that here we are trying to learn the
            parameters of the network / ising model, so we are doing a
            kind of inverse statistical mechanics, as opposed to proposing
            a symmetric, uniform random distribution of couplings. it is
            known that , during the learning phase of an rbm, it is
            necessary to either run a few steps of cd or run pcd. that is,
            1 step of cd, then 3 steps of cd, etc. this suggests to me
            that the system is evolving along something akin to a flow
            map, and that we can only invert the equations exactly when we
            are near the critical point. as we add more layers, we are
            effectively adding 1 more iteration of the rg group, as
            opposed to solving it exactly for all iterations.
            so, as in the picture, there are 2 lines of flow: the progress
            learning, and the number of rg transforms applied.
            why do we only keep local interactions? well, to me this s
            analogous to the electron correlation problem in quantum
            chemistry. we usually start with some hartree fock (i.e. mean
            field, product of experts) solution, and then add in
            correlations vi some perturbation / reid172 steps. the
            rbm model is an analogous products of experts solution, which
            we can solve easily. does this mean we have to remove all
            local correlations   well there are lots of details necessary to
            prevent the system from collapsing. indeed, i suspect that the
            rbm state, along with id173 and dropout, prevent the
            model from collapsing into a spin glass ground state. here, i
            am at odds with lecun, who thinks that the reason the deep
            learning systems work is exactly because they are a spin
            glass. of course, removing these connections may simply be
            that there are too many of them and the system overtrains,
            spin glass or not.
            imho, this needs more investigation. and i agree with the id56
            comment
            finally, i want to pose the following question. suppose we run
            an unsupervised id91 algorithms on some data, twice but
            w/ different parameters. which set produces better clusters?
            most algos have no answer, and this is a central, very
            practical problem in basic ml. here, i think we could say the
            better set is the one that more closely conserves the free
            energy between layers. this is a property of the rg approach,
            or, in quantum chemistry, is related to what we call
            size-consistencey. this allows us to search a massive space of
            parameters     which is impossible for general non-convex
            searches. and i suspect this is a key idea as to why these
            methods perform so well.
            [99]likelike
            [100]reply
              1.
             annevanrossum says:
                 [101]august 1, 2015 at 2:11 pm
                 maybe i am a bit slow. but to apply block reid172
                 in the network (n-dimensional spin lattice):
                 * what are the neighbours of a node?
                 * what exactly is the decimation step?
                 i don   t understand the exact equivalence.
                 regarding number of clusters, i   m normally following a
                 bayesian approach and would just consider a prior on the
                 distribution of items over clusters, using something like
                 a dirichlet process or pitman-yor, etc. if this
                 information is not available, i can understand that
                 you   ll have to use something that just relates
                 distributions over states on two different levels of
                 abstraction. free energy of some kind of quantity is
                 probably involved.     
                 with respect to dropout i recommend    dropout as a
                 bayesian approximation: insights and applications    by gal
                 and ghahramani. the relation with gaussian processes
                 promises a dropout method for id56s as well!
                 [102]likelike
                 [103]reply
                   1.
                  stephane says:
                      [104]march 27, 2017 at 9:31 am
                      my late (2 years later) opinion on this:    * what are
                      the neighbours of a node?    and    the problem is that
                      the local connection should be there from the
                      start.   .
                      it is not very clear that you need a strong
                      definition of neighbours or locality. in physics
                      this happens in disordered media for instance, in
                      which you can have long range interactions because
                      of the disorder. in this situation the locality does
                      not mean much. indeed if you construct clusters from
                      spins that are spatially close they may not be
                      interacting because of the disorder. therefore your
                      decimation process is locally equivalent to a
                      decimation on a free non interacting model (which is
                      not interesting). it is probably much more
                      interestng to cluster together degree of freedom
                      (dof) that are strongly interacting (eventhough they
                      may be spatially far). actually the local id91
                      is natural in local field theory (ft), but it is not
                      obvious that it is needed. actually it is not, at
                      least in some cases i know about.
                      you could also consider ising model on arbitrary
                      graphs instead of a lattice and you still expect
                      that reid172 works (it probably exists in
                      the physics literature already, i know about the
                      random planar maps case, the random non-planar case
                      should exist as well, but the key word    random   
                      simplifies things a lot here).
                      then if you follow my point above i would say that
                      you can create    natural    clusters of neighbours by
                      looking at the edge weight magnitudes between
                      layers. lets say you have a layer i with 6 nodes and
                      a layer i+1 with 3 nodes. then say that nodes 1,2,3
                      of the ith layer have all high values of their
                      weight magnitude towards node 1 of layer i+1, while
                      nodes 4,5,6 of layer i have only very small (in
                      magnitude) weights to node 1 of layer i+1. then i
                      would say that node 1,2,3 are clustered together and
                      decimated by node 1 of layer i (because the
                      magnitude of their weights towards the same node 1
                      are comparable in magnitude). of course this would
                      mean that some dof will be taken into account more
                      than once in the decimation process while some dof
                      will simply be forgotten. but intuitively i feel
                      that it still could lead to a decent reid172
                      process (as long as the over\under-counting of dof
                      is not too big, which means, i guess, that the
                      weight matrix has to be neither too sparse nor too
                      diffuse, so this relates to the choice of
                      id173 of the nn. strangely enough this
                      makes me think about another picture in
                      wetterich-like reid172).
                      dropout in this context might be seen as a way of
                      averaging different reid172 processes in
                      order to enhance the id203 of following a
                         universal flow trajectory    that leads to an
                      interesting fixed point .
                      but anyway, i still think this is only a very very
                      vague connection to reid172 and these ideas
                      should be investigated rigorously before claiming
                      that nn are just performing reid172 (that is
                      why the    exact mapping    paper title choice looks
                      like pure marketing to me).
                      [105]likelike
                      [106]reply
                        1.
                       charles h martin, phd says:
                           [107]march 27, 2017 at 12:44 pm
                           the main contribution is that vrg approach by
                           kadanoff resembles an rbm or dbm.
                           i agree, there is no discussion about
                           decimation, or the fact that these are mean
                           field methods.
                           i make a simple observation about free-energy
                           based deep nets, like rbms and dbms.
                           the idea is, when defining the total free
                           energy, one should also consider the constraint
                           on the correlation operator v(h,v) such that
                           the partition function is not changed. that is,
                           in an rbm or dbm, we can try to define a free
                           energy per layer, and ensure that the layer
                           free energies are conserved. i don   t think
                           anyone has tried this. the closest thing i know
                           to this is bath id172 (for supervised
                           nets). i don   t know if they are the same.
                           [108]likelike
                           [109]reply
   12.
   charles h martin, phd says:
       [110]august 2, 2015 at 2:45 am
       1. the variational approach is different from the real space
       decimation approach; roughly, the decimation step is like adding a
       layer
       2. what happened to the local connections ? in a hopfield network,
       all nodes are visible. in an rbm, there visible nodes are connected
       through the hidden layer, but not directly. this does not mean the
       visible nodes are not connected. to see this, one would need to
       construct an effective hamiltonian for the visible nodes that
       includes the hidden interactions as renormalized visible-visible
       interactions.
       thanks for the references on the bayesian methods   ill check them
       out.
       [111]likelike
       [112]reply
         1.
        annevanrossum says:
            [113]august 2, 2015 at 9:04 pm
            1. in a sandpile this would mean that all grains are connected
            to all grains on the next level. then there would be some
            process (an id136 process in the network). and magically,
            there will be local structure arising. and hey, this is a
            variational approximation to real-space reid172.
            that   s not how it works    in the words of the authors:
               surprisingly, this local block spin structure emerges from
            the training process, suggesting the dnn is self-organizing to
            implement block spin reid172   . i bet if you use
            textures as input of the network there is no such local
            structure arising. and local structure could just as well have
            originated from the ad-hoc applied l1 id173 (see
            appendix).
            2. the problem is that the local connection should be there
            from the start. in normal reid172 group approaches
            large-scale correlations arise from short-scale correlations.
            here the authors start with a restricted id82:
            hidden nodes are connected to all visible nodes. newman and
            watts have a reid172 group on a small-world network,
            but there the decimation operator is quite different. that   s
            why i asked what the exact decimation operator is. it is not
            in the paper, because it    emerges   . miracle!
            if i may, let me ask one more detailed question   
            equation 10, where is it actually used? does the form of the
            network matter? i think if eq. 10 would describe a black hole
            the rest would still stand, saying something very trivial.
            namely that you can minimize the kl divergence between two
            layers in a network and that this can be described by
            hamiltonians.
            [114]likelike
            [115]reply
   13.
   charles h martin, phd says:
       [116]august 3, 2015 at 1:00 am
       the form of the network     eg 10     is not used in this derivation.
       i personally don   t know how important this is.
       i have a more of experience with large scale perturbative
          reid172    techniques from quantum chemistry. to me, these
       systems more resemble machine learning systems since they are large
       but finite, highly non-uniform, and not amenable to exact
       solutions. also, it is not clear that we can apply rg arguments
       directly since we have to consider that we are learning the weights
       (i.e the h-v couplings)
       i would say that
       1. does eq 10 really matter? this is a key question
       well, generally speaking, it is necessary to choose the correct
       zeroth order description, but also one that is amenable to
       computational exploration
       the rbm is also convenient computationally and it is unclear to me
       if similar results could be obtain using a formalism that includes
       local correlations directly. perhaps adding them early on would
       improve the learning rate, or perhaps they would screw it up and
       cause it to overtrain?
       2. removing local interactions may be corrected , in many
       situations, by simply to run higher order calculations.
       if h= ho+v , we can pick ho to be diagonal in a localized basis (so
       there are no local interactions), and the off-diagonal parts just
       go into the interaction v.
       that is, as we learn the weights, i kinda assumed the rbm would be
       a mean field approximation and effectively include the local
       interactions this way
       i used this method (mean-field like h0, without overlap between
       sites) when computing effective hamiltonians for pi-electron
       systems
       [117]http://pubs.acs.org/doi/abs/10.1021/jp960617w?journalcode=jpch
       ax
       in an rg formalism this might mean taking a higher order
       approximation to the rg operator. in a dbn, this might mean adding
       more layers, running more epochs, etc.
       3. (1,2) , plus the danger of overtraining, implies to me that we
       may not / probably do not want to solve an exact rge
       exact rg calculations can lead to funny behavior and the details of
       the system do matter see, for example
       [118]http://journals.aps.org/prb/abstract/10.1103/physrevb.15.4476
       so using variational rg is fine
       so i   m not sure what the beef is here? or if this was a beef?
       it is true that the comparison is only partial; we set up the vrg
       formalism and apply the idea of finding a lower bound. what we can
       not do here is analyze the fixed point of the rg flow map using any
       kind of analytic theory. moreover, this is the other    reaction
       coordinate   , which is the learning process. so it   s not like we are
       applying the rg transform, or the decimations, iteratively over and
       over   we have maybe a few layers, which corresponds to a few
          decimations    (if you want to call them that), and the    flow map   
       is really saying that we are trying to find the hamiltonian that is
       somewhere near the fixed point of the learning process, and not the
       fixed point of the rg iterations. instead, what i assume is that
       when we get close to the the learned solution, the few
          decimations    are basically at the fixed point, so we only need a
       few to get a good solution.
       4. do we need to have local correlations explicitly in the visible
       layer to obtain long range correlations?
       this is a good point and certainly something that the authors did
       not discuss.
       it is not obvious that a simple argument by analogy would be
       sufficient to resolve this
       these are highly non-uniform systems and it is unclear to me what
       the effective interactions look like
       so i assume that as we learn the rbm h-v couplings, they would
       effectively include local interactions.
       i think to understand the rbm system in terms of classical spatial
       models it is necessary to actually compute the, effective rbm
       hamiltonian and the associated renormalized manny-body
       interactions.
       i assume the interactions of the visible units would give rise to a
       multi-scale spin hamiltonian, of the kind suggested that arises in
       protein folding
       [119]https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep
       -learning-work/
       to me, this is the interesting connection, but there remain a lot
       of un-answered questions.
       in particular, what happens at the bottom of the funnel? is it like
       a spin glass? is it like the native state of a protein?
       is the learning process driven by replica symmetry breaking? or
       does this lead to overtraining?
       5. as to the paper itself   i think the authors recognized this old
       connection, and the knocked out some simple simulations so they
       could write a paper (as opposed to a blog post ). the simulations
       were interesting but they did not provide explicit code. i would
       say   lets try to reproduce it and add some textures
       [120]likelike
       [121]reply
         1.
        annevanrossum says:
            [122]august 9, 2015 at 1:11 pm
            hi charles, i   ve now tried several times to read your paper
            about obtaining the pi-electron hamiltonians from scratch, but
            no way i   m gonna be able to understand it.     
            your suggestion about reproducing it with textures is a nice
            one. i don   t think i can do it coming month, but maybe i   ll
            have some time next month.
            [123]likelike
            [124]reply
              1.
             charles h martin, phd says:
                 [125]august 9, 2015 at 6:17 pm
                 ah so let me clarify   that paper deals with effective
                 hamiltonians for zero-t, electronic structure problems. i
                 just share where my thinking is on the problem today   a
                 lot of which might sound like a proposal for a phd
                 candidate program
                 one can think of the hopfield net / ham as the zero-t
                 limit of the boltzman net.
                 [126]https://www.cs.toronto.edu/~hinton/csc321/readings/b
                 oltz321.pdf
                 generally speaking, the bolztman net did not work well at
                 scale, even with hidden units, when it was fully
                 connected.
                 so we have 2 problems to deal with. !   should we build an
                 effective hamilonian for the zero-t case, or work at
                 finite t and try to understand the spin-glass transition
                 the ham runs hebbian (zero-t) learning and it appears to
                 have all sorts of problems.
                 i am kinda curious what happens if we introduce hidden
                 nodes into this, apply something like dropout, and still
                 run hebbian learning, to fix some of the old problems in
                 hopfield and/or boltzmann nets.
                 if we just look at a ham, this is a backwards way of
                 looking at it   one usually first introduces t, then the
                 hidden units access additional microstates via the
                 id178. in quantum mechanics, of course, the hidden
                 states are virtual states   accessible by diagonalizing the
                 energy operator. we dont even have an operator   we just
                 have the ground state semi-classicial e(v) or e(h,v)
                 still, there is precedence a for a zero-t, renormalized
                 ham; the schulten neural gas is a like a simple,
                 renormalized ham that uses hebbian (zero-t) learning
                 [127]http://www.ks.uiuc.edu/publications/papers/pdf/mart9
                 1b/mart91b.pdf
                 of course, it is one thing to propose a phenomenological
                 correction to a model; it is something else
                 entirely to compute the reid172 corrections
                 exactly. the paper i shared does this, and it requires
                 some pretty heavy quantum field theory to get this right
                     at least in the pure quantum mechanical sense (where
                 that old paper of mind comes in)
                 at one point i started a blog on the old approach
                 [128]https://charlesmartin14.wordpress.com/2012/09/21/eig
                 envalue-independent-effective-semantic-operator/
                 but it is still a bit jumbled.
                 here, we don   t really have a quantum ground state, and
                 the intent is different, so i would need to think a bit
                 more about how to do this numerically, and whether it is
                 possible to derive a form that allows learning of the
                 weights using a hebbian rule.
                 so instead (or say after doing this), we need to consider
                 the finite t / stochastic boltzmann net. can we construct
                 from e_rbm(v,h)   >e   _rbm(v). my idea is that the final
                 e   _rbm(v) and associated (unrestricted) id82
                 would be connected very differently than a vanilla, fully
                 connected ham-style id82.
                 this is where the variational rg step comes in, and i was
                 hoping one could just compute e(v) from the equations in
                 the paper and on fully trained rbm.
                 indeed, it is understood that a single layer rbm is like
                 a hopfield net
                 [129]http://arxiv.org/pdf/1105.2790v1.pdf
                 the question, what happens with multiple stacked layers,
                 convolutional layers, etc
                 that is, if we consider an rbm = stochastic hopfield net,
                 with connectivity through the hidden nodes
                 then i expect we can construct a renormalized hopfield
                 net, with no hidden nodes, and an energy function with
                 long range, multi-scale interactions.
                 the zero-t limit would look like an ising model with
                 complex, non-uniform, many-body (3,4-body) interactions
                 or course, this is final energy, and we also want to
                 understand the dynamics during learning. it is unclear if
                 such a machine could naively be trained using the
                 original id82 style hebbian learning, or
                 maybe just backprop+sgd
                 so we would want some form of e(v) that is    learnable   .
                 in spin physics one might just talke the
                 derived/calculated final e(v) and study the    non-eq
                 behavior    using something formal like glauber dynamics or
                 some other traditional approach. that is, we study small
                 deviations from the eq state by taking spin flips. for
                 learning,i think this is similar to sampling one node at
                 a time in a boltzman net. this is known to be awful, and
                 in ml we usually don   t something like wake-sleep,
                 backprop, etc.
                 [130]http://www.gatsby.ucl.ac.uk/~dayan/papers/hdfn95.pdf
                 also, learning is probably very far from eq and this kind
                 of approach may be very bad. in fact, if the spin glass
                 idea is right, then this could never work.
                 i would be happy to just find a way to compute the energy
                 landscape with bare and renormalized e(v)
                 then, we we find a spin funnel, there is the question of
                 what happens at the bottom of the funnel
                 do we encounter a spin-glass transition, or is there
                 enough id178 in the system to avoid that?
                 now as to doing something practical   what i think might
                 actually be useful is to compute the free energy at each
                 layer and study the convergence. i am thinking that this
                 could be a good metric for training an unsupervised rbm /
                 dl algo. i finally got torch up and running on a gpu and
                 i started looking at some of the old papers like
                 [131]http://www.cs.toronto.edu/~hinton/science.pdf
                 i would like to run the lua torch rbm toolbox on this
                 example and reproduce it   any idea if there is a worked
                 example?
                 [132]likelike
                 [133]reply
   14.
   [134]jordan micah bennett says:
       [135]august 14, 2015 at 3:29 am
       a quite intriguing   postulation    for any sufficiently
       non-gratuitous learning mechanism shall express rather fundamental
       equation sequences   
       [136]likelike
       [137]reply
   15.
   [138]jordan bennett says:
       [139]august 14, 2015 at 3:30 am
       a quite intriguing   postulation    for any sufficiently
       non-gratuitous learning mechanism shall express rather fundamental
       equation sequences   
       [140]likelike
       [141]reply
   16.
   openbg@abv.bg says:
       [142]october 22, 2015 at 8:46 pm
       i want to ask what is free energy ?
       please explain with few words why deep learning works in context of
       variational reid172 group? as far as i understand it scale
       the landscape and the model converge to the fixed point? do we have
       some proof that this is global optimum ?
       thanks
       [143]likelike
       [144]reply
         1.
        charles h martin, phd says:
            [145]october 23, 2015 at 8:49 pm
            the free energy is defined in the blog as the negative log of
            the partition function.
            im not sure what the question here is ?
            why is there a global optimum..please see my previous post
            which discusses this
            [146]https://charlesmartin14.wordpress.com/2015/03/25/why-does
            -deep-learning-work/
            why is this related to variational reid172 group? the
            basic conjecture is that a good model for why dl works is a
            model of a hopfield-style spin glass that is being sampled at
            a subcritical point   a point below the
            spin glass phase.
            note this is a different conjecture than what lecun suggests
            is that dl is operating inside the spin glass phase.
            see
            [147]https://drive.google.com/file/d/0bxkbnd5y2m8nbwn6xzm5uxkw
            nda/view?pli=1, and his slides on the geometry of the loss
            function
            a with many things in condensed matter theory   this conjecture
            is based on analogies. here, i note
            rbms are analogous to vrg and i assume the fixed point of the
            rbm flow map is something akin to the fixed point of the rg
            flow map
            the analogy here is a bit crude because in rg theory, one
            usually assumes the hamilonian is fixed, whereas in dl, the
            hamiltonian is being learned as part of the flow map. so i am
            conjecturing that the fixed point of the fixed hamiltonian
            flow map, the stable attractor, is also a stable attractor of
            the varying hamiltonian.
            why might we believe this? well, it depends on if you believe
            in the rb / kadanoff idea of universality and it applies here
            in some , perhaps crude, sense. if universality is a good
            approach , then the    details    of the hamiltonian (i.e the
            weights) would not really change the underlying description we
            are close enough to the critical point. so if we can get
            close, we would converge quickly.
            [148]likelike
            [149]reply
   17. pingback: [150]distilled news | data analytics & r
   18.
   amador muriel says:
       [151]november 22, 2015 at 1:50 pm
       please give an example of success with deep learning.
       [152]likelike
       [153]reply
         1.
        charles h martin, phd says:
            [154]january 17, 2017 at 9:53 am
            example of what ? rg theory ?
            [155]likelike
            [156]reply
   19. pingback: [157]when id173 fails | machine learning
   20. pingback: [158]pourquoi l   apprentissage profond et les r  seaux
       neuronaux sont-ils si prometteurs ? | thibaut cuvelier
   21.
   openbg@abv.bg says:
       [159]february 8, 2016 at 9:22 pm
       hi charlers,
       i would like to ask am i following the idea for critical points    
       when we train ta rbm with id150 the configuration of the
       rbm is moved down on the variation manifold (the 4 image sample) to
       the critical point     where the network reach equilibrium and
       converge. is that correct?
       what kind of image model exists at the critical equilibrium point    
       the average structure of the image or something else ?
       thank you!
       [160]likelike
       [161]reply
         1.
        charles h martin, phd says:
            [162]february 17, 2016 at 9:34 am
            yes, it moves along the manifold. this is the intent of the
            discussion
            still.
            we should first understand that this is a model by analogy, as
            in most of condensed matter physics. indeed, most practical
            models are very complex. given that, and assuming it is a good
            analogy, is not clear that an rbm actually samples at a
            critical point since it is a variational lower bound to the
            free energy at the critical point. additionally, the presence
            of dropout may move the system to something like a subcritical
            point that avoids overtraining. also, many systems are run
            with a finite number of steps, meaning that we may not be near
            convergence or even on the true statistical manifold. as to
            the second question, i don   t understand the question. the
            model is the model
            [163]likelike
            [164]reply
              1.
             ivan says:
                 [165]february 17, 2016 at 9:48 am
                 thank you very much!
                 the second question is what kind of image emerge on the
                 critical point     but since it is the max likelihood
                 learning it should be the most distribution with high
                 id203.
                    also, many systems are run with a finite number of
                 steps, meaning that we may not be near convergence or
                 even on the true statistical manifold.        is that the
                 case with contrastive divergence learning     where we do
                 id150 only few steps? in that case we work with
                 approximate energy landscape     in my opinion the energy
                 function is shallow than the landscape which is converged
                 at critical point.
                 two very more questions     what is dropout and
                    is not clear that an rbm actually samples at a critical
                 point since it is a variational lower bound to the free
                 energy at the critical point        the variational lower
                 bound during id150 may not reach critical point
                     the critical point might be lower than variational
                 lower bound?
                 10x very much
                 [166]likelike
                 [167]reply
                   1.
                  charles h martin, phd says:
                      [168]february 17, 2016 at 5:27 pm
                      the rbm is the lower bound.
                      the intent here is to suggest that the free energy
                      at every layer should be conserved during training.
                      that is, it remains size consistent during the
                      training phase. as for the surface itself, i suspect
                      it is fairly convex most of the way until we reach
                      the bottom.
                      the best thing would be just to compute these things
                      and look,
                      [169]likelike
                      [170]reply
   22.
   openbg@abv.bg says:
       [171]february 17, 2016 at 10:43 pm
       10x, 10x      ?
       so if the free energy for each layer is conserved what about the
       lower bound of variation free energy    
       as hinton shows:
          so maximizing the bound w.r.t. the weights in the higher layers
       is exactly equivalent to maximizing the log id203 of
       a dataset in which h0 occurs with id203 q(h0|v0). if
       the bound becomes tighter, it is possible for log p(v0) to fall
       even though the lower bound on it increases, but log p(v0)
       can never fall below its value at step 2 of the greedy algorithm
       because the bound is tight at this point and the bound
       always increases.   
       the lower bound of variation free energy is increased but the free
       energy (itself) is preserved ?
       so as far as i understand the at each layer the energy is preserved
       but he energy landscape is rescale     we see the varational
       landscape in high abstraction level     only the deeper points are
       observed and the details are omitted. we just see the higher
       picture of the manifold     am i get the main idea?
       thank you very much!
       [172]likeliked by [173]1 person
       [174]reply
         1.
        openbg@abv.bg says:
            [175]february 21, 2016 at 11:36 am
            can i know is my view on the reason for deep learning to works
            is true ?
            [176]likelike
            [177]reply
              1.
             charles h martin, phd says:
                 [178]february 21, 2016 at 1:10 pm
                 you stated    
                    so if the free energy for each layer is conserved what
                 about the lower bound of variation free energy    
                 and
                    the lower bound of variation free energy is increased
                 but the free energy (itself) is preserved    
                 i don   t understand the question? what hinton paper are
                 you referencing?
                 let us just go back to the hinton paper in science, 2006
                    it can be shown that adding an extra layer always
                 improves a lower bound on the log id203 that the
                 model assigns to the training data, provided the number
                 of feature detectors per layer does not decrease and
                 their weights are initialized correctly   
                 [179]http://www.cs.toronto.edu/~hinton/science.pdf
                 of course, this is just saying that the rbm algorithm is
                 a variational lower bound. the point here being that a
                 very similar algorithm was first developed by kadanoff
                 for rg theory   10 years earlier.
                 also note
                    this bound does not apply when the higher layers have
                 fewer feature detectors   
                 so to get the proper lower bound you have to be careful
                 how you set up the problem. in any real implementation,
                 there are so many heuristics and modifications that a
                 specific rbm implementation may not provide an exact
                 lower bound. if you want a true lower bound to any
                 problem you have to be careful. even numerical
                 instabilities could affect this.
                 as to the statement
                    the free energy (itself) is preserved ?   
                 i don   t know what this means?
                 what i said was one expects the free energy of each layer
                 to be related to the other layers via a scale
                 transformation, and that this should hold throughout the
                 optimization procedure. this would be the intent when
                 constructing a renormalized effective hamiltonian. now
                 does it hold true for any specific implement, or does the
                 implementation break this    size consistency   . does this
                 hold in practice? who knows that the code is doing   you
                 would need to run a simulation and check.
                 i have no idea what it means to    preserve    the total free
                 energy? this has meaning a physical system (i.e. a
                 chemical reaction) because energy can be lost to the
                 environment, through dissipation, etc. what do you mean?
                 [180]likelike
                 [181]reply
                   1.
                  ivan says:
                      [182]february 23, 2016 at 12:52 am
                      hi charles,
                      my question was     when adding additional layer on
                      deep belief net do we get high abstraction view of
                      the same energy landscape ? is the    scale
                      transformation    is just getting more abstract
                      (without details of the small local optima) view of
                      the energy landscape. with that renationalized high
                      level view and tweaking the step of gradient descend
                      we can get the global minimum of the energy
                      landscape. this is my understanding about how the
                      the energy surface between the layer is transformed
                      and why we get global minimum with gradient descend
                          so it is that correct ?
                      could you recommend me some book (or tutorial) about
                      rbms, stacking rbm and deep learning ?
                      thank very much!
                      [183]likelike
                      [184]reply
                        1.
                       charles says:
                           [185]february 23, 2016 at 12:56 am
                           no, it is just not an another abstract
                           representation of the energy landscape.
                           as we add additional layers, the energy
                           landscape is further refined, or
                              renormalized   . in a sense, each layer is like
                           taking 1 more iteration of a reid172
                           group transformation. i suspect this is can
                           help explain very deep nets work better than
                           shallow nets.
                           in an earlier post i explained some old ideas
                           of why the sgd may work   the final energy
                           landscape resembles what is called in protein
                           folding the    spin glass of minimal
                           frustration   .
                           i don   t know of any books on this   these are my
                           ideas.
                           [186]likelike
                           [187]reply
                             1.
                            openbg@abv.bg says:
                                [188]february 23, 2016 at 1:16 pm
                                great! it will be very good if we get some
                                tutorial on that topic.
                                it will b very useful for me to provide me
                                some intuition for the renormalized energy
                                surface    
                                   as we add additional layers, the energy
                                landscape is further refined, or
                                   renormalized   . in a sense, each layer is
                                like taking 1 more iteration of a
                                reid172 group transformation.   
                                what the reid172 group
                                transformation do with the energy
                                landscape     is there some image or video
                                to get intuition about that process ?
                                thank you very much!
                                [189]likelike
                                [190]reply
                                  1.
                                 openbg@abv.bg says:
                                     [191]february 25, 2016 at 1:32 pm
                                     do we get better energy surface that
                                     is near to the real energy surface    
                                     the same process as improving the
                                     variation low bound ?
                                     [192]likelike
                                     [193]reply
                                  1.
                                 charles h martin, phd says:
                                     [194]february 25, 2016 at 6:59 pm
                                     better than what ?
                                     [195]likelike
                                     [196]reply
                                  1.
                                 openbg@abv.bg says:
                                     [197]february 26, 2016 at 4:09 pm
                                     better than the previous (lower
                                     level) variation approximation     by
                                     adding new layer we increase the low
                                     variation bound and we get better
                                     approximation of true log
                                     id203. so my question is what
                                     is the effect on the energy landscape
                                     when    taking 1 more iteration of a
                                     reid172 group transformation   
                                     ? i read that some kind of rescaleing
                                     is happen but could you give more
                                     information ..,
                                     [198]likelike
                                     [199]reply
                                  1.
                                 charles h martin, phd says:
                                     [200]february 26, 2016 at 4:56 pm
                                     i conjecture that effect of the
                                     renormalizing the energy landscape is
                                     to make it more convex.
                                     this is based on observations about
                                     the energy landscape of random
                                     heteropolymers. see part 1 of the
                                     blog.
                                     [201]likelike
                        2.
                       [202]jozsef hegedus says:
                           [203]september 4, 2016 at 11:33 am
                           ivan, it seem that there is no problem with
                           local minima : see 40-42 min of
                           [204]https://www.youtube.com/watch?v=74vux2zszm
                           s&t=651s
                           [205]likelike
                           [206]reply
                             1.
                            charles h martin, phd says:
                                [207]september 5, 2016 at 8:53 am
                                yeah this has been known from spin glass
                                theory like forever. see the discussion in
                                this book:
                                [208]https://books.google.com/books?id=58i
                                m6efkx9cc&pg=pa130&lpg=pa130&dq=solutions+
                                of+the+tap+equations+spin+glasses&source=b
                                l&ots=zc15gav107&sig=7u5k-m2c92d-zrwf0kl4v
                                8pkb80&hl=en&sa=x&ved=0ahukewin8zek2pjoahv
                                s4mmkhrzkblaq6aeiwtaj#v=onepage&q=solution
                                s%20of%20the%20tap%20equations%20spin%20gl
                                asses&f=false
                                [209]likelike
                                [210]reply
                             2.
                            charles h martin, phd says:
                                [211]september 5, 2016 at 11:37 am
                                see:
                                [212]http://www.kdnuggets.com/2016/07/deep
                                -learning-networks-scale.html
                                and the reference to duda and stork
                                [213]https://books.google.com/books?id=br3
                                3irc3pkqc&pg=pa299&lpg=pa299&dq=section+6.
                                4.4.+pattern+classification&source=bl&ots=
                                2wcviw9aku&sig=vvts68ykoficqneez3mfrcsfnfy
                                &hl=en&sa=x&ved=0ahukewimk_qv3cvnahuu3gmkh
                                a6abvyq6aeihjab#v=onepage&q=section%206.4.
                                4.%20pattern%20classification&f=false
                                it has been suspected for a very long time
                                that local minima do not matter.
                                [214]likelike
                                [215]reply
   23.
   ivan says:
       [216]february 27, 2016 at 9:07 am
       10x very much for the information
       [217]likeliked by [218]1 person
       [219]reply
   24. pingback: [220]                             lfwin
   25. pingback: [221]tensorflow reproductions: big deep simple mnist |
       machine learning
         1.
        [222]jozsef hegedus says:
            [223]september 5, 2016 at 12:42 am
            could you give a citation to this work ?
            [224]likelike
            [225]reply
              1.
             charles h martin, phd says:
                 [226]september 5, 2016 at 11:34 am
                 [227]https://arxiv.org/abs/1605.07648
                 [228]likelike
                 [229]reply
   26.
   [230]jozsef hegedus says:
       [231]september 2, 2016 at 1:19 pm
       it seems that rbms are the past, it is just one of the     less
       successful     methods of training dnns. i wonder if this analogy to
       rg still holds with never, more successful training methods (like
       dropout, relu, batch id172).
       [232]http://stats.stackexchange.com/questions/163600/pre-training-i
       n-deep-convolutional-neural-network?rq=1
       [233]likelike
       [234]reply
         1.
        charles h martin, phd says:
            [235]september 2, 2016 at 1:22 pm
            rbms are one of the few successful models of unsupervised deep
            learning. the training methods you mention are technical
            adjustments to supervised learning algorithms.
            [236]likelike
            [237]reply
         2.
        charles h martin, phd says:
            [238]september 2, 2016 at 1:43 pm
            there is recent work on applying the tap equations to
            improving training for rbms. if i can get the code to work,
            ill post on github. next step might be to resum the tap
            correction (i.e. find the right reid172)
            [239]likelike
            [240]reply
              1.
             [241]jozsef hegedus says:
                 [242]september 5, 2016 at 12:41 am
                 could you give a reference to this work ? i   d be
                 interested to read it.
                 [243]likelike
                 [244]reply
                   1.
                  charles h martin, phd says:
                      [245]september 5, 2016 at 11:35 am
                      [246]https://arxiv.org/abs/1605.07648
                      [247]https://arxiv.org/pdf/1502.06470.pdf
                      [248]likelike
                      [249]reply
                   2.
                  charles h martin, phd says:
                      [250]september 5, 2016 at 11:39 am
                      generally speaking, i suspect this could be of value
                      anywhere mean field id136 is applied
                      [251]likelike
                      [252]reply
   27.
   [253]jozsef hegedus says:
       [254]september 4, 2016 at 11:28 am
       i wonder if the causation arrow is in the other way : rg works
       because it is a form of deep learning.
       [255]likelike
       [256]reply
   28.
   [257]jozsef hegedus says:
       [258]september 4, 2016 at 11:31 am
       like the paper writer says:    our mapping suggests that dnns
       implement a generalized rg like procedure to extract relevant
       features from structured data.    page 2 on
       [259]http://arxiv.org/pdf/1410.3831v1.pdf , so dnns are more
       general than rg, then the causation arrow should go the other way:
       deep learning works => rg works
       [260]likelike
       [261]reply
         1.
        charles h martin, phd says:
            [262]september 5, 2016 at 8:51 am
            rg is an analytic tool that is very well understood. i would
            say that dl is not well understood at all, so rg is just a
            starting point to begin to think about how statistical physic
            relates to rbms. one idea i had is to try applying rg as a
            solver for rbms, in the form of a diagrammatic resummation to
            the tap equations.
            [263]likelike
            [264]reply
              1.
             [265]joco42 says:
                 [266]september 5, 2016 at 10:35 am
                 folks say that rg does not seem to help in practice :
                 [267]https://m.reddit.com/r/machinelearning/comments/4zbr
                 2k/what_is_your_opinion_why_is_the_concept_of/
                 (my question on reddit)
                 [268]likelike
                 [269]reply
                   1.
                  charles h martin, phd says:
                      [270]september 5, 2016 at 11:11 am
                      thanks for the link. i am currently working on the
                      problem from the point of view of improving rbm
                      solvers. to that end, i view rg theory as one of a
                      number of methods for treating strongly correlated
                      systems. progress has been made in applying the tap
                      equations to rbms at low order (2nd, 3rd order). my
                      approach is to see if i can incorporate higher order
                      corrections from the tap equations. this summer, i
                      have succeeded in porting the rbm tap corrections to
                      python and i am working on producing a library for
                      scikit-learn that others can test. i am looking for
                      help finishing and testing the code if you are
                      interested
                      [271]likelike
                      [272]reply
                   2.
                  charles h martin, phd says:
                      [273]september 5, 2016 at 11:11 am
                      see: [274]https://arxiv.org/pdf/1506.02914.pdf
                      if you read the original work by hinton (very old
                      work), you will see he mentions the tap theory, but
                      back then no one could not get it to work. this has
                      changed.
                      tap theory lets us treat rbms as a mean-field
                      theory, and lets us include systematic corrections,
                      at low order. this suggests a way to apply an
                      rg-like theory to obtain higher order corrections
                      the main difficulty in applying an rg theory of
                      sorts here would be to develop a diagrammatic
                      formulation of the tap theory, and then try to sum
                      the terms to high order. to my understanding, no one
                      has yet fully developed the diagrams. but this could
                      also be done computationally.
                      even here, it is unclear which terms to resum, and
                      it may or may not work. to that extent i agree with
                      some of the reddit comments. still, there are lots
                      of things to try, and it takes time to ferret them
                      out.
                      of course, a diagrammatic resummation is not exactly
                      rg theory, but for me it is close enough.
                      more basic is to extend the existing emf-rbm (tap)
                      theory to non-binary data sets. and to test it at
                      scale. it is possible that a simple third order
                      treatment may be good enough, when combined with
                      newer ideas from dl.
                      [275]likelike
                      [276]reply
   29.
   [277]julius iannacone says:
       [278]september 8, 2016 at 11:57 am
       wonderful webpage, continue the excellent job. appreciate it.|
       [279]likelike
       [280]reply
         1.
        charles h martin, phd says:
            [281]september 8, 2016 at 4:52 pm
            thanks
            [282]likelike
            [283]reply
   30. pingback: [284]on cheap learning: partition functions and rbms |
       machine learning
   31. pingback: [285]cats, snakes, and symmetry groups | heteroskedastic
         1.
        charles h martin, phd says:
            [286]january 17, 2017 at 9:57 am
            there has been a long history of attempting to apply
            physics-like symmetry transforms to object recognition. this
            was first tried using rotation invariant kernels in an id166,
            that had something like a lie-group transform built into them.
            10-15 years ago, this is what you would see. these methods
            have pretty much disappeared, and only pop up from time to
            time.
            [287]likelike
            [288]reply
   32. pingback: [289]theory of deep learning     discventionstech
   33. pingback: [290]why deep learning works ii: the reid172
       group
   34. pingback: [291]why deep learning works 3: backprop minimizes the
       free energy     calculated content
   35. pingback: [292]page not found     calculated content
   36. pingback: [293]id172 in deep learning     calculated content
       | premium blog!
   37. pingback: [294]id172 in deep learning |                   
   38. pingback: [295]power laws in deep learning 2: universality
   39. pingback: [296]power laws in deep learning 2: universality |
       premium blog! | development code, android, ios anh tranning it

leave a reply [297]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [298]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [299]log out /
   [300]change )
   google photo

   you are commenting using your google account. ( [301]log out /
   [302]change )
   twitter picture

   you are commenting using your twitter account. ( [303]log out /
   [304]change )
   facebook photo

   you are commenting using your facebook account. ( [305]log out /
   [306]change )
   [307]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

     * [308]charles h martin, phd

calculation consulting

   we are a boutique machine learning data science consultancy. how can we
   help? email me at [309]info@calculationconsulting.com.

   or stop by:
   [310]http://calculationconsulting.com
   [311]youtube channel
   [312]quora

   set up a quick all on [313]clarity.fm

the community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

blog stats

     * 521,305 hits

   [314]follow on wordpress.com

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   join 694 other followers

   ____________________

   (button) follow

top posts & pages

     * [315]spectral id91: a quick overview
       [316]spectral id91: a quick overview
     * [317]kernels part 1: what is an rbf kernel? really?
       [318]kernels part 1: what is an rbf kernel? really?
     * [319]why deep learning works ii: the reid172 group
       [320]why deep learning works ii: the reid172 group
     * [321]id172 in deep learning
       [322]id172 in deep learning
     * [323]causality, correlation, and brownian motion
       [324]causality, correlation, and brownian motion

recent posts

     * [325]sf bay acm talk: heavy tailed self id173 in deep
       neural networks
     * [326]heavy tailed self id173 in deep neural nets: 1 year
       of research
     * [327]don   t peek part 2: predictions without test data
     * [328]machine learning and ai for the lean start up
     * [329]don   t peek: deep learning without looking     at test data

top clicks

     * [330]youtube.com/redirect?redi   
     * [331]arxiv.org/abs/1810.01075
     * [332]arxiv.org/abs/1706.02515
     * [333]github.com/calculatedcont   
     * [334]charlesmartin14.wordpress   
     * [335]arxiv.org/pdf/1412.0233.p   
     * [336]quora.com/machine-learnin   
     * [337]arxiv.org/pdf/1412.6621v3   
     * [338]di.ens.fr/~fbach/nips03_c   
     * [339]charlesmartin14.files.wor   

archives

     * [340]april 2019
     * [341]december 2018
     * [342]november 2018
     * [343]october 2018
     * [344]september 2018
     * [345]june 2018
     * [346]april 2018
     * [347]december 2017
     * [348]september 2017
     * [349]july 2017
     * [350]june 2017
     * [351]february 2017
     * [352]january 2017
     * [353]october 2016
     * [354]september 2016
     * [355]june 2016
     * [356]february 2016
     * [357]december 2015
     * [358]april 2015
     * [359]march 2015
     * [360]january 2015
     * [361]november 2014
     * [362]september 2014
     * [363]august 2014
     * [364]november 2013
     * [365]october 2013
     * [366]august 2013
     * [367]may 2013
     * [368]april 2013
     * [369]december 2012
     * [370]november 2012
     * [371]october 2012
     * [372]september 2012
     * [373]april 2012
     * [374]february 2012

social

     * [375]view calccon   s profile on twitter
     * [376]view charlesmartin14   s profile on linkedin
     * [377]view charlesmartin   s profile on github
     * [378]view ucaao8ghavcrtszdpobc4_kg   s profile on youtube

meta

     * [379]register
     * [380]log in
     * [381]entries rss
     * [382]comments rss
     * [383]wordpress.com

   logo-i

   [384]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [385]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [386]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [387]likes-master

   %d bloggers like this:

references

   visible links
   1. https://calculatedcontent.com/feed/
   2. https://calculatedcontent.com/comments/feed/
   3. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/feed/
   4. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
   5. https://calculatedcontent.com/2015/12/28/when-id173-fails/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/&for=wpcom-auto-discovery
   8. https://calculatedcontent.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#content
  11. https://calculatedcontent.com/
  12. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
  13. https://calculatedcontent.com/author/charlesmartin14/
  14. https://calculatedcontent.com/category/uncategorized/
  15. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comments
  16. http://en.wikipedia.org/wiki/restricted_boltzmann_machine
  17. https://charlesmartin14.files.wordpress.com/2015/03/rbm.png
  18. https://charlesmartin14.files.wordpress.com/2015/03/unsupervised-filters.jpeg
  19. https://charlesmartin14.files.wordpress.com/2015/03/dbn.png
  20. https://charlesmartin14.files.wordpress.com/2015/03/dbn-features.jpeg
  21. https://charlesmartin14.wordpress.com/2013/05/06/advances-in-convex-nmf-part-1-linear-programming/
  22. https://charlesmartin14.files.wordpress.com/2015/03/convex-fig1.jpg
  23. https://charlesmartin14.files.wordpress.com/2015/03/manifold1.png
  24. https://charlesmartin14.wordpress.com/2012/10/09/spectral-id91/
  25. http://vikas.sindhwani.org/mr.pdf
  26. http://www.dii.unisi.it/~melacci/lapid166p/
  27. http://papers.nips.cc/paper/4409-the-manifold-tangent-classifier.pdf
  28. https://charlesmartin14.files.wordpress.com/2015/04/kenwilson.jpeg
  29. http://www.nobelprize.org/nobel_prizes/physics/laureates/1982/wilson-lecture.pdf
  30. https://charlesmartin14.wordpress.com/2015/01/16/the-bitcoin-crash-and-how-nature-works/
  31. http://en.wikipedia.org/wiki/cray_y-mp
  32. http://en.wikipedia.org/wiki/size_consistency_and_size_extensivity
  33. https://charlesmartin14.wordpress.com/2015/01/16/the-bitcoin-crash-and-how-nature-works/
  34. https://www.quantamagazine.org/20140403-a-fundamental-theory-to-model-the-mind/
  35. http://www.scientificamerican.com/article/sand-pile-model-of-the-mind-grows-in-popularity/
  36. https://charlesmartin14.files.wordpress.com/2015/04/kadanoff.jpeg
  37. https://charlesmartin14.files.wordpress.com/2015/04/hopfield.png
  38. http://www.scientificamerican.com/article/memories-may-not-live-in-neurons-synapses
  39. https://charlesmartin14.files.wordpress.com/2015/04/rbm2.jpeg
  40. https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/
  41. http://arxiv.org/pdf/1301.6323v1.pdf
  42. http://arxiv.org/pdf/1410.3831v1.pdf
  43. http://jfi.uchicago.edu/~leop/oldies but goodies/2-variational 2-approximations for reid172 group transformations.pdf
  44. https://www.quantamagazine.org/20141204-a-common-logic-to-seeing-cats-and-cosmos
  45. http://arxiv.org/pdf/1412.6621v3.pdf
  46. https://www.quantamagazine.org/20140403-a-fundamental-theory-to-model-the-mind/
  47. https://www.cs.toronto.edu/~hinton/absps/guidetr.pdf
  48. http://www.cs.toronto.edu/~fritz/absps/momentum.pdf
  49. http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
  50. http://frankwilczek.com/2013/pnas-2013-wilczek-1312463110.pdf
  51. http://www.cs.toronto.edu/~fritz/absps/nccd.pdf
  52. http://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf
  53. http://www.nonlin-processes-geophys.net/3/102/1996/npg-3-102-1996.pdf
  54. http://www.nobelprize.org/nobel_prizes/physics/laureates/1982/wilson-lecture.pdf
  55. http://cps-www.bu.edu/hes/articles/s99a.pdf
  56. http://www.iro.umontreal.ca/~memisevr/pubs/aeenergy.pdf
  57. http://www.icml-2011.org/papers/455_icmlpaper.pdf
  58. http://jmlr.csail.mit.edu/papers/volume11/vincent10a/vincent10a.pdf
  59. http://www.quora.com/what-is-reid172-group-theory
  60. http://journals.aps.org/prb/abstract/10.1103/physrevb.15.4476
  61. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?share=twitter
  62. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?share=facebook
  63. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?share=linkedin
  64. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
  65. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?share=reddit
  66. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?share=email
  67. https://calculatedcontent.com/tag/deep-learning/
  68. https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/
  69. https://calculatedcontent.com/2015/12/28/when-id173-fails/
  70. http://tigerneil.wordpress.com/
  71. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-978
  72. https://tigerneil.wordpress.com/2015/04/09/why-deep-learning-works-ii-the-reid172-group/
  73. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=978&_wpnonce=656635ed68
  74. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=978#respond
  75. https://dineshramitc.wordpress.com/2015/04/09/why-deep-learning-works-ii-the-reid172-group/
  76. http://wubr2000.wordpress.com/
  77. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-981
  78. http://wubrfell.com/2015/04/10/why-deep-learning-works-ii-the-reid172-group/
  79. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=981&_wpnonce=3d6d5d413d
  80. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=981#respond
  81. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-983
  82. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=983&_wpnonce=39dc7846a6
  83. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=983#respond
  84. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-984
  85. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=984&_wpnonce=057cd87d5e
  86. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=984#respond
  87. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-999
  88. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=999&_wpnonce=b69390b7c7
  89. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=999#respond
  90. http://blog.offeryour.com/?p=277943
  91. http://www.justtechnews.net/why-deep-learning-works-ii-the-reid172-group/
  92. https://datawarrior.wordpress.com/2015/07/05/mathanalytics-2-learning-by-zooming-out/
  93. http://arnoldit.com/wordpress/2015/07/06/short-honk-reid172-group-equation/
  94. http://chris.cothrun.com/2015/07/08/bookmarks-for-july-8th-4/
  95. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1030
  96. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1030&_wpnonce=5a65bef0ae
  97. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1030#respond
  98. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1031
  99. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1031&_wpnonce=3e4cd60372
 100. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1031#respond
 101. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1032
 102. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1032&_wpnonce=e66242d238
 103. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1032#respond
 104. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1751
 105. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1751&_wpnonce=e7e695c7a1
 106. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1751#respond
 107. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1752
 108. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1752&_wpnonce=7e868bc4ef
 109. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1752#respond
 110. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1033
 111. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1033&_wpnonce=88987b4859
 112. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1033#respond
 113. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1034
 114. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1034&_wpnonce=12626bb986
 115. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1034#respond
 116. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1035
 117. http://pubs.acs.org/doi/abs/10.1021/jp960617w?journalcode=jpchax
 118. http://journals.aps.org/prb/abstract/10.1103/physrevb.15.4476
 119. https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/
 120. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1035&_wpnonce=1937065f81
 121. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1035#respond
 122. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1039
 123. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1039&_wpnonce=d54bd251de
 124. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1039#respond
 125. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1040
 126. https://www.cs.toronto.edu/~hinton/csc321/readings/boltz321.pdf
 127. http://www.ks.uiuc.edu/publications/papers/pdf/mart91b/mart91b.pdf
 128. https://charlesmartin14.wordpress.com/2012/09/21/eigenvalue-independent-effective-semantic-operator/
 129. http://arxiv.org/pdf/1105.2790v1.pdf
 130. http://www.gatsby.ucl.ac.uk/~dayan/papers/hdfn95.pdf
 131. http://www.cs.toronto.edu/~hinton/science.pdf
 132. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1040&_wpnonce=ec32e353b2
 133. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1040#respond
 134. http://folioverse.appspot.com/
 135. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1043
 136. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1043&_wpnonce=62d6603581
 137. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1043#respond
 138. https://plus.google.com/+jordanbennettbeyondsentience
 139. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1044
 140. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1044&_wpnonce=02834b5701
 141. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1044#respond
 142. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1068
 143. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1068&_wpnonce=e37db344d9
 144. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1068#respond
 145. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1069
 146. https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/
 147. https://drive.google.com/file/d/0bxkbnd5y2m8nbwn6xzm5uxkwnda/view?pli=1
 148. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1069&_wpnonce=7ed234b2ca
 149. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1069#respond
 150. http://advanceddataanalytics.net/2015/10/25/distilled-news-241/
 151. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1087
 152. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1087&_wpnonce=f47fd6a52e
 153. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1087#respond
 154. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1627
 155. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1627&_wpnonce=f5b1025da8
 156. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1627#respond
 157. https://charlesmartin14.wordpress.com/2015/12/28/when-id173-fails/
 158. http://blog.developpez.com/dourouc05/p12993/qt/pourquoi-lapprentissage-profond-et-les-reseaux-neuronaux-sont-ils-si-prometteurs
 159. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1154
 160. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1154&_wpnonce=35b2c2ec34
 161. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1154#respond
 162. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1165
 163. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1165&_wpnonce=218ce32073
 164. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1165#respond
 165. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1166
 166. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1166&_wpnonce=1540031afe
 167. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1166#respond
 168. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1168
 169. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1168&_wpnonce=f8a09c830e
 170. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1168#respond
 171. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1171
 172. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1171&_wpnonce=e81274f434
 173. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 174. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1171#respond
 175. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1205
 176. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1205&_wpnonce=f72dae2b16
 177. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1205#respond
 178. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1206
 179. http://www.cs.toronto.edu/~hinton/science.pdf
 180. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1206&_wpnonce=cd91394597
 181. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1206#respond
 182. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1211
 183. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1211&_wpnonce=f7d47fca87
 184. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1211#respond
 185. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1212
 186. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1212&_wpnonce=230274e56d
 187. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1212#respond
 188. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1214
 189. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1214&_wpnonce=2105699d91
 190. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1214#respond
 191. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1217
 192. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1217&_wpnonce=7901f025db
 193. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1217#respond
 194. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1218
 195. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1218&_wpnonce=7768414d92
 196. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1218#respond
 197. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1220
 198. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1220&_wpnonce=2bb632e9d7
 199. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1220#respond
 200. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1221
 201. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1221&_wpnonce=99d9fbc839
 202. https://www.facebook.com/app_scoped_user_id/1079757165/
 203. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1430
 204. https://www.youtube.com/watch?v=74vux2zszms&t=651s
 205. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1430&_wpnonce=91d0eb3962
 206. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1430#respond
 207. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1434
 208. https://books.google.com/books?id=58im6efkx9cc&pg=pa130&lpg=pa130&dq=solutions+of+the+tap+equations+spin+glasses&source=bl&ots=zc15gav107&sig=7u5k-m2c92d-zrwf0kl4v8pkb80&hl=en&sa=x&ved=0ahukewin8zek2pjoahvs4mmkhrzkblaq6aeiwtaj#v=onepage&q=solutions of the tap equations spin glasses&f=false
 209. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1434&_wpnonce=ea03cba9c9
 210. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1434#respond
 211. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1440
 212. http://www.kdnuggets.com/2016/07/deep-learning-networks-scale.html
 213. https://books.google.com/books?id=br33irc3pkqc&pg=pa299&lpg=pa299&dq=section+6.4.4.+pattern+classification&source=bl&ots=2wcviw9aku&sig=vvts68ykoficqneez3mfrcsfnfy&hl=en&sa=x&ved=0ahukewimk_qv3cvnahuu3gmkha6abvyq6aeihjab#v=onepage&q=section 6.4.4. pattern classification&f=false
 214. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1440&_wpnonce=d67665c152
 215. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1440#respond
 216. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1224
 217. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1224&_wpnonce=66a4df863f
 218. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 219. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1224#respond
 220. https://lfwin.wordpress.com/2016/04/05/                  online-address/
 221. https://charlesmartin14.wordpress.com/2016/06/08/tensorflow-reproductions-big-deep-simple-mnist/
 222. https://www.facebook.com/app_scoped_user_id/1079757165/
 223. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1432
 224. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1432&_wpnonce=15d5916bee
 225. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1432#respond
 226. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1438
 227. https://arxiv.org/abs/1605.07648
 228. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1438&_wpnonce=86bd7e369f
 229. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1438#respond
 230. https://www.facebook.com/app_scoped_user_id/1079757165/
 231. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1416
 232. http://stats.stackexchange.com/questions/163600/pre-training-in-deep-convolutional-neural-network?rq=1
 233. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1416&_wpnonce=e5e9c8fc1c
 234. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1416#respond
 235. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1418
 236. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1418&_wpnonce=7c6fddaee9
 237. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1418#respond
 238. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1424
 239. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1424&_wpnonce=bdf5a8a67b
 240. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1424#respond
 241. https://www.facebook.com/app_scoped_user_id/1079757165/
 242. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1431
 243. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1431&_wpnonce=76d58e6019
 244. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1431#respond
 245. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1439
 246. https://arxiv.org/abs/1605.07648
 247. https://arxiv.org/pdf/1502.06470.pdf
 248. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1439&_wpnonce=94c967ee6e
 249. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1439#respond
 250. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1441
 251. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1441&_wpnonce=b378a58ee8
 252. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1441#respond
 253. https://www.facebook.com/app_scoped_user_id/1079757165/
 254. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1428
 255. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1428&_wpnonce=664eb4fc62
 256. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1428#respond
 257. https://www.facebook.com/app_scoped_user_id/1079757165/
 258. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1429
 259. http://arxiv.org/pdf/1410.3831v1.pdf
 260. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1429&_wpnonce=bf32d9a15c
 261. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1429#respond
 262. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1433
 263. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1433&_wpnonce=a8687f6167
 264. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1433#respond
 265. http://joco42.wordpress.com/
 266. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1435
 267. https://m.reddit.com/r/machinelearning/comments/4zbr2k/what_is_your_opinion_why_is_the_concept_of/
 268. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1435&_wpnonce=983b40da5a
 269. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1435#respond
 270. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1436
 271. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1436&_wpnonce=de79b35219
 272. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1436#respond
 273. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1437
 274. https://arxiv.org/pdf/1506.02914.pdf
 275. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1437&_wpnonce=fa82d37725
 276. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1437#respond
 277. http://www.fbd7z7yiqj.com/fbd7z7yiqj
 278. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1447
 279. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1447&_wpnonce=0b1eba1884
 280. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1447#respond
 281. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1448
 282. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1448&_wpnonce=d6e0042cda
 283. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1448#respond
 284. https://charlesmartin14.wordpress.com/2016/09/10/on-cheap-learning-partition-functions-and-rbms/
 285. https://heteroskedasticblog.wordpress.com/2016/12/11/cats-snakes-and-symmetry-groups/
 286. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-1628
 287. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?like_comment=1628&_wpnonce=63f4d476e0
 288. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/?replytocom=1628#respond
 289. https://discventionstech.wordpress.com/2017/01/17/theory-of-deep-learning/
 290. http://www.phpdrill.com/why-deep-learning-works-ii-the-reid172-group/
 291. https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/
 292. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 293. http://phpcantho.com/id172-in-deep-learning-calculated-content/
 294. https://163ai.org/00/note/03/17/06/machine-learning/4067/share/author/machinelearning/2017/
 295. https://calculatedcontent.com/2018/09/14/power-laws-in-deep-learning-2-universality/
 296. http://www.phpcantho.com/power-laws-in-deep-learning-2-universality/
 297. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#respond
 298. https://gravatar.com/site/signup/
 299. javascript:highlandercomments.doexternallogout( 'wordpress' );
 300. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 301. javascript:highlandercomments.doexternallogout( 'googleplus' );
 302. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 303. javascript:highlandercomments.doexternallogout( 'twitter' );
 304. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 305. javascript:highlandercomments.doexternallogout( 'facebook' );
 306. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 307. javascript:highlandercomments.cancelexternalwindow();
 308. https://calculatedcontent.com/author/charlesmartin14/
 309. mailto:info@calculationconsulting.com
 310. http://calculationconsulting.com/
 311. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg
 312. http://www.quora.com/charles-h-martin
 313. https://clarity.fm/charlesmartin14
 314. https://calculatedcontent.com/
 315. https://calculatedcontent.com/2012/10/09/spectral-id91/
 316. https://calculatedcontent.com/2012/10/09/spectral-id91/
 317. https://calculatedcontent.com/2012/02/06/kernels_part_1/
 318. https://calculatedcontent.com/2012/02/06/kernels_part_1/
 319. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 320. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 321. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 322. https://calculatedcontent.com/2017/06/16/id172-in-deep-learning/
 323. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 324. https://calculatedcontent.com/2013/08/01/causality-correlation-and-brownian-motion/
 325. https://calculatedcontent.com/2019/04/01/sf-bay-acm-talk-heavy-tailed-self-id173-in-deep-neural-networks/
 326. https://calculatedcontent.com/2018/12/17/heavy-tailed-self-id173-in-deep-neural-nets-1-year-of-research/
 327. https://calculatedcontent.com/2018/11/18/dont-peek-part-2-predictions-without-test-data/
 328. https://calculatedcontent.com/2018/11/16/machine-learning-and-ai-for-the-lean-start-up/
 329. https://calculatedcontent.com/2018/10/07/dont-peek-deep-learning-without-looking-at-test-data/
 330. https://www.youtube.com/redirect?redir_token=ezgiasszjkmz1fnzp0yjtazidd98mtu1ndizmjiznkaxntu0mtq1odm2&q=https://arxiv.org/abs/1810.01075&event=video_description&v=ilv5sc8wjpy
 331. https://arxiv.org/abs/1810.01075
 332. https://arxiv.org/abs/1706.02515
 333. https://github.com/calculatedcontent/tid166
 334. https://charlesmartin14.wordpress.com/2013/11/14/metric-learning-some-quantum-statistical-mechanics/
 335. http://arxiv.org/pdf/1412.0233.pdf
 336. http://www.quora.com/machine-learning/how-does-one-decide-on-which-kernel-to-choose-for-an-id166-rbf-vs-linear-vs-poly-kernel
 337. http://arxiv.org/pdf/1412.6621v3.pdf
 338. http://www.di.ens.fr/~fbach/nips03_cluster.pdf
 339. https://charlesmartin14.files.wordpress.com/2012/10/mat1.png
 340. https://calculatedcontent.com/2019/04/
 341. https://calculatedcontent.com/2018/12/
 342. https://calculatedcontent.com/2018/11/
 343. https://calculatedcontent.com/2018/10/
 344. https://calculatedcontent.com/2018/09/
 345. https://calculatedcontent.com/2018/06/
 346. https://calculatedcontent.com/2018/04/
 347. https://calculatedcontent.com/2017/12/
 348. https://calculatedcontent.com/2017/09/
 349. https://calculatedcontent.com/2017/07/
 350. https://calculatedcontent.com/2017/06/
 351. https://calculatedcontent.com/2017/02/
 352. https://calculatedcontent.com/2017/01/
 353. https://calculatedcontent.com/2016/10/
 354. https://calculatedcontent.com/2016/09/
 355. https://calculatedcontent.com/2016/06/
 356. https://calculatedcontent.com/2016/02/
 357. https://calculatedcontent.com/2015/12/
 358. https://calculatedcontent.com/2015/04/
 359. https://calculatedcontent.com/2015/03/
 360. https://calculatedcontent.com/2015/01/
 361. https://calculatedcontent.com/2014/11/
 362. https://calculatedcontent.com/2014/09/
 363. https://calculatedcontent.com/2014/08/
 364. https://calculatedcontent.com/2013/11/
 365. https://calculatedcontent.com/2013/10/
 366. https://calculatedcontent.com/2013/08/
 367. https://calculatedcontent.com/2013/05/
 368. https://calculatedcontent.com/2013/04/
 369. https://calculatedcontent.com/2012/12/
 370. https://calculatedcontent.com/2012/11/
 371. https://calculatedcontent.com/2012/10/
 372. https://calculatedcontent.com/2012/09/
 373. https://calculatedcontent.com/2012/04/
 374. https://calculatedcontent.com/2012/02/
 375. https://twitter.com/calccon/
 376. https://www.linkedin.com/in/charlesmartin14/
 377. https://github.com/charlesmartin/
 378. https://www.youtube.com/channel/ucaao8ghavcrtszdpobc4_kg/
 379. https://wordpress.com/start?ref=wplogin
 380. https://charlesmartin14.wordpress.com/wp-login.php
 381. https://calculatedcontent.com/feed/
 382. https://calculatedcontent.com/comments/feed/
 383. https://wordpress.com/
 384. https://wordpress.com/?ref=footer_blog
 385. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 386. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#cancel
 387. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 389. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-form-guest
 390. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-form-load-service:wordpress.com
 391. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-form-load-service:twitter
 392. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/#comment-form-load-service:facebook
 393. http://nanonaren.wordpress.com/
 394. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 395. http://tablewarebox.com/
 396. http://duttatridib.wordpress.com/
 397. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 398. http://twitter.com/alxfed
 399. http://ashutoshtripathi.com/
 400. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 401. http://randomstratum.wordpress.com/
 402. https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-reid172-group/
 403. https://calculatedcontent.com/logo-i-3/
