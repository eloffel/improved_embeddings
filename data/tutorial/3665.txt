   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

   [1*mtt8tplvuqjdajjs8gorsw.jpeg]

a beginners guide to deep learning

   [9]go to the profile of kumar shridhar
   [10]kumar shridhar (button) blockedunblock (button) followfollowing
   may 26, 2017

introduction

   machine-learning technology powers many aspects of modern society: from
   web searches to content filtering on social networks to recommendations
   on e-commerce websites, and it is increasingly present in consumer
   products such as cameras and smartphones. machine-learning systems are
   used to identify objects in images, transcribe speech into text, match
   news items, posts or products with users    interests, and select
   relevant results of search.
   increasingly, these applications make use of a class of techniques
   called deep
   learning.[5]

   deep learning (also known as deep structured learning, hierarchical
   learning or deep machine learning) is a branch of machine learning
   based on a set of algorithms that attempt to model high level
   abstractions in data. in a simple case, you could have two sets of
   neurons: ones that receive an input signal and ones that send an output
   signal. when the input layer receives an input it passes on a modified
   version of the input to the next layer. in a deep network, there are
   many layers between the input and output (and the layers are not made
   of neurons but it can help to think of it that way), allowing the
   algorithm to use multiple processing layers, composed of multiple
   linear and non-linear transformations.[1][2][3][4][5][6][7][8][9]

   deep learning has revolutionized the machine learning recently with
   some of the great works being done in this field. these methods have
   dramatically
   improved the state-of-the-art in id103, visual object
   recognition, id164 and many other domains such as drug
   discovery and genomics. but, the ancient term    deep learning    was first
   introduced to machine learning by dechter (1986)[10], and to artificial
   neural networks (nns) by aizenberg et al (2000)[11]. it was further
   popularized by the development of convolutional networks architecture
   by alex krizhevsky named    alexnet    that won the competition of id163
   in 2012 by defeating all the image processing methods and creating a
   way for deep learning architectures to be used in image processing.[12]
     __________________________________________________________________

deep learning architecture

    1. generative deep architectures, which are intended to characterize
       the
       high-order correlation properties of the observed or visible data
       for
       pattern analysis or synthesis purposes, and/or characterize the
       joint
       statistical distributions of the visible data and their associated
       classes. in
       the latter case, the use of bayes rule can turn this type of
       architecture
       into a discriminative one.
    2. discriminative deep architectures, which are intended to directly
       provide discriminative power for pattern classification, often by
       characterizing the posterior distributions of classes conditioned
       on the visible data; and
    3. hybrid deep architectures, where the goal is discrimination but is
       assisted (often in a significant way) with the outcomes of
       generative
       architectures via better optimization or/and id173, or
       discriminative criteria are used to learn the parameters in any of
       the
       deep generative models in category 1) above. [13]

   despite the complex categorization of the deep learning architectures,
   the
   one   s that are in practice are deep feed-forward networks, convolution
   networks and recurrent networks.
     __________________________________________________________________

deep feed forward networks

   deep feed-forward networks, also often called feed-forward neural
   networks, or multilayer id88s (mlps), are the quintessential deep
   learning models.

   the goal of a feed-forward network is to approximate some function f   .
   for
   example, for a classifier, y = f    (x) maps an input x to a category y.
   a feedforward network defines a mapping y=f(x;  ) and learns the value
   of the
   parameters    that result in the best function approximation.[1]

   in simple terms, the network can be defined as input, hidden and output
   nodes with data coming in from input nodes, processing is done in
   hidden nodes and then the output is produced through output nodes. the
   information flows through the function being evaluated from x, through
   the intermediate computations used to define f, and finally to the
   output y. there are no feedback connections in which outputs of the
   model are fed back into itself and hence the models is called as
   feed-forward network. the model is shown in figure [1].
   [1*bu75l5fvcvtcuxsqscqspg.png]
   figure [1]: feed-forward neural network[14]
     __________________________________________________________________

convolution neural networks

   [1*z7hd8fzei_eodazwiapvaw.png]
   [11]source

   in machine learning, a convolutional neural network (id98, or convnet)
   is a type of feed-forward id158 in which the
   connectivity pattern between its neurons is inspired by the
   organization of the animal visual cortex.

   individual cortical neurons respond to stimuli in a restricted region
   of space
   known as the receptive field. the receptive fields of different neurons
   partially overlap such that they tile the visual field.

   the response of an individual neuron to stimuli within its receptive
   field can be approximated mathematically by a convolution
   operation.[15] convolutional networks were inspired by biological
   processes[16] and are variations of multilayer id88s designed to
   use minimal amounts of preprocessing.[17]

   they have wide applications in image and video recognition, recommender
   systems[18] and natural language processing.[19]

   lenet was one of the very first convolutional neural networks which
   helped
   propel the field of deep learning. this pioneering work by yann lecun
   was
   named lenet5 after many previous successful iterations since the year
   1988.
   at that time the lenet architecture was used mainly for character
   recognition tasks such as reading zip codes, digits, etc.
   [1*yhkcrrgpdewt30jce7016g.png]
   figure[2]: a simple convolutional neural network model

   there are four main components in a convnet shown in figure 2 above:
   1. convolutional layer
   2. activation function
   3. pooling layer
   4. fully connected layer
     __________________________________________________________________

convolutional layer

   convolutional layer is based on the term    convolution   , which is a
   mathematical operation performed on two variables (f*g) to produce a
   third
   variable. it is similar to cross-correlation. the input to a
   convolutional layer is a m x m x r image where m is the height and
   width of the image and r is the number of channels, e.g. an rgb image
   has r=3. the convolutional layer will have k filters (or kernels) of
   size n x n x q where n is smaller than the
   dimension of the image and q can either be the same as the number of
   channels r or smaller and may vary for each kernel. the size of the
   filters gives rise to the locally connected structure which are each
   convolved with the image to produce k feature maps of size m   n+1.8 [20]
     __________________________________________________________________

activation function

   to implement complex mapping functions, id180 are
   needed, that are non-linear in order to bring in the much needed
   non-linearity property that enables them to approximate any function.
   id180 are also important for squashing the unbounded
   linearly weighted sum from neurons.
   this is important to avoid large values accumulating high up the
   processing
   hierarchy. a lot of id180 are present that can be used
   with some of the primarily used ones being sigmoid, tanh and relu.
     __________________________________________________________________

pooling layer

   pooling is a sample-based discretization process. the objective is to
   down-sample an input representation (image, hidden-layer output matrix,
   etc.), reducing it   s dimensionality and allowing for assumptions to be
   made about features contained in the sub-regions binned.

   this is done to in part to help over-fitting by providing an abstracted
   form of
   the representation. as well, it reduces the computational cost by
   reducing the number of parameters to learn and provides basic
   translation invariance to the internal representation.

   some of the most prominently used pooling techniques are max-pooling,
   minpooling and average-pooling.
   [1*gn_o1mjjcbx_qpyn-ps5eq.png]
   figure[3]: example of max-pooling of 2*2 filters
     __________________________________________________________________

fully connected layer

   the term    fully connected    implies that every neuron in the previous
   layer is connected to every neuron on the next layer. the fully
   connected layer is a traditional multi layer id88 that uses a
   softmax activation function or any other similar function in the output
   layer. [21]
     __________________________________________________________________

recurrent neural networks

   in a traditional neural network we assume that all inputs (and outputs)
   are
   independent of each other. but for many tasks that   s a very bad idea.
   if you
   want to predict the next word in a sentence you better know which words
   came before it. id56s are called recurrent because they perform the same
   task for every element of a sequence, with the output being depended on
   the previous computations. another way to think about id56s is that they
   have a    memory    which captures information about what has been
   calculated so far. [22]

   a id56 has loops in them that allow information to be carried across
   neurons while reading in input. in the figure[4], x_t is some input, a
   is a part of the id56 and h_t is the output. essentially you can feed in
   words from the sentence or even characters from a string as x_t and
   through the id56 it will come up with a h_t.s. some of the types of id56s
   are lstm, bidirectional id56s, grus and more.
   [1*yaggus_prorkbcq5nhy8ua.png]
   figure[4]: model of a id56[30]

   id56s can be used in nlp, machine translations, id38,
   id161, video analysis, image generation, image captioning,
   and so on due to the fact that any number of inputs and outputs can be
   fixed in a id56 making it one to one         many to many model possible. some
   of the possible architectures are shown in figure[5] with possible
   explanation of the models.
   [1*s7q2rh7ba0jw5pqjoveksw.png]
   figure [5]: id56 depicting operations over sequence of vectors[23]
     __________________________________________________________________

applications

   there has been a lot of research going on in the field of deep learning
   and a
   lot of unique questions are solved using deep learning models. some of
   the
   best applications of deep learning are:

colorization of black and white images

   deep learning can be used to use the objects and their context within
   the
   photograph to color the image, much like a human operator might
   approach
   the problem. the approach involves the use of very large convolutional
   neural networks and supervised layers that recreate the image with the
   addition of color.[24][28]
     __________________________________________________________________

machine translations

   text translation can be performed without any pre-processing of the
   sequence, allowing the algorithm to learn the dependencies between
   words and their mapping to a new language. stacked networks of large
   lstm recurrent neural networks are used to perform this
   translation.[25]
     __________________________________________________________________

object classification and detection in photographs

   this task requires the classification of objects within a photograph as
   one of a set of previously known objects.
   state-of-the-art results have been achieved on benchmark examples of
   this
   problem using very large convolutional neural networks. a breakthrough
   in this problem by alex krizhevsky et al.[12] results on the id163
   classification problem called alexnet.[28]
     __________________________________________________________________

automatic handwriting generation

   this is a task where given a corpus of handwriting examples, generate
   new
   handwriting for a given word or phrase.
   the handwriting is provided as a sequence of coordinates used by a pen
   when the handwriting samples were created. from this corpus the
   relationship between the pen movement and the letters is learned and
   new examples can be generated ad hoc.[26][28]
     __________________________________________________________________

automatic game playing

   this is a task where a model learns how to play a computer game based
   only
   on the pixels on the screen.
   this very difficult task is the domain of deep reinforcement models and
   is the breakthrough that deepmind (now part of google) is renown for
   achieving.[27] [28]
     __________________________________________________________________

generative model chatbots

   a sequence to sequence based model was used to create a chatbot which
   learned to generate it   s own answers when trained on a lot of real live
   conversational datasets. to know more in detail, visit the [12]link.
   [13]generative model chatbots
   as discussed in my previous post about the types of bots and it seemed
   that the generative bots are the smartest   medium.com
     __________________________________________________________________

conclusion

   it can be concluded from the article that the deep learning models can
   be used in a variety of tasks due to their capability of simulating the
   human brain. a lot of research has been done in the area and a lot of
   research is going to be done in near future. although, trust issues are
   there at the moment, but things will be more clear in the near future.
     __________________________________________________________________

references

    1. [14]ian goodfellow, yoshua bengio, and aaron courville (2016). deep
       learning. mit press. online
    2. deng, l.; yu, d. (2014).   deep learning: methods and
       applications   (pdf).foundations and trends in signal
       processing.7(3   4): 1   199.doi:10.1561/2000000039.
    3. bengio, yoshua (2009).   learning deep architectures for
       ai   (pdf).foundations and trends in machine learning.2(1): 1   
       127.doi:10.1561/2200000006.
    4. bengio, y.; courville, a.; vincent, p. (2013).    representation
       learning: a
       review and new perspectives   .ieee transactions on pattern analysis
       and
       machineintelligence.35(8):17981828.arxiv:1206.5538.doi:10.1109/tpam
       i.
       2013.50.
    5. schmidhuber, j. (2015).    deep learning in neural networks: an
       overview   .neuralnetworks.61:85117.arxiv:1404.7828.doi:10.1016/j.neu
       n
       et.2014.09.003.
    6. bengio, yoshua; lecun, yann; hinton, geoffrey (2015).    deep
       learning   .nature.521: 436   444.doi:10.1038/nature14539.pm 6017442.
    7. deep machine learning         a new frontier in artificial intelligence
       research    a survey paper by itamar arel, derek c. rose, and thomas
       p. karnowski. ieee computational intelligence magazine, 2013
    8. schmidhuber,j  rgen(2015).   deeplearning   .scholarpedia.10(11):32832.d
       oi:10.4249/scholarpedia.32832.
    9. [15]carlos e. perez.   a pattern language for deep learning   .
   10. r. dechter (1986), university of california, computer science
       department, cognitive systems laboratory.
   11. i. aizenberg, n.n. aizenberg, and j. p.l. vandewalle (2000).
       multi-valued and universal binary neurons: theory, learning and
       applications. springer science & business media
   12. krizhevsky, alex, ilya sutskever, and geoffrey e. hinton.    id163
       classification with deep convolutional neural networks.   advances in
       neural information processing systems. 2012.
   13. deng,li, three classes of deep learning architectures and their
       applications: a tutorial survey, microsoft research, redmond, wa
       98052, usa.
   14. [16]feed forward neural network
   15.    convolutional neural networks (lenet)         deeplearning 0.1
       documentation   . deeplearning 0.1. lisa lab. retrieved 31 august
       2013.
   16. matusugu, masakazu; katsuhiko mori; yusuke mitari; yuji kaneda
       (2003).    subject independent facial expression recognition with
       robust face detection using a convolutional neural network   (pdf).
       neural networks. (5): 555   559. doi:10.1016/s0893   6080(03)00115   1.
   17. [17]lenet-5, convolutional neural networks
   18. van den oord, aaron; dieleman, sander; schrauwen, benjamin
       (2013   01- 01). burges, c. j. c.; bottou, l.; welling, m.;
       ghahramani, z.; weinberger, k. q., eds. deep content-based music
       recommendation (pdf). curran associates, inc. pp.2643   2651
   19. collobert, ronan; weston, jason (2008   01   01).    a unified
       architecture for natural language processing: deep neural networks
       with multitask
       learning   .proceedings of the 25th international conference on
       machine
       learning. icml    08. new york, ny, usa: acm:
       160   167.doi:10.1145/1390156.1390177. isbn 978   1   60558   205   4.
   20. [18]id98
   21. [19]intuitive explaination convnets
   22. [20]recurrent neural networks tutorial, part 1         introduction to
       id56s
   23. [21]id56 effectiveness
   24. cheng, zezhou, qingxiong yang, and bin sheng.    deep colorization.   
       proceedings of the ieee international conference on computer
       vision.
       2015.
   25. sutskever, ilya, oriol vinyals, and quoc v. le.    sequence to
       sequence
       learning with neural networks.    arxiv preprint arxiv:1409.3215
       (2014).
   26. graves, alex.    generating sequences with recurrent neural
       networks.   
       arxiv preprint arxiv:1308.0850 (2013).
   27. mnih, volodymyr, et al.    playing atari with deep reinforcement
       learning.    arxiv preprint arxiv:1312.5602 (2013).
   28. [22]application deep learning
   29. [23]generative model chatbots
   30. [24]understanding lstms
     __________________________________________________________________

   to know about the cool things we do at [25]botsupply every week, follow
   the [26]weekly posts by [27]giovanni toschi every week.
   [28]the week!         issue 4
   we start with dsb, get featured by topdenmark and ran a
   marathonmedium.com
     __________________________________________________________________

   check out my other posts on chatbots [29]here.
   [30]kumar shridhar - medium
   read writing from kumar shridhar on medium. helping machines in their
   quest to rule us! | co-chief scientist at   medium.com

     * [31]machine learning
     * [32]deep learning
     * [33]neural networks
     * [34]recurrent neural network
     * [35]deep neural networks

   (button)
   (button)
   (button) 312 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [36]go to the profile of kumar shridhar

[37]kumar shridhar

   helping machines in their quest to rule us! | nlp | id161|
   botsupply.ai | kumar-shridhar.github.io

     * (button)
       (button) 312
     * (button)
     *
     *

   [38]#wecocreate
   never miss a story from #wecocreate, when you sign up for medium.
   [39]learn more
   never miss a story from #wecocreate
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5ee814cf7706
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@shridhar743/a-beginners-guide-to-deep-learning-5ee814cf7706&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@shridhar743/a-beginners-guide-to-deep-learning-5ee814cf7706&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@shridhar743?source=post_header_lockup
  10. https://medium.com/@shridhar743
  11. https://adeshpande3.github.io/adeshpande3.github.io/a-beginner's-guide-to-understanding-convolutional-neural-networks/
  12. https://medium.com/@shridhar743/generative-model-chatbots-e422ab08461e
  13. https://medium.com/@shridhar743/generative-model-chatbots-e422ab08461e
  14. http://www.deeplearningbook.org/
  15. http://blog.alluviate.com/?p=119
  16. https://en.wikipedia.org/wiki/feedforward_neural_network
  17. http://yann.lecun.com/exdb/lenet/
  18. http://ufldl.stanford.edu/tutorial/supervised/convolutionalneuralnetwork/
  19. https://ujjwalkarn.em/2016/08/11/intuitive-explanation-convnets/
  20. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  21. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  22. http://machinelearningmastery.com/inspirational-applications-deeplearning/
  23. https://medium.com/@shridhar743/generative-model-chatbots-e422ab08461e
  24. http://colah.github.io/posts/2015-08-understanding-lstms/
  25. http://www.botsupply.co/
  26. https://medium.com/botsupply/theweek/home
  27. https://medium.com/@giovannitoschi
  28. https://medium.com/botsupply/the-week-issue-4-e94f0a098923
  29. https://medium.com/@shridhar743
  30. https://medium.com/@shridhar743
  31. https://medium.com/tag/machine-learning?source=post
  32. https://medium.com/tag/deep-learning?source=post
  33. https://medium.com/tag/neural-networks?source=post
  34. https://medium.com/tag/recurrent-neural-network?source=post
  35. https://medium.com/tag/deep-neural-networks?source=post
  36. https://medium.com/@shridhar743?source=footer_card
  37. https://medium.com/@shridhar743
  38. https://medium.com/botsupply
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://medium.com/@shridhar743/generative-model-chatbots-e422ab08461e
  42. https://medium.com/botsupply/the-week-issue-4-e94f0a098923
  43. https://medium.com/@shridhar743
  44. https://medium.com/p/5ee814cf7706/share/twitter
  45. https://medium.com/p/5ee814cf7706/share/facebook
  46. https://medium.com/p/5ee814cf7706/share/twitter
  47. https://medium.com/p/5ee814cf7706/share/facebook
