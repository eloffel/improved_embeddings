   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

sequence to sequence model: introduction and concepts

   [16]go to the profile of manish chablani
   [17]manish chablani (button) blockedunblock (button) followfollowing
   jun 22, 2017

   if we take a high-level view, a id195 model has encoder, decoder and
   intermediate step as its main components:
   [1*3lj8agqfwee5kctj-dxtvg.png]
   [1*ismhi-mu oowf3ziqffg.png]

   we use embedding, so we have to first compile a    vocabulary    list
   containing all the words we want our model to be able to use or read.
   the model inputs will have to be tensors containing the ids of the
   words in the sequence.

   there are four symbols, however, that we need our vocabulary to
   contain. id195 vocabularies usually reserve the first four spots for
   these elements:
     * <pad>: during training, we   ll need to feed our examples to the
       network in batches. the inputs in these batches all need to be the
       same width for the network to do its calculation. our examples,
       however, are not of the same length. that   s why we   ll need to pad
       shorter inputs to bring them to the same width of the batch
     * <eos>: this is another necessity of batching as well, but more on
       the decoder side. it allows us to tell the decoder where a sentence
       ends, and it allows the decoder to indicate the same thing in its
       outputs as well.
     * <unk>: if you   re training your model on real data, you   ll find you
       can vastly improve the resource efficiency of your model by
       ignoring words that don   t show up often enough in your vocabulary
       to warrant consideration. we replace those with <unk>.
     * <go>: this is the input to the first time step of the decoder to
       let the decoder know when to start generating output.

   note: other tags can be used to represent these functions. for example
   i   ve seen <s> and </s> used in place of <go> and <eos>. so make sure
   whatever you use is consistent through preprocessing, and model
   training/id136.

   preparing the inputs for the training graph is a little more involved
   for two reasons:

     1. these models work a lot better if we feed the decoder our target
     sequence regardless of what its timesteps actually output in the
     training run. so unlike in the graph, we will not feed the output of
     the decoder to itself in the next timestep.

   2. batching

   one of the original sequence to sequence papers, [18]sutskever et al.
   2014, reported better model performance if the inputs are reversed. so
   you may also choose to reverse the order of words in the input
   sequence.

   during the preprocessing we do the following:
     * we build our vocabulary of unique words (and count the occurrences
       while we   re at it)
     * we replace words with low frequency with <unk>
     * create a copy of conversations with the words replaced by their ids
     * we can choose to add the <go> and <eos> word ids to the target
       dataset now, or do it at training time

   credits: from lecture notes:
   [19]https://classroom.udacity.com/nanodegrees/nd101/syllabus

   resources:

   iframe: [20]/media/49289ffac13baa9c1bff9f80ec6ee55d?postid=44d9b41cd42d

   [21]https://github.com/ematvey/tensorflow-id195-tutorials

     * [22]machine learning
     * [23]id56
     * [24]deep learning
     * [25]towards data science
     * [26]batching

   (button)
   (button)
   (button) 639 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of manish chablani

[28]manish chablani

   ai in healthcare [29]@curaihq, marathoner. (past: dl for self driving
   cars [30]@cruise, ml [31]@uber, early engineer [32]@microsoftazure
   cloud storage)

     (button) follow
   [33]towards data science

[34]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 639
     * (button)
     *
     *

   [35]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [36]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/44d9b41cd42d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_tbye8ccapwg1---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@manishchablani?source=post_header_lockup
  17. https://towardsdatascience.com/@manishchablani
  18. https://arxiv.org/abs/1409.3215
  19. https://classroom.udacity.com/nanodegrees/nd101/syllabus
  20. https://towardsdatascience.com/media/49289ffac13baa9c1bff9f80ec6ee55d?postid=44d9b41cd42d
  21. https://github.com/ematvey/tensorflow-id195-tutorials
  22. https://towardsdatascience.com/tagged/machine-learning?source=post
  23. https://towardsdatascience.com/tagged/id56?source=post
  24. https://towardsdatascience.com/tagged/deep-learning?source=post
  25. https://towardsdatascience.com/tagged/towards-data-science?source=post
  26. https://towardsdatascience.com/tagged/batching?source=post
  27. https://towardsdatascience.com/@manishchablani?source=footer_card
  28. https://towardsdatascience.com/@manishchablani
  29. http://twitter.com/curaihq
  30. http://twitter.com/cruise
  31. http://twitter.com/uber
  32. http://twitter.com/microsoftazure
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/44d9b41cd42d/share/twitter
  39. https://medium.com/p/44d9b41cd42d/share/facebook
  40. https://medium.com/p/44d9b41cd42d/share/twitter
  41. https://medium.com/p/44d9b41cd42d/share/facebook
