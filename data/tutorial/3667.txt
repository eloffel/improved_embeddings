   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id97 (skip-gram model): part 1 - intuition.

   [16]go to the profile of manish chablani
   [17]manish chablani (button) blockedunblock (button) followfollowing
   jun 13, 2017

   most of the content here is from chris   s blog. i have condensed it and
   made minor adaptations.

   credits: chris mccormick

   [18]http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-gram-m
   odel/

   [19]http://mccormickml.com/2017/01/11/id97-tutorial-part-2-negative
   -sampling/

   the algorithm exists in two flavors cbow and skip-gram. given a set of
   sentences (also called corpus) the model loops on the words of each
   sentence and either tries to use the current word of to predict its
   neighbors (its context), in which case the method is called
      skip-gram   , or it uses each of these contexts to predict the current
   word, in which case the method is called    continuous bag of words   
   (cbow). the limit on the number of words in each context is determined
   by a parameter called    window size   .

intuition

   the skip-gram neural network model is actually surprisingly simple in
   its most basic form. train a simple neural network with a single hidden
   layer to perform a certain task, but then we   re not actually going to
   use that neural network for the task we trained it on! instead, the
   goal is actually just to learn the weights of the hidden layer   we   ll
   see that these weights are actually the    word vectors    that we   re
   trying to learn.

   we   re going to train the neural network to do the following. given a
   specific word in the middle of a sentence (the input word), look at the
   words nearby and pick one at random. the network is going to tell us
   the id203 for every word in our vocabulary of being the    nearby
   word    that we chose.

   the output probabilities are going to relate to how likely it is find
   each vocabulary word nearby our input word. for example, if you gave
   the trained network the input word    soviet   , the output probabilities
   are going to be much higher for words like    union    and    russia    than
   for unrelated words like    watermelon    and    kangaroo   .

   we   ll train the neural network to do this by feeding it word pairs
   found in our training documents. the below example shows some of the
   training samples (word pairs) we would take from the sentence    the
   quick brown fox jumps over the lazy dog.    i   ve used a small window size
   of 2 just for the example. the word highlighted in blue is the input
   word.
   [0*0m03cishwl4f2dae.png]

   we   re going to represent an input word like    ants    as a one-hot vector.
   this vector will have 10,000 components (one for every word in our
   vocabulary) and we   ll place a    1    in the position corresponding to the
   word    ants   , and 0s in all of the other positions.

   the output of the network is a single vector (also with 10,000
   components) containing, for every word in our vocabulary, the
   id203 that a randomly selected nearby word is that vocabulary
   word.

   here   s the architecture of our neural network.
   [0*ftfdlz7ydboq8c9w.png]

   there is no activation function on the hidden layer neurons, but the
   output neurons use softmax.

   for our example, we   re going to say that we   re learning word vectors
   with 300 features. so the hidden layer is going to be represented by a
   weight matrix with 10,000 rows (one for every word in our vocabulary)
   and 300 columns (one for every hidden neuron).

   300 features is what google used in their published model trained on
   the google news dataset (you can download it from [20]here). the number
   of features is a    hyper parameter    that you would just have to tune to
   your application (that is, try different values and see what yields the
   best results).

   if you look at the rows of this weight matrix, these are actually what
   will be our word vectors!
   [0*6doqn6gxveoix0yn.png]

   so the end goal of all of this is really just to learn this hidden
   layer weight matrix         the output layer we   ll just toss when we   re done!
   the 1 x 300 word vector for    ants    then gets fed to the output layer.
   the output layer is a softmax regression classifier.

   specifically, each output neuron has a weight vector which it
   multiplies against the word vector from the hidden layer, then it
   applies the function exp(x) to the result. finally, in order to get the
   outputs to sum up to 1, we divide this result by the sum of the results
   from all 10,000 output nodes.

   here   s an illustration of calculating the output of the output neuron
   for the word    car   .
   [0*qsweobamdaiudbtx.png]

     if two different words have very similar    contexts    (that is, what
     words are likely to appear around them), then our model needs to
     output very similar results for these two words. and one way for the
     network to output similar context predictions for these two words is
     if the word vectors are similar. so, if two words have similar
     contexts, then our network is motivated to learn similar word
     vectors for these two words! ta da!

   and what does it mean for two words to have similar contexts? i think
   you could expect that synonyms like    intelligent    and    smart    would
   have very similar contexts. or that words that are related, like
      engine    and    transmission   , would probably have similar contexts as
   well.

   this can also handle id30 for you         the network will likely learn
   similar word vectors for the words    ant    and    ants    because these
   should have similar contexts.

   we need few additional modifications to the basic skip-gram model which
   are important for actually making it feasible to train. running
   id119 on a neural network that large is going to be slow.
   and to make matters worse, you need a huge amount of training data in
   order to tune that many weights and avoid over-fitting. millions of
   weights times billions of training samples means that training this
   model is going to be a beast. the authors of id97 addressed these
   issues in their second [21]paper.

   there are three innovations in this second paper:
    1. treating common word pairs or phrases as single    words    in their
       model.
    2. subsampling frequent words to decrease the number of training
       examples.
    3. modifying the optimization objective with a technique they called
          negative sampling   , which causes each training sample to update
       only a small percentage of the model   s weights.

   it   s worth noting that subsampling frequent words and applying negative
   sampling not only reduced the compute burden of the training process,
   but also improved the quality of their resulting word vectors as well.

subsampling:

   there are two    problems    with common words like    the   :
    1. when looking at word pairs, (   fox   ,    the   ) doesn   t tell us much
       about the meaning of    fox   .    the    appears in the context of pretty
       much every word.
    2. we will have many more samples of (   the   ,    ) than we need to learn
       a good vector for    the   .

   id97 implements a    subsampling    scheme to address this. for each
   word we encounter in our training text, there is a chance that we will
   effectively delete it from the text. the id203 that we cut the
   word is related to the word   s frequency.

   if we have a window size of 10, and we remove a specific instance of
      the    from our text:
    1. as we train on the remaining words,    the    will not appear in any of
       their context windows.
    2. we   ll have 10 fewer training samples where    the    is the input word.

   note how these two effects help address the two problems stated above.

negative sampling:

   as we discussed above, the size of our word vocabulary means that our
   skip-gram neural network has a tremendous number of weights, all of
   which would be updated slightly by every one of our billions of
   training samples!

   negative sampling addresses this by having each training sample only
   modify a small percentage of the weights, rather than all of them.
   here   s how it works.

   when training the network on the word pair (   fox   ,    quick   ), recall
   that the    label    or    correct output    of the network is a one-hot
   vector. that is, for the output neuron corresponding to    quick    to
   output a 1, and for all of the other thousands of output neurons to
   output a 0.

   with negative sampling, we are instead going to randomly select just a
   small number of    negative    words (let   s say 5) to update the weights
   for. (in this context, a    negative    word is one for which we want the
   network to output a 0 for). we will also still update the weights for
   our    positive    word (which is the word    quick    in our current example).

   the paper says that selecting 5   20 words works well for smaller
   datasets, and you can get away with only 2   5 words for large datasets.

   recall that the output layer of our model has a weight matrix that   s
   300 x 10,000. so we will just be updating the weights for our positive
   word (   quick   ), plus the weights for 5 other words that we want to
   output 0. that   s a total of 6 output neurons, and 1,800 weight values
   total. that   s only 0.06% of the 3m weights in the output layer!

   in the hidden layer, only the weights for the input word are updated
   (this is true whether you   re using negative sampling or not).

   the    negative samples    (that is, the 5 output words that we   ll train to
   output 0) are chosen using a    unigram distribution   .

   essentially, the id203 for selecting a word as a negative sample
   is related to its frequency, with more frequent words being more likely
   to be selected as negative samples.

   credits: chris mccormick

   [22]http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-gram-m
   odel/

   [23]http://mccormickml.com/2017/01/11/id97-tutorial-part-2-negative
   -sampling/

     * [24]machine learning
     * [25]deep learning
     * [26]id97
     * [27]nlp
     * [28]towards data science

   (button)
   (button)
   (button) 908 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of manish chablani

[30]manish chablani

   ai in healthcare [31]@curaihq, marathoner. (past: dl for self driving
   cars [32]@cruise, ml [33]@uber, early engineer [34]@microsoftazure
   cloud storage)

     (button) follow
   [35]towards data science

[36]towards data science

   sharing concepts, ideas, and codes.

   responses
   the author has chosen not to show responses on this story. you can
   still respond by clicking the response bubble.

     * (button)
       (button) 908
     * (button)
     *
     *

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/78614e4d6e0b
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/id97-skip-gram-model-part-1-intuition-78614e4d6e0b&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/id97-skip-gram-model-part-1-intuition-78614e4d6e0b&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_nn23mvlq8f9r---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@manishchablani?source=post_header_lockup
  17. https://towardsdatascience.com/@manishchablani
  18. http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-gram-model/
  19. http://mccormickml.com/2017/01/11/id97-tutorial-part-2-negative-sampling/
  20. https://code.google.com/archive/p/id97/
  21. http://arxiv.org/pdf/1310.4546.pdf
  22. http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-gram-model/
  23. http://mccormickml.com/2017/01/11/id97-tutorial-part-2-negative-sampling/
  24. https://towardsdatascience.com/tagged/machine-learning?source=post
  25. https://towardsdatascience.com/tagged/deep-learning?source=post
  26. https://towardsdatascience.com/tagged/id97?source=post
  27. https://towardsdatascience.com/tagged/nlp?source=post
  28. https://towardsdatascience.com/tagged/towards-data-science?source=post
  29. https://towardsdatascience.com/@manishchablani?source=footer_card
  30. https://towardsdatascience.com/@manishchablani
  31. http://twitter.com/curaihq
  32. http://twitter.com/cruise
  33. http://twitter.com/uber
  34. http://twitter.com/microsoftazure
  35. https://towardsdatascience.com/?source=footer_card
  36. https://towardsdatascience.com/?source=footer_card

   hidden links:
  38. https://medium.com/p/78614e4d6e0b/share/twitter
  39. https://medium.com/p/78614e4d6e0b/share/facebook
  40. https://medium.com/p/78614e4d6e0b/share/twitter
  41. https://medium.com/p/78614e4d6e0b/share/facebook
