   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id97 (skip-gram model): part 2         implementation in tf

   [16]go to the profile of manish chablani
   [17]manish chablani (button) blockedunblock (button) followfollowing
   jun 14, 2017

   jupyter notebook:
   [18]https://github.com/mchablani/deep-learning/blob/master/embeddings/s
   kip-gram_id97.ipynb

   tensor flow has built in support for most of the scaffolding needed for
   skip-gram id97t including embedding lookup and negative sampling.
tf.nn.embedding_lookup
tf.nn.sampled_softmax_loss

preprocessing

   tokenize the input and convert the input into int representation. have
   look ups for word to int and vice versa.

subsampling

   words that show up often such as    the   ,    of   , and    for    don   t provide
   much context to the nearby words. if we discard some of them, we can
   remove some of the noise from our data and in return get faster
   training and better representations. this process is called subsampling
   by mikolov. for each word wi in the training set, we   ll discard it with
   id203 given by
   [1*5dcmgnu2upo8j-2eprm4wa.jpeg]
from collections import counter
import random
threshold = 1e-5
word_counts = counter(int_words)
total_count = len(int_words)
freqs = {word: count/total_count for word, count in word_counts.items()}
p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}
train_words = [word for word in int_words if random.random() < (1 - p_drop[word]
)]

building the graph

inputs = tf.placeholder(tf.int32, [none], name=   inputs   )
labels = tf.placeholder(tf.int32, [none, none], name=   labels   )

   note labels is 2 dimensional as required by tf.nn.sampled_softmax_loss
   used for negative sampling.

   the embedding matrix has a size of the number of words by the number of
   units in the hidden layer. so, if you have 10,000 words and 300 hidden
   units, the matrix will have size 10,000  300. remember that we   re using
   tokenized data for our inputs, usually as integers, where the number of
   tokens is the number of words in our vocabulary.
n_vocab = len(int_to_vocab)
n_embedding =  300
embedding = tf.variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))
embed = tf.nn.embedding_lookup(embedding, inputs)

negative sampling

   we   ll update the weights for the correct label, but only a small number
   of incorrect labels. this is called [19]   negative sampling   . tensorflow
   has a convenient function to do this, [20]tf.nn.sampled_softmax_loss.
# number of negative labels to sample
n_sampled = 100
softmax_w = tf.variable(tf.truncated_normal((n_vocab, n_embedding))) softmax_b =
 tf.variable(tf.zeros(n_vocab), name="softmax_bias")

# calculate the loss using negative sampling
loss = tf.nn.sampled_softmax_loss(
    weights=softmax_w,
    biases=softmax_b,
    labels=labels,
    inputs=embed,
    num_sampled=n_sampled,
    num_classes=n_vocab)

cost = tf.reduce_mean(loss)
optimizer = tf.train.adamoptimizer().minimize(cost)

training

batches = get_batches(train_words, batch_size, window_size)
for x, y in batches:
    feed = {inputs: x, labels: np.array(y)[:, none]}
    train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)

visualizing the word vectors using id167

%matplotlib inline
%config inlinebackend.figure_format = 'retina'
import matplotlib.pyplot as plt
from sklearn.manifold import tsne
embed_mat = sess.run(embedding)
viz_words = 500
tsne = tsne()
embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])
fig, ax = plt.subplots(figsize=(14, 14))
for idx in range(viz_words):
    plt.scatter(*embed_tsne[idx, :], color='steelblue')
    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), al
pha=0.7)

   credits: from lecture notes:
   [21]https://classroom.udacity.com/nanodegrees/nd101/syllabus

     * [22]machine learning
     * [23]deep learning
     * [24]id97
     * [25]nlp
     * [26]embedding

   (button)
   (button)
   (button) 69 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of manish chablani

[28]manish chablani

   ai in healthcare [29]@curaihq, marathoner. (past: dl for self driving
   cars [30]@cruise, ml [31]@uber, early engineer [32]@microsoftazure
   cloud storage)

     (button) follow
   [33]towards data science

[34]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 69
     * (button)
     *
     *

   [35]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [36]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/7efdf6f58a27
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/id97-skip-gram-model-part-2-implementation-in-tf-7efdf6f58a27&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/id97-skip-gram-model-part-2-implementation-in-tf-7efdf6f58a27&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_mchlioxl1tlj---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@manishchablani?source=post_header_lockup
  17. https://towardsdatascience.com/@manishchablani
  18. https://github.com/mchablani/deep-learning/blob/master/embeddings/skip-gram_id97.ipynb
  19. http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  20. https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss
  21. https://classroom.udacity.com/nanodegrees/nd101/syllabus
  22. https://towardsdatascience.com/tagged/machine-learning?source=post
  23. https://towardsdatascience.com/tagged/deep-learning?source=post
  24. https://towardsdatascience.com/tagged/id97?source=post
  25. https://towardsdatascience.com/tagged/nlp?source=post
  26. https://towardsdatascience.com/tagged/embedding?source=post
  27. https://towardsdatascience.com/@manishchablani?source=footer_card
  28. https://towardsdatascience.com/@manishchablani
  29. http://twitter.com/curaihq
  30. http://twitter.com/cruise
  31. http://twitter.com/uber
  32. http://twitter.com/microsoftazure
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/7efdf6f58a27/share/twitter
  39. https://medium.com/p/7efdf6f58a27/share/facebook
  40. https://medium.com/p/7efdf6f58a27/share/twitter
  41. https://medium.com/p/7efdf6f58a27/share/facebook
