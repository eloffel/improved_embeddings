   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

deep learning id98   s in tensorflow with gpus

   [14]go to the profile of cole murray
   [15]cole murray (button) blockedunblock (button) followfollowing
   may 31, 2017
   [1*4-9rcfvshvsa4mhcprhasg.jpeg]

   in [16]my last tutorial, you created a complex convolutional neural
   network from a pre-trained inception v3 model.

   in this tutorial, you   ll learn the architecture of a convolutional
   neural network (id98), how to create a id98 in tensorflow, and provide
   predictions on labels of images. finally, you   ll learn how to run the
   model on a gpu so you can spend your time creating better models, not
   waiting for them to converge.
   [1*yozche1tofcnbcxonyx3tw.gif]

overview

     * introduction to id98   s
     * creating your first id98 and training on cpu
     * training on a gpu

prerequisites

     * basic machine learning understanding
     * basic tensorflow understanding
     * aws account (for gpu)
     __________________________________________________________________

convolutional neural networks

   convolutional neural networks are the current state-of-art architecture
   for image classification. they   re used in practice today in facial
   recognition, self driving cars, and detecting whether an object is a
   hot-dog.

basic architecture

   the basics of a id98 architecture consist of 3 components. a
   convolution, pooling, and fully connected layer. these components work
   together to learn a dense feature representation of an input.

   convolution
   [1*1vjdp6qdy9-extuqveolvg.gif]
   image from [17]https://github.com/vdumoulin/conv_arithmetic

   a convolution consists of a kernel (green square above), also called
   filter, that is applied in a sliding window fashion to extract features
   from the input. this filter is shifted after each operation across the
   input by an amount called strides. at each operation, a matrix multiply
   of the kernel and current region of input is calculated. filters can be
   stacked to create high-dimensional representations of the input.

     what happens if the filter doesn   t evenly map to the size of the
     input ?

   there are two ways of handling differing filter size and input size,
   known as same padding and valid padding. same padding will pad the
   input border with zeros (as seen above) to ensure the input width and
   height are preserved. valid padding does not pad.

   typically, you   ll want to use same padding or you   ll rapidly reduce the
   dimensionality of your input.

   finally, an activation function (typically a [18]relu) is applied to
   give the convolution non-linearity. relu   s are a bit different from
   other id180, such as sigmoid or tanh, as relus are
   one-sided. this one-sided property allows the network to create sparse
   representation (zero value for hidden units), increasing computational
   efficiency.
   [1*5dklg6cdkstdhmkl2spfma.png]
   relu

   pooling

   pooling is an operation to reduce dimensionality. it applies a function
   summarizing neighboring information. two common functions are max
   pooling and average pooling. by calculating the max of an input region,
   the output summarizes intensity of surrounding values.

   pooling layers also have a kernel, padding and are moved in strides. to
   calculate the output size of a pooling operation, you can use the
   formula
 (input width - kernel width + 2 * padding) / strides + 1.

   [1*ixw2w_3zpiaerknorecxig.jpeg]

   fully connected layer

   fully connected layers you are likely familiar with from neural
   networks. each neuron in the input is connected to each neuron in the
   output; fully-connected. due to this connectivity, each neuron in the
   output will be used at most one time.
   [1*jnvyw8ehgc92byy9v9zxzg.gif]
   [1*kdnux0kw1yq4d8dq__myca.png]
   fully connected neural network

   in a id98, the input is fed from the pooling layer into the fully
   connected layer. depending on the task, a regression or classification
   algorithm can be applied to create the desired output.

   review

   you   ve now learned about what makes up a convolutional neural network.
   by passing input through a convolution, you extract highly-dimensional
   features. pooling summarizes spatial information and reduces
   dimensionality. lastly, this feature representation is passed through
   fully connected layers to a classifier or regressor.
   [1*xbuw8wurray5pc4t-9dzaq.jpeg]
   full id98 architecture ([19]source)
     __________________________________________________________________

creating a id98 in tensorflow

   now that you have the idea behind a convolutional neural network,
   you   ll code one in tensorflow.

   you   ll be creating a id98 to train against the [20]mnist (images of
   handwritten digits) dataset. after training, you   ll achieve ~98.0%
   accuracy @ 10k iterations.

   setup environment

   first you   ll need to setup your environment. additionally, you   ll
   create a setup.py file. anaconda environment files for python3.5 and
   python2.7 are listed below.

   iframe: [21]/media/a1df495e972b1de77f9c3adc4d84b7ab?postid=cba6efe0acc2

   iframe: [22]/media/441d3396ba831ae16d1f08fa4e839ea2?postid=cba6efe0acc2

   if you do not use anaconda, you can install tensorflow via pip:
$ pip install tensorflow

   iframe: [23]/media/b1089929eb500fc6e5c1646e047fed3f?postid=cba6efe0acc2

   run:
python3 setup.py develop

the data

   [1*llfcenth771mzqps3sgaba.png]
   mnist data

   here, you   ll create 3 separate inputs; a training set, validation set,
   and test set. a validation set allows you to better train your model by
   providing additional data to tune hyper parameters against.

   download the data

   the data can be retrieved with this command:
$ curl [24]https://pjreddie.com/media/files/mnist_train.csv -o data/mnist_train.
csv # 104 mb
$ curl [25]https://pjreddie.com/media/files/mnist_test.csv -o data/mnist_test.cs
v # 17.4 mb

   iframe: [26]/media/40a841ebe57607ede7a70b275d29a44b?postid=cba6efe0acc2

   mnist.py

architecture

   here, you   ll create a few helper functions for creating the network.
   these functions are used to create the individual components discussed
   earlier.

   helper functions / model definition:

   iframe: [27]/media/72e0347a8b71c9be4e6d074162208b94?postid=cba6efe0acc2

   helper functions and model definition

   model
   [1*fur04a6obbjmgq-qj87-lg.png]
   tensorflow graph of model

   here   s the code for training the model. the three public functions are
   explained below:

   iframe: [28]/media/b0f2d8c05a253ceb1dc440df42f4ee13?postid=cba6efe0acc2

     code is available here:
     [29]https://github.com/colemurray/tensorflow-id98-tutorial/tree/add_m
     odel_functions

   id136. this function is responsible for creating a prediction it
   believes the input represents. here, it will return a 1x10 tensor for
   each input. values contained in this tensor will be passed to the loss
   function to determine how far off this prediction is from ground truth.

   as indicated by the batch_size hyper parameter, you are processing 128
   images at a time. this technique is known as mini-batch. by processing
   inputs in smaller batches, as opposed to the entire dataset, input can
   be fit in memory. additionally, the model will converge more rapidly
   due to updating the weights after each batch rather than after
   processing all examples.

   loss. here, you   ll use the softmax cross id178 function to perform an
   n way classification. the softmax function is used to normalize
   (summing the tensor adds to one) the input produced from the id136
   function.

   with this normalized tensor, cross id178 is calculated against the
   one hot encoded labels. cross id178 gives a measure of how far off
   the prediction is from the ground truth. each iteration, an optimizer
   is applied to minimize this cross id178.
   [1*yovalblsqhytzmdbfx300g.gif]
   cross id178
   [1*0u4ig8nww3c1w3xbg-09ja.png]
   loss after training

train & evaluate

   below you   ll train the model for 10k iterations. each 1000 iterations,
   you   ll test the model against the validation set to get an idea of the
   accuracy. finally, you   ll evaluate the trained model against the test
   dataset to get a measure of out-of-sample accuracy. at 10k iterations,
   you should see accuracy around 98.0%.

   to execute this code, run this command:
$ python3 mnist_conv2d_medium_tutorial/train.py
(building the computational graph can take a few seconds depending on hardware)

   iframe: [30]/media/1a7b2adefff7ee60a3f1e8e9c4bd329c?postid=cba6efe0acc2

   with the model trained, you   ll now evaluate it on the test set from the
   last checkpoint.

   iframe: [31]/media/a724edf4e03e93dc8242441a41078e51?postid=cba6efe0acc2

$ python3 mnist_conv2d_medium_tutorial/evaluate.py

     code up to this point:
     [32]https://github.com/colemurray/tensorflow-id98-tutorial/tree/train
     _and_evaluate

   you can visualize your results by running:
$ tensorboard --logdir=graphs/ --port=6006
navigate in browser: localhost:6006
     __________________________________________________________________

training on a gpu

   as you noticed, training a id98 can be quite slow due to the amount of
   computations required for each iteration. you   ll now use gpu   s to speed
   up the computation.

   tensorflow, by default, gives higher priority to gpu   s when placing
   operations if both cpu and gpu are available for the given operation.
   for simplifying the tutorial, you won   t explicitly define operation
   placement. you can read more about how to do this [33]here.

create a gpu box

   for this tutorial, you   ll use a community ami. head over to the aws
   console and launch a new ec2 instance. at the ami screen, select
   community and enter this ami id: ami-5e853c48. this ami comes with
   tensorflow and nvidia drivers with cuda pre-installed.
   [1*ht_gj2wftcf1rwljfwo0zg.png]
   ami id: ami-5e853c48

   for instance type, select g2.2xlarge. after selecting an instance type,
   be sure to create a key-pair. this key-pair will allow you to [34]ssh
   into the instance and copy/execute your code.
   [1*qsxucklyy7-ldnne6r269g.png]
   g2.2xlarge

   sync your code

   now that your instance is created, you   ll need to copy your code and
   dataset onto it. the easiest way to do this is with rsync. [35]rsync is
   a unix command built on top of ssh that allows for efficient file
   transfer. it   s highly flexible, offering multiple options to directly
   alter the behavior. below, the command will copy your project directory
   to your gpu instance user   s home directory.
rsync -trucv mnist_conv2d_medium_tutorial ip-address-of-your-gpu-box:/home/ubunt
u/

   run the code

   below, you   ll ssh into the instance and install the package. after
   installation, run the train command. after running the train command,
   you   ll see output indicating where the operations are being placed. as
   shown below, operations are being placed onto the gpu as expected.
$ ssh ubuntu@ip-address-of-your-gpu-box
$ cd mnist-conv2d-medium-tutorial
$ pip3 install .
$ python3 mnist_conv2d_medium_tutorial/train.py

   [1*mjdhnjwa5pbnwabulk3hxg.png]
   operations being placed on gpu

   after ~20 mins, training will complete and you can run the evaluate
   command to test against the test set.
$ python3 mnist_conv2d_medium_tutorial/evaluate.py

   conclusion

   in this tutorial you learned the concept behind convolutional neural
   networks. additionally, you learned the tensorflow implementation of a
   basic id98 to achieve ~98.0% accuracy. finally, you learned how to run
   your code on a gpu for performance improvement.

   complete code here:
   [36]colemurray/tensorflow-id98-tutorial
   tensorflow-id98-tutorial - tensorflow tutorial on convolutional neural
   networks.github.com

   next steps:
     * play with hyperparameters (batch size, learning rate, kernel size,
       number of iterations) to see how it affects model performance
     * train and evaluate your model against other datasets ([37]cifar-10)
     * go deeper
     __________________________________________________________________

call to action:

     if you enjoyed this tutorial, follow and recommend!

     interested in learning more about deep learning / machine learning?
     check out my other tutorials:

     [38]- deep learning with keras on google compute engine

     [39]- id126s with apache spark on google compute
     engine

     other places you can find me:

     - twitter: [40]https://twitter.com/_colemurray

   [41][1*0hqoaabq7xgpt-oyngiubg.png]
   [42][1*vgw1jka6hgnvwztsfmlnpg.png]
   [43][1*gkbpq1ruui0fvk2um_i4tq.png]

     [44]hacker noon is how hackers start their afternoons. we   re a part
     of the [45]@ami family. we are now [46]accepting submissions and
     happy to [47]discuss advertising & sponsorship opportunities.

     if you enjoyed this story, we recommend reading our [48]latest tech
     stories and [49]trending tech stories. until next time, don   t take
     the realities of the world for granted!

   [1*35tcjopcvq6lbb3i6wegqw.jpeg]

     * [50]machine learning
     * [51]deep learning
     * [52]artificial intelligence
     * [53]tensorflow
     * [54]neural networks

   (button)
   (button)
   (button) 646 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [55]go to the profile of cole murray

[56]cole murray

   machine learning engineer | personalization @ amazon |
   [57]https://murraycole.com

     (button) follow
   [58]hacker noon

[59]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 646
     * (button)
     *
     *

   [60]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [61]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/cba6efe0acc2
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/deep-learning-id98s-in-tensorflow-with-gpus-cba6efe0acc2&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/deep-learning-id98s-in-tensorflow-with-gpus-cba6efe0acc2&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_7zzpaw7gwhwf---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@colemurray?source=post_header_lockup
  15. https://hackernoon.com/@colemurray
  16. https://medium.com/google-cloud/keras-inception-v3-on-google-compute-engine-a54918b0058
  17. https://github.com/vdumoulin/conv_arithmetic
  18. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  19. https://www.mathworks.com/content/mathworks/www/en/discovery/convolutional-neural-network/jcr:content/mainparsys/image_copy.adapt.full.high.jpg/1492406018870.jpg
  20. https://pjreddie.com/projects/mnist-in-csv/
  21. https://hackernoon.com/media/a1df495e972b1de77f9c3adc4d84b7ab?postid=cba6efe0acc2
  22. https://hackernoon.com/media/441d3396ba831ae16d1f08fa4e839ea2?postid=cba6efe0acc2
  23. https://hackernoon.com/media/b1089929eb500fc6e5c1646e047fed3f?postid=cba6efe0acc2
  24. https://pjreddie.com/media/files/mnist_train.csv
  25. https://pjreddie.com/media/files/mnist_test.csv
  26. https://hackernoon.com/media/40a841ebe57607ede7a70b275d29a44b?postid=cba6efe0acc2
  27. https://hackernoon.com/media/72e0347a8b71c9be4e6d074162208b94?postid=cba6efe0acc2
  28. https://hackernoon.com/media/b0f2d8c05a253ceb1dc440df42f4ee13?postid=cba6efe0acc2
  29. https://github.com/colemurray/tensorflow-id98-tutorial/tree/add_model_functions
  30. https://hackernoon.com/media/1a7b2adefff7ee60a3f1e8e9c4bd329c?postid=cba6efe0acc2
  31. https://hackernoon.com/media/a724edf4e03e93dc8242441a41078e51?postid=cba6efe0acc2
  32. https://github.com/colemurray/tensorflow-id98-tutorial/tree/train_and_evaluate
  33. https://www.tensorflow.org/tutorials/using_gpu
  34. https://en.wikipedia.org/wiki/secure_shell
  35. https://linux.die.net/man/1/rsync
  36. https://github.com/colemurray/tensorflow-id98-tutorial
  37. https://www.cs.toronto.edu/~kriz/cifar.html
  38. https://medium.com/google-cloud/keras-inception-v3-on-google-compute-engine-a54918b0058
  39. https://medium.com/google-cloud/recommendation-systems-with-spark-on-google-dataproc-bbb276c0dafd
  40. https://twitter.com/_colemurray
  41. http://bit.ly/hackernoonfb
  42. https://goo.gl/k7xybx
  43. https://goo.gl/4ofytp
  44. http://bit.ly/hackernoon
  45. http://bit.ly/atamiatami
  46. http://bit.ly/hackernoonsubmission
  47. mailto:partners@amipublications.com
  48. http://bit.ly/hackernoonlatestt
  49. https://hackernoon.com/trending
  50. https://hackernoon.com/tagged/machine-learning?source=post
  51. https://hackernoon.com/tagged/deep-learning?source=post
  52. https://hackernoon.com/tagged/artificial-intelligence?source=post
  53. https://hackernoon.com/tagged/tensorflow?source=post
  54. https://hackernoon.com/tagged/neural-networks?source=post
  55. https://hackernoon.com/@colemurray?source=footer_card
  56. https://hackernoon.com/@colemurray
  57. https://murraycole.com/
  58. https://hackernoon.com/?source=footer_card
  59. https://hackernoon.com/?source=footer_card
  60. https://hackernoon.com/
  61. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  63. https://github.com/colemurray/tensorflow-id98-tutorial
  64. https://medium.com/p/cba6efe0acc2/share/twitter
  65. https://medium.com/p/cba6efe0acc2/share/facebook
  66. https://medium.com/p/cba6efe0acc2/share/twitter
  67. https://medium.com/p/cba6efe0acc2/share/facebook
