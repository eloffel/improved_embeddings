   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]datalogue
     * [9]archive
     * [10]hiring
     * [11]datalogue.io
     __________________________________________________________________

how to visualize your recurrent neural network with attention in keras

a technical discussion and tutorial

   [12]go to the profile of zafarali ahmed
   [13]zafarali ahmed (button) blockedunblock (button) followfollowing
   jun 29, 2017

   neural networks are taking over every part of our lives. in
   particular         thanks to deep learning         siri can fetch you a taxi using
   your voice; and google can enhance and organize your photos
   automagically. here at [14]datalogue, we use deep learning to
   structurally and semantically understand data, allowing us to prepare
   it for use automatically.

   neural networks are massively successful in the domain of [15]computer
   vision. specifically, [16]convolutional neural networks(id98s) take
   images and extract relevant features from them by using small windows
   that travel over the image. this understanding can be leveraged to
   identify objects from your camera ([17]google lens) and, in the future,
   even drive your car ([18]nvidia).

   the analogous neural network for text data is the [19]recurrent neural
   network (id56). this kind of network is designed for sequential data and
   applies the same function to the words or characters of the text. these
   models are successful in translation ([20]google translate), speech
   recognition ([21]cortana) and [22]language generation.

   dr. yoshua bengio from universit   de montr  al believes that
   [23]language is going to be the next challenge for neural networks. at
   [24]datalogue, we deal with tons of text data, and we are interested in
   helping the community solve this challenge.

   in this tutorial, we will write an id56 in [25]keras that can translate
   human dates (   november 5, 2016   ,    5th november 2016   ) into a standard
   format (   2016   11   05   ). in particular, we want to gain some intuition
   into how the neural network did this. we will leverage the concept of
   attention to generate a map (like that shown in figure 1) that shows
   which input characters were important in predicting the output
   characters.
   [1*xoh3scvhxuoa2tpzektgza.png]
   figure 1: attention map for the freeform date    5 jan 2016   . we can see
   that the neural network used    16    to decide that the year was 2016,
      ja    to decide that the month was 01 and the first bit of the date to
   decide the day of the month.

what is in this tutorial

   we start off with some technical background material to get everyone on
   the same page and then move on to programming this thing! throughout
   the tutorial, i will provide links to more advanced content.

if you want to directly jump to the code:

   [26]datalogue/keras-attention
   keras-attention - visualizing id56s using the attention
   mechanismgithub.com

what you need to know

   to be able to jump into the coding part of this tutorial, it would be
   best if you have some familiarity with python and [27]keras. you should
   have some familiarity of id202, in particular that neural
   networks are just a few weight matrices with some non-linearities
   applied to it.

   the intuition of id56s and id195 models will be explained below.

recurrent neural networks (id56)

   an id56 is a function that applies the same transformation (known as the
   id56 cell or step) to every element of a sequence. the output of an id56
   layer is the output of the id56 cell applied on each element of the
   sequence. in the case of text, these are usually successive words or
   characters. in addition to this, id56 cells hold an internal memory that
   summarize the history of the sequence it has seen so far.
   [1*w1etilx-aaocdg2vbdbdzw.png]
   figure 2: an example of an id56 layer. the id56 cell is applied to each
   element xi in the original sequence to get the corresponding element hi
   in the output sequence. the id56 cell uses a previous hidden state
   (propagated between the id56 cells) and the sequence element xi to do
   its calculations.

   the output of the id56 layer is an encoded sequence, h, that can be
   manipulated and passed into another network. id56s are [28]incredibly
   flexible in their inputs and outputs:
    1. many-to-one: use the complete input sequence to make a single
       prediction h.
    2. one-to-many: transforming a single input to generate a sequence h.
    3. many-to-many: transforming the entire input sequence into another
       sequence.

   in theory, the sequences in our training data need not be of the same
   length. in practice, we pad or truncate them to be of the same length
   to take advantage of the static [29]computational graph in
   [30]tensorflow.

   we will focus on #3    many-to-many    also known as sequence-to-sequence
   (id195).

   long sequences can be difficult to learn from in id56s [31]due to
   instabilities in gradient calculations during training. to solve this,
   the id56 cell is replaced by a gated cell like the [32]gated recurrent
   unit (gru) or the [33]long-short term memory cell (lstm). to learn more
   about lstms and gated units, i highly recommend reading [34]christopher
   olah   s blog (it   s where i first started understanding the id56 cell).
   from now on, whenever we talk about id56s, we are talking about a gated
   cell. [35]since olah   s blog gives an intuitive introduction to lstms,
   we will use that.

a general framework for id195: the encoder-decoder setup

   almost all neural network approaches to solving the id195 problem
   involve:
    1. encoding the input sentences into some abstract representation.
    2. manipulating this encoding.
    3. decoding it to our target sequence.

   our encoders and decoders can be any kind and combination of neural
   networks. in practice, most people use id56s for both the encoders and
   decoders.
   [1*iin3ew_tdkzxxilihxbjaa.png]
   figure 3: set up of the encoder-decoder architecture. the encoder
   network processes the input sequence into an encoded sequence which is
   subsequently used by the decoder network to produce the output.

   figure 3 shows a simple encoder-decoder setup. the encoding step
   usually produces a sequence of vectors, h, corresponding to the
   sequence of characters in the input date, x. in an id56 encoder, each
   vector is generated by integrating information from the past sequence
   of vectors.

   before h is passed onto the decoder, we may choose to manipulate it in
   preparation for the decoder. for example, we might choose to only use
   the last encoding, as shown in figure 4, since in theory, it is a
   summary of the whole sequence.
   [1*6xlyzygtmuwjno2psywy_q.png]
   figure 4: use of a summary state in the encoder-decoder architecture.

   intuitively, this is similar to summarizing the whole input date into a
   single representation and then trying to decode that. while there may
   be enough information in this summary state for a classification
   problem like detecting sentiment (many-to-one), it might be
   insufficient for an effective translation where it helps to consider
   the full sequence of hidden states (figure 5).
   [1*-femg-czr0xyt7sd4-kbvq.png]
   figure 5: use of the complete encoded sequence in the decoder network.

   however, this is not how humans translate dates: we do not read the
   whole text and then independently write down the translation at each
   character. intuitively, a person would understand that the characters
      jan    correspond to 1st month,    5    corresponds to the day and    2016   
   corresponds to the year. as we have already seen in figure 1, this idea
   known as attention can be captured by id56s and has been applied
   successfully in [36]id134 (xu et al. 2015), [37]speech
   recognition (chan et al. 2015) and indeed [38]machine translation
   (bahdanau et al. 2014). most importantly, they produce interpretable
   models.

   a slightly more visual example of how the attention mechanism works
   comes from the xu et. al, 2015 paper (figure 6). in the most
   complicated example of the girl with the teddy, we can see that when
   generating the word    girl   , the attention mechanism correctly focuses
   on the girl and not the teddy! pretty brilliant. not only does this
   make for pretty pictures, but it also let the authors diagnose problems
   in the model.
   [1*ribyyjau7epdzthfr0ndhw.png]
   figure 6: attending to objects in an image during id134.
   the white regions indicate where the attention mechanism focused on
   during the generation of the underlined word. from [39]xu, kelvin, et
   al.    show, attend and tell: neural image id134 with visual
   attention.    international conference on machine learning. 2015.

   the creators of [40]spacy have an [41]in-depth overview of the
   encoder-attention-decoder paradigm. for other ways of modifying id56s
   you can head over to [42]this distill article.

   this tutorial will feature a single bidirectional lstm as the encoder
   and the attention decoder (more on this later). more concretely, we
   will implement a simpler version of the model presented in [43]   neural
   machine translation by jointly learning to align and translate.   
   ([44]bahdanau, dzmitry, kyunghyun cho, and yoshua bengio.2014). i   ll
   walk through some of the math, but i invite you to jump into the
   appendices of the paper to get your hands dirty!

   now that we have learned about id56s and the intuition behind the
   attention mechanism, let us learn how to implement it and subsequently
   obtain some nice visualizations. all code for subsequent sections is
   provided at [45]datalogue/keras-attention. the complete implementation
   of the model below is in [46]/models/id4.py

the encoder

   since we are trying to learn about attentionid56s, we will skip
   implementing our own vanilla id56 (lstm) and use the one that ships with
   [47]keras. we can [48]invoke it using:
blstm = bidirectional(lstm(encoder_units, return_sequences=true))

   the parameter encoder_units is the size of the weight matrix. we use
   return_sequences=true here because we'd like to access the complete
   encoded sequence rather than the final summary state as described
   above.

   our blstm will consume the characters in the input sentence
   x=(x1,...,xt) and output an encoded sequence h=(h1,...,ht) where t is
   the number of characters in the date. note that this is slightly
   different to the [49]bahdanau et al. paper where the sentence is a
   collection of words rather than characters. we also do not refer to the
   encoded sequence as the annotations like in the original paper.

the decoder

   [1*1hwgbkceovnd0o9aeob3sg.png]
   figure 7: overview of the attention mechanism in an encoder-decoder
   setup. the attention mechanism creates context vectors. the decoder
   network uses these context vectors as well as the previous prediction
   to make the next one. the red arrows highlight which characters the
   attention mechanism will weigh highly in producing the output
   characters    1    and    6   .

   now for the interesting part: the decoder. for any given character at
   position t in the sequence, our decoder accepts the encoded sequence
   h=(h1,...,ht) as well as the previous hidden state st-1(shared within
   the decoder cell) and character yt-1. our decoder layer will output
   y=(y1,...,yt)(the characters in the standardized date). our overall
   architecture is summarized in figure 7.

equations

   as shown in figure 6, the decoder is quite complicated. so let   s break
   it down into the steps executed by the decoder cell when trying to
   predict character t.in the following equations, the capital letter
   variables represent trainable parameters (note that i have dropped the
   bias terms for brevity.)
   [1*ekrqpw2bbthcrm2roejr5a.png]
   equation 1: a [50]feed-forward neural network that calculates the
   unnormalized importance of character j in predicting character t.
   equation 2: the [51]softmax operation that normalizes the id203.
    1. calculate the attention probabilities   =(  1,   ,  t) based on the
       encoded sequence and the internal hidden state of the decoder cell,
       st-1. these are shown in equation 1 and equation 2.

   [1*59ngdya-rof5p8w1_r0odq.png]
   figure 3: calculation of the context vector for the t-th character.

   2. calculate the context vector which is the weighted sum of the
   encoded sequence with the attention probabilities. intuitively, this
   vector summarizes the importance of the different encoded characters in
   predicting the t-th character.
   [1*khunml5vcvmu4mjyxe-sfw.png]
   equation 4: reset gate. equation 5: update gate. equation 6: proposal
   hidden state. equation 7: new hidden state.

   3. we then update our hidden state. if you are familiar with the
   equations of an lstm cell, these might be ring a bell as the reset
   gate, r, update gate, z, and the proposal state. we use the reset gate
   to control how much information from the previous hidden state st-1 is
   used to create a proposal hidden state. the update gate controls how we
   much of the proposal we use in the new hidden state st. (confused?
   [52]see step by step walk through of lstm equations)
   [1*ipmk_6b1-f0b1hey5li-gw.png]
   equation 8: a simple neural network to predict the next character.

   4. calculate the t-th character using a simple one layer neural network
   using the context, hidden state, and previous character. this is a
   modification from the paper which uses a [53]maxout layer. since we are
   trying to keep things as simple as possible this works fine!

   equations 1   8 are applied for every character in the encoded sequence
   to produce a decoded sequence y which represents the id203 of a
   translated character at each position.

code

   our custom layer is implemented in [54]models/custom_recurrent.py. this
   part is somewhat complicated in particular because of the manipulations
   we need to make for vectorized operations acting on the complete
   encoded sequence. it will make more sense when you think about it. i
   promise it will become easier the more you look at the equations and
   the code simultaneously.

   a minimal custom keras layer has to implement a few methods: __init__,
   compute_ouput_shape, build and call. for completeness, we also
   implement get_config which allows you to load the model back into
   memory easily. in addition to these, a keras recurrent layer implements
   a step method that will hold all the computations of our cell.

   first let us break down boiler-plate layer [55]code:
     * __init__ is what is called when the layer is first instantiated. it
       sets functions that will eventually initialize the weights,
       regularizers, and constraints. since the output of our layer is a
       sequence, we hard code self.return_sequences=true.
     * build is called when we run model.compile(   ). since our model is
       quite complicated, you can see there are a ton of weights to
       initialize here. the call self.add_weight automatically handles
       initializing the weights and setting them as trainable within the
       model. weights with the subscript a are used to calculate the
       context vector (step 1 and 2). weights with the subscript r, z, p
       will calculate the new hidden states from step 3. finally, weights
       with the subscript o will calculate the output of the layer.
     * some convenience functions are implemented as well: (a)
       compute_output_shape will calculate output shapes for any given
       input; (b)get_config let   s us load the model using just a saved
       file (once we are done training)

   now for the cell logic:
     * by default, each execution of the cell only has information from
       the previous time step. since we need to access the entire encoded
       sequence within the cell, we need to save it somewhere. therefore,
       we make a simple modification in call. the only way i could find to
       do this was to set the sequence being fed into the cell as self.x
       so that we can access it later:

   iframe: [56]/media/2abbc964f1074769bc44393de70c657b?postid=1892773a4f22

   now we need to think vectorized: the _time_distributed_dense function
   calculates the last term of equation 1 for all the elements of the
   encoded sequence.
     * we now walk through the most important part of the code, that is in
       step which executes the cell logic. recall that step is applied to
       every element of the input sequence.

   iframe: [57]/media/2f2a29d8d24f191db2f54223a61c7486?postid=1892773a4f22

   in this cell we want to access the previous character ytm and hidden
   state stm which is obtained from states in line 4.

   think vectorized: we manipulate stm to repeat it for the number of
   characters we have in our input sequence.

   on lines 11   18 we implement a version of equation 1 that does the
   calculations on all the characters in the sequence at once.

   in lines 24   28 we have implemented equation 2 in the vectorized form
   for the whole sequence. we use repeat to allow us to divide every time
   step by the respective sums.

   to calculate the context vector, we need to keep in mind that
   self.x_seq and at have a    batch dimension    and therefore we need to use
   batch_dot to avoid doing the multiplication over that dimension. the
   squeeze operation just removes left-over dimensions. this is done in
   lines 33   37.

   the next few lines of code are a more straightforward implementation of
   equations 4    8.

   now we think a bit ahead: we would like to calculate those fancy
   attention maps in figure 1. to do this, we need a    toggle    that returns
   the attention probabilities at.

training

data

   any good learning problem should have training data. in this case, it   s
   easy enough thanks to the [58]faker library which can generate fake
   dates with ease. i also use the [59]babel library to generate dates in
   different languages and formats as inspired by
   [60]rasmusbergpalm/id172. the script [61]data/generate.py will
   generate some fake data and i won   t bore you with the details but
   invite you to poke around or to make it better.

   the script also generates a vocabulary that will convert characters
   into integers so that the neural network can understand it. also
   included is a script in [62]data/reader.py to read and prepare data for
   the neural network to consume.

model

   this simple model with a bidirectional lstm and the decoder we wrote
   above is implemented in [63]models/id4.py. you can run it using python
   [64]run.py where i have set some default arguments (readme has more
   information). i   d recommend training on a machine with a gpu as it can
   be prohibitively slow on a cpu-only machine.

   if you want to skip the training part, i have provided some weights in
   [65]weights/

visualization

   now the easy part. for the visualizer implemented in [66]visualizer.py,
   we need to load the weights in twice: once with the predictive model,
   and the other to obtain the probabilities. since we implemented the
   model architecture in models/id4.py we can simply call the function
   twice:
from models.id4 import simpleid4
predictive_model = simpleid4(...)
predictive_model.load_weights(..., return_probabilities=false)
id203_model = simpleid4(..., return_probabilities=true)
id203_model.load_weights(...)

   to simply use the implemented visualizer, you can type:

   python visualizer.py -h

   to see available command line arguments.

example visualizations

   let us now examine the attentions we generated from id203_model.
   the predictive_model above returns the translated date that you see on
   the y-axis. on the x-axis is our input date. the map shows us which
   input characters (on the x-axis) were used in the prediction of the
   output character on the y-axis. the brighter the white, the more weight
   that character had. here are some i thought were quite interesting.

   a correct example which doesn   t pay attention to unnecessary
   information like days of the week:
   [1*wpqq7bi0jekrnknxbeiidg.png]
   example 1: the model has learned to ignore    saturday    during
   translation. we can observe that    9    is used in the prediction of    -09   
   (the day). the letter    m    is used to predict    -05    (the month). the
   last two digits of 2018 are used to predict the year.

   and here is an example of an incorrect translation because we submitted
   our example text in a novel order:    january 2016 05    was translated to
      2016   01   02    rather than    2016   01   05   .

   we can see that the model has mistakenly interpreted the characters
      20    from 2016 as the day of the month. the activations are weak (some
   even correctly on the actual date of    5   ) which can provide us with
   some insight into how to better train this model.
   [1*tlu7m_lyqwfxiaikrjwgfg.png]
   example 2: we can see the weirdly formatted date    january 2016 5    is
   incorrectly translated as 2016   01   02 where the    02    comes from the    20   
   in 2016

conclusion

   i hope this tutorial has shown you how to solve a machine learning
   problem from start to finish. furthermore, i hope it helps you in
   trying to visualize id195 problems with recurrent neural networks. if
   there is something i missed or you think i could do better, please feel
   free to reach out via [67]twitter or make an [68]issue on our github,
   i   d love to chat!

acknowledgements

   zaf is an intern at [69]datalogue partially supported by an [70]nserc
   experience award. the datalogue team contributed for code review and
   reading the proofs of this post.

   iframe: [71]/media/cbe560a9fe22e57e4e654ee1cfc1e880?postid=1892773a4f22

   thanks to [72]johanan ottensooser, [73]nicolas joseph, and [74]sonia
   sen.
     * [75]machine learning
     * [76]recurrent neural network
     * [77]language translation
     * [78]deep learning
     * [79]keras

   (button)
   (button)
   (button) 2k claps
   (button) (button) (button) 17 (button) (button)

     (button) blockedunblock (button) followfollowing
   [80]go to the profile of zafarali ahmed

[81]zafarali ahmed

   computer science, genomics, and machine learning
   [82]http://www.zafarali.me

     (button) follow
   [83]datalogue

[84]datalogue

   we put data into the hands of the people who need it!

     * (button)
       (button) 2k
     * (button)
     *
     *

   [85]datalogue
   never miss a story from datalogue, when you sign up for medium.
   [86]learn more
   never miss a story from datalogue
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1892773a4f22
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/datalogue/attention-in-keras-1892773a4f22&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/datalogue/attention-in-keras-1892773a4f22&source=--------------------------nav_reg&operation=register
   8. https://medium.com/datalogue?source=logo-lo_yd4icv2hflwc---5ec70ca04f8a
   9. https://medium.com/datalogue/archive
  10. https://medium.com/datalogue/open-source-hiring-84df05fdb357
  11. http://datalogue.io/
  12. https://medium.com/@zafarali?source=post_header_lockup
  13. https://medium.com/@zafarali
  14. https://www.datalogue.io/
  15. https://en.wikipedia.org/wiki/computer_vision
  16. https://en.wikipedia.org/wiki/convolutional_neural_network
  17. http://mashable.com/2017/05/17/google-lens/
  18. https://www.youtube.com/watch?v=fmvwlr0x1sk
  19. https://en.wikipedia.org/wiki/recurrent_neural_network
  20. https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
  21. https://blogs.microsoft.com/next/2016/09/13/microsoft-researchers-achieve-speech-recognition-milestone/
  22. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  23. https://www.youtube.com/watch?v=2rszkfw4lfs
  24. http://datalogue.io/
  25. https://keras.io/
  26. https://github.com/datalogue/keras-attention
  27. http://keras.io/
  28. http://karpathy.github.io/2015/05/21/id56-effectiveness/#recurrent-neural-networks
  29. https://www.tensorflow.org/get_started/get_started#the_computational_graph
  30. https://www.tensorflow.org/
  31. http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/
  32. http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-id56-with-python-and-theano/
  33. http://colah.github.io/posts/2015-08-understanding-lstms/
  34. http://colah.github.io/posts/2015-08-understanding-lstms/
  35. http://colah.github.io/posts/2015-08-understanding-lstms/
  36. https://arxiv.org/abs/1502.03044
  37. https://arxiv.org/abs/1508.01211
  38. https://arxiv.org/abs/1409.0473
  39. https://arxiv.org/abs/1502.03044
  40. https://spacy.io/
  41. https://explosion.ai/blog/deep-learning-formula-nlp
  42. http://distill.pub/2016/augmented-id56s/
  43. https://arxiv.org/abs/1409.0473
  44. https://arxiv.org/abs/1409.0473
  45. https://github.com/datalogue/keras-attention
  46. https://github.com/datalogue/keras-attention/blob/master/models/id4.py
  47. https://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py
  48. https://github.com/datalogue/keras-attention/blob/master/models/id4.py
  49. https://arxiv.org/abs/1409.0473
  50. http://ufldl.stanford.edu/tutorial/supervised/multilayerneuralnetworks/
  51. http://ufldl.stanford.edu/tutorial/supervised/softmaxregression/
  52. http://colah.github.io/posts/2015-08-understanding-lstms/#step-by-step-lstm-walk-through
  53. https://stats.stackexchange.com/questions/129698/what-is-maxout-in-neural-network
  54. https://github.com/datalogue/keras-attention/blob/master/models/custom_recurrents.py
  55. https://github.com/datalogue/keras-attention/blob/master/models/custom_recurrents.py
  56. https://medium.com/media/2abbc964f1074769bc44393de70c657b?postid=1892773a4f22
  57. https://medium.com/media/2f2a29d8d24f191db2f54223a61c7486?postid=1892773a4f22
  58. http://faker.readthedocs.io/en/master/
  59. http://babel.pocoo.org/en/latest/
  60. https://github.com/rasmusbergpalm/id172/blob/master/babel_data.py
  61. https://github.com/datalogue/keras-attention/blob/master/data/generate.py
  62. https://github.com/datalogue/keras-attention/blob/master/data/reader.py
  63. https://github.com/datalogue/keras-attention/blob/master/models/id4.py
  64. https://github.com/datalogue/keras-attention/blob/master/run.py
  65. https://github.com/datalogue/keras-attention/tree/master/weights
  66. https://github.com/datalogue/keras-attention/blob/master/visualize.py
  67. https://twitter.com/zafarali
  68. https://github.com/datalogue/keras-attention
  69. http://datalogue.io/
  70. http://www.nserc-crsng.gc.ca/students-etudiants/ug-pc/experience-experience_eng.asp
  71. https://medium.com/media/cbe560a9fe22e57e4e654ee1cfc1e880?postid=1892773a4f22
  72. https://medium.com/@oatsandsugar?source=post_page
  73. https://medium.com/@callicles?source=post_page
  74. https://medium.com/@soniazsen?source=post_page
  75. https://medium.com/tag/machine-learning?source=post
  76. https://medium.com/tag/recurrent-neural-network?source=post
  77. https://medium.com/tag/language-translation?source=post
  78. https://medium.com/tag/deep-learning?source=post
  79. https://medium.com/tag/keras?source=post
  80. https://medium.com/@zafarali?source=footer_card
  81. https://medium.com/@zafarali
  82. http://www.zafarali.me/
  83. https://medium.com/datalogue?source=footer_card
  84. https://medium.com/datalogue?source=footer_card
  85. https://medium.com/datalogue
  86. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  88. https://github.com/datalogue/keras-attention
  89. https://medium.com/p/1892773a4f22/share/twitter
  90. https://medium.com/p/1892773a4f22/share/facebook
  91. https://medium.com/p/1892773a4f22/share/twitter
  92. https://medium.com/p/1892773a4f22/share/facebook
