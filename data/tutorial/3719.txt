   #[1]aaditya prakash (adi) - random musings of a deep learning grad
   student

   [2][profile_image.jpg]

[3]aaditya prakash (adi)

   random musings of a deep learning grad student

   [4]cv/resume [5]blog [6]notes [7]about
   [8][small_email.png] [9][github.png] [10][twitter.png] [11][pgp.gif]
   [12][scholar.png] [13][lk.png] [14][bitcoin.png] [15][quora.png]

visual id53 demo in python notebook

   this is an online demo with explanation and tutorial on visual question
   answering. this is not a naive or hello-world model, this model returns
   close to state-of-the-art without using any id12, memory
   networks (other than lstm) and fine-tuning, which are essential recipe
   for current best results.

   i have tried to explain different parts, and reasoning behind their
   choices. this is meant to be an interactive tutorial, feel free to
   change the model parameters and experiment. if you have latest graphics
   card execution time should be within a minute.

   all the files required to run this ipython notebook can be obtained
   from

* [16]https://github.com/iamaaditya/vqa_demo
* [17]jupyter notebook on github </p>

table of contents

     * [18]visual id53 demo and tutorial
          + [19]load the libraries
          + [20]load the models and weights files
          + [21]model idea
     * [22]image features
          + [23]pretrained vgg net (vgg-16)
          + [24]compile the model
          + [25]plot the model
          + [26]extract image features
     * [27]id27s
          + [28]try the embeddings
     * [29]vqa model
          + [30]asketh away !
          + [31]what vehicle is in the picture ?
          + [32]results
     * [33]demo with image url
          + [34]what are they playing?
          + [35]are they playing soccer?

load the libraries[36]  

   in [30]:
%matplotlib inline
import os, argparse
import cv2, spacy, numpy as np
from keras.models import model_from_json
from keras.optimizers import sgd
from sklearn.externals import joblib

load the models and weights files[37]  

   this does not load the models yet, but we are providing the files
   in [1]:
# file paths for the model, all of these except the id98 weights are
# provided in the repo, see the models/id98/readme.md to download vgg weights
vqa_model_file_name      = 'models/vqa/vqa_model.json'
vqa_weights_file_name   = 'models/vqa/vqa_model_weights.hdf5'
label_encoder_file_name  = 'models/vqa/full_labelencoder_trainval.pkl'
id98_weights_file_name   = 'models/id98/vgg16_weights.h5'

model idea[38]  

   this uses a classical id98-lstm model like shown below, where image
   features and language features are computed separately and combined
   together and a multi-layer id88 is trained on the combined
   features.

   similar models have been presented at following links, this work takes
   ideas from them.
    1. [39]https://github.com/abhshkdz/neural-vqa
    2. [40]https://github.com/avisingh599/visual-qa
    3. [41]https://github.com/vt-vision-lab/vqa_lstm_id98

   [za5p1zz.png] [42]source

image features[43]  

pretrained vgg net (vgg-16)[44]  

   while vgg net is not the best id98 model for image features, googlenet
   (winner 2014) and resnet (winner 2015) have superior classification
   scores, but vgg net is very versatile, simple, relatively small and
   more importantly portable to use.

   for reference here is the vgg 16 performance on ilsvrc-2012
   [table_ilsvrc.png]

compile the model[45]  

   in [32]:
def get_image_model(id98_weights_file_name):
    ''' takes the id98 weights file, and returns the vgg model update
    with the weights. requires the file vgg.py inside models/id98 '''
    from models.id98.vgg import vgg_16
    image_model = vgg_16(id98_weights_file_name)

    # this is standard vgg 16 without the last two layers
    sgd = sgd(lr=0.1, decay=1e-6, momentum=0.9, nesterov=true)
    # one may experiment with "adam" optimizer, but the id168 for
    # this kind of task is pretty standard
    image_model.compile(optimizer=sgd, loss='categorical_crossid178')
    return image_model

plot the model[46]  

   keras has a function which allows you to visualize the model in block
   diagram. let's do it !
   in [35]:
from keras.utils.visualize_util import plot
model_vgg = get_image_model(id98_weights_file_name)
plot(model_vgg, to_file='model_vgg.png')

   [model_vgg.png]

extract image features[47]  

   extracting image features involves, taking a raw image, and running it
   through the model, until we reach the last layer. in this case our
   model is not 100% same as vgg net, because we are not going to use the
   last two layer of the vgg. it is because the last layer of vgg net is a
   1000 way softmax and the second last layer is the dropout.

   thus we are extracting the 4096 dimension image features from vgg-16
   in [7]:
def get_image_features(image_file_name, id98_weights_file_name):
    ''' runs the given image_file to vgg 16 model and returns the
    weights (filters) as a 1, 4096 dimension vector '''
    image_features = np.zeros((1, 4096))
    # magic_number = 4096  > comes from last layer of vgg model

    # since vgg was trained as a image of 224x224, every new image
    # is required to go through the same transformation
    im = cv2.resize(cv2.imread(image_file_name), (224, 224))
    im = im.transpose((2,0,1)) # convert the image to rgba


    # this axis dimension is required because vgg was trained on a dimension
    # of 1, 3, 224, 224 (first axis is for the batch size
    # even though we are using only one image, we have to keep the dimensions co
nsistent
    im = np.expand_dims(im, axis=0)

    image_features[0,:] = get_image_model(id98_weights_file_name).predict(im)[0]
    return image_features

id27s[48]  

   the question has to be converted into some form of id27s.
   most popular is id97 whereas these days state of the art uses
   [49]skip-thought vectors or [50]positional encodings.

   we will use id97 from stanford called [51]glove. glove reduces a
   given token into a 300 dimensional representation.
   in [8]:
def get_question_features(question):
    ''' for a given question, a unicode string, returns the time series vector
    with each word (token) transformed into a 300 dimension representation
    calculated using glove vector '''
    word_embeddings = spacy.load('en', vectors='en_glove_cc_300_1m_vectors')
    tokens = word_embeddings(question)
    question_tensor = np.zeros((1, len(tokens), 300))
    for j in xrange(len(tokens)):
            question_tensor[0,j,:] = tokens[j].vector
    return question_tensor

try the embeddings[52]  

   let's see the embeddings, and their usage with sample words like this -
    1. obama
    2. putin
    3. banana
    4. monkey

   in [23]:
word_embeddings = spacy.load('en', vectors='en_glove_cc_300_1m_vectors')

   in [24]:
obama = word_embeddings(u"obama")
putin = word_embeddings(u"putin")
banana = word_embeddings(u"banana")
monkey = word_embeddings(u"monkey")

   in [25]:
obama.similarity(putin)

   out[25]:
0.43514112534149385

   in [26]:
obama.similarity(banana)

   out[26]:
0.17831375020636123

   in [27]:
banana.similarity(monkey)

   out[27]:
0.45207779162154438

   as we can see, obama and putin are very similar in representation than
   obama and banana. this shows you there is some semantic knowledge of
   the tokens embedded in the 300 dimensional representation. we can do
   cool arithmetics with these id97 like 'queen' - 'king' + 'boy' =
   'girl'. see [53]this blog post for more details.

vqa model[54]  

   vqa is a simple model which combines features from image and word
   embeddings and runs a multiple layer id88.
   in [36]:
def get_vqa_model(vqa_model_file_name, vqa_weights_file_name):
    ''' given the vqa model and its weights, compiles and returns the model '''

    # thanks the keras function for loading a model from json, this becomes
    # very easy to understand and work. alternative would be to load model
    # from binary like cpickle but then model would be obfuscated to users
    vqa_model = model_from_json(open(vqa_model_file_name).read())
    vqa_model.load_weights(vqa_weights_file_name)
    vqa_model.compile(loss='categorical_crossid178', optimizer='rmsprop')
    return vqa_model

   in [38]:
from keras.utils.visualize_util import plot
model_vqa = get_vqa_model(vqa_model_file_name, vqa_weights_file_name)
plot(model_vqa, to_file='model_vqa.png')

   [model_vqa.png]

   as it can be seen above the model also runs a 3 layered lstm on the
   id27s. to get a naive result it is sufficient to feed the
   id27s directly to the merge layer, but as mentioned above the
   model gives close to the state-of-the-art results.

   also, four layers of fully connected layers might not be required to
   achieve a good enough results. but i settled on this model after some
   experimentation, and this model's results beat those obtained using
   only few layers.

asketh away ![55]  

   let's give a test image and a question
   in [69]:
image_file_name = 'test.jpg'
question = u"what vehicle is in the picture?"

                      what vehicle is in the picture ?

   [56]  

   [test.jpg]
   in [43]:
# get the image features
image_features = get_image_features(image_file_name, id98_weights_file_name)

   in [44]:
# get the question features
question_features = get_question_features(question)

   in [46]:
y_output = model_vqa.predict([question_features, image_features])

# this task here is represented as a classification into a 1000 top answers
# this means some of the answers were not part of training and thus would
# not show up in the result.
# these 1000 answers are stored in the sklearn encoder class
labelencoder = joblib.load(label_encoder_file_name)
for label in reversed(np.argsort(y_output)[0,-5:]):
    print str(round(y_output[0,label]*100,2)).zfill(5), "% ", labelencoder.inver
se_transform(label)

78.32 %  train
01.11 %  truck
00.98 %  passenger
00.95 %  fire truck
00.68 %  bus

results[57]  

   i am copying the output of the previous command, so that you can
   validate if your results are same as mine.

   78.32 % train
   01.11 % truck
   00.98 % passenger
   00.95 % fire truck
   00.68 % bus

demo with image url[58]  

   since cv2.imread cannot read an image from url we will have to change
   our function get_image_features
   in [49]:
def get_image_features(image_file_name, id98_weights_file_name):
    ''' runs the given image_file to vgg 16 model and returns the
    weights (filters) as a 1, 4096 dimension vector '''
    image_features = np.zeros((1, 4096))

    from skimage import io
    # if you would rather not install skimage, then use cv2.videocapture which c
an read from url
    # see this http://answers.opencv.org/question/16385/cv2imread-a-url/?answer=
16389#post-id-16389
    im = cv2.resize(io.imread(image_file_name), (224, 224))
    im = im.transpose((2,0,1)) # convert the image to rgba


    # this axis dimension is required because vgg was trained on a dimension
    # of 1, 3, 224, 224 (first axis is for the batch size
    # even though we are using only one image, we have to keep the dimensions co
nsistent
    im = np.expand_dims(im, axis=0)

    image_features[0,:] = get_image_model(id98_weights_file_name).predict(im)[0]
    return image_features

   in [61]:
image_file_name = "http://www.newarkhistory.com/indparksoccerkids.jpg"
# get the image features
image_features = get_image_features(image_file_name, id98_weights_file_name)

   feel free to change that url to any valid image, it can be any image
   format. also try to use websites which have higher bandwidth

   [indparksoccerkids.jpg]

                           what are they playing?

   [59]  
   in [62]:
question = u"what are they playing?"

# get the question features
question_features = get_question_features(question)

   in [63]:
y_output = model_vqa.predict([question_features, image_features])

labelencoder = joblib.load(label_encoder_file_name)
for label in reversed(np.argsort(y_output)[0,-5:]):
    print str(round(y_output[0,label]*100,2)).zfill(5), "% ", labelencoder.inver
se_transform(label)

40.52 %  tennis
28.45 %  soccer
17.88 %  baseball
11.67 %  frisbee
00.15 %  football

result[60]  

   copying the result to validate your output.

   40.52 % tennis
   28.45 % soccer
   17.88 % baseball
   11.67 % frisbee
   00.15 % football

   as you can see, it got this wrong, but you can see why it could be
   harder to guess soccer and easier to guess tennis, lack of soccer ball
   and double lines at the edge.

   let's ask another question for the same image.
   in [67]:
question = u"are they playing soccer?"

# get the question features
question_features = get_question_features(question)

   [indparksoccerkids.jpg]

                          are they playing soccer?

   [61]  
   in [68]:
y_output = model_vqa.predict([question_features, image_features])

labelencoder = joblib.load(label_encoder_file_name)
for label in reversed(np.argsort(y_output)[0,-5:]):
    print str(round(y_output[0,label]*100,2)).zfill(5), "% ", labelencoder.inver
se_transform(label)

93.15 %  yes
06.42 %  no
00.02 %  right
00.01 %  left
000.0 %  man

result[62]  

   93.15 % yes
   06.42 % no
   00.02 % right
   00.01 % left
   000.0 % man

   as you can see, similar information about a yes/no question elicits
   different response, or should i say correct response. this is an
   impertinent problem with classification tasks.

   feel free to experiment with different types of questions, count,
   color, location.

   more interesting results are obtained when one takes a different crop
   of a image, instead of just scaling it to 224x224. this is again
   because we extract only the top level features of id98 model which was
   trained to classify one object in the image.

   written on april 4, 2016

   please enable javascript to view the [63]comments powered by disqus.

references

   1. https://iamaaditya.github.io/feed.xml
   2. https://iamaaditya.github.io/
   3. https://iamaaditya.github.io/
   4. http://www.cs.brandeis.edu/~aprakash/cv.pdf
   5. https://iamaaditya.github.io/
   6. https://iamaaditya.github.io/notes
   7. https://iamaaditya.github.io/about
   8. mailto:aprakash@brandeis.edu
   9. https://github.com/iamaaditya
  10. https://twitter.com/aaditya_prakash
  11. https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/pgp_public_aaditya.txt
  12. https://scholar.google.co.in/citations?user=p-jhg4caaaaj&hl=en
  13. https://www.linkedin.com/in/aaditya-prakash-68453338/
  14. https://blockchain.info/address/19a2ddqdh3edphs5bhdyhnoglmtwqjoevv
  15. https://www.quora.com/profile/aaditya-prakash
  16. https://github.com/iamaaditya/vqa_demo
  17. https://github.com/iamaaditya/vqa_demo/blob/master/visual_question_answering_demo_in_python_notebook.ipynb
  18. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#visual-question-answering-demo-and-tutorial
  19. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#load-the-libraries
  20. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#load-the-models-and-weights-files
  21. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#model-idea
  22. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#image-features
  23. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#pretrained-vgg-net-(vgg-16)
  24. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#compile-the-model
  25. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#plot-the-model
  26. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#extract-image-features
  27. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#word-embeddings
  28. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#try-the-embeddings
  29. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#vqa-model
  30. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#asketh-away-!
  31. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#-what-vehicle-is-in-the-picture-?-
  32. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#results
  33. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#demo-with-image-url
  34. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#-what-are-they-playing?-
  35. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#-are-they-playing-soccer?-
  36. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#load-the-libraries
  37. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#load-the-models-and-weights-files
  38. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#model-idea
  39. https://github.com/abhshkdz/neural-vqa
  40. https://github.com/avisingh599/visual-qa
  41. https://github.com/vt-vision-lab/vqa_lstm_id98
  42. http://arxiv.org/pdf/1505.00468v4.pdf
  43. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#image-features
  44. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#pretrained-vgg-net-(vgg-16)
  45. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#compile-the-model
  46. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#plot-the-model
  47. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#extract-image-features
  48. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#word-embeddings
  49. https://github.com/ryankiros/skip-thoughts
  50. https://en.wikipedia.org/wiki/encoding_(memory)#sequence_memory
  51. http://nlp.stanford.edu/projects/glove/
  52. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#try-the-embeddings
  53. http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/
  54. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#vqa-model
  55. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#asketh-away-!
  56. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#-what-vehicle-is-in-the-picture-?-
  57. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#results
  58. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#demo-with-image-url
  59. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#-what-are-they-playing?-
  60. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#result
  61. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#-are-they-playing-soccer?-
  62. https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook#result
  63. http://disqus.com/?ref_noscript
