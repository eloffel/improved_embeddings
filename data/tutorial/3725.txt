   #[1]openai

     * [2]about
     * [3]progress
     * [4]resources
     * [5]blog

   (button)

   (button)

     * [6]about
         ______________________________________________________________

     * [7]progress
         ______________________________________________________________

     * [8]resources
         ______________________________________________________________

     * [9]blog
         ______________________________________________________________

     * [10]jobs
     __________________________________________________________________

   generative models

generative models

   june 16, 2016
   12 minute read

   this post describes [11]four projects that share a common theme of
   enhancing or using generative models, a branch of [12]unsupervised
   learning techniques in machine learning. in addition to describing our
   work, this post will tell you a bit more about generative models: what
   they are, why they are important, and where they might be going.

   one of our core aspirations at openai is to develop algorithms and
   techniques that endow computers with an understanding of our world.

   it's easy to forget just how much you know about the world: you
   understand that it is made up of 3d environments, objects that move,
   collide, interact; people who walk, talk, and think; animals who graze,
   fly, run, or bark; monitors that display information encoded in
   language about the weather, who won a basketball game, or what happened
   in 1970.

   this tremendous amount of information is out there and to a large
   extent easily accessible     either in the physical world of atoms or the
   digital world of bits. the only tricky part is to develop models and
   algorithms that can analyze and understand this treasure trove of data.

   generative models are one of the most promising approaches towards this
   goal. to train a generative model we first collect a large amount of
   data in some domain (e.g., think millions of images, sentences, or
   sounds, etc.) and then train a model to generate data like it. the
   intuition behind this approach follows a famous quote from [13]richard
   feynman:

        what i cannot create, i do not understand.   
        richard feynman

   the trick is that the neural networks we use as generative models have
   a number of parameters significantly smaller than the amount of data we
   train them on, so the models are forced to discover and efficiently
   internalize the essence of the data in order to generate it.

   generative models have many short-term [14]applications. but in the
   long run, they hold the potential to automatically learn the natural
   features of a dataset, whether categories or dimensions or something
   else entirely.
     __________________________________________________________________

generating images

   let   s make this more concrete with an example. suppose we have some
   large collection of images, such as the 1.2 million images in the
   [15]id163 dataset (but keep in mind that this could eventually be a
   large collection of images or videos from the internet or robots). if
   we resize each image to have width and height of 256 (as is commonly
   done), our dataset is one large 1,200,000x256x256x3 (about 200gb) block
   of pixels. here are a few example images from this dataset:

   these images are examples of what our visual world looks like and we
   refer to these as "samples from the true data distribution". we now
   construct our generative model which we would like to train to generate
   images like this from scratch. concretely, a generative model in this
   case could be one large neural network that outputs images and we refer
   to these as "samples from the model".
     __________________________________________________________________

dcgan

   one such recent model is the [16]dcgan network from radford et al.
   (shown below). this network takes as input 100 random numbers drawn
   from a [17]uniform distribution (we refer to these as a code, or latent
   variables, in red) and outputs an image (in this case 64x64x3 images on
   the right, in green). as the code is changed incrementally, the
   [18]generated images do too     this shows the model has learned features
   to describe how the world looks, rather than just memorizing some
   examples.

   the network (in yellow) is made up of standard [19]convolutional neural
   network components, such as [20]deconvolutional layers (reverse of
   convolutional layers), [21]fully connected layers, etc.:
   [gen_models_diag_1.svg]

   dcgan is initialized with random weights, so a random code plugged into
   the network would generate a completely random image. however, as you
   might imagine, the network has millions of parameters that we can
   tweak, and the goal is to find a setting of these parameters that makes
   samples generated from random codes look like the training data. or to
   put it another way, we want the model distribution to match the true
   data distribution in the space of images.
     __________________________________________________________________

training a generative model

   suppose that we used a newly-initialized network to generate 200
   images, each time starting with a different random code. the question
   is: how should we adjust the network's parameters to encourage it to
   produce slightly more believable samples in the future? notice that
   we're not in a simple supervised setting and don't have any explicit
   desired targets for our 200 generated images; we merely want them to
   look real. one clever approach around this problem is to follow the
   [22]generative adversarial network (gan) approach. here we introduce a
   second discriminator network (usually a standard convolutional neural
   network) that tries to classify if an input image is real or generated.
   for instance, we could feed the 200 generated images and 200 real
   images into the discriminator and train it as a standard classifier to
   distinguish between the two sources. but in addition to that     and
   here's the trick     we can also [23]backpropagate through both the
   discriminator and the generator to find how we should change the
   generator's parameters to make its 200 samples slightly more confusing
   for the discriminator. these two networks are therefore locked in a
   battle: the discriminator is trying to distinguish real images from
   fake images and the generator is trying to create images that make the
   discriminator think they are real. in the end, the generator network is
   outputting images that are indistinguishable from real images for the
   discriminator.

   there are a few other approaches to matching these distributions which
   we will discuss briefly below. but before we get there below are two
   animations that show samples from a generative model to give you a
   visual sense for the training process.
   in both cases the samples from the generator start out noisy and
   chaotic, and over time converge to have more plausible image
   statistics:
   [gen_models_anim_1.gif] [24]vae learning to generate images (log time)
   [gen_models_anim_2.gif] [25]gan learning to generate images (linear
   time)

   this is exciting     these neural networks are learning what the visual
   world looks like! these models usually have only about 100 million
   parameters, so a network trained on id163 has to (lossily)
   [26]compress 200gb of pixel data into 100mb of weights. this
   incentivizes it to discover the most salient features of the data: for
   example, it will likely learn that pixels nearby are likely to have the
   same color, or that the world is made up of horizontal or vertical
   edges, or blobs of different colors. eventually, the model may discover
   many more complex regularities: that there are certain types of
   backgrounds, objects, textures, that they occur in certain likely
   arrangements, or that they transform in certain ways over time in
   videos, etc.
     __________________________________________________________________

more general formulation

   mathematically, we think about a dataset of examples \(x_1, \ldots,
   x_n\) as samples from a true data distribution \(p(x)\). in the example
   image below, the blue region shows the part of the image space that,
   with a high id203 (over some threshold) contains real images, and
   black dots indicate our data points (each is one image in our dataset).
   now, our model also describes a distribution \(\hat{p}_{\theta}(x)\)
   (green) that is defined implicitly by taking points from a unit
   [27]gaussian distribution (red) and mapping them through a
   (deterministic) neural network     our generative model (yellow). our
   network is a function with parameters \(\theta\), and tweaking these
   parameters will tweak the generated distribution of images. our goal
   then is to find parameters \(\theta\) that produce a distribution that
   closely matches the true data distribution (for example, by having a
   small [28]kl divergence [29]loss). therefore, you can imagine the green
   distribution starting out random and then the training process
   iteratively changing the parameters \(\theta\) to stretch and squeeze
   it to better match the blue distribution.
   [gen_models_diag_2.svg]
     __________________________________________________________________

three approaches to generative models

   most generative models have this basic setup, but differ in the
   details. here are three popular examples of generative model approaches
   to give you a sense of the variation:
     * [30]id3 (gans), which we already
       discussed above, pose the training process as a game between two
       separate networks: a generator network (as seen above) and a second
       discriminative network that tries to classify samples as either
       coming from the true distribution \(p(x)\) or the model
       distribution \(\hat{p}(x)\). every time the discriminator notices a
       difference between the two distributions the generator adjusts its
       parameters slightly to make it go away, until at the end (in
       theory) the generator exactly reproduces the true data distribution
       and the discriminator is guessing at random, unable to find a
       difference.
     * [31]id5 (vaes) allow us to formalize this
       problem in the framework of [32]probabilistic id114
       where we are maximizing a [33]lower bound on the log likelihood of
       the data.
     * autoregressive models such as [34]pixelid56 instead train a network
       that models the conditional distribution of every individual pixel
       given previous pixels (to the left and to the top). this is similar
       to plugging the pixels of the image into a [35]char-id56, but the
       id56s run both horizontally and vertically over the image instead of
       just a 1d sequence of characters.

   all of these approaches have their pros and cons. for example,
   id5 allow us to perform both learning and
   efficient bayesian id136 in sophisticated probabilistic graphical
   models with latent variables (e.g. see [36]draw, or [37]attend infer
   repeat for hints of recent relatively complex models). however, their
   generated samples tend to be slightly blurry. gans currently generate
   the sharpest images but they are more difficult to optimize due to
   unstable training dynamics. pixelid56s have a very simple and stable
   training process ([38]softmax loss) and currently give the best log
   likelihoods (that is, plausibility of the generated data). however,
   they are relatively inefficient during sampling and don't easily
   provide simple low-dimensional codes for images. all of these models
   are active areas of research and we are eager to see how they develop
   in the future!
     __________________________________________________________________

our recent contributions

   we're quite excited about generative models at openai, and have just
   released four projects that advance the state of the art. for each of
   these contributions we are also releasing a technical report and source
   code.

   [39]improving gans ([40]code). first, as mentioned above gans are a
   very promising family of generative models because, unlike other
   methods, they produce very clean and sharp images and learn codes that
   contain valuable information about these textures. however, gans are
   formulated as a game between two networks and it is important (and
   tricky!) to keep them in balance: for example, they can oscillate
   between solutions, or the generator has a tendency to collapse. in this
   work, tim salimans, ian goodfellow, wojciech zaremba and colleagues
   have introduced a few new techniques for making gan training more
   stable. these techniques allow us to scale up gans and obtain nice
   128x128 id163 samples:
   [gen_models_img_2.jpg] real images (id163) [gen_models_img_3.jpg]
   generated images

   our [41]cifar-10 samples also look very sharp - amazon mechanical turk
   workers can distinguish our samples from real data with an error rate
   of 21.3% (50% would be random guessing):
   [gen_models_img_4.jpg] real images (cifar-10) [gen_models_img_5.jpg]
   generated images

   in addition to generating pretty pictures, we introduce an approach for
   [42]semi-supervised learning with gans that involves the discriminator
   producing an additional output indicating the label of the input. this
   approach allows us to obtain state of the art results on [43]mnist,
   [44]svhn, and cifar-10 in settings with very few [45]labeled examples.
   on mnist, for example, we achieve 99.14% accuracy with only 10 labeled
   examples per class with a fully connected neural network     a result
   that   s very close to the best known results with fully supervised
   approaches using all 60,000 labeled examples. this is very promising
   because labeled examples can be quite expensive to obtain in practice.

   id3 are a relatively new model (introduced
   only two years ago) and we expect to see more rapid progress in further
   improving the stability of these models during training.

   [46]improving vaes ([47]code). in this work durk kingma and tim
   salimans introduce a flexible and computationally scalable method for
   improving the accuracy of [48]variational id136. in particular,
   most vaes have so far been trained using crude [49]approximate
   posteriors, where every latent variable is independent. [50]recent
   extensions have addressed this problem by conditioning each latent
   variable on the others before it in a chain, but this is
   computationally inefficient due to the introduced sequential
   dependencies. the core contribution of this work, termed inverse
   autoregressive flow (iaf), is a new approach that, unlike previous
   work, allows us to parallelize the computation of rich approximate
   posteriors, and make them almost arbitrarily flexible.

   we show some example 32x32 image samples from the model in the image
   below, on the right. on the left are earlier samples from the [51]draw
   model for comparison (vanilla vae samples would look even worse and
   more blurry). the draw model was published only one year ago,
   highlighting again the rapid progress being made in training generative
   models.
   [gen_models_img_6.jpg] generated from a draw model
   [gen_models_img_7.jpg] generated from a vae trained with iaf

   [52]infogan ([53]code). peter chen and colleagues introduce infogan    
   an extension of gan that learns disentangled and interpretable
   representations for images. a regular gan achieves the objective of
   reproducing the data distribution in the model, but the layout and
   organization of the code space is underspecified     there are many
   possible solutions to mapping the unit gaussian to images and the one
   we end up with might be intricate and highly entangled. the infogan
   imposes additional structure on this space by adding new objectives
   that involve maximizing the [54]mutual information between small
   subsets of the representation variables and the observation. this
   approach provides quite remarkable results. for example, in the images
   of 3d faces below we vary one continuous dimension of the code, keeping
   all others fixed. it's clear from the five provided examples (along
   each row) that the resulting dimensions in the code capture
   interpretable dimensions, and that the model has perhaps understood
   that there are camera angles, facial variations, etc., without having
   been told that these features exist and are important:
   [infogan_1.jpg] (a) azimuth (pose) [ingogan_2.jpg] (b) elevation
   [infogan_3.jpg] (c) lighting [infogan_4.jpg] (d) wide or narrow

   we also note that nice, disentangled representations have been achieved
   before (such as with [55]dc-ign by kulkarni et al.), but these
   approaches rely on additional supervision, while our approach is
   entirely unsupervised.
     __________________________________________________________________

   the next two recent projects are in a [56]id23 (rl)
   setting (another area of [57]focus at openai), but they both involve a
   generative model component.

   [58]curiosity-driven exploration in deep id23 via
   bayesian neural networks ([59]code). efficient exploration in
   high-dimensional and continuous spaces is presently an unsolved
   challenge in id23. without effective exploration
   methods our agents [60]thrash around until they randomly stumble into
   rewarding situations. this is sufficient in many simple toy tasks but
   inadequate if we wish to apply these algorithms to complex settings
   with high-dimensional action spaces, as is common in robotics. in this
   paper, rein houthooft and colleagues propose vime, a practical approach
   to exploration using uncertainty on generative models. vime makes the
   agent self-motivated; it actively seeks out surprising state-actions.
   we show that vime can improve a range of [61]policy search methods and
   makes significant progress on more realistic tasks with sparse rewards
   (e.g. scenarios in which the agent has to learn locomotion primitives
   without any guidance).
   [policy_search_1.gif] policy trained with vime [policy_search_2.gif]
   policy trained with naive exploration

   finally, we would like to include a bonus fifth project: [62]generative
   adversarial imitation learning ([63]code), in which jonathan ho and
   colleagues present a new approach for imitation learning. jonathan ho
   is joining us at openai as a summer intern. he did most of this work at
   stanford but we include it here as a related and highly creative
   application of gans to rl. the standard id23 setting
   usually requires one to design a reward function that describes the
   desired behavior of the agent. however, in practice this can sometimes
   involve expensive trial-and-error process to get the details right. in
   contrast, in imitation learning the agent learns from example
   demonstrations (for example provided by teleoperation in robotics),
   eliminating the need to design a reward function.
   [running_human.gif] [running_bug.gif]

   popular imitation approaches involve a two-stage pipeline: first
   learning a reward function, then running rl on that reward. such a
   pipeline can be slow, and because it   s indirect, it is hard to
   guarantee that the resulting policy works well. this work shows how one
   can directly extract policies from data via a connection to gans. as a
   result, this approach can be used to learn policies from expert
   demonstrations (without rewards) on hard [64]openai gym environments,
   such as [65]ant and [66]humanoid.
     __________________________________________________________________

going forward

   generative models are a rapidly advancing area of research. as we
   continue to advance these models and scale up the training and the
   datasets, we can expect to eventually generate samples that depict
   entirely plausible images or videos. this may by itself find use in
   multiple applications, such as on-demand generated art, or photoshop++
   commands such as "make my smile wider". additional presently known
   applications include [67]image denoising, [68]inpainting,
   [69]super-resolution, [70]id170, [71]exploration in
   id23, and neural network [72]pretraining in cases
   where labeled data is expensive.

   however, the deeper promise of this work is that, in the process of
   training generative models, we will endow the computer with an
   understanding of the world and what it is made up of.
     __________________________________________________________________

   cover artwork
   ludwig pettersson
     __________________________________________________________________

   authors
   [73]andrej karpathy[74]pieter abbeel[75]greg brockman[76]peter
   chen[77]vicki cheung[78]rocky duan[79]ian goodfellow[80]durk
   kingma[81]jonathan ho[82]rein houthooft[83]tim salimans[84]john
   schulman[85]ilya sutskever[86]wojciech zaremba
     __________________________________________________________________

     * [87]about
     * [88]progress
     * [89]resources
     * [90]blog
     * [91]charter
     * [92]jobs
     * [93]press

     *
     *

   sign up for our newsletter
   ____________________ (button)
   right

references

   visible links
   1. https://openai.com/blog/rss/
   2. https://openai.com/about/
   3. https://openai.com/progress/
   4. https://openai.com/resources/
   5. https://openai.com/blog/
   6. https://openai.com/about/
   7. https://openai.com/progress/
   8. https://openai.com/resources/
   9. https://openai.com/blog/
  10. https://openai.com/jobs/
  11. https://openai.com/blog/generative-models/#contributions
  12. https://www.quora.com/what-is-the-difference-between-supervised-and-unsupervised-learning-algorithms
  13. https://en.wikipedia.org/wiki/richard_feynman
  14. https://openai.com/blog/generative-models/#going-forward
  15. http://www.image-net.org/
  16. https://github.com/newmu/dcgan_code
  17. https://en.wikipedia.org/wiki/uniform_distribution_(continuous)
  18. https://github.com/newmu/dcgan_code#walking-from-one-point-to-another-in-bedroom-latent-space
  19. http://cs231n.github.io/convolutional-networks/
  20. http://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers
  21. http://cs231n.github.io/convolutional-networks/#fc
  22. http://arxiv.org/abs/1406.2661
  23. http://neuralnetworksanddeeplearning.com/chap2.html
  24. https://openai.com/blog/generative-models/#vae
  25. https://openai.com/blog/generative-models/#gan
  26. http://prize.hutter1.net/
  27. https://en.wikipedia.org/wiki/normal_distribution
  28. https://en.wikipedia.org/wiki/kullback   leibler_divergence
  29. https://en.wikipedia.org/wiki/loss_function
  30. http://arxiv.org/abs/1406.2661
  31. https://arxiv.org/abs/1312.6114
  32. https://en.wikipedia.org/wiki/graphical_model
  33. https://en.wikipedia.org/wiki/variational_bayesian_methods
  34. http://arxiv.org/abs/1601.06759
  35. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  36. https://arxiv.org/abs/1502.04623
  37. http://arxiv.org/abs/1603.08575
  38. https://en.wikipedia.org/wiki/softmax_function
  39. https://arxiv.org/abs/1606.03498
  40. https://github.com/openai/improved-gan
  41. https://www.cs.toronto.edu/~kriz/cifar.html
  42. https://en.wikipedia.org/wiki/semi-supervised_learning
  43. http://yann.lecun.com/exdb/mnist/
  44. http://ufldl.stanford.edu/housenumbers/
  45. http://stackoverflow.com/questions/19170603/what-is-the-difference-between-labeled-and-unlabeled-data
  46. http://arxiv.org/abs/1606.04934
  47. http://github.com/openai/iaf
  48. https://www.cs.jhu.edu/~jason/tutorials/variational.html
  49. http://www.cs.princeton.edu/courses/archive/spr06/cos598c/papers/chapter2.pdf
  50. http://arxiv.org/abs/1505.05770
  51. https://arxiv.org/abs/1502.04623
  52. https://arxiv.org/abs/1606.03657
  53. https://github.com/openai/infogan
  54. https://en.wikipedia.org/wiki/mutual_information
  55. https://arxiv.org/abs/1503.03167
  56. https://en.wikipedia.org/wiki/reinforcement_learning
  57. https://openai.com/blog/openai-gym-beta/
  58. http://arxiv.org/abs/1605.09674
  59. https://github.com/openai/vime
  60. http://karpathy.github.io/2016/05/31/rl/
  61. https://en.wikipedia.org/wiki/reinforcement_learning#direct_policy_search
  62. http://arxiv.org/abs/1606.03476
  63. https://github.com/openai/imitation
  64. https://gym.openai.com/
  65. https://gym.openai.com/envs/ant-v1
  66. https://gym.openai.com/envs/humanoid-v1
  67. https://math.berkeley.edu/~sethian/2006/applications/imageprocessing/noiseremoval.html
  68. https://en.wikipedia.org/wiki/inpainting
  69. https://en.wikipedia.org/wiki/super-resolution_imaging
  70. https://en.wikipedia.org/wiki/structured_prediction
  71. https://en.wikipedia.org/wiki/reinforcement_learning#exploration
  72. http://image.diku.dk/shark/sphinx_pages/build/html/rest_sources/tutorials/algorithms/deep_denoising_autoencoder_network.html
  73. https://openai.com/blog/authors/andrej/
  74. https://openai.com/blog/authors/pieter/
  75. https://openai.com/blog/authors/greg/
  76. https://openai.com/blog/authors/peter-chen/
  77. https://openai.com/blog/authors/vicki-cheung/
  78. https://openai.com/blog/authors/rocky/
  79. https://openai.com/blog/authors/ian/
  80. https://openai.com/blog/authors/diederik/
  81. https://openai.com/blog/authors/jonathan-ho/
  82. https://openai.com/blog/authors/rein/
  83. https://openai.com/blog/authors/tim/
  84. https://openai.com/blog/authors/john/
  85. https://openai.com/blog/authors/ilya/
  86. https://openai.com/blog/authors/wojciech/
  87. https://openai.com/about/
  88. https://openai.com/progress/
  89. https://openai.com/resources/
  90. https://openai.com/blog/
  91. https://openai.com/charter/
  92. https://openai.com/jobs/
  93. https://openai.com/press/

   hidden links:
  95. https://openai.com/
  96. https://openai.com/
  97. https://twitter.com/openai
  98. https://www.facebook.com/openai.research
