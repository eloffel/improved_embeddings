   #[1]magenta

   [2]magenta logo

   (button)
   [3]get started [4]demos [5]blog [6]research [7]community

   [8]get started [9]demos [10]blog [11]research [12]community

generating long-term structure in songs and stories

   jul 15, 2016
       elliot waite [13]elliotwaite

   one of the difficult problems in using machine learning to generate
   sequences, such as melodies, is creating long-term structure. long-term
   structure comes very naturally to people, but it   s very hard for
   machines. basic machine learning systems can generate a short melody
   that stays in key, but they have trouble generating a longer melody
   that follows a chord progression, or follows a multi-bar song structure
   of verses and choruses. likewise, they can produce a screenplay with
   grammatically correct sentences, but not one with a compelling plot
   line. without long-term structure, the content produced by recurrent
   neural networks (id56s) often seems wandering and random.

   but what if these id56 models could recognize and reproduce longer-term
   structure? could they produce content that feels more meaningful     more
   human? today we   re open-sourcing two new magenta models, [14]lookback
   id56 and [15]attention id56, both of which aim to improve id56s    ability
   to learn longer-term structures. we hope you   ll join us in exploring
   how they might produce better songs and stories.

lookback id56

   lookback id56 introduces custom inputs and labels. the custom inputs
   allow the model to more easily recognize patterns that occur across 1
   and 2 bars. they also help the model recognize patterns related to
   where in the measure an event occurs. the custom labels make it easier
   for the model to repeat sequences of notes without having to store them
   in the id56   s cell state. the type of id56 cell used in this model is an
   lstm.

   in our introductory model, basic id56, the input to the model was a
   one-hot vector of the previous event, and the label was the target next
   event. the possible events were note-off (turn off any currently
   playing note), no event (if a note is playing, continue sustaining it,
   otherwise continue silence), and a note-on event for each pitch (which
   also turns off any other note that might be playing). in lookback id56,
   we add the following additional information to the input vector:
     * in addition to inputting the previous event, we also input the
       events from 1 and 2 bars ago. this allows the model to more easily
       recognize patterns that occur across 1 and 2 bars, such as mirrored
       or contrasting melodies.
     * we also input whether the last event was repeating the event from 1
       or 2 bars before it. this signals if the last event was creating
       something new, or just repeating an already established melody.
       this allows the model to more easily recognize patterns associated
       with being in a repetitive or non-repetitive state.
     * we also input the current position within the measure (as done
       previously by [16]daniel johnson), allowing the model to more
       easily learn patterns associated with 4/4 time music. these inputs
       are 5 values that can be thought of as a binary step clock.
       step 1:
       step 2:
       step 3:
       step 4:
       the only difference being the values are -1 and 1 instead of 0 and
       1.

   in addition to feeding the model more input information, we also add
   two new custom labels. the label to repeat the event from 1 bar ago and
   the label to repeat the event from 2 bars ago. this is where the
   lookback id56 gets its name. when creating labels for the training data,
   if the current event in the melody is repeating the same event from 2
   bars ago, we set the label for that step to be repeat-2-bars-ago. if
   it   s not repeating the event from 2 bars ago, we check if it   s
   repeating the event from 1 bar ago, and if so, we set the label for
   that step to be repeat-1-bar-ago. only when the melody isn   t repeating
   1 or 2 bars ago do we make the label for that step be a specific melody
   event. for example, if the third bar of the melody is completely
   repeating the first bar, every label for that third bar will be the
   repeat-2-bars-ago label. this allows the model to more easily repeat 1
   or 2 bar phrases without having to store those sequences in its memory
   cell. since a lot of melodies in popular music repeat events from 1 and
   2 bars ago, these extra labels reduce the complexity of information the
   model has to learn to represent.

   here are some sample melodies generated by the lookback id56 model when
   trained on a collection of popular music. the intro notes (played on
   the glockenspiel) were given to the model as a priming melody. the rest
   of the notes were generated.

   iframe: [17]https://www.youtube.com/embed/vkdqeo0ubny

   iframe: [18]https://www.youtube.com/embed/5bvzdkrvmmi

   iframe: [19]https://www.youtube.com/embed/ja_dkaiyaoe

   to train the lookback id56 on your own midi collection and generate your
   own melodies from it, follow the steps in the [20]readme

attention id56

   to learn even longer-term structure we can use attention. attention is
   one of the ways that models can access previous information without
   having to store it in the id56 cell   s state. the id56 cell used in this
   model is an lstm. the attention method used comes from the paper
   [21]id4 by jointly learning to align and
   translate (d bahdanau, k cho, y bengio, 2014). in that paper, the model
   is an encoder-decoder id56, and the model uses attention to look at all
   the encoder outputs during each decoder step. in our version, where we
   don   t have an encoder-decoder, we just always look at the outputs from
   the last steps when generating the output for the current step. the way
   we    look at    these steps is with an attention mechanism. specifically:

   the vector and matrices , are learnable parameters of the model. are
   the id56 outputs from the previous steps , and vector is the current
   step   s id56 cell state. these values are used to calculate , an length
   vector with one value for each of the previous steps. the values
   represent how much attention each step should receive. a softmax is
   used to normalize these values and create a mask-like vector , called
   the attention mask. the id56 outputs from the previous steps are then
   multiplied by these attention mask values and then summed together to
   get . for example, let   s assume we are on the 4th step of our sequence
   and = 3, which means our attention mechanism is only looking at the
   last 3 steps. for this example, the id56 output vectors will be small 4
   length vectors. if the id56 outputs from the first 3 steps are:

   step 1:
   step 2:
   step 3:

   and our calculated attention mask is:

   then the previous step would get 20% attention, 2 steps ago would get
   10% attention, and 3 steps ago would get 70% attention. so their masked
   values would be:

   step 1 (70%):
   step 2 (10%):
   step 3 (20%):

   and then they   d be summed together to get :

   the vector is essentially all previous outputs combined together, but
   each output contributing a different amount relative to how much
   attention that step received.

   this vector is then concatenated with the id56 output from the current
   step and a linear layer is applied to that concatenated vector to
   create the new output for the current step. some id12 only
   apply this vector to the id56 output, but in our model, as is also
   sometimes done, this vector is also applied to the input of the next
   step. the vector is concatenated with the next step   s input vector and
   a linear layer is applied to that concatenated vector to create the new
   input to the id56 cell. this helps attention not only affect the data
   coming out of the id56 cell, but also the data being fed into the id56
   cell.

   this vector, which is a combination of the outputs from the previous
   steps, is how attention can directly inject information from those
   previous steps into the current step   s network of calculations, making
   it easier for the model to learn longer-term dependencies without
   having to store all that information from those previous steps in the
   id56 cell   s state. if you   d like an even deeper understanding of the
   whole attention process, you can walk through [22]the code to see
   exactly what   s happening.

   here are some sample melodies generated by the attention id56 model when
   trained on a collection of popular music. these melodies were all
   primed with the first four notes of twinkle twinkle little star, then
   the rest of the notes were generated by the model:

   iframe: [23]https://www.youtube.com/embed/nhxr9u9_4_s

   iframe: [24]https://www.youtube.com/embed/iz-oqpzoqiq

   iframe: [25]https://www.youtube.com/embed/yw-svx64xoa

   melody 1 and 2 were combined in a standard song format, aaba, and
   backed up by drums to create the following song sample:

   iframe: [26]https://www.youtube.com/embed/tgkd8_r-yl8

   jason nguyen ([27]@soulgook), on the     n b   u, and alex koman
   ([28]@meloscribe), on guitar, added to that song to create this man and
   machine collaboration:

   iframe: [29]https://www.youtube.com/embed/aq3370prbi4

   the following song uses the three attention id56 melodies listed above
   by layering them all together. they compliment each other surprisingly
   well. the drums and bass line were added by a human. this demonstrates
   how musicians could use these generated melodies for building out
   larger, more elaborate songs.

   iframe: [30]https://www.youtube.com/embed/qfbqdfpyjoe

   to train the attention id56 on your own midi collection and generate
   your own melodies from it, follow the steps in the [31]readme on
   github.

   these models improve on the initial magenta [32]basic id56 by adding two
   forms of memory manipulation, simple lookback and learned attention.
   nevertheless, a lot of work remains before magenta models are writing
   complete pieces of music or telling long stories. stay tuned for more
   improvements.

   edit (@elliotwaite aug 8, 2016): updated the reference for the
   attention method used to the paper that originally introduced the idea,
   [33]id4 by jointly learning to align and
   translate (d bahdanau, k cho, y bengio, 2014).

     * [34]twitter
     * [35]blog
     * [36]github
     * [37]privacy
     * [38]terms

references

   visible links
   1. https://magenta.tensorflow.org/feed.xml
   2. https://magenta.tensorflow.org/
   3. https://magenta.tensorflow.org/get-started
   4. https://magenta.tensorflow.org/demos
   5. https://magenta.tensorflow.org/blog
   6. https://magenta.tensorflow.org/research
   7. https://magenta.tensorflow.org/community
   8. https://magenta.tensorflow.org/get-started
   9. https://magenta.tensorflow.org/demos
  10. https://magenta.tensorflow.org/blog
  11. https://magenta.tensorflow.org/research
  12. https://magenta.tensorflow.org/community
  13. https://github.com/elliotwaite
  14. https://github.com/tensorflow/magenta/blob/master/magenta/models/lookback_id56
  15. https://github.com/tensorflow/magenta/blob/master/magenta/models/attention_id56
  16. http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
  17. https://www.youtube.com/embed/vkdqeo0ubny
  18. https://www.youtube.com/embed/5bvzdkrvmmi
  19. https://www.youtube.com/embed/ja_dkaiyaoe
  20. https://github.com/tensorflow/magenta/blob/master/magenta/models/lookback_id56
  21. https://arxiv.org/abs/1409.0473
  22. https://github.com/tensorflow/magenta/blob/master/magenta/models/shared/events_id56_graph.py
  23. https://www.youtube.com/embed/nhxr9u9_4_s
  24. https://www.youtube.com/embed/iz-oqpzoqiq
  25. https://www.youtube.com/embed/yw-svx64xoa
  26. https://www.youtube.com/embed/tgkd8_r-yl8
  27. https://www.youtube.com/user/soulgook/
  28. https://www.facebook.com/meloscribe
  29. https://www.youtube.com/embed/aq3370prbi4
  30. https://www.youtube.com/embed/qfbqdfpyjoe
  31. https://github.com/tensorflow/magenta/blob/master/magenta/models/attention_id56
  32. https://github.com/tensorflow/magenta/blob/master/magenta/models/basic_id56
  33. https://arxiv.org/abs/1409.0473
  34. https://twitter.com/search?q=#madewithmagenta
  35. https://magenta.tensorflow.org/blog
  36. https://github.com/tensorflow/magenta
  37. https://www.google.com/policies/privacy/
  38. https://www.google.com/policies/terms/

   hidden links:
  40. https://ai.google/
