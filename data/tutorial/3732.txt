   #[1]bcomposes    feed [2]bcomposes    comments feed [3]bcomposes    simple
   end-to-end tensorflow examples comments feed [4]improving race
   relations: a path forward [5]my top 100 songs of 2015 [6]alternate
   [7]alternate

   [8]skip to content

   [9]bcomposes

   computational linguistics, machine learning, programming, and random
   thoughts

simple end-to-end tensorflow examples

   a walk-through with code for using tensorflow on some simple simulated
   data sets.

   i   ve been reading papers about deep learning for several years now, but
   until recently hadn   t dug in and implemented any models using deep
   learning techniques for myself. to remedy this, i started experimenting
   with [10]deeplearning4j a few weeks ago, but [11]with limited success.
   i read more [12]books, [13]primers and tutorials, especially the
   amazing series of blog posts by [14]chris olah and [15]denny britz.
   then, with incredible timing for me, google released [16]tensorflow to
   much general excitement. so, i figured i   d give it a go, especially
   given [17]delip rao   s enthusiasm for it   he even compared the move from
   [18]theano to tensorflow feeling like changing from    a honda civic to a
   ferrari.   

   here   s a quick prelude before getting to my initial simple explorations
   with tensorflow. as most people (hopefully) know, deep learning
   encompasses ideas going back many decades (done under the names of
   connectionism and neural networks) that only became viable at scale in
   the past decade with the advent of faster machines and some algorithmic
   innovations. i was first introduced to them in a class taught by my phd
   advisor, [19]mark steedman, at the university of pennsylvania in 1997.
   he was especially interested in how they could be applied to language
   understanding, which he wrote about in his 1999 paper
      [20]connectionist sentence processing in perspective.    i wish i
   understood more about that topic (and many others) back then, but then
   again that   s the nature of being a young grad student. anyway, mark   s
   interest in connectionist language processing arose in part from being
   on the dissertation committee of [21]james henderson, who completed his
   thesis    [22]description based parsing in a connectionist network    in
   1994. james was a post-doc in the institute for research in cognitive
   science at penn when i arrived in 1996. as a young grad student, i had
   little idea of what connectionist parsing entailed, and my
   understanding from more senior (and far more knowledgeable) students
   was that james    parsers were really interesting but that he had trouble
   getting the models to scale to larger data sets   at least compared to
   the data-driven parsers that others like [23]mike collins and
   [24]adwait ratnarparkhi were building at penn in the mid-1990s. (side
   note: for all the kids using id28 for nlp out there, you
   probably don   t know that adwait was the one who first applied lr/maxent
   to several nlp problems in his 1998 dissertation    [25]maximum id178
   models for natural language ambiguity resolution   , in which he
   demonstrated how amazingly effective it was for everything from
   classification to part-of-speech tagging to parsing.)

   back to tensorflow and the present day. i flew from austin to
   washington dc last week, and the morning before my flight i downloaded
   tensorflow, made sure everything compiled, downloaded the necessary
   datasets, and opened up a bunch of tabs with tensorflow tutorials. my
   goal was, while on the airplane, to run the tutorials, get a feel for
   the flow of tensorflow, and then implement my own networks for doing
   some made-up classification problems. i came away from the exercise
   extremely pleased. this post explains what i did and gives pointers to
   the code to make it happen. my goal is to help out people who could use
   a bit more explicit instruction and guidance using a complete
   end-to-end example with easy to understand data. i won   t give lots of
   code examples in this post as there are several tutorials that already
   do that quite well   the value here is in the simple end-to-end
   implementations, the data to go with them, and a bit of explanation
   along the way.

   as a preliminary, i recommend going to the excellent [26]tensorflow
   documentation, [27]downloading it, and running the first example. if
   you can do that, you should be able to run the code i   ve provided to go
   along with this post in [28]my try-tf repository on github.

simulated data

   as a researcher who works primarily on empirical methods in natural
   language processing, my usual tendency is to try new software and ideas
   out on language data sets, e.g. text classification problems and the
   like. however, after hanging out with a statistician like [29]james
   scott for many years, i   ve come to appreciate the value of using
   simulated datasets early on to reduce the number of unknowns while
   getting the basics right. so, when sitting down with tensorflow, i
   wanted to try three simulated data sets: linearly separable data, moon
   data and saturn data. the first is data that linear classifiers can
   handle easily, while the latter two require the introduction of
   non-linearities enabled by models like multi-layer neural networks.
   here   s what they look like, with brief descriptions.

   the linear data has two clusters that can be separated by a diagonal
   line from top left to bottom right:

   linear_data_train.jpg

   linear classifiers like id88s, id28, linear
   discriminant analysis, support vector machines and others do well with
   this kind of data because learning these lines (hyperplanes) is exactly
   what they do.

   the moon data has two clusters in crescent shapes that are tangled up
   such that no line can keep all the orange dots on one side without also
   including blue dots.

   moon_data_train.jpg

   note: see [30]implementing a neural network from scratch in python for
   a discussion working with the moon data using theano.

   the saturn data has a core cluster representing one class and a ring
   cluster representing the other. saturn_data_train.jpg

   with the saturn data, a line is catastrophically bad. perhaps the best
   one can do is draw a line that has all the orange points to one side.
   this ensures a small, entirely blue side, but it leaves the majority of
   blue dots in orange terroritory.

   example data has been generated in try-tf/simdata for each of these
   datasets, including a training set and test set for each. these are for
   the two dimensional cases visualized above, but you can use the scripts
   in that directory to generate data with other parameters, including
   more dimensions, greater variances, etc. see the commented out code for
   help to visualize the outputs, or adapt plot_data.r, which visualizes
   2-d data in csv format. see the  [31]readme for instructions.

   related: check out delip rao   s [32]post on learning arbitrary lambda
   expressions.

softmax regression

   let   s start with a network that can handle the linear data, which i   ve
   written in softmax.py. the tensorflow page has pretty good
   [33]instructions for how to define a single layer network for mnist,
   but no end-to-end code that defines the network, reads in data
   (consisting of label plus features), trains and evaluates the model. i
   found writing this to be a good way to familiarize myself with the
   tensorflow python api, so i recommend trying it yourself before looking
   at my code and then referring to it if you get stuck.

   let   s run it and see what we get.
$ python softmax.py --train simdata/linear_data_train.csv --test simdata/linear_
data_eval.csv
accuracy: 0.99

   this performs one pass (epoch) over the training data, so parameters
   were only updated once per example. 99% is good held-out accuracy, but
   allowing two training epochs gets us to 100%.
$ python softmax.py --train simdata/linear_data_train.csv --test simdata/linear_
data_eval.csv --num_epochs 2
accuracy: 1.0

   there   s a bit of code in softmax.py to handle options and read in data.
   the most important lines are the ones that define the input data, the
   model, and the training step. i simply adapted these from [34]the mnist
   beginners tutorial, but softmax.py puts it all together and provides a
   basis for transitioning to the network with a hidden layer discussed
   later in this post.

   to see a little more, let   s turn on the verbose flag and run for 5
   epochs.
$ python softmax.py --train simdata/linear_data_train.csv --test simdata/linear_
data_eval.csv --num_epochs 5 --verbose true
initialized!

training.
0 1 2 3 4 5 6 7 8 9
10 11 12 13 14 15 16 17 18 19
20 21 22 23 24 25 26 27 28 29
30 31 32 33 34 35 36 37 38 39
40 41 42 43 44 45 46 47 48 49

weight matrix.
[[-1.87038445 1.87038457]
[-2.23716712 2.23716712]]

bias vector.
[ 1.57296884 -1.57296848]

applying model to first test instance.
point = [[ 0.14756215 0.24351828]]
wx+b = [[ 0.7521798 -0.75217938]]
softmax(wx+b) = [[ 0.81822371 0.18177626]]

accuracy: 1.0

   consider first the weights and bias. intuitively, the classifier should
   find a separating hyperplane between the two classes, and it probably
   isn   t immediately obvious how w and b define that. for now, consider
   only the first column with w1=-1.87038457, w2=-2.23716712 and
   b=1.57296848. recall that w1 is the parameter for the `x` dimension and
   w2 is for the `y` dimension. the separating hyperplane satisfies
   wx+b=0; from which we get the standard y=mx+b form.

   wx + b = 0
   w1*x + w2*y + b = 0
   w2*y = -w1*x     b
   y = (-w1/w2)*x     b/w2

   for the parameters learned above, we have the line:

   y = -0.8360504*x + 0.7031074

   here   s the plot with the line, showing it is an excellent fit for the
   training data.

   . linear_data_hyperplane.jpg

   the second column of weights and bias separate the data points at the
   same place as the first, but mirrored 180 degrees from the first
   column. strictly speaking, it is redundant to have two output nodes
   since a multinomial distribution with n outputs can be represented with
   n-1 parameters (see [35]section 9.3 of andrew ng   s notes on supervised
   learning for details). nonetheless, it   s convenient to define the
   network this way.

   finally, let   s try the softmax network on the moon and saturn data.
python softmax.py --train simdata/moon_data_train.csv --test simdata/moon_data_e
val.csv --num_epochs 2
accuracy: 0.856

$ python softmax.py --train simdata/saturn_data_train.csv --test simdata/saturn_
data_eval.csv --num_epochs 2
accuracy: 0.45

   as expected, it doesn   t work very well!

network with a hidden layer

   the program hidden.py implements a network with a single hidden layer,
   and you can set the size of the hidden layer from the command line.
   let   s try first with a two-node hidden layer on the moon data.
$ python hidden.py --train simdata/moon_data_train.csv --test simdata/moon_data_
eval.csv --num_epochs 100 --num_hidden 2
accuracy: 0.88

   so,that was an improvement over the softmax network. let   s run it
   again, exactly the same way.
$ python hidden.py --train simdata/moon_data_train.csv --test simdata/moon_data_
eval.csv --num_epochs 100 --num_hidden 2
accuracy: 0.967

   very different! what we are seeing is the effect of random
   initialization, which has a large effect on the learned parameters
   given the small, low-dimensional data we are dealing with here. (the
   network uses xavier initialization for the weights.) let   s try again
   but using three nodes.
$ python hidden.py --train simdata/moon_data_train.csv --test simdata/moon_data_
eval.csv --num_epochs 100 --num_hidden 3
accuracy: 0.969

   if you run this several times, the results don   t vary much and hover
   around 97%. the additional node increases the representational capacity
   and makes the network less sensitive to initial weight settings.

   adding more nodes doesn   t change results much   see [36]the wildml post
   using the moon data for [37]some nice visualizations of the boundaries
   being learned between the two classes for different hidden layer sizes.

   so, a hidden layer does the trick! let   s see what happens with the
   saturn data.
$ python hidden.py --train simdata/saturn_data_train.csv --test simdata/saturn_d
ata_eval.csv --num_epochs 50 --num_hidden 2
accuracy: 0.76

   with just two hidden nodes, we already have a substantial boost from
   the 45% achieved by softmax regression. with 15 hidden nodes, we get
   100% accuracy. there is considerable variation from run to run (due to
   random initialization). as with the moon data, there is less variation
   as nodes are added. here   s a plot showing the increase in performance
   from 1 to 15 nodes, including ten accuracy measurements for each node
   count.

   hidden_node_curve.jpg

   the line through the middle is the average accuracy measurement for
   each node count.

initialization and id180 are important

   my first attempt at doing a network with a hidden layer was to merge
   what i had done in softmax.py with the network in [38]mnist.py,
   provided with tensorflow tutorials. this was a useful exercise to get a
   better feel for the tensorflow python api, and helped me understand the
   programming model much better. however, i found that i needed to have
   upwards of 25 or more hidden nodes in order to reliably get >96%
   accuracy on the moon data.

   i then looked back at [39]the wildml moon example and figured something
   was quite wrong since just three hidden nodes were sufficient there.
   the differences were that the mnist example initializes its hidden
   layers with truncated normals instead of normals divided by the square
   root of the input size, initializes biases at 0.1 instead of 0 and uses
   relu activations instead of tanh. by switching to xavier initialization
   (using delip   s handy function), 0 biases, and tanh, everything worked
   as in the wildml example. i   m including my initial version in the repo
   as truncnorm_hidden.py so that others can see the difference and play
   around with it. (it turns out that what matters most is the
   initialization of the weights.)

   this is a simple example of what is often discussed with deep learning
   methods: they can work amazingly well, but they are very sensitive to
   initialization and choices about the sizes of layers, activation
   functions, and the influence of these choices on each other. they are a
   very powerful set of techniques, but they (still) require finesse and
   understanding, compared to, say, many linear modeling toolkits that can
   effectively be used as black boxes these days.

conclusion

   i walked away from this exercise very encouraged! i   ve been programming
   in scala mostly for the last five years, so it required dusting off my
   python (which i taught in my classes at ut austin from 2005-2011, e.g.
   [40]computational linguistics i and [41]natural language processing),
   but i found it quite straightforward. since i work primarily with
   language processing tasks, i   m perfectly happy with python since it   s a
   great language for munging language data into the inputs needed by
   packages like tensorflow. also, python works well as a dsl for working
   with deep learning (it seems like there is a new python deep learning
   package announced every week these days). it took me less than four
   hours to go through initial examples, and then build the softmax and
   hidden networks and apply them to the three data sets. (and a bunch of
   that time was me remembering how to do things in python.)

   i   m now looking forward to trying deep learning models, especially
   convnets and lstm   s, on language and image tasks. i   m also going to go
   back to [42]my scala code for trying out deeplearning4j to see if i can
   get these simulation examples to run as i   ve shown here with
   tensorflow. (i would welcome pull requests if someone else gets to that
   first!) as a person who works primarily on the jvm, it would be very
   handy to be able to work with dl4j as well.

   after that, maybe i   ll write out the re-occurring rant going on in my
   head about deep learning not removing the need for feature engineering
   (as many [43]backpropagandists seem to like to claim), but instead
   changing the nature of feature engineering, as well as providing a
   really cool set of new capabilities and tricks.

share this:

     * [44]click to share on twitter (opens in new window)
     * [45]click to share on facebook (opens in new window)
     *

related

author: jasonbaldridge

   co-founder of people pattern and associate professor in the department
   of linguistics at the university of texas at austin. my primary
   specialization is computational linguistics and my core research
   interests are formal and computational models of syntax, probabilistic
   models of both syntax and discourse structure, and machine learning for
   natural language tasks in general. [46]view all posts by jasonbaldridge

   author [47]jasonbaldridgeposted on [48]november 26, 2015june 5,
   2016categories [49]computational linguistics, [50]machine learning,
   [51]programming, [52]r, [53]tutorial

18 thoughts on    simple end-to-end tensorflow examples   

    1.
   [54]arno candel says:
       [55]november 26, 2015 at 1:03 pm
       have you tried out h2o? very easy to use, distributed, open-source
       deep learning and much more: gbm, glm etc.
       [56]reply
         1.
        [57]jasonbaldridge says:
            [58]november 28, 2015 at 9:59 pm
            no, i haven   t. thanks for the suggestion!
            [59]reply
    2.
   octavian says:
       [60]december 7, 2015 at 4:38 am
       can you please tell us why didn   t you find dl4j useful ? i also
       started experimenting with it, but seems that they lack some basic
       stuff, e.g. plot validation error after each k iterations.
       [61]reply
         1.
        [62]jasonbaldridge says:
            [63]december 29, 2015 at 5:26 pm
            i tried to get these simple examples working with dl4j and the
            models i was training were not able to separate the non-linear
            examples after a fair amount of poking around. i also found
            that the example dataset evaluations on my machine didn   t
            produce numbers consistent with dl4j   s stated performance
            figures, which was concerning. tensorflow came out at that
            time, so i decided to give it a spin and it worked with very
            little effort, so that was quite encouraging.
            after doing this post, i did try briefly to get dl4j working
            with the examples again. i updated to the latest version, and
            found that even the linear example no longer worked. i banged
            at it for an evening without success, and decided to move on.
            i   d really love for dl4j to work well, so i   m hoping that will
            continue to evolve and improve. in the meantime, tensorflow
            seems quite promising, even given its current performance
            issues, so i   m going to continue to invest my time in it.
            related to this, here   s a great post that compares the major
            deep learning frameworks (dl4j doesn   t appear).
            [64]https://github.com/zer0n/deepframeworks/blob/master/readme
            .md
            [65]reply
    3.
   sujitpal says:
       [66]december 9, 2015 at 4:16 pm
       great examples, thanks for sharing. being able to run the examples
       on a (non-gpu enabled) laptop is a great help to understand the
       tensorflow api.
       [67]reply
    4.
   david moena says:
       [68]december 15, 2015 at 10:06 am
       great post! end-to-end examples are always welcome. i only have one
       recommendation: please, change your current wordpress theme to one
       more easy to read (colored links would be great, for example).
       i found this as recommended on pocket ([69]https://getpocket.com)
       and read it there. coming to the original page makes me think that
       i would probably look for another option only because is visually
       unclear, making me think that the content itself it might be
       unclear too (which is completely the opposite)
       it might look like a silly request, but i think it will help you to
       reach more people.
       thank you for sharing!
       [70]reply
         1.
        [71]jasonbaldridge says:
            [72]december 29, 2015 at 5:19 pm
            thanks! i realize things are ugly and am aiming to improve the
            theme soon!
            [73]reply
         2.
        [74]jasonbaldridge says:
            [75]june 5, 2016 at 4:38 am
            so, after a looong time, i   ve finally gotten around to moving
            my wordpress site to a self-hosted site. still getting things
            set up, but i think it is already looking much better!
            [76]reply
    5. pingback: [77]hpssjellis/easy-tensorflow-on-cloud9     gitroom
    6.
   [78]imanistn says:
       [79]january 5, 2016 at 9:04 am
       great post, it help me a lot to start using tensorflow after some
       days of searching around. keep up the good work
       [80]reply
         1.
        [81]jasonbaldridge says:
            [82]january 5, 2016 at 4:27 pm
            thanks! i   m hoping to post more in the coming month or so.
            [83]reply
    7.
   [84]lalit patel says:
       [85]february 12, 2016 at 2:52 am
       thank you for beautifully explaining some of these concepts. your
       blog will help me in tensorflow   ing.
             . deep learning methods: they can work amazingly well, but they
       are very sensitive to initialization and choices about the sizes of
       layers, id180, and the influence of these choices on
       each other.   
       i am wondering whether we can find/devise something similar to the
       central limit theorem.
       [86]reply
         1.
        [87]jasonbaldridge says:
            [88]february 16, 2016 at 12:54 pm
            sure, glad it helped!
            i   m not sure there   s a good analogy/connection to the central
            limit theorem here, though what you might be thinking of is
            ensembles of models. e.g., if you have ten models that make
            predictions using different parameters, cuts of the data, etc,
            then you can often get better performance. this is one of the
            core ideas at the heart of id79s, which are
            incredibly effective for many classification and regression
            tasks.
            more practically as regards this question   for a really in
            depth study of parameter choices in a convolutional neural net
            for id31, check out this paper by zhang and
            wallace: [89]http://arxiv.org/abs/1510.03820
            [90]reply
    8.
   [91]brittblaser says:
       [92]april 13, 2016 at 9:48 pm
       would you like to discuss a project that i think involves
          empirical methods in natural language processing   ?
       it   s for my daughter who happens to be close to jeff and heidi
       dean. jeff thinks it   s workable and so he   ll be following it but,
       of course, from a distance.
       [93]britt.blaser@gmail.com
       [94]reply
         1.
        [95]jasonbaldridge says:
            [96]june 5, 2016 at 4:55 am
            happy to hear more     feel free to email me with more details:
            [97]jason@bcomposes.com
            [98]reply
    9.
   [99]mustafa qamar-ud-din says:
       [100]may 1, 2016 at 12:55 pm
       reblogged this on [101]qamar-ud-din.
       [102]reply
   10. pingback: [103]d97: tensorflow deep learning books and examples    
       ai
   11. pingback: [104]bcomposes is now self-hosted!     bcomposes

leave a reply [105]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   [ ] notify me of follow-up comments by email.

   [ ] notify me of new posts by email.
   post comment

post navigation

   [106]previous previous post: improving race relations: a path forward
   [107]next next post: my top 100 songs of 2015

   search for: ____________________ (button) search

recent posts

     * [108]babbitt, 1922: race and immigration conversation still
       relevant
     * [109]bcomposes is now self-hosted!
     * [110]arrowsmith by sinclair lewis
     * [111]my top 100 songs of 2015
     * [112]simple end-to-end tensorflow examples

recent comments

     * [113]bcomposes is now self-hosted!     bcomposes on [114]simple
       end-to-end tensorflow examples
     * [115]jasonbaldridge on [116]simple end-to-end tensorflow examples
     * [117]jasonbaldridge on [118]simple end-to-end tensorflow examples
     * [119]d97: tensorflow deep learning books and examples     ai on
       [120]simple end-to-end tensorflow examples
     * [121]mustafa qamar-ud-din on [122]simple end-to-end tensorflow
       examples

archives

     * [123]july 2016
     * [124]june 2016
     * [125]february 2016
     * [126]january 2016
     * [127]november 2015
     * [128]october 2015
     * [129]august 2015
     * [130]june 2015
     * [131]may 2015
     * [132]september 2014
     * [133]july 2014
     * [134]august 2013
     * [135]february 2013
     * [136]january 2013
     * [137]september 2012
     * [138]may 2012
     * [139]april 2012
     * [140]march 2012
     * [141]february 2012
     * [142]december 2011
     * [143]november 2011
     * [144]october 2011
     * [145]september 2011
     * [146]august 2011

categories

     * [147]academia
     * [148]books
     * [149]computational linguistics
     * [150]geo
     * [151]life
     * [152]machine learning
     * [153]music
     * [154]programming
     * [155]r
     * [156]random thoughts
     * [157]research
     * [158]scala
     * [159]talks
     * [160]tutorial
     * [161]twitter
     * [162]uncategorized
     * [163]unix

meta

     * [164]log in
     * [165]entries rss
     * [166]comments rss
     * [167]wordpress.org

   [168]bcomposes [169]proudly powered by wordpress

references

   1. http://bcomposes.com/feed/
   2. http://bcomposes.com/comments/feed/
   3. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/feed/
   4. http://bcomposes.com/2015/11/08/improving-race-relations-a-path-forward/
   5. http://bcomposes.com/2016/01/27/my-top-100-songs-of-2015/
   6. http://bcomposes.com/wp-json/oembed/1.0/embed?url=http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/
   7. http://bcomposes.com/wp-json/oembed/1.0/embed?url=http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/&format=xml
   8. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#content
   9. http://bcomposes.com/
  10. http://deeplearning4j.org/
  11. https://github.com/jasonbaldridge/explore-dl4j
  12. http://www.iro.umontreal.ca/~bengioy/dlbook/version-30-03-2015/dlbook.html
  13. http://arxiv.org/abs/1510.00726
  14. http://colah.github.io/
  15. http://www.wildml.com/
  16. http://www.tensorflow.org/
  17. http://deliprao.com/archives/98
  18. http://deeplearning.net/software/theano/
  19. http://homepages.inf.ed.ac.uk/steedman/
  20. http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog2304_10/pdf
  21. http://cui.unige.ch/~hendersj/
  22. http://cui.unige.ch/~hendersj/papers/henderson_thesis.pdf
  23. http://www.cs.columbia.edu/~mcollins/
  24. https://labs.yahoo.com/researchers/adwaitr
  25. http://repository.upenn.edu/cgi/viewcontent.cgi?article=1061&context=ircs_reports
  26. http://www.tensorflow.org/get_started
  27. http://www.tensorflow.org/get_started/os_setup.html
  28. https://github.com/jasonbaldridge/try-tf
  29. http://jgscott.github.io/
  30. http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/
  31. https://github.com/jasonbaldridge/try-tf/blob/master/readme.md
  32. http://deliprao.com/archives/100
  33. http://www.tensorflow.org/tutorials/mnist/beginners/index.md
  34. http://www.tensorflow.org/tutorials/mnist/beginners/index.md
  35. http://cs229.stanford.edu/notes/cs229-notes1.pdf
  36. http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/
  37. http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/nn-from-scratch-hidden-layer-varying.png
  38. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/tutorials/mnist/mnist.py
  39. http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/
  40. http://www.utexas.edu/courses/jbaldrid/2006/compling1/
  41. http://nlp-s11.utcompling.com/
  42. https://github.com/jasonbaldridge/explore-dl4j
  43. https://twitter.com/nsaphra/status/573893191918837761
  44. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?share=twitter
  45. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?share=facebook
  46. http://bcomposes.com/author/jasonbaldridge/
  47. http://bcomposes.com/author/jasonbaldridge/
  48. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/
  49. http://bcomposes.com/category/computational-linguistics/
  50. http://bcomposes.com/category/machine-learning/
  51. http://bcomposes.com/category/programming/
  52. http://bcomposes.com/category/r/
  53. http://bcomposes.com/category/tutorial/
  54. http://h2o.ai/
  55. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-158
  56. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=158#respond
  57. http://bcomposes.wordpress.com/
  58. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-159
  59. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=159#respond
  60. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-160
  61. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=160#respond
  62. http://bcomposes.wordpress.com/
  63. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-164
  64. https://github.com/zer0n/deepframeworks/blob/master/readme.md
  65. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=164#respond
  66. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-161
  67. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=161#respond
  68. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-162
  69. https://getpocket.com/
  70. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=162#respond
  71. http://bcomposes.wordpress.com/
  72. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-163
  73. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=163#respond
  74. http://www.jasonbaldridge.com/
  75. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-173
  76. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=173#respond
  77. https://gitroom.org/?p=70347
  78. http://atodo.wordpress.com/
  79. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-166
  80. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=166#respond
  81. http://bcomposes.wordpress.com/
  82. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-167
  83. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=167#respond
  84. http://predicty.net/
  85. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-168
  86. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=168#respond
  87. http://bcomposes.wordpress.com/
  88. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-169
  89. http://arxiv.org/abs/1510.03820
  90. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=169#respond
  91. http://newgov.us/
  92. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-170
  93. mailto:britt.blaser@gmail.com
  94. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=170#respond
  95. http://www.jasonbaldridge.com/
  96. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-174
  97. mailto:jason@bcomposes.com
  98. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=174#respond
  99. http://mustafamahrous.wordpress.com/
 100. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-171
 101. https://mustafamahrous.wordpress.com/2016/05/01/simple-end-to-end-tensorflow-examples/
 102. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/?replytocom=171#respond
 103. https://ai25blog.wordpress.com/2016/05/23/d97-tensorflow-deep-learning-books-and-examples/
 104. http://bcomposes.com/2016/06/05/bcomposes-is-now-self-hosted/
 105. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#respond
 106. http://bcomposes.com/2015/11/08/improving-race-relations-a-path-forward/
 107. http://bcomposes.com/2016/01/27/my-top-100-songs-of-2015/
 108. http://bcomposes.com/2016/07/22/babbitt-1922-race-and-immigration-conversation-still-relevant/
 109. http://bcomposes.com/2016/06/05/bcomposes-is-now-self-hosted/
 110. http://bcomposes.com/2016/02/21/arrowsmith-by-sinclair-lewis/
 111. http://bcomposes.com/2016/01/27/my-top-100-songs-of-2015/
 112. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/
 113. http://bcomposes.com/2016/06/05/bcomposes-is-now-self-hosted/
 114. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-175
 115. http://www.jasonbaldridge.com/
 116. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-174
 117. http://www.jasonbaldridge.com/
 118. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-173
 119. https://ai25blog.wordpress.com/2016/05/23/d97-tensorflow-deep-learning-books-and-examples/
 120. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-172
 121. http://mustafamahrous.wordpress.com/
 122. http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples/#comment-171
 123. http://bcomposes.com/2016/07/
 124. http://bcomposes.com/2016/06/
 125. http://bcomposes.com/2016/02/
 126. http://bcomposes.com/2016/01/
 127. http://bcomposes.com/2015/11/
 128. http://bcomposes.com/2015/10/
 129. http://bcomposes.com/2015/08/
 130. http://bcomposes.com/2015/06/
 131. http://bcomposes.com/2015/05/
 132. http://bcomposes.com/2014/09/
 133. http://bcomposes.com/2014/07/
 134. http://bcomposes.com/2013/08/
 135. http://bcomposes.com/2013/02/
 136. http://bcomposes.com/2013/01/
 137. http://bcomposes.com/2012/09/
 138. http://bcomposes.com/2012/05/
 139. http://bcomposes.com/2012/04/
 140. http://bcomposes.com/2012/03/
 141. http://bcomposes.com/2012/02/
 142. http://bcomposes.com/2011/12/
 143. http://bcomposes.com/2011/11/
 144. http://bcomposes.com/2011/10/
 145. http://bcomposes.com/2011/09/
 146. http://bcomposes.com/2011/08/
 147. http://bcomposes.com/category/academia/
 148. http://bcomposes.com/category/books/
 149. http://bcomposes.com/category/computational-linguistics/
 150. http://bcomposes.com/category/geo/
 151. http://bcomposes.com/category/life/
 152. http://bcomposes.com/category/machine-learning/
 153. http://bcomposes.com/category/music/
 154. http://bcomposes.com/category/programming/
 155. http://bcomposes.com/category/r/
 156. http://bcomposes.com/category/random-thoughts/
 157. http://bcomposes.com/category/research/
 158. http://bcomposes.com/category/scala/
 159. http://bcomposes.com/category/talks/
 160. http://bcomposes.com/category/tutorial/
 161. http://bcomposes.com/category/twitter/
 162. http://bcomposes.com/category/uncategorized/
 163. http://bcomposes.com/category/unix-2/
 164. http://bcomposes.com/wp/wp-login.php
 165. http://bcomposes.com/feed/
 166. http://bcomposes.com/comments/feed/
 167. https://wordpress.org/
 168. http://bcomposes.com/
 169. https://wordpress.org/
