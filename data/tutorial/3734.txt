   #[1]aylien    lisbon machine learning summer school highlights comments
   feed [2]alternate [3]alternate

   [4]products [5]research [6]blog [7]company [8]contact

   [9]blog

   [10]general [11]product [12]data science [13]research [14]get started

   from the blog
   product
   product updates and how-to guides for our text analysis api, news api,
   and text analysis platform
   data science
   using text and image analysis for fun and insightful data science
   projects
   research
   the latest updates from our research team.
   general
   company updates & news
   from the aylien.com

   [15]products [16]research [17]blog [18]company [19]contact

   [20]sebastian ruder
   12 aug, 2016
   general
   lisbon machine learning summer school highlights

lisbon machine learning summer school highlights

   from july 20th to july 28th 2016, i had the opportunity of  attending
   the 6th lisbon machine learning school.[21] the lisbon machine learning
   school (lxmls) is an annual event that brings together researchers and
   graduate students in the fields of nlp and computational linguistics,
   computer scientists with an interest in statistics and ml, and industry
   practitioners with a desire for a more in-depth understanding.
   participants had a chance to join workshops and labs, where they got
   hands-on experience with building and exploring state-of-the-art deep
   learning models, as well as to attend talks and speeches by prominent
   deep learning and nlp researchers from a variety of academic and
   industrial organisations. you can find the entire programme[22] here.

   in this blog post, i am going to share some of the highlights, key
   insights and takeaways of the summer school. i am going to skip the
   lectures of the first and second day as they introduce basic python,
   id202, and id203 theory concepts and focus on the later
   lectures and talks. first, we are going to talk about sequence models.
   we will then turn to id170, a type of supervised ml
   common to nlp. we will then summarize the lecture on syntax and parsing
   and finally provide insights with regard to deep learning. the
   accompanying slides can be found as a reference at the end of this blog
   post.

   disclaimer: this blog post is not meant to give a comprehensive
   introduction of each of the topics discussed; it should rather give you
   an overview of the week-long event and provide you with pointers if you
   want to delve deeper into any of the topics.

sequence models

   [23]noah smith of the university washington kicked off the third day of
   the summer school with a compelling lecture about sequence models. to
   test your understanding of sequence models, try to answer     without
   reading further     the following question: what is the most basic
   sequence model depicted in figure 1?


   [bag_of_words.png] figure 1: the most basic sequence model

   correct! it is the bag-of-words (notice which words have    fallen    out
   of the bag-of-words). the bag-of-words model makes the strongest
   independence assumption of all sequence models: it supposes that each
   word is entirely independent of its predecessors. it is obvious why
   models that rely on this assumption do only a poor job at modelling
   language: every word naturally depends on the words that have preceded
   it.

   somewhat more sophisticated models thus relax this naive assumption to
   reduce the id178: a 1st order markov model makes each word dependent
   on the word that immediately preceded it. this way, it is already able
   to capture some context of the context that can help to disambiguate a
   new word. more generally, \(m^{\text{th}}\) order markov models make
   each word depend on its previous \(m\) words.

   in mathematical terms, in \(m^{\text{th}}\) order markov models, the
   id203 of a text sequence (we assume here that such a sequence is
   delimited by start and stop symbols) can be calculated using the chain
   rule as the product of the probabilities of the individual words:

   \(p(\text{start}, w_1, w_2,    , w_n, \text{stop}) =
   \prod\limits_{i=1}^{n+1} \gamma (w_i \: | \: w_{i-m},    , w_{i-1}) \)

   where \(\gamma\) is the id203 of the current word \(w_i\) given
   its \(m\) previous words, i.e. the id203 to transition from the
   previous words to the current word.

   we can view bag-of-words and \(m^{\text{th}}\) order markov models as
   occupying the following spectrum:


   [mth_order_markov_spectrum.png] figure 2: from bag-of-words to
   history-based models

   as we go right in figure 2, we make weaker independence assumption and
   in exchange gain richer expressive power, while requiring more
   parameters     until we eventually obtain the most expressive     and most
   rigid     model, a history-based model where each word depends on its
   entire history, i.e. all preceding words.

   as a side-note, state-of-the-art sequence modelling models such as
   recurrent neural networks and lstms can be thought of as being located
   on the right side of this spectrum, as they don   t require an explicit
   specification of context words but are     theoretically     able to take
   into account the entire history.

   in many cases, we would not only like to model just the observed
   sequence of symbols, but take some additional information into account.
   id48 (id48s) allow us to associate with each symbol
   \(w_i\) some missing information, its    state    \(s_i\). the id203
   of a word sequence in an id48 then not only depends on the transition
   id203 \(\gamma\) but also on the so-called emission id203
   \(\eta\):

   \(p(\text{start}, w_1, w_2,    , w_n, \text{stop}) =
   \prod\limits_{i=1}^{n+1} \eta (w_i \: | \: s_i) \: \gamma (s_i \: | \:
   s_{i-1}) \)

   consequently, the id48 is a joint model over observable symbols and
   hidden/latent/unknown classes. id48s have traditionally been used in
   part-of-speech tagging or id39 where the hidden
   states are pos and ner tags respectively.

   if we want to determine the most probable sequence of hidden states, we
   face a space of potential sequences that grows exponentially with the
   sequence length. the classic dynamic algorithm to cope with this
   problem is the viterbi algorithm, which is used in id48s, crfs, and
   other sequence models to calculate the most probable sequence of hidden
   states: it lays out the symbol sequence and all possible states in a
   grid and proceeds left-to-right to compute the maximum id203 to
   transition in every new state given the previous states. the most
   probable sequence can then be found by back-tracking as in figure 3.


   [viterbi.png] figure 3: the viterbi algorithm

   a close relative is the forward-backward algorithm, which is used to
   calculate the id203 of a word sequence and the probabilities of
   each word   s states, e.g. for language modelling. indeed, the only
   difference between viterbi and the forward-backward algorithm is that
   viterbi takes the maximum of the probabilities of the previous state,
   while forward-backward takes the sum. in this sense, they correspond to
   the same abstract algorithm, which is instantiated in two different
   semirings where a semiring informally is a set of values and some
   operations that obey certain properties.

   finally, if we want to learn id48s in an unsupervised way, we use the
   well-known expectation maximisation (em) algorithm, which consists of
   two steps: during the e step, we calculate the id203 of each
   possible transition and emission at every position with
   forward-backward (or viterbi for    hard    em); for the m step, we
   re-estimate the parameters with id113.

machine translation

   on the evening of the third day, philipp koehn, one of the pioneers of
   mt and inventor of phrase-based machine translation gave a talk on
   machine translation as sequence modelling, including a detailed review
   of different mt and alignment approaches. if you are interested in a
   comprehensive history of mt that takes you from ibm model 1 all the way
   to phrase-based, syntax-based and eventually neural mt, while delving
   into the details of alignment, translation, and decoding, definitely
   check out the slides [24]here.

id170

   id48s can model sequences, but as their weights are tied to the
   generative process, strong independence assumptions need to be made to
   make their computation tractable. we will now turn to a category of
   models that are more expressive and can be used to predict more complex
   structures: id170     which was introduced by [25]xavier
   carreras of xerox research on the morning of the fourth day     is used
   to refer to ml algorithms that don   t just predict scalar or real
   values, but more complex structures. as complex structures are common
   in language, so is id170; example tasks of structured
   prediction in nlp include id52, id39,
   machine translation, parsing, and many others.

   a successful category of id170 models are log-linear
   models, which are so-called because they model log-probabilities using
   a linear predictor. such models try to estimate the parameters \(w\) by
   calculating the following id203:

   \(\text{log} \: \text{pr}(\mathbf{y} \: | \: \mathbf{x}; \mathbf{w}) =
   \text{log} \:\frac {\text{exp}\{\mathbf{w} \cdot
   \mathbf{f}(\mathbf{x},\mathbf{y})\}}{z(\mathbf{x};\mathbf{w})}\)

   where \(\mathbf{x} = x_1, x_2,    , x_n \in \mathcal{x}\) is the sequence
   of symbols, \(\mathbf{y} = y_1, y_2,    , y_n \in \mathcal{y}\) is the
   corresponding sequence of labels, \(\mathbf{f}(\mathbf{x},\mathbf{y})\)
   is a feature representation of \(\mathbf{x}\) and \(\mathbf{y}\), and
   \(z(\mathbf{x};\mathbf{w}) = \sum\limits_{\mathbf{y}    \in \mathcal{y}}
   \text{exp}(\mathbf{w} \cdot \mathbf{f}(\mathbf{x},\mathbf{y}   )) \) is
   also referred to as the partition function.

   two approaches that can be used to estimate the model parameters \(w\)
   are:
    1. maximum id178 markov models (memms), which assume that
       \(\text{pr}(\mathbf{y} \: | \: \mathbf{x}; \mathbf{w})\)
       decomposes, i.e. that we can express it as a product of the
       individual label probabilities that only depend on the previous
       label (similar to id48s).
    2. id49 (crfs), which make a weaker assumption by
       only assuming that \(\mathbf{f}(\mathbf{x},\mathbf{y})\)
       decomposes.

   in memms, we assume     similarly to markov models     that the label
   \(y_i\) at the \(i\) th position does not depend on all past labels,
   but only on the previous label \(y_{i-1}\). in contrast to markov
   models, memms allow us to condition the label \(y_i\) on the entire
   symbol sequence \(x_{1:n}\). both assumptions combined lead to the
   following id203 of label \(y_i\) in memms:

   \(\text{pr}(y_i \: | \: x_{1:n}, y_{1:i-1}) = \text{pr}(y_i \: | \:
   x_{1:n}, y_{i-1})\)

   by this formulation, the objective of memms reduces sequence modelling
   to multi-class id28.

   in crfs, we factorize on label bigrams. instead of greedily predicting
   the most probable label \(y_i\) at every position \(i\), we instead aim
   to find the sequence of labels with the maximum id203:

   \(\underset{y \in \mathcal{y}}{\text{argmax}} \sum_i \mathbf{w} \cdot
   \mathbf{f}(\mathbf{x}, i, y_{i-1}, y_i)\)

   we then estimate the parameters \(w\) of our model using gradient-based
   methods where we can use forward-backward to compute the gradient.

crfs vs. memms

   by choosing between memms and crfs, we make the choice between local
   and global normalisation. while memms aim to predict the most probable
   label at every position, crfs aim to find the most probable label
   sequence. this, however, leads to the so-called    label bias problem    in
   memms: as memms choose the label with the highest id203, the
   model is biased towards more frequent labels, often irrespective of the
   local context.

   as memms reduce to multi-class classification, they are cheaper to
   train. on the other hand, crfs are more flexible and thus easier to
   extend to more complex structures.

   this distinction between local and global normalisation has been a
   recurring topic in sequence modelling and a key criterion when choosing
   an algorithm. for text generation tasks, global normalisation is still
   too expensive, however. many state-of-the-art approaches thus employ
   id125 as a compromise between local and global normalisation. in
   most sequence modelling tasks, local normalisation is very popular due
   to its ease of use, but might fall out of favour as more advanced
   models and implementations for global normalisation become available.
   to this effect, a recent outstanding paper at acl[26] (andor et al.,
   2016) shows that globally normalised models are strictly more
   expressive than locally normalised ones.

id48s vs. crfs

   another distinction that is worth investigating is the difference
   between generative and discriminative models: id48s are generative
   models, while crfs are discriminative. id48s only take into account the
   previous word as its features are tied to the generative process. in
   contrast, crf features are very flexible. they can look at the whole
   input \(x\) paired with a label bigram \((y_i , y_{i+1})\). in
   practice, for prediction tasks, such    good    discriminative features can
   improve accuracy a lot.

   regarding the parameter estimation, the distinction between generative
   and discriminative becomes apparent: id48s focus on explaining the data,
   both \(x\) and \(y\), while crfs focus on the mapping from \(x\) to
   \(y\). which model is more appropriate depends on the task: crfs are
   commonly used in tasks such as id52 and ner, while id48s have
   traditionally lain at the heart of id103.

id170 in nlp with imitation learning

   [27]andreas vlachos of the university of sheffield gave a talk on using
   imitation learning for id170 in nlp, which followed the
   same distinction between local normalisation (aka incremental
   modelling), i.e. greedily predicting one label at a time and global
   normalisation (aka joint modelling), i.e. scoring the complete outputs
   with a crf that we discussed above. andreas talked about how imitation
   learning can be used to improve incremental modelling as it allows to
   a) explore the search space, b) to address error-propagation, and c) to
   train with regard to the task-specific id168.

   there are many popular imitation learning algorithms in the literature
   such as[28] searn (daum   iii et al., 2009),[29] dagger (ross et al.
   2011), or[30] v-dagger (vlachos and clark, 2014). recently,[31] mixer
   (ranzato et al., 2016) has been proposed to directly optimise metrics
   for text generation, such as id7 or id8.

   an interesting perspective is that imitation learning can be seen as
   inverse id23: whereas we want to learn the best
   policy in id23, we know the optimal policy in
   imitation learning, i.e. the labels in the training data; we then infer
   the per-action reward function and learn a policy, i.e. a classifier
   that can generalise to unseen data.

demo day

   [demo_day.jpg] figure 4: aylien stand at demo day

   in the evening of the fourth day, we presented     along with other nlp
   companies and research labs     aylien at the lxmls demo day.

   we presented an overview of our research directions at aylien, as well
   as a 1d generative adversarial network [32]demo and visualization.

syntax and parsing

   having looked at generic models that are able to cope with sequences
   and more complex structures, we now briefly mention some of the
   techniques that are commonly used to deal with one of language   s unique
   characteristics: syntax. to this end, [33]slav petrov of google
   research gave an in-depth lecture about syntax and parsing on the fifth
   day of the summer school, which discussed, among others, successful
   parsers such as the charniak and the berkeley parser, context-free
   grammars and phrase-based parsing, projective and non-projective
   id33, as well as more recent transition and graph-based
   parsers.

   to tie this to what we   ve already discussed, figure 5 demonstrates how
   the distinction between generative and discriminative models applies to
   parsers.


   [generative_discriminative.png] figure 5: generative vs. discriminative
   parsing models

from dependencies to constituents

   in the evening of the fifth day,[34] andr   martins of unbabel gave talk
   a on an[35] acl 2015 paper of his, in which he shows that constituent
   parsing can be reduced to id33 to get the best of both
   worlds: the informativeness of constituent parser output and the speed
   of dependency parsers.

   their approach works for any out-of-the-box dependency parser, is
   competitive for english and morphologically rich languages, and
   achieves results above the state of the art for discontinuous parsing
   (where edges are allowed to intersect).

deep learning

   finally, the two last days were dedicated to deep learning and featured
   prolific researchers from academia and industry labs as speakers. on
   the morning of the sixth day,[36] wang ling of google deepmind gave one
   of the most gentle, family-friendly intro to deep learning i   ve seen    
   titled deep neural networks are our friends with a theme inspired by
   the muppets.

   the evening talk by[37] oriol vinyals of google deepmind detailed some
   of his lessons learned when working on sequence-to-sequence models at
   google and gave glimpses of interesting future challenges, among them,
   id62 for nlp[38] (vinyals et al., 2016) and enabling
   neural networks to ponder decisions[39] (graves, 2016).

   for the lecture on the last day,[40] chris dyer of cmu and google
   deepmind discussed modelling sequential data with recurrent neural
   networks (id56s) and shared some insights and intuitions with regard to
   working with id56s and lstms.

exploding / vanishing gradients

   if you   ve worked with id56s before, then you   re most likely familiar
   with the exploding/vanishing gradients problem: as the length of the
   sequence increases, computations of the model get amplified, which
   leads to either exploding or vanishing gradients and thus renders the
   model incapable to learn. the intuition why advanced models such as
   lstms and grus mitigate this problem is that they use summations
   instead of multiplications (which lead to exponential growth).

deep lstms

   [deep_lstms.png] figure 6: deep lstms

   deep or stacked lstms are by now a very common sight in the literature
   and state-of-the-art for many sequence modelling problems. still,
   descriptions of implementations often omit details, which might be
   perceived as self-evident. this, however, means that it is not always
   clear how a model looks like exactly or how it differs from similar
   architectures. the same applies to deep lstms. the most standard
   convention feeds the input not only to the first but (via skip
   connections) also to subsequent layers as in figure 6. additionally,
   dropout is generally applied only between layers and not on the
   recurrent connections as this would drop out more and more value over
   time.


   [dropout_and_deep_lstms.png] figure 7: dropout in deep lstms

does depth matter?

   generally, depth helps. however, in comparison to other applications
   such as audio/visual processing, depth plays a less significant role in
   nlp. hypotheses for this observation are: a) more transformation is
   required for speech processing, image recognition etc. than for common
   text applications; b) less effort has been made to find good
   architectures (id56s are expensive to train; have been widely used for
   less long); c) id26 through time and depth is hard and we
   need better optimisers.

   generally, 2-8 layers are standard across text applications. input skip
   connections are used often but by no means universally.

   only recently have also very deep architectures been proposed for
   nlp[41] (conneau et al., 2016).

mini-batching

   mini-batching is generally necessary to make use of optimised
   matrix-id127. in practice, however, this usually
   requires bucketing training instances based on similar lengths and
   padding with \(0\)   s, which can be a nuisance. because of this, this is
       according to chris dyer        the era of assembly language programming
   for neural networks. make the future an easier place to program!   

character-based models

   character-based models have gained more popularity recently and for
   some tasks such as language modelling, using character-based lstms
   blows the results of word-based models out of the water, achieving a
   significantly lower perplexity with a lot fewer parameters particularly
   for morphologically rich languages.


   [char_language_modeling.png] figure 8: charlstm > word lookup

attention

   finally, no overview of recurrent neural networks is complete without
   the mention of attention, one of the most influential, recently
   proposed notions with regard to lstms. attention is closely related to
      pooling    operations in convolutional neural networks (and other
   architectures) as it also allows to selectively focus on particular
   elements of the input. the most popular attention architecture
   pioneered by bahdanau et al.[42] (2015) seems to only care about
      content    in that it relies on computing the dot product, i.e. the
   cosine similarity between vectors. it contains no obvious bias in favor
   of diagonals, short jumps, fertility, or other structures that might
   guide actual attention from a psycho-linguistic perspective. some work
   has begun to add other    structural    biases ([43]luong et al., 2015;[44]
   cohn et al., 2016), but there are many more opportunities for research.

   attention is similar to alignment, but there are important differences:
   a) alignment makes stochastic but hard decisions. even if the alignment
   id203 distribution is    flat   , the model picks one word or phrase
   at a time; b) in contrast, attention is    soft    (all words are
   interpolated based on their attention weights). finally, there is a big
   difference between    flat    and    peaked    attention weights.

memory networks for language understanding

   [45]antoine bordes of facebook ai research gave the last talk of the
   summer school, in which he discussed facebook ai research   s two main
   research directions:

   on the one hand, they are working on (key-value) memory networks, which
   can be used to jointly model symbolic and continuous systems. they can
   be trained end-to-end through back propagation and with sgd and provide
   great flexibility on how to design memories.

   on the other hand, they are working on new tools for developing
   learning algorithms. they have created[46] several datasets of
   reasonable sizes, such as babi, cbt, and movieqa that are designed to
   ease interpretation of a model   s capabilities and to foster research.


   [group_picture.jpg] figure 9: lxmls 2016 group picture

   that was the lisbon machine learning summer school 2016! we had a blast
   and hope to be back next year!

slides

   [47]sequence models (noah smith)

   [48]machine translation as sequence modelling (philipp koehn)

   [49]learning structured predictors (xavier carreras)

   [50]id170 in nlp with imitation learning (andreas
   vlachos)

   syntax and parsing    [51] i,[52] ii (slav petrov)

   [53]turbo parser redux: from dependencies to constituents (andr  
   martins)

   [54]deep neural networks are our friends (wang ling)

   [55]modeling sequential data with recurrent networks (chris dyer)

   [56]memory networks for language understanding (antoine bordes)

                       [57]text analysis api - sign up

   stay informed
   subscribe to our newsletter to recieve regular updates about aylien,
   direct to your inbox.
   [58]sebastian ruder
   research scientist @ aylien
   sebastian is a phd student in natural language processing at the
   insight research centre for data analytics and a research scientist at
   aylien. previously, he worked with microsoft, ibm extreme blue, and
   google summer of code. his main research interest lies in using deep
   learning for id20 in nlp. in his spare time, he loves to
   travel, read, blog, and learn new languages.
   twitter: @seb_ruder
   read next
   [59]noel bambrick
   [60]getting started with the news api part 2: insights
   18 aug, 2016
   getting started with the news api part 2: insights
   related articles
   why text analytics is essential in the ever changing publishing
   industry
   [61]mike waldron
   16 sep, 2014
   [62]why text analytics is essential in the ever changing publishing
   industry

   the publishing industry has changed dramatically. mainstream newspapers
   and magazines have given way to desktop publishing and the internet as
   economics have changed the game. let   s look at the main [   ]
   [63]read    
   text analysis 101; a basic understanding for business users: document
   classification with id91
   [64]mike waldron
   9 jan, 2015
   [65]text analysis 101; a basic understanding for business users:
   document classification with id91

   introduction this is our second blog on harnessing machine learning
   (ml) in the form of natural language processing (nlp) for the automatic
   classification of documents. by classifying text, we aim [   ]
   [66]read    
   meet our new advisors, barry and jimi
   [67]parsa ghaffari
   1 sep, 2016
   [68]meet our new advisors, barry and jimi

   yesterday i was talking to a friend at a starbucks by the river liffey,
   and i explained to him how i, as a solo founder, approach taking advice
   from my [   ]
   [69]read    
   semantic advertising and text analysis gives more targeted ad campaigns
   [70]mike waldron
   23 sep, 2014
   [71]semantic advertising and text analysis gives more targeted ad
   campaigns

   the internet has had a massive impact on marketing and advertising in
   general. it has provided an effective way for businesses to access
   target prospects with branded and targeted marketing [   ]
   [72]read    
   let's talk

   products

   [73]text analysis api [74]news api [75]text analysis platform
   company

   [76]about us [77]in the news [78]jobs
   content

   [79]resources [80]blog
   get in touch

   [81]contact sales [82]press [83]email us
   stay informed

   [84]privacy policy [85]terms and conditions

   copyright    2018 aylien ltd. all rights reserved.

references

   visible links
   1. http://blog.aylien.com/lisbon-machine-learning-summer-school-highlights/feed/
   2. http://blog.aylien.com/wp-json/oembed/1.0/embed?url=http://blog.aylien.com/lisbon-machine-learning-summer-school-highlights/
   3. http://blog.aylien.com/wp-json/oembed/1.0/embed?url=http://blog.aylien.com/lisbon-machine-learning-summer-school-highlights/&format=xml
   4. https://aylien.com/products
   5. https://aylien.com/research
   6. http://blog.aylien.com/
   7. https://aylien.com/about
   8. https://aylien.com/contact
   9. http://blog.aylien.com/
  10. http://blog.aylien.com/category/general/
  11. http://blog.aylien.com/category/product/
  12. http://blog.aylien.com/category/data-science/
  13. http://blog.aylien.com/category/research/
  14. https://aylien.com/products/
  15. https://aylien.com/products
  16. https://aylien.com/research
  17. http://blog.aylien.com/
  18. https://aylien.com/about
  19. https://aylien.com/contact
  20. http://blog.aylien.com/author/sebastian/
  21. http://lxmls.it.pt/2016/
  22. http://lxmls.it.pt/2016/?page_id=64
  23. http://homes.cs.washington.edu/~nasmith/
  24. http://lxmls.it.pt/2016/talk.pdf
  25. https://www.cs.upc.edu/~carreras/
  26. https://arxiv.org/abs/1603.06042
  27. http://andreasvlachos.github.io/
  28. http://www.umiacs.umd.edu/~hal/docs/daume09searn.pdf
  29. https://www.ri.cmu.edu/pub_files/2011/4/ross-aistats11-noregret.pdf
  30. http://www.aclweb.org/anthology/q14-1042
  31. http://www.aclweb.org/anthology/q14-1042
  32. http://notebooks.aylien.com/research/gan/gan_simple.html
  33. http://research.google.com/pubs/author38945.html
  34. https://www.cs.cmu.edu/~afm/home.html
  35. https://www.cs.cmu.edu/~afm/home_files/acl2015_reduction.pdf
  36. http://www.cs.cmu.edu/~lingwang/
  37. http://research.google.com/pubs/oriolvinyals.html
  38. http://arxiv.org/abs/1606.04080
  39. https://arxiv.org/abs/1603.08983
  40. http://www.cs.cmu.edu/~cdyer/
  41. http://arxiv.org/abs/1606.01781
  42. https://arxiv.org/abs/1409.0473
  43. http://arxiv.org/abs/1508.04025
  44. https://arxiv.org/abs/1601.01085
  45. https://research.facebook.com/antoine-bordes
  46. https://research.facebook.com/research/babi/
  47. http://lxmls.it.pt/2016/sequencemodels.smith.7-23-16.pdf
  48. http://lxmls.it.pt/2016/talk.pdf
  49. http://lxmls.it.pt/2016/strlearn.pdf
  50. http://andreasvlachos.github.io/assets/lectures_reveal_js/lxmls22july2016/imitationlearningtutorial.html#/
  51. http://lxmls.it.pt/2016/part_1_constituency_parsing_2016.pdf
  52. http://lxmls.it.pt/2016/part2_dependency_parsing_2016.pdf
  53. http://lxmls.it.pt/2016/lxmls2016_slides.pdf
  54. http://lxmls.it.pt/2016/deep-neural-networks-are-our-friends.pdf
  55. http://lxmls.it.pt/2016/lxmls-dl2.pdf
  56. http://lxmls.it.pt/2016/abordes-lxmlss.pdf
  57. http://cta-redirect.hubspot.com/cta/redirect/1942801/02bc9816-b470-4328-9625-fad8d92a8811
  58. http://blog.aylien.com/author/sebastian/
  59. http://blog.aylien.com/author/noel/
  60. http://blog.aylien.com/getting-started-news-api-part-2-insights/
  61. http://blog.aylien.com/author/mike/
  62. http://blog.aylien.com/why-text-analytics-is-essential-in-the-ever/
  63. http://blog.aylien.com/why-text-analytics-is-essential-in-the-ever/
  64. http://blog.aylien.com/author/mike/
  65. http://blog.aylien.com/text-analysis-101-a-basic-understanding-for-2/
  66. http://blog.aylien.com/text-analysis-101-a-basic-understanding-for-2/
  67. http://blog.aylien.com/author/parsa/
  68. http://blog.aylien.com/meet-new-advisors-barry-jimi/
  69. http://blog.aylien.com/meet-new-advisors-barry-jimi/
  70. http://blog.aylien.com/author/mike/
  71. http://blog.aylien.com/semantic-advertising-and-text-analysis-gives-more/
  72. http://blog.aylien.com/semantic-advertising-and-text-analysis-gives-more/
  73. https://aylien.com/text-api/
  74. https://aylien.com/news-api/
  75. https://aylien.com/text-analysis-platform/
  76. https://aylien.com/about/
  77. https://aylien.com/in-the-news/
  78. https://aylien.com/jobs/
  79. https://aylien.com/resources/
  80. http://blog.aylien.com/
  81. https://aylien.com/contact/
  82. https://aylien.com/in-the-news/
  83. https://aylien.com/contact/
  84. https://aylien.com/privacy/
  85. https://aylien.com/terms/

   hidden links:
  87. https://aylien.com/
  88. http://blog.aylien.com/?s=
  89. http://blog.aylien.com/?s=
  90. http://blog.aylien.com/category/product/
  91. http://blog.aylien.com/category/data-science/
  92. http://blog.aylien.com/category/research/
  93. http://blog.aylien.com/category/general/
  94. http://blog.aylien.com/lisbon-machine-learning-summer-school-highlights/sebastian@aylien.com
  95. https://twitter.com/_aylien
  96. https://www.linkedin.com/company-beta/1195911/
  97. https://www.facebook.com/aylieninc
