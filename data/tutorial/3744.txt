   #[1]         

[2]         
ml     design

     * [3]archive   
     *
     *
     *
     *
     *

generating large images from latent vectors

   april 1, 2016

        neural network dreaming of mnist digits at 1080p resolution.
                 generative model combining cppn w/ gan+vae.
                                  [4]github

introduction

   in some domains of digital generative art, an artist would typically
   not work with an image editor directly to create an artwork. typically,
   the artist would program a set of routines that would generate the
   actual images. these routines compose of instructions to tell the
   machine to draw lines and shapes at certain coordinates, and manipulate
   colours in some mathematically defined way. the final artwork, which
   may be presented as a pixellated image, or printed out on physical
   medium, can be entirely captured and defined by a set of mathematical
   routines.

   many natural images have interesting mathematical properties. simple
   math functions have been written to generate natural fractal-like
   patterns such as tree branches and snowflakes. like fractals, a simple
   set of mathematical rules can sometimes generate a highly complicated
   image that can be zoomed-in or zoomed-out indefinitely.

   imagine if you can take any image, and rather than storing that image
   as a set of pixels, you try to figure out a math function to
   approximate that picture, ie:

   once such a function is found, then the image can be automatically
   scaled up and down, or stretched around, by just scaling the inputs. if
   this function has some fun properties or exhibit some internal
   structure, it will be interesting to see what the image looks like if
   we blow up the image to a very high resolution much bigger than the
   original image.

   this function can also be defined as a neural network, with arbitrary
   architectures. these networks, which some call [5]compositional pattern
   producing networks are a way to represent an entire image as a
   function. since neural networks are universal function approximators,
   given a large enough network, any image of finite resolution can be
   represented using this method.
   [panda_470_470.jpeg] [panda_200_200.png]

   train a neural net to draw an image with [6]karpathy   s convnet.js demo.

   i think the style generated by neural nets of various architectures
   look [7]really nice, so i wanted to explore whether this type of
   generative network can be used to generate an entire class of images,
   not just a single image, and see if i can use this method like the way
   [8]recent research work used neural nets to generate pixellated images
   of a certain [9]class.

   in this post i will describe my experience of using cppns to generate
   high resolution images of handwritten digits, by training it on mnist
   (28x28px), as a starting point. in the future i may try to use this
   method on more complicated image sets.

   our cppn will generate a high resolution mnist image from random latent
                                  vector z

                           [generator_example.png]

background

   in the [10]previous post, we have explored the use of cppns to produce
   high resolution images containing some interesting random patterns.
   since the input to the cppn consist of the coordinates of a certain
   pixel, and the output is the colour for that coordinate, cppns can
   generate images of arbitrary resolution, limited by the machine   s
   memory. this feature gives cppns some fractal-like characteristics,
   because you can just zoom-in, or zoom-out of an image as much as you
   want, by just adjusting a set of scaled input coordinates of the
   desired view of the image. we also find that by randomising the weights
   of the cppn, we see that we can generate many abstract patterns that
   may look aesthetically pleasing to some people. also, if we fix the
   neural network architecture, and fix set of random weights, we can
   explore the space of images that the cppn can produce by varying around
   the addition latent vector input into the network.

                          [latent_space_cppn.jpeg]
     imaged generated with an untrained network initialised with random
                                  weights.
               this same network will train on mnist dataset.

   i have used cppn [11]implementations before to generate many weird
   images, and i am constantly surprised at the wide range of pictures
   this method can produce. in addition to randomly generating
   [12]abstract art patterns, this approach has also been used for
   [13]genetic art [14]production. in previous [15]projects, where the art
   is slowly genetically evolved, it has been observed that to produce the
      best    art is to abandon the objective of actually creating a
   particular thing. for example, if an artist wants to use cppn-neat to
   generate a picture of a cat, she would most likely not end up with
   anything that resembles a cat. but if the artist goes about choosing
   patterns that she thinks look interesting, and mix them up to produce
   the next generation of images, she might end up with something that
   looks even more interesting. ken stanley has highlighted this
   phenomenon in his work on [16]novelty search, which i found to be a
   very fascinating approach to look at the ai research field, and also
   how to approach life in general.

   however, i do think it is possible to get cppns to generate specific
   desired images. given a large enough network, rather than a dinky
   little neat network, we can approximate anything, and even
   [17]karpathy   s simple js drawing demo proves that this approach can
   draw any image given a large enough network and enough training time.

   the more interesting task though, is rather than to generate a specific
   image by overfitting the network   s weights to match some target image,
   is whether this sort of network can generate new images of concepts.
   for example, we want to be able to have the network generate a random
   picture of a cat, and be able to slowly morph that cat into a dog. that
   way, the network is not really overfitting to some particular training
   picture, but understands internally the concept of a cat and a dog, to
   the point where it is able to imagine a new image that is between a cat
   and a dog. a few years ago i thought this would be considered science
   fiction, but we are actually getting there.

   recently, we have seen deep neural networks capable of generating
   images of humans, bathrooms, cat, and [18]anime characters. these
   approaches model the pixels in the images as observable random
   variables, x. unless the set of pictures is of something very trivial
   like a white wall, the joint id203 distribution of all the pixels
   inside x is a very complicated one that is unlikely to be modelled with
   simpler distributions understandable by mere humans. however, it is
   possible for a neural network to do the hard task of learning how to
   map this complicated distribution into a simple one that humans can
   understand and work with, such as a gaussian distribution. so the trick
   is to then model such complex observable random variables (all the
   pixels in an image) as a dependent variable, who   s value depends on a
   much smaller set of variables with a simpler id203 distribution,
   like a vector of a dozen unit normal gaussians. this vector is
   typically denoted as z, the latent vector.

   so the goal is to have a neural network learn figure out the
   id155 distribution p(x|z) to be able to generate a
   very complicated image x, from a very simple latent vector of real
   numbers, z. in addition, having a complimentary network to learning
   p(z|x) will also be very useful, to encode a complicated image to a
   latent vector.

   if you think back to your first course in statistics, this is related
   to principal component analysis (pca), where one can try to decompose a
   large set of observations into a small number of factors. and one can
   then vary those small set of factors to predict what will happen to the
   large set of observations. the only difference is while pca is based on
   id202 and assumes the data can be explained as a linear
   combination of smaller factors, this approach of using neural networks
   will be able decompose the large set of observations in a highly
   non-linear way, making it a lot more powerful.

   current cutting edge techniques of image generation from latent vector
   are generally based on [19]id3 (gan) or
   [20]id5 (vae) or a combination of these
   approaches, and i will describe them later in this post. to get more
   information into these methodologies, please read about [21]deep
   convolutional id3 (dcgan), which is a is
   well known state of the art technique in this area, and the [22]draw
   algorithm is a cutting edge extension of vae. my study of this area is
   based off of [23]carpedm20   s implementation of dcgan, and also [24]jan
   hendrik   s implementation of vae, both using tensorflow library. their
   code and explanations contributed greatly to my understanding. i   ve
   also studied this work on combining both approaches, and used some
   tricks to stabilise the training of gans.

   in the current literature, the training dataset is typically composed
   of small images (such as 32x32px or 64x64, though i have seen 128x128
   and 256x256 sets being used). to match with the training data, the
   generator network will have as many outputs as there are as many pixels
   from the training data. so typically, a network used to train on an
   image dataset with 64x64 pixels would output also 64x64 pixels
   directly. it is difficult for modern methods to generate images that
   have much higher resolution than 256x256, because the amount of memory
   required is likely to exceed the amount available on a modern gpu card.

   in this post, we will use cppn to generate large images from smaller
   images from the mnist training set. because cppn   s can generate images
   of arbitrarily large resolution, i thought it would be neat to try to
   train a cppn to generate images, in the same way that gan and vae
   approaches have been used, and just replace the generator network that
   generates all the output pixels directly, with an indirect way of
   generating the pixels via a cppn generator. for training, we can just
   set the output resolution of the cppn to be the same as the input.
   after the training, we can increase the resolution of the output image
   to see how the cppn    fills in the gaps    with it   s own internal style
   defined by the network. by setting the training resolution of the cppn
   to be the same size of the training data, the entire training process
   can fit inside a modern gpu board.

   due to the indirect-coding nature of of the cppn   s architecture, the
   space of possible images that can be generated will be quite limited,
   compared to a network that outputs all the pixels directly, so from
   that standpoint, training a cppn proved more challenging, and took a
   lot more time than training a vae or gan model on the simple mnist
   dataset. after we obtained some satisfactory results with the cppn
   model combined with gan and vae on a simple dataset such as mnist, we
   can think about testing this algorithm on more complicated image sets
   of real, coloured objects in the future.

generative adversarial modelling

   as discussed in the background, the challenge for image generation, is
   to be able to model the joint id203 distribution of the pixels in
   the image. once we have a realistic id203 distribution of all the
   pixels, we can just sample from that id203 distribution to
   generate a realistic image. however, because real images are very
   complicated and the id203 distribution too nasty for humans to
   model, the trick is to use a neural network to transform a much simpler
   id203 distribution that humans can understand (like unit gaussian
   random variables) into a distribution that resembles the true
   id203 distribution for normal images in our training set. the
   complexity of the generated id203 distribution will be limited by
   the complexity of the neural network   s architecture. in our case, the
   neural network that will generate an image will be a cppn, like the one
   below:

   architecture of the generator network:
   [generator.svg]

   this is the same sort of network used in the previous post. the input
   vector z is a vector of 32 real numbers that are drawn from unit
   gaussian random number generator, and all of them will be independent.
   the vectors x, y, and r are all the possible coordinates we want to
   compute the pixel intensity for in our image, so for an image size of
   (26x26), we will need to calculate a total of 676 pixel intensities, to
   form (32+676+676+676) inputs into the network. the important point to
   note is that each pixel intensity will be calculated from the exact
   same network with the same set of weights, so theoretically we can also
   feed into our network (32+1+1+1) inputs and get 1 pixel, and do this
   676 times, without moving the weights, if we are constrained by memory.

   as we have seen earlier, by keeping the weights of the network
   constant, and by controlling the input z vector, we can get a rich set
   of output images using this method. the goal here is to train our
   network in a such a way that for any random z vector we put in, the
   output would look like an image from our mnist training set, and we
   would not really be able to tell them apart. if we are able to
   successfully do this, then we would have used the cppn to model the
   id203 distribution of mnist images, and we can draw random images
   out the way we draw simple iid unit gaussian variables.

   but how on earth can we train the weights of this network to convert a
   bunch of unit gaussian random numbers into a random mnist image?
   someone living in the year 2013 will think this is science fiction. but
   if you were not cutoff from the internet and stuck in a cave, you would
   have probably heard of the [25]generative adversarial network framework
   in the last few years. the concept behind gans is introduce a
   discriminator (d) network to compliment the generator (g) network
   above.

   architecture of the discriminator network:
   [discriminator.svg]

   the job of the d network is to be able to tell whether or not an image
   belongs to the training set. so it will be used to detect fraudulent
   pictures of mnist digits that are not legit and authentic mnist digits.
   the job of the g network is to naturally attempt to generate an image
   of a random mnist digit that can fool d to think that it is the real
   deal. if d is very good at its job, and can discriminate images at the
   level of human level performance, and if g is able to still fool d,
   then g would be able to generate a fake mnist image that can fool
   humans, and our job is done.

   i followed a similar approach used in dcgan, and used three simple
   convolutional network layers. convnets have proven to be great at image
   classification, and since the output of d is binary in this case
   (real/fake), this is an even simpler classification problem than digit
   classification.

   the input to the d network is an image. the output of the d network, y,
   is a real number between zero and one. if y is close to one, then the d
   network strongly believes that the input image must be a legitimate
   mnist digit, and if y is close to zero, then the d network strongly
   believes the image is not an mnist digit, but rather a fraudulent
   attempt to fool and undermine its intelligence. if the output of d is
   close to 0.5, then the network is confused and sad, having loss its
   confidence in its ability to function as a normal neural network.

   we can define these performance measures, or cost functions, to
   evaluate the performance of the generator network:

   you can see that if the generator network is doing a really bad job,
   then the output value y from the discriminator will be a very small
   number close to zero, and the negative of the log of a very small
   number will be a very large positive number.

   if the generator is doing a fantastic job, and kicking d   s ass, then
   the output value y will be a number close to one, and the negative of
   the log of a number approaching 1 (like 0.999) will be a number close
   to zero.

   if the generator creates an image that causes d to be confused and
   can   t tell the difference, the output y will be a number close to 0.5,
   then will be a value close to -log 0.5 or around 0.69

   likewise, we can evaluate the performance of the discriminator in a
   similar way:

   if the discriminator is doing a great job, both and will be a number
   close to zero. conversely, if the discriminator is doing a poor job,
   both and will be a very large number. if the discriminator is confused,
   both and will be a number close to , so the d network will have trouble
   identifying both real and fraudulent examples. we define the loss
   function to be the average of both losses.

   given the id168s above, it is then fairly straight forward to
   train the d network using backprop to learn to distinguish a real mnist
   digit from a fake one, by adjusting the weights in the direction of the
   gradient that will make smaller after a batch of real examples, and
   fake examples freshly generated by g.

   the generator network can also be trained via backprop as well by
   adjusting the weights in the direction of the gradient that will make
   smaller (thereby also making larger).

   note that training g is much more involved, because during back
   propagation, the gradients has to flow through the layers of the d
   network first, into each pixel that g has generated (so will be first
   calculated). afterwards, the gradient of each loss derivative with
   respect to each pixel will then flow backwards into each weight inside
   the g network (). and because we are using a cppn algorithm to generate
   the image, every pixel is generated by the exact same network with
   identical weights, just with different coordinates as the inputs. since
   the weights are shared, the gradients flowing back from the pixel level
   to the weights will be accumulated to compute . this means if we are
   doing minibatch training for a set of images, training cppns means we
   will involve working with a batch within a batch, and it took a while
   for me to figure out how to implement this correctly in tensorflow, as
   many default shorthands to ignore batch processing wouldn   t work
   anymore.

training both discriminator and generator networks

   at the beginning, both networks   s initial weights are randomised, so
   both are totally stupid and don   t know what they are supposed to do.
   the key is to make them train together and get better gradually over
   time.

   architecture of the gan setup:
   [generator.svg]
   [discriminator.svg]

   as outlined in the [26]gan paper, we would take turns training g and d
   for each minibatch, so over time, both would get incrementally better
   at beating each other, and through this competitive process, both
   networks would be better at what they are supposed to do as a
   standalone network. ideally, we want both and to hover around (so that
   the output is ). if the d network is a lot stronger than the g network,
   we would find that would stay near zero, and would stay at a very high
   number during the training, since any incremental improvement in g
   would be quickly outpaced by any subsequent improvement in d.

   in practice, this is what tends to happen. d is a simple classification
   network and it is generally very easy to train a good classifier,
   especially if we use a few layers of convnets. it is easier to tell
   that something is real or fake, than to actually create a fake thing
   that looks real.

   in real life, anyone can tell apart a real masterpiece from a fake
   piece of art created by an amateur. it takes a professional art forger
   with years of experience to be able to create a fraudulent painting
   that looks like the real thing. art forgery can be an extremely
   [27]lucrative profession partly because it is so difficult.

   like a real counterfeit painter, g has a much tougher job compared to
   d. creating content is hard, and even with the error gradients advising
   g how to defeat d, there   s no real guarantee that g can do a much
   better job at fooling d. sometimes the gradient can just be stuck at
   zero, when d has gotten good enough at winning that any small change in
   g   s strategy will not beat d.

   one of the most important points of training a gan network is to not
   let the discriminator network become that much better than the
   generator, so a lot of thought should be placed on structuring the
   architecture, and size of both networks so that they are fair. for
   example, the learning rate for g   s weights are larger than the learning
   rate for d   s weights.

   in addition to the consideration for d   s network architecture, the
   number of activations for network d should be set to a low enough level
   so that it won   t improve to the point where g has absolutely no chance
   to trick d. at the same time though, d should have enough neurons so
   that its general performance should be comparable to a human doing the
   classification herself, otherwise even if g can outsmart d, the
   pictures it generate will not fool the human viewer. therefore, at the
   end of the day, network g must have enough capacity to generate a large
   space of possible images. if g can only draw a simple circle, there   s
   no way it can fool any reasonable network with any amount of
   optimisation used to fine tune its weights. you can   t beat a dead
   horse.

   there are some more tricks to slowdown d   s training so g can have a
   good chance to always catch up to d. this includes running gradient
   descent learning on g, n times for every time id119 learning
   is run on d, during every batch. we have experimented with setting n
   between 4 and 8 times.

   another trick is to calculate d   s id168 first, and only perform
   id119 on d if g   s id168 is less than some upper
   bound (so it is relatively not that weak against d in the first place),
   and also if d   s id168 is greater than some lower bound (so that
   it is not relatively that strong versus g). we have tried to use an
   upper bound of 0.80 and a lower bound of 0.45.

   i have read about some of these tricks on this great [28]post on
   training gans, and also about how to combine gans and vaes that i will
   also discuss later. after these tricks are used, and fine tuning the
   architectures of both networks, we should see both id168s of d
   and g hover around during batch training, so one network doesn   t become
   that much stronger than the other, while both incrementally getting
   better at their respective tasks.

   gan algorithm for 1 epoch of training:
 1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
 randomise sample images in training set
for each sample image in training set
  y_real = discriminator(sample image, w_d)

  for i: 1 .. n_handicap
    z = normal(mean = 0, stdev = 1, size=(1, n_z))
    image = generator(z, w_g)
    y_fake = discriminator(image, w_d)

    calculate g_loss
    calculate dg_loss/dw_g gradients via backprop
    #i.e., the direction to make d crappier
    adjust w_g's using sgd-type optimising step

  calculate d_loss

  if g_loss < th_high and d_loss > th_low
    calculate dd_loss/dw_d via backprop
    #i.e., the direction that would make d better
    adjust w_d's using sgd-type optimising step


   in our settings, n_handicap had been set to values between 4 and 8.
   th_high and th_low had been set to 0.80 and 0.45 respectively. the
   architecture and specification of the networks are similar to the setup
   in the reference github code.

   the mnist training data had resolutions of 28x28 pixels. we have
   cropped them to 26x26, and randomly sampled between coordinates (0, 0)
   -> (25, 25) to (2, 2) -> (27, 27) so we have four times more variety in
   our training samples. this might not benefit the current version of the
   gan algorithm, as the d network is a convnet, but in the next few
   sections, when we merge gan and vae, it will offer some benefits to the
   vae side later on.

   samples of generative network at 26x26 output resolution:

              [29][cppgan_5_small.png] [30][cppgan_0_small.png]
   [31][cppgan_9_small.png]

   in the figure above are are a few examples generated from the g network
   after a few epochs of training using this gan method. the images
   resemble mnist samples at 26x26 resolution. we get images that sort of
   resemble zero, five and nine digits.

   now let   s see what happens when we use the cppn network to blow up the
   images using the same latent vectors to generate a 1300x1300 resolution
   image.

   same samples at much higher resolution:

                             [32][cppgan_0.jpeg]

                   [33][cppgan_5.jpeg] [34][cppgan_9.jpeg]

   now we   re talking. i think these look kind of cool.

   the much larger image that resembles zero looks a bit different than
   the other two larger images, and the reason was because the network
   that produced the image of zero was trained with a higher n_handicap
   value. maybe this forces the generator network to overfit a bit during
   each batch and consequently the values of are pushed to levels that are
   a bit more extreme than they would otherwise have been if n_handicap
   were set lower. i kind of like this extra bit of punchiness as it adds
   character to the image when it gets blown up.

   however, i find that in the end, the space of images generated from
   cppn networks trained with this gan method often get stuck and confined
   to two or three digits only. it was difficult to train a network that
   can generate all ten numerical digits. i think this is a weakness in
   the algorithm itself. there is nothing stopping g from finding its
   niche at generating a specific digit only, and be very good at
   generating that digit, so d still gets confused even if g only
   generates zeros.

   we need to find a way to incorporate in our algorithm a penalty
   function that would penalise g if it couldn   t generate all the diverse
   examples, and in the next section i will talk about the variational
   encoder, which is a candidate to perform this penalty function. we will
   also merge the variational encoder with the generative adversarial
   encoder, so that our network can generate mnist digits that look real
   to actual mnist images, are diverse enough to cover all ten digits, and
   be able to blow up the images to very high resolutions.

combining variational encoder into the model

   to deal with the problem of generating a diverse set of examples, i
   combined a [35]variation autoencoder (vae) to our network. i am not
   going through the details of explaining vae   s here, as there have been
   some [36]great [37]posts about them, and a very nice tensorflow
   [38]implementation.

   vae   s help use do two things. firstly, they allow us to encode existing
   image into a much smaller latent z vector, kind of like compression. it
   does this by passing an image through the encoder network, which we
   will call the q network, with weights . and from this encoded latent
   vector z, the generator network will produce an image that will be as
   close as possible to the original image passed in, hence it is an
   autoencoder system. this solves the problem we had in the gan model,
   because if the generator only produces certain digits, but not other
   digits, it will get penalised as it is not reproducing many examples in
   the training set.

   so far, we have assumed the z vector to be simple independent unit
   gaussian variables. there   s no guarantee that the encoder network q
   will encode images from a random training image x, to produce values of
   z that belong to a id203 distribution we can reproduce and draw
   randomly from, like a gaussian. imagine we just stop here, and train
   this autoencoder as it is. we will lack the ability to generate random
   images, because we lack the ability to draw z from a random
   distribution. if we draw z from the gaussian distribution, it will only
   be by chance that z will look like some value corresponding to the
   training set, and will produce images that do not look like the image
   set otherwise.

   the ability to control the exact distribution of z is the second thing
   the vae will help us do, and the main why the [39]vae paper is such an
   important and influential paper. in addition to perform the
   autoencoding function, the latent z variables generated from the q
   network will also have the characteristic of being simple independent
   unit gaussian random variables. in other words, if x is a random image
   from our training set, belonging to whatever weird and complicated
   id203 distribution, the q network will make sure the z is
   constructed in a way so that p(z|x) is a simple set of independent unit
   gaussian random variables. and the amazing thing is, this difference
   between the distribution of p(z|x) and the distribution of a gaussian
   distribution (they call this the kl divergence) can be quantified and
   minimised using id119 using some elegant mathematical
   machinery, by injecting gaussian noise into the output layer of the q
   network. this vae model can be trained by minimising the sum of both
   the reconstruction error and kl divergence error using gradient
   descent, in equation 10 of the [40]vae paper.

   our final cppn model combined with gan + vae:
   [encoder.svg]
      
   [generator.svg]
      
   [discriminator.svg]

   rather than sticking with the pure vae model, i wanted to combine vae
   with gan, because i found that if i stuck with only vae, the images it
   generated in the end looked very blurry and uninteresting when we blow
   up the image. i think this is due to the error term being calculated
   off pixel errors, and this is a [41]known problem for the vae model.
   nonetheless, it is still useful for our cause, and if we are able to
   combine it with gan, we may be able to train a model that will be able
   to reproduce every digit, and look more realistic with the
   discriminator network acting as a final filter.

   training this combined model will require some tweaks to our existing
   algorithm, because we will also need to train to optimise the vae   s
   error. note that we will adjust both and when optimising for both
   g_loss and vae_loss.

   cppn+gan+vae algorithm for 1 epoch of training:
 1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
 randomise sample images in training set
for each sample image in training set
  y_real = discriminator(sample image, w_d)

  # q network optimisation
  for i: 1 .. 4
    z_vae_noise = normal(mean = 0, stdev = 1, size=(1, n_z))
    z = encoder(sample image, z_vae_noise)
    image = generator(z, w_g)

    calculate vae_loss = reconstruction loss + kl divergence loss
    calculate dvae_loss/dw_q and dvae_loss/dw_g gradients via backprop
    adjust w_q's and w_g's using sgd-type optimising step

  # d network optimisation
  for i: 1 .. 4
    z_vae_noise = normal(mean = 0, stdev = 1, size=(1, n_z))
    z = encoder(sample image, z_vae_noise)
    image = generator(z, w_g)
    y_fake = discriminator(image, w_d)

    calculate g_loss
    calculate dg_loss/dw_g and dg_loss/dw_q gradients via backprop
    #i.e., the direction that would make d crappier
    adjust w_g's w_q's using sgd-type optimising step

  # optimise d network only if it is not that far ahead.
  calculate d_loss
  if g_loss < th_high and d_loss > th_low
    calculate dd_loss/dw_d via backprop
    #i.e., the direction that would make d better
    adjust w_d's using sgd-type optimising step


   the trick here, is to structure and balance all the subnetworks   
   structure, so that g_loss and d_loss hovers around 0.69, so they are
   mutually trying to improve by fighting each other over time, and
   improve at the same rate. in addition, we should see the vae_loss
   decrease over time epoch by epoch, while the other two networks battle
   out each other. it is kind of a black art to train these things, and
   maintain balance. the vae is trying to walk across a plank connecting
   two speed boats (g and d) trying to outrace each other.

   after training the model, we can see the results of feeding random
   vectors of z, drawn from unit gaussian distribution, into our g
   network, and we can generate some random large images. let   s see what
   we end up with!

random latent vectors

   we can generate some random large samples from our trained model in
   ipython:
 1
2
3
4
5
 %run -i sampler.py
sampler = sampler()
z = sampler.generate_z() # vector of 32 random gaussian samples ~ n(0, 1)
x = sampler.generate(z)  # from z, generate an image x, which is a np.array
sampler.show_image(x)    # display this image interactively in ipython


          [42][random00.png] [43][random01.png] [44][random02.png]
          [45][random03.png] [46][random04.png] [47][random05.png]
          [48][random06.png] [49][random07.png] [50][random08.png]

   we can see how our generator network takes in any random vector z,
   consisting of 32 real numbers, and generates a random image that sort
   of looks like a number digit based on the values of z.

   the next thing we want to try is to compare actual mnist examples to
   the autoencoded ones. that is, take a random mnist image, encode the
   image to a latent vector z, and then generate back the image. we will
   first generate the image with the same dimensions as the example
   (26x26), and then an image 50 times larger (1300x1300) to see the
   network imagine what mnist should look like were it much larger.

   first, we draw a random picture from mnist and display it.
 1
2
 m = sampler.get_random_mnist()  # get a random image from the mnist dataset
sampler.show_image(m)           # show the raw mnist sample in ipython


                           [51][mnist_input_0.png]

   then, we encode that picture into z.
 1
 z = sampler.encode(m)           # encode the picture to a latent vector


   from z, we generate a 26x26 reconstruction image.
 1
2
 x_26x26 = sampler.generate(z, x_dim = 26, y_dim = 26)
sampler.show_image(x_26x26)


                          [52][mnist_output_0.png]

   we can also generate much larger reconstruction image using the same z.
 1
2
 x_1300x1300 = sampler.generate(z, x_dim = 1300, y_dim = 1300)
sampler.show_image(x_1300x1300)


                           [53][mnist_dream_0.png]

   the current vae+gan structure seems to produce cloudy versions of mnist
   image when we scale them up, like trying to draw something from smoke.

   below are more comparisons of autoencoded examples versus the
   originals. sometimes the network makes mistakes, so it is not perfect.
   there is an example of a zero being misinterpreted as a six, and a
   three getting totally messed up. you can try to generate your own
   writing samples and feed an image into ipython to see what autoencoded
   examples get generated. maybe in the future i can make a javascript
   demo to do this.

autoencoded samples

   mnist sample (26x26)      cppn output (26x26)        cppn output (1300x1300)
   [54][mnist_input_1.png]   [55][mnist_output_1.png]
   [56][mnist_dream_1.png]
   [57][mnist_input_2.png]   [58][mnist_output_2.png]
   [59][mnist_dream_2.png]
   [60][mnist_input_3.png]   [61][mnist_output_3.png]
   [62][mnist_dream_3.png]
   [63][mnist_input_4.png]   [64][mnist_output_4.png]
   [65][mnist_dream_4.png]
   [66][mnist_input_5.png]   [67][mnist_output_5.png]
   [68][mnist_dream_5.png]
   [69][mnist_input_6.png]   [70][mnist_output_6.png]
   [71][mnist_dream_6.png]
   [72][mnist_input_7.png]   [73][mnist_output_7.png]
   [74][mnist_dream_7.png]
   [75][mnist_input_8.png]   [76][mnist_output_8.png]
   [77][mnist_dream_8.png]
   [78][mnist_input_9.png]   [79][mnist_output_9.png]
   [80][mnist_dream_9.png]
   [81][mnist_input_10.png]  [82][mnist_output_10.png]
   [83][mnist_dream_10.png]
   [84][mnist_input_11.png]  [85][mnist_output_11.png]
   [86][mnist_dream_11.png]
   [87][mnist_input_12.png]  [88][mnist_output_12.png]
   [89][mnist_dream_12.png]
   [90][mnist_input_13.png]  [91][mnist_output_13.png]
   [92][mnist_dream_13.png]
   [93][mnist_input_14.png]  [94][mnist_output_14.png]
   [95][mnist_dream_14.png]
   [96][mnist_input_15.png]  [97][mnist_output_15.png]
   [98][mnist_dream_15.png]
   [99][mnist_input_16.png]  [100][mnist_output_16.png]
   [101][mnist_dream_16.png]
   [102][mnist_input_17.png] [103][mnist_output_17.png]
   [104][mnist_dream_17.png]
   [105][mnist_input_18.png] [106][mnist_output_18.png]
   [107][mnist_dream_18.png]
   [108][mnist_input_19.png] [109][mnist_output_19.png]
   [110][mnist_dream_19.png]
   [111][mnist_input_20.png] [112][mnist_output_20.png]
   [113][mnist_dream_20.png]
   [114][mnist_input_21.png] [115][mnist_output_21.png]
   [116][mnist_dream_21.png]
   [117][mnist_input_22.png] [118][mnist_output_22.png]
   [119][mnist_dream_22.png]
   [120][mnist_input_23.png] [121][mnist_output_23.png]
   [122][mnist_dream_23.png]
   [123][mnist_input_24.png] [124][mnist_output_24.png]
   [125][mnist_dream_24.png]
   [126][mnist_input_25.png] [127][mnist_output_25.png]
   [128][mnist_dream_25.png]
   [129][mnist_input_26.png] [130][mnist_output_26.png]
   [131][mnist_dream_26.png]
   [132][mnist_input_27.png] [133][mnist_output_27.png]
   [134][mnist_dream_27.png]
   [135][mnist_input_28.png] [136][mnist_output_28.png]
   [137][mnist_dream_28.png]
   [138][mnist_input_29.png] [139][mnist_output_29.png]
   [140][mnist_dream_29.png]

   as discussed earlier, the latent z vector can be interpreted as a
   compressed coded version of actual images, like a non-linear version of
   pca. embedded in these 32 numbers is information containing not only
   the digit the image represents, but also other information, such as
   size, style and orientation of the image. not everyone writes the same
   way, some people write it with a loop, or without a loop, and some
   people write digits larger than others, with a more aggressive pen
   stroke. we see that the autoencoder can capture most of these
   information successfully, and reproduce a version of the original
   image. an analogy would be a person looking at an image, and taking
   down notes to describe an image in great detail, and then having
   another person reproduce the original image from the notes.

latent vector arithmetic

   an interesting property that has been discovered from [141]previous
   papers is that we can perform arithmetic on the z-vectors to generate
   new images with interesting properties derived from the arithmetic. for
   example, if you encode the image of two women, where one lady is
   smiling and another lady looks pissed off, and take the difference
   between these two latent vectors and add this difference to the latent
   vector of an encoded image of an angry guy, you can generate a picture
   of this guy in his happy state. this is a little surprising because the
   process of encoding and decoding is highly non-linear, but i guess
   within certain bounds a certain amount of linearity holds. this
   phenomenon has been tested with vae, gan, and dcgan where the pixels
   are generated directly. let   s see if we see this work in the cppn
   version of gan-vae.

   first, let   s get a random example of a from mnist, and show the
   autoencoded version.
 1
2
3
 m = sampler.get_random_specific_mnist(9)  # get a random picture of 9 from mnis
t dataset
z_9 = sampler.encode(m)
sampler.show_image_from_z(z_9)


                               [142][x_9.png]

   then, let   s get two random examples of a from mnist, and show the
   autoencoded versions.
 1
2
3
4
 z_1_before = sampler.encode(sampler.get_random_specific_mnist(1))
sampler.show_image_from_z(z_1_before)
z_1_after = sampler.encode(sampler.get_random_specific_mnist(1))
sampler.show_image_from_z(z_1_after)


                 z_1_before            z_1_after
                 [143][x_1_before.png] [144][x_1_after.png]

   we notice that one sample of is slanted compared to the other sample.
   we can take the difference of these two z-vectors of , and add the
   result to the original z-vector of and see what we get.
 1
2
 z_9_alter = z_9 + (z_1_after - z_1_before)
sampler.show_image_from_z(z_9_alter)


                            [145][x_9_alter.png]

   we see that the altered is also slanted in sort of the same way. we can
   try with other examples too.

   below, we try to modify a normal looking by adding in the difference of
   a wide and narrow .
 1
2
 z_2 = sampler.encode(sampler.get_random_specific_mnist(2))
sampler.show_image_from_z(z_2)


                               [146][x_2.png]

 1
2
3
4
 z_5_before = sampler.encode(sampler.get_random_specific_mnist(5))
sampler.show_image_from_z(z_5_before)
z_5_after = sampler.encode(sampler.get_random_specific_mnist(5))
sampler.show_image_from_z(z_5_after)


                 z_5_before            z_5_after
                 [147][x_5_before.png] [148][x_5_after.png]

 1
2
 z_2_alter = z_2 + (z_5_after - z_5_before)
sampler.show_image_from_z(z_2_alter)


                            [149][x_2_alter.png]

   for some reason, the narrower version of is written without the loop as
   the original version.

   some more examples involving style. we choose a fat and skinny , and
   use their latent-space difference to make a normal and fatter or
   skinnier.

                  z_fat_8            z_skinny_8
                  [150][x_fat_8.png] [151][x_skinny_8.png]

    z_6                   z_6+(z_skinny_8-z_fat_8)
    [152][x_normal_6.png] [153][x_normal_6_plus_skinny_8_minus_fat_8.png]

   z_3 z_3+(z_skinny_8-z_fat_8) z_3+(z_fat_8-z_skinny_8)
   [154][x_normal_3.png] [155][x_normal_3_plus_skinny_8_minus_fat_8.png]
   [156][x_normal_3_plus_fat_8_minus_skinny_8.png]

animations

   i   ve also written some helper methods to visualise the transition from
   one latent state to another, and we can use these methods to create
   animated .gif files. for example, this is how to create an animation of
   a morphing to a , and then to a , and back to a , with a sinusoidal
   time effect:
 1
2
3
4
5
6
7
8
 z_2 = sampler.encode(sampler.get_random_specific_mnist(2))
z_9 = sampler.encode(sampler.get_random_specific_mnist(9))
z_0 = sampler.encode(sampler.get_random_specific_mnist(0))
x_array_2to9 = sampler.morph(z_2, z_9, sinusoid = true)
x_array_9to0 = sampler.morph(z_9, z_0, sinusoid = true)
x_array_0to2 = sampler.morph(z_0, z_2, sinusoid = true)
x_array = x_array_2to9 + x_array_9to0 + x_array_0to2
sampler.save_anim_gif(x_array, 'output_filename.gif', duration = 1.0 / 10.0)


                         [157][output_sinusoid.gif]

conclusions and future work

   in the future i would like to attempt to train this algorithm on more
   interesting datasets compared to mnist, which i think is a good first
   dataset to use.

   we maybe able to train the network via curriculum learning as well, on
   higher resolution datasets. for example, first train the network to on
   a dataset scaled down to to 2x2 pixels. once the training is
   satisficatory, we then train the same network on the same dataset at
   4x4, then 8x8, and so on all the way to 1024x1024. the weights learned
   from the smaller set can be the initial set of weights used for the
   larger dataset, so on a very large network, the training path can be
   better directed from this type of curriculum training setup.

   some may argue that this cppn is just doing nothing more than
   extrapolating between the pixels when the image is being expanded, and
   perhaps much simpler mathods can be used to accomplish this, but i
   think the concept of encapsulating the entire image generating process
   into a network that is able to generate an image with certain
   limitations (ie, it cannot directly dictate values for every pixel in
   the image), and being able to use id119 to train the
   encapsulted network to draw the concept of some object into an image is
   quite powerful.

   for example, we can replace the cppn, by another method, say a
   recurrent neural network that generates a set of vectorised points to
   represent an ink-brush image that can be converted to pixel format and
   compared to a training set. in other words, we can train a neural
   network to generate a set of instructions for a virtual paintbrush
   (which is limited in terms of the space of drawings it can create), to
   attempt to draw images that look like concepts in a dataset containing
   only images with pixels. this encapsulation framework allows us to
   train a network to think outside of pixel space first, and to be
   trained via back-propagation on examples from pixel space.

   finally, i want to figure out some alternatives to maximum likelihood
   as a training objective (it was used in the vae part). i think a
   generated image that has a good score based on pixel vs pixel
   comparison to a training image does not necessarily imply that it looks
   natural to humans. this has been discussed before on [158]ferenc
   husz  r   s blog. the images trained on pure gan looked punchier and has
   character, and i found them more interesting to look at compared to the
   smokey gan+vae combination, although the gan+vae combination looks much
   better than the pure vae version which just looks like blurry images. i
   think further developments and improvements in adversarial training
   methodologies might allow us to get rid of maximum likelihood, and
   recent concepts such as [159]style and structure gan shows promise
   towards this direction.

   please enable javascript to view the [160]comments powered by disqus.

      [161]otoro.net

references

   visible links
   1. http://blog.otoro.net/feed.xml
   2. http://otoro.net/
   3. http://blog.otoro.net/archive.html
   4. https://github.com/hardmaru/cppn-gan-vae-tensorflow
   5. https://en.wikipedia.org/wiki/compositional_pattern-producing_network
   6. http://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html
   7. http://blog.otoro.net/2015/06/19/neural-network-generative-art/
   8. https://github.com/soumith/dcgan.torch
   9. http://qiita.com/mattya/items/e5bfe5e04b9d2f0bbd47
  10. http://blog.otoro.net/2016/03/25/generating-abstract-patterns-with-tensorflow/
  11. http://blog.otoro.net/2015/07/31/neurogram/
  12. https://society6.com/otoro
  13. http://otoro.net/gallery/
  14. http://www.redbubble.com/people/otorography
  15. http://otoro.net/ml/neurogram/
  16. http://eplex.cs.ucf.edu/noveltysearch/userspage/
  17. http://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html
  18. http://prostheticknowledge.tumblr.com/post/136696656421/dcgan-face-generator-online-image-generator-can
  19. http://arxiv.org/abs/1406.2661
  20. http://arxiv.org/abs/1312.6114
  21. http://arxiv.org/abs/1511.06434
  22. http://arxiv.org/abs/1502.04623
  23. https://github.com/carpedm20/dcgan-tensorflow
  24. https://jmetzen.github.io/2015-11-27/vae.html
  25. http://arxiv.org/abs/1406.2661
  26. http://arxiv.org/abs/1406.2661
  27. https://en.wikipedia.org/wiki/art_forgery
  28. https://github.com/skaae/vaeblog
  29. http://blog.otoro.net/assets/20160401/cppgan/cppgan_5_small.png
  30. http://blog.otoro.net/assets/20160401/cppgan/cppgan_0_small.png
  31. http://blog.otoro.net/assets/20160401/cppgan/cppgan_9_small.png
  32. http://blog.otoro.net/assets/20160401/jpeg/cppgan_0.jpeg
  33. http://blog.otoro.net/assets/20160401/jpeg/cppgan_5.jpeg
  34. http://blog.otoro.net/assets/20160401/jpeg/cppgan_9.jpeg
  35. http://arxiv.org/abs/1312.6114
  36. http://vdumoulin.github.io/morphing_faces/
  37. https://ift6266h15.wordpress.com/2015/03/23/lecture-20-march-26th-2015-the-variational-autoencoder/
  38. https://jmetzen.github.io/2015-11-27/vae.html
  39. http://arxiv.org/abs/1312.6114
  40. http://arxiv.org/abs/1312.6114
  41. https://github.com/skaae/vaeblog
  42. http://blog.otoro.net/assets/20160401/png/random00.png
  43. http://blog.otoro.net/assets/20160401/png/random01.png
  44. http://blog.otoro.net/assets/20160401/png/random02.png
  45. http://blog.otoro.net/assets/20160401/png/random03.png
  46. http://blog.otoro.net/assets/20160401/png/random04.png
  47. http://blog.otoro.net/assets/20160401/png/random05.png
  48. http://blog.otoro.net/assets/20160401/png/random06.png
  49. http://blog.otoro.net/assets/20160401/png/random07.png
  50. http://blog.otoro.net/assets/20160401/png/random08.png
  51. http://blog.otoro.net/assets/20160401/png/mnist_input_0.png
  52. http://blog.otoro.net/assets/20160401/png/mnist_output_0.png
  53. http://blog.otoro.net/assets/20160401/png/mnist_dream_0.png
  54. http://blog.otoro.net/assets/20160401/png/mnist_input_1.png
  55. http://blog.otoro.net/assets/20160401/png/mnist_output_1.png
  56. http://blog.otoro.net/assets/20160401/png/mnist_dream_1.png
  57. http://blog.otoro.net/assets/20160401/png/mnist_input_2.png
  58. http://blog.otoro.net/assets/20160401/png/mnist_output_2.png
  59. http://blog.otoro.net/assets/20160401/png/mnist_dream_2.png
  60. http://blog.otoro.net/assets/20160401/png/mnist_input_3.png
  61. http://blog.otoro.net/assets/20160401/png/mnist_output_3.png
  62. http://blog.otoro.net/assets/20160401/png/mnist_dream_3.png
  63. http://blog.otoro.net/assets/20160401/png/mnist_input_4.png
  64. http://blog.otoro.net/assets/20160401/png/mnist_output_4.png
  65. http://blog.otoro.net/assets/20160401/png/mnist_dream_4.png
  66. http://blog.otoro.net/assets/20160401/png/mnist_input_5.png
  67. http://blog.otoro.net/assets/20160401/png/mnist_output_5.png
  68. http://blog.otoro.net/assets/20160401/png/mnist_dream_5.png
  69. http://blog.otoro.net/assets/20160401/png/mnist_input_6.png
  70. http://blog.otoro.net/assets/20160401/png/mnist_output_6.png
  71. http://blog.otoro.net/assets/20160401/png/mnist_dream_6.png
  72. http://blog.otoro.net/assets/20160401/png/mnist_input_7.png
  73. http://blog.otoro.net/assets/20160401/png/mnist_output_7.png
  74. http://blog.otoro.net/assets/20160401/png/mnist_dream_7.png
  75. http://blog.otoro.net/assets/20160401/png/mnist_input_8.png
  76. http://blog.otoro.net/assets/20160401/png/mnist_output_8.png
  77. http://blog.otoro.net/assets/20160401/png/mnist_dream_8.png
  78. http://blog.otoro.net/assets/20160401/png/mnist_input_9.png
  79. http://blog.otoro.net/assets/20160401/png/mnist_output_9.png
  80. http://blog.otoro.net/assets/20160401/png/mnist_dream_9.png
  81. http://blog.otoro.net/assets/20160401/png/mnist_input_10.png
  82. http://blog.otoro.net/assets/20160401/png/mnist_output_10.png
  83. http://blog.otoro.net/assets/20160401/png/mnist_dream_10.png
  84. http://blog.otoro.net/assets/20160401/png/mnist_input_11.png
  85. http://blog.otoro.net/assets/20160401/png/mnist_output_11.png
  86. http://blog.otoro.net/assets/20160401/png/mnist_dream_11.png
  87. http://blog.otoro.net/assets/20160401/png/mnist_input_12.png
  88. http://blog.otoro.net/assets/20160401/png/mnist_output_12.png
  89. http://blog.otoro.net/assets/20160401/png/mnist_dream_12.png
  90. http://blog.otoro.net/assets/20160401/png/mnist_input_13.png
  91. http://blog.otoro.net/assets/20160401/png/mnist_output_13.png
  92. http://blog.otoro.net/assets/20160401/png/mnist_dream_13.png
  93. http://blog.otoro.net/assets/20160401/png/mnist_input_14.png
  94. http://blog.otoro.net/assets/20160401/png/mnist_output_14.png
  95. http://blog.otoro.net/assets/20160401/png/mnist_dream_14.png
  96. http://blog.otoro.net/assets/20160401/png/mnist_input_15.png
  97. http://blog.otoro.net/assets/20160401/png/mnist_output_15.png
  98. http://blog.otoro.net/assets/20160401/png/mnist_dream_15.png
  99. http://blog.otoro.net/assets/20160401/png/mnist_input_16.png
 100. http://blog.otoro.net/assets/20160401/png/mnist_output_16.png
 101. http://blog.otoro.net/assets/20160401/png/mnist_dream_16.png
 102. http://blog.otoro.net/assets/20160401/png/mnist_input_17.png
 103. http://blog.otoro.net/assets/20160401/png/mnist_output_17.png
 104. http://blog.otoro.net/assets/20160401/png/mnist_dream_17.png
 105. http://blog.otoro.net/assets/20160401/png/mnist_input_18.png
 106. http://blog.otoro.net/assets/20160401/png/mnist_output_18.png
 107. http://blog.otoro.net/assets/20160401/png/mnist_dream_18.png
 108. http://blog.otoro.net/assets/20160401/png/mnist_input_19.png
 109. http://blog.otoro.net/assets/20160401/png/mnist_output_19.png
 110. http://blog.otoro.net/assets/20160401/png/mnist_dream_19.png
 111. http://blog.otoro.net/assets/20160401/png/mnist_input_20.png
 112. http://blog.otoro.net/assets/20160401/png/mnist_output_20.png
 113. http://blog.otoro.net/assets/20160401/png/mnist_dream_20.png
 114. http://blog.otoro.net/assets/20160401/png/mnist_input_21.png
 115. http://blog.otoro.net/assets/20160401/png/mnist_output_21.png
 116. http://blog.otoro.net/assets/20160401/png/mnist_dream_21.png
 117. http://blog.otoro.net/assets/20160401/png/mnist_input_22.png
 118. http://blog.otoro.net/assets/20160401/png/mnist_output_22.png
 119. http://blog.otoro.net/assets/20160401/png/mnist_dream_22.png
 120. http://blog.otoro.net/assets/20160401/png/mnist_input_23.png
 121. http://blog.otoro.net/assets/20160401/png/mnist_output_23.png
 122. http://blog.otoro.net/assets/20160401/png/mnist_dream_23.png
 123. http://blog.otoro.net/assets/20160401/png/mnist_input_24.png
 124. http://blog.otoro.net/assets/20160401/png/mnist_output_24.png
 125. http://blog.otoro.net/assets/20160401/png/mnist_dream_24.png
 126. http://blog.otoro.net/assets/20160401/png/mnist_input_25.png
 127. http://blog.otoro.net/assets/20160401/png/mnist_output_25.png
 128. http://blog.otoro.net/assets/20160401/png/mnist_dream_25.png
 129. http://blog.otoro.net/assets/20160401/png/mnist_input_26.png
 130. http://blog.otoro.net/assets/20160401/png/mnist_output_26.png
 131. http://blog.otoro.net/assets/20160401/png/mnist_dream_26.png
 132. http://blog.otoro.net/assets/20160401/png/mnist_input_27.png
 133. http://blog.otoro.net/assets/20160401/png/mnist_output_27.png
 134. http://blog.otoro.net/assets/20160401/png/mnist_dream_27.png
 135. http://blog.otoro.net/assets/20160401/png/mnist_input_28.png
 136. http://blog.otoro.net/assets/20160401/png/mnist_output_28.png
 137. http://blog.otoro.net/assets/20160401/png/mnist_dream_28.png
 138. http://blog.otoro.net/assets/20160401/png/mnist_input_29.png
 139. http://blog.otoro.net/assets/20160401/png/mnist_output_29.png
 140. http://blog.otoro.net/assets/20160401/png/mnist_dream_29.png
 141. https://github.com/newmu/dcgan_code
 142. http://blog.otoro.net/assets/20160401/png/x_9.png
 143. http://blog.otoro.net/assets/20160401/png/x_1_before.png
 144. http://blog.otoro.net/assets/20160401/png/x_1_after.png
 145. http://blog.otoro.net/assets/20160401/png/x_9_alter.png
 146. http://blog.otoro.net/assets/20160401/png/x_2.png
 147. http://blog.otoro.net/assets/20160401/png/x_5_before.png
 148. http://blog.otoro.net/assets/20160401/png/x_5_after.png
 149. http://blog.otoro.net/assets/20160401/png/x_2_alter.png
 150. http://blog.otoro.net/assets/20160401/png/x_fat_8.png
 151. http://blog.otoro.net/assets/20160401/png/x_skinny_8.png
 152. http://blog.otoro.net/assets/20160401/png/x_normal_6.png
 153. http://blog.otoro.net/assets/20160401/png/x_normal_6_plus_skinny_8_minus_fat_8.png
 154. http://blog.otoro.net/assets/20160401/png/x_normal_3.png
 155. http://blog.otoro.net/assets/20160401/png/x_normal_3_plus_skinny_8_minus_fat_8.png
 156. http://blog.otoro.net/assets/20160401/png/x_normal_3_plus_fat_8_minus_skinny_8.png
 157. https://cdn.rawgit.com/hardmaru/cppn-gan-vae-tensorflow/master/examples/output_sinusoid.gif
 158. http://www.id136.vc/how-to-train-your-generative-models-why-generative-adversarial-networks-work-so-well-2/
 159. http://arxiv.org/abs/1603.05631
 160. https://disqus.com/?ref_noscript
 161. http://otoro.net/

   hidden links:
 163. http://blog.otoro.net/
 164. http://otoro.net/#contact
 165. https://www.facebook.com/otorography
 166. https://github.com/hardmaru
 167. http://instagram.com/hardmaru
 168. https://twitter.com/hardmaru
 169. http://twitter.com/share?url=http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/&text=generating%20large%20images%20from%20latent%20vectors&via=hardmaru
 170. https://plus.google.com/share?url=http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/
 171. http://www.facebook.com/sharer/sharer.php?u=http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/
 172. http://www.linkedin.com/sharearticle?url=http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/&title=generating%20large%20images%20from%20latent%20vectors&source=
 173. http://twitter.com/share?url=http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/&text=generating%20large%20images%20from%20latent%20vectors&via=hardmaru
 174. https://plus.google.com/share?url=http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/
 175. http://www.facebook.com/sharer/sharer.php?u=http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/
 176. http://www.linkedin.com/sharearticle?url=http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/&title=generating%20large%20images%20from%20latent%20vectors&source=
