   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

tensorflow in a nutshell         part three: all the models

   [14]go to the profile of camron godbout
   [15]camron godbout (button) blockedunblock (button) followfollowing
   oct 3, 2016
   [1*sud-povcacwkb4qfztomdq.png]

the fast and easy guide to the most popular deep learning framework in
the world.

   make sure to check out the other articles [16]here.

overview

   in this installment we will be going over all the abstracted models
   that are currently available in tensorflow and describe use cases for
   that particular model as well as simple sample code. full sources of
   working examples are in the [17]tensorflow in a nutshell repo.
     __________________________________________________________________

   [1*lq4izz9zbhkyd8nclzpsmq.png]
   a recurrent neural network

recurrent neural networks

   use cases: id38, machine translation, id27, text
   processing.

   since the advent of long short term memory and id149,
   recurrent neural networks have made leaps and bounds above other models
   in natural language processing. they can be fed vectors representing
   characters and be trained to generate new sentences based on the
   training set. the merit in this model is that it keeps the context of
   the sentence and derives meaning that    cat sat on the mat    means the
   cat is on the mat. since the creation of tensorflow writing these
   networks have become increasingly simpler. there are even hidden
   features covered by denny britz [18]here that make writing id56   s even
   simpler heres a quick example.
import tensorflow as tf
import numpy as np
# create input data
x = np.random.randn(2, 10, 8)
# the second example is of length 6
x[1,6,:] = 0
x_lengths = [10, 6]
cell = tf.nn.id56_cell.lstmcell(num_units=64, state_is_tuple=true)
cell = tf.nn.id56_cell.dropoutwrapper(cell=cell, output_keep_prob=0.5)
cell = tf.nn.id56_cell.multiid56cell(cells=[cell] * 4, state_is_tuple=true)
outputs, last_states = tf.nn.dynamic_id56(
    cell=cell,
    dtype=tf.float64,
    sequence_length=x_lengths,
    inputs=x)
result = tf.contrib.learn.run_n(
    {"outputs": outputs, "last_states": last_states},
    n=1,
    feed_dict=none)
     __________________________________________________________________

   [1*n4h1sgwbwid4rrhszm9ejg.png]
   convolution neural network

convolution neural networks

   use cases: image processing, facial recognition, id161

   convolution neural networks are unique because they   re created in mind
   that the input will be an image. id98s perform a sliding window function
   to a matrix. the window is called a kernel and it slides across the
   image creating a convolved feature.
   [1*zcjpufrb6ehpri4eyp6aaa.gif]
   from
   [19]http://deeplearning.standford.edu/wiki/index.php/feature_extraction
   _using_convolution

   creating a convolved feature allows for edge detection which then
   allows for a network to depict objects from pictures.
   [1*3h4ho1lx_saxzqk243ic9w.jpeg]
   edge detection from gimp manual

   the convolved feature to create this looks like this matrix below:
   [1*h5xnumuf7xcmtcfru5pteq.png]
   convolved feature from gimp manual

   here   s a sample of code to identify handwritten digits from the mnist
   dataset.
### convolutional network
def max_pool_2x2(tensor_in):
  return tf.nn.max_pool(
      tensor_in, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='same')
def conv_model(x, y):
  # reshape x to 4d tensor with 2nd and 3rd dimensions being image width and
  # height final dimension being the number of color channels.
  x = tf.reshape(x, [-1, 28, 28, 1])
  # first conv layer will compute 32 features for each 5x5 patch
  with tf.variable_scope('conv_layer1'):
    h_conv1 = learn.ops.conv2d(x, n_filters=32, filter_shape=[5, 5],
                               bias=true, activation=tf.nn.relu)
    h_pool1 = max_pool_2x2(h_conv1)
  # second conv layer will compute 64 features for each 5x5 patch.
  with tf.variable_scope('conv_layer2'):
    h_conv2 = learn.ops.conv2d(h_pool1, n_filters=64, filter_shape=[5, 5],
                               bias=true, activation=tf.nn.relu)
    h_pool2 = max_pool_2x2(h_conv2)
    # reshape tensor into a batch of vectors
    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])
  # densely connected layer with 1024 neurons.
  h_fc1 = learn.ops.dnn(
      h_pool2_flat, [1024], activation=tf.nn.relu, dropout=0.5)
  return learn.models.logistic_regression(h_fc1, y)
     __________________________________________________________________

   [1*tobl6xlerkwabswtafay_g.png]

feed forward neural networks

   use cases: classification and regression

   these networks consist of id88s in layers that take inputs that
   pass information on to the next layer. the last layer in the network
   produces the output. there is no connection between each node in a
   given layer. the layer that has no original input and no final output
   is called the hidden layer.

   the goal of this network is similar to other supervised neural networks
   using back propagation, to make inputs have the desired trained
   outputs. these are some of the simplest effective neural networks for
   classification and regression problems. we will show how easy it is to
   create a feed forward network to classify handwritten digits:
def init_weights(shape):
    return tf.variable(tf.random_normal(shape, stddev=0.01))
def model(x, w_h, w_o):
    h = tf.nn.sigmoid(tf.matmul(x, w_h)) # this is a basic mlp, think 2 stacked
id28s
    return tf.matmul(h, w_o) # note that we dont take the softmax at the end bec
ause our cost fn does that for us
mnist = input_data.read_data_sets("mnist_data/", one_hot=true)
trx, try, tex, tey = mnist.train.images, mnist.train.labels, mnist.test.images,
mnist.test.labels
x = tf.placeholder("float", [none, 784])
y = tf.placeholder("float", [none, 10])
w_h = init_weights([784, 625]) # create symbolic variables
w_o = init_weights([625, 10])
py_x = model(x, w_h, w_o)
cost = tf.reduce_mean(tf.nn.softmax_cross_id178_with_logits(py_x, y)) # comput
e costs
train_op = tf.train.gradientdescentoptimizer(0.05).minimize(cost) # construct an
 optimizer
predict_op = tf.argmax(py_x, 1)
# launch the graph in a session
with tf.session() as sess:
    # you need to initialize all variables
    tf.initialize_all_variables().run()
for i in range(100):
        for start, end in zip(range(0, len(trx), 128), range(128, len(trx)+1, 12
8)):
            sess.run(train_op, feed_dict={x: trx[start:end], y: try[start:end]})
        print(i, np.mean(np.argmax(tey, axis=1) ==
                         sess.run(predict_op, feed_dict={x: tex, y: tey})))
     __________________________________________________________________

   [1*sy_6ipmbh_21kd0_dds1hq.png]

linear models

   use cases: classification and regression

   linear models take x values and produce a line of best fit used for
   classification and regression of y values. for example if you have a
   list of house sizes and their price in a neighborhood you can predict
   the price of house given the size using a linear model.

   one thing to note is that linear models can be used for multiple x
   features. for example in the housing example we can create a linear
   model given house sizes, how many rooms, how many bathrooms and price
   and predict price given a house with size, # of rooms, # of bathrooms.
import numpy as np
import tensorflow as tf
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=1)
    return tf.variable(initial)
# dataset
xx = np.random.randint(0,1000,[1000,3])/1000.
yy = xx[:,0] * 2 + xx[:,1] * 1.4 + xx[:,2] * 3
# model
x = tf.placeholder(tf.float32, shape=[none, 3])
y_ = tf.placeholder(tf.float32, shape=[none])
w1 = weight_variable([3, 1])
y = tf.matmul(x, w1)
# training and cost function
cost_function = tf.reduce_mean(tf.square(tf.squeeze(y) - y_))
train_function = tf.train.adamoptimizer(1e-2).minimize(cost_function)
# create a session
sess = tf.session()
# train
sess.run(tf.initialize_all_variables())
for i in range(10000):
    sess.run(train_function, feed_dict={x:xx, y_:yy})
    if i % 1000 == 0:
        print(sess.run(cost_function, feed_dict={x:xx, y_:yy}))
     __________________________________________________________________

   [1*xwnzplj1p-xnukrqpms6aw.png]

support vector machines

   use cases: currently only binary classification

   the general idea behind a id166 is that there is an optimal hyperplane
   for linearly separable patterns. for data that is not linearly
   separable we can use a id81 to transform the original data
   into a new space. id166s maximize the margin around separating the
   hyperplane. they work extremely well in high dimensional spaces and and
   are still effective if the dimensions are greater than the number of
   samples.
def input_fn():
      return {
          'example_id': tf.constant(['1', '2', '3']),
          'price': tf.constant([[0.6], [0.8], [0.3]]),
          'sq_footage': tf.constant([[900.0], [700.0], [600.0]]),
          'country': tf.sparsetensor(
              values=['it', 'us', 'gb'],
              indices=[[0, 0], [1, 3], [2, 1]],
              shape=[3, 5]),
          'weights': tf.constant([[3.0], [1.0], [1.0]])
      }, tf.constant([[1], [0], [1]])
price = tf.contrib.layers.real_valued_column('price')
    sq_footage_bucket = tf.contrib.layers.bucketized_column(
        tf.contrib.layers.real_valued_column('sq_footage'),
        boundaries=[650.0, 800.0])
    country = tf.contrib.layers.sparse_column_with_hash_bucket(
        'country', hash_bucket_size=5)
    sq_footage_country = tf.contrib.layers.crossed_column(
        [sq_footage_bucket, country], hash_bucket_size=10)
    id166_classifier = tf.contrib.learn.id166(
        feature_columns=[price, sq_footage_bucket, country, sq_footage_country],
        example_id_column='example_id',
        weight_column_name='weights',
        l1_id173=0.1,
        l2_id173=1.0)
id166_classifier.fit(input_fn=input_fn, steps=30)
    accuracy = id166_classifier.evaluate(input_fn=input_fn, steps=1)['accuracy']
     __________________________________________________________________

   [1*eadupdnqb1qyl6mpvr3w_q.png]

deep and wide models

   use cases: id126s, classification and regression

   deep and wide models were covered with greater detail in [20]part two,
   so we won   t get too heavy here. a wide and deep network combines a
   linear model with a feed forward neural net so that our predictions
   will have memorization and generalization. this type of model can be
   used for classification and regression problems. this allows for less
   feature engineering with relatively accurate predictions. thus, getting
   the best of both worlds. here   s a code snippet from [21]part two   s
   github.
def input_fn(df, train=false):
  """input builder function."""
  # creates a dictionary mapping from each continuous feature column name (k) to
  # the values of that column stored in a constant tensor.
  continuous_cols = {k: tf.constant(df[k].values) for k in continuous_columns}
  # creates a dictionary mapping from each categorical feature column name (k)
  # to the values of that column stored in a tf.sparsetensor.
  categorical_cols = {k: tf.sparsetensor(
    indices=[[i, 0] for i in range(df[k].size)],
    values=df[k].values,
    shape=[df[k].size, 1])
                      for k in categorical_columns}
  # merges the two dictionaries into one.
  feature_cols = dict(continuous_cols)
  feature_cols.update(categorical_cols)
  # converts the label column into a constant tensor.
  if train:
    label = tf.constant(df[survived_column].values)
      # returns the feature columns and the label.
    return feature_cols, label
  else:
    return feature_cols
m = build_estimator(model_dir)
m.fit(input_fn=lambda: input_fn(df_train, true), steps=200)
print m.predict(input_fn=lambda: input_fn(df_test))
results = m.evaluate(input_fn=lambda: input_fn(df_train, true), steps=1)
for key in sorted(results):
  print("%s: %s" % (key, results[key]))
     __________________________________________________________________

   [1*breo9la3b-us5og4kuuzqw.png]

id79

   use cases: classification and regression

   id79 model takes many different classification trees and each
   tree votes for that class. the forest chooses the classification having
   the most votes.

   id79s do not overfit, you can run as many treees as you want
   and it is relatively fast. give it a try on the iris data with this
   snippet below:
hparams = tf.contrib.tensor_forest.python.tensor_forest.foresthparams(
        num_trees=3, max_nodes=1000, num_classes=3, num_features=4)
classifier = tf.contrib.learn.tensorforestestimator(hparams)
iris = tf.contrib.learn.datasets.load_iris()
data = iris.data.astype(np.float32)
target = iris.target.astype(np.float32)
monitors = [tf.contrib.learn.tensorforestlossmonitor(10, 10)]
classifier.fit(x=data, y=target, steps=100, monitors=monitors)
classifier.evaluate(x=data, y=target, steps=10)
     __________________________________________________________________

   [1*yu7chokfj6ufut79peunlw.png]

bayesian id23

   use cases: classification and regression

   in the contrib folder of tensorflow there is a library called
   bayesflow. bayesflow has no documentation except for an example of the
   reinforce algorithm. this algorithm is proposed in a [22]paper by
   ronald williams.

     reward increment = nonnegative factor * offset reinforcement *
     characteristic eligibility

   this network trying to solve an immediate id23 task,
   adjusts the weights after getting the reinforcement value at each
   trial. at the end of each trial each weight is incremented by a
   learning rate factor multiplied by the reinforcement value minus the
   baseline multiplied by characteristic eligibility. williams paper also
   discusses the use of back propagation to train the reinforce network.
"""build the split-apply-merge model.
  route each value of input [-1, -1, 1, 1] through one of the
  functions, plus_1, minus_1.  the decision for routing is made by
  4 bernoulli r.v.s whose parameters are determined by a neural network
  applied to the input.  reinforce is used to update the nn parameters.
  returns:
    the 3-tuple (route_selection, routing_loss, final_loss), where:
      - route_selection is an int 4-vector
      - routing_loss is a float 4-vector
      - final_loss is a float scalar.
  """
  inputs = tf.constant([[-1.0], [-1.0], [1.0], [1.0]])
  targets = tf.constant([[0.0], [0.0], [0.0], [0.0]])
  paths = [plus_1, minus_1]
  weights = tf.get_variable("w", [1, 2])
  bias = tf.get_variable("b", [1, 1])
  logits = tf.matmul(inputs, weights) + bias
# reinforce forward step
  route_selection = st.stochastictensor(
      distributions.categorical, logits=logits)
     __________________________________________________________________

   [1*1mhiiexwdko75p-_4vkfnq.png]

linear chain id49

   use cases: sequential data

   crfs are id155 distributions that factoirze according
   to an undirected model. they predict a label for a single sample
   keeping context from the neighboring samples. crfs are similar to
   id48. crfs are often used for image segmentation and
   object recognition, as well as id66, named entity
   recognition and gene finding.
# train for a fixed number of iterations.
session.run(tf.initialize_all_variables())
  for i in range(1000):
    tf_unary_scores, tf_transition_params, _ = session.run(
       [unary_scores, transition_params, train_op])
    if i % 100 == 0:
      correct_labels = 0
      total_labels = 0
      for tf_unary_scores_, y_, sequence_length_ in zip(tf_unary_scores, y, sequ
ence_lengths):
        # remove padding from the scores and tag sequence.
        tf_unary_scores_ = tf_unary_scores_[:sequence_length_]
        y_ = y_[:sequence_length_]
        # compute the highest scoring sequence.
        viterbi_sequence, _ = tf.contrib.crf.viterbi_decode(
            tf_unary_scores_, tf_transition_params)
        # evaluate word-level accuracy.
        correct_labels += np.sum(np.equal(viterbi_sequence, y_))
        total_labels += sequence_length_
      accuracy = 100.0 * correct_labels / float(total_labels)
      print("accuracy: %.2f%%" % accuracy)
     __________________________________________________________________

conclusion

   ever since tensorflow has been released the community surrounding the
   project has been adding more packages, examples and cases for using
   this amazing library. even at the time of writing this article there
   are more models and sample code being written. it is amazing to see how
   much tensorflow as grown in these past few months. the ease of use and
   diversity in the package are increasing overtime and don   t seem to be
   slowing down anytime soon.

     as always         feel free to email me any questions or inquiries at
     camron@camron.xyz

     originally posted at [23]camron.xyz

   [24][1*0hqoaabq7xgpt-oyngiubg.png]
   [25][1*vgw1jka6hgnvwztsfmlnpg.png]
   [26][1*gkbpq1ruui0fvk2um_i4tq.png]

     [27]hacker noon is how hackers start their afternoons. we   re a part
     of the [28]@ami family. we are now [29]accepting submissions and
     happy to [30]discuss advertising & sponsorship opportunities.

     if you enjoyed this story, we recommend reading our [31]latest tech
     stories and [32]trending tech stories. until next time, don   t take
     the realities of the world for granted!

   [33][1*35tcjopcvq6lbb3i6wegqw.jpeg]

     * [34]machine learning
     * [35]tensorflow
     * [36]tech
     * [37]programming
     * [38]ai

   (button)
   (button)
   (button) 354 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [39]go to the profile of camron godbout

[40]camron godbout

     (button) follow
   [41]hacker noon

[42]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 354
     * (button)
     *
     *

   [43]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [44]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/be1465993930
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/tensorflow-in-a-nutshell-part-three-all-the-models-be1465993930&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/tensorflow-in-a-nutshell-part-three-all-the-models-be1465993930&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_ywgj2emwwaqu---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@camrongodbout?source=post_header_lockup
  15. https://hackernoon.com/@camrongodbout
  16. http://camron.xyz/
  17. https://github.com/c0cky/tensorflow-in-a-nutshell
  18. http://www.wildml.com/2016/08/id56s-in-tensorflow-a-practical-guide-and-undocumented-features/
  19. http://deeplearning.standford.edu/wiki/index.php/feature_extraction_using_convolution
  20. https://medium.com/@camrongodbout/tensorflow-in-a-nutshell-part-two-hybrid-learning-98c121d35392#.oubizxp18
  21. https://github.com/c0cky/tensorflow-in-a-nutshell/tree/master/part2
  22. http://incompleteideas.net/sutton/williams-92.pdf
  23. http://camron.xyz/
  24. http://bit.ly/hackernoonfb
  25. https://goo.gl/k7xybx
  26. https://goo.gl/4ofytp
  27. http://bit.ly/hackernoon
  28. http://bit.ly/atamiatami
  29. http://bit.ly/hackernoonsubmission
  30. mailto:partners@amipublications.com
  31. http://bit.ly/hackernoonlatestt
  32. https://hackernoon.com/trending
  33. https://goo.gl/ahtev1
  34. https://hackernoon.com/tagged/machine-learning?source=post
  35. https://hackernoon.com/tagged/tensorflow?source=post
  36. https://hackernoon.com/tagged/tech?source=post
  37. https://hackernoon.com/tagged/programming?source=post
  38. https://hackernoon.com/tagged/ai?source=post
  39. https://hackernoon.com/@camrongodbout?source=footer_card
  40. https://hackernoon.com/@camrongodbout
  41. https://hackernoon.com/?source=footer_card
  42. https://hackernoon.com/?source=footer_card
  43. https://hackernoon.com/
  44. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  46. https://medium.com/p/be1465993930/share/twitter
  47. https://medium.com/p/be1465993930/share/facebook
  48. https://medium.com/p/be1465993930/share/twitter
  49. https://medium.com/p/be1465993930/share/facebook
