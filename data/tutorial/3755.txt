   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

how to run text summarization with tensorflow

   [9]go to the profile of pavel surmenok
   [10]pavel surmenok (button) blockedunblock (button) followfollowing
   oct 15, 2016

   text summarization problem has many useful applications. if you run a
   website, you can create titles and short summaries for user generated
   content. if you want to read a lot of articles and don   t have time to
   do that, your virtual assistant can summarize main points from these
   articles for you.

   it is not an easy problem to solve. there are [11]multiple approaches,
   including various supervised and unsupervised algorithms. some
   algorithms rank the importance of sentences within the text and then
   construct a summary out of important sentences, others are end-to-end
   generative models.

   end-to-end machine learning algorithms are interesting to try. after
   all, end-to-end algorithms demonstrate good results in other areas,
   like image recognition, id103, language translation, and
   even question-answering.
   [1*so-sp58t4bre9ehazhsega.png]
   image credit:
   [12]https://research.googleblog.com/2015/11/computer-respond-to-this-em
   ail.html

text summarization with tensorflow

   in august 2016, peter liu and xin pan, software engineers on google
   brain team, published a blog post    [13]text summarization with
   tensorflow   . their algorithm is extracting interesting parts of the
   text and create a summary by using these parts of the text and allow
   for rephrasings to make summary more grammatically correct. this
   approach is called abstractive summarization.

   peter and xin trained a text summarization model to produce headlines
   for news articles, using [14]annotated english gigaword, a dataset
   often used in summarization research. the dataset contains about 10
   million documents. the model was trained end-to-end with a deep
   learning technique called [15]sequence-to-sequence learning.

   [16]code for training and testing the model is included into tensorflow
   models github repository. the core model is a sequence-to-sequence
   model with attention. when training, the model is using the first two
   sentences from the article as an input and generates a headline.

   when decoding, the algorithm is using [17]id125 to find the best
   headline from candidate headlines generated by the model.

   github repository doesn   t include a trained model. the dataset is not
   publicly available, a license costs $6000 for organizations which are
   not members of linguistic data consortium. but they include a toy
   dataset which is enough to run the code.

how to run

   you will need tensorflow and bazel as prerequisites for training the
   model.

   the toy dataset included into the repository, contains two files in
      data    directory:    data    and    vocab   . the first one contains a sequence
   of serialized tensorflow.core.example.example_pb2.example objects. an
   example of code to create a file with this format:

   iframe: [18]/media/5f9c62ba0b04df7e8bc9b5071bb6db84?postid=d4472587602d

      vocab    file is a text file with the frequency of words in a
   vocabulary. each line contains a word, space character and number of
   occurrences of that word in the dataset. the list is being used to
   vectorize texts.

   running the code on toy dataset is really simple. [19]readme on github
   repo lists a sequence of commands to run training and testing code.

   you can run tensorboard to monitor training process:
   [1*68qx8_lmwewh-qbclgvuzg.png]

   when running    decode    code, note that it will loop over the entire
   dataset indefinitely, so you will have to stop execution manually at
   some point. you can find results of decoding in log_root/decode folder.
   it will contain a few files, some of them have prefix    ref   , they
   contain original headlines from the test set. other files have prefix
      decode   , they contain headlines generated by the model.

troubleshooting

   you can encounter an error when running    eval    or    decode    code using
   tensorflow 0.10 or later:

      valueerror: could not flatten dictionary. key had 2 elements, but
   value had 1 elements.   

   there is an [20]open issue on github for this error. one workaround is
   to downgrade tensorflow to 0.9, it worked for me. [21]another
   workaround requires changing the code of the model: adding
      state_is_tuple=false    to instantiations of lstmcell in
   id195_attention_model.py.
     __________________________________________________________________

   if you run training and decoding on toy dataset, you will notice that
   decoding generates nonsense. here are few examples of headlines
   generated:

     <unk> to <unk> <unk> <unk> <unk> <unk> .

     <unk> <unk> <unk> <unk> of <unk> <unk> from <unk> <unk> .

     in in <unk> <unk> <unk> .

   one of the reasons for poor performance on the toy set could be
   incompleteness of the vocabulary file. vocabulary file is truncated and
   doesn   t contain many of the words which are used in the    data    file. it
   leads to too many    <unk>    tokens which represent unknown words.

how to run on another dataset

   a toy dataset is, well, a toy. to create a useful model you should
   train it on a large dataset. ideally, the dataset should be specific
   for your task. summarizing news article may be different from
   summarizing legal documents or job descriptions.

   as i don   t have access to gigaword dataset, i tried to train the model
   on smaller news article datasets, which are free: id98 and dailymail. i
   found the code to download these datasets in [22]deepmind/rcdata github
   repo, and slightly modified it to add the title of the article in the
   first line of each output file. see modified code [23]here.

   92570 articles in id98 dataset, and 219503 articles in daily mail
   dataset. it could be a few more articles, but the code from deepmind
   repo could not download all urls. 322k articles are way fewer than 10
   million articles in gigaword, so i would expect a lower performance of
   the model if training on these datasets.

   after you run the code to download the dataset you will have a folder
   with lots of files, one html file for every article. to use it in
   textsum model you will need to convert it to the binary format
   described above. you can find my code to convert id98/dailymail articles
   into binary format in [24]textsum_data_convert.py file in my
   [25]   textsum    repo on github. an example of running the code for id98
   dataset:

   iframe: [26]/media/cbaab8efcea3edc8582d67ec29692050?postid=d4472587602d

   then you can copy train/validation/test sets and vocabulary files into
      data    directory and start training the model:

   iframe: [27]/media/63bf30fcfdd059f2f7a56103aab6159f?postid=d4472587602d

   training with default parameters doesn   t go very well. here is a graph
   of running_avg_loss:
   [1*xjfo98tlppu_3qjgsorj_a.png]

   decoding results are also disappointing:

        your your <unk>   

        we   ll the <unk>   

        snow hit hit hit <unk>   

   either dataset is too small, or hyperparameters need to be changed for
   this dataset.
     __________________________________________________________________

   when running the code i found that training code doesn   t use gpu,
   though i have all the correct configuration: geforce 980ti, cuda,
   cudnn, tensorflow compiled with using gpu. while training, python.exe
   consumes 100   300+% cpu, and it appears in the list of processes when
   running nvidia-smi, but gpu utilization stays 0%.

   iframe: [28]/media/d506f8948200f7bb5e1279cc1db532db?postid=d4472587602d

   i guess it can be related to the fact that authors of the model were
   running the code using multiple gpus, and one gpu had some special
   purpose. a fragment of id195_attention_model.py file:

   iframe: [29]/media/3ae8b98cd575bef8870ab6b2f105b818?postid=d4472587602d

   the decoding code uses gpu quite well. it consumes almost all 6gb of
   gpu memory and keeps utilization over 50%.

   iframe: [30]/media/a956cc28c4b90419488d7669f55d2738?postid=d4472587602d

conclusion

   using the code from this article you can easily run text summarization
   model on your own dataset. let me know if you find something
   interesting!

   if you happen to have a license for the gigaword dataset, i will be
   happy if you share trained tensorflow model with me. i would like to
   try it on some proprietary data, not from news articles.

   do you use any other text summarization algorithms? what works the
   best?
     __________________________________________________________________

   the article was originally published on
   [31]http://pavel.surmenok.com/2016/10/15/how-to-run-text-summarization-
   with-tensorflow/
     __________________________________________________________________

     [32]hacker noon is how hackers start their afternoons. we   re a part
     of the [33]@ami family. we are now [34]accepting submissions and
     happy to [35]discuss advertising & sponsorship opportunities.

     if you enjoyed this story, we recommend reading our [36]latest tech
     stories and [37]trending tech stories. until next time, don   t take
     the realities of the world for granted!

     * [38]machine learning
     * [39]tensorflow
     * [40]nlp
     * [41]deep learning
     * [42]artificial intelligence

   (button)
   (button)
   (button) 590 claps
   (button) (button) (button) 20 (button) (button)

     (button) blockedunblock (button) followfollowing
   [43]go to the profile of pavel surmenok

[44]pavel surmenok

   machine learning engineering and self-driving cars. opinions expressed
   are solely my own and do not express the views or opinions of my
   employer.

     * (button)
       (button) 590
     * (button)
     *
     *

   [45]go to the profile of pavel surmenok
   never miss a story from pavel surmenok, when you sign up for medium.
   [46]learn more
   never miss a story from pavel surmenok
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d4472587602d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@surmenok/how-to-run-text-summarization-with-tensorflow-d4472587602d&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@surmenok/how-to-run-text-summarization-with-tensorflow-d4472587602d&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@surmenok?source=post_header_lockup
  10. https://medium.com/@surmenok
  11. https://en.wikipedia.org/wiki/automatic_summarization
  12. https://research.googleblog.com/2015/11/computer-respond-to-this-email.html
  13. https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html
  14. https://catalog.ldc.upenn.edu/ldc2012t21
  15. http://arxiv.org/abs/1409.3215
  16. https://github.com/tensorflow/models/tree/master/textsum
  17. https://en.wikipedia.org/wiki/beam_search
  18. https://medium.com/media/5f9c62ba0b04df7e8bc9b5071bb6db84?postid=d4472587602d
  19. https://github.com/tensorflow/models/tree/master/textsum
  20. https://github.com/tensorflow/models/issues/417
  21. https://github.com/tensorflow/models/issues/417#issuecomment-253363772
  22. https://github.com/deepmind/rc-data
  23. https://gist.github.com/surmenok/2224ccfff5fbf24f3905b3da995668a3
  24. https://github.com/surmenok/textsum/blob/master/textsum_data_convert.py
  25. https://github.com/surmenok/textsum
  26. https://medium.com/media/cbaab8efcea3edc8582d67ec29692050?postid=d4472587602d
  27. https://medium.com/media/63bf30fcfdd059f2f7a56103aab6159f?postid=d4472587602d
  28. https://medium.com/media/d506f8948200f7bb5e1279cc1db532db?postid=d4472587602d
  29. https://medium.com/media/3ae8b98cd575bef8870ab6b2f105b818?postid=d4472587602d
  30. https://medium.com/media/a956cc28c4b90419488d7669f55d2738?postid=d4472587602d
  31. http://pavel.surmenok.com/2016/10/15/how-to-run-text-summarization-with-tensorflow/
  32. http://bit.ly/hackernoon
  33. http://bit.ly/atamiatami
  34. http://bit.ly/hackernoonsubmission
  35. mailto:partners@amipublications.com
  36. http://bit.ly/hackernoonlatestt
  37. https://hackernoon.com/trending
  38. https://medium.com/tag/machine-learning?source=post
  39. https://medium.com/tag/tensorflow?source=post
  40. https://medium.com/tag/nlp?source=post
  41. https://medium.com/tag/deep-learning?source=post
  42. https://medium.com/tag/artificial-intelligence?source=post
  43. https://medium.com/@surmenok?source=footer_card
  44. https://medium.com/@surmenok
  45. https://medium.com/@surmenok
  46. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  48. https://medium.com/p/d4472587602d/share/twitter
  49. https://medium.com/p/d4472587602d/share/facebook
  50. https://medium.com/p/d4472587602d/share/twitter
  51. https://medium.com/p/d4472587602d/share/facebook
