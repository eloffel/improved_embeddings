   #[1]camron's blog    feed [2]creating a search engine [3]tensorflow in a
   nutshell         part two: hybrid learning [4]alternate [5]alternate

   [6]skip to content

   [7]camron's blog

   nlp and deep learning enthusiast

     * [8]about

     * [9]more
          + back

tensorflow in a nutshell         part one: basics

   posted by[10]camron [11]august 22, 2016february 21, 2019

tensorflow_nutshell the fast and easy guide to the most popular deep learning
framework in the world.

   tensorflow is a framework created by google for creating deep learning
   models. deep learning is a category of machine learning models that use
   multi-layer neural networks. the idea of deep learning has been around
   since 1943 when neurophysiologist warren mcculloch and mathematician
   walter pitts wrote a paper on how neurons might work and they model a
   simple neural network using electrical circuits.

   many, many developments have occurred since then. these highly accurate
   mathematical models are extremely computationally expensive. with
   recent advances in processing power from gpus and increasing cpu power
   deep learning has been exploding with popularity.

   tensorflow was created with processing power limitations in mind. open
   sourced in november 2015, this library can be ran on computers of all
   kinds including smartphones. it allows for instant creation of trained
   production models. it is currently the number 1 deep learning framework
   at the time of writing this article.
   [1*pmimtwrxxoiyviw7nurp5w.jpeg] created by francois chollet @fchollet
   (twitter)

basic computational graph

   everything in tensorflow is based on creating a computational graph. if
   you   ve ever used theano then this section will look familiar. think of
   a computational graph as a network of nodes, with each node known as an
   operation, running some function that can be as simple as addition or
   subtraction to as complex as some multi variate equation.

   an operation also referred to as op can return zero or more tensors
   which can be used later on in the graph. heres a list of operations
   with their output for example
   view the code on [12]gist.

   each operation can be handed a constant, array, matrix or n-dimensional
   matrix. another word for an n-dimensional matrix is a tensor, a
   2-dimensional tensor is equivalent to a m x m matrix.
   view the code on [13]gist.
   [1*mvhm5_r6ly-ehsin21rjtg.png] our computational graph

   the code above is creating two constant tensors and multiplying them
   together and outputting our result. this is a trivial example that
   demonstrates how you can create a graph and run the session. all inputs
   needed by the op are run automatically. they   re typically ran in
   parallel. this session run actually causes the execution of three
   operations in the graph, creating the two constants then the matrix
   multiplication.

graph

   the constants and operation that we created above was automagically
   added to the graph in tensorflow. the graph default is intantiated when
   the library is imported. creating a graph object instead of using the
   default graph is useful when creating multiple models in one file that
   do not depend on each other.
new_graph = tf.graph()
with new_graph.as_default():
    new_g_const = tf.constant([1., 2.])

   any variables or operations used outside of the with
   new_graph.as_default() will be added to the default graph that is
   created when the library is loaded. you can even get a handle to the
   default graph with
default_g = tf.get_default_graph()

   for most cases it   s best to stick with the default graph.

session

   there are two kinds of session objects in tensorflow:

tf.session()

   this encapsulates teh environment that operations and tensors are
   executed and evaluated. sessions can have their own variables, queues
   and readers that are allocated. so it   s important to use the close()
   method when the session is over. there are 3 arguments for a session,
   all of which are optional.
    1. target         the execution engine to connect to.
    2. graph         the graph to be launched.
    3. config         a configproto protocl buffer with configuration options
       for the session

   to have run one    step    of the tensorflow computation this function is
   called and all of the necessary dependencies for the graph to execute
   are ran.

tf.interactivesession()

   this is the exact same as tf.session() but is targeted for using
   ipython and jupyter notebooks that allows you to add things and use
   tensor.eval() and operation.run() instead of having to do session.run()
   every time you want something to be computed.
sess = tf.interactivesession()
a = tf.constant(1)
b = tf.constant(2)
c = a + b
# instead of sess.run(c)
c.eval()

   interactivesession allows so that you dont have to explicitly pass
   session object.

variables

   variables in tensorflow are managed by the session. they persist
   between sessions which are useful because tensor and operation objects
   are immutable. variables can be created by tf.variable().
tensorflow_var = tf.variable(1, name="my_variable")

   most of the time you will want to create these variables as tensors of
   zeros, ones or random values:
     * tf.zeros()         creates a matrix full of zeros
     * tf.ones()         creates a matrix full of ones
     * tf.random_normal()         a matrix with random uniform values between an
       interval
     * tf.random_uniform()         random normally distributed numbers
     * tf.truncated_normal()         same as random normal but doesn   t include
       any numbers more than 2 standard deviations.

   these functions take an inital shape parameter where the dimension of
   the matrix is defined. for example:
# 4x4x4 matrix normally distribued mean 0 std 1
normal = tf.truncated_normal([4, 4, 4], mean=0.0, stddev=1.0)

   to have your variable set to one of these matrix helper functions:
normal_var = tf.variable(tf.truncated_normal([4,4,4] , mean=0.0, stddev=1.0)

   to have these variables initialized you must use tensorflow   s variable
   initialization function then pass it to the session. this way when
   multiple sessions are ran the variables are the same.
init = tf.initialize_all_variables()
sess = tf.session()
sess.run(init)

   if you   d like to completely change the value of a variable you can use
   variable.assign() operation, this must be run in a session update the
   value.
initial_var = tf.variable(1)
changed_var = initial_var.assign(initial_var + initial_var)
init = tf.initialize_all_variables()
sess = tf.session()
sess.run(init)
sess.run(changed_var)
# 2
sess.run(changed_var)
# 3
sess.run(changed_var)
# 4
# .... and so on

   sometimes you would like to add a counter inside your model this is
   where you can do a variable.assign_add() method which takes a numeric
   parameter and increments it by the parameter. similarily there is
   variable.assign_sub().
counter = tf.variable(0)
sess.run(counter.assign_add(1))
# 1
sess.run(counter.assign_sub(1))
# -1

scope

   to control the complexity of models and make them easier to break down
   into individual pieces tensorflow has scopes. scopes are very simple
   and even help break down your model when using tensorboard (which will
   be covered in part 2). scopes can even be nested inside of other
   scopes.
with tf.name_scope("scope1"):
    with tf.name_scope("scope_nested"):
        nested_var = tf.mul(5, 5)

   scopes may not seem that powerful right now but used in collaboration
   with tensorboard and they   re very useful.

conclusion

   i   ve demonstrated many of the building blocks that tensorflow offers.
   these individual pieces added together can create very complicated
   models. there is much more that tensorflow offers, if there are any
   requests for features in upcoming parts let me know.
     __________________________________________________________________

   also published on [14]medium.
   it's only fair to share...[15] share on facebook [16]share on google+
   [17]tweet about this on twitter [18]share on linkedin [19]share on
   reddit

   posted by[20]camron[21]august 22, 2016february 21, 2019posted
   in[22]machine learningtags: [23]deep learning, [24]tensorflow

post navigation

   [25]previous post previous post:
   creating a search engine
   [26]next post next post:
   tensorflow in a nutshell         part two: hybrid learning

   [27]camron's blog, [28]proudly powered by wordpress.

references

   1. http://camron.xyz/index.php/feed/
   2. http://camron.xyz/index.php/2016/08/19/creating-a-search-engine/
   3. http://camron.xyz/index.php/2016/09/13/hybrid_learning/
   4. http://camron.xyz/index.php/wp-json/oembed/1.0/embed?url=http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/
   5. http://camron.xyz/index.php/wp-json/oembed/1.0/embed?url=http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/&format=xml
   6. http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/#content
   7. http://camron.xyz/
   8. http://camron.xyz/index.php/about-me/
   9. http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/
  10. http://camron.xyz/index.php/author/camron/
  11. http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/
  12. https://gist.github.com/c0cky/d01a5304946e081e18f674edb569a959
  13. https://gist.github.com/c0cky/9b2c4bec649c768401b09bbc07467067
  14. https://medium.com/@camrongodbout/tensorflow-in-a-nutshell-part-one-basics-3f4403709c9d
  15. http://www.facebook.com/sharer.php?u=http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/
  16. https://plus.google.com/share?url=http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/
  17. http://twitter.com/share?url=http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/&text=tensorflow+in+a+nutshell         part+one:+basics+
  18. http://www.linkedin.com/sharearticle?mini=true&url=http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/
  19. http://reddit.com/submit?url=http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/&title=tensorflow in a nutshell         part one: basics
  20. http://camron.xyz/index.php/author/camron/
  21. http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/
  22. http://camron.xyz/index.php/category/machine-learning/
  23. http://camron.xyz/index.php/tag/deep_learning/
  24. http://camron.xyz/index.php/tag/tensorflow/
  25. http://camron.xyz/index.php/2016/08/19/creating-a-search-engine/
  26. http://camron.xyz/index.php/2016/09/13/hybrid_learning/
  27. http://camron.xyz/
  28. https://wordpress.org/
