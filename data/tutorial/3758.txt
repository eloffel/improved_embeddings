   #[1]camron's blog    feed [2]tensorflow in a nutshell         part one: basics
   [3]sales automation through a deep learning platform [4]alternate
   [5]alternate

   [6]skip to content

   [7]camron's blog

   nlp and deep learning enthusiast

     * [8]about

     * [9]more
          + back

tensorflow in a nutshell         part two: hybrid learning

   posted by[10]camron [11]september 13, 2016february 21, 2019

tensorflow in a nutshell         part two: hybrid learning

   [1*sud-povcacwkb4qfztomdq.png]

the fast and easy guide to the most popular deep learning framework in
the world.

   make sure to check out [12]part one: basics

   in this article we will demonstrate a wide    n deep network that will
   use wide linear model trained simultaneously with a feed forward
   network for more accurate predictions than some tradition machine
   learning techniques. this hybrid learning method will be used to
   predict survival id203 of titanic passengers.

   these hybrid learning methods are already in production by google in
   the play store for app suggestions. even youtube is using similar
   hybrid learning techniques to suggest videos.

   the code for this article is available [13]here.

wide and deep network

   a wide and deep network combines a linear model with a feed forward
   neural net so that our predictions will have memorization and
   generalization. this type of model can be used for classification and
   regression problems. this allows for less feature engineering with
   relatively accurate predictions. thus, getting the best of both worlds.
   [1*uutpkdr3n0df6rrlnsajea.png]

the data

   we are going to be using the titanic kaggle data to predict whether or
   not the passenger will survive based on certain attributes like name,
   sex, what ticket they had, the fare they paid the cabin they stayed in
   etc. for more information on this data set check out here at
   [14]kaggle.

   first off we   re going to define all of our columns as continuos or
   categorical.

   continuous columns         any numerical value in a continuous range. pretty
   much if it is a numerical representation like money, or age.

   categorical columns         part of a finite set. like male or female, or
   even what country someone is from.
categorical_columns = ["name", "sex", "embarked", "cabin"]
continuous_columns = ["age", "sibsp", "parch", "fare", "passengerid", "pclass"]

   since we are only looking to see if a person survived, this is a binary
   classification problem. we predict a 1 if that person survives and a 0   
   if they do not      , we then create a column solely for our survived
   category.
survived_column = "survived"

the network

   now we can get to creating the columns and adding embedding layers.
   when we build our model were going to want to change our categorical
   columns into a sparse column. for our columns with a small set of
   categories such as sex or embarked (s, q, or c) we will transform them
   into sparse columns with keys
sex = tf.contrib.layers.sparse_column_with_keys(column_name="sex",
                                                     keys=["female",
                                                 "male"])
  embarked = tf.contrib.layers.sparse_column_with_keys(column_name="embarked",
                                                   keys=["c",
                                                         "s",
                                                         "q"])

   the other categorical columns have many more options than we want to
   put keys, and since we don   t have a vocab file to map all of the
   possible categories into an integer we will hash them.
cabin = tf.contrib.layers.sparse_column_with_hash_bucket(
      "cabin", hash_bucket_size=1000)
name = tf.contrib.layers.sparse_column_with_hash_bucket(
      "name", hash_bucket_size=1000)

   our continuous columns we want to use their real value. the reason
   passengerid is in continuous and not categorical is because they   re not
   in string format and they   re already an integer id.
age = tf.contrib.layers.real_valued_column("age")
passenger_id = tf.contrib.layers.real_valued_column("passengerid")
sib_sp = tf.contrib.layers.real_valued_column("sibsp")
parch = tf.contrib.layers.real_valued_column("parch")
fare = tf.contrib.layers.real_valued_column("fare")
p_class = tf.contrib.layers.real_valued_column("pclass")

   we are going to bucket the ages. bucketization allows us to find the
   survival correlation by certain age groups and not by all the ages as a
   whole, thus increasing our accuracy.
age_buckets = tf.contrib.layers.bucketized_column(age,
                                                    boundaries=[
                                                        5, 18, 25,
                                                        30, 35, 40,
                                                        45, 50, 55,
                                                        65
                                                    ])

   almost done, we are going to define our wide columns and our deep
   columns. our wide columns are going to effectively memorize
   interactions between our features. our wide columns don   t generalize
   our features, this is why we have our deep columns.
wide_columns = [sex, embarked, p_class, cabin, name, age_buckets,
                  tf.contrib.layers.crossed_column([p_class, cabin],
                                                   hash_bucket_size=int(1e4)),
                  tf.contrib.layers.crossed_column(
                      [age_buckets, sex],
                      hash_bucket_size=int(1e6)),
                  tf.contrib.layers.crossed_column([embarked, name],
                                                   hash_bucket_size=int(1e4))]

   the benefit of having these deep columns is that it takes our sparse
   high dimension features and reduces them into low dimensions.
deep_columns = [
      tf.contrib.layers.embedding_column(sex, dimension=8),
      tf.contrib.layers.embedding_column(embarked, dimension=8),
      tf.contrib.layers.embedding_column(p_class,
                                         dimension=8),
      tf.contrib.layers.embedding_column(cabin, dimension=8),
      tf.contrib.layers.embedding_column(name, dimension=8),
      age,
      passenger_id,
      sib_sp,
      parch,
      fare,
  ]

   we finish off our function by creating our classifier with our deep
   columns and wide columns,
return tf.contrib.learn.dnnlinearcombinedclassifier(
        linear_feature_columns=wide_columns,
        dnn_feature_columns=deep_columns,
        dnn_hidden_units=[100, 50])

   the last thing we will have to do before running the network is create
   mappings for our continuous and categorical columns. what we are doing
   here by creating this function, and this is standard throughout the
   tensorflow learning code, is creating an input function for our
   dataframe. this converts our dataframe into something that tensorflow
   can manipulate. the benefit of this is that we can change and tweak how
   our tensors are being created. if we wanted we could pass feature
   columns into .fit .feature .predict as an individually created column
   like we have above with our features, but this is a much cleaner
   solution.
def input_fn(df, train=false):
  """input builder function."""
  # creates a dictionary mapping from each continuous feature column name (k) to
  # the values of that column stored in a constant tensor.
  continuous_cols = {k: tf.constant(df[k].values) for k in continuous_columns}
  # creates a dictionary mapping from each categorical feature column name (k)
  # to the values of that column stored in a tf.sparsetensor.
  categorical_cols = {k: tf.sparsetensor(
    indices=[[i, 0] for i in range(df[k].size)],
    values=df[k].values,
    shape=[df[k].size, 1])
                      for k in categorical_columns}
  # merges the two dictionaries into one.
  feature_cols = dict(continuous_cols)
  feature_cols.update(categorical_cols)
  # converts the label column into a constant tensor.
  if train:
    label = tf.constant(df[survived_column].values)
      # returns the feature columns and the label.
    return feature_cols, label
  else:
    # so we can predict our results that don't exist in the csv
    return feature_cols

   now after all this we can write our training function
def train_and_eval():
  """train and evaluate the model."""
  df_train = pd.read_csv(
      tf.gfile.open("./train.csv"),
      skipinitialspace=true)
  df_test = pd.read_csv(
      tf.gfile.open("./test.csv"),
      skipinitialspace=true)
  model_dir = "./models"
  print("model directory = %s" % model_dir)
  m = build_estimator(model_dir)
  m.fit(input_fn=lambda: input_fn(df_train, true), steps=200)
  print m.predict(input_fn=lambda: input_fn(df_test))
  results = m.evaluate(input_fn=lambda: input_fn(df_train, true), steps=1)
  for key in sorted(results):
    print("%s: %s" % (key, results[key]))

   we read in our csv files that were preprocessed, like effectively
   imputed missing values, for simplicity sake. details on how the files
   were preprocessed along with the code are contained in the repo.

   these csv   s are converted to tensors using our input_fn by lambda. we
   build our estimator then we print our predictions and print out our
   evaluation results.

results

   [1*wp9rh1bvpnjyzw9-uydhwg.png] network results

   running our code as is gives us reasonably good results with out adding
   any extra columns or doing any great acts of feature engineering. with
   very little fine tuning this model can be used to achieve relatively
   good results.
   [1*cvoes2yr1puyxkwt69nmlw.png]

   the ability of adding an embedding layer along with tradition wide
   linear models allows for accurate predictions by reducing sparse
   dimensionality down to low dimensionality.

conclusion

   this part deviates from traditional deep learning to illustrate the
   many uses and applications of tensorflow. this article is heavily based
   on the paper and code provided by google for wide and deep learning.
   the research paper can be found [15]here. google uses this model as a
   product recommendation engine for the google play store and has helped
   them increase sales on app suggestions. youtube has also released a
   paper about their id126 using hybrid learning as well
   available [16]here. these models are starting to be more prevalent for
   recommendation by various companies and will likely continue to be for
   their embedding ability.
   it's only fair to share...[17] share on facebook [18]share on google+
   [19]tweet about this on twitter [20]share on linkedin [21]share on
   reddit

   posted by[22]camron[23]september 13, 2016february 21, 2019posted
   in[24]machine learning

post navigation

   [25]previous post previous post:
   tensorflow in a nutshell         part one: basics
   [26]next post next post:
   sales automation through a deep learning platform

   [27]camron's blog, [28]proudly powered by wordpress.

references

   1. http://camron.xyz/index.php/feed/
   2. http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/
   3. http://camron.xyz/index.php/2016/09/22/sales-automation-through-a-deep-learning-platform/
   4. http://camron.xyz/index.php/wp-json/oembed/1.0/embed?url=http://camron.xyz/index.php/2016/09/13/hybrid_learning/
   5. http://camron.xyz/index.php/wp-json/oembed/1.0/embed?url=http://camron.xyz/index.php/2016/09/13/hybrid_learning/&format=xml
   6. http://camron.xyz/index.php/2016/09/13/hybrid_learning/#content
   7. http://camron.xyz/
   8. http://camron.xyz/index.php/about-me/
   9. http://camron.xyz/index.php/2016/09/13/hybrid_learning/
  10. http://camron.xyz/index.php/author/camron/
  11. http://camron.xyz/index.php/2016/09/13/hybrid_learning/
  12. http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/
  13. https://github.com/c0cky/tensorflow-in-a-nutshell/tree/master/part2
  14. https://www.kaggle.com/c/titanic/data
  15. https://arxiv.org/abs/1606.07792
  16. https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf
  17. http://www.facebook.com/sharer.php?u=http://camron.xyz/index.php/2016/09/13/hybrid_learning/
  18. https://plus.google.com/share?url=http://camron.xyz/index.php/2016/09/13/hybrid_learning/
  19. http://twitter.com/share?url=http://camron.xyz/index.php/2016/09/13/hybrid_learning/&text=tensorflow+in+a+nutshell         part+two:+hybrid  learning+
  20. http://www.linkedin.com/sharearticle?mini=true&url=http://camron.xyz/index.php/2016/09/13/hybrid_learning/
  21. http://reddit.com/submit?url=http://camron.xyz/index.php/2016/09/13/hybrid_learning/&title=tensorflow in a nutshell         part two: hybrid  learning
  22. http://camron.xyz/index.php/author/camron/
  23. http://camron.xyz/index.php/2016/09/13/hybrid_learning/
  24. http://camron.xyz/index.php/category/machine-learning/
  25. http://camron.xyz/index.php/2016/08/22/in_a_nutshell_part_one/
  26. http://camron.xyz/index.php/2016/09/22/sales-automation-through-a-deep-learning-platform/
  27. http://camron.xyz/
  28. https://wordpress.org/
