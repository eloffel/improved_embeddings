   #[1]github [2]recent commits to recurrenthighwaynetworks:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]22
     * [35]star [36]400
     * [37]fork [38]76

[39]jzilly/[40]recurrenthighwaynetworks

   [41]code [42]issues 2 [43]pull requests 1 [44]projects 0 [45]insights
   [46]permalink
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [47]sign up
   branch: master
   [48]recurrenthighwaynetworks/readme.md
   [49]find file copy path
   fetching contributors   
   cannot retrieve contributors at this time
   203 lines (156 sloc) 9.74 kb
   [50]raw [51]blame [52]history
   (button) (button)

recurrent id199

   note: for tensorflow 1.0, please use [53]this branch.

   what?

   this repository contains code accompanying the paper [54]recurrent
   id199 (rhns). rhns are an extension of [55]long short term
   memory networks with [56]forget gates to enable the learning of deep
   recurrent state transitions. we provide implementations in tensorflow,
   torch7 and brainstorm libraries, and welcome additional implementations
   from the community.

   why?

   the recurrent state transition in typical recurrent networks is modeled
   with a single step non-linear function. this can be very inefficient in
   principle for modeling complicated transitions, requiring very large
   networks. increased recurrence depth allows rhns to model complex
   transitions more efficiently achieving substantially improved results.

   moreover, using depth d in the recurrent state transition is much more
   powerful than stacking d recurrent layers. the figures below illustrate
   that if we consider the functions mapping one hidden state to another t
   time steps apart, its maximum depth scales as the product of d and t
   instead of the sum. of course, in general rhns can also be stacked to
   get the best of both worlds.
     stacked id56     deep transition id56
   [57]stacked id56 [58]deep transition id56

rhn benchmarks reproducible with the provided code

influence of recurrence depth on performance

   the score (perplexity) of word-level language models on the penn
   treebank dataset dramatically improves as recurrence depth increases
   while keeping the model size fixed. wt refers to tying the input and
   output weights for id173. this idea was independently
   developed by [59]inan and khosravi and [60]press and wolf. recently,
   inan et al. also posted a more detailed follow-up [61]paper.
   rec. depth #units/layer best validation test best validation (wt) test
   (wt)
   1 1275 92.4 89.2 93.2 90.6
   2 1180 79.0 76.3 76.9 75.1
   3 1110 75.0 72.6 72.7 70.6
   4 1050 73.3 70.9 70.8 68.6
   5 1000 72.0 69.8 69.7 67.7
   6 960 71.9 69.3 69.1 66.6
   7 920 71.7 68.7 68.7 66.4
   8 890 71.2 68.5 68.2 66.1
   9 860 71.3 68.5 68.1 66.0
   10 830 71.3 68.3 68.3 66.0

comparison to sota language models on id32

   network size best validation test
   [62]lstm+dropout 66 m 82.2 78.4
   [63]variational lstm 66 m 77.3 75.0
   [64]variational lstm with mc dropout 66 m - 73.4
   [65]variational lstm + wt 51 m 75.8 73.2
   [66]pointer sentinel lstm 21 m 72.4 70.9
   [67]ensemble of 38 large lstms+dropout 66 m per lstm 71.9 68.7
   [68]ensemble of 10 large variational lstms 66 m per lstm - 68.7
   [69]variational rhn (depth=8) 32 m 71.2 68.5
   [70]variational rhn + wt (depth=10) 23 m 67.9 65.4
   [71]variational rhn + wt with mc dropout (depth=5)* 22 m - 64.4

   *we used 1000 samples for mc dropout as done by gal for lstms, but
   we've only evaluated the depth 5 model so far.

wikipedia (enwik8) next character prediction modeling

              network            network size test bpc
           [72]grid-lstm            16.8 m      1.47
            [73]mi-lstm              17 m       1.44
             [74]mlstm               21 m       1.42
    [75]layernorm hypernetworks      27 m       1.34
       [76]layernorm hm-lstm         35 m       1.32
      [77]rhn - rec. depth 5         23 m       1.31
      [78]rhn - rec. depth 10        21 m       1.30
   [79]large rhn - rec. depth 10     46 m       1.27

wikipedia (text8) next character prediction modeling

              network            network size test bpc
            [80]mi-lstm              17 m       1.44
             [81]mlstm               10 m       1.40
            [82]bn lstm              16 m       1.36
            [83]hm-lstm              35 m       1.32
       [84]layernorm hm-lstm         35 m       1.29
      [85]rhn - rec. depth 10        20 m       1.29
   [86]large rhn - rec. depth 10     45 m       1.27

code

tensorflow

   tensorflow code for rhns is built by heavily extending the lstm
   id38 example provided in tensorflow. it supports
   variational rhns as used in the paper, which use the same dropout mask
   at each time step and at all layers inside the recurrence. note that
   this implementation uses the same dropout mask for both the h and t
   non-linear transforms in rhns while the torch7 implementation uses
   different dropout masks for different transformations. the theano
   implementation can be configured either way.

requirements

   we recommend [87]installing tensorflow in a virtual environment. in
   addition to the usual tensorflow dependencies, the code uses [88]sacred
   so you need to do:
$ pip install sacred

usage

   to reproduce sota results on id32:
$ python rhn_train.py with ptb_sota

   to reproduce sota results on enwik8/text8 (wikipedia), first download
   the dataset from [89]http://mattmahoney.net/dc/enwik8.zip or for text8
   [90]http://mattmahoney.net/dc/text8.zip and unzip it into the data
   directory, then run:
$ python rhn_train.py with enwik8_sota

   or
$ python rhn_train.py with text8_sota

   change some hyperparameters and run:
$ python rhn_train.py with ptb_sota depth=20

   this is a sacred experiment, so you check the hyperparameter options
   using the print_config command, e.g.
$ python rhn_train.py print_config with ptb_sota

torch7

   torch7 code is based on yarin gal's [91]adaptation of wojciech
   zaremba's [92]code implementing variational dropout. the main additions
   to gal's code are the recurrent highway network layer, the initial
   biasing of t-gate activations to facilitate learning and a few
   adjustments to other network parameters such as id56_size and dropout
   probabilities.

requirements

   we recommend installing torch from the [93]official website. to ensure
   the code runs some packages need to be installed:
$ luarocks install nngraph
$ luarocks install cutorch
$ luarocks install nn
$ luarocks install hdf5

usage

$ th torch_rhn_ptb.lua

   to run on the enwik8 dataset, first download and prepare the data (see
   [94]data/readme for details):
$ cd data
$ python create_enwik8.py

   then you can train by running:
$ th toch_rhn_enwik8.lua

theano

   the theano code's configuration and usage is similar to that of the
   tensorflow code. in this implementation two configuration options were
   added:
     * whether the same dropout masks are used for both the h and t
       non-linear transforms.
     * how all biases other than t's bias are initialized: randomly (as in
       the tensorflow implementation) or with zeros (or any other fixed
       value).

   the following isn't implemented:
     * mc dropout

requirements

   [95]theano and [96]sacred.

usage

   as with the tensorflow code, the sota results on id32 and on
   enwik8 (wikipedia) can be reproduced:
$ python theano_rhn_train.py with ptb_sota

$ python theano_rhn_train.py with enwik8_sota

brainstorm

   an rhn layer implementation is also provided in [97]brainstorm. this
   implementation does not use variational dropout. it can be used in a
   brainstorm experiment by simply importing highwayid56coupledgates from
   [98]brainstorm_rhn.py.

citation

   if you use rhns in your work, please cite us:
@article{zilly2016recurrent,
  title="{recurrent id199}",
  author={zilly, julian georg and srivastava, rupesh kumar and koutn{\'\i}k, jan
 and schmidhuber, j{\"u}rgen},
  journal={arxiv preprint arxiv:1607.03474},
  year={2016}
}

license

   mit license.
   ____________________ (button) go

     *    2019 github, inc.
     * [99]terms
     * [100]privacy
     * [101]security
     * [102]status
     * [103]help

     * [104]contact github
     * [105]pricing
     * [106]api
     * [107]training
     * [108]blog
     * [109]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [110]reload to refresh your
   session. you signed out in another tab or window. [111]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/jzilly/recurrenthighwaynetworks/commits/master.atom
   3. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/jzilly/recurrenthighwaynetworks/blob/master/readme.md
  32. https://github.com/join
  33. https://github.com/login?return_to=/jzilly/recurrenthighwaynetworks
  34. https://github.com/jzilly/recurrenthighwaynetworks/watchers
  35. https://github.com/login?return_to=/jzilly/recurrenthighwaynetworks
  36. https://github.com/jzilly/recurrenthighwaynetworks/stargazers
  37. https://github.com/login?return_to=/jzilly/recurrenthighwaynetworks
  38. https://github.com/jzilly/recurrenthighwaynetworks/network/members
  39. https://github.com/jzilly
  40. https://github.com/jzilly/recurrenthighwaynetworks
  41. https://github.com/jzilly/recurrenthighwaynetworks
  42. https://github.com/jzilly/recurrenthighwaynetworks/issues
  43. https://github.com/jzilly/recurrenthighwaynetworks/pulls
  44. https://github.com/jzilly/recurrenthighwaynetworks/projects
  45. https://github.com/jzilly/recurrenthighwaynetworks/pulse
  46. https://github.com/jzilly/recurrenthighwaynetworks/blob/9ab03a36ed548fe4267f993e47494e417cd4bdab/readme.md
  47. https://github.com/join?source=prompt-blob-show
  48. https://github.com/jzilly/recurrenthighwaynetworks
  49. https://github.com/jzilly/recurrenthighwaynetworks/find/master
  50. https://github.com/jzilly/recurrenthighwaynetworks/raw/master/readme.md
  51. https://github.com/jzilly/recurrenthighwaynetworks/blame/master/readme.md
  52. https://github.com/jzilly/recurrenthighwaynetworks/commits/master/readme.md
  53. https://github.com/julian121266/recurrenthighwaynetworks/tree/tensorflow_1.0_branch
  54. https://arxiv.org/abs/1607.03474
  55. http://bioinf.jku.at/publications/older/2604.pdf
  56. https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf
  57. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/images/arch_stacked.png
  58. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/images/arch_dt.png
  59. https://cs224d.stanford.edu/reports/inankhosravi.pdf
  60. https://arxiv.org/abs/1608.05859
  61. https://arxiv.org/abs/1611.01462
  62. https://arxiv.org/abs/1409.2329
  63. http://arxiv.org/abs/1512.05287
  64. http://arxiv.org/abs/1512.05287
  65. http://arxiv.org/abs/1608.05859
  66. https://arxiv.org/abs/1609.07843
  67. https://arxiv.org/abs/1409.2329
  68. https://arxiv.org/abs/1409.2329
  69. https://arxiv.org/abs/1607.03474
  70. https://arxiv.org/abs/1607.03474
  71. https://arxiv.org/abs/1607.03474
  72. http://arxiv.org/abs/1507.01526
  73. https://arxiv.org/abs/1606.06630
  74. https://arxiv.org/abs/1609.07959
  75. https://arxiv.org/abs/1609.09106
  76. http://128.84.21.199/abs/1609.01704
  77. https://arxiv.org/abs/1607.03474
  78. https://arxiv.org/abs/1607.03474
  79. https://arxiv.org/abs/1607.03474
  80. https://arxiv.org/abs/1606.06630
  81. https://arxiv.org/abs/1609.07959
  82. https://arxiv.org/abs/1609.09106
  83. http://128.84.21.199/abs/1609.01704
  84. http://128.84.21.199/abs/1609.01704
  85. https://arxiv.org/abs/1607.03474
  86. https://arxiv.org/abs/1607.03474
  87. https://www.tensorflow.org/versions/master/get_started/os_setup.html#virtualenv-installation
  88. https://github.com/idsia/sacred
  89. http://mattmahoney.net/dc/enwik8.zip
  90. http://mattmahoney.net/dc/text8.zip
  91. https://github.com/yaringal/bayesianid56
  92. https://github.com/wojzaremba/lstm
  93. http://torch.ch/docs/getting-started.html#_
  94. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/data/readme.md
  95. http://deeplearning.net/software/theano/install.html
  96. https://github.com/idsia/sacred
  97. https://github.com/idsia/brainstorm
  98. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/brainstorm_rhn.py
  99. https://github.com/site/terms
 100. https://github.com/site/privacy
 101. https://github.com/security
 102. https://githubstatus.com/
 103. https://help.github.com/
 104. https://github.com/contact
 105. https://github.com/pricing
 106. https://developer.github.com/
 107. https://training.github.com/
 108. https://github.blog/
 109. https://github.com/about
 110. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md
 111. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md

   hidden links:
 113. https://github.com/
 114. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md
 115. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md
 116. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md
 117. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#recurrent-highway-networks
 118. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#rhn-benchmarks-reproducible-with-the-provided-code
 119. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#influence-of-recurrence-depth-on-performance
 120. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#comparison-to-sota-language-models-on-penn-treebank
 121. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#wikipedia-enwik8-next-character-prediction-modeling
 122. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#wikipedia-text8-next-character-prediction-modeling
 123. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#code
 124. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#tensorflow
 125. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#requirements
 126. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#usage
 127. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#torch7
 128. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#requirements-1
 129. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#usage-1
 130. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#theano
 131. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#requirements-2
 132. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#usage-2
 133. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#brainstorm
 134. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#citation
 135. https://github.com/jzilly/recurrenthighwaynetworks/blob/master/readme.md#license
 136. https://github.com/
