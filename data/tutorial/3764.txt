   #[1]posts on 'revolutions' (atom) [2]posts on 'revolutions' (rss 1.0)
   [3]posts on 'revolutions' (rss 2.0) [4]comments on 'deep learning part
   2: id21 and fine-tuning deep convolutional neural
   networks' (atom) [5]comments on 'deep learning part 2: transfer
   learning and fine-tuning deep convolutional neural networks' (rss 2.0)
   [6]home [7]edward tufte keynote presenter at data science summit, sep
   26-27 [8]r with power bi: import, transform, visualize and share

[9]revolutions

daily news about using open source r for big data analysis, predictive
modeling, data science, and visualization since 2008

   [10]   edward tufte keynote presenter at data science summit, sep 26-27
   | [11]main | [12]r with power bi: import, transform, visualize and
   share   

august 24, 2016

deep learning part 2: id21 and fine-tuning deep convolutional
neural networks

   by anusua trivedi, microsoft data scientist

   this is a blog series in several parts     where i describe my
   experiences and go deep into the reasons behind my choices. in [13]part
   1, i discussed the pros and cons of different symbolic frameworks, and
   my reasons for choosing theano (with lasagne) as my platform of choice.

   part 2 of this blog series is based on my upcoming talk at [14]the data
   science conference, 2016. here in part 2, i describe deep convolutional
   neural networks (did98s) and how id21 and fine-tuning helps
   better the training process for domain specific images.

   please feel free to email me at trivedianusua23@gmail.com if you have
   questions.

introduction

   the eye disease diabetic retinopathy (dr) is a common cause of vision
   loss. screening diabetic patients using fluorescein angiography images
   can potentially reduce the risk of blindness. current trends in the
   research have demonstrated that did98s are very effective in
   automatically analyzing large collections of images and identifying
   features that can categorize images with minimum error. did98s are
   rarely trained from scratch, as it is relatively uncommon to have a
   domain-specific dataset of sufficient size. since modern did98s take 2-3
   weeks to train across gpus, berkley vision and learning center (bvlc)
   have released some final did98 checkpoints. in this blog, we use such a
   pre-trained network: googlenet. this googlenet network is pre-trained
   on a large collection of natural id163 images. we transfer the
   learned id163 weights as initial weights for the network, and
   fine-tune these pre-trained generic network to recognize fluorescein
   angiography images of eyes and improve dr prediction.

using explicit feature extraction to predict diabetic retinopathy

   much work has been done in developing algorithms and morphological
   image processing techniques that explicitly extract features prevalent
   in patients with dr. the generic workflow used in a standard image
   classification technique is as follows:
     * image preprocessing techniques for noise removal and contrast
       enhancement
     * feature extraction technique
     * classification
     * prediction

   [15]faust et al. provide a very comprehensive analysis of models that
   use explicit feature extraction for dr screening. [16]vujosevic et al.
   build a binary classifier on a dataset of 55 patients by explicitly
   forming single lesion features. [17]these authors use morphological
   image processing techniques to extract blood vessel, and hemorrhage
   features and then train an id166 on a data set of 331 images. [18]these
   authors report accuracy of 90% and sensitivity of 90% on binary
   classification task with a dataset of 140 images.

   however, all these processes are very time and effort consuming.
   further improvements in prediction accuracy require large quantities of
   labeled data. image processing and feature extraction of image datasets
   is very complex and time-consuming. thus, we choose to automate the
   image processing and feature extraction step by using did98s.

deep convolutional neural network (did98)

   image data requires subject-matter expertise to extract key features.
   did98s extract features automatically from domain-specific images,
   without any feature engineering techniques. this process makes did98s
   suitable for image analysis:
     * did98s train networks with many layers
     * multiple layers work to build an improved feature space
     * initial layers learn 1st order features (e.g. color, edges etc.)
     * later layers learn higher order features (specific to input
       dataset)
     * lastly, final layer features are fed into classification layer(s)


   [19]figure 1
   c layers are convolutions, s layers are pool/sample



   convolution: convolution layers consist of a rectangular grid of
   neurons. the weights for this are the same for each neuron in the
   convolution layer. the convolution layer weights specify the
   convolution filter.


   [20]2
   convolution



   pooling: the pooling layer takes small rectangular blocks from the
   convolutional layer and subsamples it to produce a single output from
   that block.


   [21]3
   pooling



   in this post, we are using googlenet did98, which was developed at
   google. googlenet won the id163 challenge in 2014, setting the
   record for the best contemporaneous results. motivations for this model
   were a simultaneously deeper as well as computationally inexpensive
   architecture.


   [22]4
   googlenet



id21 and fine-tuning did98s

   in practice, we don   t usually train an entire did98 from scratch with
   random initialization. this is because it is relatively rare to have a
   dataset of sufficient size that is required for the depth of network
   required. instead, it is common to pre-train a did98 on a very large
   dataset and then use the trained did98 weights either as an
   initialization or a fixed feature extractor for the task of interest.

   fine-tuning: id21 strategies depend on various factors,
   but the two most important ones are the size of the new dataset, and
   its similarity to the original dataset. keeping in mind that did98
   features are more generic in early layers and more dataset-specific in
   later layers, there are four major scenarios:
    1. new dataset is smaller in size and similar in content compared to
       original dataset: if the data is small, it is not a good idea to
       fine-tune the did98 due to overfitting concerns. since the data is
       similar to the original data, we expect higher-level features in
       the did98 to be relevant to this dataset as well. hence, the best
       idea might be to train a linear classifier on the id98-features.
    2. new dataset is relatively large in size and similar in content
       compared to the original dataset: since we have more data, we can
       have more confidence that we would not over fit if we were to try
       to fine-tune through the full network.
    3. new dataset is smaller in size but very different in content
       compared to the original dataset: since the data is small, it is
       likely best to only train a linear classifier. since the dataset is
       very different, it might not be best to train the classifier from
       the top of the network, which contains more dataset-specific
       features. instead, it might work better to train a classifier from
       activations somewhere earlier in the network.
    4. new dataset is relatively large in size and very different in
       content compared to the original dataset: since the dataset is very
       large, we may expect that we can afford to train a did98 from
       scratch. however, in practice it is very often still beneficial to
       initialize with weights from a pre-trained model. in this case, we
       would have enough data and confidence to fine-tune through the
       entire network.

   fine-tuning did98s: for this dr prediction problem, we fall under
   scenario iv. we fine-tune the weights of the pre-trained did98 by
   continuing the id26. it is possible to fine-tune all the
   layers of the did98, or it   s possible to keep some of the earlier layers
   fixed (due to overfitting concerns) and only fine-tune some
   higher-level portion of the network. this is motivated by the
   observation that the earlier features of a did98 contain more generic
   features (e.g. edge detectors or color blob detectors) that should be
   useful to many tasks, but later layers of the did98 becomes
   progressively more specific to the details of the classes contained in
   the dr dataset.

   id21 constraints: as we use a pre-trained network, we are
   slightly constrained in terms of the model architecture. for example,
   we can   t arbitrarily take out convolutional layers from the pre-trained
   network. however, due to parameter sharing, we can easily run a
   pre-trained network on images of different spatial size. this is
   clearly evident in the case of convolutional and pool layers because
   their forward function is independent of the input volume spatial size.
   in case of fully connected (fc) layers, this still holds true because
   fc layers can be converted to a convolutional layer.

   learning rates: we use a smaller learning rate for did98 weights that
   are being fine-tuned under the assumption that the pre-trained did98
   weights are relatively good. we don   t wish to distort them too quickly
   or too much, so we keep both our learning rate and learning rate decay
   really small.

   data augmentation: one of the drawbacks of non-regularized neural
   networks is that they are extremely flexible: they learn both features
   and noise equally well, increasing the potential for overfitting. in
   our model, we apply l2 id173 to avoid overfitting. but even
   after that, we observed a large gap in model performance on the
   training and validation dr images, indicating that the fine tuning
   process is overfitting to the training set. to combat this overfitting,
   we leverage data augmentation for the dr image dataset.

   there are many ways to do data augmentation, such as the popular
   horizontally flipping, random crops and color jittering. as the color
   information in these images is very important, we only rotate the
   images at different angles     at 0, 90, 180, and 270 degrees.
   [23]figure 5
   replacing the input layer of the pre-trained googlenet network with dr
   images. we fine-tune all layers except for the top 2 pre-trained layers
   which contains more generic data-independent weights.

   fine-tuning googlenet: the googlenet network we use here for dr
   screening was initially trained on id163. the id163 dataset
   contains about 1 million natural images and 1000 labels/categories. in
   contrast, our labeled dr dataset has only about 30,000 domain-specific
   images and 4 labels/ categories. thus, the dr dataset is insufficient
   to train a network as complex as googlenet and so we use weights from
   the id163-trained googlenet network. we fine-tune all layers, except
   for the top 2 pre-trained layers which contains more generic
   data-independent weights. the original classification layer
   "loss3/classifier" outputs predictions for 1000 classes. we replace it
   with a new binary layer.
   [24]figure 6
   fine-tuning googlenet

conclusion

   fine-tuning allows us to bring the power of state-of-the-art did98
   models to new domains where insufficient data and time/cost constraints
   might otherwise prevent their use. this approach achieves a significant
   improvement of average accuracy and improves the state-of-the-art of
   image-based medical classification.

   in my part 3 of this blog series (coming soon), i will explain
   re-usability of these trained did98 models.

   posted by [25]guest blogger at 11:00 in [26]big data, [27]predictive
   analytics, [28]python | [29]permalink

comments

   [30]feed you can follow this conversation by subscribing to the
   [31]comment feed for this post.

   excellent post anusha. very informative. do you mind i re-post this on
   my platform www.gladwinanalytics.com ?

   posted by: [32]gladwin analytics | [33]august 27, 2016 at 05:31

   glad you liked it - feel free to repost

   posted by: anusua trivedi | [34]august 29, 2016 at 07:04

   great! i'm also trying to fine-tunning the googlenet model, but i'm
   getting overfit as well, can u provide some model example that i can
   see? or any advice? thanks!

   posted by: felipe | [35]september 04, 2016 at 06:51

   felipe - this will not work well on unbalanced data-set. ty and balance
   your data-set. also, data-augmentation helps with over-fitting.

   posted by: anusua trivedi | [36]september 06, 2016 at 20:30

   the comments to this entry are closed.

information

     * [37]about this blog
     * [38]comments policy
     * [39]about categories
     * [40]about the authors
     * [41]local r user group directory
     * [42]tips on starting an r user group

search revolutions blog

     * __________________ search blog

   got comments or suggestions for the blog editor?
   email [43]david smith.
   [44]follow revodavid on twitter follow david on twitter: [45]@revodavid
   get this blog via email with [46]blogtrottr

[47]categories

     * [48]academia
     * [49]advanced tips
     * [50]ai
     * [51]airoundups
     * [52]announcements
     * [53]applications
     * [54]beginner tips
     * [55]big data
     * [56]courses
     * [57]current events
     * [58]data science
     * [59]developer tips
     * [60]events
     * [61]finance
     * [62]government
     * [63]graphics
     * [64]high-performance computing
     * [65]life sciences
     * [66]microsoft
     * [67]open source
     * [68]other industry
     * [69]packages
     * [70]popularity
     * [71]predictive analytics
     * [72]profiles
     * [73]python
     * [74]r
     * [75]r is hot
     * [76]random
     * [77]reviews
     * [78]revolution
     * [79]rmedia
     * [80]roundups
     * [81]sports
     * [82]statistics
     * [83]user groups

   [84]see more

r links

     * [85]r on azure
       developer's guide and documentation
     * [86]find r packages
       cran package directory at mran
     * [87]download microsoft r open
       free, high-performance r
     * [88]r project site
       information about the r project

recommended sites

     * [89]@rlangtip
       daily tips on using r
     * [90]flowingdata
       modern data visualization
     * [91]id203 and statistics blog
       monte carlo simulations in r
     * [92]r bloggers
       daily news and tutorials about r, contributed by r bloggers
       worldwide.
     * [93]r project group on analyticbridge.com
       community and discussion forum
     * [94]statistical modeling, causal id136, and social science
       andrew gelman's statistics blog

[95]archives

     * [96]march 2019
     * [97]february 2019
     * [98]january 2019
     * [99]december 2018
     * [100]november 2018
     * [101]october 2018
     * [102]september 2018
     * [103]august 2018
     * [104]july 2018
     * [105]june 2018

   [106]subscribe to this blog's feed
      

references

   1. https://blog.revolutionanalytics.com/atom.xml
   2. https://blog.revolutionanalytics.com/index.rdf
   3. https://blog.revolutionanalytics.com/rss.xml
   4. https://blog.revolutionanalytics.com/2016/08/deep-learning-part-2/comments/atom.xml
   5. https://blog.revolutionanalytics.com/2016/08/deep-learning-part-2/comments/rss.xml
   6. https://blog.revolutionanalytics.com/
   7. https://blog.revolutionanalytics.com/2016/08/tufte-keynote.html?no_prefetch=1
   8. https://blog.revolutionanalytics.com/2016/08/powerbi-and-r.html?no_prefetch=1
   9. https://blog.revolutionanalytics.com/
  10. https://blog.revolutionanalytics.com/2016/08/tufte-keynote.html
  11. https://blog.revolutionanalytics.com/
  12. https://blog.revolutionanalytics.com/2016/08/powerbi-and-r.html
  13. https://blog.revolutionanalytics.com/2016/08/deep-learning-part-1.html
  14. http://www.thedatascienceconference.com/
  15. https://link.springer.com/article/10.1007/s10916-010-9454-7
  16. https://www.ncbi.nlm.nih.gov/pubmed/19406376
  17. https://www.ncbi.nlm.nih.gov/pubmed/19623908
  18. https://www.ncbi.nlm.nih.gov/pubmed/18461814
  19. http://a4.typepad.com/6a0105360ba1c6970c01b7c88b2a04970b-pi
  20. http://a1.typepad.com/6a0105360ba1c6970c01bb092e86d9970d-pi
  21. http://a6.typepad.com/6a0105360ba1c6970c01bb092e8716970d-pi
  22. http://a1.typepad.com/6a0105360ba1c6970c01b8d214dbc1970c-pi
  23. http://a4.typepad.com/6a0105360ba1c6970c01bb092e8a94970d-pi
  24. http://a0.typepad.com/6a0105360ba1c6970c01bb092e8ab0970d-pi
  25. https://profile.typepad.com/6p01b7c890d130970b
  26. https://blog.revolutionanalytics.com/big-data/
  27. https://blog.revolutionanalytics.com/predictive-analytics/
  28. https://blog.revolutionanalytics.com/python/
  29. https://blog.revolutionanalytics.com/2016/08/deep-learning-part-2.html
  30. https://blog.revolutionanalytics.com/2016/08/deep-learning-part-2/comments/atom.xml
  31. https://blog.revolutionanalytics.com/2016/08/deep-learning-part-2/comments/atom.xml
  32. http://www.gladwinanalytics.com/
  33. https://blog.revolutionanalytics.com/2016/08/deep-learning-part-2.html?cid=6a010534b1db25970b01b8d2163611970c#comment-6a010534b1db25970b01b8d2163611970c
  34. https://blog.revolutionanalytics.com/2016/08/deep-learning-part-2.html?cid=6a010534b1db25970b01bb09308144970d#comment-6a010534b1db25970b01bb09308144970d
  35. https://blog.revolutionanalytics.com/2016/08/deep-learning-part-2.html?cid=6a010534b1db25970b01b8d218feb5970c#comment-6a010534b1db25970b01b8d218feb5970c
  36. https://blog.revolutionanalytics.com/2016/08/deep-learning-part-2.html?cid=6a010534b1db25970b01bb0933b8dc970d#comment-6a010534b1db25970b01bb0933b8dc970d
  37. https://blog.revolutionanalytics.com/about.html
  38. https://blog.revolutionanalytics.com/comments-policy.html
  39. https://blog.revolutionanalytics.com/categories.html
  40. https://blog.revolutionanalytics.com/authors.html
  41. https://blog.revolutionanalytics.com/local-r-groups.html
  42. https://blog.revolutionanalytics.com/tips-on-starting-an-r-user-group.html
  43. mailto:davidsmi@microsoft.com
  44. https://www.twitter.com/revodavid
  45. https://twitter.com/revodavid
  46. https://blogtrottr.com/?subscribe=http://blog.revolutionanalytics.com
  47. https://blog.revolutionanalytics.com/archives.html
  48. https://blog.revolutionanalytics.com/academia/
  49. https://blog.revolutionanalytics.com/advanced-tips/
  50. https://blog.revolutionanalytics.com/ai/
  51. https://blog.revolutionanalytics.com/airoundups/
  52. https://blog.revolutionanalytics.com/announcements/
  53. https://blog.revolutionanalytics.com/applications/
  54. https://blog.revolutionanalytics.com/beginner-tips/
  55. https://blog.revolutionanalytics.com/big-data/
  56. https://blog.revolutionanalytics.com/courses/
  57. https://blog.revolutionanalytics.com/current-events/
  58. https://blog.revolutionanalytics.com/data-science/
  59. https://blog.revolutionanalytics.com/developers/
  60. https://blog.revolutionanalytics.com/events/
  61. https://blog.revolutionanalytics.com/finance/
  62. https://blog.revolutionanalytics.com/government/
  63. https://blog.revolutionanalytics.com/graphics/
  64. https://blog.revolutionanalytics.com/high-performance-computing/
  65. https://blog.revolutionanalytics.com/life-sciences/
  66. https://blog.revolutionanalytics.com/microsoft/
  67. https://blog.revolutionanalytics.com/open-source/
  68. https://blog.revolutionanalytics.com/other-industry/
  69. https://blog.revolutionanalytics.com/packages/
  70. https://blog.revolutionanalytics.com/popularity/
  71. https://blog.revolutionanalytics.com/predictive-analytics/
  72. https://blog.revolutionanalytics.com/profiles/
  73. https://blog.revolutionanalytics.com/python/
  74. https://blog.revolutionanalytics.com/r/
  75. https://blog.revolutionanalytics.com/r-is-hot/
  76. https://blog.revolutionanalytics.com/random/
  77. https://blog.revolutionanalytics.com/reviews/
  78. https://blog.revolutionanalytics.com/revolution/
  79. https://blog.revolutionanalytics.com/rmedia/
  80. https://blog.revolutionanalytics.com/roundups/
  81. https://blog.revolutionanalytics.com/sports-1/
  82. https://blog.revolutionanalytics.com/statistics/
  83. https://blog.revolutionanalytics.com/user-groups/
  84. https://blog.revolutionanalytics.com/archives.html
  85. https://docs.microsoft.com/azure/machine-learning/r-developers-guide?wt.mc_id=revolutionssidebar-blog-davidsmi
  86. https://mran.microsoft.com/packages/
  87. https://mran.microsoft.com/download
  88. https://www.r-project.org/
  89. https://twitter.com/rlangtip
  90. https://flowingdata.com/
  91. http://www.statisticsblog.com/
  92. http://www.r-bloggers.com/
  93. http://www.analyticbridge.com/group/rprojectandotherfreesoftwaretools
  94. http://www.stat.columbia.edu/~cook/movabletype/mlm/
  95. https://blog.revolutionanalytics.com/archives.html
  96. https://blog.revolutionanalytics.com/2019/03/index.html
  97. https://blog.revolutionanalytics.com/2019/02/index.html
  98. https://blog.revolutionanalytics.com/2019/01/index.html
  99. https://blog.revolutionanalytics.com/2018/12/index.html
 100. https://blog.revolutionanalytics.com/2018/11/index.html
 101. https://blog.revolutionanalytics.com/2018/10/index.html
 102. https://blog.revolutionanalytics.com/2018/09/index.html
 103. https://blog.revolutionanalytics.com/2018/08/index.html
 104. https://blog.revolutionanalytics.com/2018/07/index.html
 105. https://blog.revolutionanalytics.com/2018/06/index.html
 106. https://blog.revolutionanalytics.com/atom.xml
