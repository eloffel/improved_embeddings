   #[1]articles from distill

   [2]distill
   [3]about [4]prize [5]submit

               attention and augmented recurrent neural networks

   [6]chris olah [7]google brain
   [8]shan carter [9]google brain
   sept. 8
   2016
   citation:
   olah & carter, 2016

   recurrent neural networks are one of the staples of deep learning,
   allowing neural networks to work with sequences of data like text,
   audio and video. they can be used to boil a sequence down into a
   high-level understanding, to annotate sequences, and even to generate
   new sequences from scratch!

   x0 y0 x1 y1 x2 y2 x3 y3 one cell... can be used over... and over... and
   over... x4 y4 again.

   the basic id56 design struggles with longer sequences, but a special
   variant   [10]   long short-term memory    networks [1]   can even work with
   these. such models have been found to be very powerful, achieving
   remarkable results in many tasks including translation, voice
   recognition, and image captioning. as a result, recurrent neural
   networks have become very widespread in the last few years.

   as this has happened, we   ve seen a growing number of attempts to
   augment id56s with new properties. four directions stand out as
   particularly exciting:
   [11][id56_preview_ntm.svg] id63s have external memory
   that they can read and write to. [12][id56_preview_ai.svg] attentional
   interfaces allow id56s to focus on parts of their input.
   [13][id56_preview_act.svg] adaptive computation time allows for varying
   amounts of computation per step. [14][id56_preview_np.svg] neural
   programmers can call functions, building programs as they run.

   individually, these techniques are all potent extensions of id56s, but
   the really striking thing is that they can be combined, and seem to
   just be points in a broader space. further, they all rely on the same
   underlying trick   something called attention   to work.

   our guess is that these    augmented id56s    will have an important role to
   play in extending deep learning   s capabilities over the coming years.
     __________________________________________________________________

id63s

   id63s [2] combine a id56 with an external memory bank.
   since vectors are the natural language of neural networks, the memory
   is an array of vectors:

   memory is an array of vectors. network a writes and reads from this
   memory each step. x0 y0 x1 y1 x2 y2 x3 y3

   but how does reading and writing work? the challenge is that we want to
   make them differentiable. in particular, we want to make them
   differentiable with respect to the location we read from or write to,
   so that we can learn where to read and write. this is tricky because
   memory addresses seem to be fundamentally discrete. ntms take a very
   clever solution to this: every step, they read and write everywhere,
   just to different extents.

   as an example, let   s focus on reading. instead of specifying a single
   location, the id56 outputs an    attention distribution    that describes
   how we spread out the amount we care about different memory positions.
   as such, the result of the read operation is a weighted sum.

   attention memory the id56 gives an attention distribution which describe
   how we spread out the amount we care about different memory positions.
   the read result is a weighted sum.

   similarly, we write everywhere at once to different extents. again, an
   attention distribution describes how much we write at every location.
   we do this by having the new value of a position in memory be a convex
   combination of the old memory content and the write value, with the
   position between the two decided by the attention weight.

   attention old memory new memory write value the id56 gives an attention
   distribution, describing how much we should change each memory position
   towards the write value. instead of writing to one location, we write
   everywhere, just to different extents.

   but how do ntms decide which positions in memory to focus their
   attention on? they actually use a combination of two different methods:
   content-based attention and location-based attention. content-based
   attention allows ntms to search through their memory and focus on
   places that match what they   re looking for, while location-based
   attention allows relative movement in memory, enabling the ntm to loop.

   first, the controller gives a query vector and each memory entry is
   scored for similarity with the query. the scores are then converted
   into a distribution using softmax. next, we interpolate the attention
   from the previous time step. we convolve the attention with a shift
   filter   this allows the controller to move its focus. finally, we
   sharpen the attention distribution. this final attention distribution
   is fed to the read or write operation. the id56 gives an attention
   distribution, describing how much we should change each memory position
   towards the write value. memory blue shows high similarity, pink high
   dissimilarity. interpolation amount shift filter id56 controller
   attention mechanism query vector attention from previous step new
   attention distribution

   this capability to read and write allows ntms to perform many simple
   algorithms, previously beyond neural networks. for example, they can
   learn to store a long sequence in memory, and then loop over it,
   repeating it back repeatedly. as they do this, we can watch where they
   read and write, to better understand what they   re doing:

   [ntm-copy-readwrite.svg] see more experiments in [3]. this figure is
   based on the repeat copy experiment.

   they can also learn to mimic a lookup table, or even learn to sort
   numbers (although they kind of cheat)! on the other hand, they still
   can   t do many basic things, like add or multiply numbers.

   since the original ntm paper, there have been a number of exciting
   papers exploring similar directions. the neural gpu [4] overcomes the
   ntm   s inability to add and multiply numbers. zaremba & sutskever [5]
   train ntms using id23 instead of the differentiable
   read/writes used by the original. neural random access machines [6]
   work based on pointers. some papers have explored differentiable data
   structures, like stacks and queues [7, 8]. and memory networks [9, 10]
   are another approach to attacking similar problems.

   in some objective sense, many of the tasks these models can
   perform   such as learning how to add numbers   aren   t that objectively
   hard. the traditional program synthesis community would eat them for
   lunch. but neural networks are capable of many other things, and models
   like the id63 seem to have knocked away a very
   profound limit on their abilities.

  code

   there are a number of open source implementations of these models. open
   source implementations of the id63 include [15]taehoon
   kim   s (tensorflow), [16]shawn tan   s (theano), [17]fumin   s (go), [18]kai
   sheng tai   s (torch), and [19]snip   s (lasagne). code for the neural gpu
   publication was open sourced and put in the [20]tensorflow models
   repository. open source implementations of memory networks include
   [21]facebook   s (torch/matlab), [22]yerevann   s (theano), and [23]taehoon
   kim   s (tensorflow).
     __________________________________________________________________

attentional interfaces

   when i   m translating a sentence, i pay special attention to the word
   i   m presently translating. when i   m transcribing an audio recording, i
   listen carefully to the segment i   m actively writing down. and if you
   ask me to describe the room i   m sitting in, i   ll glance around at the
   objects i   m describing as i do so.

   neural networks can achieve this same behavior using attention,
   focusing on part of a subset of the information they   re given. for
   example, an id56 can attend over the output of another id56. at every
   time step, it focuses on different positions in the other id56.

   we   d like attention to be differentiable, so that we can learn where to
   focus. to do this, we use the same trick id63s use: we
   focus everywhere, just to different extents.

   network b focuses on different information from network a at every
   step.

   the attention distribution is usually generated with content-based
   attention. the attending id56 generates a query describing what it wants
   to focus on. each item is dot-producted with the query to produce a
   score, describing how well it matches the query. the scores are fed
   into a softmax to create the attention distribution.

   the attending id56 generates a query describing what it wants to focus
   on. each item is dot producted with the query to produce a score,
   describing how well it matches the query. the scores are fed into a
   softmax to create the attention distribution.

   one use of attention between id56s is translation [11]. a traditional
   sequence-to-sequence model has to boil the entire input down into a
   single vector and then expands it back out. attention avoids this by
   allowing the id56 processing the input to pass along information about
   each word it sees, and then for the id56 generating the output to focus
   on words as they become relevant.

0.50, 0.02, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.01, 0.00, 0.00, 0.00, 0.
05, 0.05
0.14, 0.89, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.06, 0.00, 0.00, 0.00, 0.
02, 0.00
0.02, 0.00, 0.56, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.
02, 0.00
0.00, 0.00, 0.01, 0.61, 0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.
00, 0.00
0.00, 0.00, 0.02, 0.03, 0.01, 0.21, 0.97, 0.05, 0.00, 0.00, 0.00, 0.00, 0.00, 0.
01, 0.00
0.00, 0.00, 0.02, 0.07, 0.25, 0.76, 0.02, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.
02, 0.00
0.02, 0.04, 0.03, 0.27, 0.71, 0.02, 0.00, 0.02, 0.00, 0.00, 0.02, 0.00, 0.00, 0.
05, 0.01
0.02, 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.60, 0.40, 0.01, 0.04, 0.00, 0.00, 0.
13, 0.01
0.13, 0.02, 0.02, 0.00, 0.01, 0.00, 0.00, 0.23, 0.52, 0.91, 0.07, 0.00, 0.00, 0.
10, 0.00
0.04, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.01, 0.03, 0.00, 0.67, 0.00, 0.00, 0.
16, 0.02
0.02, 0.00, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.14, 0.96, 0.00, 0.
02, 0.07
0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.02, 1.00, 0.
02, 0.03
0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.01, 0.00, 0.00, 0.
32, 0.11
0.04, 0.00, 0.08, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00, 0.
04, 0.67

   output text network b network a input text diagram derived from fig. 3
   of [24]bahdanau, et al. 2014 [pointer.svg]

   this kind of attention between id56s has a number of other applications.
   it can be used in voice recognition [12], allowing one id56 to process
   the audio and then have another id56 skim over it, focusing on relevant
   parts as it generates a transcript.

   output text network b network a input audio
0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.09,0.11,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.78,0.33,0.04,0.02,0.02,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.05,0.61,0.37,0.78,0.07,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.05,0.21,0.07,0.74,0.31,0.00,0.01,0.01,0.01,0.02,0.01,0.02,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.04,0.02,0.02,0.47,0.98,0.02,0.01,0.01,0.02,0.01,0.02,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.96,0.98,0.28,0.02,0.02,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.02,0.01,0.01,0.01,0.00,0.01,0.01,0.40,0.49,0.61,0.30,0.02,0.02,
0.01,0.01,0.01,0.02,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.05,0.07,0.09,0.18,0.96,0.89,
0.87,0.54,0.02,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.02,0.10,0.05,0.13,0.01,0.03,
0.05,0.10,0.14,0.32,0.13,0.01,0.02,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.12,0.60,0.04,0.03,0.02,0.02,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.05,0.41,0.66,0.02,0.02,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.94,0.72,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.02,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.04,0.76,0.02,0.01,0.01,0.01,0.01,0.02,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.77,0.45,0.66,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.04,0.01,0.00,0.02,0.13,0.98,0.97,0.02,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.09,0.00,0.01,0.01,0.01,0.02,0.90,0.02,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.08,0.10,0.01,0.01,0.01,0.03,0.96,0.19,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.13,0.02,0.01,0.01,0.01,0.01,0.60,
0.18
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.02,
0.60
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.02
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.02,0.02,0.02,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.00,0.01,0.01,0.01,0.01,0.01,0.01,0.01,
0.01

   [id56-attention-waveform-bw.png]

   figure derived from [25]chan, et al. 2015 [pointer.svg]

   other uses of this kind of attention include parsing text [13], where
   it allows the model to glance at words as it generates the parse tree,
   and for conversational modeling [14], where it lets the model focus on
   previous parts of the conversation as it generates its response.

   attention can also be used on the interface between a convolutional
   neural network and an id56. this allows the id56 to look at different
   position of an image every step. one popular use of this kind of
   attention is for image captioning. first, a conv net processes the
   image, extracting high-level features. then an id56 runs, generating a
   description of the image. as it generates each word in the description,
   the id56 focuses on the conv net   s interpretation of the relevant parts
   of the image. we can explicitly visualize this:

   [show-attend-tell.png] figure from [3]

   more broadly, attentional interfaces can be used whenever one wants to
   interface with a neural network that has a repeating structure in its
   output.

   attentional interfaces have been found to be an extremely general and
   powerful technique, and are becoming increasingly widespread.
     __________________________________________________________________

adaptive computation time

   standard id56s do the same amount of computation for each time step.
   this seems unintuitive. surely, one should think more when things are
   hard? it also limits id56s to doing o(n) operations for a list of length
   n.

   adaptive computation time [15] is a way for id56s to do different
   amounts of computation each step. the big picture idea is simple: allow
   the id56 to do multiple steps of computation for each time step.

   in order for the network to learn how many steps to do, we want the
   number of steps to be differentiable. we achieve this with the same
   trick we used before: instead of deciding to run for a discrete number
   of steps, we have an attention distribution over the number of steps to
   run. the output is a weighted combination of the outputs of each step.

   for every time step the id56 can do multiple computation steps. the
   output is a weighted combination of the computation step outputs. the
   process is repeated for each time step. a special bit is set to denote
   the first computation step.

   there are a few more details, which were left out in the previous
   diagram. here   s a complete diagram of a time step with three
   computation steps.

   that   s a bit complicated, so let   s work through it step by step. at a
   high-level, we   re still running the id56 and outputting a weighted
   combination of the states:

   the weight for each step is determined by a    halting neuron.    it   s a
   sigmoid neuron that looks at the id56 state and gives a halting weight,
   which we can think of as the id203 that we should stop at that
   step.

   halting neuron

   we have a total budget for the halting weights of 1, so we track that
   budget along the top. when it gets to less than epsilon, we stop.

   when we stop, might have some left over halting budget because we stop
   when it gets to less than epsilon. what should we do with it?
   technically, it   s being given to future steps but we don   t want to
   compute those, so we attribute it to the last step.

   when training adaptive computation time models, one adds a    ponder
   cost    term to the cost function. this penalizes the model for the
   amount of computation it uses. the bigger you make this term, the more
   it will trade-off performance for lowering compute time.

   adaptive computation time is a very new idea, but we believe that it,
   along with similar ideas, will be very important.

  code

   the only open source implementation of adaptive computation time at the
   moment seems to be [26]mark neumann   s (tensorflow).
     __________________________________________________________________

neural programmer

   neural nets are excellent at many tasks, but they also struggle to do
   some basic things like arithmetic, which are trivial in normal
   approaches to computing. it would be really nice to have a way to fuse
   neural nets with normal programming, and get the best of both worlds.

   the neural programmer [16] is one approach to this. it learns to create
   programs in order to solve a task. in fact, it learns to generate such
   programs without needing examples of correct programs. it discovers how
   to produce programs as a means to the end of accomplishing some task.

   the actual model in the paper answers questions about tables by
   generating sql-like programs to query the table. however, there are a
   number of details here that make it a bit complicated, so let   s start
   by imagining a slightly simpler model, which is given an arithmetic
   expression and generates a program to evaluate it.

   the generated program is a sequence of operations. each operation is
   defined to operate on the output of past operations. so an operation
   might be something like    add the output of the operation 2 steps ago
   and the output of the operation 1 step ago.    it   s more like a unix pipe
   than a program with variables being assigned to and read from.

   the program is generated one operation at a time by a controller id56.
   at each step, the controller id56 outputs a id203 distribution for
   what the next operation should be. for example, we might be pretty sure
   we want to perform addition at the first time step, then have a hard
   time deciding whether we should multiply or divide at the second step,
   and so on...

   at each step the controller id56 outputs a id203 distribution.

   the resulting distribution over operations can now be evaluated.
   instead of running a single operation at each step, we do the usual
   attention trick of running all of them and then average the outputs
   together, weighted by the id203 we ran that operation.

   at each step the controller id56 outputs a id203 distribution. we
   run all of the operations and average the outputs together.

   as long as we can define derivatives through the operations, the
   program   s output is differentiable with respect to the probabilities.
   we can then define a loss, and train the neural net to produce programs
   that give the correct answer. in this way, the neural programmer learns
   to produce programs without examples of good programs. the only
   supervision is the answer the program should produce.

   that   s the core idea of neural programmer, but the version in the paper
   answers questions about tables, rather than arithmetic expressions.
   there are a few additional neat tricks:
     * multiple types: many of the operations in the neural programmer
       deal with types other than scalar numbers. some operations output
       selections of table columns or selections of cells. only outputs of
       the same type get merged together.
     * referencing inputs: the neural programmer needs to answer questions
       like    how many cities have a population greater than 1,000,000?   
       given a table of cities with a population column. to facilitate
       this, some operations allow the network to reference constants in
       the question they   re answering, or the names of columns. this
       referencing happens by attention, in the style of id193
       [17].

   the neural programmer isn   t the only approach to having neural networks
   generate programs. another lovely approach is the neural
   programmer-interpreter [18] which can accomplish a number of very
   interesting tasks, but requires supervision in the form of correct
   programs.

   we think that this general space, of bridging the gap between more
   traditional programming and neural networks is extremely important.
   while the neural programmer is clearly not the final solution, we think
   there are a lot of important lessons to be learned from it.

  code

   the more recent version of neural programmer for [27]id53
   has been open sourced by its authors and is available as a
   [28]tensorflow model. there is also an implementation of the neural
   programmer-interpreter by [29]ken morishita (keras).
     __________________________________________________________________

the big picture

   a human with a piece of paper is, in some sense, much smarter than a
   human without. a human with mathematical notation can solve problems
   they otherwise couldn   t. access to computers makes us capable of
   incredible feats that would otherwise be far beyond us.

   in general, it seems like a lot of interesting forms of intelligence
   are an interaction between the creative heuristic intuition of humans
   and some more crisp and careful media, like language or equations.
   sometimes, the medium is something that physically exists, and stores
   information for us, prevents us from making mistakes, or does
   computational heavy lifting. in other cases, the medium is a model in
   our head that we manipulate. either way, it seems deeply fundamental to
   intelligence.

   recent results in machine learning have started to have this flavor,
   combining the intuition of neural networks with something else. one
   approach is what one might call    heuristic search.    for example,
   alphago [19] has a model of how go works and explores how the game
   could play out guided by neural network intuition. similarly, deepmath
   [20] uses neural networks as intuition for manipulating mathematical
   expressions. the    augmented id56s    we   ve talked about in this article
   are another approach, where we connect id56s to engineered media, in
   order to extend their general capabilities.

   interacting with media naturally involves making a sequence of taking
   an action, observing, and taking more actions. this creates a major
   challenge: how do we learn which actions to take? that sounds like a
   id23 problem and we could certainly take that
   approach. but the id23 literature is really attacking
   the hardest version of this problem, and its solutions are hard to use.
   the wonderful thing about attention is that it gives us an easier way
   out of this problem by partially taking all actions to varying extents.
   this works because we can design media   like the ntm memory   to allow
   fractional actions and to be differentiable. id23 has
   us take a single path, and try to learn from that. attention takes
   every direction at a fork, and then merges the paths back together.

   a major weaknesses of attention is that we have to take every    action   
   every step. this causes the computational cost to grow linearly as you
   do things like increase the amount of memory in a neural turing
   machine. one thing you could imagine doing is having your attention be
   sparse, so that you only have to touch some memories. however, it   s
   still challenging because you may want to do things like have your
   attention depend on the content of the memory, and doing that naively
   forces you to look at each memory. we   ve seen some initial attempts to
   attack this problem, such as [21], but it seems like there   s a lot more
   to be done. if we could really make such sub-linear time attention
   work, that would be very powerful!

   augmented recurrent neural networks, and the underlying technique of
   attention, are incredibly exciting. we look forward to seeing what
   happens next!

acknowledgments

   thank you to maithra raghu, dario amodei, cassandra xia, luke vilnis,
   anna goldie, jesse engel, dan man  , natasha jaques, emma pierson and
   ian goodfellow for their feedback and encouragement. we   re also very
   grateful to our team, [30]google brain, for being extremely supportive
   of our project.

references

    1. understanding id137    [31][link]
       olah, c., 2015.
    2. id63s    [32][pdf]
       graves, a., wayne, g. and danihelka, i., 2014. corr, vol
       abs/1410.5401.
    3. show, attend and tell: neural image id134 with visual
       attention
       xu, k., ba, j., kiros, r., cho, k., courville, a., salakhutdinov,
       r., zemel, r.s. and bengio, y., 2015. arxiv preprint
       arxiv:1502.03044, vol 2(3), pp. 5. corr.
    4. neural gpus learn algorithms    [33][pdf]
       kaiser, l. and sutskever, i., 2015. corr, vol abs/1511.08228.
    5. id23 id63s    [34][pdf]
       zaremba, w. and sutskever, i., 2015. corr, vol abs/1505.00521.
    6. neural random-access machines    [35][pdf]
       kurach, k., andrychowicz, m. and sutskever, i., 2015. corr, vol
       abs/1511.06392.
    7. learning to transduce with unbounded memory    [36][pdf]
       grefenstette, e., hermann, k.m., suleyman, m. and blunsom, p.,
       2015. advances in neural information processing systems 28, pp.
       1828   1836. curran associates, inc.
    8. inferring algorithmic patterns with stack-augmented recurrent nets
          [37][pdf]
       joulin, a. and mikolov, t., 2015. advances in neural information
       processing systems 28, pp. 190   198. curran associates, inc.
    9. memory networks    [38][pdf]
       weston, j., chopra, s. and bordes, a., 2014. corr, vol
       abs/1410.3916.
   10. ask me anything: dynamic memory networks for natural language
       processing    [39][pdf]
       kumar, a., irsoy, o., su, j., bradbury, j., english, r., pierce,
       b., ondruska, p., gulrajani, i. and socher, r., 2015. corr, vol
       abs/1506.07285.
   11. id4 by jointly learning to align and
       translate
       bahdanau, d., cho, k. and bengio, y., 2014. arxiv preprint
       arxiv:1409.0473.
   12. listen, attend and spell    [40][pdf]
       chan, w., jaitly, n., le, q.v. and vinyals, o., 2015. corr, vol
       abs/1508.01211.
   13. grammar as a foreign language
       vinyals, o., kaiser, l., koo, t., petrov, s., sutskever, i. and
       hinton, g., 2015. advances in neural information processing
       systems, pp. 2773   2781.
   14. a neural conversational model    [41][pdf]
       vinyals, o. and le, q.v., 2015. corr, vol abs/1506.05869.
   15. adaptive computation time for recurrent neural networks    [42][pdf]
       graves, a., 2016. corr, vol abs/1603.08983.
   16. neural programmer: inducing latent programs with id119
          [43][pdf]
       neelakantan, a., le, q.v. and sutskever, i., 2015. corr, vol
       abs/1511.04834.
   17. id193
       vinyals, o., fortunato, m. and jaitly, n., 2015. advances in neural
       information processing systems, pp. 2692   2700.
   18. neural programmer-interpreters    [44][pdf]
       reed, s.e. and freitas, n.d., 2015. corr, vol abs/1511.06279.
   19. mastering the game of go with deep neural networks and tree search
          [45][link]
       silver, d., huang, a., maddison, c.j., guez, a., sifre, l.,
       driessche, g.v.d., schrittwieser, j., antonoglou, i.,
       panneershelvam, v., lanctot, m., dieleman, s., grewe, d., nham, j.,
       kalchbrenner, n., sutskever, i., lillicrap, t., leach, m.,
       kavukcuoglu, k., graepel, t. and hassabis, d., 2016. nature, vol
       529(7587), pp. 484   489. nature publishing group. [46]doi:
       10.1038/nature16961
   20. deepmath - deep sequence models for premise selection    [47][pdf]
       alemi, a.a., chollet, f., irving, g., szegedy, c. and urban, j.,
       2016. corr, vol abs/1606.04442.
   21. learning efficient algorithms with hierarchical attentive memory
          [48][pdf]
       andrychowicz, m. and kurach, k., 2016. corr, vol abs/1602.03218.

updates and corrections

   [49]view all changes to this article since it was first published. if
   you see a mistake or want to suggest a change, please [50]create an
   issue on github.

citations and reuse

   diagrams and text are licensed under creative commons attribution
   [51]cc-by 2.0, unless noted otherwise, with the [52]source available on
   github. the figures that have been reused from other sources don't fall
   under this license and can be recognized by a note in their caption:
      figure from       .

   for attribution in academic contexts, please cite this work as
olah & carter, "attention and augmented recurrent neural networks", distill, 201
6. http://doi.org/10.23915/distill.00001

   bibtex citation
@article{olah2016attention,
  author = {olah, chris and carter, shan},
  title = {attention and augmented recurrent neural networks},
  journal = {distill},
  year = {2016},
  url = {http://distill.pub/2016/augmented-id56s},
  doi = {10.23915/distill.00001}
}

   [53]distill is dedicated to clear explanations of machine learning
   [54]about [55]submit [56]prize [57]archive [58]rss [59]github
   [60]twitter      issn 2476-0757

   understanding id137    [61][link]
   c. olah. 2015.
   id63s    [62][pdf]
   a. graves, g. wayne, i. danihelka.
   corr, vol abs/1410.5401. 2014.
   show, attend and tell: neural image id134 with visual
   attention
   k. xu, j. ba, r. kiros, k. cho, a. courville, r. salakhutdinov, r.s.
   zemel, y. bengio.
   arxiv preprint arxiv:1502.03044, vol 2(3), pp. 5. corr. 2015.
   neural gpus learn algorithms    [63][pdf]
   l. kaiser, i. sutskever.
   corr, vol abs/1511.08228. 2015.
   id23 id63s    [64][pdf]
   w. zaremba, i. sutskever.
   corr, vol abs/1505.00521. 2015.
   neural random-access machines    [65][pdf]
   k. kurach, m. andrychowicz, i. sutskever.
   corr, vol abs/1511.06392. 2015.
   learning to transduce with unbounded memory    [66][pdf]
   e. grefenstette, k.m. hermann, m. suleyman, p. blunsom.
   advances in neural information processing systems 28, pp. 1828   1836.
   curran associates, inc. 2015.
   inferring algorithmic patterns with stack-augmented recurrent nets
      [67][pdf]
   a. joulin, t. mikolov.
   advances in neural information processing systems 28, pp. 190   198.
   curran associates, inc. 2015.
   memory networks    [68][pdf]
   j. weston, s. chopra, a. bordes.
   corr, vol abs/1410.3916. 2014.
   ask me anything: dynamic memory networks for natural language
   processing    [69][pdf]
   a. kumar, o. irsoy, j. su, j. bradbury, r. english, b. pierce, p.
   ondruska, i. gulrajani, r. socher.
   corr, vol abs/1506.07285. 2015.
   id4 by jointly learning to align and translate
   d. bahdanau, k. cho, y. bengio.
   arxiv preprint arxiv:1409.0473. 2014.
   listen, attend and spell    [70][pdf]
   w. chan, n. jaitly, q.v. le, o. vinyals.
   corr, vol abs/1508.01211. 2015.
   grammar as a foreign language
   o. vinyals, l. kaiser, t. koo, s. petrov, i. sutskever, g. hinton.
   advances in neural information processing systems, pp. 2773   2781. 2015.
   a neural conversational model    [71][pdf]
   o. vinyals, q.v. le.
   corr, vol abs/1506.05869. 2015.
   show, attend and tell: neural image id134 with visual
   attention
   k. xu, j. ba, r. kiros, k. cho, a. courville, r. salakhutdinov, r.s.
   zemel, y. bengio.
   arxiv preprint arxiv:1502.03044, vol 2(3), pp. 5. corr. 2015.
   adaptive computation time for recurrent neural networks    [72][pdf]
   a. graves.
   corr, vol abs/1603.08983. 2016.
   neural programmer: inducing latent programs with id119
      [73][pdf]
   a. neelakantan, q.v. le, i. sutskever.
   corr, vol abs/1511.04834. 2015.
   id193
   o. vinyals, m. fortunato, n. jaitly.
   advances in neural information processing systems, pp. 2692   2700. 2015.
   neural programmer-interpreters    [74][pdf]
   s.e. reed, n.d. freitas.
   corr, vol abs/1511.06279. 2015.
   mastering the game of go with deep neural networks and tree search
      [75][link]
   d. silver, a. huang, c.j. maddison, a. guez, l. sifre, g.v.d.
   driessche, j. schrittwieser, i. antonoglou, v. panneershelvam, m.
   lanctot, s. dieleman, d. grewe, j. nham, n. kalchbrenner, i. sutskever,
   t. lillicrap, m. leach, k. kavukcuoglu, t. graepel, d. hassabis.
   nature, vol 529(7587), pp. 484   489. nature publishing group. 2016.
   [76]doi: 10.1038/nature16961
   deepmath - deep sequence models for premise selection    [77][pdf]
   a.a. alemi, f. chollet, g. irving, c. szegedy, j. urban.
   corr, vol abs/1606.04442. 2016.
   learning efficient algorithms with hierarchical attentive memory
      [78][pdf]
   m. andrychowicz, k. kurach.
   corr, vol abs/1602.03218. 2016.

references

   visible links
   1. https://distill.pub/rss.xml
   2. https://distill.pub/
   3. https://distill.pub/about/
   4. https://distill.pub/prize/
   5. https://distill.pub/journal/
   6. http://colah.github.io/
   7. http://g.co/brain
   8. http://shancarter.com/
   9. http://g.co/brain
  10. http://colah.github.io/posts/2015-08-understanding-lstms/
  11. https://distill.pub/2016/augmented-id56s/#neural-turing-machines
  12. https://distill.pub/2016/augmented-id56s/#attentional-interfaces
  13. https://distill.pub/2016/augmented-id56s/#adaptive-computation-time
  14. https://distill.pub/2016/augmented-id56s/#neural-programmer
  15. https://github.com/carpedm20/ntm-tensorflow
  16. https://github.com/shawntan/neural-turing-machines
  17. https://github.com/fumin/ntm
  18. https://github.com/kaishengtai/torch-ntm
  19. https://github.com/snipsco/ntm-lasagne
  20. https://github.com/tensorflow/models/tree/master/neural_gpu
  21. https://github.com/facebook/memnn
  22. https://github.com/yerevann/dynamic-memory-networks-in-theano
  23. https://github.com/carpedm20/memn2n-tensorflow
  24. https://arxiv.org/pdf/1409.0473.pdf
  25. https://arxiv.org/pdf/1508.01211.pdf
  26. https://github.com/deneutoy/act-tensorflow
  27. https://openreview.net/pdf?id=ry2yorcge
  28. https://github.com/tensorflow/models/tree/master/neural_programmer
  29. https://github.com/mokemokechicken/keras_npi
  30. http://g.co/brain
  31. http://colah.github.io/posts/2015-08-understanding-lstms
  32. http://arxiv.org/pdf/1410.5401.pdf
  33. http://arxiv.org/pdf/1511.08228.pdf
  34. http://arxiv.org/pdf/1505.00521.pdf
  35. http://arxiv.org/pdf/1511.06392.pdf
  36. http://papers.nips.cc/paper/5648-learning-to-transduce-with-unbounded-memory.pdf
  37. http://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.pdf
  38. http://arxiv.org/pdf/1410.3916.pdf
  39. http://arxiv.org/pdf/1506.07285.pdf
  40. http://arxiv.org/pdf/1508.01211.pdf
  41. http://arxiv.org/pdf/1506.05869.pdf
  42. http://arxiv.org/pdf/1603.08983.pdf
  43. http://arxiv.org/pdf/1511.04834.pdf
  44. http://arxiv.org/pdf/1511.06279.pdf
  45. http://dx.doi.org/10.1038/nature16961
  46. https://doi.org/10.1038/nature16961
  47. http://arxiv.org/pdf/1606.04442.pdf
  48. http://arxiv.org/pdf/1602.03218.pdf
  49. https://github.com/distillpub/post--augmented-id56s/compare/1596e094d8943d2dc0ea445d92071129c6419c59...fdc3526167133f5a97cee9e7cb48af293e67d66d
  50. https://github.com/distillpub/post--augmented-id56s/issues/new
  51. https://creativecommons.org/licenses/by/2.0/
  52. https://github.com/distillpub/post--augmented-id56s
  53. https://distill.pub/
  54. http://distill.pub/about/
  55. http://distill.pub/journal/
  56. http://distill.pub/prize/
  57. http://distill.pub/archive/
  58. http://distill.pub/rss.xml
  59. https://github.com/distillpub
  60. https://twitter.com/distillpub
  61. http://colah.github.io/posts/2015-08-understanding-lstms
  62. http://arxiv.org/pdf/1410.5401.pdf
  63. http://arxiv.org/pdf/1511.08228.pdf
  64. http://arxiv.org/pdf/1505.00521.pdf
  65. http://arxiv.org/pdf/1511.06392.pdf
  66. http://papers.nips.cc/paper/5648-learning-to-transduce-with-unbounded-memory.pdf
  67. http://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.pdf
  68. http://arxiv.org/pdf/1410.3916.pdf
  69. http://arxiv.org/pdf/1506.07285.pdf
  70. http://arxiv.org/pdf/1508.01211.pdf
  71. http://arxiv.org/pdf/1506.05869.pdf
  72. http://arxiv.org/pdf/1603.08983.pdf
  73. http://arxiv.org/pdf/1511.04834.pdf
  74. http://arxiv.org/pdf/1511.06279.pdf
  75. http://dx.doi.org/10.1038/nature16961
  76. https://doi.org/10.1038/nature16961
  77. http://arxiv.org/pdf/1606.04442.pdf
  78. http://arxiv.org/pdf/1602.03218.pdf

   hidden links:
  80. https://distill.pub/2016/augmented-id56s/#citation
