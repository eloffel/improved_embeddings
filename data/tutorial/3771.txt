   #[1]articles from distill

   [2]distill
   [3]about [4]prize [5]submit

             four experiments in handwriting with a neural network

let   s start with generating new strokes based on your handwriting input

   (button) play/pause (button) clear length of prediction 20 variation^1
   0.1

   [6]shan carter [7]google brain
   [8]david ha [9]google brain
   [10]ian johnson [11]google cloud
   [12]chris olah [13]google brain
   dec. 6
   2016
   citation:
   carter, et al., 2016

   neural networks are an extremely successful approach to machine
   learning, but it   s tricky to understand why they behave the way they
   do. this has sparked a lot of interest and effort around trying to
   understand and visualize them, which we think is so far just scratching
   the surface of what is possible.

   in this article we will try to push forward in this direction by taking
   a generative model of handwriting^2 and visualizing it in a number of
   ways. the model is quite simple (so as to run well in the browser) so
   the generated output mostly produces gibberish letters and words
   (albeit, gibberish that look like real handwriting), but it is still
   useful for our purposes of exploring visualization techniques.

   in the end we don   t have some ultimate answer or visualization, but we
   do have some interesting ideas to share. ultimately we hope they make
   it easier to divine some meaning from the internals of these model.

looking at the output of the model

   our first experiment is the most obvious: when we want to see how well
   someone has learned a task we usually ask them to demonstrate it. so,
   let   s ask our model to write something for us and see how well it does.

   (button) play/pause variation^[14]1 0.6

   most of the marks are gibberish, but many of them are surprisingly
   convincing. some real (or real-ish) words even start to appear. one
   surprising thing you   ll notice is that the general style of handwriting
   is more or less consistent within a sample. this is because the type of
   architecture used for this model (lstm) has a mechanism for remembering
   previous strokes. it is therefore able to remember things like how
   loopy or jerky the handwriting is, or which letter preceeded the
   current one. (for more on lstms and how they can remember, chris olah
   has a [15]good primer [3].)

   even with this memory, the technique for generating samples from neural
   networks is probabilistic, meaning each of these samples is one of a
   much, much larger possibility space. viewing them one at a time feels
   unsatisfying         how can we infer much from one or two samples when there
   are nearly infinite possibilities? if something looks wrong, how do we
   know if it was because there is something fundamentally wrong with our
   model or if it was just dumb luck?
   [color-key.svg]

   at each iteration our model could have produced many paths. instead of
   picking one and throwing the rest away, let   s draw, say, 50 of them.
   green strokes below are places where the model would have veered
   rightward from the chosen stroke, orange is where it would have veered
   leftward.

   [generated sample 3] steps 4 variation^[16]1 0.5

   with this technique we are casting a light into a wider possibility
   space. you can see some areas where there was general consensus as to
   what to do next. other areas had more of a    anything can happen next   
   feeling. others seem to show a possible fork in the road. we can be
   drawing an    a    or    g   . a few steps later it   s clear the model has
   converged on a cursive    g   .
   [color-key.svg]

   in addition to visualizing samples generated by the model, this
   technique can also be applied to human-generated samples. below we can
   see what the model would have done at each point if it had taken over
   for the person.

   [validation sample 2] steps 4 variation^[17]1 0.5

   it is obvious from these experiments that this model has learned quite
   a lot about human handwriting. which sort of raises the question, can
   we extract that knowledge in any meaningful way, rather than just
   blindly using it to mimic handwriting?

examining the internals of the model

   our model has 500 cells which act as a sort of memory which it will use
   as part of its input when deciding what to generate. if we can see what
   those cells are doing as the model progresses we may be able to gain
   some intuitive understanding about what the model is doing.

   we begin by showing the activation of the cells over time. each column
   in the heatmap below represents one line segment of the handwriting.
   each row represents one cell of the model and is colored by its
   activations on that part of the stroke. by inspecting the diagram you
   may be able to see some patterns in the way certain cells activate for
   certain types of strokes. you can change which cell is used to color
   the strokes by clicking on the diagram.

   you may have been able to pick out one or two cells with interesting
   patterns, but we have reason to believe that cells work in tandem, such
   that an individual cell   s activity may not be as interesting as a group
   of cells.

   is there a way to order the cells to make this structure clearer? we   ve
   found that applying one-dimensional [18]id167 [4] to the activations of
   the cells over time can organize them, bringing cells with similar
   behavior close together. this makes the diagram easier to read. we use
   a few small tricks to achieve the best results.^3

   in this version we can find a few clear behaviors. for example, cells
   11-70 at the top seem sensitive to slightly different directions and
   curvatures of the pen path         see in particular cells 25 and 55. on the
   other hand, at the bottom, cells below 427 seem focused on pen lifts;
   for example, cell 494 seems to predict whether the pen is about to be
   lifted. in the middle, we see a variety of cells, including some
   tracking absolute position. cell 136 and its neighbors seem concerned
   with horizontal position, while cells around 236 seem concerned with
   vertical position. cell 242 appears to track position within a word.

   another way to explore the activations is to give it a sample and
   interactively see the activations. below you can write and see the
   activations of all the cells in real time.

   (button) clear
     __________________________________________________________________

conclusion

   the black box reputation of machine learning models is well deserved,
   but we believe part of that reputation has been born from the
   programming context into which they have been locked. the experience of
   having an easily inspectable model available in the same programming
   context as the interactive visualization environment (here, javascript)
   proved to be very productive for prototyping and exploring new ideas
   for this post.

   as we are able to move them more and more into the same programming
   context that user interface work is done, we believe we will see richer
   modes of human-ai interactions flourish. this could have a marked
   impact on debugging and building models, for sure, but also in how the
   models are used. machine learning research typically seeks to mimic and
   substitute humans, and increasingly it   s able to. what seems less
   explored is using machine learning to augment humans. this sort of
   complicated human-machine interaction is best explored when the full
   capabilities of the model are available in the user interface context.

acknowledgments

   we are grateful for the comments of fernanda viegas, martin wattenberg,
   and daniel smilkov.

   this work was made possible by the support of the [19]google brain
   team.

author contributions

   shan carter wrote the article and created the interactive experiments
   in the first section. david ha created the handwriting model and ported
   it to javascript. ian johnson created the final diagrams exploring
   activations. chris olah provided guidance and core ideas for the
   diagrams and edited the article.

footnotes

    1. the model has a parameter which determines how widely it samples
       from the underlying distribution. it is labeled here as variation
       but is more commonly referred to as temperature. temperature is
       most commonly discussed in [20]boltzmann distributions, but can be
       generalized to all id203 distributions. in this more general
       form, changing the temperature by a factor of t corresponds to
       raising all probabilities to the power of 1/t and normalizing.
    2. the model used in this articles is a version of the model described
       in section 4.2 of [21]generating sequences with recurrent neural
       network by alex graves [1]. it is a small lstm, with 500 hidden
       units, trained to perform the unconditional handwriting generation
       task. for a detailed description of the model and training
       procedure, please refer to [22]this blog post [2] in addition to
       the graves paper. after training the lstm, we quantized the weights
       using 8-bit integers, and exported the weights into a small
       [23]json-base64 formatted file.
    3. we use two different tricks to make the id167 organization of
       neurons work better. first, if a cell state is mostly negative, we
       canonicalize it by flipping its sign. (from the lstm   s perspective,
       this is completely equivalent, provided we flip the sign of some
       weights. the sign of a cell state is arbitrary.) secondly, we tried
       using different metrics on points in our dataset for id167,
       encouraging it to put points we think of as similar close together.
       we got good results basing our metric on blurred data (so that
       cells offset forward or backwards by one are still close together)
       and a blurred [24]sobolev metric to encourage sharp changes to line
       up. we achieved slightly better results, presented in the article,
       yet by creating a metric based on percentiles of cell activation in
       a neighborhood. see `bin/sort` in the repository for details.

references

    1. generating sequences with recurrent neural networks    [25][pdf]
       graves, a., 2013. arxiv preprint arxiv:1308.0850.
    2. handwriting generation demo in tensorflow    [26][link]
       ha, d., 2016.
    3. understanding id137    [27][link]
       olah, c., 2015.
    4. visualizing data using id167    [28][pdf]
       maaten, l.v.d. and hinton, g., 2008. journal of machine learning
       research, vol 9(nov), pp. 2579   2605.

updates and corrections

   [29]view all changes to this article since it was first published. if
   you see a mistake or want to suggest a change, please [30]create an
   issue on github.

citations and reuse

   diagrams and text are licensed under creative commons attribution
   [31]cc-by 2.0, unless noted otherwise, with the [32]source available on
   github. the figures that have been reused from other sources don't fall
   under this license and can be recognized by a note in their caption:
      figure from       .

   for attribution in academic contexts, please cite this work as
carter, et al., "experiments in handwriting with a neural network", distill, 201
6. http://doi.org/10.23915/distill.00004

   bibtex citation
@article{carter2016experiments,
  author = {carter, shan and ha, david and johnson, ian and olah, chris},
  title = {experiments in handwriting with a neural network},
  journal = {distill},
  year = {2016},
  url = {http://distill.pub/2016/handwriting},
  doi = {10.23915/distill.00004}
}

   generating sequences with recurrent neural networks    [33][pdf]
   a. graves.
   arxiv preprint arxiv:1308.0850. 2013.
   handwriting generation demo in tensorflow    [34][link]
   d. ha. 2016.
   understanding id137    [35][link]
   c. olah. 2015.
   visualizing data using id167    [36][pdf]
   l.v.d. maaten, g. hinton.
   journal of machine learning research, vol 9(nov), pp. 2579   2605. 2008.

   the model has a parameter which determines how widely it samples from
   the underlying distribution. it is labeled here as variation but is
   more commonly referred to as temperature. temperature is most commonly
   discussed in [37]boltzmann distributions, but can be generalized to all
   id203 distributions. in this more general form, changing the
   temperature by a factor of t corresponds to raising all probabilities
   to the power of 1/t and normalizing.
   the model used in this articles is a version of the model described in
   section 4.2 of [38]generating sequences with recurrent neural network
   by alex graves [1]. it is a small lstm, with 500 hidden units, trained
   to perform the unconditional handwriting generation task. for a
   detailed description of the model and training procedure, please refer
   to [39]this blog post [2] in addition to the graves paper. after
   training the lstm, we quantized the weights using 8-bit integers, and
   exported the weights into a small [40]json-base64 formatted file.
   we use two different tricks to make the id167 organization of neurons
   work better. first, if a cell state is mostly negative, we canonicalize
   it by flipping its sign. (from the lstm   s perspective, this is
   completely equivalent, provided we flip the sign of some weights. the
   sign of a cell state is arbitrary.) secondly, we tried using different
   metrics on points in our dataset for id167, encouraging it to put
   points we think of as similar close together. we got good results
   basing our metric on blurred data (so that cells offset forward or
   backwards by one are still close together) and a blurred [41]sobolev
   metric to encourage sharp changes to line up. we achieved slightly
   better results, presented in the article, yet by creating a metric
   based on percentiles of cell activation in a neighborhood. see
   `bin/sort` in the repository for details.

   [42]distill is dedicated to clear explanations of machine learning
   [43]about [44]submit [45]prize [46]archive [47]rss [48]github
   [49]twitter      issn 2476-0757

references

   visible links
   1. https://distill.pub/rss.xml
   2. https://distill.pub/
   3. https://distill.pub/about/
   4. https://distill.pub/prize/
   5. https://distill.pub/journal/
   6. http://shancarter.com/
   7. http://g.co/brain
   8. https://github.com/hardmaru
   9. http://g.co/brain
  10. https://github.com/enjalot
  11. http://cloud.google.com/
  12. http://colah.github.io/
  13. http://g.co/brain
  14. https://distill.pub/2016/handwriting/#footnote1
  15. http://colah.github.io/posts/2015-08-understanding-lstms/
  16. https://distill.pub/2016/handwriting/#footnote1
  17. https://distill.pub/2016/handwriting/#footnote1
  18. https://lvdmaaten.github.io/tsne/
  19. https://research.google.com/teams/brain/
  20. https://en.wikipedia.org/wiki/boltzmann_distribution
  21. https://arxiv.org/abs/1308.0850
  22. http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/
  23. https://jb64.org/specification/
  24. https://en.wikipedia.org/wiki/sobolev_space
  25. https://arxiv.org/pdf/1308.0850v5.pdf
  26. http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/
  27. http://colah.github.io/posts/2015-08-understanding-lstms/
  28. http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  29. https://github.com/distillpub/post--handwriting/compare/ef8b15f18dc770cc7e16173e31b092185818491c...65862bc4aa572d405f3e3cbd930eb885159aac29
  30. https://github.com/distillpub/post--handwriting/issues/new
  31. https://creativecommons.org/licenses/by/2.0/
  32. https://github.com/distillpub/post--handwriting
  33. https://arxiv.org/pdf/1308.0850v5.pdf
  34. http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/
  35. http://colah.github.io/posts/2015-08-understanding-lstms/
  36. http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  37. https://en.wikipedia.org/wiki/boltzmann_distribution
  38. https://arxiv.org/abs/1308.0850
  39. http://blog.otoro.net/2015/12/12/handwriting-generation-demo-in-tensorflow/
  40. https://jb64.org/specification/
  41. https://en.wikipedia.org/wiki/sobolev_space
  42. https://distill.pub/
  43. http://distill.pub/about/
  44. http://distill.pub/journal/
  45. http://distill.pub/prize/
  46. http://distill.pub/archive/
  47. http://distill.pub/rss.xml
  48. https://github.com/distillpub
  49. https://twitter.com/distillpub

   hidden links:
  51. https://distill.pub/2016/handwriting/#citation
