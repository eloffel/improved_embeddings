   #[1]articles from distill

   [2]distill
   [3]about [4]prize [5]submit

   (button) play_arrow pause (button) refresh
   step:
   (button) expand_less

                          how to use id167 effectively

although extremely useful for visualizing high-dimensional data, id167 plots can
sometimes be mysterious or misleading. by exploring how it behaves in simple
cases, we can learn to use it more effectively.

   (button) play_arrowpause (button) refresh
   step
   link share this view

   [6]martin wattenberg [7]google brain
   [8]fernanda vi  gas [9]google brain
   [10]ian johnson [11]google cloud
   oct. 13
   2016
   citation:
   wattenberg, et al., 2016

   a popular method for exploring high-dimensional data is something
   called id167, introduced by van der maaten and hinton in 2008 [1]. the
   technique has become widespread in the field of machine learning, since
   it has an almost magical ability to create compelling two-dimensonal
      maps    from data with hundreds or even thousands of dimensions.
   although impressive, these images can be tempting to misread. the
   purpose of this note is to prevent some common misreadings.

   we   ll walk through a series of simple examples to illustrate what id167
   diagrams can and cannot show. the id167 technique really is useful   but
   only if you know how to interpret it.

   before diving in: if you haven   t encountered id167 before, here   s what
   you need to know about the math behind it. the goal is to take a set of
   points in a high-dimensional space and find a faithful representation
   of those points in a lower-dimensional space, typically the 2d plane.
   the algorithm is non-linear and adapts to the underlying data,
   performing different transformations on different regions. those
   differences can be a major source of confusion.

   a second feature of id167 is a tuneable parameter,    perplexity,    which
   says (loosely) how to balance attention between local and global
   aspects of your data. the parameter is, in a sense, a guess about the
   number of close neighbors each point has. the perplexity value has a
   complex effect on the resulting pictures. the original paper says,    the
   performance of sne is fairly robust to changes in the perplexity, and
   typical values are between 5 and 50.    but the story is more nuanced
   than that. getting the most from id167 may mean analyzing multiple
   plots with different perplexities.

   that   s not the end of the complications. the id167 algorithm doesn   t
   always produce similar output on successive runs, for example, and
   there are additional hyperparameters related to the optimization
   process.
     __________________________________________________________________

1. those hyperparameters really matter

   let   s start with the    hello world    of id167: a data set of two widely
   separated clusters. to make things as simple as possible, we   ll
   consider clusters in a 2d plane, as shown in the lefthand diagram. (for
   clarity, the two clusters are color coded.) the diagrams at right show
   id167 plots for five different perplexity values.

   with perplexity values in the range (5 - 50) suggested by van der
   maaten & hinton, the diagrams do show these clusters, although with
   very different shapes. outside that range, things get a little weird.
   with perplexity 2, local variations dominate. the image for perplexity
   100, with merged clusters, illustrates a pitfall: for the algorithm to
   operate properly, the perplexity really should be smaller than the
   number of points. implementations can give unexpected behavior
   otherwise.

   each of the plots above was made with 5,000 iterations with a learning
   rate (often called    epsilon   ) of 10, and had reached a point of
   stability by step 5,000. how much of a difference do those values make?
   in our experience, the most important thing is to iterate until
   reaching a stable configuration.

   the images above show five different runs at perplexity 30. the first
   four were stopped before stability. after 10, 20, 60, and 120 steps you
   can see layouts with seeming 1-dimensional and even pointlike images of
   the clusters. if you see a id167 plot with strange    pinched    shapes,
   chances are the process was stopped too early. unfortunately, there   s
   no fixed number of steps that yields a stable result. different data
   sets can require different numbers of iterations to converge.

   another natural question is whether different runs with the same
   hyperparameters produce the same results. in this simple two-cluster
   example, and most of the others we discuss, multiple runs give the same
   global shape. certain data sets, however, yield markedly different
   diagrams on different runs; we   ll give an example of one of these
   later.

   from now on, unless otherwise stated, we   ll show results from 5,000
   iterations. that   s generally enough for convergence in the (relatively
   small) examples in this essay. we   ll keep showing a range of
   perplexities, however, since that seems to make a big difference in
   every case.
     __________________________________________________________________

2. cluster sizes in a id167 plot mean nothing

   so far, so good. but what if the two clusters have different standard
   deviations, and so different sizes? (by size we mean bounding box
   measurements, not number of points.) below are id167 plots for a
   mixture of gaussians in plane, where one is 10 times as dispersed as
   the other.

   surprisingly, the two clusters look about same size in the id167 plots.
   what   s going on? the id167 algorithm adapts its notion of    distance    to
   regional density variations in the data set. as a result, it naturally
   expands dense clusters, and contracts sparse ones, evening out cluster
   sizes. to be clear, this is a different effect than the run-of-the-mill
   fact that any id84 technique will distort
   distances. (after all, in this example all data was two-dimensional to
   begin with.) rather, density equalization happens by design and is a
   predictable feature of id167.

   the bottom line, however, is that you cannot see relative sizes of
   clusters in a id167 plot.
     __________________________________________________________________

3. distances between clusters might not mean anything

   what about distances between clusters? the next diagrams show three
   gaussians of 50 points each, one pair being 5 times as far apart as
   another pair.

   at perplexity 50, the diagram gives a good sense of the global
   geometry. for lower perplexity values the clusters look equidistant.
   when the perplexity is 100, we see the global geometry fine, but one of
   the cluster appears, falsely, much smaller than the others. since
   perplexity 50 gave us a good picture in this example, can we always set
   perplexity to 50 if we want to see global geometry?

   sadly, no. if we add more points to each cluster, the perplexity has to
   increase to compensate. here are the id167 diagrams for three gaussian
   clusters with 200 points each, instead of 50. now none of the trial
   perplexity values gives a good result.

   it   s bad news that seeing global geometry requires fine-tuning
   perplexity. real-world data would probably have multiple clusters with
   different numbers of elements. there may not be one perplexity value
   that will capture distances across all clusters   and sadly perplexity is
   a global parameter. fixing this problem might be an interesting area
   for future research.

   the basic message is that distances between well-separated clusters in
   a id167 plot may mean nothing.
     __________________________________________________________________

4. random noise doesn   t always look random.

   a classic pitfall is thinking you see patterns in what is really just
   random data. recognizing noise when you see it is a critical skill, but
   it takes time to build up the right intuitions. a tricky thing about
   id167 is that it throws a lot of existing intuition out the window. the
   next diagrams show genuinely random data, 500 points drawn from a unit
   gaussian distribution in 100 dimensions. the left image is a projection
   onto the first two coordinates.

   the plot with perplexity 2 seems to show dramatic clusters. if you were
   tuning perplexity to bring out structure in the data, you might think
   you   d hit the jackpot.

   of course, since we know the cloud of points was generated randomly, it
   has no statistically interesting clusters: those    clumps    aren   t
   meaningful. if you look back at previous examples, low perplexity
   values often lead to this kind of distribution. recognizing these
   clumps as random noise is an important part of reading id167 plots.

   there   s something else interesting, though, which may be a win for
   id167. at first the perplexity 30 plot doesn   t look like a gaussian
   distribution at all: there   s only a slight density difference across
   different regions of the cloud, and the points seem suspiciously evenly
   distributed. in fact, these features are saying useful things about
   high-dimensional normal distributions, which are very close to uniform
   distributions on a sphere: evenly distributed, with roughly equal
   spaces between points. seen in this light, the id167 plot is more
   accurate than any linear projection could be.
     __________________________________________________________________

5. you can see some shapes, sometimes

   it   s rare for data to be distributed in a perfectly symmetric way.
   let   s take a look at an axis-aligned gaussian distribution in 50
   dimensions, where the standard deviation in coordinate i is 1/i. that
   is, we   re looking at a long-ish ellipsoidal cloud of points.

   for high enough perplexity values, the elongated shapes are easy to
   read. on the other hand, at low perplexity, local effects and
   meaningless    clumping    take center stage. more extreme shapes also come
   through, but again only at the right perplexity. for example, here are
   two clusters of 75 points each in 2d, arranged in parallel lines with a
   bit of noise.

   for a certain range of perplexity the long clusters look close to
   correct, which is reassuring.

   even in the best cases, though, there   s a subtle distortion: the lines
   are slightly curved outwards in the id167 diagram. the reason is that,
   as usual, id167 tends to expand denser regions of data. since the
   middles of the clusters have less empty space around them than the
   ends, the algorithm magnifies them.
     __________________________________________________________________

6. for topology, you may need more than one plot

   sometimes you can read topological information off a id167 plot, but
   that typically requires views at multiple perplexities. one of the
   simplest topological properties is containment. the plots below show
   two groups of 75 points in 50 dimensional space. both are sampled from
   symmetric gaussian distributions centered at the origin, but one is 50
   times more tightly dispersed than the other. the    small    distribution
   is in effect contained in the large one.

   the perplexity 30 view shows the basic topology correctly, but again
   id167 greatly exaggerates the size of the smaller group of points. at
   perplexity 50, there   s a new phenomenon: the outer group becomes a
   circle, as the plot tries to depict the fact that all its points are
   about the same distance from the inner group. if you looked at this
   image alone, it would be easy to misread these outer points as a
   one-dimensional structure.

   what about more complicated types of topology? this may be a subject
   dearer to mathematicians than to practical data analysts, but
   interesting low-dimensional structures are occasionally found in the
   wild.

   consider a set of points that trace a link or a knot in three
   dimensions. once again, looking at multiple perplexity values gives the
   most complete picture. low perplexity values give two completely
   separate loops; high ones show a kind of global connectivity.

   the trefoil knot is an interesting example of how multiple runs affect
   the outcome of id167. below are five runs of the perplexity-2 view.

   the algorithm settles twice on a circle, which at least preserves the
   intrinsic topology. but in three of the runs it ends up with three
   different solutions which introduce artificial breaks. using the dot
   color as a guide, you can see that the first and third runs are far
   from each other.

   five runs at perplexity 50, however, give results that (up to symmetry)
   are visually identical. evidently some problems are easier than others
   to optimize.
     __________________________________________________________________

conclusion

   there   s a reason that id167 has become so popular: it   s incredibly
   flexible, and can often find structure where other
   dimensionality-reduction algorithms cannot. unfortunately, that very
   flexibility makes it tricky to interpret. out of sight from the user,
   the algorithm makes all sorts of adjustments that tidy up its
   visualizations. don   t let the hidden    magic    scare you away from the
   whole technique, though. the good news is that by studying how id167
   behaves in simple cases, it   s possible to develop an intuition for
   what   s going on.

acknowledgments

   we are grateful to chris olah and shan carter for creating this
   platform, and for excellent design and editorial help from shan carter.
   daniel smilkov, james wexler, and chi zeng provided many helpful
   comments. we also thank andrej karpathy for creating the [12]tsnejs
   library used in the interactive diagrams.

   this work was made possible by the support of the [13]google brain
   team.

   edited on oct. 18, 2016 to describe and correct issues when perplexity
   is defined to be larger than the number of points. thanks to laurens
   van der maaten for pointing this out.

references

    1. visualizing data using id167    [14][pdf]
       maaten, l.v.d. and hinton, g., 2008. journal of machine learning
       research, vol 9(nov), pp. 2579   2605.

updates and corrections

   [15]view all changes to this article since it was first published. if
   you see a mistake or want to suggest a change, please [16]create an
   issue on github.

citations and reuse

   diagrams and text are licensed under creative commons attribution
   [17]cc-by 2.0, unless noted otherwise, with the [18]source available on
   github. the figures that have been reused from other sources don't fall
   under this license and can be recognized by a note in their caption:
      figure from       .

   for attribution in academic contexts, please cite this work as
wattenberg, et al., "how to use id167 effectively", distill, 2016. http://doi.or
g/10.23915/distill.00002

   bibtex citation
@article{wattenberg2016how,
  author = {wattenberg, martin and vi  gas, fernanda and johnson, ian},
  title = {how to use id167 effectively},
  journal = {distill},
  year = {2016},
  url = {http://distill.pub/2016/misread-tsne},
  doi = {10.23915/distill.00002}
}

   visualizing data using id167    [19][pdf]
   l.v.d. maaten, g. hinton.
   journal of machine learning research, vol 9(nov), pp. 2579   2605. 2008.

   [20]distill is dedicated to clear explanations of machine learning
   [21]about [22]submit [23]prize [24]archive [25]rss [26]github
   [27]twitter      issn 2476-0757

references

   visible links
   1. https://distill.pub/rss.xml
   2. https://distill.pub/
   3. https://distill.pub/about/
   4. https://distill.pub/prize/
   5. https://distill.pub/journal/
   6. http://hint.fm/
   7. http://g.co/brain
   8. http://hint.fm/
   9. http://g.co/brain
  10. http://enjalot.github.io/
  11. http://cloud.google.com/
  12. https://github.com/karpathy/tsnejs
  13. https://research.google.com/teams/brain/
  14. http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  15. https://github.com/distillpub/post--misread-tsne/compare/de6941fa8fc6c4db9d8e4b4f4d7f604817db562c...f2492b469bec8aaa7aa83cc828081e14a295414d
  16. https://github.com/distillpub/post--misread-tsne/issues/new
  17. https://creativecommons.org/licenses/by/2.0/
  18. https://github.com/distillpub/post--misread-tsne
  19. http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  20. https://distill.pub/
  21. http://distill.pub/about/
  22. http://distill.pub/journal/
  23. http://distill.pub/prize/
  24. http://distill.pub/archive/
  25. http://distill.pub/rss.xml
  26. https://github.com/distillpub
  27. https://twitter.com/distillpub

   hidden links:
  29. https://distill.pub/2016/misread-tsne/#citation
