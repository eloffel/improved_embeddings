   #[1]github [2]recent commits to fasttext_multilingual:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]59
     * [35]star [36]947
     * [37]fork [38]90

[39]babylonpartners/[40]fasttext_multilingual

   [41]code [42]issues 10 [43]pull requests 1 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   id73 vectors in 78 languages
   [47]word-vectors [48]machine-learning [49]machine-translation
   [50]natural-language-processing [51]nlp [52]distributed-representations
     * [53]28 commits
     * [54]1 branch
     * [55]0 releases
     * [56]fetching contributors
     * [57]bsd-3-clause

    1. [58]jupyter notebook 52.3%
    2. [59]python 47.7%

   (button) jupyter notebook python
   branch: master (button) new pull request
   [60]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/b
   [61]download zip

downloading...

   want to be notified of new releases in
   babylonpartners/fasttext_multilingual?
   [62]sign in [63]sign up

launching github desktop...

   if nothing happens, [64]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [65]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [66]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [67]download the github extension for visual studio
   and try again.

   (button) go back
   [68]@nhammerla
   [69]nhammerla [70]merge pull request [71]#17 [72]from
   babylonpartners/updated_alignment_matrices (button)    
updated alignment matrices

   latest commit [73]f81f3e4 feb 7, 2018
   [74]permalink
   type               name                 latest commit message  commit time
        failed to load latest commit information.
        [75]alignment_matrices           [76]added new alignments feb 7, 2018
        [77]license
        [78]readme.md
        [79]align_your_own.ipynb
        [80]fasttext.py
        [81]fasttext_checksums.txt
        [82]multilingual_performance.png

readme.md

aligning the fasttext vectors of 78 languages

   facebook recently open-sourced word vectors in [83]89 languages.
   however these vectors are monolingual; meaning that while similar words
   within a language share similar vectors, translation words from
   different languages do not have similar vectors. in [84]a recent paper
   at iclr 2017, we showed how the svd can be used to learn a linear
   transformation (a matrix), which aligns monolingual vectors from two
   languages in a single vector space. in this repository we provide 78
   matrices, which can be used to align the majority of the fasttext
   languages in a single space.

   this readme explains how the matrices should be used. we also present a
   simple evaluation task, where we show we are able to successfully
   predict the translations of words in multiple languages. our procedure
   relies on collecting bilingual training dictionaries of word pairs in
   two languages, but remarkably we are able to successfully predict the
   translations of words between language pairs for which we had no
   training dictionary!

   id27s define the similarity between two words by the
   normalised inner product of their vectors. the matrices in this
   repository place languages in a single space, without changing any of
   these monolingual similarity relationships. when you use the resulting
   multilingual vectors for monolingual tasks, they will perform exactly
   the same as the original vectors. to learn more about id27s,
   check out [85]colah's blog or [86]sam's introduction to vector
   representations.

   note that since we released this repository facebook have released an
   additional 204 languages; however the word vectors of the original 90
   languages have not changed, and the transformations provided in this
   repository will still work. if you would like to learn your own
   alignment matrices, we provide an example in [87]align_your_own.ipynb.

   if you use this repository, please cite:

   offline bilingual word vectors, orthogonal transformations and the
   inverted softmax
   samuel l. smith, david h. p. turban, steven hamblin and nils y.
   hammerla
   iclr 2017 (conference track)

tldr, just tell me what to do!

   clone a local copy of this repository, and download the fasttext
   vectors you need from [88]here. i'm going to assume you've downloaded
   the vectors for french and russian in the text format. let's say we
   want to compare the similarity of "chat" and "      ". we load the word
   vectors:
from fasttext import fastvector
fr_dictionary = fastvector(vector_file='wiki.fr.vec')
ru_dictionary = fastvector(vector_file='wiki.ru.vec')

   we can extract the word vectors and calculate their cosine similarity:
fr_vector = fr_dictionary["chat"]
ru_vector = ru_dictionary["      "]
print(fastvector.cosine_similarity(fr_vector, ru_vector))
# result should be 0.02

   the cosine similarity runs between -1 and 1. it seems that "chat" and
   "      " are neither similar nor dissimilar. but now we apply the
   transformations to align the two dictionaries in a single space:
fr_dictionary.apply_transform('alignment_matrices/fr.txt')
ru_dictionary.apply_transform('alignment_matrices/ru.txt')

   and re-evaluate the cosine similarity:
print(fastvector.cosine_similarity(fr_dictionary["chat"], ru_dictionary["      "]))
# result should be 0.43

   turns out "chat" and "      " are pretty similar after all. this is good,
   since they both mean "cat".

ok, so how did you obtain these matrices?

   of the 89 languages provided by facebook, 78 are supported by the
   [89]google translate api. we first obtained the 10,000 most common
   words in the english fasttext vocabulary, and then use the api to
   translate these words into the 78 languages available. we split this
   vocabulary in two, assigning the first 5000 words to the training
   dictionary, and the second 5000 to the test dictionary.

   we described the alignment procedure [90]in this blog. it takes two
   sets of word vectors and a small bilingual dictionary of translation
   pairs in two languages; and generates a matrix which aligns the source
   language with the target. sometimes google translates an english word
   to a non-english phrase, in these cases we average the word vectors
   contained in the phrase.

   to place all 78 languages in a single space, we align every language to
   the english vectors (the english matrix is the identity).

right, now prove that this procedure actually worked...

   to prove that the procedure works, we can predict the translations of
   words not seen in the training dictionary. for simplicity we predict
   translations by nearest neighbours. so for example, if we wanted to
   translate "dog" into swedish, we would simply find the swedish word
   vector whose cosine similarity to the "dog" word vector is highest.

   first things first, let's test the translation performance from english
   into every other language. for each language pair, we extract a set of
   2500 word pairs from the test dictionary. the precision @n denotes the
   id203 that, of the 2500 target words in this set, the true
   translation was one of the top n nearest neighbours of the source word.
   if the alignment was completely random, we would expect the precision
   @1 to be around 0.0004.
   target language precision @1 precision @5 precision @10
   fr              0.73         0.86         0.88
   pt              0.73         0.86         0.89
   es              0.72         0.85         0.88
   it              0.70         0.86         0.89
   nl              0.68         0.83         0.86
   no              0.68         0.85         0.89
   da              0.66         0.84         0.88
   ca              0.66         0.81         0.86
   sv              0.65         0.82         0.86
   cs              0.64         0.81         0.85
   ro              0.63         0.81         0.85
   de              0.62         0.75         0.78
   pl              0.62         0.79         0.83
   hu              0.61         0.80         0.84
   fi              0.61         0.80         0.84
   eo              0.61         0.80         0.85
   ru              0.60         0.78         0.82
   gl              0.60         0.77         0.82
   mk              0.58         0.79         0.84
   id              0.58         0.81         0.86
   bg              0.57         0.77         0.82
   ms              0.57         0.81         0.86
   uk              0.57         0.75         0.79
   sh              0.56         0.77         0.81
   hr              0.56         0.75         0.80
   tr              0.56         0.77         0.81
   sl              0.56         0.77         0.82
   el              0.54         0.75         0.80
   sk              0.54         0.75         0.81
   et              0.53         0.73         0.78
   sr              0.53         0.72         0.77
   af              0.52         0.75         0.80
   lt              0.50         0.72         0.79
   ar              0.48         0.69         0.75
   bs              0.47         0.70         0.77
   lv              0.47         0.68         0.75
   eu              0.46         0.68         0.75
   fa              0.45         0.68         0.75
   hy              0.43         0.66         0.73
   sq              0.43         0.65         0.71
   be              0.43         0.64         0.70
   zh              0.40         0.68         0.75
   ka              0.40         0.63         0.71
   cy              0.39         0.63         0.71
   hi              0.39         0.58         0.63
   az              0.38         0.60         0.67
   ko              0.37         0.58         0.66
   te              0.36         0.56         0.63
   kk              0.35         0.60         0.68
   he              0.33         0.45         0.48
   fy              0.33         0.52         0.60
   vi              0.31         0.53         0.62
   ta              0.31         0.50         0.56
   bn              0.30         0.49         0.56
   ur              0.29         0.52         0.61
   is              0.29         0.51         0.59
   tl              0.28         0.51         0.59
   kn              0.28         0.43         0.46
   gu              0.25         0.44         0.51
   mn              0.25         0.49         0.58
   uz              0.24         0.43         0.51
   si              0.22         0.40         0.45
   ml              0.21         0.35         0.39
   ky              0.20         0.40         0.49
   mr              0.20         0.37         0.44
   th              0.20         0.33         0.38
   la              0.19         0.34         0.42
   ja              0.18         0.44         0.56
   ne              0.16         0.33         0.38
   pa              0.16         0.32         0.38
   tg              0.14         0.31         0.39
   km              0.12         0.26         0.30
   my              0.10         0.19         0.23
   lb              0.09         0.18         0.21
   mg              0.07         0.18         0.25
   ceb             0.06         0.13         0.18

   as you can see, the alignment is consistently much better than random!
   in general, the procedure works best for other european languages like
   french, portuguese and spanish. we use 2500 word pairs, because of the
   5000 words in the test dictionary, not all the words found by the
   google translate api are actually present in the fasttext vocabulary.

   now let's do something much more exciting, let's evaluate the
   translation performance between all possible language pairs. we exhibit
   this translation performance on the heatmap below, where the colour of
   an element denotes the precision @1 when translating from the language
   of the row into the language of the column.

   [91]cool huh!

   we should emphasize that all of the languages were aligned to english
   only. we did not provide training dictionaries between non-english
   language pairs. yet we are still able to succesfully predict
   translations between pairs of non-english languages remarkably
   accurately.

   we expect the diagonal elements of the matrix above to be 1, since a
   language should translate perfectly to itself. however in practice this
   does not always occur, because we constructed the training and test
   dictionaries by translating common english words into the other
   languages. sometimes multiple english words translate to the same
   non-english word, and so the same non-english word may appear multiple
   times in the test set. we haven't properly accounted for this, which
   reduces the translation performance.

   intriquingly, even though we only directly aligned the languages to
   english, sometimes a language translates better to another non-english
   language than it does to english! we can calculate the inter-pair
   precision of two languages; the average precision from language 1 to
   language 2 and vice versa. we can also calculate the english-pair
   precision; the average of the precision from english to language 1 and
   from english to language 2. below we list all the language pairs for
   which the inter-pair precision exceeds the english-pair precision:
   language 1 language 2 inter-pair precision @1 english-pair precision @1
       bs         sh              0.88                     0.52
       ru         uk              0.84                     0.58
       ca         es              0.82                     0.69
       cs         sk              0.82                     0.59
       hr         sh              0.78                     0.56
       be         uk              0.77                     0.50
       gl         pt              0.76                     0.66
       bs         hr              0.74                     0.52
       be         ru              0.73                     0.51
       da         no              0.73                     0.67
       sr         sh              0.73                     0.54
       pt         es              0.72                     0.72
       ca         pt              0.70                     0.69
       gl         es              0.70                     0.66
       hr         sr              0.69                     0.54
       ca         gl              0.68                     0.63
       bs         sr              0.67                     0.50
       mk         sr              0.56                     0.55
       kk         ky              0.30                     0.28

   all of these language pairs share very close linguistic roots. for
   instance the first pair above are bosnian and serbo-croatian; bosnian
   is a variant of serbo-croatian. the second pair is russian and
   ukranian; both east-slavic languages. it seems that the more similar
   two languages are, the more similar the geometry of their fasttext
   vectors; leading to improved translation performance.

how do i know these matrices don't change the monolingual vectors?

   the matrices provided in this repository are orthogonal. intuitively,
   each matrix can be broken down into a series of rotations and
   reflections. rotations and reflections do not change the distance
   between any two points in a vector space; and consequently none of the
   inner products between word vectors within a language are changed, only
   the inner products between the word vectors of different languages are
   affected.

references

   there are a number of great papers on this topic. we've listed a few of
   them below:
    1. [92]enriching word vectors with subword information
       bojanowski et al., 2016
    2. [93]offline bilingual word vectors, orthogonal transformations and
       the inverted softmax
       smith et al., iclr 2017
    3. [94]exploiting similarities between languages for machine
       translation
       mikolov et al., 2013
    4. [95]improving vector space word representations using multilingual
       correlation
       faruqui and dyer, eacl 2014
    5. [96]improving zero-shot learning by mitigating the hubness problem
       dinu et al., 2014
    6. [97]learning principled bilingual mappings of id27s while
       preserving monolingual invariance
       artetxe et al., emnlp 2016

training and test dictionaries

   a number of readers have expressed an interest in the training and test
   dictionaries we used in this repository. we would have liked to upload
   these, however, while we have not taken legal advice, we are concerned
   that this could be interpreted as breaking the terms of the google
   translate api.

license

   the transformation matrices are distributed under the [98]creative
   commons attribution-share-alike license 3.0.

     *    2019 github, inc.
     * [99]terms
     * [100]privacy
     * [101]security
     * [102]status
     * [103]help

     * [104]contact github
     * [105]pricing
     * [106]api
     * [107]training
     * [108]blog
     * [109]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [110]reload to refresh your
   session. you signed out in another tab or window. [111]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/babylonpartners/fasttext_multilingual/commits/master.atom
   3. https://github.com/babylonpartners/fasttext_multilingual#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/babylonpartners/fasttext_multilingual
  32. https://github.com/join
  33. https://github.com/login?return_to=/babylonpartners/fasttext_multilingual
  34. https://github.com/babylonpartners/fasttext_multilingual/watchers
  35. https://github.com/login?return_to=/babylonpartners/fasttext_multilingual
  36. https://github.com/babylonpartners/fasttext_multilingual/stargazers
  37. https://github.com/login?return_to=/babylonpartners/fasttext_multilingual
  38. https://github.com/babylonpartners/fasttext_multilingual/network/members
  39. https://github.com/babylonpartners
  40. https://github.com/babylonpartners/fasttext_multilingual
  41. https://github.com/babylonpartners/fasttext_multilingual
  42. https://github.com/babylonpartners/fasttext_multilingual/issues
  43. https://github.com/babylonpartners/fasttext_multilingual/pulls
  44. https://github.com/babylonpartners/fasttext_multilingual/projects
  45. https://github.com/babylonpartners/fasttext_multilingual/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/word-vectors
  48. https://github.com/topics/machine-learning
  49. https://github.com/topics/machine-translation
  50. https://github.com/topics/natural-language-processing
  51. https://github.com/topics/nlp
  52. https://github.com/topics/distributed-representations
  53. https://github.com/babylonpartners/fasttext_multilingual/commits/master
  54. https://github.com/babylonpartners/fasttext_multilingual/branches
  55. https://github.com/babylonpartners/fasttext_multilingual/releases
  56. https://github.com/babylonpartners/fasttext_multilingual/graphs/contributors
  57. https://github.com/babylonpartners/fasttext_multilingual/blob/master/license
  58. https://github.com/babylonpartners/fasttext_multilingual/search?l=jupyter-notebook
  59. https://github.com/babylonpartners/fasttext_multilingual/search?l=python
  60. https://github.com/babylonpartners/fasttext_multilingual/find/master
  61. https://github.com/babylonpartners/fasttext_multilingual/archive/master.zip
  62. https://github.com/login?return_to=https://github.com/babylonpartners/fasttext_multilingual
  63. https://github.com/join?return_to=/babylonpartners/fasttext_multilingual
  64. https://desktop.github.com/
  65. https://desktop.github.com/
  66. https://developer.apple.com/xcode/
  67. https://visualstudio.github.com/
  68. https://github.com/nhammerla
  69. https://github.com/babylonpartners/fasttext_multilingual/commits?author=nhammerla
  70. https://github.com/babylonpartners/fasttext_multilingual/commit/f81f3e41a57d5d1e572903c327dc7f968cd34f7a
  71. https://github.com/babylonpartners/fasttext_multilingual/pull/17
  72. https://github.com/babylonpartners/fasttext_multilingual/commit/f81f3e41a57d5d1e572903c327dc7f968cd34f7a
  73. https://github.com/babylonpartners/fasttext_multilingual/commit/f81f3e41a57d5d1e572903c327dc7f968cd34f7a
  74. https://github.com/babylonpartners/fasttext_multilingual/tree/f81f3e41a57d5d1e572903c327dc7f968cd34f7a
  75. https://github.com/babylonpartners/fasttext_multilingual/tree/master/alignment_matrices
  76. https://github.com/babylonpartners/fasttext_multilingual/commit/66d52d6b32907c9a3e9dc183008af289303abb51
  77. https://github.com/babylonpartners/fasttext_multilingual/blob/master/license
  78. https://github.com/babylonpartners/fasttext_multilingual/blob/master/readme.md
  79. https://github.com/babylonpartners/fasttext_multilingual/blob/master/align_your_own.ipynb
  80. https://github.com/babylonpartners/fasttext_multilingual/blob/master/fasttext.py
  81. https://github.com/babylonpartners/fasttext_multilingual/blob/master/fasttext_checksums.txt
  82. https://github.com/babylonpartners/fasttext_multilingual/blob/master/multilingual_performance.png
  83. https://github.com/facebookresearch/fasttext/blob/master/pretrained-vectors.md
  84. https://arxiv.org/abs/1702.03859
  85. http://colah.github.io/posts/2014-07-nlp-id56s-representations/
  86. https://www.samtalksml.net/from-linear-regression-to-vector-representations/
  87. https://github.com/babylonpartners/fasttext_multilingual/blob/master/align_your_own.ipynb
  88. https://github.com/facebookresearch/fasttext/blob/master/pretrained-vectors.md
  89. https://cloud.google.com/translate/docs/
  90. https://www.samtalksml.net/aligning-vector-representations/
  91. https://github.com/babylonpartners/fasttext_multilingual/blob/master/multilingual_performance.png
  92. https://arxiv.org/abs/1607.04606
  93. https://arxiv.org/abs/1702.03859
  94. https://arxiv.org/abs/1309.4168
  95. http://repository.cmu.edu/cgi/viewcontent.cgi?article=1031&context=lti
  96. https://arxiv.org/abs/1412.6568
  97. https://pdfs.semanticscholar.org/9a2e/ed5f8175275af0d55d4aed39afc8e2b2acf2.pdf?_ga=1.8571637.130713154.1492676520
  98. https://creativecommons.org/licenses/by-sa/3.0/
  99. https://github.com/site/terms
 100. https://github.com/site/privacy
 101. https://github.com/security
 102. https://githubstatus.com/
 103. https://help.github.com/
 104. https://github.com/contact
 105. https://github.com/pricing
 106. https://developer.github.com/
 107. https://training.github.com/
 108. https://github.blog/
 109. https://github.com/about
 110. https://github.com/babylonpartners/fasttext_multilingual
 111. https://github.com/babylonpartners/fasttext_multilingual

   hidden links:
 113. https://github.com/
 114. https://github.com/babylonpartners/fasttext_multilingual
 115. https://github.com/babylonpartners/fasttext_multilingual
 116. https://github.com/babylonpartners/fasttext_multilingual
 117. https://help.github.com/articles/which-remote-url-should-i-use
 118. https://github.com/babylonpartners/fasttext_multilingual#aligning-the-fasttext-vectors-of-78-languages
 119. https://github.com/babylonpartners/fasttext_multilingual#tldr-just-tell-me-what-to-do
 120. https://github.com/babylonpartners/fasttext_multilingual#ok-so-how-did-you-obtain-these-matrices
 121. https://github.com/babylonpartners/fasttext_multilingual#right-now-prove-that-this-procedure-actually-worked
 122. https://github.com/babylonpartners/fasttext_multilingual#how-do-i-know-these-matrices-dont-change-the-monolingual-vectors
 123. https://github.com/babylonpartners/fasttext_multilingual#references
 124. https://github.com/babylonpartners/fasttext_multilingual#training-and-test-dictionaries
 125. https://github.com/babylonpartners/fasttext_multilingual#license
 126. https://github.com/
