   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]metaflow-ai
   [7]metaflow-ai
   [8]sign in[9]get started
     __________________________________________________________________

tensorflow: a proposal of good practices for files, folders and models
architecture

   [10]go to the profile of morgan
   [11]morgan (button) blockedunblock (button) followfollowing
   apr 28, 2017

   designing the right file architecture is not straightforward in machine
   learning. after struggling on that question for a few projects of my
   owns, i started to discover simple patterns that cover most of the use
   cases i stumbled upon when reading code or coding my own stuff.

   this article is about sharing those discoveries with you.

     disclaimer:

     this article is more a proposal than a definitive guide, but it
     succeeds quite well for me. it is intended to provide a starting
     point for beginners and maybe bootstrap a conversation.

     since i struggled to architecture my own works at first, i thought i
     could share this part of my work. of course, if you have better
     files architecture principles, please contact me so we can speak!

what one usually needs to achieve?

   think about what you will probably have to do when you will be doing ml
   (coding perspective):
     * you will have to code a model
     * this model will have (at least) two different stages: training
       stage and inferring stage (production)
     * you will have to feed a dataset to this model (training stage)
     * you might have to feed it single elements too (inferring state)
     * you will have to tune its hyper-parameters
     * to tune them you will have to make your model configurable and
       create a kind of    api   , at least an endpoint where you can push a
       configuration
     * you will need a nice folder architecture for training results (so
       you can skim through it and remember each experiment easily)
     * you will need to graph some metrics like the loss or accuracy (in
       training and production phase)
     * you want those graphs to be easily searchable out of the box.
     * you want to be able to replicate any experiment you do
     * you might even want to be able to jump back in time in the training
       phase to check your model a posteriori.

   that   s quite a lot and one can be easily lost in refactoring files and
   folders to make everything fit nicely, on top of that, there are
   probably other needs that i   m not even addressing in the list above so
   let   s try to find out some best practices.

the overall folder architecture

   an image is worth a thousand words:
   [1*lbq5hulyeuhppsvetedzkw.png]
   files architecture
     * readme file: most of the people are probably using github, so
       please take some time and write a good markdown with at least those
       sections:    installation   ,    usage   ,    test   ,    useful links    for any
       too heavy files to put directly into your repository.
     * main.py file: unique endpoint, simple. more information below. you
       can also do its variation with two files (train.py/infer.py) but i
       don   t really see the need and it usually ask for building two apis
       for each file.
     * data folder: create a folder and put inside a script to download
       your datasets. make your script prepare the folder nicely if
       needed, for example, the script can create the train/val/test
       sub-folders if they don   t exist.
     * models folder: this is where you put your model files. i don   t
       think there is only one way to deal with that folder, one can write
       a file per model or a filer per model type or even have
       sub-folders. just try to keep consistency.
     * __init__ file: i dig more into this file below but it   s a python
       helper to make your models more accessible and simplify the
       potential complexity of the models    folder.
     * basic_model.py file: i dig more this file below too. i think most
       of the models in tensorflow can share a common architecture, i   ll
       explain what choices i made and why.
     * hpsearch folder: this folder will contain any helpers for doing
       custom hyper-parameters search. if you use a library to do it, you
       potentially don   t need it but a lot of time you need to customise
       something anyway. keep those functions pure and separated so you
       can test them easily.
     * tests folder: a test folder     for your tests     you do them, right?
     * results folder: obviously, this one will contain results. more
       information in tf on how to provide a good sub-folder architecture
       for tensorboard below.

     note: add a    .gitkeep    file in the results folder and add the folder
     to your    .gitignore    file. you probably don   t want to push all your
     experiments to your github and yet avoid your code to break from a
     fresh install because this folder is missing.

   this is the very basics. of course, one might need to add other folders
   but somehow it seems to boil down to this basic set.

   with a good readme and other bash scripts as helpers, anyone coming to
   your repository can simply follow the    install    commands and    usage   
   commands to start reproducing your work     
   [1*f1hhgpokvpsnnq_xs4qk3w.jpeg]
   a man of culture

a basic model

   as i said, i ended up recognising patterns in the way models are
   engineered in tf. this leads me to design a very simple class which
   would be extended by all my future model.

   i   m not a fan of class inheritance but i   m neither a fan of rewriting
   the same piece of code forever and clearly, when you do ml, models
   share many similarities needed by the framework you use.

   so i tried to find an implementation avoiding the known banana problem
   of inheritance while letting one inherits as deep as it wants.

   to be completely clear, consider this class as the top parent of future
   models, leading to the construction of your model in one line with one
   argument: the configuration.

   for the sake of clarity, let me show you the commented file directly:

   iframe: [12]/media/8927cecc78c04148ad6d09a76e6d416b?postid=f23171501ae3

   basic model file

   some notes:
     * regarding the    best    attribute of my configurations: people usually
       ship their code without the model best hyper-parameters, does
       anyone know why (honest question)?
     * the random function is static because you don   t want to instantiate
       your model to have access to it and yet, one can wonder why adding
       it to the model itself? because it is usually completely tied to
       the custom parameters of the model. note that this function must be
       pure and can be made as complex as needed.
     * the example init function is the simplest version, it will always
       load the last saved models of an existing folder or (if the
       result_dir is empty) initialise randomly using the init_op

   the __init__ script

   the __init__ script you can see on the folder structure does not have
   much to do with ml. yet it is a simple way to make your code more
   accessible for yourself and others.

   it adds a few lines of codes to make any model class directly
   accessible from the namespace models: so you can type anywhere in your
   code: from models import mymodel, whatever how deep is your model
   folder architecture.

   here is an example of a script achieving this task:

   iframe: [13]/media/ffd89d853460341e50fb452b77c37b9c?postid=f23171501ae3

   nothing fancy, but i found it useful so i decided to add it to this
   post.     

the shell api

   we have a global consistent folder structure, a nice basic class to
   build our models, a nice python script to load our class easily but
   designing the    shell api    and especially its default value is of the
   same importance.

   because the main endpoint to interact with your work in ml is
   definitely the shell whatever tool you are using. the shell is the
   cornerstone of your experiments.

   the last thing you want is having to change hardcoded values in your
   code to iterate over those experiments, so you need to have access to
   all the hyper-parameters directly from the shell. you also want to have
   access to all other parameters like the result directory or the stage
   (hp search/training/inferring) etc.

   again for the sake of clarity, let me show you the commented file
   directly:

   iframe: [14]/media/c2a1329f0d11cdc9db4c145eeb6c4abb?postid=f23171501ae3

   that   s it! hope it will help newcomers in the field to bootstrap their
   own research and feel free to comment or ask questions.
     __________________________________________________________________

tensorflow best practice series

   this article is part of a more complete series of articles about
   tensorflow. i   ve not yet defined all the different subjects of this
   series, so if you want to see any area of tensorflow explored, add a
   comment! so far i wanted to explore those subjects (this list is
   subject to change and is in no particular order):
     * [15]a primer
     * [16]how to handle shapes in tensorflow
     * [17]tensorflow saving/restoring and mixing multiple models
     * [18]how to freeze a model and serve it with a python api
     * tensorflow: a proposal of good practices for files, folders and
       models architecture (this one :) )
     * [19]tensorflow howto: a universal approximator inside a neural net
     * [20]how to optimise your input pipeline with queues and
       multi-threading
     * [21]mutating variables and control flow
     * how to handle preprocessing with tensorflow.
     * how to control the gradients to create custom back-prop.
     * how to monitor and inspect my models to gain insight into them.

     note: tf is evolving fast right now, those articles are currently
     written for the 1.0.0 version and python 3.
     __________________________________________________________________

reference

     * the awesome gitxiv platform: [22]http://www.gitxiv.com/
     * my own work: [23]https://github.com/metaflow-ai &
       [24]https://github.com/morgangiraud/openai-rl
     * you can find the global architecture [25]here

     * [26]tensorflow
     * [27]machine learning
     * [28]data science
     * [29]artificial intelligence
     * [30]tutorial

   (button)
   (button)
   (button) 630 claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of morgan

[32]morgan

   machine learning enthusiast, i spend my time reading scientific papers,
   replicating work and waiting for my own creativity to kick in!

     (button) follow
   [33]metaflow-ai

[34]metaflow-ai

   we are a bunch of optimist machine learning scientist doing crazy
   projects! contact us for more information.

     * (button)
       (button) 630
     * (button)
     *
     *

   [35]metaflow-ai
   never miss a story from metaflow-ai, when you sign up for medium.
   [36]learn more
   never miss a story from metaflow-ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.metaflow.fr/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f23171501ae3
   4. https://medium.com/
   5. https://medium.com/
   6. https://blog.metaflow.fr/?source=avatar-lo_u2ndnyxeyjch-854dfaf8c32d
   7. https://blog.metaflow.fr/?source=logo-lo_u2ndnyxeyjch---854dfaf8c32d
   8. https://medium.com/m/signin?redirect=https://blog.metaflow.fr/tensorflow-a-proposal-of-good-practices-for-files-folders-and-models-architecture-f23171501ae3&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://blog.metaflow.fr/tensorflow-a-proposal-of-good-practices-for-files-folders-and-models-architecture-f23171501ae3&source=--------------------------nav_reg&operation=register
  10. https://blog.metaflow.fr/@morgangiraud?source=post_header_lockup
  11. https://blog.metaflow.fr/@morgangiraud
  12. https://blog.metaflow.fr/media/8927cecc78c04148ad6d09a76e6d416b?postid=f23171501ae3
  13. https://blog.metaflow.fr/media/ffd89d853460341e50fb452b77c37b9c?postid=f23171501ae3
  14. https://blog.metaflow.fr/media/c2a1329f0d11cdc9db4c145eeb6c4abb?postid=f23171501ae3
  15. https://blog.metaflow.fr/tensorflow-a-primer-4b3fa0978be3#.5ujc1tdw3
  16. https://blog.metaflow.fr/shapes-and-dynamic-dimensions-in-tensorflow-7b1fe79be363#.dcmxo9jf5
  17. https://blog.metaflow.fr/tensorflow-saving-restoring-and-mixing-multiple-models-c4c94d5d7125#.f78fep94t
  18. https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc#.f8cininaw
  19. https://blog.metaflow.fr/tensorflow-howto-a-universal-approximator-inside-a-neural-net-bb034430b71e#.u6epmpcam
  20. https://blog.metaflow.fr/tensorflow-how-to-optimise-your-input-pipeline-with-queues-and-multi-threading-e7c3874157e0#.n82a6uin4
  21. https://blog.metaflow.fr/tensorflow-mutating-variables-and-control-flow-2181dd238e62
  22. http://www.gitxiv.com/
  23. https://github.com/metaflow-ai
  24. https://github.com/morgangiraud/openai-rl
  25. https://github.com/metaflow-ai/blog/tree/master/tf-architecture
  26. https://blog.metaflow.fr/tagged/tensorflow?source=post
  27. https://blog.metaflow.fr/tagged/machine-learning?source=post
  28. https://blog.metaflow.fr/tagged/data-science?source=post
  29. https://blog.metaflow.fr/tagged/artificial-intelligence?source=post
  30. https://blog.metaflow.fr/tagged/tutorial?source=post
  31. https://blog.metaflow.fr/@morgangiraud?source=footer_card
  32. https://blog.metaflow.fr/@morgangiraud
  33. https://blog.metaflow.fr/?source=footer_card
  34. https://blog.metaflow.fr/?source=footer_card
  35. https://blog.metaflow.fr/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/f23171501ae3/share/twitter
  39. https://medium.com/p/f23171501ae3/share/facebook
  40. https://medium.com/p/f23171501ae3/share/twitter
  41. https://medium.com/p/f23171501ae3/share/facebook
