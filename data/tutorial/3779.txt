   #[1]piotr migda   - blog

   [2]piotr migda   - blog

   [3]projects [4]articles [5]publications [6]resume [7]about [8]photos

learning deep learning with keras

   30 apr 2017     piotr migda       [machine-learning] [deep-learning]
   [overview]
   also reprinted to kdnuggets [9]first steps of learning deep learning:
   image classification in keras on 16 aug 2017 see: [10]tweet by fran  ois
   chollet (the creator of keras) with over 140 retweets see: [11]facebook
   post by kaggle with over 200 shares see: [12]hacker news front page

   i teach deep learning both for a living (as the main [13]deepsense.ai
   instructor, in a kaggle-winning team^[14]1) and as a part of my
   volunteering with the [15]polish children   s fund giving workshops to
   gifted high-school students^[16]2. i want to share a few things i   ve
   learnt about teaching (and learning) deep learning.

   whether you want to start learning deep learning for you career, to
   have a nice adventure (e.g. with [17]detecting huggable objects) or to
   get insight into machines before they take over^[18]3, this post is for
   you! its goal is not to teach neural networks by itself, but to provide
   an overview and to point to didactically useful resources.

   deep learning meme - what i actually do (keras version)

   don   t be afraid of id158s - it is easy to start! in
   fact, my biggest regret is delaying learning it, because of the
   perceived difficulty. to start, all you need is really basic
   programming, very simple mathematics and knowledge of a few machine
   learning concepts. i will explain where to start with these
   requirements.

   in my opinion, the best way to start is from a high-level interactive
   approach (see also: [19]quantum mechanics for high-school students and
   my [20]quantum game with photons). for that reason, i suggest starting
   with image recognition tasks in [21]keras, a popular neural network
   library in python. if you like to train neural networks with less code
   than in keras, the only viable option is to [22]use pigeons. yes,
   seriously: [23]pigeons spot cancer as well as human experts!

what is deep learning and why is it cool?

   deep learning is a name for machine learning techniques using
   many-layered id158s. occasionally people use the
   term artificial intelligence, but unless you want to sound sci-fi, it
   is reserved for problems that are currently considered    too hard for
   machines    - a frontier that keeps moving rapidly. this is a field that
   exploded in the last few years, reaching human-level accuracy in visual
   recognition tasks (among many other tasks), see:
     * [24]measuring the progress of ai research by electronic frontier
       foundation (2017)

   unlike quantum computing, or fusion power - it is a technology that is
   being applied right now, not some possibility for the future. there is
   a rule of thumb:

     pretty much anything that a normal person can do in <1 sec, we can
     now automate with ai. - [25]andrew ng   s tweet

   some people go even further, [26]extrapolating that statement to
   experts. it   s not a surprise that [27]companies like google and
   facebook at the cutting-edge of progress. in fact, every few months i
   am blown away by something exceeding my expectations, e.g.:
     * [28]the unreasonable effectiveness of recurrent neural
       networks^[29]4 for generating fake shakespeare, wikipedia entries
       and latex articles
     * [30]a neural algorithm of artistic style style transfer ([31]and
       for videos!)
     * [32]real-time face capture and reenactment
     * [33]colorful image colorization
     * [34]plug & play generative networks for photorealistic image
       generation
     * [35]dermatologist-level classification of skin cancer along with
       other medical diagnostic tools
     * [36]image-to-image translation (pix2pix) - sketch to photo
     * [37]teaching machines to draw sketches of cats, dogs etc

   it looks like some sorcery. if you are curious what neural networks
   are, take a look at this series of videos for a smooth introduction:
     * [38]neural networks demystified by stephen welch - video series
     * [39]a visual and interactive guide to the basics of neural networks
       by j alammar

   these techniques are data-hungry. see a plot of [40]auc score for
   [41]id28, id79 and deep learning on higgs
   dataset (data points are in millions):

   id28 vs id79 vs deep learning on higgs dataset

   in general there is no guarantee that, even with a lot of data, deep
   learning does better than other techniques, for example tree-based such
   as id79 or [42]boosted trees.

let   s play!

   do i need some [43]skynet to run it? actually not - it   s a piece of
   software, like any other. and you can even play with it in your
   browser:
     * [44]tensorflow playground for point separation, with a visual
       interface
     * [45]convnetjs for digit and image recognition
     * [46]keras.js demo - to visualize and use real networks in your
       browser (e.g. resnet-50)

   or    if you want to use keras in python, see [47]this minimal example -
   just to get convinced you can use it on your own computer.

python and machine learning

   i mentioned basics python and machine learning as a requirement. they
   are already covered in [48]my introduction to data science in
   [49]python and [50]statistics and machine learning sections,
   respectively.

   for python, if you already have [51]anaconda distribution (covering
   most data science packages), the only thing you need is to install
   [52]tensorflow and [53]keras.

   when it comes to machine learning, you don   t need to learn many
   techniques before jumping into deep learning. though, later it would be
   a good practice to see if a given problem can be solved with much
   simpler methods. for example, [54]id79 is often a lockpick,
   working out-of-the-box for many problems. you need to understand why we
   need to train and then test a classifier (to validate its predictive
   power). to get the gist of it, start with this beautiful tree-based
   animation:
     * [55]visual introduction to machine learning by stephanie yee and
       tony chu

   also, it is good to understand [56]id28, which is a
   building block of almost any neural network for classification.

mathematics

   deep learning (that is - neural networks with many layers) uses mostly
   very simple mathematical operations - just many of them. here there are
   a few, which you can find in almost any network (look at this list, but
   don   t get intimidated):
     * vectors, matrices, multi-dimensional arrays,
     * addition, multiplication,
     * [57]convolutions to extract and process local patterns,
     * id180: [58]sigmoid, [59]tanh or [60]relu to add
       non-linearity,
     * [61]softmax to convert vectors into probabilities,
     * [62]log-loss (cross-id178) to penalize wrong guesses in a smart
       way (see also [63]id181 explained),
     * gradients and chain-rule ([64]id26) for optimizing
       network parameters,
     * stochastic id119 and its variants (e.g. [65]momentum).

   if your background is in mathematics, statistics, physics^[66]5 or
   signal processing - most likely you already know more than enough to
   start!

   if your last contact with mathematics was in high-school, don   t worry.
   its mathematics is simple to the point that a convolutional neural
   network for digit recognition can be implemented in a spreadsheet (with
   no macros), see: [67]deep spreadsheets with excelnet. it is only a
   proof-of-principle solution - not only inefficient, but also lacking
   the most crucial part - the ability to train new networks.

   the basics of vector calculus are crucial not only for deep learning,
   but also for many other machine learning techniques (e.g. in
   [68]id97 i wrote about). to learn it, i recommend starting from one
   of the following:
     * [69]getting started with id202 for deep learning by
       hadrien jean (an intro to [70]id202 from the [71]deep
       learning)
     * j. str  m, k.   str  m, and t. akenine-m  ller, [72]immersive linear
       algebra - a id202 book with fully interactive figures
     * [73]id202 cheat sheet for deep learning by brendan
       fortuner

   since there are many references to [74]numpy, it may be useful to learn
   its basics:
     * [75]from python to numpy by nicolas p. rougier
     * [76]scipy lectures: the numpy array object

   at the same time - look back at the meme, at the what mathematicians
   think i do part. it   s totally fine to start from a magically working
   code, treating neural network layers like lego blocks.

frameworks

   there is a handful of popular deep learning libraries, including
   [77]tensorflow, [78]theano, [79]torch and [80]caffe. each of them has
   python interface (now also for torch: [81]pytorch).

   so, which to choose? first, as always, screw all subtle performance
   benchmarks, as [82]premature optimization is the root of all evil. what
   is crucial is to start with one which is easy to write (and read!), one
   with many online resources, and one that you can actually install on
   your computer without too much pain.

   bear in mind that core frameworks are multidimensional array expression
   compilers with gpu support. current neural networks can be expressed as
   such. however, if you just want to work with neural networks, by
   [83]rule of least power, i recommend starting with a framework just for
   neural networks. for example   

keras

   if you like the [84]philosophy of python (brevity, readability, one
   preferred way to do things), [85]keras is for you. it is a high-level
   library for neural networks, using tensorflow or theano as its backend.
   also, if you want to have a propaganda picture, there is a possibly
   biased (or [86]overfitted?) popularity ranking:
     * [87]the state of deep learning frameworks (from github metrics),
       april 2017. - fran  ois chollet (keras creator)

   if you want to consult a different source, based on arxiv papers rather
   than github activity, see [88]a peek at trends in machine learning by
   andrej karpathy. popularity is important - it means that if you want to
   search for a network architecture, googling for it (e.g. unet keras) is
   likely to return an example. where to start learning it? documentation
   on keras is nice, and [89]its blog is a valuable resource. for a
   complete, interactive introduction to deep learning with keras in
   [90]jupyter notebook, i really recommend:
     * [91]deep learning with keras and tensorflow by valerio maggio

   for shorter ones, try one of these:
     * [92]visualizing parts of convolutional neural networks using keras
       and cats by erik reppel
     * [93]deep learning for complete beginners: convolutional neural
       networks with keras by petar veli  kovi  
     * [94]handwritten digit recognition using convolutional neural
       networks in python with keras by jason brownlee (theano tensor
       dimension order^[95]6)

   there are a few add-ons to keras, which are especially useful for
   learning it. i created [96]ascii summary for sequential models to show
   data flow inside networks (in a nicer way than model.summary()). it
   shows layers, dimensions of data (x, y, channels) and the number of
   free parameters (to be optimized). for example, for [97]a network for
   digit recognition it might look like:
           operation           data dimensions   weights(n)   weights(%)

               input   #####     32   32    3
              conv2d    \|/  -------------------       896     0.1%
                relu   #####     32   32   32
              conv2d    \|/  -------------------      9248     0.7%
                relu   #####     30   30   32
        maxpooling2d   y max -------------------         0     0.0%
                       #####     15   15   32
             dropout    | || -------------------         0     0.0%
                       #####     15   15   32
              conv2d    \|/  -------------------     18496     1.5%
                relu   #####     15   15   64
              conv2d    \|/  -------------------     36928     3.0%
                relu   #####     13   13   64
        maxpooling2d   y max -------------------         0     0.0%
                       #####      6    6   64
             dropout    | || -------------------         0     0.0%
                       #####      6    6   64
             flatten   ||||| -------------------         0     0.0%
                       #####        2304
               dense   xxxxx -------------------   1180160    94.3%
                relu   #####         512
             dropout    | || -------------------         0     0.0%
                       #####         512
               dense   xxxxx -------------------      5130     0.4%
             softmax   #####          10

   you might be also interested in nicer progress bars with
   [98]keras-tqdm, exploration of activations at each layer with
   [99]quiver, checking attention maps with [100]keras-vis or converting
   keras models to javascript, runnable in a browser with [101]keras.js.
   speaking of languages, there is also [102]r interface to keras.

   edit (march 2018): also, i wrote [103]livelossplot - a live training
   loss plot in jupyter notebook (for keras, pytorch and other
   frameworks).

tensorflow

   if not keras, then i recommend starting with bare tensorflow. it is a
   bit more low-level and verbose, but makes it straightforward to
   optimize various multidimensional array (or, well, tensor) operations.
   a few good resources:
     * the official [104]tensorflow tutorial is very good
     * [105]learn tensorflow and deep learning, without a ph.d. by martin
       g  rner
     * [106]tensorflow tutorial and examples for beginners by aymeric
       damien (with python 2.7)
     * [107]simple tutorials using google   s tensorflow framework by nathan
       lintz

   in any case, [108]tensorboard makes it easy to keep track of the
   training process. it can also be [109]used with keras, via callbacks.

other

   [110]theano is similar to tensorflow, but a bit older and harder to
   start. for example, you need to manually write updates of variables.
   typical neural network layers are not included, so one often uses
   libraries such as [111]lasagne. if you   re looking for a place to start,
   i like this introduction:
     * [112]theano tutorial by marek rei

   at the same time, if you see some nice code in torch or pytorch, don   t
   be afraid to install and run it!

   edit (july 2017): if you want a low-level framework, [113]pytorch may
   be the best way to start. it combines relatively brief and readable
   code (almost like keras) but at the same time gives low-level access to
   all features (actually, more than tensorflow). start here:
     * [114]pytorch - tutorials
     * [115]a repository showcasing examples of using pytorch

   edit (june 2018): in [116]keras or pytorch as your first deep learning
   framework i discuss pros and cons of starting learning deep learning
   with each of them.

datasets

   every machine learning problem needs data. you cannot just tell it
      detect if there is a cat in this picture    and expect the computer to
   tell you the answer. you need to show many instances of cats, and
   pictures not containing cats, and (hopefully) it will learn to
   generalize it to other cases. so, you need some data to start. and it
   is not a drawback of machine learning or just deep learning - it is a
   fundamental property of any learning!

   before you dive into uncharted waters, it is good to take a look at
   some [117]popular datasets. the key part about them is that they are   
   popular. it means that you can find a lot of examples what works. and
   have a guarantee that these problems can be solved with neural
   networks.

mnist

     many good ideas will not work well on mnist (e.g. batch norm).
     inversely many bad ideas may work on mnist and no[t] transfer to
     real [id161]. - [118]fran  ois chollet   s tweet

   still, i recommend starting with the [119]mnist digit recognition
   dataset (60k grayscale 28x28 images), included in [120]keras.datasets.
   not necessary to master it, but just to get a sense that it works at
   all (or to test the basics of keras on your local machine).

notmnist

     indeed, i once even proposed that the toughest challenge facing ai
     workers is to answer the question:    what are the letters    a    and
        i   ? - [121]douglas r. hofstadter (1995)

   a more interesting dataset, and harder for classical machine learning
   algorithms, is [122]notmnist (letters a-j from strange fonts). if you
   want to start with it, here is my [123]code for notmnist loading and
   id28 in keras.

cifar

   if you want to play with image recognition, there is [124]cifar
   dataset, a dataset of 32x32 photos (also in [125]keras.datasets). it
   comes in two versions: 10 simple classes (including cats, dogs, frogs
   and airplanes ) and 100 harder and more nuanced classes (including
   beaver, dolphin, otter, seal and whale). i strongly suggest
   [126]starting with cifar-10, the simpler version. beware, [127]more
   complicated networks may take quite some time (~12h on cpu my 7 year
   old macbook pro).

   edit (nov 2017): if you are interested in practical exercises, i wrote
   [128]starting deep learning hands-on: image classification on cifar-10.

more

   deep learning requires a lot of data. if you want to train your network
   from scratch, it may require as many as ~10k images even if
   low-resolution (32x32). especially if data is scarce, there is no
   guarantee that a network will learn anything. so, what are the ways to
   go?
     * use really low res (if your eye can see it, no need to use higher
       resolution)
     * get a lot of data (for images like 256x256 it may be: millions of
       instances)
     * re-train a network that already saw a lot
     * generate much more data (with rotations, shifts, distortions)

   often, it   s a combination of everything mentioned here.

   edit (may 2018): were to look for suitable datasets? [129]kaggle
   datasets is a place to start (along with some [130]kaggle
   competitions).

standing on the shoulders of giants

   creating a new neural network has a lot in common with cooking - there
   are typical ingredients (layers) and recipes (popular network
   architectures). the most important cooking contest is [131]id163
   large scale visual recognition challenge, with recognition of hundreds
   of classes from half a million dataset of photos. look at these
   [132]neural network architectures, typically using 224x224x3 input
   (chart by eugenio culurciello):

   deep learning architectures - a scatter plot of network sizes,
   performances and ops per run

   circle size represents the number of parameters (a lot!). it doesn   t
   mention [133]squeezenet though, an architecture vastly reducing the
   number of parameters (e.g. 50x fewer).

   a few key networks for image classification can be readily loaded from
   the [134]keras.applications module: xception, vgg16, vgg19, resnet50,
   inceptionv3. some others are not as plug & play, but still easy to find
   online - yes, there is [135]squeezenet in keras. these networks serve
   two purposes:
     * they give insight into useful building blocks and architectures
     * they are great candidates for retraining (so-called [136]transfer
       learning), when using architecture along with pre-trained weights)

   some other important network architectures for images:
     * [137]u-net: convolutional networks for biomedical image
       segmentation
          + [138]retina blood vessel segmentation with a convolution
            neural network - keras implementation
          + [139]deep learning tutorial for kaggle ultrasound nerve
            segmentation competition, using keras
     * [140]a neural algorithm of artistic style
          + [141]neural style transfer & neural doodles implemented in
            keras by somshubra majumdar
     * [142]a brief history of id98s in image segmentation: from r-id98 to
       mask r-id98 by dhruv parthasarathy

   another set of insights:
     * [143]the neural network zoo by fjodor van veen
     * [144]how to train your deep neural network - how many layers,
       parameters, etc

infrastructure

   for very small problems (e.g. mnist, notmnist), you can use your
   personal computer - even if it is a laptop and computations are on cpu.

   for small problems (e.g. cifar, the unreasonable id56), you might be
   still able to use a pc, but it requires much more patience and
   trade-offs.

   for medium and larger problems, essentially the only way to go is to
   use a machine with a strong graphic card (gpu). for example, it took us
   2 days to train a model for satellite image processing for a kaggle
   competition, see our:
     * [145]deep learning for satellite imagery via image segmentation by
       arkadiusz nowaczy  ski

   on a strong cpu it would have taken weeks, see:
     * [146]benchmarks for popular convolutional neural network models by
       justin johnson

   the easiest, and the cheapest, way to use a strong gpu is to rent a
   remote machine on a per-hour basis. you can use amazon (it is not only
   a bookstore!), here are some guides:
     * [147]keras with gpu on amazon ec2     a step-by-step instruction by
       mateusz sieniawski, my mentee
     * [148]running jupyter notebooks on gpu on aws: a starter guide by
       francois chollet

   edit (dec 2017): for a hassle-free gpu support for deep learning i
   recommend [149]neptune: machine learning lab.

further learning

   i encourage you to interact with code. for example, notmnist or
   cifar-10 can be great starting points. sometimes the best start is to
   start with someone   s else code and run it, then see what happens when
   you modify parameters.

   for learning how it works, this one is a masterpiece:
     * [150]cs231n: convolutional neural networks for visual recognition
       by andrej karpathy and the [151]lecture videos

   when it comes to books, there is a wonderful one, starting from
   introduction to mathematics and machine learning learning context (it
   even covers [152]log-loss and id178 in a way i like!):
     * [153]deep learning, an mit press book by ian goodfellow, yoshua
       bengio and aaron courville

   alternatively, you can use (it may be good for an introduction with
   interactive materials, but i   ve found the style a bit long-winded):
     * [154]neural networks and deep learning by michael nielsen

   edit (dec 2017): for a very practical introduction to deep learning
   with keras, i recommend [155]deep learning with python by fran  ois
   chollet.

other materials

   there are many applications of deep learning (it   s not only image
   recognition!). i collected some introductory materials to cover its
   various aspects (beware: they are of various difficulty). don   t try to
   read them all - i list them for inspiration, not intimidation!
     * general
          + [156]the unreasonable effectiveness of recurrent neural
            networks by andrej karpathy
          + [157]how convolutional neural networks see the world - keras
            blog
          + [158]what convolutional neural networks look at when they see
            nudity     clarifai blog (nsfw)
          + [159]convolutional neural networks for artistic style transfer
            by harish nrayanan
          + [160]dreams, drugs and convnets - my slides (nsfw); i am
            considering turning it into a longer post on machine learning
            vs human learning, based on common mistakes
     * technical
          + [161]yes you should understand backprop by andrej karpathy
          + [162]id21 using keras by prakash vanapalli
          + [163]id3 (gans) in 50 lines of
            code (pytorch)
          + [164]minimal and clean id23 examples
          + [165]an overview of id119 optimization algorithms
            by sebastian ruder
          + [166]picking an optimizer for style transfer by slav ivanov
          + [167]building autoencoders in keras by francois chollet
          + [168]understanding id137 by chris olah
          + [169]recurrent neural networks & lstms by rohan kapur
          + [170]oxford deep nlp 2017 course
     * list of resources
          + [171]how to start learning deep learning by ofir press
          + [172]a guide to deep learning by yn^2
     * staying up-to-date:
          + [173]r/machinelearning reddit channel covering most of new
            stuff
          + [174]distill.pub - an interactive, visual, open-access journal
            for machine learning research, with expository articles
          + my links at [175]pinboard.in/u:pmigdal/t:deep-learning -
            though, just saving, not an automatic recommendation
          + [176]@fastml_extra twitter channel
          + [177]gitxiv for papers with code
          + don   t be afraid to read academic papers; some are well-written
            and insightful (if you own kindle or another e-reader, i
            recommend [178]dontprint)
     * data (usually from challenges)

thanks

   i would like to thank [179]kasia kulma, [180]martina pugliese, pawe  
   subko, [181]monika paw  owska and [182]  ukasz kidzi  ski for helpful
   feedback on the content and to [183]sarah martin for polishing my
   english.

   if you recommend a source that helped you with your adventure with deep
   learning - feel invited to contact me! ([184]@pmigdal for short links,
   an email for longer remarks.)

   the [185]deep learning meme is not mine - i just rewrote it from theano
   to keras (with tensorflow backend).
    1. [186]noaa right whale recognition, winners    interview (1st place,
       jan 2016), and a fresh one: [187]deep learning for satellite
       imagery via image segmentation (4th place, apr 2017). [188]   
    2. this january during a 5-day workshop 6 high-school students
       participated in a rather nsfl project - constructing a neural
       network for detecting trypophobia triggers, see [189]trypophobia
       image detector - browser plugin using deep learning github
       repository. [190]   
    3. it made a few episodes of webcomics obsolete: [191]xkcd: tasks
       (totally, by [192]park or bird?), [193]xkcd: game ai (partially, by
       [194]alphago), [195]phd comics: if tv science was more like real
       science (not exactly, but still it   s cool, by [196]lapsrn). [197]   
    4. the title alludes to [198]the unreasonable effectiveness of
       mathematics in the natural sciences by eugene wigner (1960), one of
       my favourite texts in philosophy of science. along with [199]more
       is different by pw andreson (1972) and [200]genesis and development
       of a scientific fact ([201]pdf here) by ludwik fleck (1935). [202]   
    5. if your background is in quantum information, the only thing you
       need to change is     to    . just expect less tensor structure, but
       more convolutions. [203]   
    6. is it only me, or does theano tensor dimension order sound like
       some secret convent? before you start searching how to join it: it
       is about the shape of multi-dimensional arrays: (samples, channels,
       x, y) rather than tensorflow   s (samples, x, y, channels). [204]   

   [205]tweet
   [206][hn.gif] hn submission/discussion [207][mailchimp.png] get
   notified about new posts via email (mailchimp)

   piotr migda   - as of 2013
     * [208]pmigdal@gmail.com
     * [209]https://p.migdal.pl

     * [210]stared
     * [211]pmigdal
     *

   piotr migda   - an independent data science consultant, with phd in
   quantum physics; based in warsaw, poland. believing in [212]side
   projects, active in [213]gifted education, developing the [214]quantum
   game. collaborating with [215]deepsense.ai. co-founded [216]in browser
   ai.

   python (+    of r) for data analysis and machine learning, javascript
   for data visualization. currently focusing on deep learning.

references

   visible links
   1. https://p.migdal.pl/feed.xml
   2. https://p.migdal.pl/
   3. https://p.migdal.pl/projects/
   4. https://p.migdal.pl/articles/
   5. https://p.migdal.pl/publications/
   6. https://p.migdal.pl/resume/
   7. https://p.migdal.pl/about/
   8. http://migdal.zenfolio.com/
   9. http://www.kdnuggets.com/2017/08/first-steps-learning-deep-learning-image-classification-keras.html
  10. https://twitter.com/fchollet/status/858840192261644292
  11. https://www.facebook.com/kaggle/photos/a.10150387148668464.377856.135534208463/10155346265978464/?type=3&theater
  12. https://news.ycombinator.com/item?id=15999578
  13. http://deepsense.ai/
  14. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html#fn:deepsense
  15. http://crastina.se/gifted-children-in-poland-by-piotr-migdal/
  16. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html#fn:trypophobia
  17. https://www.reddit.com/r/machinelearning/comments/4casci/can_i_hug_that_i_trained_a_classifier_to_tell_you/
  18. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html#fn:webcomics
  19. http://p.migdal.pl/2016/08/15/quantum-mechanics-for-high-school-students.html
  20. http://quantumgame.io/
  21. https://keras.io/
  22. https://www.youtube.com/watch?v=flzgjnjlys0
  23. http://www.sciencemag.org/news/2015/11/pigeons-spot-cancer-well-human-experts
  24. https://www.eff.org/ai/metrics
  25. https://twitter.com/andrewyng/status/788548053745569792
  26. https://lukeoakdenrayner.wordpress.com/2017/04/24/the-end-of-human-doctors-understanding-medicine/
  27. http://www.economist.com/news/business/21695908-silicon-valley-fights-talent-universities-struggle-hold-their
  28. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  29. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html#fn:unreasonable
  30. https://arxiv.org/abs/1508.06576
  31. https://www.youtube.com/watch?v=khuj4asldmu
  32. https://www.youtube.com/watch?v=ohmajjtcpnk
  33. http://richzhang.github.io/colorization/
  34. http://www.evolvingai.org/ppgn
  35. http://cs.stanford.edu/people/esteva/nature/
  36. https://phillipi.github.io/pix2pix/
  37. https://research.googleblog.com/2017/04/teaching-machines-to-draw.html
  38. https://www.youtube.com/watch?v=bxe2t-v8xrs
  39. http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/
  40. https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it
  41. https://github.com/szilard/benchm-ml/tree/master/x1-data-higgs
  42. https://xgboost.readthedocs.io/en/latest/model.html
  43. https://en.wikipedia.org/wiki/skynet_(terminator)
  44. http://playground.tensorflow.org/
  45. http://cs.stanford.edu/people/karpathy/convnetjs/
  46. https://transcranial.github.io/keras-js/
  47. https://github.com/stared/keras-mini-examples/blob/master/mnist_simple.ipynb
  48. http://p.migdal.pl/2016/03/15/data-science-intro-for-math-phys-background.html
  49. http://p.migdal.pl/2016/03/15/data-science-intro-for-math-phys-background.html#python
  50. http://p.migdal.pl/2016/03/15/data-science-intro-for-math-phys-background.html#statistics-and-machine-learning
  51. https://www.continuum.io/downloads
  52. https://www.tensorflow.org/install/
  53. https://keras.io/#installation
  54. http://blog.yhat.com/posts/random-forests-in-python.html
  55. http://www.r2d3.us/visual-intro-to-machine-learning-part-1/
  56. https://en.wikipedia.org/wiki/logistic_regression
  57. http://setosa.io/ev/image-kernels/
  58. https://en.wikipedia.org/wiki/sigmoid_function
  59. https://www.wolframalpha.com/input/?i=tanh[x]
  60. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  61. https://en.wikipedia.org/wiki/softmax_function
  62. http://datascience.stackexchange.com/questions/9302/the-cross-id178-error-function-in-neural-networks
  63. https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained
  64. http://cs231n.github.io/optimization-2/
  65. http://distill.pub/2017/momentum/
  66. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html#fn:quantum
  67. http://www.deepexcel.net/
  68. https://p.migdal.pl/2017/04/30/p.migdal.pl/2017/01/06/king-man-woman-queen-why.html
  69. https://hadrienj.github.io/posts/deep-learning-book-series-introduction/
  70. http://www.deeplearningbook.org/contents/linear_algebra.html
  71. http://www.deeplearningbook.org/
  72. http://immersivemath.com/ila/index.html
  73. https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c
  74. https://docs.scipy.org/doc/numpy-dev/user/quickstart.html
  75. http://www.labri.fr/perso/nrougier/from-python-to-numpy/
  76. http://www.scipy-lectures.org/intro/numpy/array_object.html
  77. https://www.tensorflow.org/
  78. http://deeplearning.net/software/theano/
  79. http://torch.ch/
  80. http://caffe.berkeleyvision.org/
  81. http://pytorch.org/
  82. https://en.wikiquote.org/wiki/donald_knuth
  83. https://en.wikipedia.org/wiki/rule_of_least_power
  84. https://en.wikipedia.org/wiki/zen_of_python
  85. https://keras.io/
  86. https://en.wikipedia.org/wiki/overfitting
  87. https://twitter.com/fchollet/status/852194634470223873
  88. https://medium.com/@karpathy/a-peek-at-trends-in-machine-learning-ab8a1085a106
  89. https://blog.keras.io/
  90. http://jupyter.org/
  91. https://github.com/leriomaggio/deep-learning-keras-tensorflow
  92. https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59
  93. https://cambridgespark.com/content/tutorials/convolutional-neural-networks-with-keras/index.html
  94. http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/
  95. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html#fn:theano
  96. https://github.com/stared/keras-sequential-ascii
  97. https://github.com/fchollet/keras/blob/master/examples/mnist_id98.py
  98. https://github.com/bstriner/keras-tqdm
  99. https://github.com/keplr-io/quiver
 100. https://github.com/raghakot/keras-vis
 101. https://transcranial.github.io/keras-js/
 102. https://rstudio.github.io/keras/
 103. https://github.com/stared/livelossplot/
 104. https://www.tensorflow.org/versions/master/tutorials/index.html
 105. https://cloud.google.com/blog/big-data/2017/01/learn-tensorflow-and-deep-learning-without-a-phd
 106. https://github.com/aymericdamien/tensorflow-examples/
 107. https://github.com/nlintz/tensorflow-tutorials
 108. https://www.tensorflow.org/get_started/summaries_and_tensorboard
 109. http://stackoverflow.com/questions/42112260/how-do-i-use-the-tensorboard-callback-of-keras
 110. http://deeplearning.net/software/theano/
 111. https://lasagne.readthedocs.io/
 112. http://www.marekrei.com/blog/theano-tutorial/
 113. http://pytorch.org/
 114. http://pytorch.org/tutorials/
 115. https://github.com/pytorch/examples
 116. https://deepsense.ai/keras-or-pytorch/
 117. https://www.kaggle.com/benhamner/d/benhamner/nips-papers/popular-datasets-over-time/code
 118. https://twitter.com/fchollet/status/852594987527045120
 119. http://yann.lecun.com/exdb/mnist/
 120. https://keras.io/datasets/
 121. https://web.stanford.edu/group/shr/4-2/text/hofstadter.html
 122. http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html
 123. https://github.com/stared/keras-mini-examples/blob/master/notmnist_starter.ipynb
 124. https://www.cs.toronto.edu/~kriz/cifar.html
 125. https://keras.io/datasets/
 126. https://github.com/stared/keras-mini-examples/blob/master/cifar10_starter.ipynb
 127. https://github.com/stared/keras-mini-examples/blob/master/cifar10_official_example.ipynb
 128. https://blog.deepsense.ai/deep-learning-hands-on-image-classification/
 129. https://www.kaggle.com/datasets
 130. https://www.kaggle.com/competitions
 131. http://image-net.org/challenges/lsvrc/
 132. https://culurciello.github.io/tech/2016/06/04/nets.html
 133. https://gab41.lab41.org/lab41-reading-group-squeezenet-9b9d1d754c75
 134. https://keras.io/applications/
 135. https://github.com/rcmalli/keras-squeezenet
 136. http://cs231n.github.io/transfer-learning/
 137. https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/
 138. https://github.com/orobix/retina-unet
 139. https://github.com/jocicmarko/ultrasound-nerve-segmentation
 140. https://arxiv.org/abs/1508.06576
 141. https://github.com/titu1994/neural-style-transfer
 142. https://blog.athelas.com/a-brief-history-of-id98s-in-image-segmentation-from-r-id98-to-mask-r-id98-34ea83205de4
 143. http://www.asimovinstitute.org/neural-network-zoo/
 144. http://rishy.github.io/ml/2017/01/05/how-to-train-your-dnn/
 145. https://blog.deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation/
 146. https://github.com/jcjohnson/id98-benchmarks
 147. https://medium.com/@mateuszsieniawski/keras-with-gpu-on-amazon-ec2-a-step-by-step-instruction-4f90364e49ac
 148. https://blog.keras.io/running-jupyter-notebooks-on-gpu-on-aws-a-starter-guide.html
 149. https://neptune.ml/
 150. http://cs231n.github.io/
 151. https://www.youtube.com/playlist?list=plkt2usq6rbvctenovbg1tpcc7oqi31alc
 152. http://www.deeplearningbook.org/contents/prob.html
 153. http://www.deeplearningbook.org/
 154. http://neuralnetworksanddeeplearning.com/
 155. https://www.manning.com/books/deep-learning-with-python
 156. http://karpathy.github.io/2015/05/21/id56-effectiveness/
 157. https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html
 158. http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity/
 159. https://harishnarayanan.org/writing/artistic-style-transfer/
 160. https://speakerdeck.com/pmigdal/dreams-drugs-and-convnets
 161. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b
 162. https://medium.com/towards-data-science/transfer-learning-using-keras-d804b2e04ef8
 163. https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f
 164. https://github.com/rlcode/reinforcement-learning
 165. http://sebastianruder.com/optimizing-gradient-descent/
 166. https://medium.com/slavv/picking-an-optimizer-for-style-transfer-86e7b8cba84b
 167. https://blog.keras.io/building-autoencoders-in-keras.html
 168. http://colah.github.io/posts/2015-08-understanding-lstms/
 169. https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b
 170. https://github.com/oxford-cs-deepnlp-2017/lectures
 171. http://ofir.io/how-to-start-learning-deep-learning/
 172. http://yerevann.com/a-guide-to-deep-learning/
 173. https://www.reddit.com/r/machinelearning/
 174. http://distill.pub/
 175. https://pinboard.in/u:pmigdal/t:deep-learning
 176. https://twitter.com/fastml_extra
 177. http://www.gitxiv.com/
 178. http://dontprint.net/
 179. https://kkulma.github.io/
 180. https://martinapugliese.github.io/
 181. http://greenelephant.pl/shiny/rowery/
 182. http://kidzinski.com/
 183. http://goodsexlifestyle.com/
 184. https://twitter.com/pmigdal
 185. http://knowyourmeme.com/photos/1244486-what-people-think-i-do-what-i-really-do
 186. http://blog.kaggle.com/2016/01/29/noaa-right-whale-recognition-winners-interview-1st-place-deepsense-io/
 187. https://blog.deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation/
 188. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html#fnref:deepsense
 189. https://github.com/cytadela8/trypophobia
 190. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html#fnref:trypophobia
 191. https://xkcd.com/1425/
 192. https://laughingsquid.com/park-or-bird-a-national-park-and-bird-identifying-app-inspired-by-an-xkcd-comic/
 193. https://xkcd.com/1002/
 194. https://deepmind.com/research/alphago/
 195. http://phdcomics.com/comics.php?n=1156
 196. http://vllab1.ucmerced.edu/~wlai24/lapsrn/
 197. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html#fnref:webcomics
 198. http://www.dartmouth.edu/~matc/mathdrama/reading/wigner.html
 199. https://www.physics.ohio-state.edu/~jay/880/moreisdifferent.pdf
 200. https://www.amazon.com/genesis-development-scientific-ludwik-fleck/dp/0226253252/
 201. http://www.evolocus.com/textbooks/fleck1979.pdf
 202. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html#fnref:unreasonable
 203. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html#fnref:quantum
 204. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html#fnref:theano
 205. https://twitter.com/share
 206. javascript:window.location="http://news.ycombinator.com/submitlink?u="+encodeuricomponent(document.location)+"&t="+encodeuricomponent("learning deep learning with keras")
 207. http://quantumgame.us9.list-manage.com/subscribe?u=5e3bdb13b61924c4b0ec92fba&id=6c72ded7d2
 208. mailto:pmigdal@gmail.com
 209. https://p.migdal.pl/
 210. https://github.com/stared
 211. https://twitter.com/pmigdal
 212. http://crastina.se/theres-no-projects-like-side-projects/
 213. https://warsztatywww.pl/article/en-indie-camp-for-hs-geeks/
 214. http://quantumgame.io/
 215. https://deepsense.ai/
 216. http://inbrowser.ai/

   hidden links:
 218. https://p.migdal.pl/2017/04/30/teaching-deep-learning.html
 219. https://pinboard.in/u:pmigdal/
 220. https://linkedin.com/in/piotrmigdal
 221. https://stackexchange.com/users/506817/piotr-migdal
