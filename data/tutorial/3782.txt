    [tr?id=2584760171540651&ev=pageview&noscript=1] [1]deep learning
   weekly header logo (button)

     * [2]home (current)
     * [3]newsletter archive

bringing you everything new and exciting in the world of    deep learning from
academia to the grubby depth    of industry every week right to your inbox.
free.

   ____________________ sign up no spam. one-click unsubscribe.

demystifying id97

   by jan bussieck on august 22, 2017

   research into id27s is one of the most interesting in the
   deep learning world at the moment, even though they were introduced as
   early as 2003 by bengio, et al. most prominently among these new
   techniques has been a group of related algorithm commonly referred to
   as id97 which came out of google research.[^2]
   in this post we are going to investigate the significance of id97
   for nlp research going forward and how it relates and compares to prior
   art in the field. in particular we are going to examine some desired
   properties of id27s and the shortcomings of other popular
   approaches centered around the concept of a bag of words (henceforth
   referred to simply as bow) such as latent semantic analysis. this shall
   motivate a detailed exposition of how and why id97 works and
   whether the id27s derived from this method can remedy some of
   the shortcomings of bow based approaches. id97 and the concept of
   id27s originate in the domain of nlp, however as we shall see
   the idea of words in the context of a sentence or a surrounding word
   window can be generalized to any problem domain dealing with sequences
   or sets of related data points.

definition of terms

   for the sake of precision we have to define a few terms formally - a
   word is the basic unit of discrete data, defined to be an item from a
   vocabulary indexed by 1,...,v
   [math: <mrow
   class="mjx-texatom-ord"><mn>1</mn><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</
   mo><mo>,</mo><mi>v</mi></mrow> :math]
   . we represent words using unit-basis vectors that have a single
   component equal to one and all other components equal to zero (one-hot
   encoding). thus, using superscripts to denote components, the vth word
   in the vocabulary is represented by a v   
   [math: <mi>v</mi><mo>   </mo> :math]
   vector w
   [math: <mi>w</mi> :math]
   such that wv=1
   [math: <msub><mi>w</mi><mi>v</mi></msub><mo>=</mo><mn>1</mn> :math]
   and wu=0
   [math: <msub><mi>w</mi><mi>u</mi></msub><mo>=</mo><mn>0</mn> :math]
   for u   v
   [math: <mi>u</mi><mo>   </mo><mi>v</mi> :math]
   . - a document is a sequence of n words denoted by w=(w1,w2,...,wn)
   [math: <mi>w</mi><mo>=</mo><mo
   stretchy="false">(</mo><mi>w</mi><mn>1</mn><mo>,</mo><mi>w</mi><mn>2</m
   n><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><mi>w</mi><mi>n</mi
   ><mo stretchy="false">)</mo> :math]
   , where wn is the nth word in the sequence. - a corpus is a collection
   of m documents denoted by d=w1,w2,...,wm
   [math: <mi>d</mi><mo>=</mo><mrow
   class="mjx-texatom-ord"><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><msu
   b><mi>w</mi><mn>2</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><m
   o>,</mo><msub><mi>w</mi><mi>m</mi></msub></mrow> :math]
   . - a id27 w:words   >rn
   [math:
   <mi>w</mi><mo>:</mo><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>s</mi><
   mo>   </mo><mo>></mo><msup><mrow class="mjx-texatom-ord"><mi
   mathvariant="double-struck">r</mi></mrow><mi>n</mi></msup> :math]
   is a parameterized function mapping words in some language to
   high-dimensional vectors (perhaps 200 to 500 dimensions). for example,
   we might find:

   w("cat")=(0.2,   0.4,0.7,...)
   [math: <mi>w</mi><mo
   stretchy="false">(</mo><mo>"</mo><mi>c</mi><mi>a</mi><mi>t</mi><mo>"</m
   o><mo stretchy="false">)</mo><mo>=</mo><mo
   stretchy="false">(</mo><mn>0.2</mn><mo>,</mo><mo>   </mo><mn>0.4</mn><mo>
   ,</mo><mn>0.7</mn><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo
   stretchy="false">)</mo> :math]

   w("mat")=(0.0,0.6,   0.1,...)
   [math: <mi>w</mi><mo
   stretchy="false">(</mo><mo>"</mo><mi>m</mi><mi>a</mi><mi>t</mi><mo>"</m
   o><mo stretchy="false">)</mo><mo>=</mo><mo
   stretchy="false">(</mo><mn>0.0</mn><mo>,</mo><mn>0.6</mn><mo>,</mo><mo>
      </mo><mn>0.1</mn><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo
   stretchy="false">)</mo> :math]

history and landscape of vector representations

   before the advent of id27s, words have been represented by
   models that made very strong simplifying assumptions, concretely, those
   representation rely on the notion of one hot encoded vectors, where a
   word is represented by a sparse vector with a dimension equal to the
   size of the vocabulary with a 1 at the index that stands for the word
   and zeros everywhere else. from this representation one can build up to
   a bag of words representation of text which represents a document by
   simply counting the number of times a word from the vocabulary occurs
   in it. from this id194 one can derive a word
   representation by using the co-occurrence of words in documents. let us
   dig into one the most common of these methods; latent semantic analysis
   (deerwester et al., 1990).

latent semantic analysis

   latent semantic analysis starts with a matrix, a so-called term
   document matrix, consisting of rows which stand for unique words and
   columns which correspond to a document, the entry is simply the
   frequency with which the word of the row appears in the document of the
   column.
   instead of the raw term frequency one can also utilize a tf-idf
   transformation, that is a term-frequency-inverse-document-frequency
   which roughly corresponds to the frequency of a term within a document
   divided by its frequency in the entire corpus. thereby expressing that
   terms which are common such as conventional stop words ("the", "a",
   "it" etc.) contribute less discriminative power to a document than
   terms which are highly document (or topic) specific. we can factorize
   this matrix with a method called singular value decomposition which
   yields three matrices, udvt
   [math: <mi>u</mi><mi>d</mi><msup><mi>v</mi><mi>t</mi></msup> :math]
   where the columns of u
   [math: <mi>u</mi> :math]
   and v
   [math: <mi>v</mi> :math]
   are orthonormal and the matrix d is diagonal with positive real
   entries, the so-called singular values. these values are used to rank
   dimensions in u
   [math: <mi>u</mi> :math]
   which allows one to identify the dimensions across which most of the
   variance in the space of tf-idf features is captured. that means that
   vectors of words that co-occur in certain documents, that is, words
   that exhibit similar variance across documents, end up close together
   in this new vector space called a topic space.

   one shortcoming of bow based methods is that co-occurrence in documents
   encodes a rather shallow kind of topical similarity, so that, for
   instance, "hogwarts" occurs close to other harry potter related terms
   as opposed to terms we would recognize as conceptually or functionally
   similar (i.e. other elite educational institutions). the latter was
   achieved through prediction based words embeddings by levy & goldberg
   which shows that a mere topical model falls short where more subtle
   semantic relationships between words are concerned.
   id97 bag of words comparison
   from levy & goldberg (2014)

id97

   instead of relying on pre-computed co-occurrence counts, id97 takes
   'raw' text as input and learns a word by predicting its surrounding
   context (in the case of the skip-gram model) or predict a word given
   its surrounding context (in the case of the cbow model) using gradient
   descent with randomly initialized vectors.
   as an example let us look at the latter case applied to the following
   sentence.

     "the fox jumped over the lazy dog"

   now, we want to learn the word vector for 'over' from its surrounding
   context, we call this vector vin
   [math: <msub><mi>v</mi><mrow
   class="mjx-texatom-ord"><mi>i</mi><mi>n</mi></mrow></msub> :math]
   , id97 uses different vectors for id27s depending on
   whether it is the word we are conditioning on or the word we are trying
   to predict. the id203 we are trying to maximize is then:
   p(vout|vin)
   [math: <mi>p</mi><mo stretchy="false">(</mo><msub><mi>v</mi><mrow
   class="mjx-texatom-ord"><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mr
   ow class="mjx-texatom-ord"><mo
   stretchy="false">|</mo></mrow><msub><mi>v</mi><mrow
   class="mjx-texatom-ord"><mi>i</mi><mi>n</mi></mrow></msub><mo
   stretchy="false">)</mo> :math]
   , where vout
   [math: <msub><mi>v</mi><mrow
   class="mjx-texatom-ord"><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub>
   :math]
   is the output word and vin
   [math: <msub><mi>v</mi><mrow
   class="mjx-texatom-ord"><mi>i</mi><mi>n</mi></mrow></msub> :math]
   the input.
   the algorithm then moves over each word in the corpus and repeats the
   training step in an online fashion. the interesting property that word
   vectors obtained this way exhibit is that they encode not only
   syntactic but also semantic relationships between words. that means
   that not only are similar words close to each other in the vector space
   (as measured by some norm), but word analogies are reflected by the
   difference between word vectors. this property is referred to as
   'additive compositionality' in the literature (mikolov) and refers to
   the linear structure in the vector space that allows analogical
   reasoning. word vectors can be seen as representing the distribution of
   the context in which a word appears and the sum of vectors roughly
   represents an and concatenation, so if for instance 'volga river'
   appears in the same context with words like 'river' and 'russian', the
   sum of these two word vectors will be close to the vector of "volga
   river". (this property will be examined in detail in later sections).

the skip-gram model

   there are two major 'flavours' of id97; the skip-gram model and the
   continuous bag of word model, the one we are going to examine now and
   which will serve as the reference for our comparative discussion of
   id27 is the skip-gram model. the training goal of the
   skip-gram model is to arrive at vector representations of words that
   best predict a window of surrounding words. formally the objective
   function is given by:

   j(  )=1tt   t=1      c   j   c,j   0log(p(wt+j|wt))
   [math: <mi>j</mi><mo stretchy="false">(</mo><mi>  </mi><mo
   stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>t</mi></mfrac><mu
   nderover><mo>   </mo><mrow
   class="mjx-texatom-ord"><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi>
   </munderover><munder><mo>   </mo><mrow
   class="mjx-texatom-ord"><mo>   </mo><mi>c</mi><mo>   </mo><mi>j</mi><mo>   </
   mo><mi>c</mi><mo>,</mo><mi>j</mi><mo>   </mo><mn>0</mn></mrow></munder><m
   i>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>p</mi><mo
   stretchy="false">(</mo><msub><mi>w</mi><mrow
   class="mjx-texatom-ord"><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mr
   ow class="mjx-texatom-ord"><mo
   stretchy="false">|</mo></mrow><msub><mi>w</mi><mi>t</mi></msub><mo
   stretchy="false">)</mo><mo stretchy="false">)</mo> :math]
   (1)

   where c is the size of the context window. here our aim is to maximize
   log id203 of any context word given the current center word. for
   convenience with regard to taking the derivative during gradient
   descent, a popular id203 measure has been the softmax function.

   p(wt+1|wt)=exp(~vwovtwi)   vk=1exp(wti~wk)
   [math: <mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow
   class="mjx-texatom-ord"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mr
   ow class="mjx-texatom-ord"><mo
   stretchy="false">|</mo></mrow><msub><mi>w</mi><mi>t</mi></msub><mo
   stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p
   </mi><mo stretchy="false">(</mo><mrow
   class="mjx-texatom-ord"><mover><mrow><msub><mi>v</mi><mrow
   class="mjx-texatom-ord"><msub><mi>w</mi><mi>o</mi></msub></mrow></msub>
   <msubsup><mi>v</mi><mrow
   class="mjx-texatom-ord"><mi>w</mi><mi>i</mi></mrow><mi>t</mi></msubsup>
   </mrow><mo stretchy="false">~</mo></mover></mrow><mo
   stretchy="false">)</mo></mrow><mrow><munderover><mo>   </mo><mrow
   class="mjx-texatom-ord"><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>v</mi>
   </munderover><mi>e</mi><mi>x</mi><mi>p</mi><mo
   stretchy="false">(</mo><msubsup><mi>w</mi><mi>i</mi><mi>t</mi></msubsup
   ><mrow
   class="mjx-texatom-ord"><mover><msub><mi>w</mi><mi>k</mi></msub><mo
   stretchy="false">~</mo></mover></mrow><mo
   stretchy="false">)</mo></mrow></mfrac> :math]
   (2)

   where vw
   [math: <msub><mi>v</mi><mi>w</mi></msub> :math]
   and ~vw
   [math: <mrow
   class="mjx-texatom-ord"><mover><msub><mi>v</mi><mi>w</mi></msub><mo
   stretchy="false">~</mo></mover></mrow> :math]
   are the input and output vector representations of w
   [math: <mi>w</mi> :math]
   and v
   [math: <mi>v</mi> :math]
   is the number of words in the vocabulary. this expression is optimized
   using id119. however, there is an obvious problem with this
   expression, every time we move over the context we have to recompute
   the id172 factor in the denominator for every word in w
   [math: <mi>w</mi> :math]
   which can be on the order of 105   107
   [math:
   <msup><mn>10</mn><mn>5</mn></msup><mo>   </mo><msup><mn>10</mn><mn>7</mn>
   </msup> :math]
   .

   there are two major strategies that have been devised to attenuate the
   cost of computing the id172 term; negative sampling and
   hierarchical softmax. the idea in both cases is to compare the target
   word with a stochastic selection of contexts instead of all contexts.
   since the denominator computes the similarity of all other contexts and
   the target word and thereby expresses that if two words are similar the
   expression in the nominator will be large relative to the denominator,
   it will be equally as large relative to a random selection of contexts.

how and why does id97 work?

   up until now we have not really tried to understand why id97 works,
   in order to so let us look at how the aforementioned models are
   commonly presented. we can get a 'feel' for the id27 space by
   illustrating them with id167, a sophisticated method for dimensionality
   reduction:

   visualized id27s through id84

   id167 visualization of id27s: left: number region, right:
   jobs region. from turian et al. (2010), [4]original image

   this makes intuitive sense to us because similar words are close to
   each other. after the above discussion it should come as no surprise
   that the neural network seem to produce vectors that reflect the
   similarity of the words they represent. if you switch a word for a
   synonym we would expect that it occurs in similar context, i.e. that it
   predicts similar words in its context window.

   as we have seen id27s do even more, they have the remarkable
   property that analogies between words seem to be encoded in the
   difference between word vectors. for example there seems to be a
   constant male-female difference vector:

   w("woman")   w("man")~=w("aunt")   w(""uncle")
   [math: <mi>w</mi><mo
   stretchy="false">(</mo><mo>"</mo><mi>w</mi><mi>o</mi><mi>m</mi><mi>a</m
   i><mi>n</mi><mo>"</mo><mo
   stretchy="false">)</mo><mo>   </mo><mi>w</mi><mo
   stretchy="false">(</mo><mo>"</mo><mi>m</mi><mi>a</mi><mi>n</mi><mo>"</m
   o><mo stretchy="false">)</mo><mrow
   class="mjx-texatom-ord"><mover><mo>=</mo><mo
   stretchy="false">~</mo></mover></mrow><mi>w</mi><mo
   stretchy="false">(</mo><mo>"</mo><mi>a</mi><mi>u</mi><mi>n</mi><mi>t</m
   i><mo>"</mo><mo stretchy="false">)</mo><mo>   </mo><mi>w</mi><mo
   stretchy="false">(</mo><mo>""</mo><mi>u</mi><mi>n</mi><mi>c</mi><mi>l</
   mi><mi>e</mi><mo>"</mo><mo stretchy="false">)</mo> :math]

   w("woman")   w("man")~=w("queen")   w(""king")
   [math: <mi>w</mi><mo
   stretchy="false">(</mo><mo>"</mo><mi>w</mi><mi>o</mi><mi>m</mi><mi>a</m
   i><mi>n</mi><mo>"</mo><mo
   stretchy="false">)</mo><mo>   </mo><mi>w</mi><mo
   stretchy="false">(</mo><mo>"</mo><mi>m</mi><mi>a</mi><mi>n</mi><mo>"</m
   o><mo stretchy="false">)</mo><mrow
   class="mjx-texatom-ord"><mover><mo>=</mo><mo
   stretchy="false">~</mo></mover></mrow><mi>w</mi><mo
   stretchy="false">(</mo><mo>"</mo><mi>q</mi><mi>u</mi><mi>e</mi><mi>e</m
   i><mi>n</mi><mo>"</mo><mo
   stretchy="false">)</mo><mo>   </mo><mi>w</mi><mo
   stretchy="false">(</mo><mo>""</mo><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</
   mi><mo>"</mo><mo stretchy="false">)</mo> :math]

   id97 semantic relationship visualized

   from mikolov et al. (2013a)

   this may not seem entirely surprising considering how a word's context
   changes if it is replaced with its male or female counterpart. most
   generally we write "he is the king", but "she is the queen" along with
   this pronoun change we would expect other gender specific context
   discrepancies which are paralleled across words with different
   meanings. it turns out that there are a multitude of sophisticated
   relationships in several "dimensions of meaning" (pennington et al.
   2014). the difference vector of countries and their capitals, for
   instance are roughly the same as are the difference vectors of
   adjectives and their superlatives.

   considered from the perspective of id97 while the side effect of
   encoding meaning seems to make intuitive sense the exact origins remain
   somewhat murky. in order to illuminate these, let us consider a
   derivation by pennington, et al (2014) in their paper discussing the
   generation of word vectors from a matrix of word co-occurrence
   probabilities for a given context window.
   to understand the relevance of this derivation for our quest to
   understand the inner workings of id97, we briefly examine the
   paper's central idea.
   the authors present a method centered around trying to make the dot
   product of the word vectors wi
   [math: <msub><mi>w</mi><mi>i</mi></msub> :math]
   and ~wk
   [math: <mrow
   class="mjx-texatom-ord"><mover><msub><mi>w</mi><mi>k</mi></msub><mo
   stretchy="false">~</mo></mover></mrow> :math]
   equal to the logarithm of their co-occurrence count xij
   [math: <msub><mi>x</mi><mrow
   class="mjx-texatom-ord"><mi>i</mi><mi>j</mi></mrow></msub> :math]
   (which denotes the number of times word j
   [math: <mi>j</mi> :math]
   co-occurs with i
   [math: <mi>i</mi> :math]
   ).

   wti~wk+bi+~bk=log(xik)
   [math: <msubsup><mi>w</mi><mi>i</mi><mi>t</mi></msubsup><mrow
   class="mjx-texatom-ord"><mover><msub><mi>w</mi><mi>k</mi></msub><mo
   stretchy="false">~</mo></mover></mrow><mo>+</mo><msub><mi>b</mi><mi>i</
   mi></msub><mo>+</mo><mrow
   class="mjx-texatom-ord"><mover><msub><mi>b</mi><mi>k</mi></msub><mo
   stretchy="false">~</mo></mover></mrow><mo>=</mo><mi>l</mi><mi>o</mi><mi
   >g</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow
   class="mjx-texatom-ord"><mi>i</mi><mi>k</mi></mrow></msub><mo
   stretchy="false">)</mo> :math]
   (3)

   where bi
   [math: <msub><mi>b</mi><mi>i</mi></msub> :math]
   and ~bk
   [math: <mrow
   class="mjx-texatom-ord"><mover><msub><mi>b</mi><mi>k</mi></msub><mo
   stretchy="false">~</mo></mover></mrow> :math]
   are merely bias terms simplifying downstream computation. in order to
   achieve this equality the authors employ a weighted least regression
   model, given by:

   j=v   ij=1f(xij)(wti~wk+bi+~bk   log(xik))2
   [math: <mi>j</mi><mo>=</mo><munderover><mo>   </mo><mrow
   class="mjx-texatom-ord"><mi>i</mi><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
   <mi>v</mi></munderover><mi>f</mi><mo
   stretchy="false">(</mo><msub><mi>x</mi><mrow
   class="mjx-texatom-ord"><mi>i</mi><mi>j</mi></mrow></msub><mo
   stretchy="false">)</mo><mo
   stretchy="false">(</mo><msubsup><mi>w</mi><mi>i</mi><mi>t</mi></msubsup
   ><mrow
   class="mjx-texatom-ord"><mover><msub><mi>w</mi><mi>k</mi></msub><mo
   stretchy="false">~</mo></mover></mrow><mo>+</mo><msub><mi>b</mi><mi>i</
   mi></msub><mo>+</mo><mrow
   class="mjx-texatom-ord"><mover><msub><mi>b</mi><mi>k</mi></msub><mo
   stretchy="false">~</mo></mover></mrow><mo>   </mo><mi>l</mi><mi>o</mi><mi
   >g</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow
   class="mjx-texatom-ord"><mi>i</mi><mi>k</mi></mrow></msub><mo
   stretchy="false">)</mo><msup><mo
   stretchy="false">)</mo><mn>2</mn></msup> :math]
   (4)

   where v
   [math: <mi>v</mi> :math]
   is the size of the vocabulary and the weighting function serves the
   purpose of de-emphasizing extreme values. a first observation is that
   this model straightforwardly optimizes for word vectors that are close
   in terms of their dot product if their co-occurrence count is high.
   there is no neural network or online update of word vectors and it is
   very easy to understand what this model is optimizing for. let us now
   turn to the aforementioned derivation; we recall equation (2) from our
   discussion of the skip-gram model and follow the authors in their
   slight alteration of it to suit prior notation:

   qij=exp(wti~wj)   vk=1exp(wti~wk)
   [math: <msub><mi>q</mi><mrow
   class="mjx-texatom-ord"><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mf
   rac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo
   stretchy="false">(</mo><msubsup><mi>w</mi><mi>i</mi><mi>t</mi></msubsup
   ><mrow
   class="mjx-texatom-ord"><mover><msub><mi>w</mi><mi>j</mi></msub><mo
   stretchy="false">~</mo></mover></mrow><mo
   stretchy="false">)</mo></mrow><mrow><munderover><mo>   </mo><mrow
   class="mjx-texatom-ord"><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>v</mi>
   </munderover><mi>e</mi><mi>x</mi><mi>p</mi><mo
   stretchy="false">(</mo><msubsup><mi>w</mi><mi>i</mi><mi>t</mi></msubsup
   ><mrow
   class="mjx-texatom-ord"><mover><msub><mi>w</mi><mi>k</mi></msub><mo
   stretchy="false">~</mo></mover></mrow><mo
   stretchy="false">)</mo></mrow></mfrac> :math]
   (5)

   the implied global objective function can be written as:

   j=      i   corpus,j   context(i)log(qij)
   [math: <mi>j</mi><mo>=</mo><mo>   </mo><munder><mo>   </mo><mrow
   class="mjx-texatom-ord"><mi>i</mi><mo>   </mo><mi>c</mi><mi>o</mi><mi>r</
   mi><mi>p</mi><mi>u</mi><mi>s</mi><mo>,</mo><mi>j</mi><mo>   </mo><mi>c</m
   i><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo
   stretchy="false">(</mo><mi>i</mi><mo
   stretchy="false">)</mo></mrow></munder><mi>l</mi><mi>o</mi><mi>g</mi><m
   o stretchy="false">(</mo><msub><mi>q</mi><mrow
   class="mjx-texatom-ord"><mi>i</mi><mi>j</mi></mrow></msub><mo
   stretchy="false">)</mo> :math]
   (6)

   we are simply taking the log of the left-hand side and multiplying it
   by    1
   [math: <mo>   </mo><mn>1</mn> :math]
   in order to arrive at an objective function we can minimize. from here
   the authors follow a number of simplification and substitutions to
   finally arrive at:

   j=   ijxi(wti~wj   logxij)2
   [math: <mi>j</mi><mo>=</mo><munder><mo>   </mo><mrow
   class="mjx-texatom-ord"><mi>i</mi><mi>j</mi></mrow></munder><msub><mi>x
   </mi><mi>i</mi></msub><mo
   stretchy="false">(</mo><msubsup><mi>w</mi><mi>i</mi><mi>t</mi></msubsup
   ><mrow
   class="mjx-texatom-ord"><mover><msub><mi>w</mi><mi>j</mi></msub><mo
   stretchy="false">~</mo></mover></mrow><mo>   </mo><mi>l</mi><mi>o</mi><mi
   >g</mi><msub><mi>x</mi><mrow
   class="mjx-texatom-ord"><mi>i</mi><mi>j</mi></mrow></msub><msup><mo
   stretchy="false">)</mo><mn>2</mn></msup> :math]
   (7)

   which except for the weighting factor is identical to the model global
   optimization problem in equation (4).
   so when viewed through the lens of corpus-wide, that is, global
   aggregation of words in their respective context windows id97
   reduces to the glove model which explicitly aggregates global
   statistics of word co-occurrence. in this sense we have come full
   circle to the methods presented earlier that rely on matrix
   factorization (such as lsa). where lsa uses svd to find the best
   fitting subspace in terms of the co-variances of words across
   documents, the glove model explicitly optimizes wordvectors to reflect
   the likelihood of their co-occurrence. the point, however, stands that
   id97 is not an entirely novel approach to arrive at word vectors
   that capture semantic meaning in their substructure.

applications

   let us now turn to some interesting new application of id27s
   derived from id97, both to other domains of machine learning and to
   concrete engineering problems.

   to stay in the realm of nlp for a moment an interesting and natural
   application is in machine translation. socher et al 2013 show that we
   can learn to embed words from two different languages in a single,
   shared space. in this case, we learn to embed english and mandarin
   chinese words in the same space.
   in order to do so we simply train two id27 wen
   [math: <msub><mi>w</mi><mrow
   class="mjx-texatom-ord"><mi>e</mi><mi>n</mi></mrow></msub> :math]
   and wzh
   [math: <msub><mi>w</mi><mrow
   class="mjx-texatom-ord"><mi>z</mi><mi>h</mi></mrow></msub> :math]
   in a way similar to the skip-gram model, hoewever since we know that
   certain english and chinese words have similar meaning we can optimize
   for an additional property; words that we know a priori are close
   translation should be close together in the vector space.

   not surprisingly observe that words with similar meaning end up close
   together, this is after all what the model optimized for. more
   remarkably, however, words whose translation was not known at the time
   of training the model ended up close together as well.

   featured id97 used for translation by overlaying two embeddings

   id167 visualization of bilingual id27. green is chinese,
   yellow is english. (socher et al.(2013a))

   in light of our previous discussion this makes sense when we consider
   that id27 seems to pull similar words close together, so
   hence by using a few translations as anchor point we create a mapping
   between english and mandarin words. so we see that the essential
   property of id27s derived with id97 helps us in other
   machine learning task by superimposing the structure of the word vector
   space on other domains (another conceivable domain might be image
   recognition).

   another more direct application of id97 to a classical engineering
   task was recently presented by spotify. they abstracted the ideas
   behind id97 to apply them not simply to words in sentences but to
   any object in any sequence, in this case to songs in a playlist. songs
   are treated as words and other songs in a playlist as their surrounding
   context, depending on whether the playlists in question were genre
   specific the vocabulary encompassed a number of songs from that
   collection.
   now in order to recommend songs to a user one merely has to examine a
   neighborhood of the 'song embeddings' of songs the user already likes.
   similarly, we could recommend a user who to connect to in a social
   network setting by examining the graph of relationships, where the
   nodes represent words and a path through the graph represents a
   sentence, then nodes that occur in the context of similar other nodes,
   would be close together in the vector space. these examples show that
   the general applicability of id97 based algorithms is very rich and
   that it behooves practitioners to examine their problem domain with
   respect to sequences of objects that occur in some meaningful context.

literature

   bengio, y. (2009). learning deep architectures for ai. foundations and
   trends in machine learning.

   bengio, y., ducharme, r., vincent, p. & janvin, c. (2003). a neural
   probabilistic language model. jmlr, 3:1137   1155.

   deerwester, s., dumais, s. t., furnas, g. w., landauer, t. k., &
   harshman, r. (1990). indexing by latent semantic analysis. journal of
   the american society for information science , 41 , 391-407.

   levy, o. & goldberg, y. (2014). dependency based id27s. in
   proceedings of the 52nd annual meeting of the association for
   computational linguistics (volume 2: short papers), baltimore,
   maryland, usa, june. association for computational linguistics.

   dennewitz, m., playlist-to-vec, (2015), github repository,
   [5]https://github.com/mattdennewitz/playlist-to-vec

   mikolov, t., chen k., corrado g., & dean j. 2013a. efficient estimation
   of word representations in vector space. in iclr workshoppapers.

   mikolov, t., sutskever, s., chen, k., corrado, g., & dean, j. (2013b).
   distributed representations of words and phrases and their
   compositionality. in nips, pages 3111   3119.

   pennington, j., socher, r., & manning, c. (2014). glove: global vectors
   for word representation. empricial methods in natural language
   processing (emnlp)

   socher, r., zou, w. cer, d., manning c. (2013). bilingual word
   embeddings for phrase-based machine translation. (emnlp), 1393-1398

   thank you to our sponsor [6]heartbeat by [7]fritz

   deep learning weekly aims at being the premier news aggregator for all
   things deep learning. we keep tabs on major developments in industry be
   they new technologies, companies, product offerings or acquisitions so
   you don't have to. in addition our 'learning' section features new
   content that makes difficult to understand areas in deep learning
   accessible to a wider audience and our 'papers & publications' section
   brings you the most exicting new research. if you have any thoughts or
   ideas how we might improve this newsletter we are interested in hearing
   them. thank you in advance!

   we use third party cookies and scripts to improve the functionality of
   this website. accept

references

   1. https://www.deeplearningweekly.com/
   2. https://www.deeplearningweekly.com/
   3. http://digest.deeplearningweekly.com/
   4. http://metaoptimize.s3.amazonaws.com/cw-embeddings-acl2010/embeddings-mostcommon.embedding_size=50.png
   5. https://github.com/mattdennewitz/playlist-to-vec
   6. https://heartbeat.fritz.ai/?utm_source=deeplearningweekly&utm_campaign=website
   7. https://www.fritz.ai/?utm_source=deeplearningweekly&utm_campaign=website
