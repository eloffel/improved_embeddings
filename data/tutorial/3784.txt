   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]metaflow-ai
   [7]metaflow-ai
   [8]sign in[9]get started
     __________________________________________________________________

tensorflow howto: a universal approximator inside a neural net

   [10]go to the profile of morgan
   [11]morgan (button) blockedunblock (button) followfollowing
   feb 9, 2017

   today, let   s take a break from learning and implement something
   instead!

   did you hear about the [12]   universal approximation theorem   ?

   basically, this theorem states that (without all the nitty-gritty
   details):
     * for any continuous function f defined in r^n    
     *     you can find a wide enough 1-hidden layer neural net    
     *     that will approximate f as well as you want on a closed interval

   that sounds very cool!

   let   s dive directly into the code and build an implementation with
   tensorflow in the following case: f is a function from r to r.
   basically, we are going to build a 1-hidden layer neural network
   without a bias on the output layer, let   s see:

   iframe: [13]/media/8f5700049626d666baff8ec85acf8606?postid=bb034430b71e

   some notes:
     * x must be of rank 2 to be used by the tensorflow matmul function.
       this means that x is of shape [none, 1] (none holds for the batch
       size: you can see it as a capacity to compute as many values as you
       want in a single call)
     * the input_dim and output_dim are hard-coded right now, but you
       could change them as you wish to handle a lot more kinds of
       functions. in our case, we   ll keep it simple so we can graph our
       function easily.
     * finally, notice the existence of the relu function. we could have
       used a lot of different functions instead of it but it doesn   t
       really matter for the theorem as long as it is an increasing
       function: it only matters for the    speed of learning   .

   now, let   s write a very simple script to evaluate this function:

   iframe: [14]/media/a8189ffbcc992fec0c70debacaab927c?postid=bb034430b71e

   all right! we have our    universal approximator    (ua).

   we only need to train it now, to approximate any function we want on a
   given closed interval (you won   t do it on an infinite interval, would
   you ?).

   so let   s start by a function i personally didn   t believe a neural
   network would approximate well: the sine function

     sidenote: if you   re like me and wonder how is this possible, let me
     give you a mathematical hint:

     - any continuous function on a compact set (closed intervals) can be
     approximated by a [15]piecewise constant function as well as we want

     - and you can build a neural network manually which will be as close
     as you want to this piecewise function by adding as neurons as
     necessary

   we will build a script to:
     * train our ua on the sine function.
     * graph the resulting approximated function and the sine one side by
       side
     * make the hidden_dim accessible in the command line to be able to
       change it easily

   i will post the whole script directly here. it contains the explanation
   as comments.

   i believe this is more suitable for medium and also for you if you want
   to run it (don   t be afraid by the length of the file, there are a lot
   of comments and empty lines).

   iframe: [16]/media/6498aba79e34e1a14bd55f1df375a96a?postid=bb034430b71e

   now you can open two terminals and launch the following commands from
   the main directory to see the magic happens:
     * python myfile.py --nb_neurons 50
     * tensorboard --logdir results --reload_interval 5 (the default
       reload_interval is 120 seconds to avoid being too aggressive on the
       computer but in our case, we can safely speed it a little bit)

   you can go watch your ua training in real time and see it learn the
   sine function.

   remember, we should see that the more hidden_dim neurons we add, the
   more our function will approximate well!

   lets me show you for 4 different values of hidden_dim: [20, 50, 100,
   500]
   [1*4901jk6guoqzvsqcrceceg.png]
   different graph showing the effect of the number of neurons in the ua

   as expected, if we increase the number of neurons, our ua approximate
   better our sine function and in fact, we can be as close as we want
   from it. that   s pretty neat to see it working.

   yet our ua implementation has a huge drawback, we can   t reuse it if the
   input_dim starts to vary   

   what if, in our wildest dream, we would like, to approximate the
   activation function of a complex neural network! wouldn   t that be a
   cool inception?

   i think this is a very good exercise for you, how can you trick
   tensorflow to have an implementation handling dynamic input dimensions?
   (the solution is in my [17]github if you want to check it, but you
   should work it out by yourself first).

   to end this article, here is a little gift: i   ve been using the second
   implementation to train a neural network on the mnist dataset. (so we
   have a neural network using a neural network as an activation
   function).

   those are the graphs of activation function approximated on it, cheers!
   [1*ca1p_ffb9qer49hgazynzg.png]

     sidenotes:

     i   m using the elu function inside the second ua, this is why the
     resulting approximations are curved.

     all those approximations happened multiple times

     i   ve reached consistently an accuracy of 0.98 on the mnist test set,
     telling us that the activation function is potentially not very
     important to be able to learn the task.
     __________________________________________________________________

tensorflow best practice series

   this article is part of a more complete series of articles about
   tensorflow. i   ve not yet defined all the different subjects of this
   series, so if you want to see any area of tensorflow explored, add a
   comment! so far i wanted to explore those subjects (this list is
   subject to change and is in no particular order):
     * [18]a primer
     * [19]how to handle shapes in tensorflow
     * [20]tensorflow saving/restoring and mixing multiple models
     * [21]how to freeze a model and serve it with a python api
     * [22]tensorflow: a proposal of good practices for files, folders and
       models architecture
     * tensorflow howto: a universal approximator inside a neural net
       (this one :) )
     * [23]how to optimise your input pipeline with queues and
       multi-threading
     * [24]mutating variables and control flow
     * how to handle preprocessing with tensorflow.
     * how to control the gradients to create custom back-prop with, or
       fine-tune my models.
     * how to monitor and inspect my models to gain insight into them.

     note: tf is evolving fast right now, those articles are currently
     written for the 1.0.0 version.
     __________________________________________________________________

references

     * [25]https://en.wikipedia.org/wiki/universal_approximation_theorem
     * [26]http://cstheory.stackexchange.com/questions/17545/universal-app
       roximation-theorem-neural-networks

     * [27]machine learning
     * [28]neural networks
     * [29]tensorflow
     * [30]tutorial
     * [31]data science

   (button)
   (button)
   (button) 81 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of morgan

[33]morgan

   machine learning enthusiast, i spend my time reading scientific papers,
   replicating work and waiting for my own creativity to kick in!

     (button) follow
   [34]metaflow-ai

[35]metaflow-ai

   we are a bunch of optimist machine learning scientist doing crazy
   projects! contact us for more information.

     * (button)
       (button) 81
     * (button)
     *
     *

   [36]metaflow-ai
   never miss a story from metaflow-ai, when you sign up for medium.
   [37]learn more
   never miss a story from metaflow-ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.metaflow.fr/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/bb034430b71e
   4. https://medium.com/
   5. https://medium.com/
   6. https://blog.metaflow.fr/?source=avatar-lo_keokxushj6bx-854dfaf8c32d
   7. https://blog.metaflow.fr/?source=logo-lo_keokxushj6bx---854dfaf8c32d
   8. https://medium.com/m/signin?redirect=https://blog.metaflow.fr/tensorflow-howto-a-universal-approximator-inside-a-neural-net-bb034430b71e&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://blog.metaflow.fr/tensorflow-howto-a-universal-approximator-inside-a-neural-net-bb034430b71e&source=--------------------------nav_reg&operation=register
  10. https://blog.metaflow.fr/@morgangiraud?source=post_header_lockup
  11. https://blog.metaflow.fr/@morgangiraud
  12. https://en.wikipedia.org/wiki/universal_approximation_theorem
  13. https://blog.metaflow.fr/media/8f5700049626d666baff8ec85acf8606?postid=bb034430b71e
  14. https://blog.metaflow.fr/media/a8189ffbcc992fec0c70debacaab927c?postid=bb034430b71e
  15. https://en.wikipedia.org/wiki/step_function
  16. https://blog.metaflow.fr/media/6498aba79e34e1a14bd55f1df375a96a?postid=bb034430b71e
  17. https://github.com/metaflow-ai/blog/
  18. https://medium.com/@morgangiraud/tensorflow-a-primer-4b3fa0978be3#.6axq0wkd7
  19. https://blog.metaflow.fr/shapes-and-dynamic-dimensions-in-tensorflow-7b1fe79be363#.dcmxo9jf5
  20. https://blog.metaflow.fr/tensorflow-saving-restoring-and-mixing-multiple-models-c4c94d5d7125#.f78fep94t
  21. https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc#.f8cininaw
  22. https://blog.metaflow.fr/tensorflow-a-proposal-of-good-practices-for-files-folders-and-models-architecture-f23171501ae3
  23. https://blog.metaflow.fr/tensorflow-how-to-optimise-your-input-pipeline-with-queues-and-multi-threading-e7c3874157e0#.n82a6uin4
  24. https://blog.metaflow.fr/tensorflow-mutating-variables-and-control-flow-2181dd238e62
  25. https://en.wikipedia.org/wiki/universal_approximation_theorem
  26. http://cstheory.stackexchange.com/questions/17545/universal-approximation-theorem-neural-networks
  27. https://blog.metaflow.fr/tagged/machine-learning?source=post
  28. https://blog.metaflow.fr/tagged/neural-networks?source=post
  29. https://blog.metaflow.fr/tagged/tensorflow?source=post
  30. https://blog.metaflow.fr/tagged/tutorial?source=post
  31. https://blog.metaflow.fr/tagged/data-science?source=post
  32. https://blog.metaflow.fr/@morgangiraud?source=footer_card
  33. https://blog.metaflow.fr/@morgangiraud
  34. https://blog.metaflow.fr/?source=footer_card
  35. https://blog.metaflow.fr/?source=footer_card
  36. https://blog.metaflow.fr/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/bb034430b71e/share/twitter
  40. https://medium.com/p/bb034430b71e/share/facebook
  41. https://medium.com/p/bb034430b71e/share/twitter
  42. https://medium.com/p/bb034430b71e/share/facebook
