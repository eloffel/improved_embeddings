   #[1]github [2]recent commits to sentencepiece:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]82
     * [35]star [36]2,309
     * [37]fork [38]265

[39]google/[40]sentencepiece

   [41]code [42]issues 10 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   unsupervised text tokenizer for neural network-based text generation.
   [47]neural-machine-translation [48]natural-language-processing
   [49]word-segmentation
     * [50]490 commits
     * [51]2 branches
     * [52]7 releases
     * [53]24 contributors
     * [54]apache-2.0

    1. [55]c++ 98.2%
    2. [56]python 0.7%
    3. [57]jupyter notebook 0.6%
    4. [58]cmake 0.2%
    5. [59]shell 0.2%
    6. [60]perl 0.1%

   (button) c++ python jupyter notebook cmake shell perl
   branch: master (button) new pull request
   [61]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/g
   [62]download zip

downloading...

   want to be notified of new releases in google/sentencepiece?
   [63]sign in [64]sign up

launching github desktop...

   if nothing happens, [65]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [66]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [67]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [68]download the github extension for visual studio
   and try again.

   (button) go back
   [69]@taku910
   [70]taku910 [71]update readme.md
   latest commit [72]635752e mar 27, 2019
   [73]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [74]data [75]remove control characters in the default id4_* normalizers
   jan 10, 2019
   [76]doc [77]delete sentencepiece_python_module_example.ipynb mar 27,
   2019
   [78]python [79]update readme.md mar 27, 2019
   [80]src
   [81]tensorflow [82]update make_py_wheel_mac.sh mar 22, 2019
   [83]third_party [84]updated the document jan 10, 2019
   [85].gitignore
   [86].travis.yml
   [87]cmakelists.txt
   [88]contributing.md
   [89]license
   [90]readme.md
   [91]version [92]updated the version mar 22, 2019
   [93]appveyor.yml [94]fixes travis rules aug 26, 2018
   [95]config.h.in
   [96]sentencepiece.pc.in [97]use builtin protobuf-lite package in
   third_party jan 8, 2019
   [98]test.bat
   [99]test.sh

readme.md

sentencepiece

   [100]build status [101]build status [102]coverage status [103]github
   issues [104]pypi version [105]contributions welcome [106]license

   sentencepiece is an unsupervised text tokenizer and detokenizer mainly
   for neural network-based text generation systems where the vocabulary
   size is predetermined prior to the neural model training. sentencepiece
   implements subword units (e.g., byte-pair-encoding (bpe) [[107]sennrich
   et al.]) and unigram language model [[108]kudo.]) with the extension of
   direct training from raw sentences. sentencepiece allows us to make a
   purely end-to-end system that does not depend on language-specific
   pre/postprocessing.

   this is not an official google product.

technical highlights

     * purely data driven: sentencepiece trains id121 and
       deid121 models from sentences. pre-id121 ([109]moses
       tokenizer/[110]mecab/[111]kytea) is not always required.
     * language independent: sentencepiece treats the sentences just as
       sequences of unicode characters. there is no language-dependent
       logic.
     * multiple subword algorithms: bpe [[112]sennrich et al.] and unigram
       language model [[113]kudo.] are supported.
     * subword id173: sentencepiece implements subword sampling
       for [114]subword id173 which helps to improve the
       robustness and accuracy of id4 models.
     * fast and lightweight: segmentation speed is around 50k
       sentences/sec, and memory footprint is around 6mb.
     * self-contained: the same id121/deid121 is obtained as
       long as the same model file is used.
     * direct vocabulary id generation: sentencepiece manages vocabulary
       to id mapping and can directly generate vocabulary id sequences
       from raw sentences.
     * nfkc-based id172: sentencepiece performs nfkc-based text
       id172.

comparisons with other implementations

   feature sentencepiece [115]subword-id4 [116]wordpiece
   supported algorithm bpe, unigram, char, word bpe bpe*
   oss? yes yes google internal
   subword id173 [117]yes no no
   python library (pip) [118]yes no n/a
   c++ library [119]yes no n/a
   pre-segmentation required? [120]no yes yes
   customizable id172 (e.g., nfkc) [121]yes no n/a
   direct id generation [122]yes no n/a

   note that bpe algorithm used in wordpiece is slightly different from
   the original bpe.

overview

what is sentencepiece?

   sentencepiece is a re-implementation of sub-word units, an effective
   way to alleviate the open vocabulary problems in neural machine
   translation. sentencepiece supports two segmentation algorithms,
   byte-pair-encoding (bpe) [[123]sennrich et al.] and unigram language
   model [[124]kudo.]. here are the high level differences from other
   implementations.

the number of unique tokens is predetermined

   id4 models typically operate with a fixed
   vocabulary. unlike most unsupervised id40 algorithms,
   which assume an infinite vocabulary, sentencepiece trains the
   segmentation model such that the final vocabulary size is fixed, e.g.,
   8k, 16k, or 32k.

   note that sentencepiece specifies the final vocabulary size for
   training, which is different from [125]subword-id4 that uses the number
   of merge operations. the number of merge operations is a bpe-specific
   parameter and not applicable to other segmentation algorithms,
   including unigram, word and character.

trains from raw sentences

   previous sub-word implementations assume that the input sentences are
   pre-tokenized. this constraint was required for efficient training, but
   makes the preprocessing complicated as we have to run language
   dependent tokenizers in advance. the implementation of sentencepiece is
   fast enough to train the model from raw sentences. this is useful for
   training the tokenizer and detokenizer for chinese, japanese and korean
   where no explicit spaces exist between words.

whitespace is treated as a basic symbol

   the first step of natural language processing is text id121. for
   example, a standard english tokenizer would segment the text "hello
   world." into the following three tokens.

     [hello] [world] [.]

   one observation is that the original input and tokenized sequence are
   not reversibly convertible. for instance, the information that is no
   space between    world    and    .    is dropped from the tokenized sequence,
   since e.g., tokenize(   world.   ) == tokenize(   world .   )

   sentencepiece treats the input text just as a sequence of unicode
   characters. whitespace is also handled as a normal symbol. to handle
   the whitespace as a basic token explicitly, sentencepiece first escapes
   the whitespace with a meta symbol "   " (u+2581) as follows.

     hello   world.

   then, this text is segmented into small pieces, for example:

     [hello] [   wor] [ld] [.]

   since the whitespace is preserved in the segmented text, we can
   detokenize the text without any ambiguities.
  detokenized = ''.join(pieces).replace('_', ' ')

   this feature makes it possible to perform deid121 without
   relying on language-specific resources.

   note that we cannot apply the same lossless conversions when splitting
   the sentence with standard word segmenters, since they treat the
   whitespace as a special symbol. tokenized sequences do not preserve the
   necessary information to restore the original sentence.
     * (en) hello world.     [hello] [world] [.] (a space between hello and
       world)
     * (ja)                              [               ] [      ] [   ] (no space between                 and       )

subword id173

   subword id173 [[126]kudo.] is a simple id173 method
   that virtually augments training data with on-the-fly subword sampling,
   which helps to improve the accuracy as well as robustness of id4
   models.

   to enable subword id173, you would like to integrate
   sentencepiece library ([127]c++/[128]python) into the id4 system to
   sample one segmentation for each parameter update, which is different
   from the standard off-line data preparations. here's the example of
   [129]python library. you can find that 'new york' is segmented
   differently on each sampleencode call. the details of sampling
   parameters are found in [130]sentencepiece_processor.h.
>>> import sentencepiece as spm
>>> s = spm.sentencepieceprocessor()
>>> s.load('spm.model')
>>> for n in range(5):
...     s.sampleencodeaspieces('new york', -1, 0.1)
...
['   ', 'n', 'e', 'w', '   york']
['   ', 'new', '   york']
['   ', 'new', '   y', 'o', 'r', 'k']
['   ', 'new', '   york']
['   ', 'new', '   york']

installation

python module

   sentencepiece provides python wrapper that supports both sentencepiece
   training and segmentation. you can install python binary package of
   sentencepiece with.
% pip install sentencepiece

   for more detail, see [131]python module

c++ (from source)

   the following tools and libraries are required to build sentencepiece:
     * [132]cmake
     * c++11 compiler
     * [133]gperftools library (optional, 10-40% performance improvement
       can be obtained.)

   on ubuntu, the build tools can be installed with apt-get:
% sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev

build and install sentencepiece

% cd /path/to/sentencepiece
% mkdir build
% cd build
% cmake ..
% make -j $(nproc)
% sudo make install
% sudo ldconfig -v

   on osx/macos, replace the last command with sudo
   update_dyld_shared_cache

tensorflow module

   see [134]tensorflow/readme.md

usage instructions

train sentencepiece model

% spm_train --input=<input> --model_prefix=<model_name> --vocab_size=8000 --char
acter_coverage=1.0 --model_type=<type>

     * --input: one-sentence-per-line raw corpus file. no need to run
       tokenizer, normalizer or preprocessor. by default, sentencepiece
       normalizes the input with unicode nfkc. you can pass a
       comma-separated list of files.
     * --model_prefix: output model name prefix. <model_name>.model and
       <model_name>.vocab are generated.
     * --vocab_size: vocabulary size, e.g., 8000, 16000, or 32000
     * --character_coverage: amount of characters covered by the model,
       good defaults are: 0.9995 for languages with rich character set
       like japanse or chinese and 1.0 for other languages with small
       character set.
     * --model_type: model type. choose from unigram (default), bpe, char,
       or word. the input sentence must be pretokenized when using word
       type.

   use --help flag to display all parameters for training.

encode raw text into sentence pieces/ids

% spm_encode --model=<model_file> --output_format=piece < input > output
% spm_encode --model=<model_file> --output_format=id < input > output

   use --extra_options flag to insert the bos/eos markers or reverse the
   input sequence.
% spm_encode --extra_options=eos (add </s> only)
% spm_encode --extra_options=bos:eos (add <s> and </s>)
% spm_encode --extra_options=reverse:bos:eos (reverse input and add <s> and </s>
)

   sentencepiece supports nbest segmentation and segmentation sampling
   with --output_format=(nbest|sample)_(piece|id) flags.
% spm_encode --model=<model_file> --output_format=sample_piece --nbest_size=-1 -
-alpha=0.5 < input > output
% spm_encode --model=<model_file> --output_format=nbest_id --nbest_size=10 < inp
ut > output

decode sentence pieces/ids into raw text

% spm_decode --model=<model_file> --input_format=piece < input > output
% spm_decode --model=<model_file> --input_format=id < input > output

   use --extra_options flag to decode the text in reverse order.
% spm_decode --extra_options=reverse < input > output

end-to-end example

% spm_train --input=data/botchan.txt --model_prefix=m --vocab_size=1000
unigram_model_trainer.cc(494) log(info) starts training with :
input: "../data/botchan.txt"
... <snip>
unigram_model_trainer.cc(529) log(info) em sub_iter=1 size=1100 obj=10.4973 num_
tokens=37630 num_tokens/piece=34.2091
trainer_interface.cc(272) log(info) saving model: m.model
trainer_interface.cc(281) log(info) saving vocabs: m.vocab

% echo "i saw a girl with a telescope." | spm_encode --model=m.model
   i    saw    a    girl    with    a     te le s c o pe .

% echo "i saw a girl with a telescope." | spm_encode --model=m.model --output_fo
rmat=id
9 459 11 939 44 11 4 142 82 8 28 21 132 6

% echo "9 459 11 939 44 11 4 142 82 8 28 21 132 6" | spm_decode --model=m.model
--input_format=id
i saw a girl with a telescope.

   you can find that the original input sentence is restored from the
   vocabulary id sequence.

export vocabulary list

% spm_export_vocab --model=<model_file> --output=<output file>

   <output file> stores a list of vocabulary and emission log
   probabilities. the vocabulary id corresponds to the line number in this
   file.

redefine special meta tokens

   by default, sentencepiece uses unknown (<unk>), bos (<s>) and eos
   (</s>) tokens which have the ids of 0, 1, and 2 respectively. we can
   redefine this mapping in the training phase as follows.
% spm_train --bos_id=0 --eos_id=1 --unk_id=5 --input=... --model_prefix=... --ch
aracter_coverage=...

   when setting -1 id e.g., bos_id=-1, this special token is disabled.
   note that the unknow id cannot be disabled. we can define an id for
   padding (<pad>) as --pad_id=3.

   if you want to assign another special tokens, please see [135]use
   custom symbols.

vocabulary restriction

   spm_encode accepts a --vocabulary and a --vocabulary_threshold option
   so that spm_encode will only produce symbols which also appear in the
   vocabulary (with at least some frequency). the background of this
   feature is described in [136]subword-id4 page.

   the usage is basically the same as that of subword-id4. assuming that
   l1 and l2 are the two languages (source/target languages), train the
   shared spm model, and get resulting vocabulary for each:
% cat {train_file}.l1 {train_file}.l2 | shuffle > train
% spm_train --input=train --model_prefix=spm --vocab_size=8000 --character_cover
age=0.9995
% spm_encode --model=spm.model --generate_vocabulary < {train_file}.l1 > {vocab_
file}.l1
% spm_encode --model=spm.model --generate_vocabulary < {train_file}.l2 > {vocab_
file}.l2

   shuffle command is used just in case because spm_train loads the first
   10m lines of corpus by default.

   then segment train/test corpus with --vocabulary option
% spm_encode --model=spm.model --vocabulary={vocab_file}.l1 --vocabulary_thresho
ld=50 < {test_file}.l1 > {test_file}.seg.l1
% spm_encode --model=spm.model --vocabulary={vocab_file}.l2 --vocabulary_thresho
ld=50 < {test_file}.l2 > {test_file}.seg.l2

advanced topics

     * [137]sentencepiece experiments
     * [138]sentencepieceprocessor c++ api
     * [139]use custom text id172 rules
     * [140]use custom symbols
     * [141]python module
     * [142]tensorflow module
     * [segmentation and training algorithms in detail]

     *    2019 github, inc.
     * [143]terms
     * [144]privacy
     * [145]security
     * [146]status
     * [147]help

     * [148]contact github
     * [149]pricing
     * [150]api
     * [151]training
     * [152]blog
     * [153]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [154]reload to refresh your
   session. you signed out in another tab or window. [155]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/google/sentencepiece/commits/master.atom
   3. https://github.com/google/sentencepiece#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/google/sentencepiece
  32. https://github.com/join
  33. https://github.com/login?return_to=/google/sentencepiece
  34. https://github.com/google/sentencepiece/watchers
  35. https://github.com/login?return_to=/google/sentencepiece
  36. https://github.com/google/sentencepiece/stargazers
  37. https://github.com/login?return_to=/google/sentencepiece
  38. https://github.com/google/sentencepiece/network/members
  39. https://github.com/google
  40. https://github.com/google/sentencepiece
  41. https://github.com/google/sentencepiece
  42. https://github.com/google/sentencepiece/issues
  43. https://github.com/google/sentencepiece/pulls
  44. https://github.com/google/sentencepiece/projects
  45. https://github.com/google/sentencepiece/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/neural-machine-translation
  48. https://github.com/topics/natural-language-processing
  49. https://github.com/topics/word-segmentation
  50. https://github.com/google/sentencepiece/commits/master
  51. https://github.com/google/sentencepiece/branches
  52. https://github.com/google/sentencepiece/releases
  53. https://github.com/google/sentencepiece/graphs/contributors
  54. https://github.com/google/sentencepiece/blob/master/license
  55. https://github.com/google/sentencepiece/search?l=c++
  56. https://github.com/google/sentencepiece/search?l=python
  57. https://github.com/google/sentencepiece/search?l=jupyter-notebook
  58. https://github.com/google/sentencepiece/search?l=cmake
  59. https://github.com/google/sentencepiece/search?l=shell
  60. https://github.com/google/sentencepiece/search?l=perl
  61. https://github.com/google/sentencepiece/find/master
  62. https://github.com/google/sentencepiece/archive/master.zip
  63. https://github.com/login?return_to=https://github.com/google/sentencepiece
  64. https://github.com/join?return_to=/google/sentencepiece
  65. https://desktop.github.com/
  66. https://desktop.github.com/
  67. https://developer.apple.com/xcode/
  68. https://visualstudio.github.com/
  69. https://github.com/taku910
  70. https://github.com/google/sentencepiece/commits?author=taku910
  71. https://github.com/google/sentencepiece/commit/635752e49220cfd188b4d0ddb76b692bc33e968d
  72. https://github.com/google/sentencepiece/commit/635752e49220cfd188b4d0ddb76b692bc33e968d
  73. https://github.com/google/sentencepiece/tree/635752e49220cfd188b4d0ddb76b692bc33e968d
  74. https://github.com/google/sentencepiece/tree/master/data
  75. https://github.com/google/sentencepiece/commit/18c337f32d376b339f15a48c7ccc5c1c288345a1
  76. https://github.com/google/sentencepiece/tree/master/doc
  77. https://github.com/google/sentencepiece/commit/3ad1af39bca668ba4b2fc60d1e67a77cde4a1411
  78. https://github.com/google/sentencepiece/tree/master/python
  79. https://github.com/google/sentencepiece/commit/635752e49220cfd188b4d0ddb76b692bc33e968d
  80. https://github.com/google/sentencepiece/tree/master/src
  81. https://github.com/google/sentencepiece/tree/master/tensorflow
  82. https://github.com/google/sentencepiece/commit/c58588f901685234272392dba1c4a764ce473246
  83. https://github.com/google/sentencepiece/tree/master/third_party
  84. https://github.com/google/sentencepiece/commit/a938a667f5994a29877d20bad1e3b9598bc6d005
  85. https://github.com/google/sentencepiece/blob/master/.gitignore
  86. https://github.com/google/sentencepiece/blob/master/.travis.yml
  87. https://github.com/google/sentencepiece/blob/master/cmakelists.txt
  88. https://github.com/google/sentencepiece/blob/master/contributing.md
  89. https://github.com/google/sentencepiece/blob/master/license
  90. https://github.com/google/sentencepiece/blob/master/readme.md
  91. https://github.com/google/sentencepiece/blob/master/version
  92. https://github.com/google/sentencepiece/commit/c7cdef244e65723fce7a4f344b0ff6de5f86d82e
  93. https://github.com/google/sentencepiece/blob/master/appveyor.yml
  94. https://github.com/google/sentencepiece/commit/371a097dd7ddbee64302423ecbd85f6851e5e174
  95. https://github.com/google/sentencepiece/blob/master/config.h.in
  96. https://github.com/google/sentencepiece/blob/master/sentencepiece.pc.in
  97. https://github.com/google/sentencepiece/commit/7b19d68be0d516d49572a0fa877473df904f90c2
  98. https://github.com/google/sentencepiece/blob/master/test.bat
  99. https://github.com/google/sentencepiece/blob/master/test.sh
 100. https://travis-ci.org/google/sentencepiece
 101. https://ci.appveyor.com/project/taku910/sentencepiece
 102. https://coveralls.io/github/google/sentencepiece?branch=master
 103. https://github.com/google/sentencepiece/issues
 104. https://badge.fury.io/py/sentencepiece
 105. https://github.com/google/sentencepiece/blob/master/contributing.md
 106. https://opensource.org/licenses/apache-2.0
 107. http://www.aclweb.org/anthology/p16-1162
 108. https://arxiv.org/abs/1804.10959
 109. https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl
 110. http://taku910.github.io/mecab/
 111. http://www.phontron.com/kytea/
 112. http://www.aclweb.org/anthology/p16-1162
 113. https://arxiv.org/abs/1804.10959
 114. https://arxiv.org/abs/1804.10959
 115. https://github.com/rsennrich/subword-id4
 116. https://arxiv.org/pdf/1609.08144.pdf
 117. https://github.com/google/sentencepiece#subword-id173
 118. https://github.com/google/sentencepiece/blob/master/python/readme.md
 119. https://github.com/google/sentencepiece/blob/master/doc/api.md
 120. https://github.com/google/sentencepiece#whitespace-is-treated-as-a-basic-symbol
 121. https://github.com/google/sentencepiece/blob/master/doc/id172.md
 122. https://github.com/google/sentencepiece#end-to-end-example
 123. http://www.aclweb.org/anthology/p16-1162
 124. https://arxiv.org/abs/1804.10959
 125. https://github.com/rsennrich/subword-id4
 126. https://arxiv.org/abs/1804.10959
 127. https://github.com/google/sentencepiece/blob/master/doc/api.md#sampling-subword-id173
 128. https://github.com/google/sentencepiece/blob/master/python/readme.md
 129. https://github.com/google/sentencepiece/blob/master/python/readme.md
 130. https://github.com/google/sentencepiece/blob/master/src/sentencepiece_processor.h
 131. https://github.com/google/sentencepiece/blob/master/python/readme.md
 132. https://cmake.org/
 133. https://github.com/gperftools/gperftools
 134. https://github.com/google/sentencepiece/blob/master/tensorflow/readme.md
 135. https://github.com/google/sentencepiece/blob/master/doc/special_symbols.md
 136. https://github.com/rsennrich/subword-id4#best-practice-advice-for-byte-pair-encoding-in-id4
 137. https://github.com/google/sentencepiece/blob/master/doc/experiments.md
 138. https://github.com/google/sentencepiece/blob/master/doc/api.md
 139. https://github.com/google/sentencepiece/blob/master/doc/id172.md
 140. https://github.com/google/sentencepiece/blob/master/doc/special_symbols.md
 141. https://github.com/google/sentencepiece/blob/master/python/readme.md
 142. https://github.com/google/sentencepiece/blob/master/tensorflow/readme.md
 143. https://github.com/site/terms
 144. https://github.com/site/privacy
 145. https://github.com/security
 146. https://githubstatus.com/
 147. https://help.github.com/
 148. https://github.com/contact
 149. https://github.com/pricing
 150. https://developer.github.com/
 151. https://training.github.com/
 152. https://github.blog/
 153. https://github.com/about
 154. https://github.com/google/sentencepiece
 155. https://github.com/google/sentencepiece

   hidden links:
 157. https://github.com/
 158. https://github.com/google/sentencepiece
 159. https://github.com/google/sentencepiece
 160. https://github.com/google/sentencepiece
 161. https://help.github.com/articles/which-remote-url-should-i-use
 162. https://github.com/google/sentencepiece#sentencepiece
 163. https://github.com/google/sentencepiece#technical-highlights
 164. https://github.com/google/sentencepiece#comparisons-with-other-implementations
 165. https://github.com/google/sentencepiece#overview
 166. https://github.com/google/sentencepiece#what-is-sentencepiece
 167. https://github.com/google/sentencepiece#the-number-of-unique-tokens-is-predetermined
 168. https://github.com/google/sentencepiece#trains-from-raw-sentences
 169. https://github.com/google/sentencepiece#whitespace-is-treated-as-a-basic-symbol
 170. https://github.com/google/sentencepiece#subword-id173
 171. https://github.com/google/sentencepiece#installation
 172. https://github.com/google/sentencepiece#python-module
 173. https://github.com/google/sentencepiece#c-from-source
 174. https://github.com/google/sentencepiece#build-and-install-sentencepiece
 175. https://github.com/google/sentencepiece#tensorflow-module
 176. https://github.com/google/sentencepiece#usage-instructions
 177. https://github.com/google/sentencepiece#train-sentencepiece-model
 178. https://github.com/google/sentencepiece#encode-raw-text-into-sentence-piecesids
 179. https://github.com/google/sentencepiece#decode-sentence-piecesids-into-raw-text
 180. https://github.com/google/sentencepiece#end-to-end-example
 181. https://github.com/google/sentencepiece#export-vocabulary-list
 182. https://github.com/google/sentencepiece#redefine-special-meta-tokens
 183. https://github.com/google/sentencepiece#vocabulary-restriction
 184. https://github.com/google/sentencepiece#advanced-topics
 185. https://github.com/
