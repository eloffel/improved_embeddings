   #[1]sebastian ruder

   [2]sebastian ruder
     * [3]about
     * [4]tags
     * [5]papers
     * [6]talks
     * [7]news
     * [8]faq
     * [9]nlp news
     * [10]nlp progress
     * [11]contact

   19 january 2016 / [12]optimization

an overview of id119 optimization algorithms

   an overview of id119 optimization algorithms

   this post explores how many of the most popular gradient-based
   optimization algorithms actually work.

   note: if you are looking for a review paper, this blog post is also
   available as an [13]article on arxiv.

   update 09.02.2018: added [14]amsgrad.

   update 24.11.2017: most of the content in this article is now also
   available as [15]slides.

   update 15.06.2017: added derivations of [16]adamax and [17]nadam.

   update 21.06.16: this post was posted to hacker news. [18]the
   discussion provides some interesting pointers to related work and other
   techniques.

   table of contents:
     * [19]id119 variants
          + [20]batch id119
          + [21]stochastic id119
          + [22]mini-batch id119
     * [23]challenges
     * [24]id119 optimization algorithms
          + [25]momentum
          + [26]nesterov accelerated gradient
          + [27]adagrad
          + [28]adadelta
          + [29]rmsprop
          + [30]adam
          + [31]adamax
          + [32]nadam
          + [33]amsgrad
          + [34]visualization of algorithms
          + [35]which optimizer to choose?
     * [36]parallelizing and distributing sgd
          + [37]hogwild!
          + [38]downpour sgd
          + [39]delay-tolerant algorithms for sgd
          + [40]tensorflow
          + [41]elastic averaging sgd
     * [42]additional strategies for optimizing sgd
          + [43]shuffling and curriculum learning
          + [44]batch id172
          + [45]early stopping
          + [46]gradient noise
     * [47]conclusion
     * [48]references

   id119 is one of the most popular algorithms to perform
   optimization and by far the most common way to optimize neural
   networks. at the same time, every state-of-the-art deep learning
   library contains implementations of various algorithms to optimize
   id119 (e.g. [49]lasagne's, [50]caffe's, and [51]keras'
   documentation). these algorithms, however, are often used as black-box
   optimizers, as practical explanations of their strengths and weaknesses
   are hard to come by.

   this blog post aims at providing you with intuitions towards the
   behaviour of different algorithms for optimizing id119 that
   will help you put them to use. we are first going to look at the
   different variants of id119. we will then briefly summarize
   challenges during training. subsequently, we will introduce the most
   common optimization algorithms by showing their motivation to resolve
   these challenges and how this leads to the derivation of their update
   rules. we will also take a short look at algorithms and architectures
   to optimize id119 in a parallel and distributed setting.
   finally, we will consider additional strategies that are helpful for
   optimizing id119.

   id119 is a way to minimize an objective function
   \(j(\theta)\) parameterized by a model's parameters \(\theta \in
   \mathbb{r}^d \) by updating the parameters in the opposite direction of
   the gradient of the objective function \(\nabla_\theta j(\theta)\)
   w.r.t. to the parameters. the learning rate \(\eta\) determines the
   size of the steps we take to reach a (local) minimum. in other words,
   we follow the direction of the slope of the surface created by the
   objective function downhill until we reach a valley. if you are
   unfamiliar with id119, you can find a good introduction on
   optimizing neural networks [52]here.

id119 variants

   there are three variants of id119, which differ in how much
   data we use to compute the gradient of the objective function.
   depending on the amount of data, we make a trade-off between the
   accuracy of the parameter update and the time it takes to perform an
   update.

batch id119

   vanilla id119, aka batch id119, computes the
   gradient of the cost function w.r.t. to the parameters \(\theta\) for
   the entire training dataset:

   \(\theta = \theta - \eta \cdot \nabla_\theta j( \theta)\).

   as we need to calculate the gradients for the whole dataset to perform
   just one update, batch id119 can be very slow and is
   intractable for datasets that don't fit in memory. batch gradient
   descent also doesn't allow us to update our model online, i.e. with new
   examples on-the-fly.

   in code, batch id119 looks something like this:
for i in range(nb_epochs):
  params_grad = evaluate_gradient(loss_function, data, params)
  params = params - learning_rate * params_grad

   for a pre-defined number of epochs, we first compute the gradient
   vector params_grad of the id168 for the whole dataset w.r.t.
   our parameter vector params. note that state-of-the-art deep learning
   libraries provide automatic differentiation that efficiently computes
   the gradient w.r.t. some parameters. if you derive the gradients
   yourself, then gradient checking is a good idea. (see [53]here for some
   great tips on how to check gradients properly.)

   we then update our parameters in the opposite direction of the
   gradients with the learning rate determining how big of an update we
   perform. batch id119 is guaranteed to converge to the global
   minimum for convex error surfaces and to a local minimum for non-convex
   surfaces.

stochastic id119

   stochastic id119 (sgd) in contrast performs a parameter
   update for each training example \(x^{(i)}\) and label \(y^{(i)}\):

   \(\theta = \theta - \eta \cdot \nabla_\theta j( \theta; x^{(i)};
   y^{(i)})\).

   batch id119 performs redundant computations for large
   datasets, as it recomputes gradients for similar examples before each
   parameter update. sgd does away with this redundancy by performing one
   update at a time. it is therefore usually much faster and can also be
   used to learn online.
   sgd performs frequent updates with a high variance that cause the
   objective function to fluctuate heavily as in image 1.
   sgd fluctuation image 1: sgd fluctuation (source: [54]wikipedia)

   while batch id119 converges to the minimum of the basin the
   parameters are placed in, sgd's fluctuation, on the one hand, enables
   it to jump to new and potentially better local minima. on the other
   hand, this ultimately complicates convergence to the exact minimum, as
   sgd will keep overshooting. however, it has been shown that when we
   slowly decrease the learning rate, sgd shows the same convergence
   behaviour as batch id119, almost certainly converging to a
   local or the global minimum for non-convex and id76
   respectively.
   its code fragment simply adds a loop over the training examples and
   evaluates the gradient w.r.t. each example. note that we shuffle the
   training data at every epoch as explained in [55]this section.
for i in range(nb_epochs):
  np.random.shuffle(data)
  for example in data:
    params_grad = evaluate_gradient(loss_function, example, params)
    params = params - learning_rate * params_grad

mini-batch id119

   mini-batch id119 finally takes the best of both worlds and
   performs an update for every mini-batch of \(n\) training examples:

   \(\theta = \theta - \eta \cdot \nabla_\theta j( \theta; x^{(i:i+n)};
   y^{(i:i+n)})\).

   this way, it a) reduces the variance of the parameter updates, which
   can lead to more stable convergence; and b) can make use of highly
   optimized matrix optimizations common to state-of-the-art deep learning
   libraries that make computing the gradient w.r.t. a mini-batch very
   efficient. common mini-batch sizes range between 50 and 256, but can
   vary for different applications. mini-batch id119 is
   typically the algorithm of choice when training a neural network and
   the term sgd usually is employed also when mini-batches are used. note:
   in modifications of sgd in the rest of this post, we leave out the
   parameters \(x^{(i:i+n)}; y^{(i:i+n)}\) for simplicity.

   in code, instead of iterating over examples, we now iterate over
   mini-batches of size 50:
for i in range(nb_epochs):
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params)
    params = params - learning_rate * params_grad

challenges

   vanilla mini-batch id119, however, does not guarantee good
   convergence, but offers a few challenges that need to be addressed:
     * choosing a proper learning rate can be difficult. a learning rate
       that is too small leads to painfully slow convergence, while a
       learning rate that is too large can hinder convergence and cause
       the id168 to fluctuate around the minimum or even to
       diverge.
     * learning rate schedules ^[56][1] try to adjust the learning rate
       during training by e.g. annealing, i.e. reducing the learning rate
       according to a pre-defined schedule or when the change in objective
       between epochs falls below a threshold. these schedules and
       thresholds, however, have to be defined in advance and are thus
       unable to adapt to a dataset's characteristics ^[57][2].
     * additionally, the same learning rate applies to all parameter
       updates. if our data is sparse and our features have very different
       frequencies, we might not want to update all of them to the same
       extent, but perform a larger update for rarely occurring features.
     * another key challenge of minimizing highly non-convex error
       functions common for neural networks is avoiding getting trapped in
       their numerous suboptimal local minima. dauphin et al. ^[58][3]
       argue that the difficulty arises in fact not from local minima but
       from saddle points, i.e. points where one dimension slopes up and
       another slopes down. these saddle points are usually surrounded by
       a plateau of the same error, which makes it notoriously hard for
       sgd to escape, as the gradient is close to zero in all dimensions.

id119 optimization algorithms

   in the following, we will outline some algorithms that are widely used
   by the deep learning community to deal with the aforementioned
   challenges. we will not discuss algorithms that are infeasible to
   compute in practice for high-dimensional data sets, e.g. second-order
   methods such as [59]newton's method.

momentum

   sgd has trouble navigating ravines, i.e. areas where the surface curves
   much more steeply in one dimension than in another ^[60][4], which are
   common around local optima. in these scenarios, sgd oscillates across
   the slopes of the ravine while only making hesitant progress along the
   bottom towards the local optimum as in image 2.
   sgd without momentum image 2: sgd without momentum sgd with momentum
   image 3: sgd with momentum

   momentum ^[61][5] is a method that helps accelerate sgd in the relevant
   direction and dampens oscillations as can be seen in image 3. it does
   this by adding a fraction \(\gamma\) of the update vector of the past
   time step to the current update vector:

   \(
   \begin{align}
   \begin{split}
   v_t &= \gamma v_{t-1} + \eta \nabla_\theta j( \theta) \\
   \theta &= \theta - v_t
   \end{split}
   \end{align}
   \)

   note: some implementations exchange the signs in the equations. the
   momentum term \(\gamma\) is usually set to 0.9 or a similar value.

   essentially, when using momentum, we push a ball down a hill. the ball
   accumulates momentum as it rolls downhill, becoming faster and faster
   on the way (until it reaches its terminal velocity if there is air
   resistance, i.e. \(\gamma < 1\)). the same thing happens to our
   parameter updates: the momentum term increases for dimensions whose
   gradients point in the same directions and reduces updates for
   dimensions whose gradients change directions. as a result, we gain
   faster convergence and reduced oscillation.

nesterov accelerated gradient

   however, a ball that rolls down a hill, blindly following the slope, is
   highly unsatisfactory. we'd like to have a smarter ball, a ball that
   has a notion of where it is going so that it knows to slow down before
   the hill slopes up again.

   nesterov accelerated gradient (nag) ^[62][6] is a way to give our
   momentum term this kind of prescience. we know that we will use our
   momentum term \(\gamma v_{t-1}\) to move the parameters \(\theta\).
   computing \( \theta - \gamma v_{t-1} \) thus gives us an approximation
   of the next position of the parameters (the gradient is missing for the
   full update), a rough idea where our parameters are going to be. we can
   now effectively look ahead by calculating the gradient not w.r.t. to
   our current parameters \(\theta\) but w.r.t. the approximate future
   position of our parameters:

   \(
   \begin{align}
   \begin{split}
   v_t &= \gamma v_{t-1} + \eta \nabla_\theta j( \theta - \gamma v_{t-1} )
   \\
   \theta &= \theta - v_t
   \end{split}
   \end{align}
   \)

   again, we set the momentum term \(\gamma\) to a value of around 0.9.
   while momentum first computes the current gradient (small blue vector
   in image 4) and then takes a big jump in the direction of the updated
   accumulated gradient (big blue vector), nag first makes a big jump in
   the direction of the previous accumulated gradient (brown vector),
   measures the gradient and then makes a correction (red vector), which
   results in the complete nag update (green vector). this anticipatory
   update prevents us from going too fast and results in increased
   responsiveness, which has significantly increased the performance of
   id56s on a number of tasks ^[63][7].
   sgd fluctuation image 4: nesterov update (source: [64]g. hinton's
   lecture 6c)

   refer to [65]here for another explanation about the intuitions behind
   nag, while ilya sutskever gives a more detailed overview in his phd
   thesis ^[66][8].

   now that we are able to adapt our updates to the slope of our error
   function and speed up sgd in turn, we would also like to adapt our
   updates to each individual parameter to perform larger or smaller
   updates depending on their importance.

adagrad

   adagrad ^[67][9] is an algorithm for gradient-based optimization that
   does just this: it adapts the learning rate to the parameters,
   performing smaller updates
   (i.e. low learning rates) for parameters associated with frequently
   occurring features, and larger updates (i.e. high learning rates) for
   parameters associated with infrequent features. for this reason, it is
   well-suited for dealing with sparse data. dean et al. ^[68][10] have
   found that adagrad greatly improved the robustness of sgd and used it
   for training large-scale neural nets at google, which -- among other
   things -- learned to [69]recognize cats in youtube videos. moreover,
   pennington et al. ^[70][11] used adagrad to train glove word
   embeddings, as infrequent words require much larger updates than
   frequent ones.

   previously, we performed an update for all parameters \(\theta\) at
   once as every parameter \(\theta_i\) used the same learning rate
   \(\eta\). as adagrad uses a different learning rate for every parameter
   \(\theta_i\) at every time step \(t\), we first show adagrad's
   per-parameter update, which we then vectorize. for brevity, we use
   \(g_{t}\) to denote the gradient at time step \(t\). \(g_{t, i}\) is
   then the partial derivative of the objective function w.r.t. to the
   parameter \(\theta_i\) at time step \(t\):

   \(g_{t, i} = \nabla_\theta j( \theta_{t, i} )\).

   the sgd update for every parameter \(\theta_i\) at each time step \(t\)
   then becomes:

   \(\theta_{t+1, i} = \theta_{t, i} - \eta \cdot g_{t, i}\).

   in its update rule, adagrad modifies the general learning rate \(\eta\)
   at each time step \(t\) for every parameter \(\theta_i\) based on the
   past gradients that have been computed for \(\theta_i\):

   \(\theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{g_{t, ii} +
   \epsilon}} \cdot g_{t, i}\).

   \(g_{t} \in \mathbb{r}^{d \times d} \) here is a diagonal matrix where
   each diagonal element \(i, i\) is the sum of the squares of the
   gradients w.r.t. \(\theta_i\) up to time step \(t\) ^[71][12], while
   \(\epsilon\) is a smoothing term that avoids division by zero (usually
   on the order of \(1e-8\)). interestingly, without the square root
   operation, the algorithm performs much worse.

   as \(g_{t}\) contains the sum of the squares of the past gradients
   w.r.t. to all parameters \(\theta\) along its diagonal, we can now
   vectorize our implementation by performing a matrix-vector product
   \(\odot\) between \(g_{t}\) and \(g_{t}\):

   \(\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{g_{t} + \epsilon}}
   \odot g_{t}\).

   one of adagrad's main benefits is that it eliminates the need to
   manually tune the learning rate. most implementations use a default
   value of 0.01 and leave it at that.

   adagrad's main weakness is its accumulation of the squared gradients in
   the denominator: since every added term is positive, the accumulated
   sum keeps growing during training. this in turn causes the learning
   rate to shrink and eventually become infinitesimally small, at which
   point the algorithm is no longer able to acquire additional knowledge.
   the following algorithms aim to resolve this flaw.

adadelta

   adadelta ^[72][13] is an extension of adagrad that seeks to reduce its
   aggressive, monotonically decreasing learning rate. instead of
   accumulating all past squared gradients, adadelta restricts the window
   of accumulated past gradients to some fixed size \(w\).

   instead of inefficiently storing \(w\) previous squared gradients, the
   sum of gradients is recursively defined as a decaying average of all
   past squared gradients. the running average \(e[g^2]_t\) at time step
   \(t\) then depends (as a fraction \(\gamma \) similarly to the momentum
   term) only on the previous average and the current gradient:

   \(e[g^2]_t = \gamma e[g^2]_{t-1} + (1 - \gamma) g^2_t \).

   we set \(\gamma\) to a similar value as the momentum term, around 0.9.
   for clarity, we now rewrite our vanilla sgd update in terms of the
   parameter update vector \( \delta \theta_t \):

   \(
   \begin{align}
   \begin{split}
   \delta \theta_t &= - \eta \cdot g_{t, i} \\
   \theta_{t+1} &= \theta_t + \delta \theta_t \end{split}
   \end{align}
   \)

   the parameter update vector of adagrad that we derived previously thus
   takes the form:

   \( \delta \theta_t = - \dfrac{\eta}{\sqrt{g_{t} + \epsilon}} \odot
   g_{t}\).

   we now simply replace the diagonal matrix \(g_{t}\) with the decaying
   average over past squared gradients \(e[g^2]_t\):

   \( \delta \theta_t = - \dfrac{\eta}{\sqrt{e[g^2]_t + \epsilon}}
   g_{t}\).

   as the denominator is just the root mean squared (rms) error criterion
   of the gradient, we can replace it with the criterion short-hand:

   \( \delta \theta_t = - \dfrac{\eta}{rms[g]_{t}} g_t\).

   the authors note that the units in this update (as well as in sgd,
   momentum, or adagrad) do not match, i.e. the update should have the
   same hypothetical units as the parameter. to realize this, they first
   define another exponentially decaying average, this time not of squared
   gradients but of squared parameter updates:

   \(e[\delta \theta^2]_t = \gamma e[\delta \theta^2]_{t-1} + (1 - \gamma)
   \delta \theta^2_t \).

   the root mean squared error of parameter updates is thus:

   \(rms[\delta \theta]_{t} = \sqrt{e[\delta \theta^2]_t + \epsilon} \).

   since \(rms[\delta \theta]_{t}\) is unknown, we approximate it with the
   rms of parameter updates until the previous time step. replacing the
   learning rate \(\eta \) in the previous update rule with \(rms[\delta
   \theta]_{t-1}\) finally yields the adadelta update rule:

   \(
   \begin{align}
   \begin{split}
   \delta \theta_t &= - \dfrac{rms[\delta \theta]_{t-1}}{rms[g]_{t}} g_{t}
   \\
   \theta_{t+1} &= \theta_t + \delta \theta_t
   \end{split}
   \end{align}
   \)

   with adadelta, we do not even need to set a default learning rate, as
   it has been eliminated from the update rule.

rmsprop

   rmsprop is an unpublished, adaptive learning rate method proposed by
   geoff hinton in [73]lecture 6e of his coursera class.

   rmsprop and adadelta have both been developed independently around the
   same time id30 from the need to resolve adagrad's radically
   diminishing learning rates. rmsprop in fact is identical to the first
   update vector of adadelta that we derived above:

   \(
   \begin{align}
   \begin{split}
   e[g^2]_t &= 0.9 e[g^2]_{t-1} + 0.1 g^2_t \\
   \theta_{t+1} &= \theta_{t} - \dfrac{\eta}{\sqrt{e[g^2]_t + \epsilon}}
   g_{t}
   \end{split}
   \end{align}
   \)

   rmsprop as well divides the learning rate by an exponentially decaying
   average of squared gradients. hinton suggests \(\gamma\) to be set to
   0.9, while a good default value for the learning rate \(\eta\) is
   0.001.

adam

   adaptive moment estimation (adam) ^[74][14] is another method that
   computes adaptive learning rates for each parameter. in addition to
   storing an exponentially decaying average of past squared gradients
   \(v_t\) like adadelta and rmsprop, adam also keeps an exponentially
   decaying average of past gradients \(m_t\), similar to momentum.
   whereas momentum can be seen as a ball running down a slope, adam
   behaves like a heavy ball with friction, which thus prefers flat minima
   in the error surface ^[75][15]. we compute the decaying averages of
   past and past squared gradients \(m_t\) and \(v_t\) respectively as
   follows:

   \(
   \begin{align}
   \begin{split}
   m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
   v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
   \end{split}
   \end{align}
   \)

   \(m_t\) and \(v_t\) are estimates of the first moment (the mean) and
   the second moment (the uncentered variance) of the gradients
   respectively, hence the name of the method. as \(m_t\) and \(v_t\) are
   initialized as vectors of 0's, the authors of adam observe that they
   are biased towards zero, especially during the initial time steps, and
   especially when the decay rates are small (i.e. \(\beta_1\) and
   \(\beta_2\) are close to 1).

   they counteract these biases by computing bias-corrected first and
   second moment estimates:

   \(
   \begin{align}
   \begin{split}
   \hat{m}_t &= \dfrac{m_t}{1 - \beta^t_1} \\
   \hat{v}_t &= \dfrac{v_t}{1 - \beta^t_2} \end{split}
   \end{align}
   \)

   they then use these to update the parameters just as we have seen in
   adadelta and rmsprop, which yields the adam update rule:

   \(\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon}
   \hat{m}_t\).

   the authors propose default values of 0.9 for \(\beta_1\), 0.999 for
   \(\beta_2\), and \(10^{-8}\) for \(\epsilon\). they show empirically
   that adam works well in practice and compares favorably to other
   adaptive learning-method algorithms.

adamax

   the \(v_t\) factor in the adam update rule scales the gradient
   inversely proportionally to the \(\ell_2\) norm of the past gradients
   (via the \(v_{t-1}\) term) and current gradient \(|g_t|^2\):

   \(v_t = \beta_2 v_{t-1} + (1 - \beta_2) |g_t|^2\)

   we can generalize this update to the \(\ell_p\) norm. note that kingma
   and ba also parameterize \(\beta_2\) as \(\beta^p_2\):

   \(v_t = \beta_2^p v_{t-1} + (1 - \beta_2^p) |g_t|^p\)

   norms for large \(p\) values generally become numerically unstable,
   which is why \(\ell_1\) and \(\ell_2\) norms are most common in
   practice. however, \(\ell_\infty\) also generally exhibits stable
   behavior. for this reason, the authors propose adamax (kingma and ba,
   2015) and show that \(v_t\) with \(\ell_\infty\) converges to the
   following more stable value. to avoid confusion with adam, we use
   \(u_t\) to denote the infinity norm-constrained \(v_t\):

   \(
   \begin{align}
   \begin{split}
   u_t &= \beta_2^\infty v_{t-1} + (1 - \beta_2^\infty) |g_t|^\infty\\
   & = \max(\beta_2 \cdot v_{t-1}, |g_t|)
   \end{split}
   \end{align}
   \)

   we can now plug this into the adam update equation by replacing
   \(\sqrt{\hat{v}_t} + \epsilon\) with \(u_t\) to obtain the adamax
   update rule:

   \(\theta_{t+1} = \theta_{t} - \dfrac{\eta}{u_t} \hat{m}_t\)

   note that as \(u_t\) relies on the \(\max\) operation, it is not as
   suggestible to bias towards zero as \(m_t\) and \(v_t\) in adam, which
   is why we do not need to compute a bias correction for \(u_t\). good
   default values are again \(\eta = 0.002\), \(\beta_1 = 0.9\), and
   \(\beta_2 = 0.999\).

nadam

   as we have seen before, adam can be viewed as a combination of rmsprop
   and momentum: rmsprop contributes the exponentially decaying average of
   past squared gradients \(v_t\), while momentum accounts for the
   exponentially decaying average of past gradients \(m_t\). we have also
   seen that nesterov accelerated gradient (nag) is superior to vanilla
   momentum.

   nadam (nesterov-accelerated adaptive moment estimation) ^[76][16] thus
   combines adam and nag. in order to incorporate nag into adam, we need
   to modify its momentum term \(m_t\).

   first, let us recall the momentum update rule using our current
   notation :

   \(
   \begin{align}
   \begin{split}
   g_t &= \nabla_{\theta_t}j(\theta_t)\\
   m_t &= \gamma m_{t-1} + \eta g_t\\
   \theta_{t+1} &= \theta_t - m_t
   \end{split}
   \end{align}
   \)

   where \(j\) is our objective function, \(\gamma\) is the momentum decay
   term, and \(\eta\) is our step size. expanding the third equation above
   yields:

   \(\theta_{t+1} = \theta_t - ( \gamma m_{t-1} + \eta g_t)\)

   this demonstrates again that momentum involves taking a step in the
   direction of the previous momentum vector and a step in the direction
   of the current gradient.

   nag then allows us to perform a more accurate step in the gradient
   direction by updating the parameters with the momentum step before
   computing the gradient. we thus only need to modify the gradient
   \(g_t\) to arrive at nag:

   \(
   \begin{align}
   \begin{split}
   g_t &= \nabla_{\theta_t}j(\theta_t - \gamma m_{t-1})\\
   m_t &= \gamma m_{t-1} + \eta g_t\\
   \theta_{t+1} &= \theta_t - m_t
   \end{split}
   \end{align}
   \)

   dozat proposes to modify nag the following way: rather than applying
   the momentum step twice -- one time for updating the gradient \(g_t\)
   and a second time for updating the parameters \(\theta_{t+1}\) -- we
   now apply the look-ahead momentum vector directly to update the current
   parameters:

   \(
   \begin{align}
   \begin{split}
   g_t &= \nabla_{\theta_t}j(\theta_t)\\
   m_t &= \gamma m_{t-1} + \eta g_t\\
   \theta_{t+1} &= \theta_t - (\gamma m_t + \eta g_t)
   \end{split}
   \end{align}
   \)

   notice that rather than utilizing the previous momentum vector
   \(m_{t-1}\) as in the equation of the expanded momentum update rule
   above, we now use the current momentum vector \(m_t\) to look ahead. in
   order to add nesterov momentum to adam, we can thus similarly replace
   the previous momentum vector with the current momentum vector. first,
   recall that the adam update rule is the following (note that we do not
   need to modify \(\hat{v}_t\)):

   \(
   \begin{align}
   \begin{split}
   m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t\\
   \hat{m}_t & = \frac{m_t}{1 - \beta^t_1}\\
   \theta_{t+1} &= \theta_{t} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon}
   \hat{m}_t
   \end{split}
   \end{align}
   \)

   expanding the second equation with the definitions of \(\hat{m}_t\) and
   \(m_t\) in turn gives us:

   \(\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon}
   (\dfrac{\beta_1 m_{t-1}}{1 - \beta^t_1} + \dfrac{(1 - \beta_1) g_t}{1 -
   \beta^t_1})\)

   note that \(\dfrac{\beta_1 m_{t-1}}{1 - \beta^t_1}\) is just the
   bias-corrected estimate of the momentum vector of the previous time
   step. we can thus replace it with \(\hat{m}_{t-1}\):

   \(\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon}
   (\beta_1 \hat{m}_{t-1} + \dfrac{(1 - \beta_1) g_t}{1 - \beta^t_1})\)

   note that for simplicity, we ignore that the denominator is \(1 -
   \beta^t_1\) and not \(1 - \beta^{t-1}_1\) as we will replace the
   denominator in the next step anyway. this equation again looks very
   similar to our expanded momentum update rule above. we can now add
   nesterov momentum just as we did previously by simply replacing this
   bias-corrected estimate of the momentum vector of the previous time
   step \(\hat{m}_{t-1}\) with the bias-corrected estimate of the current
   momentum vector \(\hat{m}_t\), which gives us the nadam update rule:

   \(\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon}
   (\beta_1 \hat{m}_t + \dfrac{(1 - \beta_1) g_t}{1 - \beta^t_1})\)

amsgrad

   as adaptive learning rate methods have become the norm in training
   neural networks, practitioners noticed that in some cases, e.g. for
   object recognition ^[77][17] or machine translation ^[78][18] they fail
   to converge to an optimal solution and are outperformed by sgd with
   momentum.

   reddi et al. (2018) ^[79][19] formalize this issue and pinpoint the
   exponential moving average of past squared gradients as a reason for
   the poor generalization behaviour of adaptive learning rate methods.
   recall that the introduction of the exponential average was
   well-motivated: it should prevent the learning rates to become
   infinitesimally small as training progresses, the key flaw of the
   adagrad algorithm. however, this short-term memory of the gradients
   becomes an obstacle in other scenarios.

   in settings where adam converges to a suboptimal solution, it has been
   observed that some minibatches provide large and informative gradients,
   but as these minibatches only occur rarely, exponential averaging
   diminishes their influence, which leads to poor convergence. the
   authors provide an example for a simple id76 problem
   where the same behaviour can be observed for adam.

   to fix this behaviour, the authors propose a new algorithm, amsgrad
   that uses the maximum of past squared gradients \(v_t\) rather than the
   exponential average to update the parameters. \(v_t\) is defined the
   same as in adam above:

   \(
   v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
   \)

   instead of using \(v_t\) (or its bias-corrected version \(\hat{v}_t\))
   directly, we now employ the previous \(v_{t-1}\) if it is larger than
   the current one:

   \(
   \hat{v}_t = \text{max}(\hat{v}_{t-1}, v_t)
   \)

   this way, amsgrad results in a non-increasing step size, which avoids
   the problems suffered by adam. for simplicity, the authors also remove
   the debiasing step that we have seen in adam. the full amsgrad update
   without bias-corrected estimates can be seen below:

   \(
   \begin{align}
   \begin{split}
   m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
   v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\\
   \hat{v}_t &= \text{max}(\hat{v}_{t-1}, v_t) \\
   \theta_{t+1} &= \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon}
   m_t
   \end{split}
   \end{align}
   \)

   the authors observe improved performance compared to adam on small
   datasets and on cifar-10. [80]other experiments, however, show similar
   or worse performance than adam. it remains to be seen whether amsgrad
   is able to consistently outperform adam in practice. for more
   information about recent advances in deep learning optimization, refer
   to [81]this blog post.

visualization of algorithms

   the following two animations (image credit: [82]alec radford) provide
   some intuitions towards the optimization behaviour of most of the
   presented optimization methods. also have a look [83]here for a
   description of the same images by karpathy and another concise overview
   of the algorithms discussed.

   in image 5, we see their behaviour on the contours of a loss surface
   ([84]the beale function) over time. note that adagrad, adadelta, and
   rmsprop almost immediately head off in the right direction and converge
   similarly fast, while momentum and nag are led off-track, evoking the
   image of a ball rolling down the hill. nag, however, is quickly able to
   correct its course due to its increased responsiveness by looking ahead
   and heads to the minimum.

   image 6 shows the behaviour of the algorithms at a saddle point, i.e. a
   point where one dimension has a positive slope, while the other
   dimension has a negative slope, which pose a difficulty for sgd as we
   mentioned before. notice here that sgd, momentum, and nag find it
   difficulty to break symmetry, although the two latter eventually manage
   to escape the saddle point, while adagrad, rmsprop, and adadelta
   quickly head down the negative slope.
   sgd without momentum image 5: sgd optimization on loss surface contours
   sgd with momentum image 6: sgd optimization on saddle point

   as we can see, the adaptive learning-rate methods, i.e. adagrad,
   adadelta, rmsprop, and adam are most suitable and provide the best
   convergence for these scenarios.

   note: if you are interested in visualizing these or other optimization
   algorithms, refer to [85]this useful tutorial.

which optimizer to use?

   so, which optimizer should you now use? if your input data is sparse,
   then you likely achieve the best results using one of the adaptive
   learning-rate methods. an additional benefit is that you won't need to
   tune the learning rate but likely achieve the best results with the
   default value.

   in summary, rmsprop is an extension of adagrad that deals with its
   radically diminishing learning rates. it is identical to adadelta,
   except that adadelta uses the rms of parameter updates in the numinator
   update rule. adam, finally, adds bias-correction and momentum to
   rmsprop. insofar, rmsprop, adadelta, and adam are very similar
   algorithms that do well in similar circumstances. kingma et al.
   ^[86][14:1] show that its bias-correction helps adam slightly
   outperform rmsprop towards the end of optimization as gradients become
   sparser. insofar, adam might be the best overall choice.

   interestingly, many recent papers use vanilla sgd without momentum and
   a simple learning rate annealing schedule. as has been shown, sgd
   usually achieves to find a minimum, but it might take significantly
   longer than with some of the optimizers, is much more reliant on a
   robust initialization and annealing schedule, and may get stuck in
   saddle points rather than local minima. consequently, if you care about
   fast convergence and train a deep or complex neural network, you should
   choose one of the adaptive learning rate methods.

parallelizing and distributing sgd

   given the ubiquity of large-scale data solutions and the availability
   of low-commodity clusters, distributing sgd to speed it up further is
   an obvious choice.
   sgd by itself is inherently sequential: step-by-step, we progress
   further towards the minimum. running it provides good convergence but
   can be slow particularly on large datasets. in contrast, running sgd
   asynchronously is faster, but suboptimal communication between workers
   can lead to poor convergence. additionally, we can also parallelize sgd
   on one machine without the need for a large computing cluster. the
   following are algorithms and architectures that have been proposed to
   optimize parallelized and distributed sgd.

hogwild!

   niu et al. ^[87][20] introduce an update scheme called hogwild! that
   allows performing sgd updates in parallel on cpus. processors are
   allowed to access shared memory without locking the parameters. this
   only works if the input data is sparse, as each update will only modify
   a fraction of all parameters. they show that in this case, the update
   scheme achieves almost an optimal rate of convergence, as it is
   unlikely that processors will overwrite useful information.

downpour sgd

   downpour sgd is an asynchronous variant of sgd that was used by dean et
   al. ^[88][10:1] in their distbelief framework (predecessor to
   tensorflow) at google. it runs multiple replicas of a model in parallel
   on subsets of the training data. these models send their updates to a
   parameter server, which is split across many machines. each machine is
   responsible for storing and updating a fraction of the model's
   parameters. however, as replicas don't communicate with each other e.g.
   by sharing weights or updates, their parameters are continuously at
   risk of diverging, hindering convergence.

delay-tolerant algorithms for sgd

   mcmahan and streeter ^[89][21] extend adagrad to the parallel setting
   by developing delay-tolerant algorithms that not only adapt to past
   gradients, but also to the update delays. this has been shown to work
   well in practice.

tensorflow

   [90]tensorflow ^[91][22] is google's recently open-sourced framework
   for the implementation and deployment of large-scale machine learning
   models. it is based on their experience with distbelief and is already
   used internally to perform computations on a large range of mobile
   devices as well as on large-scale distributed systems. for distributed
   execution, a computation graph is split into a subgraph for every
   device and communication takes place using send/receive node pairs.
   however, the open source version of tensorflow currently does not
   support distributed functionality (see [92]here).
   update 13.04.16: a distributed version of tensorflow has [93]been
   released.

elastic averaging sgd

   zhang et al. ^[94][23] propose elastic averaging sgd (easgd), which
   links the parameters of the workers of asynchronous sgd with an elastic
   force, i.e. a center variable stored by the parameter server. this
   allows the local variables to fluctuate further from the center
   variable, which in theory allows for more exploration of the parameter
   space. they show empirically that this increased capacity for
   exploration leads to improved performance by finding new local optima.

additional strategies for optimizing sgd

   finally, we introduce additional strategies that can be used alongside
   any of the previously mentioned algorithms to further improve the
   performance of sgd. for a great overview of some other common tricks,
   refer to ^[95][24].

shuffling and curriculum learning

   generally, we want to avoid providing the training examples in a
   meaningful order to our model as this may bias the optimization
   algorithm. consequently, it is often a good idea to shuffle the
   training data after every epoch.

   on the other hand, for some cases where we aim to solve progressively
   harder problems, supplying the training examples in a meaningful order
   may actually lead to improved performance and better convergence. the
   method for establishing this meaningful order is called curriculum
   learning ^[96][25].

   zaremba and sutskever ^[97][26] were only able to train lstms to
   evaluate simple programs using curriculum learning and show that a
   combined or mixed strategy is better than the naive one, which sorts
   examples by increasing difficulty.

batch id172

   to facilitate learning, we typically normalize the initial values of
   our parameters by initializing them with zero mean and unit variance.
   as training progresses and we update parameters to different extents,
   we lose this id172, which slows down training and amplifies
   changes as the network becomes deeper.

   batch id172 ^[98][27] reestablishes these id172s for
   every mini-batch and changes are back-propagated through the operation
   as well. by making id172 part of the model architecture, we are
   able to use higher learning rates and pay less attention to the
   initialization parameters. batch id172 additionally acts as a
   regularizer, reducing (and sometimes even eliminating) the need for
   dropout.

early stopping

   according to geoff hinton: "early stopping (is) beautiful free lunch"
   ([99]nips 2015 tutorial slides, slide 63). you should thus always
   monitor error on a validation set during training and stop (with some
   patience) if your validation error does not improve enough.

gradient noise

   neelakantan et al. ^[100][28] add noise that follows a gaussian
   distribution \(n(0, \sigma^2_t)\) to each gradient update:

   \(g_{t, i} = g_{t, i} + n(0, \sigma^2_t)\).

   they anneal the variance according to the following schedule:

   \( \sigma^2_t = \dfrac{\eta}{(1 + t)^\gamma} \).

   they show that adding this noise makes networks more robust to poor
   initialization and helps training particularly deep and complex
   networks. they suspect that the added noise gives the model more
   chances to escape and find new local minima, which are more frequent
   for deeper models.

conclusion

   in this blog post, we have initially looked at the three variants of
   id119, among which mini-batch id119 is the most
   popular. we have then investigated algorithms that are most commonly
   used for optimizing sgd: momentum, nesterov accelerated gradient,
   adagrad, adadelta, rmsprop, adam, as well as different algorithms to
   optimize asynchronous sgd. finally, we've considered other strategies
   to improve sgd such as shuffling and curriculum learning, batch
   id172, and early stopping.

   i hope that this blog post was able to provide you with some intuitions
   towards the motivation and the behaviour of the different optimization
   algorithms. are there any obvious algorithms to improve sgd that i've
   missed? what tricks are you using yourself to facilitate training with
   sgd? let me know in the comments below.

acknowledgements

   thanks to [101]denny britz and [102]cesar salgado for reading drafts of
   this post and providing suggestions.

printable version and citation

   this blog post is also available as an [103]article on arxiv, in case
   you want to refer to it later.

   in case you found it helpful, consider citing the corresponding arxiv
   article as:
   sebastian ruder (2016). an overview of id119 optimisation
   algorithms. arxiv preprint arxiv:1609.04747.

translations

   this blog post has been translated into the following languages:
     * [104]japanese
     * [105]chinese
     * [106]korean

   image credit for cover photo: [107]karpathy's beautiful id168s
   tumblr
     __________________________________________________________________

    1. h. robinds and s. monro,    a stochastic approximation method,   
       annals of mathematical statistics, vol. 22, pp. 400   407, 1951.
       [108]      
    2. darken, c., chang, j., & moody, j. (1992). learning rate schedules
       for faster stochastic gradient search. neural networks for signal
       processing ii proceedings of the 1992 ieee workshop, (september),
       1   11. [109]http://doi.org/10.1109/nnsp.1992.253713 [110]      
    3. dauphin, y., pascanu, r., gulcehre, c., cho, k., ganguli, s., &
       bengio, y. (2014). identifying and attacking the saddle point
       problem in high-dimensional non-id76. arxiv, 1   14.
       retrieved from [111]http://arxiv.org/abs/1406.2572 [112]      
    4. sutton, r. s. (1986). two problems with id26 and other
       steepest-descent learning procedures for networks. proc. 8th annual
       conf. cognitive science society. [113]      
    5. qian, n. (1999). on the momentum term in id119 learning
       algorithms. neural networks : the official journal of the
       international neural network society, 12(1), 145   151.
       [114]http://doi.org/10.1016/s0893-6080(98)00116-6 [115]      
    6. nesterov, y. (1983). a method for unconstrained convex minimization
       problem with the rate of convergence o(1/k2). doklady ansssr
       (translated as soviet.math.docl.), vol. 269, pp. 543    547. [116]      
    7. bengio, y., boulanger-lewandowski, n., & pascanu, r. (2012).
       advances in optimizing recurrent networks. retrieved from
       [117]http://arxiv.org/abs/1212.0901 [118]      
    8. sutskever, i. (2013). training recurrent neural networks. phd
       thesis. [119]      
    9. duchi, j., hazan, e., & singer, y. (2011). adaptive subgradient
       methods for online learning and stochastic optimization. journal of
       machine learning research, 12, 2121   2159. retrieved from
       [120]http://jmlr.org/papers/v12/duchi11a.html [121]      
   10. dean, j., corrado, g. s., monga, r., chen, k., devin, m., le, q. v,
           ng, a. y. (2012). large scale distributed deep networks. nips
       2012: neural information processing systems, 1   11.
       [122]http://papers.nips.cc/paper/4687-large-scale-distributed-deep-
       networks.pdf [123]       [124]      
   11. pennington, j., socher, r., & manning, c. d. (2014). glove: global
       vectors for word representation. proceedings of the 2014 conference
       on empirical methods in natural language processing, 1532   1543.
       [125]http://doi.org/10.3115/v1/d14-1162 [126]      
   12. duchi et al. [3] give this matrix as an alternative to the full
       matrix containing the outer products of all previous gradients, as
       the computation of the matrix square root is infeasible even for a
       moderate number of parameters \(d\). [127]      
   13. zeiler, m. d. (2012). adadelta: an adaptive learning rate method.
       retrieved from [128]http://arxiv.org/abs/1212.5701 [129]      
   14. kingma, d. p., & ba, j. l. (2015). adam: a method for stochastic
       optimization. international conference on learning representations,
       1   13. [130]       [131]      
   15. heusel, m., ramsauer, h., unterthiner, t., nessler, b., &
       hochreiter, s. (2017). gans trained by a two time-scale update rule
       converge to a local nash equilibrium. in advances in neural
       information processing systems 30 (nips 2017). [132]      
   16. dozat, t. (2016). incorporating nesterov momentum into adam. iclr
       workshop, (1), 2013   2016. [133]      
   17. huang, g., liu, z., weinberger, k. q., & van der maaten, l. (2017).
       densely connected convolutional networks. in proceedings of cvpr
       2017. [134]      
   18. johnson, m., schuster, m., le, q. v, krikun, m., wu, y., chen, z.,
           dean, j. (2016). google   s multilingual id4
       system: enabling zero-shot translation. arxiv preprint
       arxiv:1611.0455. [135]      
   19. reddi, sashank j., kale, satyen, & kumar, sanjiv. on the
       convergence of adam and beyond. proceedings of iclr 2018. [136]      
   20. niu, f., recht, b., christopher, r., & wright, s. j. (2011).
       hogwild! : a lock-free approach to parallelizing stochastic
       id119, 1   22. [137]      
   21. mcmahan, h. b., & streeter, m. (2014). delay-tolerant algorithms
       for asynchronous distributed online learning. advances in neural
       information processing systems (proceedings of nips), 1   9.
       retrieved from
       [138]http://papers.nips.cc/paper/5242-delay-tolerant-algorithms-for
       -asynchronous-distributed-online-learning.pdf [139]      
   22. abadi, m., agarwal, a., barham, p., brevdo, e., chen, z., citro,
       c.,     zheng, x. (2015). tensorflow : large-scale machine learning
       on heterogeneous distributed systems. [140]      
   23. zhang, s., choromanska, a., & lecun, y. (2015). deep learning with
       elastic averaging sgd. neural information processing systems
       conference (nips 2015), 1   24. retrieved from
       [141]http://arxiv.org/abs/1412.6651 [142]      
   24. lecun, y., bottou, l., orr, g. b., & m  ller, k. r. (1998).
       efficient backprop. neural networks: tricks of the trade, 1524,
       9   50. [143]http://doi.org/10.1007/3-540-49430-8_2 [144]      
   25. bengio, y., louradour, j., collobert, r., & weston, j. (2009).
       curriculum learning. proceedings of the 26th annual international
       conference on machine learning, 41   48.
       [145]http://doi.org/10.1145/1553374.1553380 [146]      
   26. zaremba, w., & sutskever, i. (2014). learning to execute, 1   25.
       retrieved from [147]http://arxiv.org/abs/1410.4615 [148]      
   27. ioffe, s., & szegedy, c. (2015). batch id172 : accelerating
       deep network training by reducing internal covariate shift. arxiv
       preprint arxiv:1502.03167v3. [149]      
   28. neelakantan, a., vilnis, l., le, q. v., sutskever, i., kaiser, l.,
       kurach, k., & martens, j. (2015). adding gradient noise improves
       learning for very deep networks, 1   11. retrieved from
       [150]http://arxiv.org/abs/1511.06807 [151]      

   sebastian ruder

[152]sebastian ruder

   read [153]more posts by this author.
   [154]read more

       sebastian ruder    

[155]optimization

     * [156]optimization for deep learning highlights in 2017

   [157]1 post    

   [158]on id27s - part 1

   id27s

on id27s - part 1

   id27s popularized by id97 are pervasive in current nlp
   applications. the history of id27s, however, goes back a lot
   further. this post explores the history of id27s in the
   context of language modelling.

     * sebastian ruder
       [159]sebastian ruder

   [160]sebastian ruder
      
   an overview of id119 optimization algorithms
   share this
   please enable javascript to view the [161]comments powered by disqus.

   [162]sebastian ruder    2019

   [163]latest posts [164]twitter [165]ghost

references

   visible links
   1. http://ruder.io/rss/
   2. http://ruder.io/
   3. http://ruder.io/about/
   4. http://ruder.io/tags/
   5. http://ruder.io/publications/
   6. http://ruder.io/talks/
   7. http://ruder.io/news/
   8. http://ruder.io/faq/
   9. http://ruder.io/nlp-news/
  10. https://nlpprogress.com/
  11. http://ruder.io/contact/
  12. http://ruder.io/tag/optimization/index.html
  13. http://arxiv.org/abs/1609.04747
  14. http://ruder.io/optimizing-gradient-descent/index.html#amsgrad
  15. https://www.slideshare.net/sebastianruder/optimization-for-deep-learning
  16. http://ruder.io/optimizing-gradient-descent/index.html#adamax
  17. http://ruder.io/optimizing-gradient-descent/index.html#nadam
  18. https://news.ycombinator.com/item?id=11943685
  19. http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants
  20. http://ruder.io/optimizing-gradient-descent/index.html#batchgradientdescent
  21. http://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent
  22. http://ruder.io/optimizing-gradient-descent/index.html#minibatchgradientdescent
  23. http://ruder.io/optimizing-gradient-descent/index.html#challenges
  24. http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms
  25. http://ruder.io/optimizing-gradient-descent/index.html#momentum
  26. http://ruder.io/optimizing-gradient-descent/index.html#nesterovacceleratedgradient
  27. http://ruder.io/optimizing-gradient-descent/index.html#adagrad
  28. http://ruder.io/optimizing-gradient-descent/index.html#adadelta
  29. http://ruder.io/optimizing-gradient-descent/index.html#rmsprop
  30. http://ruder.io/optimizing-gradient-descent/index.html#adam
  31. http://ruder.io/optimizing-gradient-descent/index.html#adamax
  32. http://ruder.io/optimizing-gradient-descent/index.html#nadam
  33. http://ruder.io/optimizing-gradient-descent/index.html#amsgrad
  34. http://ruder.io/optimizing-gradient-descent/index.html#visualizationofalgorithms
  35. http://ruder.io/optimizing-gradient-descent/index.html#whichoptimizertochoose
  36. http://ruder.io/optimizing-gradient-descent/index.html#parallelizinganddistributingsgd
  37. http://ruder.io/optimizing-gradient-descent/index.html#hogwild
  38. http://ruder.io/optimizing-gradient-descent/index.html#downpoursgd
  39. http://ruder.io/optimizing-gradient-descent/index.html#delaytolerantalgorithmsforsgd
  40. http://ruder.io/optimizing-gradient-descent/index.html#tensorflow
  41. http://ruder.io/optimizing-gradient-descent/index.html#elasticaveragingsgd
  42. http://ruder.io/optimizing-gradient-descent/index.html#additionalstrategiesforoptimizingsgd
  43. http://ruder.io/optimizing-gradient-descent/index.html#shufflingandcurriculuid113arning
  44. http://ruder.io/optimizing-gradient-descent/index.html#batchid172
  45. http://ruder.io/optimizing-gradient-descent/index.html#earlystopping
  46. http://ruder.io/optimizing-gradient-descent/index.html#gradientnoise
  47. http://ruder.io/optimizing-gradient-descent/index.html#conclusion
  48. http://ruder.io/optimizing-gradient-descent/index.html#references
  49. http://lasagne.readthedocs.org/en/latest/modules/updates.html
  50. http://caffe.berkeleyvision.org/tutorial/solver.html
  51. http://keras.io/optimizers/
  52. http://cs231n.github.io/optimization-1/
  53. http://cs231n.github.io/neural-networks-3/
  54. https://upload.wikimedia.org/wikipedia/commons/f/f3/stogra.png
  55. http://ruder.io/optimizing-gradient-descent/index.html#shufflingandcurriculuid113arning
  56. http://ruder.io/optimizing-gradient-descent/index.html#fn1
  57. http://ruder.io/optimizing-gradient-descent/index.html#fn2
  58. http://ruder.io/optimizing-gradient-descent/index.html#fn3
  59. https://en.wikipedia.org/wiki/newton's_method_in_optimization
  60. http://ruder.io/optimizing-gradient-descent/index.html#fn4
  61. http://ruder.io/optimizing-gradient-descent/index.html#fn5
  62. http://ruder.io/optimizing-gradient-descent/index.html#fn6
  63. http://ruder.io/optimizing-gradient-descent/index.html#fn7
  64. http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
  65. http://cs231n.github.io/neural-networks-3/
  66. http://ruder.io/optimizing-gradient-descent/index.html#fn8
  67. http://ruder.io/optimizing-gradient-descent/index.html#fn9
  68. http://ruder.io/optimizing-gradient-descent/index.html#fn10
  69. http://www.wired.com/2012/06/google-x-neural-network/
  70. http://ruder.io/optimizing-gradient-descent/index.html#fn11
  71. http://ruder.io/optimizing-gradient-descent/index.html#fn12
  72. http://ruder.io/optimizing-gradient-descent/index.html#fn13
  73. http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
  74. http://ruder.io/optimizing-gradient-descent/index.html#fn14
  75. http://ruder.io/optimizing-gradient-descent/index.html#fn15
  76. http://ruder.io/optimizing-gradient-descent/index.html#fn16
  77. http://ruder.io/optimizing-gradient-descent/index.html#fn17
  78. http://ruder.io/optimizing-gradient-descent/index.html#fn18
  79. http://ruder.io/optimizing-gradient-descent/index.html#fn19
  80. https://fdlm.github.io/post/amsgrad/
  81. http://ruder.io/deep-learning-optimization-2017/
  82. https://twitter.com/alecrad
  83. http://cs231n.github.io/neural-networks-3/
  84. https://www.sfu.ca/~ssurjano/beale.html
  85. http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/
  86. http://ruder.io/optimizing-gradient-descent/index.html#fn14
  87. http://ruder.io/optimizing-gradient-descent/index.html#fn20
  88. http://ruder.io/optimizing-gradient-descent/index.html#fn10
  89. http://ruder.io/optimizing-gradient-descent/index.html#fn21
  90. https://www.tensorflow.org/
  91. http://ruder.io/optimizing-gradient-descent/index.html#fn22
  92. https://github.com/tensorflow/tensorflow/issues/23
  93. http://googleresearch.blogspot.ie/2016/04/announcing-tensorflow-08-now-with.html
  94. http://ruder.io/optimizing-gradient-descent/index.html#fn23
  95. http://ruder.io/optimizing-gradient-descent/index.html#fn24
  96. http://ruder.io/optimizing-gradient-descent/index.html#fn25
  97. http://ruder.io/optimizing-gradient-descent/index.html#fn26
  98. http://ruder.io/optimizing-gradient-descent/index.html#fn27
  99. http://www.iro.umontreal.ca/~bengioy/talks/dl-tutorial-nips2015.pdf
 100. http://ruder.io/optimizing-gradient-descent/index.html#fn28
 101. https://twitter.com/dennybritz
 102. https://twitter.com/cesarsvs
 103. http://arxiv.org/abs/1609.04747
 104. http://postd.cc/optimizing-gradient-descent/
 105. http://blog.csdn.net/google19890102/article/details/69942970
 106. https://brunch.co.kr/@chris-song/50
 107. http://lossfunctions.tumblr.com/
 108. http://ruder.io/optimizing-gradient-descent/index.html#fnref1
 109. http://doi.org/10.1109/nnsp.1992.253713
 110. http://ruder.io/optimizing-gradient-descent/index.html#fnref2
 111. http://arxiv.org/abs/1406.2572
 112. http://ruder.io/optimizing-gradient-descent/index.html#fnref3
 113. http://ruder.io/optimizing-gradient-descent/index.html#fnref4
 114. http://doi.org/10.1016/s0893-6080(98)00116-6
 115. http://ruder.io/optimizing-gradient-descent/index.html#fnref5
 116. http://ruder.io/optimizing-gradient-descent/index.html#fnref6
 117. http://arxiv.org/abs/1212.0901
 118. http://ruder.io/optimizing-gradient-descent/index.html#fnref7
 119. http://ruder.io/optimizing-gradient-descent/index.html#fnref8
 120. http://jmlr.org/papers/v12/duchi11a.html
 121. http://ruder.io/optimizing-gradient-descent/index.html#fnref9
 122. http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf
 123. http://ruder.io/optimizing-gradient-descent/index.html#fnref10
 124. http://ruder.io/optimizing-gradient-descent/index.html#fnref10:1
 125. http://doi.org/10.3115/v1/d14-1162
 126. http://ruder.io/optimizing-gradient-descent/index.html#fnref11
 127. http://ruder.io/optimizing-gradient-descent/index.html#fnref12
 128. http://arxiv.org/abs/1212.5701
 129. http://ruder.io/optimizing-gradient-descent/index.html#fnref13
 130. http://ruder.io/optimizing-gradient-descent/index.html#fnref14
 131. http://ruder.io/optimizing-gradient-descent/index.html#fnref14:1
 132. http://ruder.io/optimizing-gradient-descent/index.html#fnref15
 133. http://ruder.io/optimizing-gradient-descent/index.html#fnref16
 134. http://ruder.io/optimizing-gradient-descent/index.html#fnref17
 135. http://ruder.io/optimizing-gradient-descent/index.html#fnref18
 136. http://ruder.io/optimizing-gradient-descent/index.html#fnref19
 137. http://ruder.io/optimizing-gradient-descent/index.html#fnref20
 138. http://papers.nips.cc/paper/5242-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning.pdf
 139. http://ruder.io/optimizing-gradient-descent/index.html#fnref21
 140. http://ruder.io/optimizing-gradient-descent/index.html#fnref22
 141. http://arxiv.org/abs/1412.6651
 142. http://ruder.io/optimizing-gradient-descent/index.html#fnref23
 143. http://doi.org/10.1007/3-540-49430-8_2
 144. http://ruder.io/optimizing-gradient-descent/index.html#fnref24
 145. http://doi.org/10.1145/1553374.1553380
 146. http://ruder.io/optimizing-gradient-descent/index.html#fnref25
 147. http://arxiv.org/abs/1410.4615
 148. http://ruder.io/optimizing-gradient-descent/index.html#fnref26
 149. http://ruder.io/optimizing-gradient-descent/index.html#fnref27
 150. http://arxiv.org/abs/1511.06807
 151. http://ruder.io/optimizing-gradient-descent/index.html#fnref28
 152. http://ruder.io/author/sebastian/index.html
 153. http://ruder.io/author/sebastian/index.html
 154. http://ruder.io/author/sebastian/index.html
 155. http://ruder.io/tag/optimization/index.html
 156. http://ruder.io/deep-learning-optimization-2017/index.html
 157. http://ruder.io/tag/optimization/index.html
 158. http://ruder.io/index.html
 159. http://ruder.io/author/sebastian/index.html
 160. http://ruder.io/
 161. https://disqus.com/?ref_noscript
 162. http://ruder.io/
 163. http://ruder.io/
 164. https://twitter.com/seb_ruder
 165. https://ghost.org/

   hidden links:
 167. https://twitter.com/seb_ruder
 168. http://ruder.io/rss/index.rss
 169. http://ruder.io/index.html
 170. https://twitter.com/share?text=an%20overview%20of%20gradient%20descent%20optimization%20algorithms&url=http://ruder.io/optimizing-gradient-descent/
 171. https://www.facebook.com/sharer/sharer.php?u=http://ruder.io/optimizing-gradient-descent/
