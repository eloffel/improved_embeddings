   #[1]sebastian ruder

   [2]sebastian ruder
     * [3]about
     * [4]tags
     * [5]papers
     * [6]talks
     * [7]news
     * [8]faq
     * [9]nlp news
     * [10]nlp progress
     * [11]contact

   11 april 2016 / [12]id27s

on id27s - part 1

   on id27s - part 1

   this post presents the most well-known models for learning word
   embeddings based on language modelling.

   table of contents:
     * [13]a brief history of id27s
     * [14]id27 models
          + [15]a note on language modelling
          + [16]classic neural language model
          + [17]c&w model
          + [18]id97
               o [19]cbow
               o [20]skip-gram

   unsupervisedly learned id27s have been exceptionally
   successful in many nlp tasks and are frequently seen as something akin
   to a silver bullet. in fact, in many nlp architectures, they have
   almost completely replaced traditional distributional features such as
   brown clusters and lsa features.

   proceedings of last year's [21]acl and [22]emnlp conferences have been
   dominated by id27s, with some people musing that embedding
   methods in natural language processing was a more fitting name for
   emnlp. this year's acl features not [23]one but [24]two workshops on
   id27s.

   semantic relations between id27s seem nothing short of
   magical to the uninitiated and deep learning nlp talks frequently
   prelude with the notorious \(king - man + woman \approx queen \) slide,
   while [25]a recent article in communications of the acm hails word
   embeddings as the primary reason for nlp's breakout.

   this post will be the first in a series that aims to give an extensive
   overview of id27s showcasing why this hype may or may not be
   warranted. in the course of this review, we will try to connect the
   disperse literature on id27 models, highlighting many models,
   applications and interesting features of id27s, with a focus
   on multilingual embedding models and id27 evaluation tasks in
   later posts.
   this first post lays the foundations by presenting current word
   embeddings based on language modelling. while many of these models have
   been discussed at length, we hope that investigating and discussing
   their merits in the context of past and current research will provide
   new insights.

   a brief note on nomenclature: in the following we will use the
   currently prevalent term id27s to refer to dense
   representations of words in a low-dimensional vector space.
   interchangeable terms are word vectors and distributed representations.
   we will particularly focus on neural id27s, i.e. word
   embeddings learned by a neural network.

a brief history of id27s

   since the 1990s, vector space models have been used in distributional
   semantics. during this time, many models for estimating continuous
   representations of words have been developed, including latent semantic
   analysis (lsa) and id44 (lda). have a look at
   [26]this blog post for a more detailed overview of distributional
   semantics history in the context of id27s.

   bengio et al. coin the term id27s in 2003 and train them in a
   neural language model jointly with the model's parameters. first to
   show the utility of pre-trained id27s were arguably collobert
   and weston in 2008. their landmark paper a unified architecture for
   natural language processing not only establishes id27s as a
   useful tool for downstream tasks, but also introduces a neural network
   architecture that forms the foundation for many current approaches.
   however, the eventual popularization of id27s can be
   attributed to mikolov et al. in 2013 who created id97, a toolkit
   that allows the seaid113ss training and use of pre-trained embeddings. in
   2014, pennington et al. released glove, a competitive set of
   pre-trained id27s, signalling that id27s had
   reached the main stream.

   id27s are one of the few currently successful applications of
   unsupervised learning. their main benefit arguably is that they don't
   require expensive annotation, but can be derived from large unannotated
   corpora that are readily available. pre-trained embeddings can then be
   used in downstream tasks that use small amounts of labeled data.

id27 models

   naturally, every feed-forward neural network that takes words from a
   vocabulary as input and embeds them as vectors into a lower dimensional
   space, which it then fine-tunes through back-propagation, necessarily
   yields id27s as the weights of the first layer, which is
   usually referred to as embedding layer.

   the main difference between such a network that produces word
   embeddings as a by-product and a method such as id97 whose explicit
   goal is the generation of id27s is its computational
   complexity. generating id27s with a very deep architecture is
   simply too computationally expensive for a large vocabulary. this is
   the main reason why it took until 2013 for id27s to explode
   onto the nlp stage; computational complexity is a key trade-off for
   id27 models and will be a recurring theme in our review.

   another difference is the training objective: id97 and glove are
   geared towards producing id27s that encode general semantic
   relationships, which are beneficial to many downstream tasks; notably,
   id27s trained this way won't be helpful in tasks that do not
   rely on these kind of relationships. in contrast, regular neural
   networks typically produce task-specific embeddings that are only of
   limited use elsewhere. note that a task that relies on semantically
   coherent representations such as language modelling will produce
   similar embeddings to id27 models, which we will investigate
   in the next chapter.
   as a side-note, id97 and glove might be said to be to nlp what
   [27]vggnet is to vision, i.e. a common weight initialisation that
   provides generally helpful features without the need for lengthy
   training.

   to facilitate comparison between models, we assume the following
   notational standards: we assume a training corpus containing a sequence
   of \(t\) training words \(w_1, w_2, w_3, \cdots, w_t\) that belong to a
   vocabulary \(v\) whose size is \(|v|\). our models generally consider a
   context of \( n \) words. we associate every word with an input
   embedding \( v_w \) (the eponymous id27 in the embedding
   layer) with \(d\) dimensions and an output embedding \( v'_w \)
   (another word representation whose role will soon become clearer). we
   finally optimize an objective function \(j_\theta\) with regard to our
   model parameters \(\theta\) and our model outputs some score
   \(f_\theta(x)\) for every input \( x \).

a note on language modelling

   id27 models are quite closely intertwined with language
   models. the quality of language models is measured based on their
   ability to learn a id203 distribution over words in \( v \). in
   fact, many state-of-the-art id27 models try to predict the
   next word in a sequence to some extent. additionally, id27
   models are often evaluated using [28]perplexity, a cross-id178 based
   measure borrowed from language modelling.

   before we get into the gritty details of id27 models, let us
   briefly talk about some language modelling fundamentals.

   language models generally try to compute the id203 of a word
   \(w_t\) given its \(n - 1\) previous words, i.e. \(p(w_t : | : w_{t-1}
   , \cdots w_{t-n+1})\). by applying the chain rule together with the
   markov assumption, we can approximate the id203 of a whole
   sentence or document by the product of the probabilities of each word
   given its \(n\) previous words:

   \(p(w_1 , \cdots , w_t) = \prod\limits_i p(w_i : | : w_{i-1} , \cdots ,
   w_{i-n+1}) \).

   in id165 based language models, we can calculate a word's id203
   based on the frequencies of its constituent id165s:
   \( p(w_t : | : w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{count(w_{t-n+1},
   \cdots , w_{t-1},w_t)}{count({w_{t-n+1}, \cdots , w_{t-1}})}\).

   setting \(n = 2\) yields bigram probabilities, while \(n = 5\) together
   with kneser-ney smoothing leads to smoothed 5-gram models that have
   been found to be a strong baseline for language modelling. for more
   details, you can refer to [29]these slides from stanford.

   in neural networks, we achieve the same objective using the well-known
   softmax layer:

   \(p(w_t : | : w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{\text{exp}({h^\top
   v'_{w_t}})}{\sum_{w_i \in v} \text{exp}({h^\top v'_{w_i}})} \).

   the inner product \( h^\top v'_{w_t} \) computes the (unnormalized)
   log-id203 of word \( w_t \), which we normalize by the sum of the
   log-probabilities of all words in \( v \). \(h\) is the output vector
   of the penultimate network layer (the hidden layer in the feed-forward
   network in figure 1), while \(v'_w\) is the output embedding of word
   \(w \), i.e. its representation in the weight matrix of the softmax
   layer. note that even though \(v'_w\) represents the word \(w\), it is
   learned separately from the input id27 \(v_w\), as the
   multiplications both vectors are involved in differ (\(v_w\) is
   multiplied with an index vector, \(v'_w\) with \(h\)).
   sgd without momentum figure 1: a neural language model (bengio et al.,
   2006)

   note that we need to calculate the id203 of every word \( w \) at
   the output layer of the neural network. to do this efficiently, we
   perform a id127 between \(h\) and a weight matrix whose
   rows consist of \(v'_w\) of all words \(w\) in \(v\). we then feed the
   resulting vector, which is often referred to as a logit, i.e. the
   output of a previous layer that is not a id203, with \(d = |v|\)
   into the softmax, while the softmax layer "squashes" the vector to a
   id203 distribution over the words in \(v\).

   note that the softmax layer (in contrast to the previous id165
   calculations) only implicitly takes into account \(n\) previous words:
   lstms, which are typically used for neural language models, encode
   these in their state \(h\), while bengio's neural language model, which
   we will see in the next chapter, feeds the previous \(n\) words through
   a feed-forward layer.

   keep this softmax layer in mind, as many of the subsequent word
   embedding models will use it in some fashion.

   using this softmax layer, the model tries to maximize the id203
   of predicting the correct word at every timestep \( t \). the whole
   model thus tries to maximize the averaged log id203 of the whole
   corpus:

   \(j_\theta = \frac{1}{t} \text{log} \space p(w_1 , \cdots , w_t)\).

   analogously, through application of the chain rule, it is usually
   trained to maximize the average of the log probabilities of all words
   in the corpus given their previous \( n \) words:

   \(j_\theta = \frac{1}{t}\sum\limits_{t=1}^t\ \text{log} \space p(w_t :
   | : w_{t-1} , \cdots , w_{t-n+1})\).

   to sample words from the language model at test time, we can either
   greedily choose the word with the highest id203 \(p(w_t : | :
   w_{t-1} \cdots w_{t-n+1})\) at every time step \( t \) or use beam
   search. we can do this for instance to generate arbitrary text
   sequences as in [30]karpathy's char-id56 or as part of a sequence
   prediction task, where an lstm is used as the decoder.

classic neural language model

   the classic neural language model proposed by bengio et al. ^[31][1] in
   2003 consists of a one-hidden layer feed-forward neural network that
   predicts the next word in a sequence as in figure 2.
   language model by bengio et al. figure 2: classic neural language model
   (bengio et al., 2003)

   their model maximizes what we've described above as the prototypical
   neural language model objective (we omit the id173 term for
   simplicity):

   \(j_\theta = \frac{1}{t}\sum\limits_{t=1}^t\ \text{log} \space f(w_t ,
   w_{t-1} , \cdots , w_{t-n+1})\).

   \( f(w_t , w_{t-1} , \cdots , w_{t-n+1}) \) is the output of the model,
   i.e. the id203 \( p(w_t : | : w_{t-1} , \cdots , w_{t-n+1}) \) as
   computed by the softmax, where \(n \) is the number of previous words
   fed into the model.

   bengio et al. are one of the first to introduce what we now refer to as
   a id27, a real-valued word feature vector in \(\mathbb{r}\).
   their architecture forms very much the prototype upon which current
   approaches have gradually improved. the general building blocks of
   their model, however, are still found in all current neural language
   and id27 models. these are:
    1. embedding layer: a layer that generates id27s by
       multiplying an index vector with a id27 matrix;
    2. intermediate layer(s): one or more layers that produce an
       intermediate representation of the input, e.g. a fully-connected
       layer that applies a non-linearity to the concatenation of word
       embeddings of \(n\) previous words;
    3. softmax layer: the final layer that produces a id203
       distribution over words in \(v\).

   additionally, bengio et al. identify two issues that lie at the heart
   of current state-of-the-art-models:
     * they remark that 2. can be replaced by an lstm, which is used by
       state-of-the-art neural language models ^[32][2] ^[33][3].
     * they identify the final softmax layer (more precisely: the
       id172 term) as the network's main bottleneck, as the cost
       of computing the softmax is proportional to the number of words in
       \(v\), which is typically on the order of hundreds of thousands or
       millions.

   finding ways to mitigate the computational cost associated with
   computing the softmax over a large vocabulary ^[34][4] is thus one of
   the key challenges both in neural language models as well as in word
   embedding models.

c&w model

   after bengio et al.'s first steps in neural language models, research
   in id27s stagnated as computing power and algorithms did not
   yet allow the training of a large vocabulary.

   collobert and weston ^[35][5] (thus c&w) showcase in 2008 that word
   embeddings trained on a sufficiently large dataset carry syntactic and
   semantic meaning and improve performance on downstream tasks. they
   elaborate upon this in their 2011 paper ^[36][6].

   their solution to avoid computing the expensive softmax is to use a
   different objective function: instead of the cross-id178 criterion of
   bengio et al., which maximizes the id203 of the next word given
   the previous words, collobert and weston train a network to output a
   higher score \(f_\theta\) for a correct word sequence (a probable word
   sequence in bengio's model) than for an incorrect one. for this
   purpose, they use a pairwise ranking criterion, which looks like this:

   \(j_\theta\ = \sum\limits_{x \in x} \sum\limits_{w \in v} \text{max}
   \lbrace 0, 1 - f_\theta(x) + f_\theta(x^{(w)}) \rbrace \).

   they sample correct windows \(x\) containing \(n\) words from the set
   of all possible windows \(x\) in their corpus. for each window \(x\),
   they then produce a corrupted, incorrect version \(x^{(w)}\) by
   replacing \(x\)'s centre word with another word \(w\) from \(v\). by
   minimizing the objective, the model will now learn to assign a score
   for the correct window that is higher than the score for the incorrect
   window by at least a margin of \(1\). their model architecture,
   depicted in figure 3 without the ranking objective, is analogous to
   bengio et al.'s model.
   c&w model figure 3: the c&w model without ranking objective (collobert
   et al., 2011)

   the resulting language model produces embeddings that already possess
   many of the relations id27s have become known for, e.g.
   countries are clustered close together and syntactically similar words
   occupy similar locations in the vector space. while their ranking
   objective eliminates the complexity of the softmax, they keep the
   intermediate fully-connected hidden layer (2.) of bengio et al. around
   (the hardtanh layer in figure 3), which constitutes another source of
   expensive computation. partially due to this, their full model trains
   for seven weeks in total with \(|v| = 130000\).

id97

   let us now introduce arguably the most popular id27 model,
   the model that launched a thousand id27 papers: id97, the
   subject of two papers by mikolov et al. in 2013. as id27s are
   a key building block of deep learning models for nlp, id97 is often
   assumed to belong to the same group. technically however, id97 is
   not be considered to be part of deep learning, as its architecture is
   neither deep nor uses non-linearities (in contrast to bengio's model
   and the c&w model).

   in their first paper ^[37][7], mikolov et al. propose two architectures
   for learning id27s that are computationally less expensive
   than previous models. in their second paper ^[38][8], they improve upon
   these models by employing additional strategies to enhance training
   speed and accuracy.
   these architectures offer two main benefits over the c&w model and
   bengio's language model:
     * they do away with the expensive hidden layer.
     * they enable the language model to take additional context into
       account.

   as we will later show, the success of their model is not only due to
   these changes, but especially due to certain training strategies.

   in the following, we will look at both of these architectures:

continuous bag-of-words (cbow)

   while a language model is only able to look at the past words for its
   predictions, as it is evaluated on its ability to predict each next
   word in the corpus, a model that just aims to generate accurate word
   embeddings does not suffer from this restriction. mikolov et al. thus
   use both the \(n\) words before and after the target word \( w_t \) to
   predict it as depicted in figure 4. they call this continuous
   bag-of-words (cbow), as it uses continuous representations whose order
   is of no importance.
   continuous bag-of-words figure 4: continuous bag-of-words (mikolov et
   al., 2013)

   the objective function of cbow in turn is only slightly different than
   the language model one:

   \(j_\theta = \frac{1}{t}\sum\limits_{t=1}^t\ \text{log} \space p(w_t :
   | : w_{t-n} , \cdots , w_{t-1}, w_{t+1}, \cdots , w_{t+n})\).

   instead of feeding \( n \) previous words into the model, the model
   receives a window of \( n \) words around the target word \( w_t \) at
   each time step \( t \).

skip-gram

   while cbow can be seen as a precognitive language model, skip-gram
   turns the language model objective on its head: instead of using the
   surrounding words to predict the centre word as with cbow, skip-gram
   uses the centre word to predict the surrounding words as can be seen in
   figure 5.
   skip-gram figure 5: skip-gram (mikolov et al., 2013)

   the skip-gram objective thus sums the log probabilities of the
   surrounding \( n \) words to the left and to the right of the target
   word \( w_t \) to produce the following objective:

   \(j_\theta = \frac{1}{t}\sum\limits_{t=1}^t\ \sum\limits_{-n \leq j
   \leq n , \neq 0} \text{log} \space p(w_{t+j} : | : w_t)\).

   to gain a better intuition of how the skip-gram model computes \(
   p(w_{t+j} : | : w_t) \), let's recall the definition of our softmax:

   \(p(w_t : | : w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{\text{exp}({h^\top
   v'_{w_t}})}{\sum_{w_i \in v} \text{exp}({h^\top v'_{w_i}})} \).

   instead of computing the id203 of the target word \( w_t \) given
   its previous words, we calculate the id203 of the surrounding
   word \( w_{t+j} \) given \( w_t \). we can thus simply replace these
   variables in the equation:

   \(p(w_{t+j} : | : w_t ) = \dfrac{\text{exp}({h^\top
   v'_{w_{t+j}}})}{\sum_{w_i \in v} \text{exp}({h^\top v'_{w_i}})} \).

   as the skip-gram architecture does not contain a hidden layer that
   produces an intermediate state vector \(h\), \(h\) is simply the word
   embedding \(v_{w_t}\) of the input word \(w_t\). this also makes it
   clearer why we want to have different representations for input
   embeddings \(v_w\) and output embeddings \(v'_w\), as we would
   otherwise multiply the id27 by itself. replacing \(h \) with
   \(v_{w_t}\) yields:

   \(p(w_{t+j} : | : w_t ) = \dfrac{\text{exp}({v^\top_{w_t}
   v'_{w_{t+j}}})}{\sum_{w_i \in v} \text{exp}({v^\top_{w_t} v'_{w_i}})}
   \).

   note that the notation in mikolov's paper differs slightly from ours,
   as they denote the centre word with \( w_i \) and the surrounding words
   with \( w_o \). if we replace \( w_t \) with \( w_i \), \( w_{t+j} \)
   with \( w_o \), and swap the vectors in the inner product due to its
   commutativity, we arrive at the softmax notation in their paper:

   \(p(w_o|w_i) = \dfrac{\text{exp}(v'^\top_{w_o}
   v_{w_i})}{\sum^v_{w=1}\text{exp}(v'^\top_{w} v_{w_i})}\).

   in the next post, we will discuss different ways to approximate the
   expensive softmax as well as key training decisions that account for
   much of skip-gram's success. we will also introduce glove ^[39][9], a
   id27 model based on matrix factorisation and discuss the link
   between id27s and methods from id65.

   did i miss anything? let me know in the comments below.

other blog posts on id27s

   if you want to learn more about id27s, these other blog posts
   on id27s are also available:
     * [40]on id27s - part 2: approximating the softmax
     * [41]on id27s - part 3: the secret ingredients of id97
     * [42]unofficial part 4: a survey of cross-lingual embedding models
     * [43]unofficial part 5: id27s in 2017 - trends and future
       directions

translations

   this blog post has been translated into the following languages:
     * [44]chinese

   credit for the post image goes to [45]christopher olah.
     __________________________________________________________________

    1. bengio, y., ducharme, r., vincent, p., & janvin, c. (2003). a
       neural probabilistic language model. the journal of machine
       learning research, 3, 1137   1155. retrieved from
       [46]http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
       [47]      
    2. kim, y., jernite, y., sontag, d., & rush, a. m. (2016).
       character-aware neural language models. aaai. retrieved from
       [48]http://arxiv.org/abs/1508.06615 [49]      
    3. jozefowicz, r., vinyals, o., schuster, m., shazeer, n., & wu, y.
       (2016). exploring the limits of id38. retrieved from
       [50]http://arxiv.org/abs/1602.02410 [51]      
    4. chen, w., grangier, d., & auli, m. (2015). strategies for training
       large vocabulary neural language models, 12. retrieved from
       [52]http://arxiv.org/abs/1512.04906 [53]      
    5. collobert, r., & weston, j. (2008). a unified architecture for
       natural language processing. proceedings of the 25th international
       conference on machine learning - icml    08, 20(1), 160   167.
       [54]http://doi.org/10.1145/1390156.1390177 [55]      
    6. collobert, r., weston, j., bottou, l., karlen, m., kavukcuoglu, k.,
       & kuksa, p. (2011). natural language processing (almost) from
       scratch. journal of machine learning research, 12 (aug), 2493   2537.
       retrieved from [56]http://arxiv.org/abs/1103.0398 [57]      
    7. mikolov, t., corrado, g., chen, k., & dean, j. (2013). efficient
       estimation of word representations in vector space. proceedings of
       the international conference on learning representations (iclr
       2013), 1   12. [58]      
    8. mikolov, t., chen, k., corrado, g., & dean, j. (2013). distributed
       representations of words and phrases and their compositionality.
       nips, 1   9. [59]      
    9. pennington, j., socher, r., & manning, c. d. (2014). glove: global
       vectors for word representation. proceedings of the 2014 conference
       on empirical methods in natural language processing, 1532   1543.
       [60]http://doi.org/10.3115/v1/d14-1162 [61]      

   sebastian ruder

[62]sebastian ruder

   read [63]more posts by this author.
   [64]read more

       sebastian ruder    

[65]id27s

     * [66]aaai 2019 highlights: dialogue, reproducibility, and more
     * [67]emnlp 2018 highlights: inductive bias, cross-lingual learning,
       and more
     * [68]a review of the neural history of natural language processing

   [69]see all 9 posts    

   [70]on id27s - part 2: approximating the softmax

   id27s

on id27s - part 2: approximating the softmax

   the softmax layer is a core part of many current neural network
   architectures. when the number of output classes is very large, such as
   in the case of language modelling, computing the softmax becomes very
   expensive. this post explores approximations to make the computation
   more efficient.

     * sebastian ruder
       [71]sebastian ruder

   [72]an overview of id119 optimization algorithms

   optimization

an overview of id119 optimization algorithms

   id119 is the preferred way to optimize neural networks and
   many other machine learning algorithms but is often used as a black
   box. this post explores how many of the most popular gradient-based
   optimization algorithms such as momentum, adagrad, and adam actually
   work.

     * sebastian ruder
       [73]sebastian ruder

   [74]sebastian ruder
      
   on id27s - part 1
   share this
   please enable javascript to view the [75]comments powered by disqus.

   [76]sebastian ruder    2019

   [77]latest posts [78]twitter [79]ghost

references

   visible links
   1. http://ruder.io/rss/
   2. http://ruder.io/
   3. http://ruder.io/about/
   4. http://ruder.io/tags/
   5. http://ruder.io/publications/
   6. http://ruder.io/talks/
   7. http://ruder.io/news/
   8. http://ruder.io/faq/
   9. http://ruder.io/nlp-news/
  10. https://nlpprogress.com/
  11. http://ruder.io/contact/
  12. http://ruder.io/tag/word-embeddings/index.html
  13. http://ruder.io/word-embeddings-1/index.html#abriefhistoryofwordembeddings
  14. http://ruder.io/word-embeddings-1/index.html#wordembeddingmodels
  15. http://ruder.io/word-embeddings-1/index.html#anoteonlanguagemodelling
  16. http://ruder.io/word-embeddings-1/index.html#classicneurallanguagemodel
  17. http://ruder.io/word-embeddings-1/index.html#cwmodel
  18. http://ruder.io/word-embeddings-1/index.html#id97
  19. http://ruder.io/word-embeddings-1/index.html#continuousbagofwordscbow
  20. http://ruder.io/word-embeddings-1/index.html#skipgram
  21. https://aclweb.org/anthology/p/p15/
  22. https://aclweb.org/anthology/d/d15/
  23. https://sites.google.com/site/repl4nlp2016/
  24. https://sites.google.com/site/repevalacl16/
  25. http://cacm.acm.org/magazines/2016/3/198856-deep-or-shallow-nlp-is-breaking-out/fulltext
  26. https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/
  27. https://github.com/bvlc/caffe/wiki/model-zoo
  28. https://en.wikipedia.org/wiki/perplexity
  29. https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf
  30. https://github.com/karpathy/char-id56
  31. http://ruder.io/word-embeddings-1/index.html#fn1
  32. http://ruder.io/word-embeddings-1/index.html#fn2
  33. http://ruder.io/word-embeddings-1/index.html#fn3
  34. http://ruder.io/word-embeddings-1/index.html#fn4
  35. http://ruder.io/word-embeddings-1/index.html#fn5
  36. http://ruder.io/word-embeddings-1/index.html#fn6
  37. http://ruder.io/word-embeddings-1/index.html#fn7
  38. http://ruder.io/word-embeddings-1/index.html#fn8
  39. http://ruder.io/word-embeddings-1/index.html#fn9
  40. http://ruder.io/word-embeddings-softmax/index.html
  41. http://ruder.io/secret-id97/index.html
  42. http://ruder.io/cross-lingual-embeddings/index.html
  43. http://ruder.io/word-embeddings-2017/index.html
  44. http://geek.csdn.net/news/detail/111466
  45. http://colah.github.io/
  46. http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
  47. http://ruder.io/word-embeddings-1/index.html#fnref1
  48. http://arxiv.org/abs/1508.06615
  49. http://ruder.io/word-embeddings-1/index.html#fnref2
  50. http://arxiv.org/abs/1602.02410
  51. http://ruder.io/word-embeddings-1/index.html#fnref3
  52. http://arxiv.org/abs/1512.04906
  53. http://ruder.io/word-embeddings-1/index.html#fnref4
  54. http://doi.org/10.1145/1390156.1390177
  55. http://ruder.io/word-embeddings-1/index.html#fnref5
  56. http://arxiv.org/abs/1103.0398
  57. http://ruder.io/word-embeddings-1/index.html#fnref6
  58. http://ruder.io/word-embeddings-1/index.html#fnref7
  59. http://ruder.io/word-embeddings-1/index.html#fnref8
  60. http://doi.org/10.3115/v1/d14-1162
  61. http://ruder.io/word-embeddings-1/index.html#fnref9
  62. http://ruder.io/author/sebastian/index.html
  63. http://ruder.io/author/sebastian/index.html
  64. http://ruder.io/author/sebastian/index.html
  65. http://ruder.io/tag/word-embeddings/index.html
  66. http://ruder.io/aaai-2019-highlights/index.html
  67. http://ruder.io/emnlp-2018-highlights/index.html
  68. http://ruder.io/a-review-of-the-recent-history-of-nlp/index.html
  69. http://ruder.io/tag/word-embeddings/index.html
  70. http://ruder.io/index.html
  71. http://ruder.io/author/sebastian/index.html
  72. http://ruder.io/index.html
  73. http://ruder.io/author/sebastian/index.html
  74. http://ruder.io/
  75. https://disqus.com/?ref_noscript
  76. http://ruder.io/
  77. http://ruder.io/
  78. https://twitter.com/seb_ruder
  79. https://ghost.org/

   hidden links:
  81. https://twitter.com/seb_ruder
  82. http://ruder.io/rss/index.rss
  83. http://ruder.io/index.html
  84. http://ruder.io/index.html
  85. https://twitter.com/share?text=on%20word%20embeddings%20-%20part%201&url=http://ruder.io/word-embeddings-1/
  86. https://www.facebook.com/sharer/sharer.php?u=http://ruder.io/word-embeddings-1/
