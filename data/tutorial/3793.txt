   #[1]sebastian ruder

   [2]sebastian ruder
     * [3]about
     * [4]tags
     * [5]papers
     * [6]talks
     * [7]news
     * [8]faq
     * [9]nlp news
     * [10]nlp progress
     * [11]contact

   28 november 2016 / [12]cross-lingual

a survey of cross-lingual id27 models

   a survey of cross-lingual id27 models

   this post gives an overview of methods that learn a joint cross-lingual
   id27 space between different languages.

   note: if you are looking for a survey paper, this blog post is also
   available as an [13]article on arxiv.

   in past blog posts, we discussed different [14]models, [15]objective
   functions, and [16]hyperparameter choices that allow us to learn
   accurate id27s. however, these models are generally
   restricted to capture representations of words in the language they
   were trained on. the availability of resources, training data, and
   benchmarks in english leads to a disproportionate focus on the english
   language and a negligence of the plethora of other languages that are
   spoken around the world.
   in our globalised society, where national borders increasingly blur,
   where the internet gives everyone equal access to information, it is
   thus imperative that we do not only seek to eliminate bias pertaining
   to [17]gender or race inherent in our representations, but also aim to
   address our bias towards language.

   to remedy this and level the linguistic playing field, we would like to
   leverage our existing knowledge in english to equip our models with the
   capability to process other languages.
   perfect machine translation (mt) would allow this. however, we do not
   need to actually translate examples, as long as we are able to project
   examples into a common subspace such as the one in figure 1.
   bilingual embedding space figure 1: a shared embedding space between
   two languages ([18]luong et al., 2015)

   ultimately, our goal is to learn a shared embedding space between words
   in all languages. equipped with such a vector space, we are able to
   train our models on data in any language. by projecting examples
   available in one language into this space, our model simultaneously
   obtains the capability to perform predictions in all other languages
   (we are glossing over some considerations here; for these, refer to
   [19]this section). this is the promise of cross-lingual embeddings.

   over the course of this blog post, i will give an overview of models
   and algorithms that have been used to come closer to this elusive goal
   of capturing the relations between words in multiple languages in a
   common embedding space.

   note: while neural mt approaches implicitly learn a shared
   cross-lingual embedding space by optimizing for the mt objective, we
   will focus on models that explicitly learn cross-lingual word
   representations throughout this blog post. these methods generally do
   so at a much lower cost than mt and can be considered to be to mt what
   id27 models (id97, glove, etc.) are to language
   modelling.

types of cross-lingual embedding models

   in recent years, various models for learning cross-lingual
   representations have been proposed. in the following, we will order
   them by the type of approach that they employ.
   note that while the nature of the parallel data used is equally
   discriminatory and has been shown to account for inter-model
   performance differences ^[20][1], we consider the type of approach more
   conducive to understanding the assumptions a model makes and --
   consequently -- its advantages and deficiencies.
   cross-lingual embedding models generally use four different approaches:
    1. monolingual mapping: these models initially train monolingual word
       embeddings on large monolingual corpora. they then learn a linear
       mapping between monolingual representations in different languages
       to enable them to map unknown words from the source language to the
       target language.
    2. pseudo-cross-lingual: these approaches create a
       pseudo-cross-lingual corpus by mixing contexts of different
       languages. they then train an off-the-shelf id27 model on
       the created corpus. the intuition is that the cross-lingual
       contexts allow the learned representations to capture cross-lingual
       relations.
    3. cross-lingual training: these models train their embeddings on a
       parallel corpus and optimize a cross-lingual constraint between
       embeddings of different languages that encourages embeddings of
       similar words to be close to each other in a shared vector space.
    4. joint optimization: these approaches train their models on parallel
       (and optionally monolingual data). they jointly optimise a
       combination of monolingual and cross-lingual losses.

   in terms of parallel data, methods may use different supervision
   signals that depend on the type of data used. these are, from most to
   least expensive:
    1. word-aligned data: a parallel corpus with word alignments that is
       commonly used for machine translation; this is the most expensive
       type of parallel data to use.
    2. sentence-aligned data: a parallel corpus without word alignments.
       if not otherwise specified, the model uses the [21]europarl corpus
       consisting of sentence-aligned text from the proceedings of the
       european parliament that is generally used for training statistical
       machine translation models.
    3. document-aligned data: a corpus containing documents in different
       languages. the documents can be topic-aligned (e.g. wikipedia) or
       label/class-aligned (e.g. id31 and multi-class
       classification datasets).
    4. lexicon: a bilingual or cross-lingual dictionary with pairs of
       translations between words in different languages.
    5. no parallel data: no parallel data whatsoever. learning
       cross-lingual representations from only monolingual resources would
       enable zero-shot learning across languages.

   to make the distinctions clearer, we provide the following table, which
   serves equally as the table of contents and a springboard to delve
   deeper into the different cross-lingual models:
   approach method parallel data
   [22]mono-lingual mapping [23]linear projection (mikolov et al., 2013)
   lexicon
   [24]projection via cca (faruqui and dyer, 2014)
   [25]normalisation and orthogonal transformation (xing et al., 2015)
   [26]max-margin and intruders (lazaridou et al., 2015)
   [27]alignment-based projection (guo et al., 2015) word-aligned
   [28]multilingual cca (ammar et al., 2016) lexicon
   [29]hybrid mapping with symmetric seed lexicon (vuli   and korhonen,
   2016) lexicon, document-aligned
   [30]orthogonal transformation, normalisation, and mean centering
   (artetxe et al., 2016) lexicon
   [31]adversarial auto-encoder (barone, 2016) -
   [32]pseudo-cross-lingual [33]mapping of translations to same
   representation (xiao and guo, 2014) lexicon
   [34]random translation replacement (gouws and sogaard, 2015)
   [35]on-the-fly replacement and polysemy handling (duong et al., 2016)
   [36]multilingual cluster (ammar et al., 2016)
   [37]document merge and shuffle (vuli   and moens, 2016) document-aligned
   [38]cross-lingual training [39]bilingual compositional sentence model
   (hermann and blunsom, 2013) sentence-aligned
   [40]bilingual bag-of-words autoencoder (lauly et al., 2013)
   [41]distributed word alignment (ko  isk   et al., 2014) sentence-aligned
   [42]bilingual compositional document model (hermann and blunsom, 2014)
   [43]bag-of-words autoencoder with correlation (chandar et al., 2014)
   [44]bilingual paragraph vectors (pham et al., 2015)
   [45]translation-invariant lsa (gardner et al., 2015) lexicon
   [46]inverted indexing on wikipedia (s  gaard et al., 2015)
   document-aligned
   [47]joint optimisation [48]multi-task language model (klementiev et
   al., 2012) word-aligned
   [49]bilingual matrix factorisation (zou et al., 2013)
   [50]bilingual skip-gram (luong et al., 2015)
   [51]bilingual bag-of-words without word alignments (gouws et al., 2015)
   sentence-aligned
   [52]bilingual skip-gram without word alignments (coulmance et al.,
   2015)
   [53]joint matrix factorisation (shi et al., 2015)
   [54]bilingual sparse representations (vyas and carpuat, 2016)
   word-aligned
   [55]bilingual paragraph vectors (without parallel data) (mogadala and
   rettinger, 2016) sentence-aligned/-

   after the discussion of cross-lingual embedding models, we will
   additionally look into how to [56]incorporate visual information into
   word representations, discuss the [57]challenges that still remain in
   learning cross-lingual representations, and finally summarize which
   models perform best and how to [58]evaluate them.

monolingual mapping

   methods that employ monolingual mapping train monolingual word
   representations independently on large monolingual corpora. they then
   seek to learn a transformation matrix that maps representations in one
   language to the representations of the other language. they usually
   employ a set of source word-target word pairs that are translations of
   each other, which are used as anchor words for learning the mapping.

   note that all of the following methods presuppose that monolingual
   embedding spaces have already been trained. if not stated otherwise,
   these embedding spaces have been learned using the id97 variants,
   skip-gram with negative sampling (sgns) or continuous bag-of-words
   (cbow) on large monolingual corpora.

linear projection

   mikolov et al. have popularised the notion that vector spaces can
   encode meaningful relations between words. in addition, they notice
   that the geometric relations that hold between words are similar across
   languages ^[59][2], e.g. numbers and animals in english show a similar
   geometric constellation as their spanish counterparts in figure 2.
   similar geometric relations between two languages figure 2: similar
   geometric relations between numbers and animals in english and spanish
   (mikolov et al., 2013)

   this suggests that it might be possible to transform one language's
   vector space into the space of another simply by utilising a linear
   projection with a transformation matrix \(w\).

   in order to achieve this, they translate the 5,000 most frequent words
   from the source language and use these 5,000 translations pairs as
   bilingual dictionary. they then learn \(w\) using stochastic gradient
   descent by minimising the distance between the previously learned
   monolingual representations \(x_i\) of the source word \(w_i\) that is
   transformed using \(w\) and its translation \(z_i\) in the bilingual
   dictionary:

   \(\min\limits_w \sum\limits^n_{i=1} |wx_i - z_i|^2 \).

projection via cca

   faruqui and dyer ^[60][3] propose to use another technique to learn the
   linear mapping. they use canonical correlation analysis (cca) to
   project words from two languages into a shared embedding space.
   different to linear projection, cca learns a transformation matrix for
   every language, as can be seen in figure 3, where the transformation
   matrix \(v\) is used to project word representations from the embedding
   space \(\sigma\) to a new space \(\sigma^\ast\), while \(w\) transforms
   words from \(\omega\) to \(\omega^\ast\). note that \(\sigma^\ast\) and
   \(\omega^\ast\) can be seen as the same shared embedding space.
   similar geometric relations between two languages figure 3:
   cross-lingual projection using cca (faruqui and dyer, 2014)

   similar to linear projection, cca also requires a number of translation
   pairs in \(\sigma'\) and \(\omega'\) whose correlation can be
   maximised. faruqui and dyer obtain these pairs by selecting for each
   source word the target word to which it has been aligned most often in
   a parallel corpus. alternatively, they could have also used a bilingual
   dictionary.
   as cca sorts the correlation vectors in \(v\) and \(w\) in descending
   order, faruqui and dyer perform experiments using only the top \(k\)
   correlated projection vectors and find that using the \(80\) %
   projection vectors with the highest correlation generally yields the
   highest performance.
   similar geometric relations between two languages figure 4: monolingual
   (top) and multi-lingual (bottom; marked with apostrophe) projections of
   the synonyms and antonyms of "beautiful" (faruqui and dyer, 2014)

   interestingly, they find that using multilingual projection helps to
   separate synonyms and antonyms in the source language, as can be seen
   in figure 4, where the unprotected antonyms of "beautiful" are in two
   clusters in the top, whereas the cca-projected vectors of the synonyms
   and antonyms form two distinct clusters in the bottom.

normalisation and orthogonal transformation

   xing et al. ^[61][4] notice inconsistencies in the linear projection
   method by mikolov et al. (2013), which they set out to resolve. recall
   that mikolov et al. initially learn monolingual id27s. for
   this, they use the skip-gram objective, which is the following:

   \(\dfrac{1}{n} \sum\limits_{i=1}^n \sum\limits_{-c \leq j \leq c, j
   \neq 0} \text{log} p(w_{i+j} | w_i) \)

   where \(c\) is the context length and \(p(w_{i+j} | w_i)\) is computed
   using the softmax:

   \(p(w_{i+j} | w_i) = \dfrac{\text{exp}(c_{w_{i+j}}^t c_{w_i})}{\sum_w
   \text{exp}(c_w^t c_{w_i})}\).

   they then learn a linear transformation between the two monolingual
   vector spaces with:

   \(\text{min} \sum\limits_i |wx_i - z_i|^2 \)

   where \(w\) is the projection matrix that should be learned and \(x_i\)
   and \(z_i\) are word vectors in the source and target language
   respectively that are similar in meaning.

   xing et al. argue that there is a mismatch between the objective
   function used to learn word representations (maximum likelihood based
   on inner product), the distance measure for word vectors (cosine
   similarity), and the objective function used to learn the linear
   transformation (mean squared error), which may lead to degradation in
   performance.

   they subsequently propose a method to resolve each of these
   inconsistencies: in order to fix the mismatch between the inner product
   similarity measure \(c_w^t c_{w'}\) during training and the cosine
   similarity measure \(\dfrac{c_w^t c_w'}{|c_w| |c_{w'}|}\) for testing,
   the inner product could also be used for testing. cosine similarity,
   however, is used conventionally as an evaluation measure in nlp and
   generally performs better than the inner product. for this reason, they
   propose to normalise the word vectors to be unit length during
   training, which makes the inner product the same as cosine similarity
   and places all word vectors on a hypersphere as a side-effect, as can
   be seen in figure 5.
   similar geometric relations between two languages figure 5: word
   representations before (left) and after (right) normalisation (xing et
   al., 2015)

   they resolve the inconsistency between the cosine similarity measure
   now used in training and the mean squared error employed for learning
   the transformation by replacing the mean squared error with cosine
   similarity for learning the mapping, which yields:

   \(\max\limits_w \sum\limits_i (wx_i)^t z_i \).

   finally, in order to also normalise the projected vector \(wx_i\) to be
   unit length, they constrain \(w\) to be an orthogonal matrix by solving
   a separate optimisation problem.

max-margin and intruders

   lazaridou et al. ^[62][5] identify another issue with the linear
   transformation objective of mikolov et al. (2013): they discover that
   using least-squares as objective for learning a projection matrix leads
   to hubness, i.e. some words tend to appear as nearest neighbours of
   many other words. to resolve this, they use a margin-based (max-margin)
   ranking loss (collobert et al. ^[63][6]) to train the model to rank the
   correct translation vector \(y_i\) of a source word \(x_i\) that is
   projected to \(\hat{y_i}\) higher than any other target words \(y_j\):

   \(\sum\limits^k_{j\neq i} \max \{ 0, \gamma + cos(\hat{y_i}, y_i) -
   cos(\hat{y_i}, y_j) \} \)

   where \(k\) is the number of negative examples and \(\gamma\) is the
   margin.

   they show that selecting max-margin over the least-squares loss
   consistently improves performance and reduces hubness. in addition, the
   choice of the negative examples, i.e. the target words compared to
   which the model should rank the correct translation higher, is
   important. they hypothesise that an informative negative example is an
   intruder ("truck" in the example), i.e. it is near the current
   projected vector \(\hat{y_i}\) but far from the actual translation
   vector \(y_i\) ("cat") as depicted in figure 6.
   similar geometric relations between two languages figure 6: the
   intruder "truck" is selected over "dog" as the negative example for
   "cat". (lazaridou et al., 2015)

   these intruders should help the model identify cases where it is
   failing considerably to approximate the target function and should thus
   allow it to correct its behaviour. at every step of id119,
   they compute \(s_j = cos(\hat{y_i}, y_j) - cos(y_i, y_j) \) for all
   vectors \(y_t\) in the target embedding space with \(j \neq i\) and
   choose the vector with the largest \(s_j\) as negative example for
   \(x_i\). using intruders instead of random negative examples yields a
   small improvement of 2 percentage points on their comparison task.

alignment-based projection

   guo et al. ^[64][7] propose another projection method that solely
   relies on word alignments. they count the number of times each word in
   the source language is aligned with each word in the target language in
   a parallel corpus and store these counts in an alignment matrix
   \(\mathcal{a}\).

   in order to project a word \(w_i\) from its source representation
   \(v(w_i^s)\) to its representation in the target embedding space
   \(v(w_i)^t\) in the target embedding space, they simply take the
   average of the embeddings of its translations \(v(w_j)^t\) weighted by
   their alignment id203 with the source word:

   \(v(w_i)^t = \sum\limits_{i, j \in \mathcal{a}} \dfrac{c_{i, j}}{\sum_j
   c_{i,j}} \cdot v(w_j)^t\)

   where \(c_{i,j}\) is the number of times the \(i^{th}\) source word has
   been aligned to the \(j^{th}\) target word.

   the problem with this method is that it only assigns embeddings for
   words that are aligned in the reference parallel corpus. gou et al.
   thus propagate alignments from in-vocabulary to oov words by using edit
   distance as a metric for morphological similarity. they set the
   projected vector of an oov source word \(v(w_{oov}^t)\) as the average
   of the projected vectors of source words that are similar to it in edit
   distance:

   \(v(w_{oov}^t) = avg(v(w_t))\)

   where \(c = \{ w | editdist(w_{oov}^t, w) \leq \tau \} \). they set the
   threshold \(\tau\) empirically to \(1\).
   even though this approach seems simplistic, they actually observe
   significant improvements over projection via cca in their experiments.

multilingual cca

   ammar et al. ^[65][8] extend the bilingual cca projection method of
   faruqui and dyer (2014) to the multi-lingual setting using the english
   embedding space as the foundation for their multilingual embedding
   space.

   they learn the two projection matrices for every other language with
   english. the transformation from each target language space \(\omega\)
   to the english embedding space \(\sigma\) can then be obtained by
   projecting the vectors in \(\omega\) into the cca space \(\omega^\ast\)
   using the transformation matrix \(w\) as in figure 3. as
   \(\omega^\ast\) and \(\sigma^\ast\) lie in the same space, vectors in
   \(\sigma^\ast\) can be projected into the english embedding space
   \(\sigma\) using the inverse of \(v\).

hybrid mapping with symmetric seed lexicon

   the previous mapping approaches used a bilingual dictionary as inherent
   component of their model, but did not pay much attention to the quality
   of the dictionary entries, using either automatic translations of
   frequent words or word alignments of all words.

   vuli   and korhonen ^[66][9] in turn emphasise the role of the seed
   lexicon that is used for learning the projection matrix. they propose a
   hybrid model that initially learns a first shared bilingual embedding
   space based on an existing cross-lingual embedding model. they then use
   this initial vector space to obtain translations for a list of frequent
   source words by projecting them into the space and using the nearest
   neighbour in the target language as translation. with these translation
   pairs as seed words, they learn a projection matrix analogously to
   mikolov et al. (2013).
   in addition, they propose a symmetry constraint, which enforces that
   words are only included if their projections are neighbours of each
   other in the first embedding space. additionally, one can retain pairs
   whose second nearest neighbours are less similar than the first nearest
   neighbours up to some threshold.
   they run experiments showing that their model with the symmetry
   constraint outperforms comparison models and that a small threshold of
   \(0.01\) or \(0.025\) leads to slightly improved performance.

orthogonal transformation, normalisation, and mean centering

   the previous approaches have introduced models that imposed different
   constraints for mapping monolingual representations of different
   languages to each other. the relation between these methods and
   constraints, however, is not clear.

   artetxe et al. ^[67][10] thus propose to generalise previous work on
   learning a linear transformation between monolingual vector spaces:
   starting with the basic optimisation objective, they propose several
   constraints that should intuitively help to improve the quality of the
   learned cross-lingual representations. recall that the linear
   transformation learned by mikolov et al. (2013) aims to find a
   parameter matrix \(w\) that satisfies:

   \(\declaremathoperator*{\argmin}{argmin} \argmin\limits_w \sum\limits_i
   |wx_i - z_i|^2 \)

   where \(x_i\) and \(z_i\) are similar words in the source and target
   language respectively.

   if the performance of the embeddings on a monolingual evaluation task
   should not be degraded, the dot products need to be preserved after the
   mapping. this can be guaranteed by requiring \(w\) to be an orthogonal
   matrix.

   secondly, in order to ensure that all embeddings contribute equally to
   the objective, embeddings in both languages can be normalised to be
   unit vectors:

   \(\argmin\limits_w \sum\limits_i | w \dfrac{x_i}{|x_i|} -
   \dfrac{z_i}{|z_i|}|^2 \).

   as the norm of an orthogonal matrix is \(1\), if \(w\) is orthogonal,
   we can add it to the denominator and move \(w\) to the numerator:

   \(\argmin\limits_w \sum\limits_i | \dfrac{wx_i}{|wx_i|} -
   \dfrac{z_i}{|z_i|}|^2 \).

   through expansion of the above binomial, we obtain:

   \(\argmin\limits_w \sum\limits_i |\dfrac{wx_i}{|wx_i|}|^2 +
   |\dfrac{z_i}{|z_i||}|^2 - 2 \dfrac{wx_i}{|wx_i|}^t \dfrac{z_i}{|z_i|}
   \).

   as the norm of a unit vector is \(1\) the first two terms reduce to
   \(1\), which leaves us with the following:

   \(\argmin\limits_w \sum\limits_i 2 - 2 \dfrac{wx_i}{|wx_i|}^t
   \dfrac{z_i}{|z_i|} ) \).

   the latter term now is just the cosine similarity of \(wx_i\) and
   \(z_i\):

   \(\argmin\limits_w \sum\limits_i 2 - 2 \text{cos}(wx_i, z_i) \).

   as we are interested in finding parameters \(w\) that minimise our
   objective, we can remove the constants above:

   \(\argmin\limits_w \sum\limits_i - \text{cos}(wx_i, z_i) \).

   minimising the sum of negative cosine similarities is then equal to
   maximising the sum of cosine similarities, which gives us the
   following:

   \(\declaremathoperator*{\argmax}{argmax} \argmax\limits_w \sum\limits_i
   \text{cos}(wx_i, z_i) \).

   this is equal to the objective by xing et al. (2015), although they
   motivated it via an inconsistency of the objectives.

   finally, artetxe et al. argue that two randomly selected words are
   generally expected not to be similar. for this reason, the cosine of
   their embeddings in any dimension -- as well as their cosine similarity
   -- should be zero. they capture this intuition by performing
   dimension-wise mean centering with a centering matrix \(c_m\):

   \(\argmin\limits_w \sum\limits_i ||c_mwx_i - c_mz_i||^2 \).

   this reduces to maximizing the sum of dimension-wise covariance as long
   as \(w\) is orthogonal similar as above:

   \(\argmax\limits_w \sum\limits_i \text{cov}(wx_i, z_i) \).

   interestingly, the method by faruqui and dyer (2014) is similar to this
   objective, as cca maximizes the dimension-wise covariance of both
   projections. this is equivalent to the single projection here, as it is
   constrained to be orthogonal. the only difference is that, while cca
   changes the monolingual embeddings so that different dimensions have
   the same variance and are uncorrelated -- which might degrade
   performance -- artetxe et al. enforce monolingual invariance.

adversarial auto-encoder

   all previous approaches to learning a transformation matrix between
   monolingual representations in different languages require either a
   dictionary or word alignments as a source of parallel data.

   barone ^[68][11], in contrast, seeks to get closer to the elusive goal
   of creating cross-lingual representations without parallel data. he
   proposes to use an adversarial auto-encoder to transform source
   embeddings into the target embedding space. the auto-encoder is then
   trained to reconstruct the source embeddings, while the discriminator
   is trained to differentiate the projected source embeddings from the
   actual target embeddings as in figure 7.
   similar geometric relations between two languages figure 7:
   cross-lingual mapping with an adversarial auto-encoder (barone, 2016)

   while intriguing, learning a transformation between languages without
   any parallel data at all seems unfeasible at this point. however,
   future approaches that aim to learn a mapping with fewer and fewer
   parallel data may bring us closer to this goal.

   more generally, however, it remains unclear if a projection can
   reliably transform the embedding space of one language into the
   embedding space of another language. additionally, the reliance on
   lexicon data or word alignment information is expensive.

pseudo-cross-lingual

   the second type of cross-lingual models seeks to construct a
   pseudo-cross-lingual corpus that captures interactions between the
   words in different languages. most approaches aim to identify words
   that can be translated to each other in monolingual corpora of
   different languages and replace these with placeholders to ensure that
   translations of the same word have the same vector representation.

mapping of translations to same representation

   xiao and guo ^[69][12] propose the first pseudo-cross-lingual method
   that leverages translation pairs: they first translate all words that
   appear in the source language corpus into the target language using
   wiktionary. as these translation pairs are still very noisy, they
   filter them by removing polysemous words in the source and target
   language and translations that do not appear in the target language
   corpus. from this bilingual dictionary, they now create a joint
   vocabulary, in which each translation pair has the same vector
   representation.

   for training, they use the margin-based ranking loss of collobert et
   al. (2008) to rank correct word windows higher than corrupted ones,
   where the middle word is replaced by an arbitrary word.
   in contrast to the subsequent methods, they do not construct a
   pseudo-cross-lingual corpus explicitly. instead, they feed windows of
   both the source and target corpus into the model during training,
   thereby essentially interpolating source and target language.
   it is thus most likely that, for ease of training, the authors replace
   translation pairs in source and target corpus with a placeholder to
   ensure a common vector representation, similar to the procedure of
   subsequent models.

random translation replacement

   gouws and s  gaard ^[70][13] in turn explicitly create a
   pseudo-cross-lingual corpus: they leverage translation pairs of words
   in the source and in the target language obtained via google translate.
   they concatenate the source and target corpus and replace each word
   that is part of a translation pair with its translation equivalent with
   a id203 of 50%. they then train cbow on this corpus.
   it is interesting to note that they also experiment with replacing
   words not based on translation but part-of-speech equivalence, i.e.
   words with the same part-of-speech in different languages will be
   replaced with one another. while replacement based on part-of-speech
   leads to small improvements for cross-lingual part-of-speech tagging,
   replacement based on translation equivalences yields even better
   performance for the task.

on-the-fly replacement and polysemy handling

   duong et al. ^[71][14] propose a similar approach to gouws and s  gaard
   (2015). they also use cbow, which predicts the centre word in a window
   given the surrounding words. instead of randomly replacing every word
   in the corpus with its translation during pre-processing, they replace
   each centre word with a translation on-the-fly during training.

   in addition to past approaches, they also seek to handle polysemy
   explicitly by proposing an em-inspired method that chooses as
   replacement the translation \(\bar{w_i}\) whose representation is most
   similar to the combination of the representations of the source word
   \(v_{w_i}\) and the context vector \(h_i\):

   \(\bar{w_i} = \text{argmax}_{w \in \text{dict}(w_i)} \text{cos}(v_{w_i}
   + h_i, v_w) \)

   where \(\text{dict}(w_i)\) contains the translations of \(w_i\).

   they then jointly learn to predict both the words and their appropriate
   translations. they use panlex as bilingual dictionary, which covers
   around 1,300 language with about 12 million expressions. consequently,
   translations are high coverage but often noisy.

multilingual cluster

   ammar et al. (2016) propose another approach that is similar to the
   previous method by gouws and s  gaard (2015): they use bilingual
   dictionaries to find clusters of synonymous words in different
   languages. they then concatenate the monolingual corpora of different
   languages and replace tokens in the same cluster with the cluster id.
   they then train sgns on the concatenated corpus.

document merge and shuffle

   the previous methods all use a bilingual dictionary or a translation
   tool as a source of translation pairs that can be used for replacement.

   vuli   and moens ^[72][15] present a model that does without translation
   pairs and learns cross-lingual embeddings only from document-aligned
   data. in contrast to the previous methods, the authors propose not to
   merge two monolingual corpora but two aligned documents of different
   languages into a pseudo-bilingual document.

   they concatenate the documents and then shuffle them by randomly
   permutating the words. the intuition is that as most methods rely on
   learning id27s based on their context, shuffling the
   documents would lead to bilingual contexts for each word that will
   enable the creation of a robust embedding space. as shuffling is
   necessarily random, however, it might lead to sub-optimal
   configurations.
   for this reason, they propose another merging strategy that assumes
   that the structures of the document are similar: they then
   alternatingly insert words from each language into the pseudo-bilingual
   document in the order in which they appear in their monolingual
   document and based on the mono-lingual documents' length ratio.

   while pseudo-cross-lingual approaches are attractive due to their
   simplicity and ease of implementation, relying on naive replacement and
   permutation does not allow them to capture more sophisticated facets of
   cross-lingual relations.

cross-lingual training

   cross-lingual training approaches focus exclusively on optimising the
   cross-lingual objective. these approaches typically rely on sentence
   alignments rather than a bilingual lexicon and require a parallel
   corpus for training.

bilingual compositional sentence model

   the first approach that optimizes only a cross-lingual objective is the
   bilingual compositional sentence model by hermann and blunsom
   ^[73][16]. they train two models to produce sentence representations of
   aligned sentences in two languages and use the distance between the two
   sentence representations as objective. they minimise the following
   loss:

   \(e_{dist}(a,b) = |a_{\text{root}} - b_{\text{root}} |^2 \)

   where \(a_{\text{root}}\) and \(b_{\text{root}}\) are the
   representations of two aligned sentences from different languages. they
   compose \(a_{\text{root}}\) and \(b_{\text{root}}\) simply as the sum
   of the embeddings of the words in the corresponding sentence. the full
   model is depicted in figure 8.
   similar geometric relations between two languages figure 8: the
   bilingual compositional sentence model (hermann and blunsom, 2013)

   they train the model then to output a higher score for correct
   translations than for randomly sampled incorrect translations using the
   max-margin hinge loss of collobert et al. (2008).

bilingual bag-of-words autoencoder

   instead of minimising the distance between two sentence representations
   in different languages, lauly et al. ^[74][17] aim to reconstruct the
   target sentence from the original source sentence. they start with a
   monolingual autoencoder that encodes an input sentence as a sum of its
   id27s and tries to reconstruct the original source sentence.
   for efficient reconstruction, they opt for a tree-based decoder that is
   similar to a hierarchical softmax. they then augment this autoencoder
   with a second decoder that reconstructs the aligned target sentence
   from the representation of the source sentence as in figure 9.
   similar geometric relations between two languages figure 9: a bilingual
   autoencoder (lauly et al., 2013)

   encoders and decoders have language-specific parameters. for an aligned
   sentence pair, they then train the model with four reconstruction
   losses: for each of the two sentences, they reconstruct from the
   sentence to itself and to its equivalent in the other language.

distributed word alignment

   while the previous approaches required word alignments as a
   prerequisite for learning cross-lingual embeddings, ko  isk   et al.
   ^[75][18] simultaneously learn id27s and alignments. their
   model, distributed word alignment, combines a distributed version of
   fastalign (dyer et al. ^[76][19]) with a language model. similar to
   other bilingual approaches, they use the word in the source language
   sentence of an aligned sentence pair to predict the word in the target
   language sentence.

   they replace the standard multinomial translation id203 of
   fastalign with an energy function that tries to bring the
   representation of a target word \(f\) close to the sum of the context
   words around the word \(e_i\) in the source sentence:

   \(e(f, e_i) = - ( \sum\limits_{s=-k}^k r^t_{e_{i+s}} t_s) r_f - b_r^t
   r_f - b_f \)

   where \(r_{e_{i+s}}\) and \(r_f\) are vector representations for source
   and target words, \(t_s\) is a projection matrix, and \(b_r\) and
   \(b_f\) are representation and target biases respectively. for
   calculating the translation id203 \(p(f|e_i)\), we then simply
   need to apply the softmax to the translation probabilities between the
   source word and all words in the target language.

   in addition, the authors speed up training by using a class
   factorisation strategy similar to the hierarchical softmax and predict
   frequency-based class representations instead of word representations.
   for training, they also use em but fix the alignment counts learned by
   fastalign that was initially trained for 5 epochs during the e-step and
   optimise the translation probabilities in the m-step only.

bilingual compositional document model

   hermann and blunsom ^[77][20] extend their approach (hermann and
   blunsom, 2013) to documents, by applying their composition and
   objective function recursively to compose sentences into documents.
   first, sentence representations are computed as [78]before. these
   sentence representations are then fed into a document-level
   compositional vector model, which integrates the sentence
   representations in the same way as can be seen in figure 10.
   compositional document model figure 10: a bilingual compositional
   document model (hermann and blunsom, 2014)

   the advantage of this method is that weaker supervision in the form of
   document-level alignment can be used instead of or in conjunction with
   sentence-level alignment. the authors run experiments both on europarl
   as well as on a newly created corpus of multilingual aligned ted talk
   transcriptions and find that the document signal helps considerably.

   in addition, they propose another composition function that -- instead
   of summing the representations -- applies a non-linearity to bigram
   pairs:

   \(f(x) = \sum\limits_{i=1}^n \text{tanh}(x_{i-1} + x_i)\)

   they find that this composition slightly outperforms addition, but
   underperforms it on smaller training datasets.

bag-of-words autoencoder with correlation

   chandar et al. ^[79][21] extend the approach by lauly et al. (2013) in
   two ways: instead of using a tree-based decoder for calculating the
   reconstruction loss, they reconstruct a sparse binary vector of word
   occurrences as in figure 11. due to the high-dimensionality of the
   binary bag-of-words vector, reconstruction is slower. as they perform
   training using mini-batch id119, where each mini-batch
   consists of adjacent sentences, they propose to merge the bags-of-words
   of the mini-batch into a single bag-of-words and to perform updates
   based on the merged bag-of-words. they find that this yields good
   performance and even outperforms the tree-based decoder.
   compositional document model figure 11: a bilingual autoencoder with
   binary reconstruction error (chandar et al., 2014)

   secondly, they propose to add a term \(cor(a(x), a(y))\) to the
   objective function that encourages correlation between the
   representations \(a(x)\) , \(a(y)\) of the source and target language
   respectively by summing the scalar correlations between all dimensions
   of the two vectors.

bilingual paragraph vectors

   similar to the previous methods, pham et al. ^[80][22] learn sentence
   representations as a means for learning cross-lingual id27s.
   they extend paragraph vectors (mikolov et al. ^[81][23]) to the
   multilingual setting by forcing aligned sentences of different
   languages to share the same vector representation as in figure 12 where
   \(sent\) is the shared sentence representation. the shared sentence
   representation is concatenated with the sum of the previous \(n\) words
   in the sentence and the model is trained to predict the next word in
   the sentence.
   compositional document model figure 12: bilingual paragraph vectors
   (pham et al., 2015)

   the authors use a hierarchical softmax to speed-up training. as the
   model only learns representations for the sentences it has seen during
   training, at test time for an unknown sentence, the sentence
   representation is randomly initialised and the model is trained to
   predict only the words in the sentence. only the sentence vector is
   updated, while the other model parameters are frozen.

translation-invariant lsa

   besides id27 models such as skip-gram, matrix factorisation
   approaches have historically been used successfully to learn
   representations of words. one of the most popular methods is lsa, which
   gardner et al. ^[82][24] extend as translation-invariant lsa to to
   learn cross-lingual id27s. they factorise a multilingual
   co-occurrence matrix with the restriction that it should be invariant
   to translation, i.e. it should stay the same if multiplied with the
   respective word or context dictionary.

inverted indexing on wikipedia

   all previous approaches to learn cross-lingual representations have
   been based on some form of language model or matrix factorisation. in
   contrast, s  gaard et al. ^[83][25] propose an approach that does
   without any of these methods, but instead relies on the structure of
   the multilingual knowledge base wikipedia, which they exploit by
   inverted indexing. their method is based on the intuition that similar
   words will be used to describe the same concepts across different
   languages.

   in wikipedia, articles in multiple languages deal with the same
   concept. we would typically represent every concept with the terms that
   are used to describe it across different languages. to learn
   cross-lingual word representations, we can now simply invert the index
   and instead represent a word by the wikipedia concepts it is used to
   describe. this way, we are directly provided with cross-lingual
   representations of words without performing any optimisation
   whatsoever. as a post-processing step, we can perform dimensionality
   reduction on the produced word representations.

   while the previous methods are able to make effective use of parallel
   sentence and documents to learn cross-lingual word representations,
   they neglect the monolingual quality of the learned representations.
   ultimately, we do not only want to embed languages into a shared
   embedding space, but also want the monolingual representations do well
   on the task at hand.

joint optimisation

   models that use joint optimisation aim to do exactly this: they not
   only consider a cross-lingual constraint, but jointly optimize
   mono-lingual and cross-lingual objectives.

   in practice, for two languages \(l_1\) and \(l_2\), these models
   optimize a monolingual loss \(\mathcal{m}\) for each language and one
   or multiple terms \(\omega\) that regularize the transfer from language
   \(l_1\) to \(l_2\) (and vice versa):

   \(\mathcal{m}_{l_1} + \mathcal{m}_{l_2} + \lambda (\omega_{l_1
   \rightarrow l_2} + \omega_{l_2 \rightarrow l_1}) \)

   where \(\lambda\) is an interpolation parameter that adjusts the impact
   of the cross-lingual id173.

multi-task language model

   the first jointly optimised model for learning cross-lingual
   representations was created by klementiev et al. ^[84][26]. they train
   a neural language model for each language and jointly optimise the
   monolingual maximum likelihood objective of each language model with a
   word-alignment based mt id173 term as the cross-lingual
   objective. the monolingual objective is thus to maximise the
   id203 of the current word \(w_t\) given its \(n\) surrounding
   words:

   \(\mathcal{m} = \text{log} p(w_t^ | w_{t-n+1:t-1}) \).

   this is optimised using the classic language model of bengio et al.
   ^[85][27]. the cross-lingual regularisation term in turn encourages the
   representations of words that are often aligned to each other to be
   similar:

   \(\omega = \dfrac{1}{2} c^t (a \otimes i) c\)

   where \(a\) is the matrix capturing alignment scores, \(i\) is the
   identity matrix, \(\otimes\) is the kronecker product, and \(c\) is the
   representation of word \(w_t\).

bilingual matrix factorisation

   zou et al. ^[86][28] use a matrix factorisation approach in the spirit
   of glove (pennington et al. ^[87][29]) to learn cross-lingual word
   representations for english and chinese. they create two alignment
   matrices \(a_{en \rightarrow zh}\) and \(a_{zh \rightarrow en}\) using
   alignment counts automatically learned from the chinese gigaword
   corpus. in \(a_{en \rightarrow zh}\), each element \(a_{ij}\) contains
   the number of times the \(i\)-th chinese word was aligned with the
   \(j\)-th english word, with each row normalised to sum to \(1\).
   intuitively, if a word in the source language is only aligned with one
   word in the target language, then those words should have the same
   representation. if the target word is aligned with more than one source
   word, then its representation should be a combination of the
   representations of its aligned words. consequently, the authors
   represent the embeddings in the target language as the product of the
   source embeddings \(v_{en}\) and their corresponding alignment counts
   \(a_{en \rightarrow zh}\). they then minimise the squared difference
   between these two terms:

   \(\omega_{en\rightarrow zh} = || v_{zh} - a_{en \rightarrow zh}
   v_{en}||^2 \)

   \(\omega_{zh\rightarrow en} = || v_{en} - a_{zh \rightarrow en}
   v_{zh}||^2 \)

   where \(v_{en}\) and \(v_{zh}\) are the embedding matrices of the
   english and chinese id27s respectively.

   they employ the max-margin hinge loss objective by collobert et al.
   (2008) as monolingual objective \(\mathcal{m}\) and train the english
   and chinese id27s to minimise the corresponding objective
   above together with a monolingual objective. for instance, for english,
   the training objective is:

   \(\mathcal{m}_{en} + \lambda \omega_{zh\rightarrow en} \).

   it is interesting to observe that the authors learn embeddings using a
   curriculum, training different frequency bands of the vocabulary at a
   time. the entire training process takes 19 days.

bilingual skip-gram

   luong et al. ^[88][30] in turn extend skip-gram to the cross-lingual
   setting and use the skip-gram objectives as monolingual and
   cross-lingual objectives. rather than just predicting the surrounding
   words in the source language, they use the words in the source language
   to additionally predict their aligned words in the target language as
   in figure 13.
   bilingual skip-gram figure 13: bilingual skip-gram (luong et al., 2015)

   for this, they require word alignment information. they propose two
   ways to predict aligned words: for their first method, they
   automatically learn alignment information; if a word is unaligned, the
   alignments of its neighbours are used for prediction. in their second
   method, they assume that words in the source and target sentence are
   monotonically aligned, with each source word at position \(i\) being
   aligned to the target word at position \(i \cdot t/s\) where \(s\) and
   \(t\) are the source and target sentence lengths. they find that a
   simple monotonic alignment is comparable to the unsupervisedly learned
   alignment in performance.

bilingual bag-of-words without word alignments

   gouws et al. ^[89][31] propose a bilingual bag-of-words without word
   alignments (bilbowa) that leverages additional monolingual data. they
   use the skip-gram objective as a monolingual objective and a novel
   sampled \(l_2\) loss as cross-lingual regularizer as in figure 14.
   the bilbowa model figure 14: the bilbowa model (gouws et al., 2015)

   more precisely, instead of relying on expensive word alignments, they
   simply assume that each word in a source sentence is aligned with every
   word in the target sentence under a uniform alignment model. thus,
   instead of minimising the distance between words that were aligned to
   each other, they minimise the distance between the means of the word
   representations in the aligned sentences, which is shown in figure 15,
   where \(s^e\) and \(s^f\) are the sentences in source and target
   language respectively.
   approximating word alignments figure 15: approximating word alignments
   with uniform alignments (gouws et al., 2015)

   the cross-lingual objective in the bilbowa model is thus:

   \(\omega = |\dfrac{1}{m} \sum\limits_{w_i \in s^{l_1}}^m r_i^{l_1} -
   \dfrac{1}{n} \sum\limits_{w_j \in s^{l_2}}^n r_j^{l_2}
   |^2 \)

   where \(r_i\) and \(r_j\) are the id27s of word \(w_i\) and
   \(w_j\) in each sentence \(s^{l_1}\) and \(s^{l_2}\) of length \(m\)
   and \(n\) in languages \(l_1\) and \(l_2\) respectively.

bilingual skip-gram without word alignments

   another extension of skip-gram to learning cross-lingual
   representations is proposed by coulmance et al. ^[90][32]. they also
   use the regular skip-gram objective as monolingual objective. for the
   cross-lingual objective, they make a similar assumption as gouws et al.
   (2015) by supposing that every word in the source sentence is uniformly
   aligned to every word in the target sentence.

   under the skip-gram formulation, they treat every word in the target
   sentence as context of every word in the source sentence and thus train
   their model to predict all words in the target sentence with the
   following skip-gram objective:

   \(\omega_{e,f} = \sum\limits_{(s_{l_1}, s_{l_2}) \in c_{l_1, l_2}}
   \sum\limits_{w_{l_1} \in s_{l_1}} \sum\limits_{c_{l_2} \in s_{l_2}} -
   \text{log} \sigma(w_{l_1}, c_{l_2}) \)

   where \(s\) is the sentence in the respective language, \(c\) is the
   sentence-aligned corpus, \(w\) are word and \(c\) are context
   representations respectively, and \( - \text{log} \sigma(\centerdot)\)
   is the standard skip-gram id168.
   trans-gram figure 16: the trans-gram model (coulmance et al., 2015)

   as the cross-lingual objective is asymmetric, they use one
   cross-lingual objective for the source-to-target and another one for
   the target-to-source direction. the complete trans-gram objective
   including two monolingual and two cross-lingual skip-gram objectives is
   displayed in figure 16.

joint matrix factorisation

   shi et al. ^[91][33] use a joint matrix factorisation model to learn
   cross-lingual representations. in contrast to zou et al. (2013), they
   also take into account additional monolingual data. similar to the
   former, they also use the glove objective (pennington et al., 2014) as
   monolingual objective:

   \(\mathcal{m}_{l_i} = \sum\limits_{j,k} f(x_{jk}^{l_i})(w_j^{l_i} \cdot
   c_k^{l_i} + b_{w_j}^{l_i} + b_{c_k}^{l_i} + b^{l_i} - m_{jk}^{l_{i}})
   \)

   where \(w_j^{l_i}\) and \(c_k^{l_i}\) are the embeddings and
   \(m_{jk}^{l_{i}}\) the pmi value of a word-context pair \((j,k) \) in
   language \(l_{i}\), while \( b_{w_j}^{l_i}\) and \(b_{c_k}^{l_i}\) and
   \(b^{l_i}\) are the word-specific and language-specific bias terms
   respectively.
   cross-lingual matrix factorisation figure 17: learning cross-lingual
   word representations via matrix factorisation (shi et al., 2015)

   they then place cross-lingual constraints on the monolingual
   representations as can be seen in figure 17. the authors propose two
   cross-lingual regularisation objectives: the first one is based on
   calculating cross-lingual co-occurrence counts. these co-occurrences
   can be calculated without alignment information using a uniform
   alignment model as in gouws et al. (2015). alternatively, co-occurrence
   counts can also be calculated by leveraging automatically learned word
   alignments. the co-occurrence counts are then stored in a matrix
   \(x^{\text{bi}}\) where every entry \(x_{jk}^{\text{bi}}\) contains the
   number of times the source word \(j\) occurred with the target word
   \(k\) in an aligned sentence pair in the parallel corpus.
   for optimisation, a pmi matrix \(m^{\text{bi}}_{jk}\) can be calculated
   based on the co-occurrence counts in \(x^{\text{bi}}\). this matrix can
   again be factorised as in the glove objective, where now the context
   word representation \(c_k^{l_i}\) is replaced with the representation
   of the word in the target language \(w_k^{l_2}\):

   \(\omega = \sum\limits_{j \in v^{l_1}, k \in v^{l_2}}
   f(x_{jk}^{l_1})(w_j^{l_1} \cdot w_k^{l_2} + b_{w_j}^{l_1} +
   b_{w_k}^{l_2} + b^{\text{bi}} - m_{jk}^{\text{bi}}) \).

   the second cross-lingual regularisation term they propose leverages the
   translation probabilities produced by a machine translation system and
   involves minimising the distances of the representations of related
   words in the two languages weighted by their similarities:

   \(\omega = \sum\limits_{j \in v^{l_1}, k \in v^{l_2}} sim(j,k) \cdot
   ||w_j^{l_1} - w_k^{l_2}||^2\)

   where \(j\) and \(k\) are words in the source and target language
   respectively and \(sim(j,k)\) is their translation id203.

bilingual sparse representations

   vyas and carpuat ^[92][34] propose another method based on matrix
   factorisation that -- in contrast to previous approaches -- allows
   learning sparse cross-lingual representations. they first independently
   train two monolingual word representations \(x_e\) and \(x_f\) in two
   different languages using glove (pennington et al., 2014) on two large
   monolingual corpora.

   they then learn monolingual sparse representations from these dense
   representations by decomposing \(x\) into two matrices \(a\) and \(d\)
   such that the \(l_2\) reconstruction error is minimised, with an
   additional constraint on \(a\) for sparsity:

   \(\mathcal{m}_{l_i} = \sum\limits_{i=1}^{v_{l_i}} |a_{l_ii}d_{l_i}^t -
   x_{l_ii}| + \lambda_{l_i} |a_{l_ii}|_1 \)

   where \(v_{l_i}\) is the number of dense word representations in
   language \(l_i\).

   the above equation, however, only creates sparse monolingual
   embeddings. to learn bilingual embeddings, they add another constraint
   based on automatically learned word alignment that minimises the
   \(l_2\) reconstruction error between words that were strongly aligned
   to each other:

   \(\omega = \sum\limits_{i=1}^{v_{l_1}} \sum\limits_{j=1}^{v_{l_2}}
   \dfrac{1}{2} \lambda_x s_{ij} |a_{l_1i} - a_{l_2j}|_2^2 \)

   where \(s\) is the alignment matrix where each entry \(s_{ij}\)
   contains the alignment score of source word \(x_{l_1i}\) with target
   word \(x_{l_2j}\).

   the complete objective function is thus the following:

   \(\mathcal{m}_{l_1} + \mathcal{m}_{l_2} + \omega\).

bilingual paragraph vectors (without parallel data)

   mogadala and rettinger ^[93][35] use an approach similar to pham et al.
   (2015), but extend it to also work without parallel data. they use the
   paragraph vectors objective as monolingual objective \(\mathcal{m}\).
   they jointly optimise this objective together with a cross-lingual
   id173 function \(\omega\) that encourages the representations
   of words in languages \(l_1\) and \(l_2\) to be close to each other.

   their main innovation is that the cross-lingual regularizer \(\omega\)
   is adjusted based on the nature of the training corpus. in addition to
   regularising the mean of word vectors in a sentence to be close to the
   mean of word vectors in the aligned sentence similar to gouws et al.
   (2015) (the second term in the below equation), they also regularise
   the paragraph vectors \(sp^{l_1}\) and \(sp^{l_2}\) of aligned
   sentences in languages \(l_1\) and \(l_2\) to be close to each other.
   the complete cross-lingual objective then uses elastic net
   id173 to combine both terms:

   \(\omega = \alpha ||sp^{l_1}_j - sp^{l_2}_j||^2 + (1-\alpha)
   \dfrac{1}{m} \sum\limits_{w_i \in s_j^{l_1}}^m w_i^{l_1} - \dfrac{1}{n}
   \sum\limits_{w_k \in s_j^{l_2}}^n w_k^{l_2} \)

   where \(w_i^{l_1}\) and \(w_k^{l_2}\) are the id27s of word
   \(w_i\) and \(w_k\) in each sentence \(s_j\) of length \(m\) and \(n\)
   in languages \(l_1\) and \(l_2\) respectively.

   to leverage data that is not sentence-aligned, but where an alignment
   is still present on the document level, they propose a two-step
   approach: they use [94]procrustes analysis, a method for statistical
   shape analysis, to find for each document in language \(l_1\) the most
   similar document in language \(l_2\). this is done by first learning
   monolingual representations of the documents in each language using
   paragraph vectors on each corpus. subsequently, procrustes analysis
   aims to learn a transformation between the two vector spaces by
   translating, rotating, and scaling the embeddings in the first space
   until they most closely align to the id194s in the
   second space.
   in the second step, they then simply use the previously described
   method to learn cross-lingual word representations from the alignment
   documents, this time treating the entire documents as paragraphs.

incorporating visual information

   a recent branch of research proposes to incorporate visual information
   to improve the performance of monolingual ^[95][36] or cross-lingual
   ^[96][37] representations. these methods show good performance on
   comparison tasks. they additionally demonstrate application for
   zero-shot learning and might thus ultimately be helpful in learning
   cross-lingual representations without (linguistic) parallel data.

challenges

functional modeling

   models for learning cross-linguistic representations share weaknesses
   with other vector space models of language: while they are very good at
   modelling the conceptual aspect of meaning evaluated in word similarity
   tasks, they fail to properly model the functional aspect of meaning,
   e.g. to distinguish whether one remarks "give me a pencil" or "give me
   that pencil".

word order

   secondly, due to the reliance on bag-of-words representations, current
   models for learning cross-lingual id27s completely ignore
   word order. models that are oblivious to word order, for instance,
   assign to the following sentence pair (landauer & dumais ^[97][38]) the
   exact same representation as they contain the same set of words, even
   though they are completely different in meaning:
     * "that day the office manager, who was drinking, hit the problem
       sales worker with a bottle, but it was not serious."
     * "it was not the sales manager, who hit the bottle that day, but the
       office worker with a serious drinking problem".

compositionality

   most approaches for learning cross-lingual representations focus on
   word representations. these approaches are not able to easily compose
   word representations to form representations of sentences and
   documents. even approaches that learn jointly learn word and sentence
   representations do so by via simple summation of words in the sentence.
   in the future, it will be interesting to see if lstms or id98s that can
   form more composable sentence representations can be applied
   efficiently to learn cross-lingual representations.

polysemy

   while conflating multiple senses of a word is already problematic for
   learning mono-lingual word representations, this issue is amplified in
   a cross-lingual embedding space: monosemous words in one language might
   align with polysemous words in another language and thus fail to
   capture the entirety of the cross-lingual relations. there has already
   been promising work on learning monolingual multi-sense embeddings. we
   hypothesize that learning cross-lingual multi-sense embeddings will
   become increasingly relevant, as it enables us to capture more
   fine-grained cross-lingual meaning.

feasibility

   the final challenge pertains to the feasibility of the venture of
   learning cross-lingual embeddings itself: languages are incredibly
   complex, human artefacts. learning a monolingual embedding space is
   already difficult; sharing such a vector space between two languages
   and expecting that inter-language and intra-language relations are
   reliably reflected then seems utopian.
   additionally, some languages show linguistic features, which other
   languages lack. the ease of constructing a shared embedding space
   between languages and consequently the success of cross-lingual
   transfer is intuitively proportional to the similarity of the
   languages: an embedding space shared between spanish and portuguese
   tends to capture more linguistic nuances of meaning than an embedding
   space populated with english and chinese representations. furthermore,
   if two languages are too dissimilar, cross-linguistic transfer might
   not be possible at all -- similar to the negative transfer that occurs
   in id20 between very dissimilar domains.

evaluation

   having surveyed models to learn cross-lingual word representations, we
   would now like to know which is the best method to use for the task we
   care about. cross-lingual representation models have been evaluated on
   a wide range of tasks such as cross-lingual document classification
   (cldc), machine translation (mt), word similarity, as well as
   cross-lingual variations of the following tasks: named entity
   recognition, part-of-speech tagging, super sense tagging, dependency
   parsing, and dictionary induction.
   in the context of the cldc evaluation setup by klementiev et al. (2012)
   \(40\)-dimensional cross-lingual id27s are learned to
   classify documents in one language and evaluated on the documents of
   another language. as cldc is among the most widely used, we show below
   exemplarily the evaluation table of mogadala and rettinger (2016) for
   this task:
   method en -> de de -> en en -> fr fr -> en en -> es es -> en
   majority class 46.8 46.8 22.5 25.0 15.3 22.2
   mt 68.1 67.4 76.3 71.1 52.0 58.4
   [98]multi-task language model (klementiev et al., 2012) 77.6 71.1 74.5
   61.9 31.3 63.0
   [99]bag-of-words autoencoder with correlation (chandar et al., 2014)
   91.8 74.2 84.6 74.2 49.0 64.4
   [100]bilingual compositional document model (hermann and blunsom, 2014)
   86.4 74.7 - - - -
   [101]distributed word alignment (ko  isk   et al., 2014) 83.1 75.4 - - -
   -
   [102]bilingual bag-of-words without word alignments (gouws et al.,
   2015) 86.5 75.0 - - - -
   [103]bilingual skip-gram (luong et al., 2015) 87.6 77.8 - - - -
   [104]bilingual skip-gram without word alignments (coulmance et al.,
   2015) 87.8 78.7 - - - -
   [105]bilingual paragraph vectors (without parallel data) (mogadala and
   rettinger, 2016) 88.1 78.9 79.2 77.8 56.9 67.6

   these results, however, should not be considered as representative of
   the general performance of cross-lingual embedding models as different
   methods tend to well on different tasks depending on the type of
   approach and the type of data used.
   upadhyay et al. ^[106][39] evaluate cross-lingual embedding models that
   require different forms of supervision on various tasks. they find that
   on word similarity datasets, models that require cheaper forms of
   supervision (sentence-aligned and document-aligned data) are almost as
   good as models with more expensive supervision in the form of word
   alignments. for cross-lingual classification and dictionary induction,
   more informative supervision is better. finally, for parsing, models
   with word-level alignment are able to capture syntax more accurately
   and thus perform better overall.

   the findings by upadhyay et al. are further proof for the intuition
   that the choice of the data is important. levy et al. (2016) go even
   further than this in comparing models for learning cross-lingual word
   representations to traditional alignment models on dictionary induction
   and word alignment tasks. they argue that whether or not an algorithm
   uses a particular feature set is more important than the choice of the
   algorithm. in their experiments, using sentence ids, i.e. creating a
   sentence's language-independent representation (for instance with
   doc2vec) achieves better results than just using the source and target
   words.

   finally, to facilitate evaluation of cross-lingual id27s,
   ammar et al. (2016) make a [107]website available where learned
   representations can be uploaded and automatically evaluated on a wide
   range of tasks.

conclusion

   models that allow us to learn cross-lingual representations have
   already been useful in a variety of tasks such as machine translation
   (decoding and evaluation), automated bilingual dictionary generation,
   cross-lingual information retrieval, parallel corpus extraction and
   generation, as well as cross-language plagiarism detection. it will be
   interesting to see what further progress the future will bring.

   let me know your thoughts about this post and about any errors you
   found in the comments below.

printable version and citation

   this blog post is also available as an [108]article on arxiv, in case
   you want to refer to it later.

   in case you found it helpful, consider citing the corresponding arxiv
   article as:
   sebastian ruder (2017). a survey of cross-lingual embedding models.
   arxiv preprint arxiv:1706.04902.

other blog posts on id27s

   if you want to learn more about id27s, these other blog posts
   on id27s are also available:
     * [109]on id27s - part 1
     * [110]on id27s - part 2: approximating the softmax
     * [111]on id27s - part 3: the secret ingredients of
       id97
     * [112]unofficial part 5: id27s in 2017 - trends and future
       directions

   cover image courtesy of zou et al. (2013)
     __________________________________________________________________

    1. levy, o., s  gaard, a., & goldberg, y. (2016). reconsidering
       cross-lingual id27s. arxiv preprint arxiv:1608.05426.
       retrieved from [113]http://arxiv.org/abs/1608.05426 [114]      
    2. mikolov, t., le, q. v., & sutskever, i. (2013). exploiting
       similarities among languages for machine translation. retrieved
       from [115]http://arxiv.org/abs/1309.4168 [116]      
    3. faruqui, m., & dyer, c. (2014). improving vector space word
       representations using multilingual correlation. proceedings of the
       14th conference of the european chapter of the association for
       computational linguistics, 462     471. retrieved from
       [117]http://repository.cmu.edu/lti/31 [118]      
    4. xing, c., liu, c., wang, d., & lin, y. (2015). normalized word
       embedding and orthogonal transform for bilingual word translation.
       naacl-2015, 1005   1010. [119]      
    5. lazaridou, a., dinu, g., & baroni, m. (2015). hubness and
       pollution: delving into cross-space mapping for zero-shot learning.
       proceedings of the 53rd annual meeting of the association for
       computational linguistics and the 7th international joint
       conference on natural language processing, 270   280. [120]      
    6. collobert, r., & weston, j. (2008). a unified architecture for
       natural language processing. proceedings of the 25th international
       conference on machine learning - icml    08, 20(1), 160   167.
       [121]http://doi.org/10.1145/1390156.1390177 [122]      
    7. guo, j., che, w., yarowsky, d., wang, h., & liu, t. (2015).
       cross-lingual id33 based on distributed
       representations. proceedings of the 53rd annual meeting of the
       association for computational linguistics and the 7th international
       joint conference on natural language processing (volume 1: long
       papers), 1234   1244. retrieved from
       [123]http://www.aclweb.org/anthology/p15-1119 [124]      
    8. ammar, w., mulcaire, g., tsvetkov, y., lample, g., dyer, c., &
       smith, n. a. (2016). massively multilingual id27s.
       retrieved from [125]http://arxiv.org/abs/1602.01925 [126]      
    9. vulic, i., & korhonen, a. (2016). on the role of seed lexicons in
       learning bilingual id27s. proceedings of acl, 247   257.
       [127]      
   10. artetxe, m., labaka, g., & agirre, e. (2016). learning principled
       bilingual mappings of id27s while preserving monolingual
       invariance. proceedings of the 2016 conference on empirical methods
       in natural language processing (emnlp-16), 2289   2294. [128]      
   11. barone, a. v. m. (2016). towards cross-lingual distributed
       representations without parallel text trained with adversarial
       autoencoders. proceedings of the 1st workshop on representation
       learning for nlp, 121   126. retrieved from
       [129]http://arxiv.org/pdf/1608.02996.pdf [130]      
   12. xiao, m., & guo, y. (2014). distributed word representation
       learning for cross-lingual id33. conll. [131]      
   13. gouws, s., & s  gaard, a. (2015). simple task-specific bilingual
       id27s. naacl, 1302   1306. [132]      
   14. duong, l., kanayama, h., ma, t., bird, s., & cohn, t. (2016).
       learning crosslingual id27s without bilingual corpora.
       proceedings of the 2016 conference on empirical methods in natural
       language processing (emnlp-16). [133]      
   15. vulic, i., & moens, m.-f. (2016). bilingual distributed word
       representations from document-aligned comparable data. journal of
       artificial intelligence research, 55, 953   994. retrieved from
       [134]http://arxiv.org/abs/1509.07308 [135]      
   16. hermann, k. m., & blunsom, p. (2013). multilingual distributed
       representations without word alignment. arxiv preprint
       arxiv:1312.6173. [136]      
   17. lauly, s., boulanger, a., & larochelle, h. (2013). learning
       id73 representations using a bag-of-words autoencoder.
       nips ws on deep learning, 1   8. retrieved from
       [137]http://arxiv.org/abs/1401.1803 [138]      
   18. ko  isk  , t., hermann, k. m., & blunsom, p. (2014). learning
       bilingual word representations by marginalizing alignments.
       retrieved from [139]http://arxiv.org/abs/1405.0947 [140]      
   19. dyer, c., victor ch., & smith, n. a. (2013). a simple, fast, and
       effective reparameterization of ibm model 2. association for
       computational linguistics. [141]      
   20. hermann, k. m., & blunsom, p. (2014). multilingual models for
       compositional distributed semantics. acl, 58   68. [142]      
   21. chandar, s., lauly, s., larochelle, h., khapra, m. m., ravindran,
       b., raykar, v., & saha, a. (2014). an autoencoder approach to
       learning bilingual word representations. advances in neural
       information processing systems. retrieved from
       [143]http://arxiv.org/abs/1402.1454 [144]      
   22. pham, h., luong, m.-t., & manning, c. d. (2015). learning
       distributed representations for multilingual text sequences.
       workshop on vector modeling for nlp, 88   94. [145]      
   23. le, q. v., & mikolov, t. (2014). distributed representations of
       sentences and documents. international conference on machine
       learning - icml 2014, 32, 1188   1196. retrieved from
       [146]http://arxiv.org/abs/1405.4053 [147]      
   24. gardner, m., huang, k., paplexakis, e., fu, x., talukdar, p.,
       faloutsos, c.,     sidiropoulos, n. (2015). translation invariant
       id27s. emnlp. [148]      
   25. s  gaard, a., agic, z., alonso, h. m., plank, b., bohnet, b., &
       johannsen, a. (2015). inverted indexing for cross-lingual nlp. the
       53rd annual meeting of the association for computational
       linguistics and the 7th international joint conference of the asian
       federation of natural language processing (acl-ijcnlp 2015),
       1713   1722. [149]      
   26. klementiev, a., titov, i., & bhattarai, b. (2012). inducing
       crosslingual distributed representations of words. [150]      
   27. bengio, y., ducharme, r., vincent, p., & janvin, c. (2003). a
       neural probabilistic language model. the journal of machine
       learning research, 3, 1137   1155.
       [151]http://doi.org/10.1162/153244303322533223 [152]      
   28. zou, w. y., socher, r., cer, d., & manning, c. d. (2013). bilingual
       id27s for phrase-based machine translation. emnlp.
       [153]      
   29. pennington, j., socher, r., & manning, c. d. (2014). glove: global
       vectors for word representation. proceedings of the 2014 conference
       on empirical methods in natural language processing, 1532   1543.
       [154]http://doi.org/10.3115/v1/d14-1162 [155]      
   30. luong, m.-t., pham, h., & manning, c. d. (2015). bilingual word
       representations with monolingual quality in mind. workshop on
       vector modeling for nlp, 151   159. [156]      
   31. gouws, s., bengio, y., & corrado, g. (2015). bilbowa: fast
       bilingual distributed representations without word alignments.
       proceedings of the 32nd international conference on machine
       learning, 748   756. retrieved from
       [157]http://jmlr.org/proceedings/papers/v37/gouws15.html [158]      
   32. coulmance, j., marty, j.-m., wenzek, g., & benhalloum, a. (2015).
       trans-gram, fast cross-lingual word-embeddings. emnlp 2015,
       (september), 1109   1113. [159]      
   33. shi, t., liu, z., liu, y., & sun, m. (2015). learning cross-lingual
       id27s via matrix co-factorization. annual meeting of the
       association for computational linguistics, 567   572. [160]      
   34. vyas, y., & carpuat, m. (2016). sparse bilingual word
       representations for cross-lingual lexical entailment. naacl,
       1187   1197. [161]      
   35. mogadala, a., & rettinger, a. (2016). bilingual id27s
       from parallel and non-parallel corpora for cross-language text
       classification. naacl, 692   702. retrieved from
       [162]http://www.aifb.kit.edu/images/b/b4/naacl-hlt-2016-camera-read
       y.pdf [163]      
   36. lazaridou, a., nghia, t. p., & baroni, m. (2015). combining
       language and vision with a multimodal skip-gram model. proceedings
       of human language technologies: the 2015 annual conference of the
       north american chapter of the acl, denver, colorado, may 31     june
       5, 2015, 153   163. [164]      
   37. vuli  , i., kiela, d., clark, s., & moens, m.-f. (2016). multi-modal
       representations for improved bilingual lexicon learning. acl.
       [165]      
   38. landauer, t. k. & dumais, s. t. (1997). a solution to plato   s
       problem: the latent semantic analysis theory of acquisition,
       induction and representation of knowledge, psychological review,
       104(2), 211-240. [166]      
   39. upadhyay, s., faruqui, m., dyer, c., & roth, d. (2016).
       cross-lingual models of id27s: an empirical comparison.
       retrieved from [167]http://arxiv.org/abs/1604.00425 [168]      

   sebastian ruder

[169]sebastian ruder

   read [170]more posts by this author.
   [171]read more

       sebastian ruder    

[172]cross-lingual

     * [173]neural id21 for natural language processing (phd
       thesis)
     * [174]the 4 biggest open problems in nlp
     * [175]10 exciting ideas of 2018 in nlp

   [176]see all 7 posts    

   [177]highlights of nips 2016: adversarial learning, meta-learning, and
   more

   meta-learning

highlights of nips 2016: adversarial learning, meta-learning, and more

   the conference on neural information processing systems (nips) is one
   of the top ml conferences. this post discusses highlights of nips 2016
   including gans, the nuts and bolts of ml, id56s, improvements to classic
   algorithms, rl, meta-learning, and yann lecun's infamous cake.

     * sebastian ruder
       [178]sebastian ruder

   [179]highlights of emnlp 2016: dialogue, deep learning, and more

   natural language processing

highlights of emnlp 2016: dialogue, deep learning, and more

   this post discusses highlights of the 2016 conference on empirical
   methods in natural language processing (emnlp 2016). these include work
   on id23, dialogue, sequence-to-sequence models,
   id29, id86, and many more.

     * sebastian ruder
       [180]sebastian ruder

   [181]sebastian ruder
      
   a survey of cross-lingual id27 models
   share this
   please enable javascript to view the [182]comments powered by disqus.

   [183]sebastian ruder    2019

   [184]latest posts [185]twitter [186]ghost

references

   visible links
   1. http://ruder.io/rss/
   2. http://ruder.io/
   3. http://ruder.io/about/
   4. http://ruder.io/tags/
   5. http://ruder.io/publications/
   6. http://ruder.io/talks/
   7. http://ruder.io/news/
   8. http://ruder.io/faq/
   9. http://ruder.io/nlp-news/
  10. https://nlpprogress.com/
  11. http://ruder.io/contact/
  12. http://ruder.io/tag/cross-lingual/index.html
  13. https://arxiv.org/abs/1706.04902
  14. http://ruder.io/word-embeddings-1/index.html
  15. http://ruder.io/word-embeddings-softmax/index.html
  16. http://ruder.io/word-embeddings-softmax/index.html
  17. http://arxiv.org/abs/1607.06520
  18. http://stanford.edu/~lmthang/bivec/
  19. http://ruder.io/cross-lingual-embeddings/index.html#challenges
  20. http://ruder.io/cross-lingual-embeddings/index.html#fn1
  21. http://www.statmt.org/europarl/
  22. http://ruder.io/cross-lingual-embeddings/index.html#monolingualmapping
  23. http://ruder.io/cross-lingual-embeddings/index.html#linearprojection
  24. http://ruder.io/cross-lingual-embeddings/index.html#projectionviacca
  25. http://ruder.io/cross-lingual-embeddings/index.html#normalisationandorthogonaltransformation
  26. http://ruder.io/cross-lingual-embeddings/index.html#maxmarginandintruders
  27. http://ruder.io/cross-lingual-embeddings/index.html#alignmentbasedprojection
  28. http://ruder.io/cross-lingual-embeddings/index.html#multilingualcca
  29. http://ruder.io/cross-lingual-embeddings/index.html#hybridmappingwithsymmetricseedlexicon
  30. http://ruder.io/cross-lingual-embeddings/index.html#orthogonaltransformationnormalisationandmeancentering
  31. http://ruder.io/cross-lingual-embeddings/index.html#adversarialautoencoder
  32. http://ruder.io/cross-lingual-embeddings/index.html#pseudocrosslingual
  33. http://ruder.io/cross-lingual-embeddings/index.html#mappingoftranslationstosamerepresentation
  34. http://ruder.io/cross-lingual-embeddings/index.html#randomtranslationreplacement
  35. http://ruder.io/cross-lingual-embeddings/index.html#ontheflyreplacementandpolysemyhandling
  36. http://ruder.io/cross-lingual-embeddings/index.html#multilingualcluster
  37. http://ruder.io/cross-lingual-embeddings/index.html#documentmergeandshuffle
  38. http://ruder.io/cross-lingual-embeddings/index.html#crosslingualtraining
  39. http://ruder.io/cross-lingual-embeddings/index.html#bilingualcompositionalsentencemodel
  40. http://ruder.io/cross-lingual-embeddings/index.html#bilingualbagofwordsautoencoder
  41. http://ruder.io/cross-lingual-embeddings/index.html#distributedwordalignment
  42. http://ruder.io/cross-lingual-embeddings/index.html#bilingualcompositionaldocumentmodel
  43. http://ruder.io/cross-lingual-embeddings/index.html#bagofwordsautoencoderwithcorrelation
  44. http://ruder.io/cross-lingual-embeddings/index.html#bilingualparagraphvectors
  45. http://ruder.io/cross-lingual-embeddings/index.html#translationinvariantlsa
  46. http://ruder.io/cross-lingual-embeddings/index.html#invertedindexingonwikipedia
  47. http://ruder.io/cross-lingual-embeddings/index.html#jointoptimisation
  48. http://ruder.io/cross-lingual-embeddings/index.html#multitasklanguagemodel
  49. http://ruder.io/cross-lingual-embeddings/index.html#bilingualmatrixfactorisation
  50. http://ruder.io/cross-lingual-embeddings/index.html#bilingualskipgram
  51. http://ruder.io/cross-lingual-embeddings/index.html#bilingualbagofwordswithoutwordalignments
  52. http://ruder.io/cross-lingual-embeddings/index.html#bilingualskipgramwithoutwordalignments
  53. http://ruder.io/cross-lingual-embeddings/index.html#jointmatrixfactorisation
  54. http://ruder.io/cross-lingual-embeddings/index.html#bilingualsparserepresentations
  55. http://ruder.io/cross-lingual-embeddings/index.html#bilingualparagraphvectorswithoutparalleldata
  56. http://ruder.io/cross-lingual-embeddings/index.html#incorporatingvisualinformation
  57. http://ruder.io/cross-lingual-embeddings/index.html#challenges
  58. http://ruder.io/cross-lingual-embeddings/index.html#evaluation
  59. http://ruder.io/cross-lingual-embeddings/index.html#fn2
  60. http://ruder.io/cross-lingual-embeddings/index.html#fn3
  61. http://ruder.io/cross-lingual-embeddings/index.html#fn4
  62. http://ruder.io/cross-lingual-embeddings/index.html#fn5
  63. http://ruder.io/cross-lingual-embeddings/index.html#fn6
  64. http://ruder.io/cross-lingual-embeddings/index.html#fn7
  65. http://ruder.io/cross-lingual-embeddings/index.html#fn8
  66. http://ruder.io/cross-lingual-embeddings/index.html#fn9
  67. http://ruder.io/cross-lingual-embeddings/index.html#fn10
  68. http://ruder.io/cross-lingual-embeddings/index.html#fn11
  69. http://ruder.io/cross-lingual-embeddings/index.html#fn12
  70. http://ruder.io/cross-lingual-embeddings/index.html#fn13
  71. http://ruder.io/cross-lingual-embeddings/index.html#fn14
  72. http://ruder.io/cross-lingual-embeddings/index.html#fn15
  73. http://ruder.io/cross-lingual-embeddings/index.html#fn16
  74. http://ruder.io/cross-lingual-embeddings/index.html#fn17
  75. http://ruder.io/cross-lingual-embeddings/index.html#fn18
  76. http://ruder.io/cross-lingual-embeddings/index.html#fn19
  77. http://ruder.io/cross-lingual-embeddings/index.html#fn20
  78. http://ruder.io/cross-lingual-embeddings/index.html#bilingualcompositionalsentencemodel
  79. http://ruder.io/cross-lingual-embeddings/index.html#fn21
  80. http://ruder.io/cross-lingual-embeddings/index.html#fn22
  81. http://ruder.io/cross-lingual-embeddings/index.html#fn23
  82. http://ruder.io/cross-lingual-embeddings/index.html#fn24
  83. http://ruder.io/cross-lingual-embeddings/index.html#fn25
  84. http://ruder.io/cross-lingual-embeddings/index.html#fn26
  85. http://ruder.io/cross-lingual-embeddings/index.html#fn27
  86. http://ruder.io/cross-lingual-embeddings/index.html#fn28
  87. http://ruder.io/cross-lingual-embeddings/index.html#fn29
  88. http://ruder.io/cross-lingual-embeddings/index.html#fn30
  89. http://ruder.io/cross-lingual-embeddings/index.html#fn31
  90. http://ruder.io/cross-lingual-embeddings/index.html#fn32
  91. http://ruder.io/cross-lingual-embeddings/index.html#fn33
  92. http://ruder.io/cross-lingual-embeddings/index.html#fn34
  93. http://ruder.io/cross-lingual-embeddings/index.html#fn35
  94. https://en.wikipedia.org/wiki/procrustes_analysis
  95. http://ruder.io/cross-lingual-embeddings/index.html#fn36
  96. http://ruder.io/cross-lingual-embeddings/index.html#fn37
  97. http://ruder.io/cross-lingual-embeddings/index.html#fn38
  98. http://ruder.io/cross-lingual-embeddings/index.html#multitasklanguagemodel
  99. http://ruder.io/cross-lingual-embeddings/index.html#bagofwordsautoencoderwithcorrelation
 100. http://ruder.io/cross-lingual-embeddings/index.html#bilingualcompositionaldocumentmodel
 101. http://ruder.io/cross-lingual-embeddings/index.html#distributedwordalignment
 102. http://ruder.io/cross-lingual-embeddings/index.html#bilingualbagofwordswithoutwordalignments
 103. http://ruder.io/cross-lingual-embeddings/index.html#bilingualskipgram
 104. http://ruder.io/cross-lingual-embeddings/index.html#bilingualskipgramwithoutwordalignments
 105. http://ruder.io/cross-lingual-embeddings/index.html#bilingualparagraphvectorswithoutparalleldata
 106. http://ruder.io/cross-lingual-embeddings/index.html#fn39
 107. http://128.2.220.95/multilingual
 108. https://arxiv.org/abs/1706.04902
 109. http://ruder.io/word-embeddings-1/index.html
 110. http://ruder.io/word-embeddings-softmax/index.html
 111. http://ruder.io/secret-id97/index.html
 112. http://ruder.io/word-embeddings-2017/index.html
 113. http://arxiv.org/abs/1608.05426
 114. http://ruder.io/cross-lingual-embeddings/index.html#fnref1
 115. http://arxiv.org/abs/1309.4168
 116. http://ruder.io/cross-lingual-embeddings/index.html#fnref2
 117. http://repository.cmu.edu/lti/31
 118. http://ruder.io/cross-lingual-embeddings/index.html#fnref3
 119. http://ruder.io/cross-lingual-embeddings/index.html#fnref4
 120. http://ruder.io/cross-lingual-embeddings/index.html#fnref5
 121. http://doi.org/10.1145/1390156.1390177
 122. http://ruder.io/cross-lingual-embeddings/index.html#fnref6
 123. http://www.aclweb.org/anthology/p15-1119
 124. http://ruder.io/cross-lingual-embeddings/index.html#fnref7
 125. http://arxiv.org/abs/1602.01925
 126. http://ruder.io/cross-lingual-embeddings/index.html#fnref8
 127. http://ruder.io/cross-lingual-embeddings/index.html#fnref9
 128. http://ruder.io/cross-lingual-embeddings/index.html#fnref10
 129. http://arxiv.org/pdf/1608.02996.pdf
 130. http://ruder.io/cross-lingual-embeddings/index.html#fnref11
 131. http://ruder.io/cross-lingual-embeddings/index.html#fnref12
 132. http://ruder.io/cross-lingual-embeddings/index.html#fnref13
 133. http://ruder.io/cross-lingual-embeddings/index.html#fnref14
 134. http://arxiv.org/abs/1509.07308
 135. http://ruder.io/cross-lingual-embeddings/index.html#fnref15
 136. http://ruder.io/cross-lingual-embeddings/index.html#fnref16
 137. http://arxiv.org/abs/1401.1803
 138. http://ruder.io/cross-lingual-embeddings/index.html#fnref17
 139. http://arxiv.org/abs/1405.0947
 140. http://ruder.io/cross-lingual-embeddings/index.html#fnref18
 141. http://ruder.io/cross-lingual-embeddings/index.html#fnref19
 142. http://ruder.io/cross-lingual-embeddings/index.html#fnref20
 143. http://arxiv.org/abs/1402.1454
 144. http://ruder.io/cross-lingual-embeddings/index.html#fnref21
 145. http://ruder.io/cross-lingual-embeddings/index.html#fnref22
 146. http://arxiv.org/abs/1405.4053
 147. http://ruder.io/cross-lingual-embeddings/index.html#fnref23
 148. http://ruder.io/cross-lingual-embeddings/index.html#fnref24
 149. http://ruder.io/cross-lingual-embeddings/index.html#fnref25
 150. http://ruder.io/cross-lingual-embeddings/index.html#fnref26
 151. http://doi.org/10.1162/153244303322533223
 152. http://ruder.io/cross-lingual-embeddings/index.html#fnref27
 153. http://ruder.io/cross-lingual-embeddings/index.html#fnref28
 154. http://doi.org/10.3115/v1/d14-1162
 155. http://ruder.io/cross-lingual-embeddings/index.html#fnref29
 156. http://ruder.io/cross-lingual-embeddings/index.html#fnref30
 157. http://jmlr.org/proceedings/papers/v37/gouws15.html
 158. http://ruder.io/cross-lingual-embeddings/index.html#fnref31
 159. http://ruder.io/cross-lingual-embeddings/index.html#fnref32
 160. http://ruder.io/cross-lingual-embeddings/index.html#fnref33
 161. http://ruder.io/cross-lingual-embeddings/index.html#fnref34
 162. http://www.aifb.kit.edu/images/b/b4/naacl-hlt-2016-camera-ready.pdf
 163. http://ruder.io/cross-lingual-embeddings/index.html#fnref35
 164. http://ruder.io/cross-lingual-embeddings/index.html#fnref36
 165. http://ruder.io/cross-lingual-embeddings/index.html#fnref37
 166. http://ruder.io/cross-lingual-embeddings/index.html#fnref38
 167. http://arxiv.org/abs/1604.00425
 168. http://ruder.io/cross-lingual-embeddings/index.html#fnref39
 169. http://ruder.io/author/sebastian/index.html
 170. http://ruder.io/author/sebastian/index.html
 171. http://ruder.io/author/sebastian/index.html
 172. http://ruder.io/tag/cross-lingual/index.html
 173. http://ruder.io/thesis/index.html
 174. http://ruder.io/4-biggest-open-problems-in-nlp/index.html
 175. http://ruder.io/10-exciting-ideas-of-2018-in-nlp/index.html
 176. http://ruder.io/tag/cross-lingual/index.html
 177. http://ruder.io/index.html
 178. http://ruder.io/author/sebastian/index.html
 179. http://ruder.io/index.html
 180. http://ruder.io/author/sebastian/index.html
 181. http://ruder.io/
 182. https://disqus.com/?ref_noscript
 183. http://ruder.io/
 184. http://ruder.io/
 185. https://twitter.com/seb_ruder
 186. https://ghost.org/

   hidden links:
 188. https://twitter.com/seb_ruder
 189. http://ruder.io/rss/index.rss
 190. http://ruder.io/index.html
 191. http://ruder.io/index.html
 192. https://twitter.com/share?text=a%20survey%20of%20cross-lingual%20word%20embedding%20models&url=http://ruder.io/cross-lingual-embeddings/
 193. https://www.facebook.com/sharer/sharer.php?u=http://ruder.io/cross-lingual-embeddings/
